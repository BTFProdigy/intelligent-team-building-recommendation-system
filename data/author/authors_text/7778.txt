Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 1?8, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Improving LSA-based Summarization with Anaphora Resolution
Josef Steinberger
University of West Bohemia
Univerzitni 22, Pilsen 30614,
Czech Republic
jstein@kiv.zcu.cz
Mijail A. Kabadjov
University of Essex
Wivenhoe Park, Colchester CO4 3SQ,
United Kingdom
malexa@essex.ac.uk
Massimo Poesio
University of Essex
Wivenhoe Park, Colchester CO4 3SQ,
United Kingdom
poesio@essex.ac.uk
Olivia Sanchez-Graillet
University of Essex
Wivenhoe Park, Colchester CO4 3SQ,
United Kingdom
osanch@essex.ac.uk
Abstract
We propose an approach to summarization
exploiting both lexical information and
the output of an automatic anaphoric re-
solver, and using Singular Value Decom-
position (SVD) to identify the main terms.
We demonstrate that adding anaphoric
information results in significant perfor-
mance improvements over a previously
developed system, in which only lexical
terms are used as the input to SVD. How-
ever, we also show that how anaphoric in-
formation is used is crucial: whereas using
this information to add new terms does re-
sult in improved performance, simple sub-
stitution makes the performance worse.
1 Introduction
Many approaches to summarization can be very
broadly characterized as TERM-BASED: they at-
tempt to identify the main ?topics,? which gen-
erally are TERMS, and then to extract from the
document the most important information about
these terms (Hovy and Lin, 1997). These ap-
proaches can be divided again very broadly in ?lex-
ical? approaches, among which we would include
LSA-based approaches, and ?coreference-based? ap-
proaches . Lexical approaches to term-based sum-
marization use lexical relations to identify cen-
tral terms (Barzilay and Elhadad, 1997; Gong and
Liu, 2002); coreference- (or anaphora-) based ap-
proaches (Baldwin and Morton, 1998; Boguraev and
Kennedy, 1999; Azzam et al, 1999; Bergler et al,
2003; Stuckardt, 2003) identify these terms by run-
ning a coreference- or anaphoric resolver over the
text.1 We are not aware, however, of any attempt to
use both lexical and anaphoric information to iden-
tify the main terms. In addition, to our knowledge no
authors have convincingly demonstrated that feed-
ing anaphoric information to a summarizer signif-
icantly improves the performance of a summarizer
using a standard evaluation procedure (a reference
corpus and baseline, and widely accepted evaluation
measures).
In this paper we compare two sentence extraction-
based summarizers. Both use Latent Semantic
Analysis (LSA) (Landauer, 1997) to identify the
main terms of a text for summarization; however,
the first system (Steinberger and Jezek, 2004), dis-
cussed in Section 2, only uses lexical information
to identify the main topics, whereas the second sys-
tem exploits both lexical and anaphoric information.
This second system uses an existing anaphora reso-
lution system to resolve anaphoric expressions, GUI-
TAR (Poesio and Kabadjov, 2004); but, crucially,
two different ways of using this information for
summarization were tested. (Section 3.) Both sum-
marizers were tested over the CAST corpus (Orasan
et al, 2003), as discussed in Section 4, and sig-
1The terms ?anaphora resolution? and ?coreference resolu-
tion? have been variously defined (Stuckardt, 2003), but the lat-
ter term is generally used to refer to the coreference task as de-
fined in MUC and ACE. We use the term ?anaphora resolution? to
refer to the task of identifying successive mentions of the same
discourse entity, realized via any type of noun phrase (proper
noun, definite description, or pronoun), and whether such dis-
course entities ?refer? to objects in the world or not.
1
nificant improvements were observed over both the
baseline CAST system and our previous LSA-based
summarizer.
2 An LSA-based Summarizer Using
Lexical Information Only
LSA (Landauer, 1997) is a technique for extracting
the ?hidden? dimensions of the semantic representa-
tion of terms, sentences, or documents, on the basis
of their contextual use. It is a very powerful tech-
nique already used for NLP applications such as in-
formation retrieval (Berry et al, 1995) and text seg-
mentation (Choi et al, 2001) and, more recently,
multi- and single-document summarization.
The approach to using LSA in text summariza-
tion we followed in this paper was proposed in
(Gong and Liu, 2002). Gong and Liu propose to
start by creating a term by sentences matrix A =
[A1, A2, . . . , An], where each column vector Ai rep-
resents the weighted term-frequency vector of sen-
tence i in the document under consideration. If there
are a total of m terms and n sentences in the docu-
ment, then we will have an m ? n matrix A for the
document. The next step is to apply Singular Value
Decomposition (SVD) to matrix A. Given an m? n
matrix A, the SVD of A is defined as:
(1) A = U?V T
where U = [uij ] is an m ? n column-orthonormal
matrix whose columns are called left singular vec-
tors, ? = diag(?1, ?2, . . . , ?n) is an n ? n di-
agonal matrix, whose diagonal elements are non-
negative singular values sorted in descending order,
and V = [vij ] is an n?n orthonormal matrix, whose
columns are called right singular vectors.
From a mathematical point of view, applying
SVD to a matrix derives a mapping between the m-
dimensional space spawned by the weighted term-
frequency vectors and the r-dimensional singular
vector space. From a NLP perspective, what the SVD
does is to derive the latent semantic structure of the
document represented by matrix A: a breakdown
of the original document into r linearly-independent
base vectors (?topics?). Each term and sentence from
the document is jointly indexed by these ?topics?.
A unique SVD feature is that it is capable of cap-
turing and modelling interrelationships among terms
so that it can semantically cluster terms and sen-
tences. Furthermore, as demonstrated in (Berry et
al., 1995), if a word combination pattern is salient
and recurring in document, this pattern will be cap-
tured and represented by one of the singular vec-
tors. The magnitude of the corresponding singular
value indicates the importance degree of this pattern
within the document. Any sentences containing this
word combination pattern will be projected along
this singular vector, and the sentence that best repre-
sents this pattern will have the largest index value
with this vector. As each particular word combi-
nation pattern describes a certain topic in the doc-
ument, each singular vector can be viewed as repre-
senting a salient topic of the document, and the mag-
nitude of its corresponding singular value represents
the degree of importance of the salient topic.
The summarization method proposed by Gong
and Liu (2002) should now be easy to understand.
The matrix V T describes the importance degree of
each ?implicit topic? in each sentence: the summa-
rization process simply chooses the most informa-
tive sentence for each term. In other words, the kth
sentence chosen is the one with the largest index
value in the kth right singular vector in matrix V T .
The summarization method proposed by Gong
and Liu has some disadvantages as well, the main of
which is that it is necessary to use the same number
of dimensions as is the number of sentences we want
to choose for a summary. However, the higher the
number of dimensions of reduced space is, the less
significant topic we take into a summary. In order
to remedy this problem, we (Steinberger and Jezek,
2004) proposed the following modifications to Gong
and Liu?s summarization method. After computing
the SVD of a term by sentences matrix, we compute
the length of each sentence vector in matrix V . This
is to favour the index values in the matrix V that
correspond to the highest singular values (the most
significant topics). Formally:
(2) sk =
?
?r
i=1 v2k,i ? ?2i ,
where sk is the length of the vector of k?th sentence
in the modified latent vector space, and its signif-
icance score for summarization too. The level of
dimensionality reduction (r) is essentially learned
from the data. Finally, we put into the summary the
sentences with the highest values in vector s. We
showed in previous work (Steinberger and Jezek,
2
2004) that this modification results in a significant
improvement over Gong and Liu?s method.
3 Using Anaphora Resolution for
Summarization
3.1 The case for anaphora resolution
Words are the most basic type of ?term? that can
be used to characterize the content of a document.
However, being able to identify the most important
objects mentioned in the document clearly would
lead to an improved analysis of what is important in
a text, as shown by the following news article cited
by Boguraev and Kennedy (1999):
(3) PRIEST IS CHARGED WITH POPE ATTACK
A Spanish priest was charged here today with attempt-
ing to murder the Pope. Juan Fernandez Krohn, aged
32, was arrested after a man armed with a bayonet ap-
proached the Pope while he was saying prayers at Fa-
tima on Wednesday night. According to the police, Fer-
nandez told the investigators today that he trained for
the past six months for the assault. . . . If found guilty,
the Spaniard faces a prison sentence of 15-20 years.
As Boguraev and Kennedy point out, the title of the
article is an excellent summary of the content: an en-
tity (the priest) did something to another entity (the
pope). Intuitively, understanding that Fernandez and
the pope are the central characters is crucial to pro-
vide a good summary of texts like these.2 Among
the clues that help us to identify such ?main charac-
ters?, the fact that an entity is repeatedly mentioned
is clearly important.
Purely lexical methods, including the LSA-based
methods discussed in the previous section, can only
capture part of the information about which enti-
ties are frequently repeated in the text. As exam-
ple (3) shows, stylistic conventions forbid verbatim
repetition, hence the six mentions of Fernandez in
the text above contain only one lexical repetition,
?Fernandez?. The main problem are pronouns, that
tend to share the least lexical similarity with the
form used to express the antecedent (and anyway are
usually removed by stopword lists, therefore do not
2It should be noted that for many newspaper articles, indeed
many non-educational texts, only a ?entity-centered? structure
can be clearly identified, as opposed to a ?relation-centered?
structure of the type hypothesized in Rhetorical Structures The-
ory (Knott et al, 2001; Poesio et al, 2004).
get included in the SVD matrix). The form of defi-
nite descriptions (the Spaniard) doesn?t always over-
lap with that of their antecedent, either, especially
when the antecedent was expressed with a proper
name. The form of mention which more often over-
laps to a degree with previous mentions is proper
nouns, and even then at least some way of dealing
with acronyms is necessary (cfr. European Union
/ E.U.). The motivation for anaphora resolution is
that it should tell us which entities are repeatedly
mentioned.
In this work, we tested a mixed approach to in-
tegrate anaphoric and word information: using the
output of the anaphoric resolver GUITAR to modify
the SVD matrix used to determine the sentences to
extract. In the rest of this section we first briefly in-
troduce GUITAR, then discuss the two methods we
tested to use its output to help summarization.
3.2 GUITAR: A General-Purpose Anaphoric
Resolver
The system we used in these experiments, GUITAR
(Poesio and Kabadjov, 2004), is an anaphora resolu-
tion system designed to be high precision, modular,
and usable as an off-the-shelf component of a NL
processing pipeline. The current version of the sys-
tem includes an implementation of the MARS pro-
noun resolution algorithm (Mitkov, 1998) and a par-
tial implementation of the algorithm for resolving
definite descriptions proposed by Vieira and Poe-
sio (2000). The current version of GUITAR does not
include methods for resolving proper nouns.
3.2.1 Pronoun Resolution
Mitkov (1998) developed a robust approach to
pronoun resolution which only requires input text
to be part-of-speech tagged and noun phrases to be
identified. Mitkov?s algorithm operates on the ba-
sis of antecedent-tracking preferences (referred to
hereafter as ?antecedent indicators?). The approach
works as follows: the system identifies the noun
phrases which precede the anaphor within a distance
of 2 sentences, checks them for gender and number
agreement with the anaphor, and then applies genre-
specific antecedent indicators to the remaining can-
didates (Mitkov, 1998). The noun phrase with the
highest aggregate score is proposed as antecedent.
3
3.2.2 Definite Description Resolution
The Vieira / Poesio algorithm (Vieira and Poesio,
2000) attempts to classify each definite description
as either direct anaphora, discourse-new, or bridg-
ing description. The first class includes definite de-
scriptions whose head is identical to that of their an-
tecedent, as in a house . . . the house. Discourse-
new descriptions are definite descriptions that refer
to objects not already mentioned in the text and not
related to any such object. Bridging descriptions are
all definite descriptions whose resolution depends
on knowledge of relations between objects, such as
definite descriptions that refer to an object related
to an entity already introduced in the discourse by
a relation other than identity, as in the flat . . . the
living room. The Vieira / Poesio algorithm also at-
tempts to identify the antecedents of anaphoric de-
scriptions and the anchors of bridging ones. The
current version of GUITAR incorporates an algorithm
for resolving direct anaphora derived quite directly
from Vieira / Poesio, as well as a statistical version
of the methods for detecting discourse new descrip-
tions (Poesio et al, 2005).
3.3 SVD over Lexical and Anaphoric Terms
SVD can be used to identify the ?implicit topics? or
main terms of a document not only when on the basis
of words, but also of coreference chains, or a mix-
ture of both. We tested two ways of combining these
two types of information.
3.3.1 The Substitution Method
The simplest way of integrating anaphoric in-
formation with the methods used in our earlier
work is to use anaphora resolution simply as a pre-
processing stage of the SVD input matrix creation.
Firstly, all anaphoric relations are identified by the
anaphoric resolver, and anaphoric chains are identi-
fied. Then a second document is produced, in which
all anaphoric nominal expressions are replaced by
the first element of their anaphoric chain. For exam-
ple, suppose we have the text in (4).
(4) S1: Australia?s new conservative government on
Wednesday began selling its tough deficit-slashing bud-
get, which sparked violent protests by Aborigines,
unions, students and welfare groups even before it was
announced.
S2: Two days of anti-budget street protests preceded
spending cuts officially unveiled by Treasurer Peter
Costello.
S3: ?If we don?t do it now, Australia is going to be in
deficit and debt into the next century.?
S4: As the protesters had feared, Costello revealed a
cut to the government?s Aboriginal welfare commission
among the hundreds of measures implemented to claw
back the deficit.
An ideal resolver would find 8 anaphoric chains:
Chain 1 Australia - we - Australia
Chain 2 its new conservative government (Australia?s new
conservative government) - the government
Chain 3 its tough deficit-slashing budget (Australia?s tough
deficit-slashing budget) - it
Chain 4 violent protests by Aborigines, unions, students and
welfare groups - anti-budget street protests
Chain 5 Aborigines, unions, students and welfare groups - the
protesters
Chain 6 spending cuts - it - the hundreds of measures imple-
mented to claw back the deficit
Chain 7 Treasurer Peter Costello - Costello
Chain 8 deficit - the deficit
By replacing each element of the 8 chains above
in the text in (4) with the first element of the chain,
we get the text in (5).
(5) S1: Australia?s new conservative government on
Wednesday began selling Australia?s tough deficit-
slashing budget, which sparked violent protests by Abo-
rigines, unions, students and welfare groups even be-
fore Australia?s tough deficit-slashing budget was an-
nounced.
S2: Two days of violent protests by Aborigines, unions,
students and welfare groups preceded spending cuts of-
ficially unveiled by Treasurer Peter Costello.
S3: ?If Australia doesn?t do spending cuts now, Aus-
tralia is going to be in deficit and debt into the next
century.?
S4: As Aborigines, unions, students and welfare
groups had feared, Treasurer Peter Costello revealed a
cut to Australia?s new conservative government?s Abo-
riginal welfare commission among the spending cuts.
This text is then used to create the SVD input matrix,
as done in the first system.
3.3.2 The Addition Method
An alternative approach is to use SVD to identify
?topics? on the basis of two types of ?terms?: terms in
the lexical sense (i.e., words) and terms in the sense
of objects, which can be represented by anaphoric
4
chains. In other words, our representation of sen-
tences would specify not only if they contain a cer-
tain word, but also if they contain a mention of a
discourse entity (See Figure 1.) This matrix would
then be used as input to SVD.
Figure 1: Addition method.
The chain ?terms? tie together sentences that con-
tain the same anaphoric chain. If the terms are
lexically the same (direct anaphors - like deficit
and the deficit) the basic summarizer works suffi-
ciently. However, Gong and Liu showed that the best
weighting scheme is boolean (i.e., all terms have the
same weight); our own previous results confirmed
this. The advantage of the addition method is the
opportunity to give higher weights to anaphors.
4 Evaluation
4.1 The CAST Corpus
To evaluate our system, we used the corpus of
manually produced summaries created by the CAST
project3 (Orasan et al, 2003). The CAST cor-
pus contains news articles taken from the Reuters
Corpus and a few popular science texts from the
British National Corpus. It contains information
about the importance of the sentences (Hasler et
al., 2003). Sentences are marked as essential or im-
portant. The corpus also contains annotations for
3The goal of this project was to investigate to what extent
Computer-Aided Summarization can help humans to produce
high quality summaries with less effort.
linked sentences, which are not significant enough
to be marked as important/essential, but which have
to be considered as they contain information essen-
tial for the understanding of the content of other sen-
tences marked as essential/important.
Four annotators were used for the annotation,
three graduate students and one postgraduate. Three
of the annotators were native English speakers, and
the fourth had advanced knowledge of English. Un-
fortunately, not all of the documents were annotated
by all of the annotators. To maximize the reliability
of the summaries used for evaluation, we chose the
documents annotated by the greatest number of the
annotators; in total, our evaluation corpus contained
37 documents.
For acquiring manual summaries at specified
lengths and getting the sentence scores (for relative
utility evaluation) we assigned a score 3 to the sen-
tences marked as essential, a score 2 to important
sentences and a score 1 to linked sentences. The
sentences with highest scores are then selected for
ideal summary (at specified lenght).
4.2 Evaluation Measures
Evaluating summarization is a notoriously hard
problem, for which standard measures like Preci-
sion and Recall are not very appropriate. The main
problem with P&R is that human judges often dis-
agree what are the top n% most important sentences
in a document. Using P&R creates the possibility
that two equally good extracts are judged very dif-
ferently. Suppose that a manual summary contains
sentences [1 2] from a document. Suppose also that
two systems, A and B, produce summaries consist-
ing of sentences [1 2] and [1 3], respectively. Us-
ing P&R, system A will be ranked much higher than
system B. It is quite possible that sentences 2 and 3
are equally important, in which case the two systems
should get the same score.
To address the problem with precision and recall
we used a combination of evaluation measures. The
first of these, relative utility (RU) (Radev et al,
2000) allows model summaries to consist of sen-
tences with variable ranking. With RU, the model
summary represents all sentences of the input doc-
ument with confidence values for their inclusion in
the summary. For example, a document with five
sentences [1 2 3 4 5] is represented as [1/5 2/4 3/4
5
Evaluation Lexical LSA Manual Manual
Method Substitution Additition
Relative Utility 0.595 0.573 0.662
F-score 0.420 0.410 0.489
Cosine Similarity 0.774 0.806 0.823
Main Topic Similarity 0.686 0.682 0.747
Table 1: Evaluation of the manual annotation improvement - summarization ratio: 15%.
Evaluation Lexical LSA Manual Manual
Method Substitution Addition
Relative Utility 0.645 0.662 0.688
F-score 0.557 0.549 0.583
Cosine Similarity 0.863 0.878 0.886
Main Topic Similarity 0.836 0.829 0.866
Table 2: Evaluation of the manual annotation improvement - summarization ratio: 30%.
4/1 5/2]. The second number in each pair indicates
the degree to which the given sentence should be
part of the summary according to a human judge.
This number is called the utility of the sentence.
Utility depends on the input document, the summary
length, and the judge. In the example, the system
that selects sentences [1 2] will not get a higher score
than a system that chooses sentences [1 3] given
that both summaries [1 2] and [1 3] carry the same
number of utility points (5+4). Given that no other
combination of two sentences carries a higher util-
ity, both systems [1 2] and [1 3] produce optimal
extracts. To compute relative utility, a number of
judges, (N ? 1) are asked to assign utility scores to
all n sentences in a document. The top e sentences
according to utility score4 are then called a sentence
extract of size e. We can then define the following
system performance metric:
(6) RU =
?n
j=1 ?j
?N
i=1 uij
?n
j=1 ?j
?N
i=1 uij
,
where uij is a utility score of sentence j from anno-
tator i, ?j is 1 for the top e sentences according to the
sum of utility scores from all judges and ?j is equal
to 1 for the top e sentences extracted by the system.
For details see (Radev et al, 2000).
The second measure we used is Cosine Similarity,
according to the standard formula:
(7) cos(X,Y ) =
?
i xi?yi
?
?
i(xi)2?
?
?
i(yi)2
,
4In the case of ties, some arbitrary but consistent mecha-
nism is used to decide which sentences should be included in
the summary.
where X and Y are representations of a system sum-
mary and its reference summary based on the vector
space model. The third measure is Main Topic Sim-
ilarity. This is a content-based evaluation method
based on measuring the cosine of the angle between
first left singular vectors of a system summary?s
and its reference summary?s SVDs. (For details see
(Steinberger and Jezek, 2004).) Finally, we mea-
sured ROUGE scores, with the same settings as in the
Document Understanding Conference (DUC) 2004.
4.3 How Much May Anaphora Resolution
Help? An Upper Bound
We annotated all the anaphoric relations in the 37
documents in our evaluation corpus by hand us-
ing the annotation tool MMAX (Mueller and Strube,
2003).5 Apart from measuring the performance of
GUITAR over the corpus, this allowed us to establish
the upper bound on the performance improvements
that could be obtained by adding an anaphoric re-
solver to our summarizer. We tested both methods
of adding the anaphoric knowledge to the summa-
rizer discussed above. Results for the 15% and 30%
ratios6 are presented in Tables 1 and 2. The baseline
is our own previously developed LSA-based sum-
marizer without anaphoric knowledge. The result
is that the substitution method did not lead to sig-
nificant improvement, but the addition method did:
5We annotated personal pronouns, possessive pronouns, def-
inite descriptions and also proper nouns, who will be handled by
a future GUITAR version.
6We used the same summarization ratios as in CAST.
6
Evaluation Lexical LSA CAST GUITAR GUITAR
Method Substitution Addition
Relative Utility 0.595 0.527 0.530 0.640
F-score 0.420 0.348 0.347 0.441
Cosine Similarity 0.774 0.726 0.804 0.805
Main Topic Similarity 0.686 0.630 0.643 0.699
Table 3: Evaluation of the GUITAR improvement - summarization ratio: 15%.
Evaluation Lexical LSA CAST GUITAR GUITAR
Method Substitution Addittion
Relative Utility 0.645 0.618 0.626 0.678
F-score 0.557 0.522 0.524 0.573
Cosine Similarity 0.863 0.855 0.873 0.879
Main Topic Similarity 0.836 0.810 0.818 0.868
Table 4: Evaluation of the GUITAR improvement - summarization ratio: 30%.
addition could lead to an improvement in Relative
Utility score from .595 to .662 for the 15% ratio, and
from .645 to .688 for the 30% ratio. Both of these
improvements were significant by t-test at 95% con-
fidence.
4.4 Results with GUITAR
To use GUITAR, we first parsed the texts using Char-
niak?s parser (Charniak, 2000). The output of the
parser was then converted into the MAS-XML for-
mat expected by GUITAR by one of the preproces-
sors that come with the system. (This step includes
heuristic methods for guessing agreement features.)
Finally, GUITAR was ran to add anaphoric infor-
mation to the files. The resulting files were then
processed by the summarizer.
GUITAR achieved a precision of 56% and a recall
of 51% over the 37 documents. For definite descrip-
tion resolution, we found a precision of 69% and
a recall of 53%; for possessive pronoun resolution,
the precision was 53%, recall was 53%; for personal
pronouns, the precision was 44%, recall was 46%.
The results with the summarizer are presented
in Tables 3 and 4 (relative utility, f-score, cosine,
and main topic). The contribution of the differ-
ent anaphora resolution components is addressed in
(Kabadjov et al, 2005). All versions of our summa-
rizer (the baseline version without anaphora resolu-
tion and those using substitution and addition) out-
performed the CAST summarizer, but we have to em-
phasize that CAST did not aim at producing a high-
performance generic summarizer; only a system that
could be easily used for didactical purposes. How-
ever, our tables also show that using GUITAR and the
addition method lead to significant improvements
over our baseline LSA summarizer. The improve-
ment in Relative Utility measure was significant by
t-test at 95% confidence. Using the ROUGE mea-
sure we obtained improvement (but not significant).
On the other hand, the substitution method did not
lead to significant improvements, as was to be ex-
pected given that no improvement was obtained with
?perfect? anaphora resolution (see previous section).
5 Conclusion and Further Research
Our main result in this paper is to show that using
anaphora resolution in summarization can lead to
significant improvements, not only when ?perfect?
anaphora information is available, but also when
an automatic resolver is used, provided that the
anaphoric resolver has reasonable performance. As
far as we are aware, this is the first time that such
a result has been obtained using standard evaluation
measures over a reference corpus. We also showed
however that the way in which anaphoric informa-
tion is used matters: with our set of documents at
least, substitution would not result in significant im-
provements even with perfect anaphoric knowledge.
Further work will include, in addition to extend-
ing the set of documents and testing the system with
other collections, evaluating the improvement to be
achieved by adding a proper noun resolution algo-
rithm to GUITAR.
7
References
S. Azzam, K. Humphreys and R. Gaizauskas. 1999. Using
coreference chains for text summarization. In Proceedings
of the ACL Workshop on Coreference. Maryland.
B. Baldwin and T. S. Morton. 1998. Dynamic coreference-
based summarization. In Proceedings of EMNLP. Granada,
Spain.
R. Barzilay and M. Elhadad. 1997. Using lexical chains for text
summarization. In Proceedings of the ACL/EACL Workshop
on Intelligent Scalable Text Summarization. Madrid, Spain.
S. Bergler, R. Witte, M. Khalife, Z. Li, and F. Rudzicz.
2003. Using Knowledge-poor Coreference Resolution for
Text Summarization. In Proceedings of DUC. Edmonton.
M. W. Berry, S. T. Dumais and G. W. O?Brien. 1995. Using
Linear Algebra for Intelligent IR. In SIAM Review, 37(4).
B. Boguraev and C. Kennedy. 1999. Salience-based content
characterization of text documents. In I. Mani and M. T.
Maybury (eds), Advances in Automatic Text Summarization,
MIT Press. Cambridge, MA.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proceedings of NAACL. Philadelphia.
F. Y. Y. Choi, P. Wiemer-Hastings and J. D. Moore. 2001. La-
tent Semantic Analysis for Text Segmentation. In Proceed-
ings of EMNLP. Pittsburgh.
Y. Gong and X. Liu. 2002. Generic Text Summarization Us-
ing Relevance Measure and Latent Semantic Analysis. In
Proceedings of ACM SIGIR. New Orleans.
L. Hasler, C. Orasan and R. Mitkov. 2003. Building better
corpora for summarization. In Proceedings of Corpus Lin-
guistics. Lancaster, United Kingdom.
E. Hovy and C. Lin. 1997. Automated text summarization in
SUMMARIST. In ACL/EACL Workshop on Intelligent Scal-
able Text Summarization. Madrid, Spain.
M. A. Kabadjov, M. Poesio and J. Steinberger. 2005. Task-
Based Evaluation of Anaphora Resolution: The Case of
Summarization. In RANLP Workshop ?Crossing Barriers
in Text Summarization Research?. Borovets, Bulgaria.
A. Knott, J. Oberlander, M. O?Donnell, and C. Mellish. 2001.
Beyond elaboration: The interaction of relations and focus in
coherent text. In Sanders, T., Schilperoord, J., and Spooren,
W. (eds), Text representation: linguistic and psycholinguistic
aspects. John Benjamins.
T. K. Landauer and S. T. Dumais. 1997. A solution to Plato?s
problem: The latent semantic analysis theory of the acqui-
sition, induction, and representation of knowledge. In Psy-
chological Review, 104, 211-240.
R. Mitkov. 1998. Robust pronoun resolution with limited
knowledge. In Proceedings of COLING. Montreal.
C. Mueller and M. Strube. 2001. MMAX: A Tool for the Anno-
tation of Multi-modal Corpora. In Proceedings of the IJCAI
Workshop on Knowledge and Reasoning in Practical Dia-
logue Systems. Seattle.
C. Orasan, R. Mitkov and L. Hasler. 2003. CAST: a Computer-
Aided Summarization Tool. In Proceedings of EACL. Bu-
dapest, Hungary.
M. Poesio and M. A. Kabadjov. 2004. A General-Purpose, off-
the-shelf Anaphora Resolution Module: Implementation and
Preliminary Evaluation. In Proceedings of LREC. Lisbon,
Portugal.
M. Poesio, R. Stevenson, B. Di Eugenio, and J. M. Hitzeman.
2004. Centering: A parametric theory and its instantiations.
Computational Linguistics, 30(3).
M. Poesio, M. A. Kabadjov, R. Vieira, R. Goulart, and
O. Uryupina. 2005. Do discourse-new detectors help def-
inite description resolution? In Proceedings of IWCS.
Tilburg, The Netherlands.
D. R. Radev, H. Jing, and M. Budzikowska. 2000.
Centroid-based summarization of multiple documents. In
ANLP/NAACL Workshop on Automatic Summarization.
Seattle.
J. Steinberger and K. Jezek. 2004. Text Summarization and
Singular Value Decomposition. In Proceedings of ADVIS.
Izmir, Turkey.
R. Stuckardt. 2003. Coreference-Based Summarization and
Question Answering: a Case for High Precision Anaphor
Resolution. In International Symposium on Reference Reso-
lution. Venice, Italy.
R. Vieira and M. Poesio. 2000. An empirically-based system
for processing definite descriptions. In Computational Lin-
guistics, 26(4).
8
Proceedings of the ACL 2010 Conference Short Papers, pages 382?386,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Wrapping up a Summary:
from Representation to Generation
Josef Steinberger and Marco Turchi and
Mijail Kabadjov and Ralf Steinberger
EC Joint Research Centre
21027, Ispra (VA), Italy
{Josef.Steinberger, Marco.Turchi,
Mijail.Kabadjov, Ralf.Steinberger}
@jrc.ec.europa.eu
Nello Cristianini
University of Bristol,
Bristol, BS8 1UB, UK
nello@support-vector.net
Abstract
The main focus of this work is to investi-
gate robust ways for generating summaries
from summary representations without re-
curring to simple sentence extraction and
aiming at more human-like summaries.
This is motivated by empirical evidence
from TAC 2009 data showing that human
summaries contain on average more and
shorter sentences than the system sum-
maries. We report encouraging prelimi-
nary results comparable to those attained
by participating systems at TAC 2009.
1 Introduction
In this paper we adopt the general framework
for summarization put forward by Spa?rck-Jones
(1999) ? which views summarization as a three-
fold process: interpretation, transformation and
generation ? and attempt to provide a clean in-
stantiation for each processing phase, with a par-
ticular emphasis on the last, summary-generation
phase often omitted or over-simplified in the main-
stream work on summarization.
The advantages of looking at the summarization
problem in terms of distinct processing phases are
numerous. It not only serves as a common ground
for comparing different systems and understand-
ing better the underlying logic and assumptions,
but it also provides a neat framework for devel-
oping systems based on clean and extendable de-
signs. For instance, Gong and Liu (2002) pro-
posed a method based on Latent Semantic Anal-
ysis (LSA) and later J. Steinberger et al (2007)
showed that solely by enhancing the first source
interpretation phase, one is already able to pro-
duce better summaries.
There has been limited work on the last sum-
mary generation phase due to the fact that it is
unarguably a very challenging problem. The vast
amount of approaches assume simple sentence se-
lection, a type of extractive summarization, where
often the summary representation and the end
summary are, indeed, conflated.
The main focus of this work is, thus, to in-
vestigate robust ways for generating summaries
from summary representations without recurring
to simple sentence extraction and aiming at more
human-like summaries. This decision is also mo-
tivated by empirical evidence from TAC 2009 data
(see table 1) showing that human summaries con-
tain on average more and shorter sentences than
the system summaries. The intuition behind this is
that, by containing more sentences, a summary is
able to capture more of the important content from
the source.
Our initial experimental results show that our
approach is feasible, since it produces summaries,
which when evaluated against the TAC 2009 data1
yield ROUGE scores (Lin and Hovy, 2003) com-
parable to the participating systems in the Sum-
marization task at TAC 2009. Taking into account
that our approach is completely unsupervised and
language-independent, we find our preliminary re-
sults encouraging.
The remainder of the paper is organised as fol-
lows: in the next section we briefly survey the
related work, in ?3 we describe our approach to
summarization, in ?4 we explain how we tackle
the generation step, in ?5 we present and discuss
our experimental results and towards the end we
conclude and give pointers to future work.
2 Related Work
There is a large body of literature on summariza-
tion (Hovy, 2005; Erkan and Radev, 2004; Kupiec
et al, 1995). The most closely related work to the
approach presented hereby is work on summariza-
tion attempting to go beyond simple sentence ex-
1http://www.nist.gov/tac/
382
traction and to a lesser degree work on sentence
compression. We survey below work along these
lines.
Although our approach is related to sentence
compression (Knight and Marcu, 2002; Clarke
and Lapata, 2008), it is subtly different. Firstly, we
reduce the number of terms to be used in the sum-
mary at a global level, not at a local per-sentence
level. Secondly, we directly exploit the resulting
structures from the SVD making the last genera-
tion step fully aware of previous processing stages,
as opposed to tackling the problem of sentence
compression in isolation.
A similar approach to our sentence reconstruc-
tion method has been developed by Quirk et al
(2004) for paraphrase generation. In their work,
training and test sets contain sentence pairs that
are composed of two different proper English sen-
tences and a paraphrase of a source sentence is
generated by finding the optimal path through a
paraphrases lattice.
Finally, it is worth mentioning that we are aware
of the ?capsule overview? summaries proposed by
Boguraev and Kennedy (1997) which is similar to
our TSR (see below), however, as opposed to their
emphasis on a suitable browsing interface rather
than producing a readable summary, we precisely
attempt the latter.
3 Three-fold Summarization:
Interpretation, Transformation and
Generation
We chose the LSA paradigm for summarization,
since it provides a clear and direct instantiation of
Spa?rck-Jones? three-stage framework.
In LSA-based summarization the interpreta-
tion phase takes the form of building a term-by-
sentence matrix A = [A1, A2, . . . , An], where
each column Aj = [a1j , a2j , . . . , anj ]T represents
the weighted term-frequency vector of sentence j
in a given set of documents. We adopt the same
weighting scheme as the one described in (Stein-
berger et al, 2007), as well as their more general
definition of term entailing not only unigrams and
bigrams, but also named entities.
The transformation phase is done by applying
singular value decomposition (SVD) to the initial
term-by-sentence matrix defined as A = U?V T .
The generation phase is where our main contri-
bution comes in. At this point we depart from stan-
dard LSA-based approaches and aim at produc-
ing a succinct summary representation comprised
only of salient terms ? Term Summary Represen-
tation (TSR). Then this TSR is passed on to an-
other module which attempts to produce complete
sentences. The module for sentence reconstruc-
tion is described in detail in section 4, in what fol-
lows we explain the method for producing a TSR.
3.1 Term Summary Representation
To explain how a term summary representation
(TSR) is produced, we first need to define two con-
cepts: salience score of a given term and salience
threshold. Salience score for each term in matrix
A is given by the magnitude of the corresponding
vector in the matrix resulting from the dot product
of the matrix of left singular vectors with the diag-
onal matrix of singular values. More formally, let
T = U ? ? and then for each term i, the salience
score is given by |~Ti|. Salience threshold is equal
to the salience score of the top kth term, when all
terms are sorted in descending order on the basis
of their salience scores and a cutoff is defined as a
percentage (e.g., top 15%). In other words, if the
total number of terms is n, then 100?k/n must be
equal to the percentage cutoff specified.
The generation of a TSR is performed in two
steps. First, an initial pool of sentences is selected
by using the same technique as in (Steinberger and
Jez?ek, 2009) which exploits the dot product of the
diagonal matrix of singular values with the right
singular vectors: ? ? V T .2 This initial pool of sen-
tences is the output of standard LSA approaches.
Second, the terms from the source matrix A are
identified in the initial pool of sentences and those
terms whose salience score is above the salience
threshold are copied across to the TSR. Thus, the
TSR is formed by the most (globally) salient terms
from each one of the sentences. For example:
? Extracted Sentence: ?Irish Prime Minister Bertie
Ahern admitted on Tuesday that he had held a series of
private one-on-one meetings on the Northern Ireland
peace process with Sinn Fein leader Gerry Adams, but
denied they had been secret in any way.?
? TSR Sentence at 10%: ?Irish Prime Minister
Bertie Ahern Tuesday had held one-on-one meetings
Northern Ireland peace process Sinn Fein leader Gerry
Adams?3
2Due to space constraints, full details on that step are
omitted here, see (Steinberger and Jez?ek, 2009).
3The TSR sentence is stemmed just before feeding it to
the reconstruction module discussed in the next section.
383
Average Human System At 100% At 15% At 10% At 5% At 1%
number of: Summaries Summaries
Sentences/summary 6.17 3.82 3.8 3.95 4.39 5.18 12.58
Words/sentence 15.96 25.01 26.24 25.1 22.61 19.08 7.55
Words/summary 98.46 95.59 99.59 99.25 99.18 98.86 94.96
Table 1: Summary statistics on TAC?09 data (initial summaries).
Metric LSAextract At 100% At 15% At 10% At 5% At 1%
ROUGE-1 0.371 0.361 0.362 0.365 0.372 0.298
ROUGE-2 0.096 0.08 0.081 0.083 0.083 0.083
ROUGE-SU4 0.131 0.125 0.126 0.128 0.131 0.104
Table 2: Summarization results on TAC?09 data (initial summaries).
4 Noisy-channel model for sentence
reconstruction
This section describes a probabilistic approach to
the reconstruction problem. We adopt the noisy-
channel framework that has been widely used in a
number of other NLP applications. Our interpre-
tation of the noisy channel consists of looking at a
stemmed string without stopwords and imagining
that it was originally a long string and that some-
one removed or stemmed some text from it. In our
framework, reconstruction consists of identifying
the original long string.
To model our interpretation of the noisy chan-
nel, we make use of one of the most popular
classes of SMT systems: the Phrase Based Model
(PBM) (Zens et al, 2002; Och and Ney, 2001;
Koehn et al, 2003). It is an extension of the noisy-
channel model and was introduced by Brown et al
(1994), using phrases rather than words. In PBM,
a source sentence f is segmented into a sequence
of I phrases f I = [f1, f2, . . . fI ] and the same is
done for the target sentence e, where the notion of
phrase is not related to any grammatical assump-
tion; a phrase is an n-gram. The best translation
ebest of f is obtained by:
ebest = argmaxe p(e|f) = argmaxe
I?
i=1
?(fi|ei)
??
d(ai ? bi?1)
?d
|e|?
i=1
pLM (ei|e1 . . . ei?1)
?LM
where ?(fi|ei) is the probability of translating
a phrase ei into a phrase fi. d(ai ? bi?1) is
the distance-based reordering model that drives
the system to penalize substantial reorderings of
words during translation, while still allowing some
flexibility. In the reordering model, ai denotes the
start position of the source phrase that was trans-
lated into the ith target phrase, and bi?1 denotes
the end position of the source phrase translated
into the (i?1th) target phrase. pLM (ei|e1 . . . ei?1)
is the language model probability that is based on
the Markov chain assumption. It assigns a higher
probability to fluent/grammatical sentences. ??,
?LM and ?d are used to give a different weight to
each element (for more details see (Koehn et al,
2003)).
In our reconstruction problem, the difference
between the source and target sentences is not in
terms of languages, but in terms of forms. In fact,
our source sentence f is a stemmed sentence with-
out stopwords, while the target sentence e is a
complete English sentence. ?Translate? means to
reconstruct the most probable sentence e given f
inserting new words and reproducing the inflected
surface forms of the source words.
4.1 Training of the model
In Statistical Machine Translation, a PBM system
is trained using parallel sentences, where each sen-
tence in a language is paired with another sentence
in a different language and one is the translation of
the other.
In the reconstruction problem, we use a set, S1
of 2,487,414 English sentences extracted from the
news. This set is duplicated, S2, and for each sen-
tence in S2, stopwords are removed and the re-
maining words are stemmed using Porter?s stem-
mer (Porter, 1980). Our stopword list contains 488
words. Verbs are not included in this list, because
they are relevant for the reconstruction task. To
optimize the lambda parameters, we select 2,000
pairs as development set.
384
An example of training sentence pair is:
? Source Sentence: ?royal mail ha doubl profit 321
million huge fall number letter post?
? Target Sentence: ?royal mail has doubled its prof-
its to 321 million despite a huge fall in the number of
letters being posted?
In this work we use Moses (Koehn et al, 2007),
a complete phrase-based translation toolkit for
academic purposes. It provides all the state-of-the-
art components needed to create a phrase-based
machine translation system. It contains different
modules to preprocess data, train the Language
Models and the Translation Models.
5 Experimental Results
For our experiments we made use of the TAC
2009 data which conveniently contains human-
produced summaries against which we could eval-
uate the output of our system (NIST, 2009).
To begin our inquiry we carried out a phase
of exploratory data analysis, in which we mea-
sured the average number of sentences per sum-
mary, words per sentence and words per summary
in human vs. system summaries in the TAC 2009
data. Additionally, we also measured these statis-
tics of summaries produced by our system at five
different percentage cutoffs: 100%, 15%, 10%,
5% and 1%. 4 The results from this exploration
are summarised in table 1. The most notable thing
is that human summaries contain on average more
and shorter sentences than the system summaries
(see 2nd and 3rd column from left to right). Sec-
ondly, we note that as the percentage cutoff de-
creases (from 4th column rightwards) the charac-
teristics of the summaries produced by our system
are increasingly more similar to those of the hu-
man summaries. In other words, within the 100-
word window imposed by the TAC guidelines, our
system is able to fit more (and hence shorter) sen-
tences as we decrease the percentage cutoff.
Summarization performance results are shown
in table 2. We used the standard ROUGE evalu-
ation (Lin and Hovy, 2003) which has been also
used for TAC. We include the usual ROUGE met-
rics: R1 is the maximum number of co-occurring
unigrams, R2 is the maximum number of co-
occurring bigrams and RSU4 is the skip bigram
measure with the addition of unigrams as counting
4Recall from section ?3 that the salience threshold is a
function of the percentage cutoff.
unit. The last five columns of table 2 (from left to
right) correspond to summaries produced by our
system at various percentage cutoffs. The 2nd col-
umn, LSAextract, corresponds to the performance
of our system at producing summaries by sentence
extraction only.5
In the light of the above, the decrease in per-
formance from column LSAextract to column ?At
100%? can be regarded as reconstruction error.6
Then, as we decrease the percentage cutoff (from
4th column rightwards) we are increasingly cover-
ing more of the content comprised by the human
summaries (as far as the ROUGE metrics are able
to gauge this, of course). In other words, the im-
provement of content coverage makes up for the
reconstruction error, and at 5% cutoff we already
obtain ROUGE scores comparable to LSAextract.
This suggests that if we improve the quality of our
sentence reconstruction we would potentially end
up with a better performing system than a typical
LSA system based on sentence selection. Hence,
we find these results very encouraging.
Finally, we admittedly note that by applying a
percentage cutoff on the initial term set and further
performing the sentence reconstruction we gain in
content coverage, to a certain extent, on the ex-
pense of sentence readability.
6 Conclusion
In this paper we proposed a novel approach to
summary generation from summary representa-
tion based on the LSA summarization framework
and on a machine-translation-inspired technique
for sentence reconstruction.
Our preliminary results show that our approach
is feasible, since it produces summaries which re-
semble better human summaries in terms of the av-
erage number of sentences per summary and yield
ROUGE scores comparable to the participating
systems in the Summarization task at TAC 2009.
Bearing in mind that our approach is completely
unsupervised and language-independent, we find
our results promising.
In future work we plan on working towards im-
proving the quality of our sentence reconstruction
step in order to produce better and more readable
sentences.
5These are, effectively, what we called initial pool of sen-
tences in section 3, before the TSR generation.
6The only difference between the two types of summaries
is the reconstruction step, since we are including 100% of the
terms.
385
References
B. Boguraev and C. Kennedy. 1997. Salience-
based content characterisation of text documents. In
I. Mani, editor, Proceedings of the Workshop on In-
telligent and Scalable Text Summarization at the An-
nual Joint Meeting of the ACL/EACL, Madrid.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1994. The mathematic of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19(2):263?311.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Re-
search, 31:273?318.
G. Erkan and D. Radev. 2004. LexRank: Graph-based
centrality as salience in text summarization. Journal
of Artificial Intelligence Research (JAIR).
Y. Gong and X. Liu. 2002. Generic text summarization
using relevance measure and latent semantic analy-
sis. In Proceedings of ACM SIGIR, New Orleans,
US.
E. Hovy. 2005. Automated text summarization. In
Ruslan Mitkov, editor, The Oxford Handbook of
Computational Linguistics, pages 583?598. Oxford
University Press, Oxford, UK.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1):91?107.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL
?03, pages 48?54, Morristown, NJ, USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of ACL ?07, demonstration session.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable
document summarizer. In Proceedings of the ACM
SIGIR, pages 68?73, Seattle, Washington.
C. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, Edmonton, Canada.
NIST, editor. 2009. Proceeding of the Text Analysis
Conference, Gaithersburg, MD, November.
F. Och and H. Ney. 2001. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of ACL ?02, pages
295?302, Morristown, NJ, USA.
M. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation.
In Proceedings of EMNLP, volume 149. Barcelona,
Spain.
K. Spa?rck-Jones. 1999. Automatic summarising: Fac-
tors and directions. In I. Mani and M. Maybury,
editors, Advances in Automatic Text Summarization.
MIT Press.
J. Steinberger and K. Jez?ek. 2009. Update summariza-
tion based on novel topic distribution. In Proceed-
ings of the 9th ACM DocEng, Munich, Germany.
J. Steinberger, M. Poesio, M. Kabadjov, and K. Jez?ek.
2007. Two uses of anaphora resolution in summa-
rization. Information Processing and Management,
43(6):1663?1680. Special Issue on Text Summari-
sation (Donna Harman, ed.).
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Proceedings of KI
?02, pages 18?32, London, UK. Springer-Verlag.
386
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 817?822,
Dublin, Ireland, August 23-24, 2014.
UWB: Machine Learning Approach to Aspect-Based Sentiment Analysis
Tom
?
a
?
s Brychc??n
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plze?n
Czech Republic
brychcin@kiv.zcu.cz
Michal Konkol
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plze?n
Czech Republic
konkol@kiv.zcu.cz
Josef Steinberger
Department of Computer
Science and Engineering,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plze?n
Czech Republic
jstein@kiv.zcu.cz
Abstract
This paper describes our system partici-
pating in the aspect-based sentiment anal-
ysis task of Semeval 2014. The goal
was to identify the aspects of given tar-
get entities and the sentiment expressed to-
wards each aspect. We firstly introduce
a system based on supervised machine
learning, which is strictly constrained and
uses the training data as the only source
of information. This system is then ex-
tended by unsupervised methods for latent
semantics discovery (LDA and semantic
spaces) as well as the approach based on
sentiment vocabularies. The evaluation
was done on two domains, restaurants and
laptops. We show that our approach leads
to very promising results.
1 Introduction
The majority of current sentiment analysis ap-
proaches tries to detect the overall polarity of a
sentence (or a document) regardless of the target
entities (e.g. restaurants) and their aspects (e.g.
food, price). By contrast, the ABSA (aspect based
sentiment analysis) task is concerned with identi-
fying the aspects of given target entities and esti-
mating the sentiment polarity for each mentioned
aspect.
The aspect scenario can be decomposed into
two tasks: aspect extraction and aspect sentiment
classification (Liu, 2012).
The task of aspect extraction is to recognize
aspects of the entity and more generally can be
seen as an information extraction task. The ba-
sic approach is finding frequent nouns and noun
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails: http://creativecommons.org/licenses/
by/4.0/
phrases (Liu et al., 2005; Blair-Goldensohn et
al., 2008; Moghaddam and Ester, 2010; Long et
al., 2010). Aspect extraction can be also seen as
a special case of the general information extrac-
tion problem. The most dominant methods are
based on sequential learning (e.g. HMM ? Hidden
Markov Models (Rabiner, 2010) or CRF ? Condi-
tional Random Fields (Lafferty et al., 2001)). An-
other group of methods use topic models (Mei et
al., 2007; Titov and McDonald, 2008; Blei et al.,
2003).
Aspect sentiment classification determines
whether the opinions on different aspects are
positive, negative, or neutral. While lexicon-based
approaches use a list of aspect-related sentiment
phrases as the core resource (Ding et al., 2008; Hu
and Liu, 2004), the key issue for learning methods
is to determine the scope of each sentiment
expression, i.e., whether it covers the aspect in
the sentence (Jiang et al., 2011; Boiy and Moens,
2009).
The most of the research in aspect-level senti-
ment analysis has been done in English, however,
there were some attempts to tackle the aspect-level
task in other languages (e.g. in Czech (Steinberger
et al., 2014)).
The rest of the article is organized as follows.
In Section 2, we summarize the ABSA shared task
(Pontiki et al., 2014). Then, we give a description
of our participating system (Section 3). In Section
4, we discuss our results in the task. We partic-
ipated with both the constrained and the uncon-
strained variants of the system.
2 The ABSA task
Datasets consisting of customer reviews with
human-authored annotations identifying the men-
tioned aspects of the target entities and the senti-
ment polarity of each aspect were provided. The
experiments were run in two domains: restaurant
and laptop reviews.
817
Each team could submit two versions of sys-
tems ? constrained and unconstrained. The con-
strained system uses only the training data and
other resources (such as lexicons) for training. The
unconstrained system can use additional data.
We use another definition of these types, which
is not against the rules. Our constrained systems
are based purely on ABSA training data, with-
out any external knowledge such as dictionaries or
rules. Our unconstrained systems use additional
dictionaries, rule-based extensions and unlabeled
data. From our point of view, hand-crafted dictio-
naries and rules are external knowledge and thus it
is the same as adding external data.
The task consists of the four subtasks.
2.1 Subtask 1: Aspect term extraction
Given a set of sentences with pre-identified enti-
ties (restaurants or laptops), the task is to identify
the aspect terms present in the sentence and return
a list containing all the distinct aspect terms.
I liked the service and the staff, but not the food.
? {service, staff, food}
2.2 Subtask 2: Aspect term polarity
For a given set of aspect terms within a sentence,
the task is to determine the polarity of each aspect
term: positive, negative, neutral or conflict (i.e.,
both positive and negative).
I hated their fajitas, but their salads were great.
? {fajitas: negative, salads: positive}
2.3 Subtask 3: Aspect category detection
Given a predefined set of aspect categories, the
task is to identify the aspect categories discussed
in a given sentence. Aspect categories are typi-
cally coarser than the aspect terms of Subtask 1,
and they do not necessarily occur as terms in the
given sentence.
For example, the following categories were de-
fined for the restaurants? domain: food, service,
price, ambience and anecdotes/miscellaneous.
The restaurant was expensive, but the menu was
great. ? {price, food}
2.4 Subtask 4: Aspect category polarity
Given a set of pre-identified aspect categories, the
task is to determine the polarity (positive, nega-
tive, neutral or conflict) of each aspect category.
The restaurant was expensive, but the menu was
great. ? {price: negative, food: positive}
3 System description
We use machine learning approach to all subtasks.
For aspect term extraction we use CRF. For the
other three tasks we use the Maximum Entropy
classifier. We use the Brainy (Konkol, 2014) im-
plementation of these algorithms.
During the data preprocessing, we use simple
word tokenizer based on regular expressions. All
tokens are lowercased for tasks 2 and 4.
We will firstly describe all the features used in
this paper because the tasks share some of them.
These features are then referenced in the descrip-
tions of individual subtasks.
Words (W) ? Word occurrence on a given posi-
tion in the context window.
Bag of Words (BoW) ? Occurrence of a word in
a sentence (or context window).
Bigrams (B) ? Bigram occurrence on a given po-
sition in the context window.
Bag of Bigrams (BoB) ? Occurrence of a bigram
in a sentence (or context window).
Tf-idf ? Term frequency?inverse document fre-
quency for all tokens in the sentence.
Learned Dictionary (LD) ? Dictionary of terms
based on training data.
Suffixes (S) ? Suffix of a word (2-4 characters).
Sentiment Dictionary (SD) ? Dictionary created
using semi-automatic triangulation method
(Steinberger et al., 2012). The score is nor-
malized.
Senti Wordnet (SW) ? See (Baccianella et al.,
2010).
LDA ? See Section 3.1.
Word Clusters (WC) ? See Section 3.2. Cluster
occurrence on a given position in the context
window.
Bag of Clusters (BoC) ? Same as word clusters,
but without information about position.
818
We use two features that are not in common
use in similar tasks ? Latent Dirichlet Allocation
and word clusters based on semantic spaces. Both
these features use large amount of unlabeled data
to discover latent semantics. We downloaded the
restaurant reviews from http://opentable.
com. This corpus consists of 409,665 reviews
(documents) with about 27 million words. The
opentable corpus is used as the training data for
these features. Unfortunately, we did not find any
large corpus for laptop domain, thus presented un-
supervised features are used in restaurant domain
only.
We devote the following two subsections to de-
scribe these features. Then we introduce our ap-
proach to the individual tasks.
3.1 Latent Dirichlet Allocation
The Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) is a topic model that is assumed to provide
useful information for particular subtasks. We use
LDA implementation from the MALLET (McCal-
lum, 2002) software package. For each experi-
ment we always train the 400 topics LDA (no sig-
nificant difference was observed between different
numbers of topics) with 1,000 iterations of Gibbs
sampling. The hyperparameters of Dirichlet dis-
tributions were initially set to ? = 50/K, where
K is the number of topics and ? = 0.1. This set-
ting is recommended by (Griffiths and Steyvers,
2004). The topic probabilities are directly used as
new features to the classifier.
3.2 Word clusters
We use same approach as presented in (Brychc??n
and Konop??k, 2014), where word clusters derived
from semantic spaces improved language model-
ing. As recommended by these authors, we use
COALS (Correlated Occurrence Analogue to Lex-
ical Semantics) (Rohde et al., 2004) and HAL
(Hyperspace Analogue to Language) (Lund and
Burgess, 1996) for representing the word mean-
ing and the Repeated Bisection algorithm for clus-
tering. Similar approach has been already used
for sentiment analysis in (Habernal and Brychc??n,
2013) and (Brychc??n and Habernal, 2013).
The parameters of semantic spaces are set as
follows. For both semantic spaces we use a four-
word context window (in both directions). HAL
uses a matrix consisting of 50,000 columns, which
keeps the largest amount of information. COALS
uses a matrix with only 14,000 columns (as rec-
ommended by the authors of the algorithm). The
SVD reduction was not used in our experiments.
Implementation of the HAL, COALS algo-
rithms is available in an open source package S-
Space (Jurgens and Stevens, 2010). For cluster-
ing, we use the implementation from the CLUTO
software package (Karypis, 2003). As a measure
of the similarity between two words, we use the
cosine similarity of word vectors.
For both semantic spaces the word vectors are
clustered into four different depths: 100, 500,
1,000, and 5,000 clusters (i.e. eight different clus-
ter sets). The occurrences of particular clusters
represent additional features to the classifiers.
3.3 Aspect term extraction
Our approach for aspect term extraction is based
on Conditional Random Fields (CRF). The choice
was based on similarity with the named entity
recognition task, where CRF are regarded as the
current state of the art (Konkol and Konop??k,
2013). We use the BIO model for representing as-
pect terms (Ramshaw and Marcus, 1999).
The constrained feature set consists of: W, BoW,
B, LD, S. It is extended by WC for the uncon-
strained case.
3.4 Aspect term polarity
During the detection of the aspect term polarities,
the words affecting the sentiment of the aspect
term are assumed to be close in most of cases.
Thus we use a context window of 10 words in both
directions around the target aspect term. We as-
sume the further the word or bigram is from the
target aspect term, the lower impact it has on the
polarity label. To model this assumption we use
a weight for each word and bigram feature taken
from the Gaussian distribution according to the
distance from the aspect term. The mean is set
to 0 and the variance is optimized on training data.
As a feature set for the constrained approach we
use only BoW, BoB and for the unconstrained ap-
proach we use BoC, SD, SW above that.
3.5 Aspect category detection
Aspect category detection is based on a set of bi-
nary Maximum Entropy classifiers, one for each
class. The final decision is simply assembled from
decisions of individual classifiers.
For this task we use BoW, Tf-Idf for the con-
strained approach and add LDA, BoC for uncon-
strained approach.
819
Team Const. Rank P [%] R[%] F
1
[%] Rank ACC[%]
A
s
p
e
c
t
t
e
r
m
s
R
e
s
t
a
u
r
a
n
t
s
Best ? 1. 85.35 82.71 84.01 1. 80.95
UWB U 7. 82.70 76.28 79.36 4. 77.69
UWB C 12. 83.28 70.28 76.23 12. 72.13
Average ? 14-15. 76.74 67.26 70.78 18. 69.15
Semeval Baseline ? ? ? ? 47.15 ? 64.28
L
a
p
t
o
p
s
Best ? 1. 84.80 66.51 74.55 1. 70.49
UWB U ? ? ? ? 4. 66.67
UWB C 14. 77.33 49.54 60.39 10. 62.54
Average ? 14. 68.97 50.45 56.20 16. 59.01
Semeval Baseline ? ? ? ? 35.64 ? 51.07
A
s
p
e
c
t
c
a
t
e
g
o
r
i
e
s
Best ? 1. 91.04 86.24 87.58 1. 82.92
UWB U 4. 84.36 78.93 81.55 8. 72.78
UWB C 5. 85.09 77.37 81.04 9. 72.78
Average ? 11. 76.00 72.26 73.79 12-13. 69.51
Semeval Baseline ? ? ? ? 63.89 ? 65.66
Table 1: Comparison of our constrained (C) and unconstrained (U) system with Semeval baseline, best
and average results. P , R, and F
1
denote the precision, recall and F-measure, respectively, used for
measuring aspect term and category detection. ACC denotes the accuracy, used for measuring aspect
term and category sentiment polarity detection.
3.6 Aspect category polarity
For this task we always take the whole sentence
into account. We cannot take a limited window
as we do not know where exactly the category is
mentioned in the sentence. Moreover, it can be at
several positions. To distinguish between differ-
ent categories we again use standalone Maximum
Entropy classifier for each category.
The constrained feature set consists of: BoW,
BoB, Tf-Idf. It is extended by BoC, LDA, SD, SW
for the unconstrained case.
4 Results
The ABSA task was a competition between re-
search teams from around the world. There were
21 to 32 submitted systems for individual tasks.
We have submitted both constrained (no ex-
ternal knowledge, dictionaries or rules) and un-
constrained systems for all tasks, except uncon-
strained system for aspect term extraction in the
laptops domain.
Table 1 shows results of our systems (UWB)
and compares them with the best and average sys-
tems as well as with the Semeval baseline. The
average system is not any particular system. It is
represented by average rank and metrics (metrics
are averaged separately).
Our systems performed quite well. In all
tasks, we outperform the Semeval baseline sys-
tem. Moreover, we are always above average (F-
measure and accuracy) in all tasks. We were three
times in the fourth place and our unconstrained
systems were always in top ten.
Table 2 presents the 10-fold cross-validation re-
sults on restaurant training data. We can clearly
see, that any of our extension (LDA, clusters, sen-
timent vocabularies) brings at least some improve-
ment.
5 Conclusion
This paper covers our participation in the ABSA
task of Semeval 2014. The ABSA task consists
of 4 subtasks. For each subtask we propose both
constrained (no external knowledge) and uncon-
strained approach. The constrained versions of
our system are based purely on machine learning
techniques. The unconstrained versions extend the
constrained feature set by LDA, semantic spaces
and sentiment dictionaries.
The proposed approaches achieved very good
results. The constrained versions were always
above average, often by a large margin. The un-
constrained versions were ranked among the best
systems.
820
P [%] R[%] F
1
[%]
Constrained 68.72 82.14 74.83
Constrained + WC 76.77 82.51 79.53
(a) Aspect term extraction
ACC[%]
Constrained 65.91
Constrained+BoC 70.05
Constrained+SD+SW 68.13
All 71.02
(b) Aspect term polarity
P [%] R[%] F
1
[%]
Constrained 74.56 80.69 77.51
Constrained + LDA 75.96 81.94 78.84
Constrained + BoC 77.01 81.42 79.16
All 77.28 81.62 79.39
(c) Aspect category extraction
ACC[%]
Constrained 66.69
Constrained+LDA 67.85
Constrained+BoC 68.61
Constrained+SD+SW 69.28
All 70.20
(d) Aspect category polarity
Table 2: 10 fold cross-validation results on the restaurants training data for individual features. P , R,
and F
1
denote the precision, recall and F-measure, respectively, used for measuring aspect term and
category detection. ACC denotes the accuracy, used for measuring aspect term and category sentiment
polarity detection.
Acknowledgements
This work was supported by grant no. SGS-
2013-029 Advanced computing and information
systems, by the European Regional Development
Fund (ERDF) and by project ?NTIS - New Tech-
nologies for Information Society?, European Cen-
tre of Excellence, CZ.1.05/1.1.00/02.0090, and by
project MediaGist, EU?s FP7 People Programme
(Marie Curie Actions), no. 630786.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC?10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George Reis, and Jeff Reynar.
2008. Building a sentiment summarizer for lo-
cal service reviews. In Proceedings of WWW-2008
workshop on NLP in the Information Explosion Era.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Erik Boiy and Marie-Francine Moens. 2009. A
machine learning approach to sentiment analysis
in multilingual web texts. Information retrieval,
12(5):526?558.
Tom?a?s Brychc??n and Ivan Habernal. 2013. Un-
supervised improving of sentiment analysis using
global target context. In Proceedings of the In-
ternational Conference Recent Advances in Natu-
ral Language Processing RANLP 2013, pages 122?
128, Hissar, Bulgaria, September. INCOMA Ltd.
Shoumen, BULGARIA.
Tom?a?s Brychc??n and Miloslav Konop??k. 2014. Seman-
tic spaces for improving language modeling. Com-
puter Speech & Language, 28(1):192 ? 209.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the Conference on Web Search and
Web Data Mining.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235, April.
Ivan Habernal and Tom?a?s Brychc??n. 2013. Semantic
spaces for sentiment analysis. In Text, Speech and
Dialogue, volume 8082 of Lecture Notes in Com-
puter Science, pages 482?489, Berlin Heidelberg.
Springer.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
821
timent classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics.
David Jurgens and Keith Stevens. 2010. The s-space
package: An open source package for word space
models. In In Proceedings of the ACL 2010 System
Demonstrations.
George Karypis. 2003. Cluto - a clustering toolkit.
Michal Konkol and Miloslav Konop??k. 2013. Crf-
based czech named entity recognizer and consolida-
tion of czech ner research. In Ivan Habernal and
V?aclav Matou?sek, editors, Text, Speech and Dia-
logue, volume 8082 of Lecture Notes in Computer
Science, pages 153?160. Springer Berlin Heidel-
berg.
Michal Konkol. 2014. Brainy: A machine learn-
ing library. In Leszek Rutkowski, Marcin Ko-
rytkowski, Rafa Scherer, Ryszard Tadeusiewicz,
Lotfi A. Zadeh, and Jacek M. Zurada, editors, Artifi-
cial Intelligence and Soft Computing, volume 8468
of Lecture Notes in Computer Science. Springer
Berlin Heidelberg.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference on Machine Learning.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opin-
ions on the web. In Proceedings of International
Conference on World Wide Web.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publishers.
Chong Long, Jie Zhang, and Xiaoyan Zhu. 2010. A
review selection approach for accurate feature rating
estimation. In Proceedings of Coling 2010: Poster
Volume.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods Instru-
ments and Computers, 28(2):203?208.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of International Conference on World
Wide Web.
Samaneh Moghaddam and Martin Ester. 2010. Opin-
ion digger: an unsupervised opinion miner from
unstructured product reviews. In Proceeding of
the ACM conference on Information and knowledge
management.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval 2014), Dublin, Ireland.
Lawrence Rabiner. 2010. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. In Proceedings of the IEEE, pages 257?286.
Lance A Ramshaw and Mitchell P Marcus. 1999. Text
chunking using transformation-based learning. In
Natural language processing using very large cor-
pora, pages 157?176. Springer.
Douglas L. T. Rohde, Laura M. Gonnerman, and
David C. Plaut. 2004. An improved method for
deriving word meaning from lexical co-occurrence.
Cognitive Psychology, 7:573?605.
Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann,
Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova,
Ralf Steinberger, Hristo Tanev, Silvia Vzquez, and
Vanni Zavarella. 2012. Creating sentiment dictio-
naries via triangulation. Decision Support Systems,
53(4):689 ? 694.
Josef Steinberger, Tom?a?s Brychc??n, and Michal
Konkol. 2014. Aspect-level sentiment analysis
in czech. In Proceedings of the 5th Workshop on
Computational Approaches to Subjectivity, Senti-
ment and Social Media Analysis, Baltimore, USA,
June. Association for Computational Linguistics.
Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of International Conference on World Wide
Web.
822
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 28?36,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Creating Sentiment Dictionaries via Triangulation
Josef Steinberger,
Polina Lenkova, Mohamed Ebrahim,
Maud Ehrmann, Ali Hurriyetoglu,
Mijail Kabadjov, Ralf Steinberger,
Hristo Tanev and Vanni Zavarella
EC Joint Research Centre
21027, Ispra (VA), Italy
Name.Surname@jrc.ec.europa.eu
Silvia Va?zquez
Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona
silvia.vazquez@upf.edu
Abstract
The paper presents a semi-automatic approach
to creating sentiment dictionaries in many lan-
guages. We first produced high-level gold-
standard sentiment dictionaries for two lan-
guages and then translated them automatically
into third languages. Those words that can
be found in both target language word lists
are likely to be useful because their word
senses are likely to be similar to that of the
two source languages. These dictionaries can
be further corrected, extended and improved.
In this paper, we present results that verify
our triangulation hypothesis, by evaluating tri-
angulated lists and comparing them to non-
triangulated machine-translated word lists.
1 Introduction
When developing software applications for senti-
ment analysis or opinion mining, there are basi-
cally two main options: (1) writing rules that assign
sentiment values to text or text parts (e.g. names,
products, product features), typically making use of
dictionaries consisting of sentiment words and their
positive or negative values, and (2) inferring rules
(and sentiment dictionaries), e.g. using machine
learning techniques, from previously annotated doc-
uments such as product reviews annotated with an
overall judgment of the product. While movie or
product reviews for many languages can frequently
be found online, sentiment-annotated data for other
fields are not usually available, or they are almost
exclusively available for English. Sentiment dictio-
naries are also mostly available for English only or,
if they exist for other languages, they are not com-
parable, in the sense that they have been developed
for different purposes, have different sizes, are based
on different definitions of what sentiment or opinion
means.
In this paper, we are addressing the resource bot-
tleneck for sentiment dictionaries, by developing
highly multilingual and comparable sentiment dic-
tionaries having similar sizes and based on a com-
mon specification. The aim is to develop such dic-
tionaries, consisting of typically one or two thou-
sand words, for tens of languages, although in this
paper we only present results for eight languages
(English, Spanish, Arabic, Czech, French, German,
Italian and Russian). The task raises the obvious
question how the human effort of producing this re-
source can be minimized. Simple translation, be it
using standard dictionaries or using machine trans-
lation, is not very efficient as most words have two,
five or ten different possible translations, depending
on context, part-of-speech, etc.
The approach we therefore chose is that of trian-
gulation. We first produced high-level gold-standard
sentiment dictionaries for two languages (English
and Spanish) and then translated them automatically
into third languages, e.g. French. Those words that
can be found in both target language word lists (En
Fr and Es Fr) are likely to be useful because their
word senses are likely to be similar to that of the
two source languages. These word lists can then be
used as they are or better they can be corrected, ex-
tended and improved. In this paper, we present eval-
uation results verifying our triangulation hypothesis,
by evaluating triangulated lists and comparing them
28
to non-triangulated machine-translated word lists.
Two further issues need to be addressed. The
first one concerns morphological inflection. Auto-
matic translation will yield one word form (often,
but not always the base form), which is not suffi-
cient when working with highly inflected languages:
A single English adjective typically has four Spanish
or Italian word forms (two each for gender and for
number) and many Russian word forms (due to gen-
der, number and case distinctions). The target lan-
guage word lists thus need to be expanded to cover
all these morphological variants with minimal effort
and considering the number of different languages
involved without using software, such as morpho-
logical analysers or generators. The second issue
has to do with the subjectivity involved in the human
annotation and evaluation effort. First of all, it is im-
portant that the task is well-defined (this is a chal-
lenge by itself) and, secondly, the inter-annotator
agreement for pairs of human evaluators working on
different languages has to be checked in order to get
an idea of the natural variation involved in such a
highly subjective task.
Our main field of interest is news opinion min-
ing. We would like to answer the question how cer-
tain entities (persons, organisations, event names,
programmes) are discussed in different media over
time, comparing different media sources, media in
different countries, and media written in different
languages. One possible end product would be a
graph showing how the popularity of a certain en-
tity has changed over time across different languages
and countries. News differs significantly from those
text types that are typically analysed in opinion min-
ing work, i.e. product or movie reviews: While a
product review is about a product (e.g. a printer)
and its features (e.g. speed, price or printing qual-
ity), the news is about any possible subject (news
content), which can by itself be perceived to be pos-
itive or negative. Entities mentioned in the news can
have many different roles in the events described.
If the method does not specifically separate positive
or negative news content from positive or negative
opinion about that entity, the sentiment analysis re-
sults will be strongly influenced by the news context.
For instance, the automatically identified sentiment
towards a politician would most likely to be low if
the politician is mentioned in the context of nega-
tive news content such as bombings or disasters. In
our approach, we therefore aim to distinguish news
content from sentiment values, and this distinction
has an impact on the sentiment dictionaries: unlike
in other approaches, words like death, killing, award
or winner are purposefully not included in the sen-
timent dictionaries as they typically represent news
content.
The rest of the paper is structured as follows: the
next section (2) describes related work, especially
in the context of creating sentiment resources. Sec-
tion 3 gives an overview of our approach to dic-
tionary creation, ranging from the automatic learn-
ing of the sentiment vocabulary, the triangulation
process, the expansion of the dictionaries in size
and regarding morphological inflections. Section 4
presents a number of results regarding dictionary
creation using simple translation versus triangula-
tion, morphological expansion and inter-annotator
agreement. Section 5 summarises, concludes and
points to future work.
2 Related Work
Most of the work in obtaining subjectivity lexicons
was done for English. However, there were some
authors who developed methods for the mapping of
subjectivity lexicons to other languages. Kim and
Hovy (2006) use a machine translation system and
subsequently use a subjectivity analysis system that
was developed for English. Mihalcea et al (2007)
propose a method to learn multilingual subjective
language via cross-language projections. They use
the Opinion Finder lexicon (Wilson et al, 2005)
and two bilingual English-Romanian dictionaries to
translate the words in the lexicon. Since word am-
biguity can appear (Opinion Finder does not mark
word senses), they filter as correct translations only
the most frequent words. The problem of translat-
ing multi-word expressions is solved by translating
word-by-word and filtering those translations that
occur at least three times on the Web. Another ap-
proach in obtaining subjectivity lexicons for other
languages than English was explored in Banea et al
(2008b). To this aim, the authors perform three dif-
ferent experiments, with good results. In the first
one, they automatically translate the annotations of
the MPQA corpus and thus obtain subjectivity an-
29
notated sentences in Romanian. In the second ap-
proach, they use the automatically translated entries
in the Opinion Finder lexicon to annotate a set of
sentences in Romanian. In the last experiment, they
reverse the direction of translation and verify the as-
sumption that subjective language can be translated
and thus new subjectivity lexicons can be obtained
for languages with no such resources. Finally, an-
other approach to building lexicons for languages
with scarce resources is presented in Banea et al
(2008a). In this research, the authors apply boot-
strapping to build a subjectivity lexicon for Roma-
nian, starting with a set of seed subjective entries,
using electronic bilingual dictionaries and a training
set of words. They start with a set of 60 words per-
taining to the categories of noun, verb, adjective and
adverb obtained by translating words in the Opin-
ion Finder lexicon. Translations are filtered using a
measure of similarity to the original words, based on
Latent Semantic Analysis (Landauer and Dumais,
1997) scores. Wan (2008) uses co-training to clas-
sify un-annotated Chinese reviews using a corpus
of annotated English reviews. He first translates
the English reviews into Chinese and subsequently
back to English. He then performs co-training using
all generated corpora. Banea et al (2010) translate
the MPQA corpus into five other languages (some
with a similar ethimology, others with a very differ-
ent structure). Subsequently, they expand the fea-
ture space used in a Naive Bayes classifier using the
same data translated to 2 or 3 other languages. Their
conclusion is that expanding the feature space with
data from other languages performs almost as well
as training a classifier for just one language on a
large set of training data.
3 Approach Overview
Our approach to dictionary creation starts with semi-
automatic way of colleting subjective terms in En-
glish and Spanish. These pivot language dictionaries
are then projected to other languages. The 3rd lan-
guage dictionaries are formed by the overlap of the
translations (triangulation). The lists are then man-
ually filtered and expanded, either by other relevant
terms or by their morphological variants, to gain a
wider coverage.
3.1 Gathering Subjective Terms
We started with analysing the available English
dictionaries of subjective terms: General Inquirer
(Stone et al, 1966), WordNet Affect (Strapparava
and Valitutti, 2004), SentiWordNet (Esuli and Se-
bastiani, 2006), MicroWNOp (Cerini et al, 2007).
Additionally, we used the resource of opinion words
with associated polarity from Balahur et al (2009),
which we denote as JRC Tonality Dictionary. The
positive effect of distinguishing two levels of inten-
sity was shown in (Balahur et al, 2010). We fol-
lowed the idea and each of the emloyed resources
was mapped to four categories: positive, negative,
highly positive and highly negative. We also got
inspired by the results reported in that paper and
we selected as the base dictionaries the combination
of MicroWNOp and JRC Tonality Dictionary which
gave the best results. Terms in those two dictionar-
ies were manually filtered and the other dictionar-
ies were used as lists of candidates (their highly fre-
quent terms were judged and the relevant ones were
included in the final English dictionary). Keeping in
mind the application of the dictionaries we removed
at this step terms that are more likely to describe bad
or good news content, rather than a sentiment to-
wards an entity. In addition, we manually collected
English diminishers (e.g. less or approximately), in-
tensifiers (e.g. very or indeed) and invertors (e.g.
not or barely). The English terms were translated to
Spanish and the same filtering was performed. We
extended all English and Spanish lists with the miss-
ing morphological variants of the terms.
3.2 Automatic Learning of Subjective Terms
We decided to expand our subjective term lists by
using automatic term extraction, inspired by (Riloff
and Wiebe, 2003). We look at the problem of ac-
quisition of subjective terms as learning of seman-
tic classes. Since we wanted to do this for two dif-
ferent languages, namely English and Spanish, the
multilingual term extraction algorithm Ontopopulis
(Tanev et al, 2010) was a natural choice.
Ontopopulis performs weakly supervised learning
of semantic dictionaries using distributional similar-
ity. The algorithm takes on its input a small set of
seed terms for each semantic class, which is to be
learnt, and an unannotated text corpus. For example,
30
if we want to learn the semantic class land vehicles,
we can use the seed set - bus, truck, and car. Then
it searches for the terms in the corpus and finds lin-
ear context patterns, which tend to co-occur imme-
diately before or after these terms. Some of the
highest-scored patterns, which Ontopopulis learned
about land vehicles were driver of the X, X was
parked, collided with another X, etc. Finally, the
algorithm searches for these context patterns in the
corpus and finds other terms which tend to fill the
slot of the patterns (designated by X). Considering
the land vehicles example, new terms which the sys-
tem learned were van, lorry, taxi, etc. Ontopop-
ulis is similar to the NOMEN algorithm (Lin et al,
2003). However, Ontopopulis has the advantage to
be language-independent, since it does not use any
form of language-specific processing, nor does it use
any language-specific resources, apart from a stop
word list.
In order to learn new subjective terms for each
of the languages, we passed the collected subjective
terms as an input to Ontopopulis. For English, we
divided the seed set in two classes: class A ? verbs
and class B ? nouns and adjectives. It was necessary
because each of these classes has a different syn-
tactic behaviour. It made sense to do the same for
Spanish, but we did not have enough Spanish speak-
ers available to undertake this task, therefore we put
together all the subjective Spanish words - verbs, ad-
jectives and nouns in one class. We ran Ontopopulis
for each of the three classes - the class of subjective
Spanish words and the English classes A and B. The
top scored 200 new learnt terms were taken for each
class and manually reviewed.
3.3 Triangulation and Expansion
After polishing the pivot language dictionaries we
projected them to other languages. The dictionaries
were translated by Google translator because of its
broad coverage of languages. The overlapping terms
between English and Spanish translations formed
the basis for further manual efforts. In some cases
there were overlapping terms in English and Span-
ish translations but they differed in intensity. There
was the same term translated from an English posi-
tive term and from a Spanish very positive term. In
these cases the term was assigned to the positive cat-
egory. However, more problematic cases arose when
the same 3rd language term was assigned to more
than one category. There were also cases with dif-
ferent polarity. We had to review them manually.
However, there were still lots of relevant terms in the
translated lists which were not translated from the
other language. These complement terms are a good
basis for extending the coverage of the dictionaries,
however, they need to be reviewed manually. Even if
we tried to include in the pivot lists all morpholog-
ical variants, in the triangulation output there were
only a few variants, mainly in the case of highly in-
flected languages. To deal with morphology we in-
troduced wild cards at the end of the term stem (*
stands for whatever ending and for whatever char-
acter). This step had to be performed carefully be-
cause some noise could be introduced. See the Re-
sults section for examples. Although this step was
performed by a human, we checked the most fre-
quent terms afterwards to avoid irrelavant frequent
terms.
4 Results
4.1 Pivot dictionaries
We gathered and filtered English sentiment terms
from the available corpora (see Section 3.1). The
dictionaries were then translated to Spanish (by
Google translator) and filtered afterwards. By ap-
plying automatic term extraction, we enriched the
sets of terms by 54 for English and 85 for Spanish,
after evaluating the top 200 candidates suggested by
the Ontopolulis tool for each language. The results
are encouraging, despite the relevance of the terms
(27% for English and 42.5% for Spanish where
some missing morphological variants were discov-
ered) does not seem to be very high, considering the
fact that we excluded the terms already contained
in the pivot lists. If we took them into account, the
precision would be much better. The initial step re-
sulted in obtaining high quality pivot sentiment dic-
tionaries for English and Spanish. Their statistics
are in table 1. We gathered more English terms than
Spanish (2.4k compared to 1.7k). The reason for
that is that some translations from English to Span-
ish have been filtered. Another observation is that
there is approximately the same number of negative
terms as positive ones, however, much more highly
negative than highly positive terms. Although the
31
Language English Spanish
HN 554 466
N 782 550
P 772 503
HP 171 119
INT 78 62
DIM 31 27
INV 15 10
TOTAL 2.403 1.737
Table 1: The size of the pilot dictionaries. HN=highly
negative terms, N=negative, P=positive, HP=highly posi-
tive, INV=invertors, DIM=diminishers, INV=invertors.
frequency analysis we carried out later showed that
even if there are fewer highly positive terms, they are
more frequent than the highly negative ones, which
results in almost uniform distribution.
4.2 Triangulation and Expansion
After running triangulation to other languages the
resulted terms were judged for relevance. Native
speakers could suggest to change term?s category
(e.g. negative to highly negative) or to remove it.
There were several reasons why the terms could
have been marked as ?non-sentiment?. For instance,
the term could tend to describe rather negative news
content than negative sentiment towards an entity
(e.g. dead, quake). In other cases the terms were
too ambiguous in a particular language. Examples
from English are: like or right.
Table 2 shows the quality of the triangulated dic-
tionaries. In all cases except for Italian we had only
one annotator assessing the quality. We can see that
the terms were correct in around 90% cases, how-
ever, it was a little bit worse in the case of Russian
in which the annotator suggested to change category
very often.
Terms translated from English but not from Span-
ish are less reliable but, if reviewed manually, the
dictionaries can be expanded significantly. Table 3
gives the statistics concerning these judgments. We
can see that their correctness is much lower than in
the case of the triangulated terms - the best in Italian
(54.4%) and the worst in Czech (30.7%). Of course,
the translation performance affects the results here.
However, this step extended the dictionaries by ap-
proximately 50%.
When considering terms out of context, the most
common translation error occurs when the original
word has several meanings. For instance, the En-
glish word nobility refers to the social class of no-
bles, as well as to the quality of being morally good.
In the news context we find this word mostly in the
second meaning. However, in the Russian triangu-
lated list we have found dvoryanstvo , which refers
to a social class in Russian. Likewise, we need to
keep in mind that a translation of a monosemantic
word might result polysemantic in the target lan-
guage, thereby leading to confusion. For example,
the Italian translation of the English word champion
campione is more frequently used in Italian news
context in a different meaning - sample, therefore
we must delete it from our sentiment words list for
Italian. Another difficulty we might encounter es-
pecially when dealing with inflectional languages is
the fact that a translation of a certain word might be
homographic with another word form in the target
language. Consider the English negative word ban-
dit and its Italian translation bandito, which is more
frequently used as a form of the verb bandire (to an-
nounce) in the news context. Also each annotator
had different point of view on classifying the bor-
derline cases (e.g. support, agreement or difficult).
Two main reasons are offered to explain the low
performance in Arabic. On the one hand, it seems
that some Google translation errors will be repeated
in different languages if the translated words have
the same etymological root. For example both words
? the English fresh and the Spanish fresca ? are
translated to the Arabic as YK
Yg. meaning new. The
Other reason is a more subtle one and is related to
the fact that Arabic words are not vocalized and to
the way an annotator perceive the meaning of a given
word in isolation. To illustrate this point, consider
the Arabic word ? J. ?A
	
J ?? @ , which could be used
as an adjective, meaning appropriate, or as a noun,
meaning The occasion. It appears that the annotator
would intuitively perceive the word in isolation as a
noun and not as an adjective, which leads to disre-
garding the evaluative aspects of a given word.
We tried to include in the pivot dictionaries all
morphological variants of the terms. However, in
highly inflected languages there are much more vari-
ants than those translated from English or Spanish.
32
We manually introduced wild cards to capture the
variants. We had to be attentive when compiling
wild cards for languages with a rich inflectional sys-
tem, as we might easily get undesirable words in the
output. To illustrate this, consider the third person
plural of the Italian negative word perdere (to lose)
perdono, which is also homographic with the word
meaning forgiveness in English. Naturally, it could
happen that the wildcard captures a non-sentiment
term or even a term with a different polarity. For in-
stance, the pattern care% would capture either care,
careful, carefully, but also career or careless. That
is way we perform the last manual checking after
matching the lists expanded by wildcards against a
large number of texts. The annotators were unable
to check all the variants, but only the most frequent
terms, which resulted in reviewing 70-80% of the
term mentions. This step has been performed for
only English, Czech and Russian so far. Table 5
gives the statistics. By introducing the wildcards,
the number of distinct terms grew up significantly
- 12x for Czech, 15x for Russian and 4x for En-
glish. One reason why it went up also for English
is that we captured compounds like: well-arranged,
well-balanced, well-behaved, well-chosen by a sin-
gle pattern. Another reason is that a single pat-
tern can capture different POSs: beaut% can cap-
ture beauty, beautiful, beautifully or beautify. Not
all of those words were present in the pivot dictio-
naries. For dangerous cases like care% above we
had to rather list all possible variants than using a
wildcard. This is also the reason why the number
of patterns is not much lower than the number of
initial terms. Even if this task was done manually,
some noise was added into the dictionaries (92-94%
of checked terms were correct). For example, highly
positive pattern hero% was introduced by an anno-
tator for capturing hero, heroes, heroic, heroical or
heroism. If not checked afterwards heroin would
score highly positively in the sentiment system. An-
other example is taken from Russian: word meaning
to steal ukra% - might generate Ukraine as one most
frequent negative word in Russian.
4.3 How subjective is the annotation?
Sentiment annotation is a very subjective task. In ad-
dition, annotators had to judge single terms without
any context: they had to think about all the senses of
Metric Percent Agreement Kappa
HN 0.909 0.465
N 0.796 0.368
P 0.714 0.281
HP 0.846 0
N+HN 0.829 0.396
P+HP 0.728 0.280
ALL 0.766 0.318
Table 6: Inter-annotator agreement on checking the trian-
gulated list. In the case of HP all terms were annotated as
correct by one of the annotators resulting in Kappa=0.
Metric Percent Agreement Kappa
HN 0.804 0.523
N 0.765 0.545
P 0.686 0.405
HP 0.855 0.669
N+HN 0.784 0.553
P+HP 0.783 0.559
ALL 0.826 0.614
Table 7: Inter-annotator agreement on checking the can-
didates. In ALL diminishers, intensifiers and invertors
are included as well.
the term. Only if the main sense was subjective they
agreed to leave it in the dictionary. Another sub-
jectivity level was given by concentrating on distin-
guishing news content and news sentiment. Defining
the line between negative and highly negative terms,
and similarly with positive, is also subjective. In the
case of Italian we compared judgments of two anno-
tators. The figures of inter-annotator agreement of
annotating the triangulated terms are in table 6 and
the complement terms in table 7. Based on the per-
cent agreement the annotators agree a little bit less
on the triangulated terms (76.6%) compared to the
complement terms (82.6%). However, if we look at
Kappa figures, the difference is clear. Many terms
translated only from English were clearly wrong
which led to a higher agreement between the annota-
tors (0.318 compared to 0.614). When looking at the
difference between positive and negative terms, we
can see that there was higher agreement on the neg-
ative triangulated terms then on the positive ones.
33
Language Triangulated Correct Removed Changed category
Arabic 926 606 (65.5%) 316 (34.1%) 4 (0.4%)
Czech 908 809 (89.1%) 68 (7.5%) 31 (3.4%)
French 1.085 956 (88.1%) 120 (11.1%) 9 (0.8%)
German 1.053 982 (93.3%) 50 (4.7%) 21 (2.0%)
Italian 1.032 918 (89.0%) 36 (3.5%) 78 (7.5%)
Russian 966 816 (84.5%) 49 (5.1%) 101 (10.4%)
Table 2: The size and quality of the triangulated dictionaries. Triangulated=No. of terms coming directly from triangu-
lation, Correct=terms annotated as correct, Removed=terms not relevant to sentiment analysis, Change category=terms
in wrong category (e.g., positive from triangulation, but annotator changed the category to highly positive).
Language Terms Correct Removed Changed category
Czech 1.092 335 (30.7%) 675 (61.8%) 82 (7.5%)
French 1.226 617 (50.3%) 568 (46.3%) 41 (3.4%)
German 1.182 548 (46.4%) 610 (51.6%) 24 (2.0%)
Italian 1.069 582 (54.4%) 388 (36.3%) 99 (9.3%)
Russian 1.126 572 (50.8%) 457 (40.6%) 97 (8.6%)
Table 3: The size and quality of the candidate terms (translated from English but not from Spanish). Terms=No. of
terms translated from English but not from Spanish, Correct=terms annotated as correct, Removed=terms not relevant
to sentiment analysis, Change category=terms in wrong category (e.g., positive in the original list, but annotator
changed the category to highly positive).
Language Terms Correct Removed Changed category
Czech 2.000 1.144 (57.2%) 743 (37.2%) 113 (5.6%)
French 2.311 1.573 (68.1%) 688 (29.8%) 50 (2.1%)
German 2.235 1.530 (68.5%) 660 (29.5%) 45 (2.0%)
Italian 2.101 1.500 (71.4%) 424 (20.2%) 177 (8.4%)
Russian 2.092 1.388 (66.3%) 506 (24.2%) 198 (9.5%)
Table 4: The size and quality of the translated terms from English. Terms=No. of (distinct) terms translated from En-
glish, Correct=terms annotated as correct, Removed=terms not relevant to sentiment analysis, Change category=terms
in wrong category (e.g., positive in the original list, but annotator changed the category to highly positive).
Language Initial terms Patterns Matched terms
Count Correct Checked
Czech 1.257 1.063 15.604 93.0% 74.4%
English 2.403 2.081 10.558 93.8% 81.1%
Russian 1.586 1.347 33.183 92.2% 71.0%
Table 5: Statistics of introducing wild cards and its evaluation. Initial terms=checked triangulated terms extended by
relevant translated terms from English, Patterns=number of patterns after introducing wildcards, Matched terms=terms
matched in the large corpus - their count and correctness + checked=how many mentions were checked (based on the
fact that the most frequent terms were annotated).
34
4.4 Triangulation vs. Translation
Table 4 present the results of simple translation from
English (summed up numbers from tables 2 and 3).
We can directly compare it to table 2 where only
results of triangulated terms are reported. The per-
formance of triangulation is significantly better than
the performance of translation in all languages. The
highest difference was in Czech (89.1% and 57.2%)
and the lowest was in Italian (89.0% and 71.4%).
As a task-based evaluation we used the triangu-
lated/translated dictionaries in the system analysing
news sentiment expressed towards entities. The sys-
tem analyses a fixed word window around entity
mentions. Subjective terms are summed up and the
resulting polarity is attached to the entity. Highly
negative terms score twice more than negative, di-
minishers lower and intensifiers lift up the score. In-
vertors invert the polarity but for instance inverted
highly positive terms score as only negative pre-
venting, for instance, not great to score as worst.
The system searches for the invertor only two words
around the subjective term.
We ran the system on 300 German sentences
taken from news gathered by the Europe Media
Monitor (EMM)1. In all these cases the system at-
tached a polarity to an entity mention. We ran it with
three different dictionaries - translated terms from
English, raw triangulated terms (without the man-
ual checking) and the checked triangulated terms.
This pilot experiment revealed the difference in per-
formance on this task. When translated terms were
used there were only 41.6% contexts with correct
polarity assigned by the system, with raw triangu-
lated terms 56.5%, and with checked triangulated
terms 63.4%. However, the number does not contain
neutral cases that would increase the overall perfor-
mance. There are lots of reasons why it goes wrong
here: the entity may not be the target of the sub-
jective term (we do not use parser because of deal-
ing with many languages and large amounts of news
texts), the system can miss or apply wrongly an in-
vertor, the subjective term is used in different sense,
and irony is hard to detect.
1http://emm.newsbrief.eu/overview.html
4.5 State of progress
We finished all the steps for English, Czech and Rus-
sian. French, German, Italian and Spanish dictio-
naries miss only the introduction of wild cards. In
Arabic we have checked only the triangulated terms.
For other 7 languages (Bulgarian, Dutch, Hungarian,
Polish, Portuguese, Slovak and Turkish) we have
only projected the terms by triangulation. However,
we have capabilities to finish all the steps also for
Bulgarian, Dutch, Slovak and Turkish. We haven?t
investigated using more than two pivot languages for
triangulation. It would probably results in more ac-
curate but shortened dictionaires.
5 Conclusions
We presented our semi-automatic approach and cur-
rent state of work of producing multilingual senti-
ment dictionaries suitable of assessing the sentiment
in news expressed towards an entity. The triangula-
tion approach works significantly better than simple
translation but additional manual effort can improve
it a lot in both recall and precision. We believe that
we can predict the sentiment expressed towards an
entity in a given time period based on large amounts
of data we gather in many languages even if the per-
case performance of the sentiment system as on a
moderate level. Now we are working on improving
the dictionaries in all the discussed languages. We
also run experiments to evaluate the system on vari-
ous languages.
Acknowledgments
We thank Alexandra Balahur for her collaboration
and useful comments. This research was partly sup-
ported by a IULA-Universitat Pompeu Fabra grant.
35
References
Alexandra Balahur, Ralf Steinberger, Erik van der Goot,
and Bruno Pouliquen. 2009. Opinion mining from
newspaper quotations. In Proceedings of the Work-
shop on Intelligent Analysis and Processing of Web
News Content at the IEEE / WIC / ACM International
Conferences on Web Intelligence and Intelligent Agent
Technology (WI-IAT).
A. Balahur, R. Steinberger, M. Kabadjov, V. Zavarella,
E. van der Goot, M. Halkia, B. Pouliquen, and
J. Belyaeva. 2010. Sentiment analysis in the news.
In Proceedings of LREC?10.
C. Banea, R. Mihalcea, and J. Wiebe. 2008a. A boot-
strapping method for building subjectivity lexicons for
languages with scarce resources. In Proceedings of
LREC.
C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan.
2008b. Multilingual subjectivity analysis using ma-
chine translation. In Proceedings of EMNLP.
C. Banea, R. Mihalcea, and J. Wiebe. 2010. Multilingual
subjectivity: Are more languages better? In Proceed-
ings of COLING.
S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli,
and G. Gandini. 2007. Micro-WNOp: A gold stan-
dard for the evaluation of automatically compiled lex-
ical resources for opinion mining. In Andrea Sanso`,
editor, Language resources and linguistic theory: Ty-
pology, second language acquisition, English linguis-
tics. Franco Angeli, Milano, IT.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A pub-
licly available resource for opinion mining. In Pro-
ceeding of the 6th International Conference on Lan-
guage Resources and Evaluation, Italy, May.
S.-M. Kim and E. Hovy. 2006. Extracting opinions,
opinion holders, and topics expressed in online news
media text. In Proceedings of the ACL Workshop on
Sentiment and Subjectivity in Text.
T. Landauer and S. Dumais. 1997. A solution to plato?s
problem: The latent semantic analysis theory of the ac-
quisition, induction, and representation of knowledge.
Psychological Review, 104:211?240.
W. Lin, R. Yangarber, and R. Grishman. 2003. Boot-
strapped learning of semantic classes from positive
and negative examples. In Proceedings of the ICML-
2003 Workshop on The Continuum from Labeled to
Unlabeled Data, Washington DC.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of ACL.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceeding of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
P.J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie.
1966. The general inquirer: a computer approach to
content analysis. M.I.T. studies in comparative poli-
tics, M.I.T. Press, Cambridge, MA.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of wordnet. In Proceeding of the
4th International Conference on Language Resources
and Evaluation, pages 1083?1086, Lisbon, Portugal,
May.
H. Tanev, V. Zavarella, J. Linge, M. Kabadjov, J. Pisko-
rski, M. Atkinson, and R.Steinberger. 2010. Exploit-
ing machine learning techniques to build an event ex-
traction system for portuguese and spanish. Lingua-
matica: Revista para o Processamento Automatico das
Linguas Ibericas.
X. Wan. 2008. Co-training for cross-lingual sentiment
classification. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the Association
for Computational Linguistics and 4th International
Joint Conference on Natural Language Processing of
the Asian Federation of Natural Language Processing.
T. Wilson, J. Wiebe, and P. Hoffman. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis.
In Proceedings of HLT-EMNLP.
36
Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 19?27,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Machine Translation for Multilingual Summary Content Evaluation
Josef Steinberger and Marco Turchi
Joint Research Centre,
European Commission,
Via E. Fermi 2749,
21027 Ispra (VA), Italy
[name].[surname]@jrc.ec.europa.eu
Abstract
The multilingual summarization pilot task at
TAC?11 opened a lot of problems we are fac-
ing when we try to evaluate summary qual-
ity in different languages. The additional lan-
guage dimension greatly increases annotation
costs. For the TAC pilot task English arti-
cles were first translated to other 6 languages,
model summaries were written and submit-
ted system summaries were evaluated. We
start with the discussion whether ROUGE can
produce system rankings similar to those re-
ceived from manual summary scoring by mea-
suring their correlation. We study then three
ways of projecting summaries to a different
language: projection through sentence align-
ment in the case of parallel corpora, sim-
ple summary translation and summarizing ma-
chine translated articles. Building such sum-
maries gives opportunity to run additional ex-
periments and reinforce the evaluation. Later,
we investigate whether an evaluation based on
machine translated models can perform close
to an evaluation based on original models.
1 Introduction
Evaluation of automatically produced summaries in
different languages is a challenging problem for the
summarization community, because human efforts
are multiplied to create model summaries for each
language. Unavailability of parallel corpora suitable
for news summarization adds even another annota-
tion load because documents need to be translated to
other languages. At the last TAC?11 campaign, six
research groups spent a lot of work on creating eval-
uation resources in seven languages (Giannakopou-
los et al, 2012). Thus compared to the monolingual
evaluation, which requires writing model summaries
and evaluating outputs of each system by hand, in
the multilingual setting we need to obtain transla-
tions of all documents into the target language, write
model summaries and evaluate the peer summaries
for all the languages.
In the last fifteen years, research on Machine
Translation (MT) has made great strides allowing
human beings to understand documents written in
various languages. Nowadays, on-line services such
as Google Translate and Bing Translator1 can trans-
late text into more than 50 languages showing that
MT is not a pipe-dream.
In this paper we investigate how machine trans-
lation can be plugged in to evaluate quality of sum-
marization systems, which would reduce annotation
efforts. We also discuss projecting summaries to dif-
ferent languages with the aim to reinforce the evalu-
ation procedure (e.g. obtaining additional peers for
comparison in different language or studying their
language-independence).
This paper is structured as follows: after dis-
cussing the related work in section 2, we give a
short overview of the TAC?11 multilingual pilot task
(section 3). We compare average model and system
manual scores and we also study ROUGE correla-
tion to the manual scores. We run our experiments
on a subset of languages of the TAC multilingual
task corpus (English, French and Czech). Section
4 introduces our translation system. We mention its
1http://translate.google.com/ and http://
www.microsofttranslator.com/
19
translation quality for language pairs used later in
this study. Then we move on to the problem of pro-
jecting summaries to different languages in section
5. We discuss three approaches: projecting sum-
mary through sentence alignment in a parallel cor-
pus, translating a summary, and summarizing trans-
lated source texts. Then, we try to answer the ques-
tion whether using translated models produces sim-
ilar system rankings as when using original models
(section 6), accompanied by a discussion of discrim-
inative power difference and cross-language model
comparison.
2 Related work
Attempts of using machine translation in different
natural language processing tasks have not been
popular due to poor quality of translated texts, but
recent advance in Machine Translation has mo-
tivated such attempts. In Information Retrieval,
Savoy and Dolamic (2009) proposed a comparison
between Web searches using monolingual and trans-
lated queries. On average, the results show a limited
drop in performance, around 15% when translated
queries are used.
In cross-language document summarization, Wan
et al (2010) and Boudin et al (2010) combined the
MT quality score with the informativeness score of
each sentence to automatically produce summary in
a target language. In Wan et al (2010), each sen-
tence of the source document is ranked according to
both scores, the summary is extracted and then the
selected sentences translated to the target language.
Differently, in Boudin et al (2010), sentences are
first translated, then ranked and selected. Both ap-
proaches enhance the readability of the generated
summaries without degrading their content.
Automatic evaluation of summaries has been
widely investigated in the past. In the task of
cross-lingual summarization evaluation Saggion et
al. (2002) proposed different metrics to assess the
content quality of a summary. Evaluation of sum-
maries without the use of models has been intro-
duced by Saggion et al (2010). They showed that
substituting models by full document in the com-
putation of the Jensen-Shannon divergence measure
can produce reliable rankings. Yeloglu et al (2011)
concluded that the pyramid method partially re-
flects the manual inspection of the summaries and
ROUGE can only be used when there is a manually
created summary. A method, and related resources,
which allows saving precious annotation time and
that makes the evaluation results across languages
directly comparable was introduced by Turchi et
al. (2010). This approach relies on parallel data and
it is based on the manual selection of the most im-
portant sentences in a cluster of documents from a
sentence-aligned parallel corpus, and by projecting
the sentence selection to various target languages.
Our work addresses the same problem of reducing
annotation time and generating models, but from a
different prospective. Instead of using parallel data
and annotation projection or full documents, we in-
vestigate the use of machine translation at different
level of summary evaluation. While the aproach of
Turchi et al (2010) is focussed on sentence selection
evaluation our strategy can also evaluate generative
summaries, because it works on summary level.
3 TAC?11 Multilingual Pilot
The Multilingual task of TAC?11 (Giannakopoulos
et al, 2012) aimed to evaluate the application of
(partially or fully) language-independent summa-
rization algorithms on a variety of languages. The
task was to generate a representative summary (250
words) of a set of 10 related news articles.
The task included 7 languages (English, Czech,
French, Hebrew, Hindi, Greek and Arabic). Anno-
tation of each language sub-corpus was performed
by a different group. English articles were manu-
ally translated to the target languages, 3 model sum-
maries were written for each topic.
8 groups (systems) participated in the task, how-
ever, not all systems produced summaries for all lan-
guages. In addition there were 2 baselines: Cen-
troid Baseline ? the start of the centroid article and
GA Topline ? summary based on genetic algorithm
using model summary information, which should
serve as an upper bound.
Human annotators scored each summary, both
models and peers, on the 5-to-1 scale (5 = the best, 1
= the worst) ? human grades. The score corresponds
to the overall responsiveness of the main TAC task ?
equal weight of content and readability. 2
2In this article we focus on raw human grades. The task
20
English French Czech average English French Czech average
Manual grades Manual grades
average model 4.06 4.03 4.73 4.27 4.06 4.03 4.73 4.27
average peer 2.73 2.18 2.56 2.50 2.73 2.18 2.56 2.50
ROUGE-2 ROUGE-SU4
average model .194 .222 .206 .207 .235 .255 .237 .242
average peer .139 .167 .182 .163 .183 .207 .211 .200
correlation to manual grading ? peers and models not stemmed
peers only .574 .427 .444 .482 .487 .362 .519 .456
(p-value) (< .1)
models & peers .735 .702 .484 .640 .729 .703 .549 .660
(p-value) (< .01) (< .02) (< .02) (< .02)
correlation to manual grading ? peers and models stemmed
Peers only .573 .445 .500 .506 .484 .336 .563 .461
(p-value) (< .1)
models & peers .744 .711 .520 .658 .723 .700 .636 .686
(p-value) (< .01) (< .01) (< .02) (< .02) (< .1)
Table 1: Average ROUGE-2 and ROUGE-SU4 scores for models and peers, and their correlation to the manual
evaluation (grades). We report levels of significance (p) for two-tailed test. Cells with missing p-values denote non-
significant correlations (p > .1).
3.1 Manual Evaluation
When we look at the manually assigned grades we
see that there is a clear gap between human and au-
tomatic summaries (see the first two rows in table
1). While the average grade for models were always
over 4, peers were graded lower by 33% for English
and by 54% for French and Czech. However, there
were 5 systems for English and 1 system for French
which were not significantly worse than at least one
model.
3.2 ROUGE
The first question is: can an automatic metric rank
the systems similarly as manual evaluation? This
would be very useful when we test different config-
urations of our systems, in which case manual scor-
ing is almost impossible. Another question is: can
the metric distinguish well the gap between mod-
els and peers? ROUGE is widely used because of
its simplicity and its high correlation with manually
assigned content quality scores on overall system
rankings, although per-case correlation is lower.
We investigated how the two most common
ROUGE scores (ROUGE-2 and ROUGE-SU4) cor-
overview paper (Giannakopoulos et al, 2012) discusses, in ad-
dition, scaling down the grades of shorter summaries to avoid
assigning better grades to shorter summaries.
relate with human grades. Although using n-grams
with n greater than 1 gives limited possibility to
reflect readability in the scores when compared to
reference summaries, ROUGE is considered mainly
as a content evaluation metric. Thus we cannot
expect a perfect correlation because half of the
grade assigned by humans reflects readability issues.
ROUGE could not also evaluate properly the base-
lines. The centroid baseline contains a continuous
text (the start of an article) and it thus gets higher
grades by humans because of its good readability,
but from the ROUGE point of view the baseline is
weak. On the other hand, the topline used informa-
tion from models and it is naturally more similar to
them when evaluated by ROUGE. Its low readabil-
ity ranked it lower in the case of human evaluation.
Because of these problems we include in the correla-
tion figures only the submitted systems, neither the
baseline nor the topline.
Table 1 compares average model and peer
ROUGE scores for the three analyzed languages. It
adds two correlations3 to human grades: for mod-
els+systems and for systems only. The first case
should answer the question whether the automatic
metric can distinguish between human and auto-
matic summaries. The second settings could show
3We used the classical Pearson correlation.
21
whether the automatic metric accurately evaluates
the quality of automatic summaries. To ensure a fair
comparison of models and non-models, each model
summary is evaluated against two other models, and
each non-model summary is evaluated three times,
each time against a different couple of models, and
these three scores are averaged out (the jackknif-
ing procedure).4 The difference of the model and
system ROUGE scores is significant, although it is
not that distinctive as in the case of human grades.
The distinction results in higher correlations when
we include models than in the more difficult systems
only case. This is shown by both correlation figures
and their confidence. The only significant correla-
tion for the systems only case was for English and
ROUGE-2. Other correlations did not cross the 90%
confidence level. If we run ROUGE for morpholog-
ically rich languages (e.g. Czech), stemming plays
more important role than in the case of English. In
the case of French, which stands in between, we
found positive effect of stemming only for ROUGE-
2. ROUGE-2 vs. ROUGE-SU4: for English and
French we see better correlation with ROUGE-2 but
the free word ordering in Czech makes ROUGE-
SU4 correlate better.
4 In-house Translator
Our translation service (Turchi et al, 2012) is
based on the most popular class of Statistical Ma-
chine Translation systems (SMT): the Phrase-Based
model (Koehn et al, 2003). It is an extension of
the noisy channel model introduced by Brown et
al. (1993), and uses phrases rather than words. A
source sentence f is segmented into a sequence of
I phrases f I = {f1, f2, . . . fI} and the same is
done for the target sentence e, where the notion of
phrase is not related to any grammatical assumption;
a phrase is an n-gram. The best translation ebest of
f is obtained by:
ebest = arg maxe p(e|f) = arg maxe p(f |e)pLM (e)
4In our experiments we used the same ROUGE settings as at
TAC. The summaries were truncated to 250 words. For English
we used the Porter stemmer included in the ROUGE package,
for Czech the aggressive version from http://members.
unine.ch/jacques.savoy/clef/index.html and
for French http://jcs.mobile-utopia.com/jcs/
19941\_FrenchStemmer.java.
= arg max
e
I
?
i=1
?(fi|ei)??d(ai ? bi?1)?d
|e|
?
i=1
pLM(ei|e1 . . . ei?1)?LM
where ?(fi|ei) is the probability of translating a
phrase ei into a phrase fi. d(ai ? bi?1) is the
distance-based reordering model that drives the sys-
tem to penalize significant word reordering during
translation, while allowing some flexibility. In the
reordering model, ai denotes the start position of
the source phrase that is translated into the ith tar-
get phrase, and bi?1 denotes the end position of
the source phrase translated into the (i ? 1)th target
phrase. pLM (ei|e1 . . . ei?1) is the language model
probability that is based on the Markov?s chain as-
sumption. It assigns a higher probability to flu-
ent/grammatical sentences. ??, ?LM and ?d are
used to give a different weight to each element. For
more details see (Koehn et al, 2003). In this work
we use the open-source toolkit Moses (Koehn et al,
2007).
Furthermore, our system takes advantage of a
large in-house database of multi-lingual named and
geographical entities. Each entity is identified in the
source language and its translation is suggested to
the SMT system. This solution avoids the wrong
translation of those words which are part of a named
entity and also common words in the source lan-
guage, (e.g. ?Bruno Le Maire? which can be
wrongly translated to ?Bruno Mayor?), and enlarges
the source language coverage.
We built four models covering the following lan-
guage pairs: En-Fr, En-Cz, Fr-En and Cz-En. To
train them we use the freely available corpora: Eu-
roparl (Koehn, 2005), JRC-Acquis (Steinberger et
al., 2006), CzEng0.9 (Bojar and ?Zabokrtsky?, 2009),
Opus (Tiedemann, 2009), DGT-TM5 and News Cor-
pus (Callison-Burch et al, 2010), which results
in more than 4 million sentence pairs for each
model. Our system was tested on the News test set
(Callison-Burch et al, 2010) released by the orga-
nizers of the 2010 Workshop on Statistical Machine
Translation. Performance was evaluated using the
Bleu score (Papineni et al, 2002): En-Fr 0.23, En-
Cz 0.14, Fr-En 0.26 and Cz-En 0.22. The Czech
5http://langtech.jrc.it/DGT-TM.html
22
language is clearly more challenging than French for
the SMT system, this is due to the rich morphology
and the partial free word order. These aspects are
more evident when we translate to Czech, for which
we have poor results.
5 Mapping Peers to Other Languages
When we want to generate a summary of a set of ar-
ticles in a different language we have different pos-
sibilities. The first case is when we have articles in
the target language and we run our summarizer on
them. This was done in the Multilingual TAC task.
If we have parallel corpora we can take advantage of
projecting a sentence-extractive summary from one
language to another (see Section 5.1).
If we do not have the target language articles we
can apply machine translation to get them and run
the summarizer on them (see Section 5.3). If we
miss a crucial resource for running the summarizer
for the target language we can simply translate the
summaries (see Section 5.2).
In the case of the TAC Multilingual scenario these
projections can also give us summaries for all lan-
guages from the systems which were applied only
on some languages.
5.1 Aligned Summaries
Having a sentence-aligned (parallel) corpus gives
access to additional experiments. Because the cur-
rent trend is still on the side of pure sentence extrac-
tion we can investigate whether the systems select
the same sentences across the languages. While cre-
ating the TAC corpus each research group translated
the English articles into their language, thus the re-
sulting corpus was close to be parallel. However,
sentences are not always aligned one-to-one because
a translator may decide, for stylistic or other reasons,
to split a sentence into two or to combine two sen-
tences into one. Translations and original texts are
never perfect, so that it is also possible that the trans-
lator accidentally omits or adds some information,
or even a whole sentence. For these reasons, align-
ers such as Vanilla6, which implements the Gale and
Church algorithm (Gale and Church, 1994), typi-
cally also allow two-to-one, one-to-two, zero-to-one
and one-to-zero sentence alignments. Alignments
6http://nl.ijs.si/telri/Vanilla/
other than one-to-one thus present a challenge for
the method of aligning two text, in particular one-
to-two and two-to-one alignments. We used Vanilla
to align Czech and English article sentences, but be-
cause of high error rate we corrected the alignment
by hand.
The English summaries were then aligned to
Czech (and the opposite direction as well) accord-
ing to the following approach. Sentences in a source
language system summary were split. For each sen-
tence we found the most similar sentence in the
source language articles based on 3-gram overlap.
The alignment information was used to select sen-
tences for the target language summary. Some sim-
plification rules were applied: if the most similar
sentence found in the source articles was aligned
with more sentences in the target language articles,
all the projected sentences were selected (one-to-two
alignment); if the sentence to be projected covered
only a part of sentences aligned with one target lan-
guage sentence, the target language sentence was se-
lected (two-to-one alignment).
The 4th row in table 2 shows average peer
ROUGE scores of aligned summaries.7 When com-
paring the scores to the peers in original language
(3rd row) we notice that the average peer score is
slightly better in the case of English (cz?en projec-
tion) and significantly worse for Czech (en?cz pro-
jection) indicating that Czech summaries were more
similar to English models than English summaries
to Czech models.
Having the alignment we can study the overlap
of the same sentences selected by a summarizer in
different languages. The peer average for the en-
cz language pair was 31%, meaning that only a bit
less than one third of sentences was selected both to
English and Czech summaries by the same system.
The percentage differed a lot from a summarizer to
another one, from 13% to 57%. This number can be
seen as an indicator of summarizer?s language inde-
pendence.
However, the system rankings of aligned sum-
maries did not correlate well with human grades.
There are many inaccuracies in the alignment sum-
mary creation process. At first, finding the sentence
7Models are usually not sentence-extractive and thus align-
ing them would not make much sense.
23
ROUGE-2 ROUGE-SU4
fr?en cz?en en?fr en?cz avg. fr?en cz?en en?fr en?cz avg.
average ROUGE scores
orig. model .194 .194 .222 .206 .207 .235 .235 .255 .237 .242
transl. model .128 .162 .187 .123 .150 .184 .217 .190 .160 .188
orig. peer .139 .139 .167 .182 .163 .183 .183 .207 .211 .200
aligned peer .148 .146 .147 .175 .140 .180
transl. peer .100 .119 .128 .102 .112 .155 .174 .179 .140 .162
correlation to source language manual grading for translated summaries
peers only .411 .483 .746 .456 .524 .233 .577 .754 .571 .534
(p-value) (< .05) (< .05)
models & peers .622 .717 .835 .586 .690 .581 .777 .839 .620 .704
(p-value) (< .05) (< .05) (< .01) (< .1) (< .05) (< .02) (< .01) (< .05)
correlation to target language manual grading for translated summaries
peers only .685 .708 .555 .163 .528 .516 .754 .529 .267 .517
(p-value) (< .1)
Table 2: ROUGE results of translated summaries, evaluated against target language models (e.g., cz?en against
English models).
in the source data that was probably extracted is
strongly dependent on the sentence splitting each
summarizer used. At second, alignment relations
different from one-to-one results in selecting con-
tent with different length compared to the original
summary. And since ROUGE measures recall, and
truncates the summaries, it introduces another inac-
curacy. There were also relations one-to-zero (sen-
tences not translated to the target language). In that
case no content was added to the target summary.
5.2 Translated Summaries
The simplest way to obtain a summary in a different
language is to apply machine translation software on
summaries. Here we investigate (table 2) whether
machine translation errors affect the system order
by correlation to human grades again. In this case
we have two reference human grade sets: one for
the source language (from which we translate) and
one for the target language (to which we translate).
Since there were different models for each language
we can include models only in computing the corre-
lation against source language manual grading.
At first, we can see that ROUGE scores are af-
fected by the translation errors. Average model
ROUGE-2 score went down by 28% and average
peer ROUGE-2 by 31%. ROUGE-SU4 seems to be
more robust to deal with the translation errors: mod-
els went down by 21%, peers by 19%. The gap be-
tween models and peers is still distinguishable, sys-
tem ranking correlation to human grades holds sim-
ilar levels although less statistically significant cor-
relations can be seen. Clearly, quality of the trans-
lator affects these results because our worst transla-
tor (en?cz) produced the worst summaries. Cor-
relation to the source language manual grades in-
dicates how the ranking of the summarizers is af-
fected (changed) by translation errors. For exam-
ple it compares ranking for English based on man-
ual grades with ranking computed on the same sum-
maries translated from English to French. The sec-
ond scenario (correlation to target language scores)
shows how similar is the ranking of summarizers
based on translated summaries with the target lan-
guage ranking based on original summaries. If we
omit translation inaccuracies, low correlation in the
latter case indicates qualitatively different output of
participating peers (e.g. en and cz summaries).
5.3 Summarizing Translated Articles
To complete the figure we tested the configuration
in which we first translate the full articles to the
target language and then apply a summarizer. As
we have at disposal an implementation of system
3 from the TAC multilingual task we used it on 4
translated document sets (en?cz, cz?en, fr?en,
en?fr). This system was the best according to hu-
man grades in all three discussed languages.
24
method ROUGE-2 ROUGE-SU4
en .177 .209
cz ? en alignment .200 .235
cz ? en translation .142 .194
en from (cz ? en source translation) .132 .181
fr ? en translation .120 .172
en from (fr ? en source translation) .129 .185
fr .214 .241
en ? fr translation .167 .212
fr from (en ? fr source translation) .156 .202
cz .204 .225
en ? cz alignment .176 .196
en ? cz translation .115 .150
cz from (en ? cz source translation) .138 .178
Table 3: ROUGE results of different variants of summaries produced by system 3. The first line shows the ROUGE
scores of the original English summaries submitted by system 3. The second line gives average scores of the cz?en
aligned summaries (see Section 5.1), in the 3rd and 5th lines there are figures of cz?en and fr?en translated sum-
maries, and 4th and 6th lines show scores when the summarizer was applied on translated source texts (cz?en and
fr?en). Similarly, lines further down show performance for French and Czech.
The system is based on the latent semantic anal-
ysis framework originally proposed by Gong and
Liu (2002) and later improved by J. Steinberger
and Jez?ek (2004). It first builds a term-by-sentence
matrix from the source articles, then applies Singu-
lar Value Decomposition (SVD) and finally uses the
resulting matrices to identify and extract the most
salient sentences. SVD finds the latent (orthogonal)
dimensions, which in simple terms correspond to the
different topics discussed in the source (for details
see (Steinberger et al, 2011)).
Table 3 shows all results of summaries generated
by the summarizer. The first part compares English
summaries. We see that when projecting the sum-
mary through alignment from Czech, see Section
5.1, a better summary was obtained. When using
translation the summaries are always significantly
worse compared to original (TAC) summaries, with
the lowest performing en?cz translation. It is in-
teresting that in the case of this low-performing
translator it was significantly better to translate the
source articles and to use the summarizer afterwards.
The advantage of this configuration is that the core
of the summarizer (LSA) treats all terms the same
way, thus even English terms that were not trans-
lated work well for sentence selection. On the other
hand, when translating the summary ROUGE will
not match the English terms in Czech models.
6 Using Translated Models
With growing number of languages the annotation
effort rises (manual creation of model summaries).
Now we investigate whether we can produce models
in one pivot language (e.g., English) and translate
them automatically to all other languages. The fact
that in the TAC corpus we have manual summaries
for each language gives us opportunity to reinforce
the evaluation by translating all model summaries
to a common language and thus obtaining a larger
number of models. This way we can also evaluate
similarity among models coming from different lan-
guages and it lowers the annotators? subjectivity.
6.1 Evaluation Against Translated Models
Table 4 shows ROUGE figures when peers were
evaluated against translated models. We discuss also
the case when English peer summaries (and mod-
els as well) are evaluated against both French and
Czech models translated to English. We can see
again lower ROUGE scores caused by translation er-
rors, however, there is more or less the same gap
between peers and models and the correlation holds
similar levels as when using the original target lan-
guage models. Exceptions are using English models
translated to French and Czech models translated to
English in combination with the systems only cor-
relation. If we used both French and Czech mod-
25
ROUGE-2 ROUGE-SU4
peers from en fr cz avg. en fr cz avg.
models tr. from fr cz fr / cz en en fr cz fr / cz en en
average model .144 .167 .155 .165 .144 .155 .207 .221 .206 .215 .190 .208
average peer .110 .111 .104 .135 .125 .117 .170 .162 .153 .186 .172 .169
correlation to target language manual grading
peers only .639 .238 .424 .267 .541 .422 .525 .136 .339 .100 .624 .345
(p-value) < .1
models & peers .818 .717 .782 .614 .520 .690 .785 .692 .759 .559 .651 .793
(p-value) < .01 < .02 < .01 < .05 < .01 < .02 < .01 < .1 < .1
Table 4: ROUGE results of using translated model summaries, which evaluate both peer and model summaries in the
particular language.
els translated to English, higher correlation of En-
glish peers with translated French models was av-
eraged out by lower correlation with Czech models.
And because the TAC Multilingual task contained 7
languages the experiment can be extended to using
translated models from 6 languages. However, our
results rather indicate that using the best translator is
better choice.
Given the small scale of the experiment we cannot
draw strong conclusions on discriminative power8
when using translated models. However, our ex-
periments indicate that by using translated sum-
maries we are partly loosing discriminative power
(i.e. ROUGE finds fewer significant differences be-
tween systems).
6.2 Comparing Models Across Languages
By translating both Czech and French models to
English we could compare all models against each
other. For each topic we had 9 models: 3 original
English models, 3 translated from French and 3 from
Czech. In this case we reached slightly better cor-
relations for the models+systems case: ROUGE-2:
.790, ROUGE-SU4: .762. It was mainly because of
the fact that this time also models only rankings from
ROUGE correlated with human grades (ROUGE-2:
.475, ROUGE-SU4: .445). When we used only En-
glish models, the models ranking did not correlate at
all (? -0.1). Basically, one English model was less
similar to the other two, but it did not mean that it
was worse which was shown by adding models from
8Discriminative power measures how successful the auto-
matic measure is in finding the same significant differences be-
tween systems as manual evaluation.
other languages. If we do not have enough reference
summaries this could be a way to lower subjectivity
in the evaluation process.
7 Conclusion
In this paper we discuss the synergy between ma-
chine translation and multilingual summarization
evaluation. We show how MT can be used to obtain
both peer and model evaluation data.
Summarization evaluation mostly aims to achieve
two main goals a) to identify the absolute perfor-
mance of each system and b) to rank all the sys-
tems according to their performances. Our results
show that the use of translated summaries or mod-
els does not alter much the overall system ranking.
It maintains a fair correlation with the source lan-
guage ranking although without statistical signifi-
cance in most of the systems only cases given the
limited data set. A drop in ROUGE score is evident,
and it strongly depends on the translation perfor-
mance. The use of aligned summaries, which lim-
its the drop, requires high quality parallel data and
alignments, which are not always available and have
a significant cost to be created.
The study leaves many opened questions: What
is the required translation quality which would let
us substitute target language models? Are transla-
tion errors averaged out when using translated mod-
els from more languages? Can we add a new lan-
guage to the TAC multilingual corpus just by using
MT having in mind lower quality (? lower scores)
and being able to quantify the drop? Experiment-
ing with a larger evaluation set could try to find the
answers.
26
References
O. Bojar and Z. ?Zabokrtsky?. 2009. CzEng0.9: Large Par-
allel Treebank with Rich Annotation. Prague Bulletin
of Mathematical Linguistics, 92. in print.
F. Boudin, S. Huet, J.M. Torres-Moreno, and J.M. Torres-
Moreno. 2010. A graph-based approach to cross-
language multi-document summarization. Research
journal on Computer science and computer engineer-
ing with applications (Polibits), 43:113?118.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational linguis-
tics, 19(2):263?311.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O.F. Zaidan. 2010. Findings of
the 2010 joint workshop on statistical machine trans-
lation and metrics for machine translation. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 17?53. Associa-
tion for Computational Linguistics.
W.A. Gale and K.W. Church. 1994. A program for align-
ing sentences in bilingual corpora. Computational lin-
guistics, 19.
G. Giannakopoulos, M. El-Haj, B. Favre, M. Litvak,
J. Steinberger, and V. Varma. 2012. Tac 2011 multil-
ing pilot overview. In Proceedings of TAC?11. NIST.
Y. Gong and X. Liu. 2002. Generic text summarization
using relevance measure and latent semantic analysis.
In Proceedings of ACM SIGIR, New Orleans, US.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology-Volume 1, pages 48?54. Asso-
ciation for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177?
180. Association for Computational Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In Proceedings of the MT
summit, volume 5.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meet-
ing on association for computational linguistics, pages
311?318. Association for Computational Linguistics.
H. Saggion, D. Radev, S. Teufel, W. Lam, and S.M.
Strassel. 2002. Developing infrastructure for the eval-
uation of single and multi-document summarization
systems in a cross-lingual environment. In Proceed-
ings of LREC 2002, pages 747?754.
H. Saggion, J.M. Torres-Moreno, I. Cunha, and E. San-
Juan. 2010. Multilingual summarization evaluation
without human models. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Posters, pages 1059?1067. Association for Computa-
tional Linguistics.
J. Savoy and L. Dolamic. 2009. How effective is
google?s translation service in search? Communica-
tions of the ACM, 52(10):139?143.
J. Steinberger and K. Jez?ek. 2004. Text summarization
and singular value decomposition. In Proceedings of
the 3rd ADVIS conference, Izmir, Turkey.
R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-
javec, D. Tufis, and D. Varga. 2006. The jrc-acquis:
A multilingual aligned parallel corpus with 20+ lan-
guages. Arxiv preprint cs/0609058.
J. Steinberger, M. Kabadjov, R. Steinberger, H. Tanev,
M. Turchi, and V. Zavarella. 2011. Jrcs participation
at tac 2011: Guided and multilingual summarization
tasks. In Proceedings of the Text Analysis Conference
(TAC).
J. Tiedemann. 2009. News from opus-a collection of
multilingual parallel corpora with tools and interfaces.
In Proceedings of the Recent Advances in Natural Lan-
guage Processing Conference, volume 5, pages 237?
248. John Benjamins Amsterdam.
M. Turchi, J. Steinberger, M. Kabadjov, and R. Stein-
berger. 2010. Using parallel corpora for multilin-
gual (multi-document) summarisation evaluation. In
Proceedings of the Multilingual and Multimodal Infor-
mation Access Evaluation Conference, pages 52?63.
Springer.
M. Turchi, M. Atkinson, A. Wilcox, B. Crawley,
S. Bucci, R. Steinberger, and E. Van der Goot. 2012.
Onts:optima news translation system. In Proceedings
of EACL 2012, page 25.
X. Wan, H. Li, and J. Xiao. 2010. Cross-language
document summarization based on machine transla-
tion quality prediction. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 917?926. Association for Computa-
tional Linguistics.
O. Yeloglu, E. Milios, and N. Zincir-Heywood. 2011.
Multi-document summarization of scientific corpora.
In Proceedings of the 2011 ACM Symposium on Ap-
plied Computing, pages 252?258. ACM.
27
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 65?74,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Sentiment Analysis in Czech Social Media Using Supervised Machine
Learning
Ivan Habernal
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plzen?
Czech Republic
habernal@kiv.zcu.cz
Toma?s? Pta?c?ek
Department of Computer
Science and Engineering,
Faculty of Applied Sciences
University of West Bohemia,
Univerzitn?? 8, 306 14 Plzen?
Czech Republic
tigi@kiv.zcu.cz
Josef Steinberger
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plzen?
Czech Republic
jstein@kiv.zcu.cz
Abstract
This article provides an in-depth research of
machine learning methods for sentiment ana-
lysis of Czech social media. Whereas in En-
glish, Chinese, or Spanish this field has a
long history and evaluation datasets for vari-
ous domains are widely available, in case of
Czech language there has not yet been any
systematical research conducted. We tackle
this issue and establish a common ground for
further research by providing a large human-
annotated Czech social media corpus. Fur-
thermore, we evaluate state-of-the-art super-
vised machine learning methods for sentiment
analysis. We explore different pre-processing
techniques and employ various features and
classifiers. Moreover, in addition to our newly
created social media dataset, we also report re-
sults on other widely popular domains, such
as movie and product reviews. We believe
that this article will not only extend the current
sentiment analysis research to another family
of languages, but will also encourage competi-
tion which potentially leads to the production
of high-end commercial solutions.
1 Introduction
Sentiment analysis has become a mainstream re-
search field in the past decade. Its impact can be
seen in many practical applications, ranging from
analyzing product reviews (Stepanov and Riccardi,
2011) to predicting sales and stock markets using so-
cial media monitoring (Yu et al, 2013). The users?
opinions are mostly extracted either on a certain po-
larity scale, or binary (positive, negative); various
levels of granularity are also taken into account, e.g.,
document-level, sentence-level, or aspect-based sen-
timent (Hajmohammadi et al, 2012).
Most of the research in automatic sentiment ana-
lysis of social media has been performed in English
and Chinese, as shown by several recent surveys,
i.e., (Liu and Zhang, 2012; Tsytsarau and Palpanas,
2012). For Czech language, there have been very
few attempts, although the importance of sentiment
analysis of social media became apparent, i.e., dur-
ing the recent presidential elections 1. Many Czech
companies also discovered a huge potential in social
media marketing and started launching campaigns,
contests, and even customer support on Facebook?
the dominant social network of the Czech online
community with approximately 3.5 million users.2
However, one aspect still eludes many of them: au-
tomatic analysis of customer sentiment of products,
services, or even a brand or a company name. In
many cases, sentiment is still labeled manually, ac-
cording to our information from one of the leading
Czech companies for social media monitoring.
Automatic sentiment analysis in the Czech envi-
ronment has not yet been thoroughly targeted by the
research community. Therefore it is necessary to
create a publicly available labeled dataset as well as
to evaluate the current state of the art for two rea-
sons. First, many NLP methods must deal with high
flection and rich syntax when processing the Czech
language. Facing these issues may lead to novel
1http://www.mediaguru.cz/2013/01/
analyza-facebook-rozhodne-o-volbe-prezidenta/ [in
Czech]
2http://www.czso.cz/csu/redakce.nsf/i/
uzivatele facebooku [in Czech]
65
approaches to sentiment analysis as well. Second,
freely accessible and well-documented datasets, as
known from many shared NLP tasks, may stimulate
competition which usually leads to the production of
cutting-edge solutions.3
This article focuses on document-level sentiment
analysis performed on three different Czech datasets
using supervised machine learning. As the first
dataset, we created a Facebook corpus consisting
of 10,000 posts. The dataset was manually la-
beled by two annotators. The other two datasets
come from online databases of movie and prod-
uct reviews, whose sentiment labels were derived
from the accompanying star ratings from users of
the databases. We provide all these labeled datasets
under Creative Commons BY-NC-SA licence4
at http://liks.fav.zcu.cz/sentiment ,
together with the sources for all the presented exper-
iments.
The rest of this article is organized as follows.
Section 2 examines the related work with a focus
on the Czech research and social media. Section 3
thoroughly describes the datasets and the annotation
process. In section 4, we list the employed features
and describe our approach to classification. Finally,
section 5 contains the results with a thorough discus-
sion.
2 Related work
There are two basic approaches to sentiment ana-
lysis: dictionary-based and machine learning-based.
While dictionary-based methods usually depend on
a sentiment dictionary (or a polarity lexicon) and a
set of handcrafted rules (Taboada et al, 2011), ma-
chine learning-based methods require labeled train-
ing data that are later represented as features and
fed into a classifier. Recent attempts have also in-
vestigated semi-supervised methods that incorporate
auxiliary unlabeled data (Zhang et al, 2012).
3E.g., named entity recognition based on Conditional Ran-
dom Fields emerged from CoNLL-2003 named entity recogni-
tion shared task.
4http://creativecommons.org/licenses/
by-nc-sa/3.0/
2.1 Supervised machine learning for sentiment
analysis
The key point of using machine learning for senti-
ment analysis lies in engineering a representative set
of features. Pang et al (2002) experimented with
unigrams (presence of a certain word, frequencies of
words), bigrams, part-of-speech (POS) tags, and ad-
jectives on a Movie Review dataset. Martineau and
Finin (2009) tested various weighting schemes for
unigrams based on TFIDF model (Manning et al,
2008) and proposed delta weighting for a binary sce-
nario (positive, negative). Their approach was later
extended by Paltoglou and Thelwall (2010) who pro-
posed further improvement in delta TFIDF weight-
ing.
The focus of the current sentiment analysis re-
search is shifting towards social media, mainly tar-
geting Twitter (Kouloumpis et al, 2011; Pak and
Paroubek, 2010) and Facebook (Go et al, 2009;
Ahkter and Soria, 2010; Zhang et al, 2011; Lo?pez et
al., 2012). Analyzing media with very informal lan-
guage benefits from involving novel features, such
as emoticons (Pak and Paroubek, 2010; Montejo-
Ra?ez et al, 2012), character n-grams (Blamey et al,
2012), POS and POS ratio (Ahkter and Soria, 2010;
Kouloumpis et al, 2011), or word shape (Go et al,
2009; Agarwal et al, 2011).
In many cases, the gold data for training and test-
ing the classifiers are created semi-automatically, as
in, e.g., (Kouloumpis et al, 2011; Go et al, 2009;
Pak and Paroubek, 2010). In the first step, random
samples from a large dataset are drawn according to
presence of emoticons (usually positive and nega-
tive) and are then filtered manually. Although large
high-quality collections can be created very quickly
using this approach, it makes a strong assumption
that every positive or negative post must contain an
emoticon.
Balahur and Tanev (2012) performed experiments
with Twitter posts as part of the CLEF 2012 Re-
pLab5. They classified English and Spanish tweets
by a small but precise lexicon, which contained also
slang, combined with a set of rules that capture the
manner in which sentiment is expressed in social
media.
5http://www.limosine-project.eu/events/
replab2012
66
Since the limited space of this paper does not al-
low us to present detailed evaluation from the related
work, we recommend an in-depth survey by Tsytsa-
rau and Palpanas (2012) for actual results obtained
from the abovementioned methods.
2.2 Sentiment analysis in Czech environment
Veselovska? et al (2012) presented an initial research
on Czech sentiment analysis. They created a corpus
which contains polarity categories of 410 news sen-
tences. They used the Naive Bayes classifier and
a classifier based on a lexicon generated from an-
notated data. The corpus is not publicly available,
moreover, due to the small size of the corpus no
strong conclusions can be drawn.
Steinberger et al (2012) proposed a semi-
automatic ?triangulation? approach to creating sen-
timent dictionaries in many languages, including
Czech. They first produced high-level gold-standard
sentiment dictionaries for two languages and then
translated them automatically into the third lan-
guage by a state-of-the-art machine translation ser-
vice. Finally, the resulting sentiment dictionaries
were merged by taking overlap from the two auto-
matic translations.
A multilingual parallel news corpus annotated
with opinions towards entities was presented in
(Steinberger et al, 2011). Sentiment annotations
were projected from one language to several others,
which saved annotation time and guaranteed compa-
rability of opinion mining evaluation results across
languages. The corpus contains 1,274 news sen-
tences where an entity (the target of the sentiment
analysis) occurs. It contains 7 languages including
Czech. Their research targets fundamentally differ-
ent objectives from our research as they focus on
news media and aspect-based sentiment analysis.
3 Datasets
3.1 Social media dataset
The initial selection of Facebook brand pages for our
dataset was based on the ?top? Czech pages, accord-
ing to the statistics from SocialBakers.6 We focused
on pages with a large Czech fan base and a sufficient
number of Czech posts. Using Facebook Graph API
6http://www.socialbakers.com/facebook-pages/
brands/czech-republic/
and Java Language Detector7 we acquired 10,000
random posts in the Czech language from nine dif-
ferent Facebook pages. The posts were then com-
pletely anonymized as we kept only their textual
contents.
Sentiment analysis of posts at Facebook brand
pages usually serves as a marketing feedback of user
opinions about brands, services, products, or current
campaigns. Thus we consider the sentiment target
to be the given product, brand, etc. Typically, users?
complaints hold negative sentiment, whereas joy or
happiness about the brand is taken as positive. We
also added another class called bipolar which rep-
resents both positive and negative sentiment in one
post.8 In some cases, the user?s opinion, although
being somehow positive, does not relate to the given
page.9 Therefore the sentiment is treated as neutral
in these cases, according to our above-mentioned as-
sumption.
The complete 10k dataset was independently an-
notated by two annotators. The inter-annotator
agreement (Cohen?s ?) between these two anno-
tators reaches 0.66 which represents a substantial
agreement level (Pustejovsky and Stubbs, 2013),
therefore the task can be considered as well-defined.
The gold data were created based on the agree-
ment of the two annotators. They disagreed in
2,216 cases. To solve these conflicts, we involved
a third super-annotator to assign the final sentiment
label. However, even after the third annotator?s la-
beling, there was still no agreement for 308 labels.
These cases were later solved by a fourth annotator.
We discovered that most of these conflicting cases
were classified as either neutral or bipolar. These
posts were often difficult to label because the author
used irony, sarcasm or the context or previous posts.
These issues remain open.
The Facebook dataset contains of 2,587 positive,
5,174 neutral, 1,991 negative, and 248 bipolar posts,
respectively. We ignore the bipolar class later in all
experiments. The sentiment distribution among the
7http://code.google.com/p/jlangdetect/
8For example ?to bylo moc dobry ,fakt jsem se nadlabla :-D
skoda ze uz neni v nabidce???It was very tasty, I really stuffed
myself :-D sad it?s not on the menu anymore?.
9Certain campaigns ask the fans for, i.e., writing a poem?
these posts are mostly positive (or funny, at least) but are irrele-
vant for the desired task.
67
source pages is shown in Figure 1. The statistics
reveal negative opinions towards cell phone oper-
ators and positive opinions towards, e.g., perfumes
and ZOO.
Figure 1: Social media dataset statistics
3.2 Movie review dataset
Movie reviews as a corpus for sentiment analysis
has been used in research since the pioneering re-
search conducted by Pang et al (2002). Therefore
we covered the same domain in our experiments as
well. We downloaded 91,381 movie reviews from
the Czech Movie Database10 and split them into 3
categories according to their star rating (0?2 stars as
negative, 3?4 stars as neutral, 5?6 stars as positive).
The dataset contains of 30,897 positive, 30,768 neu-
tral, and 29,716 negative reviews, respectively.
3.3 Product review dataset
Another very popular domain for sentiment analy-
sis deals with product reviews (Hu and Liu, 2004).
We crawled all user reviews from a large Czech e-
shop Mall.cz11 which offers a wide range of prod-
ucts. The product reviews are accompanied with star
ratings on the scale 0?5. We took a different strat-
egy for assigning sentiment labels. Whereas in the
movie dataset the distribution of stars was rather uni-
form, in the product review domain the ratings were
skewed towards the higher values. After a manual
inspection we discovered that 4-star ratings mostly
correspond to neutral opinions and 3 or less stars de-
note mostly negative comments. Thus we split the
10http://www.csfd.cz/
11http://www.mall.cz
dataset into three categories according to this obser-
vation. The final dataset consists of 145,307 posts
(102,977 positive, 31,943 neutral, and 10,387 nega-
tive).
4 Classification
4.1 Preprocessing
As pointed out by Laboreiro et al (2010), tokeniza-
tion significantly affects sentiment analysis, espe-
cially in case of social media. Although Ark-tweet-
nlp tool (Gimpel et al, 2011) was developed and
tested in English, it yields satisfactory results in
Czech as well, according to our initial experiments
on the Facebook corpus. Its significant feature is
proper handling of emoticons and other special char-
acter sequences that are typical for social media.
Furthermore, we remove stopwords using the stop-
word list from Apache Lucene project.12
In many NLP applications, a very popular pre-
processing technique is stemming. We tested Czech
light stemmer (Dolamic and Savoy, 2009) and High
Precision Stemmer13. Another widely-used method
for reducing the vocabulary size, and thus the feature
space, is lemmatization. For Czech language the
only currently available lemmatizer is shipped with
Prague Dependency Treebank (PDT) toolkit (Hajic?
et al, 2006). However, we use our in-house Java
HMM-based implementation using the PDT train-
ing data as we need a better control over each pre-
processing step.
Part-of-speech tagging is done using our in-house
Java solution that exploits Prague Dependency Tree-
bank (PDT) data as well. However, since PDT is
trained on news corpora, we doubt it is suitable for
tagging social media that are written in very infor-
mal language (consult, i.e., (Gimpel et al, 2011)
where similar issues were tackled in English).
Since the Facebook dataset contains a huge num-
ber of grammar mistakes and misspellings (typ-
ically ?i/y?,?e?/je/ie?, and others), we incorporated
phonetic transcription to International Phonetic Al-
phabet (IPA) in order to reduce the effect of these
mistakes. We rely on eSpeak14 implementation. An-
12http://lucene.apache.org/core/
13Publication pending; please visit
http://liks.fav.zcu.cz/HPS/.
14http://espeak.sourceforge.net
68
Pipe 1 Pipe 2 Pipe 3
Tokenizing
ArkTweetNLP
POS tagging
PDT
Stem (S) Lemma (L)
none (n) PDT (p)
light (l)
HPS (h)
Stopwords
remove
Casing (C) Phonetic (P) ?
keep (k) eSpeak (e)
lower (l)
Table 1: The preprocessing pipes (top-down). Various
combinations of methods can be denoted using the ap-
propriate labels, e.g. ?SnCk? means 1. tokenizing, 2.
POS-tagging, 3. no stemming, 4. removing stopwords,
and 5. no casing, or ?Lp? means 1. tokenizing, 2. POS-
tagging, 3. lemmatization using PDT, and 4. removing
stopwords.
other preprocessing step might involve removing di-
acritics, as many Czech users type only using unac-
cented characters. However, posts without diacritics
represent only about 8% of our datasets, thus we de-
cided to keep diacritics unaffected.
The complete preprocessing diagram and its vari-
ants is depicted in Table 1. Overall, there are 10
possible preprocessing ?pipe? configurations.
4.2 Features
N-gram features We use presence of unigrams
and bigrams as binary features. The feature space is
pruned by minimum n-gram occurrence which was
empirically set to 5. Note that this is the baseline
feature in most of the related work.
Character n-gram features Similarly to the word
n-gram features, we added character n-gram fea-
tures, as proposed by, e.g., (Blamey et al, 2012). We
set the minimum occurrence of a particular charac-
ter n-gram to 5, in order to prune the feature space.
Our feature set contains 3-grams to 6-grams.
POS-related features Direct usage of part-of-
speech n-grams that would cover sentiment patterns
has not shown any significant improvement in the re-
lated work. Still, POS tags provide certain character-
istics of a particular post. We implemented various
POS features that include, e.g., the number of nouns,
verbs, and adjectives (Ahkter and Soria, 2010), the
ratio of nouns to adjectives and verbs to adverbs
(Kouloumpis et al, 2011), and number of negative
verbs.
Emoticons We adapted the two lists of emoticons
that were considered as positive and negative from
(Montejo-Ra?ez et al, 2012). The feature captures
number of occurrences of each class of emoticons
within the text.
Delta TFIDF variants for binary scenarios Al-
though simple binary word features (presence of a
certain word) reach surprisingly good performance,
they have been surpassed by various TFIDF-based
weighting, such as Delta TFIDF (Martineau and
Finin, 2009), or Delta BM25 TFIDF (Paltoglou and
Thelwall, 2010). Delta-TFIDF still uses traditional
TFIDF word weighting but treats positive and nega-
tive documents differently. However, all the exist-
ing related works which use this kind of features
deal only with binary decisions (positive/negative),
thus we filtered out neutral documents from the
datasets.15 We implemented the most promising
weighting schemes from (Paltoglou and Thelwall,
2010), namely Augmented TF, LogAve TF, BM25
TF, Delta Smoothed IDF, Delta Prob. IDF, Delta
Smoothed Prob. IDF, and Delta BM25 IDF.
4.3 Classifiers
All evaluation tests were performed using two clas-
sifiers, Maximum Entropy (MaxEnt) and Support
Vector Machines (SVM). Although Naive Bayes
classifier is also widely used in the related work, we
did not include it as it usually performs worse than
SVM or MaxEnt. We used a pure Java framework
for machine learning16 with default settings (linear
kernel for SVM).
5 Results
For each combination from the preprocessing
pipeline (refer to Table 1) we assembled various sets
of features and employed two classifiers. In the first
15Opposite to leave-one-out cross validation in (Paltoglou
and Thelwall, 2010), we still use 10-fold cross validation in all
experiments.
16http://liks.fav.zcu.cz/ml
69
scenario, we classify into all three classes (positive,
negative, and neutral).17 In the second scenario,
we follow a strand of related research, e.g., (Mar-
tineau and Finin, 2009; Celikyilmaz et al, 2010),
that deals only with positive and negative classes.
For these purposes we filtered out all the neutral doc-
uments from the datasets. Furthermore, in this sce-
nario we evaluate only features based on weighted
delta-TFIDF, as, e.g., in (Paltoglou and Thelwall,
2010). We also involved only MaxEnt classifier into
the second scenario.
All tests were conducted in the 10-fold cross val-
idation manner. We report macro F-measure, as
it allows comparing classifier results on different
datasets. Moreover, we do not report micro F-
measure (accuracy) as it tends to prefer performance
on dominant classes in highly unbalanced datasets
(Manning et al, 2008), which is, e.g., the case of
our Product Review dataset where most of the labels
are positive.
5.1 Social media
Table 2 shows the results for the 3-class classifica-
tion scenario on the Facebook dataset. The row la-
bels denote the preprocessing configuration accord-
ing to Table 1. In most cases, maximum entropy
classifier significantly outperforms SVM. The com-
bination of all features (the last column) yields the
best results regardless to the preprocessing steps.
The reason might be that the involved character n-
gram feature captures subtle sequences which repre-
sent subjective punctuation or emoticons, that were
not covered by the emoticon feature. On average,
the best results were obtained when HPS stemmer
and lowercasing or phonetic transcription were in-
volved (lines ShCl and ShPe). This configuration
significantly outperforms other preprocessing tech-
niques for token-based features (see column Unigr
+ bigr + POS + emot.).
In the second scenario we evaluated various
TFIDF weighting schemes for binary sentiment
classification. The results are shown in Table 3.
The three-character notation consists of term fre-
quency, inverse document frequency, and normal-
ization. Due to a large number of possible combi-
nations, we report only the most successful ones,
17We ignore the bipolar posts in the current research.
namely Augmented?a and LogAve?L term fre-
quency, followed by Delta Smoothed??(t?), Delta
Smoothed Prob.??(p?), and Delta BM25??(k)
inverse document frequency; normalization was not
involved. We can see that the baseline (the first col-
umn bnn) is usually outperformed by any weighted
TFIDF technique. Moreover, using any kind of
stemming (the row entitled various*) significantly
improves the results. For the exact formulas of the
delta TFIDF variants please refer to (Paltoglou and
Thelwall, 2010).
We also tested the impact of TFIDF word fea-
tures when added to other features from the first sce-
nario (refer to Table 2). Column FS1 in Table 3 dis-
plays results for a feature set with the simple binary
presence-of-the-word feature (binary unigrams). In
the last column FS2 we replaced this binary feature
with TFIDF weighted feature a?(t?)n. It turned out
that the weighed form of word feature does not im-
prove the performance, when compared with sim-
ple binary unigram feature. Furthermore, a set of
different features (words, bigrams, POS, emoticons,
character n-grams) significantly outperforms a sin-
gle TFIDF weighted feature.
We also report the effect of the dataset size on
the performance. We randomly sampled 10 subsets
from the dataset (1k, 2k, etc.) and tested the per-
formance; still using 10-fold cross validation. We
took the most promising preprocessing configura-
tion (ShCl) and MaxEnt classifier. As can be seen in
Figure 2, while the dataset grows to approx 6k?7k
items, the performance rises for most combinations
of features. At 7k-items dataset, the performance
begins to reach its limits for most combinations of
features and hence adding more data does not lead
to a significant improvement.
5.1.1 Upper limits of automatic sentiment
analysis
To see the upper limits of the task itself, we also
evaluate the annotator?s judgments. Although the
gold labels were chosen after a consensus of at least
two people, there were many conflicting cases that
must have been solved by a third or even a fourth
person. Thus even the original annotators do not
achieve 1.00 F-measure on the gold data.
We present ?performance? results of both annota-
tors and of the best system as well (MaxEnt classi-
70
Facebook dataset, 3 classes
Unigrams Unigr + bigrams Unigr + bigr + Unigr + bigr + Unigr + bigr + POS +
POS features POS + emot. emot. + char n-grams
MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM
SnCk 0.63 0.64 0.63 0.64 0.66 0.64 0.66 0.64 0.69 0.67
SnCl 0.63 0.64 0.63 0.64 0.66 0.63 0.66 0.63 0.69 0.68
SlCk 0.65 0.67 0.66 0.67 0.68 0.66 0.67 0.66 0.69 0.67
SlCl 0.65 0.67 0.65 0.67 0.68 0.66 0.69 0.66 0.69 0.67
ShCk 0.66 0.67 0.66 0.67 0.68 0.67 0.67 0.67 0.69 0.67
ShCl 0.66 0.66 0.66 0.67 0.69 0.67 0.69 0.67 0.69 0.67
SnPe 0.64 0.65 0.64 0.65 0.67 0.65 0.67 0.65 0.68 0.68
SlPe 0.65 0.67 0.65 0.67 0.68 0.67 0.67 0.66 0.68 0.67
ShPe 0.66 0.67 0.66 0.67 0.69 0.66 0.69 0.66 0.68 0.67
Lp 0.64 0.65 0.63 0.65 0.67 0.64 0.67 0.65 0.68 0.67
Table 2: Results on the Facebook dataset, classification into 3 classes. Macro F-measure, 95% confidence interval
= ?0.01. Bold numbers denote the best results.
Facebook dataset, positive and negative classes only
bnn a?(t?)n a?(p?)n a?(k)n L?(t?)n L?(p?)n L?(k)n FS1 FS2
SnCk 0.83 0.86 0.86 0.86 0.85 0.86 0.86 0.90 0.89
SnCl 0.84 0.86 0.86 0.86 0.86 0.86 0.86 0.90 0.90
various* 0.85 0.88 0.88 0.88 0.88 0.88 0.88 0.90 0.90
SnPe 0.84 0.86 0.86 0.86 0.86 0.86 0.86 0.90 0.90
Lp 0.84 0.86 0.85 0.85 0.86 0.86 0.86 0.88 0.88
* same results for ShCk, ShCl, SlCl, SlPe, SlCk, and ShPe
FS1: Unigr + bigr + POS + emot. + char n-grams
FS2: a?(t?)n + bigr + POS + emot. + char n-grams
Table 3: Results on the Facebook dataset for various TFIDF-weighted features, classification into 2 classes. Macro F-
measure, 95% confidence interval = ?0.01. Underlined numbers show the best results for TFIDF-weighted features.
Bold numbers denote the best overall results.
Figure 2: Performance wrt. data size. Using ShCl pre-
processing and MaxEnt classifier.
fier, all features, ShCl preprocessing). Table 4 shows
the results as confusion matrices. For each class
(p?positive, n?negative, 0?neutral) we also re-
port precision, recall, and F-measure. The row head-
ings denote gold labels, the column headings repre-
sent values assigned by the annotators or the sys-
tem.18 The annotators? results show what can be ex-
pected from a ?perfect? system that would solve the
task the way a human would.
In general, both annotators judge all three classes
with very similar F-measure. By contrast, the sys-
tem?s F-measure is very low for negative posts (0.54
vs. ? 0.75 for neutral and positive). We offer the
following explanation. First, many of the negative
posts surprisingly contain happy emoticons, which
18Even though the task has three classes, the annotators also
used ?b? for ?bipolar and ??? for ?cannot decide?.
71
Annotator 1
0 n p ? b P R Fm
0 4867 136 115 2 54 .93 .94 .93
n 199 1753 6 0 33 .93 .88 .90
p 175 6 2376 0 30 .95 .92 .93
Macro Fm: .92
Annotator 2
0 n p ? b P R Fm
0 4095 495 573 3 8 .95 .79 .86
n 105 1878 6 0 2 .79 .94 .86
p 100 12 2468 3 4 .81 .95 .88
Macro Fm: .86
Best system
0 n p P R Fm
0 4014 670 490 .74 .78 .76
n 866 1027 98 .57 .52 .54
p 563 102 1922 .77 .74 .75
Macro Fm: .69
Table 4: Confusion matrices for three-class classification.
?Best system? configuration: all features (unigram, bi-
gram, POS, emoticons, character n-grams), ShCl prepro-
cessing, and MaxEnt classifier. 95% confidence interval
= ?0.01.
could be a misleading feature for the classifier. Sec-
ond, the language of the negative posts in not as ex-
plicit as for the positive ones in many cases; the neg-
ativity is ?hidden? in irony, or in a larger context (i.e.,
?Now I?m sooo satisfied with your competitor :))?).
This remains an open issue for the future research.
5.2 Product and movie reviews
For the other two datasets, the product reviews and
movie reviews, we slightly changed the configura-
tion. First, we removed the character n-grams from
the feature sets, otherwise the feature space would
become too large for feasible computing. Second,
we abandoned SVM as it became computationally
infeasible for such a large datasets.
Table 5 (left-hand part) presents results on the
product reviews. The combination of unigrams and
bigrams works best, almost regardless of the prepro-
cessing. By contrast, POS features rapidly decrease
the performance. We suspect that POS features do
not carry any useful information in this case and by
introducing a lot of ?noise? they cause that the op-
timization function in the MaxEnt classifier fails to
find a global minimum.
In the right-hand part of Table 5 we can see the
results on the movie reviews. Again, the bigram fea-
ture performs best, paired with combination of HPS
stemmer and phonetic transcription (ShPe). Adding
POS-related features causes a large drop in perfor-
mance. We can conclude that for larger texts, the
bigram-based feature outperforms unigram features
and, in some cases, a proper preprocessing may fur-
ther significantly improve the results.
6 Conclusion
This article presented an in-depth research of super-
vised machine learning methods for sentiment ana-
lysis of Czech social media. We created a large
Facebook dataset containing 10,000 posts, accom-
panied by human annotation with substantial agree-
ment (Cohen?s ? 0.66). The dataset is freely avail-
able for non-commercial purposes.19 We thoroughly
evaluated various state-of-the-art features and clas-
sifiers as well as different language-specific prepro-
cessing techniques. We significantly outperformed
the baseline (unigram feature without preprocess-
ing) in three-class classification and achieved F-
measure 0.69 using a combination of features (un-
igrams, bigrams, POS features, emoticons, charac-
ter n-grams) and preprocessing techniques (unsu-
pervised stemming and phonetic transcription). In
addition, we reported results in two other domains
(movie and product reviews) with a significant im-
provement over the baseline.
To the best of our knowledge, this article is the
only of its kind that deals with sentiment analysis
in Czech social media in such a thorough manner.
Not only it uses a dataset that is magnitudes larger
than any from the related work, but also incorporates
state-of-the-art features and classifiers. We believe
that the outcomes of this article will not only help
to set the common ground for sentiment analysis for
the Czech language but also help to extend the re-
search outside the mainstream languages in this re-
search field.
Acknowledgement
This work was supported by grant no. SGS-
2013-029 Advanced computing and information
19We encourage other researchers to download our dataset
for their research in the sentiment analysis field.
72
Product reviews, 3 classes Movie reviews, 3 classes
FS1 FS2 FS3 FS4 FS1 FS2 FS3 FS4
SnCk 0.70 0.74 0.52 0.49 0.76 0.77 0.71 0.61
SnCl 0.71 0.75 0.51 0.52 0.76 0.77 0.71 0.70
SlCk 0.67 0.75 0.59 0.55 0.78 0.78 0.73 0.72
SlCl 0.67 0.75 0.56 0.57 0.78 0.78 0.71 0.71
ShCk 0.67 0.75 0.57 0.57 0.78 0.78 0.74 0.72
ShCl 0.67 0.74 0.55 0.57 0.77 0.78 0.73 0.73
SnPe 0.69 0.74 0.50 0.55 0.77 0.78 0.69 0.72
SlPe 0.67 0.75 0.55 0.57 0.78 0.78 0.73 0.73
ShPe 0.68 0.74 0.56 0.59 0.78 0.79 0.74 0.73
Lp 0.66 0.75 0.56 0.57 0.77 0.77 0.68 0.70
Table 5: Results on the product and movie review datasets, classification into 3 classes. FSx denote different feature
sets. FS1 = Unigrams; FS2 = Uni + bigrams; FS3 = Uni + big + POS features; FS4 = Uni + big + POS + emot. Macro
F-measure, 95% confidence interval ?0.002 (products), ?0.003 (movies). Bold numbers denote the best results.
systems and by the European Regional Develop-
ment Fund (ERDF), project ?NTIS - New Tech-
nologies for Information Society?, European Cen-
ter of Excellence, CZ.1.05/1.1.00/02.0090. The
access to computing and storage facilities owned
by parties and projects contributing to the Na-
tional Grid Infrastructure MetaCentrum, provided
under the programme ?Projects of Large Infrastruc-
ture for Research, Development, and Innovations?
(LM2010005) is highly acknowledged.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, LSM ?11, pages 30?38,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Julie Kane Ahkter and Steven Soria. 2010. Sentiment
analysis: Facebook status messages. Technical report,
Stanford University. Final Project CS224N.
Alexandra Balahur and Hristo Tanev. 2012. Detecting
entity-related events and sentiments from tweets us-
ing multilingual resources. In Proceedings of the 2012
Conference and Labs of the Evaluation Forum Infor-
mation Access Evaluation meets Multilinguality, Mul-
timodality, and Visual Analytics.
Ben Blamey, Tom Crick, and Giles Oatley. 2012. R U
: -) or : -( ? character- vs. word-gram feature selec-
tion for sentiment classification of OSN corpora. In
Proceedings of AI-2012, The Thirty-second SGAI In-
ternational Conference on Innovative Techniques and
Applications of Artificial Intelligence, pages 207?212.
Springer.
A. Celikyilmaz, D. Hakkani-Tu?r, and Junlan Feng. 2010.
Probabilistic model-based sentiment analysis of twit-
ter messages. In Spoken Language Technology Work-
shop (SLT), 2010 IEEE, pages 79?84. IEEE.
Ljiljana Dolamic and Jacques Savoy. 2009. Indexing and
stemming approaches for the czech language. Infor-
mation Processing and Management, 45(6):714?720,
November.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 42?47, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Jarmila
Panevova?, Petr Sgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, and Marie Mikulova?. 2006. Prague de-
pendency treebank 2.0. Linguistic Data Consortium,
Philadelphia.
Mohammad Sadegh Hajmohammadi, Roliana Ibrahim,
and Zulaiha Ali Othman. 2012. Opinion mining and
sentiment analysis: A survey. International Journal of
Computers & Technology, 2(3).
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
73
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages 168?
177, New York, NY, USA. ACM.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proceedings of the Fifth In-
ternational Conference on Weblogs and Social Media,
Barcelona, Catalonia, Spain, July 17-21, 2011. The
AAAI Press.
Gustavo Laboreiro, Lu??s Sarmento, Jorge Teixeira, and
Euge?nio Oliveira. 2010. Tokenizing micro-blogging
messages using a text classification approach. In Pro-
ceedings of the fourth workshop on Analytics for noisy
unstructured text data, AND ?10, pages 81?88, New
York, NY, USA. ACM.
Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining Text Data,
pages 415?463. Springer.
Roque Lo?pez, Javier Tejada, and Mike Thelwall. 2012.
Spanish sentistrength as a tool for opinion mining pe-
ruvian facebook and twitter. In Artificial Intelligence
Driven Solutions to Business and Engineering Prob-
lems, pages 82?85. ITHEA, Sofia, Bulgaria.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
Justin Martineau and Tim Finin. 2009. Delta TFIDF:
An improved feature space for sentiment analysis. In
Proceedings of the Third International Conference on
Weblogs and Social Media, ICWSM 2009, San Jose,
California, USA. The AAAI Press.
A. Montejo-Ra?ez, E. Mart??nez-Ca?mara, M. T. Mart??n-
Valdivia, and L. A. Uren?a Lo?pez. 2012. Random
walk weighting over sentiwordnet for sentiment po-
larity detection on twitter. In Proceedings of the 3rd
Workshop in Computational Approaches to Subjectiv-
ity and Sentiment Analysis, WASSA ?12, pages 3?10,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the International Conference on Language Re-
sources and Evaluation, LREC 2010. European Lan-
guage Resources Association.
Georgios Paltoglou and Mike Thelwall. 2010. A study of
information retrieval weighting schemes for sentiment
analysis. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
?10, pages 1386?1395, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing - Volume 10, EMNLP ?02, pages 79?
86, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
James Pustejovsky and Amber Stubbs. 2013. Natural
Language Annotation for Machine Learning. O?Reilly
Media, Sebastopol, CA 95472.
Josef Steinberger, Polina Lenkova, Mijail Alexandrov
Kabadjov, Ralf Steinberger, and Erik Van der Goot.
2011. Multilingual entity-centered sentiment analy-
sis evaluated by parallel corpora. In Proceedings of
the 8th International Conference on Recent Advances
in Natural Language Processing, RANLP?11, pages
770?775.
Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann,
Ali Hurriyetoglu, Mijail Alexandrov Kabadjov, Polina
Lenkova, Ralf Steinberger, Hristo Tanev, Silvia
Va?zquez, and Vanni Zavarella. 2012. Creating senti-
ment dictionaries via triangulation. Decision Support
Systems, 53:689??694.
E.A. Stepanov and G. Riccardi. 2011. Detecting gen-
eral opinions from customer surveys. In Data Mining
Workshops (ICDMW), 2011 IEEE 11th International
Conference on, pages 115?122.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Mikalai Tsytsarau and Themis Palpanas. 2012. Survey
on mining subjective data on the web. Data Mining
and Knowledge Discovery, 24(3):478?514, May.
Kater?ina Veselovska?, Jan Hajic? Jr., and Jana S?indlerova?.
2012. Creating annotated resources for polarity classi-
fication in Czech. In Proceedings of KONVENS 2012,
pages 296?304. O?GAI, September. PATHOS 2012
workshop.
Liang-Chih Yu, Jheng-Long Wu, Pei-Chann Chang, and
Hsuan-Shou Chu. 2013. Using a contextual entropy
model to expand emotion words and their intensity
for the sentiment classification of stock market news.
Knowledge Based Syst, 41:89?97.
Kunpeng Zhang, Yu Cheng, Yusheng Xie, Daniel Honbo,
Ankit Agrawal, Diana Palsetia, Kathy Lee, Wei keng
Liao, and Alok N. Choudhary. 2011. SES: Sentiment
elicitation system for social media data. In Data Min-
ing Workshops (ICDMW), 2011 IEEE 11th Confer-
ence on, Vancouver, BC, Canada, December 11, 2011,
pages 129?136. IEEE.
Dan Zhang, Luo Si, and Vernon J. Rego. 2012. Senti-
ment detection with auxiliary data. Information Re-
trieval, 15(3-4):373?390.
74
Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 110?118,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Semi-automatic Acquisition of Lexical Resources and Grammars for
Event Extraction in Bulgarian and Czech
Hristo Tanev
Joint Research Centre
European Commission
via Fermi 2749, Ispra
Italy
hristo.tanev@jrc.ec.europa.eu
Josef Steinberger
University of West Bohemia
Faculty of Applied Sciences
Department of Computer Science and Engineering
NTIS Centre Univerzini 8, 30614 Plzen
Czech Republic
jstein@kiv.zcu.cz
Abstract
In this paper we present a semi-automatic
approach for acqusition of lexico-syntactic
knowledge for event extraction in two
Slavic languages, namely Bulgarian and
Czech. The method uses several weakly-
supervised and unsupervised algorithms,
based on distributional semantics. More-
over, an intervention from a language ex-
pert is envisaged on different steps in the
learning procedure, which increases its ac-
curacy, with respect to unsupervised meth-
ods for lexical and grammar learning.
1 Introduction
Automatic detection and extraction of events from
online news provide means for tracking the devel-
opments in the World politics, economy and other
important areas of life.
Event extraction is a branch of information ex-
traction, whose goal is the automatic retrieval of
structured information about events described in
natural language texts. Events include interac-
tions among different entities, to each of which
an event-specific semantic role can be assigned.
This role reflects the way in which the entity par-
ticipates in the event and interacts with the other
entities. For example, in the fragment ?Three peo-
ple were injured in a building collapse?, the phrase
?three people? may be assigned a semantic role
injured ? victim. The list of semantic roles de-
pends on the adopted event model.
The event extraction technology may decrease
the information overload, it allows automatic con-
version of unstructured text data into structured
one, it can be used to pinpoint interesting news ar-
ticles, also extracted entities and their correspond-
ing semantic roles can provide brief summaries of
the articles.
Using lexico-syntactic knowledge is one of
the promising directions in modeling the event-
specific semantic roles (Hogenboom et al, 2011).
While for English linear patterns seem to work
quite well (Tanev et al, 2008), for other lan-
guages,where word ordering is more free, cas-
caded grammars proved to improve the results
(Zavarella et al, 2008). In particular, Slavic lan-
guages are more free-order than English; conse-
quently, using cascaded grammars may be consid-
ered a relevant approach.
In this paper we present an ongoing effort
to build event extraction cascaded grammars for
Bulgarian and Czech in the domain of violent
news. To achieve this goal we put forward a
semi-automatic approach for building of event ex-
traction grammars, which uses several weakly-
supervised algorithms for acquisition of lexical
knowledge, based on distributional semantics and
clustering. Moreover, the lexical knowledge is
learned in the form of semantic classes, which then
can be used as a basis for building of a domain-
specific ontology.
To the best of our knowledge, there are no
previous attempts to perform event extraction for
Slavic languages, apart from the work presented in
(Turchi et al, 2011).
The importance of Czech and Bulgarian lan-
guages comes from the geopolitical positions of
the countries where they are spoken: Czech Re-
public is in a central geographical position be-
tween Eastern and Western Europe; Bulgaria is on
the borders of the European Union, on a crossroad
between Europe and Asia, surrounded by different
cultures, languages and religions. These geopo-
litical factors contribute to the importance of the
news from Czech Republic and Bulgaria and con-
sequently make automatic event extraction from
these news an useful technology for political an-
alysts.
The paper has the following structure: In sec-
tion 2 we make a short overview of the related ap-
110
proaches; in section 3 we describe our method for
lexical and grammar learning; section 4 presents
our experiments and evaluation for Bulgarian and
Czech languages and section 5 discusses the out-
come of the experiments and some future direc-
tions.
2 Related Work
There are different approaches for event extrac-
tion. Most of the work up to now has aimed
at English (see among the others (Naughton et
al., 2006) and (Yangarber et al, 2000)), however
(Turchi et al, 2011) presented automatic learning
of event extraction patterns for Russian, English
and Italian.
Our work is based on weakly supervised algo-
rithms for learning of semantic classes and pat-
terns, presented in (Tanev et al, 2009) and (Tanev
and Zavarella, 2013); these approaches are based
on distributional semantics. There are different
other methods which use this paradigm: A con-
cept and pattern learning Web agent, called NELL
(Never Ending Language Learning) is presented in
(Carlson et al, 2010). Parallel learning of seman-
tic classes and patterns was presented in (Riloff
and Jones, 1999). However these approaches do
not try to derive grammars from the acquired re-
sources, but stop at purely lexical level.
Relevant to our approach are the grammar learn-
ing approaches. A survey of supervised and unsu-
pervised approaches is presented in (D?Ulizia et
al., 2011). The supervised ones require annotation
of big amounts of data which makes the develop-
ment process long and laborious. On the other
hand, unsupervised methods try to generalize all
the training data by using different heuristics like
the minimal description length. Since for event
extraction only specific parts of the text are ana-
lyzed, in order to use unsupervised grammar ac-
quisition methods for learning of event extraction
grammars, one should collect the exact phrases
which describe the events. In practice, this would
transform the unsupervised methods into super-
vised ones. With respect to the state-of-the art
grammar inference approaches, our method allows
for more interaction between the grammar expert
and the learning system. Moreover, our learning
starts from lexical items and not from annotated
texts, which decreases the development efforts.
3 Semi-automatic Learning of Lexica
and Grammars
The event extraction grammar, exploited in our ap-
proach is a cascaded grammar which on the first
levels detects references to entities, like people,
groups of people, vehicles, etc. On the upper lev-
els our cascaded grammar detects certain events
in which these entities participate: In the domain
of violent news, people may get killed, wounded,
kidnapped, arrested, etc. If we consider as an ex-
ample the following Bulgarian text: ????? ???-
????????? ???? ?????????? ????? ?? ?????
?? ???????????? ? ??????? ?? ?????????
(?A group of protesters were arrested yesterday
during demonstrations in the centre of the capi-
tal?), our grammar will detect first that ?????
???????????? (?A group of protesters?) refers
to a group of people and then, it will find that
????? ???????????? ???? ??????????'? (?A
group of protesters were arrested?) refers to an ar-
rest event where the aforementioned group of peo-
ple is assigned the semantic role arrested.
In order to build such a grammar, we acquire
semi-automatically the following resources:
1. a dictionary of words which refer to peo-
ple and other entities in the required domain-
specific context, e.g. ?????? , ?voja?k? (
?soldier? in Bulgarian and Czech), ???? ,
zena ( ?woman? in Bulgarian and Czech),
etc.
2. a list of modifiers and other words which
appear in phrases referring to those entities,
e.g. ??????? , ?civiln??? (?civil? in Bulgar-
ian and Czech), ???? (?NATO?), etc.
3. grammar rules for parsing entity-referring
phrases. For example, a simple rule can be:
PERSON PHRASE ? PER
connector ORG
where PER and ORG are words and multi-
words, referring to people and organizations,
connector ? ?? for Bulgarian or
connector ? ?? (empty string) for Czech.
This rule can parse phrases like ?????? ??
???? or ?voja?k NATO? (?NATO soldier?)
4. a list of words which participate in event
patterns like ????????? , ?zadrz?en? (?ar-
rested? in Bulgarian and Czech) or ???? ,
?zabit? ( ?killed? in Bulgarian and Czech).
111
5. a set of grammar rules which parse event-
description phrases. For example, a simple
rule can be:
KILLING ? PER connector
KILLED PARTICIPLE
where connector ? ???? for Bulgarian
or connector ? byl for Czech.
This rule will recognize phrases like ???-
??? ?? ???? ???? ???? or ?Voja?k
NATO byl zabit? (?A NATO soldier was
killed? in Bulgarian and Czech?)
In order to acquire this type of domain lexica
and a grammar, we make use of a semi-automatic
method which acquires in parallel grammar rules
and dictionaries. Our method exploits several
state-of-the-art algorithms for expanding of se-
mantic classes, distributional clustering, learning
of patterns and learning of modifiers, described in
(Tanev and Zavarella, 2013). The semantic class
expansion algorithm was presented also in (Tanev
et al, 2009). These algorithms are multilingial and
all of them are based on distributional semantics.
They use a non-annotated text corpus for training.
We integrated these algorithms in a semi-
automatic schema for grammar learning, which is
still in phase of development. Here is the basic
schema of the approach:
1. The user provides a small list of seed words,
which designate people or other domain-
specific entities, e.g.? soldiers?,?civilians?,
?fighters? (We will use only English-
language examples for short, however the
method is multilingual and consequently ap-
plicable for Czech and Bulgarian).
2. Using the multilingual semantic class ex-
pansion algorithm (Tanev et al, 2009)
other words are learned (e.g. ?policemen?,
?women?, etc.), which are likely to belong
to the same semantic class. First, the algo-
rithm finds typical contextual patterns for the
seed words from not annotated text. For ex-
ample, all the words, referring to people tend
to appear in linear patterns like [PEOPLE]
were killed, thousands of [PEOPLE] , [PEO-
PLE] are responsible, etc. Then, other words
which tend to participatre in the same con-
textual patterns are extracted from the unan-
notated text corpus. In such a way the al-
gorithm learns additional words like ?police-
men?, ?killers?, ?terrorists?, ?women?, ?chil-
dren?, etc.
3. Since automatic approaches for learning of
semantic classes always return some noise
in the output, a manual cleaning by a do-
main expert takes place as a next step of our
method.
4. Learning modifiers: At this step, for each se-
mantic class learned at the previous step (e.g.
PEOPLE, we run the modifier learning algo-
rithm, put forward by (Tanev and Zavarella,
2013) , which learns domain-specific syn-
tactic modifiers. Regarding the class PEO-
PLE), the modifiers will be words like ?
Russian?, ?American?, ?armed?, ?unarmed?,
?masked?, etc. The modifier learning algo-
rithm exploits the principle that the context
distribution of words from a semantic class
is most likely similar to the context distribu-
tion of these words with syntactic modifiers
attached. The algorithm uses this heuristic
and does not use any morphological infor-
mation to ensure applications in multilingual
settings.
5. Manual cleaning of the modifier list
6. Adding the following grammar rule at the
first level of the cascaded grammar, which
uses the semantic classes and modifiers,
learned at the previous steps:
Entity(class : C) ? (LModif(class :
C))? Word(class : C) (RModif(class :
C))?
This rule parses phrases, like ?masked gun-
men from IRA?, referring to an entity from
a semantic class C, e.g. PERSON. It should
consist of a sequence of 0 or more left mod-
ifiers for this class, e.g. ?masked?, a word
from this class (?gunmen? in this example)
and a sequence of 0 or more right modifiers
(?from IRA? in the example?).
7. Modifiers learned by the modifier learning
algorithm do not cover all the variations in
the structure of the entity-referring phrases,
since sometimes the structure is more com-
plex and cannot be encoded through a list of
lexical patterns. Consider, for example, the
following phrase ?soldiers from the special
forces of the Russian Navy?. There is a little
112
chance that our modifier learning algorithm
acquires the string ?from the special forces
of the Russian Navy?, on the other hand
the following two grammar rules can do the
parsing:
RIGHT PEOPLE MODIFIER ?
?from??MILITARY FORMATION
MILITARY FORMATION ?
LeftModMF ? MFW RightModMF?
where MILITARY FORMATION is a
phrase which refers to some organization (in
the example, shown above, the phrase is ?the
special forces of the Russian Navy?), MFW
is a term which refers to a military formation
(?the special forces?) and LeftModMF and
RightModMF are left and right modifiers
of the military formation entity (for example,
a right modifier is?of the Russian Navy?).
In order to learn such more complex struc-
ture, we propose the following procedure:
(a) The linguistic expert chooses seman-
tic classes, for which more elaborated
grammar rules should be developed.
Let?s take for example the class PEO-
PLE.
(b) Using the context learning sub-
algorithm of the semantic class expan-
sion, used in step 2, we find contextual
patterns which tend to co-occur with
this class. Apart from the patterns
shown in step 2, we also learn patterns
like [PEOPLE] from the special forces,
[PEOPLE] from the Marines, [PEO-
PLE] from the Russian Federation,
[PEOPLE] from the Czech Republic,
[PEOPLE] with guns, [PEOPLE] with
knives, [PEOPLE] with masks, etc.
(c) We generalize contextual patterns, in or-
der to create grammar rules. In the first
step we create automatically syntactic
clusters separately for left and right
contextual patterns. Syntactic clustering
puts in one cluster patterns where the
slot and the content-bearing words are
connected by the same sequence of stop
words. In the example, shown above,
we will have two syntactic clusters of
patterns: The first consists of patterns
which begin with [PEOPLE] from the
and the second contains the patterns,
which start with [PEOPLE] with. These
clusters can be represented via grammar
rules in the following way:
RIGHT PEOPLE MODIFIER ??from
the? X
X? (special forces | Marines | Russian
Federation | Czech Republic)
RIGHT PEOPLE MODIFIER ?
?with? Y
Y? (knives | guns | masks)
(d) Now, several operations can be done
with the clusters of words inside the
grammar rules:
? Words inside a cluster can be clus-
tered further on the basis of their
semantics. In our system we use
bottom up agglomerative cluster-
ing, where each word is represented
as a vector of its context features.
Manual cleaning and merging of
the clusters may be necessary af-
ter this automatic process. If words
are not many, only manual clus-
tering can also be an option. In
the example above ?special forces?
and ?Marines? may form one clus-
ter, since both words designate the
class MILITARY FORMATION and
the other two words designate coun-
tries and also form a separate seman-
tic class.
? In the grammar introduce new non-
terminal symbols, corresponding to
the newly learnt semantic classes.
Then, in the grammar rules substi-
tute lists of words with references
to these symbols. (Still we do
modification of the grammar rules
manually, however we envisage to
automate this process in the future).
For example, the rule
X ? (special forces | Marines
| Russian Federation | Czech Re-
public)
will be transformed into
X ? (MILITARY FORMATION |
COUNTRY)
MILITARY FORMATION ? (spe-
cial forces | Marines)
COUNTRY ? (Russian Federation
113
PEOPLE? (NUMBER ?? (from) )? PEOPLEa
Example: ????? ?? ??????????? ??????? (?two of the Bulgarian soldiers?)
PEOPLEa? PEOPLEb ((?? (from) | ?? (of) | ? (in)) (ORG | PLACE ))*
Example: ????????? ?? ??? (?staff from the MVR (Ministry of the Internal Affairs)?)
PEOPLEb? LeftPM* PEOPLE W RightPM*
Example: ?????????? ?????????? ? ??????? (?unknown attackers with hoods?)
Table 1: Rules for entity recognition for the Bulgarian language
| Czech Republic)
? Clusters can be expanded by using
the semantic class expansion algo-
rithm, introduced before, followed
by manual cleaning. In our example,
this will add other words for MIL-
ITARY FORMATION and COUN-
TRY. Consequently, the range of the
phrases, parsable by the grammar
rules will be augmented.
(e) The linguistic expert may choose a sub-
set of the semantic classes, obtained
on the previous step, (e.g. the the se-
mantic class MILITARY FORMATION)
to be modeled further via extending the
grammar with rules about their left and
right modifiers. Then, the semantic class
is recursively passed to the input of this
grammar learning procedure.
8. Learning event patterns: In this step we learn
patterns like [PEOPLE] ???? ??????????
or [PEOPLE] ?byl zadrz?en? ([PEOPLE]
were/was arrested in Bulgarian and Czech).
The pattern learning algorithm collects con-
text patterns for one of the considered en-
tity categories (e.g. [PEOPLE]. This is done
through the context learning sub-algorithm
described in step 2. Then, it searches for
such context patterns, which contain words,
having distributional similarity to words, de-
scribing the target event (e.g. ?????????? ,
?zadrz?en? (?arrested?)).
For example, if we want to learn patterns for
arrest events in Bulgarian, the algorithm first
learns contexts of [PEOPLE]. These con-
texts are [PEOPLE] ???? ????? ([PEO-
PLE] were killed), ?????? [PEOPLE]
(thousands of [PEOPLE]), [PEOPLE] ????
???????? ([PEOPLE] were captured), etc.
Then, we pass to the semantic expansion al-
gorithm (see step 2) seed words which ex-
press the event arrest, namely ?????????,
?????????? (?apprehended?, ?arrested?),
etc. Then, it will discover other similar words
like ???????? (?captured?). Finally, the
algorithm searches such contextual patterns,
which contain any of the seed and learnt
words. For example, the pattern [PEOPLE]
???? ???????? ([PEOPLE] were captured)
is one of the newly learnt patterns for arrest
events.
9. Generalizing the patterns: In this step we ap-
ply a generalization algorithm, described in
step 7 to learn grammar rules which parse
events. For example, two of the learned rules
for parsing of arrest events in Bulgarian are:
ARREST ? PEOPLE ???? (?were?)
ARREST PARTICIPLE
ARREST PARTICIPLE ? ( ??????????
(arrested) | ????????(captured) |
????????? (handcuffed) )
The outcome of this learning schema is a gram-
mar and dictionaries which recognize descriptions
of different types of domain-specific entities and
events, which happened with these entities. More-
over, the dictionaries describe semantic classes
from the target domain and can be used further for
creation of a domain ontology.
4 Experiments and Evaluation
In our experiments, we applied the procedure
shown above to learn grammars and dictionaries
for parsing of phrases, referring to people, groups
of people and violent events in Bulgarian and
Czech news. We used for training 1 million news
titles for Bulgarian and Czech, downloaded from
114
KILLING? KILL VERB (a (and) | i (and) | jeden (one) | jeden z (one of) )? [PEOPLE]
KILL VERB? (zabit (killed) | zabila | zahynul (died) | zabiti | ubodal (stabbed) | ubodala | ...)
KILLING? KILL ADJ [PEOPLE]
KILL ADJ? (mrtvou (dead) | mrtve?ho (dead) | ...)
KILLING? [PEOPLE] KILL VERBa
KILL VERBa? (zahynul (died) | zamr?el (died) | ...)
KILLING? [PEOPLE] byl (was) KILL VERBb
KILL VERBb? (zabit (killed) | ...)
Table 2: Rules for parsing of killing events and their victims in Czech
the Web and a small number of seed terms, refer-
ring to people and actions. We had more available
time to work for the Bulgarian language, that is
why we learned more complex grammar for Bul-
garian. Both for Czech and Bulgarian, we learned
grammar rules parsing event description phrases
with one participating entity, which is a person or
a group of people. This is simplification, since of-
ten an event contains more than one participant,
in such cases our grammar can detect the separate
phrases with their corresponding participants, but
currently it is out of the scope of the grammar to
connect these entities. The event detection rules
in our grammar are divided into semantic classes,
where each class of rules detects specific type of
events like arrest, killing, wounding, etc. and also
assigns an event specific semantic role to the par-
ticipating entity, e.g. victim, perpetrator, arrested,
kidnapped.
In order to implement our grammars, we used
the EXPRESS grammar engine (Piskorski, 2007).
It is a tool for building of cascaded grammars
where specific parts of the parsed phrase are as-
signed semantic roles. We used this last feature of
EXPRESS to assign semantic roles of the partici-
pating person entities.
For Czech we learned a grammar which de-
tects killings and their victims. For Bulgarian, we
learned a grammar, which parses phrases referring
to killings, woundings and their victims, arrests
and who is arrested, kidnappings and other violent
events with their perpetrators and targeted people.
4.1 Learning people-recognition rules
For Czech our entity extraction grammar was rel-
atively simple, since we learned just a dictionary
of left modifiers. Therefore, we skipped step 7 in
the learning schema, via which more elaborated
entity recognition grammars are learned. Thus,
the Czech grammar for recognizing phrases,
referring to people contains the following rules:
PEOPLE? LeftMod* PEOPLE TERM
LeftMod ? (?mladou? (?young?) |
?nezna?me?mu?(?unknown?) | ?stars???? (?old?) |
...)
PEOPLE TERM ? (?voja?ci? (?soldiers?) |
?civiliste??(?civilians?) | ?z?enu? (?woman?) |
...)
This grammar recognizes phrases like ?mladou
z?enu? (?young woman? in Czech). Two dictionar-
ies were acquired in the learning process: A dic-
tionary of nouns, referring to people and left mod-
ifiers of people. The dictionary of people-referring
nouns contains 268 entries, obtained as a result
of the semantic class expansion algorithm. We
used as a seed set 17 words like ?muz?i? (?men?),
?voia?ci? (?soldiers?), etc. The algorithm learned
1009 new words and bigrams, 251 of which were
correct (25%), that is refer to people. One problem
here was that not all morphological forms were
learned by our class expansion algorithm. In a
language with rich noun morphology, as Czech is,
this influenced on the coverage of our dictionaries.
After manual cleaning of the output from the
modifier learning algorithm, we obtained 603
terms; the learning accuracy of the algorithm was
found to be 55% .
For Bulgarian we learned a more elaborated
people recognition grammar, which is able to
parse more complex phrases like ???? ?? ?????-
?????? ?????????? (?one of the masked attack-
ers?) and ????? ?? ?????????? ?????????? ?
???? (?soldiers from the Bulgarian contingent
in Iraq?). The most important rules which we
learned are shown in Table 1. In these rules PEO-
PLE W encodes a noun or a bigram which refers
to people, ORG is an organization; we learned
mostly organizations, related to the domain of se-
curity, such as different types of military and other
armed formations like ?????? ?? ???? (?secu-
115
rity forces?), also governmental organizations, etc.
PLACE stands for names of places and common
nouns, referring to places such as ?????????
(?the capital?). We also learned modifiers for these
categories and added them to the grammar. (For
simplicity, we do not show the grammar rules for
parsing ORG abd PLACE; we will just mention
that both types of phrases are allowed to have a se-
quence of left modifiers, one or more nouns from
the corresponding class and a sequence of 0 or
more right modifiers.) Both categories PLACE
and ORG were obtained in step 7 of the learn-
ing schema, when exploring the clusters of words
which appear as modifiers after the nouns, refer-
ring to people, like in the following example ???-
?? ?? ?????????? ?????????? (?soldiers from
the Bulgarian contingent? ); then, we applied man-
ual unification of the clusters and their subsequent
expansion, using the semantic class expansion al-
gorithm.
Regarding the semantic class expansion, with
20 seed terms we acquired around 2100 terms,
from which we manually filtered the wrong ones
and we left 1200 correct terms, referring to peo-
ple; the accuracy of the algorithm was found to be
57% in this case.
We learned 1723 nouns for organizations and
523 place names and common nouns. We did not
track the accuracy of the learning for these two
classes. We also learned 319 relevant modifiers
for people-referring phrases; the accuracy of the
modifier learning algorithm was found to be 67%
for this task.
4.2 Learning of event detection rules
This learning takes place in step 8 and 9 of
our learning schema. As it was explained, first
linear patterns like [PEOPLE] ?byl zadrz?en?
([PEOPLE] was arrested ) are learned, then
through a semi-automatic generalization process
these patterns are transformed into rules like:
ARREST? PEOPLE ?byl? ARREST VERB
In our experiments for Czech we learned gram-
mar rules and a dictionary which recognize dif-
ferent syntactic constructions, expressing killing
events and the victims. These rules encode 156
event patterns. The most important of these rules
are shown in Table 2. Part of the event rule learn-
ing process is expansion of a seed set of verbs, and
other words, referring to the considered event (in
this case killing).For this task the semantic class
expansion algorithm showed significantly lower
accuracy with respect to expanding sets of nouns -
only 5%. Nevertheless, the algorithm learned 54
Czech words, expressing killing and death.
For Bulgarian we learned rules for detection of
killing and its victims, but also rules for parsing of
wounding events, arrests, targeting of people in vi-
olent events, kidnapping, and perpetrators of vio-
lent events. These rules encode 605 event patterns.
Some of the rules are shown in Table 3.
4.3 Evaluation of event extraction
In order to evaluate the performance of our gram-
mars, we created two types of corpora: For the
precision evaluation we created bigger corpus of
randomly picked excerpts of news from Bulgar-
ian and Czech online news sources. More pre-
cisely, we used 7?550 news titles for Czech and
12?850 news titles in Bulgarian. We also car-
ried out a preliminary recall evaluation on a very
small text collection: We manually chose sen-
tences which report about violent events of the
types which our grammars are able to capture. We
selected 17 sentences for Czech and 28 for Bul-
garian. We parsed the corpora with our EXPRESS
grammars and evaluated the correctness of the ex-
tracted events. Since each event rule has assigned
an event type and a semantic role for the partic-
ipating people reference, we considered a correct
match only when both a correct event type and a
correct semantic role are assigned to the matched
text fragment. Table 4 shows the results from our
evaluation. The low recall in Czech was mostly
due to the insufficient lexicon for people and the
too simplistic grammar.
Language Precision Recall
Bulgarian 93% 39%
Czech 88% 6%
Table 4: Event extraction accuracy
5 Discussion
In this paper we presented a semi-automatic ap-
proach for learning of grammar and lexical knowl-
edge from unannotated text corpora. The method
is multilingual and relies on distributional ap-
proaches for semantic clustering and class expan-
sion.
116
KILLING? KILL VERB (???? (were) | ?? (are)) [PEOPLE]
KILL VERB? (???????? (killed) | ????? (killed) | ???????????? (shot to death) | ...)
KILLING? KILL PHRASE ?? (of) [PEOPLE]
KILL PHRASE? (???? ?????? (took the life) | ??????? ??????? (caused the death) | ...)
WOUNDING? WOUND VERB (???? (were) | ?? (are)) [PEOPLE]
WOUND VERB? (?????? (wounded) | ???????????? (injured) | ...)
ARREST? [PEOPLE] ARREST VERB
ARREST VERB? (?????????? (arrested) | ????????? (detained) | ...)
Table 3: Some event parsing rules for Bulgarian
We are currently developing event extraction
grammars for Czech and Bulgarian. Preliminary
evaluation shows promising results for the preci-
sion, while the recall is still quite low. One of
the factors which influences the law recall was
the insufficient number of different morphological
word variations in the learned dictionaries. The
morphological richness of Slavic languages can be
considered by adding morphological dictionaries
to the system or creating an automatic procedure
which detects the most common endings of the
nouns and other words and expands the dictionar-
ies with morphological forms.
Another problem in the processing of the
Slavic languages is their relatively free order.
To cope with that, often the grammar engineer
should introduce additional variants of already
learned grammar rules. This can be done semi-
automatically, where the system may suggest ad-
ditional rules to the grammar developer. This can
be done through development of grammar meta-
rules.
With respect to other approaches, grammars
provide transparent, easy to expand model of the
domain. The automatically learned grammars can
be corrected and extended manually with hand-
crafted rules and linguistic resources, such as mor-
phological dictionaries. Moreover, one can try
to introduce grammar rules from already existing
grammars. This, of course, is not trivial because of
the different formalisms exploited by each gram-
mar. It is noteworthy that the extracted semantic
classes can be used to create an ontology of the
domain. In this clue, parallel learning of a domain-
specific grammars and ontologies could be an in-
teresting direction for future research.
The manual efforts in the development of the
grammars and the lexical resources were mainly
cleaning of already generated lists of words and
manual selection and unification of word clus-
ters. Although we did not evaluate precisely the
invested manual efforts, one can estimate them
by the size of the automatically acquired word
lists and their accuracy, given in section Semi-
automatic Learning of Lexica and Grammars.
We plan to expand the Czech grammar with
rules for more event types. Also, we think to ex-
tend both the Bulgarian and the Czech event ex-
traction grammars and the lexical resources, so
that it will be possible to detect also disasters, hu-
manitarian crises and their consequences. This
will increase the applicability and usefulness of
our event extraction grammars.
Acknowledgments
This work was partially supported by project
?NTIS - New Technologies for Information
Society?, European Center of Excellence,
CZ.1.05/1.1.00/02.0090.
References
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, R. Este-
vam, J. Hruschka, and T. Mitchell. 2010. Toward an
architecture for never-ending language learning. In
Proceedings of the Twenty-Fourth AAAI Conference
on Artificial Intelligence (AAAI-10).
A. D?Ulizia, F. Ferri, and P. Grifoni. 2011. A survey of
grammatical inference methods for natural language
learning. Artificial Intelligence Review vol. 36 issue
1.
F. Hogenboom, F. Frasincar, U. Kaymak, and F. Jong.
2011. An overview of event extraction from text.
In Workshop on Detection, Representation, and Ex-
ploitation of Events in the Semantic Web (DeRiVE
2011) at ISWC 2011.
M. Naughton, N. Kushmerick, and J. Carthy.
2006. Event Extraction from Heterogeneous News
Sources. In Proceedings of the AAAI 2006 workshop
on Event Extraction and Synthesis, Menlo Park, Cal-
ifornia, USA.
117
J. Piskorski. 2007. ExPRESS ? Extraction Pattern
Recognition Engine and Specification Suite. In Pro-
ceedings of FSMNLP 2007.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence (AAAI 99).
H. Tanev and V. Zavarella. 2013. Multilingual learn-
ing and population of event ontologies. a case study
for social media. In P. Buitelaar and P. Cimiano, ed-
itors, Towards Multilingual Semantic Web (in press).
Springer, Berlin & New York.
H. Tanev, J. Piskorski, and M. Atkinson. 2008. Real-
Time News Event Extraction for Global Crisis Mon-
itoring. In Proceedings of NLDB 2008., pages 207?
218.
H. Tanev, V. Zavarella, J. Linge, M. Kabadjov, J. Pisko-
rski, M. Atkinson, and R. Steinberger. 2009. Ex-
ploiting Machine Learning Techniques to Build an
Event Extraction System for Portuguese and Span-
ish. Linguama?tica: Revista para o Processamento
Automa?tico das L??nguas Ibe?ricas, 2:550?566.
M. Turchi, V. Zavarella, and H. Tanev. 2011. Pat-
tern learning for event extraction using monolingual
statistical machine translation. In Proceedings of
Recent Advances in Natural Language Processing
(RANLP 2011), Hissar, Bulgaria.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Unsupervised Discovery of Scenario-
Level Patterns for Information Extraction. In
Proceedings of ANLP-NAACL 2000, Seattle, USA,
2000.
V. Zavarella, H. Tanev, and J. Piskorski. 2008. Event
Extraction for Italian using a Cascade of Finite-State
Grammars. In Proceedings of FSMNLP 2008.
118
Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 13?19,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Multi-document multilingual summarization corpus preparation, Part 2:
Czech, Hebrew and Spanish
Michael Elhadad
Ben-Gurion Univ.
in the Negev, Israel
elhadad@cs.bgu.ac.il
Sabino Miranda-Jim?nez
Instituto Polit?cnico
Nacional, Mexico
sabino_m@hotmail.com
Josef Steinberger
Univ. of
West Bohemia,
Czech Republic
jstein@kiv.zcu.cz
George Giannakopoulos
NCSR Demokritos, Greece
SciFY NPC, Greece
ggianna@iit.demokritos.gr
Abstract
This document overviews the strategy, ef-
fort and aftermath of the MultiLing 2013
multilingual summarization data collec-
tion. We describe how the Data Contrib-
utors of MultiLing collected and gener-
ated a multilingual multi-document sum-
marization corpus on 10 different lan-
guages: Arabic, Chinese, Czech, English,
French, Greek, Hebrew, Hindi, Romanian
and Spanish. We discuss the rationale be-
hind the main decisions of the collection,
the methodology used to generate the mul-
tilingual corpus, as well as challenges and
problems faced per language. This paper
overviews the work on Czech, Hebrew and
Spanish languages.
1 Introduction
In this document we present the language-
specific problems and challenges faced by Con-
tributors during the corpus creation process. To
facilitate the reader we repeat some information
found in the first part of the overview (Li et al,
2013): the MultiLing tasks and the main steps of
the corpus creation process.
2 The MultiLing tasks
There are two main tasks (and a single-
document multilingual summarization pilot de-
scribed in a separate paper) in MultiLing 2013:
Summarization Task This MultiLing task aims
to evaluate the application of (partially or
fully) language-independent summarization
algorithms on a variety of languages. Each
system participating in the task was called
to provide summaries for a range of differ-
ent languages, based on corresponding cor-
pora. In the MultiLing Pilot of 2011 the lan-
guages used were 7, while this year systems
were called to summarize texts in 10 differ-
ent languages: Arabic, Chinese, Czech, En-
glish, French, Greek, Hebrew, Hindi, Roma-
nian, Spanish. Participating systems were re-
quired to apply their methods to a minimum
of two languages.
The task was aiming at the real problem of
summarizing news topics, parts of which may
be described or may happen in different mo-
ments in time. We consider, similarly to Mul-
tiLing 2011(Giannakopoulos et al, 2011) that
news topics can be seen as event sequences:
Definition 1 An event sequence is a set of
atomic (self-sufficient) event descriptions, se-
quenced in time, that share main actors, lo-
cation of occurence or some other important
factor. Event sequences may refer to topics
such as a natural disaster, a crime investiga-
tion, a set of negotiations focused on a single
political issue, a sports event.
The summarization task requires to generate
a single, fluent, representative summary from
a set of documents describing an event se-
quence. The language of the document set
will be within the given range of 10 languages
and all documents in a set share the same lan-
guage. The output summary should be of the
same language as its source documents. The
output summary should be between 240 and
250 words.
Evaluation Task This task aims to examine how
well automated systems can evaluate sum-
maries from different languages. This task
takes as input the summaries generated from
automatic systems and humans in the Sum-
marization Task. The output should be a grad-
ing of the summaries. Ideally, we would want
the automatic evaluation to maximally corre-
late to human judgement.
13
The first task was aiming at the real problem of
summarizing news topics, parts of which may be
described or happen in different moments in time.
The implications of including multiple aspects of
the same event, as well as time relations at a vary-
ing level (from consequtive days to years), are still
difficult to tackle in a summarization context. Fur-
thermore, the requirement for multilingual appli-
cability of the methods, further accentuates the dif-
ficulty of the task.
The second task, summarization evaluation has
come to be a prominent research problem, based on
the difficulty of the summary evaluation process.
While commonly used methods build upon a few
human summaries to be able to judge automatic
summaries (e.g., (Lin, 2004; Hovy et al, 2005)),
there also exist works on fully automatic evalua-
tion of summaries, without human?model? sum-
maries (Louis and Nenkova, 2012; Saggion et al,
2010). The Text Analysis Conference has a sepa-
rate track, named AESOP (Dang and Owczarzak,
2009) aiming to test and evaluate different auto-
matic evaluation methods of summarization sys-
tems.
Given the tasks, a corpus needed to be gener-
ated, that would be able to:
? provide input texts in different languages to
summarization systems.
? provide model summaries in different lan-
guages as gold standard summaries, to also
allow for automatic evaluation using model-
dependent methods.
? provide human grades to automatic and hu-
man summaries in different languages, to
support the testing of summary evaluation
systems.
In the following section we show how these re-
quirements were met in MultiLing 2013.
3 Corpus collection and generation
The overall process of creating the corpus of
MultiLing 2013 was, similarly to MultiLing 2011,
based on a community effort. The main processes
consisting the generation of the corpus are as fol-
lows:
? Selection of a source corpus in a single lan-
guage.
? Translation of the source corpus to different
languages.
? Human summarization of corpus topics per
language.
? Evaluation of human summaries, as well as of
submitted system runs.
4 Language specific notes
In the following paragraphs we provide
language-specific overviews related to the corpus
contribution effort. The aim of these overviews is
to provide a reusable pool of knowledge for future
similar efforts.
In this document we elaborate on Czech, He-
brew, and Spanish languages. A second document
(Elhadad et al, 2013) elaborates on the rest of the
languages.
4.1 Czech language
The first part of the Czech subcorpus (10 top-
ics) was created for the multilingual pilot task at
TAC 2011. Five new topics were added for Mul-
tiling 2013. In total, 14 annotators participated in
the Czech corpus creation.
The most time consuming part of the annota-
tion work was the translation of the articles. The
annotators were not professional translators and
many topics required domain knowledge for cor-
rect translation. To be able to translate a per-
son name, the translator needs to know its correct
spelling in Czech, which is usually different from
English. The gender also plays an important role
in the translation, because a suffix ?ov?? must be
added to female surnames.
Translation of organisation names or person?s
functions within an organisation needs some do-
main knowledge as well. Complicated morphol-
ogy and word order in Czech (more free but some-
times very different fromEnglish) makes the trans-
lation even more difficult.
For the creation of model summaries the anno-
tator needed to analyse the topic well in order to
decide what is important and what is redundant.
Sometimes, it was very difficult, mainly in the
case of topics which covered a long period (even
5 years) and which contained articles sharing very
little information.
The main question of the evaluation part was
how to evaluate a summary which contains a read-
able, continuous text ? mainly the case of the
14
Group SysID Avg Perf
a B 4.75
a A 4.63
ab C 4.61
b D 4.21
b E 4.10
Table 1: Czech: Tukey?s HSD test groups for hu-
man summarizers
baseline system with ID6) ? however not impor-
tant information from the article cluster point of
view.
An overview of the Overall Responsiveness and
the corresponding average grades of the human
summarizers can be seen in Table 1. We note
that on average the human summaries are consid-
ered excellent (graded above 4 out of 5), but that
there exist statistically significant differences be-
tween summarizers, essentially forming two dis-
tinct groups.
4.2 Hebrew language
This section describes the process of preparing
the dataset for MultiLing 2013 in Hebrew: transla-
tion of source texts from English, and the summa-
rization for the translated texts, by the Ben Gurion
University Natural Language Processing team.
4.2.1 Translation Process
Four people participated in the translation and
the summarization of the dataset of the 50 news
articles: three graduate students, one a native En-
glish speaker with fluent Hebrew and the other two
with Hebrew as a mother tongue and very good
English skills. The process was supervised by a
professional translator with a doctoral degree with
experience in translation and scientific editing.
The average times to read an article was 2.5min-
utes (std. dev 1.2min), the average translation time
was 30 minutes (std. dev 15min), and the average
proofing time was 18.5min (std. dev 10.5min).
4.2.2 Translation Methodology
We tested two translation methodologies by dif-
ferent translators. In some of the cases, translation
was aided with Google Translate1, while in other
cases, translation was performed from scratch.
In the cases where texts were first translated
using Google Translate, the translator reviewed
1See http://translate.google.com/.
the text and edited changes according to her judg-
ment. Relying on the time that was reported for the
proofreading of each translation, we could tell that
texts that were translated using this method, re-
quired longer periods of proofreading (and some-
times more time was required to proofread than to
translate). This is most likely because once the au-
tomatic translation was available, the human trans-
lator was biased by the automatic outcome, re-
maining anchored? to the given text with reduced
criticism and creativity.
Translating the text manually, aided with online
or offline dictionaries, Wikipedia and news site on
the subject that was translated, showed better qual-
ity as analysis of time shows, where the ratio be-
tween the time needed to proofread was less than
half.
In addition, we found, that inmost cases the time
that the translation took for the first texts of a given
subject (for each article cluster), tends to be signif-
icantly longer than the subsequent articles in the
same cluster. This reflects the ?learning phase? ex-
perienced by the translators who approached each
cluster, getting to know the vocabulary of each
subject.
4.2.3 Topic Clusters
The text collection includes five clusters of ten
articles each. Some of the topics were very famil-
iar to the Hebrew-speaking readers, and some sub-
jects were less familiar or relevant. The Iranian
Nuclear issue is very common in the local news
and terminology is well known. Moreover, it was
possible to track the articles from the news as they
were published in Hebrew news websites at that
time; this was important for the usage of actual
and correct news-wise terminology. The hardest
batch to translate was on the Paralympics champi-
onship, which had no publicity in Hebrew, and the
terminology of winter sports is culturally foreign
to native Hebrew speakers.
4.2.4 Special Issues in Hebrew
A couple of issues have surfaced during the
translation and should be noted. Many words in
Hebrew have a foreign transliterated usage and an
original Hebrew word as well. For instance, the
Latin word Atomic is very common in Hebrew
and, therefore, it will be equally acceptable to use
it in the Hebrew form, ????? / ?atomi?but also
the Hebrew word ?????? (?gar? ini? / nuclear).
Traditional HebrewNews Agencies have for many
15
Summarizer Reading time Summarization
A 43 min 49 min
B 22 min 84 min
C 35 min 62 min
Table 2: Summarization process times (averaged)
years adopted an editorial line which strongly en-
courages using original Hebrew words whenever
possible. In recent years, however, this approach
is relaxed, and both registers are equally accepted.
We have tried to use a ?common notion? in all texts
using the way terms are written inWikipedia as the
voice of majority. In most cases, this meant using
many transliterations.
Another issue in Hebrew concerns the orthog-
raphy variations of plene vs. deficient spelling.
Since Hebrew can be written with or without vo-
calization, words may be written with variations.
For instance, the vocalized version of the word
?air? is ?????? (?avir? ) while the non-vocalized
version is ????? (?avvir?). The rules of spelling
related to these variations are complicated and are
not common knowledge. Even educated people
write words with high variability, and in many
cases, usage is skewed by the rules embedded in
the Microsoft Word editor. We did not make any
specific effort to enforce standard spelling in the
dataset.
4.2.5 Summarization Process
Each cluster of articles was summarized by three
persons, and each summary was proof-read by the
other summarizers. Most of the summarizers read
the texts before summarization, while translating
or proofreading them, and, therefore, the time that
was required to read all texts was reduced.
The time spent reading and summarizing was
extremely different for each of the three summa-
rizers, reflecting widely different summarization
strategies, as indicated in the Table 2 (average
times over the 5 new clusters of MultiLing 2013):
The trend indicates that investing more time up
front reading the clusters pays off later in summa-
rization time.
The instructions did not explicitly recommend
abstractive vs. extractive summarization. Two
summarizers applied abstractive methods, one
tended to use mostly extractive (C). The extractive
method did not take markedly less time than the
abstractive one. In the evaluation, the extractive
Group SysID Avg Perf
a A 4.80
ab B 4.40
b C 4.13
Table 3: Hebrew: Tukey?s HSD test groups for hu-
man summarizers
summary was found markedly less fluent.
As the best technique to summarize efficiently,
all summarizers found that ordering the texts by
date of publication was the best way to conduct the
summaries in the most fluent manner.
However, it was not completely a linear process,
since it was often found that general information,
which should be located at the beginning of the
summary as background information, appeared in
a later text. In such cases, summarizers changed
their usual strategy and consciously moved infor-
mation from a later text to the beginning of the
summary. This was felt as a distinct deviation ?
as the dominant strategy was to keep track of the
story told across the chronology of the cluster, and
to only add new and important information to the
summary that was collected so far.
The most difficult subject to summarize was
the set on Paralympic winter sports championship
which was a collection of anecdotal descriptions
which were not necessarily a developing or a se-
quential story and had no natural coherence as a
cluster.
4.2.6 Human evaluation
The results of human evaluation over the human
summarizers are provided in Table 3. It is inter-
esting to note that even between humans there ex-
ist two groups with statistically significant differ-
ences in their grades. On the other hand, the hu-
man grades are high enough to show high quality
summaries (over 4 on a 5 point scale).
4.3 Spanish language
Thirty undergraduate students, from National
Institute Polytechnic and Autonomous University
of the State of Mexico, were involved in creating
of Spanish corpus for MultiLing 2013.
The Spanish corpus built upon the Text Analy-
sis Conference (TAC) MultiLing Corpus of 2011.
The source documents were news fromWikiNews
website, in English language. The source corpus
for translating consisted of 15 topics and 10 docu-
ments per topic. In the following paragraphs, we
16
show the measured times for each stage and prob-
lems that people had to face during the generation
of corpus that includes translation of documents,
multi-document summarization, and evaluation of
human (manual) summaries.
At the translation step, people had to translate
sentence by sentence or paraphrase a sentence up
to completing the whole document. When a docu-
ment was translated, it was sent to another person
to verify the quality of the translated document.
The effort was measured by three different time
measurements: reading time, translation time, and
verification time.
The reading average at document level was 7.6
minutes (with a standard deviation of 3.4 minutes),
the average translation of each document was 19.2
minutes (with a standard deviation of 7.8 min-
utes), and the average verification was 14.9 min-
utes (with a standard deviation of 7.7 minutes).
The translation stage took 104.5 man-hours.
At summarization step, people had to read the
whole set of translated documents (topic) and cre-
ate a summary per each set of documents. The
length of a summary is between 240 and 250
words. Three summaries were created for each
topic. Also, reading time of the topic and time of
writing the summary were measured.
The average reading of a set of documents was
31.6 minutes (with a standard deviation of 10.2
minutes), and the average time to generate a sum-
mary was 27.7 minutes (with a standard deviation
of 6.5 minutes). This stage took 44.5 man-hours.
At evaluation step, people had to read the whole
set of translated documents and assess its corre-
sponding summary. The summary quality was
evaluated. Three evaluations were done for each
summary. The human judges assessed the overall
responsiveness of the summary based on covering
all important aspects of the document set, fluent
and readable language. The human summary qual-
ity average was 3.8 (on a scale 1 to 5) (with a stan-
dard deviation of 0.81). The results are detailed in
Table 4. It is interesting to note that all humans
have no statistically significant differences in their
grades. On the other hand, the human grades are
not excellent on average (i.e. exceeding 4 out of 5)
which shows that the evaluators considered human
summaries non-optimal.
Group SysID Avg Perf
a C 3.867
a B 3.778
a A 3.667
Table 4: Spanish: Tukey?s HSD test groups for hu-
man summarizers
4.3.1 Problems during Generation of Spanish
Corpus
During the translation step, translators had to
face problems related to proper names, acronyms,
abbreviations, and specific themes. For instance,
the proper name?United States?can be depicted
with different Spanish words such as ?EE. UU.?
2,?Estados Unidos?, and?EUA??all of them
are valid words. Even though translators know
all the correct translations, they decided to use the
frequent terms in a context of news (the first two
terms are frequently used).
In relation to acronyms, well-known acronyms
were translated into equivalent well-known (or fre-
quent) Spanish translations such as UN (United
Nations) became into ONU (Organizaci?n de las
Naciones Unidas), or they were kept in the source
language, because they are frequently used in
Spanish, for example, UNICEF, BBC, AP (the
news agency, Associated Press), etc.
On the contrary, for not well-known acronyms
of agencies, monitoring centers, etc., translators
looked for the common translation of the proper
name on Spanish news websites in order to cre-
ate the acronym based on the name. Other trans-
lators chose to translate the proper name, but they
kept the acronym from the source document beside
the translated name. In cases where acronyms ap-
peared alone, they kept the acronym from source
language. It is a serious problem because a set of
translated documents has a mix of acronyms.
Abbreviations were mainly faced with ranks
such as lieutenant (Lt.), Colonel (Col.), etc. Trans-
lators used an equivalent rank in Spanish. For in-
stance, lieutenant (Lt.) is translated into?teniente
(Tte.)?; however, translators preferred to use the
complete word rather than the abbreviation.
In case of specific topics, translators used Span-
ish websites related to the topic in order to know
the particular vocabulary and to decide what (tech-
2The double E and double U indicate that the letter rep-
resents a plural: e.g. EE. may stand for Asuntos Exteriores
(Foreign Affairs).
17
nical) words should be translated and how they
should be expressed.
As regards at text summarization step, sum-
marizers dealt with how to organize the sum-
mary because there were ten documents per topic,
and all documents involved dates. Two strategies
were employed to solve the problem: generating
the summary according to representative dates, or
starting the summary based on a particular date.
In the first case, summarizers took the chain
of events and wrote the summary considering the
dates of events. They gathered important events
and put together under one date, typically, the lat-
est date according to a part of the chain of events.
They grouped all events in several dates; thus, the
summary is a sequence of dates that gather events.
However, the dates are chosen arbitrary according
to the summarizers.
In the second case, summarizers started the sum-
mary based on a specific date, and continued writ-
ing the sequence of important events. The se-
quence of events represents the temporality start-
ing from a specific point of time (usually, the
first date in the set of documents). Finally, in
most cases, evaluators think that human sum-
maries meet the requirements of covering all im-
portant aspects of the document set, fluent and
readable language.
5 Conclusions and lessons learnt
The findings from the languages presented in
this paper appear to second the claims found in the
rest of the languages (Li et al, 2013):
? Translation is a non-trivial process, often re-
quiring expert know-how to be performed.
? The distribution of time in summarization can
significantly vary among human summariz-
ers: it essentially sketches different strate-
gies of summarization. It would be interest-
ing to follow different strategies and record
their effectiveness in the multilingual setting,
similarly to previous works on human-style
summarization (Endres-Niggemeyer, 2000;
Endres-Niggemeyer and Wansorra, 2004).
Our find may be related to the (implied) ef-
fort of taking notes while reading, which can
be a difficult cognitive process (Piolat et al,
2005).
? The time aspect is important when generat-
ing a summary. The exact use of time (a sim-
ple timeline? a grouping of events based on
time?) is apparently arbitrary.
We remind the reader that extended technical re-
ports recapitulating discussions and findings from
the MultiLingWorkshop will be available after the
workshop at the MultiLing Community website3,
as an addenum to the proceedings.
What can definitely be derived from all the ef-
fort and discussion related to the gathering of sum-
marization corpora is that it is a research challenge
in itself. If the future we plan to broaden the scope
of the MultiLing effort, integrating all the findings
in tools that will support the whole process and al-
low quantifying the apparent problems in the dif-
ferent stages of corpus creation. We have also been
considering to generate comparable corpora (e.g.,
see (Saggion and Szasz, 2012)) for future Multi-
Ling efforts. We examine this course of action
to avoid the significant overhead by the transla-
tion process required for parallel corpus genera-
tion. We should note here that so far we have been
using parallel corpora to:
? allow for secondary studies, related to the
human summarization effort in different lan-
guages. Having a parallel corpus is such cases
can prove critical, in that it provides a com-
mon working base.
? be able to study topic-related or domain-
related summarization difficulty across lan-
guages.
? highlight language-specific problems (such
as ambiguity in word meaning, named entity
representation across languages).
? fixes the setting in which methods can show
their cross-language applicability. Exam-
ining significantly varying results in differ-
ent languages over a parallel corpus offers
some background on how to improve exist-
ing methods and may highlight the need for
language-specific resources.
On the other hand, the significant organizational
and implementaion effort required for the transla-
tion may turn the balance towards comparable cor-
pora for future MultiLing endeavours.
3See http://multiling.iit.demokritos.gr/
pages/view/1256/proceedings-addenum)
18
Acknowledgments
MultiLing is a community effort and this com-
munity is what keeps it alive and interesting. We
would like to thank contributors for their organi-
zational effort, which made MultiLing possible in
so many languages and all volunteers, helpers and
researchers that helped realize individual steps of
the process. A more detailed reference of the con-
tributor teams can be found in the Appendix.
The MultiLing 2013 organization has been par-
tially supported by the NOMAD FP7 EU Project
(cf. http://www.nomad-project.eu).
References
[Dang and Owczarzak2009] Hoa Trang Dang and
K. Owczarzak. 2009. Overview of the tac 2009
summarization track, Nov.
[Elhadad et al2013] Michael Elhadad, Sabino
Miranda-Jim?nez, Josef Steinberger, and George
Giannakopoulos. 2013. Multi-document multi-
lingual summarization corpus preparation, part 2:
Czech, hebrew and spanish. In MultiLing 2013
Workshop in ACL 2013, Sofia, Bulgaria, August.
[Endres-Niggemeyer and Wansorra2004] Brigitte
Endres-Niggemeyer and Elisabeth Wansorra. 2004.
Making cognitive summarization agents work in
a real-world domain. In Proceedings of NLUCS
Workshop, pages 86?96. Citeseer.
[Endres-Niggemeyer2000] Brigitte Endres-
Niggemeyer. 2000. Human-style WWW sum-
marization. Technical report.
[Giannakopoulos et al2011] G. Giannakopoulos,
M. El-Haj, B. Favre, M. Litvak, J. Steinberger,
and V. Varma. 2011. TAC 2011 MultiLing pilot
overview. In TAC 2011 Workshop, Maryland MD,
USA, November.
[Hovy et al2005] E. Hovy, C. Y. Lin, L. Zhou, and
J. Fukumoto. 2005. Basic elements.
[Li et al2013] Lei Li, Corina Forascu, Mahmoud El-
Haj, and George Giannakopoulos. 2013. Multi-
document multilingual summarization corpus prepa-
ration, part 1: Arabic, english, greek, chinese, ro-
manian. In MultiLing 2013 Workshop in ACL 2013,
Sofia, Bulgaria, August.
[Lin2004] C. Y. Lin. 2004. Rouge: A package for
automatic evaluation of summaries. Proceedings of
the Workshop on Text Summarization Branches Out
(WAS 2004), pages 25?26.
[Louis and Nenkova2012] Annie Louis and Ani
Nenkova. 2012. Automatically assessing ma-
chine summary content without a gold standard.
Computational Linguistics, 39(2):267?300, Aug.
[Piolat et al2005] Annie Piolat, Thierry Olive, and
Ronald T Kellogg. 2005. Cognitive effort dur-
ing note taking. Applied Cognitive Psychology,
19(3):291?312.
[Saggion and Szasz2012] Horacio Saggion and Sandra
Szasz. 2012. The concisus corpus of event sum-
maries. In LREC, pages 2031?2037.
[Saggion et al2010] H. Saggion, J. M. Torres-Moreno,
I. Cunha, and E. SanJuan. 2010. Multilingual sum-
marization evaluation without human models. In
Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, page 1059?
1067.
Appendix: Contributor teams
Czech language team
Team members Brychc?n Tom??, Campr Michal,
Fiala Dalibor, Habernal Ivan, Habernalov?
Anna, Je?ek Karel, Konkol Michal, Konop?k
Miloslav, Kr?m?? Lubom?r, Nejezchlebov?
Pavla, Pelechov? Blanka, Pt??ek Tom??,
Steinberger Josef, Z?ma Martin.
Team affiliation University of West Bohemia,
Czech Republic
Contact e-mail jstein@kiv.zcu.cz
Hebrew language team
Team members Tal Baumel, Raphael Cohen,
Michael Elhadad, Sagit Fried, Avi Hayoun,
Yael Netzer
Team affiliation Computer Science Dept. Ben-
Gurion University in the Negev, Israel
Contact e-mail elhadad@cs.bgu.ac.il
Spanish language team
Team members Sabino Miranda-Jim?nez, Grig-
ori Sidorov, Alexander Gelbukh (Natural
Language and Text Processing Laboratory,
Center for Computing Research, National In-
stitute Polytechnic, Mexico City, Mexico)
Obdulia Pichardo-Lagunas (Interdisciplinary
Professional Unit on Engineering and Ad-
vanced Technologies (UPIITA), National In-
stitute Polytechnic, Mexico City, Mexico)
Contact e-mail sabino_m@hotmail.com
19
Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 50?54,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
The UWB Summariser at Multiling-2013
Josef Steinberger
University of West Bohemia
Faculty of Applied Sciences
Department of Computer Science and Engineering, NTIS Centre
Univerzitni 8, 30614 Plzen?, Czech Republic
jstein@kiv.zcu.cz
Abstract
The paper describes our participation in
the Multi-document summarization task of
Multiling-2013. The community initiative
was born as a pilot task for the Text Analy-
sis Conference in 2011. This year the cor-
pus was extended by new three languages
and another five topics, covering in total
15 topics in 10 languages. Our summariser
is based on latent semantic analysis and it
is in principle language independent. Its
results on the Multiling-2011 corpus were
promising. The generated summaries were
ranked first in several languages based on
various metrics. The summariser with mi-
nor changes was run on the updated 2013
corpus. Although we do not have the man-
ual evaluation results yet the ROUGE-2
score indicates good results again. The
summariser produced best summaries in 6
from 10 considered languages according
to the ROUGE-2 metric.
1 Introduction
Multi-document summarization has received in-
creasing attention during the last decade. This
was mainly due to the requirement of news mon-
itoring to reduce the big bulk of highly redun-
dant news data. More and more interest arises
for approaches that will be able to be applied on
a variety of languages. The summariser should
be of high quality. However, when applied in
a highly multilingual environment, it has to be
enough language-independent to guarantee simi-
lar performance across languages.
Given the lack of multilingual summarisation
evaluation resources, the summarisation commu-
nity started to discuss the topic at Text Analy-
sis Conference (TAC1) 2010. It resulted in the
1http://www.nist.gov/tac/
first multilingual shared task organised as part of
TAC 2011 ? Multiling-2011 (Giannakopoulos et
al., 2012). Each group took an active role in the
creation of their language subcorpus. Because no
freely available parallel corpus suitable for multi-
document summarisation was found, news clus-
ters from WikiNews (in English) needed to be first
translated to six other languages. Three model
summaries for each cluster were then written and
both model and peer summaries were manually
evaluated. For Multiling-2013, three new lan-
guages were added (Chinese, Romanian and Span-
ish) and 5 new topics (news clusters) were added
to the corpus.
This article contains the description of our
system based on latent semantic analysis (LSA)
which participated in Multiling-2013. We first
briefly discuss the multi-document task in sec-
tion 2. Then we show our summarisation ap-
proach based on LSA (Section 3). The next sec-
tion (4) compares the participating systems based
on the ROUGE-2 score. Manually assigned scores
were not available at the time of creation of this
report. We conclude by a discussion of possi-
ble improvements of the method which require
language-specific resources.
2 Multi-document summarisation task at
Multiling?13
MultiLing-2013 is a community effort, a set of re-
search tasks and a corresponding workshop which
covers three summarisation tasks, focused on the
multilingual aspect. It aims to evaluate the appli-
cation of (partially or fully) language-independent
summarization algorithms on a variety of lan-
guages.
The annotation part consisted of four phases.
The first phase was to select English WikiNews ar-
ticles about the same event and to create the topics.
The articles were then manually translated to the
other languages. Model summaries were created
50
separately for each language by native speakers.
In a certain time frame, participating groups ran
their summarisers and the automatic summaries
were then evaluated, both manually (on a 5-to-1
scale) and automatically by ROUGE (Lin, 2004)
and the AutoSummENG metric (Giannakopoulos
and Karkaletsis, 2010).
We participated with our summariser in the
main multi-document task, which requires to gen-
erate a single, fluent, representative summary from
a set of 10 documents describing an event se-
quence. The language of the document set (topic)
was within a given range of 10 languages (Arabic,
Chinese, Czech, English, French, Greek, Hebrew,
Hindi, Romanian and Spanish) and all documents
in a set share the same language. The output sum-
mary should be of the same language as its source
documents. The output summary should be 250
words at most. The corpus was extended to 15 top-
ics (Chinese, French and Hindi subcorpora con-
tained only 10 topics).
3 LSA-based summarisation approach
Originally proposed by Gong and Liu (2002) and
later improved by Steinberger and Jez?ek (2004),
this approach first builds a term-by-sentence ma-
trix from the source, then applies Singular Value
Decomposition (SVD) and finally uses the result-
ing matrices to identify and extract the most salient
sentences. SVD finds the latent (orthogonal) di-
mensions, which in simple terms correspond to the
different topics discussed in the source.
More formally, we first build matrix A where
each column represents the weighted term-
frequency vector of a sentence in a given set of
documents. The weighting scheme we found to
work best is using a binary local weight and an
entropy-based global weight (for details see Stein-
berger and Jez?ek (2009)).
After that step Singular Value Decomposition
(SVD) is applied to the above matrix as A =
USVT , and subsequently matrix F = S ? VT re-
duced to r dimensions2 is derived.
Sentence selection starts with measuring the
length of sentence vectors in matrix F computed as
the Euclidean norm. The length of the vector (the
sentence score) can be viewed as a measure for
2The degree of importance of each ?latent? topic is given
by the singular values and the optimal number of latent topics
(i.e., dimensions) r can be fine-tuned on training data. Our
previous experiments led us to set r to 8% from the number
of sentences for 250-word summaries.
importance of that sentence within the top cluster
topics.
The sentence with the largest score is selected as
the first to go to the summary (its corresponding
vector in F is denoted as fbest). After placing it
in the summary, the topic/sentence distribution in
matrix F is changed by subtracting the information
contained in that sentence:
F(it+1) = F(it) ?
fbest ? fTbest
|fbest|
2 ? F
(it). (1)
The vector lengths of similar sentences are de-
creased, thus preventing within summary redun-
dancy. After the subtraction of information in
the selected sentence, the process continues with
the sentence which has the largest score computed
on the updated matrix F. The process is itera-
tively repeated until the required summary length
is reached.
4 Experiments and results
Although the approach works only with term co-
occurrence, and thus it is completely language-
independent, pre-processing plays an important
role and greatly affects the performance. When
generating the summaries for Multiling-2013 each
article was split into sentences. We used the
old DUC sentence splitter3, although a different
sentence-splitting character was used for Chinese.
It was a simplification because the sentence split-
ter should be adapted for each language (e.g. a
different list of abbreviations should be used or
language specific features should be added). If
LSA is applied on a large matrix stopwords can be
found in the first linear combination which could
be then filtered out. However, in our case we apply
it on rather small matrices and stopwords could
affect negatively the topic distribution. Thus the
safer option is to filter them out. This brings a
dependency on a language but, on the other hand,
acquiring lists of stop-words for various languages
is not difficult. Filtering these insignificant terms
does not also slow down the system. The stop-
words were filtered out for all the languages of
Multiling. The approach discussed in section 3
was then used to select sentences until the re-
quired summary length (250 words) has not been
reached. Sentence order is important for event-
based stories. In the case of the Multiling corpus,
3http://duc.nist.gov/duc2004/software/duc2003.breakSent.tar.gz
51
Language Topics Avg. Model ID1 ID11 ID2 ID21 ID3 ID4 (rank/total) ID5 ID51 Baseline
Arabic 15 .137 .132 .132 .118 .105 .052 .167 (1/9) .105 .088 .086
Chinese 10 .462 .430 .457 .212 .354 .354 (5/6) .867
Czech 15 .195 .155 .166 .123 .151 .179 (1/6) .085
English 15 .185 .161 .161 .147 .142 .083 .171 (1/9) .117 .101 .118
French 10 .198 .201 .201 .166 .177 .214 (1/6) .130
Greek 15 .111 .120 .124 .100 .112 .110 (4/6) .088
Hebrew 15 .076 .088 .100 .076 .084 .092 (2/8) .087 .084 .072
Hindi 10 .342 .125 .132 .123 .123 .129 (2/6) .114
Romanian 15 .543 .147 .139 .120 .138 .166 (1/6) .098
Spanish 15 .239 .198 .218 .180 .175 .228 (1/6) .164
Avg. rank 2.7 1.9 5.0 4.3 9 1.9 5.7 7.0 5.9
Table 1: ROUGE-2 scores of the average model and paricipating systems. Our LSA-based system is ID4
and we report its rank from the total number of systems which submitted summaries for the particular
language. We included the baseline (the start of a centroid article) and excluded the topline which uses
model sentences.
much attention has to be given to sentence order-
ing because some topics contained articles spread
over a long period, even 5 years. We did not
perform any temporal analysis at sentence level.
The sentences in the summary were ordered based
on the date of the article they came from. Sen-
tences from the same article followed their order
in the full text. Even if they were sometimes out
of context, when extracted, the adjacent sentences
at least dealt with the same (or temporary close)
event.
We analysed ROUGE scores which we received
from the organisers. We discuss here ROUGE-2
(bigram) score, a traditionally used metric in sum-
marisation evaluation (Table 1). ROUGE-2 ranked
our summariser on the top of the list for 6 from 10
languages (Arabic, Czech, English, French, Ro-
manian, Spanish). System ID11 performed better
twice (Hebrew and Hindi), there were three bet-
ter systems in Greek and the baseline won in Chi-
nese. In the following, we will discuss the results
for each language separately.
For Arabic, our system received the best
ROUGE-2 score. It was significantly better (at
confidence 95%) then 5 other systems, including
baseline. It performed on the same level as mod-
els.
It was our first attempt to run the summariser
on Chinese. We did not use any specific word-
splitting tool and we considered each character to
be a context feature for LSA. The ROUGE results
say that the summariser was not that successful
compared to the others. It was significantly bet-
ter than one system and worse than two and the
baseline which received suspiciously high score.
We annotated the Czech part of the corpus, and
therefore the result of our system can be consid-
ered only as another baseline for this language.
It received the largest ROUGE-2 score, however,
there was no significant difference among the top
four systems.
For English, our system together with the fol-
lowing systems ID1 and ID11 were significantly
better than the rest. A similar conclusion can be
driven by observing the French results. In the case
of Greek only baseline performed poorly. Our
approach was ranked fourth although there were
marginal differences between the systems. For
Hebrew and Hindi system ID11 performed the
best, followed by our system. For Romanian, a
newly introduced language this year, our system
received a high score, however, a larger confidence
interval did not show much significance. For an-
other newly-introduced language, Spanish, only
system ID11 was not significantly worse than our
system.
As a try to compare the systems across lan-
guages, an average rank was computed. (Comput-
ing an average of absolute ROUGE-2 scores did
not seem to have sense.) Our system and system
ID11 received the best average rank: 1.9.
For several languages (Arabic, French, He-
brew), our summaries were better (not signif-
icantly) then the average model according to
ROUGE-2.
The AutoSummENG method (Giannakopoulos
and Karkaletsis, 2010) gave results similar to
those of ROUGE. The only difference was in Chi-
52
nese: ROUGE-2 ranked our system 5th, Auto-
SummENG 1st.
One question remains: are the ROUGE scores
correlated with human grades? Unfortunately, the
human grades were not available at the time of the
system reports submission. However, because we
were managing annotation of the Czech subcorpus
we had access to human grades for that language.
The system ranking provided by ROUGE mostly
agree with the human grades, reaching Pearson
correlation of .97 for the systems-only scenario.
The human grades ranked our system as signifi-
cantly better than any other submission in the case
of Czech.
5 Conclusion
The evaluation indicates good results of our sum-
mariser, mainly for European Latin-script lan-
guages (Czech, English, French, Romanian and
Spanish). It could be connected to good-enough
pre-processing (sentence and word splitting). The
last two languages were added this year and the
good results show that the LSA-based summariser
can produce good summaries when run on an ?un-
seen? language.
We experiment with several improvements of
the method which require language-specific re-
sources. Entity detection can improve the LSA
model by adding entity features as new rows in
the SVD input matrix (Steinberger et al, 2007).
From the Multiling-2013 languages we have de-
veloped the NER tool only for 6 languages (Ara-
bic, Czech, English, French, Romanian and Span-
ish) so far (Pouliquen and Steinberger, 2009). A
coreference- (anaphora-) resolution can help in
checking and rewriting the entity references in a
summary (Steinberger et al, 2007) although there
is usually a high dependency on the language (e.g.
in the case of pronouns).
Event extraction can detect important aspects
related to the category of the topic (e.g. detect-
ing victims in a topic about an accident) (Stein-
berger et al, 2011). The aspect information can
be used in the model weighting or during sen-
tence selection. We have developed the tool for
5 languages considered in Multiling-2013 (Ara-
bic, Czech, English, French and Spanish). Tem-
poral analysis could improve sentence ordering if
a correct temporal mark, which contains informa-
tion about time of a discussed event, is attached to
each summary sentence (Steinberger et al, 2012).
So far, we experimented with English, French and
Spanish from the list of the Multiling languages.
By compressing and/or rephrasing the saved space
in the summary could be filled in by the next most
salient sentences, and thus the summary can cover
more content from the source texts. We have
already tried to investigate language-independent
possibilities in that direction (Turchi et al, 2010).
Acknowledgments
This work was supported by project ?NTIS - New
Technologies for Information Society?, European
Center of Excellence, CZ.1.05/1.1.00/02.0090.
References
G. Giannakopoulos and V. Karkaletsis. 2010. Sum-
marization system evaluation variations based on n-
gram graphs. In Proceedings of the Text Analysis
Conference (TAC).
G. Giannakopoulos, M. El-Haj, B. Favre, M. Litvak,
J. Steinberger, and V. Varma. 2012. Tac 2011 multi-
ling pilot overview. In Proceedings of the Text Anal-
ysis Conference (TAC). NIST.
Y. Gong and X. Liu. 2002. Generic text summarization
using relevance measure and latent semantic analy-
sis. In Proceedings of ACM SIGIR, New Orleans,
US.
C.-Y. Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out,
Barcelona, Spain.
B. Pouliquen and R. Steinberger. 2009. Auto-
matic construction of multilingual name dictionar-
ies. In Cyril Goutte, Nicola Cancedda, Marc Dymet-
man, and George Foster, editors, Learning Machine
Translation. MIT Press, NIPS series.
J. Steinberger and K. Jez?ek. 2004. Text summarization
and singular value decomposition. In Proceedings
of the 3rd ADVIS conference, Izmir, Turkey.
J. Steinberger and K. Jez?ek. 2009. Update summa-
rization based on novel topic distribution. In Pro-
ceedings of the 9th ACM Symposium on Document
Engineering, Munich, Germany.
J. Steinberger, M. Poesio, M. Kabadjov, and K. Jez?ek.
2007. Two uses of anaphora resolution in summa-
rization. Information Processing and Management,
43(6):1663?1680. Special Issue on Text Summari-
sation (Donna Harman, ed.).
J. Steinberger, H. Tanev, M. Kabadjov, and R. Stein-
berger. 2011. Aspect-driven news summarization.
International Journal of Computational Linguistics
and Applications, 2(1-2).
53
J. Steinberger, M. Kabadjov, R. Steinberger, H. Tanev,
M. Turchi, and V. Zavarella. 2012. Towards
language-independent news summarization. In Pro-
ceedings of the Text Analysis Conference (TAC).
NIST.
M. Turchi, J. Steinberger, M. Kabadjov, R. Steinberger,
and N. Cristianini. 2010. Wrapping up a summary:
from representation to generation. In Proceedings
of CLEF.
54
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 24?30,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Aspect-Level Sentiment Analysis in Czech
Josef Steinberger
Department of Computer
Science and Engineering,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plze?n
Czech Republic
jstein@kiv.zcu.cz
Tom
?
a
?
s Brychc??n
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plze?n
Czech Republic
brychcin@kiv.zcu.cz
Michal Konkol
NTIS ? New Technologies
for the Information Society,
Faculty of Applied Sciences,
University of West Bohemia,
Univerzitn?? 8, 306 14 Plze?n
Czech Republic
konkol@kiv.zcu.cz
Abstract
This paper presents a pioneering re-
search on aspect-level sentiment analysis
in Czech. The main contribution of the
paper is the newly created Czech aspect-
level sentiment corpus, based on data from
restaurant reviews. We annotated the cor-
pus with two variants of aspect-level senti-
ment ? aspect terms and aspect categories.
The corpus consists of 1,244 sentences and
1,824 annotated aspects and is freely avail-
able to the research community. Further-
more, we propose a baseline system based
on supervised machine learning. Our
system detects the aspect terms with F-
measure 68.65% and their polarities with
accuracy 66.27%. The categories are rec-
ognized with F-measure 74.02% and their
polarities with accuracy 66.61%.
1 Introduction
The interest in sentiment analysis (SA) is increas-
ing with the amount of easily accessible content on
the web, especially from the social media. Sen-
timent polarity is one of the critical information
needed for many analysis of the data. Its use
ranges from analysing product reviews (Stepanov
and Riccardi, 2011) to predicting sales and stock
markets using social media monitoring (Yu et al.,
2013).
The majority of current approaches tries to de-
tect the overall polarity of a sentence (or a docu-
ment) regardless of the target entities (e.g., restau-
rants, laptops) and their aspects (e.g., food, price,
battery, screen). By contrast, the aspect-driven
sentiment analysis identifies the aspects of a given
target entity and estimates the sentiment polarity
for each mentioned aspect. This opens up com-
pletely new possibilities how to analyse the data.
The most of the research in automatic sentiment
analysis has been devoted to English. There were
several attempts in Czech (Steinberger et al., 2011;
Veselovsk?a, 2012; Habernal et al., 2013; Brychc??n
and Habernal, 2013), but all were focused on the
global (sentence- or document-level) sentiment.
Although Czech is not a widely-spoken language
on the global scale, it is in many ways similar
to other Slavic languages and their speakers al-
together represent an important group. The rich
morphology and the free word order also makes it
interesting from the linguistic perspective.
Our main goal is the creation of a aspect-level
corpus as there is no such resource for Czech.
We would like to support the beginning of aspect-
level sentiment analysis for Czech and a human-
annotated corpus is the first step in this direc-
tion. In addition, we want to provide results of
a baseline system (based on machine leaning tech-
niques). This creates an easily reproducible start-
ing point and allows anyone to quickly join the re-
search of this task.
The rest of the paper is organised as follows.
Section 2 is devoted to related work. It covers the
aspect-level SA and sentiment analysis in Czech.
Then we introduce the aspect-level architecture
(Section 3) used for both the annotation of the cor-
pus (Section 4) and for the automatic supervised
approach (Section 5). In Section 6 we sumarize
our contribution and reveal our future plans.
2 Related work
The impact of SA can be seen in many practical
applications, The users? opinions are mostly ex-
tracted either on a certain polarity scale, or binary
(positive, negative). From the point of view of
the granularity, the polarity has been assigned to
a document or to a sentence. However, classify-
ing opinions at the document level or the sentence
level is often insufficient for applications because
they do not identify opinion targets or assign sen-
timents to such targets (Liu, 2012). Even if we
recognize the target entity (as the entity-centered
24
approaches do (e.g. Steinberger et al. (2011)), a
positive opinion about the entity does not mean
that the author has positive opinions about all as-
pects of the entity. Aspect-based sentiment analy-
sis, which has been also called ?feature-based? (Hu
and Liu, 2004), goes even deeper as it attempts to
identify (and assign the polarity to) aspects of the
target entity within a sentence (Hajmohammadi et
al., 2012). Whenever we talk about an aspect,
we must know which entity it belongs to. In the
further discussion, we often omit the entity as we
analysed restaurant reviews and thus our target en-
tities are the reviewed restaurants.
2.1 Aspect-based sentiment analysis
The aspect scenario can be decomposed into two
tasks: aspect extraction and aspect sentiment clas-
sification (Liu, 2012).
2.1.1 Aspect extraction
The task of aspect extraction, which can also be
seen as an information extraction task, is to detect
aspects that have been evaluated. For example, in
the sentence, The voice quality of this phone is
amazing, the aspect is voice quality of the entity
represented by this phone.
The basic approach is finding frequent nouns
and noun phrases. In (Liu et al., 2005), a specific
method based on a sequential learning method was
proposed to extract aspects from pros and cons,
Blair-Goldensohn et al. (2008) refined the frequent
noun and noun phrase approach by considering
mainly those noun phrases that are in sentiment-
bearing sentences or in some syntactic patterns
which indicate sentiments. Moghaddam and Ester
(2010) augmented the frequency-based approach
with an additional pattern-based filter to remove
some non-aspect terms. Long et al. (2010) ex-
tracted aspects (nouns) based on frequency and in-
formation distance.
Using supervised learning is another option.
Aspect extraction can be seen as a special case
of the general information extraction problem.
The most dominant methods are based on sequen-
tial learning. Since these are supervised tech-
niques, they need manually labeled data for train-
ing. One needs to manually annotate aspects
and non-aspects in a corpus. The current state-
of-the-art sequential learning methods are Hid-
den Markov Models (HMM) (Rabiner, 2010) and
Conditional Random Fields (CRF) (Lafferty et al.,
2001).
The last group of methods use topic models
(Mei et al., 2007; Titov and McDonald, 2008;
Blei et al., 2003). There are two main basic mod-
els, pLSA (Probabilistic Latent Semantic Analy-
sis) (Hofmann, 1999) and LDA (Latent Dirichlet
allocation) (Blei et al., 2003). In the SA context,
one can design a joint model to model both senti-
ment words and topics at the same time, due to the
observation that every opinion has a target.
2.1.2 Aspect sentiment classification
This task is to determine whether the opinions on
different aspects are positive, negative, or neutral.
The classification approaches can be divided to
supervised learning approaches and lexicon-based
approaches. Supervised learning performs bet-
ter in a particular application domain but it has
difficulty to scale up to a large number of do-
mains. Lexicon-based techniques often lose the
fight against the learning but they are suitable for
open-domain applications (Liu, 2012).
The key issue for learning methods is to de-
termine the scope of each sentiment expression,
i.e., whether it covers the aspect of interest in the
sentence. In (Jiang et al., 2011), a dependency
parser was used to generate a set of aspect de-
pendent features for classification. A related ap-
proach was also used in (Boiy and Moens, 2009),
which weights each feature based on the position
of the feature relative to the target aspect in the
parse tree.
Lexicon-based approaches use a list of senti-
ment phrases as the core resource. The method
in (Ding et al., 2008) has four steps to assign
a polarity to an aspect: mark sentiment words
and phrases, apply sentiment shifters, handle but-
clauses and aggregate opinions using an aggrega-
tion function (e.g. Hu and Liu (2004)).
2.2 Sentiment analysis for Czech
Pilot study of Czech sentiment analysis was shown
in (Steinberger et al., 2012) where sentiment dic-
tionaries for many languages (including Czech)
were created using semi-automatic ?triangulation?
method.
Veselovsk?a (2012) created a small corpus con-
taining polarity categories for 410 news sentences
and used the Naive Bayes and lexicon-based clas-
sifiers.
Three large labeled corpora (10k Facebook
posts, 90k movie reviews, and 130k product
reviews) were introduced in (Habernal et al.,
25
2013).Authors also evaluate three different classi-
fiers, namely Naive Bayes, SVM (Support Vector
Machines) and Maximum Entropy on these data.
Recently, Habernal and Brychc??n (2013) experi-
mented with building word clusters, obtained from
semantic spaces created on unlabeled data, as an
additional source of information to tackle the high
flection issue in Czech.
These results were later outperformed by
another unsupervised extension (Brychc??n and
Habernal, 2013), where the global target context
was shown to be very useful source of informa-
tion.
3 The task definition
The aspect-level sentiment analysis firstly identi-
fies the aspects of the target entity and then assigns
a polarity to each aspect. There are several ways
to define aspects and polarities. We use the defini-
tion based on the Semeval2014?s Aspect-based SA
task, which distinguishes two types of aspect-level
sentiment ? aspect terms and aspect categories.
The task is decomposed into the following 4
subtasks. We briefly describe each subtask and
give some examples of source sentences and the
expected results of the subtask.
3.1 Subtask 1: Aspect term extraction
Given a set of sentences with pre-identified enti-
ties (e.g., restaurants), the task is to identify the
aspect terms present in the sentence and return a
list containing all the distinct aspect terms. An as-
pect term names a particular aspect of the target
entity.
Examples:
D?eti dostaly naprosto krvav?e maso.
(They brought a totally bloody meat to the kids.)
? {maso (meat)}
Tla?cenka se rozpadla, pol?evka u?sla.
(The porkpie broke down, the soup was ok.)
? {Tla?cenka (porkpie), pol?evka (soup)}
3.2 Subtask 2: Aspect term polarity
For a given set of aspect terms within a sentence,
the task is to determine the polarity of each aspect
term: positive, negative, neutral or bipolar (i.e.,
both positive and negative).
Examples:
D?eti dostaly naprosto krvav?e maso.
(They brought a totally bloody meat to the kids.)
? {maso (meat): negative}
Tla?cenka se rozpadla, pol?evka u?sla.
(The porkpie broke down, the soup was ok.)
?{Tla?cenka (porkpie): negative, pol?evka (soup):
positive}
3.3 Subtask 3: Aspect category detection
Given a predefined set of aspect categories (e.g.,
price, food), the task is to identify the aspect cat-
egories discussed in a given sentence. Aspect cat-
egories are typically coarser than the aspect terms
of Subtask 1, and they do not necessarily occur as
terms in the given sentence.
For example, given the set of aspect categories
food, service, price, ambience:
P?riv??tala n?as velmi p?r??jemn?a serv??rka, ale tak?e
m??stnost s o?sunt?el?ym n?abytkem.
(We found a very nice waitress but also a room
with time-worn furniture.)
? {service, ambience}
Tla?cenka se rozpadla, pol?evka u?sla.
(The porkpie broke down, the soup was ok.)
? {food}
3.4 Subtask 4: Aspect category polarity
Given a set of pre-identified aspect categories
(e.g., {food, price}), the task is to determine the
polarity (positive, negative, neutral or bipolar) of
each aspect category.
Examples:
P?riv??tala n?as velmi p?r??jemn?a serv??rka, ale tak?e
m??stnost s o?sunt?el?ym n?abytkem.
(We found a very nice waitress but also a room
with time-worn furniture.)
? {service: positive, ambience: negative}
Tla?cenka se rozpadla, pol?evka u?sla.
(The porkpie broke down, the soup was ok.)
? {food: bipolar}
4 Building the aspect-level corpus
Aspect-level annotations are strictly connected to
the analysed domain. As our final goal is going
multilingual, we work on the domains selected for
the Semeval2014?s Aspect-based SA task (restau-
rants, laptop) which will allow us to compare ap-
proaches for both English and Czech on the same
domains.
We started with the restaurants and in the future,
we would also like to cover the laptops.
26
We downloaded restaurant reviews from www.
nejezto.cz. Ten restaurants with the largest
number of reviews were selected. The reviews
were splitted into sentences. Average number of
sentences per restaurant was 223.
4.1 Guidelines
The purpose of this annotation was to detect as-
pects and their sentiment polarity within sen-
tences. The target entities were particular restau-
rants. For a given restaurant, the annotator had
following tasks:
1. Identify irrelevant sentences: Sentences
that do not contain any information rele-
vant to the topic of restaurants. They were
later filtered out of the corpus. Example:
Ur?a?zet n?ekoho pro jeho n?azor je ned?ustojn?e
dosp?el?eho ?clov?eka. (Offencing somebody for
his opinion is discreditable for an adult.)
2. Identify aspect terms: Single or multiword
terms naming particular aspects of the target
entity. These are either full nominal phrases
(?sp??z a restovan?e brambory ? skewer with
fried potatoes) or verbs (stoj?? ? priced). Ref-
erences, names or pronouns should not be an-
notated.
3. Aspect term polarity: Each aspect term has
to be assigned one of the following polarities
based on the sentiment that is expressed in the
sentence about it: positive, negative, bipo-
lar (both positive and negative sentiment) and
neutral (neither positive nor negative senti-
ment).
4. Aspect category: The task of the annotator is
to identify the aspect categories discussed in
a sentence given the following five aspect cat-
egories: food, service, price, ambience, gen-
eral (sentences mentioning the restaurant as
a whole). Example: Celkov?e doporu?cuji a
vr?at??m se tam ? Overall I would recommend
it and go back again. ? general.
5. Aspect category polarity: Each aspect cat-
egory discussed by a particular sentence has
to be assigned one of the following polarities
based on the sentiment that is expressed in the
sentence about it: positive, negative, bipolar,
neutral.
4.2 Annotation statistics
Three native Czech speakers annotated in total
1,532 sentences. 18.8% of the sentences were
marked as irrelevant, leaving 1,244 sentences for
further analysis. Their average agreement for the
task of aspect terms? identification was 82.6%
(measured by F-measure). Only strict matches
were considered correct. In the case of identi-
fying the categories, their average agreement (F-
measure) was 91.8%. The annotators agreed on
85.5% (accuracy) in the task of assigning polarity
to terms and on 82.4% (accuracy) in the case of
the category polarity assignment. It corresponds
to Cohen?s  of 0.762, resp. 0.711, which rep-
resents a substantial agreement level (Pustejovsky
and Stubbs, 2013), therefore the task can be con-
sidered as well-defined.
There were several reasons of disagreement.
The annotators did not always itentify the same
terms, mainly in the cases with general meaning.
In the case of polarity, the annotators did not agree
on the most difficult cases to which bipolar class
could be assigned:
Trochu p?resolen?a om?a?cka, ale jinak luxus.
(Too salted sauce, but luxury otherwise.)
? {food: bipolar vs. positive}
The cases, on which the two annotators did not
agree, were judged by the third super-annotator
and golden standard data were created. The final
dataset
1
contains 1244 sentences. The sentences
contain 1824 annotated aspect terms (679 positive,
725 negative, 403 neutral, 17 bipolar) and 1365
categories (521 positive, 569 negative, 246 neu-
tral, 28 bipolar).
5 Results of the supervised approach
5.1 Overview
We use machine learning approach in all subtasks.
For aspect term extraction we use Conditional
Random Fields (CRF). For the other three tasks
we use the Maximum Entropy classifier. We use
the Brainy
2
implementation of these algorithms.
During the data preprocessing, we use simple
word tokenizer based on regular expressions. All
tokens are lowercased for tasks 3 and 4. Due to the
complex morphology of Czech we also use the un-
1
We will provide the dataset at http://liks.fav.
zcu.cz/sentiment.
2
Available at http://home.zcu.cz/
?
konkol/
brainy.php
27
supervised stemmer called HPS
3
, that has already
proved to be useful in sentiment analysis (Haber-
nal et al., 2013; Habernal and Brychc??n, 2013;
Brychc??n and Habernal, 2013).
All particular subtasks share following features:
? Bag of words: The occurrence of a word.
? Bag of bigrams: The occurrence of a bigram.
? Bag of stems: The occurrence of a stem.
? Bag of stem bigrams: The occurrence of a
stem bigram.
5.2 Aspect term extraction
The system for aspect term extraction is based on
CRF. The choice of CRF is based on a current state
of the art in named entity recognition (see for ex-
ample (Konkol and Konop??k, 2013)) as it is a very
similar task. We use the BIO (Ramshaw and Mar-
cus, 1999) model to represent aspect terms. In ad-
dition to the previously mentioned features we use
affixes and learned dictionaries. Affixes are sim-
ply prefixes and suffixes of length 2 to 4. Learned
dictionaries are phrases that are aspect terms in the
training data.
Our system achieved 58.14 precision, 83.80 re-
call and 68.65 F-measure.
5.3 Aspect term polarity
During the detection of the aspect term polarities,
the words affecting the sentiment of the aspect
term are assumed to be close in most of cases.
Thus we use a small window (10 words in both
directions) around the target aspect term. We as-
sume the further the word or bigram is from the
target aspect term, the lower impact it has on sen-
timent label. To model this assumption we use
a weight for each word and bigram feature taken
from the Gaussian distribution according to dis-
tance from aspect term. The mean is set to 0 and
variance is optimized on training data. The classi-
fier uses only the features presented in section 5.1.
The results are presented in table 1.
5.4 Aspect category detection
Aspect category detection is based on the Maxi-
mum Entropy classifiers. We use one binary clas-
sifier for each category. Each classifier then de-
cides whether the sentence has the given category
3
Available at http://liks.fav.zcu.cz/HPS.
Table 1: Aspect term polarity results. P , R and
F
m
denote the precision, recall and F-measure.
The results are expressed by percentages.
label P [%] R[%] F
m
[%]
negative 76.41 63.31 69.25
neutral 33.75 50.18 40.36
positive 74.78 76.82 75.78
Accuracy: 66.27%
or not. For this task we use only the bag of stems
and Tf-Idf features.
Our system achieved 68.71 precision, 80.21 re-
call and 74.02 F-measure.
5.5 Aspect category polarity
For the category polarity detection we use the
same features as for aspect term polarity detec-
tion. However in this case, we always take the
whole sentence into account. We cannot take a
limited window as we do not know where exactly
the category is mentioned in the sentence. More-
over, it can be at several positions. To distinguish
between different categories we use multiple Max-
imum Entropy classifiers, one for each category.
The results are shown in table 2.
Table 2: Aspect category polarity results. P ,
R and F
m
denote the precision, recall and F-
measure. The results are expressed by percent-
ages.
label P [%] R[%] F
m
[%]
negative 74.07 66.04 69.83
neutral 37.80 46.73 41.80
positive 72.12 75.30 73.67
Accuracy: 66.61%
5.6 Discussion
In section 5 we described our system for aspect-
level sentiment analysis and showed the results.
We do not use any language-dependent features,
everything is learned from the training data. It is
thus possible to say that our system is both lan-
guage and domain independent, i.e. the system is
able to work for any domain or language, if the
training data are provided.
From another perspective, the already trained
model is language and domain dependent (i.e. the
model trained on restaurant domain probably will
not perform well on laptop domain). The depen-
28
dence on the domain has multiple reasons. First,
the categories are defined strictly for one domain
(e.g. food, price, etc.). Second, many words can
have different sentiment polarity in different do-
mains.
In general, the sentiment analysis deals with
many problems. These problems are much more
evident for Czech as a representative of language
with rich morphology and also with almost free
word order. Here are two examples, where our
system wrongly estimate the sentiment label.
Na nic si nejde st?e?zovat.
(There is nothing to complain about.)
? {general: positive}
The sentence contains words that frequently oc-
cur in negative reviews: nic - nothing, st?e?zovat -
complain; but the sentence is positive.
O t?ech labu?znick?ych a delikatesn??ch z?a?zitc??ch si
?clov?ek pouze p?re?cte, ale realita je jin?a.
(One can only read about these gourmand and de-
licious experiences, but the reality is completely
different.)
? {food: negative}
Sentence contains words like labu?znick?ych -
gourmand and delikatesn??ch - delicious that are
strictly positive, but in this context it is mentioned
negatively.
As we already said, this is the pilot study of
aspect-level sentiment analysis in Czech. Several
studies about sentence-level sentiment analysis of
Czech have been already published, and thus it
is worth comparing how these two tasks differ in
terms of difficulty. Note that the aspect-level sen-
timent analysis has to deal with multiple aspects
and categories in a given sentence, and thus it is
apparently a much more difficult task.
We believe the results of (Brychc??n and Haber-
nal, 2013) on Czech movie reviews dataset can be
a comparable example of sentence-level sentiment
analysis as they also distinguish 3 sentiment labels
(positive, negative and neutral) and the data are
taken from a closed domain (movies). Their best
result (given by the model with all extensions) is
81.53%. Our best results are 66.27% and 66.61%
for aspect and category polarity detection, respec-
tively.
6 Conclusion
The aspect level sentiment analysis has not been
studied for Czech yet. The main reason for this is
the lack of annotated data. In this paper, we create
a high quality gold data for this task, we describe
our approach to their annotation and discuss their
properties. Corpus is available for free at http:
//liks.fav.zcu.cz/sentiment.
We also propose a baseline model based on
state-of-the-art supervised machine learning tech-
niques. Our system is language and domain inde-
pendent, i.e. it can be easily trained on data from
another domain or language. It achieved 68.65%
F-measure in the aspect term detection, 74.02% F-
measure in the aspect category assigning, 66.27%
accuracy in the aspect term polarity classification,
and 66.61% accuracy in the aspect category polar-
ity classification.
In the future, we would like to continue the
aspect-level research direction in three ways. We
would like to extend the currently created restau-
rant reviews? corpus, to add the second (laptop?s)
domain to the corpus, and finally, to experiment
with extensions to the baseline system. As the
corpus for the Semeval2014 aspect-based SA task
contains review sentences from the same domains,
we will be able to compare the results of the sys-
tem cross-lingually.
Acknowledgments
This work was supported by grant no. SGS-
2013-029 Advanced computing and information
systems, by the European Regional Development
Fund (ERDF), by project ?NTIS - New Tech-
nologies for Information Society?, European Cen-
tre of Excellence, CZ.1.05/1.1.00/02.0090, and by
project MediaGist, EU?s FP7 People Programme
(Marie Curie Actions), no 630786.
References
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George Reis, and Jeff Reynar.
2008. Building a sentiment summarizer for lo-
cal service reviews. In Proceedings of WWW-2008
workshop on NLP in the Information Explosion Era.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Erik Boiy and Marie-Francine Moens. 2009. A
machine learning approach to sentiment analysis
in multilingual web texts. Information retrieval,
12(5):526?558.
Tom?a?s Brychc??n and Ivan Habernal. 2013. Unsuper-
vised improving of sentiment analysis using global
29
target context. In Proceedings of the International
Conference Recent Advances in Natural Language
Processing RANLP 2013, pages 122?128, Hissar,
Bulgaria, September. Incoma Ltd. Shoumen, Bul-
garia.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the Conference on Web Search and
Web Data Mining.
Ivan Habernal and Tom?a?s Brychc??n. 2013. Semantic
spaces for sentiment analysis. In Text, Speech and
Dialogue, volume 8082 of Lecture Notes in Com-
puter Science, pages 482?489, Berlin Heidelberg.
Springer.
Ivan Habernal, Tom?a?s Pt?a?cek, and Josef Steinberger.
2013. Sentiment analysis in czech social media us-
ing supervised machine learning. In Proceedings of
the 4th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis,
pages 65?74, Atlanta, Georgia, June. Association
for Computational Linguistics.
Mohammad Sadegh Hajmohammadi, Roliana Ibrahim,
and Zulaiha Ali Othman. 2012. Opinion mining and
sentiment analysis: A survey. International Journal
of Computers & Technology, 2(3).
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of Conference on Uncer-
tainty in Artificial Intelligence.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics.
Michal Konkol and Miloslav Konop??k. 2013. Crf-
based czech named entity recognizer and consolida-
tion of czech ner research. In Ivan Habernal and
V?aclav Matou?sek, editors, Text, Speech and Dia-
logue, volume 8082 of Lecture Notes in Computer
Science, pages 153?160. Springer Berlin Heidel-
berg.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference on Machine Learning.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opin-
ions on the web. In Proceedings of International
Conference on World Wide Web.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publishers.
Chong Long, Jie Zhang, and Xiaoyan Zhu. 2010. A
review selection approach for accurate feature rating
estimation. In Proceedings of Coling 2010: Poster
Volume.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of International Conference on World
Wide Web.
Samaneh Moghaddam and Martin Ester. 2010. Opin-
ion digger: an unsupervised opinion miner from
unstructured product reviews. In Proceeding of
the ACM conference on Information and knowledge
management.
James Pustejovsky and Amber Stubbs. 2013. Natural
Language Annotation for Machine Learning. OR-
eilly Media, Sebastopol, CA 95472.
Lawrence Rabiner. 2010. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. In Proceedings of the IEEE, pages 257?286.
Lance A Ramshaw and Mitchell P Marcus. 1999. Text
chunking using transformation-based learning. In
Natural language processing using very large cor-
pora, pages 157?176. Springer.
J. Steinberger, P. Lenkova, M. Kabadjov, R. Stein-
berger, and E. van der Goot. 2011. Multilingual
entity-centered sentiment analysis evaluated by par-
allel corpora. In Proceedings of the 8th Interna-
tional Conference Recent Advances in Natural Lan-
guage Processing, RANLP?11, pages 770?775.
J. Steinberger, M. Ebrahim, Ehrmann M., A. Hur-
riyetoglu, M. Kabadjov, P. Lenkova, R. Steinberger,
H. Tanev, S. Vzquez, and V. Zavarella. 2012. Cre-
ating sentiment dictionaries via triangulation. Deci-
sion Support Systems, 53:689?694.
E.A. Stepanov and G. Riccardi. 2011. Detecting gen-
eral opinions from customer surveys. In Data Min-
ing Workshops (ICDMW), 2011 IEEE 11th Interna-
tional Conference on, pages 115?122.
Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of International Conference on World Wide
Web.
Kate?rina Veselovsk?a. 2012. Sentence-level sentiment
analysis in czech. In Proceedings of the 2nd Interna-
tional Conference on Web Intelligence, Mining and
Semantics. ACM.
Liang-Chih Yu, Jheng-Long Wu, Pei-Chann Chang,
and Hsuan-Shou Chu. 2013. Using a contextual en-
tropy model to expand emotion words and their in-
tensity for the sentiment classification of stock mar-
ket news. Knowledge Based Syst, 41:89?97, March.
30
