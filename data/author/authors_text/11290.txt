Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 497?504
Manchester, August 2008
 
Understanding and Summarizing Answers in Community-Based 
Question Answering Services 
Yuanjie Liu1, Shasha Li2, Yunbo Cao1,3, Chin-Yew Lin3, Dingyi Han1, Yong Yu1 
1Shanghai Jiao Tong University, 
Shanghai, China, 200240 
{lyjgeorge,handy,yyu} 
@apex.sjtu.edu.cn 
2National University of 
Defense Technology, 
Changsha, China, 410074
Shashali 
@nudt.edu.cn 
3Microsoft Research Asia,
Beijing, China, 100080 
{yunbo.cao,cyl} 
@microsoft.com 
 
Abstract 
Community-based question answering 
(cQA) services have accumulated millions 
of questions and their answers over time. 
In the process of accumulation, cQA ser-
vices assume that questions always have 
unique best answers. However, with an in-
depth analysis of questions and answers 
on cQA services, we find that the assump-
tion cannot be true. According to the anal-
ysis, at least 78% of the cQA best answers 
are reusable when similar questions are 
asked again, but no more than 48% of 
them are indeed the unique best answers. 
We conduct the analysis by proposing 
taxonomies for cQA questions and an-
swers. To better reuse the cQA content, 
we also propose applying automatic sum-
marization techniques to summarize an-
swers. Our results show that question-type 
oriented summarization techniques can 
improve cQA answer quality significantly. 
1 Introduction 
Community-based question and answering (cQA) 
service is becoming a popular type of search re-
lated activity. Major search engines around the 
world have rolled out their own versions of cQA 
service. Yahoo! Answers, Baidu Zhidao, and 
Naver Ji-Sik-In1 are some examples.  
In general, a cQA service has the following 
workflow. First, a question is posted by the asker 
in a cQA service and then people in the commu-
nity can answer the question. After enough num-
ber of answers are collected, a best answer can 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution- Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
be chosen by the asker or voted by the communi-
ty. The resulting question and answer archives 
are large knowledge repositories and can be used 
to complement online search. For example, Nav-
er?s Ji-Sik-In (Knowledge iN) has accumulated 
about 70 million entries2.  
In an ideal scenario, a search engine can serve 
similar questions or use best answers as search 
result snippets when similar queries are submit-
ted. To support such applications, we have to 
assume the best answers from cQA services are 
good and relevant answers for their pairing ques-
tions. However, the assumption might not be true 
as exemplified by the following examples. 
Question Title 
Which actress has the most seductive 
voice?..could range from a giggly goldie 
hawn..to a sultry anne bancroft? 
Question  
Description 
or any other type of voice that you find allur-
ing. .. 
Best Answer 
(Polls & Surveys) Fenella Fielding, wow!!!! 
Best Answer 
(Movies) i think joanna lumlley has a really sexy voice 
Table 1. Same Question / Different Best Answers
 
Question Title Does anyone know of any birthdays coming up soon? 
Question  
Description 
Celerities, people you know, you? Anyway I 
need the name and the date. If you want to 
know it is for my 
site,  http://www.jessicaparke2.piczo.com... 
and that is not site advertising.  
Answer 
Novembers Are: 
Paul Dickov nov 1st 
Nelly (not furtado) nov 2nd ? 
Best Answer Check imdb.com, they have this celebrity birthdays listed. 
Table 2. Question with Alternative Answers 
Table 1 presents a question asking communi-
ty opinions about ?who is the actress has the 
most seductive voice?. The asker posted the same 
question twice at different Yahoo! Answers cate-
gories: one in Polls & Surveys and one in Movies. 
                                                 
1 Yahoo! Answers: answers.yahoo.com; Baidu Zhidao: zhi-
dao.baidu.com; Naver Ji-Sik-In: kin.naver.com 
2www.iht.com/articles/2007/07/04/technology/naver.php 
497
 Two different best answers were chosen by the 
same asker due to non-overlapping of answers. 
Table 2 shows another example, it asks about 
?the coming birthdays of stars?. The best answer 
chosen by the asker is very good because it pro-
vides useful URL information where the asker 
can find her answers. However, other answers 
listed a variety of birthdays of stars that also 
answered the question. These two examples indi-
cate that the conventional cQA policy of allow-
ing askers or voters to choose best answers might 
be working fine with the purpose of cQA but it 
might not be a good one if we want to reuse these 
best answers without any post-processing. 
To find out what might be the alternatives to 
the best answers, we first carried out an in-depth 
analysis of cQA data by developing taxonomies 
for questions and answers. Then we propose 
summarizing answers in a consideration of ques-
tion type, as the alternative to the best answers. 
For example, for the ?actress voice? question, a 
summary of different people?s opinions ranked 
by popularity might be a better way for express-
ing the question?s answers. Similar to the ?ac-
tress voice? question, the ?celebrity birthday? 
question does not have a fix set of answers but is 
different from the ?actress voice? question that its 
answers are facts not opinions. For fact-based 
open ended questions, combining different an-
swers will be useful for reuse of those answers.  
The rest of this paper is arranged as follows. 
We review related work in Section 2. We devel-
op a framework for answer type taxonomy in 
Section 3 and a cQA question taxonomy in Sec-
tion 4. Section 5 presents methods to summarize 
cQA answers. Finally, we conclude this paper 
and discuss future work in Section 6. 
2 Related Work 
Previous research on cQA (community-based 
Question and Answering) domain focused on 
three major areas: (1) how to find similar ques-
tions given a new question (Jeon et al 2005a; 
Jeon et al, 2005b), (2) how to find experts given 
a community network(Liu et al, 2005; Jurczyk & 
Agichtein, 2007), and (3) how to measure answer 
quality and its effect on question retrieval. The 
third area of focus is the most relevant to our re-
search. Jeon et al (2006)?s work on assessing 
cQA answer quality is one typical example. They 
found that about 1/3 of the answers among the 
1,700 Q&A pairs from Naver.com cQA data 
have quality problems and approximately 1/10 of 
them have bad answers 3 . They used 13 non-
textual features and trained a maximum entropy 
model to predict answer quality. They showed 
that retrieval relevance was significantly im-
proved when answer quality measure was inte-
grated in a log likelihood retrieval model.  
As mentioned in Section 1, cQA services 
provide an alternative way for users to find in-
formation online. Questions posted on cQA sites 
should reflect users? needs as queries submitted 
to search engines do. Broder (2002) proposed 
that search queries can be classified into three 
categories, i.e. navigational, informational, and 
transactional. Ross and Levinson (2004) sug-
gested a more elaborated taxonomy with five 
more subcategories for informational queries and 
four more subcategories for resource (transac-
tional) queries. In open-domain question answer-
ing research that automatic systems are required 
to extract exact answers from a text database 
given a set of factoid questions (Voorhees and M. 
Ellen, 2003), all top performing systems had in-
corporated question taxonomies (Hovy et al, 
2001; Moldovan et al, 2000; Lytinen et al, 2002; 
Jijkoun et al, 2005). Based on the past expe-
riences from the annual NIST TREC Question 
and Answering Track 4  (TREC QA Track), an 
international forum dedicating to evaluate and 
compare different open-domain question answer-
ing systems, we conjecture that a cQA question 
taxonomy would help us determine what type of 
best answer is expected given a question type.  
Automatic summarization of cQA answers is 
one of the main focuses of this paper. We pro-
pose that summarization techniques (Hovy and 
Lin, 1999; Lin and Hovy, 2002) can be used to 
create cQA answer summaries for different ques-
tion types. Creating an answer summary given a 
question and its answers can be seen as a multi-
document summarization task. We simply re-
place documents with answers and apply these 
techniques to generate the answer summary. The 
task has been one of the main tasks the Docu-
ment Understanding Conference5 since 2004. 
3 A Framework  for Answer Type 
To study how to exploit the best answers of cQA, 
we need to first analyze cQA answers. We would 
like to know whether the existing best answer of 
a specific question is good for reuse. If not, we 
                                                 
3 Answers in Jeon el al.?s work were rated in three levels: 
good, medium, and bad. 
4 http://trec.nist.gov/data/qamain.html 
5 http://duc.nist.gov 
498
want to 
are. We 
by cQA 
ferentiat
tomatica
We m
swers fo
for answ
al categ
examini
the 4 mo
ries (100
tainmen
(S&C), H
we dev
based on
termines
can be r
ilar to th
One o
ercise an
The tax
cussions
notators 
category
annotato
on a sin
made t
taxonom
discuss 
answer t
Figur
Figur
onomy. 
Reusabl
means th
similar 
while a 
reused. 
Factual 
that can 
jective B
as the be
F
Unique
understand w
will refer to
askers or vo
e it with be
lly generate
ade use of
r developin
er type. The
ories in Yah
ng 400 rando
st popular 
 questions f
t & Music 
ealth, and 
eloped a cQ
 the princip
 a BA?s ans
eused or not
e BA?s ques
f the author
d developed
onomy was 
 among the 
to do the a
 label that 
rs. If none o
gle categor
he final d
y is describ
the question
axonomy in 
e 1. cQA Ser
e 1 shows th
It first divid
e and Not 
at it can be 
question to 
Not Reusab
The Reusabl
and Subject
be used as t
A is one of 
st answer. 
Reusable
actual
Not?
Unique
Direct Indire
Su
hy and wha
 the ?best a
ters as BA h
st answers 
d in our expe
 questions fr
g and testing
re are over 
oo! Answe
mly selecte
top Yahoo! 
rom each ca
(E&M), So
Computers &
A answer 
le of BA reu
wer type bas
 when a que
tion is asked
s carried ou
 the initial a
then modif
authors. We
nnotation. W
was agreed 
f the three a
y label, one
ecision. Th
ed in this 
 type and t
next section
vices BA Ty
e resulting 
es BA into
Reusable. A
reused as the
its question
le BA mea
e BA is fur
ive. A Fact
he best answ
the opinions
Best?
Answer
ct
bjective Re
t the alterna
nswers? sele
enceforth to
annotated or
riments. 
om Yahoo!
 our framew
1,000 hierar
rs. By manu
d questions 
Answers cat
tegory) ? E
ciety & Cu
 Internet (C
type taxon
sability tha
ed on ?if th
stion that is 
 again?. 
t this manua
nswer taxon
ied through
 asked three
e assigned
by at least
nnotators ag
 of the aut
e answer 
section and
he relation 
. 
pe Taxonom
answer type
 two catego
 Reusable
 best answe
 is asked a
ns it canno
ther divided
ual BA is a
er; while a 
 that can be 
Not??
Reusable
levant Irre
 
tives 
cted 
 dif-
 au-
 An-
ork 
chic-
ally 
from 
ego-
nter-
lture 
&I), 
omy 
t de-
e BA 
sim-
l ex-
omy. 
 dis-
 an-
 the 
 two 
reed 
hors 
type 
 we 
with 
 
y. 
 tax-
ries: 
BA 
r if a 
gain; 
t be 
 into 
 fact 
Sub-
used 
T
Uniq
a un
answ
Uniq
The 
type
its q
swer
ple, 
Indi
whil
birth
A
for o
ques
Each
T
vant
used
leva
aske
Nick
?I'm
ly So
answ
er?s 
ques
best
its q
tion 
give
answ
ema
To b
Answ
Uniqu
Direc
Indire
Factu
Subje
Reusa
Relev
Irrelev
Not R
T
type
mor
ries 
two 
 
A
mos
(50%
the o
or v
answ
levant
he Factual
ue and Not
ique best an
er add m
ue BA has
Not Unique
s: Direct an
uestion dire
s its questio
the question
rect BA wh
e there is al
day lists. 
 Subjective
pinions or r
tion asked ?
 answerer w
he Not Reus
 and Irrelev
 as a best an
nt to its qu
d ?Why was
 Lachey so 
 not sure wh
uth Jersey, 
er is releva
location wh
tion; an Irre
answer to i
uestion. Th
period has 
n that meets
er?.? of th
il without sh
ox? is in thi
er Type 
e 
t 
ct 
al Total 
ctive 
ble Total 
ant
ant
eusable Total
Table 3. D
able 3 show
s on four ca
e than 48%. 
tend to hav
categories.
mong the fo
tly not uniq
) of subjec
ne BA per c
oters is not g
er. Howeve
BA type 
 Unique. A 
swer to its q
ore informa
 other alter
BA type is d
d Indirect. A
ctly; while 
n through i
 mentioned
ich gives a
so a Direct a
BA answers
ecommenda
Which is th
ould have h
able BA has
ant. A Relev
swer to its 
estion, for 
 "I Can't Ha
shortlived??
ere you live
that song wa
nt but witho
ich does n
levant BA c
ts question a
e BA ?It ap
expired. If 
 your needs
e question ?
owing the em
s case. 
C&I
47%
28%
9%
84%
4%
88%
3%
9%
12%
istribution o
s the dist
tegories. Un
The C&I an
e more fac
ur categorie
ue and hav
tive answers
QA questio
ood enough
r, we might
has two s
Unique BA 
uestion and 
tion; while
native best 
ivided into 
 Direct BA
an Indirect
nference. Fo
 in section 1
 website re
nswer just g
 questions t
tions. For ex
e best sci-fi 
is own idea.
 two subtyp
ant BA cou
question bu
example, a 
te You Anym
 A Relevant
, but in NJ, 
s played ou
ut knowing
ot really an
ould not be u
nd it is irre
pears that t
an answer h
, please pic
how to for
ail address
E&M Heal
28% 48
7% 30
3% 5
38% 83%
40% 7
78% 90%
1% 1
21% 9
22% 10%
f Answer Ty
ribution of 
ique answer
d the Health
tual BAs th
s, S&C ans
e a high pe
. This indic
n chosen by
 for reuse as
 be able to a
ubtypes: 
has only 
no other 
 a Not 
answers. 
two sub-
 answers 
 BA an-
r exam-
 has the 
ference, 
ives the 
hat look 
ample, a 
movie?? 
 
es: Rele-
ld not be 
t it is re-
question 
ore" by 
BA said 
especial-
t??, this 
 the ask-
swer the 
sed as a 
levant to 
he ques-
as been 
k a ?best 
ward an 
es in the 
th S&C
% 13%
% 18%
% 2%
 33%
% 50%
 83%
% 0%
% 17%
 17%
pe 
Answer 
s are no 
 catego-
an other 
wers are 
rcentage 
ates that 
 its asker 
 the best 
pply au-
499
tomatic 
summar
(but not 
ible solu
E
T
Table
over wh
on a sin
the ques
stable (o
4 A C
As we w
my, we 
themselv
well. As
question
best answ
Rose an
engine q
their tax
engine q
we follo
onomy a
modate t
Fi
Figur
my. We
and prop
Informa
ilar as in
ry consi
an answ
with peo
Navig
seeking 
would li
know the
Trans
tend to g
compute
Navigat
summariza
ized answers
unique) answ
tions in Sect
Category
Computer & In
ntertainment &
Health 
Society & Cul
able 4. Disag
 4 shows t
ich none of
gle category
tion taxonom
ver at least 7
QA Quest
ere develop
often could
es and had 
 we discuss
 would help
er types.  
d Levinson?
ueries has 
onomy was 
ueries. Inste
wed the ba
nd made so
he particula
gure 2.  Que
e 2 shows th
 retain Brod
ose a new S
tional and Tr
 Broder?s ta
sts of questi
er but just w
ple in cQA 
ational ca
URLs of sp
ke to visit, 
 fan sites of
actional ca
et resources
r program th
ional Inform
Constant
Opinion
tion techni
 for at least
ers. We pro
ion 5. 
 
ternet 
 Music 
ture 
reement on 
he percenta
 the three a
 label. The r
y develope
9% question
ion Taxon
ing our answ
 not solely 
to consider t
ed in Sectio
 us determ
s (2004) tax
similar goal
developed t
ad of starti
sic hierarchy
me modific
r of cQA ser
stion Type T
e resulting 
er?s taxono
ocial catego
ansactional
xonomy whi
ons that do 
ere used to 
services. 
tegory con
ecific websit
for example
 Hannah Mo
tegory con
. A typical o
at lets you c
cQA
Question
ational
Dynamic
Context?
Dependent
Trans
ques to c
 half of reus
vide some p
Percenta
18%
17%
21%
20%
Answer Typ
ge of ques
nnotators ag
esults show
d above is p
s). 
omy 
er type tax
rely on ans
heir question
n 2, the typ
ine the expe
onomy of se
 to ours th
o classify se
ng from scr
 of R&L?s 
ations to acc
vices. 
axonomy
question tax
my at top le
ry. Navigati
 are defined 
le Social cat
not intend to
elicit intera
tains ques
es that the a
, ?Does any
ntana?? 
tains ques
ne is ?Is the
reate a plan
Open
actional
 
reate 
able 
oss-
ge 
e 
tions 
reed 
 that 
retty 
ono-
wers 
s as 
e of 
cted 
arch 
ough 
arch 
atch, 
tax-
om-
 
ono-
vels 
onal, 
sim-
ego-
 get 
ction 
tions 
sker 
body 
tions 
re a 
et?? 
F
to t
Con
answ
dich
port
betw
taxo
R&L
ques
latio
wou
F
tego
Opin
Que
peop
think
jects
ple.
tion
diffe
?Wh
diffe
Ope
som
have
selv
tion 
com
follo
clud
cont
T
serv
to g
joke
tially
or o
lazy
toge
beco
goog
will 
they
a ne
who
T
ques
cate
only
ques
occu
sinc
sear
Social
or Informati
wo subcateg
stant questio
ers while d
otomy of in
 our intentio
een the que
nomy. Cons
?s closed q
tion is ?Whi
n?? but ?W
ld be a dyna
or Dynamic
ries: Opinio
ion questio
stions in th
le in cQA 
 of some p
. ?Is Micros
Context-dep
s having dif
rent contex
at is the pop
rent answer
n category
e facts or m
 a variety o
es may have
?Can you li
ing week??
ws R&L?s 
es what is n
ext-depende
he new Soc
ices. Questio
et an answer
s and expre
, askers trea
nline forums
people com
ther with th
me a hacker
le search?
continue to 
 can give up
gative sentim
 asked how t
able 5 show
tion types 
gories. We 
 occupy 1
tions are ev
r in the sam
e people ve
ch engines 
on category,
ories: Con
ns have a f
ynamic que
formational
n to establi
stion taxon
tant questio
uery type. A
ch country h
hat is the po
mic question
category, w
n, Context-D
ns are those 
is category 
communiti
eople, some
oft Vista wo
endent ques
ferent answ
t. For exa
ulation of C
s according 
contains q
ethods. Th
f answers o
 unconstrain
st some birt
is an exam
open query 
ot covered 
nt categories
ial category
ns in this ca
. These que
ssing askers
t cQA servi
. The questi
e on here si
e question 
? It really is
hopefully so
ask, will cli
 faster? ? a
ent toward
o become a 
s the distr
on 4 differe
observe tha
1% ~ 20%
en fewer su
ple question
ry likely w
to discover 
 we first div
stant and D
ixed or stab
stions do n
 category is
sh intuitive 
omy and the
n type is s
n example 
as the large
pulation of 
. 
e define thre
ependent an
asking for o
seek opinio
es about w
 events, or s
rth it?? is a
tions are tho
ers accordin
mple, the 
hina?? sho
to the differ
uestions ask
e questions
r their answ
ed depth. T
hdays of sta
ple. This es
category. It
by the opin
. 
 is specific
tegory do n
stions includ
? own ideas
ce as chattin
on ?Why do 
mply just to
description 
n't that har
me of the pe
ck the link b
ctually is ex
s a number o
hacker. 
ibution of 
nt Yahoo! 
t constant q
 while nav
ch that they
s. This is re
ould be abl
answers of
ide it in-
ynamic. 
le set of 
ot. This 
 to sup-
mapping 
 answer 
imilar to 
constant 
st popu-
China?? 
e subca-
d Open. 
pinions. 
ns from 
hat they 
ome ob-
n exam-
se ques-
g to the 
question 
uld have 
ent date. 
ing for 
 usually 
er them-
he ques-
rs in the 
sentially 
 also in-
ion and 
 to cQA 
ot intend 
e telling 
. Essen-
g rooms 
so many 
 ask...?? 
?how to 
d to do a 
ople that 
elow so 
pressing 
f people 
different 
Answers 
uestions 
igational 
 do not 
asonable 
e to use 
 naviga-
500
 tional and constant questions. They do not have 
to ask these types of question on community-
based question answering services. On the con-
trary, open and opinion questions are frequently 
asked, it ranges from 56%~83%.  
Question Type C&I E&M Health S&C
Navigational Total 0% 0% 0% 0%
Constant 15% 20% 15% 11%
Opinion 8% 37% 16% 60%
Context     Dependent 0% 1% 1% 0%
Open 59% 19% 67% 18%
Dynamic Total 67% 57% 84% 78%
Informational Total 82% 77% 99% 89%
Transactional Total 14% 8% 0% 1%
Social Total 4% 15% 1% 10%
Table 5 Distribution of Question Type 
Intersection Number UNI DIR IND SUB REL IRR
Navigational 0 0 0 0 0 0
Constant 48 9 3 0 1 0
Open 51 62 13 15 5 17
Context-dep 0 0 1 0 0 1
Opinion 15 13 1 84 0 8
Transactional 10 7 4 1 0 1
Social 0 0 0 1 0 29
Table 6. Question Answer Correlation 
Table 6 (UNI: unique, DIR: direct, IND: indi-
rect, SUB: subjective, REL: relevant, IRR: irre-
levant) gives the correlation statistics of question 
type vs. answer type. There exists a strong corre-
lation between question type and answer type. 
Every question type tends to be associated with 
only one or two answer types (bold numbers in 
Table 6).  
5 Question-Type Oriented Answer 
Summarization 
Since the BAs for at least half of questions do 
not cover all useful information of other answers, 
it is better to adopt post-processing techniques 
such as answer summarization for better reuse of 
the BAs. As observed in the previous sections, 
answer types can be basically predicted by ques-
tion type. Thus, in this section, we propose to use 
multi-document summarization (MDS) tech-
niques for summarizing answers according to 
question type. Here we assume that question type 
can be determined automatically. In the follow-
ing sub-sections, we will focus on the summari-
zation of answers to open or opinion questions as 
they occupy more than half of the cQA questions. 
5.1 Open Questions 
Algorithm: For open questions, we follow typi-
cal MDS procedure: topic identification, inter-
pretation & fusion, and then summary generation 
(Hovy and Lin, 1999; Lin and Hovy, 2002). Ta-
ble 7 describes the algorithm.  
1. Employ the clustering algorithm on answers 
2. Extract the noun phrases in each cluster, using a shallow parser.6 
3. For each cluster and each label (or noun phrase), calculate the 
score by using the Relevance Scoring Function:  
?p?w|??PMI?w, l|C? ?  D??|C?
?
 
Where ? is the cluster, w is the word, l is the label or noun phrase, C 
is the background context which is composed of 5,000 questions 
in the same category, p(?) is conditional probability, PMI(?) is 
pointwise mutual information, and D(?) is KL-divergence 
4. Extract the key answer which contains the noun phrase that has 
the highest score in each cluster 
5. Rank these key answers by cluster size and present the results. 
Table 7. Summarization Algorithm(Open-Type) 
In the first step, we use a bottom-up approach 
for clustering answers to do topic identification. 
Initially, each answer forms a cluster. Then we 
combine the most similar two clusters as a new 
cluster if their similarity is higher than a thre-
shold. This process is repeated until no new clus-
ters can be formed. For computing similarities, 
we regard the highest cosine similarity of two 
sentences from two different clusters as the simi-
larity of the two clusters. Then we extract salient 
noun phrases, i.e. cluster labels, from each clus-
ter using the first-order relevance scoring func-
tion proposed by Mei et al (2007), (step 2,3 in 
Table 7).  In the fusion phase (step 4), these 
phrases are then used to rank answers within 
their cluster. Finally in the generation phase (step 
5), we present the summarized answer by ex-
tracting the most important answer in every clus-
ter and sort them according to the cluster size 
where they come from.  
Case Example: Table 8 presents an example 
of summarization results of open-type questions. 
The question asks how to change Windows XP 
desktop to Mac style. There are many softwares 
providing such functionalities. The BA only lists 
one choice ? the StarDock products, while other 
answers suggest Flyakite and LiteStep. The au-
tomatic summarized answer (ASA) contains a 
variety of for turning Windows XP desktop into 
Mac style with their names highlighted as cluster 
labels. Compared with manually-summarized 
answer (MSA), ASA contains most information 
of MSA while retains similar length with BA and 
MSA. 
5.2 Opinion Questions 
Algorithm: For opinion questions, a comprehen-
sive investigation of this topic would be beyond 
the scope of this paper since this is still a field 
                                                 
6 http://opennlp.sourceforge.net 
501
 under active development (Wiebe et al, 2003; 
Kim and Hovy, 2004). We build a simple yet 
novel opinion-focused answer summarizer which 
provides a global view of all answers. We divide 
opinion questions into two subcategories. One is 
sentiment-oriented question that asks the senti-
ment about something, for example, ?what do 
you think of ??. The other is list-oriented ques-
tion that intends to get a list of answers and see 
what item is the most popular.  
For sentiment-oriented questions, askers care 
about how many people support or against some-
thing. We use an opinion word dictionary7, a cue 
phrase list, a simple voting strategy, and some 
heuristic rules to classify the sentences into Sup-
port, Neutral, or Against category and use the 
overall attitude with key sentences to build sum-
marization. For list-oriented questions, a simple 
counting algorithm that tallies different answers 
of questions together with their supporting votes 
would be good answer summaries. Details of the 
algorithm are shown in Table 9, 10. 
Case Example: Table 11 presents the summa-
rization result of an sentiment-oriented question, 
it asks ?whether it is strange for a 16-year child 
to talk to a teddy bear??, the BA is a negative 
response. However, if we consider all answers, 
                                                 
7 Inquirer dictionary  http://www.wjh.harvard.edu/~inquirer. 
we find that half of the answers agree but another 
half of them disagree. The distribution of differ-
ent sentiments is similar as MSA. Table 12 
shows the summarization result of a list-oriented 
question, the question asks ?what is the best sci-fi 
movie?? The BA just gives one choice ?Indepen-
dence day? while the summarized answer gives a 
list of best sci-fi movies with the number of sup-
porting vote. Though it is not complete compared 
with MSA, it contains most of the options which 
has highest votes among all answers. 
1. Employ the same cluster procedure of Open-Type question. 
2. If an answer begins with negative cue phrase (e.g. ?No, it isn?t? 
etc.), it is annotated as Against. If a response begins with positive 
cue phrase (e.g. ?Yes, it is? etc.), it is annotated as Support. 
3. For a clause, if number of positive sentiment word is larger than 
negative sentiment word, the sentiment of the clause is Positive. 
Otherwise, the sentiment of the clause is Negative. 
4. If there are negative indicators such as ?don?t/never/?? in front 
of the clause, the sentiment should be reversed. 
5. If number of negative clauses is larger than number of positive 
clauses, the sentiment of the answer is Negative. Otherwise, the 
sentiment of the answer is Positive. 
6. Denote the sentiment value of question as s(q), the sentiment 
value of an answer as s(a), and then the final sentiment of the an-
swer is logical AND of s(q) and s(a) 
7. Present key sentiments with attitude label 
Table 9. Summarization Algorithm (Senti-
ment-Opinion) 
1. Segment the answers into sentences 
2. Cluster sentences  by using similar process in open-type 
3. For each cluster, choose the key sentence based on mutual infor-
mation between itself and other sentences within the cluster 
4. Rank the key sentences by the cluster size and present them 
ogether with votes 
Table 10. Summarization Algorithm (List-
Opinion) 
Question (http://answers.yahoo.com/question/?qid=1006050125145) 
I am 16 and i stil talk to my erm..teddy bear..am i wierd??? 
Best Answer Chosen 
not at all i'm 14 and i too do that 
Auto-summarized Answer 
Support 
A: It's might be a little uncommon for a 16 year old to talk to a 
teddy bear but there would be a serious problem if you told me that 
your teddy bear answered back as you talked to him!!:)  
A: I slept with my teddy bear until I graduated.  Can't say that I 
ever had a conversation with him, but if I had I'm sure he would've 
been a very good listener. 
Against 
A: i talk to a  seed im growing .. its not weird .... :)  
A: No, you're not weird.....you're Pratheek! :D  
A: no, i like to hold on to my old memories too. i do it sometimes 
too.  
A: It will get weird when he starts to answer back!  
A: not really. it depends how you talk i mean not if you talk to it 
like its a little kid like my brother does.  
Overall Attitude: Support 5 / Neutral 1 / Against 5  
Manually-summarized Answer 
support (vote 4) 
neutral (vote 2) 
against (vote 5) reasons: i like to hold on to my old memories too. 
(vote 1) I slept with my teddy bear until I graduated. (vote 1) i'm 14 
and i too do that (vote 1) 
Table 11. Summary of Sentiment-Opinion 
Question 
 
 
Question 
(http://answers.yahoo.com/question/?qid=1005120801427) 
What is the best way to make XP look like Mac osX? 
Best Answer Chosen 
I found the best way to do this is to use WindowsBlinds. A pro-
gram that, if you use the total StarDock, package will allow you to 
add the ObjectBar in addition to changed the toolbars to be OS X 
stylized. If you want added functionality you can download pro-
grams off the internet that will mimic the Expose feature which will 
show you a tiled set of all open windows. Programs that will do this 
include: WinPlosion, Windows Exposer, and Top Desk 
Auto-summarized Answer 
LiteStep:An additional option is LiteStep - a "Shell Replacement" 
for Windows that has a variety of themes you can install. Undoub-
tedly there are various Mac OSX themes avaialable for LiteStep. I 
have included a source to a max osx theme for Litestep at custom-
ize.org.  
Flyakite:Flyakite is a transformation pack and the most compre-
hensive in terms of converting an XP system's look to that of an OS 
X system, google it up and you should find it, v3 seems to be in 
development and should be out soon. 
Window Blinds:http://www.stardock.com/products/windowb... 
Manually-summarized Answer 
One way is to use WindowsBlinds. The package will allow you to 
add the ObjectBar for changing to the OSX theme. You can also 
make added functionality of Expose feature by downloading the 
programs like WinPlosion, Windows Exposer and Top Desk. The  
URL of it is http://www.stardock.com/products/windowblinds/. 
Another option is to use Flyakite which is a transformation pack. 
The third Option is the LiteStep, it is a "Shell Replacement" for 
windows that has a variety of Mac OSX tehmes you can install. 
The url is http://litestep.net and I have included a source of Mac OS 
theme for Litestep at http://www.customize.org/details/33409. 
Table 8. Summary of Open-Question 
502
  
Question (http://answers.yahoo.com/question/?qid= 
20060718083151AACYQJn) 
What is the best sci-fi movie u ever saw? 
Best Answer Chosen 
Independance Day 
Auto-summarized Answer 
star wars (5)  
Blade Runner (3) 
fi movie has to be Night of the Lepus (2)  
But the best "B" sci (2)  
I liked Stargate it didn't scare me and I thought they did a great job 
recreating Egypt (3)  
Independance Day (3) 
Manually-summarized Answer 
Star Wars (vote 6); The Matrix (vote 3); Independence Day (vote 
2); Blade Runner (vote 2); Starship Troopers (vote 2); Alien (vote 
2); Alien v.s Predator (vote 1); MST3K (vote 1);  
Table 12. Summary of List-Opinion Question
5.3 Experiments 
Information Content: To evaluate the effec-
tiveness of automatic summarization, we use the 
information content criterion for comparing ASA 
with BA. It focuses on whether ASA or BA con-
tains more useful information to the question. 
Information point is used in the evaluation. 
Usually, one kind of solution for open questions 
or one kind of reason for opinion questions can 
contribute one information point. By summing 
all information points in both ASA and BA, we 
then can compare which one contains more in-
formation. Intuitively, longer texts would contain 
more information. Thus, when comparing the 
information content, we limit the length of ASA 
with several levels to do the evaluation. Take 
question in Table 8 as an example, the BA just 
gives one software, which contributes one infor-
mation point while the ASA lists three kinds of 
software which contributes three information 
points. Thus, ASA is considered better than BA.  
For each question, we generate 100%, 150%, 
and 200% BA word-length ASAs. Three annota-
tors are asked to determine whether an ASA is 
better than, equal to, or worse than its corres-
ponding BA in terms of information content. 
Voting strategy is used to determine the final 
label. If three labels are all different, it is labeled 
as Unknown. We extract 163 open questions and 
121 opinion questions from all four categories by 
using final question category labels mentioned in 
Section 4. To make meaningful comparison, 
questions having unique answers or having only 
one answer are excluded. After the removal, 
there are 104 open questions and 99 opinion 
questions left for comparison. The results are 
shown in Table 13.  
We are encouraged by the evaluation results 
that our automatic summarization methods gen-
erate better coverage of contents in most of the 
cases at every answer summary length. We ob-
serve a big difference between 100% and 150% 
answer summaries. It should not be a surprise 
since a 150% answer summary contains 50% 
more content than its corresponding BA. While 
at the 100% length, we still have about 30% 
ASAs better than BA. Questions which have bet-
ter ASA than BA usually have a long BA but 
with little information. Table 14 provides the 
example. By using summarization, answers that 
are compact and direct to the question can be 
included. The results indicate that summary 
compression technique might be helpful to pack 
more information in short answers. 
Open ASA Better BA Better Equal Unknown 
100% 30% 12% 45% 13% 
150% 55% 7% 28% 10% 
200% 63% 4% 24% 9% 
Opinion ASA Better BA Better Equal Unknown
100% 37% 20% 32% 11% 
150% 44% 16% 30% 10% 
200% 54% 16% 23% 7% 
Table 13. Evaluation by Information Content 
Q Why wont japanese characters burn onto the DVD? 
BA man, the answers here are too stupid for hteir own.You are 
creating a DVD on Western Platform. I take it, you are 
using an OS that is in English?In order to "view" japanese 
as part of your filenames, you need your operating system 
to accept Japanese coding (characters).If you are using 
Windows, then you will need ot isntall the Japanese cha-
racter Set for your operating system 
If you are using MacOS . i have no idea. 
100% 
ASA
 
The dvd writer 
Probably because your burner, the DVD writer, doesn't 
support double bytes code, such as Japanese, Korean, and 
Chinese. Check the supporting language of your software. 
Or change all the file name in single byte code, like alpha-
bets. man, the answers here are too stupid for hteir own. 
You are creating a DVD on Western Platform. I take it, 
you are using an OS that is in English? 
Table 14. Examples of 100% ASA 
Readability: Besides the information content, 
we would also like to study the readability of 
automatic summarized answers. 10 questions 
(each from open and opinion category) are ex-
tracted and we make both manual summarized 
answer (MSA) and automatic summarized an-
swer (ASA) for comparison with BA. We used 
the information content (INFO) and readability 
(READ) criteria for evaluation. The readability is 
judged basically by the time for understanding. 
We make two kinds of comparison: ASA vs. BA 
and MSA vs. BA. The first one is used to judge 
whether the current summarization method is 
better than current cQA scenario. The second one 
is used as an expectation for how much the 
summarization methods can be better than BA. 
503
 For ASA vs. BA, the results in Table 15 show 
that all the annotators agree ASAs providing 
more information content but not being with sa-
tisfying readability. For MSA vs. BA, better re-
sults in readability can be achieved as Table 16. 
This suggests that the proposed approach can 
succeed as more sophisticated summarization 
techniques are developed. 
Open Annotator 1 Annotator 2 Annotator 3 
ASA INFO READ INFO READ INFO READ
Better 40% 10% 90% 10% 80% 0%
Equal 60% 60% 10% 80% 20% 60%
Worse 0% 30% 0% 10% 0% 40%
Opinion Annotator 1 Annotator 2 Annotator 3 
ASA INFO READ INFO READ INFO READ
Better 90% 10% 90% 10% 70% 40%
Equal 10% 60% 10% 60% 10% 20%
Worse 0% 30% 0% 30% 20% 40%
Table 15. ASA vs. BA Evaluation 
Open Annotator 1 Annotator 2 Annotator 3 
MSA INFO READ INFO READ INFO READ
Better 100% 30% 100% 90% 100% 90%
Equal 0% 50% 0% 0% 0% 0%
Worse 0% 20% 0% 10% 0% 10%
Opinion Annotator 1 Annotator 2 Annotator 3 
MSA INFO READ INFO READ INFO READ
Better 90% 20% 60% 70% 100% 100%
Equal 10% 80% 40% 30% 0% 0%
Worse 0% 0% 0% 0% 0% 0%
Table 16. MSA vs. BA Evaluation 
6 Conclusion and Future Work 
In this paper, we have carried out a comprehen-
sive analysis of the question types in community-
based question answering (cQA) services and 
have developed taxonomies for questions and 
answers. We find that questions do not always 
have unique best answers. Open and opinion 
questions usually have multiple good answers. 
They occupied about 56%~83% and most of 
their best answers can be improved. By using 
question type as a guide, we propose applying 
automatic summarization techniques to summa-
rization answers or improving cQA best answers 
through answer editing. Our results show that 
customized question-type focused summarization 
techniques can improve cQA answer quality sig-
nificantly.  
Looking into the future, we are to develop au-
tomatic question type identification methods to 
fully automate answer summarization. Further-
more, we would also like to utilize more sophis-
ticated summarization techniques to improve 
content compaction and readability. 
Acknowledgements 
We thank the anonymous reviewers for their val-
uable suggestions and comments to this paper. 
References 
Broder  A. A taxonomy of web search. 2002. SIGIR 
Forum Vol.36, No. 2, 3-10. 
Hovy Edward, Laurie Gerber,  Ulf Hermjakob, Chin-
Yew Lin,   Deepak Ravichandran. 2001. Toward 
Semantics-Based Answer Pinpointing. In Proc. of 
HLT?01. 
Hovy E., C. Lin. 1999. Automated Text Summariza-
tion and the SUMMARIST System. In Advances 
in Automated Text Summarization 
Jeon J., W. B. Croft, and J. Lee. 2005a. Finding se-
mantically similar questions based on their an-
swers. In Proc. of SIGIR?05. 
Jeon J., W. B. Croft, and J. Lee. 2005b. Finding simi-
lar questions in large question and answer arc-
hives. In Proc. of CIKM?05. 
Jurczyk P., E. Agichtein. 2007. Hits on question an-
swer portals: exploration of link analysis for au-
thor ranking. In Proc. of SIGIR '07. 
Jeon J. , W.B. Croft, J. Lee, S. Park. 2006. A Frame-
work to predict the quality of answers with non-
textual features. In Proc. of SIGIR ?06. 
Jijkoun V., M. R. 2005. Retrieving Answers from 
Frequently Asked Questions Pages on the Web. In 
Proc. of CIKM?05. 
Kleinberg J. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, vol. 46,  
Kim S., E.  Hovy. 2004. Determining the Sentiment of 
Opinions. In Proc. of COLING?04. 
Liu X., W.B. Croft, M. Koll. 2005. Finding experts in 
community-based question-answering services. 
In Proc. of CIKM '05. 
Lin C.Y., E. Hovy. 2002.  From single to multi-
document summarization: a prototype system and 
its evaluation.  In Proc. of ACL'02. 
Lytinen S., N. Tomuro. 2002. The Use of Question 
Types to Match Questions in FAQFinder. In Proc. 
of AAAI?02. 
Moldovan D., S. Harabagiu, et al 2000. The Structure 
and an Open-Domain Question Answering Sys-
tem. In Proc. of ACL?00. 
Mei Q., X. Shen, C. Zhai. 2007. Automatic labeling of 
multinomial topic models. In Proc. of   KDD'07. 
Rose  D. E., D. Levinson. 2004. Understanding user 
goals in web search. In Proc. of WWW '04. 
Voorhees, M. Ellen. 2003. Overview of the TREC 
2003 Question Answering Track. In Proc. of 
TREC?03. 
Wiebe J., E. Breck, et al 2003. Recognizing and Or-
ganizing Opinions Expressed in the World Press 
504
Proceedings of ACL-08: HLT, pages 156?164,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Searching Questions by Identifying Question Topic and Question Focus 
 
Huizhong Duan1, Yunbo Cao1,2, Chin-Yew Lin2 and Yong Yu1 
1Shanghai Jiao Tong University,  
Shanghai, China, 200240 
{summer, yyu}@apex.sjtu.edu.cn 
2Microsoft Research Asia,  
Beijing, China, 100080 
{yunbo.cao, cyl}@microsoft.com 
 
 
Abstract 
This paper is concerned with the problem of 
question search. In question search, given a 
question as query, we are to return questions 
semantically equivalent or close to the queried 
question. In this paper, we propose to conduct 
question search by identifying question topic 
and question focus. More specifically, we first 
summarize questions in a data structure con-
sisting of question topic and question focus. 
Then we model question topic and question 
focus in a language modeling framework for 
search. We also propose to use the MDL-
based tree cut model for identifying question 
topic and question focus automatically. Expe-
rimental results indicate that our approach of 
identifying question topic and question focus 
for search significantly outperforms the base-
line methods such as Vector Space Model 
(VSM) and Language Model for Information 
Retrieval (LMIR).  
1 Introduction 
Over the past few years, online services have been 
building up very large archives of questions and 
their answers, for example, traditional FAQ servic-
es and emerging community-based Q&A services 
(e.g., Yahoo! Answers1 , Live QnA2, and Baidu 
Zhidao3).   
To make use of the large archives of questions 
and their answers, it is critical to have functionality 
facilitating users to search previous answers. Typi-
cally, such functionality is achieved by first re-
trieving questions expected to have the same 
answers as a queried question and then returning 
the related answers to users. For example, given 
question Q1 in Table 1, question Q2 can be re-
                                                          
1 http://answers.yahoo.com 
2 http://qna.live.com 
3 http://zhidao.baidu.com 
turned and its answer will then be used to answer 
Q1 because the answer of Q2 is expected to par-
tially satisfy the queried question Q1. This is what 
we called question search. In question search, re-
turned questions are semantically equivalent or 
close to the queried question.  
 
Query: 
Q1: Any cool clubs in Berlin or Hamburg? 
Expected: 
Q2: What are the best/most fun clubs in Berlin? 
Not Expected: 
Q3: Any nice hotels in Berlin or Hamburg? 
Q4: How long does it take to Hamburg from Berlin? 
Q5: Cheap hotels in Berlin? 
Table 1. An Example on Question Search 
Many methods have been investigated for tack-
ling the problem of question search. For example, 
Jeon et al have compared the uses of four different 
retrieval methods, i.e. vector space model, Okapi, 
language model, and translation-based model, 
within the setting of question search (Jeon et al, 
2005b).  However, all the existing methods treat 
questions just as plain texts (without considering 
question structure). For example, obviously, Q2 
can be considered semantically closer to Q1 than 
Q3-Q5 although all questions (Q2-Q5) are related 
to Q1. The existing methods are not able to tell the 
difference between question Q2 and questions Q3, 
Q4, and Q5 in terms of their relevance to question 
Q1. We will clarify this in the following. 
In this paper, we propose to conduct question 
search by identifying question topic and question 
focus.  
The question topic usually represents the major 
context/constraint of a question (e.g., Berlin, Ham-
burg) which characterizes users? interests. In con-
trast, question focus (e.g., cool club, cheap hotel) 
presents certain aspect (or descriptive features) of 
the question topic. For the aim of retrieving seman-
tically equivalent (or close) questions, we need to 
156
assure that returned questions are related to the 
queried question with respect to both question top-
ic and question focus. For example, in Table 1, Q2 
preserves certain useful information of Q1 in the 
aspects of both question topic (Berlin) and ques-
tion focus (fun club) although it loses some useful 
information in question topic (Hamburg). In con-
trast, questions Q3-Q5 are not related to Q1 in 
question focus (although being related in question 
topic, e.g. Hamburg, Berlin), which makes them 
unsuitable as the results of question search.  
We also propose to use the MDL-based (Mini-
mum Description Length) tree cut model for auto-
matically identifying question topic and question 
focus. Given a question as query, a structure called 
question tree is constructed over the question col-
lection including the queried question and all the 
related questions, and then the MDL principle is 
applied to find a cut of the question tree specifying 
the question topic and the question focus of each 
question. 
In a summary, we summarize questions in a data 
structure consisting of question topic and question 
focus. On the basis of this, we then propose to 
model question topic and question focus in a lan-
guage modeling framework for search. To the best 
of our knowledge, none of the existing studies ad-
dressed question search by modeling both question 
topic and question focus. 
We empirically conduct the question search with 
questions about ?travel? and ?computers & internet?. 
Both kinds of questions are from Yahoo! Answers. 
Experimental results show that our approach can 
significantly improve traditional methods (e.g. 
VSM, LMIR) in retrieving relevant questions.  
The rest of the paper is organized as follow. In 
Section 2, we present our approach to question 
search which is based on identifying question topic 
and question focus. In Section 3, we empirically 
verify the effectiveness of our approach to question 
search. In Section 4, we employ a translation-based 
retrieval framework for extending our approach to 
fix the issue called ?lexical chasm?. Section 5 sur-
veys the related work. Section 6 concludes the pa-
per by summarizing our work and discussing the 
future directions.  
2 Our Approach to Question Search 
Our approach to question search consists of two 
steps: (a) summarize questions in a data structure 
consisting of question topic and question focus; (b) 
model question topic and question focus in a lan-
guage modeling framework for search. 
In the step (a), we employ the MDL-based (Min-
imum Description Length) tree cut model for au-
tomatically identifying question topic and question 
focus. Thus, this section will begin with a brief 
review of the MDL-based tree cut model and then 
follow that by an explanation of steps (a) and (b). 
2.1 The MDL-based tree cut model 
Formally, a tree cut model ? (Li and Abe, 1998) 
can be represented by a pair consisting of a tree cut 
?, and a probability parameter vector ? of the same 
length, that is, 
? ? ??, ??  (1) 
where ? and ? are 
? ? ???, ??, . . ???,  
? ? ??????, ?????, ? , ??????  
(2) 
where ??, ??, ????are classes determined by a cut 
in the tree and ? ????? ? 1
?
??? . A ?cut? in a tree is 
any set of nodes in the tree that defines a partition 
of all the nodes, viewing each node as representing 
the set of child nodes as well as itself. For example, 
the cut indicated by the dash line in Figure 1 cor-
responds to three classes:???, ????,????, ????, and 
????, ???, ???, ????. 
Figure 1. An Example on the Tree Cut Model 
A straightforward way for determining a cut of a 
tree is to collapse the nodes of less frequency into 
their parent nodes. However, the method is too 
heuristic for it relies much on manually tuned fre-
quency threshold. In our practice, we turn to use a 
theoretically well-motivated method based on the 
MDL principle. MDL is a principle of data com-
pression and statistical estimation from informa-
tion theory (Rissanen, 1978). 
Given a sample ? and a tree cut ?, we employ 
MLE to estimate the parameters of the correspond-
ing tree cut model ?? ? ??, ??? , where ??  denotes 
the estimated parameters.  
According to the MDL principle, the description 
length (Li and Abe, 1998)  ????, ?? of the tree cut 
model ??  and the sample ?? is the sum of the model 
?? 
??? ??? ??? 
??? ??? ??? ??? 
157
description length ????, the parameter description 
length ????|?? , and the data description length 
???|?, ???, i.e. 
????, ?? ? ???? ? ??????? ? ???|?, ???  (3) 
The model description length ???? is a subjec-
tive quantity which depends on the coding scheme 
employed. Here, we simply assume that each tree 
cut model is equally likely a priori. 
The parameter description length ????|?? is cal-
culated as  
??????? ? ?
?
? log |?|  (4) 
where |?|  denotes the sample size and ?  denotes 
the number of free parameters in the tree cut model, 
i.e. ? equals the number of nodes in ? minus one. 
The data description length ???|?, ???  is calcu-
lated as 
?????, ??? ? ?? ???????????   (5) 
where 
 ????? ? ?
|?|
? ????
|?|
 (6) 
where ?  is the class that ?  belongs to and ???? 
denotes the total frequency of instances in class ? 
in the sample ?. 
With the description length defined as (3), we 
wish to select a tree cut model with the minimum 
description length and output it as the result. Note 
that the model description length ???? can be ig-
nored because it is the same for all tree cut models. 
The MDL-based tree cut model was originally 
introduced for handling the problem of generaliz-
ing case frames using a thesaurus (Li and Abe, 
1998). To the best of our knowledge, no existing 
work utilizes it for question search. This may be 
partially because of the unavailability of the re-
sources (e.g., thesaurus) which can be used for 
embodying the questions in a tree structure. In Sec-
tion 2.2, we will introduce a tree structure called 
question tree for representing questions. 
2.2 Identifying question topic and question 
focus 
In principle, it is possible to identify question topic 
and question focus of a question by only parsing 
the question itself (for example, utilizing a syntac-
tic parser). However, such a method requires accu-
rate parsing results which cannot be obtained from 
the noisy data from online services. 
Instead, we propose using the MDL-based tree 
cut model which identifies question topics and 
question foci for a set of questions together. More 
specifically, the method consists of two phases: 
1) Constructing a question tree: represent the 
queried question and all the related questions 
in a tree structure called question tree; 
2) Determining a tree cut: apply the MDL prin-
ciple to the question tree, which yields the cut 
specifying question topic and question focus.  
2.2.1 Constructing a question tree 
In the following, with a series of definitions, we 
will describe how a question tree is constructed 
from a collection of questions. 
Let?s begin with explaining the representation of 
a question. A straightforward method is to 
represent a question as a bag-of-words (possibly 
ignoring stop words). However, this method cannot 
discern ?the hotels in Paris? from ?the Paris hotel?. 
Thus, we turn to use the linguistic units carrying on 
more semantic information. Specifically, we make 
use of two kinds of units: BaseNP (Base Noun 
Phrase) and WH-ngram. A BaseNP is defined as a 
simple and non-recursive noun phrase (Cao and Li, 
2002). A WH-ngram is an ngram beginning with 
WH-words. The WH-words that we consider in-
clude ?when?, ?what?, ?where?, ?which?, and ?how?.  
We refer to these two kinds of units as ?topic 
terms?. With ?topic terms?, we represent a question 
as a topic chain and a set of questions as a question 
tree.  
Definition 1 (Topic Profile) The topic profile 
?? of a topic term ? in a categorized question col-
lection is a probability distribution of categories 
????|??????  where ? is a set of categories.  
???|?? ? ???????,??
? ???????,?????
  (7) 
where ???????, ??  is the frequency of the topic 
term ?  within category ? . Clearly, we 
have?? ???|????? ? 1.  
By ?categorized questions?, we refer to the ques-
tions that are organized in a tree of taxonomy. For 
example, at Yahoo! Answers, the question ?How 
do I install my wireless router? is categorized as 
?Computers & Internet ? Computer Networking?. 
Actually, we can find categorized questions at oth-
er online services such as FAQ sites, too. 
Definition 2 (Specificity) The specificity ?????of 
a topic term ?? is the inverse of the entropy of the 
topic profile???. More specifically, 
???? ? 1 ??? ???|?? log ???|????? ? ???
  (8) 
158
where ?  is a smoothing parameter used to cope 
with the topic terms whose entropy is 0. In our ex-
periments, the value of ? was set 0.001. 
We use the term specificity to denote how spe-
cific a topic term is in characterizing information 
needs of users who post questions. A topic term of 
high specificity (e.g., Hamburg, Berlin) usually 
specifies the question topic corresponding to the 
main context of a question because it tends to oc-
cur only in a few categories. A topic term of low 
specificity is usually used to represent the question 
focus (e.g., cool club, where to see) which is rela-
tively volatile and might occur in many categories. 
Definition 3 (Topic Chain) A topic chain ?? of 
a question ? is a sequence of ordered topic terms 
?? ? ?? ? ? ? ?? such that  
1) ?? is included in 1  ,? ? ? ? ?;  
2) ????? ? ?????,  1 ? ? ? ? ? ?.  
For example, the topic chain of ?any cool clubs 
in Berlin or Hamburg?? is ?Hamburg ? Berlin ?
cool?club? because the specificities for ?Hamburg?, 
?Berlin?, and ?cool club? are 0.99, 0.62, and 0.36. 
Definition 4 (Question Tree) A question tree of 
a question set ? ? ???????
?  is a prefix tree built 
over the topic chains ?? ? ???
?????
?  of the question 
set ?. Clearly, if a question set contains only one 
question, its question tree will be exactly same as 
the topic chain of the question. 
Note that the root node of a question tree is as-
sociated with empty string as the definition of pre-
fix tree requires (Fredkin, 1960). 
 
Figure 2. An Example of a Question Tree 
 
Given the topic chains with respect to the ques-
tions in Table 1 as follow, 
? Q1: Hamburg ? Berlin ? cool?club?
? Q2: Berlin ? fun?club?
? Q3: Hamburg ? Berlin ? nice?hotel?
? Q4: Hamburg ? Berlin ? how?long?does?it?take?
? Q5: Berlin ? cheap?hotel?
we can have the question tree presented in Figure 2.  
2.2.2 Determining the tree cut 
According to the definition of a topic chain, the 
topic terms in a topic chain of a question are or-
dered by their specificity values. Thus, a cut of a 
topic chain naturally separates the topic terms of 
low specificity (representing question focus) from 
the topic terms of high specificity (representing 
question topic). Given a topic chain of a question 
consisting of ?  topic terms, there exist (? ? 1? 
possible cuts. The question is: which cut is the best?  
We propose using the MDL-based tree cut mod-
el for the search of the best cut in a topic chain. 
Instead of dealing with each topic chain individual-
ly, the proposed method handles a set of questions 
together. Specifically, given a queried question, we 
construct a question tree consisting of both the 
queried question and the related questions, and 
then apply the MDL principle to select the best cut 
of the question tree. For example, in Figure 2, we 
hope to get the cut indicated by the dashed line. 
The topic terms on the left of the dashed line 
represent the question topic and those on the right 
of the dashed line represent the question focus. 
Note that the tree cut yields a cut for each individ-
ual topic chain (each path) within the question tree 
accordingly.  
A cut of a topic chain ??? of a question q sepa-
rates the topic chain in two parts: HEAD and TAIL. 
HEAD (denoted as ?????) is the subsequence of 
the original topic chain ???  before the cut. TAIL 
(denoted as ?????) is the subsequence of ??? after 
the cut. Thus,??? ? ????? ? ?????. For instance, 
given the tree cut specified in Figure 2, for the top-
ic chain of Q1 ?Hamburg ? Berlin ? cool?club?, 
the HEAD and TAIL are ?Hamburg ? Berlin? 
and ?cool?club? respectively. 
2.3 Modeling question topic and question fo-
cus for search 
We employ the framework of language modeling 
(for information retrieval) to develop our approach 
to question search. 
In the language modeling approach to informa-
tion retrieval, the relevance of a targeted question 
?? to a queried question ? is given by the probabili-
ty ???|???  of generating the queried question ? 
Q1: Any cool clubs in Berlin or Hamburg? 
Q2: What are the most/best fun clubs in Berlin? 
Q3: Any nice hotels in Berlin or Hamburg? 
Q4: How long does it take to Hamburg from Berlin? 
Q5: Cheap hotels in Berlin? 
ROOT 
Hamburg 
Berlin 
Berlin 
cheap hotel 
fun club 
cool club
nice hotel
how long does it take
159
from the language model formed by the targeted 
question ??.  The targeted question ?? is from a col-
lection ? of questions. 
Following the framework, we propose a mixture 
model for modeling question structure (namely, 
question topic and question focus) within the 
process of searching questions: 
???|??? ? ? ? ??????|??????
????????1 ? ??? ? ??????|??????
 (9) 
In the mixture model, it is assumed that the 
process of generating question topics and the 
process of generating question foci are independent 
from each other.  
In traditional language modeling, a single multi-
nomial model ???|??? over terms is estimated for 
each targeted question ?? . In our case, two multi-
nomial models ??????????  and ??????????  need to 
be estimated for each targeted question ??. 
If unigram document language models are used, 
the equation (9) can then be re-written as, 
???|??? ? ? ? ? ??????????
???????,??
?????? ?
?1 ? ??? ? ? ??????????
???????,??
??????   
(10)
where ???????, ?? is the frequency of ? within ?. 
To avoid zero probabilities and estimate more 
accurate language models, the HEAD and TAIL of 
questions are smoothed using background collec-
tion, 
?????????? ? ? ? ???????????  
?????????????????????????1 ? ?? ? ????|??  
 
(11)
?????????? ? ? ? ???????????  
??????????????????????????1 ? ?? ? ????|??  
 
(12)
where ????|?????? , ????|?????? , and ????|??  are the 
MLE  estimators with respect to the HEAD of ??, 
the TAIL of ??, and the collection ?.  
3 Experimental Results  
We have conducted experiments to verify the ef-
fectiveness of our approach to question search. 
Particularly, we have investigated the use of identi-
fying question topic and question focus for search. 
3.1 Dataset and evaluation measures 
We made use of the questions obtained from Ya-
hoo! Answers for the evaluation. More specifically, 
we utilized the resolved questions under two of the 
top-level categories at Yahoo! Answers, namely 
?travel? and ?computers & internet?. The questions 
include 314,616 items from the ?travel? category 
and 210,785 items from the ?computers & internet? 
category. Each resolved question consists of three 
fields: ?title?, ?description?, and ?answers?. For 
search we use only the ?title? field. It is assumed 
that the titles of the questions already provide 
enough semantic information for understanding 
users? information needs. 
We developed two test sets, one for the category 
?travel? denoted as ?TRL-TST?, and the other for 
?computers & internet? denoted as ?CI-TST?. In 
order to create the test sets, we randomly selected 
200 questions for each category.  
To obtain the ground-truth of question search, 
we employed the Vector Space Model (VSM) (Sal-
ton et al, 1975) to retrieve the top 20 results and 
obtained manual judgments. The top 20 results 
don?t include the queried question itself. Given a 
returned result by VSM, an assessor is asked to 
label it with ?relevant? or ?irrelevant?. If a returned 
result is considered semantically equivalent (or 
close) to the queried question, the assessor will 
label it as ?relevant?; otherwise, the assessor will 
label it as ?irrelevant?. Two assessors were in-
volved in the manual judgments. Each of them was 
asked to label 100 questions from ?TRL-TST? and 
100 from ?CI-TST?. In the process of manually 
judging questions, the assessors were presented 
only the titles of the questions (for both the queried 
questions and the returned questions). Table 2 pro-
vides the statistics on the final test set. 
 
 # Queries # Returned # Relevant
TRL-TST 200 4,000 256 
CI-TST 200 4,000 510 
Table 2. Statistics on the Test Data 
 
We utilized two baseline methods for demon-
strating the effectiveness of our approach, the 
VSM and the LMIR (language modeling method 
for information retrieval) (Ponte and Croft, 1998).  
We made use of three measures for evaluating 
the results of question search methods. They are 
MAP, R-precision, and MRR.  
3.2 Searching questions about ?travel? 
In the experiments, we made use of the questions 
about ?travel? to test the performance of our ap-
proach to question search. More specifically, we 
used the 200 queries in the test set ?TRL-TST? to 
search for ?relevant? questions from the 314,616 
160
questions categorized as ?travel?. Note that only the 
questions occurring in the test set can be evaluated. 
We made use of the taxonomy of questions pro-
vided at Yahoo! Answers for the calculation of 
specificity of topic terms. The taxonomy is orga-
nized in a tree structure. In the following experi-
ments, we only utilized as the categories of 
questions the leaf nodes of the taxonomy tree (re-
garding ?travel?), which includes 355 categories. 
We randomly divided the test queries into five 
even subsets and conducted 5-fold cross-validation 
experiments. In each trial, we tuned the parameters 
?, ?, and ? in the equation (10)-(12) with four of 
the five subsets and then applied it to one remain-
ing subset. The experimental results reported be-
low are those averaged over the five trials. 
 
Methods MAP R-Precision  MRR 
VSM 0.198 0.138 0.228 
LMIR 0.203 0.154 0.248 
LMIR-CUT 0.236 0.192 0.279 
Table 3. Searching Questions about ?Travel? 
 
In Table 3, our approach denoted by LMIR-
CUT is implemented exactly as equation (10).  
Neither VSM nor LMIR uses the data structure 
composed of question topic and question focus.  
From Table 3, we see that our approach outper-
forms the baseline approaches VSM and LMIR in 
terms of all the measures. We conducted a signi-
ficance test (t-test) on the improvements of our 
approach over VSM and LMIR. The result indi-
cates that the improvements are statistically signif-
icant (p-value < 0.05) in terms of all the evaluation 
measures.  
 
 
Figure 3. Balancing between Question Topic and Ques-
tion Focus 
 
In equation (9), we use the parameter ? to bal-
ance the contribution of question topic and the con-
tribution of question focus. Figure 3 illustrates how 
influential the value of ? is on the performance of 
question search in terms of MRR. The result was 
obtained with the 200 queries directly, instead of 
5-fold cross-validation. From Figure 3, we see that 
our approach performs best when ? is around 0.7. 
That is, our approach tends to emphasize question 
topic more than question focus.  
We also examined the correctness of question 
topics and question foci of the 200 queried ques-
tions. The question topics and question foci were 
obtained with the MDL-based tree cut model au-
tomatically. In the result, 69 questions have incor-
rect question topics or question foci. Further 
analysis shows that the errors came from two cate-
gories: (a) 59 questions have only the HEAD parts 
(that is, none of the topic terms fall within the 
TAIL part), and (b) 10 have incorrect orders of 
topic terms because the specificities of topic terms 
were estimated inaccurately. For questions only 
having the HEAD parts, our approach (equation (9)) 
reduces to traditional language modeling approach.  
Thus, even when the errors of category (a) occur, 
our approach can still work not worse than the tra-
ditional language modeling approach. This also 
explains why our approach performs best when ? is 
around 0.7. The error category (a) pushes our mod-
el to emphasize more in question topic. 
 
Methods Results 
VSM 
1. How cold does it usually get in Charlotte, 
NC during winters? 
2. How long and cold are the winters in 
Rochester, NY? 
3. How cold is it in Alaska? 
LMIR 
1. How cold is it in Alaska? 
2. How cold does it get really in Toronto in 
the winter? 
3. How cold does the Mojave Desert get in 
the winter? 
LMIR-
CUT 
1. How cold is it in Alaska? 
2. How cold is Alaska in March and out-
door activities? 
3. How cold does it get in Nova Scotia in the 
winter? 
Table 4. Search Results for 
?How cold does it get in winters in Alaska?? 
 
Table 4 provides the TOP-3 search results which 
are given by VSM, LMIR, and LMIR-CUT (our 
approach) respectively. The questions in bold are 
labeled as ?relevant? in the evaluation set. The que-
ried question seeks for the ?weather? information 
about ?Alaska?. Both VSM and LMIR rank certain 
0.05
0.1
0.15
0.2
0.25
0.3
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
M
RR
?
161
?irrelevant? questions higher than ?relevant? ques-
tions. The ?irrelevant? questions are not about 
?Alaska? although they are about ?weather?. The 
reason is that neither VSM nor PVSM is aware that 
the query consists of the two aspects ?weather? 
(how cold, winter) and ?Alaska?.  In contrast, our 
approach assures that both aspects are matched. 
Note that the HEAD part of the topic chain of the 
queried question given by our approach is ?Alaska? 
and the TAIL part is ?winter ? how?cold?. 
3.3 Searching questions about ?computers & 
internet? 
In the experiments, we made use of the questions 
about ?computers & internet? to test the perfor-
mance of our proposed approach to question search. 
More specifically, we used the 200 queries in the 
test set ?CI-TST?? to search for ?relevant? questions 
from the 210,785 questions categorized as ?com-
puters & internet?. For the calculation of specificity 
of topic terms, we utilized as the categories of 
questions the leaf nodes of the taxonomy tree re-
garding ?computers & Internet?, which include 23 
categories.  
We conducted 5-fold cross-validation for the pa-
rameter tuning. The experimental results reported 
in Table 5 are averaged over the five trials. 
 
Methods MAP R-Precision  MRR 
VSM 0.236 0.175 0.289 
LMIR 0.248 0.191 0.304 
LMIR-CUT 0.279 0.230 0.341 
Table 5. Searching Questions about ?Computers & In-
ternet? 
 
Again, we see that our approach outperforms the 
baseline approaches VSM and LMIR in terms of 
all the measures. We conducted a significance test 
(t-test) on the improvements of our approach over 
VSM and LMIR. The result indicates that the im-
provements are statistically significant (p-value < 
0.05) in terms of all the evaluation measures.  
We also conducted the experiment similar to 
that in Figure 3. Figure 4 provides the result. The 
trend is consistent with that in Figure 3.  
We examined the correctness of (automatically 
identified) question topics and question foci of the 
200 queried questions, too. In the result, 65 ques-
tions have incorrect question topics or question 
foci. Among them, 47 fall in the error category (a) 
and 18 in the error category (b). The distribution of 
errors is also similar to that in Section 3.2, which 
also justifies the trend presented in Figure 4. 
 
 
Figure 4. Balancing between Question Topic and Ques-
tion Focus 
4 Using Translation Probability 
In the setting of question search, besides the topic 
what we address in the previous sections, another 
research topic is to fix lexical chasm between ques-
tions.  
Sometimes, two questions that have the same 
meaning use very different wording. For example, 
the questions ?where to stay in Hamburg?? and 
?the best hotel in Hamburg?? have almost the same 
meaning but are lexically different in question fo-
cus (where to stay vs. best hotel). This is the so-
called ?lexical chasm?. 
Jeon and Bruce (2007) proposed a mixture mod-
el for fixing the lexical chasm between questions. 
The model is a combination of the language mod-
eling approach (for information retrieval) and 
translation-based approach (for information re-
trieval). Our idea of modeling question structure 
for search can naturally extend to Jeon et al?s 
model. More specifically, by using translation 
probabilities, we can rewrite equation (11) and (12) 
as follow: 
?????????? ? ?? ? ???????????  
??? ? ? ????|??? ? ????????????????????  
??1 ? ?? ? ??? ? ????|??  
(13)
?????????? ? ?? ? ???????????  
??? ? ? ????|??? ? ????????????????????   
??1 ? ?? ? ??? ? ????|??  
 
(14)
where ????|???  denotes the probability that topic 
term ? is the translation of ??. In our experiments, 
to estimate the probability ????|???, we used the 
collections of question titles and question descrip-
tions as the parallel corpus and the IBM model 1 
(Brown et al, 1993) as the alignment model. 
0.15
0.2
0.25
0.3
0.35
0.4
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
M
RR
?
162
Usually, users reiterate or paraphrase their ques-
tions (already described in question titles) in ques-
tion descriptions. 
We utilized the new model elaborated by equa-
tion (13) and (14) for searching questions about 
?travel? and ?computers & internet?. The new mod-
el is denoted as ?SMT-CUT?. Table 6 provides the 
evaluation results. The evaluation was conducted 
with exactly the same setting as in Section 3. From 
Table 6, we see that the performance of our ap-
proach can be further boosted by using translation 
probability.  
 
Data Methods MAP R-Precision MRR
TRL-
TST 
LMIR-CUT 0.236 0.192 0.279
SMT-CUT 0.266 0.225 0.308
CI-
TST 
LMIR-CUT 0.279 0.230 0.341
SMT-CUT 0.282 0.236 0.337
Table 6. Using Translation Probability 
5 Related Work 
The major focus of previous research efforts on 
question search is to tackle the lexical chasm prob-
lem between questions.  
The research of question search is first con-
ducted using FAQ data. FAQ Finder (Burke et al, 
1997) heuristically combines statistical similarities 
and semantic similarities between questions to rank 
FAQs. Conventional vector space models are used 
to calculate the statistical similarity and WordNet 
(Fellbaum, 1998) is used to estimate the semantic 
similarity. Sneiders (2002) proposed template 
based FAQ retrieval systems. Lai et al (2002) pro-
posed an approach to automatically mine FAQs 
from the Web. Jijkoun and Rijke (2005) used su-
pervised learning methods to extend heuristic ex-
traction of Q/A pairs from FAQ pages, and treated 
Q/A pair retrieval as a fielded search task.  
Harabagiu et al (2005) used a Question Answer 
Database (known as QUAB) to support interactive 
question answering. They compared seven differ-
ent similarity metrics for selecting related ques-
tions from QUAB and found that the concept-
based metric performed best. 
Recently, the research of question search has 
been further extended to the community-based 
Q&A data. For example, Jeon et al (Jeon et al, 
2005a; Jeon et al, 2005b) compared four different 
retrieval methods, i.e. vector space model, Okapi, 
language model (LM), and translation-based model, 
for automatically fixing the lexical chasm between 
questions of question search. They found that the 
translation-based model performed best. 
However, all the existing methods treat ques-
tions just as plain texts (without considering ques-
tion structure). In this paper, we proposed to 
conduct question search by identifying question 
topic and question focus. To the best of our know-
ledge, none of the existing studies addressed ques-
tion search by modeling both question topic and 
question focus. 
Question answering (e.g., Pasca and Harabagiu, 
2001; Echihabi and Marcu, 2003; Voorhees, 2004; 
Metzler and Croft, 2005) relates to question search. 
Question answering automatically extracts short 
answers for a relatively limited class of question 
types from document collections. In contrast to that, 
question search retrieves answers for an unlimited 
range of questions by focusing on finding semanti-
cally similar questions in an archive. 
6 Conclusions and Future Work 
In this paper, we have proposed an approach to 
question search which models question topic and 
question focus in a language modeling framework. 
The contribution of this paper can be summa-
rized in 4-fold: (1) A data structure consisting of 
question topic and question focus was proposed for 
summarizing questions; (2) The MDL-based tree 
cut model was employed to identify question topic 
and question focus automatically; (3) A new form 
of language modeling using question topic and 
question focus was developed for question search; 
(4) Extensive experiments have been conducted to 
evaluate the proposed approach using a large col-
lection of real questions obtained from Yahoo! An-
swers.  
Though we only utilize data from community-
based question answering service in our experi-
ments, we could also use categorized questions 
from forum sites and FAQ sites. Thus, as future 
work, we will try to investigate the use of the pro-
posed approach for other kinds of web services.  
Acknowledgement 
We would like to thank Xinying Song, Shasha Li, 
and Shilin Ding for their efforts on developing the 
evaluation data. We would also like to thank Ste-
phan H. Stiller for his proof-reading of the paper. 
 
163
References  
A. Echihabi and D. Marcu. 2003. A Noisy-Channel Ap-
proach to Question Answering. In Proc. of ACL?03. 
C. Fellbaum. 1998. WordNet: An electronic lexical da-
tabase. MIT Press. 
D. Metzler and W. B. Croft. 2005. Analysis of statistical 
question classification for fact-based questions. In-
formation Retrieval, 8(3), pages 481-504. 
E. Fredkin. 1960. Trie memory. Communications of the 
ACM, D. 3(9):490-499. 
E. M. Voorhees. 2004. Overview of the TREC 2004 
question answering track. In Proc. of TREC?04. 
E. Sneiders. 2002. Automated question answering using 
question templates that cover the conceptual model 
of the database. In Proc. of the 6th International 
Conference on Applications of Natural Language to 
Information Systems, pages 235-239. 
G. Salton, A. Wong, and C. S. Yang 1975. A vector 
space model for automatic indexing. Communica-
tions of the ACM, vol. 18, nr. 11, pages 613-620.  
H.  Li and N. Abe. 1998. Generalizing case frames us-
ing a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2), pages 217-244. 
J. Jeon and W.B. Croft. 2007. Learning translation-
based language models using Q&A archives. Tech-
nical report, University of Massachusetts. 
J. Jeon, W. B. Croft, and J. Lee. 2005a. Finding seman-
tically similar questions based on their answers. In 
Proc. of SIGIR?05. 
J. Jeon, W. B. Croft, and J. Lee. 2005b. Finding similar 
questions in large question and answer archives. In 
Proc. of CIKM ?05, pages 84-90. 
J. Rissanen. 1978. Modeling by shortest data description. 
Automatica, vol. 14,  pages. 465-471 
J.M. Ponte, W.B. Croft. 1998. A language modeling 
approach to information retrieval. In Proc. of 
SIGIR?98. 
M. A. Pasca and S. M. Harabagiu. 2001. High perfor-
mance question/answering. In Proc. of SIGIR?01, 
pages 366-374. 
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L. 
Mercer. 1993. The mathematics of statistical machine 
translation: parameter estimation. Computational 
Linguistics, 19(2):263-311. 
R. D. Burke, K. J. Hammond, V. A. Kulyukin, S. L. 
Lytinen, N. Tomuro, and S. Schoenberg. 1997. Ques-
tion answering from frequently asked question files: 
Experiences with the FAQ finder system. Technical 
report, University of Chicago. 
S. Harabagiu, A. Hickl, J. Lehmann and D. Moldovan. 
2005. Experiments with Interactive Question-
Answering. In Proc. of ACL?05. 
V. Jijkoun, M. D. Rijke. 2005. Retrieving Answers from 
Frequently Asked Questions Pages on the Web. In 
Proc. of CIKM?05. 
Y. Cao and H. Li. 2002. Base noun phrase translation 
using web data and the EM algorithm. In Proc. of 
COLING?02. 
Y.-S. Lai, K.-A. Fung, and C.-H. Wu. 2002. Faq mining 
via list detection. In Proc. of the Workshop on Multi-
lingual Summarization and Question Answering, 
2002. 
 
164
Proceedings of ACL-08: HLT, pages 914?922,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Probabilistic Model for Fine-Grained Expert Search   Shenghua Bao1, Huizhong Duan1, Qi Zhou1, Miao Xiong1, Yunbo Cao1,2, Yong Yu1 1Shanghai Jiao Tong University,  2Microsoft Research Asia Shanghai, China, 200240 Beijing, China, 100080 {shhbao,summer,jackson,xiongmiao,yyu} @apex.sjtu.edu.cn yunbo.cao@microsoft.com    Abstract 
Expert search, in which given a query a ranked list of experts instead of documents is returned, has been intensively studied recently due to its importance in facilitating the needs of both information access and knowledge discovery. Many approaches have been pro-posed, including metadata extraction, expert profile building, and formal model generation. However, all of them conduct expert search with a coarse-grained approach. With these, further improvements on expert search are hard to achieve. In this paper, we propose conducting expert search with a fine-grained approach. Specifically, we utilize more spe-cific evidences existing in the documents. An evidence-oriented probabilistic model for ex-pert search and a method for the implementa-tion are proposed. Experimental results show that the proposed model and the implementa-tion are highly effective. 
1 Introduction Nowadays, team work plays a more important role than ever in problem solving. For instance, within an enterprise, people handle new problems usually by leveraging the knowledge of experienced col-leagues. Similarly, within research communities, novices step into a new research area often by learning from well-established researchers in the research area. All these scenarios involve asking the questions like ?who is an expert on X?? or ?who knows about X?? Such questions, which cannot be answered easily through traditional document search, raise a new requirement of searching people with certain expertise.  To meet that requirement, a new task, called ex-pert search, has been proposed and studied inten-sively. For example, TREC 2005, 2006, and 2007 
provide the task of expert search within the enter-prise track. In the TREC setting, expert search is defined as: given a query, a ranked list of experts is returned. In this paper, we engage our study in the same setting.  Many approaches to expert search have been proposed by the participants of TREC and other researchers. These approaches include metadata extraction (Cao et al, 2005), expert profile build-ing (Craswell, 2001, Fu et al, 2007), data fusion (Maconald and Ounis, 2006), query expansion (Macdonald and Ounis, 2007), hierarchical lan-guage model (Petkova and Croft, 2006), and for-mal model generation (Balog et al, 2006; Fang et al, 2006). However, all of them conduct expert search with what we call a coarse-grained ap-proach. The discovering and use of evidence for expert locating is carried out under a grain of document. With it, further improvements on expert search are hard to achieve. This is because differ-ent blocks (or segments) of electronic documents usually present different functions and qualities and thus different impacts for expert locating.  In contrast, this paper is concerned with propos-ing a probabilistic model for fine-grained expert search. In fine-grained expert search, we are to extract and use evidence of expert search (usually blocks of documents) directly. Thus, the proposed probabilistic model incorporates evidence of expert search explicitly as a part of it. A piece of fine-grained evidence is formally defined as a quadru-ple, <topic, person, relation, document>, which denotes the fact that a topic and a person, with a certain relation between them, are found in a spe-cific document. The intuition behind the quadruple is that a query may be matched with phrases in various forms (denoted as topic here) and an expert candidate may appear with various name masks (denoted as person here), e.g., full name, email, or abbreviated names. Given a topic and person, rela-tion type is used to measure their closeness and 
914
document serves as a context indicating whether it is good evidence. Our proposed model for fine-grained expert search results in an implementation of two stages.  1) Evidence Extraction: document segments in various granularities are identified and evidences are extracted from them. For example, we can have segments in which an expert candidate and a que-ried topic co-occur within a same section of docu-ment-001:  ??later, Berners-Lee describes a semantic web search engine experience?? As the result, we can extract an evidence by using same-section relation, i.e., <semantic web search engine, Berners-Lee, same-section, document-001>.   2) Evidence Quality Evaluation: the quality (or reliability) of evidence is evaluated. The quality of a quadruple of evidence consists of four aspects, namely topic-matching quality, person-name-matching quality, relation quality, and document quality. If we regard evidence as link of expert candidate and queried topic, the four aspects will correspond to the strength of the link to query, the strength of the link to expert candidate, the type of the link, and the document context of the link re-spectively. All the evidences with their scores of quality are merged together to generate a single score for each expert candidate with regard to a given query. We empirically evaluate our proposed model and im-plementation on the W3C corpus which is used in the expert search task at TREC 2005 and 2006. Experimental results show that both explored evi-dences and evaluation of evidence quality can im-prove the expert search significantly. Compared with existing state-of-the-art expert search methods, the probabilistic model for fine-grained expert search shows promising improvement.  The rest of the paper is organized as follows. Section 2 surveys existing studies on expert search. Section 3 and Section 4 present the proposed prob-abilistic model and its implementation, respec-tively. Section 5 gives the empirical evaluation. Finally, Section 6 concludes the work. 2 Related Work  
2.1 Expert Search Systems One setting for automatic expert search is to as-sume that data from specific resources are avail-able. For example, Expertise Recommender (Kautz 
et al, 1996), Expertise Browser (Mockus and Herbsleb, 2002) and the system in (McDonald and Ackerman, 1998) make use of log data in software development systems to find experts. Yet another approach is to mine expert and expertise from email communications (Campbell et al, 2003; Dom et al 2003; Sihn and Heeren, 2001). Searching expert from general documents has also been studied (Davenport and Prusak, 1998; Mattox et al, 1999; Hertzum and Pejtersen, 2000). P@NOPTIC employs what is referred to as the ?profile-based? approach in searching for experts (Craswell et al, 2001). Expert/Expert-Locating (EEL) system (Steer and Lochbaum, 1988) uses the same approach in searching for expert groups. DEMOIR (Yimam, 1996) enhances the profile-based approach by separating co-occurrences into different types. In essence, the profile-based ap-proach utilizes the co-occurrences between query words and people within documents. 2.2 Expert Search at TREC A task on expert search was organized within the enterprise track at TREC 2005, 2006 and 2007 (Craswell et al, 2005; Soboroff  et al, 2006; Bai-ley et al, 2007).  Many approaches have been proposed for tack-ling the expert search task within the TREC track. Cao et al (2005) propose a two-stage model with a set of extracted metadata. Balog et al (2006) com-pare two generative models for expert search. Fang et al (2006) further extend their generative model by introducing the prior of expert distribution and relevance feedback. Petkova and Croft (2006) fur-ther extend the profile based method by using a hierarchical language model. Macdonald and Ounis (2006) investigate the effectiveness of the voting approach and the associated data fusion techniques. However, such models are conducted in a coarse-grain scope of document as discussed before. In contrast, our study focuses on proposing a model for conducting expert search in a fine-grain scope of evidence (local context). 3 Fine-grained Expert Search Our research is to investigate a direct use of the local contexts for expert search. We call each local context of such kind as fine-grained evidence.  In this work, a fine-grained evidence is formally defined as a quadruple, <topic, person, relation, 
915
document>. Such a quadruple denotes that a topic and a person occurrence, with a certain relation between them, are found in a specific document.  Recall that topic is different from query. For ex-ample, given a query ?semantic web coordination?, the corresponding topic may be either ?semantic web? or ?web coordination?. Similarly, person here is different from expert candidate. E.g, given an expert candidate ?Ritu Raj Tiwari?, the matched person may be ?Ritu Raj Tiwari?, ?Tiwari?, or ?RRT? etc. Although both the topics and persons may not match the query and expert candidate ex-actly, they do have certain indication on the con-nection of query ?semantic web coordination? and expert ?Ritu Raj Tiwari?. 3.1 Evidence-Oriented Expert Search Model We conduct fine-grained expert search by incorpo-rating evidence of local context explicitly in a probabilistic model which we call an evidence-oriented expert search model. Given a query q, the probability of a candidate c being an expert (or knowing something about q) is estimated as 
( | ) ( , | )
( | , ) ( | )
e
e
P c q P c e q
P c e q P e q
=
=
?
?
, (1) 
where e denotes a quadruple of evidence.  Using the relaxation that the probability of c is independent of a query q given an evidence e, we can reduce Equation (1) as, 
( | ) ( | ) ( | )
e
P c q P c e P e q=
?
. (2) Compared to previous work, our model conducts expert search with a new way in which local con-texts of evidence are used to bridge a query q and an expert candidate c. The new way enables the expert search system to explore various local con-texts in a precise manner.  In the following sub-sections, we will detail two sub-models: the expert matching model P(c|e) and the evidence matching model P(e|q).  3.2 Expert Matching Model We expand the evidence e as quadruple <topic, people, relation, document> (<t, p, r, d> for short) for expert matching. Given a set of related evi-dences, we assume that the generation of an expert candidate c is independent with topic t and omit it 
in expert matching. Therefore, we simplify the ex-pert matching formula as below:  
),|()|(),,|()|( drpPpcPdrpcPecP ==
, (3) where P(c|p) depends on how an expert candidate c matches to a person occurrence p (e.g. full name or email of a person). The different ways of matching an expert candidate c with a person occurrence p results in varied qualities. P(c|p) represents the quality. P(p|r,d) expresses the probability of an occurrence p given a relation r and a document d. P(p|r,d) is estimated in MLE as, 
),(
),,(
),|(
drL
drpfreq
drpP = , (4) 
where freq(p,r,d) is the frequency of person p matched by relation r in document d, and L(r, d) is the frequency of all the persons matched by rela-tion r in d. This estimation can further be smoothed by using the evidence collection as follows:  
?
?
?+=
Dd
S
D
drpP
drpPdrpP
'
||
)',|(
)1(),|(),|( ?? , (5) 
where D denotes the whole document collection. |D| is the total number of documents.  We use Dirichlet prior in smoothing of parame-ter ?:  
KdrL
drL
+
=
),(
),(? , (6) 
where K is the average frequency of all the experts in the collection.  
3.3 Evidence Matching Model By expanding the evidence e and employing inde-pendence assumption, we have the following for-mula for evidence matching: 
)|()|()|()|(
)|,,,()|(
qdPqrPqpPqtP
qdrptPqeP
=
= . (7) 
In the following, we are to explain what these four terms represent and how they can be estimated. The first term P(t|q) represents the probability that a query q matches to a topic t in evidence. Re-call that a query q may match a topic t in various ways, not necessarily being identical to t. For ex-ample, both topic ?semantic web? and ?semantic web search engine? can match the query ?semantic web search engine?. The probability is defined as 
916
( )),()|( qttypePqtP ? , (8) where type(t, q) represents the way that q matches to t, e.g., phrase matching. Different matching methods are associated with different probabilities. The second term P(p|q) represents the probabil-ity that a person p is generated from a query q. The probability is further approximated by the prior probability of p, 
)()|( pPqpP ? . (9) The prior probability can be estimated by MLE, i.e., the ratio of total occurrences of person p in the collection. The third term represents the probability that a relation r is generated from a query q. Here, we approximate the probability as  
))(()|( rtypePqrP ?
, (10) where type(r) represents the way r connecting query and expert. P(type(r)) represents the reliabil-ity of relation type of r. Following the Bayes rule, the last term can be transformed as 
)()|(
)(
)()|(
)|( dPdqP
qP
dPdqP
qdP ?= , (11) 
where priority distribution P(d) can be estimated based on static rank, e.g., PageRank (Brin and Page, 1998). P(q|d) can be estimated by using a standard language model for IR (Ponte and Croft, 1998). In summary, Equation (7) is converted to 
( ) )()|())(()(),()|( dPdqPrtypePpPqttypePqeP ?
. (12) 
3.4 Evidence Merging  We assume that the ranking score of an expert can be acquired by summing up together all scores of the supporting evidences. Thus we calculate ex-perts? scores by aggregating the scores from all evidences as in Equation (1). 4 Implementation  The implementation of the proposed model con-sists of two stages, namely evidence extraction and evidence quality evaluation.  
4.1 Evidence Extraction Recall that we define an evidence for expert search as a quadruple <topic, person, relation, document>. The evidence extraction covers the extraction of the first three elements, namely person identifica-tion, topic discovering and relation extraction. 4.1.1 Person Identification  The occurrences of an expert can be in various forms, such as name and email address. We call each type of form an expert mask. Table 1 provides a statistic on various masks on the basis of W3C corpus. In Table 1, rate is the proportion of the person occurrences with relevant masks to the per-son occurrences with any of the masks, and ambi-guity is defined as the probability that a mask is shared by more than one expert.   Mask Rate/Ambiguity Sample Full Name(NF) 48.2% / 0.0000 Ritu Raj Tiwari  Email Name(NE) 20.1% / 0.0000 rtiwari@nuance.com Combined Name (NC) 4.2% /0.3992 Tiwari, Ritu R;            R R Tiwari Abbr. Name(NA) 21.2% / 0.4890 Ritu Raj ; Ritu Short Name(NS) 0.7% / 0.6396 RRT Alias, new email (NAE)  7% / 0.4600 Ritiwari rti-wari@hotmail.com Table 1. Various masks and their ambiguity 
1) Every occurrence of a candidate?s email address is normalized to the appropriate candidate_id. 2) Every occurrence of a candidate?s full_name is normalized to the appropriate candidate_id if there is no ambiguity; otherwise, the occurrence is normalized to the candidate_id of the most frequent candidate with that full_name. 3) Every occurrence of combined name, abbrevi-ated name, and email alias is normalized to the appropriate candidate_id if there is no ambigu-ity; otherwise, the occurrence may be normal-ized to the candidate_id of a candidate whose full name also appears in the document. 4) All the personal occurrences other than those covered by Heuristic 1) ~ 3) are ignored. Table 2. Heuristic rules for expert extraction As Table 1 demonstrates, it is not an easy task to identify all the masks with regards to an expert. On one hand, the extraction of full name and email address is straightforward but suffers from low coverage. On the other hand, the extraction of 
917
combined name and abbreviated name can com-plement the coverage, while needs handling of am-biguity.  Table 2 provides the heuristic rules that we use for expert identification. In the step 2) and 3), the rules use frequency and context discourse for re-solving ambiguities respectively. With frequency, each expert candidate actually is assigned a prior probability. With context discourse, we utilize the intuition that person names appearing similar in a document usually refers to the same person. 4.1.2 Topic Discovering A queried topic can occur within documents in various forms, too. We use a set of query process-ing techniques to handle the issue. After the proc-essing, a set of topics transformed from an original query will be obtained and then be used in the search for experts. Table 3 shows five forms of topic discovering from a given query.   Forms Description Sample Phrase Match(QP) The exact match with origi-nal query given by users ?semantic web search engine? Bi-gram Match(QB) A set of matches formed by extracting bi-gram of words in the original query ?semantic web? ?search en-gine? Proximity Match(QPR) Each query term appears as a neighborhood within a window of specified size ?semantic web enhanced search engine? Fuzzy Match(QF) A set of matches, each of which resembles the origi-nal query in appearance. ?sementic web seerch engine? Stemmed Match(QS) A match formed by stem-ming the original query. ?sementic web seerch engin? Table 3. Discovered topics from query ?semantic web search engine? 
4.1.3 Relation Extraction We focus on extracting relations between topics and expert candidates within a span of a document. To make the extraction easier, we partition a document into a pre-defined layout. Figure 1 pro-vides a template in Backus?Naur form. Figure 2 provides a practical use of the template. Note that we are not restricting the use of the template only for certain corpus. Actually the tem-plate can be applied to many kinds of documents. For example, for web pages, we can construct the <Title> from either the ?title? metadata or the con-
tent of web pages (Hu et al, 2006). As for e-mail, we can use the ?subject? field as the <Title>. 
 Figure. 1. A template of document layout 
...
...
...
RDF Primer
Editors: Frank Manola, fmanola@acm.org
   Eric Miller, W3C, em@w3.org
2. Making Statements About Resources
RDF is intended to provide a simple way to make state
These capabilities (the normative specification describe)
2.1 Basic Concepts
Imagine trying to state that someone named John Smith
The form of a simple statement such as:
<Title>
<Author>
<Body>
<Section Title>
<Section>
<Section Body>
 Figure 2. An example use of the layout template  With the layout of partitioned documents, we can then explore many types of relations among different blocks. In this paper, we demonstrate the use of five types of relations by extending the study in (Cao et al, 2005).  Section Relation (RS): The queried topic and the expert candidate occur in the same <Section>. Windowed Section Relation (RWS): The que-ried topic and the expert candidate occur within a fixed window of a <Section>. In our experiment, we used a window of 200 words. Reference Section Relation (RRS): Some <Sec-tion>s should be treated specially. For example, the <Section> consisting of reference information like a list of <book, author> can serve as a reliable source connecting a topic and an expert candidate. We call the relation appearing in a special type of <Section> a special reference section relation. It might be argued whether the use of special sections can be generalized. According to our survey, the special <Section>s can be found in various sites such as Wikipedia as well as W3C. Title-Author Relation (RTA): The queried topic appears in the <Title> and the expert candidate appears in the <Author>.  
918
Section Title-Body Relation (RSTB): The que-ried topic and the expert candidate appear in the <Section Title> and <Section Body> of the same <Section>, respectively. Reversely, the queried topic and the expert candidate can appear in the <Section Body> and <Section Title> of a <Section>. The latter case is used to characterize the docu-ments introducing certain expert or the expert in-troducing certain document. Note that our model is not restricted to use these five relations. We use them only for the aim of demonstrating the flexibility and effectiveness of fine-grained expert search. 4.2 Evidence Quality Evaluation In this section, we elaborate the mechanism used for evaluating the quality of evidence.  4.2.1 Topic-Matching Quality In Section 4.1.2, we use five techniques in process-ing query matches, which yield five sets of match types for a given query. Obviously, the different query matches should be associated with different weights because they represent different qualities.  We further note that different bi-grams gener-ated from the same query with the bi-gram match-ing method might also present different qualities. For example, both topic ?css test? and ?test suite? are the bi-gram matching for query ?css test suite?; however, the former might be more informative. To model that, we use the number of returned documents to refine the query weight. The intuition behind that is similar to the thought of IDF popu-larly used in IR as we prefer to the distinctive bi-grams. Taking into consideration the above two factors, we calculate the topic-matching quality Qt (corre-sponding to P(type(t,q)) in Equation (12) ) for the given query q as 
t
tt
t
df
dfMIN
qttypeWQ
)(
)),((
''
= , (13) 
where t means the discovered topic from a docu-ment and type(t,q) is the matching type between topic t and query q. W(type(t,q)) is the weight for a certain query type, dft is the number of returned documents matched by topic t. In our experiment, we use the 10 training topics of TREC2005 as our training data, and the best quality scores for phrase match, bi-gram match, proximity match, fuzzy 
match, and stemmed match are 1, 0.01, 0.05, 10-8, and 10-4, respectively.  4.2.2 Person-Matching Quality An expert candidate can occur in the documents in various ways. The most confident occurrence should be the ones in full name or email address. Others can include last name only, last name plus initial of first name, etc. Thus, the action of reject-ing or accepting a person from his/her mask (the surface expression of a person in the text) is not simply a Boolean decision, but a probabilistic one with a reliability weight Qp (corresponding to P(c|p) in Equation (3) ). Similarly, the best trained weights for full name, email name, combined name, abbreviated name, short name, and alias email are set to 1, 1, 0.8, 0.2, 0.2, and 0.1, respectively. 4.2.3 Relation Type Quality The relation quality consists of two factors. One factor is about the type of the relation. Different types of relations indicate different strength of the connection between expert candidates and queried topics. In our system, the section title-body rela-tion is given the highest confidence. The other fac-tor is about the degree of proximity between a query and an expert candidate. The intuition is that, the more distant are a query and an expert candi-date within a relation, the looser the connection between them is. To include these two factors, the quality score Qr (corresponding to P(type(r)) in Equation (12) )of a relation r is defined as:  
1),( +
=
tpdis
C
WQ
r
rr
, (14) 
where Wr is the weight of relation type r, dis(p, t) is the distance from the person occurrence p to the queried topic t and Cr is a constant for normaliza-tion. Again, we optimize the Wr based on the train-ing topics, the best weights for section relation, windowed section relation, reference section rela-tion, title-author relation, and section title-body relation  are 1, 4, 10, 45, and 1000 respectively.  4.2.4 Document Quality The quality of evidence also depends on the quality of the document, the context in which it is found. The document context can affect the credibility of the evidence in two ways:  
919
Static quality: indicating the authority of a document. In our experiment, the static quality Qd (corresponding to P(d) in Equation (12) ) is esti-mated by the PageRank, which is calculated using a standard iterative algorithm with a damping fac-tor of 0.85 (Brin and Page, 1998).  Dynamic quality: by ?dynamic?, we mean the quality score varies for different queries q. We de-note the dynamic quality as QDY(d,q) (correspond-ing to P(q|d) in Equation (12) ), which is actually the document relevance score returned by a stan-dard language model for IR(Ponte and Croft, 1998). 5 Experimental Results 
5.1  The Evaluation Data In our experiment, we used the data set in the ex-pert search task of enterprise search track at TREC 2005 and 2006. The document collection is a crawl of the public W3C sites in June 2004. The crawl comprises in total 331,307 web pages. In the fol-lowing experiments, we used the training set of 10 topics of TREC 2005 for tuning the parameters aforementioned in Section 4.2, and used the test set of 50 topics of TREC 2005 and 49 topics of TREC 2006 as the evaluation data sets.  5.2 Evaluation Metrics We used three measures in evaluation: Mean aver-age precision (MAP), R-precision (R-P), and Top N precision (P@N). They are also the standard measures used in the expert search task of TREC. 5.3 Evidence Extraction In the following experiments, we constructed the baseline by using the query matching methods of phrase matching, the expert matching methods of full name matching and email matching, and the relation of section relation. To show the contribu-tion of each individual method for evidence extrac-tion, we incrementally add the methods to the baseline method. In the following description, we will use ?+? to denote applying new method on the previous setting. 5.3.1 Query Matching Table 4 shows the results of expert search achieved by applying different methods of query matching. 
QB, QPR, QF, and QS denote bi-gram match, prox-imity match, fuzzy match, and stemmed match, respectively. The performance of the proposed model increases stably on MAP when new query matches are added incrementally. We also find that the introduction of QF and QS bring some drop on R-Precision and P@10. It is reasonable because both QF and QS bring high recall while affect the precision a bit. The overall relative improvement of using query matching compared to the baseline is presented in the row ?Improv.?. We performed t-tests on MAP. The p-values (< 0.05) are presented in the ?T-test? row, which shows that the im-provement is statistically significant.   TREC 2005 TREC 2006  MAP R-P P@10 MAP R-P P@10 Baseline 0.1840 0.2136 0.3060 0.3752 0.4585 0.5604 +QB 0.1957 0.2438 0.3320 0.4140 0.4910 0.5799 +QPR  0.2024 0.2501 0.3360 0.4530 0.5137 0.5922 +QF ,QS 0.2030 0.2501 0.3360 0.4580 0.5112 0.5901 Improv. 10.33% 17.09% 9.80% 22.07% 11.49% 5.30% T-test 0.0084   0.0000   Table 4. The effects of query matching 
5.3.2 Person Matching For person matching, we considered four types of masks, namely combined name (NC), abbreviated name (NA), short name (NS) and alias and new email (NAE). Table 5 provides the results on person matching at TREC 2005 and 2006. The baseline is the best model achieved in previous section. It seems that there is little improvement on P@10 while an improvement of 6.21% and 14.00% is observed on MAP. This might be due to the fact that the matching method such as NC has a higher recall but lower precision.    TREC 2005 TREC 2006  MAP R-P P@10 MAP R-P P@10 Baseline 0.2030 0.2501 0.3360 0.4580 0.5112 0.5901 +NC 0.2056 0.2539 0.3463 0.4709 0.5152 0.5931 +NA  0.2106 0.2545 0.3400 0.5010 0.5181 0.6000 +NS  0.2111 0.2578 0.3400 0.5121 0.5192 0.6000 +NAE  0.2156 0.2591 0.3400 0.5221 0.5212 0.6000 Improv. 6.21% 3.60% 1.19% 14.00% 1.96% 1.68% T-test 0.0064   0.0057   Table 5. The effects of person matching 
920
5.3.3 Multiple Relations For relation extraction, we experimentally demon-strated the use of each of the five relations pro-posed in Section 4.1.3, i.e., section relation (RS), windowed section relation (RWS), reference section relation (RRS), title-author relation (RTA), and sec-tion title-body relation (RSTB). We used the best model achieved in previous section as the baseline. From Table 6, we can see that the section title-body relation contributes the most to the improve-ment of the performance. By using all the discov-ered relations, a significant improvement of 19.94% and 8.35% is achieved.    TREC 2005 TREC 2006  MAP R-P P@10 MAP R-P P@10 Baseline 0.2156 0.2591 0.3400 0.5221 0.5212 0.6000 +RWS 0.2158 0.2633 0.3380 0.5255 0.5311 0.6082 +RRS 0.2160 0.2630 0.3380 0.5272 0.5314 0.6061 +RTA 0.2234 0.2634 0.3580 0.5354 0.5355 0.6245 +RSTB 0.2586 0.3107 0.3740 0.5657 0.5669 0.6510 Improv. 19.94% 19.91% 10.00% 8.35% 8.77% 8.50% T-test 0.0013   0.0043   Table 6. The effects of relation extraction 
5.4 Evidence Quality  The performance of expert search can be further improved by considering the evidence quality. Ta-ble 7 shows the results by considering the differ-ences in quality.  We evaluated two kinds of evidence quality: context static quality (Qd) and context dynamic quality (QDY). Each of the evidence quality con-tributes about 1%-2% improvement for MAP. The improvement from the PageRank that we calcu-lated from the corpus implies that the web scaled rank technique is also effective in the corpus of documents. Finally, we find a significant relative improvement of 6.13% and 2.86% on MAP by us-ing evidence qualities.    TREC 2005 TREC 2006  MAP R-P P@10 MAP R-P P@10 Baseline 0.2586 0.3107 0.3740 0.5657 0.5669 0.6510 +Qd 0.2711 0.3188 0.3720 0.5900 0.5813 0.6796 +QDY 0.2755 0.3252 0.3880 0.5943 0.5877 0.7061 Improv. 6.13% 4.67% 3.74% 2.86% 3.67% 8.61% T-test 0.0360   0.0252   Table 7. The effects of using evidence quality  
5.5 Comparison with Other Systems In Table 8, we juxtapose the results of our prob-abilistic model for fine-grained expert search with automatic expert search systems from the TREC evaluation. The performance of our proposed model is rather encouraging, which achieved com-parable results to the best automatic systems on the TREC 2005 and 2006.    MAP R-prec Prec@10 TREC2005 0.2749 0.3330 0.4520 Rank-1 System TREC20061 0.5947 0.5783 0.7041 TREC2005 0.2755 0.3252 0.3880 Our System TREC2006 0.5943 0.5877 0.7061 Table 8. Comparison with other systems 
6 Conclusions This paper proposed to conduct expert search using a fine-grained level of evidence. Specifically, quadruple evidence was formally defined and served as the basis of the proposed model. Differ-ent implementations of evidence extraction and evidence quality evaluation were also comprehen-sively studied. The main contributions are:  1. The proposal of fine-grained expert search, which we believe to be a promising direc-tion for exploring subtle aspects of evidence.  2. The proposal of probabilistic model for fine-grained expert search. The model facilitates investigating the subtle aspects of evidence.  3. The extensive evaluation of the proposed probabilistic model and its implementation on the TREC data set. The evaluation shows promising expert search results.   In future, we are to explore more domain inde-pendent evidences and evaluate the proposed model on the basis of the data from other domains. Acknowledgments The authors would like to thank the three anony-mous reviewers for their elaborate and helpful comments. The authors also appreciate the valu-able suggestions of Hang Li, Nick Craswell, Yangbo Zhu and Linyun Fu.                                                            1 This system, where cluster-based re-ranking is used, is a variation of the fine-grained model proposed in this paper. 
921
References  Bailey, P.,  Soboroff , I., Craswell, N., and Vries A.P., Overview of the TREC 2007 Enterprise Track. In: Proc. of TREC 2007.  Balog, K., Azzopardi, L., and Rijke, M. D., 2006. Formal models for expert finding in enterprise cor-pora. In: Proc. of SIGIR?06,pp.43-50. Brin, S. and Page, L., 1998. The anatomy of a rlarge-scale hypertextual Web search engine, Computer Networks and ISDN Systems (30), pp.107-117. Campbell, C.S., Maglio, P., Cozzi, A. and Dom, B., 2003. Expertise identification using email communi-cations. In: Proc. of CIKM ?03 pp.528?531.  Cao, Y., Liu, J., and Bao, S., and Li, H., 2005. Research on expert search at enterprise track of TREC 2005. In: Proc. of TREC 2005. Craswell, N., Hawking, D., Vercoustre, A. M. and Wil-kins, P., 2001. P@NOPTIC Expert: searching for ex-perts not just for documents. In: Proc. of Ausweb?01. Craswell, N., Vries, A.P., and Soboroff, I., 2005. Over-view of the TREC 2005 Enterprise Track. In: Proc. of TREC 2005. Davenport, T. H. and Prusak, L., 1998. Working Knowledge: how organizations manage what they know. Howard Business, School Press, Boston, MA. Dom, B., Eiron, I., Cozzi A. and Yi, Z., 2003. Graph-based ranking algorithms for e-mail expertise analy-sis, In: Proc. of SIGMOD?03 workshop on Research issues in data mining and knowledge discovery. Fang, H., Zhou, L., Zhai, C., 2006. Language models for expert finding-UIUC TREC 2006 Enterprise Track Experiments, In: Proc. of TREC2006. Fu, Y., Xiang, R., Liu, Y., Zhang, M., Ma, S., 2007. A CDD-based Formal Model for Expert Finding. In Proc. of CIKM 2007. Hertzum, M. and Pejtersen, A. M., 2000. The informa-tion-seeking practices of engineers: searching for documents as well as for people. Information Proc-essing and Management, 36(5), pp.761?778. Hu, Y., Li, H., Cao, Y., Meyerzon, D. Teng, L., and Zheng, Q., 2006. Automatic extraction of titles from general documents using machine learning, IPM. Kautz, H., Selman, B. and Milewski, A., 1996. Agent amplified communication. In: Proc. of AAAI?96, pp. 3?9. Mattox, D., Maybury, M. and Morey, D., 1999. Enter-prise expert and knowledge discovery. Technical Re-port.  McDonald, D. W. and Ackerman, M. S., 1998. Just Talk to Me: a field study of expertise location. In: Proc. of CSCW?98, pp.315-324. Mockus, A. and Herbsleb, J.D., 2002. Expertise Browser: a quantitative approach to identifying ex-pertise, In: Proc. of ICSE?02. 
Maconald, C. and Ounis, I., 2006. Voting for candi-dates: adapting data fusion techniques for an expert search task. In: Proc. of CIKM'06, pp.387-396. Macdonald, C. and Ounis, I., 2007. Expertise Drift and Query Expansion in Expert Search. In Proc. of CIKM 2007.  Petkova, D., and Croft, W. B., 2006. Hierarchical lan-guage models for expert finding in enterprise cor-pora, In: Proc. of ICTAI?06, pp.599-608. Ponte, J. and Croft, W., 1998. A language modeling approach to information retrieval, In: Proc. of SIGIR?98, pp.275-281. Sihn, W. and Heeren F., 2001. Xpertfinder-expert find-ing within specified subject areas through analysis of e-mail communication. In: Proc. of the 6th Annual Scientific conference on Web Technology.  Soboroff, I., Vries, A.P., and Craswell, N., 2006. Over-view of the TREC 2006 Enterprise Track. In: Proc. of TREC 2006. Steer, L.A. and Lochbaum, K.E., 1988. An ex-pert/expert locating system based on automatic repre-sentation of semantic structure, In: Proc. of the 4th IEEE Conference on Artificial Intelligence Applica-tions. Yimam, D., 1996. Expert finding systems for organiza-tions: domain analysis and the DEMOIR approach. In: ECSCW?99 workshop of beyond knowledge man-agement: managing expertise, pp. 276?283. 
922
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1?9,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Heterogeneous Transfer Learning for Image Clustering via the Social Web
Qiang Yang
Hong Kong University of Science and Technology, Clearway Bay, Kowloon, Hong Kong
qyang@cs.ust.hk
Yuqiang Chen Gui-Rong Xue Wenyuan Dai Yong Yu
Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai 200240, China
{yuqiangchen,grxue,dwyak,yyu}@apex.sjtu.edu.cn
Abstract
In this paper, we present a new learning
scenario, heterogeneous transfer learn-
ing, which improves learning performance
when the data can be in different feature
spaces and where no correspondence be-
tween data instances in these spaces is pro-
vided. In the past, we have classified Chi-
nese text documents using English train-
ing data under the heterogeneous trans-
fer learning framework. In this paper,
we present image clustering as an exam-
ple to illustrate how unsupervised learning
can be improved by transferring knowl-
edge from auxiliary heterogeneous data
obtained from the social Web. Image
clustering is useful for image sense dis-
ambiguation in query-based image search,
but its quality is often low due to image-
data sparsity problem. We extend PLSA
to help transfer the knowledge from social
Web data, which have mixed feature repre-
sentations. Experiments on image-object
clustering and scene clustering tasks show
that our approach in heterogeneous trans-
fer learning based on the auxiliary data is
indeed effective and promising.
1 Introduction
Traditional machine learning relies on the avail-
ability of a large amount of data to train a model,
which is then applied to test data in the same
feature space. However, labeled data are often
scarce and expensive to obtain. Various machine
learning strategies have been proposed to address
this problem, including semi-supervised learning
(Zhu, 2007), domain adaptation (Wu and Diet-
terich, 2004; Blitzer et al, 2006; Blitzer et al,
2007; Arnold et al, 2007; Chan and Ng, 2007;
Daume, 2007; Jiang and Zhai, 2007; Reichart
and Rappoport, 2007; Andreevskaia and Bergler,
2008), multi-task learning (Caruana, 1997; Re-
ichart et al, 2008; Arnold et al, 2008), self-taught
learning (Raina et al, 2007), etc. A commonality
among these methods is that they all require the
training data and test data to be in the same fea-
ture space. In addition, most of them are designed
for supervised learning. However, in practice, we
often face the problem where the labeled data are
scarce in their own feature space, whereas there
may be a large amount of labeled heterogeneous
data in another feature space. In such situations, it
would be desirable to transfer the knowledge from
heterogeneous data to domains where we have rel-
atively little training data available.
To learn from heterogeneous data, researchers
have previously proposed multi-view learning
(Blum and Mitchell, 1998; Nigam and Ghani,
2000) in which each instance has multiple views in
different feature spaces. Different from previous
works, we focus on the problem of heterogeneous
transfer learning, which is designed for situation
when the training data are in one feature space
(such as text), and the test data are in another (such
as images), and there may be no correspondence
between instances in these spaces. The type of
heterogeneous data can be very different, as in the
case of text and image. To consider how hetero-
geneous transfer learning relates to other types of
learning, Figure 1 presents an intuitive illustration
of four learning strategies, including traditional
machine learning, transfer learning across differ-
ent distributions, multi-view learning and hetero-
geneous transfer learning. As we can see, an
important distinguishing feature of heterogeneous
transfer learning, as compared to other types of
learning, is that more constraints on the problem
are relaxed, such that data instances do not need to
correspond anymore. This allows, for example, a
collection of Chinese text documents to be classi-
fied using another collection of English text as the
1
training data (c.f. (Ling et al, 2008) and Section
2.1).
In this paper, we will give an illustrative exam-
ple of heterogeneous transfer learning to demon-
strate how the task of image clustering can ben-
efit from learning from the heterogeneous social
Web data. A major motivation of our work is
Web-based image search, where users submit tex-
tual queries and browse through the returned result
pages. One problem is that the user queries are of-
ten ambiguous. An ambiguous keyword such as
?Apple? might retrieve images of Apple comput-
ers and mobile phones, or images of fruits. Im-
age clustering is an effective method for improv-
ing the accessibility of image search result. Loeff
et al (2006) addressed the image clustering prob-
lem with a focus on image sense discrimination.
In their approach, images associated with textual
features are used for clustering, so that the text
and images are clustered at the same time. Specif-
ically, spectral clustering is applied to the distance
matrix built from a multimodal feature set associ-
ated with the images to get a better feature repre-
sentation. This new representation contains both
image and text information, with which the per-
formance of image clustering is shown to be im-
proved. A problem with this approach is that when
images contained in the Web search results are
very scarce and when the textual data associated
with the images are very few, clustering on the im-
ages and their associated text may not be very ef-
fective.
Different from these previous works, in this pa-
per, we address the image clustering problem as
a heterogeneous transfer learning problem. We
aim to leverage heterogeneous auxiliary data, so-
cial annotations, etc. to enhance image cluster-
ing performance. We observe that the World Wide
Web has many annotated images in Web sites such
as Flickr (http://www.flickr.com), which
can be used as auxiliary information source for
our clustering task. In this work, our objective
is to cluster a small collection of images that we
are interested in, where these images are not suf-
ficient for traditional clustering algorithms to per-
form well due to data sparsity and the low level of
image features. We investigate how to utilize the
readily available socially annotated image data on
the Web to improve image clustering. Although
these auxiliary data may be irrelevant to the im-
ages to be clustered and cannot be directly used
to solve the data sparsity problem, we show that
they can still be used to estimate a good latent fea-
ture representation, which can be used to improve
image clustering.
2 Related Works
2.1 Heterogeneous Transfer Learning
Between Languages
In this section, we summarize our previous work
on cross-language classification as an example of
heterogeneous transfer learning. This example
is related to our image clustering problem be-
cause they both rely on data from different feature
spaces.
As the World Wide Web in China grows rapidly,
it has become an increasingly important prob-
lem to be able to accurately classify Chinese Web
pages. However, because the labeled Chinese Web
pages are still not sufficient, we often find it diffi-
cult to achieve high accuracy by applying tradi-
tional machine learning algorithms to the Chinese
Web pages directly. Would it be possible to make
the best use of the relatively abundant labeled En-
glish Web pages for classifying the Chinese Web
pages?
To answer this question, in (Ling et al, 2008),
we developed a novel approach for classifying the
Web pages in Chinese using the training docu-
ments in English. In this subsection, we give a
brief summary of this work. The problem to be
solved is: we are given a collection of labeled
English documents and a large number of unla-
beled Chinese documents. The English and Chi-
nese texts are not aligned. Our objective is to clas-
sify the Chinese documents into the same label
space as the English data.
Our key observation is that even though the data
use different text features, they may still share
many of the same semantic information. What we
need to do is to uncover this latent semantic in-
formation by finding out what is common among
them. We did this in (Ling et al, 2008) by us-
ing the information bottleneck theory (Tishby et
al., 1999). In our work, we first translated the
Chinese document into English automatically us-
ing some available translation software, such as
Google translate. Then, we encoded the training
text as well as the translated target text together,
in terms of the information theory. We allowed all
the information to be put through a ?bottleneck?
and be represented by a limited number of code-
2
Figure 1: An intuitive illustration of different kinds learning strategies using classification/clustering of
image apple and banana as the example.
words (i.e. labels in the classification problem).
Finally, information bottleneck was used to main-
tain most of the common information between the
two data sources, and discard the remaining irrel-
evant information. In this way, we can approxi-
mate the ideal situation where similar training and
translated test pages shared in the common part are
encoded into the same codewords, and are thus as-
signed the correct labels. In (Ling et al, 2008), we
experimentally showed that heterogeneous trans-
fer learning can indeed improve the performance
of cross-language text classification as compared
to directly training learning models (e.g., Naive
Bayes or SVM) and testing on the translated texts.
2.2 Other Works in Transfer Learning
In the past, several other works made use of trans-
fer learning for cross-feature-space learning. Wu
and Oard (2008) proposed to handle the cross-
language learning problem by translating the data
into a same language and applying kNN on the
latent topic space for classification. Most learning
algorithms for dealing with cross-language hetero-
geneous data require a translator to convert the
data to the same feature space. For those data that
are in different feature spaces where no transla-
tor is available, Davis and Domingos (2008) pro-
posed a Markov-logic-based transfer learning al-
gorithm, which is called deep transfer, for trans-
ferring knowledge between biological domains
and Web domains. Dai et al (2008a) proposed
a novel learning paradigm, known as translated
learning, to deal with the problem of learning het-
erogeneous data that belong to quite different fea-
ture spaces by using a risk minimization frame-
work.
2.3 Relation to PLSA
Our work makes use of PLSA. Probabilistic la-
tent semantic analysis (PLSA) is a widely used
probabilistic model (Hofmann, 1999), and could
be considered as a probabilistic implementation of
latent semantic analysis (LSA) (Deerwester et al,
1990). An extension to PLSA was proposed in
(Cohn and Hofmann, 2000), which incorporated
the hyperlink connectivity in the PLSA model by
using a joint probabilistic model for connectivity
and content. Moreover, PLSA has shown a lot
of applications ranging from text clustering (Hof-
mann, 2001) to image analysis (Sivic et al, 2005).
2.4 Relation to Clustering
Compared to many previous works on image clus-
tering, we note that traditional image cluster-
ing is generally based on techniques such as K-
means (MacQueen, 1967) and hierarchical clus-
tering (Kaufman and Rousseeuw, 1990). How-
ever, when the data are sparse, traditional clus-
tering algorithms may have difficulties in obtain-
ing high-quality image clusters. Recently, several
researchers have investigated how to leverage the
auxiliary information to improve target clustering
3
performance, such as supervised clustering (Fin-
ley and Joachims, 2005), semi-supervised cluster-
ing (Basu et al, 2004), self-taught clustering (Dai
et al, 2008b), etc.
3 Image Clustering with Annotated
Auxiliary Data
In this section, we present our annotation-based
probabilistic latent semantic analysis algorithm
(aPLSA), which extends the traditional PLSA
model by incorporating annotated auxiliary im-
age data. Intuitively, our algorithm aPLSA per-
forms PLSA analysis on the target images, which
are converted to an image instance-to-feature co-
occurrence matrix. At the same time, PLSA is
also applied to the annotated image data from so-
cial Web, which is converted into a text-to-image-
feature co-occurrence matrix. In order to unify
those two separate PLSA models, these two steps
are done simultaneously with common latent vari-
ables used as a bridge linking them. Through
these common latent variables, which are now
constrained by both target image data and auxil-
iary annotation data, a better clustering result is
expected for the target data.
3.1 Probabilistic Latent Semantic Analysis
Let F = {fi}|F|i=1 be an image feature space, and
V = {vi}|V|i=1 be the image data set. Each image
vi ? V is represented by a bag-of-features {f |f ?
vi ? f ? F}.
Based on the image data set V , we can esti-
mate an image instance-to-feature co-occurrence
matrix A|V|?|F| ? R|V|?|F|, where each element
Aij (1 ? i ? |V| and 1 ? j ? |F|) in the matrix
A is the frequency of the feature fj appearing in
the instance vi.
LetW = {wi}|W|i=1 be a text feature space. The
annotated image data allow us to obtain the co-
occurrence information between images v and text
features w ? W . An example of annotated im-
age data is the Flickr (http://www.flickr.
com), which is a social Web site containing a large
number of annotated images.
By extracting image features from the annotated
images v, we can estimate a text-to-image fea-
ture co-occurrence matrix B|W|?|F| ? R|W|?|F|,
where each element Bij (1 ? i ? |W| and
1 ? j ? |F|) in the matrix B is the frequency
of the text feature wi and the image feature fj oc-
curring together in the annotated image data set.
V Z F
P (z|v) P (f |z)
Figure 2: Graphical model representation of PLSA
model.
LetZ = {zi}|Z|i=1 be the latent variable set in our
aPLSA model. In clustering, each latent variable
zi ? Z corresponds to a certain cluster.
Our objective is to estimate a clustering func-
tion g : V 7? Z with the help of the two co-
occurrence matrices A and B as defined above.
To formally introduce the aPLSA model, we
start from the probabilistic latent semantic anal-
ysis (PLSA) (Hofmann, 1999) model. PLSA is
a probabilistic implementation of latent seman-
tic analysis (LSA) (Deerwester et al, 1990). In
our image clustering task, PLSA decomposes the
instance-feature co-occurrence matrix A under the
assumption of conditional independence of image
instances V and image features F , given the latent
variables Z .
P (f |v) =
?
z?Z
P (f |z)P (z|v). (1)
The graphical model representation of PLSA is
shown in Figure 2.
Based on the PLSA model, the log-likelihood can
be defined as:
L =
?
i
?
j
Aij
?
j? Aij?
logP (fj |vi) (2)
where A|V|?|F| ? R|V|?|F| is the image instance-
feature co-occurrence matrix. The term AijP
j? Aij?
in Equation (2) is a normalization term ensuring
each image is giving the same weight in the log-
likelihood.
Using EM algorithm (Dempster et al, 1977),
which locally maximizes the log-likelihood of
the PLSA model (Equation (2)), the probabilities
P (f |z) and P (z|v) can be estimated. Then, the
clustering function is derived as
g(v) = argmax
z?Z
P (z|v). (3)
Due to space limitation, we omit the details for the
PLSA model, which can be found in (Hofmann,
1999).
3.2 aPLSA: Annotation-based PLSA
In this section, we consider how to incorporate
a large number of socially annotated images in a
4
VW
Z F
P (z
|v)
P (z|w)
P (f |z)
Figure 3: Graphical model representation of
aPLSA model.
unified PLSA model for the purpose of utilizing
the correlation between text features and image
features. In the auxiliary data, each image has cer-
tain textual tags that are attached by users. The
correlation between text features and image fea-
tures can be formulated as follows.
P (f |w) =
?
z?Z
P (f |z)P (z|w). (4)
It is clear that Equations (1) and (4) share a same
term P (f |z). So we design a new PLSA model by
joining the probabilistic model in Equation (1) and
the probabilistic model in Equation (4) into a uni-
fied model, as shown in Figure 3. In Figure 3, the
latent variables Z depend not only on the corre-
lation between image instances V and image fea-
tures F , but also the correlation between text fea-
turesW and image featuresF . Therefore, the aux-
iliary socially-annotated image data can be used
to help the target image clustering performance by
estimating good set of latent variables Z .
Based on the graphical model representation in
Figure 3, we derive the log-likelihood objective
function, in a similar way as in (Cohn and Hof-
mann, 2000), as follows
L =
?
j
[
?
?
i
Aij
?
j? Aij?
logP (fj |vi)
+(1? ?)
?
l
Blj
?
j? Blj?
logP (fj |wl)
]
,
(5)
where A|V|?|F| ? R|V|?|F| is the image instance-
feature co-occurrence matrix, and B|W|?|F| ?
R|W|?|F| is the text-to-image feature-level co-
occurrence matrix. Similar to Equation (2),
Aij
P
j? Aij?
and BljP
j? Blj?
in Equation (5) are the nor-
malization terms to prevent imbalanced cases.
Furthermore, ? acts as a trade-off parameter be-
tween the co-occurrence matrices A and B. In
the extreme case when ? = 1, the log-likelihood
objective function ignores all the biases from the
text-to-image occurrence matrix B. In this case,
the aPLSA model degenerates to the traditional
PLSA model. Therefore, aPLSA is an extension
to the PLSA model.
Now, the objective is to maximize the log-
likelihood L of the aPLSA model in Equation (5).
Then we apply the EM algorithm (Dempster et
al., 1977) to estimate the conditional probabilities
P (f |z), P (z|w) and P (z|v) with respect to each
dependence in Figure 3 as follows.
? E-Step: calculate the posterior probability of
each latent variable z given the observation
of image features f , image instances v and
text features w based on the old estimate of
P (f |z), P (z|w) and P (z|v):
P (zk|vi, fj) =
P (fj |zk)P (zk|vi)
?
k? P (fj |zk?)P (zk? |vi)
(6)
P (zk|wl, fj) =
P (fj |zk)P (zk|wl)
?
k? P (fj |zk?)P (zk? |wl)
(7)
? M-Step: re-estimates conditional probabili-
ties P (zk|vi) and P (zk|wl):
P (zk|vi) =
?
j
Aij
?
j? Aij?
P (zk|vi, fj) (8)
P (zk|wl) =
?
j
Blj
?
j? Blj?
P (zk|wl, fj) (9)
and conditional probability P (fj |zk), which
is a mixture portion of posterior probability
of latent variables
P (fj |zk) ? ?
?
i
Aij
?
j? Aij?
P (zk|vi, fj)
+ (1? ?)
?
l
Blj
?
j? Blj?
P (zk|wl, fj)
(10)
Finally, the clustering function for a certain im-
age v is
g(v) = argmax
z?Z
P (z|v). (11)
From the above equations, we can derive
our annotation-based probabilistic latent semantic
analysis (aPLSA) algorithm. As shown in Algo-
rithm 1, aPLSA iteratively performs the E-Step
and the M-Step in order to seek local optimal
points based on the objective function L in Equa-
tion (5).
5
Algorithm 1 Annotation-based PLSA Algorithm
(aPLSA)
Input: The V-F co-occurrence matrix A andW-
F co-occurrence matrix B.
Output: A clustering (partition) function g : V 7?
Z , which maps an image instance v ? V to a latent
variable z ? Z .
1: Initial Z so that |Z| equals the number clus-
ters desired.
2: Initialize P (z|v), P (z|w), P (f |z) randomly.
3: while the change of L in Eq. (5) between two
sequential iterations is greater than a prede-
fined threshold do
4: E-Step: Update P (z|v, f) and P (z|w, f)
based on Eq. (6) and (7) respectively.
5: M-Step: Update P (z|v), P (z|w) and
P (f |z) based on Eq. (8), (9) and (10) re-
spectively.
6: end while
7: for all v in V do
8: g(v)? argmax
z
P (z|v).
9: end for
10: Return g.
4 Experiments
In this section, we empirically evaluate the aPLSA
algorithm together with some state-of-art base-
line methods on two widely used image corpora,
to demonstrate the effectiveness of our algorithm
aPLSA.
4.1 Data Sets
In order to evaluate the effectiveness of our algo-
rithm aPLSA, we conducted experiments on sev-
eral data sets generated from two image corpora,
Caltech-256 (Griffin et al, 2007) and the fifteen-
scene (Lazebnik et al, 2006). The Caltech-256
data set has 256 image objective categories, rang-
ing from animals to buildings, from plants to au-
tomobiles, etc. The fifteen-scene data set con-
tains 15 scenes such as store and forest.
From these two corpora, we randomly generated
eleven image clustering tasks, including seven 2-
way clustering tasks, two 4-way clustering task,
one 5-way clustering task and one 8-way cluster-
ing task. The detailed descriptions for these clus-
tering tasks are given in Table 1. In these tasks,
bi7 and oct1 were generated from fifteen-scene
data set, and the rest were from Caltech-256 data
set.
DATA SET INVOLVED CLASSES DATA SIZE
bi1 skateboard, airplanes 102, 800
bi2 billiards, mars 278, 155
bi3 cd, greyhound 102, 94
bi4 electric-guitar, snake 122, 112
bi5 calculator, dolphin 100, 106
bi6 mushroom, teddy-bear 202, 99
bi7 MIThighway, livingroom 260, 289
quad1 calculator, diamond-ring, dolphin,
microscope 100, 118, 106, 116
quad2 bonsai, comet, frog, saddle 122, 120, 115, 110
quint1 frog, kayak, bear, jesus-christ, watch 115, 102, 101, 87,201
oct1
MIThighway, MITmountain,
kitchen, MITcoast, PARoffice, MIT-
tallbuilding, livingroom, bedroom
260, 374, 210, 360,
215, 356, 289, 216
tune1 coin, horse 123, 270
tune2 socks, spider 111, 106
tune3 galaxy, snowmobile 80, 112
tune4 dice, fern 98, 110
tune5 backpack, lightning, mandolin, swan 151, 136, 93, 114
Table 1: The descriptions of all the image clus-
tering tasks used in our experiment. Among
these data sets, bi7 and oct1 were generated
from fifteen-scene data set, and the rest were from
Caltech-256 data set.
To empirically investigate the parameter ? and
the convergence of our algorithm aPLSA, we gen-
erated five more date sets as the development sets.
The detailed description of these five development
sets, namely tune1 to tune5 is listed in Table 1
as well.
The auxiliary data were crawled from the Flickr
(http://www.flickr.com/) web site dur-
ing August 2007. Flickr is an internet community
where people share photos online and express their
opinions as social tags (annotations) attached to
each image. From Flicker, we collected 19, 959
images and 91, 719 related annotations, among
which 2, 600 words are distinct. Based on the
method described in Section 3, we estimated the
co-occurrence matrix B between text features and
image features. This co-occurrence matrix B was
used by all the clustering tasks in our experiments.
For data preprocessing, we adopted the bag-of-
features representation of images (Li and Perona,
2005) in our experiments. Interesting points were
found in the images and described via the SIFT
descriptors (Lowe, 2004). Then, the interesting
points were clustered to generate a codebook to
form an image feature space. The size of code-
book was set to 2, 000 in our experiments. Based
on the codebook, which serves as the image fea-
ture space, each image can be represented as a cor-
responding feature vector to be used in the next
step.
To set our evaluation criterion, we used the
6
Data Set KMeans PLSA STC aPLSA
separate combined separate combined
bi1 0.645?0.064 0.548?0.031 0.544?0.074 0.537?0.033 0.586?0.139 0.482?0.062
bi2 0.687?0.003 0.662?0.014 0.464?0.074 0.692?0.001 0.577?0.016 0.455?0.096
bi3 1.294?0.060 1.300?0.015 1.085?0.073 1.126?0.036 1.103?0.108 1.029?0.074
bi4 1.227?0.080 1.164?0.053 0.976?0.051 1.038?0.068 1.024?0.089 0.919?0.065
bi5 1.450?0.058 1.417?0.045 1.426?0.025 1.405?0.040 1.411?0.043 1.377?0.040
bi6 1.969?0.078 1.852?0.051 1.514?0.039 1.709?0.028 1.589?0.121 1.503?0.030
bi7 0.686?0.006 0.683?0.004 0.643?0.058 0.632?0.037 0.651?0.012 0.624?0.066
quad1 0.591?0.094 0.675?0.017 0.488?0.071 0.662?0.013 0.580?0.115 0.432?0.085
quad2 0.648?0.036 0.646?0.045 0.614?0.062 0.626?0.026 0.591?0.087 0.515?0.098
quint1 0.557?0.021 0.508?0.104 0.547?0.060 0.539?0.051 0.538?0.100 0.502?0.067
oct1 0.659?0.031 0.680?0.012 0.340?0.147 0.691?0.002 0.411?0.089 0.306?0.101
average 0.947?0.029 0.922?0.017 0.786?0.009 0.878?0.006 0.824?0.036 0.741?0.018
Table 2: Experimental result in term of entropy for all data sets and evaluation methods.
entropy to measure the quality of our clustering
results. In information theory, entropy (Shan-
non, 1948) is a measure of the uncertainty as-
sociated with a random variable. In our prob-
lem, entropy serves as a measure of randomness
of clustering result. The entropy of g on a sin-
gle latent variable z is defined to be H(g, z) ,
??c?C P (c|z) log2 P (c|z), where C is the class
label set of V and P (c|z) = |{v|g(v)=z?t(v)=c}||{v|g(v)=z}| ,
in which t(v) is the true class label of image v.
Lower entropy H(g,Z) indicates less randomness
and thus better clustering result.
4.2 Empirical Analysis
We now empirically analyze the effectiveness of
our aPLSA algorithm. Because, to our best of
knowledge, few existing methods addressed the
problem of image clustering with the help of so-
cial annotation image data, we can only compare
our aPLSA with several state-of-the-art cluster-
ing algorithms that are not directly designed for
our problem. The first baseline is the well-known
KMeans algorithm (MacQueen, 1967). Since our
algorithm is designed based on PLSA (Hofmann,
1999), we also included PLSA for clustering as a
baseline method in our experiments.
For each of the above two baselines, we have
two strategies: (1) separated: the baseline
method was applied on the target image data only;
(2) combined: the baseline method was applied
to cluster the combined data consisting of both
target image data and the annotated image data.
Clustering results on target image data were used
for evaluation. Note that, in the combined data, all
the annotations were thrown away since baseline
methods evaluated in this paper do not leverage
annotation information.
In addition, we compared our algorithm aPLSA
to a state-of-the-art transfer clustering strategy,
known as self-taught clustering (STC) (Dai et al,
2008b). STC makes use of auxiliary data to esti-
mate a better feature representation to benefit the
target clustering. In these experiments, the anno-
tated image data were used as auxiliary data in
STC, which does not use the annotation text.
In our experiments, the performance is in the
form of the average entropy and variance of five
repeats by randomly selecting 50 images from
each of the categories. We selected only 50 im-
ages per category, since this paper is focused on
clustering sparse data. Table 2 shows the perfor-
mance with respect to all comparison methods on
each of the image clustering tasks measured by
the entropy criterion. From the tables, we can see
that our algorithm aPLSA outperforms the base-
line methods in all the data sets. We believe that is
because aPLSA can effectively utilize the knowl-
edge from the socially annotated image data. On
average, aPLSA gives rise to 21.8% of entropy re-
duction and as compared to KMeans, 5.7% of en-
tropy reduction as compared to PLSA, and 10.1%
of entropy reduction as compared to STC.
4.2.1 Varying Data Size
We now show how the data size affects aPLSA,
with two baseline methods KMeans and PLSA as
reference. The experiments were conducted on
different amounts of target image data, varying
from 10 to 80. The corresponding experimental
results in average entropy over all the 11 clustering
tasks are shown in Figure 4(a). From this figure,
we observe that aPLSA always yields a significant
reduction in entropy as compared with two base-
line methods KMeans and PLSA, regardless of the
size of target image data that we used.
7
10 20 30 40 50 60 70 80
0.7
0.75
0.8
0.85
0.9
0.95
1
Data size per category
En
tro
py
 
 
KMeans
PLSA
aPLSA
(a)
0 0.2 0.4 0.6 0.8 1
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
?
En
tro
py
 
 
average over 5 development sets
(b)
0 50 100 150 200 250 300
0.5
0.55
0.6
0.65
0.7
0.75
Number of Iteration
En
tro
py
 
 
average over 5 development sets
(c)
Figure 4: (a) The entropy curve as a function of different amounts of data per category. (b) The entropy
curve as a function of different number of iterations. (c) The entropy curve as a function of different
trade-off parameter ?.
4.2.2 Parameter Sensitivity
In aPLSA, there is a trade-off parameter ? that af-
fects how the algorithm relies on auxiliary data.
When ? = 0, the aPLSA relies only on annotated
image data B. When ? = 1, aPLSA relies only
on target image data A, in which case aPLSA de-
generates to PLSA. Smaller ? indicates heavier re-
liance on the annotated image data. We have done
some experiments on the development sets to in-
vestigate how different ? affect the performance
of aPLSA. We set the number of images per cate-
gory to 50, and tested the performance of aPLSA.
The result in average entropy over all development
sets is shown in Figure 4(b). In the experiments
described in this paper, we set ? to 0.2, which is
the best point in Figure 4(b).
4.2.3 Convergence
In our experiments, we tested the convergence
property of our algorithm aPLSA as well. Fig-
ure 4(c) shows the average entropy curve given
by aPLSA over all development sets. From this
figure, we see that the entropy decreases very fast
during the first 100 iterations and becomes stable
after 150 iterations. We believe that 200 iterations
is sufficient for aPLSA to converge.
5 Conclusions
In this paper, we proposed a new learning scenario
called heterogeneous transfer learning and illus-
trated its application to image clustering. Image
clustering, a vital component in organizing search
results for query-based image search, was shown
to be improved by transferring knowledge from
unrelated images with annotations in a social Web.
This is done by first learning the high-quality la-
tent variables in the auxiliary data, and then trans-
ferring this knowledge to help improve the cluster-
ing of the target image data. We conducted experi-
ments on two image data sets, using the Flickr data
as the annotated auxiliary image data, and showed
that our aPLSA algorithm can greatly outperform
several state-of-the-art clustering algorithms.
In natural language processing, there are many
future opportunities to apply heterogeneous trans-
fer learning. In (Ling et al, 2008) we have shown
how to classify the Chinese text using English text
as the training data. We may also consider cluster-
ing, topic modeling, question answering, etc., to
be done using data in different feature spaces. We
can consider data in different modalities, such as
video, image and audio, as the training data. Fi-
nally, we will explore the theoretical foundations
and limitations of heterogeneous transfer learning
as well.
Acknowledgement Qiang Yang thanks Hong
Kong CERG grant 621307 for supporting the re-
search.
References
Alina Andreevskaia and Sabine Bergler. 2008. When spe-
cialists and generalists work together: Overcoming do-
main dependence in sentiment tagging. In ACL-08: HLT,
pages 290?298, Columbus, Ohio, June.
Andrew Arnold, Ramesh Nallapati, and William W. Cohen.
2007. A comparative study of methods for transductive
transfer learning. In ICDM 2007 Workshop on Mining
and Management of Biological Data, pages 77-82.
Andrew Arnold, Ramesh Nallapati, and William W. Cohen.
2008. Exploiting feature hierarchy for transfer learning in
named entity recognition. In ACL-08: HLT.
Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney.
2004. A probabilistic framework for semi-supervised
clustering. In ACM SIGKDD 2004, pages 59?68.
John Blitzer, Ryan Mcdonald, and Fernando Pereira. 2006.
Domain adaptation with structural correspondence learn-
ing. In EMNLP 2006, pages 120?128, Sydney, Australia.
8
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classification. In ACL 2007,
pages 440?447, Prague, Czech Republic.
Avrim Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In COLT 1998, pages
92?100, New York, NY, USA. ACM.
Rich Caruana. 1997. Multitask learning. Machine Learning,
28(1):41?75.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation
with active learning for word sense disambiguation. In
ACL 2007, Prague, Czech Republic.
David A. Cohn and Thomas Hofmann. 2000. The missing
link - a probabilistic model of document content and hy-
pertext connectivity. In NIPS 2000, pages 430?436.
Wenyuan Dai, Yuqiang Chen, Gui-Rong Xue, Qiang Yang,
and Yong Yu. 2008a. Translated learning: Transfer learn-
ing across different feature spaces. In NIPS 2008, pages
353?360.
Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong Yu.
2008b. Self-taught clustering. In ICML 2008, pages 200?
207. Omnipress.
Hal Daume, III. 2007. Frustratingly easy domain adaptation.
In ACL 2007, pages 256?263, Prague, Czech Republic.
Jesse Davis and Pedro Domingos. 2008. Deep transfer via
second-order markov logic. In AAAI 2008 Workshop on
Transfer Learning, Chicago, USA.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. L, and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American Society
for Information Science, pages 391?407.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em algo-
rithm. J. of the Royal Statistical Society, 39:1?38.
Thomas Finley and Thorsten Joachims. 2005. Supervised
clustering with support vector machines. In ICML 2005,
pages 217?224, New York, NY, USA. ACM.
G. Griffin, A. Holub, and P. Perona. 2007. Caltech-256 ob-
ject category dataset. Technical Report 7694, California
Institute of Technology.
Thomas Hofmann. 1999 Probabilistic latent semantic anal-
ysis. In Proc. of Uncertainty in Artificial Intelligence,
UAI99. Pages 289?296
Thomas Hofmann. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning. vol-
ume 42, number 1-2, pages 177?196. Kluwer Academic
Publishers.
Jing Jiang and Chengxiang Zhai. 2007. Instance weighting
for domain adaptation in NLP. In ACL 2007, pages 264?
271, Prague, Czech Republic, June.
Leonard Kaufman and Peter J. Rousseeuw. 1990. Finding
groups in data: an introduction to cluster analysis. John
Wiley and Sons, New York.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006.
Beyond bags of features: Spatial pyramid matching for
recognizing natural scene categories. In CVPR 2006,
pages 2169?2178, Washington, DC, USA.
Fei-Fei Li and Pietro Perona. 2005. A bayesian hierarchi-
cal model for learning natural scene categories. In CVPR
2005, pages 524?531, Washington, DC, USA.
Xiao Ling, Gui-Rong Xue, Wenyuan Dai, Yun Jiang, Qiang
Yang, and Yong Yu. 2008. Can chinese web pages be
classified with english data source? In WWW 2008, pages
969?978, New York, NY, USA. ACM.
Nicolas Loeff, Cecilia Ovesdotter Alm, and David A.
Forsyth. 2006. Discriminating image senses by clustering
with multimodal features. In COLING/ACL 2006 Main
conference poster sessions, pages 547?554.
David G. Lowe. 2004. Distinctive image features from scale-
invariant keypoints. International Journal of Computer
Vision (IJCV) 2004, volume 60, number 2, pages 91?110.
J. B. MacQueen. 1967. Some methods for classification and
analysis of multivariate observations. In Proceedings of
Fifth Berkeley Symposium on Mathematical Statistics and
Probability, pages 1:281?297, Berkeley, CA, USA.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the effec-
tiveness and applicability of co-training. In Proceedings
of the Ninth International Conference on Information and
Knowledge Management, pages 86?93, New York, USA.
Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer,
and Andrew Y. Ng. 2007. Self-taught learning: transfer
learning from unlabeled data. In ICML 2007, pages 759?
766, New York, NY, USA. ACM.
Roi Reichart and Ari Rappoport. 2007. Self-training for
enhancement and domain adaptation of statistical parsers
trained on small datasets. In ACL 2007.
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rap-
poport. 2008. Multi-task active learning for linguistic
annotations. In ACL-08: HLT, pages 861?869.
C. E. Shannon. 1948. A mathematical theory of communi-
cation. Bell system technical journal, 27.
J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T.
Freeman. 2005. Discovering object categories in image
collections. In ICCV 2005.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The
information bottleneck method. 1999. In Proc. of the 37-
th Annual Allerton Conference on Communication, Con-
trol and Computing, pages 368?377.
Pengcheng Wu and Thomas G. Dietterich. 2004. Improving
svm accuracy by training on auxiliary data sources. In
ICML 2004, pages 110?117, New York, NY, USA.
Yejun Wu and Douglas W. Oard. 2008. Bilingual topic as-
pect classification with a few training examples. In ACM
SIGIR 2008, pages 203?210, New York, NY, USA.
Xiaojin Zhu. 2007. Semi-supervised learning literature sur-
vey. Technical Report 1530, Computer Sciences, Univer-
sity of Wisconsin-Madison.
9
