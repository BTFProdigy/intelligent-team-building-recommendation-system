Dialogue Act Modeling for 
Automatic Tagging and Recognition 
of Conversational Speech 
Andreas Stolcke* 
SRI International 
Noah Coccaro 
University of Colorado at Boulder 
Rebecca Bates 
University of Washington 
Paul Taylor 
University of Edinburgh 
Carol Van Ess-Dykema 
U.S. Department of Defense 
Klaus Ries 
Carnegie Mellon University and 
University of Karlsruhe 
Elizabeth Shriberg 
SRI International 
Daniel Jurafsky 
University of Colorado at Boulder 
Rachel Martin 
Johns Hopkins University 
Marie Meteer 
BBN Technologies 
We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech- 
act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREE- 
MENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, colloca- 
tional, and prosodic ues, as well as on the discourse coherence of the dialogue act sequence. 
The dialogue model is based on treating the discourse structure of a conversation as a hidden 
Markov model and the individual dialogue acts as observations emanating from the model states. 
Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The 
statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks 
modeling the idiosyncratic lexical and prosodic manifestations ofeach dialogue act. We develop 
a probabilistic ntegration of speech recognition with dialogue modeling, to improve both speech 
recognition and dialogue act classification accuracy. Models are trained and evaluated using a 
large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous 
human-to-human telephone speech. We achieved good dialogue act labeling accuracy (65% based 
on errorful, automatically recognized words and prosody, and 71% based on word transcripts, 
compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reduction 
in word recognition error. 
? Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 
94025, 1-650-859-2544. E-mail: stolcke@speech.sri.com. 
@ 2000 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 3 
Table 1 
Fragment of a labeled conversation (from the Switchboard corpus). 
Speaker Dialogue Act Utterance 
A YEs-No-QuESTION So do you go to college right now? 
A ABANDONED Are yo-, 
B YES- ANSWER Yeah, 
B STATEMENT it's my last year \[laughter\]. 
A DECLARATIVE-QUESTION You're a, so you're a senior now. 
B YEs-ANSWER Yeah, 
B STATEMENT I'm working on my projects trying to graduate 
\[laughter\]. 
A APPRECIATION Oh, good for you. 
B BACKCHANNEL Yeah. 
A APPRECIATION That's great, 
A YEs-No-QUESTION um, is, is N C University is that, uh, State, 
B STATEMENT N C State. 
A SIGNAL-NoN-UNDERSTANDING What did you say? 
B STATEMENT N C State. 
1. Introduction 
The ability to model and automatically detect discourse structure is an important 
step toward understanding spontaneous dialogue. While there is hardly consensus 
on exactly how discourse structure should be described, some agreement exists that 
a useful first level of analysis involves the identification of dialogue acts (DAs). A 
DA represents he meaning of an utterance at the level of illocutionary force (Austin 
1962). Thus, a DA is approximately the equivalent of the speech act of Searle (1969), 
the conversational game move of Power (1979), or the adjacency pair part of Schegloff 
(1968) and Saks, Schegloff, and Jefferson (1974). 
Table 1 shows a sample of the kind of discourse structure in which we are inter- 
ested. Each utterance is assigned a unique DA label (shown in column 2), drawn from 
a well-defined set (shown in Table 2). Thus, DAs can be thought of as a tag set that 
classifies utterances according to a combination of pragmatic, semantic, and syntactic 
criteria. The computational community has usually defined these DA categories so as 
to be relevant to a particular application, although efforts are under way to develop 
DA labeling systems that are domain-independent, such as the Discourse Resource 
Initiative's DAMSL architecture (Core and Allen 1997). 
While not constituting dialogue understanding in any deep sense, DA tagging 
seems clearly useful to a range of applications. For example, a meeting summarizer 
needs to keep track of who said what to whom, and a conversational agent needs to 
know whether it was asked a question or ordered to do something. In related work 
DAs are used as a first processing step to infer dialogue games (Carlson 1983; Levin 
and Moore 1977; Levin et al 1999), a slightly higher level unit that comprises a small 
number of DAs. Interactional dominance (Linell 1990) might be measured more ac- 
curately using DA distributions than with simpler techniques, and could serve as an 
indicator of the type or genre of discourse at hand. In all these cases, DA labels would 
enrich the available input for higher-level processing of the spoken words. Another im- 
portant role of DA information could be feedback to lower-level processing. For exam- 
ple, a speech recognizer could be constrained by expectations of likely DAs in a given 
context, constraining the potential recognition hypotheses so as to improve accuracy. 
340 
Stolcke et al Dialogue Act Modeling 
Table 2 
The 42 dialogue act labels. DA frequencies are given as percentages of the total 
number of utterances in the overall corpus. 
Tag Example % 
STATEMENT 
BACKCHANNEL/ACKNOWLEDGE 
OPINION 
ABANDONED/UNINTERPRETABLE 
AGREEMENT/ACCEPT 
APPRECIATION 
YEs-No-QUESTION 
NON-VERBAL 
YES ANSWERS 
CONVENTIONAL-CLOSING 
WH-QUESTION 
NO ANSWERS 
RESPONSE ACKNOWLEDGMENT 
HEDGE 
DECLARATIVE YES-No-QuESTION 
OTHER 
BACKCHANNEL-QUESTION 
QUOTATION 
SUMMARIZE/REFORMULATE 
AFFIRMATIVE NON-YES ANSWERS 
ACTION-DIRECTIVE 
COLLABORATIVE COMPLETION 
REPEAT-PHRASE 
OPEN-QUESTION 
RHETORICAL-QUESTIONS 
HOLD BEFORE ANSWER/AGREEMENT 
REJECT 
NEGATIVE NON-NO ANSWERS 
SIGNAL-NON-UNDERSTANDING 
OTHER ANSWERS 
CONVENTIONAL-OPENING 
OR-CLAUSE 
DISPREFERRED ANSWERS 
3RD-PARTY-TALK 
OFFERS, OPTIONS ~ COMMITS 
SELF-TALK 
D OWNPLAYER 
MAYBE/AcCEPT-PART 
TAG-QUESTION 
DECLARATIVE WH-QUESTION 
APOLOGY 
THANKING 
Me, I'm in the legal department. 36% 
Uh-huh. 19% 
I think it's great 13% 
So, -/ 6% 
That's exactly it. 5% 
I can imagine. 2% 
Do you have to have any special training? 2% 
<Laughter>, < Throat_clearing> 2% 
Yes. 1% 
Well, it's been nice talking to you. 1% 
What did you wear to work today? 1% 
No. 1% 
Oh, okay. 1% 
I don't know if I'm making any sense or not. 1% 
So you can afford to get a house? 1% 
Well give me a break, you know. 1% 
Is that right? 1% 
You can't be pregnant and have cats .5% 
Oh, you mean you switched schools for the kids. .5% 
It is. .4% 
Why don't you go first .4% 
Who aren't contributing. .4% 
Oh, fajitas .3% 
How about you ? .3% 
Who would steal a newspaper? .2% 
I'm drawing a blank. .3% 
Well, no .2% 
Uh, not a whole lot. .1% 
Excuse me? .1% 
I don't know .1% 
How are you? .1% 
or is it more of a company? .1% 
Well, not so much that. .1% 
My goodness, Diane, get down from there. .1% 
I'I1 have to check that out .1% 
What's the word I'm looking for .1% 
That's all right. .1% 
Something like that <.1% 
Right? <.1% 
You are what kind of buff? <.1% 
I'm sorry. <.1% 
Hey thanks a lot <.1% 
The goal of this article is twofold: On the one hand, we aim to present a com- 
prehensive f ramework for model ing and automatic lassification of DAs, founded on 
wel l -known statistical methods. In doing so, we will pull together previous approaches 
as well as new ideas. For example, our model draws on the use of DA n-grams and the 
hidden Markov models of conversation present in earlier work, such as Nagata and 
Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7). However,  
our f ramework generalizes earlier models, giving us a clean probabilistic approach for 
performing DA classification from unreliable words and nonlexical evidence. For the 
341 
Computational Linguistics Volume 26, Number 3 
speech recognition task, our framework provides a mathematically principled way to 
condition the speech recognizer on conversation context through dialogue structure, as 
well as on nonlexical information correlated with DA identity. We will present meth- 
ods in a domain-independent framework that for the most part treats DA labels as an 
arbitrary formal tag set. Throughout the presentation, we will highlight he simplifi- 
cations and assumptions made to achieve tractable models, and point out how they 
might fall short of reality. 
Second, we present results obtained with this approach on a large, widely available 
corpus of spontaneous conversational speech. These results, besides validating the 
methods described, are of interest for several reasons. For example, unlike in most 
previous work on DA labeling, the corpus is not task-oriented in nature, and the 
amount of data used (198,000 utterances) exceeds that in previous tudies by at least 
an order of magnitude (see Table 14). 
To keep the presentation i teresting and concrete, we will alternate between the 
description of general methods and empirical results. Section 2 describes the task 
and our data in detail. Section 3 presents the probabilistic modeling framework; a
central component of this framework, the discourse grammar, is further discussed in 
Section 4. In Section 5 we describe xperiments for DA classification. Section 6 shows 
how DA models can be used to benefit speech recognition. Prior and related work is 
summarized in Section 7. Further issues and open problems are addressed in Section 8, 
followed by concluding remarks in Section 9. 
2. The Dialogue Act Labeling Task 
The domain we chose to model is the Switchboard corpus of human-human con- 
versational telephone speech (Godfrey, Holliman, and McDaniel 1992) distributed by 
the Linguistic Data Consortium. Each conversation i volved two randomly selected 
strangers who had been charged with talking informally about one of several, self- 
selected general-interest topics. To train our statistical models on this corpus, we com- 
bined an extensive ffort in human hand-coding of DAs for each utterance, with a 
variety of automatic and semiautomatic tools. Our data consisted of a substantial 
portion of the Switchboard waveforms and corresponding transcripts, totaling 1,155 
conversations. 
2.1 Utterance Segmentation 
Before hand-labeling each utterance in the corpus with a DA, we needed to choose an 
utterance segmentation, as the raw Switchboard ata is not segmented in a linguis- 
tically consistent way. To expedite the DA labeling task and remain consistent with 
other Switchboard-based research efforts, we made use of a version of the corpus that 
had been hand-segmented into sentence-level units prior to our own work and in- 
dependently of our DA labeling system (Meteer et al 1995). We refer to the units of 
this segmentation as utterances. The relation between utterances and speaker turns 
is not one-to-one: a single turn can contain multiple utterances, and utterances can 
span more than one turn (e.g., in the case of backchanneling by the other speaker in 
midutterance). Each utterance unit was identified with one DA, and was annotated 
with a single DA label. The DA labeling system had special provisions for rare cases 
where utterances seemed to combine aspects of several DA types. 
Automatic segmentation f spontaneous speech is an open research problem in its 
own right (Mast et al 1996; Stolcke and Shriberg 1996). A rough idea of the difficulty 
of the segmentation problem on this corpus and using the same definition of utterance 
units can be derived from a recent study (Shriberg et al 2000). In an automatic labeling 
342 
Stolcke t al. Dialogue Act Modeling 
of word boundaries as either utterance or nonboundaries u ing a combination oflexical 
and prosodic ues, we obtained 96% accuracy based on correct word transcripts, and 
78% accuracy with automatically recognized words. The fact that the segmentation 
and labeling tasks are interdependent (Warnke et al 1997; Finke et al 1998) further 
complicates the problem. 
Based on these considerations, we decided not to confound the DA classification 
task with the additional problems introduced by automatic segmentation a d assumed 
the utterance-level s gmentations as given. An important consequence of this decision 
is that we can expect utterance l ngth and acoustic properties at utterance boundaries 
to be accurate, both of which turn out to be important features of DAs (Shriberg et al 
1998; see also Section 5.2.1). 
2.2 Tag Set 
We chose to follow a recent standard for shallow discourse structure annotation, the 
Dialog Act Markup in Several Layers (DAMSL) tag set, which was designed by the 
natural language processing community under the auspices of the Discourse Resource 
Initiative (Core and Allen 1997). We began with the DAMSL markup system, but modi- 
fied it in several ways to make it more relevant to our corpus and task. DAMSL aims to 
provide a domain-independent framework for dialogue annotation, as reflected by the 
fact that our tag set can be mapped back to DAMSL categories (Jurafsky, Shriberg, and 
Biasca 1997). However, our labeling effort also showed that content- and task-related 
distinctions will always play an important role in effective DA labeling. 
The Switchboard omain itself is essentially "task-free," thus giving few external 
constraints on the definition of DA categories. Our primary purpose in adapting the 
tag set was to enable computational DA modeling for conversational speech, with 
possible improvements to conversational speech recognition. Because of the lack of a 
specific task, we decided to label categories that seemed inherently interesting linguis- 
tically and that could be identified reliably. Also, the focus on conversational speech 
recognition led to a certain bias toward categories that were lexically or syntactically 
distinct (recognition accuracy is traditionally measured including all lexical elements 
in an utterance). 
While the modeling techniques described in this paper are formally independent of 
the corpus and the choice of tag set, their success on any particular task will of course 
crucially depend on these factors. For different asks, not all the techniques used in 
this study might prove useful and others could be of greater importance. However, 
we believe that this study represents a fairly comprehensive application of technology 
in this area and can serve as a point of departure and reference for other work. 
The resulting SWBD-DAMSL tag set was multidimensional; pproximately 50ba- 
sic tags (e.g., QUESTION, STATEMENT) could each be combined with diacritics indicat- 
ing orthogonal information, for example, about whether or not the dialogue function 
of the utterance was related to Task-Management and Communication-Management. 
Approximately 220 of the many possible unique combinations ofthese codes were used 
by the coders (Jurafsky, Shriberg, and Biasca 1997). To obtain a system with somewhat 
higher interlabeler agreement, as well as enough data per class for statistical mod- 
eling purposes, a less fine-grained tag set was devised. This tag set distinguishes 42 
mutually exclusive utterance types and was used for the experiments reported here. 
Table 2 shows the 42 categories with examples and relative frequencies. 1 While some 
1 For the study focusing on prosodic modeling ofDAs reported elsewhere (Shriberg et al 1998), the tag 
set was further reduced to six categories. 
343 
Computational Linguistics Volume 26, Number 3 
of the original infrequent classes were collapsed, the resulting DA type distribution 
is still highly skewed. This occurs largely because there was no basis for subdividing 
the dominant DA categories according to task-independent a d reliable criteria. 
The tag set incorporates both traditional sociolinguistic and discourse-theoretic 
notions, such as rhetorical relations and adjacency pairs, as well as some more form- 
based labels. Furthermore, the tag set is structured so as to allow labelers to annotate 
a Switchboard conversation from transcripts alone (i.e., without listening) in about 
30 minutes. Without hese constraints he DA labels might have included some finer 
distinctions, but we felt that this drawback was balanced by the ability to cover a large 
amount of data. 2
Labeling was carried out in a three-month period in 1997 by eight linguistics 
graduate students at CU Boulder. Interlabeler agreement for the 42-1abel tag set used 
here was 84%, resulting in a Kappa statistic of 0.80. The Kappa statistic measures 
agreement ormalized for chance (Siegel and Castellan, Jr. 1988). As argued in Carletta 
(1996), Kappa values of 0.8 or higher are desirable for detecting associations between 
several coded variables; we were thus satisfied with the level of agreement achieved. 
(Note that, even though only a single variable, DA type, was coded for the present 
study, our goal is, among other things, to model associations between several instances 
of that variable, e.g., between adjacent DAs.) 
A total of 1,155 Switchboard conversations were labeled, comprising 205,000 ut- 
terances and 1.4 million words. The data was partitioned into a training set of 1,115 
conversations (1.4M words, 198K utterances), used for estimating the various compo- 
nents of our model, and a test set of 19 conversations (29K words, 4K utterances). 
Remaining conversations were set aside for future use (e.g., as a test set uncompro- 
mised of tuning effects). 
2.3 Major Dialogue Act Types 
The more frequent DA types are briefly characterized below. As discussed above, the 
focus of this paper is not on the nature of DAs, but on the computational framework 
for their recognition; full details of the DA tag set and numerous motivating examples 
can be found in a separate report (Jurafsky, Shriberg, and Biasca 1997). 
Statements and Opinions. The most common types of utterances were STATEMENTS 
and OPINIONS. This split distinguishes "descriptive, narrative, or personal" statements 
(STATEMENT) from "other-directed opinion statements" (OPINION). The distinction was 
designed to capture the different kinds of responses we saw to opinions (which are 
often countered or disagreed with via further opinions) and to statements (which more 
often elicit continuers or backchannels): 
Dialogue Act 
STATEMENT 
STATEMENT 
STATEMENT 
OPINION 
OPINION 
Example Utterance 
Well, we have a cat, um, 
He's probably, oh, a good two years old, 
big, old, fat and sassy tabby. 
He's about five months old 
Well, rabbits are darling. 
I think it would be kind of stressful. 
2 The effect of lacking acoustic information on labeling accuracy was assessed by relabeling a subset of 
the data with listening, and was found to be fairly small (Shriberg et al 1998). A conservative estimate 
based on the relabeling study is that, for most DA types, at most 2% of the labels might have changed 
based on listening. The only DA types with higher uncertainty were BACKCHANNELS and 
AGREEMENTS, which are easily confused with each other without acoustic ues; here the rate of change 
was no more than 10%. 
344 
Stolcke t al. Dialogue Act Modeling 
OPINIONS often include such hedges as I think, I believe, it seems, and I mean. We 
combined the STATEMENT and OPINION classes for other studies on dimensions in 
which they did not differ (Shriberg et al 1998). 
Questions. Questions were of several types. The YES-No-QUESTION label includes only 
utterances having both the pragmatic force of a yes-no-question and the syntactic mark- 
ings of a yes-no-question (i.e., subject-inversion r sentence-final t gs). DECLARATIVE- 
QUESTIONS are utterances that function pragmatically as questions but do not have 
"question form." By this we mean that declarative questions normally have no wh- 
word as the argument of the verb (except in "echo-question" format), and have "declar- 
ative" word order in which the subject precedes the verb. See Weber (1993) for a survey 
of declarative questions and their various realizations. 
Dialogue Act Example Utterance 
YEs-No-QUESTION 
YEs-No-QUESTION 
YEs-No-QuESTION 
DECLARATIVE- QUESTION 
WH-QUESTION 
Do you have to have any special training? 
But that doesn't eliminate it, does it? 
Uh, I guess a year ago you're probably 
watching C N N a lot, right? 
So you're taking a government course? 
Well, how old are you? 
Backchannels. A backchannel is a short utterance that plays discourse-structuring roles, 
e.g., indicating that the speaker should go on talking. These are usually referred to in 
the conversation analysis literature as "continuers" and have been studied extensively 
(Jefferson 1984; Schegloff 1982; Yngve 1970). We expect recognition of backchannels to
be useful because of their discourse-structuring role (knowing that the hearer expects 
the speaker to go on talking tells us something about the course of the narrative) 
and because they seem to occur at certain kinds of syntactic boundaries; detecting a 
backchannel may thus help in predicting utterance boundaries and surrounding lexical 
material. 
For an intuition about what backchannels look like, Table 3 shows the most com- 
mon realizations of the approximately 300 types (35,827 tokens) of backchannel in 
our Switchboard subset. The following table shows examples of backchannels in the 
context of a Switchboard conversation: 
Speaker Dialogue Act Utterance 
B STATEMENT 
A BACKCHANNEL 
B STATEMENT 
B STATEMENT 
A BACKCHANNEL 
B STATEMENT 
B STATEMENT 
A APPRECIATION 
but, uh, we're to the point now where our 
financial income is enough that we can consider 
putting some away - 
Uh-huh. / 
- for college, / 
so we are going to be starting a regular payroll 
deduction - 
Urn. / 
- -  in the fall / 
and then the money that I will be making this 
summer we'll be putting away for the college 
fund. 
Urn. Sounds good. 
Turn Exits and Abandoned Utterances. Abandoned utterances are those that the speaker 
breaks off without finishing, and are followed by a restart. Turn exits resemble aban- 
doned utterances in that they are often syntactically broken off, but they are used 
345 
Computational Linguistics Volume 26, Number 3 
Table 3 
Most common realizations of backchannels in Switchboard. 
Frequency Form Frequency Form Frequency Form 
38% uh-huh 2% yes 1% sure 
34% yeah 2% okay 1.% um 
9% right 2% oh yeah 1% huh-uh 
3% oh 1% huh 1% uh 
mainly as a way of passing speakership to the other speaker. Turn exits tend to be 
single words, often so or or. 
Speaker Dialogue Act Utterance 
A STATEMENT we're from, uh, I 'm from Ohio / 
A STATEMENT and my wife's from Florida / 
A TURN-ExIT SO, -/  
B BACKCHANNEL Uh-huh./ 
A HEDGE 
A ABANDONED 
A STATEMENT 
so, I don't know, / 
it's Klipsmack>, - / 
I 'm glad it's not the kind of problem I have to 
come up with an answer to because it's not - 
Answers and Agreements. YES-ANSWERS include yes, yeah, yep, uh-huh, and other varia- 
tions on yes, when they are acting as an answer to a YES-NO-QUESTION or DECLARA- 
TWE-0UESTION. Similarly, we also coded NO-ANSWERS. Detecting ANSWERS can help 
tell us that the previous utterance was a YES-NO-QUESTION. Answers are also seman- 
tically significant since they are likely to contain new information. 
AGREEMENT/ACCEPT, REJECT, and MAYBE/ACCEPT-PART all mark the degree 
to which a speaker accepts ome previous proposal, plan, opinion, or statement. The 
most common of these are the AGREEMENT/AccEPTS. These are very often yes or yeah, 
so they look a lot like ANSWERS. But where ANSWERS follow questions, AGREEMENTS 
often follow opinions or proposals, so distinguishing these can be important for the 
discourse. 
3. Hidden Markov Modeling of Dialogue 
We will now describe the mathematical  nd computational  f ramework used in our 
study. Our goal is to perform DA classification and other tasks using a probabilis- 
tic formulation, giving us a principled approach for combining multiple knowledge 
sources (using the laws of probability), as well as the ability to derive model  parame- 
ters automatical ly from a corpus, using statistical inference techniques. 
Given all available evidence E about a conversation, the goal is to find the DA 
sequence U that has the highest posterior probabil ity P(UIE ) given that evidence. 
Apply ing Bayes' rule we get 
U* = argmaxP(UIE ) 
U 
P(U)P(ElU) = argmax u P(E) 
= argmaxP(U)P(ElU) (1) 
U 
Here P(U) represents the prior probabil ity of a DA sequence, and P(EIU ) is the like- 
346 
Stolcke t al. Dialogue Act Modeling 
Table 4 
Summary of random variables used in dialogue modeling. 
(Speaker labels are introduced in Section 4.) 
Symbol Meaning 
U 
E 
F 
A 
W 
T 
sequence of DA labels 
evidence (complete speech signal) 
prosodic evidence 
acoustic evidence (spectral features used in ASR) 
sequence of words 
speakers labels 
lihood of U given the evidence. The likelihood is usually much more straightforward 
to model than the posterior itself. This has to do with the fact that our models are 
generative or causal in nature, i.e., they describe how the evidence is produced by the 
underlying DA sequence U. 
Estimating P (U) requires building a probabilistic discourse grammar, i.e., a statisti- 
cal model of DA sequences. This can be done using familiar techniques from language 
modeling for speech recognition, although the sequenced objects in this case are DA 
labels rather than words; discourse grammars will be discussed in detail in Section 4. 
3.1 Dialogue Act Likelihoods 
The computation of likelihoods P(EIU ) depends on the types of evidence used. In our 
experiments we used the following sources of evidence, ither alone or in combination: 
Transcribed words: The likelihoods used in Equation 1 are P(WIU ), where W 
refers to the true (hand-transcribed) words spoken in a conversation. 
Recognized words: The evidence consists of recognizer acoustics A, and we seek 
to compute P(A I U). As described later, this involves considering multiple 
alternative recognized word sequences. 
Prosodic features- Evidence is given by the acoustic features F capturing various 
aspects of pitch, duration, energy, etc., of the speech signal; the associated 
likelihoods are P(F I U). 
For ease of reference, all random variables used here are summarized in Table 4. 
The same variables are used with subscripts to refer to individual utterances. For 
example, Wi is the word transcription of the ith utterance within a conversation ( ot 
the ith word). 
To make both the modeling and the search for the best DA sequence feasible, we 
further equire that our likelihood models are decomposable byutterance. This means 
that the likelihood given a complete conversation can be factored into likelihoods 
given the individual utterances. We use Ui for the ith DA label in the sequence U, 
i.e., U = (U1 . . . . .  Ui,..., Un), where n is the number of utterances in a conversation. 
In addition, we use Ei for that portion of the evidence that corresponds to the ith 
utterance, .g., the words or the prosody of the ith utterance. Decomposability of the 
likelihood means that 
P(EIU) = P(E11 U1).... .  P(En \[Un) (2) 
Applied separately to the three types of evidence Ai, Wi, and Fi mentioned above, 
it is clear that this assumption is not strictly true. For example, speakers tend to reuse 
347 
Computational Linguistics Volume 26, Number 3 
E1 Ei E.  
T T T 
<start> , U1 , . . .  ~ Ui ) . . . - - - *  Un 
Figure 1 
The discourse HMM as Bayes network. 
<end> 
words found earlier in the conversation (Fowler and Housum 1987) and an answer 
might actually be relevant o the question before it, violating the independence of the 
P(WilUi). Similarly, speakers adjust their pitch or volume over time, e.g., to the con- 
versation partner or because of the structure of the discourse (Menn and Boyce 1982), 
violating the independence of the P(FilUi). As in other areas of statistical modeling, 
we count on the fact that these violations are small compared to the properties actually 
modeled, namely, the dependence of Ei on Ui. 
3.2 Markov  Mode l ing  
Returning to the prior distribution of DA sequences P(U), it is convenient to make 
certain independence assumptions here, too. In particular, we assume that the prior 
distribution of U is Markovian, i.e., that each Ui depends only on a fixed number k of 
preceding DA labels: 
P(U i lU l ,  . . . ,  U i -1 )  ~- P (U i lU i -k  . . . . .  Ui -1 )  (3) 
(k is the order of the Markov process describing U). The n-gram-based discourse gram- 
mars we used have this property. As described later, k = 1 is a very good choice, i.e., 
conditioning on the DA types more than one removed from the current one does not 
improve the quality of the model by much, at least with the amount of data available 
in our experiments. 
The importance of the Markov assumption for the discourse grammar is that 
we can now view the whole system of discourse grammar and local utterance-based 
likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986). 
The HMM states correspond to DAs, observations correspond to utterances, transition 
probabilities are given by the discourse grammar (see Section 4), and observation 
probabilities are given by the local likelihoods P(Eil Ui). 
We can represent the dependency structure (as well as the implied conditional 
independences) as a special case of Bayesian belief network (Pearl 1988). Figure 1 
shows the variables in the resulting HMM with directed edges representing conditional 
dependence. To keep things simple, a first-order HMM (bigram discourse grammar) 
is assumed. 
3.3 D ia logue  Act  Decod ing  
The HMM representation allows us to use efficient dynamic programming algorithms 
to compute relevant aspects of the model, such as 
? the most probable DA sequence (the Viterbi algorithm) 
? the posterior probability of various DAs for a given utterance, after 
considering all the evidence (the forward-backward algorithm) 
The Viterbi algorithm for HMMs (Viterbi 1967) finds the globally most probable 
state sequence. When applied to a discourse model with locally decomposable ike- 
lihoods and Markovian discourse grammar, it will therefore find precisely the DA 
348 
Stolcke et al Dialogue Act Modeling 
sequence with the highest posterior probability: 
U* = argmaxP(UIE ) (4) 
u 
The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is 
fundamentally the same as the standard probabilistic approaches to speech recognition 
(Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988). It maximizes the prob- 
ability of getting the entire DA sequence correct, but it does not necessarily find the 
DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995). To 
minimize the total number of utterance labeling errors, we need to maximize the prob- 
ability of getting each DA label correct individually, i.e., we need to maximize P(UilE) 
for each i = 1 . . . . .  n. We can compute the per-utterance posterior DA probabilities by 
summing: 
P(u\[E) = E P(UIE) (5) 
U: Ui=u 
where the summation is over all sequences U whose ith element matches the label in 
question. The summation is efficiently carried out by the forward-backward algorithm 
for HMMs (Baum et al 1970). 3
For zeroth-order (unigram) discourse grammars, Viterbi decoding and forward- 
backward decoding necessarily ield the same results. However, for higher-order 
discourse grammars we found that forward-backward decoding consistently gives 
slightly (up to 1% absolute) better accuracies, as expected. Therefore, we used this 
method throughout. 
The formulation presented here, as well as all our experiments, uses the entire 
conversation as evidence for DA classification. Obviously, this is possible only during 
off-line processing, when the full conversation is available. Our paradigm thus follows 
historical practice in the Switchboard omain, where the goal is typically the off-line 
processing (e.g., automatic transcription, speaker identification, indexing, archival) of 
entire previously recorded conversations. However, the HMM formulation used here 
also supports computing posterior DA probabilities based on partial evidence, e.g., 
using only the utterances preceding the current one, as would be required for on-line 
processing. 
4. Discourse Grammars 
The statistical discourse grammar models the prior probabilities P(U) of DA sequences. 
In the case of conversations for which the identities of the speakers are known (as 
in Switchboard), the discourse grammar should also model turn-taking behavior. A 
straightforward approach is to model sequences of pairs (Ui, Ti) where Ui is the DA 
label and Ti represents he speaker. We are not trying to model speaker idiosyncrasies, 
so conversants are arbitrarily identified as A or B, and the model is made symmetric 
with respect o the choice of sides (e.g., by replicating the training sequences with 
sides switched). Our discourse grammars thus had a vocabulary of 42 x 2 = 84 labels, 
plus tags for the beginning and end of conversations. For example, the second DA tag 
in Table 1 would be predicted by a trigram discourse grammar using the fact that the 
same speaker previously uttered a YES-NO-QUESTION, which in turn was preceded by 
the start-of-conversation. 
3 We note in passing that he Viterbi and Baum algorithms have quivalent formulations in the Bayes 
network framework (Pearl 1988). The HMM terminology was chosen here mainly for historical reasons. 
349 
Computational Linguistics Volume 26, Number 3 
Table 5 
Perplexities of DAs with and without urn 
information. 
Discourse Grammar P(U) P(U, T) P(UIT )
None 42 84 42 
Unigram 11.0 18.5 9.0 
Bigram 7.9 10.4 5.1 
Trigram 7.5 9.8 4.8 
4.1 N-gram Discourse Mode ls  
A computationally convenient type of discourse grammar is an n-gram model based on 
DA tags, as it allows efficient decoding in the HMM framework. We trained standard 
backoff n-gram models (Katz 1987), using the frequency smoothing approach of Witten 
and Bell (1991). Models of various orders were compared by their perplexities, i.e., 
the average number of choices the model predicts for each tag, conditioned on the 
preceding tags. 
Table 5 shows perplexities for three types of models: P(U), the DAs alone; P(U, T), 
the combined DA/speaker ID sequence; and P(UIT ), the DAs conditioned on known 
speaker IDs (appropriate for the Switchboard task). As expected, we see an improve- 
ment (decreasing perplexities) for increasing n-gram order. However, the incremental 
gain of a trigram is small, and higher-order models did not prove useful. (This ob- 
servation, initially based on perplexity, is confirmed by the DA tagging experiments 
reported in Section 5.) Comparing P(U) and P(U\[T), we see that speaker identity adds 
substantial information, especially for higher-order models. 
The relatively small improvements from higher-order models could be a result of 
lack of training data, or of an inherent independence of DAs from DAs further re- 
moved. The near-optimality of the bigram discourse grammar is plausible given con- 
versation analysis accounts of discourse structure in terms of adjacency pairs (Schegloff 
1968; Sacks, Schegloff, and Jefferson 1974). Inspection of bigram probabilities estimated 
from our data revealed that conventional djacency pairs receive high probabilities, as 
expected. For example, 30% of YES-NO-QUESTIONS are followed by YES-ANSWERS, 
14% by NO-ANSWERS (confirming that the latter are dispreferred). COMMANDS are fol- 
lowed by AGREEMENTS in 23% of the cases, and STATEMENTS elicit BACKCHANNELS 
in 26% of all cases. 
4.2 Other Discourse Mode ls  
We also investigated non-n-gram discourse models, based on various language model- 
ing techniques known from speech recognition. One motivation for alternative models 
is that n-grams enforce a one-dimensional representation  DA sequences, whereas 
we saw above that the event space is really multidimensional (DA label and speaker 
labels). Another motivation is that n-grams fail to model long-distance dependencies, 
such as the fact that speakers may tend to repeat certain DAs or patterns throughout 
the conversation. 
The first alternative approach was a standard cache model (Kuhn and de Mori 
1990), which boosts the probabilities of previously observed unigrams and bigrams, on 
the theory that tokens tend to repeat hemselves over longer distances. However, this 
does not seem to be true for DA sequences in our corpus, as the cache model showed 
no improvement over the standard N-gram. This result is somewhat surprising since 
unigram dialogue grammars are able to detect speaker gender with 63% accuracy (over 
350 
Stolcke t al. Dialogue Act Modeling 
a 50% baseline) on Switchboard (Ries 1999b), indicating that there are global variables 
in the DA distribution that could potentially be exploited by a cache dialogue grammar. 
Clearly, dialogue grammar adaptation needs further esearch. 
Second, we built a discourse grammar that incorporated constraints on DA se- 
quences in a nonhierarchical way, using maximum entropy (ME) estimation (Berger, 
Della Pietra, and Della Pietra 1996). The choice of features was informed by similar 
ones commonly used in statistical language models, as well our general intuitions 
about potentially information-bearing elements in the discourse context. Thus, the 
model was designed so that the current DA label was constrained by features uch as 
unigram statistics, the previous DA and the DA once removed, DAs occurring within a 
window in the past, and whether the previous utterance was by the same speaker. We 
found, however, that an ME model using n-gram constraints performed only slightly 
better than a corresponding backoff n-gram. 
Additional constraints such as DA triggers, distance-1 bigrams, separate ncoding 
of speaker change and bigrams to the last DA on the same/other channel did not 
improve relative to the trigram model. The ME model thus confirms the adequacy of 
the backoff n-gram approach, and leads us to conclude that DA sequences, at least 
in the Switchboard omain, are mostly characterized by local interactions, and thus 
modeled well by low-order n-gram statistics for this task. For more structured tasks this 
situation might be different. However, we have found no further exploitable structure. 
5. Dialogue Act Classification 
We now describe in more detail how the knowledge sources of words and prosody 
are modeled, and what automatic DA labeling results were obtained using each of the 
knowledge sources in turn. Finally, we present results for a combination of all knowl- 
edge sources. DA labeling accuracy results hould be compared to a baseline (chance) 
accuracy of 35%, the relative frequency of the most frequent DA type (STATEMENT) in 
our test set. 4 
5.1 Dialogue Act Classification Using Words 
DA classification using words is based on the observation that different DAs use 
distinctive word strings. It is known that certain cue words and phrases (Hirschberg 
and Litman 1993) can serve as explicit indicators of discourse structure. Similarly, 
we find distinctive correlations between certain phrases and DA types. For example, 
92.4% of the uh-huh's occur in BACKCHANNELS, and 88.4% of the trigrams "<start> 
do you" occur in YES-NO-QUESTIONS. To leverage this information source, without 
hand-coding knowledge about which words are indicative of which DAs, we will use 
statistical language models that model the full word sequences associated with each 
DA type. 
5.1.1 Classification from True Words. Assuming that the true (hand-transcribed) words 
of utterances are given as evidence, we can compute word-based likelihoods P(WIU ) 
in a straightforward way, by building a statistical language model for each of the 42 
DAs. All DAs of a particular type found in the training corpus were pooled, and 
a DA-specific trigram model was estimated using standard techniques (Katz backoff 
\[Katz 1987\] with Witten-Bell discounting \[Witten and Bell 1991\]). 
4 The frequency of STATEMENTS across all labeled ata was slightly different, cf. Table 2. 
351 
Computational Linguistics Volume 26, Number 3 
A1 Ai An 
T T 
wl wi w.  
T T T 
<start> ~ U1 ~ . . . .  ~ Ui ~ . . . .  Un ~ <end> 
Figure 2 
Modified Bayes network including word hypotheses and recognizer acoustics. 
5.1.2 Classification from Recognized Words. For fully automatic DA classification, 
the above approach is only a partial solution, since we are not yet able to recognize 
words in spontaneous speech with perfect accuracy. A standard approach is to use 
the 1-best hypothesis from the speech recognizer in place of the true word transcripts. 
While conceptually simple and convenient, his method will not make optimal use of 
all the information in the recognizer, which in fact maintains multiple hypotheses as 
well as their relative plausibilities. 
A more thorough use of recognized speech can be derived as follows. The classifi- 
cation framework is modified such that the recognizer's acoustic information (spectral 
features) A appear as the evidence. We compute P(A\[U) by decomposing it into an 
acoustic likelihood P(A\]W) and a word-based likelihood P(W\[ U), and summing over 
all word sequences: 
P(AlU) = ~-~ P(AIW, U)P(WIU) 
w 
= ~P(A IW)P(W\ [U  ) 
w 
(6) 
The second line is justified under the assumption that the recognizer acoustics (typ- 
ically, cepstral coefficients) are invariant o DA type once the words are fixed. Note 
that this is another approximation i our modeling. For example, different DAs with 
common words may be realized by different word pronunciations. Figure 2 shows the 
Bayes network resulting from modeling recognizer acoustics through word hypothe- 
ses under this independence assumption; note the added Wi variables (that have to 
be summed over) in comparison to Figure 1. 
The acoustic likelihoods P(A\[W) correspond to the acoustic scores the recognizer 
outputs for every hypothesized word sequence W. The summation over all W must 
be approximated; in our experiments we summed over the (up to) 2,500 best hypothe- 
ses generated by the recognizer for each utterance. Care must be taken to scale the 
recognizer acoustic scores properly, i.e., to exponentiate he recognizer acoustic scores 
by 1/~, where A is the language model weight of the recognizer, s 
5 In a standard recognizer the total og score of a hypothesis Wi is computed as 
logP(AdWi ) + )~ logP(Wi) - I~\]Wi\], 
where \[Wi\] is the number of words in the hypothesis, and both A and/~ are parameters optimized to 
minimize the word error rate. The word insertion penalty/~ represents a correction to the language 
model that allows balancing insertion and deletion errors. The language model weight ,~ compensates 
for acoustic score variances that are effectively too large due to severe independence assumptions in 
the recognizer acoustic model. According to this rationale, it is more appropriate o divide all score 
components by ),. Thus, in all our experiments, we computed a summand in Equation 6whose 
352 
Stolcke t al. Dialogue Act Modeling 
Table 6 
DA classification accuracies (in %) from transcribed and recognized 
words (chance = 35%). 
Discourse Grammar True Recognized Relative Error Increase 
None 54.3 42.8 25.2% 
Unigram 68.2 61.8 20.1% 
Bigram 70.6 64.3 21.4% 
Trigram 71.0 64.8 21.4% 
5.1.3 Results. Table 6 shows DA classification accuracies obtained by combining the 
word- and recognizer-based likelihoods with the n-gram discourse grammars de- 
scribed earlier. The best accuracy obtained from transcribed words, 71%, is encour- 
aging given a comparable human performance of84% (the interlabeler agreement, see 
Section 2.2). We observe about a 21% relative increase in classification error when us- 
ing recognizer words; this is remarkably small considering that the speech recognizer 
used had a word error rate of 41% on the test set. 
We also compared the n-best DA classification approach to the more straightfor- 
ward 1-best approach. In this experiment, only the single best recognizer hypothesis 
is used, effectively treating it as the true word string. The 1-best method increased 
classification error by about 7% relative to the n-best algorithm (61.5% accuracy with 
a bigram discourse grammar). 
5.2 Dialogue Act Classification Using Prosody 
We also investigated prosodic information, i.e., information i dependent of the words 
as well as the standard recognizer acoustics. Prosody is important for DA recogni- 
tion for two reasons. First, as we saw earlier, word-based classification suffers from 
recognition errors. Second, some utterances are inherently ambiguous based on words 
alone. For example, some YES-NO-QUESTiONS have word sequences identical to those 
of STATEMENTS, but can often be distinguished by their final F0 rise. 
A detailed study aimed at automatic prosodic lassification of DAs in the Switch- 
board domain is available in a companion paper (Shriberg et al 1998). Here we investi- 
gate the interaction of prosodic models with the dialogue grammar and the word-based 
DA models discussed above. We also touch briefly on alternative machine learning 
models for prosodic features. 
5.2.1 Prosodic Features. Prosodic DA classification was based on a large set of fea- 
tures computed automatically from the waveform, without reference to word or phone 
information. The features can be broadly grouped as referring to duration (e.g., utter- 
ance duration, with and without pauses), pauses (e.g., total and mean of nonspeech 
regions exceeding 100 ms), pitch (e.g., mean and range of F0 over utterance, slope of 
F0 regression line), energy (e.g., mean and range of RMS energy, same for signal-to- 
logarithm was 
1 logP(Ai\]Wi) + logP(WilUi) - ~lWil. -d 
We found this approach to give better esults than the standard multiplication of logP(W) by ,L Note 
that for selecting the best hypothesis in a recognizer only the relative magnitudes of the score weights 
matter; however, for the summation in Equation 6 the absolute values become important. The 
parameter values for )~ and # were those used by the standard recognizer; they were not specifically 
optimized for the DA classification task. 
353 
Computational Linguistics Volume 26, Number 3 
~ 23.403 
~ an utt < 0 .3" /~U >= 0.3"/17 
Figure 3 
Decision tree for the classification of BACKCHANNELS (B) and AGREEMENTS (A). Each node is 
labeled with the majority class for that node, as well as the posterior probabilities of the two 
classes. The following features are queried in the tree: number of frames in continuous (> 1 s) 
speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration 
excluding pauses > 100 ms (ling_dur_minus_minlOpause), andmean signal-to-noise ratio 
(snr_mean_utt ). 
noise ratio \[SNR\]), speaking rate (based on the "enrate" measure of Morgan, Fosler, 
and Mirghafori \[1997\]), and gender (of both speaker and listener). In the case of ut- 
terance duration, the measure correlates both with length in words and with overall 
speaking rate. The gender feature that classified speakers as either male or female was 
used to test for potential inadequacies in F0 normalizations. Where appropriate, we 
included both raw features and values normalized by utterance and/or  conversation. 
We also included features that are the output of the pitch accent and boundary tone 
event detector of Taylor (2000) (e.g., the number of pitch accents in the utterance). A 
complete description of prosodic features and an analysis of their usage in our models 
can be found in Shriberg et al (1998). 
5.2.2 Prosodic Decision Trees. For our Prosodic classifiers, we used CART-style deci- 
sion trees (Breiman et al 1984). Decision trees allow the combination of discrete and 
continuous features, and can be inspected to help in understanding the role of different 
features and feature combinations. 
To illustrate one area in which prosody could aid our classification task, we applied 
trees to DA classifications known to be ambiguous from words alone. One frequent 
example in our corpus was the distinction between BACKCHANNELS and AGREEMENTS 
(see Table 2), which share terms such as right and yeah. As shown in Figure 3, a prosodic 
tree trained on this task revealed that agreements have consistently longer durations 
and greater energy (as reflected by the SNR measure) than do backchannels. 
354 
Stolcke t al. Dialogue Act Modeling 
Table 7 
DA classification using prosodic 
decision trees (chance = 35%). 
Discourse Grammar Accuracy (%) 
None 38.9 
Unigram 48.3 
Bigram 49.7 
The HMM framework requires that we compute prosodic likelihoods of the form 
P(FilUi) for each utterance Ui and associated prosodic feature values Fi. We have 
the apparent difficulty that decision trees (as well as other classifiers, such as neural 
networks) give estimates for the posterior probabilities, P(Ui\[Fi). The problem can be 
overcome by applying Bayes' rule locally: 
P(Ui) t rue)  
(7) 
Note that P(Fi) does not depend on Ui and can be treated as a constant for the purpose 
of DA classification. A quantity proportional to the required likelihood can therefore 
be obtained either by dividing the posterior tree probability by the prior P(Ui), 6 or by 
training the tree on a uniform prior distribution of DA types. We chose the second 
approach, downsampling our training data to equate DA proportions. This also coun- 
teracts a common problem with tree classifiers trained on very skewed distributions 
of target classes, i.e., that low-frequency classes are not modeled in sufficient detail 
because the majority class dominates the tree-growing objective hznction. 
5.2.3 Results with Dec is ion Trees. As a preliminary experiment to test the integra- 
tion of prosody with other knowledge sources, we trained a single tree to discriminate 
among the five most frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABAN- 
DONED, and AGREEMENT, totaling 79% of the data) and an Other category comprising 
all remaining DA types. The decision tree was trained on a downsampled training 
subset containing equal proportions of these six DA classes. The tree achieved a clas- 
sification accuracy of 45.4% on an independent test set with the same uniform six-class 
distribution. The chance accuracy on this set is 16.6%, so the tree clearly extracts useful 
information from the prosodic features. 
We then used the decision tree posteriors as scaled DA likelihoods in the dialogue 
model HMM, combining it with various n-gram dialogue grammars for testing on our 
full standard test set. For the purpose of model integration, the likelihoods of the Other 
class were assigned to all DA types comprised by that class. As shown in Table 7, the 
tree with dialogue grammar performs ignificantly better than chance on the raw DA 
distribution, although not as well as the word-based methods (cf. Table 6). 
5.2.4 Neural  Network  Classifiers. Although we chose to use decision trees as prosodic 
classifiers for their relative ase of inspection, we might have used any suitable proba- 
bilistic classifier, i.e., any model that estimates the posterior probabilities of DAs given 
the prosodic features. We conducted preliminary experiments o assess how neural 
6 Bourlard and Morgan (1993) use this approach tointegrate neural network phonetic models in a 
speech recognizer. 
355 
Computational Linguistics Volume 26, Number 3 
Table 8 
Performance ofvarious prosodic neural network classifiers on 
an equal-priors, ix-class DA set (chance = 16.6%). 
Network Architecture Accuracy (%) 
Decision tree 45.4 
No hidden layer, linear output function 44.6 
No hidden layer, softmax output function 46.0 
40-unit hidden layer, softmax output function 46.0 
networks compare to decision trees for the type of data studied here. Neural networks 
are worth investigating since they offer potential advantages over decision trees. They 
can learn decision surfaces that lie at an angle to the axes of the input feature space, 
unlike standard CART trees, which always split continuous features on one dimen- 
sion at a time. The response function of neural networks is continuous (smooth) at 
the decision boundaries, allowing them to avoid hard decisions and the complete 
fragmentation f data associated with decision tree questions. 
Most important, however, related work (Ries 1999a) indicated that similarly struc- 
tured networks are superior classifiers if the input features are words and are therefore 
a plug-in replacement for the language model classifiers described in this paper. Neural 
networks are therefore a good candidate for a jointly optimized classifier of prosodic 
and word-level information since one can show that they are a generalization of the 
integration approach used here. 
We tested various neural network models on the same six-class downsampled 
data used for decision tree training, using a variety of network architectures and out- 
put layer functions. The results are summarized in Table 8, along with the baseline 
result obtained with the decision tree model. Based on these experiments, a softmax 
network (Bridle 1990) without hidden units resulted in only a slight improvement 
over the decision tree. A network with hidden units did not afford any additional 
advantage, ven after we optimized the number of hidden units, indicating that com- 
plex combinations of features (as far as the network could learn them) do not predict 
DAs better than linear combinations of input features. While we believe alternative 
classifier architectures should be investigated further as prosodic models, the results 
so far seem to confirm our choice of decision trees as a model class that gives close to 
optimal performance for this task. 
5.2.5 Intonation Event Likel ihoods. An alternative way to compute prosodically based 
DA likelihoods uses pitch accents and boundary phrases (Taylor et al 1997). The ap- 
proach relies on the intuition that different utterance types are characterized by dif- 
ferent intonational "tunes" (Kowtko 1996), and has been successfully applied to the 
classification ofmove types in the DCIEM Map Task corpus (Wright and Taylor 1997). 
The system detects equences ofdistinctive pitch patterns by training one continuous- 
density HMM for each DA type. Unfortunately, the event classification accuracy on 
the Switchboard corpus was considerably poorer than in the Map Task domain, and 
DA recognition results when coupled with a discourse grammar were substantially 
worse than with decision trees. The approach could prove valuable in the future, 
however, if the intonation event detector can be made more robust to corpora like 
OURS. 
356 
Stolcke et al Dialogue Act Modeling 
A1 Ai An 
T 1 1 
Wl Wi W,, 
T T t 
<start> - , 0"1 , . . .---* U/ , ... ~ Un , <end> 
1 1 ,t 
F1 Fi G 
Figure 4 
Bayes network for discourse HMM incorporating both word recognition and prosodic features. 
5.3 Using Multiple Knowledge Sources 
As mentioned earlier, we expect improved performance from combining word and 
prosodic information. Combining these knowledge sources requires estimating a com- 
bined likelihood P(Ai, Fi\[Ui) for each utterance. The simplest approach is to assume 
that the two types of acoustic observations (recognizer acoustics and prosodic features) 
are approximately conditionally independent once Ui is given: 
P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) 
~, P(ai, Wi\[Ui)P(FilUi) (8) 
Since the recognizer acoustics are modeled by way of their dependence on words, it 
is particularly important o avoid using prosodic features that are directly correlated 
with word identities, or features that are also modeled by the discourse grammars, 
such as utterance position relative to turn changes. Figure 4 depicts the Bayes network 
incorporating evidence from both word recognition and prosodic features. 
One important respect in which the independence assumption is violated is in the 
modeling of utterance length. While utterance length itself is not a prosodic feature, 
it is an important feature to condition on when examining prosodic characteristics 
of utterances, and is thus best included in the decision tree. Utterance length is cap- 
tured directly by the tree using various duration measures, while the DA-specific 
LMs encode the average number of words per utterance indirectly through n-gram 
parameters, but still accurately enough to violate independence in a significant way 
(Finke et al 1998). As discussed in Section 8, this problem is best addressed by joint 
lexical-prosodic models. 
We need to allow for the fact that the models combined in Equation 8 give es- 
timates of differing qualities. Therefore, we introduce an exponential weight a on 
P(Fi\[Ui) that controls the contribution of the prosodic likelihood to the overall likeli- 
hood. Finally, a second exponential weight fl on the combined likelihood controls its 
dynamic range relative to the discourse grammar scores, partially compensating for 
any correlation between the two likelihoods. The revised combined likelihood estimate 
thus becomes: 
P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi\[Ui)~}  (9) 
In our experiments, the parameters a and fl were optimized using twofold jackknifing. 
The test data was split roughly in half (without speaker overlap), each half was used 
to separately optimize the parameters, and the best values were then tested on the 
respective other half. The reported results are from the aggregate outcome on the two 
test set halves. 
357 
Computational Linguistics Volume 26, Number 3 
Table 9 
Combined utterance classification accuracies (chance = 
35%). The first two columns correspond to Tables 7 
and 6, respectively. 
Discourse Grammar Accuracy (%) 
Prosody Recognizer Combined 
None 38.9 42.8 56.5 
Unigram 48.3 61.8 62.4 
Bigram 49.7 64.3 65.0 
Table 10 
Accuracy (in %) for individual 
subtasks, using uniform priors 
and combined models for two 
(chance = 50%). 
Classification Task True Words Recognized Words 
Knowledge Source 
QUESTIONS/STATEMENTS 
prosody only 76.0 76.0 
words only 85.9 75.4 
words+prosody 87.6 79.8 
AGREEMENTS / BACKCHANNELS 
prosody only 72.9 72.9 
words only 81.0 78.2 
words+prosody 84.7 81.7 
5.3.1 Results. In this experiment we combined the acoustic n-best likelihoods based 
on recognized words with the Top-5 tree classifier mentioned in Section 5.2.3. Results 
are summarized in Table 9. 
As shown, the combined classifier presents a slight improvement over the rec- 
ognizer-based classifier, The experiment without discourse grammar indicates that 
the combined evidence is considerably stronger than either knowledge source alone, 
yet this improvement seems to be made largely redundant by the use of priors and 
the discourse grammar. For example, by definition DECLARATIVE-QUESTIONS are not 
marked by syntax (e.g., by subject-auxiliary inversion) and are thus confusable with 
STATEMENTS and OPINIONS. While prosody is expected to help disambiguate hese 
cases, the ambiguity can also be removed by examining the context of the utterance, 
e.g., by noticing that the following utterance is a YEs-ANswER or NO-ANSWER. 
5.3.2 Focused Classifications. To gain a better understanding of the potential for 
prosodic DA classification i dependent of the effects of discourse grammar and the 
skewed DA distribution i  Switchboard, we examined several binary DA classification 
tasks. The choice of tasks was motivated by an analysis of confusions committed by a 
purely word-based DA detector, which tends to mistake QUESTIONS for STATEMENTS, 
and BACKCHANNELS for AGREEMENTS (and vice versa). We tested aprosodic lassifier, 
a word-based classifier (with both transcribed and recognized words), and a combined 
classifier on these two tasks, downsampling the DA distribution to equate the class 
sizes in each case. Chance performance in all experiments i  therefore 50%. Results 
are summarized in Table 10. 
358 
Stolcke et al Dialogue Act Modeling 
As shown, the combined classifier was consistently more accurate than the classi- 
fier using words alone. Although the gain in accuracy was not statistically significant 
for the small recognizer test set because of a lack of power, replication for a larger 
hand-transcribed test set showed the gain to be highly significant for both subtasks 
by a Sign test, p < .001 and p < .0001 (one-tailed), respectively. Across these, as well 
as additional subtasks, the relative advantage of adding prosody was larger for recog- 
nized than for true words, suggesting that prosody is particularly helpful when word 
information is not perfect. 
6. Speech Recognition 
We now consider ways to use DA modeling to enhance automatic speech recognition 
(ASR). The intuition behind this approach is that discourse context constrains the 
choice of DAs for a given utterance, and the DA type in turn constrains the choice of 
words. The latter can then be leveraged for more accurate speech recognition. 
6.1 Integrating DA Modeling and ASR 
Constraints on the word sequences hypothesized by a recognizer are expressed prob- 
abilistically in the recognizer language model (LM). It provides the prior distribution 
P(Wi) for finding the a posteriori most probable hypothesized words for an utterance, 
given the acoustic evidence Ai (Bahl, Jelinek, and Mercer 1983): 7
W 7 = argmaxP(WilAi) 
wi 
P(Wi)P(AilWi) = argmax 
wi P(Ai) 
= argmaxP(Wi)P(AilWi) (10) 
wi 
The likelihoods P(AilWi) are estimated by the recognizer's acoustic model. In a stan- 
dard recognizer the language model P(Wi) is the same for all utterances; the idea here 
is to obtain better-quality LMs by conditioning on the DA type Ui, since presumably 
the word distributions differ depending on DA type. 
W7 -- argmaxP(WilAi, Ui) 
wi 
P( WilUi)P(AilWi, Ui) = argmax 
Wi P(AiIUi) 
argmaxP(WilUi)P(AirWi) (11) 
wi 
As before in the DA classification model, we tacitly assume that the words Wi depend 
only on the DA of the current utterance, and also that the acoustics are independent of
the DA type if the words are fixed. The DA-conditioned language models P(Wil Ui) are 
readily trained from DA-specific training data, much as we did for DA classification 
from words. 8 
7 Note the similarity of Equations 10 and 1. They are identical except for the fact that we are now 
operating at the level of an individual utterance, the evidence isgiven by the acoustics, and the targets 
are word hypotheses instead of DA hypotheses. 
8 In Equation 11 and elsewhere in this section we gloss over the issue of proper weighting of model 
probabilities, which is extremely important in practice. The approach explained in detail in footnote 5
applies here as well. 
359 
Computational Linguistics Volume 26, Number 3 
The problem with applying Equation 11, of course, is that the DA type Ui is 
generally not known (except maybe in applications where the user interface can be 
engineered to allow only one kind of DA for a given utterance). Therefore, we need 
to infer the likely DA types for each utterance, using available evidence E from the 
entire conversation. This leads to the following formulation: 
W~ = argmaxP(WilAi, E)
wi 
---- argmax ~-~ P(WilAi, Ui, E)P(UilE) 
Wi Ui 
argmax ~\[\] P( WiiAi, Ui)P( Ui\[E) 
W~ U~ 
(12) 
The last step in Equation 12 is justified because, as shown in Figures 1 and 4, the 
evidence E (acoustics, prosody, words) pertaining to utterances other than i can affect 
the current utterance only through its DA type Ui. 
We call this the mixture-of-posteriors approach, because it amounts to a mixture of 
the posterior distributions obtained from DA-specific speech recognizers (Equation 11), 
using the DA posteriors as weights. This approach is quite expensive, however, as it 
requires multiple full recognizer or rescoring passes of the input, one for each DA 
type. 
A more efficient, though mathematically ess accurate, solution can be obtained 
by combining uesses about the correct DA types directly at the level of the LM. We 
estimate the distribution of likely DA types for a given utterance using the entire 
conversation E as evidence, and then use a sentence-level mixture (Iyer, Ostendorf, 
and Rohlicek 1994) of DA-specific LMs in a single recognizer run. In other words, we 
replace P(WilUi) in Equation 11 with 
~_~ P(WilUi)P(Ui\]E), 
ui 
a weighted mixture of all DA-specific LMs. We call this the mixture-of-LMs ap- 
proach. In practice, we would first estimate DA posteriors for each utterance, us- 
ing the forward-backward algorithm and the models described in Section 5, and then 
rerecognize the conversation or rescore the recognizer output, using the new posterior- 
weighted mixture LM. Fortunately, as shown in the next section, the mixture-of-LMs 
approach seems to give results that are almost identical to (and as good as) the mixture- 
of-posteriors approach. 
6.2 Computational Structure of Mixture Modeling 
It is instructive to compare the expanded scoring formulas for the two DA mixture 
modeling approaches for ASK The mixture-of-posteriors approach yields 
P(WilAi, E) = ~ P(ailui) 
ui 
(13) 
whereas the mixture-of-LMs approach gives 
) P(A,Iw,) 
P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) 
360 
Stolcke t al. Dialogue Act Modeling 
Table 11 
Switchboard word recognition error rates and 
LM perplexities. 
Model WER (%) Perplexity 
Baseline 41.2 76.8 
1-best LM 41.0 69.3 
Mixture-of-posteriors 41.0 n/a 
Mixture-of-LMs 40.9 66.9 
Oracle LM 40.3 66.8 
We see that the second equation reduces to the first under the crude approximation 
P(Ai\] Ui) ~ P(Ai). In practice, the denominators are computed by summing the numer- 
ators over a finite number of word hypotheses Wi, so this difference translates into 
normalizing either after or before summing over DAs. When the normalization takes 
place as the final step it can be omitted for score maximization purposes; this shows 
why the mixture-of-LMs approach is less computationally expensive. 
6.3 Experiments and Results 
We tested both the mixture-of-posteriors and the mixture-of-LMs approaches on our 
Switchboard test set of 19 conversations. Instead of decoding the data from scratch 
using the modified models, we manipulated n-best lists consisting of up to 2,500 best 
hypotheses for each utterance. This approach is also convenient since both approaches 
require access to the full word string for hypothesis scoring; the overall model is no 
longer Markovian, and is therefore inconvenient to use in the first decoding stage, or 
even in lattice rescoring. 
The baseline for our experiments was obtained with a standard backoff trigram 
language model estimated from all available training data. The DA-specific language 
models were trained on word transcripts of all the training utterances of a given type, 
and then smoothed further by interpolating them with the baseline LM. Each DA- 
specific LM used its own interpolation weight, obtained by minimizing the perplexity 
of the interpolated model on held-out DA-specific training data. Note that this smooth- 
ing step is helpful when using the DA-specific LMs for word recognition, but not for 
DA classification, since it renders the DA-specific LMs less discriminative. 9 
Table 11 summarizes both the word error rates achieved with the various models 
and the perplexities of the corresponding LMs used in the rescoring (note that per- 
plexity is not meaningful in the mixture-of-posteriors approach). For comparison, we 
also included two additional models: the 'q-best LM" refers to always using the DA- 
specific LM corresponding to the most probable DA type for each utterance. It is thus 
an approximation to both mixture approaches where only the top DA is considered. 
Second, we included an "oracle LM," i.e., always using the LM that corresponds to 
the hand-labeled DA for each utterance. The purpose of this experiment was to give us 
an upper bound on the effectiveness of the mixture approaches, by assuming perfect 
DA recognition. 
It was somewhat disappointing that the word error rate (WER) improvement in
the oracle experiment was small (2.2% relative), even though statistically highly sig- 
nificant (p < .0001, one-tailed, according to a Sign test on matched utterance pairs). 
9 Indeed, during our DA classification experiments, wehad observed that smoothed DA-specific LMs 
yield lower classification accuracy. 
361 
Computational Linguistics Volume 26, Number 3 
Table 12 
Word error reductions through DA oracle, by DA type. 
Dialogue Act Baseline WER Oracle WER WER Reduction 
NO-ANSWER 29.4 11.8 -17.6 
BACKCHANNEL 25.9 18.6 -7.3 
BACKCHANNEL-QUESTION 15.2 9.1 -6.1 
ABANDONED/UNINTERPRETABLE 48.9 45.2 -3.7 
WH-QUESTION 38.4 34.9 -3.5 
YES-No-QUESTION 55.5 52.3 --3.2 
STATEMENT 42.0 41.5 --0.5 
OPINION 40.8 40.4 --0.4 
Other 8% 
onded/Uninterpretable 3% 
kchannel 3% 
as-No-Question 3% 
Statement 53% 
Dpinion 30% 
Figure 5 
Relative contributions to test set word counts by DA type. 
The WER reduction achieved with the mixture-of-LMs approach did not achieve sta- 
tistical significance (0.25 > p > 0.20). The 1-best DA and the two mixture models 
also did not differ significantly on this test set. In interpreting these results one must 
realize, however, that WER results depend on a complex combination of factors, most 
notably interaction between language models and the acoustic models. Since the ex- 
periments only varied the language models used in rescoring, it is also informative to 
compare the quality of these models as reflected by perplexity. On this measure, we 
see a substantial 13% (relative) reduction, which is achieved by both the oracle and 
the mixture-of-LMs. The perplexity reduction for the 1-best LM is only 9.8%, showing 
the advantage of the mixture approach. 
To better understand the lack of a more substantial reduction in word error, we an- 
alyzed the effect of the DA-conditioned rescoring on the individual DAs, i.e., grouping 
the test utterances by their true DA types. Table 12 shows the WER improvements for 
a few DA types, ordered by the magnitude of improvement achieved. As shown, all 
frequent DA types saw improvement, but the highest wins were observed for typically 
short DAs, such as ANSWERS and BACKCHANNELS. This is to be expected, as such DAs 
tend to be syntactically and lexically highly constrained. Furthermore, the distribution 
of number of words across DA types is very uneven (Figure 5). STATEMENTS and 
OPINIONS, the DA types dominating in both frequency and number of words (83% of 
total), see no more than 0.5% absolute improvement, thus explaining the small overall 
improvement. In hindsight, this is also not surprising, since the bulk of the training 
data for the baseline LM consists of these DAs, allowing only little improvement in
362 
Stolcke et al Dialogue Act Modeling 
the DA-specific LMs. A more detailed analysis of the effect of DA modeling on speech 
recognition errors can be found elsewhere (Van Ess-Dykema nd Ries 1998). 
In summary, our experiments confirmed that DA modeling can improve word 
recognition accuracy quite substantially in principle, at least for certain DA types, 
but that the skewed distribution of DAs (especially in terms of number of words per 
type) limits the usefulness of the approach on the Switchboard corpus. The benefits 
of DA modeling might therefore be more pronounced on corpora with more even 
DA distribution, as is typically the case for task-oriented ialogues. Task-oriented 
dialogues might also feature specific subtypes of general DA categories that might 
be constrained by discourse. Prior research on task-oriented dialogues ummarized in 
the next section, however, has also found only small reductions in WER (on the order 
of 1%). This suggests that even in task-oriented domains more research is needed to 
realize the potential of DA modeling for ASR. 
7. Prior and Related Work 
As indicated in the introduction, our work builds on a number of previous efforts 
in computational discourse modeling and automatic discourse processing, most of 
which occurred over the last half-decade. It is generally not possible to directly com- 
pare quantitative results because of vast differences in methodology, tag set, type and 
amount of training data, and, principally, assumptions made about what information 
is available for "free" (e.g., hand-transcribed versus automatically recognized words, 
or segmented versus unsegmented utterances). Thus, we will focus on the conceptual 
aspects of previous research efforts, and while we do offer a summary of previous 
quantitative results, these should be interpreted as informative datapoints only, and 
not as fair comparisons between algorithms. 
Previous research on DA modeling has generally focused on task-oriented ia- 
logue, with three tasks in particular garnering much of the research effort. The Map 
Task corpus (Anderson et al 1991; Bard et al 1995) consists of conversations between 
two speakers with slightly different maps of an imaginary territory. Their task is to 
help one speaker eproduce a route drawn only on the other speaker's map, all with- 
out being able to see each other's maps. Of the DA modeling algorithms described 
below, Taylor et al (1998) and Wright (1998) were based on Map Task. The VERBMO- 
BIL corpus consists of two-party scheduling dialogues. A number of the DA m6deling 
algorithms described below were developed for VERBMOBIL, including those of Mast 
et al (1996), Warnke et al (1997), Reithinger et al (1996), Reithinger and Klesen (1997), 
and Samuel, Carberry, and Vijay-Shanker (1998). The ATR Conference corpus is a sub- 
set of a larger ATR Dialogue database consisting of simulated dialogues between a
secretary and a questioner at international conferences. Researchers using this corpus 
include Nagata (1992), Nagata and Morimoto (1993, 1994), and Kita et al (1996). Ta- 
ble 13 shows the most commonly used versions of the tag sets from those three tasks. 
As discussed earlier, these domains differ from the Switchboard corpus in being 
task-oriented. Their tag sets are also generally smaller, but some of the same problems 
of balance occur. For example, in the Map Task domain, 33% of the words occur in 1 
of the 12 DAs 0NSTRUCT). Table 14 shows the approximate size of the corpora, the tag 
set, and tag estimation accuracy rates for various recent models of DA prediction. The 
results summarized in the table also illustrate the differences in inherent difficulty of 
the tasks. For example, the task of Warnke et al (1997) was to simultaneously segment 
and tag DAs, whereas the other results rely on a prior manual segmentation. Similarly, 
the task in Wright (1998) and in our study was to determine DA types from speech 
input, whereas work by others is based on hand-transcribed textual input. 
363 
Computational Linguistics Volume 26, Number 3 
Table 13 
Dialogue act tag sets used in three other extensively studied corpora. 
VERBMOBIL. These 18 high-level DAs used in VERBMOBIL-1 are 
abstracted over a total of 43 more specific DAs; most experiments on 
VERBMOBIL DAs use the set of 18 rather than 43. Examples are from 
Jekat et al (1995). 
Tag Example 
THANK 
GREET 
INTRODUCE 
BYE 
REQUEST~COMMENT 
SUGGEST 
REJECT 
ACCEPT 
REQUEST-SUGGEST 
INIT 
GIVE_REASON 
FEEDBACK 
DELIBERATE 
CONFIRM 
CLARIFY 
DIGRESS 
MOTIVATE 
GARBAGE 
Thanks 
Hello Dan 
It's me again 
Alright bye 
How does that look? 
from thirteenth through seventeenth June 
No Friday I'm booked all day 
Saturday sounds fine, 
What is a good day of the week for you? 
I wanted to make an appointment with you 
Because I have meetings all afternoon 
Okay 
Let me check my calendar here 
Okay, that would be wonderful 
Okay, do you mean Tuesday the 23rd? 
\[we could meet for lunch\] and eat lots of ice cream 
We should go to visit our subsidiary in Munich 
Oops, I- 
Maptask. The 12 DAs or "move types" used in Map Task. Examples are 
from Taylor et al (1998). 
Tag Example 
INSTRUCT 
EXPLAIN 
ALIGN 
CHECK 
QUERY-YN 
QUERY-W 
ACKNOWLEDGE 
CLARIFY 
REPLY-Y 
REPLY-N 
REPLY-W 
READY 
Go round, ehm horizontally underneath diamond mine 
I don't have a ravine 
Okay? 
So going down to Indian Country? 
Have you got the graveyard written down ? 
In where? 
Okay 
{you want to go. . .  diagonally} Diagonally down 
I do. 
No, I don't 
{And across to?} The pyramid. 
Okay 
ATR. The 9 DAs ("illocutionary force types") used in the ATR Dialogue 
database task; some later models used an extended set of 15 DAs. 
Examples are from the English translations given by Nagata (1992). 
Tag Example 
PHATIC 
EXPRESSIVE 
RESPONSE 
PROMISE 
REQUEST 
INFORM 
QUESTIONIP 
QUESTIONREF 
QUESTIONCONF 
Hello 
Thank you 
That's right 
I will send you a registration form 
Please go to Kitaooji station by subway 
We are not giving any discount his time 
Do you have the announcement of the conference ? 
What should I do? 
You have already transferred the registration fee, right ? 
364 
Stolcke et al D ia logue Act  Mode l ing  
,.0 
C 
~'~ o 
% > 
.~  to 
.~~ ~ 
g,..l 
m m ~ 
c~8 ,~.~ ~ 
c ~ Z 
~m 
.~  K 
, -~  
NS~ 
~ ~.~ 
?? l l  ~ ~  
v 
r--~ ~ , O', ?~ 
365 
Computational Linguistics Volume 26, Number 3 
The use of n-grams to model the probabilities of DA sequences, or to predict 
upcoming DAs on-line, has been proposed by many authors. It seems to have been 
first employed by Nagata (1992), and in follow-up papers by Nagata and Morimoto 
(1993, 1994) on the ATR Dialogue database. The model predicted upcoming DAs by 
using bigrams and trigrams conditioned on preceding DAs, trained on a corpus of 
2,722 DAs. Many others subsequently relied on and enhanced this n-grams-of-DAs 
approach, often by applying standard techniques from statistical language modeling. 
Reithinger et al (1996), for example, used deleted interpolation tosmooth the dialogue 
n-grams. Chu-Carroll (1998) uses knowledge of subdialogue structure to selectively 
skip previous DAs in choosing conditioning for DA prediction. 
Nagata and Morimoto (1993, 1994) may also have been the first to use word n- 
grams as a miniature grammar for DAs, to be used in improving speech recognition. 
The idea caught on very quickly: Suhm and Waibel (1994), Mast et aL (1996), Warnke 
et al (1997), Reithinger and Klesen (1997), and Taylor et al (1998) all use variants of 
backoff, interpolated, orclass n-gram language models to estimate DA likelihoods. Any 
kind of sufficiently powerful, trainable language model could perform this function, of 
course, and indeed Alexandersson and Reithinger (1997) propose using automatically 
learned stochastic context-free grammars. Jurafsky, Shriberg, Fox, and Curl (1998) show 
that the grammar of some DAs, such as appreciations, can be captured by finite-state 
automata over part-of-speech tags. 
N-gram models are likelihood models for DAs, i.e., they compute the conditional 
probabilities of the word sequence given the DA type. Word-based posterior probability 
estimators are also possible, although less common. Mast et al (1996) propose the use 
of semantic lassification trees, a kind of decision tree conditioned on word patterns 
as features. Finally, Ries (1999a) shows that neural networks using only unigram fea- 
tures can be superior to higher-order n-gram DA models. Warnke et al (1999) and 
Ohler, Harbeck, and Niemann (1999) use related discriminative training algorithms 
for language models. 
Woszczyna nd Waibel (1994) and Suhm and Waibel (1994), followed by Chu- 
Carroll (1998), seem to have been the first to note that such a combination of word 
and dialogue n-grams could be viewed as a dialogue HMM with word strings as 
the observations. (Indeed, with the exception of Samuel, Carberry, and Vijay-Shanker 
(1998), all models listed in Table 14 rely on some version of this HMM metaphor.) 
Some researchers explicitly used HMM induction techniques to infer dialogue gram- 
mars. Woszczyna nd Waibel (1994), for example, trained an ergodic HMM using 
expectation-maximization o model speech act sequencing. Kita et al (1996) made 
one of the few attempts at unsupervised iscovery of dialogue structure, where a 
finite-state grammar induction algorithm is used to find the topology of the dialogue 
grammar. 
Computational pproaches to prosodic modeling of DAs have aimed to auto- 
matically extract various prosodic parameters--such as duration, pitch, and energy 
patterns--from the speech signal (Yoshimura et al \[1996\]; Taylor et al \[1997\]; Kompe 
\[1997\], among others). Some approaches model F0 patterns with techniques such as 
vector quantization and Gaussian classifiers to help disambiguate utterance types. An 
extensive comparison of the prosodic DA modeling literature with our work can be 
found in Shriberg et al (1998). 
DA modeling has mostly been geared toward automatic DA classification, and 
much less work has been done on applying DA models to automatic speech recog- 
nition. Nagata and Morimoto (1994) suggest conditioning word language models on 
DAs to lower perplexity. Suhm and Waibel (1994) and Eckert, Gallwitz, and Niemann 
(1996) each condition a recognizer LM on left-to-right DA predictions and are able to 
366 
Stolcke et al Dialogue Act Modeling 
show reductions in word error rate of 1% on task-oriented corpora. Most similar to 
our own work, but still in a task-oriented omain, the work by Taylor et al (1998) 
combines DA likelihoods from prosodic models with those from 1-best recognition 
output o condition the recognizer LM, again achieving an absolute reduction in word 
error rate of 1%, as disappointing as the 0.3% improvement in our experiments. 
Related computational tasks beyond DA classification and speech recognition have 
received even less attention to date. We already mentioned Warnke et al (1997) and 
Finke et al (1998), who both showed that utterance segmentation a d classification can 
be integrated into a single search process. Fukada et al (1998) investigate augmenting 
DA tagging with more detailed semantic "concept" tags, as a preliminary step toward 
an interlingua-based dialogue translation system. Levin et al (1999) couple DA clas- 
sification with dialogue game classification; dialogue games are units above the DA 
level, i.e., short DA sequences such as question-answer pairs. 
All the work mentioned so far uses statistical models of various kinds. As we have 
shown here, such models offer some fundamental dvantages, uch as modularity and 
composability (e.g., of discourse grammars with DA models) and the ability to deal 
with noisy input (e.g., from a speech recognizer) in a principled way. However, many 
other classifier architectures are applicable to the tasks discussed, in particular to DA 
classification. A nonprobabilistic approach for DA labeling proposed by Samuel, Car- 
berry, and Vijay-Shanker (1998) is transformation-based l arning (Brill 1993). Finally 
it should be noted that there are other tasks with a mathematical structure similar to 
that of DA tagging, such as shallow parsing for natural anguage processing (Munk 
1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999), from which 
further techniques could be borrowed. 
How does the approach presented here differ from these various earlier models, 
particularly those based on HMMs? Apart from corpus and tag set differences, our 
approach differs primarily in that it generalizes the simple HMM approach to cope 
with new kinds of problems, based on the Bayes network representations depicted in 
Figures 2 and 4. For the DA classification task, our framework allows us to do classifi- 
cation given unreliable words (by marginalizing over the possible word strings corre- 
sponding to the acoustic input) and given nonlexical (e.g., prosodic) evidence. For the 
speech recognition task, the generalized model gives a clean probabilistic framework 
for conditioning word probabilities on the conversation context via the underlying DA 
structure. Unlike previous models that did not address peech recognition or relied 
only on an intuitive 1-best approximation, our model allows computation of the opti- 
mum word sequence by effectively summing over all possible DA sequences as well 
as all recognition hypotheses throughout the conversation, using evidence from both 
past and future. 
8. D iscuss ion and Issues for Future Research 
Our approach to dialogue modeling has two major components: tatistical dialogue 
grammars modeling the sequencing of DAs, and DA likelihood models expressing 
the local cues (both lexical and prosodic) for DAs. We made a number of significant 
simplifications to arrive at a computationally and statistically tractable formulation. 
In this formulation, DAs serve as the hinges that join the various model components, 
but also decouple these components through statistical independence assumptions. 
Conditional on the DAs, the observations across utterances are assumed to be inde- 
pendent, and evidence of different kinds from the same utterance (e.g., lexical and 
prosodic) is assumed to be independent. Finally, DA types themselves are assumed 
to be independent beyond a short span (corresponding to the order of the dialogue 
367 
Computational Linguistics Volume 26, Number 3 
n-gram). Further research within this framework can be characterized by which of 
these simplifications are addressed. 
Dialogue grammars for conversational speech need to be made more aware of the 
temporal properties of utterances. For example, we are currently not modeling the fact 
that utterances by the conversants may actually overlap (e.g., backchannels interrupt- 
ing an ongoing utterance). In addition, we should model more of the nonlocal aspects 
of discourse structure, despite our negative results so far. For example, a context-free 
discourse grammar could potentially account for the nested structures proposed in 
Grosz and Sidner (1986). 1? 
The standard n-gram models for DA discrimination with lexical cues are probably 
suboptimal for this task, simply because they are trained in the maximum likelihood 
framework, without explicitly optimizing discrimination between DA types. This may 
be overcome by using discriminative training procedures (Warnke et al 1999; Ohler, 
Harbeck, and Niemann 1999). Training neural networks directly with posterior prob- 
ability (Ries 1999a) seems to be a more principled approach and it also offers much 
easier integration with other knowledge sources. Prosodic features, for example, can 
simply be added to the lexical features, allowing the model to capture dependencies 
and redundancies across knowledge sources. Keyword-based techniques from the field 
of message classification should also be applicable here (Rose, Chang, and Lippmann 
1991). Eventually, it is desirable to integrate dialogue grammar, lexical, and prosodic 
cues into a single model, e.g., one that predicts the next DA based on DA history and 
all the local evidence. 
The study of automatically extracted prosodic features for DA modeling is likewise 
only in its infancy. Our preliminary experiments with neural networks have shown that 
small gains are obtainable with improved statistical modeling techniques. However, 
we believe that more progress can be made by improving the underlying features 
themselves, in terms of both better understanding of how speakers use them, and 
ways to reliably extract hem from data. 
Regarding the data itself, we saw that the distribution of DAs in our corpus limits 
the benefit of DA modeling for lower-level processing, in particular speech recognition. 
The reason for the skewed distribution was in the nature of the task (or lack thereof) in 
Switchboard. It remains to be seen if more fine-grained DA distinctions can be made 
reliably in this corpus. However, it should be noted that the DA definitions are really 
arbitrary as far as tasks other than DA labeling are concerned. This suggests using 
unsupervised, self-organizing learning schemes that choose their own DA definitions 
in the process of optimizing the primary task, whatever it may be. Hand-labeled DA 
categories may still serve an important role in initializing such an algorithm. 
We believe that dialogue-related tasks have much to benefit from corpus-driven, 
automatic learning techniques. To enable such research, we need fairly large, stan- 
dardized corpora that allow comparisons over time and across approaches. Despite 
its shortcomings, the Switchboard omain could serve this purpose. 
9. Conclusions 
We have developed an integrated probabilistic approach to dialogue act modeling for 
conversational speech, and tested it on a large speech corpus. The approach combines 
models for lexical and prosodic realizations of DAs, as well as a statistical discourse 
10 The inadequacy of n-gram models for nested discourse structures i  pointed out by Chu-Carroll (1998), 
although the suggested solution is a modified n-gram approach. 
368 
Stolcke et al Dialogue Act Modeling 
grammar. All components of the model are automatically trained, and are thus appli- 
cable to other domains for which labeled data is available. Classification accuracies 
achieved so far are highly encouraging, relative to the inherent difficulty of the task as 
measured by human labeler performance. We investigated several modeling alterna- 
tives for the components of the model (backoff n-grams and maximum entropy models 
for discourse grammars, decision trees and neural networks for prosodic lassification) 
and found performance largely independent of these choices. Finally, we developed a
principled way of incorporating DA modeling into the probability model of a contin- 
uous speech recognizer, by constraining word hypotheses using the discourse context. 
However, the approach gives only a small reduction in word error on our corpus, 
which can be attributed to a preponderance of a single dialogue act type (statements). 
Note 
The research described here is based on a 
project at the 1997 Workshop on Innovative 
Techniques in LVCSR at the Center for Speech 
and Language Processing at Johns Hopkins 
University (Jurafsky et al 1997; Jurafsky et 
al. 1998). The DA-labeled Switchboard tran- 
scripts as well as other project-related publi- 
cations are available at http://www.colorado. 
edu/ling/jurafsky/ws97/. 
Acknowledgments 
We thank the funders, researchers, and 
support staff of the 1997 Johns Hopkins 
Summer Workshop, especially Bill Byrne, 
Fred Jelinek, Harriet Nock, Joe Picone, 
Kimberly Shiring, and Chuck Wooters. 
Additional support came from the NSF via 
grants IRI-9619921 and IRI-9314967, and 
from the UK Engineering and Physical 
Science Research Council (grant 
GR/J55106). Thanks to Mitch Weintraub, to 
Susann LuperFoy, Nigel Ward, James Allen, 
Julia Hirschberg, and Marilyn Walker for 
advice on the design of the SWBD-DAMSL 
tag set, to the discourse labelers at CU 
Boulder (Debra Biasca, Marion Bond, Traci 
Curl, Anu Erringer, Michelle Gregory, Lori 
Heintzelman, Taimi Metzler, and Amma 
Oduro) and the intonation labelers at the 
University of Edinburgh (Helen Wright, 
Kurt Dusterhoff, Rob Clark, Cassie Mayo, 
and Matthew Bull). We also thank Andy 
Kehler and the anonymous reviewers for 
valuable comments on a draft of this paper. 
References 
Alexandersson, Jan and Norbert Reithinger. 
1997. Learning dialogue structures from a 
corpus. In G. Kokkinakis, N. Fakotakis, 
and E. Dermatas, editors, Proceedings ofthe 
5th European Conference on Speech 
Communication a d Technology, volume 4, 
pages 2,231-2,234. Rhodes, Greece, 
September. 
Anderson, Anne H., Miles Bader, Ellen G. 
Bard, Elizabeth H. Boyle, Gwyneth M. 
Doherty, Simon C. Garrod, Stephen D. 
Isard, Jacqueline C. Kowtko, Jan M. 
McAllister, Jim Miller, Catherine F. Sotillo, 
Henry S. Thompson, and Regina Weinert. 
1991. The HCRC Map Task corpus. 
Language and Speech, 34(4):351-366. 
Austin, J. L. 1962. How to do Things with 
Words. Clarendon Press, Oxford. 
Bahl, Lalit R., Frederick Jelinek, and 
Robert L. Mercer. 1983. A maximum 
likelihood approach to continuous speech 
recognition. IEEE Transactions on Pattern 
Analysis and Machine Intelligence, 
5(2):179-190, March. 
Bard, Ellen G., Catherine Sotillo, Anne H. 
Anderson, and M. M. Taylor. 1995. The 
DCIEM Map Task corpus: Spontaneous 
dialogues under sleep deprivation and 
drug treatment. In Isabel Trancoso and 
Roger Moore, editors, Proceedings ofthe 
ESCA-NATO Tutorial and Workshop on 
Speech under Stress, pages 25-28, Lisbon, 
September. 
Baum, Leonard E., Ted Petrie, George 
Soules, and Norman Weiss. 1970. A 
maximization technique occurring in the 
statistical analysis of probabilistic 
functions in Markov chains. The Annals of 
Mathematical Statistics, 41(1):164-171. 
Berger, Adam L., Stephen A. Della Pietra, 
and Vincent J. Della Pietra. 1996. A 
maximum entropy approach to natural 
language processing. Computational 
Linguistics, 22(1):39-71. 
Bourlard, Herv6 and Nelson Morgan. 1993. 
Connectionist Speech Recognition. A Hybrid 
Approach. Kluwer Academic Publishers, 
Boston, MA. 
Breiman, L., J. H. Friedman, R. A. Olshen, 
and C. J. Stone. 1984. Classification and 
Regression Trees. Wadsworth and Brooks, 
Pacific Grove, CA. 
369 
Computational Linguistics Volume 26, Number 3 
Bridle, J. S. 1990. Probabilistic interpretation 
of feedforward classification etwork 
outputs, with relationships to statistical 
pattern recognition. In F. Fogleman Soulie 
and J. Herault, editors, Neurocomputing: 
Algorithms, Architectures and Applications. 
Springer, Berlin, pages 227-236. 
Brill, Eric. 1993. Automatic grammar 
induction and parsing free text: A 
transformation-based approach. In 
Proceedings ofthe ARPA Workshop on Human 
Language Technology, Plainsboro, NJ, 
March. 
Carletta, Jean. 1996. Assessing agreement on 
classification tasks: The Kappa statistic. 
Computational Linguistics, 22(2):249-254. 
Carlson, Lari. 1983. Dialogue Games: An 
Approach to Discourse Analysis. D. Reidel. 
Chu-Carroll, Jennifer. 1998. A statistical 
model for discourse act recognition in 
dialogue interactions. In Jennifer 
Chu-Carroll and Nancy Green, editors, 
Applying Machine Learning to Discourse 
Processing. Papers from the 1998 AAAI 
Spring Symposium. Technical Report 
SS-98-01, pages 12-17. AAAI Press, Menlo 
Park, CA. 
Church, Kenneth Ward. 1988. A stochastic 
parts program and noun phrase parser 
for unrestricted text. In Second Conference 
on Applied Natural Language Processing, 
pages 136-143, Austin, TX. 
Core, Mark and James Allen. 1997. Coding 
dialogs with the DAMSL annotation 
scheme. In Working Notes of the AAAI Fall 
Symposium on Communicative Action in 
Humans and Machines, pages 28-35, 
Cambridge, MA, November. 
Dermatas, Evangelos and George 
Kokkinakis. 1995. Automatic stochastic 
tagging of natural anguage texts. 
Computational Linguistics, 21(2):137-163. 
Eckert, Wieland, Florian Gallwitz, and 
Heinrich Niemann. 1996. Combining 
stochastic and linguistic language models 
for recognition of spontaneous speech. In 
Proceedings ofthe IEEE Conference on 
Acoustics, Speech, and Signal Processing, 
volume 1, pages 423-426, Atlanta, GA, 
May. 
Finke, Michael, Maria Lapata, Alon Lavie, 
Lori Levin, Laura Mayfield Tomokiyo, 
Thomas Polzin, Klaus Ries, Alex Waibel, 
and Klaus Zechner. 1998. Clarity: 
Inferring discourse structure from speech. 
In Jennifer Chu-Carroll and Nancy Green, 
editors, Applying Machine Learning to 
Discourse Processing. Papers from the 1998 
AAAI Spring Symposium. Technical Report 
SS-98-01, pages 25-32. AAAI Press, Menlo 
Park, CA. 
Fowler, Carol A. and Jonathan Housum. 
1987. Talkers' signaling of "new" and 
"old" words in speech and listeners' 
perception and use of the distinction. 
Journal of Memory and Language, 26:489-504. 
Fukada, Toshiaki, Detlef Koll, Alex Waibel, 
and Kouichi Tanigaki. 1998. Probabilistic 
dialogue act extraction for concept based 
multilingual translation systems. In 
Robert H. Mannell and Jordi 
Robert-Ribes, editors, Proceedings ofthe 
International Conference on Spoken Language 
Processing, volume 6, pages 2,771-2,774, 
Sydney, December. Australian Speech 
Science and Technology Association. 
Godfrey, J. J., E. C. Holliman, and 
J. McDaniel. 1992. SWITCHBOARD: 
Telephone speech corpus for research and 
development. In Proceedings ofthe IEEE 
Conference on Acoustics, Speech, and Signal 
Processing, volume 1, pages 517-520, San 
Francisco, CA, March. 
Grosz, Barbara J. and Candace L. Sidner. 
1986. Attention, intention, and the 
structure of discourse. Computational 
Linguistics, 12(3):175-204. 
Hirschberg, Julia B. and Diane J. Litman. 
1993. Empirical studies on the 
disambiguation of cue phrases. 
Computational Linguistics, 19(3):501-530. 
Iyer, Rukmini, Mari Ostendorf, and J. Robin 
Rohlicek. 1994. Language modeling with 
sentence-level mixtures. In Proceedings of
the ARPA Workshop on Human Language 
Technology, pages 82-86, Plainsboro, NJ, 
March. 
Jefferson, Gail. 1984. Notes on a systematic 
deployment of the acknowledgement 
tokens 'yeah' and 'mm hm'. Papers in 
Linguistics, 17:197-216. 
Jekat, Susanne, Alexandra Klein, Elisabeth 
Maier, Ilona Maleck, Marion Mast, and 
Joachim Quantz. 1995. Dialogue acts in 
VERBMOBIL. Verbmobil-Report 65, 
Universit~it Hamburg, DFKI GmbH, 
Universit~it Erlangen, and TU Berlin, 
April. 
Jurafsky, Dan, Rebecca Bates, Noah Coccaro, 
Rachel Martin, Marie Meteer, Klaus Ries, 
Elizabeth Shriberg, Andreas Stolcke, Paul 
Taylor, and Carol Van Ess-Dykema. 1997. 
Automatic detection of discourse 
structure for speech recognition and 
understanding. In Proceedings ofthe IEEE 
Workshop on Speech Recognition and 
Understanding, pages 88-95, Santa 
Barbara, CA, December. 
Jurafsky, Daniel, Rebecca Bates, Noah 
Coccaro, Rachel Martin, Marie Meteer, 
Klaus Ries, Elizabeth Shriberg, Andreas 
Stolcke, Paul Taylor, and Carol Van 
370 
Stolcke et al Dialogue Act Modeling 
Ess-Dykema. 1998. Switchboard iscourse 
language modeling project final report. 
Research Note 30, Center for Language 
and Speech Processing, Johns Hopkins 
University, Baltimore, MD, January. 
Jurafsky, Daniel, Elizabeth Shriberg, and 
Debra Biasca. 1997. Switchboard-DAMSL 
Labeling Project Coder's Manual. 
Technical Report 97-02, University of 
Colorado, Institute of Cognitive Science, 
Boulder, CO. http://www.colorado.edu/ 
ling/jurafsky/manual.augustl.html. 
Jurafsky, Daniel, Elizabeth E. Shriberg, 
Barbara Fox, and Traci Curl. 1998. Lexical, 
prosodic, and syntactic ues for dialog 
acts. In Proceedings ofACL/COLING-98 
Workshop on Discourse Relations and 
Discourse Markers, pages 114-120. 
Association for Computational 
Linguistics. 
Katz, Slava M. 1987. Estimation of 
probabilities from sparse data for the 
language model component of a speech 
recognizer. IEEE Transactions on Acoustics, 
Speech, and Signal Processing, 35(3):400-401, 
March. 
Kita, Kenji, Yoshikazu Fukui, Masaaki 
Nagata, and Tsuyoshi Morimoto. 1996. 
Automatic acquisition of probabilistic 
dialogue models. In H. Timothy Bunnell 
and William Idsardi, editors, Proceedings of
the International Conference on Spoken 
Language Processing, volume 1, 
pages 196-199, Philadelphia, PA, October. 
Kompe, Ralf. 1997. Prosody in speech 
understanding systems. Springer, Berlin. 
Kowtko, Jacqueline C. 1996. The Function of 
Intonation in Task Oriented Dialogue. Ph.D. 
thesis, University of Edinburgh, 
Edinburgh. 
Kuhn, Roland and Renato de Mori. 1990. A 
cache-base natural anguage model for 
speech recognition. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 
12(6):570-583, June. 
Levin, Joan A. and Johanna A. Moore. 1977. 
Dialogue games: Metacommunication 
structures for natural anguage 
interaction. Cognitive Science, 1(4):395-420. 
Levin, Lori, Klaus Ries, Ann Thym~-Gobbel, 
and Alon Lavie. 1999. Tagging of speech 
acts and dialogue games in Spanish 
CallHome. In Towards Standards and Tools 
for Discourse Tagging (Proceedings ofthe ACL 
Workshop at ACL'99), pages 42-47, College 
Park, MD, June. 
Linell, Per. 1990. The power of dialogue 
dynamics. In Ivana Markov~ and Klaus 
Foppa, editors, The Dynamics of Dialogue. 
Harvester, Wheatsheaf, New York, 
London, pages 147-177. 
Mast, M., R. Kompe, S. Harbeck, 
A. Kiel~ling, H. Niemann, E. NOth, E. G. 
Schukat-Talamazzini, and V. Warnke. 
1996. Dialog act classification with the 
help of prosody. In H. Timothy Bunnell 
and William Idsardi, editors, Proceedings of
the International Conference on Spoken 
Language Processing, volume 3, 
pages 1,732-1,735, Philadelphia, PA, 
October. 
Menn, Lise and Suzanne E. Boyce. 1982. 
Fundamental frequency and discourse 
structure. Language and Speech, 25:341-383. 
Meteer, Marie, Ann Taylor, Robert 
MacIntyre, and Rukmini Iyer. 1995. 
Dysfluency annotation stylebook for the 
Switchboard corpus. Distributed by LDC, 
ftp://ftp.cis.upenn.edu/pub/treebank/ 
swbd/doc/DFL-book.ps, February. 
Revised June 1995 by Ann Taylor. 
Morgan, Nelson, Eric Fosler, and Nikki 
Mirghafori. 1997. Speech recognition 
using on-line estimation of speaking rate. 
In G. Kokkinakis, N. Fakotakis, and E. 
Dermatas, editors, Proceedings ofthe 5th 
European Conference on Speech 
Communication a d Technology, volume 4, 
pages 2,079-2,082, Rhodes, Greece, 
September. 
Munk, Marcus. 1999. Shallow Statistical 
Parsing for Machine Translation. Diploma 
thesis, Carnegie Mellon University. 
Nagata, Masaaki. 1992. Using pragmatics to 
rule out recognition errors in cooperative 
task-oriented dialogues. In John J. Ohala, 
Terrance M. Nearey, Bruce L. Derwing, 
Megan M. Hodge, and Grace E. Wiebe, 
editors, Proceedings ofthe International 
Conference on Spoken Language Processing, 
volume 1, pages 647-650, Banff, Canada, 
October. 
Nagata, Masaaki and Tsuyoshi Morimoto. 
1993. An experimental statistical dialogue 
model to predict he speech act type of 
the next utterance. In Katsuhiko Shirai, 
Tetsunori Kobayashi, and Yasunari 
Harada, editors, Proceedings ofthe 
International Symposium on Spoken Dialogue, 
pages 83-86, Tokyo, November. 
Nagata, Masaaki and Tsuyoshi Morimoto. 
1994. First steps toward statistical 
modeling of dialogue to predict he 
speech act type of the next utterance. 
Speech Communication, 15:193-203. 
Ohler, Uwe, Stefan Harbeck, and Heinrich 
Niemann. 1999. Discriminative training of 
language model classifiers. In Proceedings 
of the 6th European Conference on Speech 
Communication a d Technology, volume 4, 
pages 1607-1610, Budapest, September. 
371 
Computational Linguistics Volume 26, Number 3 
Pearl, Judea. 1988. Probabilistic Reasoning in 
Intelligent Systems: Networks of Plausible 
Inference. Morgan Kaufmann, San Mateo, 
CA. 
Power, Richard J. D. 1979. The organization 
of purposeful dialogues. Linguistics, 
17:107-152. 
Rabiner, L. R. and B. H. Juang. 1986. An 
introduction to hidden Markov models. 
IEEE ASSP Magazine, 3(1):4-16, January. 
Reithinger, Norbert, Ralf Engel, Michael 
Kipp, and Martin Klesen. 1996. Predicting 
dialogue acts for a speech-to-speech 
translation system. In H. Timothy Bunnell 
and William Idsardi, editors, Proceedings of
the International Conference on Spoken 
Language Processing, volume 2, 
pages 654-657, Philadelphia, PA, October. 
Reithinger, Norbert and Martin Klesen. 
1997. Dialogue act classification using 
language models. In G. Kokkinakis, N. 
Fakotakis, and E. Dermatas, editors, 
Proceedings ofthe 5th European Conference on 
Speech Communication a d Technology, 
volume 4, pages 2,235-2,238, Rhodes, 
Greece, September. 
Ries, Klaus. 1999a. HMM and neural 
network based speech act classification. In
Proceedings ofthe IEEE Conference on 
Acoustics, Speech, and Signal Processing, 
volume 1, pages 497-500, Phoenix, AZ, 
March. 
Ries, Klaus. 1999b. Towards the detection 
and description of textual meaning 
indicators in spontaneous conversations. 
In Proceedings ofthe 6th European Conference 
on Speech Communication a d Technology, 
volume 3, pages 1,415--1,418, Budapest, 
September. 
Rose, R. C., E. I. Chang, and R. P. 
Lippmann. 1991. Techniques for 
information retrieval from voice 
messages. In Proceedings ofthe IEEE 
Conference on Acoustics, Speech, and Signal 
Processing, volume 1, pages 317-320, 
Toronto, May. 
Sacks, H., E. A. Schegloff, and G. Jefferson. 
1974. A simplest semantics for the 
organization of turn-taking in 
conversation. Language, 50(4):696-735. 
Samuel, Ken, Sandra Carberry, and 
K. Vijay-Shanker. 1998. Dialogue act 
tagging with transformation-based 
learning. In Proceedings ofthe 36th Annual 
Meeting of the Association for Computational 
Linguistics and 17th International Conference 
on Computational Linguistics, volume 2, 
pages 1,150-1,156, Montreal. 
Schegloff, Emanuel A. 1968. Sequencing in
conversational openings. American 
Anthropologist, 70:1,075-1,095. 
Schegloff, Emanuel A. 1982. Discourse as an 
interactional chievement: Some uses of 
'uh huh' and other things that come 
between sentences. In Deborah Tannen, 
editor, Analyzing Discourse: Text and Talk. 
Georgetown University Press, 
Washington, D.C., pages 71-93. 
Searle, J. R. 1969. Speech Acts. Cambridge 
University Press, London-New York. 
Shriberg, Elizabeth, Rebecca Bates, Andreas 
Stolcke, Paul Taylor, Daniel Jurafsky, 
Klaus Ries, Noah Coccaro, Rachel Martin, 
Marie Meteer, and Carol Van 
Ess-Dykema. 1998. Can prosody aid the 
automatic lassification of dialog acts in 
conversational speech? Language and 
Speech, 41(3-4):439--487. 
Shriberg, Elizabeth, Andreas Stolcke, Dilek 
Hakkani-Ti~r, and GOkhan Tiir. 2000. 
Prosody-based automatic segmentation f 
speech into sentences and topics. Speech 
Communication, 32(1-2). Special Issue on 
Accessing Information in Spoken Audio. 
To appear. 
Siegel, Sidney and N. John Castellan, Jr. 
1988. Nonparametric Statistics for the 
Behavioral Sciences. Second edition. 
McGraw-Hill, New York. 
Stolcke, Andreas and Elizabeth Shriberg. 
1996. Automatic linguistic segmentation 
of conversational speech. In H. Timothy 
Bunnell and William Idsardi, editors, 
Proceedings ofthe International Conference on 
Spoken Language Processing, volume 2, 
pages 1,005-1,008, Philadelphia, PA, 
October. 
Suhm, B. and A. Waibel. 1994. Toward better 
language models for spontaneous speech. 
In Proceedings ofthe International Conference 
on Spoken Language Processing, volume 2, 
pages 831-834, Yokohama, September. 
Taylor, Paul A. 2000. Analysis and synthesis 
of intonation using the tilt model. Journal 
of the Acoustical Society of America, 
107(3):1,697-1,714. 
Taylor, Paul A., Simon King, Stephen Isard, 
and Helen Wright. 1998. Intonation and 
dialog context as constraints for speech 
recognition. Language and Speech, 
41(3-4):489-508. 
Taylor, Paul A., Simon King, Stephen Isard, 
Helen Wright, and Jacqueline Kowtko. 
1997. Using intonation to constrain 
language models in speech recognition. In 
G. Kokkinakis, N. Fakotakis, and E. 
Dermatas, editors, Proceedings ofthe 5th 
European Conference on Speech 
Communication a d Technology, volume 5, 
pages 2,763-2,766, Rhodes, Greece, 
September. 
372 
Stolcke et al Dialogue Act Modeling 
Van Ess-Dykema, Carol and Klaus Ries. 
1998. Linguistically engineered tools for 
speech recognition error analysis. In 
Robert H. Mannell and Jordi 
Robert-Ribes, editors, Proceedings ofthe 
International Conference on Spoken Language 
Processing, volume 5, pages 2,091-2,094, 
Sydney, December. Australian Speech 
Science and Technology Association. 
Viterbi, A. 1967. Error bounds for 
convolutional codes and an 
asymptotically optimum decoding 
algorithm. IEEE Transactions on Information 
Theory, 13:260-269. 
Warnke, Volker, Stefan Harbeck, Elmar 
N0th, Heinrich Niemann, and Michael 
Levit. 1999. Discriminative estimation of 
interpolation parameters for language 
model classifiers. In Proceedings ofthe IEEE 
Conference on Acoustics, Speech, and Signal 
Processing, volume 1, pages 525-528, 
Phoenix, AZ, March. 
Warnke, Volker, R. Kompe, Heinrich 
Niemann, and Elmar NOth. 1997. 
Integrated ialog act segmentation a d 
classification using prosodic features and 
language models. In G. Kokkinakis, N. 
Fakotakis, and E. Dermatas, editors, 
Proceedings ofthe 5th European Conference on 
Speech Communication a d Technology, 
volume 1, pages 207-210, Rhodes, Greece, 
September. 
Weber, Elizabeth G. 1993. Varieties of 
Questions in English Conversation. John 
Benjamins, Amsterdam. 
Witten, Ian H. and Timothy C. Bell. 1991. 
The zero-frequency problem: Estimating 
the probabilities of novel events in 
adaptive text compression. IEEE 
Transations on Information Theory, 
37(4):1,085-1,094, July. 
Woszczyna, M. and A. Waibel. 1994. 
Inferring linguistic structure in spoken 
language. In Proceedings ofthe International 
Conference on Spoken Language Processing, 
volume 2, pages 847-850, Yokohama, 
September. 
Wright, Helen. 1998. Automatic utterance 
type detection using suprasegmental 
features. In Robert H. Mannell and Jordi 
Robert-Ribes, editors, Proceedings ofthe 
International Conference on Spoken Language 
Processing, volume 4, pages 1,403-1,406, 
Sydney, December. Australian Speech 
Science and Technology Association. 
Wright, Helen and Paul A. Taylor. 1997. 
Modelling intonational structure using 
hidden Markov models. In Intonation: 
Theory, Models and Applications. Proceedings 
of an ESCA Workshop, pages 333-336, 
Athens, September. 
Yngve, Victor H. 1970. On getting a word in 
edgewise. In Papers from the Sixth Regional 
Meeting of the Chicago Linguistic Society, 
pages 567-577, Chicago, April. University 
of Chicago. 
Yoshimura, Takashi, Satoru Hayamizu, 
Hiroshi Ohmura, and Kazuyo Tanaka. 
1996. Pitch pattern clustering of user 
utterances in human-machine dialogue. In 
H. Timothy Bunnell and William Idsardi, 
editors, Proceedings ofthe International 
Conference on Spoken Language Processing, 
volume 2, pages 837-840, Philadelphia, 
PA, October. 
373 

c? 2002 Association for Computational Linguistics
Automatic Labeling of Semantic Roles
Daniel Gildea? Daniel Jurafsky?
University of California, Berkeley, and
International Computer Science Institute
University of Colorado, Boulder
We present a system for identifying the semantic relationships, or semantic roles, filled by
constituents of a sentence within a semantic frame. Given an input sentence and a target word
and frame, the system labels constituents with either abstract semantic roles, such as Agent or
Patient, or more domain-specific semantic roles, such as Speaker,Message, and Topic.
The system is based on statistical classifiers trained on roughly 50,000 sentences that were
hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed
each training sentence into a syntactic tree and extracted various lexical and syntactic features,
including the phrase type of each constituent, its grammatical function, and its position in the
sentence. These features were combined with knowledge of the predicate verb, noun, or adjective,
as well as information such as the prior probabilities of various combinations of semantic roles.
We used various lexical clustering algorithms to generalize across possible fillers of roles. Test
sentences were parsed, were annotated with these features, and were then passed through the
classifiers.
Our system achieves 82% accuracy in identifying the semantic role of presegmented con-
stituents. At the more difficult task of simultaneously segmenting constituents and identifying
their semantic role, the system achieved 65% precision and 61% recall.
Our study also allowed us to compare the usefulness of different features and feature combi-
nation methods in the semantic role labeling task. We also explore the integration of role labeling
with statistical syntactic parsing and attempt to generalize to predicates unseen in the training
data.
1. Introduction
Recent years have been exhilarating ones for natural language understanding. The
excitement and rapid advances that had characterized other language-processing tasks
such as speech recognition, part-of-speech tagging, and parsing have finally begun to
appear in tasks in which understanding and semantics play a greater role. For example,
there has been widespread commercial deployment of simple speech-based natural
language understanding systems that answer questions about flight arrival times, give
directions, report on bank balances, or perform simple financial transactions. More
sophisticated research systems generate concise summaries of news articles, answer
fact-based questions, and recognize complex semantic and dialogue structure.
But the challenges that lie ahead are still similar to the challenge that the field
has faced since Winograd (1972): moving away from carefully hand-crafted, domain-
dependent systems toward robustness and domain independence. This goal is not as
? Currently at Institute for Research in Cognitive Science, University of Pennsylvania, 3401 Walnut
Street, Suite 400A, Philadelphia, PA 19104. E-mail: dgildea@cis.upenn.edu
? Departments of Linguistics and Computer Science, University of Colorado, Boulder, CO 80309. E-mail:
jurafsky@colorado.edu
246
Computational Linguistics Volume 28, Number 3
far away as it once was, thanks to the development of large semantic databases such
as WordNet (Fellbaum 1998) and progress in domain-independent machine learning
algorithms.
Current information extraction and dialogue understanding systems, however, are
still based on domain-specific frame-and-slot templates. Systems for booking airplane
information use domain-specific frames with slots like orig city, dest city, or de-
part time (Stallard 2000). Systems for studying mergers and acquisitions use slots
like products, relationship, joint venture company, and amount (Hobbs et al
1997). For natural language understanding tasks to proceed beyond these specific do-
mains, we need semantic frames and semantic understanding systems that do not
require a new set of slots for each new application domain.
In this article we describe a shallow semantic interpreter based on semantic roles
that are less domain specific than to airport or joint venture company. These
roles are defined at the level of semantic frames of the type introduced by Fillmore
(1976), which describe abstract actions or relationships, along with their participants.
For example, the Judgement frame contains roles like judge, evaluee, and reason,
and the Statement frame contains roles like speaker, addressee, and message, as
the following examples show:
(1) [Judge She ] blames [Evaluee the Government ] [Reason for failing to do
enough to help ] .
(2) [Message ?I?ll knock on your door at quarter to six? ] [Speaker Susan] said.
These shallow semantic roles could play an important role in information extrac-
tion. For example, a semantic role parse would allow a system to realize that the ruling
that is the direct object of change in (3) plays the same Theme role as the ruling that
is the subject of change in (4):
(3) The canvassing board changed its ruling on Wednesday.
(4) The ruling changed because of the protests.
The fact that semantic roles are defined at the frame level means, for example, that
the verbs send and receive would share the semantic roles (sender, recipient, goods,
etc.) defined with respect to a common Transfer frame. Such common frames might
allow a question-answering system to take a question like (5) and discover that (6) is
relevant in constructing an answer to the question:
(5) Which party sent absentee ballots to voters?
(6) Both Democratic and Republican voters received absentee ballots from
their party.
This shallow semantic level of interpretation has additional uses outside of gen-
eralizing information extraction, question answering, and semantic dialogue systems.
One such application is in word sense disambiguation, where the roles associated with
a word can be cues to its sense. For example, Lapata and Brew (1999) and others have
shown that the different syntactic subcategorization frames of a verb such as serve can
be used to help disambiguate a particular instance of the word. Adding semantic role
subcategorization information to this syntactic information could extend this idea to
247
Gildea and Jurafsky Automatic Labeling of Semantic Roles
use richer semantic knowledge. Semantic roles could also act as an important interme-
diate representation in statistical machine translation or automatic text summarization
and in the emerging field of text data mining (TDM) (Hearst 1999). Finally, incorpo-
rating semantic roles into probabilistic models of language may eventually yield more
accurate parsers and better language models for speech recognition.
This article describes an algorithm for identifying the semantic roles filled by con-
stituents in a sentence. We apply statistical techniques that have been successful for the
related problems of syntactic parsing, part-of-speech tagging, and word sense disam-
biguation, including probabilistic parsing and statistical classification. Our statistical
algorithms are trained on a hand-labeled data set: the FrameNet database (Baker, Fill-
more, and Lowe 1998; Johnson et al 2001). The FrameNet database defines a tag set
of semantic roles called frame elements and included, at the time of our experiments,
roughly 50,000 sentences from the British National Corpus hand-labeled with these
frame elements.
This article presents our system in stages, beginning in Section 2 with a more de-
tailed description of the data and the set of frame elements or semantic roles used. We
then introduce (in Section 3) the statistical classification technique used and examine
in turn the knowledge sources of which our system makes use. Section 4 describes
the basic syntactic and lexical features used by our system, which are derived from
a Penn Treebank?style parse of individual sentences to be analyzed. We break our
task into two subproblems: finding the relevant sentence constituents (deferred until
Section 5), and giving them the correct semantic labels (Sections 4.2 and 4.3). Section 6
adds higher-level semantic knowledge to the system, attempting to model the selec-
tional restrictions on role fillers not directly captured by lexical statistics. We compare
hand-built and automatically derived resources for providing this information. Sec-
tion 7 examines techniques for adding knowledge about systematic alternations in
verb argument structure with sentence-level features. We combine syntactic parsing
and semantic role identification into a single probability model in Section 8. Section 9
addresses the question of generalizing statistics from one target predicate to another,
beginning with a look at domain-independent thematic roles in Section 9.1. Finally we
draw conclusions and discuss future directions in Section 10.
2. Semantic Roles
Semantic roles are one of the oldest classes of constructs in linguistic theory, dating
back thousands of years to Panini?s ka?raka theory (Misra 1966; Rocher 1964; Dahiya
1995). Longevity, in this case, begets variety, and the literature records scores of pro-
posals for sets of semantic roles. These sets of roles range from the very specific to
the very general, and many have been used in computational implementations of one
type or another.
At the specific end of the spectrum are domain-specific roles such as the from air-
port, to airport, or depart time discussed above, or verb-specific roles such as
eater and eaten for the verb eat. The opposite end of the spectrum consists of theo-
ries with only two ?proto-roles? or ?macroroles?: Proto-Agent and Proto-Patient
(Van Valin 1993; Dowty 1991). In between lie many theories with approximately 10
roles, such as Fillmore?s (1971) list of nine: Agent, Experiencer, Instrument, Ob-
ject, Source, Goal, Location, Time, and Path.1
1 There are scores of other theories with slightly different sets of roles, including those of Fillmore (1968),
Jackendoff (1972), and Schank (1972); see Somers (1987) for an excellent summary.
248
Computational Linguistics Volume 28, Number 3
banter?v
debate?v
converse?v
gossip?v
dispute?n
discussion?n
tiff?n
ConversationFrame:
Protagonist?1
Protagonist?2
Protagonists
Topic
Medium
Frame Elements:
argue?v
Domain: Communication Domain: Cognition
Frame: Questioning
Topic
Medium
Frame Elements: Speaker
Addressee
Message
Frame:
Topic
Medium
Frame Elements: Speaker
Addressee
Message
Statement
Frame:
Frame Elements:
Judgment
Judge
Evaluee
Reason
Role
dispute?n
blame?v fault?n
admire?v
admiration?n disapprove?v
blame?n
appreciate?v
Frame:
Frame Elements:
Categorization
Cognizer
Item
Category
Criterion
Figure 1
Sample domains and frames from the FrameNet lexicon.
Many of these sets of roles have been proposed by linguists as part of theories
of linking, the part of grammatical theory that describes the relationship between
semantic roles and their syntactic realization. Other sets have been used by computer
scientists in implementing natural language understanding systems. As a rule, the
more abstract roles have been proposed by linguists, who are more concerned with
explaining generalizations across verbs in the syntactic realization of their arguments,
whereas the more specific roles have more often been proposed by computer scientists,
who are more concerned with the details of the realization of the arguments of specific
verbs.
The FrameNet project (Baker, Fillmore, and Lowe 1998) proposes roles that are
neither as general as the 10 abstract thematic roles, nor as specific as the thousands of
potential verb-specific roles. FrameNet roles are defined for each semantic frame. A
frame is a schematic representation of situations involving various participants, props,
and other conceptual roles (Fillmore 1976). For example, the frame Conversation,
shown in Figure 1, is invoked by the semantically related verbs argue, banter, debate,
converse, and gossip, as well as the nouns dispute, discussion, and tiff, and is defined as
follows:
(7) Two (or more) people talk to one another. No person is construed as
only a speaker or only an addressee. Rather, it is understood that both
(or all) participants do some speaking and some listening: the process is
understood to be symmetrical or reciprocal.
The roles defined for this frame, and shared by all its lexical entries, include
Protagonist-1 and Protagonist-2 or simply Protagonists for the participants
in the conversation, as well as Medium and Topic. Similarly, the Judgment frame
mentioned above has the roles Judge, Evaluee, and Reason and is invoked by verbs
such as blame, admire, and praise and nouns such as fault and admiration. We refer to
the roles for a given frame as frame elements. A number of hand-annotated exam-
ples from the Judgment frame are included below to give a flavor of the FrameNet
database:
(8) [Judge She ] blames [Evaluee the Government ] [Reason for failing to do
enough to help ] .
(9) Holman would characterise this as blaming [Evaluee the poor ] .
249
Gildea and Jurafsky Automatic Labeling of Semantic Roles
(10) The letter quotes Black as saying that [Judge white and Navajo ranchers ]
misrepresent their livestock losses and blame [Reason everything ] [Evaluee
on coyotes ] .
(11) The only dish she made that we could tolerate was [Evaluee syrup tart
which2 ] [Judge we ] praised extravagantly with the result that it became
our unhealthy staple diet.
(12) I?m bound to say that I meet a lot of [Judge people who ] praise [Evaluee
me ] [Reason for speaking up ] but don?t speak up themselves.
(13) Specimens of her verse translations of Tasso (Jerusalem Delivered) and
Verri (Roman Nights) circulated to [ Manner warm ] [Judge critical ] praise;
but ?unforeseen circumstance? prevented their publication.
(14) And if Sam Snort hails Doyler as monumental is he perhaps erring on
the side of being excessive in [ Judge his ] praise?
Defining semantic roles at this intermediate frame level helps avoid some of
the well-known difficulties of defining a unique small set of universal, abstract the-
matic roles while also allowing some generalization across the roles of different verbs,
nouns, and adjectives, each of which adds semantics to the general frame or high-
lights a particular aspect of the frame. One way of thinking about traditional ab-
stract thematic roles, such as Agent and Patient, in the context of FrameNet is to
conceive them as frame elements defined by abstract frames, such as action and mo-
tion, at the top of an inheritance hierarchy of semantic frames (Fillmore and Baker
2000).
The examples above illustrate another difference between frame elements and
thematic roles as commonly described in the literature. Whereas thematic roles tend
to be arguments mainly of verbs, frame elements can be arguments of any predicate,
and the FrameNet database thus includes nouns and adjectives as well as verbs.
The examples above also illustrate a few of the phenomena that make it hard to
identify frame elements automatically. Many of these are caused by the fact that there
is not always a direct correspondence between syntax and semantics. Whereas the
subject of blame is often the Judge, the direct object of blame can be an Evaluee (e.g.,
the poor in ?blaming the poor?) or a Reason (e.g., everything in ?blame everything
on coyotes?). The identity of the Judge can also be expressed in a genitive pronoun,
(e.g., his in ?his praise?) or even an adjective (e.g., critical in ?critical praise?).
The corpus used in this project is perhaps best described in terms of the method-
ology used by the FrameNet team. We outline the process here; for more detail see
Johnson et al (2001). As the first step, semantic frames were defined for the general
domains chosen; the frame elements, or semantic roles for participants in a frame,
were defined; and a list of target words, or lexical predicates whose meaning includes
aspects of the frame, was compiled for each frame. Example sentences were chosen
by searching the British National Corpus for instances of each target word. Separate
searches were performed for various patterns over lexical items and part-of-speech
sequences in the target words? context, producing a set of subcorpora for each tar-
get word, designed to capture different argument structures and ensure that some
examples of each possible syntactic usage of the target word would be included in
2 The FrameNet annotation includes both the relative pronoun and its antecedent in the target word?s
clause.
250
Computational Linguistics Volume 28, Number 3
the final database. Thus, the focus of the project was on completeness of examples for
lexicographic needs, rather than on statistically representative data. Sentences from
each subcorpus were then annotated by hand, marking boundaries of each frame el-
ement expressed in the sentence and assigning tags for the annotated constituent?s
frame semantic role, syntactic category (e.g., noun phrase or prepositional phrase),
and grammatical function in relation to the target word (e.g., object or complement
of a verb). In the final phase of the process, the annotated sentences for each tar-
get word were checked for consistency. In addition to the tags just mentioned, the
annotations include certain other information, which we do not make use of in this
work, such as word sense tags for some target words and tags indicating metaphoric
usages.
Tests of interannotator agreement were performed for data from a small num-
ber of predicates before the final consistency check. Interannotator agreement at the
sentence level, including all frame element judgments and boundaries for one predi-
cate, varied from .66 to .82 depending on the predicate. The kappa statistic (Siegel
and Castellan 1988) varied from .67 to .82. Because of the large number of pos-
sible categories when boundary judgments are considered, kappa is nearly iden-
tical to the interannotator agreement. The system described in this article (which
gets .65/.61 precision/recall on individual frame elements; see Table 15) correctly
identifies all frame elements in 38% of test sentences. Although this .38 is not di-
rectly comparable to the .66?.82 interannotator agreements, it?s clear that the per-
formance of our system still falls significantly short of human performance on the
task.
The British National Corpus was chosen as the basis of the FrameNet project
despite differences between British and American usage because, at 100 million words,
it provides the largest corpus of English with a balanced mixture of text genres. The
British National Corpus includes automatically assigned syntactic part-of-speech tags
for each word but does not include full syntactic parses. The FrameNet annotators did
not make use of, or produce, a complete syntactic parse of the annotated sentences,
although some syntactic information is provided by the grammatical function and
phrase type tags of the annotated frame elements.
The preliminary version of the FrameNet corpus used for our experiments con-
tained 67 frame types from 12 general semantic domains chosen for annotation. A
complete list of the semantic domains represented in our data is shown in Table 1,
along with representative frames and predicates. Within these frames, examples of a
total of 1,462 distinct lexical predicates, or target words, were annotated: 927 verbs,
339 nouns, and 175 adjectives. There are a total of 49,013 annotated sentences and
99,232 annotated frame elements (which do not include the target words them-
selves).
How important is the particular set of semantic roles that underlies our sys-
tem? For example, could the optimal choice of semantic roles be very dependent on
the application that needs to exploit their information? Although there may well be
application-specific constraints on semantic roles, our semantic role classifiers seem
in practice to be relatively independent of the exact set of semantic roles under con-
sideration. Section 9.1 describes an experiment in which we collapsed the FrameNet
roles into a set of 18 abstract thematic roles. We then retrained our classifier and
achieved roughly comparable results; overall performance was 82.1% for abstract the-
matic roles, compared to 80.4% for frame-specific roles. Although this doesn?t show
that the detailed set of semantic roles is irrelevant, it does suggest that our statistical
classification algorithm, at least, is relatively robust to even quite large changes in role
identities.
251
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 1
Semantic domains with sample frames and predicates from the FrameNet lexicon.
Domain Sample Frames Sample Predicates
Body Action flutter, wink
Cognition Awareness attention, obvious
Judgment blame, judge
Invention coin, contrive
Communication Conversation bicker, confer
Manner lisp, rant
Emotion Directed angry, pleased
Experiencer-Obj bewitch, rile
General Imitation bogus, forge
Health Response allergic, susceptible
Motion Arriving enter, visit
Filling annoint, pack
Perception Active glance, savour
Noise snort, whine
Society Leadership emperor, sultan
Space Adornment cloak, line
Time Duration chronic, short
Iteration daily, sporadic
Transaction Basic buy, spend
Wealthiness broke, well-off
3. Related Work
Assignment of semantic roles is an important part of language understanding, and
the problem of how to assign such roles has been attacked by many computational
systems. Traditional parsing and understanding systems, including implementations of
unification-based grammars such as Head-Driven Phrase Structure Grammar (HPSG)
(Pollard and Sag 1994), rely on hand-developed grammars that must anticipate each
way in which semantic roles may be realized syntactically. Writing such grammars is
time consuming, and typically such systems have limited coverage.
Data-driven techniques have recently been applied to template-based semantic
interpretation in limited domains by ?shallow? systems that avoid complex feature
structures and often perform only shallow syntactic analysis. For example, in the
context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et
al. (1996) computed the probability that a constituent such as Atlanta filled a semantic
slot such as Destination in a semantic frame for air travel. In a data-driven approach
to information extraction, Riloff (1993) builds a dictionary of patterns for filling slots
in a specific domain such as terrorist attacks, and Riloff and Schmelzenbach (1998)
extend this technique to derive automatically entire ?case frames? for words in the
domain. These last systems make use of a limited amount of hand labor to accept or
reject automatically generated hypotheses. They show promise for a more sophisticated
approach to generalizing beyond the relatively small number of frames considered in
the tasks. More recently, a domain-independent system has been trained by Blaheta
and Charniak (2000) on the function tags, such as Manner and Temporal, included
in the Penn Treebank corpus. Some of these tags correspond to FrameNet semantic
roles, but the Treebank tags do not include all the arguments of most predicates. In this
article, we aim to develop a statistical system for automatically learning to identify all
semantic roles for a wide variety of predicates in unrestricted text.
252
Computational Linguistics Volume 28, Number 3
4. Probability Estimation for Roles
In this section we describe the first, basic version of our statistically trained system
for automatically identifying frame elements in text. The system will be extended in
later sections. We first describe in detail the sentence- and constituent-level features
on which our system is based and then use these features to calculate probabilities
for predicting frame element labels in Section 4.2. In this section we give results for
a system that labels roles using the human-annotated boundaries for the frame ele-
ments within the sentence; we return to the question of automatically identifying the
boundaries in Section 5.
4.1 Features Used in Assigning Semantic Roles
Our system is a statistical one, based on training a classifier on a labeled training set
and testing on a held-out portion of the data. The system is trained by first using an
automatic syntactic parser to analyze the 36,995 training sentences, matching annotated
frame elements to parse constituents and extracting various features from the string
of words and the parse tree. During testing, the parser is run on the test sentences
and the same features are extracted. Probabilities for each possible semantic role r are
then computed from the features. The probability computation is described in the next
section; here we discuss the features used.
The features used represent various aspects of the syntactic structure of the sen-
tence as well as lexical information. The relationship between such surface manifes-
tations and semantic roles is the subject of linking theory (see Levin and Rappaport
Hovav [1996] for a synthesis of work in this area). In general, linking theory argues
that the syntactic realization of arguments of a predicate is predictable from semantics;
exactly how this relationship works, however, is the subject of much debate. Regardless
of the underlying mechanisms used to generate syntax from semantics, the relation-
ship between the two suggests that it may be possible to learn to recognize semantic
relationships from syntactic cues, given examples with both types of information.
4.1.1 Phrase Type. Different semantic roles tend to be realized by different syntactic
categories. For example, in communication frames, the Speaker is likely to appear
as a noun phrase, Topic as a prepositional phrase or noun phrase, and Medium as a
prepositional phrase, as in: ?[Speaker We ] talked [Topic about the proposal ] [Medium over
the phone ] .?
The phrase type feature we used indicates the syntactic category of the phrase
expressing the semantic roles, using the set of syntactic categories of the Penn Treebank
project, as described in Marcus, Santorini, and Marcinkiewicz (1993). In our data, frame
elements are most commonly expressed as noun phrases (NPs, 47% of frame elements
in the training set), and prepositional phrases (PPs, 22%). The next most common
categories are adverbial phrases (ADVPs, 4%), particles (e.g. ?make something up?;
PRTs, 2%) and clauses (SBARs, 2%, and Ss, 2%). (Tables 22 and 23 in the Appendix
provides a listing of Penn Treebank?s part-of-speech tags and constituent labels.)
We used Collins? (1997) statistical parser trained on examples from the Penn Tree-
bank to generate parses of the same format for the sentences in our data. Phrase types
were derived automatically from parse trees generated by the parser, as shown in Fig-
ure 2. Given the automatically generated parse tree, the constituent spanning the same
set of words as each annotated frame element was found, and the constituent?s nonter-
minal label was taken as the phrase type. In cases in which more than one constituent
matches because of a unary production in the parse tree, the higher constituent was
chosen.
253
Gildea and Jurafsky Automatic Labeling of Semantic Roles
He
PRP
NP
heard
VBD
the sound of liquid slurping in a metal container
NP
as
IN
Farrell
NNP
NP
approached
VBD
him
PRP
NP
from
IN
behind
NN
NP
PP
VP
S
SBAR
VP
S
target SourceGoalTheme
Figure 2
A sample sentence with parser output (above) and FrameNet annotation (below). Parse
constituents corresponding to frame elements are highlighted.
The matching was performed by calculating the starting and ending word po-
sitions for each constituent in the parse tree, as well as for each annotated frame
element, and matching each frame element with the parse constituent with the same
beginning and ending points. Punctuation was ignored in this computation. Because
of parsing errors, or, less frequently, mismatches between the parse tree formalism and
the FrameNet annotation standards, for 13% of the frame elements in the training set,
there was no parse constituent matching an annotated frame element. The one case of
systematic mismatch between the parse tree formalism and the FrameNet annotation
standards is the FrameNet convention of including both a relative pronoun and its
antecedent in frame elements, as in the first frame element in the following sentence:
(15) In its rough state he showed it to [Agt the Professor, who ] bent [BPrt his
grey beard ] [Path over the neat script ] and read for some time in silence.
Mismatch caused by the treatment of relative pronouns accounts for 1% of the frame
elements in the training set.
During testing, the largest constituent beginning at the frame element?s left bound-
ary and lying entirely within the element was used to calculate the frame element?s
features. We did not use this technique on the training set, as we expected that it
would add noise to the data, but instead discarded examples with no matching parse
constituent. Our technique for finding a near match handles common parse errors
such as a prepositional phrase being incorrectly attached to a noun phrase at the
right-hand edge, and it guarantees that some syntactic category will be returned: the
part-of-speech tag of the frame element?s first word in the limiting case.
4.1.2 Governing Category. The correlation between semantic roles and syntactic re-
alization as subject or direct object is one of the primary facts that linking theory at-
tempts to explain. It was a motivation for the case hierarchy of Fillmore (1968), which
254
Computational Linguistics Volume 28, Number 3
allowed such rules as ?If there is an underlying Agent, it becomes the syntactic sub-
ject.? Similarly, in his theory of macroroles, Van Valin (1993) describes the Actor as
being preferred in English for the subject. Functional grammarians consider syntactic
subjects historically to have been grammaticalized agent markers. As an example of
how such a feature can be useful, in the sentence ?He drove the car over the cliff,?
the subject NP is more likely to fill the Agent role than the other two NPs. We will
discuss various grammatical-function features that attempt to indicate a constituent?s
syntactic relation to the rest of the sentence, for example, as a subject or object of a
verb.
The first such feature, which we call ?governing category,? or gov, has only two
values, S and VP, corresponding to subjects and objects of verbs, respectively. This
feature is restricted to apply only to NPs, as it was found to have little effect on other
phrase types. As with phrase type, the feature was read from parse trees returned by
the parser. We follow links from child to parent up the parse tree from the constituent
corresponding to a frame element until either an S or VP node is found and assign
the value of the feature according to whether this node is an S or a VP. NP nodes
found under S nodes are generally grammatical subjects, and NP nodes under VP
nodes are generally objects. In most cases the S or VP node determining the value of
this feature immediately dominates the NP node, but attachment errors by the parser
or constructions such as conjunction of two NPs can cause intermediate nodes to
be introduced. Searching for higher ancestor nodes makes the feature robust to such
cases. Even given good parses, this feature is not perfect in discriminating grammatical
functions, and in particular it confuses direct objects with adjunct NPs such as temporal
phrases. For example, town in the sentence ?He left town? and yesterday in the sentence
?He left yesterday? will both be assigned a governing category of VP. Direct and
indirect objects both appear directly under the VP node. For example, in the sentence
?He gave me a new hose,? me and a new hose are both assigned a governing category
of VP. More sophisticated handling of such cases could improve our system.
4.1.3 Parse Tree Path. Like the governing-category feature described above, the parse
tree path feature (path) is designed to capture the syntactic relation of a constituent
to the rest of the sentence. The path feature, however, describes the syntactic relation
between the target word (that is, the predicate invoking the semantic frame) and the
constituent in question, whereas the gov feature is independent of where the target
word appears in the sentence; that is, it identifies all subjects whether they are the
subject of the target word or not.
The path feature is defined as the path from the target word through the parse
tree to the constituent in question, represented as a string of parse tree nonterminals
linked by symbols indicating upward or downward movement through the tree, as
shown in Figure 3. Although the path is composed as a string of symbols, our system
treats the string as an atomic value. The path includes, as the first element of the
string, the part of speech of the target word and, as the last element, the phrase type
or syntactic category of the sentence constituent marked as a frame element. After
some experimentation, we settled on a version of the path feature that collapses the
various part-of-speech tags for verbs, including past-tense verb (VBD), third-person
singular present-tense verb (VBZ), other present-tense verb (VBP), and past participle
(VBN), into a single verb tag denoted ?VB.?
Our path feature is dependent on the syntactic representation used, which in our
case is the Treebank-2 annotation style (Marcus et al 1994), as our parser is trained
on this later version of the Treebank data. Figure 4 shows the annotation for the
sentence ?They expect him to cut costs throughout the organization,? which exhibits
255
Gildea and Jurafsky Automatic Labeling of Semantic Roles
S
NP VP
NP
He ate some pancakes
PRP
DT NN
VB
Figure 3
In this example, the path from the target word ate to the frame element He can be represented
as VB?VP?S?NP, with ? indicating upward movement in the parse tree and ? downward
movement. The NP corresponding to He is found as described in Section 4.1.1.
Figure 4
Treebank annotation of raising constructions.
the syntactic phenomenon known as subject-to-object raising, in which the main verb?s
object is interpreted as the embedded verb?s subject. The Treebank-2 style tends to be
generous in its usage of S nodes to indicate clauses, a decision intended to make
possible a relatively straightforward mapping from S nodes to predications. In this
example, the path from cut to the frame element him would be VB?VP?VP?S?NP,
which typically indicates a verb?s subject, despite the accusative case of the pronoun
him. For the target word of expect in the sentence of Figure 4, the path to him would
be VB?VP?S?NP, rather than the typical direct-object path of VB?VP?NP.
An example of Treebank-2 annotation of an ?equi? construction, in which a noun
phrase serves as an argument of both the main and subordinate verbs, is shown in
Figure 5. Here, an empty category is used in the subject position of the subordinate
clause and is co-indexed with the NP Congress in the direct-object position of the main
clause. The empty category, however, is not used in the statistical model of the parser
or shown in its output and is also not used by the FrameNet annotation, which would
mark the NP Congress as a frame element of raise in this example. Thus, the value
of our path feature from the target word raise to the frame element Congress would
256
Computational Linguistics Volume 28, Number 3
Figure 5
Treebank annotation of equi constructions. An empty category is indicated by an asterisk, and
co-indexing by superscript numeral.
Table 2
Most frequent values of the path feature in the training data.
Frequency Path Description
14.2% VB?VP?PP PP argument/adjunct
11.8 VB?VP?S?NP Subject
10.1 VB?VP?NP Object
7.9 VB?VP?VP?S?NP Subject (embedded VP)
4.1 VB?VP?ADVP Adverbial adjunct
3.0 NN?NP?NP?PP Prepositional complement of noun
1.7 VB?VP?PRT Adverbial particle
1.6 VB?VP?VP?VP?S?NP Subject (embedded VP)
14.2 No matching parse constituent
31.4 Other
be VB?VP?VP?S?VP?NP, and from the target word of persuaded the path to Congress
would be the standard direct-object path VB?VP?NP.
Other changes in annotation style from the original Treebank style were specifi-
cally intended to make predicate argument structure easy to read from the parse trees
and include new empty (or null) constituents, co-indexing relations between nodes,
and secondary functional tags such as subject and temporal. Our parser output, how-
ever, does not include this additional information, but rather simply gives trees of
phrase type categories. The sentence in Figure 4 is one example of how the change in
annotation style of Treebank-2 can affect this level of representation; the earlier style
assigned the word him an NP node directly under the VP of expect.
The most common values of the path feature, along with interpretations, are shown
in Table 2.
For the purposes of choosing a frame element label for a constituent, the path
feature is similar to the gov feature defined above. Because the path captures more
information than the governing category, it may be more susceptible to parser errors
and data sparseness. As an indication of this, our path feature takes on a total of
2,978 possible values in the training data when frame elements with no matching
257
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Figure 6
Example of target word renting in a small clause.
parse constituent are not counted and 4,086 possible values when paths are found to
the best-matching constituent in these cases. The governing-category feature, on the
other hand, which is defined only for NPs, has only two values (S, corresponding to
subjects, and VP, corresponding to objects). In cases in which the path feature includes
an S or VP ancestor of an NP node as part of the path to the target word, the gov
feature is a function of the path feature. This is the case most of the time, including for
our prototypical subject (VB?VP?S?NP) and object (VB?VP?NP) paths. Of the 35,138
frame elements identified as NPs by the parser, only 4% have a path feature that does
not include a VP or S ancestor. One such example is shown in Figure 6, where the
small clause ?the remainder renting . . .? has no S node, giving a path feature from
renting to the remainder of VB?VP?NP?NP. The value of the gov feature here is VP, as
the algorithm finds the VP of the sentence?s main clause as it follows parent links up
the tree. The feature is spurious in this case, because the main VP is not headed by,
or relevant to, the target word renting.
Systems based on the path and gov features are compared in Section 4.3. The
differences between the two are relatively small for the purpose of identifying semantic
roles when frame element boundaries are known. The path feature will, however, be
important in identifying which constituents are frame elements for a given target word,
as it gives us a way of navigating through the parse tree to find the frame elements
in the sentence.
4.1.4 Position. To overcome errors due to incorrect parses, as well as to see how
much can be done without parse trees, we introduced position as a feature. The position
feature simply indicates whether the constituent to be labeled occurs before or after the
predicate defining the semantic frame. We expected this feature to be highly correlated
with grammatical function, since subjects will generally appear before a verb and
objects after.
Although we do not have hand-checked parses against which to measure the per-
formance of the automatic parser on our corpus, the result that 13% of frame elements
have no matching parse constituent gives a rough idea of the parser?s accuracy. Al-
258
Computational Linguistics Volume 28, Number 3
most all of these cases in which no matching parse constituent was found are due to
parser error. Other parser errors include cases in which a constituent is found, but
with the incorrect label or internal structure. This result also considers only the indi-
vidual constituent representing the frame element: the parse for the rest of the sentence
may be incorrect, resulting in an incorrect value for the grammatical function features
described in the previous two sections. Collins (1997) reports 88% labeled precision
and recall on individual parse constituents on data from the Penn Treebank, roughly
consistent with our finding of at least 13% error.
4.1.5 Voice. The distinction between active and passive verbs plays an important role
in the connection between semantic role and grammatical function, since direct objects
of active verbs often correspond in semantic role to subjects of passive verbs. From
the parser output, verbs were classified as active or passive by building a set of 10
passive-identifying patterns. Each of the patterns requires both a passive auxiliary
(some form of to be or to get) and a past participle. Roughly 5% of the examples were
identified as passive uses.
4.1.6 Head Word. As previously noted, we expected lexical dependencies to be ex-
tremely important in labeling semantic roles, as indicated by their importance in re-
lated tasks such as parsing. Head words of noun phrases can be used to express
selectional restrictions on the semantic types of role fillers. For example, in a commu-
nication frame, noun phrases headed by Bill, brother, or he are more likely to be the
Speaker, whereas those headed by proposal, story, or question are more likely to be the
Topic. (We did not attempt to resolve pronoun references.)
Since the parser we used assigns each constituent a head word as an integral
part of the parsing model, we were able to read the head words of the constituents
from the parser output, employing the same set of rules for identifying the head child
of each constituent in the parse tree. The rules for assigning a head word are listed
in Collins (1999). Prepositions are considered to be the head words of prepositional
phrases. The rules for assigning head words do not attempt to distinguish between
cases in which the preposition expresses the semantic content of a role filler, such as
Path frame elements expressed by prepositional phrases headed by along, through, or
in, and cases in which the preposition might be considered to be purely a case marker,
as in most uses of of, where the semantic content of the role filler is expressed by
the preposition?s object. Complementizers are considered to be heads, meaning that
infinitive verb phrases are always headed by to and subordinate clauses such as in the
sentence ?I?m sure that he came? are headed by that.
4.2 Probability Estimation
For our experiments, we divided the FrameNet corpus as follows: one-tenth of the
annotated sentences for each target word were reserved as a test set, and another one-
tenth were set aside as a tuning set for developing our system. A few target words
where fewer than 10 examples had been chosen for annotation were removed from the
corpus. (Section 9 will discuss generalization to unseen predicates.) In our corpus, the
average number of sentences per target word is only 34, and the number of sentences
per frame is 732, both relatively small amounts of data on which to train frame element
classifiers.
To label the semantic role of a constituent automatically, we wish to estimate a
probability distribution indicating how likely the constituent is to fill each possible
259
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 3
Distributions calculated for semantic role identification: r indicates semantic role, pt phrase
type, gov grammatical function, h head word, and t target word, or predicate.
Distribution Coverage Accuracy Performance
P(r | t) 100.0% 40.9% 40.9%
P(r | pt, t) 92.5 60.1 55.6
P(r | pt, gov, t) 92.0 66.6 61.3
P(r | pt, position, voice) 98.8 57.1 56.4
P(r | pt, position, voice, t) 90.8 70.1 63.7
P(r | h) 80.3 73.6 59.1
P(r | h, t) 56.0 86.6 48.5
P(r | h, pt, t) 50.1 87.4 43.8
role, given the features described above and the predicate, or target word, t:
P(r | h, pt , gov, position , voice, t)
where r indicates semantic role, h head word, and pt phrase type. It would be possible
to calculate this distribution directly from the training data by counting the number
of times each role appears with a combination of features and dividing by the total
number of times the combination of features appears:
P(r | h, pt , gov , position , voice, t) = #(r, h, pt , gov, position , voice, t)
#(h, pt , gov, position , voice, t)
In many cases, however, we will never have seen a particular combination of features
in the training data, and in others we will have seen the combination only a small
number of times, providing a poor estimate of the probability. The small number
of training sentences for each target word and the large number of values that the
head word feature in particular can take (any word in the language) contribute to the
sparsity of the data. Although we expect our features to interact in various ways, we
cannot train directly on the full feature set. For this reason, we built our classifier by
combining probabilities from distributions conditioned on a variety of subsets of the
features.
Table 3 shows the probability distributions used in the final version of the system.
Coverage indicates the percentage of the test data for which the conditioning event had
been seen in training data. Accuracy is the proportion of covered test data for which
the correct role is given the highest probability, and Performance, which is the product
of coverage and accuracy, is the overall percentage of test data for which the correct
role is predicted.3 Accuracy is somewhat similar to the familiar metric of precision
in that it is calculated over cases for which a decision is made, and performance is
similar to recall in that it is calculated over all true frame elements. Unlike in a tra-
ditional precision/recall trade-off, however, these results have no threshold to adjust,
and the task is a multiway classification rather than a binary decision. The distribu-
tions calculated were simply the empirical distributions from the training data. That is,
occurrences of each role and each set of conditioning events were counted in a table,
and probabilities calculated by dividing the counts for each role by the total number
3 Ties for the highest-probabilty role are resolved at random.
260
Computational Linguistics Volume 28, Number 3
Table 4
Sample probabilities for P(r | pt, gov, t) calculated from training data for the verb abduct. The
variable gov is defined only for noun phrases. The roles defined for the removing frame in the
motion domain are Agent (Agt), Theme (Thm), CoTheme (CoThm) (?. . . had been
abducted with him?), and Manner (Manr).
P(r | pt, gov, t) Count in training data
P(r = Agt | pt = NP, gov = S, t = abduct) = .46 6
P(r = Thm | pt = NP, gov = S, t = abduct) = .54 7
P(r = Thm | pt = NP, gov = VP, t = abduct) = 1 9
P(r = Agt | pt = PP, t = abduct) = .33 1
P(r = Thm | pt = PP, t = abduct) = .33 1
P(r = CoThm | pt = PP, t = abduct) = .33 1
P(r =Manr | pt = ADVP, t = abduct) = 1 1
of observations for each conditioning event. For example, the distribution P(r | pt, t)
was calculated as follows:
P(r | pt , t) = #(r, pt , t)
#(pt , t)
Some sample probabilities calculated from the training are shown in Table 4.
As can be seen from Table 3, there is a trade-off between more-specific distri-
butions, which have high accuracy but low coverage, and less-specific distributions,
which have low accuracy but high coverage. The lexical head word statistics, in par-
ticular, are valuable when data are available but are particularly sparse because of the
large number of possible head words.
To combine the strengths of the various distributions, we merged them in various
ways to obtain an estimate of the full distribution P(r | h, pt , gov, position , voice, t). The
first combination method is linear interpolation, which simply averages the probabil-
ities given by each of the distributions:
P(r | constituent) = ?1P(r | t) + ?2P(r | pt , t)
+ ?3P(r | pt , gov, t) + ?4P(r | pt , position , voice)
+ ?5P(r | pt , position , voice, t) + ?6P(r | h)
+ ?7P(r | h, t) + ?8P(r | h, pt , t)
where
?
i ?i = 1. The geometric mean, when expressed in the log domain, is similar:
P(r | constituent) = 1Z exp{ ?1 log P(r | t) + ?2 log P(r | pt , t)
+ ?3 log P(r | pt , gov, t) + ?4 log P(r | pt , position , voice)
+ ?5 log P(r | pt , position , voice, t) + ?6 log P(r | h)
+ ?7 log P(r | h, t) + ?8 log P(r | h, pt , t) }
where Z is a normalizing constant ensuring that
?
r P(r | constituent) = 1.
Results for systems based on linear interpolation are shown in the first row of
Table 5. These results were obtained using equal values of ? for each distribution
defined for the relevant conditioning event (but excluding distributions for which the
conditioning event was not seen in the training data). As a more sophisticated method
of choosing interpolation weights, the expectation maximization (EM) algorithm was
261
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 5
Results on development set, 8,167 observations.
Combining Method Correct
Equal linear interpolation 79.5%
EM linear interpolation 79.3
Geometric mean 79.6
Backoff, linear interpolation 80.4
Backoff, geometric mean 79.6
Baseline: Most common role 40.9
Table 6
Results on test set, 7,900 observations.
Combining Method Correct
EM linear interpolation 78.5%
Backoff, linear interpolation 76.9
Baseline: Most common role 40.6
used to estimate the likelihood of the observed role?s being produced by each of the
distributions in the general techniques of Jelinek and Mercer (1980). Because a number
of the distributions used may have no training data for a given set of variables, the
data were divided according to the set of distributions available, and a separate set of
interpolation weights was trained for each set of distributions. This technique (line 2
of Table 5) did not outperform equal weights even on the data used to determine
the weights. Although the EM algorithm is guaranteed to increase the likelihood of
the training data, that likelihood does not always correspond to our scoring, which is
based only on whether the correct outcome is assigned the highest probability. Results
of the EM interpolation on held-out test data are shown in Table 6.
Experimentation has shown that the weights used have relatively little impact in
our interpolation scheme, no doubt because the evaluation metric depends only on the
ranking of the probabilities and not on their exact values. Changing the interpolation
weights rarely changes the probabilities of the roles enough to change their ranking.
What matters most is whether a combination of variables has been seen in the training
data or not.
Results for the geometric mean are shown in row 3 of Table 5. As with linear
interpolation, the exact weights were found to have little effect, and the results shown
reflect equal weights. An area we have not explored is the use of the maximum-entropy
techniques of, for example, Pietra, Pietra, and Lafferty (1997), to set weights for the
log-linear model, either at the level of combining our probability distributions or at
the level of calculating weights for individual values of the features.
In the ?backoff? combination method, a lattice was constructed over the distribu-
tions in Table 3 from more-specific conditioning events to less-specific, as shown in
Figure 7. The lattice is used to select a subset of the available distributions to com-
bine. The less-specific distributions were used only when no data were present for any
more-specific distribution. Thus, the distributions selected are arranged in a cut across
the lattice representing the most-specific distributions for which data are available. The
selected probabilities were combined with both linear interpolation and a geometric
mean, with results shown in Table 5. The final row of the table represents the baseline
262
Computational Linguistics Volume 28, Number 3
P(r | h, t) P(r | pt, t) P(r | pt, position, voice)
P(r | pt, position, voice, t)P(r | pt, gf, t)
P(r | t)P(r | h)
P(r | h, pt, t)
Figure 7
Lattice organization of the distributions from Table 3, with more-specific distributions toward
the top.
of always selecting the most common role of the target word for all its constituents,
that is, using only P(r | t).
Although this lattice is reminiscent of techniques of backing off to less specific
distributions commonly used in n-gram language modeling, it differs in that we use the
lattice only to select distributions for which the conditioning event has been seen in the
training data. Discounting and deleted interpolation methods in language modeling
typically are used to assign small, nonzero probability to a predicted variable unseen in
the training data even when a specific conditioning event has been seen. In our case,
we are perfectly willing to assign zero probability to a specific role (the predicted
variable). We are interested only in finding the role with the highest probability, and
a role given a small, nonzero probability by smoothing techniques will still not be
chosen as the classifier?s output.
The lattice presented in Figure 7 represents just one way of choosing subsets of
features for our system. Designing a feature lattice can be thought of as choosing
a set of feature subsets: once the probability distributions of the lattice have been
chosen, the graph structure of the lattice is determined by the subsumption relations
among the sets of conditioning variables. Given a set of N conditioning variables,
there are 2N possible subsets, and 22
N
possible sets of subsets, giving us a doubly
exponential number of possible lattices. The particular lattice of Figure 7 was chosen to
represent some expected interaction between features. For example, we expect position
and voice to interact, and they are always used together. We expect the head word h
and the phrase type pt to be relatively independent predictors of the semantic role
and therefore include them separately as roots of the backoff structure. Although we
will not explore all the possibilities for our lattice, some of the feature interactions are
examined more closely in Section 4.3.
The final system performed at 80.4% accuracy, which can be compared to the 40.9%
achieved by always choosing the most probable role for each target word, essentially
chance performance on this task. Results for this system on test data, held out during
development of the system, are shown in Table 6. Surprisingly, the EM-based interpo-
lation performed better than the lattice-based system on the held-out test set, but not
on the data used to set the weights in the EM-based system. We return to an analysis
of which roles are hardest to classify in Section 9.1.
4.3 Interaction of Features
Three of our features, position, gov, and path, attempt to capture the syntactic relation
between the target word and the constituent to be labeled, and in particular to dif-
ferentiate the subjects from objects of verbs. To compare these three features directly,
experiments were performed using each feature alone in an otherwise identical sys-
263
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 7
Different estimators of grammatical function. The columns of the table correspond to
Figures 8a, 8b, and 8c.
Feature W/o voice Independent In conjunction
voice feature with voice
path 79.4% 79.2% 80.4%
gov 79.1 79.2 80.7
position 79.9 79.7 80.5
? 76.3 76.0 76.0
P(r | h, t) P(r | pt, t)
P(r | t)P(r | h)
P(r | h, pt, t) P(r | pt, GF, voice, t)
P(r | pt, GF, voice)
P(r | h, t) P(r | pt, t)
P(r | t)P(r | h)
P(r | h, pt, t) P(r | pt, voice, t)
P(r | pt, voice)
P(r | pt, GF, t)
P(r | h, t) P(r | pt, t)
P(r | t)P(r | h)
P(r | h, pt, t) P(r | pt, GF, t)a)
b)
c)
Figure 8
Lattice structures for comparing grammatical-function features.
tem. Results are shown in Table 7. For the first set of experiments, corresponding to
the first column of Table 7, no voice information was used, with the result that the
remaining distributions formed the lattice of Figure 8a. (?GF? (grammatical function)
in the figure represents one of the features position, gov, and path.) Adding voice infor-
mation back into the system independently of the grammatical-function feature results
264
Computational Linguistics Volume 28, Number 3
P(r | h, t)
P(r | t)
P(r | pt, path, t)
Figure 9
Minimal lattice.
in the lattice of Figure 8b, corresponding to the second column of Table 7. Choosing
distributions such that the grammatical function and voice features are always used
together results in Figure 8c, corresponding to the third column of Table 7. In each
case, as in previous results, the grammatical function feature was used only when
the candidate constituent was an NP. The last row of Table 7 shows results using no
grammatical-function feature: the distributions making use of GF are removed from
the lattices of Figure 8.
As a guideline for interpreting these results, with 8,167 observations, the threshold
for statistical significance with p < .05 is a 1.0% absolute difference in performance.
It is interesting to note that looking at a constituent?s position relative to the target
word performed as well as either of our features that read grammatical function off
the parse tree, both with and without passive information. The gov and path features
seem roughly equivalent in performance.
Using head word, phrase type, and target word without either position or gram-
matical function yielded only 76.3% accuracy, indicating that although the two features
accomplish a similar goal, it is important to include some measure of the constituent?s
relationship to the target word, whether relative position or either of the syntactic
features.
Use of the active/passive voice feature seems to be beneficial only when the feature
is tied to grammatical function: the second column in Table 7 shows no improvement
over the first, while the right-hand column, where grammatical function and voice are
tied, shows gains (although only trends) of at least 0.5% in all cases. As before, our
three indicators of grammatical function seem roughly equivalent, with the best result
in this case being the gov feature. The lattice of Figure 8c performs as well as our
system of Figure 7, indicating that including both position and either of the syntactic
relations is redundant.
As an experiment to see how much can be accomplished with as simple a system
as possible, we constructed the minimal lattice of Figure 9, which includes just two
distributions, along with a prior for the target word to be used as a last resort when no
data are available. This structure assumes that head word and grammatical function
are independent. It further makes no use of the voice feature. We chose the path feature
as the representation of grammatical function in this case. This system classified 76.3%
of frame elements correctly, indicating that one can obtain roughly nine-tenths the
performance of the full system with a simple approach. (We will return to a similar
system for the purposes of cross-domain experiments in Section 9.)
5. Identification of Frame Element Boundaries
In this section we examine the system?s performance on the task of locating the frame
elements in a sentence. Although our probability model considers the question of
finding the boundaries of frame elements separately from the question of finding the
correct label for a particular frame element, similar features are used to calculate both
265
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 8
Sample probabilities of a constituent?s being a frame element.
Distribution Sample Probability Count in training data
P(fe | path) P(fe | path = VB?VP?ADJP?ADVP) = 1 1
P(fe | path = VB?VP?NP) = .73 3,963
P(fe | path = VB?VP?NP?PP?S) = 0 22
P(fe | path, t) P(fe | path = JJ?ADJP?PP, t = apparent) = 1 10
P(fe | path = NN?NP?PP?VP?PP, t = departure) = .4 5
P(fe | h, t) P(fe | h = sudden, t = apparent) = 0 2
P(fe | h = to, t = apparent) = .11 93
P(fe | h = that, t = apparent) = .21 81
probabilities. In the experiments below, the system is no longer given frame element
boundaries but is still given as inputs the human-annotated target word and the frame
to which it belongs. We do not address the task of identifying which frames come into
play in a sentence but envision that existing word sense disambiguation techniques
could be applied to the task.
As before, features are extracted from the sentence and its parse and are used to
calculate probability tables, with the predicted variable in this case being fe, a binary
indicator of whether a given constituent in the parse tree is or is not a frame element.
The features used were the path feature of Section 4.1.3, the identity of the target
word, and the identity of the constituent?s head word. The probability distributions
calculated from the training data were P(fe | path), P(fe | path , t), and P(fe | h, t), where
fe indicates an event where the parse constituent in question is a frame element, path
the path through the parse tree from the target word to the parse constituent, t the
identity of the target word, and h the head word of the parse constituent. Some sample
values from these distributions are shown in Table 8. For example, the path VB?VP?NP,
which corresponds to the direct object of a verbal target word, had a high probability
of being a frame element. The table also illustrates cases of sparse data for various
feature combinations.
By varying the probability threshold at which a decision is made, one can plot a
precision/recall curve as shown in Figure 10. P(fe | path , t) performs relatively poorly
because of fragmentation of the training data (recall that only about 30 sentences are
available for each target word). Although the lexical statistic P(fe | h, t) alone is not
useful as a classifier, using it in linear interpolation with the path statistics improves
results. The curve labeled ?interpolation? in Figure 10 reflects a linear interpolation of
the form
P(fe | p, h, t) = ?1P(fe | p) + ?2P(fe | p, t) + ?3P(fe | h, t) (16)
Note that this method can identify only those frame elements that have a correspond-
ing constituent in the automatically generated parse tree. For this reason, it is in-
teresting to calculate how many true frame elements overlap with the results of the
system, relaxing the criterion that the boundaries must match exactly. Results for par-
tial matching are shown in Table 9. Three types of overlap are possible: the identified
constituent entirely within the true frame element, the true frame element entirely
within the identified constituent, and each sequence partially contained by the other.
An example of the first case is shown in Figure 11, where the true Message frame ele-
ment is Mandarin by a head, but because of an error in the parser output, no constituent
exactly matches the frame element?s boundaries. In this case, the system identifies
266
Computational Linguistics Volume 28, Number 3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
re
ca
ll
precision
P(fe|path)
P(fe|path, t)
interpolation
Figure 10
Plot of precision/recall curve for various methods of identifying frame elements. Recall is
calculated over only frame elements with matching parse constituents.
Table 9
Results on identifying frame elements (FEs), including partial matches. Results obtained using
P(fe | path) with threshold at 0.5. A total of 7,681 constituents were identified as FEs, and 8,167
FEs were present in hand-annotations, of which matching parse constituents were present for
7,053 (86%).
Type of Overlap Identified Constituents Number
Exactly matching boundaries 66% 5,421
Identified constituent entirely within true frame element 8 663
True frame element entirely within identified constituent 7 599
Both partially within the other 0 26
No overlap with any true frame element 13 972
two frame elements, indicated by shading, which together span the true frame ele-
ment.
When the automatically identified constituents were fed through the role-labeling
system described above, 79.6% of the constituents that had been correctly identified
in the first stage were assigned the correct role in the second, roughly equivalent
to the performance when roles were assigned to constituents identified by hand. A
more sophisticated integrated system for identifying and labeling frame elements is
described in Section 7.1.
6. Generalizing Lexical Statistics
As can be seen from Table 3, information about the head word of a constituent
is valuable in predicting the constituent?s role. Of all the distributions presented,
267
Gildea and Jurafsky Automatic Labeling of Semantic Roles
As the horses were led back... ,
SBAR
the
DT
result
NN
NP
was
VBD
announced :
VBN
Mandarin
NN
NP
by
IN
a
DT
head
NN
NP
PP
VP
VP
S
target Message
Figure 11
An example of overlap between identified frame elements and the true boundaries, caused by
parser error. In this case two frame elements identified by the classifier (shaded subtrees) are
entirely within the human annotation (indicated below the sentence), contributing two
instances to row 2 of Table 9.
P(r | h, pt , t) predicts the correct role most often (87.4% of the time) when training
data for a particular head word have been seen. Because of the large vocabulary of
possible head words, however, it also has the smallest coverage, meaning that it is
likely that, for a given case in the test data, no frame element with the same head
word will have been seen in the set of training sentences for the target word in ques-
tion. To capitalize on the information provided by the head word, we wish to find
a way to generalize from head words seen in the training data to other head words.
In this section we compare three different approaches to the task of generalizing over
head words: automatic clustering of a large vocabulary of head words to identify
words with similar semantics; use of a hand-built ontological resource, WordNet, to
organize head words in a semantic hierarchy; and bootstrapping to make use of un-
labeled data in training the system. We will focus on frame elements filled by noun
phrases, which constitute roughly half the total.
6.1 Automatic Clustering
To find groups of head words that are likely to fill the same semantic roles, an auto-
matic clustering of nouns was performed using word co-occurrence data from a large
corpus. This technique is based on the expectation that words with similar semantics
will tend to co-occur with the same other sets of words. For example, nouns describing
foods will tend to occur as direct objects of verbs such as eat devour, and savor. The
clustering algorithm attempts to find such patterns of co-occurrence from the counts
of grammatical relations between pairs of specific words in the corpus, without the
use of any external knowledge or semantic representation.
We extracted verb?direct object relations from an automatically parsed version of
the British National Corpus, using the parser of Carroll and Rooth (1998).4 Clustering
4 We are indebted to Mats Rooth and Sabine Schulte im Walde for providing us with the parsed corpus.
268
Computational Linguistics Volume 28, Number 3
was performed using the probabilistic model of co-occurrence described in detail by
Hofmann and Puzicha (1998). (For other natural language processing [NLP] applica-
tions of the probabilistic clustering algorithm, see, e.g., Rooth [1995], Rooth et al [1999];
for application to language modeling, see Gildea and Hofmann [1999]. According to
this model, the two observed variables, in this case the verb and the head noun of its
object, can be considered independent given the value of a hidden cluster variable, c:
P(n, v) =
?
c
P(c)P(n | c)P(v | c)
One begins by setting a priori the number of values that c can take and using the
EM algorithm to estimate the distributions P(c), P(n | c), and P(v | c). Deterministic
annealing was used to prevent overfitting of the training data.
We are interested only in the clusters of nouns given by the distribution P(n | c):
the verbs and the distribution P(v | c) are thrown away once training is complete. Other
grammatical relations besides direct object could be used, as could a set of relations.
We used the direct object (following other clustering work such as Pereira, Tishby, and
Lee [1993]) because it is particularly likely to exhibit semantically significant selectional
restrictions.
A total of 2,610,946 verb-object pairs were used as training data for the clustering,
with a further 290,105 pairs used as a cross-validation set to control the parameters of
the clustering algorithm. Direct objects were identified as noun phrases directly under
a verb phrase node?not a perfect technique, since it also finds nominal adjuncts such
as ?I start today.? Forms of the verb to be were excluded from the data, as its co-
occurrence patterns are not semantically informative. The number of values possible
for the latent cluster variable was set to 256. (Comparable results were found with 64
clusters; the use of deterministic annealing prevents large numbers of clusters from
resulting in overfitting.)
The soft clustering of nouns thus generated is used as follows: for each example
in the frame element?annotated training data, probabilities for values of the hidden
cluster variable were calculated using Bayes? rule:
P(c | h) = P(h | c)P(c)?
c? P(h | c?)P(c?)
The clustering was applied only to noun phrase constituents; the distribution P(n | c)
from the clustering is used as a distribution P(h | c) over noun head words.
Using the cluster probabilities, a new estimate of P(r | c, pt , t) is calculated for
cases where pt , the phrase type or syntactic category of the constituent, is NP:
P(r | c, pt , t) =
?
j:pt j = pt ,tj = t,rj = r
P(cj | hj)
?
j:pt j = pt ,tj = t
P(cj | hj)
where j is an index ranging over the frame elements in the training set and their
associated features pt , t, h and their semantic roles r.
During testing, a smoothed estimate of the head word?based role probability is
calculated by marginalizing over cluster values:
P(r | h, pt , t) =
?
c
P(r | c, pt , t)P(c | h)
again using P(c | h) = P(h|c)P(c)?
c?
P(h|c?)P(c?) .
269
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 10
Clustering results on NP constituents only, 4,086 instances.
Distribution Coverage Accuracy Performance
P(r | h, pt, t) 41.6% 87.0% 36.1%
?
c P(r | c, pt, t)P(c | h) 97.9 79.7 78.0
Interpolation of unclustered distributions 100.0 83.4 83.4
Unclustered distributions + clustering 100.0 85.0 85.0
As with the other methods of generalization described in this section, automatic
clustering was applied only to noun phrases, which represent 50% of the constituents
in the test data. We would not expect head word to be as valuable for other phrase
types. The second most common category is prepositional phrases. The head of a
prepositional phrase (PP) is considered to be the preposition, according to the rules
we use, and because the set of prepositions is small, coverage is not as great a problem.
Furthermore, the preposition is often a direct indicator of the semantic role. (A more
complete model might distinguish between cases in which the preposition serves as a
case or role marker and others in which it is semantically informative, with clustering
performed on the preposition?s object in the former case. We did not attempt to make
this distinction.) Phrase types other than NP and PP make up only a small proportion
of the data.
Table 10 shows results for the use of automatic clustering on constituents identified
by the parser as noun phrases. As can be seen in the table, the vocabulary used for
clustering includes almost all (97.9%) of the test data, and the decrease in accuracy from
direct lexical statistics to clustered statistics is relatively small (from 87.0% to 79.7%).
When combined with the full system described above, clustered statistics increase
performance on NP constituents from 83.4% to 85.0% (statistically significant at p <
.05). Over the entire test set, this translates into an improvement from 80.4% to 81.2%.
6.2 Using a Semantic Hierarchy: WordNet
The automatic clustering described above can be seen as an imperfect method of
deriving semantic classes from the vocabulary, and we might expect a hand-developed
set of classes to do better. We tested this hypothesis using WordNet (Fellbaum 1998), a
freely available semantic hierarchy. The basic technique, when presented with a head
word for which no training examples had been seen, was to ascend the type hierarchy
until reaching a level for which training data are available. To do this, counts of
training data were percolated up the semantic hierarchy in a technique similar to that
of, for example, McCarthy (2000). For each training example, the count #(r, s, pt , t)
was incremented in a table indexed by the semantic role r, WordNet sense s, phrase
type pt , and target word t, for each WordNet sense s above the head word h in the
hypernym hierarchy. In fact, the WordNet hierarchy is not a tree, but rather includes
multiple inheritance. For example, person has as hypernyms both life form and causal
agent. In such cases, we simply took the first hypernym listed, effectively converting
the structure into a tree. A further complication is that several WordNet senses are
possible for a given head word. We simply used the first sense listed for each word; a
word sense disambiguation module capable of distinguishing WordNet senses might
improve our results.
As with the clustering experiments reported above, the WordNet hierarchy was
used only for noun phrases. The WordNet hierarchy does not include pronouns; to
270
Computational Linguistics Volume 28, Number 3
Table 11
WordNet results on NP constituents only, 4,086 instances.
Distribution Coverage Accuracy Performance
P(r | h, pt, t) 41.6% 87.0% 36.1%
WordNet: P(r | s, pt, t) 80.8 79.5 64.1
Interpolation of unclustered distributions 100.0 83.4 83.4
Unclustered distributions + WordNet 100.0 84.3 84.3
increase coverage, the personal pronouns I, me, you, he, she, him, her, we, and us were
added as hyponyms of person. Pronouns that refer to inanimate, or both animate and
inanimate, objects were not included. In addition, the CELEX English lexical database
(Baayen, Piepenbrock, and Gulikers 1995) was used to convert plural nouns to their
singular forms.
As shown in Table 11, accuracy for the WordNet technique is roughly the same as
that in the automatic clustering results in Table 10: 84.3% on NPs, as opposed to 85.0%
with automatic clustering. This indicates that the error introduced by the unsupervised
clustering is roughly equivalent to the error caused by our arbitrary choice of the
first WordNet sense for each word and the first hypernym for each WordNet sense.
Coverage for the WordNet technique is lower, however, largely because of the absence
of proper nouns from WordNet, as well as the absence of nonanimate pronouns (both
personal pronouns such as it and they and indefinite pronouns such as something and
anyone). A dictionary of proper nouns would likely help improve coverage, and a
module for anaphora resolution might help cases with pronouns, with or without
the use of WordNet. The conversion of plural forms to singular base forms was an
important part of the success of the WordNet system, increasing coverage from 71.0%
to 80.8%. Of the remaining 19.2% of all noun phrases not covered by the combination of
lexical and WordNet sense statistics, 22% consisted of head words defined in WordNet,
but for which no training data were available for any hypernym, and 78% consisted
of head words not defined in WordNet.
6.3 Bootstrapping from Unannotated Data
A third way of attempting to improve coverage of the lexical statistics is to ?bootstrap,?
or label unannotated data with the automatic system described in Sections 4 and
5 and use the (imperfect) result as further training data. This can be considered a
variant of the EM algorithm, although we use the single most likely hypothesis for
the unannotated data, rather than calculating the expectation over all hypotheses. Only
one iteration of training on the unannotated data was performed.
The unannotated data used consisted of 156,590 sentences containing the target
words of our corpus, increasing the total amount of data available to roughly six times
the 36,995 annotated training sentences.
Table 12 shows results on noun phrases for the bootstrapping method. The accu-
racy of a system trained only on data from the automatic labeling (Pauto) is 81.0%, rea-
sonably close to the 87.0% for the system trained only on annotated data (Ptrain ). Com-
bining the annotated and automatically labeled data increases coverage from 41.6%
to 54.7% and performance to 44.5%. Because the automatically labeled data are not
as accurate as the annotated data, we can do slightly better by using the automatic
data only in cases where no training data are available, backing off to the distribution
Pauto from Ptrain . The fourth row of Table 12 shows results with Pauto incorporated
271
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 12
Bootstrapping results on NP constituents only, 4,086 instances.
Distribution Coverage Accuracy Performance
Ptrain(r | h, pt, t) 41.6% 87.0% 36.1%
Pauto(r | h, pt, t) 48.2 81.0 39.0
Ptrain+auto(r | h, pt, t) 54.7 81.4 44.5
Ptrain, backoff to Pauto 54.7 81.7 44.7
Interpolation of unclustered distributions 100.0 83.4 83.4
Unclustered distributions + Pauto 100.0 83.2 83.2
into the backoff lattice of all the features of Figure 7, which actually resulted in a slight
decrease in performance from the system without the bootstrapped data, shown in the
third row. This is presumably because, although the system trained on automatically
labeled data performed with reasonable accuracy, many of the cases it classifies cor-
rectly overlap with the training data. In fact our backing-off estimate of P(r | h, pt , t)
classifies correctly only 66% of the additional cases that it covers over Ptrain(r | h, pt , t).
6.4 Discussion
The three methods of generalizing lexical statistics each had roughly equivalent accu-
racy on cases for which they were able to derive an estimate of the role probabilities
for unseen head words. The differences between the three were primarily due to how
much they could improve the coverage of the estimator, that is, how many new noun
heads they were able to handle. The automatic-clustering method performed by far
the best on this metric; only 2.1% of test cases were unseen in the data used for the
automatic clustering. This indicates how much can be achieved with unsupervised
methods given very large training corpora. The bootstrapping technique described
here, although it has a similar unsupervised flavor, made use of much less data than
the corpus used for noun clustering. Unlike probabilistic clustering, the bootstrapping
technique can make use of only those sentences containing the target words in ques-
tion. The WordNet experiment, on the other hand, indicates both the usefulness of
hand-built resources when they apply and the difficulty of attaining broad coverage
with such resources. Combining the three systems described would indicate whether
their gains are complementary or overlapping.
7. Verb Argument Structure
One of the primary difficulties in labeling semantic roles is that one predicate may
be used with different argument structures: for example, in the sentences ?He opened
the door? and ?The door opened,? the verb open assigns different semantic roles to
its syntactic subject. In this section we compare two strategies for handling this type
of alternation in our system: a sentence-level feature for frame element groups and a
subcategorization feature for the syntactic uses of verbs. Then a simple system using
the predicate?s argument structure, or syntactic signature, as the primary feature will
be contrasted with previous systems based on local, independent features.
7.1 Priors on Frame Element Groups
The system described in previous sections for classifying frame elements makes an
important simplifying assumption: it classifies each frame element independent of the
decisions made for the other frame elements in the sentence. In this section we remove
272
Computational Linguistics Volume 28, Number 3
Table 13
Sample frame element groups for the verb blame.
Frame Element Group Example Sentences
{Evaluee} Holman would characterize this
as blaming [Evaluee the poor ] .
{Judge, Evaluee, Reason} The letter quotes Black as
saying that [Judge white and Navajo ranchers ]
misrepresent their livestock losses and
blame [Reason everything ] [Evaluee on coyotes ] .
[Judge She ] blames [Evaluee the Government ]
[Reason for failing to do enough to help ] .
{Judge, Evaluee} The only dish she made that we could tolerate was
[Evaluee syrup tart which ] [Judge we ]
praised extravagantly with the result that it became
our unhealthy staple diet.
Table 14
Frame element groups for the verb blame in the Judgment frame.
Frame Element Group Probability
{Evaluee, Judge, Reason} 0.549
{Evaluee, Judge} 0.160
{Evaluee, Reason} 0.167
{Evaluee} 0.097
{Evaluee, Judge, Role } 0.014
{Judge} 0.007
{Judge, Reason} 0.007
this assumption and present a system that can make use of the information that, for
example, a given target word requires that one role always be present or that having
two instances of the same role is extremely unlikely.
To capture this information, we introduce the notion of a frame element group,
which is the set of frame element roles present in a particular sentence (technically a
multiset, as duplicates are possible, though quite rare). Frame element groups (FEGs)
are unordered: examples are shown in Table 13. Sample probabilities from the training
data for the frame element groups of the target word blame are shown in Table 14.
The FrameNet corpus recognizes three types of ?null-instantiated? frame elements
(Fillmore 1986), which are implied but do not appear in the sentence. An example of
null instantiation is the sentence ?Have you eaten?? where food is understood. We
did not attempt to identify such null elements, and any null-instantiated roles are not
included in the sentence?s FEG. This increases the variability of observed FEGs, as a
predicate may require a certain role but allow it to be null instantiated.
Our system for choosing the most likely overall assignment of roles for all the
frame elements of a sentence uses an approximation that we derive beginning with
the true probability of the optimal role assignment r?:
r? = argmaxr1...n P(r1...n | t, f1...n)
where P(r1...n | t, f1...n) represents the probability of an overall assignment of roles ri
to each of the n constituents of a sentence, given the target word t and the various
features fi of each of the constituents. In the first step we apply Bayes? rule to this
273
Gildea and Jurafsky Automatic Labeling of Semantic Roles
quantity,
r? = argmaxr1...n P(r1...n | t)
P(f1...n | r1...n, t)
P(f1...n | t)
and in the second we make the assumption that the features of the various constituents
of a sentence are independent given the target word and each constituent?s role and
discard the term P(f1...n | t), which is constant with respect to r:
r? = argmaxr1...n P(r1...n | t)
?
i
P(fi | ri, t)
We estimate the prior over frame element assignments as the probability of the frame
element groups, represented with the set operator {}:
r? = argmaxr1...n P({r1...n} | t)
?
i
P(fi | ri, t)
We then apply Bayes? rule again,
r? = argmaxr1...n P({r1...n} | t)
?
i
P(ri | fi, t)P(fi | t)
P(ri | t)
and finally discard the feature prior P(fi | t) as being constant over the argmax expres-
sion:
r? = argmaxr1...n P({r1...n} | t)
?
i
P(ri | fi, t)
P(ri | t)
This leaves us with an expression in terms of the prior for frame element groups of
a particular target word P({r1...n} | t), the local probability of a frame element given
a constituent?s features P(ri | fi, t) on which our previous system was based, and the
individual priors for the frame elements chosen P(ri | t). This formulation can be used
to assign roles either when the frame element boundaries are known or when they
are not, as we will discuss later in this section.
Calculating empirical FEG priors from the training data is relatively straightfor-
ward, but the sparseness of the data presents a problem. In fact, 15% of the test
sentences had an FEG not seen in the training data for the target word in question.
Using the empirical value for the FEG prior, these sentences could never be correctly
classified. For this reason, we introduce a smoothed estimate of the FEG prior con-
sisting of a linear interpolation of the empirical FEG prior and the product, for each
possible frame element, of the probability of being present or not present in a sentence
given the target word:
?P({r1...n} | t) + (1 ? ?)
?
?
?
r?r1...n
P(r ? FEG | t)
?
r/?r1...n
P(r /? FEG | t)
?
?
The value of ? was empirically set to maximize performance on the development set;
a value of 0.6 yielded performance of 81.6%, a significant improvement over the 80.4%
of the baseline system. Results were relatively insensitive to the exact value of ?.
Up to this point, we have considered separately the problems of labeling roles
given that we know where the boundaries of the frame elements lie (Section 4, as
well as Section 6) and finding the constituents to label in the sentence (Section 5).
274
Computational Linguistics Volume 28, Number 3
Table 15
Combined results on boundary identification and role labeling.
Unlabeled Labeled
Method Precision Recall Precision Recall
Boundary id. + baseline role labeler 72.6 63.1 67.0 46.8
Boundary id. + labeler w/FEG priors 72.6 63.1 65.9 46.2
Integrated boundary id. and labeling 74.0 70.1 64.6 61.2
We now turn to combining the two systems described above into a complete role
labeling system. We use equation (16), repeated below, to estimate the probability that
a constituent is a frame element:
P(fe | p, h, t) = ?1P(fe | p) + ?2P(fe | p, t) + ?3P(fe | h, t)
where p is the path through the parse tree from the target word to the constituent, t
is the target word, and h is the constituent?s head word.
The first two rows of Table 15 show the results when constituents are determined
to be frame elements by setting the threshold on the probability P(fe | p, h, t) to 0.5 and
then running the labeling system of Section 4 on the resulting set of constituents. The
first two columns of results show precision and recall for the task of identifying frame
element boundaries correctly. The second pair of columns gives precision and recall
for the combined task of boundary identification and role labeling; to be counted as
correct, the frame element must both have the correct boundary and be labeled with
the correct role.
Contrary to our results using human-annotated boundaries, incorporating FEG
priors into the system based on automatically identified boundaries had a negative
effect on labeled precision and recall. No doubt this is due to introducing a dependency
on other frame element decisions that may be incorrect: the use of FEG priors causes
errors in boundary identification to be compounded.
One way around this problem is to integrate boundary identification with role
labeling, allowing the FEG priors and the role-labeling decisions to affect which con-
stituents are frame elements. This was accomplished by extending the formulation
argmaxr1...n P({r1...n} | t)
?
i
P(ri | fi, t)
P(ri | t)
to include frame element identification decisions:
argmaxr1...n P({r1...n} | t)
?
i
P(ri | fi, fei, t)P(fei | fi)
P(ri | t)
where fei is a binary variable indicating that a constituent is a frame element and
P(fei | fi) is calculated as above. When fei is true, role probabilities are calculated as
before; when fei is false, ri assumes an empty role with probability one and is not
included in the FEG represented by {r1...n}.
One caveat in using this integrated approach is its exponential complexity: each
combination of role assignments to constituents is considered, and the number of
combinations is exponential in the number of constituents. Although this did not pose
a problem when only the annotated frame elements were under consideration, now we
275
Gildea and Jurafsky Automatic Labeling of Semantic Roles
S
NP VP
NP
He
PRP VB
DT NN
doorthe
opened
S
VP
NP
VBDT NN
door openedThe
Figure 12
Two subcategorizations for the target word open. The relevant production in the parse tree is
highlighted. On the left, the value of the feature is ?VP ? VB NP?; on the right it is ?VP ?
VB.?
must include every parse constituent with a nonzero probability for P(fei | fi). To make
the computation tractable, we implement a pruning scheme: hypotheses are extended
by choosing assignments for one constituent at a time, and only the top m hypotheses
are retained for extension by assignments to the next constituent. Here we set m = 10
after experimentation showed that increasing m yielded no significant improvement.
Results for the integrated approach are shown in the last row of Table 15. Allowing
role assignments to influence boundary identification improves results both on the
unlabeled boundary identification task and on the combined identification and labeling
task. The integrated approach puts us in a different portion of the precision/recall
curve from the results in the first two rows, as it returns a higher number of frame
elements (7,736 vs. 5,719). A more direct comparison can be made by lowering the
probability threshold for frame element identification from 0.5 to 0.35 to force the
nonintegrated system to return the same number of frame elements as the integrated
system. This yields a frame element identification precision of 71.3% and recall of
67.6% and a labeled precision of 60.8% and recall of 57.6%, which is dominated by
the result for the integrated system. The integrated system does not have a probability
threshold to set; nonetheless it comes closer to identifying the correct number of frame
elements (8,167) than does the independent boundary identifier when the theoretically
optimal threshold of 0.5 is used with the latter.
7.2 Subcategorization
Recall that use of the FEG prior was motivated by the tendency of verbs to assign dif-
fering roles to the same syntactic position. For example, the verb open assigns different
roles to the syntactic subject in He opened the door and The door opened. In this section
we consider a different feature motivated by these problems: the syntactic subcate-
gorization of the verb. For example, the verb open seems to be more likely to assign
the role Patient to its subject in an intransitive context and Agent to its subject in a
transitive context. Our use of a subcategorization feature was intended to differentiate
between transitive and intransitive uses of a verb.
The feature used was the identity of the phrase structure rule expanding the target
word?s parent node in the parse tree, as shown in Figure 12. For example, for He closed
the door, with close as the target word, the subcategorization feature would be ?VP ?
VB NP.? The subcategorization feature was used only when the target word was a
276
Computational Linguistics Volume 28, Number 3
verb. The various part-of-speech tags for verb forms (VBD for past-tense verb forms,
VBZ for third-person singular present tense, VBP for other present tense, VBG for
present participles, and VBN for past participles) were collapsed into a single tag VB.
It is important to note that we are not able to distinguish complements from adjuncts,
and our subcategorization feature could be sabotaged by cases such as The door closed
yesterday. In the Penn Treebank style, yesterday is considered an NP with tree structure
equivalent to that of a direct object. Our subcategorization feature is fairly specific: for
example, the addition of an ADVP to a verb phrase will result in a different value. We
tested variations of the feature that counted the number of NPs in a VP or the total
number of children of the VP, with no significant change in results.
The subcategorization feature was used in conjunction with the path feature, which
represents the sequence of nonterminals along the path through the parse tree from
the target word to the constituent representing a frame element. Making use of the
new subcategorization (subcat) feature by adding the distribution P(r | subcat , path , t)
to the lattice of distributions in the baseline system resulted in a slight improvement
to 80.8% performance from 80.4%. As with the gov feature in the baseline system, it
was found beneficial to use the subcat feature only for NP constituents.
7.3 Discussion
Combining the FEG priors and subcategorization feature into a single system resulted
in performance of 81.6%, no improvement over using FEG priors without subcatego-
rization. We suspect that the two seemingly different approaches in fact provide similar
information. For example, in our hypothetical example of the sentence He opened the
door vs. the sentence The door opened, the verb open would have high priors for the FEGs
{Agent, Theme} and {Theme}, but a low prior for {Agent}. In sentences with only
one candidate frame element (the subject in The door closed), the use of the FEG prior
will cause it to be labeled Theme, even when the feature probabilities prefer labeling a
subject as Agent. Thus the FEG prior, by representing the set of arguments the predi-
cate is likely to take, essentially already performs the function of the subcategorization
feature.
The FEG prior allows us to introduce a dependency between the classifications
of the sentence?s various constituents with a single parameter. Thus, it can handle
the alternation of our example without, for example, introducing the role chosen for
one constituent as an additional feature in the probability distribution for the next
constituent?s role. It appears that because introducing additional features can further
fragment our already sparse data, it is preferable to have a single parameter for the
FEG prior.
An interesting result reinforcing this conclusion is that some of the argument-
structure features that aided the system when individual frame elements were consid-
ered independently are unnecessary when using FEG priors. Removing the features
passive and position from the system and using a smaller lattice of only the distribu-
tions not employing these features yields an improved performance of 82.8% on the
role-labeling task using hand-annotated boundaries. We believe that, because these
features pertain to syntactic alternations in how arguments are realized, they overlap
with the function of the FEG prior. Adding unnecessary features to the system can
reduce performance by fragmenting the training data.
8. Integrating Syntactic and Semantic Parsing
In the experiments reported in previous sections, we have used the parse tree returned
by a statistical parser as input to the role-labeling system. In this section, we explore
277
Gildea and Jurafsky Automatic Labeling of Semantic Roles
the interaction between semantic roles and syntactic parsing by integrating the parser
with the semantic-role probability model. This allows the semantic-role assignment
to affect the syntactic attachment decisions made by the parser, with the hope of
improving the accuracy of the complete system.
Although most statistical parsing work measures performance in terms of syntac-
tic trees without semantic information, an assignment of role fillers has been incor-
porated into a statistical parsing model by Miller et al (2000) for the domain-specific
templates of the Message Understanding Conference (Defense Advanced Research
Projects Agency 1998) task. A key finding of Miller et al?s work was that a system
developed by annotating role fillers in text and training a statistical system performed
at the same level as one based on writing a large system of rules, which requires much
more highly skilled labor to design.
8.1 Incorporating Roles into the Parsing Model
We use as the baseline of all our parsing experiments the model described in Collins
(1999). The algorithm is a form of chart parsing, which uses dynamic programming to
search through the exponential number of possible parses by considering subtrees for
each subsequence of the sentence independently. To apply chart parsing to a proba-
bilistic grammar, independence relations must be assumed to hold between the prob-
abilities of a parse tree and the internal structure of its subtrees.
In the case of stochastic context-free grammar, the probability of a tree is inde-
pendent of the internal structure of its subtrees, given the topmost nonterminal of
the subtree. The chart-parsing algorithm can simply find the highest-probability parse
for each nonterminal for each substring of the input sentence. No lower-probability
subtrees will ever be used in a complete parse, and they can be thrown away. Recent
lexicalized stochastic parsers such as Collins (1999), Charniak (1997), and others add
additional features to each constituent, the most important being the head word of
the parse constituent.
The statistical system for assigning semantic roles described in the previous sec-
tions does not fit easily into the chart-parsing framework, as it relies on long-distance
dependencies between the target word and its frame elements. In particular, the path
feature, which is used to ?navigate? through the sentence from the target word to its
likely frame elements, may be an arbitrarily long sequence of syntactic constituents.
A path feature looking for frame elements for a target word in another part of the
sentence may examine the internal structure of a constituent, violating the indepen-
dence assumptions of the chart parser. The use of priors over FEGs further complicates
matters by introducing sentence-level features dependent on the entire parse.
For these reasons, we use the syntactic parsing model without frame element
probabilities to generate a number of candidate parses, compute the best frame element
assignment for each, and then choose the analysis with the highest overall probability.
The frame element assignments are computed as in Section 7.1, with frame element
probabilities being applied to every constituent in the parse.
To return a large number of candidate parses, the parser was modified to include
constituents in the chart even when they were equivalent, according to the parsing
model, to a higher-probability constituent. Rather than choosing a fixed n and keeping
the n best constituents for each entry in the chart, we chose a probability threshold
and kept all constituents within a margin of the highest-probability constituent. Thus
the mechanism is similar to the beam search used to prune nonequivalent edges, but
a lower threshold was used for equivalent edges ( 1e vs.
1
100 ).
Using these pruning parameters, an average of 14.9 parses per sentence were
obtained. After rescoring with frame element probabilities, 18% of the sentences were
278
Computational Linguistics Volume 28, Number 3
Table 16
Results on rescoring parser output.
Frame Frame Labeled Labeled
Method Element Precision Element Recall Precision Recall
Single-best parse 74.0 70.1 64.6 61.2
Rescoring parses 73.8 70.7 64.6 61.9
assigned a parse different from the original best parse. Nevertheless, the impact on
identification of frame elements was small; results are shown in Table 16. The results
show a slight, but not statistically significant, increase in recall of frame elements. One
possible reason that the improvement is not greater is the relatively small number of
parses per sentence available for rescoring. Unfortunately, the parsing algorithm used
to generate n-best parses is inefficient, and generating large numbers of parses seems
to be computationally intractable. In theory, the complexity of n-best variations of the
Viterbi chart-parsing algorithm is quadratic in n. One can simply expand the dynamic
programming chart to have n slots for the best solutions to each subproblem, rather
than one. As our grammar forms new constituents from pairs of smaller constituents
(that is, it internally uses a binarized grammar), for each pair of constituents considered
in a single-best parser, up to n2 pairs would be present in the n-best variant. The
beam search used by modern parsers, however, makes the analysis more complex.
Lexicalization of parse constituents dramatically increases the number of categories
that must be stored in the chart, and efficient parsing requires that constituents below
a particular probability threshold be dropped from further consideration. In practice,
returning a larger number of parses with our algorithm seems to require increasing
the pruning beam size to a degree that makes run times prohibitive.
In addition to the robustness of even relatively simple parsing models, one expla-
nation for the modest improvement may be the fact that even our integrated system
includes semantic information for only one word in the sentence. As the coverage of
our frame descriptions increases, it may be possible to do better and to model the
interactions between the frames invoked by a text.
9. Generalizing to Unseen Predicates
Most of the statistics used in the system as described above are conditioned on the
target word, or predicate, for which semantic roles are being identified. This limits the
applicability of the system to words for which training data are available. In Section 6,
we attempted to generalize across fillers for the roles of a single predicate. In this
section, we turn to the related but somewhat more difficult question of generalizing
from seen to unseen predicates.
Many ways of attempting this generalization are possible, but the simplest is
provided by the frame-semantic information of the FrameNet database. We can use
data from target words in the same frame to predict behavior for an unseen word,
or, if no data are available for the frame in question, we can use data from the same
broad semantic domain into which the frames are grouped.
9.1 Thematic Roles
To investigate the degree to which our system is dependent on the set of semantic
roles used, we performed experiments using abstract, general semantic roles such as
279
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Agent, Patient, and Goal. Such roles were proposed in theories of linking such as
Fillmore (1968) and Jackendoff (1972) to explain the syntactic realization of semantic
arguments. This level of roles, often called thematic roles, was seen as useful for
expressing generalizations such as ?If a sentence has anAgent, theAgentwill occupy
the subject position.? Such correlations might enable a statistical system to generalize
from one semantic domain to another.
Recent work on linguistic theories of linking has attempted to explain syntactic
realization in terms of the fundamentals of verbs? meaning (see Levin and Rappaport
Hovav [1996] for a survey of a number of theories). Although such an explanation is
desirable, our goal is more modest: an automatic procedure for identifying semantic
roles in text. We aim to use abstract roles as a means of generalizing from limited
training data in various semantic domains. We see this effort as consistent with various
theoretical accounts of the underlying mechanisms of argument linking, since the
various theories all postulate some sort of generalization between the roles of specific
predicates.
To this end, we developed a correspondence from frame-specific roles to a set
of abstract thematic roles. For each frame, an abstract thematic role was assigned to
each frame element in the frame?s definition. Since there is no canonical set of abstract
semantic roles, we decided upon the list shown in Table 17. We are interested in
adjuncts as well as arguments, leading to roles such as Degree not found in many
theories of verb-argument linking. The difficulty of fitting many relations into standard
categories such as Agent and Patient led us to include other roles such as Topic.
In all, we used 18 roles, a somewhat richer set than is often used, but still much more
restricted than the frame-specific roles. Even with this enriched set, not all frame-
specific roles fit neatly into one category.
An experiment was performed replacing each role tag in the training and test
data with the corresponding thematic role and training the system as described above
on the new dataset. Results were roughly comparable for the two types of semantic
roles: overall performance was 82.1% for thematic roles, compared to 80.4% for frame-
specific roles. This reflects the fact that most frames had a one-to-one mapping from
frame-specific to abstract roles, so the tasks were largely equivalent. We expect abstract
roles to be most useful when one is generalizing to predicates and frames not found
in the training data, the topic of the following sections.
One interesting consequence of using abstract roles is that they allow us to compare
more easily the system?s performance on different roles because of the smaller number
of categories. This breakdown is shown in Table 18. Results are given for two systems:
the first assumes that the frame element boundaries are known and the second finds
them automatically. The second system, which is described in Section 7.1, corresponds
to the rightmost two columns in Table 18. The ?Labeled Recall? column shows how
often the frame element is correctly identified, whereas the ?Unlabeled Recall? column
shows how often a constituent with the given role is correctly identified as being a
frame element, even if it is labeled with the wrong role.
Experiencer and Agent, two similar roles generally found as the subject for
complementary sets of verbs, are the roles that are correctly identified the most often.
The ?Unlabeled Recall? column shows that these roles are easy to find in the sentence,
as a predicate?s subject is almost always a frame element, and the ?Known Boundaries?
column shows that they are also not often confused with other roles when it is known
that they are frame elements. The two most difficult roles in terms of unlabeled recall,
Manner and Degree, are typically realized by adverbs or prepositional phrases and
considered adjuncts. It is interesting to note that these are considered in FrameNet to
be general frame elements that can be used in any frame.
280
Computational Linguistics Volume 28, Number 3
Table 17
Abstract semantic roles, with representative examples from the FrameNet corpus.
Role Example
Agent Henry pushed the door open and went in.
Cause Jeez, that amazes me as well as riles me.
Degree I rather deplore the recent manifestation of Pop; it doesn?t seem to me to
have the intellectual force of the art of the Sixties.
Experiencer It may even have been that John anticipating his imminent doom ratified
some such arrangement perhaps in the ceremony at the Jordan.
Force If this is the case can it be substantiated by evidence from the history of
developed societies?
Goal Distant across the river the towers of the castle rose against the sky strad-
dling the only land approach into Shrewsbury.
Instrument In the children with colonic contractions fasting motility did not differentiate
children with and without constipation.
Location These fleshy appendages are used to detect and taste food amongst the
weed and debris on the bottom of a river.
Manner His brow arched delicately.
Null Yet while she had no intention of surrendering her home, it would be foolish
to let the atmosphere between them become too acrimonious.
Path The dung-collector ambled slowly over, one eye on Sir John.
Patient As soon as a character lays a hand on this item, the skeletal Cleric grips it
more tightly.
Percept What is apparent is that this manual is aimed at the non-specialist techni-
cian, possibly an embalmer who has good knowledge of some medical
procedures.
Proposition It says that rotation of partners does not demonstrate independence.
Result All the arrangements for stay-behind agents in north-west Europe collapsed,
but Dansey was able to charm most of the governments in exile in London
into recruiting spies.
Source He heard the sound of liquid slurping in a metal container as Farrell ap-
proached him from behind.
State Rex spied out Sam Maggott hollering at all and sundry and making good
use of his over-sized red gingham handkerchief.
Topic He said, ?We would urge people to be aware and be alert with fireworks
because your fun might be someone else?s tragedy.?
This section has shown that our system can use roles defined at a more abstract
level than the corpus?s frame-level roles and in fact that when we are looking at a
single predicate, the choice has little effect. In the following sections, we attempt to
use the abstract roles to generalize the behavior of semantically related predicates.
9.2 Unseen Predicates
We will present results at different, successively broader levels of generalization, mak-
ing use of the categorization of FrameNet predicates into frames and more general
semantic domains. We first turn to using data from the appropriate frame when no
data for the target word are available.
Table 19 shows results for various probability distributions using a division of
training and test data constructed such that no target words are in common. Every
tenth target word was included in the test set. The amount of training data available
for each frame varied, from just one target word in some cases to 167 target words
in the ?perception/noise? frame. The training set contained a total of 75,919 frame
elements and the test set 7,801 frame elements.
281
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 18
Performance broken down by abstract role. The third column represents accuracy when frame
element boundaries are given to the system, and the fourth and fifth columns reflect finding
the boundaries automatically. Unlabeled recall includes cases that were identified as a frame
element but given the wrong role.
Known Boundaries Unknown Boundaries
Role Number % Correct Labeled Recall Unlabeled Recall
Agent 2401 92.8 76.7 80.7
Experiencer 333 91.0 78.7 83.5
Source 503 87.3 67.4 74.2
Proposition 186 86.6 56.5 64.5
State 71 85.9 53.5 62.0
Patient 1161 83.3 63.1 69.1
Topic 244 82.4 64.3 72.1
Goal 694 82.1 60.2 69.6
Cause 424 76.2 61.6 73.8
Path 637 75.0 63.1 63.4
Manner 494 70.4 48.6 59.7
Percept 103 68.0 51.5 65.1
Degree 61 67.2 50.8 60.7
Null 55 65.5 70.9 85.5
Result 40 65.0 55.0 70.0
Location 275 63.3 47.6 63.6
Force 49 59.2 40.8 63.3
Instrument 30 43.3 30.0 73.3
(other) 406 57.9 40.9 63.1
Total 8167 82.1 63.6 72.1
Table 19
Cross-frame performance of various distributions. f represents the FrameNet semantic frame.
Distribution Coverage Accuracy Performance
P(r | path) 95.3% 44.5% 42.4%
P(r | path, f ) 87.4 68.7 60.1
P(r | h) 91.7 54.3 49.8
P(r | h, f ) 74.1 81.3 60.3
P(r | pt, position, voice) 100.0 43.9 43.9
P(r | pt, position, voice, f ) 98.7 68.3 67.4
The results show a familiar trade-off between coverage and accuracy. Conditioning
both the head word and path features on the frame reduces coverage but improves
accuracy. A linear interpolation,
?1P(r | path , f ) + ?2P(r | h, f ) + ?3P(r | pt , position , voice, f )
achieved 79.4% performance on the test set, significantly better than any of the in-
dividual distributions and approaching the result of 82.1% for the original system,
using target-specific statistics and thematic roles. This result indicates that predicates
in the same frame behave similarly in terms of their argument structure, a finding
generally consistent with theories of linking that claim that the syntactic realization
of verb arguments can be predicted from their semantics. We would expect verbs in
the same frame to be semantically similar and to have the same patterns of argument
structure. The relatively high performance of frame-level statistics indicates that the
282
Computational Linguistics Volume 28, Number 3
Table 20
Cross-frame performance of various distributions. d represents the FrameNet semantic domain.
Distribution Coverage Accuracy Performance
P(r | path) 96.2% 41.2% 39.7%
P(r | path, d) 85.7 42.7 36.6
P(r | h) 91.0 44.7 40.6
P(r | h, d) 75.2 54.3 40.9
P(r | d) 95.1 29.9 28.4
P(r) 100.0 28.7 28.7
P(r | h, d) P(r | pt, path, d)
P(r | d)
Figure 13
Minimal lattice for cross-frame generalization.
frames defined by FrameNet are fine-grained enough to capture the relevant semantic
similarities.
This result is encouraging in that it indicates that a relatively small amount of data
can be annotated for a few words in a semantic frame and used to train a system that
can then bootstrap to a larger number of predicates.
9.3 Unseen Frames
More difficult than the question of unseen predicates in a known frame are frames
for which no training data are present. The 67 frames in the current data set cover
only a fraction of the English language, and the high cost of annotation makes it
difficult to expand the data set to cover all semantic domains. The FrameNet project is
defining additional frames and annotating data to expand the scope of the database.
The question of how many frames exist, however, remains unanswered for the time
being; a full account of frame semantics is expected to include multiple frames being
invoked by many words, as well as an inheritance hierarchy of frames and a more
detailed representation of each frame?s meaning.
In this section, we examine the FrameNet data by holding out an entire frame for
testing and using other frames from the same general semantic domain for training.
Recall from Figure 1 that domains like Communication include frames like Conver-
sation, Questioning, and Statement. Because of the variation in difficulty between
different frames and the dependence of the results on which frames are held out for
testing, we used a jackknifing methodology. Each frame was used in turn as test data,
with all other frames used as training data. The results in Table 20 show average
results over the entire data set.
Combining the distributions gives a system based on the (very restricted) backoff
lattice of Figure 13. This system achieves performance of 51.0%, compared to 82.1%
for the original system and 79.4% for the within-frame generalization task. The results
show that generalizing across frames, even within a domain, is more difficult than
generalizing across target words within a frame. There are several factors that may
account for this: the FrameNet domains were intended primarily as a way of orga-
nizing the project, and their semantics have not been formalized. Thus, it may not be
283
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Table 21
Cross-domain performance of various distributions.
Distribution Coverage Accuracy Performance
P(r | path) 96.5% 35.3% 33.4%
P(r | h) 88.8 36.0 31.9
P(r) 100.0 28.7 28.7
surprising that they do not correspond to significant generalizations about argument
structure. The domains are fairly broad, as indicated by the fact that always choosing
the most common role for a given domain (the baseline for cross-frame, within-domain
generalization, given as P(r | d) in Table 20, classifies 28.4% of frame elements cor-
rectly) does not do better than the cross-domain baseline of always choosing the most
common role from the entire database regardless of domain (P(r) in Table 20, which
yields 28.7% correct). This contrasts with a 40.9% baseline for P(r | t), that is, always
choosing the most common role for a particular target word (Table 5, last line). Do-
main information does not seem to help a great deal, given no information about the
frame.
Furthermore, the cross-frame experiments here are dependent on the mapping of
frame-level roles to abstract thematic roles. This mapping was done at the frame level;
that is, FrameNet roles with the same label in two different frames may be translated
into two different thematic roles, but all target words in the same frame make use of
the same mapping. The mapping of roles within a frame is generally one to one, and
therefore the choice of mapping has little effect when using statistics conditioned on
the target word and on the frame, as in the previous section. When we are attempting
to generalize between frames, the mapping determines which roles from the training
frame are used to calculate probabilities for the roles in the test frames, and the choice
of mapping is much more significant. The mapping used is necessarily somewhat
arbitrary.
It is interesting to note that the path feature performs better when not conditioned
on the domain. The head word, however, seems to be more domain-specific: although
coverage declines when the context is restricted to the semantic domain, accuracy
improves. This seems to indicate that the identity of certain role fillers is domain-
specific, but that the syntax/semantics correspondence captured by the path feature is
more general, as predicted by theories of syntactic linking.
9.4 Unseen Domains
As general as they are, the semantic domains of the current FrameNet database cover
only a small portion of the language. The domains are defined at the level of, for
example, Communication and Emotion; a list of the 12 domains in our corpus is
given in Table 1. Whether generalization is possible across domains is an important
question for a general language-understanding system.
For these experiments, a jackknifing protocol similar to that of the previous section
was used, this time holding out one entire domain at a time and using all the others as
training material. Results for the path and head word feature are shown in Table 21.
The distributions P(r | path), P(r | h), and P(r) of Table 21 also appeared in Table 20;
the difference between the experiments is only in the division of training and test sets.
A linear interpolation, ?1P(r | path)+?2P(r | h), classifies 39.8% of frame elements
correctly. This is no better than our result of 40.9% (Table 3) for always choosing a
284
Computational Linguistics Volume 28, Number 3
predicate?s most frequent role; however, the cross-domain system does not have role
frequencies for the test predicates.
9.5 Discussion
As one might expect, as we make successively broader generalizations to semantically
more distant predicates, performance degrades. Our results indicate that frame seman-
tics give us a level at which generalizations relevant to argument linking can be made.
Our results for unseen predicates within the same frame are encouraging, indicating
that the predicates are semantically similar in ways that result in similar argument
structure, as the semantically based theories of linking advocated by Levin (1993) and
Levin and Rappaport Hovav (1996) would predict. We hope that corpus-based sys-
tems such as ours can provide a way of testing and elaborating such theories in the
future. We believe that some level of skeletal representation of the relevant aspects
of a word?s meaning, along the lines of Kipper et al (2000) and of the frame hierar-
chy being developed by the FrameNet project, could be used in the future to help a
statistical system generalize from similar words for which training data are available.
10. Conclusion
Our system is able to label semantic roles automatically with fairly high accuracy,
indicating promise for applications in various natural language tasks. Semantic roles
do not seem to be simple functions of a sentence?s syntactic tree structure, and lexical
statistics were found to be extremely valuable, as has been the case in other natural
language processing applications. Although lexical statistics are quite accurate on the
data covered by observations in the training set, the sparsity of their coverage led
us to introduce semantically motivated knowledge sources, which in turn allowed us
to compare automatically derived and hand-built semantic resources. Various meth-
ods of extending the coverage of lexical statistics indicated that the broader coverage
of automatic clustering outweighed its imprecision. Carefully choosing sentence-level
features for representing alternations in verb argument structure allowed us to intro-
duce dependencies between frame element decisions within a sentence without adding
too much complexity to the system. Integrating semantic interpretation and syntactic
parsing yielded only the slightest gain, showing that although probabilistic models
allow easy integration of modules, the gain over an unintegrated system may not be
large because of the robustness of even simple probabilistic systems.
Many aspects of our system are still quite preliminary. For example, our system
currently assumes knowledge of the correct frame type for the target word to deter-
mine the semantic roles of its arguments. A more complete semantic analysis system
would thus require a module for frame disambiguation. It is not clear how difficult
this problem is and how much it overlaps with the general problem of word-sense
disambiguation.
Much else remains to be done to apply the system described here to the inter-
pretation of general text. One technique for dealing with the sparseness of lexical
statistics would be the combination of FrameNet data with named-entity systems for
recognizing times, dates, and locations, the effort that has gone into recognizing these
items, typically used as adjuncts, should complement the FrameNet data, which is
more focused on arguments. Generalization to predicates for which no annotated data
are available may be possible using other lexical resources or automatic clustering of
predicates. Automatically learning generalizations about the semantics and syntactic
behavior of predicates is an exciting problem for the years to come.
285
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Appendix
Table 22
Penn Treebank part-of-speech tags (including punctuation).
Tag Description Example Tag Description Example
CC Coordin. Conjunction and, but, or SYM Symbol +,%, &
CD Cardinal number one, two, three TO ?to? to
DT Determiner a, the UH Interjection ah, oops
EX Existential ?there? there VB Verb, base form eat
FW Foreign word mea culpa VBD Verb, past tense ate
IN Preposition/sub-conj of, in, by VBG Verb, gerund eating
JJ Adjective yellow VBN Verb, past participle eaten
JJR Adj., comparative bigger VBP Verb, non-3sg pres eat
JJS Adj., superlative wildest VBZ Verb, 3sg pres eats
LS List item marker 1, 2, One WDT Wh-determiner which, that
MD Modal can, should WP Wh-pronoun what, who
NN Noun, sing. or mass llama WP$ Possessive wh- whose
NNS Noun, plural llamas WRB Wh-adverb how, where
NNP Proper noun, singular IBM $ Dollar sign $
NNPS Proper noun, plural Carolinas # Pound sign #
PDT Predeterminer all, both ? Left quote (? or ?)
POS Possessive ending ?s ? Right quote (? or ?)
PRP Personal pronoun I, you, he ( Left parenthesis ( [, (, {, <)
PRP$ Possessive pronoun your, one?s ) Right parenthesis ( ], ), }, >)
RB Adverb quickly, never , Comma ,
RBR Adverb, comparative faster . Sentence-final punc (. ! ?)
RBS Adverb, superlative fastest : Mid-sentence punc (: ; ... ? -)
RP Particle up, off
Table 23
Penn Treebank constituent (or nonterminal) labels.
Label Description
ADJP Adjective Phrase
ADVP Adverb Phrase
CONJP Conjunction Phrase
FRAG Fragment
INTJ Interjection
NAC Not a constituent
NP Noun Phrase
NX Head subphrase of complex noun phrase
PP Prepositional Phrase
QP Quantifier Phrase
RRC Reduced Relative Clause
S Simple declarative clause (sentence)
SBAR Clause introduced by complementizer
SBARQ Question introduced by wh-word
SINV Inverted declarative sentence
SQ Inverted yes/no question
UCP Unlike Co-ordinated Phrase
VP Verb Phrase
WHADJP Wh-adjective Phrase
WHADVP Wh-adverb Phrase
WHNP Wh-noun Phrase
WHPP Wh-prepositional Phrase
286
Computational Linguistics Volume 28, Number 3
Acknowledgments
We are grateful to Chuck Fillmore, Andreas
Stolcke, Jerry Feldman, and three
anonymous reviewers for their comments
and suggestions, to Collin Baker for his
assistance with the FrameNet data, and to
Mats Rooth and Sabine Schulte
im Walde for making available their parsed
corpus. This work was primarily funded by
National Science Foundation grant ITR/HCI
#0086132 to the FrameNet project.
References
Baayen, R. H., R. Piepenbrock, and
L. Gulikers. 1995. The CELEX Lexical
Database (Release 2) [CD-ROM]. Linguistic
Data Consortium, University of
Pennsylvania [Distributor], Philadelphia,
PA.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. ?The Berkeley
FrameNet project.? In Proceedings of
COLING/ACL, pages 86?90, Montreal,
Canada.
Blaheta, Don and Eugene Charniak. 2000.
?Assigning function tags to parsed text.?
In Proceedings of the First Annual Meeting of
the North American Chapter of the ACL
(NAACL), pages 234?240, Seattle,
Washington.
Carroll, Glenn and Mats Rooth. 1998.
?Valence induction with a
head-lexicalized PCFG.? In Proceedings of
the Third Conference on Empirical Methods in
Natural Language Processing (EMNLP 3),
Granada, Spain.
Charniak, Eugene. 1997. ?Statistical parsing
with a context-free grammar and word
statistics.? In AAAI-97, pages 598?603,
Menlo Park, August. AAAI Press, Menlo
Park, California.
Collins, Michael. 1997. ?Three generative,
lexicalised models for statistical parsing.?
In Proceedings of the 35th Annual Meeting of
the ACL, pages 16?23, Madrid, Spain.
Collins, Michael. 1999. Head-Driven
Statistical Models for Natural Language
Parsing. Ph.D. dissertation, University of
Pennsylvania, Philadelphia.
Dahiya, Yajan Veer. 1995. Panini as a Linguist:
Ideas and Patterns. Eastern Book Linkers,
Delhi, India.
Defense Advanced Research Projects
Agency, editor. 1998. Proceedings of the
Seventh Message Understanding Conference.
Dowty, David R. 1991. Thematic proto-roles
and argument selection. Language
67(3):547?619.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, Massachusetts.
Fillmore, Charles J. 1968. ?The case for
case.? In Emmon W. Bach and Robert T.
Harms, editors, Universals in Linguistic
Theory. Holt, Rinehart & Winston, New
York, pages 1?88.
Fillmore, Charles J. 1971. ?Some problems
for case grammar.? In R. J. O?Brien,
editor, 22nd Annual Round Table.
Linguistics: Developments of the
Sixties?Viewpoints of the Seventies.
Volume 24 of Monograph Series on
Language and Linguistics. Georgetown
University Press, Washington, D.C.,
pages 35?56.
Fillmore, Charles J. 1976. ?Frame semantics
and the nature of language.? In Annals of
the New York Academy of Sciences:
Conference on the Origin and Development of
Language and Speech, Volume 280,
pages 20?32. New York Academy of
Sciences, New York.
Fillmore, Charles J. 1986. ?Pragmatically
controlled zero anaphora.? In Proceedings
of Berkeley Linguistics Society, pages 95?107,
Berkeley, California.
Fillmore, Charles J. and Collin F. Baker.
2000. ?FrameNet: Frame semantics meets
the corpus.? Poster presentation, 74th
Annual Meeting of the Linguistics Society
of America.
Gildea, Daniel and Thomas Hofmann. 1999.
?Probabilistic topic analysis for language
modeling.? In Eurospeech-99,
pages 2167?2170, Budapest.
Hearst, Marti. 1999. ?Untangling text data
mining.? In Proceedings of the 37th Annual
Meeting of the ACL, pages 3?10, College
Park, Maryland.
Hobbs, Jerry R., Douglas Appelt, John Bear,
David Israel, Megumi Kameyama,
Mark E. Stickel, and Mabry Tyson. 1997.
?FASTUS: A cascaded finite-state
transducer for extracting information
from natural-language text.? In
Emmanuel Roche and Yves Schabes,
editors, Finite-State Language Processing.
MIT Press, Cambridge, Massachusetts,
pages 383?406.
Hofmann, Thomas and Jan Puzicha. 1998.
?Statistical models for co-occurrence
data.? Memorandum, Massachussetts
Institute of Technology Artificial
Intelligence Laboratory, Cambridge,
Massachusetts.
Jackendoff, Ray. 1972. Semantic Interpretation
in Generative Grammar. MIT Press,
287
Gildea and Jurafsky Automatic Labeling of Semantic Roles
Cambridge, Massachusetts.
Jelinek, Frederick and Robert L. Mercer.
1980. ?Interpolated estimation of Markov
source parameters from sparse data.? In
Proceedings: Workshop on Pattern Recognition
in Practice, pages 381?397. Amsterdam.
North Holland.
Johnson, Christopher R., Charles J. Fillmore,
Esther J. Wood, Josef Ruppenhofer,
Margaret Urban, Miriam R. L. Petruk, and
Collin F. Baker. 2001. The FrameNet
project: Tools for lexicon building. Version
0.7. Available at http://www.icsi.
berkeley.edu/?framenet/book.html.
Kipper, Karin, Hoa Trang Dang, William
Schuler, and Martha Palmer. 2000.
?Building a class-based verb lexicon using
TAGs.? In TAG+5 Fifth International
Workshop on Tree Adjoining Grammars and
Related Formalisms, Paris, May.
Lapata, Maria and Chris Brew. 1999. ?Using
subcategorization to resolve verb class
ambiguity.? In Joint SIGDAT Conference on
Empirical Methods in NLP and Very Large
Corpora, pages 266?274, College Park,
Maryland.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Levin, Beth and Malka Rappaport Hovav.
1996. ?From lexical semantics to
argument realization.? Unpublished
manuscript.
Marcus, Mitchell P., Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann
Bies, Mark Ferguson, Karen Katz, and
Britta Schasberger. 1994. ?The Penn
Treebank: Annotating predicate argument
structure.? In ARPA Human Language
Technology Workshop, pages 114?119,
Plainsboro, New Jersey. Morgan
Kaufmann, San Francisco.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn treebank. Computational Linguistics
19(2):313?330.
McCarthy, Diana. 2000. ?Using semantic
preferences to identify verbal
participation in role switching
alternations.? In Proceedings of the First
Annual Meeting of the North American
Chapter of the ACL (NAACL),
pages 256?263, Seattle, Washington.
Miller, Scott, Heidi Fox, Lance Ramshaw,
and Ralph Weischedel. 2000. ?A novel use
of statistical parsing to extract
information from text.? In Proceedings of
the First Annual Meeting of the North
American Chapter of the ACL (NAACL),
pages 226?233, Seattle, Washington.
Miller, Scott, David Stallard, Robert Bobrow,
and Richard Schwartz. 1996. ?A fully
statistical approach to natural language
interfaces.? In Proceedings of the 34th
Annual Meeting of the ACL, pages 55?61,
Santa Cruz, California.
Misra, Vidya Niwas. 1966. The Descriptive
Technique of Panini. Mouton, The
Hague.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. ?Distributional
clustering of English words.? In
Proceedings of the 31st Annual Meeting of the
ACL, pages 183?190, Columbus, Ohio.
Pietra, Stephen Della, Vincent Della Pietra,
and John Lafferty. 1997. Inducing features
of random fields. IEEE Transactions on
Pattern Analysis and Machine Intelligence
19(4):380?393.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago.
Riloff, Ellen. 1993. Automatically
constructing a dictionary for information
extraction tasks. In Proceedings of the 11th
National Conference on Artificial Intelligence
(AAAI), pages 811?816, Washington, D.C.
Riloff, Ellen and Mark Schmelzenbach. 1998.
?An empirical approach to conceptual
case frame acquisition.? In Proceedings of
the Sixth Workshop on Very Large Corpora,
pages 49?56, Montreal, Canada.
Rocher, Rosane. 1964. ?Agent? et ?Objet?
chez Panini. Journal of the American
Oriental Society 84:44?54.
Rooth, Mats. 1995. ?Two-dimensional
clusters in grammatical relations.? In
AAAI Symposium on Representation and
Acquisition of Lexical Knowledge, Stanford,
California.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
?Inducing a semantically annotated
lexicon via EM-based clustering.? In
Proceedings of the 37th Annual Meeting of the
ACL, pages 104?111, College Park,
Maryland.
Schank, Roger C. 1972. Conceptual
dependency: A theory of natural
language understanding. Cognitive
Psychology 3:552?631.
Siegel, Sidney and N. John Castellan, Jr.
1988. Nonparametric Statistics for the
Behavioral Sciences. 2nd ed. McGraw-Hill,
New York.
Somers, Harold L. 1987. Valency and Case in
Computational Linguistics. Edinburgh
University Press, Edinburgh, Scotland.
Stallard, David. 2000. ?Talk?n?travel: A
conversational system for air travel
planning.? In Proceedings of the Sixth
288
Computational Linguistics Volume 28, Number 3
Applied Natural Language Processing
Conference (ANLP?00), pages 68?75.
Van Valin, Robert D. 1993. A synopsis of
role and reference grammar. In Robert D.
Van Valin, editor, Advances in Role and
Reference Grammar. John Benjamins
Publishing Company, Amsterdam,
pages 1?166.
Winograd, Terry. 1972. Understanding
natural language. Cognitive Psychology,
3(1). Reprinted as a book by Academic
Press, 1972.
Knowledge-Free Induction of Inflectional Morphologies
Patrick SCHONE                                 Daniel JURAFSKY
University of Colorado at Boulder           University of Colorado at Boulder
Boulder, Colorado 80309                        Boulder, Colorado 80309
schone@cs.colorado.edu                        jurafsky@cs.colorado.edu
Abstract
We propose an algorithm  to automatically induce
the morphology of inflectional languages using only
text corpora and no human input.  Our algorithm
combines cues from orthography, semantics, and
syntactic distributions to induce morphological
relationships in German, Dutch, and English. Using
CELEX as a gold standard for evaluation, we show
our algorithm to be an improvement over any
knowledge-free algorithm yet proposed.
1       Introduction
Many NLP tasks, such as building machine-readable
dictionaries, are dependent on the results of
morphological analysis.  While morphological
analyzers have existed since the early 1960s, current
algorithms require human labor to build rules for
morphological structure.  In an attempt to avoid this
labor-intensive process, recent work has focused on
machine-learning approaches to induce
morphological structure using large corpora. 
In this paper, we propose a knowledge-free
algorithm to automatically induce the morphology
structures of a language.  Our algorithm takes as
input a large corpus and  produces as output a set of
conflation sets indicating the various inflected and
derived forms for each word in the language.  As an
example, the conflation set of the word ?abuse?
would contain ?abuse?,  ?abused?, ?abuses?,
?abusive?, ?abusively?, and so forth. Our algorithm
extends earlier approaches to morphology induction
by combining various induced information sources:
the semantic relatedness of the affixed forms using
a Latent Semantic Analysis approach to corpus-
based semantics (Schone and Jurafsky, 2000), affix
frequency, syntactic context, and transitive closure.
Using the hand-labeled CELEX lexicon  (Baayen, et
al., 1993) as our gold standard, the current version
of our algorithm achieves an F-score of 88.1% on
the task of identifying conflation sets in English,
outperforming earlier algorithms.  Our algorithm is
also applied to German and Dutch and evaluated on
its ability to find  prefixes, suffixes, and circumfixes
in these languages.  To our knowledge, this serves
as the first evaluation of complete regular
morphological induction of German or Dutch
(although researchers such as Nakisa and Hahn
(1996) have evaluated induction algorithms on
morphological sub-problems in German).
2 Previous Approaches
Previous morphology induction approaches have
fallen into three categories.  These categories differ
depending on whether human input is provided and
on whether the goal is to obtain affixes or complete
morphological analysis.  We here briefly describe
work in each category.
2.1 Using a Knowledge Source to Bootstrap
Some researchers begin with some initial human-
labeled source from which they induce other
morphological components. In particular, Xu and
Croft (1998) use word context derived from  a
corpus to refine Porter stemmer output. Gaussier
(1999) induces derivational morphology using an
inflectional lexicon which includes part of speech
information.  Grabar and Zweigenbaum (1999) use
the SNOMED corpus of semantically-arranged
medical terms to find semantically-motivated
morphological relationships. Also, Yarowsky and
Wicentowski (2000) obtained outstanding results at
inducing English past tense after beginning with a
list of the open class roots in the language, a table of
a language?s inflectional parts of speech, and the
canonical suffixes for each part of speech.
2.2 Affix Inventories
A second, knowledge-free category of research has
focused on obtaining affix inventories.  Brent, et al
(1995) used minimum description length (MDL) to
find the most data-compressing suffixes. Kazakov
(1997) does something akin to this using MDL as a
fitness metric for evolutionary computing. D?Jean
(1998) uses a strategy similar to that of Harris
(1951). He declares that a stem has ended when the
number of characters following it exceed some
given threshold and identifies any residual following semantic relations, we identified those word pairs
the stems as suffixes.  that have strong semantic correlations as being
2.3 Complete morphological analysis
Due to the existence of morphological ambiguity
(such as with the word ?caring? whose stem is
?care? rather than ?car?), finding affixes alone does
not constitute a complete morphological analysis.
Hence, the last category of research is also
knowledge-free but attempts to induce, for each
word of a corpus, a complete analysis.  Since our Most of the existing algorithms described focus on
approach falls into this category (expanding upon suffixing in inflectional languages (though
our earlier approach (Schone and Jurafsky, 2000)), Jacquemin and D?Jean describe work on prefixes).
we describe work in this area in more detail. None of these algorithms consider the general
2.3.1 Jacquemin?s multiword approach
Jacquemin (1997) deems pairs of word n-grams as
morphologically related if two words in the first n-
gram have the same first few letters (or stem) as two
words in the second n-gram and if there is a suffix
for each stem whose length is less than k. He also
clusters groups of words having the same kinds of
word endings, which gives an added performance
boost.  He applies his algorithm to a French term list
and scores based on sampled, by-hand evaluation. 
2.3.2. Goldsmith: EM and MDLs
Goldsmith (1997/2000) tries to automatically sever
each word in exactly one place in order to establish
a potential set of stems and suffixes.  He uses the
expectation-maximization algorithm (EM) and MDL
as well as some triage procedures to help eliminate
inappropriate parses for every word in a corpus.  He
collects the possible suffixes for each stem and calls
these signatures which give clues about word
classes. With the exceptions of capitalization
removal and some word segmentation, Goldsmith's
algorithm is otherwise knowledge-free. His
algorithm, Linguistica, is freely available on the
Internet.  Goldsmith applies his algorithm to various
languages but evaluates in English and French.
2.3.3  Schone and Jurafsky: induced semantics
In our earlier work, we (Schone and Jurafsky
(2000)) generated a list of N candidate suffixes and
used this list to identify word pairs which share the
same stem but conclude with distinct candidate
suffixes.  We then applied  Latent Semantic
Analysis (Deerwester, et al, 1990) as a method of
automatically determining semantic relatedness
between word pairs.  Using statistics from the
morphological variants of each other.  With the
exception of word segmentation, we provided  no
human information to our system.  We applied our
system to an English corpus and evaluated by
comparing each word?s conflation set as produced
by our algorithm to those derivable from CELEX.
2.4 Problems with earlier approaches 
conditions of circumfixing or infixing, nor are they
applicable to other language types such as
agglutinative languages (Sproat, 1992).
Additionally, most approaches have centered
around statistics of orthographic properties.  We had
noted previously (Schone and Jurafsky, 2000),
however, that errors can arise from strictly
orthographic systems.  We had observed in other
systems such errors as inappropriate removal of
valid affixes (?ally?<?all?), failure to resolve
morphological ambiguities (?hated?<?hat?), and
pruning of semi-productive affixes (?dirty?h?dirt?).
Yet we illustrated that induced semantics can help
overcome some of these errors.
However, we have since observed that induced
semantics can give rise to different kinds of
problems.  For instance, morphological variants may
be semantically opaque such that the meaning of
one variant cannot be readily determined by the
other (?reusability?h?use?).  Additionally,  high-
frequency function words may be conflated due to
having weak semantic information (?as?<?a?).
Coupling  semantic and orthographic statistics, as
well as introducing induced syntactic information
and relational transitivity can help in overcoming
these problems.  Therefore, we begin with an
approach similar to our previous algorithm.  Yet we
build upon this algorithm in several ways in that we:
[1] consider circumfixes, [2] automatically identify
capitalizations by treating them similar to prefixes
[3] incorporate frequency information, [4] use
distributional information to help identify syntactic
properties, and [5] use transitive closure to help find
variants that may not have been found to be
semantically related but which are related to mutual
variants.  We then apply these strategies to English,
Figure 1: Strategy and evaluation
Figure 2: Inserting the residual lexicon into a trie
German, and Dutch.  We evaluate our algorithm Figure 2). Yet using this approach, there may be
against the human-labeled CELEX lexicon in all circumfixes whose endings will be overlooked in
three languages and compare our results to those the search for suffixes unless we first remove all
that the Goldsmith and Schone/Jurafsky algorithms candidate prefixes.  Therefore, we build a lexicon
would have obtained on our same data. We show consisting of all words in our corpus and identify all
how each of our additions result in progressively word beginnings with frequencies in excess of some
better overall solutions. threshold (T ). We call these pseudo-prefixes. We
3  Current Approach
3.1 Finding Candidate Circumfix Pairings
As in our earlier approach (Schone and Jurafsky,
2000), we begin by generating, from an untagged
corpus, a list of word pairs that might be
morphological variants.  Our algorithm has changed
somewhat, though, since we previously sought word
pairs that vary only by a prefix or a suffix, yet we
now wish to generalize to those with circumfixing
differences.  We use ?circumfix? to mean true
circumfixes like the German ge-/-t as well as
combinations of prefixes and suffixes. It should be
mentioned also that we assume the existence of
languages having valid circumfixes that are not
composed merely of a prefix  and a suffix that
appear independently elsewhere.  
To find potential morphological variants, our first
goal is to find word endings which could serve as
suffixes. We had shown in our earlier work how one
might do this using a character tree, or  trie  (as in
1
strip all pseudo-prefixes from each word in our
lexicon and add the word residuals back into the
lexicon as if they were also words.  Using this final
lexicon, we can now seek for suffixes in a manner
equivalent to what we had done before (Schone and
Jurafsky, 2000).  
To demonstrate how this is done, suppose our
initial  lexicon / contained the words ?align,?
?real,? ?aligns,? ?realign?, ?realigned?,  ?react?,
?reacts,? and ?reacted.? Due to the high frequency
occurrence of ?re-? suppose it is identified as a
pseudo-prefix.  If we strip off ?re-? from all words,
and add all residuals to a trie, the branch of the trie
of words beginning with ?a? is depicted in Figure 2.
In our earlier work, we showed that a majority of
the regular suffixes in the corpus can be found by
identifying trie branches that appear repetitively.
By ?branch? we mean those places in the trie where
some splitting occurs.  In the case of Figure 2, for
example, the branches  NULL (empty circle), ?-s?
and ?-ed? each appear twice.  We assemble a list of
all trie branches that occur some minimum number
of times (T ) and refer to such as potential suffixes.2
Given this list, we can now  find potential prefixes
using a similar strategy. Using our original lexicon,
we can now strip off all potential suffixes from each
word and form a new augmented lexicon.  Then, (as
we had proposed before) if we reverse the ordering
on the words and insert them into a trie, the
branches that are formed will be potential prefixes
(in reverse order).
Before describing the last steps of this procedure,
it is beneficial to define a few terms (some of which
appeared in our previous work):
[a] potential circumfix: A pair B/E where B and E
occur respectively in potential prefix and suffix lists
[b] pseudo-stem: the residue of a word after its
potential circumfix is removed
[c] candidate circumfix: a potential circumfix which
appears affixed to at least T  pseudo-stems that are3
shared by other potential circumfixes
[d] rule: a pair of candidate circumfixes sharing at
least T  pseudo-stems4
[e] pair of potential morphological variants
(PPMV): two words sharing the same rule but
distinct candidate circumfixes
[f] ruleset: the set of all PPMVs for a common rule
Our final goal in this first stage of induction is to
find all of the possible rules and their corresponding
rulesets. We therefore re-evaluate each word in the
original lexicon to identify all potential circumfixes
that could have been valid for the word.  For
example, suppose that the lists of potential suffixes
and prefixes contained ?-ed? and  ?re-? respectively.
Note also that NULL exists by default in both lists
as well.  If we consider the word ?realigned? from
our lexicon /, we would find that its potential
circumfixes would be NULL/ed, re/NULL, and
re/ed and the corresponding pseudo-stems would be
?realign,? ?aligned,? and ?align,? respectively,
From /, we also note that circumfixes re/ed and
NULL/ing share the pseudo-stems ?us,? ?align,? and
?view? so a rule could be created: re/ed<NULL/ing.
This means that word pairs such as ?reused/using?
and ?realigned/aligning? would be deemed PPMVs.
Although the choices in T  through T  is1 4
somewhat arbitrary, we chose  T =T =T =10 and1 2 3
T =3. In English, for example, this yielded 305354
possible rules. Table 1 gives a sampling of these
potential rules in each of the three languages in
terms of frequency-sorted rank.  Notice that several
?rules? are quite valid, such as the indication of an
English suffix -s. There are also valid circumfixes
like the ge-/-t circumfix of German. Capitalization
also appears (as a ?prefix?), such as C< c in English,
D<d in German, and V<v in Dutch. Likewise,there
are also some rules that may only be true in certain
circumstances, such as -d<-r in English (such as
worked/worker, but certainly not for steed/steer.)
However, there are some rules that are
Table 1: Outputs of the trie stage: potential rules
Rank ENGLISH GERMAN DUTCH
1 -s< L -n< L -en< L
2 -ed< -ing -en< L -e< L
4 -ing< L -s< L -n< L
8 -ly< L -en< -t de-< L
12 C-< c- -en< -te -er< L
16 re-< L 1-< L -r< L
20 -ers< -ing er-< L V-< v-
24 1-< L 1-< 2- -ingen < -e
28 -d< -r ge-/-t < -en ge-< -e
32 s-< L D-< d- -n< -rs
 
wrong: the potential ?s-? prefix of English  is never
valid although word combinations like stick/tick
spark/park, and slap/lap happen frequently in
English. Incorporating semantics can help determine
the validity of each rule.
3.2 Computing Semantics
Deerwester, et al (1990) introduced an algorithm
called Latent Semantic Analysis (LSA) which
showed that valid semantic relationships between
words and documents in a corpus can be induced
with virtually no human intervention. To do this,
one typically begins by applying singular value
decomposition (SVD) to a matrix, M, whose entries
M(i,j) contains the frequency of word i as seen in
document j of the corpus. The SVD decomposes M
into the product of three matrices, U, D, and V  suchT
that U and V  are orthogonal matrices and D is aT
diagonal matrix whose entries are the singular
values of M.  The LSA approach then zeros out all
but the top k singular values of the SVD, which has
the effect of projecting vectors into an optimal k-
dimensional subspace. This methodology is
well-described in the literature (Landauer, et al,
1998; Manning and Sch?tze, 1999). 
In order to obtain semantic representations of each
word, we apply our previous strategy (Schone and
Jurafsky (2000)). Rather than using a term-
document matrix, we had followed an approach akin
to that of Sch?tze (1993), who performed SVD on
a Nx2N  term-term matrix.  The N here represents
the N-1 most-frequent words as well as a glob
position to account for all other words not in the top
N-1.  The matrix  is structured such that for a given
word w?s row, the first N columns denote words that
-NCS (?,1) 

P

NCS
exp[	((x	?)/1)2]dx
NCS(w1,w2 ) 

min
k(1,2)
cos(
w1 ,w2)	?k
1k
(1)
Pr(NCS)


nT-NCS(?T,1T)
(nR	nT)-NCS(0,1)  nT-NCS(?T,1T)
.
precede w by up to 50 words, and the second N
columns represent those words that follow by up to
50 words.  Since SVDs are more designed to work then, if there were n  items in the ruleset, the
with  normally-distributed data (Manning and probability that a NCS is non-random is
Sch?tze, 1999, p. 565), we fill each entry with a
normalized count (or Z-score) rather than straight
frequency.   We then compute the SVD and keep the
top 300 singular values to form semantic vectors for We define Pr (w <w )=Pr(NCS(w ,w )). We
each word.  Word w would be assigned the semantic choose to accept as valid relationships only those
vector  U D , where U  represents the row ofW= w k w
U corresponding to w and D  indicates that only thek
top k diagonal entries of D have been preserved. 
As a last comment, one would like to be able to
obtain a separate semantic vector for every word
(not just those in the top N).  SVD computations can
be expensive and impractical for large values of N.
Yet due to the fact that U and V  are orthogonalT
matrices, we can start with a matrix of reasonable-
sized N and ?fold in? the remaining terms, which is
the approach we have followed.  For details about
folding in terms, the reader is referred to Manning
and Sch?tze (1999, p. 563).
3.3 Correlating Semantic Vectors
To correlate these semantic vectors, we use
normalized cosine scores (NCSs) as we had
illustrated before (Schone and Jurafsky (2000)).
The normalized cosine score between two words w1
and w  is determined by first computing cosine2
values between each word?s semantic vector and
200 other randomly selected semantic vectors.  This
provides a mean (?) and variance (1 ) of correlation2
for each word.  The NCS is given to be 
We had previously illustrated NCS values on
various PPMVs and showed that this type of score
seems to be appropriately identifying semantic
relationships. (For example, the PPMVs of car/cars
and ally/allies had NCS values of 5.6 and 6.5
respectively, whereas car/cares and ally/all had
scored only -0.14 and -1.3.)  Further, we showed
that by performing this normalizing process, one can
estimate the probability that an NCS is random or
not.  We expect that random NCSs will be
approximately normally distributed according to
N(0,1). We can also estimate the distribution
N(? ,1 ) of true correlations and number of  termsT T2
in that distribution (n ).  If we define  a functionT
R
sem 1 2 1 2
PPMVs with Pr T , where T  is an acceptancesem 5 5
threshold. We showed in our earlier work that
T =85% affords high overall precision while still5
identifying most valid morphological relationships.
 3.4 Augmenting with Affix Frequencies
The first major change to our previous algorithm is
an attempt to overcome some of the weaknesses of
purely semantic-based morphology induction by
incorporating information about affix frequencies.
As validated by Kazakov (1997), high frequency
word endings and beginnings in inflectional
languages are very likely to be legitimate affixes.  In
English, for example, the highest frequency rule is
-s<L. CELEX suggests that 99.7% of our PPMVs
for this rule would be true. However, since the
purely semantic-based approach tends to select only
relationships with contextually similar meanings,
only 92% of the PPMVs are retained.  This suggests
that one might improve the analysis by
supplementing semantic probabilities with
orthographic-based probabilities (Pr ). orth
Our approach to obtaining Pr   is motivated byorth
an appeal to minimum edit distance (MED). MED
has been applied to the morphology induction
problem by other researchers (such as Yarowsky
and Wicentowski, 2000).  MED determines the
minimum-weighted set of insertions, substitutions,
and deletions required to transform one word into
another. For example, only a single deletion is
required to transform ?rates? into ?rate? whereas
two substitutions and an insertion are required to
transform it into ?rating.? Effectively, if Cost(&) is
transforming cost, Cost(rates<rate) = Cost(s<L)
whereas Cost(rates<rating)=Cost(es<ing). More
generally, suppose word X has circumfix C =B /E1 1 1
and pseudo-stem -S-, and word Y has circumfix
C =B /E  also with pseudo-stem -S-. Then,2 2 2
Cost(X<Y)=Cost(B SE <B SE )=Cost(C <C ).1 1 2 2 1 2
Since we are free to choose whatever cost function
we desire, we can equally choose one whose range
Cost(C1<C2)
1	
2 . f (C1<C2 )
max f (C1<Z) 
~Z
max f (W<C2)
~W
lies in the interval of [0,1]. Hence, we can assign Consider Table 2 which is a sample of PPMVs
Pr (X<Y) = 1-Cost(X<Y). This calculation implies from the ruleset for ?-s<L? along with theirorth
that the orthographic probability that X and Y are probabilities of validity.  A validity threshold (T ) of
morphological variants is directly derivable from the 85% would mean that the four bottom PPMVs
cost of transforming C  into C . would be deemed invalid.  Yet if we find that the1 2
The only question remaining is how to determine local contexts of these low-scoring word pairs
Cost(C <C ). This cost should depend on a number match the contexts of other PPMVs having high1 2
of factors: the frequency of the rule f(C <C ),  the scores (i.e., those whose scores exceed T ), then1 2
reliability of the metric in comparison to that of their probabilities of validity should increase.  If we
semantics (., where .  [0,1]), and the frequencies could compute a syntax-based probability for these
of other rules involving C  and C .  We define the words, namely Pr , then assuming independence1 2
orthographic probability of validity as we would have:
  Figure 3 describes the pseudo-code for an
We suppose that orthographic information is less (L) and right-hand (R) sides of each valid PPMV of
reliable than semantic information, so we arbitrarily a given ruleset, try to find a collection of words
set .=0.5.  Now since Pr (X<Y)=1-Cost(C <C ), from the corpus that are collocated with L and R butorth 1 2
we can readily combine it with Pr  if we assume which occur statistically too many or too few timessem
independence using the ?noisy or? formulation: in these collocations.  Such word sets form
  Pr (valid) = Pr  +Pr  - (Pr  Pr ).  (2) signatures.  Then, determine similar signatures fors-o sem orth sem orth  
By using this formula, we obtain 3% (absolute)
more of the correct PPMVs than semantics alone
had provided for the -s<L rule and, as will be
shown later, gives reasonable improvements overall.
3.5 Local Syntactic Context
Since a primary role of morphology ? inflectional
morphology in particular ?  is to convey syntactic
information, there is no guarantee that two words
that are morphological variants need to share similar
semantic properties.  This suggests that performance
could improve if the induction process took
advantage of  local, syntactic contexts around words
in addition to the more global, large-window
contexts used in semantic processing.  
Table 2: Sample probabilities for ?-s<L?
Word+s Word Pr Word+s Word Pr
agendas agenda .968 legends legend .981
ideas idea .974 militias militia 1.00
pleas plea 1.00 guerrillas guerrilla 1.00
seas sea 1.00 formulas formula 1.00
areas area 1.00 railroads railroad 1.00
Areas Area .721 pads pad .731
Vegas Vega .641 feeds feed .543
5
5
syntax
Pr (valid) = Pr  +Pr  - (Pr  Pr )
  s-o syntax s-o syntax
algorithm to compute Pr .  Essentially, thesyntax
algorithm has two major components.  First, for left
a randomly-chosen set of words from the corpus as
well as for each of the PPMVs of the ruleset that are
not yet validated.  Lastly, compute the NCS and
their corresponding probabilities (see equation 1)
between the ruleset?s signatures and those of the to-
be-validated PPMVs to see if they can be validated.
Table 3 gives an example of the kinds of
contextual words one might expect for the ?-s<L?
rule. In fact, the syntactic signature for ?-s<L? does
indeed include such words as are, other, these, two,
were, and have as indicators of words that occur on
the left-hand side of the ruleset, and a, an, this, is,
has, and A as indicators of the right-hand side.
These terms help distinguish plurals from singulars.
Table 3: Examples of ?-s<L? contexts
Context for L Context for R
agendas are seas were a legend this formula
two red pads pleas have militia is an area
these ideas other areas railroad has A guerrilla
There is an added benefit from following this
approach: it can also be used to find rules that,
though different, seem to convey similar
information . Table 4 illustrates a number of such
agreements.  We have yet to take advantage of this
feature, but it clearly could be of use for part-of-
speech induction.
procedure SyntaxProb(ruleset,corpus)
    leftSig  =GetSignature(ruleset,corpus,left)
    rightSig=GetSignature(ruleset,corpus,right)
         =Concatenate(leftSig, rightSig)ruleset
    (? ,1 )=ComparetoRandom( )ruleset ruleset ruleset
    foreach PPMV in ruleset
       if   (Pr (PPMV)  T  )   continueS-O 5
       wLSig=GetSignature(PPMV,corpus,left)
       wRSig=GetSignature(PPMV,corpus,right)
         =Concatenate(wLSig, wRSig)PPMV
       (? ,1 )=ComparetoRandom( )PPMV PPMV PPMV
       prob[PPMV]=Pr(NCS(PPMV,ruleset))
end procedure
function GetSignature(ruleset,corpus,side)
    foreach PPMV in ruleset
        if   (Pr (PPMV) < T  )   continueS-O 5
        if  (side=left) X = LeftWordOf(PPMV)
        else  X = RightWordOf(PPMV)
        CountNeighbors(corpus,colloc,X)
    colloc  =SortWordsByFreq(colloc)
    for i = 1 to 100 signature[i]=colloc[i]
    return signature
end function
procedure CountNeighbors(corpus,colloc,X)
   foreach W in Corpus
       push(lexicon,W)
       if (PositionalDistanceBetween(X,W)2)
          count[W] = count[W]+1
    foreach W in lexicon
       if ( Zscore(count[W]) 3.0   or
             Zscore(count[W]) -3.0)
           colloc[W]=colloc[W]+1
end procedure
Figure 3: Pseudo-code to find Probability  syntax Figure 4: Semantic strengths
Table 4: Relations amongst rules
Rule Relative Cos Rule Relative Cos
-s<L -ies<y 83.8 -ed<L -d<L 95.5
-s<L -es<L 79.5 -ing<L -e<L 94.3
-ed<L -ied<y 81.9 -ing<L -ting<L 70.7
3.6 Branching Transitive Closure
Despite the semantic, orthographic, and syntactic
components of the algorithm, there are still valid
PPMVs, (X<Y), that may seem unrelated due to
corpus choice or weak distributional properties.
However, X and Y may appear as members of other
valid PPMVs such as (X<Z) and (Z<Y) containing
variants (Z, in this case) which are either
semantically or syntactically related to both of the
other words.  Figure 4 demonstrates this property in
greater detail.  The words conveyed in Figure 4 are
all words from the corpus that have potential
relationships between variants of the word ?abuse.?
Links between two words, such as ?abuse? and
?Abuse,? are labeled with a weight which is the
semantic correlation derived by LSA. Solid lines
represent valid relationships with Pr 0.85 andsem
dashed lines indicate relationships with lower-than-
threshold scores. The absence of a link suggests that
either the potential relationship was never identified
or discarded at an earlier stage.  Self loops are
assumed for each node since clearly each word
should be related morphologically to itself. Since
there are seven words that are valid morphological
relationships of ?abuse,? we would like to see a
complete graph containing 21 solid edges.  Yet, only
eight connections can be found by semantics alone
(Abuse<abuse, abusers<abusing, etc.).  
However, note that there is a path that can be
followed along solid edges from every correct word
to every other correct variant.  This suggests that
taking into consideration link transitivity (i.e., if
X<Y , Y <Y , Y <Y ,... and Y<Z, then X<Z)1 1 2 2 3 t
may drastically reduce the number of deletions. 
There are two caveats that need to be considered
for transitivity to be properly pursued.  The first
caveat: if no rule exists that would transform X into
Z, we will assume that despite the fact that there
may be a probabilistic path between the two, we
Pr
? i 
 
t
N
t
j
0 pj.
function BranchProbBetween(X,Z)
prob=0
foreach independent path ?j
    prob = prob+Pr (X<Z) - (prob*Pr (X<Z) )
?j ?j
return prob
Figure 5: Pseudocode for Branching Probability
Figure 6: Morphologic relations of ?conduct?
will disregard such a path. The second caveat is that  the algorithms we test against.  Furthermore, since
we will say that paths can only consist of solid CELEX has limited coverage, many of these lower-
edges, namely each Pr(Y<Y ) on every path must frequency words could not be scored anyway.  Thisi i+1
exceed  the specified  threshold. cut-off also helps each of the algorithms to obtain
Given these constraints, suppose now there is a stronger statistical information on the words they do
transitive relation from X to Z by way of some process which means that any observed failures
intermediate path ?={Y Y Y }.  That is, assume cannot be attributed to weak statistics.i 1, 2,.. t
there is a path X<Y  Y <Y ,...,Y<Z.  Suppose Morphological relationships can be represented as1, 1 2 t
also that the probabilities of these relationships are directed graphs.  Figure 6, for instance, illustrates
respectively p , p , p ,...,p .  If  is a decay factor in the directed graph, according to CELEX, of words0 1 2 t
the unit interval accounting for the number of link associated with ?conduct.?  We will call the words
separations, then we will say that the Pr(X<Z) of such a directed graph the conflation set for any of
along path ?  has probability                       .       We the words in the graph.   Due to the difficulty ini
combine the probabilities of all independent paths developing a scoring algorithm to compare directed
between X and Z according to Figure 5: graphs, we will follow our earlier approach and only
If the returned probability exceeds T , we declare X5
and Z to be morphological variants of each other.
4 Evaluation
We compare this improved algorithm to our former
algorithm (Schone and Jurafsky (2000)) as well as
to Goldsmith's Linguistica (2000).  We use as input
to our system 6.7 million words of English
newswire, 2.3 million of German, and 6.7 million of
Dutch.  Our gold standards are the hand-tagged
morphologically-analyzed CELEX lexicon in each
of these languages (Baayen, et al, 1993). We apply
the algorithms only to those words of our corpora
with frequencies of 10 or more.   Obviously this cut-
off slightly limits the generality of our results, but
it also greatly decreases processing time for all of
compare induced conflation sets to those of
CELEX. To evaluate, we compute the number of
correct (&), inserted (,), and deleted (') words each
algorithm predicts for each hypothesized conflation
set.  If X  represents word w's conflation setw
according to an algorithm, and if Y   represents itsw
CELEX-based conflation set, then, 
& = ~w(|X Y |/|Y |), w w w
' = ~w(|Y -(X Y )|/|Y |), andw w w w
,  = ~w (|X -(X Y )|/|Y |),w w w w
In making these computations, we disregard any
CELEX words absent from our data set and vice
versa. Most capital words are not in CELEX so this
process also discards them. Hence, we also make an
augmented CELEX to incorporate capitalized forms.
Table 5 uses the above scoring mechanism to
compare the F-Scores (product of precision and
recall divided by average of the two ) of our system
at a cutoff threshold of 85% to those of our earlier
algorithm (?S/J2000?) at the same threshold;
Goldsmith; and a baseline system which performs
no analysis (claiming that for any word, its
conflation set only consists of itself). The ?S? and
?C? columns respectively indicate performance of
systems when scoring for suffixing and
circumfixing (using the unaugmented CELEX). The
?A? column shows circumfixing performance using
the augmented CELEX. Space limitations required
that we illustrate ?A? scores for one language only,
but performance in the other two language is
similarly degraded. Boxes are shaded out for
algorithms not designed to produce circumfixes. 
Note that each of our additions resulted in an
overall improvement which held true across each of
the three languages.  Furthermore, using ten-fold
cross validation on the English data, we find that F-
score differences of the S column are each
statistically significant at least at the 95% level.
Table 5: Computation of F-Scores
Algorithms English German Dutch
S C A S C S C
None 62.8 59.9 51.7 75.8 63.0 74.2 70.0
Goldsmith 81.8 84.0 75.8
S/J2000 85.2 88.3 82.2
+orthogrph 85.7 82.2 76.9 89.3 76.1 84.5 78.9
+ syntax 87.5 84.0 79.0 91.6 78.2 85.6 79.4
+ transitive 84.5 79.7 78.9 79.688.1 92.3 85.8
5 Conclusions
We have illustrated three extensions to our earlier
morphology induction work (Schone and Jurafsky
(2000)). In addition to induced semantics, we
incorporated induced orthographic, syntactic, and
transitive information resulting in almost a 20%
relative reduction in overall  induction error.  We
have also extended the work by illustrating
performance in German and Dutch where, to our
knowledge, complete morphology induction
performance measures have not previously been
obtained.  Lastly, we showed a mechanism whereby
circumfixes as well as combinations of prefixing
and suffixing can be induced in lieu of the suffix-
only strategies prevailing in most previous research.
For the future, we expect improvements could be
derived by coupling this work, which focuses
primarily on inducing regular morphology, with that
of Yarowsky and Wicentowski (2000), who assume
some information about regular morphology in order
to induce irregular morphology. We also believe
that some findings of this work can benefit other
areas of linguistic induction, such as part of speech.
Acknowledgments
The authors wish to thank the anonymous reviewers
for their thorough review and insightful comments.
References
Baayen, R.H., R. Piepenbrock, and H. van Rijn. (1993)
The CELEX lexical database (CD-ROM), LDC, Univ.
of Pennsylvania, Philadelphia, PA.
Brent, M., S. K. Murthy, A. Lundberg. (1995).
Discovering morphemic suffixes: A case study in
MDL induction. Proc. Of 5  Int?l Workshop onth
Artificial Intelligence and Statistics
D?Jean, H. (1998) Morphemes as necessary concepts for
structures: Discovery from untagged corpora.
Workshop on paradigms and Grounding in Natural
Language Learning, pp. 295-299.Adelaide, Australia
Deerwester, S., S.T. Dumais, G.W. Furnas, T.K.
Landauer, and R. Harshman. (1990) Indexing by
Latent Semantic Analysis. Journal of the American
Society of Information Science, Vol. 41, pp.391-407.
Gaussier, ?. (1999) Unsupervised learning of derivational
morphology from inflectional lexicons. ACL '99
Workshop: Unsupervised Learning in Natural
Language Processing, Univ. of Maryland.
Goldsmith, J. (1997/2000) Unsupervised learning of the
morphology of a natural language. Univ. of Chicago.
http://humanities.uchicago.edu/faculty/goldsmith.
Grabar, N. and P. Zweigenbaum. (1999) Acquisition
automatique de connaissances morphologiques sur le
vocabulaire  m?dical, TALN, Carg?se, France.
Harris, Z.  (1951) Structural Linguistics. University of
Chicago Press.
Jacquemin, C. (1997) Guessing morphology from terms
and corpora. SIGIR'97, pp. 156-167, Philadelphia, PA.
Kazakov, D. (1997) Unsupervised learning of na?ve
morphology with genetic algorithms. In W.
Daelemans, et al, eds., ECML/Mlnet Workshop on
Empirical Learning of Natural Language Processing
Tasks, Prague, pp. 105-111.
Landauer, T.K., P.W. Foltz, and D. Laham. (1998)
Introduction to Latent Semantic Analysis. Discourse
Processes. Vol. 25, pp. 259-284.
Manning, C.D. and H. Sch?tze. (1999) Foundations of
Statistical Natural Language Processing, MIT Press,
Cambridge, MA.
Nakisa, R.C., U.Hahn. (1996) Where defaults don't help:
the case of the German plural system.  Proc. of the
18th Conference of the Cognitive Science Society.
Schone, P. and D. Jurafsky. (2000) Knowledge-free
induction of morphology using latent semantic
analysis.  Proc. of the Computational Natural
Language Learning Conference, Lisbon, pp. 67-72.
Sch?tze, H. (1993) Distributed syntactic representations
with an application to part-of-speech tagging.
Proceedings of the IEEE International Conference on
Neural Networks, pp. 1504-1509.
Sproat, R. (1992) Morphology and Computation. MIT
Press, Cambridge, MA.
Xu, J., B.W. Croft. (1998) Corpus-based stemming using
co-occurrence of word variants. ACM Transactions on
Information Systems, 16 (1), pp. 61-81.
Yarowsky, D. and R. Wicentowski. (2000) Minimally
supervised morphological analysis by multimodal
alignment.  Proc. of the ACL 2000, Hong Kong.
  
Shallow Semantic Parsing of Chinese 
 
 
Honglin Sun1 
Center for Spoken Language Research 
University of Colorado at Boulder 
Daniel Jurafsky2 
Center for Spoken Language Research 
University of Colorado at Boulder 
 
Abstract 
 
In this paper we address the question of assigning 
semantic roles to sentences in Chinese. We show 
that good semantic parsing results for Chinese can 
be achieved with a small 1100-sentence training set. 
In order to extract features from Chinese, we 
describe porting the Collins parser to Chinese, 
resulting in the best performance currently reported 
on Chinese syntactic parsing; we include our head-
rules in the appendix. Finally, we compare English 
and Chinese semantic-parsing performance. While 
slight differences in argument labeling make a 
perfect comparison impossible, our results 
nonetheless suggest significantly better 
performance for Chinese. We show that much of 
this difference is due to grammatical differences    
between English and Chinese, such as the 
prevalence of passive in English, and the strict 
word order constraints on adjuncts in Chinese. 
 
 
1  Introduction 
 
Thematic roles (AGENT, THEME,  LOCATION, etc) 
provide a natural level of shallow semantic 
representation for a sentence.  A number of algorithms 
have been proposed for automatically assigning such 
shallow semantic structure to English sentences. But 
little is understood about how these algorithms may 
perform in other languages, and in general the role of 
language-specific idiosyncracies in the extraction of 
semantic content and how to train these algorithms 
when large hand-labeled training sets are not available. 
In this paper we address the question of assigning 
semantic roles to sentences in Chinese.   Our  work  is  
1 Currently at Department of Computer Science, Queens 
College, City University of New York. Email: sunh@qc.edu. 
2 Currently at Department of Linguistics, Stanford University. 
Email: jurafsky@stanford.edu. 
 
based on the SVM-based algorithm proposed for 
English by Pradhan et al(2003).  We first describe our 
creation of a small 1100-sentence Chinese corpus 
labeled according to principles from the English and 
(in-progress) Chinese PropBanks. We then introduce 
the features used by our SVM classifier, and show their 
performance on semantic parsing for both seen and 
unseen verbs, given hand-corrected (Chinese TreeBank) 
syntactic parses.  We then describe our port of the 
Collins (1999) parser to Chinese.  Finally, we apply our 
SVM semantic parser to a matching English corpus, 
and discuss the differences between English and 
Chinese that lead to significantly better performance on 
Chinese. 
 
2  Semantic Annotation and the Corpus 
 
Work on semantic parsing in English has generally 
related on the PropBank, a portion of the Penn 
TreeBank in which the arguments of each verb are 
annotated with semantic roles. Although a project to 
produce a Chinese PropBank is underway (Xue and 
Palmer 2003), this data is not expected to be available 
for another year. For these experiments, we therefore 
hand-labeled a small corpus following the Penn 
Chinese Propbank labeling guidelines (Xue, 2002). In 
this section, we first describe the semantic roles we 
used in the annotation and then introduce the data for 
our experiments. 
 
2.1  Semantic roles  
Semantic roles in the English (Kingsbury et al2002) 
and Chinese (Xue 2002) PropBanks are grouped into 
two major types: 
(1) arguments, which represent central participants in 
an event. A verb may require one, two or more 
arguments and they are represented with a contiguous 
sequence of numbers prefixed by arg, as arg0, arg1.  
(2) adjuncts, which are optional for an event but supply 
more information about an event, such as time, location, 
reason, condition, etc. An adjunct role is represented 
with argM plus a tag. For example, argM-TMP stands 
for temporal, argM-LOC for location. 
   In our corpus three argument roles and 15 adjunct 
roles appear. The whole set of roles is given at Table 1. 
 
Table1  The list of semantic roles 
Role Freq 
train 
Freq 
Test 
Note 
arg0  556 63  
arg1 872 91  
arg2  23 5  
argM-ADV 191 32 adverbial 
argM-BFY 26 2 beneficiary(e.g. give 
support [to the plan]) 
argM-CMP 35 3 object to be compared
argM-CND 14 1 condition 
argM-CPN 7 3 companion (e.g. talk 
[with you]) 
argM-DGR 53 4 degree 
argM-FRQ 3 0 frequency 
argM-LOC 207 31 location 
argM-MNR 10 1 manner 
argM-PRP 11 0 purpose or reason 
argM-RNG  7 2 range(e.g. help you [in 
this aspect]) 
argM-RST    15 1 result(e.g. increase [to 
$100]) 
argM-SRC
   
11 1 source(e.g. increase 
[from $50] to $100) 
argM-TMP 376 45 temporal 
argM-TPC 12 2 topic 
 
2.2 The training and test sets 
We created our training and test corpora by choosing 
10 Chinese verbs, and then selecting all sentences 
containing these 10 verbs from the 250K-word Penn 
Chinese Treebank 2.0. We chose the 10 verbs by 
considering frequency, syntactic diversity, and word 
sense.  We chose words that were frequent enough to 
provide sufficient training data. The frequencies of the 
10 verbs range from 41 to 230, with an average of 114. 
We chose verbs that were representative of the variety 
of verbal syntactic behavior in Chinese, including verbs 
with one, two, and three arguments, and verbs with 
various patterns of argument linking. Finally, we chose 
verbs that varied in their number of word senses.  
In total, we selected 1138 sentences.  The first author 
then labeled each verbal argument/adjunct in each 
sentence with a role label. We created our training and 
test sets by splitting the data for each verb into two 
parts: 90% for training and 10% for test. Thus there are 
1025 sentences in the training set and 113 sentences in 
the test set, and each test set verb has been seen in the 
training set. The list of verbs chosen and their number 
of senses, argument numbers and frequencies are given 
in Table 2. 
 
Table 2    List of verbs for experiments 
Verb # of 
senses 
Arg 
number 
Freq 
??/set up 1 2 106 
??/emerge 1 1 80 
??/publish 1 2 113 
??/give 2 3/2 41 
??/build into 2 2/3 113 
??/enter 1 2 123 
??/take place 1 2 230 
??/pass 3 2 75 
??/hope 1 2 90 
??/increase 1 2 167 
 
3  Semantic Parsing 
 
3.1 Architecture and Classifier 
Following the architecture of earlier semantic parsers 
like Gildea and Jurafsky (2002), we treat the semantic 
parsing task as a 1-of-N classification problem.  For 
each (non-aux/non-copula) verb in each sentence, our 
classifier examines each node in the syntactic parse tree 
for the sentence and assigns it a semantic role label. 
Most constituents are not arguments of the verb, and so 
the most common label is NULL.  Our architecture is 
based on a Support Vector Machine classifier,  
following Pradhan et al (2003). Since SVMs are binary 
classifiers, we represent this 1-of-19 classification 
problem (18 roles plus NULL) by training 19 binary 
one-versus-all classifiers.  
Following Pradhan et al (2003), we used tinySVM 
along with YamCha (Kudo and Matsumoto 2000, 2001) 
as the SVM training and test software.  The system 
uses a polynominal kernel with degree 2; the cost per 
unit violation of the margin, C=1; tolerance of the 
termination criterion e=0.001. 
 
3.2 Features 
The literature on semantic parsing in English relies on 
a number of features extracted from the input sentence 
and its parse. These include the constituent?s syntactic 
phrase type, head word, and governing category, the 
syntactic path in the parse tree connecting it to the verb,  
whether the constitutent is before or after the verb,  the 
subcategorization bias of the verb, and the voice 
(active/passive) of the verb. We investigated each of 
these features in Chinese; some acted quite similarly to 
English, while others showed interesting differences. 
Features that acted similarly to English include the 
target verb, the  phrase type,  the syntactic category of 
the constituent. (NP, PP, etc), and the subcategorization 
of the target verb.  The sub-categorization feature 
represents the phrase structure rule for the verb phrase 
containing the target verb (e.g., VP -> VB NP, etc). 
Five features (path, position, governing category, 
headword, and voice) showed interesting patterns that 
are discussed below. 
 
3.2.1 Path in the syntactic parse tree. The path 
feature represents the path from a constituent to the 
target verb in the syntactic parse tree, using "^" for 
ascending a parse tree, and "!" for descending. This 
feature manifests the syntactic relationship between the 
constituent and the target verb. For example the path 
?NP^IP!VP!VP!VV? indicates that the constituent is an 
?NP? which is the subject of the predicate verb. In 
general, we found the path feature to be sparse. In our 
test set, 60% of path types and 39% of path tokens are 
unseen in the training. The distributions of paths are 
very uneven. In the whole corpus, paths for roles have 
an average frequency of 14.5 while paths for non-roles 
have an average of 2.7. Within the role paths, a small 
number of paths account for majority of the total 
occurrences; among the 188 role path types, the top 20 
paths account for 86% of the tokens. Thus, although 
the path feature is sparse, its sparsity may not be a 
major problem in role recognition.  Of the 291 role 
tokens in our test set, only 9 have unseen paths, i.e., 
most of the unseen paths are due to non-roles. 
 
Table 3   The positional distribution of roles 
 
Role 
 
 
Before 
verb 
 
 
After 
verb 
 
 
Total 
 
arg0 
arg1 
arg2 
argM-ADV 
argM-BFY 
argM-CMP 
argM-CND 
argM-CPN 
argM-DGR 
argM-FRQ 
argM-LOC 
argM-MNR 
argM-PRP 
argM-RNG 
argM-RST 
argM-SRC 
argM-TMP 
argM-TPC 
 
547
319
223
28
38
15
10
233
11
11
9
12
408
14
 
72 
644 
28 
 
 
 
 
 
57 
3 
5 
 
 
 
16 
 
13 
 
 
619
963
28
223
28
38
15
10
57
3
238
11
11
9
16
12
421
14
 
Total 
 
1878
 
838 
 
2716   
 
 
3.2.2 Position before or after the verb. The position 
feature indicates that a constituent is before or after the 
target verb. In our corpus, 69% of the roles are before 
the verb while 31% are after the verb. As in English, 
the position is a useful cue for role identity. For 
example, 88% of arg0s are before the verb, 67% of 
arg1s are after the verb and all the arg2s are after the 
verb. Adjuncts have even a stronger bias. Ten of the 
adjunct types can only occur before the verb, while 
three are always after the verb. The two most common 
adjunct roles, argM-LOC and argM-TMP are almost 
always before the verb, a sharp difference from English. 
The details are shown seen in Table 3.  
 
3.2.3 Governing Category. The governing category 
feature is only applicable for NPs.  In the original 
formulation for English in Gildea and Jurafsky (2002), 
it answers the question: Is the NP governed by IP or 
VP? An NP governed by an IP is likely to be a subject, 
while an NP governed by a VP is more likely to be an 
object.  For Chinese, we added a third option in which 
the governing category of an NP is neither IP nor VP, 
but an NP. This is caused by the ?DE? construction, in 
which a clause is used as a modifier of an NP. For 
instance, in the example indicated in Figure 1, for the 
last NP, ??????????(?international Olympic 
conference?)  the parent node is NP, from where it goes 
down to the target verb ????(?taking place?).   
                                                
NP 
                                          
                         
                             CP 
                         
           VP                                                        NP 
                                              DEC                  
 
          ?????              ?       ???????? 
     in Paris take place          DE      intl Olympic conf. 
 
  ?the international Olympic Conference held in Paris? 
Figure 1  Example of  DE construction 
 
Since the governing category information is encoded in 
the path feature, it may be redundant; indeed this 
redundancy might explain why the governing category 
feature was used in Gildea & Jurafsky(2002) but not in 
Gildea and Palmer(2002). Since the ?DE? construction 
caused us to modify the feature for Chinese, we 
conducted several experiments to test whether the 
governing category feature is useful or whether it is 
redundant with the path and position features. Using 
the paradigm to be described in section 3.4, we found a 
small improvement using governing category, and so 
we include it in our model. 
 
3.2.4 Head word and its part of speech. The head 
word is a useful but sparse feature. In our corpus, of the 
2716 roles, 1016 head words (type) are used, in which 
646 are used only once. The top 20 words are given in 
Table 4. 
 
 
Table 4   Top 20 head words for roles 
Word Freq Word Freq
?/in  214 ??/China 25 
??/meeting 43 ?/for 23 
??/today 41 ??/statement 19 
?/at 40 ??/speech 18 
?/already 38 ??/stage 17 
??/enterprise 35 ??/government 16 
??/company 32 ??/present 16 
?/than  31 ??/bank 15 
?/will  30 ??/recently 14 
??/ceremony
  
28
  
??/base 14 
In the top 20 words, 4 are prepositions (??/in??
/at??/than??/for?) and 3 are temporal nouns(??
? /today??? /present??? /recently?) and 2 are 
adverbs(?? /already, ? /will?). These closed class 
words are highly correlated with specific semantic 
roles. For example,"?/for" occurs 195 times as the 
head of a constituent, of which 172 are non-roles, 19 
are argM-BFYs, 3 are arg1s and 1 is an argM-TPC."?
/in" occurs 644 times as a head, of which 430 are non-
roles, 174 are argM-LOCs, 24 are argM-TMPs, 9 are 
argM-RNGs, and 7 are argM-CND. "? /already" 
occurs 135 times as a head, of which 97 are non-roles 
and 38 are argM-ADVs. "??/today" occurs 69 times 
as a head, of which 41 are argM-TMPs and 28 are non-
roles. 
Within the open class words, some are closely 
correlated to the target verb. For example, "??
/meeting; conference" occurs 43 times as a head for 
roles, of which 24 are for the target "??/take place" 
and 19 for "??/pass". "??/ceremony" occurs 28 
times and all are arguments of "??"(take place)."?
?/statement" occurs 19 times, 18 for "??/release; 
publish" and one for "??/hope".   
These statistics emphasize the key role of the 
lexicalized head word feature in capturing the 
collocation between verbs and their arguments. Due to 
the sparsity of the head word feature, we also use the 
part-of-speech of the head word, following Surdeanu et 
al (2003). For example, ?7? 26?/July 26? may not 
be seen in the training,  but its POS, NT(temporal 
noun) , is a good indicator that it is a temporal. 
 
3.2.5  Voice. The passive construction in English gives 
information about surface location of arguments. In 
Chinese the marked passive voice is indicated by the 
use of the preposition "?/by" (POS tag LB in Penn 
Chinese Treebank). This passive, however, is seldom 
used in Chinese text. In our entire 1138-sentence 
corpus, only 13 occurrences of "LB" occur, and only 
one (in the training set) is related to the target verb. 
Thus we do not use the voice feature in our system. 
 
3.3 Experimental Results for Seen Verbs 
We now test the performance of our classifier, trained 
on the 1025-sentence training set and tested on the 113-
sentence test set introduced in Section 2.2. Recall that 
in this ?stratified? test set, each verb has been seen in 
the training data. The last row in Table 5 shows the 
current best performance of our system on this test set. 
The preceding rows show various subsets of the feature 
set, beginning with the path feature. 
 
Table 5  Semantic parsing results on seen verbs 
feature set                             P              R           F 
                                             (%)         (%)        (%) 
path                                     71.8        59.4       65.0 
path + pt                              72.9        62.9       67.5 
path + position                    72.5  60.8  66.2 
path + head POS                 77.6  63.3        69.7 
path + sub-cat                      80.8       63.6        71.2 
path + head word                 85.0       66.0  74.3 
path + target verb                85.8  68.4  76.1 
path + pt + gov + position 
        + subcat + target 
        + head word  
        + head POS                  91.7       76.0        83.1 
 
As Table 5 shows, the most important feature is path, 
followed by target verb and head word.  In general, the 
lexicalized features are more important than the other 
features. The combined feature set outperforms any 
other feature sets with less features and it has an F-
score of 83.1. The performance is better for the 
arguments (i.e., only ARG0-2), 86.7 for arg0 and 89.4 
for arg1. 
 
3.4 Experimental Results for Unseen Verbs 
To test the performance of the semantic parser on 
unseen verbs, we used cross-validation, selecting one 
verb as test and the other 9 as training, and iterating 
with each verb as test. All the results are given in Table 
6. The results for some verbs are almost equal to the 
performance on seen verbs. For example for ???? 
and ????, the F-scores are over 80. However, for 
some verbs, the results are much worse. The worst case 
is the verb ????, which has an F-score of 11.  This is 
due to the special syntactic characteristics of this verb. 
This verb can only have one argument and this 
argument most often follows the verb, in object 
position. In the surface structure, there is often an NP 
before the verb working as its subject, but semantically 
this subject cannot be analyzed as arg0.  For example: 
(1)??/China ?/not ?/will ??/emerge ??/food 
??/crisis.  (A food crisis won't emerge in China.) 
(2)??/Finland ??/economy ??/emerge  ?/AUX  
?? /post-war ? /most ?? /serious ? /AUX ??
/depression.  (The most severe post-war depression 
emerged  in the Finland economy.) 
The subjects, ???/China? in (1) and ???/Finland 
??/economy?, are locatives, i.e. argM-LOC, and the 
objects, ???/food ??/crisis? in (1) and ???/post-
war ?/most ??/serious ?/AUX ??/depression? in 
(2), are analyzed as arg0. But the parser classified the 
subjects as arg0 and the objects as arg1. These are 
correct for most common verbs but wrong for this 
particular verb. It is difficult to know how common this 
problem would be in a larger, test set. The fact that we 
considered diversity of syntactic behavior when 
selecting verbs certainly helps make this test set reflect 
the difficult cases.  
If most verbs prove not to be as idiosyncratic as ???
/emerge?, the real performance of the parser on unseen 
verbs may be better than the average given here. 
Table 6   Experimental Results for Unseen Verbs 
    target               P(%) R(%) F(%) 
??/publish 90.7 72.9 80.8 
??/increase 49.6 34.3 40.5 
??/take place 90.1 63.3 74.4 
??/build into 65.2 55.5 60.0 
??/give 65.7 37.9 48.1 
??/pass 85.9 77.0 81.2 
??/emerge 12.6 10.2 11.3 
??/enter 81.9 58.8 68.4 
??/set up 79.0 61.1 68.9 
??/hope 77.7 35.9 49.1 
Average          69.8 50.7 58.3 
Another important difficulty in processing unseen 
verbs is the fact that roles in PropBank are defined in a 
verb-dependent way. This may be easiest to see with an 
English example. The roles arg2, arg3, arg4 have 
different meaning for different verbs; underlined in the 
following are some examples of arg2: 
(a) The state gave  CenTrust 30 days to sell the Rubens. 
(b) Revenue increased 11 to 2.73 billion from 2.46 
billion. 
(c) One of Ronald Reagan 's attributes as President was 
that he rarely gave his blessing to the claptrap that 
passes for consensus in various international 
institutions. 
In (a), arg2 represents the goal of ?give?, in (b), it 
represents the amount of increase, and in (c) it 
represents yet another role. These complete different 
semantic relations are given the same semantic label. 
For unseen verbs, this makes it difficult for the 
semantic parser to know what would count as an arg2.  
 
4 Using Automatic Parses 
 
The results in the last section are based on the use of 
perfect (hand-corrected) parses drawn from the Penn 
Chinese Treebank. In practical use, of course, 
automatic parses will not be as accurate. In this section 
we describe experiments on semantic parsing when 
given automatic parses produced by an automatic 
parser, the Collins (1999) parser, ported to Chinese. 
We first describe how we ported the Collins parser to 
Chinese and then present the results of the semantic 
parser with features drawn from the automatic parses.  
 
4.1 The Collins parser for Chinese 
The Collins parser is a state-of-the-art statistical parser 
that has high performance on English (Collins, 1999) 
and Czech(Collins et al 1999). There have been 
attempts in applying other algorithms in Chinese 
parsing (Bikel and Chiang, 2000; Chiang and Bikel 
2002; Levy and Manning 2003), but there has been no 
report on applying the Collins parser on Chinese. 
The Collins parser is a lexicalized statistical parser 
based on a head-driven extended PCFG model; thus the 
choice of head node is crucial to the success of the 
parser. We analyzed the Penn Chinese Treebank data 
and worked out head rules for the Chinese Treebank 
grammar (we were unable to find any published head 
rules for Chinese in the literature). There are two major 
differences in the head rules between English and 
Chinese. First, NP heads in Chinese are rigidly 
rightmost, that is to say, no modifiers of an NP can 
follow the head. In contrast, in English a modifier may 
follow the head. Second, just as with NPs in Chinese, 
the head of ADJP is rigidly rightmost. In English, by 
contrast, the head of an ADJP is mainly the leftmost 
constituent. Our head rules for the Chinese Treebank 
grammar are given in the Appendix. 
In addition to the head rules, we modified the POS tags 
for all punctuation.  This is because all cases of 
punctuation in the Penn Chinese Treebank are assigned 
the same POS tag ?PU?. The Collins parser, on the 
other hand, expects the punctuation tags in the English 
TreeBank format, where the tag for a punctuation mark 
is the punctuation mark itself. We therefore replaced 
the POS tags for all punctuation marks in the Chinese 
data to conform to the conventions in English. 
Finally, we made one further augmentation also related 
to punctuation. Chinese has one punctuation mark that 
does not exist in English. This commonly used mark,  
?semi-stop?, is used in Chinese to link coordinates 
within a sentence (for example between elements of a 
list). This function is represented in English by a 
comma. But the comma in English is ambiguous; in 
addition to its use in coordination and lists, it can also 
represent the end of a clause. In Chinese, by contrast 
the semi-stop has only the conjunction/list function. 
Chinese thus uses the regular comma only for 
representing clause boundaries. We investigated two 
ways to model the use of the Chinese semi-stop: (1) 
just converting the semi-stop to the comma, thus 
conflating the two functions as in English; and (2) by 
giving the semi-stop the POS tag ?CC?, a conjunction. 
We compared parsing results with these two methods; 
the latter (conjunction) method gained 0.5% net 
improvement in F-score over the former one. We 
therefore include it in our Collins parser port. 
We trained the Collins parser on the Penn Chinese 
Treebank(CTB) Release 2 with 250K words, first 
removing from the training set any sentences that occur 
in the test set for the semantic parsing experiments. We 
then tested on the test set used in the semantic parsing 
which includes 113 sentences(TEST1). The results of 
the syntactic parsing on the test set are shown in Table 
7. 
 
Table 7     Results for syntactic parsing, trained on 
CTB Release 2, tested on test set in semantic parsing 
    LP(%) LR(%) F1(%) 
overall            81.6 82.1 81.0 
len<=40          86.1 85.5 86.7 
 
To compare the performance of the Collins parser on 
Chinese with those of other parsers, we conducted an 
experiment in which we used the same training and test 
data (Penn Chinese Treebank Release 1, with 100K 
words) as used in those reports. In this experiment, we 
used articles 1-270 for training and 271-300 as 
test(TEST2). Table 8 shows the results and the 
comparison with other parsers. 
Table 8 only shows the performance on sentences ? 40 
words. Our performance on all the sentences TEST2 is 
P/R/F=82.2/83.3/82.7.  It may seem surprising that the 
overall F-score on TEST2 (82.7) is higher than the 
overall F-score on TEST1 (81.0) despite the fact that 
our TEST1 system had more than twice as much 
training as our TEST2 system.  The reason lies in the 
makeup of the two test sets; TEST1 consists of 
randomly selected long sentences; TEST2 consists of 
sequential text, including many short sentences. The 
average sentence length in TEST1 is 35.2 words, vs. 
22.1 in TEST2. TEST1 has 32% long sentences (>40 
words) while TEST2 has only 13%.  
 
Table 8      Comparison with other parsers: TEST2 
 ? 40 words 
 LP(%) LR(%) F1(%)
Bikel & Chiang 2000      77.2 76.2 76.7 
Chiang & Bikel 2002      81.1 78.8 79.9 
Levy & Manning 2003   78.4 79.2 78.8 
  Collins parser                86.4 85.5 85.9 
 
4.2 Semantic parsing using Collins parses 
In the test set of 113 sentences, there are 3 sentences in 
which target verbs are given the wrong POS tags, so 
they can not be used for semantic parsing. For the 
remaining 100 sentences, we used the feature set 
containing eight features (path, pt, gov,  position, 
subcat, target, head word and head POS) , the same as 
that used in the experiment on perfect parses.  The 
results are shown in Table 9. 
 
Table 9  Result for semantic parsing using automatic 
syntactic parses 
 P(%) R(%) F(%) 
110 sentences 86.0 70.8 77.6 
113 sentences 86.0 69.2 76.7 
 
Compared to the F-score using hand-corrected 
syntactic parses from the TreeBank, using automatic 
parses decreases the F-score by 6.4. 
 
5  Comparison with English 
 
Recent research on English semantic parsing has 
achieved quite good results by relying on the large 
amounts of training data available in the Propbank and 
Framenet (Baker et al 1998) databases.  But in 
extending the semantic parsing approach to other 
languages, we are unlikely to always have large data 
sets available. Thus it is crucial to understand how 
small amounts of data affect semantic parsing. At the 
same time, there have been no comparisons between 
English and other languages with respect to semantic 
parsing. It is thus not clear what language-specific 
issues may arise in general with the automatic mapping 
of syntactic structures to semantic relations. In this 
section, we compare English and Chinese by using the 
same semantic parser, similar verbs and similar 
amounts of data. Our goals are two-folds: (1) to 
compare the performance of the parser on English and 
Chinese; and (2) to understand differences between 
English and Chinese that affect automatic mapping 
between syntax and semantics. At first, we introduce 
the data used in the experiments and then we  present 
the results and give analysis. 
 
5.1 The English data  
In order to create an English corpus which matched our 
small Chinese corpus, we selected 10 English verbs 
which corresponded to our 10 Chinese verbs in 
meaning and frequency; exact translations of the 
Chinese when possible, or the closest possible word 
when an extract translation did not exist. The English 
verbs and their Chinese correspondents are given in 
Table 10. 
Table 10   English verbs chosen for experiments 
English   Freq  Chinese English Freq Chinese
build 46 ?? hold 120 ?? 
emerge 30 ?? hope 63 ?? 
enter 108 ?? increase 231 ?? 
found 248 ?? pass 143 ?? 
give 124 ?? publish 77 ?? 
 
Table 12       The comparison between adjuncts in English and Chinese 
 English Chinese 
Role Before  
verb 
After 
verb 
Freq in 
test 
P     R     F 
(%) 
Before 
verb 
After 
verb 
Freq in 
test 
P      R       F 
      (%) 
argM-ADV 22 43 5 0      0      0 223 0 37 91.3    56.8   70 
argM-LOC 25 82 11 80   36.4   50 233 5 31 90.0    87.1  88.5
argM-MNR 22 75 14 0      0      0 11 0 1 0        0        0 
argM-TMP 119 164 37 66.7   27    38.5 408 13 44 96.7   65.9   78.4
 
After the verbs were chosen, we extracted every 
sentence containing these verbs from section 02 to 
section 21 of the Wall Street Journal data from the 
Penn English Propbank. The number of sentences for 
each verb is given in Table 10. 
 
5.2 Experimental Results 
As in our Chinese experiments, we used our SVM-
based classifier, using N one-versus-all classifiers. 
Table 11 shows the performance on our English test set 
(with Chinese for comparison), beginning with the path 
feature, and incrementally adding features until in the 
last row we combine all 8 features together.  
 
Table 11       Experimental results of English 
 Chinese English 
feature set R/F/P P/R/F 
path 71.8/59.4/65.0 78.2/48.3/59.7 
path + pt 72.9/62.9/67.5 77.4/51.2/61.6 
path + position 72.5/60.8/66.2 75.7/50.9/60.8 
path + hd POS 77.6/63.3/69.7 79.1/49.7/61.0 
path + sub-cat 80.8/63.6/71.2 79.9/45.3/57.8 
path + hd word 85.0/66.0/74.3 84.0/47.7/60.8 
path + target 85.8/68.4/76.1 85.7/49.1/62.5 
COMBINED 91.7/76.0/83.1 84.1/62.2/71.5 
 
It is immediately clear from Table 11 that using similar 
verbs, the same amount of data, the same classifier, the 
same number of roles, and the same features, the 
results from English are much worse than those for 
Chinese. While some part of the difference is probably 
due to idiosyncracies of particular sentences in the 
English and Chinese data, other aspects of the 
difference might be accounted for systematically, as we 
discuss in the next section. 
 
5.3 Discussion: English/Chinese differences  
We first investigated whether the differences between 
English and Chinese could be attributed to particular 
semantic roles.  We found that this was indeed the case. 
The great bulk of the error rate difference between 
English and Chinese was caused by the 4 adjunct 
classes argM-ADV, argM-LOC, argM-MNR, and 
argM-TMP, which together account for 19.6% of the 
role tokens in our English corpus. The average F-score 
in English for the four roles is 36.7, while in Chinese 
the F-score for the four roles is 78.6. Why should these 
roles be so much more difficult to identify in English 
than Chinese? We believe the answer lies in the 
analysis of the position feature in section 3.2.2. This is 
repeated, with error rate information in Table 12. We 
see there that adjuncts in English have no strong 
preference for occurring before or after the verb. 
Chinese adjuncts, by contrast, are well-known to have 
an extremely strong preference to be preverbal, as 
Table 12 shows. The relatively fixed word order of 
adjuncts makes it much easier in Chinese to map these 
roles from surface syntactic constituents than in 
English. 
If the average F-score of the four adjuncts in English is 
raised to the level of that in Chinese, the overall F-
score on English would be raised from 71.5 to 79.7, 
accounting for 8.2 of the 11.6 difference in F-scores 
between the two languages.  
We next investigated the one feature from our original 
English-specific feature set that we had dropped in our 
Chinese system: passive. Recall that we dropped this 
feature because marked passives are extremely rare in 
Chinese. When we added this feature back into our 
English system, the performance rose from 
P/R/F=84.1/62.2/71.5 to 86.4/65.1/74.3.  As might be 
expected, this effect of voice is mainly reflected in an 
improvement on arg0 and arg1, as Table 13 shows 
below: 
 
Table 13.  Improvement in English semantic parsing 
with the addition of the voice feature 
 -voice +voice 
 P      R      F P      R        F 
arg0 88.9  75.3  81.5 94.4   80     86.6 
arg1 86.5  82.8  84.6 88.5  86.2   87.3 
A third source of English-Chinese differences is the 
distribution of roles; the Chinese data has 
proportionally more adjuncts (ARGMs), while the 
English data has proportionally more oblique 
arguments (ARG2, ARG3, ARG4).  Oblique arguments 
are more difficult to process than other arguments, as 
was discussed in section 3.4.  This difference is most 
likely to be caused by labeling factors rather than by 
true structural differences between English in Chinese. 
In summary, the higher performance in our Chinese 
system is due to 3 factors: the importance of passive in 
English; the strict word-order constraints of Chinese 
adverbials, and minor labeling differences. 
 
6  Conclusions  
 
We can draw a number of conclusions from our 
investigation of semantic parsing in Chinese. First, 
reasonably good performance can be achieved with a 
very small (1100 sentences) training set. Second, the 
features that we extracted for English semantic parsing 
worked well when applied to Chinese.  Many of these 
features required creating an automatic parse; in doing 
so we showed that the Collins (1999) parser when 
ported to Chinese achieved the best reported 
performance on Chinese syntactic parsing.  Finally, we 
showed that semantic parsing is significantly easier in 
Chinese than in English. We show that this 
counterintuitive result seems to be due to the strict 
constraints on adjunct ordering in Chinese, making 
adjuncts easier to find and label. 
 
Acknowledgements 
 
This work was partially supported by the National 
Science Foundation via a KDD Supplement to NSF 
CISE/IRI/Interactive Systems Award  IIS-9978025. 
Many thanks to Ying Chen for her help on the Collins 
parser port, and to Nianwen Xue and Sameer Pradhan 
for providing the data. Thanks to Kadri Hacioglu, 
Wayne Ward, James Martin, Martha Palmer, and three 
anonymous reviewers for helpful  advice. 
 
 
Appendix: Head rules for Chinese  
 
Parent     Direction          Priority List 
ADJP      Right        ADJP  JJ  AD 
ADVP     Right        ADVP AD CS JJ NP PP P VA VV 
CLP         Right       CLP  M  NN  NP 
CP           Right        CP  IP  VP 
DNP        Right        DEG   DNP  DEC   QP 
DP           Left          M(r)   DP  DT  OD 
DVP        Right        DEV  AD  VP 
IP            Right        VP  IP  NP 
LCP        Right        LCP  LC 
LST        Right        CD  NP  QP 
NP          Right        NP  NN  IP  NR  NT 
PP           Left          P   PP 
PRN        Left          PU 
QP           Right       QP  CLP  CD 
UCP        Left          IP  NP  VP 
VCD       Left          VV  VA  VE 
VP          Left          VE VC VV VNV VPT VRD   
                                VSB VCD VP 
VPT         Left         VA  VV 
VRD        Left         VVl VA 
VSB         Right      VV  VE 
 
References 
 
Baker, Collin F., Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkekey FrameNet Project. In 
Proceeding of COLING/ACL. 
 
Bikel, Daniel and David Chiang. 2000. Two Statistical 
Parsing models Applied to the Chinese Treebank. In 
Proceedings of the Second Chinese Language 
Processing Workshop, pp. 1-6. 
 
Chiang, David and Daniel Bikel. 2002. Recovering 
Latent Information in Treebanks. In Proceedings of 
COLING-2002, pp.183-189. 
  
Collins, Michael. 1999. Head-driven Statistical Models 
for Natural Language Parsing. Ph.D. dissertation, 
University of Pennsylvannia.  
 
Collins, Michael, Jan Hajic, Lance Ramshaw and 
Christoph Tillmann. 1999. A Statistical Parser for 
Czech. In Proceedings of the 37th Meeting of the ACL, 
pp. 505-512. 
 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics, 28(3):245-288. 
 
Gildea, Daniel and Martha Palmer. 2002. The 
Necessity of Parsing for Predicate Argument 
Recognition, In Proceedings of the 40th Meeting of the 
ACL, pp. 239-246. 
 
Kingsbury, Paul, Martha Palmer, and Mitch Marcus. 
2002. Adding semantic annotation to the Penn 
Treebank. In Proceedings of HLT-02. 
 
Kudo, Taku and Yuji Matsumoto. 2000. Use of support 
vector learning for chunk Identification. In 
Proceedings of the 4th Conference on CoNLL, pp. 
142-144. 
 
Kudo, Taku and Yuji Matsumoto. 2001 Chunking with 
Support Vector Machines. In Proceeding of the 2nd 
Meeting of the NAACL. pp.192-199. 
 
Levy, Roger and Christopher Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank? 
ACL 2003, pp. 439-446. 
 
Pradhan, Sameer, Kadri Hacioglu,. Wayne Ward, 
James Martin, and Daniel Jurafsky. 2003. ?Semantic 
Role Parsing: Adding Semantic Structure to 
Unstructured Text?. In the Proceedings of the 
International Conference on Data Mining (ICDM-
2003), Melbourne, FL, 2003 
 
Surdeanu, Mihai, Sanda Harabagiu, John Williams and 
Paul Aarseth. 2003. Using Predicate-Argument 
Structures for Information Extraction, In Proceedings 
of ACL. 
 
Xue, Nianwen. 2002. Guidelines for the Penn Chinese 
Proposition Bank (1st Draft), UPenn. 
 
Xue, Nianwen, Fu-Dong Chiou and Martha Palmer. 
2002. Building a large-scale annotated Chinese corpus. 
In Proceedings of COLING-2002. 
 
Xue, Nianwen, Martha Palmer. 2003. Annotating the 
propositions in the Penn Chinese Treebank. In 
Proceedings of the 2nd SIGHAN Workshop on Chinese 
Language Processing. 
Automatic Labeling of Semantic Roles
Daniel Gildea
University of California, Berkeley, and
International Computer Science Institute
gildea@cs.berkeley.edu
Daniel Jurafsky
Department of Linguistics
University of Colorado, Boulder
jurafsky@colorado.edu
Abstract
We present a system for identify-
ing the semantic relationships, or se-
mantic roles, lled by constituents of
a sentence within a semantic frame.
Various lexical and syntactic fea-
tures are derived from parse trees
and used to derive statistical clas-
siers from hand-annotated training
data.
1 Introduction
Identifying the semantic roles lled by con-
stituents of a sentence can provide a level of
shallow semantic analysis useful in solving a
number of natural language processing tasks.
Semantic roles represent the participants in
an action or relationship captured by a se-
mantic frame. For example, the frame for one
sense of the verb \crash" includes the roles
Agent, Vehicle and To-Location.
This shallow semantic level of interpreta-
tion can be used for many purposes. Cur-
rent information extraction systems often use
domain-specic frame-and-slot templates to
extract facts about, for example, nancial
news or interesting political events. A shal-
low semantic level of representation is a more
domain-independent, robust level of represen-
tation. Identifying these roles, for example,
could allow a system to determine that in
the sentence \The rst one crashed" the sub-
ject is the vehicle, but in the sentence \The
rst one crashed it" the subject is the agent,
which would help in information extraction in
this domain. Another application is in word-
sense disambiguation, where the roles associ-
ated with a word can be cues to its sense. For
example, Lapata and Brew (1999) and others
have shown that the dierent syntactic sub-
catgorization frames of a verb like \serve" can
be used to help disambiguate a particular in-
stance of the word \serve". Adding seman-
tic role subcategorization information to this
syntactic information could extend this idea
to use richer semantic knowledge. Semantic
roles could also act as an important inter-
mediate representation in statistical machine
translation or automatic text summarization
and in the emerging eld of Text Data Mining
(TDM) (Hearst, 1999). Finally, incorporat-
ing semantic roles into probabilistic models of
language should yield more accurate parsers
and better language models for speech recog-
nition.
This paper proposes an algorithm for au-
tomatic semantic analysis, assigning a se-
mantic role to constituents in a sentence.
Our approach to semantic analysis is to
treat the problem of semantic role labeling
like the similar problems of parsing, part of
speech tagging, and word sense disambigua-
tion. We apply statistical techniques that
have been successful for these tasks, including
probabilistic parsing and statistical classica-
tion. Our statistical algorithms are trained
on a hand-labeled dataset: the FrameNet
database (Baker et al, 1998). The FrameNet
database denes a tagset of semantic roles
called frame elements, and includes roughly
50,000 sentences from the British National
Corpus which have been hand-labeled with
these frame elements. The next section de-
scribes the set of frame elements/semantic
roles used by our system. In the rest of this
paper we report on our current system, as well
as a number of preliminary experiments on
extensions to the system.
2 Semantic Roles
Historically, two types of semantic roles have
been studied: abstract roles such as Agent
and Patient, and roles specic to individual
verbs such as Eater and Eaten for \eat".
The FrameNet project proposes roles at an in-
termediate level, that of the semantic frame.
Frames are dened as schematic representa-
tions of situations involving various partici-
pants, props, and other conceptual roles (Fill-
more, 1976). For example, the frame \conver-
sation", shown in Figure 1, is invoked by the
semantically related verbs \argue", \banter",
\debate", \converse", and \gossip" as well
as the nouns \argument", \dispute", \discus-
sion" and \ti". The roles dened for this
frame, and shared by all its lexical entries,
include Protagonist1 and Protagonist2
or simply Protagonists for the participants
in the conversation, as well as Medium, and
Topic. Example sentences are shown in Ta-
ble 1. Dening semantic roles at the frame
level avoids some of the diculties of at-
tempting to nd a small set of universal, ab-
stract thematic roles, or case roles such as
Agent, Patient, etc (as in, among many
others, (Fillmore, 1968) (Jackendo, 1972)).
Abstract thematic roles can be thought of
as being frame elements dened in abstract
frames such as \action" and \motion" which
are at the top of in inheritance hierarchy of
semantic frames (Fillmore and Baker, 2000).
The preliminary version of the FrameNet
corpus used for our experiments contained 67
frames from 12 general semantic domains cho-
sen for annotation. Examples of domains (see
Figure 1) include \motion", \cognition" and
\communication". Within these frames, ex-
amples of a total of 1462 distinct lexical pred-
icates, or target words, were annotated: 927
verbs, 339 nouns, and 175 adjectives. There
are a total of 49,013 annotated sentences, and
99,232 annotated frame elements (which do
not include the target words themselves).
3 Related Work
Assignment of semantic roles is an impor-
tant part of language understanding, and has
been attacked by many computational sys-
tems. Traditional parsing and understand-
ing systems, including implementations of
unication-based grammars such as HPSG
(Pollard and Sag, 1994), rely on hand-
developed grammars which must anticipate
each way in which semantic roles may be real-
ized syntactically. Writing such grammars is
time-consuming, and typically such systems
have limited coverage.
Data-driven techniques have recently been
applied to template-based semantic interpre-
tation in limited domains by \shallow" sys-
tems that avoid complex feature structures,
and often perform only shallow syntactic
analysis. For example, in the context of
the Air Traveler Information System (ATIS)
for spoken dialogue, Miller et al (1996) com-
puted the probability that a constituent such
as \Atlanta" lled a semantic slot such as
Destination in a semantic frame for air
travel. In a data-driven approach to infor-
mation extraction, Rilo (1993) builds a dic-
tionary of patterns for lling slots in a spe-
cic domain such as terrorist attacks, and
Rilo and Schmelzenbach (1998) extend this
technique to automatically derive entire case
frames for words in the domain. These last
systems make use of a limited amount of hand
labor to accept or reject automatically gen-
erated hypotheses. They show promise for
a more sophisticated approach to generalize
beyond the relatively small number of frames
considered in the tasks. More recently, a do-
main independent system has been trained on
general function tags such as Manner and
Temporal by Blaheta and Charniak (2000).
4 Methodology
We divide the task of labeling frame elements
into two subtasks: that of identifying the
boundaries of the frame elements in the sen-
tences, and that of labeling each frame ele-
ment, given its boundaries, with the correct
role. We rst give results for a system which
confer?v
debate?v
converse?v
gossip?v
dispute?n
discussion?n
tiff?n
ConversationFrame:
Protagonist?1
Protagonist?2
Protagonists
Topic
Medium
Frame Elements:
talk?v
Domain: Communication Domain: Cognition
Frame: Questioning
Topic
Medium
Frame Elements: Speaker
Addressee
Message
Frame:
Topic
Medium
Frame Elements: Speaker
Addressee
Message
Statement
Frame:
Frame Elements:
Judgment
Judge
Evaluee
Reason
Role
dispute?n
blame?v fault?n
admire?v
admiration?n disapprove?v
blame?n
appreciate?v
Frame:
Frame Elements:
Categorization
Cognizer
Item
Category
Criterion
Figure 1: Sample domains and frames from the FrameNet lexicon.
Frame Element Example (in italics) with target verb Example (in italics) with target noun
Protagonist 1 Kim argued with Pat Kim had an argument with Pat
Protagonist 2 Kim argued with Pat Kim had an argument with Pat
Protagonists Kim and Pat argued Kim and Pat had an argument
Topic Kim and Pat argued about politics Kim and Pat had an argument about politics
Medium Kim and Pat argued in French Kim and pat had an argument in French
Table 1: Examples of semantic roles, or frame elements, for target words \argue" and \argu-
ment" from the \conversation" frame
labels roles using human-annotated bound-
aries, returning to the question of automat-
ically identifying the boundaries in Section
5.3.
4.1 Features Used in Assigning
Semantic Roles
The system is a statistical one, based on train-
ing a classier on a labeled training set, and
testing on an unlabeled test set. The sys-
tem is trained by rst using the Collins parser
(Collins, 1997) to parse the 36,995 train-
ing sentences, matching annotated frame el-
ements to parse constituents, and extracting
various features from the string of words and
the parse tree. During testing, the parser is
run on the test sentences and the same fea-
tures extracted. Probabilities for each possi-
ble semantic role r are then computed from
the features. The probability computation
will be described in the next section; the fea-
tures include:
Phrase Type: This feature indicates the
syntactic type of the phrase expressing
the semantic roles: examples include
noun phrase (NP), verb phrase (VP), and
clause (S). Phrase types were derived au-
tomatically from parse trees generated by
the parser, as shown in Figure 2. The
parse constituent spanning each set of
words annotated as a frame element was
found, and the constituent's nonterminal
label was taken as the phrase type. As
an example of how this feature is useful,
in communication frames, the Speaker
is likely appear a a noun phrase, Topic
as a prepositional phrase or noun phrase,
and Medium as a prepostional phrase, as
in: \We talked about the proposal over
the phone." When no parse constituent
was found with boundaries matching
those of a frame element during testing,
the largest constituent beginning at the
frame element's left boundary and lying
entirely within the element was used to
calculate the features.
Grammatical Function: This feature at-
tempts to indicate a constituent's syntac-
tic relation to the rest of the sentence,
SNP
PRP
VP
VBD
NP
SBAR
IN
S
NNP
VP
VBD
NP PP
PRP IN
NP
NN
Goal SourceTheme Target
NP
He heard the sound of liquid slurping in a metal container as approached him from behindFarrell
Figure 2: A sample sentence with parser output (above) and FrameNet annotation (below).
Parse constituents corresponding to frame elements are highlighted.
for example as a subject or object of a
verb. As with phrase type, this feature
was read from parse trees returned by
the parser. After experimentation with
various versions of this feature, we re-
stricted it to apply only to NPs, as it was
found to have little eect on other phrase
types. Each NP's nearest S or VP ances-
tor was found in the parse tree; NPs with
an S ancestor were given the grammati-
cal function subject and those with a VP
ancestor were labeled object. In general,
agenthood is closely correlated with sub-
jecthood. For example, in the sentence
\He drove the car over the cli", the rst
NP is more likely to ll the Agent role
than the second or third.
Position: This feature simply indicates
whether the constituent to be labeled oc-
curs before or after the predicate den-
ing the semantic frame. We expected
this feature to be highly correlated with
grammatical function, since subjects will
generally appear before a verb, and
objects after. Moreover, this feature
may overcome the shortcomings of read-
ing grammatical function from a con-
stituent's ancestors in the parse tree, as
well as errors in the parser output.
Voice: The distinction between active and
passive verbs plays an important role
in the connection between semantic role
and grammatical function, since direct
objects of active verbs correspond to sub-
jects of passive verbs. From the parser
output, verbs were classied as active or
passive by building a set of 10 passive-
identifying patterns. Each of the pat-
terns requires both a passive auxiliary
(some form of \to be" or \to get") and a
past participle.
Head Word: As previously noted, we ex-
pected lexical dependencies to be ex-
tremely important in labeling semantic
roles, as indicated by their importance
in related tasks such as parsing. Since
the parser used assigns each constituent
a head word as an integral part of the
parsing model, we were able to read the
head words of the constituents from the
parser output. For example, in a commu-
nication frame, noun phrases headed by
\Bill", \brother", or \he" are more likely
to be the Speaker, while those headed
by \proposal", \story", or \question" are
more likely to be the Topic.
For our experiments, we divided the
FrameNet corpus as follows: one-tenth of the
annotated sentences for each target word were
reserved as a test set, and another one-tenth
were set aside as a tuning set for developing
our system. A few target words with fewer
than ten examples were removed from the cor-
pus. In our corpus, the average number of
sentences per target word is only 34, and the
number of sentences per frame is 732 | both
relatively small amounts of data on which to
train frame element classiers.
Although we expect our features to inter-
act in various ways, the data are too sparse
to calculate probabilities directly on the full
set of features. For this reason, we built our
classier by combining probabilities from dis-
tributions conditioned on a variety of combi-
nations of features.
An important caveat in using the FrameNet
database is that sentences are not chosen for
annotation at random, and therefore are not
necessarily statistically representative of the
corpus as a whole. Rather, examples are cho-
sen to illustrate typical usage patterns for
each word. We intend to remedy this in fu-
ture versions of this work by bootstrapping
our statistics using unannotated text.
Table 2 shows the probability distributions
used in the nal version of the system. Cov-
erage indicates the percentage of the test data
for which the conditioning event had been
seen in training data. Accuracy is the propor-
tion of covered test data for which the correct
role is predicted, and Performance, simply
the product of coverage and accuracy, is the
overall percentage of test data for which the
correct role is predicted. Accuracy is some-
what similar to the familiar metric of pre-
cision in that it is calculated over cases for
which a decision is made, and performance is
similar to recall in that it is calculated over all
true frame elements. However, unlike a tradi-
tional precision/recall trade-o, these results
have no threshold to adjust, and the task is a
multi-way classication rather than a binary
decision. The distributions calculated were
simply the empirical distributions from the
training data. That is, occurrences of each
role and each set of conditioning events were
counted in a table, and probabilities calcu-
lated by dividing the counts for each role by
the total number of observations for each con-
ditioning event. For example, the distribution
P (rjpt; t) was calculated sas follows:
P (rjpt; t) =
#(r; pt; t)
#(pt; t)
Some sample probabilities calculated from
the training are shown in Table 3.
5 Results
Results for dierent methods of combining
the probability distributions described in the
previous section are shown in Table 4. The
linear interpolation method simply averages
the probabilities given by each of the distri-
butions in Table 2:
P (rjconstituent) = 
1
P (rjt) +

2
P (rjpt; t) + 
3
P (rjpt; gf; t) +

4
P (rjpt; position; voice) +

5
P (rjpt; position; voice; t) + 
6
P (rjh) +

7
P (rjh; t) + 
8
P (rjh; pt; t)
where
P
i

i
= 1. The geometric mean, ex-
pressed in the log domain, is similar:
P (rjconstituent) =
1
Z
expf
1
logP (rjt) +

2
logP (rjpt; t) + 
3
logP (rjpt; gf; t) +

4
logP (rjpt; position; voice) +

5
logP (rjpt; position; voice; t) +

6
logP (rjh) + 
7
logP (rjh; t) +

8
logP (rjh; pt; t)g
where Z is a normalizing constant ensuring
that
P
r
P (rjconstituent) = 1.
The results shown in Table 4 reect equal
values of  for each distribution dened for
the relevant conditioning event (but exclud-
ing distributions for which the conditioning
event was not seen in the training data).
Distribution Coverage Accuracy Performance
P (rjt) 100% 40.9% 40.9%
P (rjpt; t) 92.5 60.1 55.6
P (rjpt; gf; t) 92.0 66.6 61.3
P (rjpt; position; voice) 98.8 57.1 56.4
P (rjpt; position; voice; t) 90.8 70.1 63.7
P (rjh) 80.3 73.6 59.1
P (rjh; t) 56.0 86.6 48.5
P (rjh; pt; t) 50.1 87.4 43.8
Table 2: Distributions Calculated for Semantic Role Identication: r indicates semantic role,
pt phrase type, gf grammatical function, h head word, and t target word, or predicate.
P (rjpt; gf; t) Count in training data
P (r =Agtjpt =NP; gf =Subj; t =abduct) = :46 6
P (r =Thmjpt =NP; gf =Subj; t =abduct) = :54 7
P (r =Thmjpt =NP; gf =Obj; t =abduct) = 1 9
P (r =Agtjpt =PP; t =abduct) = :33 1
P (r =Thmjpt =PP; t =abduct) = :33 1
P (r =CoThmjpt =PP; t =abduct) = :33 1
P (r =Manrjpt =ADVP; t =abduct) = 1 1
Table 3: Sample probabilities for P (rjpt; gf; t) calculated from training data for the verb abduct.
The variable gf is only dened for noun phrases. The roles dened for the removing frame in
the motion domain are: Agent, Theme, CoTheme (\... had been abducted with him") and
Manner.
Other schemes for choosing values of , in-
cluding giving more weight to distributions
for which more training data was available,
were found to have relatively little eect. We
attribute this to the fact that the evaluation
depends only the the ranking of the probabil-
ities rather than their exact values.
P(r | h, t) P(r | pt, t) P(r | pt, position, voice)
P(r | pt, position, voice, t)P(r | pt, gf, t)
P(r | t)P(r | h)
P(r | h, pt, t)
Figure 3: Lattice organization of the distri-
butions from Table 2, with more specic dis-
tributions towards the top.
In the \backo" combination method, a
lattice was constructed over the distributions
in Table 2 from more specic conditioning
events to less specic, as shown in Figure
3. The less specic distributions were used
only when no data was present for any more
specic distribution. As before, probabilities
were combined with both linear interpolation
and a geometric mean.
Combining Method Correct
Linear Interpolation 79.5%
Geometric Mean 79.6
Backo, linear interpolation 80.4
Backo, geometric mean 79.6
Baseline: Most common role 40.9
Table 4: Results on Development Set, 8148
observations
The nal system performed at 80.4% ac-
curacy, which can be compared to the 40.9%
achieved by always choosing the most prob-
able role for each target word, essentially
chance performance on this task. Results for
this system on test data, held out during de-
velopment of the system, are shown in Table
Linear
Backo Baseline
Development Set 80.4% 40.9%
Test Set 76.9 40.6%
Table 5: Results on Test Set, using backo
linear interpolation system. The test set con-
sists of 7900 observations.
5.
5.1 Discussion
It is interesting to note that looking at a con-
stituent's position relative to the target word
along with active/passive information per-
formed as well as reading grammatical func-
tion o the parse tree. A system using gram-
matical function, along with the head word,
phrase type, and target word, but no passive
information, scored 79.2%. A similar system
using position rather than grammatical func-
tion scored 78.8% | nearly identical perfor-
mance. However, using head word, phrase
type, and target word without either position
or grammatical function yielded only 76.3%,
indicating that while the two features accom-
plish a similar goal, it is important to include
some measure of the constituent's syntactic
relationship to the target word. Our nal sys-
tem incorporated both features, giving a fur-
ther, though not signicant, improvement. As
a guideline for interpreting these results, with
8176 observations, the threshold for statisti-
cal signifance with p < :05 is a 1.0% absolute
dierence in performance.
Use of the active/passive feature made a
further improvement: our system using po-
sition but no grammatical function or pas-
sive information scored 78.8%; adding passive
information brought performance to 80.5%.
Roughly 5% of the examples were identied
as passive uses.
Head words proved to be very accurate in-
dicators of a constituent's semantic role when
data was available for a given head word,
conrming the importance of lexicalization
shown in various other tasks. While the dis-
tribution P (rjh; t) can only be evaluated for
56.0% of the data, of those cases it gets 86.7%
correct, without use of any of the syntactic
features.
5.2 Lexical Clustering
In order to address the sparse coverage of lex-
ical head word statistics, an experiment was
carried out using an automatic clustering of
head words of the type described in (Lin,
1998). A soft clustering of nouns was per-
formed by applying the co-occurrence model
of (Hofmann and Puzicha, 1998) to a large
corpus of observed direct object relationships
between verbs and nouns. The clustering was
computed from an automatically parsed ver-
sion of the British National Corpus, using the
parser of (Carroll and Rooth, 1998). The ex-
periment was performed using only frame el-
ements with a noun as head word. This al-
lowed a smoothed estimate of P (rjh; nt; t) to
be computed as
P
c
P (rjc; nt; t)P (cjh), sum-
ming over the automatically derived clusters c
to which a nominal head word h might belong.
This allows the use of head word statistics
even when the headword h has not been seen
in conjunction was the target word t in the
training data. While the unclustered nominal
head word feature is correct for 87.6% of cases
where data for P (rjh; nt; t) is available, such
data was available for only 43.7% of nominal
head words. The clustered head word alone
correctly classied 79.7% of the cases where
the head word was in the vocabulary used
for clustering; 97.9% of instances of nominal
head words were in the vocabulary. Adding
clustering statistics for NP constituents into
the full system increased overall performance
from 80.4% to 81.2%.
5.3 Automatic Identication of
Frame Element Boundaries
The experiments described above have used
human annotated frame element boundaries
| here we address how well the frame ele-
ments can be found automatically. Exper-
iments were conducted using features simi-
lar to those described above to identify con-
stituents in a sentence's parse tree that were
likely to be frame elements. The system
was given the human-annotated target word
and the frame as inputs, whereas a full lan-
guage understanding system would also iden-
tify which frames come into play in a sen-
tence | essentially the task of word sense
disambiguation. The main feature used was
the path from the target word through the
parse tree to the constituent in question, rep-
resented as a string of parse tree nonterminals
linked by symbols indicating upward or down-
ward movement through the tree, as shown in
Figure 4.
S
NP VP
V NP
Det N
Pro
He
ate
some
target
word
frame
element pancakes
Figure 4: In this example, the path from the
frame element \He" to the target word \ate"
can be represented as NP " S # VP # V, with
" indicating upward movement in the parse
tree and # downward movement.
The other features used were the iden-
tity of the target word and the identity of
the constituent's head word. The probabil-
ity distributions calculated from the train-
ing data were P (fejpath), P (fejpath; t), and
P (fejh; t), where fe indicates an event where
the parse constituent in question is a frame el-
ement, path the path through the parse tree
from the target word to the parse constituent,
t the identity of the target word, and h the
head word of the parse constituent. By vary-
ing the probability threshold at which a deci-
sion is made, one can plot a precision/recall
curve as shown in Figure 5. P (fejpath; t)
performs relatively poorly due to fragmenta-
tion of the training data (recall only about 30
sentences are available for each target word).
While the lexical statistic P (fejh; t) alone is
not useful as a classier, using it in linear in-
terpolation with the path statistics improves
results. Note that this method can only iden-
tify frame elements that have a correspond-
ing constituent in the automatically gener-
ated parse tree. For this reason, it is inter-
esting to calculate how many true frame el-
ements overlap with the results of the sys-
tem, relaxing the criterion that the bound-
aries must match exactly. Results for partial
matching are shown in Table 6.
When the automatically identied con-
stituents were fed through the role labeling
system described above, 79.6% of the con-
stituents which had been correctly identied
in the rst stage were assigned the correct role
in the second, roughly equivalent to the per-
formance when assigning roles to constituents
identied by hand.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
re
ca
ll
precision
P(fe|path)
P(fe|path, t)
.75*P(fe | path)+.25*P(fe | h, t)
Figure 5: Precison/Recall plot for various
methods of identifying frame elements. Recall
is calculated over only frame elements with
matching parse constituents.
6 Conclusion
Our preliminary system is able to automati-
cally label semantic roles with fairly high ac-
curacy, indicating promise for applications in
various natural language tasks. Lexical statis-
tics computed on constituent head words were
found to be the most important of the fea-
tures used. While lexical statistics are quite
accurate on the data covered by observations
in the training set, the sparsity of the data
when conditioned on lexical items meant that
combining features was the key to high over-
all performance. While the combined sys-
tem was far more accurate than any feature
Type of Overlap Identied Constituents Number
Exactly Matching Boundaries 66% 5421
Identied constituent entirely within true frame element 8 663
True frame element entirely within identied constituent 7 599
Partial overlap 0 26
No match to true frame element 13 972
Table 6: Results on Identifying Frame Elements (FEs), including partial matches. Results
obtained using P (fejpath) with threshold at .5. A total of 7681 constituents were identied as
FEs, 8167 FEs were present in hand annotations, of which matching parse constituents were
present for 7053 (86%).
taken alone, the specic method of combina-
tion used was less important.
We plan to continue this work by integrat-
ing semantic role identication with parsing,
by bootstrapping the system on larger, and
more representative, amounts of data, and by
attempting to generalize from the set of pred-
icates chosen by FrameNet for annotation to
general text.
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The berkeley framenet project.
In Proceedings of the COLING-ACL, Montreal,
Canada.
Dan Blaheta and Eugene Charniak. 2000. As-
signing function tags to parsed text. In Pro-
ceedings of the 1st Annual Meeting of the North
American Chapter of the ACL (NAACL), Seat-
tle, Washington.
Glenn Carroll and Mats Rooth. 1998. Va-
lence induction with a head-lexicalized pcfg. In
Proceedings of the 3rd Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP 3), Granada, Spain.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the
ACL.
Charles J. Fillmore and Collin F. Baker. 2000.
Framenet: Frame semantics meets the corpus.
In Linguistic Society of America, January.
Charles Fillmore. 1968. The case for case. In
Bach and Harms, editors, Universals in Lin-
guistic Theory, pages 1{88. Holt, Rinehart, and
Winston, New York.
Charles J. Fillmore. 1976. Frame semantics
and the nature of language. In Annals of the
New York Academy of Sciences: Conference on
the Origin and Development of Language and
Speech, volume 280, pages 20{32.
Marti Hearst. 1999. Untangling text data mining.
In Proceedings of the 37rd Annual Meeting of
the ACL.
Thomas Hofmann and Jan Puzicha. 1998. Sta-
tistical models for co-occurrence data. Memo,
Massachussetts Institute of Technology Arti-
cial Intelligence Laboratory, February.
Ray Jackendo. 1972. Semantic Interpretation in
Generative Grammar. MIT Press, Cambridge,
Massachusetts.
Maria Lapata and Chris Brew. 1999. Using
subcategorization to resolve verb class ambigu-
ity. In Joint SIGDAT Conference on Empiri-
cal Methods in NLP and Very Large Corpora,
Maryland.
Dekang Lin. 1998. Automatic retrieval and clus-
tering of similar words. In Proceedings of the
COLING-ACL, Montreal, Canada.
Scott Miller, David Stallard, Robert Bobrow, and
Richard Schwartz. 1996. A fully statistical
approach to natural language interfaces. In
Proceedings of the 34th Annual Meeting of the
ACL.
Carl Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. University
of Chicago Press, Chicago.
Ellen Rilo and Mark Schmelzenbach. 1998. An
empirical approach to conceptual case frame ac-
quisition. In Proceedings of the Sixth Workshop
on Very Large Corpora.
Ellen Rilo. 1993. Automatically constructing
a dictionary for information extraction tasks.
In Proceedings of the Eleventh National Con-
ference on Articial Intelligence (AAAI).
In: Proceedings of CoNLL-2000 and LLL-2000, pages 67-72, Lisbon, Portugal, 2000. 
Knowledge-Free Induction of Morphology 
Using Latent Semantic Analysis 
Pat r i ck  Schone and Dan ie l  Ju ra fsky  
University of Colorado 
Boulder, Colorado 80309 
{schone, jurafsky}@cs.colorado.edu 
Abst ract  
Morphology induction is a subproblem of 
important tasks like automatic learning of 
machine-readable dictionaries and grammar in- 
duction. Previous morphology induction ap- 
proaches have relied solely on statistics of hy- 
pothesized stems and affixes to choose which 
affixes to consider legitimate. Relying on stem- 
and-affix statistics rather than semantic knowl- 
edge leads to a number of problems, such as the 
inappropriate use of valid affixes ("ally" stem- 
ming to "all"). We introduce a semantic-based 
algorithm for learning morphology which only 
proposes affixes when the stem and stem-plus- 
affix are sufficiently similar semantically. We 
implement our approach using Latent Seman- 
tic Analysis and show that our semantics-only 
approach provides morphology induction results 
that rival a current state-of-the-art system. 
1 In t roduct ion  
Computational morphological analyzers have 
existed in various languages for years and it has 
been said that "the quest for an efficient method 
for the analysis and generation of word-forms is 
no longer an academic research topic" (Karlsson 
and Karttunen, 1997). However, development 
of these analyzers typically begins with human 
intervention requiring time spans from days to 
weeks. If it were possible to build such ana- 
lyzers automatically without human knowledge, 
significant development time could be saved. 
On a larger scale, consider the task 
of inducing machine-readable dictionaries 
(MRDs) using no human-provided information 
("knowledge-free"). In building an MRD, 
"simply expanding the dictionary to encompass 
every word one is ever likely to encounter...fails 
to take advantage of regularities" (Sproat, 
1992, p. xiii). Hence, automatic morphological 
analysis is also critical for selecting appropriate 
and non-redundant MRD headwords. 
For the reasons expressed above, we are in- 
terested in knowledge-free morphology induc- 
tion. Thus, in this paper, we show how to au- 
tomatically induce morphological relationships 
between words. 
Previous morphology induction approaches 
(Goldsmith, 1997, 2000; D4Jean, 1998; Gauss- 
ier, 1999) have focused on inflectional languages 
and have used statistics of hypothesized stems 
and affixes to choose which affixes to consider 
legitimate. Several problems can arise using 
only stem-and-affix statistics: (1) valid affixes 
may be applied inappropriately ("ally" stem- 
ming to "all"), (2) morphological ambiguity 
may arise ("rating" conflating with "rat" in- 
stead of "rate"), and (3) non-productive affixes 
may get accidentally pruned (the relationship 
between "dirty" and "dirt" may be lost)3 
Some of these problems could be resolved 
if one could incorporate word semantics. For 
instance, "all" is not semantically similar to 
"ally," so with knowledge of semantics, an algo- 
rithm could avoid conflating these two words. 
To maintain the "knowledge-free" paradigm, 
such semantics would need to be automati- 
cally induced. Latent Semantic Analysis (LSA) 
(Deerwester, et al, 1990); Landauer, et al, 
1998) is a technique which automatically iden- 
tifies semantic information from a corpus. We 
here show that incorporating LSA-based seman- 
tics alone into the morphology-induction pro- 
cess can provide results that rival a state-oh 
the-art system based on stem-and-affix statis- 
tics (Goldsmith's Linguistica). 
1Error examples are from Goldsmith's Linguistica 
67 
Our algorithm automatically extracts poten- 
tial affixes from an untagged corpus, identifies 
word pairs sharing the same proposed stem but 
having different affixes, and uses LSA to judge 
semantic relatedness between word pairs. This 
process erves to identify valid morphological re- 
lations. Though our algorithm could be applied 
to any inflectional language, we here restrict 
it to English in order to perform evaluations 
against the human-labeled CELEX database 
(Baayen, et al, 1993). 
2 P rev ious  work  
Existing induction algorithms all focus on iden- 
tifying prefixes, suffixes, and word stems in in- 
flectional languages (avoiding infixes and other 
language types like concatenative or aggluti- 
native languages (Sproat, 1992)). They also 
observe high frequency occurrences of some 
word endings or beginnings, perform statistics 
thereon, and propose that some of these ap- 
pendages are valid morphemes. 
However, these algorithms differ in specifics. 
D~Jean (1998) uses an approach derived from 
Harris (1951) where word-splitting occurs if the 
number of distinct letters that follows a given 
sequence of characters urpasses a threshoid. 
He uses these hypothesized affixes to resegment 
words and thereby identify additional affixes 
that were initially overlooked. His overall goal is 
different from ours: he primarily seeks an affix 
inventory. 
Goldsmith (1997) tries cutting each word 
in exactly one place based on probability and 
lengths of hypothesized stems and affixes. He 
applies the EM algorithm to eliminate inappro- 
priate parses. He collects the possible suffixes 
for each stem calling these a signature which 
aid in determining word classes. Goldsmith 
(2000) later incorporates minimum description 
length to identify stemming characteristics that 
most compress the data, but his algorithm oth- 
erwise remains similar in nature. Goldsmith's 
algorithm is practically knowledge-free, though 
he incorporates capitalization removal and some 
word segmentation. 
Gaussier (1999) begins with an inflectional 
lexicon and seeks to find derivational morphol- 
ogy. The words and parts of speech from his 
inflectional lexicon serve for building relational 
families of words and identifying sets of word 
pairs and suffixes therefrom. Gaussier splits 
words based on p-similarity - words that agree 
in exactly the first p characters. He also builds 
a probabilistic model which indicates that the 
probability of two words being morphological 
variants is based upon the probability of their 
respective changes in orthography and morpho- 
syntactics. 
3 Cur rent  approach  
Our algorithm also focuses on inflectional lan- 
guages. However, with the exception of word 
segmentation, we provide it no human informa- 
tion and we consider only the impact of seman- 
tics. Our approach (see Figure 1) can be de- 
composed into four components: (1) initially 
selecting candidate affixes, (2) identifying af- 
fixes which are potential morphological vari- 
ants of each other, (3) computing semantic vec- 
tors for words possessing these candidate affixes, 
and (4) selecting as valid morphological variants 
those words with similar semantic vectors. 
Figure 1: Processing Architecture 
Stage 1 Stage 2 Stage 3 Stage 4 
Identify I\[ paa~~ l~ I\[ semantic II variants 
potential \[lare pos lmell vectors II that have 
affixes I I morplm- I I for I I slmuar 
........ ) ( logical \]( words \] ( semantic 
3.1 Hypothes iz ing  affixes 
To select candidate affixes, we, like Gaussier, 
identify p-similar words. We insert words into a 
trie (Figure 2) and extract potential affixes by 
observing those places in the trie where branch- 
ing occurs. Figure 2's hypothesized suffixes are 
NULL, "s," "ed," "es," "ing," "e," and "eful." 
We retain only the K most-frequent candidate 
affixes for subsequent processing. The value for 
K needs to be large enough to account for the 
number of expected regular affixes in any given 
language as well as some of the more frequent 
irregular affixes. We arbitrarily chose K to be 
200 in our system. (It should also be mentioned 
that we can identify potential prefixes by insert- 
ing words into the trie in reversed order. This 
prefix mode can additionally serve for identify- 
ing capitalization.) 
68 
F igure  2: Trie structure 
( 
0 0 
() 
( ) 
3.2 Morpho log ica l  variants 
We next identify pairs of candidate affixes that 
descend from a common ancestor node in the 
trie. For example, ("s", NULL) constitutes such 
a pair from Figure 2. We call these pairs rules. 
Two words sharing the same root and the 
same affix rule, such as "cars" and "car," form 
what we call a pair of potential morphological 
variants (PPMVs). We define the ruleset of a 
given rule to be the set of all PPMVs that have 
that rule in common. For instance, from Figure 
2, the ruleset for ("s", NULL) would be the pairs 
"cars/car" and "cares/care." Our algorithm es- 
tablishes a list which identifies the rulesets for 
every hypothesized rule extracted from the data 
and then it must proceed to determine which 
rulesets or PPMVs describe true morphological 
relationships. 
3.3 Computing Semantic Vectors 
Deerwester, et al (1990) showed that it is 
possible to find significant semantic relation- 
ships between words and documents in a corpus 
with virtually no human intervention (with the 
possible exception of a human-built stop word 
list). This is typically done by applying singu- 
lar value decomposition (SVD) to a matrix, M, 
where each entry M(i,j) contains the frequency 
of word i as seen in document j of the corpus. 
This methodology is referred to as Latent Se- 
mantic Analysis (LSA) and is well-described in
the literature (Landauer, et al, 1998; Manning 
and Schfitze, 1999). 
SVDs seek to decompose a matrix A into the 
product of three matrices U, D, and V T where 
U and V T are  orthogonal matrices and D is 
a diagonal matrix containing the singular val- 
ues (squared eigenvalues) of A. Since SVD's 
can be performed which identify singular val- 
ues by descending order of size (Berry, et al, 
1993), LSA truncates after finding the k largest 
singular values. This corresponds to projecting 
the vector representation of each word into a 
k-dimensional subspace whose axes form k (la- 
tent) semantic directions. These projections are 
precisely the rows of the matrix product UkDk. 
A typical k is 300, which is the value we used. 
However, we have altered the algorithm some- 
what to fit our needs. First, to stay as close to 
the knowledge-free scenario as possible, we nei- 
ther apply a stopword list nor remove capitaliza- 
tion. Secondly, since SVDs are more designed 
to work on normally-distributed data (Manning 
and Schiitze, 1999, p. 565), we operate on Z- 
scores rather than counts. Lastly, instead of 
generating a term-document matrix, we build a 
term-term atrix. 
Schiitze (1993) achieved excellent perfor- 
mance at classifying words into quasi-part- 
of-speech classes by building and perform- 
ing an SVD on an Nx4N term-term matrix, 
M(i,Np+j). The indices i and j represent the 
top N highest frequency words. The p values 
range from 0 to 3 representing whether the word 
indexed by j is positionally offset from the word 
indexed by i by -2, -1, +1, or +2, respectively. 
For example, if "the" and "people" were re- 
spectively the 1st and 100th highest frequency 
words, then upon seeing the phrase "the peo- 
ple," Schfitze's approach would increment the 
counts of M(1,2N+100) and M(100,N+i).  
We used Schfitze's general framework but tai- 
lored it to identify local semantic information. 
We built an Nx2N matrix and our p values cor- 
respond to those words whose offsets from word 
i are in the intervals \[-50,-1\] and \[1,501, respec- 
tively. We also reserve the Nth position as a 
catch-all position to account for all words that 
are not in the top (N-l). An important issue to 
resolve is how large should N be. We would like 
69 
to be able to incorporate semantics for an arbi- 
trarily large number of words and LSA quickly 
becomes impractical on large sets. Fortunately, 
it is possible to build a matrix with a smaller 
value of N (say, 2500), perform an SVD thereon, 
and then fold in remaining terms (Manning and 
Schfitze, 1999, p. 563). Since the U and V ma- 
trices of an SVD are orthogonal matrices, then 
uuT:vvT : I .  This implies that AV=UD.  
This means that for a new word, w, one can 
build a vector ~T which identifies how w relates 
to the top N words according to the p different 
conditions described above. For example, if w 
were one of the top N words, then ~w T would 
simply represent w's particular ow from the A 
matrix. The product f~w = ~wTVk is the projec- 
tion of ~T into the k-dimensional latent seman- 
tic space. By storing an index to the words of 
the corpus as well as a sorted list of these words, 
one can efficiently build a set of semantic vec- 
tors which includes each word of interest. 
3.4 Stat i s t i ca l  Computat ions  
Morphologically-related words frequently share 
similar semantics, so we want to see how well se- 
mantic vectors of PPMVs correlate. If we know 
how PPMVs correlate in comparison to other 
word pairs from their same rulesets, we can ac- 
tually determine the semantic-based probability 
that the variants are legitimate. In this section, 
we identify a measure for correlating PPMVs 
and illustrate how ruleset-based statistics help 
identify legitimate PPMVs. 
3.4.1 Semant ic  Cor re la t ion  of  Words  
The cosine of the angle between two vectors v l  
and v2 is given by, 
cos(v l ,v2) -  v l -v2  
II v l  llll v2 H" 
We want to determine the correlation between 
each of the words of every PPMV. We use what 
we call a normalized cosine score (NCS) as a cor- 
relation. To obtain a NCS, we first calculate the 
cosine between each semantic vector, nw, and 
the semantic vectors from 200 randomly chosen 
words. By this means we obtain w's correlation 
mean (#w) and standard deviation (aw). If v 
is one of w's variants, then we define the NCS 
between ~w and nv  to be 
cos(nw, nv)  - #y ). min ( 
ye{w,v} ay 
Table 1 provides normalized cosine scores for 
several PPMVs from Figure 2 and from among 
words listed originally as errors in other sys- 
tems. (NCSs are effectively Z-scores.) 
Table  1: Normalized Cosines for various PPMVs 
PPMVs I NCSs PPMVs NCSs I 
car/cars 5.6 ally/allies 6.5 
car/caring -0.71 ally/all -1.3 
car/cares -0.14 dirty/dirt  2.4 
car/cared i -0.96 rat ing/rate 0.97 
3.4.2 Ru leset - leve l  S ta t i s t i cs  
By considering NCSs for all word pairs cou- 
pled under a particular rule, we can deter- 
mine semantic-based probabilities that indicate 
which PPMVs are legitimate. We expect ran- 
dom NCSs to be normally-distributed accord- 
ing to Af(0,1). Given that a particular uleset 
contains nR PPMVs, we can therefore approx- 
imate the number (nT), mean (#T) and stan- 
dard deviation (aT) of true correlations. If we 
_C .~___~ 2 .  define ~z(#,a)  to be fee  " - J ax, then we 
can compute the probability that the particular 
correlation is legitimate: 
Pr( true) = nT ~ Z(~T ,aT) 
(nR--nT ~z(O, 1) +nT~Z(~T, aT)" 
3.4.3 Subru les  
It is possible that a rule can be hypothesized 
at the trie stage that is true under only certain 
conditions. A prime example of such a rule is 
("es", NULL). Observe from Table 1 that the 
word "cares" poorly correlates with "car." Yet, 
it is true that "-es" is a valid suffix for the words 
"flashes," "catches," "kisses," and many other 
words where the "-es" is preceded by a voiceless 
sibilant. 
Hence, there is merit to considering subrules 
that arise while performing analysis on a par- 
ticular rule. For instance, while evaluating the 
("es", NULL) rule, it is desirable to also con- 
sider potential subrules such as ("ches", "ch") 
and ("tes", "t"). One might expect hat the av- 
erage NCS for the ("ches", "ch") subrule might 
be higher than the overall rule ("es", NULL) 
whereas the opposite will likely be true for 
("tes', "t"). Table 2 confirms this. 
70 
Table 2: Analysis of subrules 
Rule/Subrule I Average StDev t#instances 
("es", NULL) 1.62 
("ches", "ch" ) 2.20 
("shes", "sh") 2.39 
("res", "r") -0.69 
("tes","t") -0.58 
2.43 173 
1.66 32 
1.52 15 
0.47 6 
0.93 11 
4 Resu l t s  
We compare our algorithm to Goldsmith's Lin- 
guistica (2000) by using CELEX's (Baayen, 
et al, 1993) suffixes as a gold standard. 
CELEX is a hand-tagged, morphologically- 
analyzed atabase of English words. CELEX 
has limited coverage of the words from our data 
set (where our data consists of over eight mil- 
lion words from random subcollections of TREC 
data (Voorhees, et a1,1997/8)), so we only con- 
sidered words with frequencies of 10 or more. 
F igure 3: Morphological directed graphs 
(b) (f) 
concerned concerted 
(a) / (c) (g) k 
concerns concerts , ~eoncer~ conceri~ (e) . 
\ conc(edr)ning con~eh!ting / 
(i) (j) 
concerto ~-- concertos 
Morphological relationships can be represented 
graphically as directed graphs (see Figure 3, 
where three separate graphs are depicted). De- 
veloping a scoring algorithm to compare di- 
rected graphs is likely to be prone to disagree- 
ments. Therefore, we score only the vertex sets 
of directed graphs. We will refer to these ver- 
tex sets as conflation sets. For example, con- 
cern's conflation set contains itself as well as 
"concerned," "concerns," and "concerning" (or, 
in shorthand notation, the set is {a,b,c,d}). 
To evaluate an algorithm, we sum the num- 
ber of correct (C), inserted (Z), and deleted (D) 
words it predicts for each hypothesized confla- 
tion set. If Xw represents word w's conflation 
set according to the algorithm, and if Yw repre- 
sents its CELEX-based conflation set, then 
C = Ew( Ixw AYwl/lYwl), 
= Evw(lYw - (Xw NYw)I/IYwl), and 
z = Ew( lx~ - (xw AY~)I/IYwl). 
However, in making these computations, we dis- 
regard any CELEX words that are not in the 
algorithm's data set and vice versa. 
For example, suppose two algorithms were be- 
ing compared on a data set where all the words 
from Figure 3 were available except "concert- 
ing" and "concertos." Suppose further that one 
algorithm proposed that {a,b,c,d,e,f,g,i} formed 
a single conflation set whereas the other algo- 
rithm proposed the three sets {a,b,c,d},{e,g,i}, 
and {f}. Then Table 3 illustrates how the two 
algorithms would be scored. 
Table 3: Example of scoring 
I II a I b I c I d I e I f I g I i IITotalt 
C1 4/4 4/4 4/4 4/4 3/3 3/3 3/3 1/1 8 
D1 0/4 0/4 0/4 0/4 0/3 0/3 0/3 0/1 0 
Zl 4/4 4/4 4/4 4/4 5/3 5/3 5/3 7/1 16 
C2 4/4 4/4 4/4 4/4 2/3 2/3 1/3 1/1 20/3 
D2 0/4 0/4 0/4 0/4 1/3 1/3 2/3 0/1 4/3 
Z2 0/4i0/4 0/4 0/4 1/3 1/3 0/3 2/1 8/3 
To explain Table 3, consider algorithm one's 
entries for 'a.' Algorithm one had pro- 
posed that Xa={a,b,c,d,e,f,g,i} when in reality, 
Ya={a,b,c,d}. Since IXa NYal = 4 and IYal=4, 
then CA=4/4. The remaining values of the table 
can be computed accordingly. 
Using the values from Table 3, we can 
also compute precision, recall, and F-Score. 
Precision is defined to be C/(C+Z), recall is 
C/(C+D), and F-Score is the product of pre- 
cision and recall divided by the average of the 
two. For the first algorithm, the precision, re- 
call, and F-Score would have respectively been 
1/3, 1, and 1/2. In the second algorithm, these 
numbers would have been 5/7, 5/6, and 10/13. 
Table 4 uses the above scoring mechanism to 
compare between Linguistica nd our system (at 
various probability thresholds). Note that since 
Linguistica removes capitalization, it will have 
a different otal word count than our system. 
71 
Table 4: Performance on English CELEX 
Algorithm Linguistica 
LSA- LSA- LSA- 
based based based 
pr_> 0.5 pr_> 0.7 pr> 0.85 
#Correct 10515 10529 10203 9863 
#Inserts 2157 1852 1138 783 
#Deletes 2571 2341 i 2667 3007 
Precision 83.0% 85.0% 90.0% 92.6% 
Recall 80.4% 81.8% 79.3% 76.6% 
F-Score 81.6% 83.4% 84.3% 83.9% 
5 Conc lus ions  
These results suggest hat semantics and LSA 
can play a key part in knowledge-free mor- 
phology induction. Semantics alone worked at 
least as well as Goldsmith's frequency-based ap- 
proach. Yet we believe that semantics-based 
and frequency-based approaches play comple- 
mentary roles. In current work, we are examin- 
ing how to combine these two approaches. 
Re ferences  
Albright, A. and B. P. Hayes. 1999. An au- 
tomated learner for phonology and mor- 
phology. Dept. of Linguistics, UCLA. At 
http: //www.humnet.ucla.edu/humnet/linguis- 
tics/people/hayes/learning/learner.pdf. 
Baayen, R. H., R. Piepenbrock, and H. van Rijn. 
1993. The CELEX lexical database (CD-ROM), 
Linguistic Data Consortium, University of Penn- 
sylvania, Philadelphia, PA. 
Berry, M., T. Do, G. O'Brien, V. Krishna, and 
S. Varadhan. 1993. SVDPACKC user's guide. CS- 
93-194, University of Tennessee. 
D~jean, H. 1998. Morphemes as necessary con- 
cepts for structures: Discovery from untagged 
corpora. University of Caen-Basse Normandie. 
http://www.info.unicaen.fr/~ DeJean/travail/ar - 
ticles/pgl 1.htm. 
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. 
Landauer, and R. Harshman. 1990. Indexing by 
Latent Semantic Analysis. Journal of the Ameri- 
can Society for Information Science. 
Gaussier, l~. 1999. Unsupervised learning of deriva- 
tional morphology from inflectional lexicons. A CL 
'99 Workshop Proceedings: Unsupervised Learn- 
ing in Natural Language Processing, University of 
Maryland. 
Goldsmith, J. 1997. Unsupervised learning of the 
morphology of a natural anguage. University of 
Chicago. 
Goldsmith, J. 2000. Unsupervised learning of 
the morphology of a natural language. Uni- 
versity of Chicago. http://humanities.uchi- 
cago.edu/faculty/goldsmith. 
Harris, Z. 1951. Structural Linguistics. University of 
Chicago Press. 
Hull, D. A. and G. Grefenstette. 1996. A de- 
tailed analysis of English stemming algorithms. 
XEROX Technical Report, http://www.xrce.xe- 
rox.com/publis/mltt/mltt-023.ps. 
Krovetz, R. 1993. Viewing morphology as an infer- 
ence process. Proceedings of the 16thA CM/SIGIR 
Conference, pp. 191-202. 
Jurafsky, D. S. and J. H. Martin. 2000. Speech and 
Language Processing. Prentice Hall, Inc., Engle- 
wood, N.J. 
Karlsson, F. and L. Karttunen,. 1997. "Sub- 
sentencial Processing." In Survey of the State of 
the Art in Human Language Technology, R. Cole, 
Ed., Giardini Editori e Stampatori, Italy. 
Koskenniemi, K. 1983. Two-level Morphology: a 
General Computational Model for Word-Form 
Recognition and Production. Ph.D. thesis, Univer- 
sity of Helsinki. 
Landauer,T. K., P. W. Foltz, and D. Laham. 1998. 
Introduction to Latent Semantic Analysis. Dis- 
course Processes. Vol. 25, pp. 259-284. 
Lovins, J. 1968. Development of a stemming al- 
gorithm. Mechanical Translation and Computa- 
tional Linguistics, Vol. 11, pp.22-31 
Manning, C. D. and H. Schfitze. 1999. Foundations 
of Statistical Natural Language Processing, MIT 
Press, Cambridge, MA. 
Porter, M. 1980. An algorithm for suffix stripping. 
Program, Vol. 14(3), pp.130-137. 
Ritchie, G. and G. J. Russell. 1992. Computational 
morphology: Practical Mechanisms for the En- 
glish Lexicon. MIT. 
Schfitze, H. 1993. Distributed syntactic representa- 
tions with an application to part-of-speech tag- 
ging. Proceedings of the IEEE International Con- 
ference on Neural Networks, pp. 1504-1509. 
Scott, D. 1992. Multivariate Density Estimation: 
Theory, Practice, and Visualization. John Wiley 
& Sons, New York. 
Sproat, R. 1992. Morphology and Computation. MIT 
Press, Cambridge, MA. 
Van den Bosch, A. and W. Daelemans. 1999. 
Memory-based morphological nalysis. Proc. of 
the 37th Annual Meeting of the ACL, University 
of Maryland, pp. 285-292. 
Voorhees, E., D. Hoffman, and C. Barnes. 1996-7. 
TREC Information Retrieval: Text Research Col- 
lection, Vols. 4-5 (CD-ROM), National Institute 
of Standards and Technology. 
Woods, W. 2000. Aggressive morphology for robust 
lexical coverage. Proceedings of the 6th ANLP/lst 
NAACL, Seattle, WA. 
72 
Verb Subcategorization Frequency Differences between Business- 
News and Balanced Corpora: The Role of Verb Sense 
IDouglas Roland, ~"Danid Jurafsky, "3Lise Menn,'Susanne Gahl, IElizabeth Elder and IChris 
Riddoch 
~Department of Linguistics, 2Department of
Computer Science, 3Institute of Cognitive 
Science 
University of Colorado 
Boulder, CO 80309-0295 
{ douglas.roland, jurafslQ1, lise.menn, 
elizabeth.elder, 
christopher.b.riddoch } @colorado.edu 
'Department ofLinguistics 
Harvard University 
Cambridge MA 02138 
sgahl @ fas.harvard.edu 
Abstract 
We explore the differences in verb 
subeategorization frequencies across several 
corpora in an effort to obtain stable cross 
corpus subcategonzation probabilities for 
use in norming psychological experiments. 
For the 64 single sense verbs we looked at, 
subeategorizatlon preferences were 
remarkably stable between British and 
American corpora, and between balanced 
corpora and financial news corpora. Of the 
verbs that did show differences, these 
differences were generally found between 
the balanced corpora nd the financial news 
data. We show that all or nearly all of 
these shifts in subcategorization arerealised 
via (often subtle) word sense differences. 
This is an interesting observation i  itself, 
and also suggests that stable cross corpus 
subcategorization frequencies may be found 
when verb sense is adequately controlled. 
Introduction 
Verb subcategorizafion probabilities play an 
important role in both computational linguistic 
applications (e.g. Carroll, Minnen, and Briscoe 
1998, Charniak 1997, Collins 1996/1997, Joshi 
and Srinivas 1994, Kim, Srinivas, and Tmeswell 
1997, Stolcke et al 1997) and psycholinguisfic 
models of language processing (e.g. Boland 
1997, Clifton et al 1984, Ferreira & McClure 
1997, Fodor 1978, Garnsey et al 1997, Jurafsky 
1996, MacDonald 1994, Mitchell & Holmes 
1985, Tanenhaus et al 1990, Trueswell et al 
1993). 
Previous research, however, has shown that 
subcategorization probabilities vary widely in 
different corpora. Studies such as Merlo 
(1994), Gibson et al (1996), and Roland & 
Jurafsky (1997) have found subcategorization 
frequency differences between traditional corpus 
data and data from psychological experiments. 
Biber (1993) and Biber et al (1998) have shown 
that that word frequency, word sense (as defined 
by collocates), the distribution of synonymous 
words and the use of syntactic structures varies 
with corpus genre. Roland & Jurafsky (1998, 
2000 in press) showed that there were 
subcategorization frequency differences between 
various written and spoken corpora, and 
furthermore showed that that these 
subcategorization frequency differences are 
caused by variation in word sense as well as 
genre and discourse type differences among the 
corpora. 
While the subcategorization probabilities in a 
computational l nguage model can be adjusted 
to match a particular corpus, cross corpus 
differences in such probabilities pose an 
important problem when using corpora for 
norming psychological experiments. If each 
corpus generates a separate set of probabilities, 
which probabilities are the correct ones to use as 
a model of human language processing? 
In an attempt to use corpora to provide norming 
data for 64 verbs for experimental purposes, we 
investigate in detail how verb frequencies and 
verb subcategorization frequencies differ among 
three corpora: the British National Corpus 
28 
(BNC), the Wall Street Journal corpus (WSJ), 
and the Brown Corpus (Brown). For the 64 
verbs, we randomly selected a set of sentences 
from each corpus and hand-coded them for 
transitivity, passive versus active voice, and 
whether the selected usage was an instance of 
the most common sense of the verb. 
We then ask two questions: Do these verbs 
have the same subcategorizafion probabilities 
across corpora, and, when there are differences, 
what is the cause. If a set of factors causing the 
differences can be identified and controlled for, 
then a stable set of cross-corpus probabilities 
suitable for norming psychological experiments 
can be generated. 
While previous work has shown that differences 
between corpora do exist, and that word sense 
differences play a large role in realising these 
differences, much less is known about he effect 
of other factors on subcategorizafion variation 
across corpora. For example, are there gross 
subcategorization differences between British 
and American English? To what extent does the 
business-genre nature of the Wall Street Journal 
corpus affect subcategorization probabilities? 
Finally, while Roland and Jurafsky (2000 in 
press) suggested that sense differences played a 
major ole in subcategorization biases, they were 
only able to test their hypothesis on a small 
number of verbs. 
Our eventual goal is an understanding of many 
levels of verb differences across corpora, 
including verb frequency, frequency of transitive 
versus intransitive uses, frequency of other 
subcategonzafion frames, and frequency of 
active versus passive use. This paper reports 
our preliminary results on the first two of these 
issues. Verb usage was surprisingly unaffected 
by differences between British and American 
English. Those differences that did occur seem 
mostly to be caused by differences in the 
distribution of verb senses across corpora. The 
business-genre nature of the Wall Street Journal 
corpus caused certain verbs to appear more often 
in particular senses that had a strong effect on its 
subcategorization frequencies. Even after 
controlfing for the broad sense of the verb, we 
found subcategorization differences caused by 
the "micro-differences" in sense, including quite 
specific arguments othe verb. 
1 Data 
Data for 64 verbs (shown in Table 1) was 
collected from three corpora; The British 
National Corpus (BNC) 
(http'J/info.ox.ac.uk/bnc/index.html), he Penn 
Treehank parsed version of the Brown Corpus 
(Brown), and the Penn Treebank Wall Street 
Journal corpas (WSJ) (Marcus et al 1993). 
The 64 verbs were chosen on the basis of the 
requirements of separate psychological 
experiments including having a single dominant 
sense, being easily imagable" and participating 
in one of several subcategorization alternations. 
A random sample of 100 examples of each verb 
was selected from each of the three corpora. 
When the corpus contained less than 100 tokens 
of the verb, as was frequently the case in the 
Brown and WSJ corpora, the entire available 
data was used. This data was coded for several 
properties: Transitive/Intransitive" 
Active/Passive" and whether the example 
involved the major sense of the verb or not. 
The BNC data was ceded entirely by hand, 
while the Brown and WSJ was hand coded after 
a first pass of subcategorization labelling via a 
tgrep search siring algorithm. The same coder 
labelled the data for all three corpora for any 
given verb, in order to reduce any problems in 
intercoder rehability. 
adjust, advance, appoint, arrest, break, burst, 
carve, crack, crumble, dance, design, 
dissolve, distract, disturb, drop, elect, 
encourage, entertain, excite, fight., float, 
flood, fly, frighten, glide, grow, hang, harden, 
heat, hurry, impress, jump, kick, knit, lean, 
leap, lecture,, locate, march, melt, merge, 
mutate, offend, play, pour, race, relax, rise, 
rotate, rush, sail shut, soften, spill, stand, 
study, surrender, tempt, terrify, type, walk, 
wandex, wash, watch 
Table 1- 64 verbs chosen for analysis 
2 Verb Frequency 
Because word frequency is known to vary with 
corpus genre, we used the frequency differences 
for our target verbs as a measure of corpus 
difference. We would expect factors such as 
corpus genre (Business for WSJ vs. mixed for 
BNC and Brown), American vs. British English, 
and the era the corpus sample was taken in to 
influence word frequency. 
We calculated the frequencies tbr each verb, and 
used Chi Square to test whether the difference in 
frequency was significant for each corpus 
pairing. We then counted the number of verbs 
that showed a significant difference using p = 
0.05 as a cut-off poim: This result is shown in 
Table 2. Although there were verbs that had a 
significant difference in distribution between the 
two mixed genre corpora (BNC, Brown), there 
were more differences in word frequency 
between the general corpora and the business 
corpus. The difference between the 
BNC/Brown comparison and the BNC and 
Brown vs. WSJ comparison is significant (Chi 
Square, p < .01). 
BNC vs Brown BNC vs WSJ  Brown vs WSJ  
30/64 46/64 46/64 
Table 2 - Number of verbs showing a significant 
difference in frequency between corpora. 
Table 3 shows the list of words that were 
significantly more frequent in both of the 
general corpora than they were in the business 
oriented corpus. Notice that most of the verbs 
describe leisure activities. 
amuse, boil, burst, dance, dL~turb, entertain, 
frighten, bang, harden, hurry', impress, knit, 
lean, paint, play, race, sail, stand, tempt, 
walk, wander, wash, watch 
Table 3 - Verbs which BNC and Brown both 
have more of than WSJ: 
Alternatively, when one looks at the words that 
had a significantly higher frequency in the WSJ 
corpus than in either of the other corpora (Table 
4), one finds predominately verbs that can 
describe stock price changes and business 
transactions. 
adjust, advance, crumble, drop, elect, fall, 
grow, jump, merge, quote, rise, shrink, shut, 
slip,,, 
Table 4 - Verbs which WSJ has more of than 
both Brown and WSJ: 
We are currently examining the nature of the 
differences between the British and American 
corpora. 
3 Subcategorization Frequency 
3.1 Methodology: 
For the second experiment, we coded the 
examples of the 64 verbs from each of the three 
corpora for transitivity. We counted any use 
with a direct object as transitive, and any other 
use, such as with a prepositional phrase, as 
intransitive. Passive uses were also included in 
the transitive category. Examples ( 1 ) and ( 2 ) 
illustrate intransitive uses, example ( 3 ) 
illustrates transitive (and active) while examples 
( 4 ) and ( 5 ) illustrate transitive (and passive) 
uses of the verb 'race'. 
( 1 ) Pretax profits 6r_o.p.Imd by 37 million. 
( 2 ) Something dropped to the floor. 
( 3 ) Lift them from the elbows, and then 
drop them down to the floor. 
( 4 ) Plans for an OSF binary interface have 
been dropped. 
( 5 ) It was ... the tinsel paper dropped by 
bombers. 
Roland and Jurafsky (2000 in press) showed that 
verb sense can affect verb subcategofization. 
We therefore controlled for verb sense by only 
including sentences from the majority sense of 
the verb in our counts. For example, we did 
not include instances of drop which were phrasal 
verbs with distinct senses like "drop in" or "drop 
off". We did however, include metaphorical 
extensions of the main sense, such as a company 
"dropping a product line". We thus used a 
broadly defined notion of sense rather than the 
more narrowly defined word senses used in 
some on-line word sense resources such as 
Wordnet. This was partly for logistic reasons, 
since such fine-grained senses are very hard to 
code, and partially because we suspected that 
very narrowly defined senses frequently have 
only one possible subcategorization. Coding 
for such senses would have thus biased our 
experiment s rongly toward finding a strong link 
between sense and subeategorization-bias. 
30 
We calculated transitivity biases for each of the 
64 verbs in each of the three corpora. We 
classed the verbs as high transitivity if more than 
2/3 of the tokens of the major sense were 
transitive, low transitivity if more than 2/3 of the 
tokens of the major sense were intransitive, and 
as mixed otherwise. We removed from 
consideration any token of the verb which was 
not used in its major sense. If  
subcategorization biases are related to verb 
sense, we would expect he transitivity biases to 
be stable across corpora once secondary senses 
are removed from consideration. 
3.2 Results:  
Nine of the 64 verbs, shown in Table 5, had a 
significant shift in transitivity bias. These 
verbs had a different high/mixed/low transitivity 
bias in at least one of the three corpora. 
Verb 
i 
advance 
crack 
BNC 
transitivity 
mixed 
(48%) 
Brown 
transitivity 
mixed 
(55%) 
WSJ 
transitivity 
low 
(19%) 
mixed mixed high 
(58%) (58%) (86%) 
fight low mixed high 
(29%) (49%) (64%) 
float low low mixed 
(22 %) (11%) (44%) 
flood mixed 
(52%) 
relax low 
(27%) 
soften 
high 
(100%) 
high 
(\]00%) 
low mixed 
(30%) (65%) 
high 
(71%) 
high 
(70%) 
mixed 
(43%) 
study high mixed high 
(84%) (39%) (92%) 
surrender 
Table 5 - 
mixed 
(48%) 
mixed 
(39%) 
high 
(73%) 
Transitivity bias in each corpus 
3.3 Discussion: 
In general, these shifts in transitivity were a 
result of the verbs having differences in sense 
between the corpora such that the senses had 
different subcategorizations, but were still 
within our broadly defined 'main sense' for that 
verb. 
For seven out of the nine verbs, the shifts in 
transitivity are a result of differences between 
the WSJ data and the other data, which are a 
result of the WSJ being biased towards 
business-specifie uses of these verbs. For 
example, in the BNC and Brown data, 'advance' 
is a mixture of transitive and intransitive uses, 
shown in ( 6 ) and ( 7 ), while intransitive share 
price changes ( 8 ) dominated in the WSJ data. 
( 6 ) BNC intransitive: In films, they 
advance in droves of armour across open 
fields ... 
( 7 ) BNC transitive: We have advanced 
~'moral careers" as another useful concept ._ 
( 8 ) WSJ intransitive: Of the 4,345 stocks 
that T changed hands, 1,174 declined and 
1,040 advanced. 
'Crack' is used to mean 'make a sound' ( 9 ) or 
'break' ( 10 ) in the Brown and BNC data (both 
of which have transitive and intransitive uses), 
while it is more likely to be used to mean 'enter 
or dominate a group/market' ( transitive use) in 
the WSJ data; ( 11 ) and ( 12 ). 
( 9 ) Brown intransitive: A carbine cracked 
more loudly ,.. 
( 10 ) Brown intransitive: Use well-wedged 
clay, free of air bubbles and pliable enough to 
bend without cracking. 
( 11 ) WSJ transitive: But the outsiders 
haven't yet been able to crack Saatchi's clubby 
inner circle, or to have significant influence on 
company strategy. 
( 12 ) WSJ transitive: ... big investments in
"domestic" industries uch as beer will make 
it even tougher for foreign competitors to 
crack the Japanese market. 
'Float' is generally used as an intransitive verb 
( 13 ), but nmst be used transitively when used 
in a financial sense ( 14 ). 
( 13 ) Brown i~:ransitive: The ball floated 
downstream. 
( 14 ) WSJ transitive: B.A.T aims to ... float 
its big paper and British retailing businesses 
via share issues to existing holders. 
=11 
'Relax' is generally used intransitively ( 15 ), 
but is used transitively in the WSJ data when 
discussing the relaxation of rules and credit 
(16).  
( 15 ) BNC intransitive: The moment Joseph 
stepped out onto the terrace the worried faces 
of Tran Van Hien and his wife relaxed with 
relief. 
( 16 ) WSJ transitive: Ford is willing to bid 
for 100% of Jaguar's hares if both the 
government and Jaguar shareholders agree to 
relax the anti-takeover barrier prematurely. 
'Soften" is generally used transitively ( 17 ), but 
is used intransitively in the WSJ data when 
discussing the softening of prices ( 18 ) and 
(19).  
( 17 ) Brown transitive: Hardy would not 
allow sentiment to soften his sense of the 
irredeemable pastness of the past, and the 
eternal deadness of the dead. 
( 18 ) WSJ intransitive: A spokesman for 
Scott says that assuming the price of pulp 
continues to soften, "We should do well." 
( 19 ) WSJ intransitive: The stock has since 
softened, trading around $25 a share last week 
and closing yesterday at$2.3.00 in national 
over-the-counter trading. 
'Surrender' is used both transitively ( 20 ) and 
intransitively ( 21 ), but must be used 
transitively when discussing the surrender of 
particular items uch as 'stocks' ( 22 ) and ( 23 ). 
( 20 ) BNC transitive: In 1475 Stanley 
surrendered his share to the crown... 
( 21 ) Brown intransitive: ... the defenders, 
to save bloodshed, surrendered under the 
promise that they would be treated as 
neighbors 
( 22 ) WSJ transitive: Holders can... 
surrender their shares at the per-share price of 
$1,000, plus accumulated dividends of $6.71 a 
share. 
( 23 ) WSJ transitive: ... Nelson Peltz and 
Peter W. May surrendered warrants and 
preferred stock in exchange for a larger stake 
in Avery's common shares. 
The verb 'fight" is the only verb that has a 
different ransitivity bias in each of the three 
corpora; with all other verbs, at least two 
corpora share the same bias. In the WSJ, fight 
tends to be used transitivdy, describing action 
against a specific entity or concept ( 24 ). In 
the other two corpora, there are more 
descriptions of actions for or against more 
abstract concepts ( 25 ) and ( 26 ). In addition, 
the WSJ differences may further be influenced 
by a journalistic style practice of dropping the 
preposition 'against' in the phrase 'fight 
against'. 
( 24 ) WSJ lrarlsifive: Los Angeles County 
Supervisor Kenneth Hahn yesterday vowed to 
fight the introduction of double-decking in the 
area. 
( 25 ) BNC intransitive: He fought against 
the United Nations troops in the attempted 
Katangese secession of nineteen sixty to sixty- 
two. 
( 26 ) Brown intransitive: But he would fight 
for his own liberty rather than for any abstract 
principle connected with it -- such as "cause". 
The verb 'study' is generally transitive ( 27 ), 
except in the Brown data, where study is 
frequently used with a prepositional phrase 
( 28 ) or to generically describe the act of 
studying ( 29 ). We are currently investigating 
what might be causing this difference; possible 
candidates include language change (since 
Brown is much older than BNC and WSJ), 
British-American differences, or micro-sense 
differences. 
( 27 ) BNC transitive: A much more useful 
and realistic approach is to study recordings of 
different speakers' natural, spontaneous ...
( 28 ) Brown intransitive: Inaddition, Dr. 
Clark has studied at Rhode Island State 
College and Massachusetts Institute of 
Technology. 
( 29 ) Brown intransitive: She discussed in 
her letters to Winslow some of the questions 
that came to her as she studied alone. 
The verb 'flood" is used intransitively more 
often in the BNC than in the other corpora. 
The Brown and WSJ uses tend to be transitive 
non-weather uses of the verb flood ( 30 ) and 
32 
( 31 ), while the BNC uses include more weather 
uses, which are more likely to be intransitive 
( 32 ). We are investigating whether this is a 
result of the BNC discussing weather more often, 
or a result of which particular grammatical 
structures are used to describe the weather 
floods in British and American English. 
( 30 ) WSJ transitive: Lawsuits over the 
harm caused by DES have flooded federal and 
state courts in the past decade. 
( 31 ) Brown transitive: The terrible vision 
of the ghetto streets flooded his mind. 
( 32 ) BNC intransitive: ,.. should the river 
flood, as he'd observed it did after heavy rain, 
the house was safe upon its hill. 
Conclusion 
The goal of the work performed in this paper 
was to find a stable set of transitivity biases for 
64 verbs to provide norming data for 
psychological experiments. 
The first result is that 55 out of 64 single sense 
verbs analyzed id not change in transitivity bias 
across corpora. This suggests that for our goal 
of providing transitivity biases for single sense 
verbs, the influence of American vs. British 
English and broad based vs. narrow corpora may 
not be large. We would, however, expect 
larger cross corpus differences for verbs that are 
more polysemous than our particular set of 
verbs. 
The second result is that for the 9 out of 64 verbs 
that did change in transitivity bias, the shift in 
transitivity bias was largely a result of subtle 
shifts in verb sense between the genres present 
in each corpus. These two results suggest that 
when verb sense is adequately controlled for, 
verbs have stable suboategorization probabilities 
across corpora. 
One possible future application of our work is 
that it might be possible to use verb frequencies 
and subeategodzafion probabilities of multi- 
sense verbs can be used to measure the degree of 
difference between corpora. 
Acknowledgements 
This project was partially supported by NSF 
BCS-9818827 and IRI-9618838. Many thanks 
to the three anonymous reviewers. 
References 
Biber, D. (1988) Variation across speech and 
writing. Cambridge University Press, Cambridge. 
Biber, D. (1993) Using Register-DiversO$ed 
Corpora for General Language Studies. 
Computational Linguistics, 19(2), 219-241. 
Biber, D, Conrad, S., & Reppen, R. (1998) 
Corpus Linguistics. Cambridge University Press, 
Cambridge. 
Boland, J. (1997). Resolving syntactic category 
ambiguities in discourse context: probabilistic and 
discourse constraints. Journal of Memory and 
Language 36, 588-615. 
Carrol, J., Minnen, G., & Briscoc, T. (1998). Can 
subcategorizafion probabilities help a statistical 
parser? In Proceedings of the 6 ~ ACL/SIGDAT 
Workshop on Very Large Corpora, Montreal, 
Canada. 
Charniak, E. (1997). Statistical parsing with a 
context-free grammar and word statistics. In 
AAAI-97, Menlo Park. AAAI Press. 
Clifton, C., Prazier, L., & Connine, C. (1984) 
Lexical expectations in sentence comprehension. 
Journal of Verbal Learning and Verbal Behavior, 
23, 696-708. 
Collins, M. J. (1996) A new statistical parser based 
on bigram lexical dependencies. In Proceedings of 
ACL-96, 184---191, Santa Cruz, CA. 
Collins, M. J. (1997) Three generative, iexicalised 
models for statistical parsing. In Proceedings of 
ACL-97. 
Ferreira, F., and McClure, K.K. (1997). Parsing 
of Garden-path Sentences with Reciprocal Verbs. 
Language and Cognitive Processes 12, 273-306. 
Fodor, J. (1978). Parsing strategies and 
constraints on traraformations. Linguistic 
Inquiry, 9, 427-473. 
Garnsey, S. M., Pearlmutter, N. J., Myers, E. & 
Lotocky, M. A. (1997). The contributions of 
verb bias and p!a_u~ibility o the comprehension f
temporarily ambiguous sentences. Journal of 
Memory and Language 37, 58-93. 
Gibson, E., Sehatze, C., & Salmon, A. (1996). 
The relationship between the frequency and the 
processing complexity of linguistic structure. 
Journal of Psycholinguistic Rese,~ch 25(1), 59-92. 
Joshi, A. & B. Sfinivas. (1994) Disambiguation of 
super parts of speech (or supertags): almost 
parsing. Proceedings of COLING '94. 
RR 
Jurafsky, D. (1996) A probabilistic model of 
lexical and syntactic access cmd disambiguazion. 
Cognitive Science, 20, 137-194. 
Kim A, Srinivas B and T~aeswell J (1997). 
Incremental Processing Using Lexicalized Tree- 
Adjoining Grammar: Symbolic' and Connectionist 
Approaches, Conference (m Computational 
Psycholinguistics, Berkeley, California, August 
1997. 
MacDonald, M. C. (1994) Probabilistic 
constraints and syntactic ambiguity resolution. 
Language and Cognitive Processes 9, 157-201. 
Marcus, M.P., Santorini, B. & Marcinkiewicz, M.A.. 
(1993) Building a Large Atmotated Corpus of 
English: The Penn Treebank. Computational 
Linguistics 19.2:313-330. 
Merlo, P. (1994). A Corpus-Based Analysis of Verb 
Continuation Frequencies for $~ntactic Processing. 
Journal of Pyscholinguistic Research 23.6:435- 
457. 
Mitchcll, D. C. and V. M. Holmes. (1985) 
The role of specific information about the verb in 
parsing sentences with local structural ambiguity. 
Journal of Memory and Language 24, 542-559. 
Roland, Douglas and Daniel Jurafsky. (2000 in press). 
Verb sense and verb subcategofization probabilities. 
In Paola Merlo and Suzarme Stevenson (Eds.) John 
Bcnjamins. 
Roland, Douglas and Daniel Jmafskky. (1998). How 
verb subcatcgorization frequencies are affected by 
corpus choice. Proceedings of COLING-ACL 1998. 
p 1117-1121. 
Roland, D. and Jurafsky, D. (1997) Computing 
verbal valence frequencies: corpora versus 
norm/ng studies. Poster session presented at the 
CUNY sentence processing conference, Santa 
Monica, CA. 
Stolcke, A., C. Chelba, D. Engle, V. Jimenez, L. 
Mangu, H. Printz, E. Ristad, R. Rosenfeld, D. Wu, 
F. Jelinck and S. Khudanpur. (1997) Dependency 
Language Modeling. Cen~r for Language and 
Speech Processing Research Note No. 24. Johns 
Hopkins University, Baltimore. 
Tanenhans, M. K., Garnsey, S. M., & Boland, J. 
(1990). Combinatory lexical information and 
language comprehension. In Altmann, Gerry T. 
M. (Ed); et al Cognitive models of speech 
processing: Psycholinguistic and computational 
perspectives. Cambridge, MA, USA: Mit Press. 
Trueswell, J., M. Tanenhaus and C. Kello. (1993) 
Verb.Specific Constraint~ in Sentence Processing: 
Separating Effects of Lexical Preference from 
Garden-Paths. Journal of Experimental 
Psychology: Learning, Memory and Cognition 
19.3, 528-553 
34 
Is Knowledge-Free Induction of Multiword Unit 
Dictionary Headwords a Solved Problem?
Patrick Schone and Daniel Jurafsky
University of Colorado, Boulder CO 80309
{schone, jurafsky}@cs.colorado.edu
Abstract
We seek a knowledge-free method for inducing
multiword units from text corpora for use as
machine-readable dictionary headwords.  We
provide two major evaluations of nine existing
collocation-finders and  illustrate the continuing
need for improvement.  We use Latent Semantic
Analysis to make modest gains in performance, but
we show the significant challenges encountered  in
trying this approach.
1 Introduction
A multiword unit (MWU) is a connected
collocation: a sequence of neighboring words
?whose exact and unambiguous meaning or
connotation cannot be derived from the meaning or
connotation of its components? (Choueka, 1988).  In
other words, MWUs are typically non-compositional
at some linguistic level. For example, phonological
non-compositionality has been observed (Finke &
Weibel, 1997; Gregory, et al 1999) where words
like ?got? [g<t] and ?to? [tu] change phonetically to
?gotta? [g<rF] when combined.  We have interest in
inducing headwords for machine-readable
dictionaries (MRDs), so our interest is in semantic
rather than phonological non-compositionality.  As
an example of semantic non-compositionality,
consider ?compact disk?: one could not deduce that
it was a music medium by only considering the
semantics of ?compact? and ?disk.?
MWUs may also be non-substitutable and/or
non-modifiable (Manning and Sch?tze, 1999).  Non-
substitutability implies that substituting a word of
the MWU with its synonym should no longer
convey the same original content: ?compact disk?
does not readily imply ?densely-packed disk.? Non-
modifiability, on the other hand, suggests one
cannot modify the MWU?s structure and still convey
the same content: ?compact disk? does not signify
?disk that is compact.?
MWU dictionary headwords generally satisfy at
least one of these constraints. For example, a
compositional phrase would typically be excluded
from a hard-copy dictionary since its constituent
words would already be listed. These strategies
allow hard-copy dictionaries to remain compact.  
As mentioned, we wish to find MWU headwords
for machine-readable dictionaries (MRDs).
Although space is not an issue in MRDs, we desire
to follow the lexicographic practice of reducing
redundancy.  As Sproat indicated, "simply
expanding the dictionary to encompass every word
one is ever likely to encounter is wrong: it fails to
take advantage of regularities" (1992, p. xiii).  Our
goal is to identify an automatic, knowledge-free
algorithm that finds all and only those collocations
where it is necessary to supply a definition.
?Knowledge-free? means that the process should
proceed without human input (other than, perhaps,
indicating whitespace and punctuation).
This seems like a solved problem.  Many
collocation-finders exist, so one might suspect that
most could suffice for finding MWU dictionary
headwords.  To verify this, we evaluate nine
existing collocation-finders to see which best
identifies valid headwords.  We evaluate using two
completely separate gold standards: (1) WordNet
and (2) a compendium of Internet dictionaries.
Although web-based resources are dynamic and
have better coverage than WordNet (especially for
acronyms and names), we show that WordNet-based
scores are comparable to those using Internet
MRDs. Yet the evaluations indicate that significant
improvement is still needed in MWU-induction.
As an attempt to improve MWU headword
induction, we introduce several algorithms using
Latent Semantic Analysis (LSA). LSA is a
technique which automatically induces semantic
relationships between words.  We use LSA to try to
eliminate proposed MWUs which are semantically
compositional.  Unfortunately, this does not help.
Yet when we use LSA to identify substitutable delimiters.  This suggests that in a language with
MWUs, we do show modest performance gains. whitespace, one might prefer to begin at the word
2 Previous Approaches
For decades, researchers have explored various
techniques for identifying interesting collocations.
There have essentially been three separate kinds of
approaches for accomplishing this task.  These
approaches could be broadly classified into (1)
segmentation-based, (2) word-based and knowledge-
driven, or (3) word-based and probabilistic. We will
illustrate strategies that have been attempted in each
of the approaches. Since we assume knowledge of
whitespace, and since many of the first and all of the
second categories rely upon human input, we will be
most interested in the third category.
2.1 Segmentation-driven Strategies
Some researchers view MWU-finding as a natural
by-product of segmentation. One can regard text as
a stream of symbols and segmentation as a means of
placing delimiters in that stream so as to separate
logical groupings of symbols from one another.  A
segmentation process may find that a symbol stream
should not be delimited even though subcomponents
of the stream have been seen elsewhere.  In such
cases, these larger units may be MWUs.
The principal work on segmentation has focused
either on identifying words in phonetic streams
(Saffran, et. al, 1996; Brent, 1996; de Marcken,
1996) or on tokenizing Asian and Indian languages
that do not normally include word delimiters in their
orthography (Sproat, et al 1996; Ponte and Croft
1996; Shimohata, 1997; Teahan, et al, 2000; and
many others). Such efforts have employed various
strategies for segmentation, including the use of
hidden Markov models, minimum description
length, dictionary-based approaches, probabilistic
automata, transformation-based learning, and text
compression. Some of these approaches require
significant sources of human knowledge, though
others, especially those that follow data
compression or HMM schemes, do not.  
These approaches could be applied to languages
where word delimiters exist (such as in European
languages delimited by the space character).
However, in such languages, it seems more prudent
to simply take advantage of delimiters rather than
introducing potential errors by trying to find word
boundaries while ignoring knowledge of the
level and identify appropriate word combinations.
2.2 Word-based, knowledge-driven Strategies
Some researchers start with words and propose
MWU induction methods that make use of parts of
speech, lexicons, syntax or other linguistic structure
(Justeson and Katz, 1995; Jacquemin, et al, 1997;
Daille, 1996). For example, Justeson and Katz
indicated that the patterns NOUN  NOUN and ADJ
NOUN are very typical of MWUs.  Daille also
suggests that in French, technical MWUs follow
patterns such as ?NOUN de NOUN" (1996, p. 50).
To find word combinations that satisfy such patterns
in both of these situations necessitates the use of a
lexicon equipped with part of speech tags. Since we
are interested in knowledge-free induction of
MWUs, these approaches are less directly related to
our work. Furthermore, we are not really interested
in identifying constructs such as general noun
phrases as the above rules might generate, but
rather, in finding only those collocations that one
would typically need to define.  
2.3 Word-based, Probabilistic Approaches
The third category assumes at most whitespace
and punctuation knowledge and attempts to infer
MWUs using word combination probabilities.
Table 1 (see next page) shows nine commonly-used
probabilistic MWU-induction approaches.  In the
table,  f  and P  signify frequency and probabilityX X
of a word X. A variable XY indicates a word bigram
and   indicates its expected frequency at random.XY
An overbar signifies a variable?s complement. For
more details, one can consult the original sources as
well as Ferreira and Pereira (1999) and Manning
and Sch?tze (1999).
3 Lexical Access
Prior to applying the algorithms, we lemmatize
using a weakly-informed tokenizer that knows only
that whitespace and punctuation separate words.
Punctuation can either be discarded or treated as
words.  Since we are equally interested in finding
units like ?Dr.? and ?U. S.,? we opt to treat
punctuation as words.
Once we tokenize, we use Church?s (1995) suffix
array approach to identify word n-grams that occur
at least T times (for T=10).   We then rank-order the
PX|YMIXY
MZ PrZ|YMIZY
	2log
[PX PYPX PY]
fY
[PXYPXY]
fXY [PXYPXY]
fXY
M
i{X,X}
j{Y,Y}
(fij 	 ij )2
ij
fXY 	 XY
XY (1	(XY/N))
fXY 	 XY
fXY (1	(fXY/N))
Table 1: Probabilistic Approaches
METHOD FORMULA
Frequency
(Guiliano, 1964)
fXY
Pointwise Mutual
Information (MI)
(Fano, 1961;
Church and Hanks,
1990)
log  (P  / P P )2 XY X Y
Selectional
Association
(Resnik, 1996)
Symmetric
Conditional
Probability
(Ferreira and
Pereira, 1999)
P  / P PXY X Y2
Dice Formula
(Dice, 1945) 2 f / (f +f )XY X Y
Log-likelihood
(Dunning, 1993; (Daille, 1996). Since we need knowledge-poor
Daille, 1996) induction, we cannot use human-suggested filtering
Chi-squared ($ )2
(Church and Gale,
1991)
Z-Score
(Smadja, 1993;
Fontenelle, et al,
1994)
Student?s t-Score
(Church and
Hanks, 1990)
n-gram list in accordance to each probabilistic
algorithm.  This task is non-trivial since most
algorithms were originally suited for finding two-
word collocations.  We must therefore decide how
to expand the algorithms to identify general n-grams
(say, C=w w ...w ).  We can either generalize or1 2 n
approximate. Since generalizing requires
exponential compute time and memory for several
of the algorithms, approximation is an attractive
alternative. 
One approximation redefines X and Y to be,
respectively, the word sequences w w ...w  and1 2 i
w w ...w  where i is chosen to maximize P P .i+1 i+2 n, X Y
This has a natural interpretation of being the
expected probability of concatenating the two most
probable substrings in order to form the larger unit.
Since it can be computed rapidly with low memory
costs, we use this approximation.
Two additional issues need addressing before
evaluation.  The first regards document sourcing. If
an n-gram appears in multiple sources (eg.,
Congressional Record versus Associated Press),  its
likelihood of accuracy should increase. This is
particularly true if we are looking for MWU
headwords for a general versus specialized
dictionary.  Phrases that appear in one source may
in fact be general MWUs, but frequently, they are
text-specific units. Hence, precision gained by
excluding single-source n-grams may be worth
losses in recall.  We will measure this trade-off.
Second, evaluating with punctuation as words and
applying no filtering mechanism may unfairly bias
against some algorithms.  Pre- or post-processing of
n-grams with a linguistic filter has shown to
improve some induction algorithms? performance
rules as in Section 2.2.  Yet we can filter by pruning
n-grams whose beginning or ending word is among
the top N most frequent words.  This unfortunately
eliminates acronyms like ?U.  S.? and phrasal verbs
like ?throw up.? However, discarding some words
may be worthwhile if the final list of n-grams is
richer in terms of MRD headwords. We therefore
evaluate with such an automatic filter, arbitrarily
(and without optimization) choosing  N=75.
4 Evaluating Performance
A natural scoring standard is to select a language
and evaluate against headwords from existing
dictionaries in that language.  Others have used
similar standards (Daille, 1996), but to our
knowledge, none to the extent described here.  We
evaluate thousands of hypothesized units from an
unconstrained corpus. Furthermore, we use two
separate evaluation gold standards: (1) WordNet
(Miller, et al 1990) and (2) a collection of Internet
MRDs.  Using  two gold standards helps valid
MWUs.  It also provides evaluation using both static
and dynamic resources.  We choose to evaluate in
English  due  to the wealth of  linguistic resources.
Rank ZScore $2 SCP Dice MutualInfo.
Select
Assoc.
Log
Like. TScore Freq
1 Iwo
Jima
Buenos
Aires
Buenos
Aires
Buenos
Aires
Iwo
Jima
United
States
United
States
United
States
United
States
2
bona
fide
Iwo
Jima
Iwo
Jima
Iwo
Jima
bona
fide
House
of
Repre-
sentatives
Los
Angeles
Los
Angeles
Los
Angeles
4 Burkina
Faso Suu Kyi Suu Kyi Suu Kyi
Wounded 
Knee
Los
Angeles
New
York
New
York
New
York
8 Satanic
Verses Sault Ste Sault Ste Sault Ste
Hubble
Space
Telescope
my
colleagues
Soviet
Union
my
colleagues
my
colleagues
16 Ku 
Klux
Ku 
Klux
Ku 
Klux
Ku 
Klux
alma
mater
H . R SocialSecurity
High
School
High
School
32
Pledge of
Allegiance
Pledge of
Allegiance
Pledge of
Allegiance
Pledge of
Allegiance
Coca -
Cola War II
House of
Repre-
sentatives
Wednesday
* * * *
64 Telephone
& amp ;
Telegraph
Telephone
& amp ;
Telegraph
Telephone
& amp ;
Telegraph
Internal
Revenue
Planned
Parent-
hood
Prime
Minister * * *
real
estate
New
Jersey
128 Prime
Minister
Prime
Minister
Prime
Minister
Salman
Rushdie
Sault Ste
. Marie
both  
sides
At the 
same time
Wall
Street
term
care
256 Lehman
Hutton
Lehman
Hutton
Lehman
Hutton
tongue -
in -
cheek
o ? clock At the
same
del Mar all
over
grand
jury
512
La Habra La Habra La Habra
compens-
atory and
punitive
20th -
Century
Monday
night
days
later
80
percent
Great
Northern
1024 telephone
interview
telephone
interview
telephone
interview
Food and
Agriculture
Sheriff ?s
deputies
South
Dakota
County
Jail
where
you
300
million
  Table 2: Outputs from each algorithm at different sorted ranks
The ?* *? and ?* * *? are actual units.
In particular, we use a randomly-selected corpus the first five columns as ?information-like.?
consisting of a 6.7 million word subset of  the TREC Similarly, since the last four columns share
databases (DARPA, 1993-1997). properties of the frequency approach, we will refer
Table 2 illustrates a sample of rank-ordered output to them as ?frequency-like.? 
from each of the different algorithms (following the One?s application may dictate which set of
cross-source, filtered paradigm described in section algorithms to use.  Our gold standard selection
3).  Note that algorithms in the first four columns reflects our interest in general word dictionaries, so
produce results that are similar to each other as do results we obtain may differ from results we might
those in the last four columns. Although the mutual have obtained using terminology lexicons.
information results seem to be almost in a class of If our gold standard contains K MWUs with
their own, they actually are similar overall to the corpus frequencies satisfying threshold (T=10), our
first four sets of results; therefore, we will refer to figure of merit (FOM) is given by
1
K M
K
i
1 Pi ,
          little or even negative impact.  On the other hand,
where P  (precision at i) equals i/H , and H  is thei i i
number of hypothesized MWUs required to find the
i  correct MWU. This FOM corresponds to areath
under a precision-recall curve.
4.1 WordNet-based Evaluation
WordNet has definite advantages as an evaluation
resource. It has in excess of 50,000 MWUs, is freely
accessible, widely used, and is in electronic form.
Yet, it obviously cannot contain every MWU.  For
instance, our corpus contains 177,331 n-grams (for
2n10) satisfying T10, but WordNet contains
only 2610 of these. It is unclear, therefore, if
algorithms are wrong when they propose  MWUs
that are not in WordNet. We will assume they are
wrong but with a special caveat for proper nouns.
WordNet includes few proper noun MWUs.  Yet
several algorithms produce large numbers of proper
nouns. This biases against them.  One could contend
that all proper nouns MWUs are valid,  but we
disagree.  Although such may be MWUs, they are
not necessarily MRD headwords; one would not
include every proper noun  in a dictionary, but
rather, those needing definitions.  To overcome this,
we will have two scoring modes.  The first, ?S?
mode (standing for some) discards any proposed
capitalized n-gram whose uncapitalized version is
not in WordNet.  The second mode ?N? (for none)
disregards all capitalized n-grams. 
Table 3 illustrates algorithmic performance as
compared to the 2610 MWUs from WordNet.  The
first double column illustrates ?out-of-the-box?
performance on all 177,331 possible n-grams. The
second double column shows cross-sourcing: only
hypothesizing MWUs that appear in at least two
separate datasets  (124,952 in all), but being
evaluated against all of the 2610 valid units.  Double
columns 3 and 4 show effects from high-frequency
filtering the n-grams of the first and second columns
(reporting only 29,716 and 17,720 n-grams)
respectively.  
As Table 3 suggests, for every condition, the
information-like algorithms seem to perform best at
identifying valid, general MWU headwords.
Moreover, they are enhanced when cross-sourcing
is considered; but since much of their strength
comes from identifying proper nouns, filtering has
the frequency-like approaches are independent of
data source.  They also improve significantly with
filtering. Overall, though, after the algorithms are
judged, even the best score of 0.265 is far short of
the maximum possible, namely 1.0.
Table 3: WordNet-based scores
Prob (1) (2) (3) (4)
algo- WordNet WordNet WordNet WordNet
rithm cross- +Filter cross-
source source
+Filter
S N S N S N S N
Zscore .222 .146 .263 .193 .220 .129 .265 .173
SCP .221 .145 .262 .192 .220 .129 .265 .173
Chi-sqr .222 .146 .263 .193 .220 .129 .265 .173
Dice .242 .167 .265 .199 .230 .142 .256 .172
MI .191 .122 .245 .169 .185 .111 .233 .151
SA .057 .051 .058 .053 .182 .125 .202 .143
Loglike .049 .050 .068 .064 .118 .095 .177 .129
T-score .050 .051 .050 .052 .150 .109 .160 .118
Freq .035 .037 .034 .037 .144 .105 .152 .112
4.2   Web-based Evaluation
Since WordNet is static and cannot report on all of
a corpus? n-grams, one may expect different
performance by using a more all-encompassing,
dynamic resource. The Internet houses dynamic
resources which can judge practically every induced
n-gram.  With permission and sufficient time, one
can repeatedly query websites that host large
collections of MRDs and evaluate each n-gram. 
Having approval, we queried: (1) onelook.com,
(2) acronymfinder.com, and (3) infoplease.com. The
first website interfaces with over 600 electronic
dictionaries.  The second is devoted to identifying
proper acronyms.  The third focuses on world facts
such as historical figures and organization names. 
To minimize disruption to websites by reducing
the total number of queries needed for evaluation,
we use an evaluation approach from the information
retrieval community (Sparck-Jones and van
Rijsbergen, 1975). Each algorithm reports its top
5000 MWU choices and  the union of these choices
(45192 possible n-grams) is looked up on the
Internet.  Valid MWUs identified at any website are
assumed to be the only valid units in the data.
{Xi}ni
1 {Xi
}n
i
1
cos(X,Y) 
 X #Y||X|| ||Y|| .
Algorithms are then evaluated based on this showed how one could compute latent semantic
collection.  Although this strategy for evaluation is vectors for any word in a corpus (Schone and
not flawless, it is reasonable and makes dynamic Jurafsky, 2000).  Using the same approach, we
evaluation tractable. Table 4 shows the algorithms? compute semantic vectors for every proposed word
performance (including proper nouns). n-gram C=X X ...X   Since LSA involves word
Though Internet dictionaries and WordNet are counts, we can also compute semantic vectors
completely separate ?gold standards,? results are
surprisingly consistent.   One can conclude that
WordNet may safely be used as a gold standard in
future MWU headword evaluations. Also,
Table 4 Performance on Internet data
Prob (1) (2) (3) (4)
algorithm Internet Internet Internet Internet
cross- +Filter cross-
source source
+Filter
Z-Score .165 .260 .169 .269
SCP .166 .259 .170 .270
Chi-sqr .166 .260 .170 .270
Dice .183 .258 .187 .267
MI .139 .234 .140 .234
SA .027 .033 .107 .194
Log Like .023 .043 .087 .162
T-score .025 .027 .110 .142
Freq .016 .017 .104 .134
one can see that Z-scores, $ , and2
SCP have virtually identical results and seem to best
identify MWU headwords (particularly if proper
nouns are desired).  Yet there is still significant
room for improvement.
5 Improvement strategies
Can performance be improved?  Numerous
strategies could be explored. An idea we discuss
here tries using induced semantics to rescore the
output of the best algorithm (filtered, cross-sourced
Zscore) and eliminate semantically compositional or
modifiable MWU hypotheses.
Deerwester, et al(1990) introduced Latent
Semantic Analysis (LSA) as a computational
technique for inducing semantic relationships
between words and documents.  It forms high-
dimensional vectors using word counts and uses
singular value decomposition to project those
vectors into an optimal k-dimensional, ?semantic?
subspace (see Landauer, et al 1998).  
Following an approach from Sch?tze (1993), we
1 2 n.
(denoted by ) for C?s subcomponents.  These can
either  include  (           ) or exclude (             )  C?s
counts. We seek to see if induced semantics can
help eliminate incorrectly-chosen MWUs.  As will
be shown, the effort using semantics in this nature
has a very small payoff for the expended cost.
5.1    Non-compositionality
Non-compositionality is a key component of valid
MWUs, so we may desire to emphasize n-grams that
are semantically non-compositional.   Suppose we
wanted to determine if C (defined above) were non-
compositional.  Then given some meaning function,
, C should satisfy an equation like:  
g(  (C) , h( (X ),...,(X ) )  )0,           (1)1 n
where h combines the semantics of C?s
subcomponents and g measures semantic
differences.  If C were a bigram, then if g(a,b) is
defined to be |a-b|, if h(c,d) is the sum of c and d,
and if (e) is set to -log P , then equation (1) woulde
become the pointwise mutual information of the
bigram. If g(a,b) were defined to be (a-b)/b , and if?
h(a,b)=ab/N and  (X)=f  , we essentially get Z-X
scores.  These formulations suggest that several of
the probabilistic algorithms we have seen include
non-compositionality measures already.  However,
since the probabilistic algorithms rely only on
distributional information obtained by considering
juxtaposed words,  they tend to incorporate a
significant amount of non-semantic information
such as syntax. Can semantic-only rescoring help?
To find out, we must select g, h, and .  Since we
want to eliminate MWUs that are compositional, we
want h?s output to correlate well with C when there
is compositionality and correlate poorly otherwise.
Frequently, LSA vectors are correlated using the
cosine between them:
A large cosine indicates strong correlation, so large
values for g(a,b)=1-|cos(a,b)| should signal weak
correlation or non-compositionality. h could
Mn
i
1 wi ai
cos
cos(Xi,Y) 

min
k{Xi,Y}
cos(Xi ,Y)	?k
1k
.
represent a weighted vector sum of the components? required for this task.  This seems to be a significant
semantic vectors with weights (w ) set to either 1.0 component.  Yet there is still another: maybei
or the reciprocal of the words? frequencies. semantic compositionality is not always bad.
Table 5 indicates several results using these Interestingly, this is often the case.  Consider
settings.  As the first four rows indicate and as vice_president, organized crime, and
desired, non-compositionality is more apparent for Marine_Corps. Although these are MWUs, one
 * (i.e., the vectors derived from excluding C?sX
counts) than for  .  Yet, performance overall isX
horrible, particularly considering we are rescoring
Z-score output whose score was 0.269.  Rescoring
caused five-fold degradation!
Table 5: Equation 1 settings
g(a,b) h(a) (X) w Score oni
Internet
1-|cos(a,b)|
X 1 0.0517
1/fi 0.0473
 *X 1 0.0598
1/fi* 0.0523
|cos(a,b)|
X 1 0.174
1/fi 0.169
 *X 1 0.131
1/fi* 0.128
What happens if we instead emphasize
compositionality?  Rows 5-8 illustrate the effect:
there is a significant recovery in performance.  The
most reasonable explanation for this is that if
MWUs and their components are strongly
correlated, the components may rarely occur except
in context with the MWU.  It takes about 20 hours
to compute the  * for each possible n-gramX
combination. Since the probabilistic algorithms
already identify n-grams that share strong
distributional properties with their components, it
seems imprudent to exhaust resources on this LSA-
based strategy for non-compositionality.
These findings warrant some discussion.  Why did
non-compositionality fail?  Certainly there is the
possibility that better choices for g, h, and  could
yield improvements.  We actually spent months
trying to find an optimal combination as well as a
strategy for coupling LSA-based scores with the Z-
scores, but without avail. Another possibility:
although LSA can find semantic relationships, it
may not make semantic decisions at the level
would still expect that the first is related to
president, the second relates to crime, and the last
relates to Marine.  Similarly, tokens such as
Johns_Hopkins and Elvis are anaphors for
Johns_Hopkins_University and Elvis_Presley, so
they should have similar meanings.
This begs the question: can induced semantics
help at all?  The answer is ?yes.? The key is using
LSA where it does best: finding things that are
similar ? or substitutable. 
5.2 Non-substitutivity
For every collocation C=X X ..X X X ..X , we1 2 i-1 i+1 ni
attempt to find other similar patterns in the data,
X X ..X YX ..X .  If X  and Y are semantically1 2 i-1 i+1 n i
related, chances are that C is substitutable.
Since LSA excels at finding semantic correlations,
we can compare   and   to see if C isXi Y 
substitutable.  We use our earlier approach (Schone
and Jurafsky, 2000) for performing the comparison;
namely, for every word W, we compute cos(  )w, R
for 200 randomly chosen words, R. This allows for
computation of a correlaton mean (? ) and standardW
deviation (1 ) between W  and other words.   AsW  
before, we then compute a normalized cosine score
(      ) between words of interest, defined by
With this set-up, we now look for substitutivity.
Note that phrases may be substitutable and still be
headword if their substitute phrases are themselves
MWUs.  For example, dioxide in carbon_dioxide is
semantically similar to monoxide in
carbon_monoxide.   Moreover, there are other
important instances of valid substitutivity: 
& Abbreviations 
AlAlbert   <   Al_GoreAlbert_Gore
& Morphological similarities
RicoRican <  Puerto_RicoPuerto_Rican
& Taxonomic relationships 
bachelormaster<  
bachelor_?_s_degreemaster_?_s_degree. 
Figure 1: Precision-recall curve for rescoringHowever, guilty and innocent are semantically
related, but pleaded_guilty and pleaded_innocent
are not MWUs.  We would like to emphasize only n-
grams whose substitutes are valid MWUs.
To show how we do this using LSA, suppose we
want to rescore a list L whose entries are potential
MWUs.  For every entry X in L, we seek out all
other entries whose sorted order is less than some
maximum value (such as 5000) that have all but one
word in common.  For example, suppose X is
?bachelor_?_s_degree.?  The only other entry that
matches in all but one word is ?master_?_s_degree.?
If the semantic vectors for ?bachelor? and ?master?
have a normalized cosine score greater than a
threshold of 2.0, we then say that the two MWUs
are in each others substitution set.  To rescore, we
assign a new score to each entry in substitution set.
Each element in the substitution set gets the same
score.  The score is derived using a combination of
the previous Z-scores for each element in the
substitution set.  The combining function may be an
averaging, or a computation of the median, the
maximum, or something else.  The maximum
outperforms the average and the median on our data.
By applying in to our data, we observe a small but
visible improvement of 1.3% absolute to .282 (see
Fig. 1). It is also possible that other improvements
could be gained using other combining strategies.  
6 Conclusions
This paper identifies several new results in the area
of MWU-finding.  We saw that MWU headword
evaluations using WordNet provide similar results
to those obtained from far more extensive web-
based resources. Thus, one could safely use
WordNet as a gold standard for future evaluations.
We also noted that information-like algorithms,
particularly Z-scores, SCP, and $2, seem to perform
best at finding MRD headwords regardless of
filtering mechanism, but that improvements are still
needed. We proposed two new LSA-based
approaches which attempted to address issues of
non-compositionality and non-substitutivity.
Apparently,  either current algorithms already
capture much non-compositionality or LSA-based
models of non-compositionality are of little help.
LSA does help somewhat as a model of
substitutivity. However, LSA-based gains are small
compared to the effort required to obtain them.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their comments and insights.
References
AcronymFinder.com(2000-1). http://www.acronymfinder.
com.  Searches between March 2000 and April 2001.
Brent, M.R. and Cartwright, T.A. (1996). Distributional
regularity and phonotactic constraints are useful for
segmentation.  Cognition, 61, 93-125.
Choueka, Y. (1988).  Looking for needles in a haystack
or locating interesting collocation expressions in large
textual databases. Proceedings of the RIAO, pp. 38-43.
Church, K.W. (1995). N-grams.  Tutorial at ACL, ?95.
MIT, Cambridge, MA.
Church, K.W., & Gale, W.A. (1991). Concordances for
parallel text.  Proc. of the 7  Annual Conference of theth
UW Center for ITE New OED & Text Research, pp.
40-62, Oxford.
Church, K.W., & Hanks, P. (1990). Word association
norms, mutual information and lexicography.
Computational Linguistics, Vol. 16, No. 1, pp. 22-29.
Daille, B. (1996). ?Study and Implementation of
Combined Techniques from Automatic Extraction of
Terminology? Chap. 3 of "The Balancing Act":
Combining Symbolic and Statistical Approaches to
Language (Klavans, J., Resnik, P. (eds.)), pp. 49-66
DARPA (1993-1997). DARPA text collections: A.P.
Material, 1988-1990, Ziff Communications Corpus,
1989, Congressional Record of the 103  Congress,rd
and Los Angeles Times.
Deerwester, S., S.T. Dumais, G.W. Furnas, T.K.
Landauer, and R. Harshman. (1990) Indexing by
Latent Semantic Analysis. Journal of the American
Society of Information Science, Vol. 41
de Marcken, C. (1996) Unsupervised Language
Acquisition, Ph.D., MIT Manning, C.D., Sch?tze, H. (1999) Foundations of
Dias, G., S. Guillor?, J.G. Pereira Lopes (1999). Statistical Natural Language Processing, MIT Press,
Language independent automatic acquisition of rigid Cambridge, MA, 1999.
multiword units from unrestricted text corpora. TALN, Mikheev, A., Finch, S. (1997). Collocation lattices and
Carg?se. maximum entropy models. WVLC, Hong Kong.
Dice, L.R. (1945). Measures of the amount of ecologic
 associations between species. Journal of Ecology, 26,
1945. 
Dunning, T (1993). Accurate methods for the statistics of
surprise and coincidence.  Computational Linguistics.
Vol. 19, No. 1.
Fano, R. (1961).  Transmission of Information.  MIT
Press,   Cambridge, MA.
Finke, M. and Weibel, A. (1997) Speaking mode
dependent pronunciation modeling in large vocabulary
conversational speech recognition.  Eurospeech-97.
Ferreira da Silva, J., Pereira Lopes, G. (1999). A local
maxima method and a fair dispersion normalization for
extracting multi-word units from corpora. Sixth
Meeting on Mathematics of Language, pp. 369-381.
Fontenelle, T., Br?ls, W., Thomas, L., Vanallemeersch,
T., Jansen, J. (1994). DECIDE, MLAP-Project 93-19,
deliverable D-1a: Survey of collocation extraction
tools.  Tech. Report, Univ. of Liege, Liege, Belgium.
Giuliano, V. E. (1964) "The interpretation of word
associations." In M.E. Stevens et al (Eds.) Statistical
association methods for mechanized documentation,
pp. 25-32. National Bureau of Standards
Miscellaneous Publication 269, Dec. 15, 1965.
Gregory, M. L., Raymond, W.D., Bell, A., Fosler-
Lussier, E., Jurafsky, D. (1999). The effects of
collocational strength and contextual predictability in
lexical production. CLS99, University of Chicago.
Heid, U. (1994). On ways words work together. Euralex-
99.
Hindle, D. (1990). Noun classification from predicate-
argument structures.  Proceedings of the Annual
Meeting of the ACL, pp. 268-275.
InfoPlease.com (2000-1). http://www.infoplease.com.
Searches between March 2000 and April 2001.
Jacquemin, C., Klavans, J.L., & Tzoukermann, E. (1997).
Expansion of multi-word terms for indexing and
retrieval using morphology and syntax.  Proc. of ACL
1997, Madrid, pp. 24-31.
Justeson, J.S. and S.M.Katz (1995).  Technical
terminology: some linguistic properties and an
algorithm for identification in text. Natural Language
Engineering 1:9-27.
Kilgariff, A., & Rose, T. (1998).  Metrics for corpus
similarity & homogeneity. Manuscript, ITRI,
University of Brighton.
Landauer, T.K., P.W. Foltz, and D. Laham. (1998)
Introduction to Latent Semantic Analysis. Discourse
Processes. Vol. 25, pp. 259-284.
Miller, G. (1990).?WordNet: An on-line lexical
database,? International Journal of Lexicography, 3(4).
OneLook.com (2000-1). http://www.onelook.com.
Searches between March 2000 and April 2001.
Ponte, J.M., Croft, B.W. (1996). Useg: A Retargetable
word segmentation procedure for information retrieval.
Symposium on Document Analysis and Information
Retrieval ?96.  Technical Report TR96-2, University of
Massachusetts.
Resnik, P. (1996). Selectional constraints: an
information-theoretic model and its computational
realization. Cognition. Vol. 61, pp. 127-159.
Saffran, J.R., Newport, E.L., and Aslin, R.N. (1996).
Word segmentation: the role of distributional cues.
Journal of Memory and Language, Vol. 25, pp. 606-
621.
Schone, P. and D. Jurafsky. (2000) Knowledge-free
induction of morphology using latent semantic
analysis.  Proc. of the Computational Natural
Language Learning Conference, Lisbon, pp. 67-72.
Sch?tze, H. (1993) Distributed syntactic representations
with an application to part-of-speech tagging.
Proceedings of the IEEE International Conference on
Neural Networks, pp. 1504-1509.
Shimohata, S., Sugio, T., Nagata, J. (1997). Retrieving
collocations by co-occurrences and word order
constraints.  Proceedings of the 35  Annual Mtg. of theth
Assoc. for Computational Linguistics.  Madrid.
Morgan-Kauffman Publishers, San Francisco.  Pp.
476-481.
Smadja, F. (1993).  Retrieving collocations from text:
Xtract.  Computational Linguistics, 19:143-177.
Sparck-Jones, K., C. van Rijsbergen (1975) Report on the
need for and provision of an ?ideal? information
retrieval text collection, British Library Research and
Development Report, 5266, Computer Laboratory,
University of Cambridge.
Sproat R, Shih, C. (1990) A statistical method for finding
word boundaries in Chinese text.  Computer
Processing of Chinese & Oriental Languages, Vol. 4,
No. 4.
Sproat, R. (1992) Morphology and Computation. MIT
Press, Cambridge, MA.
Sproat, R.W., Shih, C., Gale, W., Chang, N. (1996) A
stochastic finite-state word segmentation algorithm for
Chinese. Computational Linguistics, Vol. 22, #3.
Teahan, W.J., Yingyin, W. McNab, R, Witten, I.H.
(2000). A Compression-based algorithm for Chinese
word segmentation. ACL Vol. 26, No. 3, pp. 375-394.
  
 
The Effect of Rhythm on Structural Disambiguation in Chinese 
 
Honglin Sun                     Dan Jurafsky 
Center for Spoken Language Research 
University of Colorado at Boulder 
{honglin.sun, jurafsky}@colorado.edu 
 
 
 
Abstract 
 
The length of a constituent (number of 
syllables in a word or number of words in a 
phrase), or rhythm, plays an important role 
in Chinese syntax. This paper systematically 
surveys the distribution of rhythm in 
constructions in Chinese from the statistical 
data acquired from a shallow tree bank. 
Based on our survey, we then used the 
rhythm feature in a practical shallow parsing 
task by using rhythm as a statistical feature 
to augment a PCFG model. Our results show 
that using the probabilistic rhythm feature 
significantly improves the performance of 
our shallow parser. 
 
1 Introduction 
 
Syntactic research indicates that prosodic features, 
including stress, rhythm, intonation, and others, 
have an impact on syntactic structure. For example, 
normally in a coordination construction like ?A 
and B?, A and B are interchangeable, that is to say, 
you can say ?B and A? and the change of word 
order does not change the meaning. However, 
sometimes A and B are not interchangeable. Quirk 
et al(1985) gives the following examples: 
   man and woman          * woman and man 
   ladies and gentleman   *gentleman and ladies 
Obviously, the examples above cannot be 
explained by gender preference. A reasonable 
explanation is that the length of the words (perhaps 
in syllables) is playing a role; the first constituent 
tends to be shorter than the second constituent. 
This feature of the length in syllables of a 
constituent plays an even more important role in 
Chinese syntax than in English (Feng, 2000). For 
example, in the verb-object construction in 
Chinese, there is a preference for the object to be 
equal to or longer than the verb. Thus while both 
???(plant)  and  ????(plant) are verbs and have 
the same meaning,  ? ? /plant ? /tree? is 
grammatical while ? ? ? /plant ? /tree? is 
ungrammatical. However, both verbs allow bi-
syllabic nouns as objects (e.g., ????(fruit tree), 
????(cotton) etc.). The noun phrases formed by 
?noun + verb? give us another example in which 
rhythm feature places constraints on syntax, as 
indicated in the following examples 
(ungrammatical with *): 
    ??/cotton   ??/planting 
     *??/cotton   ?/planting 
     *?/flower       ??/planting 
     *?/flower       ?/planting 
???/cotton   ??/planting? is grammatical but 
???/cotton   ?/planting? , ??/flower   ??
/planting? and ??/flower   ?/planting? are all 
ungrammatical, although ??? /cotton? and ??
/flour? , ???/planting? and ??/planting? have 
the same POS and the same or similar meaning. 
The only difference lies in that they have different 
number of syllables or different length. 
This paper systematically surveys the effect of 
rhythm on Chinese syntax from the statistical data 
from a shallow tree bank. Based on the observation 
that rhythm places constraints on syntax in Chinese, 
we try to deploy a feature based on rhythm to 
improve disambiguation in a probabilistic parser 
by mixing the rhythm feature into a statistical 
parsing model. 
The rest of the paper is organized as follows: we 
present specific statistical analyses of rhythm 
feature in Chinese syntax in Section 2. Section 3 
introduces the content chunk parsing which is the 
task in our experiment. Section 4 presents the 
statistical model used in our experiment in which a 
probabilistic rhythm feature is integrated. Section 5 
gives the experimental results and finally Section 6 
draws some conclusions. 
   
2 Analysis of Rhythmic Constraints 
 
We divide our analysis of the use of rhythm in 
Chinese phrases into two categories, based on two 
types of phrases in Chinese: (1) simple phrases, 
containing only words, i.e. all the child nodes are 
POS tag in the derivation tree; and (2) complex 
phrases in which at least one constituent is a phrase 
itself, i.e. it has at least one child node with phrase 
type symbol (like NP, VP) in its derivation tree. 
Below we will give the statistical analysis of the 
distribution of rhythm feature in different 
constructions from both simple and complex 
phrases. The corpus from which the statistical data 
is drawn contains 200K words of newspaper text 
from the People?s Daily. The texts are word-
segmented, POS tagged and labeled with content 
chunks. The content chunk is a phrase containing 
only content words, akin to a generalization of a 
BaseNP. These content chunks are parsed into 
binary shallow trees. More details about content 
chunks can be found in Section 3. 
 
2.1 Rhythm feature in simple phrases 
 
Simple phrases contain two lexical words (since, as 
discussed above, our parse trees are binary). The 
rhythm feature of each word is defined to be the 
number of syllables in it. Thus the rhythm feature 
for a word can take on one of the following three 
values: (1) monosyllabic; (2) bi-syllabic; and (3) 
multi-syllabic, meaning with three syllables or 
more. 
Since each binary phrase contains two words, 
the set of rhythm features for a simple phrase is: 
F = { (0?0), (0,1), (0,2), (1,0), (1,1), (1,2), (2,0), 
(2,1), (2,2) } 
where 0, 1, 2 represent monosyllabic, bi-syllabic 
and multi-syllabic respectively. 
In the following sections, we will present three 
case studies on the distributions of rhythm feature 
in different constructions: (1) verbs as modifier or 
head in NP; (2) the contrast between NPs and VPs 
formed by ? verb + noun? sequences; (3) ?noun + 
verb? sequences. 
 
2.1.1  Case 1: Verb as modifier/head  in NP 
 
In Chinese, verbs can function as modifier or head 
in a noun phrase without any change of forms. For 
example, in ???/fruit tree ??/growing?, ??
?? is the head while in ??? /growing ??
/technique?, ???? is a modifier. However, in 
such constructions, there are strong constraints on 
the length of both verbs and nouns. Table 1 gives 
the distributions of the rhythm feature in the rule 
?NP -> N V?(?N? and ?V? represent noun and verb 
respectively)  in which the verb is the head and 
?NP -> V N? in which the verb is a modifier.
Table 1  Distribution of rhythm feature in NP with verb as modifier or head 
 [0,0] [0,1] [0,2] [1,0] [1,1] [1,2] [2,0] [2,1] [2,2] Total 
NP -> V N 0 4 0 0 1275 4 0 88 0 1371 
NP -> N V 13 10 0 401 2328 91 0 44 2 2889 
   
Table 2  Distribution of rhythm feature in NP and VP formed by ?V N? 
 [0,0] [0,1] [0,2] [1,0] [1,1] [1,2] [2,0] [2,1] [2,2] Total 
VP -> V N 826 640 49 80 1221 121 0 11 1 2777 
NP -> V N 13 10 0 401 2328 91 0 44 2 2889 
 
Table 3  Distribution of rhythm feature in phrases  formed by ?N V? sequence 
 [0,0] [0,1] [0,2] [1,0] [1,1] [1,2] [2,0] [2,1] [2,2] Total 
NP -> N V 0 4 0 0 1275 4 0 88 0 1371 
NC -> N V 384 578 42 1131 3718 143 90 435 15 6536 
S    -> N V 28 1 2 17 347 22 2 43 8 470 
 
Table 1 indicates that in both rules, the rhythm 
pattern [1,1], ie. ?bi-syllabic + bi-syllabic?, 
prevails. In the rule ?NP -> V N?, this pattern 
accounts for 93% among the nine possible patterns 
while in the rule ?NP -> N V?, this pattern 
accounts for 81%. We can also find that in both 
cases, [0,2] and [2,0]  are prohibited, that is to say, 
both verbs and nouns cannot be longer than two 
syllables.  
 
2.1.2  Case 2: Contrast between NP and VP 
formed by ?V N? sequence 
 
The sequence ?V N?(?verb + noun?) can constitute 
an NP or a VP. The rhythm patterns in the two 
types of phrases are significantly different, 
however, as shown in Table 2. We see that in the 
NP case, verbs are mainly bi-syllabic. The total 
number of examples with bi-syllabic verbs in NP is 
2820, accounting for 98% of all the cases. On the 
other hand, mono-syllabic verbs are less likely to 
appear in this position. The total number of 
examples with mono-syllabic verbs in NP is 23, 
accounting for only 0.8% of all the cases. That is to 
say, the likelihood of bi-syllabic verbs appearing in 
this syntactic position is 122 times the likelihood 
of mono-syllabic verbs. On the other hand, there is 
no big difference between bi-syllabic verbs and 
mono-syllabic verbs in the VP formed by ?V + N?.  
The ratios of bi-syllabic and mono-syllabic verbs 
in VP are 48 % and 55% respectively. The 
statistical facts tell us that for a ?verb + noun? 
sequence, if the verb is not bi-syllabic then it is 
very unlikely to be an NP. Figure 1 depicts more 
clearly the difference between NP and VP formed 
by ?V N? sequence in the distribution of rhythm 
feature. 
0
500
1000
1500
2000
2500
0,0 0,2 1,1 2,0 2,2
VP-> v n
NP-> v n
 
Figure 1   Distributions of rhythm feature in NP 
and VP formed by ?verb + noun? 
.  
2.1 3  Case 3: ?N  V? sequence 
 
An ?N V?(?noun + verb?) sequence can be mainly 
divided into three types by the dominating phrasal 
category:  
(1) NP(noun phrase), e.g.  ???/fruit tree ??
/growth?;  
(2) S(subject-verb construction), e.g. ? ? ?
/colored flag ??/flutter?;  
(3)NC(non-constituent), eg. ???/economy ??
/develop? in ???/China ?/DE ??/economy 
??/develop ?/DE ?/very ?/fast?. (?China?s  
economy develops very fast?) 
Table 3 gives the distribution of rhythm feature in 
the three types of cases. 
We see in Table 3, in rule ?NP -> N V?, that the 
verb cannot be mono-syllabic since the first row is 
0 in all the patterns in which verb is mono-
syllabic([0,0], [1,0],[2,0]). The ?bi-syllabic + bi-
syllabic? ([1,1]) pattern accounts for 93% 
(1275/1371) of the total number. Let?s look at the 
cases with mono-syllabic verbs in all the three 
types. The total number of such examples is 1652 
in the corpus (adding all the numbers in columns 
[0,0], [1,0] and [2,0] on the three rows). Among 
these 1652 cases, there is not one example in 
which the ?N V? is an NP. The sequence has a 
probability of 3%(47/1652) to be an S and 97 
%(1605/1652) of being an NC(non-constituent). 
 
2.2 Rhythm feature in complex phrases 
 
Just as we saw with two word simple phrases, the 
rhythm feature also has an effect on complex 
phrases where at least one component is a phrase, 
i.e. spanning over two words or more. For example, 
for the following fragment of a sentence:   
      ?/stride   ?/into   ??/the Three Gorges     
      ??/project    ??/gate 
   ?enter into the gate of the Three Gorges Project? 
according to PCFG, the parse as indicated in 
Figure 2 (a) is incorrectly assigned the greatest  
                                   
                                   VP 
                                              NP 
 
                              VP 
                                            NP 
                                       
       ?       ?      ??           ??          ?? 
(a) 
                             
                               VP 
 
                                                            NP 
 
       VP                       NP 
  
     ?         ?        ??         ??         ?? 
(b) 
Figure 2  (a) incorrect parse and 
     (b) correct parse 
 
probability but the correct parse is that given in 
Figure 2 (b). One major error in (a) is that it 
applies the rule ?NP-> VP N? (i.e. ??  ??  ?
? ? modifying ? ? ? ?). This rule has 216 
occurrences in the corpus, of which 168 times it 
contains a VP of 2 words, 30 times a VP of 3 
words and 18 times a VP of more than 3 words. 
These statistics indicate that this rule prefers to 
choose a short VP acting as the modifier of a noun, 
as in ?NP(VP( ? /grow  ? /grain) ?? /large 
family)? and ?NP(VP(?/learn ??/Lei Feng) ?
?/model)?. But in the example in Figure 2(a), the 
VP contains 3 words, so it is less likely to be a 
modifier in an NP.    
When a phrase works as a constituent in a larger 
phrase, its rhythm feature is defined as the number 
of words in it. Thus a phrase may take on one of 
the three values for the rhythm feature: (1) two 
words; (3) three words; and (3) more than three 
words. Similar to that in the simple phrases, we 
may use 0, 1, 2 to represent the three values 
respectively. Therefore, for every construction 
containing two constituents, its rhythm feature can 
be described by a 3?3 matrix uniformly. For 
example, in the examples for rule ?NP -> VP N? 
above, the feature value for  ?NP(VP(?/grow  ?
/grain) ??/large family)? is [0, 1] in which 0 
indicates the VP contains 2 words and 1 represents 
that the noun is bi-syllabic. The rule helps to 
interpret the meaning of the feature value, i.e. the 
value is for a word or a phrase. For example, for 
rule ?VP -> V N?, feature value [0, 1] means that 
the verb is mono-syllabic and the noun is bi-
syllabic, while for rule ?NP-> VP N?, feature [0,1] 
means that the VP contains two words and the 
noun is bi-syllabic. 
 
3   Content Chunk Parsing  
 
We have chosen the task of content chunk parsing 
to test the usefulness of our rhythm feature to 
Chinese text. In this section we address two 
questions: (1) What is a content chunk? (2) Why 
are we interested in content chunk parsing? 
A content chunk is a phrase formed by a sequence 
of content words, including nouns, verbs, 
adjectives and content adverbs. There are three 
kinds of cases for the mapping between content 
word sequences and content chunks: 
(1) A content word sequence is a content chunk. A 
special case of this is that a whole sentence is a 
content chunk when all the words in it are content 
words, eg. [[??/Prospect  ??/company]NP [?
?/release [??/advanced [??/computer [??
/typesetting ? ? /system]NP]NP]NP]VP 
(?Prospect Company released an advanced 
computer typesetting system.?). 
 (2) A content word sequence is not a content 
chunk. For example, in ???/China ?/AUX ??
/economy ? ? /develop ? /AUX ? /very ?
/fast?(?China?s economy develops very fast.?), ??
? /economy ?? /develop? is a content word 
sequence, but it?s not a phrase in the sentence.  
(3) A part of a content word sequence is a content 
chunk. For example, in ? ? ? /private ? ?
/economy ?? /develop ? /AUX ?? /trend ?
/very ? /good?(?The developmental trend of 
private economy is very good.?), ???/private ?
? /economy ?? /develop? is a content word 
sequence, but it?s not a phrase; only ???/private 
??/economy? in it is a phrase.  
The purpose of content chunk parsing is to 
recognize phrases in a sequence of content words. 
Specifically speaking, the content chunking 
contains two subtasks: (1) to recognize the 
maximum phrase in a sequence of content words; 
(2) to analyze the hierarchical structure within the 
phrase down to words. Like baseNP 
chunking(Church, 1988; Ramshaw & Marcus 
1995), content chunk parsing is also a kind of 
shallow parsing. Content chunk parsing is deeper 
than baseNP chunking in two aspects: (1) a content 
chunk may contain verb phrases and other phrases 
even a full sentence as long as the all the 
components are content words; (2) it may contain 
recursive NPs. Thus the content chunk can supply 
more structural information than a baseNP.  
The motives for content chunk parsing are two-
fold: (1) Like other shallow parsing tasks, it can 
simplify the parsing task. This can be explained in 
two aspects. First, it can avoid the ambiguities 
brought up by functional words. In Chinese, the 
most salient syntactic ambiguities are prepositional 
phrases and the ?DE? construction. For 
prepositional phrases, the difficulty lies in how to 
determine the right boundary, because almost any 
constituent can be the object of a preposition. For 
?DE? constructions, the problem is how to 
determine its left boundary, since almost any 
constituent can be followed by ?DE? to form a 
?DE? construction. Second, content chunk parsing 
can simplify the structure of a sentence. When a 
content chunk is acquired, it can be replaced by its 
head word, thus reducing the length of the original 
sentence. If we get a parse from the reduced 
sentence with a full parser, then we can get a parse 
for the original sentence by replacing the head-
word nodes with the content chunks from which 
the head-words are extracted. (2) The content 
chunk parsing may be useful for applications like 
information extraction and question answering. 
When using template matching, a content chunk 
may be just the correct level of shallow structure 
for matching with an element in a template.  
 
4   PCFG + PF Model 
 
In the experiment we propose a statistical model 
integrating probabilistic context-free grammar 
(PCFG) model with a simple probabilistic features 
(PF) model. In this section we first give the 
definition for the statistical model and then we will 
give the method for parameter estimation. 
 
4.1   Definition 
 
According to PCFG, each rule r used to expand a 
node n in a parse is assigned a probability, i.e.: 
      )|())(( APnrP ?=                                            (1) 
where A -> ?  is a CFG rule. The probability of a 
parse T is the product of each rule used to expand 
each node n in T: 
     ?
?
=
Tn
nrPSTP ))(()|(                                 (2) 
We expand PCFG by the way that when a left 
hand side category A is expanded into a string ?, a 
feature set FS related to ? is also generated. Thus, a 
probability is assigned for expansion of each node 
n when a rule r is applied: 
)|,())(( AFSPnrP ?=                         (3) 
where A -> ?  is a CFG rule and FS is a feature set 
related to ?. From Equation (3) we get: 
 )|(*),|())(( APAFSPnrP ??=     (4) 
where P(FS| ?, A) is probabilistic feature(PF) 
model and P(? | A) is PCFG model. PF model 
describes the probability of each feature in feature 
set FS taking on specific values when a CFG rule 
A -> ?  is given. To make the model more practical 
in parameter estimation, we assume the features in 
feature set FS are independent from each other, 
thus: 
?
?
=
FSFi
AFiPAFSP ),|(),|( ??                 (5) 
Under this PCFG+PF model, the goal of a parser 
is to choose a parse that maximizes the following 
score: 
    )|,(maxarg)|(
1
AFS iii
n
iT
PSTScore ??
=
=      (6) 
  Our model is thus a simplification of more 
sophisticated models which integrate PCFGs with 
features, such as those in Magerman(1995), 
Collins(1997) and Goodman(1997). Compared 
with these models, our model is more practical 
when only small training data is available, since 
we assume the independence between features. For 
example, in Goodman?s probabilistic feature 
grammar (PFG), each symbol in a PCFG is 
replaced by a set of features, so it can describe 
specific constraints on the rule. In the PFG model 
the generation of each feature is dependent on all 
the previously generated features, thus likely 
leading to severe sparse data problem in parameter 
estimation. Our simplified model assumes 
independence between the features, thus data 
sparseness problem can be significantly alleviated.  
 
4.2  Parameter Estimation 
 
Let F be a feature associated with a string ?, where 
the possible values for F are f1,f2,?,fn, E is the set 
of observations of rule A ? ? in the training 
corpus, and thus E can be divided into n disjoint 
subsets: E1,E2,?,En, corresponding to f1,f2,?,fn 
respectively. The probability of F taking on a value 
of fi given A ? ? can be estimated as follows, 
according to MLE: 
E
E
AfFP ii == ),|( ?                     (7) 
This indicates that feature F adds constraints on 
CFG rule A ? ? by dividing ?, the state space of 
A ? ?, into n disjoint subspaces ?1, ?2,?, ?n, and 
each case of F taking a value of fi given A ? ? is 
viewed as a random event.  
 
5  Experimental Results 
 
5.1 Training and Test Data  
 
A Chinese corpus of 200K words extracted from 
the People?s Daily are segmented, POS-tagged and 
hand-labeled with content chunks in which all the 
trees are binary. The corpus is divided into two 
parts: (1) 180K for training set and (2) 20K for test 
set.  
 
5.2 Metrics and results 
 
We take two kinds of criteria to measure the 
system?s performance: labeled and unlabeled. 
According to the labeled criterion, a recognized 
phrase is correct only if a phrase with the same 
starting position, ending position and the same 
label is found in the gold standard. According to 
the unlabeled criterion, a recognized phrase is 
correct as long as a phrase with the same starting 
position and ending position is found in the gold 
standard.  
 
Table 4   Experimental  Results  
 Labeled Unlabeled 
 P R F P R F 
PCFG 49.91 64.96 56.45 53.33 80.73 65.66 
PCFG+RF in simple phrases 53.25 68.46 59.90 57.46    81.21 67.30 
PCFG +RF in all the phrases 56.47 72.08 63.33 60.07 83.57 69.90 
 
Table 5    Effect of rhythm feature on structural disambiguation 
   Word sequence Rule P(?|A) RF P(RF=[0,1] 
|A,?) 
P(RF=(0,1),?)|A) 
?     ?? 
country  sacrifice 
NC? N V  0.120273 [0,1] 0.08843   0.010636 
?     ?? S   ?  N V 0.161679 [0,1] 0.00292   0.000344 
?    ?? NP? N V 0.063159 [0,1] 0.00213   0.000184 
?    ?? V  ? N V 0.011573 [0,1] 0.0   0.0 
 
Within each criterion, precision, recall and F-
measure are given as metrics for the system?s 
performance. Precision represents how many 
phrases are correct among the phrases recognized, 
recall represents how many phrases in the gold 
standard are correctly recognized, and F-measure 
is defined as follows: 
callecision
callecisionmeasureF
RePr
2RePr
+
??=?  
Table 4 gives the experimental results in three 
different conditions: the first row gives the result 
of PCFG model; the second row gives the result of 
PCFG model integrated with rhythm feature model 
(RF) where only the features of simple phrases are 
considered; the last row gives the result of PCFG 
model plus RF where the rhythm features in all the 
phrases are considered. The results indicate that 
the rhythm features in both simple and complex 
phrases contribute to the improvement of 
performance over PCFG model. We see that the 
rhythm feature improves the labeled F-measure 
6.88 percent and the unlabeled F-measure 4.24 
percent over the unaugmented PCFG model.  
 
5.3 Effect of rhythm feature on parsing 
 
The experiment shows that the rhythm feature can 
help the performance of a parser in Chinese. 
Specifically, the effects of rhythm feature on 
parsing are shown in two ways: 
(1) Help for the disambiguation of phrasal type.  
Table 5 shows the difference of the results 
between PCFG model and PCFG + RF model for 
the sequence ??/country ??/sacrifice? in the 
sentence ?? /the ? /school ? /have 900 ??
/students ?/for ?/country ??/sacrifice? (`900 
students from this school gave their lives for their 
country?). 
In the sentence above, ??/country? is the object 
of preposition ? ? /for?, ? ? /country ? ?
/sacrifice? is not a constituent. But the 
unaugmented PCFG model incorrectly parses it as 
a S(subject-predicate construction). Contrarily, 
according to PCFG+RF model, the type with 
greatest probability is the (correct) NC(non-
constituent) parse. 
(2) Help for pruning.  
Let?s give an example to explain it. For the 
sentence ??? /solve ?? /resident ? /eat ?
/vegetable ? ? /problem ? ? /very ? ?
/difficult?(?It?s very difficult to solve the vegetable 
problem for the residents.?), the number of edges 
generated by the PCFG is 1236, but the number 
decreases to 348 after the rhythm feature is applied, 
thus pruning 73% of the edges. As indicated in 
Table 1, in the rule ?NP -> N V?, P(RF = [1,0] ) = 
0, so ?[??/N ?/V]NP? is pruned after adding 
RF. Similarly, in rule ?NP -> V N?, P(RF = [0, 1] ) 
= 0.003, so ?[?/V ?/N]NP? is pruned since it has 
very low probability. With these two edges pruned, 
more potential edges containing them will not be 
generated. 
 
6 Conclusion 
 
In this paper, we systematically survey the 
distribution of rhythm (number of syllables per 
word or numbers of words per phrase for a 
constituent) in different constructions in Chinese. 
Our analysis suggests that rhythm places strong 
constraints on Chinese syntax. Based on this 
observation, we used the rhythm feature in a 
practical shallow parsing task in which a PCFG 
model is augmented with a probabilistic 
representation of the rhythm feature. The 
experimental results show that the probabilistic 
rhythm feature aids in disambiguation in Chinese 
and thus helps to improve the performance of a 
Chinese parser. We can expect that the 
performance of the parser may further improve 
when more features are considered under the 
probabilistic feature (PF) model. 
 
Acknowledgments 
 
This research was partially supported by the NSF, 
via a KDD extension to NSF IIS-9978025.  
 
References 
 
Church,,K.,1988.A stochastic parts program and 
noun phrase parser for unrestricted text. In 
Proceedings of the Second Conference on 
Applied Natural Language Processing, pp.136-
143.  
 
Collins, M. 1997. Three generative lexicalized 
models for statistical parsing, in Proceedings of 
the 35th Annual Meeting of the ACL, pp. 16-23. 
 
Feng, Shengli. 2000. The Rhythmic syntax of 
Chinese(in Chinese), Shanghai Education Press. 
 
Goodman, J. 1997. Probabilistic Feature 
Grammars, In Proceedings of the International 
Workshop on Parsing Technologies, September 
1997 
 
Magerman, D. 1995. Statistical decision-tree 
models for parsing, in Proceedings of the 33rd 
Annual Meeting of the Association for 
Computational Linguistics, pp.276-283. 
 
Quirk et al 1985. A Comprehensive Grammar of 
English Languge, Longman. 
 
Ramshaw L., and Marcus M. 1995. Text chunking 
using transformation-based learning. In Proc-
eedings of the Third Workshop on Very Large 
Corpora.pp.86-95.  
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1005?1014, Prague, June 2007. c?2007 Association for Computational Linguistics
Learning to Merge Word Senses
Rion Snow Sushant Prakash
Computer Science Department
Stanford University
Stanford, CA 94305 USA
{rion,sprakash}@cs.stanford.edu
Daniel Jurafsky
Linguistics Department
Stanford University
Stanford, CA 94305 USA
jurafsky@stanford.edu
Andrew Y. Ng
Computer Science Department
Stanford University
Stanford, CA 94305 USA
ang@cs.stanford.edu
Abstract
It has been widely observed that different NLP appli-
cations require different sense granularities in order to
best exploit word sense distinctions, and that for many
applications WordNet senses are too fine-grained. In
contrast to previously proposed automatic methods for
sense clustering, we formulate sense merging as a su-
pervised learning problem, exploiting human-labeled
sense clusterings as training data. We train a discrimi-
native classifier over a wide variety of features derived
from WordNet structure, corpus-based evidence, and
evidence from other lexical resources. Our learned
similarity measure outperforms previously proposed
automatic methods for sense clustering on the task of
predicting human sense merging judgments, yielding
an absolute F-score improvement of 4.1% on nouns,
13.6% on verbs, and 4.0% on adjectives. Finally, we
propose a model for clustering sense taxonomies us-
ing the outputs of our classifier, and we make avail-
able several automatically sense-clustered WordNets
of various sense granularities.
1 Introduction
Defining a discrete inventory of senses for a word is
extremely difficult (Kilgarriff, 1997; Hanks, 2000;
Palmer et al, 2005). Perhaps the greatest obstacle is
the dynamic nature of sense definition: the correct
granularity for word senses depends on the appli-
cation. For language learners, a fine-grained set of
word senses may help in learning subtle distinctions,
while coarsely-defined senses are probably more
useful in NLP tasks like information retrieval (Gon-
zalo et al, 1998), query expansion (Moldovan and
Mihalcea, 2000), and WSD (Resnik and Yarowsky,
1999; Palmer et al, 2005).
Lexical resources such as WordNet (Fellbaum,
1998) use extremely fine-grained notions of word
sense, which carefully capture even minor distinc-
tions between different possible word senses (e.g.,
the 8 noun senses of bass shown in Figure 1). Pro-
ducing sense-clustered inventories of arbitrary sense
granularity is thus crucial for tasks which depend on
lexical resources like WordNet, and is also impor-
tant for the task of automatically constructing new
WordNet-like taxonomies. A solution to this prob-
lem must also deal with the constraints of the Word-
Net taxonomy itself; for example when clustering
two senses, we need to consider the transitive effects
of merging synsets.
The state of the art in sense clustering is insuffi-
cient to meet these needs. Current sense clustering
algorithms are generally unsupervised, each relying
on a different set of useful features or hand-built
rules. But hand-written rules have little flexibility
to produce clusterings of different granularities, and
previously proposed methods offer little in the di-
rection of intelligently combining and weighting the
many proposed features.
In response to these challenges, we propose a
new algorithm for clustering large-scale sense hier-
archies like WordNet. Our algorithm is based on a
supervised classifier that learns to make graduated
judgments corresponding to the estimated probabil-
ity that each particular sense pair should be merged.
This classifier is trained on gold standard sense clus-
tering judgments using a diverse feature space. We
are able to use the outputs of our classifier to produce
a ranked list of sense merge judgments by merge
probability, and from this create sense-clustered in-
ventories of arbitrary sense granularity.1
In Section 2 we discuss past work in sense cluster-
1We have made sense-clustered Wordnets using the al-
gorithms discussed in this paper available for download at
http://ai.stanford.edu/?rion/swn.
1005
INSTRUMENT 7: ...the lowest range of a family of musical instruments
FISH
4: the lean flesh of a saltwater fish of the family Serranidae
5: any of various North American freshwater fish with lean flesh
8: nontechnical name for any of numerous... fishes
SINGER
3: an adult male singer with the lowest voice
6: the lowest adult male singing voice
PITCH
1: the lowest part of the musical range
2: the lowest part in polyphonic music
Figure 1: Sense clusters for the noun bass; the eight
WordNet senses as clustered into four groups in the
SENSEVAL-2 coarse-grained evaluation data
ing, and the gold standard datasets that we use in our
work. In Section 3 we introduce our battery of fea-
tures; in Section 4 we show how to extend our sense-
merging model to cluster full taxonomies like Word-
Net. In Section 5 we evaluate our classifier against
thirteen previously proposed methods.
2 Background
A wide number of manual and automatic techniques
have been proposed for clustering sense inventories
and mapping between sense inventories of different
granularities. Much work has gone into methods for
measuring synset similarity; early work in this direc-
tion includes (Dolan, 1994), which attempted to dis-
cover sense similarities between dictionary senses.
A variety of synset similarity measures based on
properties of WordNet itself have been proposed;
nine such measures are discussed in (Pedersen et al,
2004), including gloss-based heuristics (Lesk, 1986;
Banerjee and Pedersen, 2003), information-content
based measures (Resnik, 1995; Lin, 1998; Jiang and
Conrath, 1997), and others. Other approaches have
used specific cues from WordNet structure to inform
the construction of semantic rules; for example, (Pe-
ters et al, 1998) suggest clustering two senses based
on a wide variety of structural cues from Word-
Net, including if they are twins (if two synsets share
more than one word in their synonym list) or if
they represent an example of autohyponymy (if one
sense is the direct descendant of the other). (Mihal-
cea and Moldovan, 2001) implements six semantic
rules, using twin and autohyponym features, in addi-
tion to other WordNet-structure-based rules such as
whether two synsets share a pertainym, antonym, or
are clustered together in the same verb group.
A large body of work has attempted to capture
corpus-based estimates of word similarity (Pereira
et al, 1993; Lin, 1998); however, the lack of
large sense-tagged corpora prevent most such tech-
niques from being used effectively to compare dif-
ferent senses of the same word. Some corpus-based
attempts that are capable of estimating similarity
between word senses include the topic signatures
method; here, (Agirre and Lopez, 2003) collect con-
texts for a polysemous word based either on sense-
tagged corpora or by using a weighted agglomera-
tion of contexts of a polysemous word?s monose-
mous relatives (i.e., single-sense synsets related by
hypernym, hyponym, or other relations) from some
large untagged corpus. Other corpus-based tech-
niques developed specifically for sense clustering
include (McCarthy, 2006), which uses a combina-
tion of word-to-word distributional similarity com-
bined with the JCN WordNet-based similarity mea-
sure, and work by (Chugur et al, 2002) in find-
ing co-occurrences of senses within documents in
sense-tagged corpora. Other attempts have exploited
disagreements between WSD systems (Agirre and
Lopez, 2003) or between human labelers (Chklovski
and Mihalcea, 2003) to create synset similarity
measures; while promising, these techniques are
severely limited by the performance of the WSD
systems or the amount of available labeled data.
Some approaches for clustering have made use of
regular patterns of polysemy among words. (Pe-
ters et al, 1998) uses the COUSIN relation defined
in WordNet 1.5 to cluster hyponyms of categorically
related noun synsets, e.g., ?container/quantity? (e.g.,
for clustering senses of ?cup? or ?barrel?) or ?or-
ganization/construction? (e.g., for the building and
institution senses of ?hospital? or ?school?); other
approaches based on systematic polysemy include
the hand-constructed CORELEX database (Buite-
laar, 1998), and automatic attempts to extract pat-
terns of systematic polysemy based on minimal de-
scription length principles (Tomuro, 2001).
Another family of approaches has been to
use either manually-annotated or automatically-
constructed mappings to coarser-grained sense in-
ventories; an attempt at providing coarse-grained
sense distinctions for the SENSEVAL-1 exercise in-
cluded a mapping between WordNet and the Hec-
tor lexicon (Palmer et al, 2005). Other attempts in
1006
this vein include mappings between WordNet and
PropBank (Palmer et al, 2004) and mappings to
Levin classes (Levin, 1993; Palmer et al, 2005).
(Navigli, 2006) presents an automatic approach for
mapping between sense inventories; here similari-
ties in gloss definition and structured relations be-
tween the two sense inventories are exploited in or-
der to map between WordNet senses and distinc-
tions made within the coarser-grained Oxford En-
glish Dictionary. Other work has attempted to ex-
ploit translational equivalences of WordNet senses
in other languages, for example using foreign lan-
guage WordNet interlingual indexes (Gonzalo et al,
1998; Chugur et al, 2002).
2.1 Gold standard sense clustering data
Our approach for learning how to merge senses
relies upon the availability of labeled judgments
of sense relatedness. In this work we focus on
two datasets of hand-labeled sense groupings for
WordNet: first, a dataset of sense groupings over
nouns, verbs, and adjectives provided as part of
the SENSEVAL-2 English lexical sample WSD task
(Kilgarriff, 2001), and second, a corpus-driven map-
ping of nouns and verbs in WordNet 2.1 to the
Omega Ontology (Philpot et al, 2005), produced as
part of the ONTONOTES project (Hovy et al, 2006).
A wide variety of semantic and syntactic criteria
were used to produce the SENSEVAL-2 groupings
(Palmer et al, 2004; Palmer et al, 2005); this data
covers all senses of 411 nouns, 519 verbs, and 257
adjectives, and has been used as gold standard sense
clustering data in previous work (Agirre and Lopez,
2003; McCarthy, 2006)2. The number of judgments
within this data (after mapping to WordNet 2.1) is
displayed in Table 1.
Due to a lack of interannotator agreement data for
this dataset, (McCarthy, 2006) performed an anno-
tation study using three labelers on a 20-noun sub-
set of the SENSEVAL-2 groupings; the three label-
ers were given the task of deciding whether the 351
potentially-related sense pairs were ?Related?, ?Un-
related?, or ?Don?t Know?.3 In this task the pair-
2In order to facilitate future work in this area, we
have made cleaned versions of these groupings available at
http://ai.stanford.edu/?rion/swn along with a ?diff? with the
original files.
3McCarthy?s gold standard data is available at
SENSEVAL-2
POS Total Pairs Merged Pairs Proportion
Nouns 16403 2593 0.1581
Verbs 30688 3373 0.1099
Adjectives 8368 2209 0.2640
ONTONOTES
POS Total Pairs Merged Pairs Proportion
Nouns 3552 347 0.0977
Verbs 4663 1225 0.2627
Table 1: Gold standard datasets for sense merging;
only sense pairs that share a word in common are
included; proportion refers to the fraction of synsets
sharing a word that have been merged
POS Overlap ON-True ON-False F-Score
S-T S-F S-T S-F
Nouns 2116 121 55 181 1759 0.5063
Verbs 3297 351 503 179 2264 0.5072
Table 2: Agreement data for gold standard datasets
wise interannotator F-scores were (0.4874, 0.5454,
0.7926), for an average F-score of 0.6084.
The ONTONOTES dataset4 covers a smaller set
of nouns and verbs, but it has been created with a
more rigorous corpus-based iterative annotation pro-
cess. For each of the nouns and verbs in question, a
50-sentence sample of instances is annotated using
a preliminary set of sense distinctions; if the word
sense interannotator agreement for the sample is less
than 90%, then the sense distinctions are revised and
the sample is re-annotated, and so forth, until an in-
terannotator agreement of at least 90% is reached.
We construct a combined gold standard set from
these SENSEVAL-2 and ONTONOTES groupings,
removing disagreements. The overlap and agree-
ment/disagreement data between the two groupings
is given in Table 2; here, for example, the column
with ON-True and S-F indicates the count of senses
that ONTONOTES judged as positive examples of
sense merging, but that SENSEVAL-2 data did not
merge. We also calculate the F-score achieved by
considering only one of the datasets as a gold stan-
dard, and computing precision and recall for the
other. Since the two datasets were created indepen-
dently, with different annotation guidelines, we can-
ftp://ftp.informatics.susx.ac.uk/pub/users/dianam/relateGS/.
4The OntoNotes groupings will be available through the
LDC at http://www.ldc.upenn.edu.
1007
not consider this as a valid estimate of interannota-
tor agreement; nonetheless the F-score for the two
datasets on the overlapping set of sense judgments
(50.6% for nouns and 50.7% for verbs) is roughly
in the same range as those observed in (McCarthy,
2006).
3 Learning to merge word senses
3.1 WordNet-based features
Here we describe the feature space we construct for
classifying whether or not a pair of synsets should be
merged; first, we employ a wide variety of linguistic
features based on information derived from Word-
Net. We use eight similarity measures implemented
within the WordNet::Similarity package5, described
in (Pedersen et al, 2004); these include three mea-
sures derived from the paths between the synsets
in WordNet: HSO (Hirst and St-Onge, 1998), LCH
(Leacock and Chodorow, 1998), and WUP (Wu and
Palmer, 1994); three measures based on information
content: RES (Resnik, 1995), LIN (Lin, 1998), and
JCN (Jiang and Conrath, 1997); the gloss-based Ex-
tended Lesk Measure LESK, (Banerjee and Peder-
sen, 2003), and finally the gloss vector similarity
measure VECTOR (Patwardan, 2003). We imple-
ment the TWIN feature (Peters et al, 1998), which
counts the number of shared synonyms between
the two synsets. Additionally we produce pair-
wise features indicating whether two senses share an
ANTONYM, PERTAINYM, or derivationally-related
forms (DERIV). We also create the verb-specific
features of whether two verb synsets are linked in
a VERBGROUP (indicating semantic similarity) or
share a VERBFRAME, indicating syntactic similar-
ity. Also, we encode a generalized notion of sib-
linghood in the MN features, recording the distance
of the synset pair?s nearest least common subsumer
(i.e., closest shared hypernym) from the two synsets,
and, separately, the maximum of those distances (in
the MAXMN feature.
Previous attempts at categorizing systematic pol-
ysemy patterns within WordNet has resulted in the
COUSIN feature6; we create binary features which
5We choose not to use the PATH measure due to its negligible
difference from the LCH measure.
6This data is included in the WordNet 1.6 distribution as the
?cousin.tops? file.
indicate whether a synset pair belong to hypernym
ancestries indicated by one or more of these COUSIN
features, and the specific cousin pair(s) involved.
Finally we create sense-specific features, including
SENSECOUNT, the total number of senses associ-
ated with the shared word between the two synsets
with the highest number of senses, and SENSENUM,
the specific pairing of senses for the shared word
with the highest number of senses (which might al-
low us to learn whether the most frequent sense of a
word has a higher chance of having similar deriva-
tive senses with lower frequency).
3.2 Features derived from corpora and other
lexical resources
In addition to WordNet-based features, we use
a number of features derived from corpora and
other lexical resources. We use the publicly avail-
able topic signature data7 described in (Agirre and
Lopez, 2004), yielding representative contexts for
all nominal synsets from WordNet 1.6. These topic
signatures were obtained by weighting the contexts
of monosemous relatives of each noun synset (i.e.,
single-sense synsets related by hypernym, hyponym,
or other relations); the text for these contexts were
extracted from snippets using the Google search en-
gine. We then create a sense similarity feature by
taking a thresholded cosine similarity between pairs
of topic signatures for these noun synsets.
Additionally, we use the WordNet domain dataset
described in (Magnini and Cavaglia, 2000; Ben-
tivogli et al, 2004). This dataset contains one or
more labels indicating of 164 hierarchically orga-
nized ?domains? or ?subject fields? for each noun,
verb, and adjective synset in WordNet; we derive a
set of binary features from this data, with a single
feature indicating whether or not two synsets share
a domain, and one indicator feature per pair of do-
mains indicating respective membership of the sense
pair within those domains.
Finally, we use as a feature the mappings pro-
duced in (Navigli, 2006) of WordNet senses to Ox-
ford English Dictionary senses. This OED dataset
was used as the coarse-grained sense inventory in the
Coarse-grained English all-words task of SemEval-
7The topic signature data is available for download at
http://ixa.si.ehu.es/Ixa/resources/sensecorpus.
1008
20078; we specify a single binary feature for each
pair of synsets from this data; this feature is true if
the words are clustered in the OED mapping, and
false otherwise.
3.3 Classifier, training, and feature selection
For each part of speech, we split the merged gold
standard data into a part-of-speech-specific train-
ing set (70%) and a held-out test set (30%). For
every synset pair we use the binary ?merged? or
?not-merged? labels to train a support vector ma-
chine classifier9 (Joachims, 2002) for each POS-
specific training set. We perform feature selection
and regularization parameter optimization using 10-
fold cross-validation.
4 Clustering Senses in WordNet
The previous section describes a classifier which
predicts whether two synsets should be merged; we
would like to use the pairwise judgments of this
classifier to cluster the senses within a sense hierar-
chy. In this section we present the challenge implicit
in applying sense merging to full taxonomies, and
present our model for clustering within a taxonomy.
4.1 Challenges of clustering a sense taxonomy
The task of clustering a sense taxonomy presents
certain challenges not present in the problem of clus-
tering the senses of a word; in order to create a
consistent clustering of a sense hierarchy an algo-
rithm must consider the transitive effects of merging
synsets. This problem is compounded in sense tax-
onomies like WordNet, where each synset may have
additional structured relations, e.g., hypernym (IS-
A) or holonym (is-part-of) links. In order to consis-
tently merge two noun senses with different hyper-
nym ancestries within WordNet, for example, an al-
gorithm must decide whether to have the new sense
inherit both hypernym ancestries, or whether to in-
herit only one, and if so it must decide which ances-
try is more relevant for the merged sense.
Without strict checking, human labelers will
likely find it difficult to label a sense inventory with
8http://lcl.di.uniroma1.it/coarse-grained-aw/index.html
9We use the SV Mperf package, freely available for non-
commercial use from http://svmlight.joachims.org; we use the
default settings in v2.00, except for the regularization parameter
(set in 10-fold cross-validation).
Clusering based on ??need??
Clustering based on ??require??
need#v#1
require#v#1 require as useful, just, or proper
need#v#2
require#v#4 have need of
need#v#3 have or feel a need for
require#v#1
need#v#1 require as useful, just, or proper
require#v#4
need#v#2 have need of
require#v#2 consider obligatory; request and expect
require#v#3 make someone do something
Figure 2: Inconsistent sense clusters for the verbs
require and need from SENSEVAL-2 judgments
transitively-consistent judgments. As an example,
consider the SENSEVAL-2 clusterings of the verbs
require and need, as shown in Figure 2. In WN 2.1
require has four verb senses, of which the first has
synonyms {necessitate, ask, postulate, need, take,
involve, call for, demand}, and gloss ?require as use-
ful, just, or proper?; and the fourth has synonyms
{want, need}, and gloss ?have need of.?
Within the word require, the SENSEVAL-2 dataset
clusters senses 1 and 4, leaving the rest unclustered.
In order to make a consistent clustering with respect
to the sense inventory, however, we must enforce
the transitive closure by merging the synset corre-
sponding to the first sense (necessitate, ask, need
etc.), with the senses of want and need in the fourth
sense. In particular, these two senses correspond
to WordNet 2.1 senses need#v#1 and need#v#2, re-
spectively, which are not clustered according to
the SENSEVAL-2 word-specific labeling for need ?
need#v#1 is listed as a singleton (i.e., unclustered)
sense, though need#v#2 is clustered with need#v#3,
?have or feel a need for.?
While one might hope that such disagreements
between sense clusterings are rare, we found
178 such transitive closure disagreements in the
SENSEVAL-2 data. The ONTONOTES data is much
cleaner in this respect, most likely due to the
stricter annotation standard (Hovy et al, 2006);
we found only one transitive closure disagreement
1009
in the OntoNotes data, specifically WordNet 2.1
synsets (head#n#2, lead#n#7: ?be in charge of?) and
(head#n#3, lead#v#4: ?travel in front of?) are clus-
tered under head but not under lead.
4.2 Sense clustering within a taxonomy
As a solution to the previously mentioned chal-
lenges, in order to produce taxonomies of different
sense granularities with consistent sense distinctions
we propose to apply agglomerative clustering over
all synsets in WordNet 2.1. While one might con-
sider recalculating synset similarity features after
each synset merge operation, depending on the fea-
ture set this could be prohibitively expensive; for our
purposes we use average-link agglomerative cluster-
ing, in effect approximating the the pairwise similar-
ity score between a given synset and a merged sense
as the average of the similarity scores between the
given synset and the clustered sense?s component
synsets. Further, for the purpose of sense cluster-
ing we assume a zero sense similarity score between
synsets with no intersecting words.
Without exploiting additional hypernym or
coordinate-term evidence, our algorithm does
not distinguish between judgments about which
hypernym ancestry or other structured relationships
to keep or remove upon merging two synsets. In
lieu of additional evidence, for our experiments
we choose to retain only the hypernym ancestry of
the sense with the highest frequency in SEMCOR,
breaking frequency ties by choosing the first-listed
sense in WordNet. We add every other relationship
(meronyms, entailments, etc.) to the new merged
sense (except in the rare case where adding a
relation would cause a cycle in acyclic relations like
hypernymy or holonymy, in which case we omit
it). Using this clustering method we have produced
several sense-clustered WordNets of varying sense
granularity, which we evaluate in Section 5.3.
5 Evaluation
We evaluate our classifier in a comparison with thir-
teen previously proposed similarity measures and
automatic methods for sense clustering. We conduct
a feature ablation study to explore the relevance of
the different features in our system. Finally, we eval-
uate the sense-clustered taxonomies we create on
the problem of providing improved coarse-grained
sense distinctions for WSD evaluation.
5.1 Evaluation of automatic sense merging
We evaluate our classifier on two held-out test
sets; first, a 30% sample of the sense judgments
from the merged gold standard dataset consisting
of both the SENSEVAL-2 and ONTONOTES sense
judgments; and, second, a test set consisting of only
the ONTONOTES subset of our first held-out test set.
For comparison we implement thirteen of the meth-
ods discussed in Section 2. First, we evaluate each
of the eight WordNet::Similarity measures individu-
ally. Next, we implement cosine similarity of topic
signatures (TOPSIG) built from monosemous rela-
tives (Agirre and Lopez, 2003), which provides a
real-valued similarity score for noun synset pairs.
Additionally, we implement the two methods
proposed in (Peters et al, 1998), namely using
metonymy clusters (MetClust) and generalization
clusters (GenClust) based on the COUSIN relation-
ship in WordNet. While (Peters et al, 1998) only
considers four cousin pairs, we re-implement their
method for general purpose sense clustering by us-
ing all 226 cousin pairs defined in WordNet 1.6,
mapped to WordNet 2.1 synsets. These methods
each provide a single clustering of noun synsets.
Next, we implement the set of semantic rules de-
scribed in (Mihalcea and Moldovan, 2001) (MIMO);
this algorithm for merging senses is based on 6 se-
mantic rules, in effect using a subset of the TWIN,
MAXMN, PERTAINYM, ANTONYM, and VERB-
GROUP features; in our implementation we set the
parameter for when to cluster based on number of
twins to K = 2; this results in a single clustering
for each of nouns, verbs, and adjectives. Finally, we
compare against the mapping from WordNet to the
Oxford English Dictionary constructed in (Navigli,
2006), equivalent to clustering based solely on the
OED feature.
Considering merging senses as a binary classifi-
cation task, Table 3 gives the F-score performance
of our classifier vs. the thirteen other classifiers and
an uninformed ?merge all synsets? baseline on our
held-out gold standard test set. This table shows that
our SVM classifier outperforms all implemented
methods on the basis of F-score on both datasets
1010
SENSEVAL-2 + ONTONOTES
ONTONOTES
Method Nouns Verbs Adj Nouns Verbs
SVM 0.4228 0.4319 0.4727 0.3698 0.4545
RES 0.3817 0.2703 ? 0.2807 0.3156
WUP 0.3763 0.2782 ? 0.3036 0.3451
LCH 0.3700 0.2440 ? 0.2857 0.3396
OED 0.3310 0.2878 0.3712 0.2183 0.3962
LESK 0.3174 0.2956 0.4323 0.2914 0.3774
HSO 0.3090 0.2784 0.4312 0.3025 0.3156
TOPSIG 0.3072 ? ? 0.2581 ?
VEC 0.2960 0.2315 0.4321 0.2454 0.3420
JCN 0.2818 0.2292 ? 0.2222 0.3156
LIN 0.2759 0.2464 ? 0.2056 0.3471
Baseline 0.2587 0.2072 0.4312 0.1488 0.3156
MIMO 0.0989 0.2142 0.0759 0.1833 0.2157
GenClust 0.0973 ? ? 0.0264 ?
MetClust 0.0876 ? ? 0.0377 ?
Table 3: F-score sense merging evaluation on hand-
labeled testsets
for all parts of speech. In Figure 3 we give a pre-
cision/recall plot for noun sense merge judgments
for the SENSEVAL-2 + ONTONOTES dataset. For
sake of simplicity we plot only the two best mea-
sures (RES and WUP) of the eight WordNet-based
similarity measures; we see that our classifier, RES,
and WUP each have higher precision all levels of
recall compared to the other tested measures.
Of the methods we compare against, only the
WordNet-based similarity measures, (Mihalcea and
Moldovan, 2001), and (Navigli, 2006) provide a
method for predicting verb similarities; our learned
measure widely outperforms these methods, achiev-
ing a 13.6% F-score improvement over the LESK
similarity measure. In Figure 4 we give a pre-
cision/recall plot for verb sense merge judgments,
plotting the performance of the three best WordNet-
based similarity measures; here we see that our clas-
sifier has significantly higher precision than all other
tested measures at nearly every level of recall.
Only the measures provided by LESK, HSO,
VEC, (Mihalcea and Moldovan, 2001), and (Nav-
igli, 2006) provide a method for predicting adjective
similarities; of these, only LESK and VEC outper-
form the uninformed baseline on adjectives, while
our learned measure achieves a 4.0% improvement
over the LESK measure on adjectives.
5.2 Feature analysis
Next we analyze our feature space. Table 4 gives the
ablation analysis for all features used in our system
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pr
ec
is
io
n
Precision/Recall for Merging Nouns
SVM Classifier
Resnik Measure
Wu & Palmer Measure
Topic Signatures
OED Mapping
Generalization Clusters
Metonym Clusters
Semantic Rules
Figure 3: Precision/Recall plot for noun sense merge
judgments
as evaluated on our held-out test set; here the quan-
tity listed in the table is the F-score loss obtained by
removing that single feature from our feature space,
and retraining and retesting our classifiers, keeping
everything else the same. Here negative scores cor-
respond to an improvement in classifier performance
with the removal of the feature.
For noun classification, the three features that
yield the highest gain in testset F-score are the
topic signature, OED, and derivational link features,
yielding a 4.0%, 3.6%, and 3.5% gain, respectively.
For verb classification, we find that three features
yield more than a 5% F-score gain; by far the largest
single-feature performance gain for verb classifica-
tion found in our ablation study was the DERIV fea-
ture, i.e., the count of shared derivational links be-
tween the two synsets; this single feature improves
our maximum F-score by 9.8% on the testset. This
is a particularly interesting discovery, as none of the
referenced automatic techniques for sense clustering
presently make use of this very useful feature. We
also achieve large gains with the LIN and LESK sim-
ilarity features, with F-score improvement of 7.4%
and 5.4% gain respectively.
For adjective classification again the DERIV fea-
ture proved very helpful, with a 3.5% gain on the
testset. Interestingly, only the DERIV feature and
the SENSECNT features helped across all parts of
speech; in many cases a feature which proved to be
1011
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Recall
Pr
ec
is
io
n
Precision/Recall for Merging Verbs
SVM Classifier
Lesk Measure
Hirst & St?Onge
Wu & Palmer
OED Mapping
Semantic Rules
Figure 4: Precision/Recall plot for verb sense merge
judgments
very helpful for one part of speech actually hurt per-
formance on another part of speech (e.g., LIN on
nouns and OED on adjectives).
5.3 Evaluation of sense-clustered Wordnets
Our goal in clustering a sense taxonomy is to pro-
duce fully sense-clustered WordNets, and to be able
to produce coarse-grained Wordnets at many differ-
ent levels of resolution. In order to evaluate the en-
tire sense-clustered taxonomy, we have employed an
evaluation method inspired by Word Sense Disam-
biguation (this is similar to an evaluation used in
Navigli, 2006, however we do not remove monose-
mous clusters). Given past system responses in the
SENSEVAL-3 English all-words task, we can eval-
uate past systems on the same corpus, but using
the coarse-grained sense hierarchy provided by our
sense-clustered taxonomy. We may then compare
the scores of each system on the coarse-grained task
against their scores given a random clustering at the
same resolution. Our expectation is that, if our sense
clustering is much better than a random sense clus-
tering (and, of course, that the WSD algorithms per-
form better than random guessing), we will see a
marked improvement in the performance of WSD
algorithms using our coarse-grained sense hierarchy.
We consider the outputs of the top 3 all-
words WSD systems that participated in Senseval-3:
Gambl (Decadt et al, 2004), SenseLearner (Mihal-
cea and Faruque, 2004), and KOC University (Yuret,
Nouns Verbs Adjectives
F-SCORE 0.4228 0.4319 0.4727
Feature F-Score Ablation Difference
TOPSIG 0.0403 ? ?
OED 0.0355 0.0126 -0.0124
DERIV 0.0351 0.0977 0.0352
RES 0.0287 0.0147 ?
TWIN 0.0285 0.0109 -0.0130
MN 0.0188 0.0358 ?
LESK 0.0183 0.0541 -0.0250
SENSENUM 0.0155 0.0146 -0.0147
SENSECNT 0.0121 0.0160 0.0168
DOMAIN 0.0119 0.0082 -0.0265
LCH 0.0099 0.0068 ?
WUP 0.0036 0.0168 ?
JCN 0.0025 0.0190 ?
ANTONYM 0.0000 0.0295 0.0000
MAXMN -0.0013 0.0179 ?
VEC -0.0024 0.0371 -0.0062
HSO -0.0073 0.0112 -0.0246
LIN -0.0086 0.0742 ?
COUSIN -0.0094 ? ?
VERBGRP ? 0.0327 ?
VERBFRM ? 0.0102 ?
PERTAINYM ? ? -0.0029
Table 4: Feature ablation study; F-score difference
obtained by removal of the single feature
2004). A guess by a system is given full credit if it
was either the correct answer or if it was in the same
cluster as the correct answer.
Clearly any amount of clustering will only in-
crease WSD performance. Therefore, to account for
this natural improvement and consider only the ef-
fect of our particular clustering, we also calculate
the expected score for a random clustering of the
same granularity, as follows: Let C represent the set
of clusters over the possible N synsets containing a
given word; we then calculate the expectation that an
incorrectly-chosen sense and the actual correct sense
would be clustered together in the random clustering
as
P
c?C |c|(|c|?1)
N(N?1) .
Our sense clustering algorithm provides little im-
provement over random clustering when too few or
too many clusters are chosen; however, with an ap-
propriate threshold for average-link clustering we
find a maximum of 3.55% F-score improvement in
WSD over random clustering (averaged over the de-
cisions of the top 3 WSD algorithms). Table 5 shows
the improvement of the three top WSD algorithms
given a sense clustering created by our algorithm vs.
a random clustering at the same granularity.
1012
0 0.5 1 1.5 2 2.5 3 3.5
x 104
0.65
0.7
0.75
0.8
Sense Merge Iterations
W
SD
 F
?S
co
re
Group Average Agglomerative Clustering
Random Clustering
Figure 5: WSD Improvement with coarse-grained
sense hierarchies
System F-score Avg-link Random Impr.
Gambl 0.6516 0.7702 0.7346 0.0356
SenseLearner 0.6458 0.7536 0.7195 0.0341
KOC Univ. 0.6414 0.7521 0.7153 0.0368
Table 5: Improvement in SENSEVAL-3 WSD perfor-
mance using our average-link agglomerative cluster-
ing vs. random clustering at the same granularity
6 Conclusion
We have presented a classifier for automatic sense
merging that significantly outperforms previously
proposed automatic methods. In addition to its novel
use of supervised learning and the integration of
many previously proposed features, it is interest-
ing that one of our new features, the DERIV count
of shared derivational links between two synsets,
proved an extraordinarily useful new cue for sense-
merging, particularly for verbs.
We also show how to integrate this sense-merging
algorithm into a model for sense clustering full sense
taxonomies like WordNet, incorporating taxonomic
constraints such as the transitive effects of merging
synsets. Using this model, we have produced several
WordNet taxonomies of various sense granularities;
we hope these new lexical resources will be useful
for NLP applications that require a coarser-grained
sense hierarchy than that already found in WordNet.
Acknowledgments
Thanks to Marie-Catherine de Marneffe, Mona
Diab, Christiane Fellbaum, Thad Hughes, and Ben-
jamin Packer for useful discussions. Rion Snow is
supported by an NSF Fellowship. This work was
supported in part by the Disruptive Technology Of-
fice (DTO)?s Advanced Question Answering for In-
telligence (AQUAINT) Phase III Program.
References
Eneko Agirre and Oier Lopez de Lacalle. 2003. Cluster-
ing WordNet word senses. In Proceedings of RANLP
2003.
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nomi-
nal senses. In Proceedings of LREC 2004.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
Gloss Overlaps as a Measure of Semantic Relatedness.
In Proceedings of IJCAI 2003.
Lisa Bentivogli, Pamela Forner, Bernardo Magnini, and
Emanuele Pianta. 2004. Revising the WordNet Do-
mains Hierarchy: Semantics, Coverage, and Balanc-
ing. In Proceedings of COLING Workshop on Multi-
lingual Linguistic Resources, 2004.
Timothy Chklovski and Rada Mihalcea. 2003. Exploit-
ing Agreement and Disagreement of Human Annota-
tors for Word Sense Disambiguation. In Proceedings
of RANLP 2003.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and Sense Proximity in the Senseval-2 Test
Suite. In Proceedings of ACL 2002 WSD Workshop.
Bart Decadt, Veronique Hoste, Walter Daelemans, and
Antal van den Bosch. 2004. Gamble, genetic algo-
rithm optimization of memory-based wsd. In Proceed-
ings of ACL/SIGLEX Senseval-3.
William Dolan. 1994. Word Sense Ambiguation: Clus-
tering Related Senses. In Proceedings of ACL 1994.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
Julio Gonzalo, Felia Verdejo, Irina Chugur, and Juan
Cigarran. 1998. Indexing with WordNet synsets can
improve text retrieval. In Proceedings of COLING-
ACL 1998 Workshop on WordNet in NLP Systems.
Patrick Hanks. 2000. Do word meanings exist? Com-
puters and the Humanities, 34(1-2): 171-177.
1013
Graeme Hirst and David St-Onge. 1998. Lexical chains
as representations of context for the detection and cor-
rection of malapropisms. In WordNet: An Electronic
Lexical Database.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. Proceedings of HLT-NAACL 2006.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Re-
search in Computational Linguistics, 19-33.
Thorsten Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines. Dissertation, Kluwer,
2002.
Adam Kilgariff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31(1-2): 1-13.
Adam Kilgarriff. 2001. English lexical sample task de-
scription. In Proceedings of the SENSEVAL-2 work-
shop, 17-20.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for word
sense identification. In WordNet: An Electronic Lexi-
cal Database.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of SIG-
DOC 1986.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago, IL.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML 1998.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL 1998.
Bernardo Magnini and Gabriela Cavaglia. 2000. Inte-
grating Subject Field Codes into WordNet. In Pro-
ceedings of LREC 2000.
Diana McCarthy. 2006. Relating WordNet Senses for
Word Sense Disambiguation. In Proceedings of ACL
Workshop on Making Sense of Sense, 2006.
Rada Mihalcea and Dan I. Moldovan. 2001. Automatic
Generation of a Coarse Grained WordNet. In Proceed-
ings of NAACL Workshop on WordNet and Other Lex-
ical Resources.
Rada Mihalcea and Ehsanul Faruque. 2004. Sense-
learner: Minimally supervised word sense disam-
biguation for all words in open text. In Proceedings
of ACL/SIGLEX Senseval-3.
Dan I. Moldovan and Rada Mihalcea. 2000. Using
WordNet and lexical operators to improve Internet
searches. IEEE Internet Computing, 4(1):34-43.
Roberto Navigli. 2006. Meaningful Clustering of
Senses Helps Boost Word Sense Disambiguation Per-
formance. In Proceedings of COLING-ACL 2006.
Martha Palmer, Olga Babko-Malaya, Hoa Trang Dang.
2004. Different Sense Granularities for Different Ap-
plications. In Proceedings of Workshop on Scalable
Natural Language Understanding.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2005. Making fine-grained and coarse-grained
sense distinctions. Journal of Natural Language Engi-
neering.
Siddharth Patwardhan. 2003. Incorporating dictionary
and corpus information into a context vector measure
of semantic relatedness. Master?s thesis, Univ. of Min-
nesota, Duluth.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the
Relatedness of Concepts. In Proceedings of NAACL
2004.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional Clustering of English Words. In Pro-
ceedings of ACL 1993.
Wim Peters, Ivonne Peters, and Piek Vossen. 1998. Au-
tomatic Sense Clustering in EuroWordNet. In Pro-
ceedings of LREC 1998.
Andrew Philpot, Eduard Hovy, and Patrick Pantel. 2005.
The Omega Ontology. In Proceedings of the ON-
TOLEX Workshop at IJCNLP 2005.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the IJCAI 1995, 448-453.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: new evaluation
methods for word sense disambiguation. Natural Lan-
guage Engineering, 5(2):113-134.
Noriko Tomuro. 2001. Tree-cut and A Lexicon based
on Systematic Polysemy. In Proceedings of NAACL
2001.
Zhibiao Wu and Martha Palmer. 1994. Verb Semantics
and Lexical Selection. In Proceedings of ACL 1994.
Deniz Yuret. 2004. Some experiments with a naive
bayes wsd system. In Proceedings of ACL/SIGLEX
Senseval-3.
1014
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 254?263,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Cheap and Fast ? But is it Good?
Evaluating Non-Expert Annotations for Natural Language Tasks
Rion Snow? Brendan O?Connor? Daniel Jurafsky? Andrew Y. Ng?
?Computer Science Dept.
Stanford University
Stanford, CA 94305
{rion,ang}@cs.stanford.edu
?Dolores Labs, Inc.
832 Capp St.
San Francisco, CA 94110
brendano@doloreslabs.com
?Linguistics Dept.
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
Human linguistic annotation is crucial for
many natural language processing tasks but
can be expensive and time-consuming. We ex-
plore the use of Amazon?s Mechanical Turk
system, a significantly cheaper and faster
method for collecting annotations from a
broad base of paid non-expert contributors
over the Web. We investigate five tasks: af-
fect recognition, word similarity, recognizing
textual entailment, event temporal ordering,
and word sense disambiguation. For all five,
we show high agreement between Mechani-
cal Turk non-expert annotations and existing
gold standard labels provided by expert label-
ers. For the task of affect recognition, we also
show that using non-expert labels for training
machine learning algorithms can be as effec-
tive as using gold standard annotations from
experts. We propose a technique for bias
correction that significantly improves annota-
tion quality on two tasks. We conclude that
many large labeling tasks can be effectively
designed and carried out in this method at a
fraction of the usual expense.
1 Introduction
Large scale annotation projects such as TreeBank
(Marcus et al, 1993), PropBank (Palmer et
al., 2005), TimeBank (Pustejovsky et al, 2003),
FrameNet (Baker et al, 1998), SemCor (Miller et
al., 1993), and others play an important role in
natural language processing research, encouraging
the development of novel ideas, tasks, and algo-
rithms. The construction of these datasets, how-
ever, is extremely expensive in both annotator-hours
and financial cost. Since the performance of many
natural language processing tasks is limited by the
amount and quality of data available to them (Banko
and Brill, 2001), one promising alternative for some
tasks is the collection of non-expert annotations.
In this work we explore the use of Amazon Me-
chanical Turk1 (AMT) to determine whether non-
expert labelers can provide reliable natural language
annotations. We chose five natural language under-
standing tasks that we felt would be sufficiently nat-
ural and learnable for non-experts, and for which
we had gold standard labels from expert labelers,
as well as (in some cases) expert labeler agree-
ment information. The tasks are: affect recogni-
tion, word similarity, recognizing textual entailment,
event temporal ordering, and word sense disam-
biguation. For each task, we used AMT to annotate
data and measured the quality of the annotations by
comparing them with the gold standard (expert) la-
bels on the same data. Further, we compare machine
learning classifiers trained on expert annotations vs.
non-expert annotations.
In the next sections of the paper we introduce
the five tasks and the evaluation metrics, and offer
methodological insights, including a technique for
bias correction that improves annotation quality.2
1 http://mturk.com
2 Please see http://blog.doloreslabs.com/?p=109
for a condensed version of this paper, follow-ups, and on-
going public discussion. We encourage comments to be di-
rected here in addition to email when appropriate. Dolores
Labs Blog, ?AMT is fast, cheap, and good for machine learning
data,? Brendan O?Connor, Sept. 9, 2008. More related work at
http://blog.doloreslabs.com/topics/wisdom/.
254
2 Related Work
The idea of collecting annotations from volunteer
contributors has been used for a variety of tasks.
Luis von Ahn pioneered the collection of data via
online annotation tasks in the form of games, includ-
ing the ESPGame for labeling images (von Ahn and
Dabbish, 2004) and Verbosity for annotating word
relations (von Ahn et al, 2006). The Open Mind
Initiative (Stork, 1999) has taken a similar approach,
attempting to make such tasks as annotating word
sense (Chklovski and Mihalcea, 2002) and common-
sense word relations (Singh, 2002) sufficiently ?easy
and fun? to entice users into freely labeling data.
There have been an increasing number of experi-
ments using Mechanical Turk for annotation. In (Su
et al, 2007) workers provided annotations for the
tasks of hotel name entity resolution and attribute
extraction of age, product brand, and product model,
and were found to have high accuracy compared
to gold-standard labels. Kittur et al (2008) com-
pared AMT evaluations of Wikipedia article qual-
ity against experts, finding validation tests were im-
portant to ensure good results. Zaenen (Submitted)
studied the agreement of annotators on the problem
of recognizing textual entailment (a similar task and
dataset is explained in more detail in Section 4).
At least several studies have already used AMT
without external gold standard comparisons. In
(Nakov, 2008) workers generated paraphrases of
250 noun-noun compounds which were then used
as the gold standard dataset for evaluating an au-
tomatic method of noun compound paraphrasing.
Kaisser and Lowe (2008) use AMT to help build a
dataset for question answering, annotating the an-
swers to 8107 questions with the sentence contain-
ing the answer. Kaisser et al (2008) examines the
task of customizing the summary length of QA out-
put; non-experts from AMT chose a summary length
that suited their information needs for varying query
types. Dakka and Ipeirotis (2008) evaluate a docu-
ment facet generation system against AMT-supplied
facets, and also use workers for user studies of the
system. Sorokin and Forsyth (2008) collect data for
machine vision tasks and report speed and costs sim-
ilar to our findings; their summaries of worker be-
havior also corroborate with what we have found.
In general, volunteer-supplied or AMT-supplied
data is more plentiful but noisier than expert data.
It is powerful because independent annotations can
be aggregated to achieve high reliability. Sheng et
al. (2008) explore several methods for using many
noisy labels to create labeled data, how to choose
which examples should get more labels, and how to
include labels? uncertainty information when train-
ing classifiers. Since we focus on empirically val-
idating AMT as a data source, we tend to stick to
simple aggregation methods.
3 Task Design
In this section we describe Amazon Mechanical
Turk and the general design of our experiments.
3.1 Amazon Mechanical Turk
We employ the Amazon Mechanical Turk system
in order to elicit annotations from non-expert label-
ers. AMT is an online labor market where workers
are paid small amounts of money to complete small
tasks. The design of the system is as follows: one is
required to have an Amazon account to either sub-
mit tasks for annotations or to annotate submitted
tasks. These Amazon accounts are anonymous, but
are referenced by a unique Amazon ID. A Requester
can create a group of Human Intelligence Tasks (or
HITs), each of which is a form composed of an arbi-
trary number of questions. The user requesting an-
notations for the group of HITs can specify the num-
ber of unique annotations per HIT they are willing
to pay for, as well as the reward payment for each
individual HIT. While this does not guarantee that
unique people will annotate the task (since a single
person could conceivably annotate tasks using mul-
tiple accounts, in violation of the user agreement),
this does guarantee that annotations will be collected
from unique accounts. AMT also allows a requester
to restrict which workers are allowed to annotate a
task by requiring that all workers have a particular
set of qualifications, such as sufficient accuracy on
a small test set or a minimum percentage of previ-
ously accepted submissions. Annotators (variously
referred to as Workers or Turkers) may then annotate
the tasks of their choosing. Finally, after each HIT
has been annotated, the Requester has the option of
approving the work and optionally giving a bonus
to individual workers. There is a two-way commu-
255
nication channel between the task designer and the
workers mediated by Amazon, and Amazon handles
all financial transactions.
3.2 Task Design
In general we follow a few simple design principles:
we attempt to keep our task descriptions as succinct
as possible, and we attempt to give demonstrative
examples for each class wherever possible. We have
published the full experimental design and the data
we have collected for each task online3. We have
restricted our study to tasks where we require only
a multiple-choice response or numeric input within
a fixed range. For every task we collect ten inde-
pendent annotations for each unique item; this re-
dundancy allows us to perform an in-depth study of
how data quality improves with the number of inde-
pendent annotations.
4 Annotation Tasks
We analyze the quality of non-expert annotations on
five tasks: affect recognition, word similarity, rec-
ognizing textual entailment, temporal event recogni-
tion, and word sense disambiguation. In this section
we define each annotation task and the parameters
of the annotations we request using AMT. Addition-
ally we give an initial analysis of the task results,
and summarize the cost of the experiments.
4.1 Affective Text Analysis
This experiment is based on the affective text an-
notation task proposed in Strapparava and Mihalcea
(2007), wherein each annotator is presented with a
list of short headlines, and is asked to give numeric
judgments in the interval [0,100] rating the headline
for six emotions: anger, disgust, fear, joy, sadness,
and surprise, and a single numeric rating in the inter-
val [-100,100] to denote the overall positive or nega-
tive valence of the emotional content of the headline,
as in this sample headline-annotation pair:
Outcry at N Korea ?nuclear test?
(Anger, 30), (Disgust,30), (Fear,30), (Joy,0),
(Sadness,20), (Surprise,40), (Valence,-50).
3All tasks and collected data are available at
http://ai.stanford.edu/
?
rion/annotations/.
For our experiment we select a 100-headline sample
from the original SemEval test set, and collect 10
affect annotations for each of the seven label types,
for a total of 7000 affect labels.
We then performed two comparisons to evaluate
the quality of the AMT annotations. First, we asked
how well the non-experts agreed with the experts.
We did this by comparing the interannotator agree-
ment (ITA) of individual expert annotations to that
of single non-expert and averaged non-expert anno-
tations. In the original experiment ITA is measured
by calculating the Pearson correlation of one anno-
tator?s labels with the average of the labels of the
other five annotators. For each expert labeler, we
computed this ITA score of the expert against the
other five; we then average these ITA scores across
all expert annotators to compute the average expert
ITA (reported in Table 1 as ?E vs. E?. We then do the
same for individual non-expert annotations, averag-
ing Pearson correlation across all sets of the five ex-
pert labelers (?NE vs. E?). We then calculate the ITA
for each expert vs. the averaged labels from all other
experts and non-experts (marked as ?E vs. All?) and
for each non-expert vs. the pool of other non-experts
and all experts (?NE vs. All?). We compute these
ITA scores for each emotion task separately, aver-
aging the six emotion tasks as ?Avg. Emo? and the
average of all tasks as ?Avg. All?.
Emotion E vs. E E vs. All NE vs. E NE vs. All
Anger 0.459 0.503 0.444 0.573
Disgust 0.583 0.594 0.537 0.647
Fear 0.711 0.683 0.418 0.498
Joy 0.596 0.585 0.340 0.421
Sadness 0.645 0.650 0.563 0.651
Surprise 0.464 0.463 0.201 0.225
Valence 0.759 0.767 0.530 0.554
Avg. Emo 0.576 0.603 0.417 0.503
Avg. All 0.580 0.607 0.433 0.510
Table 1: Average expert and non-expert ITA on test-set
The results in Table 1 conform to the expectation
that experts are better labelers: experts agree with
experts more than non-experts agree with experts,
although the ITAs are in many cases quite close. But
we also found that adding non-experts to the gold
standard (?E vs. All?) improves agreement, suggest-
ing that non-expert annotations are good enough to
increase the overall quality of the gold labels. Our
256
first comparison showed that individual experts were
better than individual non-experts. In our next com-
parison we ask how many averaged non-experts it
would take to rival the performance of a single ex-
pert. We did this by averaging the labels of each pos-
sible subset of n non-expert annotations, for value
of n in {1, 2, . . . , 10}. We then treat this average as
though it is the output of a single ?meta-labeler?, and
compute the ITA with respect to each subset of five
of the six expert annotators. We then average the
results of these studies across each subset size; the
results of this experiment are given in Table 2 and in
Figure 1. In addition to the single meta-labeler, we
ask: what is the minimum number of non-expert an-
notations k from which we can create a meta-labeler
that has equal or better ITA than an expert annotator?
In Table 2 we give the minimum k for each emotion,
and the averaged ITA for that meta-labeler consist-
ing of k non-experts (marked ?k-NE?). In Figure 1
we plot the expert ITA correlation as the horizontal
dashed line.
Emotion 1-Expert 10-NE k k-NE
Anger 0.459 0.675 2 0.536
Disgust 0.583 0.746 2 0.627
Fear 0.711 0.689 ? ?
Joy 0.596 0.632 7 0.600
Sadness 0.645 0.776 2 0.656
Surprise 0.464 0.496 9 0.481
Valence 0.759 0.844 5 0.803
Avg. Emo. 0.576 0.669 4 0.589
Avg. All 0.603 0.694 4 0.613
Table 2: Average expert and averaged correlation over
10 non-experts on test-set. k is the minimum number of
non-experts needed to beat an average expert.
These results show that for all tasks except ?Fear?
we are able to achieve expert-level ITA with the
held-out set of experts within 9 labelers, and fre-
quently within only 2 labelers. Pooling judgments
across all 7 tasks we find that on average it re-
quires only 4 non-expert annotations per example to
achieve the equivalent ITA as a single expert anno-
tator. Given that we paid US$2.00 in order to collect
the 7000 non-expert annotations, we may interpret
our rate of 3500 non-expert labels per USD as at
least 875 expert-equivalent labels per USD.
4.2 Word Similarity
This task replicates the word similarity task used in
(Miller and Charles, 1991), following a previous
2 4 6 8 100
.4
5
0.
55
0.
65
co
rr
e
la
tio
n
anger
2 4 6 8 10
0.
55
0.
65
0.
75
co
rr
e
la
tio
n
disgust
2 4 6 8 100
.4
0
0.
50
0.
60
0.
70
co
rr
e
la
tio
n
fear
2 4 6 8 10
0.
35
0.
45
0.
55
0.
65
co
rr
e
la
tio
n
joy
2 4 6 8 100
.5
5
0.
65
0.
75
annotators
co
rr
e
la
tio
n
sadness
2 4 6 8 100
.2
0
0.
30
0.
40
0.
50
annotators
co
rr
e
la
tio
n
surprise
Figure 1: Non-expert correlation for affect recognition
task initially proposed by (Rubenstein and Good-
enough, 1965). Specifically, we ask for numeric
judgments of word similarity for 30 word pairs on
a scale of [0,10], allowing fractional responses4 .
These word pairs range from highly similar (e.g.,
{boy, lad}), to unrelated (e.g., {noon, string}). Nu-
merous expert and non-expert studies have shown
that this task typically yields very high interannota-
tor agreement as measured by Pearson correlation;
(Miller and Charles, 1991) found a 0.97 correla-
tion of the annotations of 38 subjects with the an-
notations given by 51 subjects in (Rubenstein and
Goodenough, 1965), and a following study (Resnik,
1999) with 10 subjects found a 0.958 correlation
with (Miller and Charles, 1991).
In our experiment we ask for 10 annotations each
of the full 30 word pairs, at an offered price of $0.02
for each set of 30 annotations (or, equivalently, at
the rate of 1500 annotations per USD). The most
surprising aspect of this study was the speed with
which it was completed; the task of 300 annotations
was completed by 10 annotators in less than 11 min-
4(Miller and Charles, 1991) and others originally used a
numerical score of [0,4].
257
utes from the time of submission of our task to AMT,
at the rate of 1724 annotations / hour.
As in the previous task we evaluate our non-
expert annotations by averaging the numeric re-
sponses from each possible subset of n annotators
and computing the interannotator agreement with
respect to the gold scores reported in (Miller and
Charles, 1991). Our results are displayed in Figure
2, with Resnik?s 0.958 correlation plotted as the hor-
izontal line; we find that at 10 annotators we achieve
a correlation of 0.952, well within the range of other
studies of expert and non-expert annotations.
2 4 6 8 10
0.
84
0.
90
0.
96
annotations
co
rr
e
la
tio
n
Word Similarity ITA
Figure 2: ITA for word similarity experiment
4.3 Recognizing Textual Entailment
This task replicates the recognizing textual entail-
ment task originally proposed in the PASCAL Rec-
ognizing Textual Entailment task (Dagan et al,
2006); here for each question the annotator is pre-
sented with two sentences and given a binary choice
of whether the second hypothesis sentence can be
inferred from the first. For example, the hypothesis
sentence ?Oil prices drop? would constitute a true
entailment from the text ?Crude Oil Prices Slump?,
but a false entailment from ?The government an-
nounced last week that it plans to raise oil prices?.
We gather 10 annotations each for all 800 sen-
tence pairs in the PASCAL RTE-1 dataset. For this
dataset expert interannotator agreement studies have
been reported as achieving 91% and 96% agreement
over various subsections of the corpus. When con-
sidering multiple non-expert annotations for a sen-
tence pair we use simple majority voting, breaking
ties randomly and averaging performance over all
possible ways to break ties. We collect 10 annota-
tions for each of 100 RTE sentence pairs; as dis-
played in Figure 3, we achieve a maximum accu-
racy of 89.7%, averaging over the annotations of 10
workers5.
2 4 6 8 100
.7
0
0.
80
0.
90
annotations
a
cc
u
ra
cy
RTE ITA
Figure 3: Inter-annotator agreement for RTE experiment
4.4 Event Annotation
This task is inspired by the TimeBank corpus (Puste-
jovsky et al, 2003), which includes among its anno-
tations a label for event-pairs that represents the tem-
poral relation between them, from a set of fourteen
relations (before, after, during, includes, etc.). We
implement temporal ordering as a simplified version
of the TimeBank event temporal annotation task:
rather than annotating all fourteen event types, we
restrict our consideration to the two simplest labels:
?strictly before? and ?strictly after?. Furthermore,
rather than marking both nouns and verbs in the text
as possible events, we only consider possible verb
events. We extract the 462 verb event pairs labeled
as ?strictly before? or ?strictly after? in the Time-
Bank corpus, and we present these pairs to annota-
tors with a forced binary choice on whether the event
described by the first verb occurs before or after the
second. For example, in a dialogue about a plane
explosion, we have the utterance: ?It just blew up in
the air, and then we saw two fireballs go down to the,
5It might seem pointless to consider an even number of an-
notations in this circumstance, since the majority voting mech-
anism and tie-breaking yields identical performance for 2n + 1
and 2n + 2 annotators; however, in Section 5 we will consider
methods that can make use of the even annotations.
258
to the water, and there was a big small, ah, smoke,
from ah, coming up from that?. Here for each anno-
tation we highlight the specific verb pair of interest
(e.g., go/coming, or blew/saw) and ask which event
occurs first (here, go and blew, respectively).
The results of this task are presented in Figure 4.
We achieve high agreement for this task, at a rate
of 0.94 with simple voting over 10 annotators (4620
total annotations). While an expert ITA of 0.77 was
reported for the more general task involving all four-
teen labels on both noun and verb events, no expert
ITA numbers have been reported for this simplified
temporal ordering task.
2 4 6 8 100
.7
0
0.
80
0.
90
annotators
a
cc
u
ra
cy
Temp. Ordering ITA
Figure 4: ITA for temporal ordering experiment
4.5 Word Sense Disambiguation
In this task we consider a simple problem on which
machine learning algorithms have been shown to
produce extremely good results; here we annotate
part of the SemEval Word Sense Disambiguation
Lexical Sample task (Pradhan et al, 2007); specif-
ically, we present the labeler with a paragraph of
text containing the word ?president? (e.g., a para-
graph containing ?Robert E. Lyons III...was ap-
pointed president and chief operating officer...?) and
ask the labeler which one of the following three
sense labels is most appropriate:
1) executive officer of a firm, corporation, or university
2) head of a country (other than the U.S.)
3) head of the U.S., President of the United States
We collect 10 annotations for each of 177 examples
of the noun ?president? for the three senses given in
SemEval. As shown in Figure 5, performing simple
majority voting (with random tie-breaking) over an-
notators results in a rapid accuracy plateau at a very
high rate of 0.994 accuracy. In fact, further analy-
sis reveals that there was only a single disagreement
between the averaged non-expert vote and the gold
standard; on inspection it was observed that the an-
notators voted strongly against the original gold la-
bel (9-to-1 against), and that it was in fact found to
be an error in the original gold standard annotation.6
After correcting this error, the non-expert accuracy
rate is 100% on the 177 examples in this task. This
is a specific example where non-expert annotations
can be used to correct expert annotations.
Since expert ITA was not reported per word on
this dataset, we compare instead to the performance
of the best automatic system performance for dis-
ambiguating ?president? in SemEval Task 17 (Cai et
al., 2007), with an accuracy of 0.98.
2 4 6 8 10
0.
98
0
0.
99
0
1.
00
0
annotators
a
cc
u
ra
cy
WSD ITA
Figure 5: Inter-annotator agreement for WSD experiment
4.6 Summary
Cost Time Labels Labels
Task Labels (USD) (hrs) per USD per hr
Affect 7000 $2.00 5.93 3500 1180.4
WSim 300 $0.20 0.174 1500 1724.1
RTE 8000 $8.00 89.3 1000 89.59
Event 4620 $13.86 39.9 333.3 115.85
WSD 1770 $1.76 8.59 1005.7 206.1
Total 21690 25.82 143.9 840.0 150.7
Table 3: Summary of costs for non-expert labels
6The example sentence began ?The Egyptian president said
he would visit Libya today...? and was mistakenly marked as
the ?head of a company? sense in the gold annotation (example
id 24:0@24@wsj/23/wsj 2381@wsj@en@on).
259
0 200 400 600 800
0.
4
0.
6
0.
8
1.
0
number of annotations
a
cc
u
ra
cy
Figure 6: Worker accuracies on the RTE task. Each point
is one worker. Vertical jitter has been added to points on
the left to show the large number of workers who did the
minimum amount of work (20 examples).
In Table 3 we give a summary of the costs asso-
ciated with obtaining the non-expert annotations for
each of our 5 tasks. Here Time is given as the to-
tal amount of time in hours elapsed from submitting
the group of HITs to AMT until the last assignment
is submitted by the last worker.
5 Bias correction for non-expert
annotators
The reliability of individual workers varies. Some
are very accurate, while others are more careless and
make mistakes; and a small few give very noisy re-
sponses. Furthermore, for most AMT data collec-
tion experiments, a relatively small number of work-
ers do a large portion of the task, since workers may
do as much or as little as they please. Figure 6 shows
accuracy rates for individual workers on one task.
Both the overall variability, as well as the prospect
of identifying high-volume but low-quality workers,
suggest that controlling for individual worker qual-
ity could yield higher quality overall judgments.
In general, there are at least three ways to enhance
quality in the face of worker error. More work-
ers can be used, as described in previous sections.
Another method is to use Amazon?s compensation
mechanisms to give monetary bonuses to highly-
performing workers and deny payments to unreli-
able ones; this is useful, but beyond the scope of
this paper. In this section we explore a third alterna-
tive, to model the reliability and biases of individual
workers and correct for them.
A wide number of methods have been explored to
correct for the bias of annotators. Dawid and Skene
(1979) are the first to consider the case of having
multiple annotators per example but unknown true
labels. They introduce an EM algorithm to simul-
taneously estimate annotator biases and latent label
classes. Wiebe et al (1999) analyze linguistic anno-
tator agreement statistics to find bias, and use a sim-
ilar model to correct labels. A large literature in bio-
statistics addresses this same problem for medical
diagnosis. Albert and Dodd (2004) review several
related models, but argue they have various short-
comings and emphasize instead the importance of
having a gold standard.
Here we take an approach based on gold standard
labels, using a small amount of expert-labeled train-
ing data in order to correct for the individual biases
of different non-expert annotators. The idea is to re-
calibrate worker?s responses to more closely match
expert behavior. We focus on categorical examples,
though a similar method can be used with numeric
data.
5.1 Bias correction in categorical data
Following Dawid and Skene, we model labels and
workers with a multinomial model similar to Naive
Bayes. Every example i has a true label xi. For sim-
plicity, assume two labels {Y,N}. Several differ-
ent workers give labels yi1, yi2, . . . yiW . A worker?s
conditional probability of response is modeled as
multinomial, and we model each worker?s judgment
as conditionally independent of other workers given
the true label xi, i.e.:
P (yi1, . . . , yiW , xi) =
(
?
w
P (yiw|xi)
)
p(xi)
To infer the posterior probability of the true label
for a new example, worker judgments are integrated
via Bayes rule, yielding the posterior log-odds:
log
P (xi = Y |yi1 . . . yiW )
P (xi = N |yi1 . . . yiW )
=
?
w
log
P (yiw|xi = Y )
P (yiw|xi = N)
+ log
P (xi = Y )
P (xi = N)
260
The worker response likelihoods P (yw|x = Y )
and P (yw|x = N) can be directly estimated from
frequencies of worker performance on gold standard
examples. (If we used maximum likelihood esti-
mation with no Laplace smoothing, then each yw|x
is just the worker?s empirical confusion matrix.)
For MAP label estimation, the above equation de-
scribes a weighted voting rule: each worker?s vote is
weighted by their log likelihood ratio for their given
response. Intuitively, workers who are more than
50% accurate have positive votes; workers whose
judgments are pure noise have zero votes; and an-
ticorrelated workers have negative votes. (A simpler
form of the model only considers accuracy rates,
thus weighting worker votes by log accw1?accw . But we
use the full unconstrained multinomial model here.)
5.1.1 Example tasks: RTE-1 and event
annotation
We used this model to improve accuracy on the
RTE-1 and event annotation tasks. (The other cate-
gorical task, word sense disambiguation, could not
be improved because it already had maximum accu-
racy.) First we took a sample of annotations giving
k responses per example. Within this sample, we
trained and tested via 20-fold cross-validation across
examples. Worker models were fit using Laplace
smoothing of 1 pseudocount; label priors were uni-
form, which was reasonably similar to the empirical
distribution for both tasks.
annotators
a
cc
u
ra
cy
0.
7
0.
8
0.
9
RTE
annotators
0.
7
0.
8
0.
9
before/after
Gold calibrated
Naive voting
Figure 7: Gold-calibrated labels versus raw labels
Figure 7 shows improved accuracy at different
numbers of annotators. The lowest line is for the
naive 50% majority voting rule. (This is equivalent
to the model under uniform priors and equal accu-
racies across workers and labels.) Each point is the
data set?s accuracy against the gold labels, averaged
across resamplings each of which obtains k annota-
tions per example. RTE has an average +4.0% ac-
curacy increase, averaged across 2 through 10 anno-
tators. We find a +3.4% gain on event annotation.
Finally, we experimented with a similar calibration
method for numeric data, using a Gaussian noise
model for each worker: yw|x ? N(x + ?w, ?w).
On the affect task, this yielded a small but consis-
tent increases in Pearson correlation at all numbers
of annotators, averaging a +0.6% gain.
6 Training a system with non-expert
annotations
In this section we train a supervised affect recogni-
tion system with expert vs. non-expert annotations.
6.1 Experimental Design
For the purpose of this experiment we create a sim-
ple bag-of-words unigram model for predicting af-
fect and valence, similar to the SWAT system (Katz
et al, 2007), one of the top-performing systems on
the SemEval Affective Text task.7 For each token
t in our training set, we assign t a weight for each
emotion e equal to the average emotion score ob-
served in each headline H that t participates in. i.e.,
if Ht is the set of headlines containing the token t,
then:
Score(e, t) =
?
H?Ht Score(e,H)
|Ht|
With these weights of the individual tokens we
may then compute the score for an emotion e of a
new headline H as the average score over the set of
tokens t ? H that we?ve observed in the training set
(ignoring those tokens not in the training set), i.e.:
Score(e,H) =
?
t?H
Score(e, t)
|H|
Where |H| is simply the number of tokens in
headline H , ignoring tokens not observed in the
training set.
7 Unlike the SWAT system we perform no lemmatization,
synonym expansion, or any other preprocessing of the tokens;
we simply use whitespace-separated tokens within each head-
line.
261
6.2 Experiments
We use 100 headlines as a training set (examples
500-599 from the test set of SemEval Task 14), and
we use the remaining 900 headlines as our test set.
Since we are fortunate to have the six separate ex-
pert annotations in this task, we can perform an ex-
tended systematic comparison of the performance of
the classifier trained with expert vs. non-expert data.
Emotion 1-Expert 10-NE k k-NE
Anger 0.084 0.233 1 0.172
Disgust 0.130 0.231 1 0.185
Fear 0.159 0.247 1 0.176
Joy 0.130 0.125 ? ?
Sadness 0.127 0.174 1 0.141
Surprise 0.060 0.101 1 0.061
Valence 0.159 0.229 2 0.146
Avg. Emo 0.116 0.185 1 0.135
Avg. All 0.122 0.191 1 0.137
Table 4: Performance of expert-trained and non-expert-
trained classifiers on test-set. k is the minimum number
of non-experts needed to beat an average expert.
For this evaluation we compare the performance
of systems trained on expert and non-expert annota-
tions. For each expert annotator we train a system
using only the judgments provided by that annota-
tor, and then create a gold standard test set using the
average of the responses of the remaining five label-
ers on that set. In this way we create six indepen-
dent expert-trained systems and compute the aver-
age across their performance, calculated as Pearson
correlation to the gold standard; this is reported in
the ?1-Expert? column of Table 4.
Next we train systems using non-expert labels;
for each possible subset of n annotators, for n ?
{1, 2, . . . , 10} we train a system, and evaluate by
calculating Pearson correlation with the same set of
gold standard datasets used in the expert-trained sys-
tem evaluation. Averaging the results of these stud-
ies yields the results in Table 4.
As in Table 2 we calculate the minimum number
of non-expert annotations per example k required on
average to achieve similar performance to the ex-
pert annotations; surprisingly we find that for five
of the seven tasks, the average system trained with a
single set of non-expert annotations outperforms the
average system trained with the labels from a sin-
gle expert. One possible hypothesis for the cause
of this non-intuitive result is that individual labelers
(including experts) tend to have a strong bias, and
since multiple non-expert labelers may contribute to
a single set of non-expert annotations, the annotator
diversity within the single set of labels may have the
effect of reducing annotator bias and thus increasing
system performance.
7 Conclusion
We demonstrate the effectiveness of using Amazon
Mechanical Turk for a variety of natural language
annotation tasks. Our evaluation of non-expert la-
beler data vs. expert annotations for five tasks found
that for many tasks only a small number of non-
expert annotations per item are necessary to equal
the performance of an expert annotator. In a detailed
study of expert and non-expert agreement for an af-
fect recognition task we find that we require an av-
erage of 4 non-expert labels per item in order to em-
ulate expert-level label quality. Finally, we demon-
strate significant improvement by controlling for la-
beler bias.
Acknowledgments
Thanks to Nathanael Chambers, Annie Zaenen,
Rada Mihalcea, Qi Su, Panos Ipeirotis, Bob Car-
penter, David Vickrey, William Morgan, and Lukas
Biewald for useful discussions, and for the gener-
ous support of Dolores Labs. This work was sup-
ported in part by the Disruptive Technology Office
(DTO)?s Advanced Question Answering for Intelli-
gence (AQUAINT) Phase III Program.
References
Paul S. Albert and Lori E. Dodd. 2004. A Cautionary
Note on the Robustness of Latent Class Models for
Estimating Diagnostic Error without a Gold Standard.
Biometrics, Vol. 60 (2004), pp. 427-435.
Collin F. Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc. of
COLING-ACL 1998.
Michele Banko and Eric Brill. 2001. Scaling to Very
Very Large Corpora for Natural Language Disam-
biguation. In Proc. of ACL-2001.
Junfu Cai, Wee Sun Lee and Yee Whye Teh. 2007. Im-
proving Word Sense Disambiguation Using Topic Fea-
tures. In Proc. of EMNLP-2007 .
262
Timothy Chklovski and Rada Mihalcea. 2002. Building
a sense tagged corpus with Open Mind Word Expert.
In Proc. of the Workshop on ?Word Sense Disam-
biguation: Recent Successes and Future Directions?,
ACL 2002.
Timothy Chklovski and Yolanda Gil. 2005. Towards
Managing Knowledge Collection from Volunteer Con-
tributors. Proceedings of AAAI Spring Symposium
on Knowledge Collection from Volunteer Contributors
(KCVC05).
Ido Dagan, Oren Glickman and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
Challenge. Machine Learning Challenges. Lecture
Notes in Computer Science, Vol. 3944, pp. 177-190,
Springer, 2006.
Wisam Dakka and Panagiotis G. Ipeirotis. 2008. Au-
tomatic Extraction of Useful Facet Terms from Text
Documents. In Proc. of ICDE-2008.
A. P. Dawid and A. M. Skene. 1979. Maximum Like-
lihood Estimation of Observer Error-Rates Using the
EM Algorithm. Applied Statistics, Vol. 28, No. 1
(1979), pp. 20-28.
Michael Kaisser and John B. Lowe. 2008. A Re-
search Collection of QuestionAnswer Sentence Pairs.
In Proc. of LREC-2008.
Michael Kaisser, Marti Hearst, and John B. Lowe.
2008. Evidence for Varying Search Results Summary
Lengths. In Proc. of ACL-2008.
Phil Katz, Matthew Singleton, Richard Wicentowski.
2007. SWAT-MP: The SemEval-2007 Systems for
Task 5 and Task 14. In Proc. of SemEval-2007.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with Mechanical Turk. In
Proc. of CHI-2008.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics 19:2, June 1993.
George A. Miller and William G. Charles. 1991. Con-
textual Correlates of Semantic Similarity. Language
and Cognitive Processes, vol. 6, no. 1, pp. 1-28, 1991.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunke. 1993. A semantic concordance. In
Proc. of HLT-1993.
Preslav Nakov. 2008. Paraphrasing Verbs for Noun
Compound Interpretation. In Proc. of the Workshop
on Multiword Expressions, LREC-2008.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: A Corpus Annotated with Se-
mantic Roles. Computational Linguistics, 31:1.
Sameer Pradhan, Edward Loper, Dmitriy Dligach and
Martha Palmer. 2007. SemEval-2007 Task-17: En-
glish Lexical Sample, SRL and All Words. In Proc.
of SemEval-2007 .
James Pustejovsky, Patrick Hanks, Roser Saur, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus. In
Proc. of Corpus Linguistics 2003, 647-656.
Philip Resnik. 1999. Semantic Similarity in a Taxon-
omy: An Information-Based Measure and its Applica-
tion to Problems of Ambiguity in Natural Language.
JAIR, Volume 11, pages 95-130.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Communications
of the ACM, 8(10):627?633.
Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeiro-
tis. 2008. Get Another Label? Improving Data Qual-
ity and Data Mining Using Multiple, Noisy Labelers.
In Proc. of KDD-2008.
Push Singh. 2002. The public acquisition of common-
sense knowledge. In Proc. of AAAI Spring Sympo-
sium on Acquiring (and Using) Linguistic (and World)
Knowledge for Information Access, 2002.
Alexander Sorokin and David Forsyth. 2008. Util-
ity data annotation with Amazon Mechanical Turk.
To appear in Proc. of First IEEE Workshop on
Internet Vision at CVPR, 2008. See also:
http://vision.cs.uiuc.edu/annotation/
David G. Stork. 1999. The Open Mind Initiative.
IEEE Expert Systems and Their Applications pp. 16-
20, May/June 1999.
Carlo Strapparava and Rada Mihalcea. 2007. SemEval-
2007 Task 14: Affective Text In Proc. of SemEval-
2007.
Qi Su, Dmitry Pavlov, Jyh-Herng Chow, and Wendell C.
Baker. 2007. Internet-Scale Collection of Human-
Reviewed Data. In Proc. of WWW-2007.
Luis von Ahn and Laura Dabbish. 2004. Labeling Im-
ages with a Computer Game. In ACM Conference on
Human Factors in Computing Systems, CHI 2004.
Luis von Ahn, Mihir Kedia and Manuel Blum. 2006.
Verbosity: A Game for Collecting Common-Sense
Knowledge. In ACM Conference on Human Factors
in Computing Systems, CHI Notes 2006.
Ellen Voorhees and Hoa Trang Dang. 2006. Overview of
the TREC 2005 question answering track. In Proc. of
TREC-2005.
Janyce M. Wiebe, Rebecca F. Bruce and Thomas P.
O?Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In
Proc. of ACL-1999.
Annie Zaenen. Submitted. Do give a penny for their
thoughts. International Journal of Natural Language
Engineering (submitted).
263
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 363?371,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Studying the History of Ideas Using Topic Models
David Hall
Symbolic Systems
Stanford University
Stanford, CA 94305, USA
dlwh@stanford.edu
Daniel Jurafsky
Linguistics
Stanford University
Stanford, CA 94305, USA
jurafsky@stanford.edu
Christopher D. Manning
Computer Science
Stanford University
Stanford, CA 94305, USA
manning@stanford.edu
Abstract
How can the development of ideas in a sci-
entific field be studied over time? We ap-
ply unsupervised topic modeling to the ACL
Anthology to analyze historical trends in the
field of Computational Linguistics from 1978
to 2006. We induce topic clusters using Latent
Dirichlet Allocation, and examine the strength
of each topic over time. Our methods find
trends in the field including the rise of prob-
abilistic methods starting in 1988, a steady in-
crease in applications, and a sharp decline of
research in semantics and understanding be-
tween 1978 and 2001, possibly rising again
after 2001. We also introduce a model of the
diversity of ideas, topic entropy, using it to
show that COLING is a more diverse confer-
ence than ACL, but that both conferences as
well as EMNLP are becoming broader over
time. Finally, we apply Jensen-Shannon di-
vergence of topic distributions to show that all
three conferences are converging in the topics
they cover.
1 Introduction
How can we identify and study the exploration of
ideas in a scientific field over time, noting periods of
gradual development, major ruptures, and the wax-
ing and waning of both topic areas and connections
with applied topics and nearby fields? One im-
portant method is to make use of citation graphs
(Garfield, 1955). This enables the use of graph-
based algorithms like PageRank for determining re-
searcher or paper centrality, and examining whether
their influence grows or diminishes over time.
However, because we are particularly interested
in the change of ideas in a field over time, we have
chosen a different method, following Kuhn (1962).
In Kuhn?s model of scientific change, science pro-
ceeds by shifting from one paradigm to another.
Because researchers? ideas and vocabulary are con-
strained by their paradigm, successive incommensu-
rate paradigms will naturally have different vocabu-
lary and framing.
Kuhn?s model is intended to apply only to very
large shifts in scientific thought rather than at the
micro level of trends in research foci. Nonetheless,
we propose to apply Kuhn?s insight that vocabulary
and vocabulary shift is a crucial indicator of ideas
and shifts in ideas. Our operationalization of this in-
sight is based on the unsupervised topic model La-
tent Dirichlet Allocation (LDA; Blei et al (2003)).
For many fields, doing this kind of historical study
would be very difficult. Computational linguistics
has an advantage, however: the ACL Anthology, a
public repository of all papers in the Computational
Linguistics journal and the conferences and work-
shops associated with the ACL, COLING, EMNLP,
and so on. The ACL Anthology (Bird, 2008), and
comprises over 14,000 documents from conferences
and the journal, beginning as early as 1965 through
2008, indexed by conference and year. This re-
source has already been the basis of citation anal-
ysis work, for example, in the ACL Anthology Net-
work of Joseph and Radev (2007). We apply LDA
to the text of the papers in the ACL Anthology to
induce topics, and use the trends in these topics over
time and over conference venues to address ques-
tions about the development of the field.
363
Venue # Papers Years Frequency
Journal 1291 1974?Present Quarterly
ACL 2037 1979-Present Yearly
EACL 596 1983?Present ?2 Years
NAACL 293 2000?Present ?Yearly
Applied NLP 346 1983?2000 ?3 Years
COLING 2092 1965-Present 2 Years
HLT 957 1986?Present ?2 Years
Workshops 2756 1990-Present Yearly
TINLAP 128 1975?1987 Rarely
MUC 160 1991?1998 ?2 Years
IJCNLP 143 2005 ??
Other 120 ?? ??
Table 1: Data in the ACL Anthology
Despite the relative youth of our field, computa-
tional linguistics has witnessed a number of research
trends and shifts in focus. While some trends are
obvious (such as the rise in machine learning meth-
ods), others may be more subtle. Has the field got-
ten more theoretical over the years or has there been
an increase in applications? What topics have de-
clined over the years, and which ones have remained
roughly constant? How have fields like Dialogue or
Machine Translation changed over the years? Are
there differences among the conferences, for exam-
ple between COLING and ACL, in their interests
and breadth of focus? As our field matures, it is im-
portant to go beyond anecdotal description to give
grounded answers to these questions. Such answers
could also help give formal metrics to model the dif-
ferences between the many conferences and venues
in our field, which could influence how we think
about reviewing, about choosing conference topics,
and about long range planning in our field.
2 Methodology
2.1 Data
The analyses in this paper are based on a text-
only version of the Anthology that comprises some
12,500 papers. The distribution of the Anthology
data is shown in Table 1.
2.2 Topic Modeling
Our experiments employ Latent Dirichlet Allocation
(LDA; Blei et al (2003)), a generative latent variable
model that treats documents as bags of words gener-
ated by one or more topics. Each document is char-
acterized by a multinomial distribution over topics,
and each topic is in turn characterized by a multino-
mial distribution over words. We perform parame-
ter estimation using collapsed Gibbs sampling (Grif-
fiths and Steyvers, 2004).
Possible extensions to this model would be to in-
tegrate topic modelling with citations (e.g., Dietz et
al. (2007), Mann et al (2006), and Jo et al (2007)).
Another option is the use of more fine-grained or hi-
erarchical model (e.g., Blei et al (2004), and Li and
McCallum (2006)).
All our studies measure change in various as-
pects of the ACL Anthology over time. LDA, how-
ever, does not explicitly model temporal relation-
ships. One way to model temporal relationships is
to employ an extension to LDA. The Dynamic Topic
Model (Blei and Lafferty, 2006), for example, rep-
resents each year?s documents as generated from a
normal distribution centroid over topics, with the
following year?s centroid generated from the pre-
ceding year?s. The Topics over Time Model (Wang
and McCallum, 2006) assumes that each document
chooses its own time stamp based on a topic-specific
beta distribution.
Both of these models, however, impose con-
straints on the time periods. The Dynamic Topic
Model penalizes large changes from year to year
while the beta distributions in Topics over Time are
relatively inflexible. We chose instead to perform
post hoc calculations based on the observed proba-
bility of each topic given the current year. We define
p?(z|y) as the empirical probability that an arbitrary
paper d written in year y was about topic z:
p?(z|y) =
?
d:td=y
p?(z|d)p?(d|y)
=
1
C
?
d:td=y
p?(z|d)
=
1
C
?
d:td=y
?
z?i?d
I(z?i = z)
(1)
where I is the indicator function, td is the date docu-
ment d was written, p?(d|y) is set to a constant 1/C.
3 Summary of Topics
We first ran LDA with 100 topics, and took 36 that
we found to be relevant. We then hand-selected seed
364
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Class
ificatio
n
Proba
bilistic
 Mode
ls
Stat. P
arsing Stat. M
T
Lex. S
em
Figure 1: Topics in the ACL Anthology that show a
strong recent increase in strength.
words for 10 more topics to improve coverage of the
field. These 46 topics were then used as priors to a
new 100-topic run. The top ten most frequent words
for 43 of the topics along with hand-assigned labels
are listed in Table 2. Topics deriving from manual
seeds are marked with an asterisk.
4 Historical Trends in Computational
Linguistics
Given the space of possible topics defined in the pre-
vious section, we now examine the history of these
in the entire ACL Anthology from 1978 until 2006.
To visualize some trends, we show the probability
mass associated with various topics over time, plot-
ted as (a smoothed version of) p?(z|y).
4.1 Topics Becoming More Prominent
Figure 1 shows topics that have becomemore promi-
nent more recently.
Of these new topics, the rise in probabilistic mod-
els and classification/tagging is unsurprising. In or-
der to distinguish these two topics, we show 20 of
the strongly weighted words:
Probabilistic Models: model word probability set data
number algorithm language corpus method figure proba-
bilities table test statistical distribution function al values
performance
Classification/Tagging: features data corpus set feature
table word tag al test accuracy pos classification perfor-
mance tags tagging text task information class
Some of the papers with the highest weights for
the probabilistic models class include:
N04-1039 Goodman, Joshua. Exponential Priors For Maximum
Entropy Models (HLT-NAACL, 2004)
W97-0309 Saul, Lawrence, Pereira, Fernando C. N. Aggregate And
Mixed-Order Markov Models For Statistical Language
Processing (EMNLP, 1997)
P96-1041 Chen, Stanley F., Goodman, Joshua. An Empirical
Study Of Smoothing Techniques For Language Model-
ing (ACL, 1996)
H89-2013 Church, Kenneth Ward, Gale, William A. Enhanced
Good-Turing And CatCal: Two New Methods For Esti-
mating Probabilities Of English Bigrams (Workshop On
Speech And Natural Language, 1989)
P02-1023 Gao, Jianfeng, Zhang, Min Improving Language Model
Size Reduction Using Better Pruning Criteria (ACL,
2002)
P94-1038 Dagan, Ido, Pereira, Fernando C. N. Similarity-Based
Estimation Of Word Cooccurrence Probabilities (ACL,
1994)
Some of the papers with the highest weights for
the classification/tagging class include:
W00-0713 Van Den Bosch, Antal Using Induced Rules As Com-
plex Features In Memory-Based Language Learning
(CoNLL, 2000)
W01-0709 Estabrooks, Andrew, Japkowicz, Nathalie AMixture-Of-
Experts Framework For Text Classification (Workshop
On Computational Natural Language Learning CoNLL,
2001)
A00-2035 Mikheev, Andrei. Tagging Sentence Boundaries (ANLP-
NAACL, 2000)
H92-1022 Brill, Eric. A Simple Rule-Based Part Of Speech Tagger
(Workshop On Speech And Natural Language, 1992)
As Figure 1 shows, probabilistic models seem to
have arrived significantly before classifiers. The
probabilistic model topic increases around 1988,
which seems to have been an important year for
probabilistic models, including high-impact papers
like A88-1019 and C88-1016 below. The ten papers
from 1988 with the highest weights for the proba-
bilistic model and classifier topics were the follow-
ing:
C88-1071 Kuhn, Roland. Speech Recognition and the Frequency
of Recently Used Words (COLING)
J88-1003 DeRose, Steven. Grammatical Category Disambiguation
by Statistical Optimization. (CL Journal)
C88-2133 Su, Keh-Yi, and Chang, Jing-Shin. Semantic and Syn-
tactic Aspects of Score Function. (COLING)
A88-1019 Church, Kenneth Ward. A Stochastic Parts Program and
Noun Phrase Parser for Unrestricted Text. (ANLP)
C88-2134 Sukhotin, B.V. Optimization Algorithms of Deciphering
as the Elements of a Linguistic Theory. (COLING)
P88-1013 Haigh, Robin, Sampson, Geoffrey, and Atwell, Eric.
Project APRIL: a progress report. (ACL)
A88-1005 Boggess, Lois. Two Simple Prediction Algorithms to Fa-
cilitate Text Production. (ANLP)
C88-1016 Peter F. Brown, et al A Statistical Approach to Machine
Translation. (COLING)
A88-1028 Oshika, Beatrice, et al. Computational Techniques for
Improved Name Search. (ANLP)
C88-1020 Campbell, W.N. Speech-rate Variation and the Prediction
of Duration. (COLING)
What do these early papers tell us about how
365
Anaphora Resolution resolution anaphora pronoun discourse antecedent pronouns coreference reference definite algorithm
Automata string state set finite context rule algorithm strings language symbol
Biomedical medical protein gene biomedical wkh abstracts medline patient clinical biological
Call Routing call caller routing calls destination vietnamese routed router destinations gorin
Categorial Grammar proof formula graph logic calculus axioms axiom theorem proofs lambek
Centering* centering cb discourse cf utterance center utterances theory coherence entities local
Classical MT japanese method case sentence analysis english dictionary figure japan word
Classification/Tagging features data corpus set feature table word tag al test
Comp. Phonology vowel phonological syllable phoneme stress phonetic phonology pronunciation vowels phonemes
Comp. Semantics* semantic logical semantics john sentence interpretation scope logic form set
Dialogue Systems user dialogue system speech information task spoken human utterance language
Discourse Relations discourse text structure relations rhetorical relation units coherence texts rst
Discourse Segment. segment segmentation segments chain chains boundaries boundary seg cohesion lexical
Events/Temporal event temporal time events tense state aspect reference relations relation
French Function de le des les en une est du par pour
Generation generation text system language information knowledge natural figure domain input
Genre Detection genre stylistic style genres fiction humor register biber authorship registers
Info. Extraction system text information muc extraction template names patterns pattern domain
Information Retrieval document documents query retrieval question information answer term text web
Lexical Semantics semantic relations domain noun corpus relation nouns lexical ontology patterns
MUC Terrorism slot incident tgt target id hum phys type fills perp
Metaphor metaphor literal metonymy metaphors metaphorical essay metonymic essays qualia analogy
Morphology word morphological lexicon form dictionary analysis morphology lexical stem arabic
Named Entities* entity named entities ne names ner recognition ace nes mentions mention
Paraphrase/RTE paraphrases paraphrase entailment paraphrasing textual para rte pascal entailed dagan
Parsing parsing grammar parser parse rule sentence input left grammars np
Plan-Based Dialogue plan discourse speaker action model goal act utterance user information
Probabilistic Models model word probability set data number algorithm language corpus method
Prosody prosodic speech pitch boundary prosody phrase boundaries accent repairs intonation
Semantic Roles* semantic verb frame argument verbs role roles predicate arguments
Yale School Semantics knowledge system semantic language concept representation information network concepts base
Sentiment subjective opinion sentiment negative polarity positive wiebe reviews sentence opinions
Speech Recognition speech recognition word system language data speaker error test spoken
Spell Correction errors error correction spelling ocr correct corrections checker basque corrected detection
Statistical MT english word alignment language source target sentence machine bilingual mt
Statistical Parsing dependency parsing treebank parser tree parse head model al np
Summarization sentence text evaluation document topic summary summarization human summaries score
Syntactic Structure verb noun syntactic sentence phrase np subject structure case clause
TAG Grammars* tree node trees nodes derivation tag root figure adjoining grammar
Unification feature structure grammar lexical constraints unification constraint type structures rule
WSD* word senses wordnet disambiguation lexical semantic context similarity dictionary
Word Segmentation chinese word character segmentation corpus dictionary korean language table system
WordNet* synset wordnet synsets hypernym ili wordnets hypernyms eurowordnet hyponym ewn wn
Table 2: Top 10 words for 43 of the topics. Starred topics are hand-seeded.
366
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Comp
utatio
nal Se
manti
cs
Conce
ptual 
Sema
ntics
Plan-
Base
d Dia
logue
 and D
iscour
se
Figure 2: Topics in the ACL Anthology that show a
strong decline from 1978 to 2006.
probabilistic models and classifiers entered the
field? First, not surprisingly, we note that the vast
majority (9 of 10) of the papers appeared in con-
ference proceedings rather than the journal, con-
firming that in general new ideas appear in confer-
ences. Second, of the 9 conference papers, most
of them appeared in the COLING conference (5) or
the ANLP workshop (3) compared to only 1 in the
ACL conference. This suggests that COLING may
have been more receptive than ACL to new ideas
at the time, a point we return to in Section 6. Fi-
nally, we examined the background of the authors of
these papers. Six of the 10 papers either focus on
speech (C88-1010, A88-1028, C88-1071) or were
written by authors who had previously published on
speech recognition topics, including the influential
IBM (Brown et al) and AT&T (Church) labs (C88-
1016, A88-1005, A88-1019). Speech recognition
is historically an electrical engineering field which
made quite early use of probabilistic and statistical
methodologies. This suggests that researchers work-
ing on spoken language processing were an impor-
tant conduit for the borrowing of statistical method-
ologies into computational linguistics.
4.2 Topics That Have Declined
Figure 2 shows several topics that were more promi-
nent at the beginning of the ACL but which have
shown the most precipitous decline. Papers strongly
associated with the plan-based dialogue topic in-
clude:
J99-1001 Carberry, Sandra, Lambert, Lynn. A Process Model For
Recognizing Communicative Acts And Modeling Nego-
tiation Subdialogues (CL, 1999)
J95-4001 McRoy, Susan W., Hirst, Graeme. The Repair Of Speech
Act Misunderstandings By Abductive Inference (CL,
1995)
P93-1039 Chu, Jennifer. Responding To User Queries In A Collab-
orative Environment (ACL, 1993)
P86-1032 Pollack, Martha E. A Model Of Plan Inference That
Distinguishes Between The Beliefs Of Actors And Ob-
servers (ACL, 1986)
T78-1017 Perrault, Raymond C., Allen, James F. Speech Acts As
A Basis For Understanding Dialogue Coherence (Theo-
retical Issues In Natural Language Processing, 1978)
P84-1063 Litman, Diane J., Allen, James F. A Plan Recognition
Model For Clarification Subdialogues (COLING-ACL,
1984)
Papers strongly associated with the computational
semantics topic include:
J90-4002 Haas, Andrew R. Sentential Semantics For Propositional
Attitudes (CL, 1990)
P83-1009 Hobbs, Jerry R. An Improper Treatment Of Quantifica-
tion In Ordinary English (ACL, 1983)
J87-1005 Hobbs, Jerry R., Shieber, Stuart M. An Algorithm For
Generating Quantifier Scopings (CL, 1987)
C90-1003 Johnson, Mark, Kay, Martin. Semantic Abstraction And
Anaphora (COLING, 1990)
P89-1004 Alshawi, Hiyan, Van Eijck, Jan. Logical Forms In The
Core Language Engine (ACL, 1989)
Papers strongly associated with the conceptual se-
mantics/story understanding topic include:
C80-1022 Ogawa, Hitoshi, Nishi, Junichiro, Tanaka, Kokichi. The
Knowledge Representation For A Story Understanding
And Simulation System (COLING, 1980)
A83-1012 Pazzani, Michael J., Engelman, Carl. Knowledge Based
Question Answering (ANLP, 1983)
P82-1029 McCoy, Kathleen F. Augmenting A Database Knowl-
edge Representation For Natural Language Generation
(ACL, 1982)
H86-1010 Ksiezyk, Tomasz, Grishman, Ralph An Equipment
Model And Its Role In The Interpretation Of Nominal
Compounds (Workshop On Strategic Computing - Natu-
ral Language, 1986)
P80-1030 Wilensky, Robert, Arens, Yigal. PHRAN - A
Knowledge-Based Natural Language Understander
(ACL, 1980)
A83-1013 Boguraev, Branimir K., Sparck Jones, Karen. How To
Drive A Database Front End Using General Semantic In-
formation (ANLP, 1983)
P79-1003 Small, Steven L. Word Expert Parsing (ACL, 1979)
The declines in both computational semantics and
conceptual semantics/story understanding suggests
that it is possible that the entire field of natural lan-
guage understanding and computational semantics
broadly construed has fallen out of favor. To see
if this was in fact the case we created a metatopic
called semantics in which we combined various se-
mantics topics (not including pragmatic topics like
anaphora resolution or discourse coherence) includ-
ing: lexical semantics, conceptual semantics/story
367
 
0
 
0.05 0.1
 
0.15 0.2
 
0.25
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Sema
ntics
Figure 3: Semantics over time
understanding, computational semantics, WordNet,
word sense disambiguation, semantic role labeling,
RTE and paraphrase, MUC information extraction,
and events/temporal. We then plotted p?(z ? S|y),
the sum of the proportions per year for these top-
ics, as shown in Figure 3. The steep decrease in se-
mantics is readily apparent. The last few years has
shown a levelling off of the decline, and possibly a
revival of this topic; this possibility will need to be
confirmed as we add data from 2007 and 2008.
We next chose two fields, Dialogue and Machine
Translation, in which it seemed to us that the topics
discovered by LDA suggested a shift in paradigms in
these fields. Figure 4 shows the shift in translation,
while Figure 5 shows the change in dialogue.
The shift toward statistical machine translation is
well known, at least anecdotally. The shift in di-
alogue seems to be a move toward more applied,
speech-oriented, or commercial dialogue systems
and away from more theoretical models.
Finally, Figure 6 shows the history of several top-
ics that peaked at intermediate points throughout the
history of the field. We can see the peak of unifica-
tion around 1990, of syntactic structure around 1985
of automata in 1985 and again in 1997, and of word
sense disambiguation around 1998.
5 Is Computational Linguistics Becoming
More Applied?
We don?t know whether our field is becoming more
applied, or whether perhaps there is a trend to-
wards new but unapplied theories. We therefore
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Statis
tical M
T
Class
ical M
T
Figure 4: Translation over time
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Dialo
gue S
ystem
s
Plan-
Base
d Dia
logue
 and D
iscour
se
Figure 5: Dialogue over time
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
TAG
Gene
ration Autom
ata
Unific
ation
Synta
ctic S
tructu
re Event
s WSD
Figure 6: Peaked topics
368
 
0
 
0.05 0.1
 
0.15 0.2
 
0.25
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Applic
ations
Figure 7: Applications over time
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Statis
tical M
T
Dialo
gue S
ystem
s
Spelli
ng Co
rrectio
n
Call R
outing
Speec
h Rec
ogniti
on
Biom
edica
l
Figure 8: Six applied topics over time
looked at trends over time for the following appli-
cations: Machine Translation, Spelling Correction,
Dialogue Systems, Information Retrieval, Call Rout-
ing, Speech Recognition, and Biomedical applica-
tions.
Figure 7 shows a clear trend toward an increase
in applications over time. The figure also shows an
interesting bump near 1990. Why was there such
a sharp temporary increase in applications at that
time? Figure 8 shows details for each application,
making it clear that the bump is caused by a tempo-
rary spike in the Speech Recognition topic.
In order to understand why we see this temporary
spike, Figure 9 shows the unsmoothed values of the
Speech Recognition topic prominence over time.
Figure 9 clearly shows a huge spike for the years
1989?1994. These years correspond exactly to the
DARPA Speech and Natural Language Workshop,
 
0
 
0.05 0.1
 
0.15 0.2
 
1980
 
1985
 
1990
 
1995
 
2000
 
2005
Figure 9: Speech recognition over time
held at different locations from 1989?1994. That
workshop contained a significant amount of speech
until its last year (1994), and then it was revived
in 2001 as the Human Language Technology work-
shop with a much smaller emphasis on speech pro-
cessing. It is clear from Figure 9 that there is still
some speech research appearing in the Anthology
after 1995, certainly more than the period before
1989, but it?s equally clear that speech recognition
is not an application that the ACL community has
been successful at attracting.
6 Differences and Similarities Among
COLING, ACL, and EMNLP
The computational linguistics community has two
distinct conferences, COLING and ACL, with dif-
ferent histories, organizing bodies, and philoso-
phies. Traditionally, COLING was larger, with par-
allel sessions and presumably a wide variety of top-
ics, while ACL had single sessions and a more nar-
row scope. In recent years, however, ACL has
moved to parallel sessions, and the conferences are
of similar size. Has the distinction in breadth of top-
ics also been blurred? What are the differences and
similarities in topics and trends between these two
conferences?
More recently, the EMNLP conference grew out
of the Workshop on Very Large Corpora, sponsored
by the Special Interest Group on Linguistic Data
and corpus-based approaches to NLP (SIGDAT).
EMNLP started as a much smaller and narrower
369
conference but more recently, while still smaller
than both COLING and ACL, it has grown large
enough to be considered with them. How does the
breadth of its topics compare with the others?
Our hypothesis, based on our intuitions as con-
ference attendees, is that ACL is still more narrow
in scope than COLING, but has broadened consid-
erably. Similarly, our hypothesis is that EMNLP has
begun to broaden considerably as well, although not
to the extent of the other two.
In addition, we?re interested in whether the topics
of these conferences are converging or not. Are the
probabilistic and machine learning trends that are
dominant in ACL becoming dominant in COLING
as well? Is EMNLP adopting some of the topics that
are popular at COLING?
To investigate both of these questions, we need a
model of the topic distribution for each conference.
We define the empirical distribution of a topic z at a
conference c, denoted by p?(z|c) by:
p?(z|c) =
?
d:cd=c
p?(z|d)p?(d|c)
=
1
C
?
d:cd=c
p?(z|d)
=
1
C
?
d:cd=c
?
z?i?d
I(z?i = z)
(2)
We also condition on the year for each conference,
giving us p?(z|y, c).
We propose to measure the breadth of a confer-
ence by using what we call topic entropy: the condi-
tional entropy of this conference topic distribution.
Entropy measures the average amount of informa-
tion expressed by each assignment to a random vari-
able. If a conference has higher topic entropy, then it
more evenly divides its probability mass across the
generated topics. If it has lower, it has a far more
narrow focus on just a couple of topics. We there-
fore measured topic entropy:
H(z|c, y) = ?
K?
i=1
p?(zi|c, y) log p?(zi|c, y) (3)
Figure 10 shows the conditional topic entropy
of each conference over time. We removed from
the ACL and COLING lines the years when ACL
 
3.6
 
3.8 4
 
4.2
 
4.4
 
4.6
 
4.8 5
 
5.2
 
5.4
 
5.6  1
980
 
1985
 
1990
 
1995
 
2000
 
2005
ACL C
onfere
nce COLIN
G
EMNL
P
Joint 
COLIN
G/AC
L
Figure 10: Entropy of the three major conferences per
year
and COLING are colocated (1984, 1998, 2006),
and marked those colocated years as points separate
from either plot. As expected, COLING has been
historically the broadest of the three conferences,
though perhaps slightly less so in recent years. ACL
started with a fairly narrow focus, but became nearly
as broad as COLING during the 1990?s. However, in
the past 8 years it has become more narrow again,
with a steeper decline in breadth than COLING.
EMNLP, true to its status as a ?Special Interest? con-
ference, began as a very narrowly focused confer-
ence, but now it seems to be catching up to at least
ACL in terms of the breadth of its focus.
Since the three major conferences seem to be con-
verging in terms of breadth, we investigated whether
or not the topic distributions of the conferences were
also converging. To do this, we plotted the Jensen-
Shannon (JS) divergence between each pair of con-
ferences. The Jensen-Shannon divergence is a sym-
metric measure of the similarity of two pairs of dis-
tributions. The measure is 0 only for identical dis-
tributions and approaches infinity as the two differ
more and more. Formally, it is defined as the aver-
age of the KL divergence of each distribution to the
average of the two distributions:
DJS(P ||Q) =
1
2
DKL(P ||R) +
1
2
DKL(Q||R)
R =
1
2
(P + Q)
(4)
Figure 11 shows the JS divergence between each
pair of conferences over time. Note that EMNLP
370
 
0
 
0.05 0.1
 
0.15 0.2
 
0.25 0.3
 
0.35 0.4
 
0.45 0.5  
1980
 
1985
 
1990
 
1995
 
2000
 
2005
ACL/C
OLING
ACL/E
MNLP
EMNL
P/COL
ING
Figure 11: JS Divergence between the three major con-
ferences
and COLING have historically met very infre-
quently in the same year, so those similarity scores
are plotted as points and not smoothed. The trend
across all three conferences is clear: each confer-
ence is not only increasing in breadth, but also in
similarity. In particular, EMNLP and ACL?s differ-
ences, once significant, are nearly erased.
7 Conclusion
Our method discovers a number of trends in the
field, such as the general increase in applications,
the steady decline in semantics, and its possible re-
versal. We also showed a convergence over time in
topic coverage of ACL, COLING, and EMNLP as
well an expansion of topic diversity. This growth
and convergence of the three conferences, perhaps
influenced by the need to increase recall (Church,
2005) seems to be leading toward a tripartite real-
ization of a single new ?latent? conference.
Acknowledgments
Many thanks to Bryan Gibson and Dragomir Radev
for providing us with the data behind the ACL An-
thology Network. Also to Sharon Goldwater and the
other members of the Stanford NLP Group as well
as project Mimir for helpful advice. Finally, many
thanks to the Office of the President, Stanford Uni-
versity, for partial funding.
References
Steven Bird. 2008. Association of Computational Lin-
guists Anthology. http://www.aclweb.org/anthology-
index/.
David Blei and John D. Lafferty. 2006. Dynamic topic
models. ICML.
David Blei, Andrew Ng, , and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
D. Blei, T. Gri, M. Jordan, and J. Tenenbaum. 2004. Hi-
erarchical topic models and the nested Chinese restau-
rant process.
Kenneth Church. 2005. Reviewing the reviewers. Com-
put. Linguist., 31(4):575?578.
Laura Dietz, Steffen Bickel, and Tobias Scheffer. 2007.
Unsupervised prediction of citation influences. In
ICML, pages 233?240. ACM.
Eugene Garfield. 1955. Citation indexes to science: A
new dimension in documentation through association
of ideas. Science, 122:108?111.
Tom L. Griffiths and Mark Steyvers. 2004. Finding sci-
entific topics. PNAS, 101 Suppl 1:5228?5235, April.
Yookyung Jo, Carl Lagoze, and C. Lee Giles. 2007.
Detecting research topics via the correlation between
graphs and texts. In KDD, pages 370?379, New York,
NY, USA. ACM.
Mark T. Joseph and Dragomir R. Radev. 2007. Citation
analysis, centrality, and the ACL anthology. Techni-
cal Report CSE-TR-535-07, University of Michigan.
Department of Electrical Engineering and Computer
Science.
Thomas S. Kuhn. 1962. The Structure of Scientific Rev-
olutions. University Of Chicago Press.
Wei Li and Andrew McCallum. 2006. Pachinko alloca-
tion: DAG-structured mixture models of topic correla-
tions. In ICML, pages 577?584, New York, NY, USA.
ACM.
Gideon S. Mann, David Mimno, and Andrew McCal-
lum. 2006. Bibliometric impact measures leveraging
topic analysis. In JCDL ?06: Proceedings of the 6th
ACM/IEEE-CS joint conference on Digital libraries,
pages 65?74, New York, NY, USA. ACM.
Xuerui Wang and Andrew McCallum. 2006. Topics over
time: a non-Markov continuous-time model of topical
trends. In KDD, pages 424?433, New York, NY, USA.
ACM.
371
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 698?706,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Jointly Combining Implicit Constraints Improves Temporal Ordering
Nathanael Chambers and Dan Jurafsky
Department of Computer Science
Stanford University
Stanford, CA 94305
{natec,jurafsky}@stanford.edu
Abstract
Previous work on ordering events in text has
typically focused on local pairwise decisions,
ignoring globally inconsistent labels. How-
ever, temporal ordering is the type of domain
in which global constraints should be rela-
tively easy to represent and reason over. This
paper presents a framework that informs lo-
cal decisions with two types of implicit global
constraints: transitivity (A before B and B be-
fore C implies A before C) and time expression
normalization (e.g. last month is before yes-
terday). We show how these constraints can
be used to create a more densely-connected
network of events, and how global consis-
tency can be enforced by incorporating these
constraints into an integer linear programming
framework. We present results on two event
ordering tasks, showing a 3.6% absolute in-
crease in the accuracy of before/after classifi-
cation over a pairwise model.
1 Introduction
Being able to temporally order events is a neces-
sary component for complete document understand-
ing. Interest in machine learning approaches for this
task has recently been encouraged through the cre-
ation of the Timebank Corpus (Pustejovsky et al,
2003). However, most work on event-event order-
ing has focused on improving classifiers for pair-
wise decisions, ignoring obvious contradictions in
the global space of events when misclassifications
occur. A global framework to repair these event or-
dering mistakes has not yet been explored.
This paper addresses three main factors involved
in a global framework: the global optimization al-
gorithm, the constraints that are relevant to the task,
and the level of connectedness across pairwise de-
cisions. We employ Integer Linear Programming to
address the first factor, drawing from related work
in paragraph ordering (Bramsen et al, 2006). After
finding minimal gain with the initial model, we ex-
plore reasons for and solutions to the remaining two
factors through temporal reasoning and transitivity
rule expansion.
We analyze the connectivity of the Timebank Cor-
pus and show how textual events can be indirectly
connected through a time normalization algorithm
that automatically creates new relations between
time expressions. We show how this increased con-
nectivity is essential for a global model to improve
performance.
We present three progressive evaluations of our
global model on the Timebank Corpus, showing a
3.6% gain in accuracy over its original set of re-
lations, and an 81% increase in training data size
from previous work. In addition, we present the first
results on Timebank that include an unknown rela-
tion, establishing a benchmark for performance on
the full task of document ordering.
2 Previous Work
Recent work on classifying temporal relations
within the Timebank Corpus built 6-way relation
classifiers over 6 of the corpus? 13 relations (Mani et
al., 2006; Mani et al, 2007; Chambers et al, 2007).
A wide range of features are used, ranging from sur-
face indicators to semantic classes. Classifiers make
698
local pairwise decisions and do not consider global
implications between the relations.
The TempEval-07 (Verhagen et al, 2007) contest
recently used two relations, before and after, in a
semi-complete textual classification task with a new
third relation to distinguish relations that can be la-
beled with high confidence from those that are un-
certain, called vague. The task was a simplified clas-
sification task from Timebank in that only one verb,
the main verb, of each sentence was used. Thus, the
task can be viewed as ordering the main events in
pairwise sentences rather than the entire document.
This paper uses the core relations of TempEval
(before,after,vague) and applies them to a full docu-
ment ordering task that includes every labeled event
in Timebank. In addition, we extend the previous
work by including a temporal reasoning component
and embedding it within a global constraint model.
3 The Timebank Corpus
The Timebank Corpus (Pustejovsky et al, 2003) is
a corpus of 186 newswire articles that are tagged
for events, time expressions, and relations between
the events and times. The individual events are fur-
ther tagged for temporal information such as tense,
modality and grammatical aspect. Time expressions
use the TimeML (Ingria and Pustejovsky, 2002)
markup language. There are 6 main relations and
their inverses in Timebank: before, ibefore, includes,
begins, ends and simultaneous.
This paper describes work that classifies the re-
lations between events, making use of relations be-
tween events and times, and between the times
themselves to help inform the decisions.
4 The Global Model
Our initial model has two components: (1) a pair-
wise classifier between events, and (2) a global con-
straint satisfaction layer that maximizes the confi-
dence scores from the classifier. The first is based
on previous work (Mani et al, 2006; Chambers et
al., 2007) and the second is a novel contribution to
event-event classification.
4.1 Pairwise Classification
Classifying the relation between two events is the
basis of our model. A soft classification with confi-
dence scores is important for the global maximiza-
tion step that is described in the next section. As
in Chambers et al (2007), we build support vec-
tor machine (SVM) classifiers and use the probabili-
ties from pairwise SVM decisions as our confidence
scores. These scores are then used to choose an op-
timal global ordering.
Following our previous work, we use the set of
features summarized in figure 1. They vary from
POS tags and lexical features surrounding the event,
to syntactic dominance, to whether or not the events
share the same tense, grammatical aspect, or aspec-
tual class. These features are the highest performing
set on the basic 6-way classification of Timebank.
Feature Description
Word* The text of the event
Lemma* The lemmatized head word
Synset* The WordNet synset of head word
POS* 4 POS tags, 3 before, and 1 event
POS bigram* The POS bigram of the event and its
preceding tag
Prep* Preposition lexeme, if in a preposi-
tional phrase
Tense* The event?s tense
Aspect* The event?s grammatical aspect
Modal* The modality of the event
Polarity* Positive or negative
Class* The aspecual class of the event
Tense Pair The two concatenated tenses
Aspect Pair The two concatenated aspects
Class Pair The two concatenated classes
POS Pair The two concatenated POS tags
Tense Match true if the events have the same tense
Aspect Match true if the events have the same as-
pect
Class Match true if the events have the same class
Dominates true if the first event syntactically
dominates the second
Text Order true if the first event occurs first in
the document
Entity Match true if they share an entity as an ar-
gument
Same Sent true if both events are in the same
sentence
Figure 1: The features to learn temporal relations be-
tween two events. Asterisks (*) indicate features that are
duplicated, one for each of the two events.
We use Timebank?s hand tagged attributes in the
feature values for the purposes of this comparative
699
before after unknown
A r1 B .5 .3 .2
B r2 C .4 .3 .3
A r3 C .4 .5 .1
total 1.3 1.1 .6
A r1 B .5 .3 .2
B r2 C .4 .3 .3
A r3 C .2 .7 .1
total 1.1 1.3 .6
Figure 2: Two sets of confidence scores. The first set
chooses before for all three labels, and the second chooses
after. Other lower-scoring valid relation sets also exist,
such as before, unknown, and before.
study of global constraints, described next.
4.2 Global Constraints
Pairwise classifiers can make contradictory classifi-
cations due to their inability to consider other deci-
sions. For instance, the following three decisions are
in conflict:
A before B
B before C
A after C
Transitivity is not taken into account. In fact, there
are several ways to resolve the conflict in this exam-
ple. Given confidence scores (or probabilities) for
each possible relation between the three pairs, we
can compute an optimal label assignment. Differ-
ent scores can lead to different conflict resolutions.
Figure 2 shows two resolutions given different sets
of scores. The first chooses before for all three rela-
tions, while the second chooses after.
Bramsen et al (2006) presented a variety of ap-
proaches to using transitivity constraints to help in-
form pairwise decisions. They found that Integer
Linear Programming (ILP) performed the best on a
paragraph ordering task, consistent with its property
of being able to find the optimal solution for a set
of constraints. Other approaches are variations on
a greedy strategy of adding pairs of events one at a
time, ordered by their confidence. These can lead to
suboptimal configurations, although they are guar-
anteed to find a solution. Mani et al (2007) sub-
sequently proposed one of these greedy strategies as
well, but published results are not available. We also
implemented a greedy best-first strategy, but found
ILP outperformed it.
Our Integer Linear Programming framework uses
the following objective function:
max
?
i
?
j
pijxij (1)
with added constraints:
?i?j xij ? {0, 1} (2)
?i xi1 + xi2 + ... + xim = 1 (3)
where xij represents the ith pair of events classified
as the jth relation of m relations. Thus, each pair
of events generates m variables. Given n pairs of
events, there are n ? m variables. pij is the proba-
bility of classifying pair i with relation j. Equation
2 (the first constraint) simply says that each variable
must be 0 or 1. Equation 3 contains m variables for
a single pair of events i representing its m possible
relations. It states that one relation must be set to 1
and the rest to 0. In other words, a pair of events
cannot have two relations at the same time. Finally,
a transitivity constraint is added for all connected
pairs i, j, k, for each transitivity condition that infers
relation c given a and b:
xia + xjb ? xkc <= 1 (4)
We generated the set of constraints for each doc-
ument and used lpsolve1 to solve the ILP constraint
problem.
The transitivity constraints are only effective if
the available pairwise decisions constitute a con-
nected graph. If pairs of events are disconnected,
then transitivity makes little to no contribution be-
cause these constraints are only applicable to con-
nected chains of events.
4.3 Transitive Closure
In order to connect the event graph, we draw on
work from (Mani et al, 2006) and apply transitive
closure to our documents. Transitive closure was
first proposed not to address the problem of con-
nected event graphs, but rather to expand the size
of training data for relations such as before. Time-
bank is a relatively small corpus with few examples
1http://sourceforge.net/projects/lpsolve
700
Total Event-Event Relations After Closure
before after
Timebank 592 656
+ closure 3919 3405
Figure 3: The number of event-event relations after tran-
sitive closure.
of each relation. One way of expand the training
set is through transitive rules. A few rules are given
here:
A simultaneous B ?A before C ? B before C
A includes B ?A ibefore C ? B before C
A before B ?A ends C ? B after C
While the original motivation was to expand the
training size of tagged relations, this approach also
creates new connections in the graph, replacing pre-
viously unlabeled event pairs with their true rela-
tions. We adopted this approach and closed the orig-
inal set of 12 relations to help connect the global
constraint model.
4.4 Initial Experiment
The first evaluation of our global temporal model
is on the Timebank Corpus over the labeled rela-
tions before and after. We merged ibefore and iafter
into these two relations as well, ignoring all oth-
ers. We use this task as a reduced evaluation to
study the specific contribution of global constraints.
We also chose this strict ordering task because it is
well defined from a human understanding perspec-
tive. Snow et al (2008) shows that average inter-
net users can make before/after decisions with very
high confidence, although the distinction with an un-
known relation is not as clear. An evaluation includ-
ing unknown (or vague as in TempEval) is presented
later.
We expanded the corpus (prior to selecting the be-
fore/after relations) using transitive closure over all
12 relations as described above. Figure 3 shows the
increase in data size. The number of before and after
relations increase by a factor of six.
We trained and tested the system with 10-fold
cross validation and micro-averaged accuracies. The
folds were randomly generated to separate the 186
files into 10 folds (18 or 19 files per fold). The same
10-way split is used for all the evaluations. We used
Comparative Results
Training Set Accuracy
Timebank Pairwise 66.8%
Global Model 66.8%
Figure 4: Using the base Timebank annotated tags for
testing, accuracy on before/after tags in the two models.
libsvm2 to implement our SVM classifiers.
Figure 4 shows the results from our ILP model
with transitivity constraints. The first row is the
baseline pairwise classification trained and tested on
the original Timebank relations. The second row
gives performance with ILP. The model shows no
improvement. The global ILP constraints did affect
local decisions, changing 175 of them (out of 7324),
but the changes cancelled out and had no affect on
overall accuracy.
4.5 Loosely Connected Graph
Why didn?t a global model help? The problem lies
in the graph structure of Timebank?s annotated rela-
tions. The Timebank annotators were not required
to annotate relations between any particular pair of
events. Instead, they were instructed to annotate
what seemed appropriate due to the almost insur-
mountable task of annotating all pairs of events. A
modest-sized document of 30 events, for example,
would contain
(30
2
)
= 435 possible pairs. Anno-
tators thus marked relations where they deemed fit,
most likely between obvious and critical relations to
the understanding of the article. The vast majority of
possible relations are untagged, thus leaving a large
set of unlabeled (and disconnected) unknown rela-
tions.
Figure 5 graphically shows all relations that are
annotated between events and time expressions in
one of the shorter Timebank documents. Nodes rep-
resent events and times (event nodes start with the
letter ?e?, times with ?t?), and edges represent tempo-
ral relations. Solid lines indicate hand annotations,
and dotted lines indicate new rules from transitive
closure (only one, from event e4 to time t14). As
can be seen, the graph is largely disconnected and
a global model contributes little information since
transitivity constraints cannot apply.
2http://www.csie.ntu.edu.tw/? cjlin/libsvm
701
Timebank Annotation of wsj 0551
Figure 5: Annotated relations in document wsj 0551.
The large amount of unlabeled relations in the
corpus presents several problems. First, building a
classifier for these unknown relations is easily over-
whelmed by the huge training set. Second, many of
the untagged pairs have non-unknown ordering rela-
tions between them, but were missed by the annota-
tors. This point is critical because one cannot filter
this noise when training an unknown classifier. The
noise problem will appear later and will be discussed
in our final experiment. Finally, the space of an-
notated events is very loosely connected and global
constraints cannot assist local decisions if the graph
is not connected. The results of this first experiment
illustrate this latter problem.
Bethard et al (2007) strengthen the claim that
many of Timebank?s untagged relations should not
be left unlabeled. They performed an independent
annotation of 129 of Timebank?s 186 documents,
tagging all events in verb-clause relationships. They
found over 600 valid before/after relations that are
untagged in Timebank, on average three per docu-
ment. One must assume that if these nearby verb-
clause event pairs were missed by the annotators,
the much larger number of pairs that cross sentence
boundaries were also missed.
The next model thus attempts to fill in some of the
gaps and further connect the event graph by using
two types of knowledge. The first is by integrating
Bethard?s data, and the second is to perform tempo-
ral reasoning over the document?s time expressions
(e.g. yesterday or january 1999).
5 A Global Model With Time
Our initial model contained two components: (1) a
pairwise classifier between events, and (2) a global
constraint satisfaction layer. However, due to the
sparseness in the event graph, we now introduce
a third component addressing connectivity: (3) a
temporal reasoning component to inter-connect the
global graph and assist in training data expansion.
One important aspect of transitive closure in-
cludes the event-time and time-time relations during
closure, not just the event-event links. Starting with
5,947 different types of relations, transitive rules in-
crease the dataset to approximately 12,000. How-
ever, this increase wasn?t enough to be effective in
global reasoning. To illustrate the sparsity that still
remains, if each document was a fully connected
graph of events, Timebank would contain close to
160,000 relations3, more than a 13-fold increase.
More data is needed to enrich the Timebank event
graph. Two types of information can help: (1) more
event-event relations, and (2) a separate type of in-
formation to indirectly connect the events: event-
X-event. We incorporate the new annotations from
Bethard et al (2007) to address (1) and introduce
a new temporal reasoning procedure to address (2).
The following section describes this novel approach
to adding time expression information to further
connect the graph.
5.1 Time-Time Information
As described above, we use event-time relations to
produce the transitive closure, as well as annotated
time-time relations. It is unclear if Mani et al (2006)
used these latter relations in their work.
However, we also add new time-time links that
are deduced from the logical time intervals that they
describe. Time expressions can be resolved to time
intervals with some accuracy through simple rules.
New time-time relations can then be added to our
space of events through time stamp comparisons.
Take this newswire example:
The Financial Times 100-share index shed 47.3 points to
close at 2082.1, down 4.5% from the previous Friday,
and 6.8% from Oct. 13, when Wall Street?s plunge helped
spark the current weakness in London.
3Sum over the # of events nd in each document d,
(
nd
2
)
702
The first two expressions (?previous Friday?
and ?Oct. 13?) are in a clear before relation-
ship that Timebank annotators captured. The
?current? expression, is correctly tagged with the
PRESENT REF attribute to refer to the document?s
timestamp. Both ?previous Friday? and ?Oct. 13?
should thus be tagged as being before this expres-
sion. However, the annotators did not tag either
of these two before relations, and so our timestamp
resolution procedure fills in these gaps. This is a
common example of two expressions that were not
tagged by the annotators, yet are in a clear temporal
relationship.
We use Timebank?s gold standard TimeML an-
notations to extract the dates and times from the
time expressions. In addition, those marked as
PRESENT REF are resolved to the document times-
tamp. Time intervals that are strictly before or after
each other are thus labeled and added to our space
of events. We create new before relations based on
the following procedure:
if event1.year < event2.year
return true
if event1.year == event2.year
if event1.month < event2.month
return true
if event1.month == event2.month
if event1.day < event2.day
return true
end
end
return false
All other time-time orderings not including the
before relation are ignored (i.e. includes is not cre-
ated, although could be with minor changes).
This new time-time knowledge is used in two sep-
arate stages of our model. The first is just prior to
transitive closure, enabling a larger expansion of our
tagged relations set and reduce the noise in the un-
known set. The second is in the constraint satisfac-
tion stage where we add our automatically computed
time-time relations (with the gold event-time rela-
tions) to the global graph to help correct local event-
event mistakes.
Total Event-Event Relations After Closure
before after
Timebank 3919 3405
+ time-time 5604 5118
+ time/bethard 7111 6170
Figure 6: The number of event-event before and after re-
lations after transitive closure on each dataset.
Comparative Results with Closure
Training Set Accuracy
Timebank Pairwise 66.8%
Global Model 66.8%
Global + time/bethard 70.4%
Figure 7: Using the base Timebank annotated tags for
testing, the increase in accuracy on before/after tags.
5.2 Temporal Reasoning Experiment
Our second evaluation continues the use of the two-
way classification task with before and after to ex-
plore the contribution of closure, time normaliza-
tion, and global constraints.
We augmented the corpus with the labeled rela-
tions from Bethard et al (2007) and added the au-
tomatically created time-time relations as described
in section 5.1. We then expanded the corpus using
transitive closure. Figure 6 shows the progressive
data size increase as we incrementally add each to
the closure algorithm.
The time-time generation component automati-
cally added 2459 new before and after time-time re-
lations into the 186 Timebank documents. This is
in comparison to only 157 relations that the human
annotators tagged, less than 1 per document on av-
erage. The second row of figure 6 shows the dras-
tic effect that these time-time relations have on the
number of available event-event relations for train-
ing and testing. Adding both Bethard?s data and
the time-time data increases our training set by 81%
over closure without it.
We again performed 10-fold cross validation with
micro-averaged accuracies, but each fold tested only
on the transitively closed Timebank data (the first
row of figure 6). The training set used all available
data (the third row of figure 6) including the Bethard
data as well as our new time-time links.
703
Figure 7 shows the results from the new model.
The first row is the baseline pairwise classification
trained and tested on the original relations only. Our
model improves by 3.6% absolute. This improve-
ment is statistically significant (p < 0.000001, Mc-
Nemar?s test, 2-tailed).
5.3 Discussion
To further illustrate why our model now improves
local decisions, we continue our previous graph ex-
ample. The actual text for the graph in figure 5 is
shown here:
docstamp: 10/30/89 (t14)
Trustcorp Inc. will become(e1) Society Bank & Trust
when its merger(e3) is completed(e4) with Society Corp.
of Cleveland, the bank said(e5). Society Corp., which is
also a bank, agreed(e6) in June(t15) to buy(e8) Trustcorp
for 12.4 million shares of stock with a market value of
about $450 million. The transaction(e9) is expected(e10)
to close(e2) around year end(t17).
The automatic time normalizer computes and adds
three new time-time relations, two connecting t15
and t17 with the document timestamp, and one con-
necting t15 and t17 together. These are not other-
wise tagged in the corpus.
Time-Time + Closure
Figure 8: Before and after time-time links with closure.
Figure 8 shows the augmented document. The
double-line arrows indicate the three new time-time
relations and the dotted edges are the new relations
added by our transitive closure procedure. Most crit-
ical to this paper, three of the new edges are event-
event relations that help to expand our training data.
If this document was used in testing (rather than
training), these new edges would help inform our
transitive rules during classification.
Even with this added information, disconnected
segments of the graph are still apparent. However,
the 3.6% performance gain encourages us to move
to the final full task.
6 Final Experiment with Unknowns
Our final evaluation expands the set of relations to
include unlabeled relations and tests on the entire
dataset available to us. The following is now a clas-
sification task between the three relations: before,
after, and unknown.
We duplicated the previous evaluation by adding
the labeled relations from Bethard et al (2007) and
our automatically created time-time relations. We
then expanded this dataset using transitive closure.
Unlike the previous evaluation, we also use this en-
tire dataset for testing, not just for training. Thus, all
event-event relations in Bethard as well as Timebank
are used to expand the dataset with transitive closure
and are used in training and testing. We wanted to
fully evaluate document performance on every pos-
sible event-event relation that logically follows from
the data.
As before, we converted IBefore and IAfter into
before and after respectively, while all other rela-
tions are reduced to unknown. This relation set co-
incides with TempEval-07?s core three relations (al-
though they use vague instead of unknown).
Rather than include all unlabeled pairs in our un-
known set, we only include the unlabeled pairs that
span at most one sentence boundary. In other words,
events in adjacent sentences are included in the un-
known set if they were not tagged by the Timebank
annotators. The intuition is that annotators are more
likely to label nearby events, and so events in adja-
cent sentences are more likely to be actual unknown
relations if they are unlabeled. It is more likely that
distant events in the text were overlooked by con-
venience, not because they truly constituted an un-
known relationship.
The set of possible sentence-adjacent unknown re-
lations is very large (approximately 50000 unknown
compared to 7000 before), and so we randomly se-
lect a percentage of these relations for each evalu-
704
Classification Accuracy
% unk base global global+time
0 72.0% 72.2% 74.0%
1 69.4% 69.5% 71.3%
3 65.5% 65.6% 67.1%
5 63.7% 63.8% 65.3%
7 61.2% 61.6% 62.8%
9 59.3% 59.5% 60.6%
11 58.1% 58.4% 59.4%
13 57.1% 57.1% 58.1%
Figure 9: Overall accuracy when training with different
percentages of unknown relations included. 13% of un-
knowns is about equal to the number of befores.
ation. We used the same SVM approach with the
features described in section 4.1.
6.1 Results
Results are presented in figure 9. The rows in the
table are different training/testing runs on varying
sizes of unknown training data. There are three
columns with accuracy results of increasing com-
plexity. The first, base, are results from pairwise
classification decisions over Timebank and Bethard
with no global model. The second, global, are re-
sults from the Integer Linear Programming global
constraints, using the pairwise confidence scores
from the base evaluation. Finally, the global+time
column shows the ILP results when all event-time,
time-time, and automatically induced time-time re-
lations are included in the global graph.
The ILP approach does not alone improve perfor-
mance on the event-event tagging task, but adding
the time expression relations greatly increases the
global constraint results. This is consistent with the
results from out first two experiments. The evalua-
tion with 1% of the unknown tags shows an almost
2% improvement in accuracy. The gain becomes
smaller as the unknown set increases in size (1.0%
gain with 13% unknown). Unknown relations will
tend to be chosen as more weight is given to un-
knowns. When there is a constraint conflict in the
global model, unknown tends to be chosen because
it has no transitive implications. All improvements
from base to global+time are statistically significant
(p < 0.000001, McNemar?s test, 2-tailed).
Base Pairwise Classification
precision recall f1-score
before 61.4 55.4 58.2
after 57.6 53.1 55.3
unk 53.0 62.8 57.5
Global+Time Classification
precision recall f1-score
before 63.7 (+2.3) 57.1 (+2.2) 60.2 (+2.0)
after 60.3 (+2.7) 54.3 (+2.9) 57.1 (+1.8)
unk 52.0 (-1.0) 62.9 (+0.1) 56.9 (-0.6)
Figure 10: Precision and Recall for the base pairwise de-
cisions and the global constraints with integrated time in-
formation.
The first row of figure 9 corresponds to the re-
sults in our second experiment in figure 7, but shows
higher accuracy. The reason is due to our different
test sets. This final experiment includes Bethard?s
event-event relations in testing. The improved per-
formance suggests that the clausal event-event rela-
tions are easier to classify, agreeing with the higher
accuracies originally found by Bethard et al (2007).
Figure 10 shows the precision, recall, and f-score
for the evaluation with 13% unknowns. This set was
chosen for comparison because it has a similar num-
ber of unknown labels as before labels. We see an
increase in precision in both the before and after de-
cisions by up to 2.7%, an increase in recall up to
2.9%, and an fscore by as much as 2.0%. The un-
known relation shows mixed results, possibly due to
its noisy behavior as discussed throughout this pa-
per.
6.2 Discussion
Our results on the two-way (before/after) task show
that adding additional implicit temporal constraints
and then performing global reasoning results in
significant improvements in temporal ordering of
events (3.6% absolute over simple pairwise deci-
sions).
Both before and after also showed increases in
precision and recall in the three-way evaluation.
However, unknown did not parallel this improve-
ment, nor are the increases as dramatic as in the two-
way evaluation. We believe this is consistent with
the noise that exists in the Timebank corpus for un-
labeled relations. Evidence from Bethard?s indepen-
705
dent annotations directly point to missing relations,
but the dramatic increase in the size of our closure
data (81%) from adding a small amount of time-time
relations suggests that the problem is widespread.
This noise in the unknown relation may be damp-
ening the gains that the two way task illustrates.
This work is also related to the task of event-time
classification. While not directly addressed in this
paper, the global methods described within clearly
apply to pairwise models of event-time ordering as
well.
Further progress in improving global constraints
will require new methods to more accurately iden-
tify unknown events, as well as new approaches to
create implicit constraints over the ordering. We ex-
pect such an improved ordering classifier to be used
to improve the performance of tasks such as summa-
rization and question answering about the temporal
nature of events.
Acknowledgments
This work is funded in part by DARPA through IBM
and by the DTO Phase III Program for AQUAINT.
We also thank our anonymous reviewers for many
helpful suggestions.
References
Steven Bethard, James H. Martin, and Sara Klingenstein.
2007. Timelines from text: Identification of syntac-
tic temporal relations. In International Conference on
Semantic Computing.
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Inducing temporal graphs.
In Proceedings of EMNLP-06.
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying temporal relations between events.
In Proceedings of ACL-07, Prague, Czech Republic.
R Ingria and James Pustejovsky. 2002. TimeML specifi-
cation 1.0. In http://www.time2002.org.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In Proceedings of ACL-06, July.
Inderjeet Mani, Ben Wellner, Marc Verhagen, and James
Pustejovsky. 2007. Three approaches to learning
tlinks in timeml. Technical Report CS-07-268, Bran-
deis University.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew
See, David Day, Lisa Ferro, Robert Gaizauskas, Mar-
cia Lazo, Andrea Setzer, and Beth Sundheim. 2003.
The timebank corpus. Corpus Linguistics, pages 647?
656.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Ng. 2008. Cheap and fast - but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP-08, Waikiki, Hawaii,
USA.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Workshop on Semantic Evalu-
ations.
706
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 334?342,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
It?s Not You, it?s Me: Detecting Flirting and its Misperception in
Speed-Dates
Rajesh Ranganath
Computer Science Department
Stanford University
rajeshr@cs.stanford.edu
Dan Jurafsky
Linguistics Department
Stanford University
jurafsky@stanford.edu
Dan McFarland
School of Education
Stanford University
dmcfarla@stanford.edu
Abstract
Automatically detecting human social in-
tentions from spoken conversation is an
important task for dialogue understand-
ing. Since the social intentions of the
speaker may differ from what is perceived
by the hearer, systems that analyze human
conversations need to be able to extract
both the perceived and the intended social
meaning. We investigate this difference
between intention and perception by using
a spoken corpus of speed-dates in which
both the speaker and the listener rated the
speaker on flirtatiousness. Our flirtation-
detection system uses prosodic, dialogue,
and lexical features to detect a speaker?s
intent to flirt with up to 71.5% accuracy,
significantly outperforming the baseline,
but also outperforming the human inter-
locuters. Our system addresses lexical fea-
ture sparsity given the small amount of
training data by using an autoencoder net-
work to map sparse lexical feature vectors
into 30 compressed features. Our analy-
sis shows that humans are very poor per-
ceivers of intended flirtatiousness, instead
often projecting their own intended behav-
ior onto their interlocutors.
1 Introduction
Detecting human social meaning is a difficult task
for automatic conversational understanding sys-
tems. One cause of this difficulty is the pervasive
difference between intended social signals and the
uptake by the perceiver. The cues that a speaker
may use to attempt to signal a particular social
meaning may not be the cues that the hearer fo-
cuses on, leading to misperception.
In order to understand the impact of this dif-
ference between perception and intention, in this
paper we describe machine learning models that
can detect both the social meaning intended by the
speaker and the social meaning perceived by the
hearer. Automated systems that detect and model
these differences can lead both to richer socially
aware systems for conversational understanding
and more sophisticated analyses of conversational
interactions like meetings and interviews.
This task thus extends the wide literature on
social meaning and its detection, including the
detection of emotions such as annoyance, anger,
sadness, or boredom (Ang et al, 2002; Lee and
Narayanan, 2002; Liscombe et al, 2003), speaker
characteristics such as charisma (Rosenberg and
Hirschberg, 2005), personality features like ex-
troversion or agreeability (Mairesse et al, 2007;
Mairesse and Walker, 2008), speaker depression
or stress (Rude et al, 2004; Pennebaker and Lay,
2002; Cohn et al, 2004), and dating willingness
or liking (Madan et al, 2005; Pentland, 2005).
We chose to work on the domain of flirtation
in speed-dating. Our earlier work on this cor-
pus showed that it is possible to detect whether
speakers are perceived as flirtatious, awkward, or
friendly with reasonable accuracy (Jurafsky et al,
2009). In this paper we extend that work to de-
tect whether speakers themselves intended to flirt,
explore the differences in these variables, and ex-
plore the ability and inability of humans to cor-
rectly perceive the flirtation cues.
While many of the features that we use to build
these detectors are drawn from the previous liter-
ature, we also explore new features. Conventional
methods for lexical feature extraction, for exam-
ple, generally consist of hand coded classes of
words related to concepts like sex or eating (Pen-
nebaker et al, 2007). The classes tend to per-
form well in their specific domains, but may not
be robust across domains, suggesting the need for
unsupervised domain-specific lexical feature ex-
traction. The naive answer to extracting domain-
334
specific lexical features would just be to throw
counts for every word into a huge feature vector,
but the curse of dimensionality rules this method
out in small training set situations. We propose
a new solution to this problem, using an unsuper-
vised deep autoencoder to automatically compress
and extract complex high level lexical features.
2 Dataset
Our experiments make use of the SpeedDate Cor-
pus collected by the third author, and described
in Jurafsky et al (2009). The corpus is based
on three speed-dating sessions run at an Ameri-
can university in 2005, inspired by prior speed-
dating research (Madan et al, 2005). The grad-
uate student participants volunteered to be in the
study and were promised emails of persons with
whom they reported mutual liking. All partici-
pants wore audio recorders on a shoulder sash,
thus resulting in two audio recordings of the ap-
proximately 1100 4-minute dates. Each date was
conducted in an open setting where there was sub-
stantial background noise. This noisy audio was
thus hand-transcribed and turn start and end were
hand-aligned with the audio. In addition to the au-
dio, the corpus includes various attitude and de-
mographic questions answered by the participants.
Each speaker was also asked to report how of-
ten their date?s speech reflected different conver-
sational styles (awkward, flirtatious, funny, as-
sertive) on a scale of 1-10 (1=never, 10=con-
stantly): ?How often did the other person behave
in the following ways on this ?date???. In addition
they were also asked to rate their own intentions:
?How often did you behave in the following ways
on this ?date??? on a scale of 1-10.
In this study, we focus on the flirtation ratings,
examining how often each participant said they
were flirting, as well as how often each participant
was judged by the interlocutor as flirting.
Of the original 1100 dates only 991 total dates
are in the SpeedDate corpus due to various losses
during recording or processing. The current study
focuses on 946 of these, for which we have com-
plete audio, transcript, and survey information.
3 Experiment
To understand how the perception of flirting dif-
fers from the intention of flirting, we trained bi-
nary classifiers to predict both perception and in-
tention. In each date, the speaker and the inter-
locutor both labeled the speaker?s behavioral traits
on a Likert scale from 1-10. To generate binary
responses we took the top ten percent of Likert
ratings in each task and labeled those as positive
examples. We similarly took the bottom ten per-
cent of Likert ratings and labeled those as negative
examples. We ran our binary classification exper-
iments to predict this output variable. Our experi-
ments were split by gender. For the female exper-
iment the speaker was female and the interlocu-
tor was male, while for the male experiment the
speaker was male and the interlocutor was female.
For each speaker side of each 4-minute conver-
sation, we extracted features from wavefiles and
transcripts, as described in the next section. We
then trained four separate binary classifiers (for
each gender for both perception and intention).
4 Feature Descriptions
We used the features reported by Jurafsky et
al. (2009), which are briefly summarized here.
The features for a conversation side thus indicate
whether a speaker who talks a lot, laughs, is more
disfluent, has higher F0, etc., is more or less likely
to consider themselves flirtatious, or be considered
flirtatious by the interlocutor. We also computed
the same features for the alter interlocutor. Al-
ter features thus indicate the conversational behav-
ior of the speaker talking with an interlocutor they
considered to be flirtatious or not.
4.1 Prosodic Features
F0 and RMS amplitude features were extracted us-
ing Praat scripts (Boersma and Weenink, 2005).
Since the start and end of each turn were time-
marked by hand, each feature was easily extracted
over a turn, and then averages and standard devia-
tions were taken over the turns in an entire conver-
sation side. Thus the feature F0 MIN for a conver-
sation side was computed by taking the F0 min of
each turn in that side (not counting zero values of
F0), and then averaging these values over all turns
in the side. F0 MIN SD is the standard deviation
across turns of this same measure.
4.2 Dialogue and Disfluency Features
A number of discourse features were extracted,
following Jurafsky et al (2009) and the dialogue
literature. The dialog acts shown in Table 2
were detected by hand-built regular expressions,
based on analyses of the dialogue acts in the
335
F0 MIN minimum (non-zero) F0 per turn, averaged
over turns
F0 MIN SD standard deviation from F0 min
F0 MAX maximum F0 per turn, averaged over turns
F0 MAX SD standard deviation from F0 max
F0 MEAN mean F0 per turn, averaged over turns
F0 MEAN SD standard deviation (across turns) from F0
mean
F0 SD standard deviation (within a turn) from F0
mean, averaged over turns
F0 SD SD standard deviation from the f0 sd
PITCH RANGE f0 max - f0 min per turn, averaged over
turns
PITCH RANGE SD standard deviation from mean pitch range
RMS MIN minimum amplitude per turn, averaged
over turns
RMS MIN SD standard deviation from RMS min
RMS MAX maximum amplitude per turn, averaged
over turns
RMS MAX SD standard deviation from RMS max
RMS MEAN mean amplitude per turn, averaged over
turns
RMS MEAN SD standard deviation from RMS mean
TURN DUR duration of turn in seconds, averaged over
turns
TIME total time for a speaker for a conversation
side, in seconds
RATE OF
SPEECH
number of words in turn divided by dura-
tion of turn in seconds, averaged over turns
Table 1: Prosodic features from Jurafsky et al
(2009) for each conversation side, extracted using
Praat from the hand-segmented turns of each side.
hand-labeled Switchboard corpus of dialog acts.
Collaborative completions, turns where a speaker
completes the utterance begun by the alter, were
detected by finding sentences for which the first
word of the speaker was extremely predictable
from the last two words of the previous speaker,
based on a trigram grammar trained on the Tree-
bank 3 Switchboard transcripts. Laughter, disflu-
encies, and overlap were all marked in the tran-
scripts by the transcribers.
4.3 Lexical Features
We drew our lexical features from the LIWC lex-
icons of Pennebaker et al (2007), the standard
for social psychological analysis of lexical fea-
tures. We chose ten LIWC categories that have
proven useful in detecting personality-related fea-
tures (Mairesse et al, 2007): Anger, Assent, In-
gest, Insight, Negemotion, Sexual, Swear, I, We,
and You. We also added two new lexical features:
?past tense auxiliary?, a heuristic for automati-
cally detecting narrative or story-telling behavior,
and Metadate, for discussion about the speed-date
itself. The features are summarized in Table 3.
4.4 Inducing New Lexical Features
In Jurafsky et al (2009) we found the LIWC lex-
ical features less useful in detecting social mean-
ing than the dialogue and prosodic features, per-
haps because lexical cues to flirtation lie in differ-
ent classes of words than previously investigated.
We therefore investigated the induction of lexical
features from the speed-date corpus, using a prob-
abilisitic graphical model.
We began with a pilot investigation to see
whether lexical cues were likely to be useful; with
a small corpus, it is possible that lexical fea-
tures are simply too sparse to play a role given
the limited data. The pilot was based on us-
ing Naive Bayes with word existence features (bi-
nomial Naive Bayes). Naive Bayes assumes all
features are conditionally independent given the
class, and is known to perform well with small
amounts of data (Rish, 2001). Our Naive Bayes
pilot system performed above chance, suggesting
that lexical cues are indeed informative.
A simple approach to including lexical fea-
tures in our more general classification system
would be to include the word counts in a high di-
mensional feature vector with our other features.
This method, unfortunately, would suffer from
the well-known high dimensionality/small train-
ing set problem. We propose a method for build-
ing a much smaller number of features that would
nonetheless capture lexical information. Our ap-
proach is based on using autoencoders to con-
struct high level lower dimension features from the
words in a nonlinear manner.
A deep autoencoder is a hierarchichal graphical
model with multiple layers. Each layer consists of
a number of units. The input layer has the same
number of units as the output layer, where the out-
put layer is the model?s reconstruction of the input
layer. The number of units in the intermediate lay-
ers tends to get progressively smaller to produce a
compact representation.
We defined our autoencoder with visible units
modeling the probabilities of the 1000 most com-
mon words in the conversation for the speaker
and the probabilities of the 1000 most common
words for the interlocutor (after first removing
a stop list of the most common words). We
train a deep autoencoder with stochastic nonlin-
ear feature detectors and linear feature detectors
in the final layer. As shown in Figure 1, we used
a 2000-1000-500-250-30 autoencoder. Autoen-
336
BACKCHANNELS number of backchannel utterances in side (Uh-huh., Yeah., Right., Oh, okay.)
APPRECIATIONS number of appreciations in side (Wow, That?s true, Oh, great)
QUESTIONS number of questions in side
NTRI repair question (Next Turn Repair Indicator) (Wait, Excuse me)
COMPLETION (an approximation to) utterances that were ?collaborative completions?
LAUGH number of instances of laughter in side
TURNS total number of turns in side
DISPREFERRED (approximation to) dispreferred responses, beginning with discourse marker well
UH/UM total number of filled pauses (uh or um) in conversation side
RESTART total number of disfluent restarts in conversation side
OVERLAP number of turns in side which the two speakers overlapped
Table 2: Dialog act and disfluency features from Jurafsky et al (2009).
TOTAL WORDS total number of words
PAST TENSE uses of past tense auxiliaries was, were, had
METADATE horn, date, bell, survey, speed, form, questionnaire, rushed, study, research
YOU you, you?d, you?ll, your, you?re, yours, you?ve (not counting you know)
WE lets, let?s, our, ours, ourselves, us, we, we?d, we?ll, we?re, we?ve
I I?d, I?ll, I?m, I?ve, me, mine, my, myself (not counting I mean)
ASSENT yeah, okay, cool, yes, awesome, absolutely, agree
SWEAR hell, sucks, damn, crap, shit, screw, heck, fuck*
INSIGHT think*/thought, feel*/felt, find/found, understand*, figure*, idea*, imagine, wonder
ANGER hate/hated, hell, ridiculous*, stupid, kill*, screwed, blame, sucks, mad, bother, shit
NEGEMOTION bad, weird, hate, crazy, problem*, difficult, tough, awkward, boring, wrong, sad, worry,
SEXUAL love*, passion*, virgin, sex, screw
INGEST food, eat*, water, bar/bars, drink*, cook*, dinner, coffee, wine, beer, restaurant, lunch, dish
Table 3: Lexical features from Jurafsky et al (2009). Each feature value is a total count of the words in
that class for each conversation side; asterisks indicate including suffixed forms (e.g., love, loves, loving).
All except the first three are from LIWC (Pennebaker et al, 2007) (modified slightly, e.g., by removing
you know and I mean). The last five classes include more words in addition to those shown.
coders tend to perform poorly if they are initialized
incorrectly, so we use the Restricted Boltzmann
Machine (RBM) pretraining procedure described
in Hinton and Salakhutdinov (2006) to initialize
the encoder. Each individual RBM is trained using
contrastive divergence as an update rule which has
been shown to produce reasonable results quickly
(Hinton et al, 2006). Finally, we use backpropa-
gation to fine tune the weights of our encoder by
minimizing the cross entropy error. To extract fea-
tures from each conversation, we sample the code
layer (30 unit layer in our encoder) with the visi-
ble units corresponding to the most common word
probabilities from that document, creating 30 new
features that we can use for classification. The
conditional distributions of the first layer features
can be given by the softmax of the activations for
each gender:
p(v
i
|h) =
exp(bias
i
+
?
j
h
j
? w
ij
)
?
k?K
exp(bias
k
+
?
j
v
j
? w
kj
)
(1)
p(h
j
|v) =
1
1 + exp(bias
(
j) +
?
i
v
i
? w
ij
)
(2)
where K is the set of all the units representing the
same speaker as i
1
, v
i
is the ith visible unit, h
j
is
the jth hidden unit, w
ij
is the weight between visi-
ble unit i and hidden unit j, and bias
m
is the offset
of unit m. Intuitively, this means that the proba-
bility that a hidden unit is activated by the visible
layer is sigmoid of the weighted sum of all the vis-
ible units plus the unit?s bias term. Similarly, the
visible units are activated through a weighted sum
of the hidden units, but they undergo an additional
normalization (softmax) over all the other visible
units from the speaker to effectively model the
multinomial distribution from each speaker. Since
in a RBM hidden units are conditionally indepen-
dent given the visible units, and visible units are
1
The visible unit i models word probabilities of either the
speaker or the interlocutor, so the softmax is done over the
distribution of words for the speaker that unit i is modeling.
337
conditionally independent given hidden layer, the
above equations completely specify the first layer
of the model.
To account for the fact that each visible unit in
the first layer contained 1000 observations from
the underlying distribution we upweighted our fea-
tures by that factor. During pretraining the ?train-
ing data? for the higher layers is the activation
probabilities of the hidden units of layer directly
below when driven by that layer?s input data. The
intermediate layers in the model are symmetric
where the activation probabilities for both the vis-
ible and hidden units are of the same form as
p(h
j
|v) in layer 1. To produce real valued features
in the code layer we used linear hidden units. In
addition to the likelihood portion of the objective
we penalized large weights by using l2 regulariza-
tion and penalize all weights by applying a small
constant weight cost that gets applied at every up-
date. After training to find a good initial point
for the autoencoder we unroll the weights and use
backpropogation to fine tune our autoencoder.
While interpreting high level nonlinear features
can be challenging, we did a pilot analysis of one
of the 30 features fixing a large (positive or neg-
ative) weight on the feature unit (code layer) and
sampling the output units.
The top weighted words for a positive weight
are: O did, O live, S did, S friends, S went,
O live, S lot, S wait, O two, and O wasn?t (S for
speaker and O for interlocutor). The top weighted
words for a negative weight are: S long, O school,
S school, S phd, O years, S years, O stanford,
S lot, O research, O interesting and O education.
At least for this one feature, a large positive value
seemed to indicate the prevalence of questions
(wait, did) or storytelling (
em live, wasn?t). A large negative weight indicates
the conversation focused on the mundane details
of grad student life.
5 Classification
Before performing the classification task, we pre-
processed the data in two ways. First, we stan-
dardized all the variables to have zero mean and
unit variance. We did this to avoid imposing a
prior on any of the features based on their numer-
ical values. Consider a feature A with mean 100
and a feature B with mean .1 where A and B are
correlated with the output. Since the SVM prob-
lem minimizes the norm of the weight vector, there
2000
2000
1000
500
500
250
250
30
1000
2000
1000
500
500
250
250
30
1000
2000
1000
500
500
250
250
30
1000
Dialogue:
F: ...
M: ...
F: ...
Dialogue: 
F: ...
M: ...
F: ...
Diaogue:
F: ...
M: ...
F: ...
Reconstruct
Dialogue:
F: ...
M: ...
Reconstruct
Dialogue:
F: ...
M: ...
W
1
W
2
W
3
W
4
W
5
W
1
W
2
W
3
W
4
W
T
5
W
T
4
W
5
2000 2000
W
T
3
W
T
2
RBM
RBM
RBM
 RBM
Code layer
Decoder
Encoder
Pretraining
Unrolling
Fine-tuning
W
T
1
W
1
+?
1
W
2
+?
2
W
3
+?
3
W
4
+?
4
W
5
+?
5
W
T
5
+?
6
W
T
4
+?
7
W
T
3
+?
8
W
T
2
+?
9
W
T
1
+?
10
Figure 1: Pretraining is a fully unsupervised pro-
cedure that trains an RBM at each layer. Once the
pretraining of one layer is complete, the top layer
units are used as input to the next layer. We then
fine-tune our weights using backprop. The 30 fea-
tures are extracted from the code layer.
is a bias to put weight on feature A because intu-
itively the weight on feature B would need to be
1000 times larger to carry the same effect. This
argument holds similarly for the reduction to unit
variance. Second, we removed features correlated
greater than .7. One goal of removing correlated
features was to remove as much colinearity as pos-
sible from the regression so that the regression
weights could be ranked for their importance in the
classification. In addition, we hoped to improve
classification because a large number of features
require more training examples (Ng, 2004). For
example for perception of female flirt we removed
the number of turns by the alter (O turns) and the
number of sentence from the ego (S sentences) be-
cause they were highly correlated with S turns.
To ensure comparisons (see Section 7) between
the interlocutors? ratings and our classifier (and
because of our small dataset) we use k-fold cross
validation to learn our model and evaluate our
model. We train our binary model with the top
ten percent of ratings labeled as positive class ex-
amples and bottom ten percent of ratings as the
negative class examples. We used five-fold cross
validation in which the data is split into five equal
folds of size 40. We used four of the folds for
training and one for test. K-fold cross validation
does this in a round robin manner so every exam-
338
ple ends up in the test set. This yields a datasplit
of 160 training examples and 40 test examples. To
ensure that we were not learning something spe-
cific to our data split, we randomized our data or-
dering.
For classification we used a support vector ma-
chine (SVM). SVMs generally do not produce ex-
plicit feature weights for analysis because they are
a kernelized classifier. We solved the linear C-
SVM problem. Normally the problem is solved
in the dual form, but to facilitate feature analysis
we expand back to the primal form to retrieve w,
the weight vector. Our goal in the C-SVM is to
solve, in primal form,
min
?,w,b
1
2
||w||
2
+ C
m
?
i=1
?
i
s.t. y
(i)
(w
T
x
(i)
+ b) ? 1? ?
i
, i = 1, . . . ,m
?
i
? 0, i = 1, . . . ,m (3)
where m is the number of training examples, x
(i)
is the ith training examples, and y
(i)
is the ith class
(1 for the positive class, -1 for the negative class).
The ?
i
are the slack variables that allow this algo-
rithm to work for non linearly separable datasets.
A test example is classified by looking at the
sign of y(x) = w
T
x
(test)
+ b. To explore mod-
els that captured interactions, but do not allow for
direct feature analysis we solved the C-SVM prob-
lem using a radial basis function (RBF) as a kernel
(Scholkopf et al, 1997). Our RBF kernel is based
on a Gaussian with unit variance.
K(x
(i)
, x
(j)
) = exp(
?||x
(i)
? x
(j)
||
2
2?
) (4)
In this case predictions can be made by looking
at y(x
(test)
) =
?
m
i=1
?
(i)
y
(i)
rbf(x
(i)
, t
(test)
) + b,
where each ?
(i)
, for i = 1, . . . ,m is a member
of the set of dual variables that comes from trans-
forming the primal form into the dual form. The
SVM kernel trick allows us to explore higher di-
mensions while limiting the curse of dimensional-
ity that plagues small datasets like ours.
We evaluated both our linear C-SVM and our
radial basis function C-SVM using parameters
learned on the training sets by computing the ac-
curacy on the test set. Accuracy is the number of
correct examples / total number of test examples.
We found that the RBM classifier that handled in-
teraction terms outperformed linear methods like
logistic regression.
For feature weight extraction we aggregated the
feature weights calculated from each of the test
folds by taking the mean between them.
2
6 Results
We report in Table 4 the results for detecting flirt
intention (whether a speaker said they were flirt-
ing) as well as flirt perception (whether the listener
said the speaker was flirting).
Flirt Intention Flirt Perception
by M by F of M of F
RBM SVM 61.5% 70.0% 77.0% 59.5%
+autoencoder 69.0% 71.5% 79.5% 68.0%
features
Table 4: Accuracy of binary classification of each
conversation side, where chance is 50%. The first
row uses all the Jurafsky et al (2009) features for
both the speaker and interlocutor. The second row
adds the new autoencoder features.
In our earlier study of flirt perception, we
achieved 71% accuracy for men and 60% for
women (Jurafsky et al, 2009). Our current num-
bers for flirt perception are much better for both
men (79.5%), and women (68.0%). The improve-
ment is due both to the new autoencoder features
and the RBF kernel that considers feature inter-
actions (feature interactions were not included in
the logistic regression classifiers of Jurafsky et al
(2009)).
Our number for flirt intention are 69.0% for men
and 71.5% for women. Note that our accuracies
are better for detecting women?s intentions as well
as women?s perceptions (of men) than men?s in-
tentions and perceptions.
7 Feature Analysis
We first considered the features that helped clas-
sification of flirt intention. Table 5 shows feature
weights for the features (features were normed so
weights are comparable), and is summarized in the
following paragraphs:
? Men who say they are flirting ask more ques-
tions, and use more you and we. They laugh more,
and use more sexual, anger, and negative emo-
tional words. Prosodically they speak faster, with
higher pitch, but quieter (lower intensity min).
2
We could not use the zero median criteria used in Juraf-
sky et al (2009) because C-SVMs under the l-2 metric pro-
vide no sparse weight guarantees.
339
FEMALE FLIRT MALE FLIRT
O backchannel -0.0369 S you 0.0279
S appreciation -0.0327 S negemotion 0.0249
O appreciation -0.0281 S we 0.0236
O question 0.0265 S anger 0.0190
O avimin -0.0249 S sexual 0.0184
S turns -0.0247 O negemotion 0.0180
S backchannel -0.0245 O avpmax 0.0174
O you 0.0239 O swear 0.0172
S avtndur 0.0229 O laugh 0.0164
S avpmin -0.0227 O wordcount 0.0151
O rate 0.0212 S laugh 0.0144
S laugh 0.0204 S rate 0.0143
S wordcount 0.0192 S well 0.0131
S well 0.0192 S question 0.0131
O negemotion 0.019 O sexual 0.0128
S repair q 0.0188 S completion 0.0128
O sexual 0.0176 S avpmax 0.011
O overlap -0.0176 O completion 0.010
O sdpmean 0.0171 O sdimin 0.010
O avimax -0.0151 O metatalk -0.012
S avpmean -0.015 S sdpsd -0.015
S question -0.0146 S avimin -0.015
O sdimin 0.0136 S backchannel -0.022
S avpmax 0.0131
S we -0.013
S I 0.0117
S assent 0.0114
S metatalk -0.0107
S sexual 0.0105
S avimin -0.0104
O uh -0.0102
Table 5: Feature weights (mean weights of the ran-
domized runs) for the predictors with |weight| >
0.01 for the male and female classifiers. An S pre-
fix indicates features of the speaker (the candidate
flirter) while an O prefix indicates features of the
other. Weights for autoencoder features were also
significant but are omitted for compactness.
Features of the alter (the woman) that helped
our system detect men who say they are flirting
include the woman?s laughing, sexual words or
swear words, talking more, and having a higher
f0 (max).
?Women who say they are flirting have a much
expanded pitch range (lower pitch min, higher
pitch max), laugh more, use more I and well, use
repair questions but not other kinds of questions,
use more sexual terms, use far less appreciations
and backchannels, and use fewer, longer turns,
with more words in general. Features of the alter
(the man) that helped our system detect women
who say they are flirting include the male use of
you, questions, and faster and quieter speech.
We also summarize here the features for the per-
ception classification task; predicting which peo-
ple will be labeled by their dates as flirting. Here
the task is the same as for Jurafsky et al (2009)
and the values are similar.
? Men who are labeled by their female date as
flirting present many of the same linguistic behav-
iors as when they express their intention to flirt.
Some of the largest differences are that men are
perceived to flirt when they use less appreciations
and overlap less, while these features were not sig-
nificant for men who said they were flirting. We
also found that fast speech and more questions are
more important features for flirtation perception
than intention.
? Women who are labeled by their male date
as flirting also present much of the same linguis-
tic behavior as women who intend to flirt. Laugh-
ter, repair questions, and taking fewer, longer turns
were not predictors of women labeled as flirting,
although these were strong predictors of women
intending to flirt.
Both genders convey intended flirtation by
laughing more, speaking faster, and using higher
pitch. However, we do find gender differences;
men ask more questions when they say they are
flirting, women ask fewer, although they do use
more repair questions, which men do not. Women
use more ?I? and less ?we?; men use more ?we?
and ?you?. Men labeled as flirting are softer, but
women labeled as flirting are not. Women flirting
use much fewer appreciations; appreciations were
not a significant factor in men flirting.
8 Human Performance on this task
To evaluate the performance of our classifiers we
compare against human labeled data.
We used the same test set as for our machine
classifier; recall that this was created by taking the
top ten percent of Likert ratings of the speaker?s
intention ratings by gender and called those posi-
tive for flirtation intention. We constructed nega-
tive examples by taking the bottom ten percent of
intention Likert ratings. We called the interlocu-
tor correct on the positive examples if the inter-
locutor?s rating was greater than 5. Symmetrically
for the negative examples, we said the interlocutor
was correct if their rating was less than or equal
to 5. Note that this metric is biased somewhat to-
ward the humans and against our systems, because
we do not penalize for intermediate values, while
the system is trained to make binary predictions
only on extremes. The results of the human per-
ceivers on classifying flirtation intent are shown in
Table 6.
340
Male speaker Female speaker
(Female perceiver) (Male perceiver)
62.2% 56.2%
Table 6: Accuracy of human listeners at labeling
speakers as flirting or not.
We were quite surprised by the poor quality of
the human results. Our system outperforms both
men?s performance in detecting women flirters
(system 71.5% versus human 56.2%) and also
women?s performance in detecting male flirters
(system 69.0% versus human 62.2%).
Why are humans worse than machines at detect-
ing flirtation? We found a key insight by examin-
ing how the participants in a date label themselves
and each other. Table 7 shows the 1-10 Likert val-
ues for the two participants in one of the dates,
between Male 101 and Female 127. The two par-
ticipants clearly had very different perspectives on
the date. More important, however, we see that
each participant labels their own flirting (almost)
identically with their partner?s flirting.
I am flirting Other is flirting
Male 101 says: 8 7
Female 127 says: 1 1
Table 7: Likert scores for the date between Female
127 and Male 101.
We therefore asked whether speakers in general
tend to assign similar values to their own flirting
and their partner?s flirting. The Pearson correla-
tion coefficient between these two variables (my
perception of my own flirting, and my perception
of other?s flirting) is .73. By contrast, the poor per-
formance of subjects at detecting flirting in their
partners is coherent with the lower (.15) correla-
tion coefficient between those two variables (my
perception of the other?s flirting, and the other?s
perception of their own flirting). This discrepancy
is summarized in boldface in Table 8.
Since the speed-date data was also labeled for
three other variables, we then asked the same
question about these variables. As Table 8 shows,
for all four styles, speakers? perception of others
is strongly correlated with the speakers? percep-
tion of themselves, far more so than with what the
others actually think they are doing.
3
3
This was true no matter how the correlations were run,
whether with raw Likert values, with ego-centered (trans-
formed) values and with self ego-centered but other raw.
Variable Self-perceive-Other
& Self-perceive-Self
Self-perceive-Other &
Other-perceive-Other
Flirting .73 .15
Friendly .77 .05
Awkward .58 .07
Assertive .58 .09
Table 8: Correlations between speaker intentions
and perception for all four styles.
Note that although perception of the other does
not correlate highly with the other?s intent for any
of the styles, the correlations are somewhat bet-
ter (.15) for flirting, perhaps because in the speed-
date setting speakers are focusing more on detect-
ing this behavior (Higgins and Bargh, 1987). It is
also possible that for styles with positive valence
(friendliness and flirting) speakers see more simi-
larity between the self and the other than for nega-
tive styles (awkward and assertive) (Krah?e, 1983).
Why should this strong bias exist to link self-
flirting with perception of the other? One pos-
sibility is that speakers are just not very good at
capturing the intentions of others in four minutes.
Speakers instead base their judgments on their
own behavior or intentions, perhaps because of a
bias to maintain consistency in attitudes and rela-
tions (Festinger, 1957; Taylor, 1970) or to assume
there is reciprocation in interpersonal perceptions
(Kenny, 1998).
9 Conclusion
We have presented a new system that is able to
predict flirtation intention better than humans can,
despite humans having access to vastly richer in-
formation (visual features, gesture, etc.). This sys-
tem facilitates the analysis of human perception
and human interaction and provides a framework
for understanding why humans perform so poorly
on intention prediction.
At the heart of our system is a core set of
prosodic, dialogue, and lexical features that al-
low for accurate prediction of both flirtation inten-
tion and flirtation perception. Since previous word
lists don?t capture sufficient lexical information,
we used an autoencoder to automatically capture
new lexical cues. The autoencoder shows potential
for being a promising feature extraction method
for social tasks where cues are domain specific.
Acknowledgments: Thanks to the anonymous review-
ers and to a Google Research Award for partial funding.
341
References
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and
A. Stolcke. 2002. Prosody-Based Automatic De-
tection of Annoyance and Frustration in Human-
Computer Dialog. In INTERSPEECH-02.
Paul Boersma and David Weenink. 2005. Praat: doing
phonetics by computer (version 4.3.14). [Computer
program]. Retrieved May 26, 2005, from http://
www.praat.org/.
M. A. Cohn, M. R. Mehl, and J. W. Pennebaker.
2004. Linguistic markers of psychological change
surrounding September 11, 2001. Psychological
Science, 15:687?693.
Leon Festinger. 1957. A Theory of Cognitive Disso-
nance. Row, Peterson, Evanston, IL.
E. Tory Higgins and John A. Bargh. 1987. Social cog-
nition and social perception. Annual Review of Psy-
chology, 38:369?425.
G. E. Hinton and R. R Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
G. E. Hinton, S. Osindero, and Y. Teh. 2006. A
fast learning algorithm for deep belief nets. Neural
Computation, 18:1527?1554.
Dan Jurafsky, Rajesh Ranganath, and Dan McFarland.
2009. Extracting social meaning: Identifying inter-
actional style in spoken conversation. In NAACL
HLT 2009, Boulder, CO.
David Kenny. 1998. Interpersonal Perception: A So-
cial Relations Analysis. Guilford Press, New York,
NY.
B. Krah?e. 1983. Self-serving biases in perceived simi-
larity and causal attributions of other people?s per-
formance. Social Psychology Quarterly, 46:318?
329.
C. M. Lee and Shrikanth S. Narayanan. 2002. Com-
bining acoustic and language information for emo-
tion recognition. In ICSLP-02, pages 873?876,
Denver, CO.
Jackson Liscombe, Jennifer Venditti, and Julia
Hirschberg. 2003. Classifying Subject Ratings
of Emotional Speech Using Acoustic Features. In
INTERSPEECH-03.
Anmol Madan, Ron Caneel, and Alex Pentland. 2005.
Voices of attraction. Presented at Augmented Cog-
nition, HCI 2005, Las Vegas.
Franc?ois Mairesse and Marilyn Walker. 2008. Train-
able generation of big-five personality styles through
data-driven parameter estimation. In ACL-08,
Columbus.
Franc?ois Mairesse, Marilyn Walker, Matthias Mehl,
and Roger Moore. 2007. Using linguistic cues for
the automatic recognition of personality in conver-
sation and text. Journal of Artificial Intelligence Re-
search (JAIR), 30:457?500.
Andrew Y. Ng. 2004. Feature selection, L1 vs. L2
regularization, and rotational invariance. In ICML
2004.
J. W. Pennebaker and T. C. Lay. 2002. Language use
and personality during crises: Analyses of Mayor
Rudolph Giuliani?s press conferences. Journal of
Research in Personality, 36:271?282.
J. W. Pennebaker, R.E. Booth, and M.E. Francis. 2007.
Linguistic inquiry and word count: LIWC2007 op-
erator?s manual. Technical report, University of
Texas.
Alex Pentland. 2005. Socially aware computation and
communication. Computer, pages 63?70.
Irina Rish. 2001. An empirical study of the naive
bayes classifier. In IJCAI 2001 Workshop on Em-
pirical Methods in Artificial Intelligence.
Andrew Rosenberg and Julia Hirschberg. 2005.
Acoustic/prosodic and lexical correlates of charis-
matic speech. In EUROSPEECH-05, pages 513?
516, Lisbon, Portugal.
S. S. Rude, E. M. Gortner, and J. W. Pennebaker.
2004. Language use of depressed and depression-
vulnerable college students. Cognition and Emo-
tion, 18:1121?1133.
B. Scholkopf, K.K. Sung, CJC Burges, F. Girosi,
P. Niyogi, T. Poggio, and V. Vapnik. 1997. Com-
paring support vector machines with Gaussian ker-
nels to radialbasis function classifiers. IEEE Trans-
actions on Signal Processing, 45(11):2758?2765.
Howard Taylor. 1970. Chapter 2. In Balance in
Small Groups. Von Nostrand Reinhold Company,
New York, NY.
342
Morphological features help POS tagging of unknown words across 
language varieties 
Huihsin Tseng 
Dept. of Linguistics 
University of Colorado 
Boulder, CO 80302 
tseng@colorado.edu
Daniel Jurafsky 
Dept. of Linguistics 
Stanford University 
Stanford, CA 94305 
jurafsky@stanford.edu
Christopher Manning 
Dept. of Computer Science 
Stanford University 
Stanford, CA 94305 
manning@stanford.edu
Abstract
Part-of-speech tagging, like any supervised statistical 
NLP task, is more difficult when test sets are very 
different from training sets, for example when tag-
ging across genres or language varieties. We exam-
ined the problem of POS tagging of different 
varieties of Mandarin Chinese (PRC-Mainland, PRC-
Hong Kong, and Taiwan). An analytic study first 
showed that unknown words were a major source of 
difficulty in cross-variety tagging. Unknown words 
in English tend to be proper nouns. By contrast, we 
found that Mandarin unknown words were mostly 
common nouns and verbs. We showed these results 
are caused by the high frequency of morphological 
compounding in Mandarin; in this sense Mandarin is 
more like German than English. Based on this analy-
sis, we propose a variety of new morphological un-
known-word features for POS tagging, extending 
earlier work by others on unknown-word tagging in 
English and German. Our features were implemented 
in a maximum entropy Markov model. Our system 
achieves state-of-the-art performance in Mandarin 
tagging, including improving unknown-word tagging 
performance on unseen varieties in Chinese Treebank 
5.0 from 61% to 80% correct. 
1 Introduction 
Part-of-speech tagging is an important enabling task 
for natural language processing, and state-of-the-art 
taggers perform quite well, when training and test 
data are drawn from the same corpus. Part-of-speech 
tagging is more difficult, however, when a test set is 
drawn from a corpus that includes significantly dif-
ferent varieties of the language. One factor that may 
play a role in this cross-variety difficulty is the pres-
ence of test-set words that were unseen in cross-
variety training sets. 
We chose Mandarin Chinese to study this question of 
cross-variety and unknown-word POS tagging. Man-
darin is both a spoken and a written language; as a 
written language, it is the official written language of 
the PRC (Mainland and Hong Kong), and Taiwan. 
Thus regardless of which dialect people speak at 
home, they write in Mandarin. But the varieties of 
Mandarin written in the PRC (Mainland and Hong 
Kong) and Taiwan differ in orthography, lexicon, 
and even grammar about as much as the British, 
American, and Australian varieties of English (or 
more in some cases). The corpus we use, Chinese 
Treebank 5.0 (Palmer et al, 2005), contains data 
from the three language varieties as well as different 
genres within the varieties. It thus provides a good 
data set for the impact of language variation on tag-
ging performance. 
Previous work on POS tagging of unknown words 
has proposed a number of features based on prefixes 
and suffixes and spelling cues like capitalization 
(Toutanova et al 2003, Brants 2000, Ratnaparkhi 
1996). For example, these systems followed 
Samuelsson (1993) in using n-grams of letter se-
quences ending and starting each word as unknown 
word features. But these features have mainly been 
tested on inflectional languages like English and 
German, whose derivational and inflectional affixes 
tend to be a strong indicator of word classes; Brants 
(2000), for example, showed that an English word 
ending in the suffix -able was very likely to be an 
adjective. Chinese, by contrast, has more than 4000 
frequent affix characters. The amount of training data 
for each affix is thus quite sparse and (as we will 
show later) Chinese affixes are quite ambiguous in 
their part-of-speech identity. Furthermore, it is possi-
ble that n-gram features may not be suited to Chinese 
at all, since Chinese words are much shorter than 
English (averaging 2.4 characters per word compared 
with 7.7 for English, for unknown words in CTB 5.0 
and Wall Street Journal (Marcus et el., 1993)). 
In order to deal with these difficulties, we first per-
formed an analytic study with the goal of understand-
ing the morphological properties of unknown words 
in Chinese. Based on this analysis, we then propose 
new morphological features for addressing the un-
known word problem. We also showed how these 
features could make use of a non-CTB corpus that 
had been labeled with very different POS tags, by 
converting those tags into features. 
32
The remainder of the paper is organized as follows. 
The next section is concerned with a corpus analysis 
of cross language variety differences and introduces 
Chinese morphology. In Section 3, we evaluate a 
number of lexical, sequence, and linguistic features. 
Section 4 reviews related work and summarizes our 
contribution.  
2 Data
Chinese Treebank 5.0 (CTB) contains 500K words of 
newspaper and magazine articles annotated with seg-
mentation, part-of-speech, and syntactic constituency 
information. It includes data from three major media 
sources, XH1 from PRC, HKSAR2 from Hong Kong, 
and SM3 from Taiwan. In terms of genre, both XH 
and HKSAR focus on politics and economic issues, 
and SM more on topics such as culture, health, edu-
cation and travel. All of the files in CTB are encoded 
using Guo Biao (GB) and use simplified characters.  
We did some cleanup of character encoding errors in 
CTB before running our experiments. Taiwan and 
Hong Kong still use the traditional forms of charac-
ters, while PRC-Mainland has adopted simplified 
forms of many characters, which also collapse some 
distinctions between characters. Additionally a dif-
ferent character set encoding is standardly used. The 
articles in HKSAR and SM originally used tradi-
tional characters and Big 5 encoding, but prior to 
inclusion in the CTB corpus they had been converted 
into simplified characters and GB. Some errors seem 
to have crept into this conversion process, acciden-
tally leaving traditional characters such as ?  instead 
of simplified ? (after),  ?  for ? (for),  ?? and 
?? and ??  (what), all of which we fixed. We 
also normalized half width numbers, alphabets, and 
punctuation to full width. Finally we removed the -
NONE- traces left over from CTB parse trees.  
3 Corpus analysis 
We begin with an analytic study of potential prob-
lems for POS tagging on cross language variety data.  
3.1 More unknown words across varieties? 
We first test our hypothesis that a test set from a dif-
ferent language variety will contain more unknown 
words. Table 1 has the number of words in our 
devset that were unseen in the XH-only training set 
(we describe our training/dev/test split more fully in 
the next section). The devset contains equal amounts 
of data from all three varieties (XH, HKSAR, and 
SM). As table 1 shows, in data taken from the same 
                                                          
1 Xinhua Agency 
2 Information Services Department of Hong Kong Special Admin-
istrative Region 
3 Sinorama magazine 
source as the training data (XH), 4.63% of the words 
were unseen in training, compared to the much larger 
numbers of unknown words in the cross-variety data-
sets (14.3% and 16.7%). Some of this difference is 
probably due to genre as well, especially for the out-
lier-genre SM set. 
Table 1 Percent of words in devset that were unseen in an 
XH-only  training set. See Table 4 for more details. 
Data Set Lang Variety Source Genre % unk
XH  Mainland 
Mandarin
Xinhua News 4.6
HKSAR Hong Kong 
Mandarin
HKSAR News 14.2
SM Taiwan 
Mandarin
Sino-
rama
Magazine 16.7
Devset Mix Mix Mix 12.0
3.2 What are the unknown words? 
In this section, we analyze the part-of-speech charac-
teristics of the unknown words in our devset. 
Table 2 Word class distribution of unknown words in 
devset, XH, HKSAR, SM. Devset represents the conjunc-
tion of the three varieties. CC, DT, LC, P, PN, PU, and SP 
are considered as closed classes by CTB. 
Word class Devset XH HKSAR SM
AD (adverb) 74 2 23 49
CC (coordinating conj.) 7 - - 7
CD (cardinal number) 151 108 23 20
DT (determiner) 10 - 6 4
FW (foreign words) 2 2 - -
JJ (other noun modifier) 79 14 38 27
LC (localizer/postposit) 1 - 1 -
M (measure word) 12 2 4 6
NN (common noun) 1128 131 520 477
NR (proper noun) 400 92 156 152
NT (temporal noun) 53 3 38 12
OD (ordinal number) 4 - 4 -
P (preposition) 16 1 8 7
PN (pronoun) 10 - 3 7
PU (punctuation) 361 - 110 251
SP(sentence final particle) 1 - - 1
VA(predicative adjective) 43 1 19 23
VV (other verbs) 497 25 215 257
Total 2849 381 1168 1300
Table 2 shows that the majority of Chinese unknown 
words are common nouns (NN) and verbs (VV). This 
holds both within and across different varieties. Be-
yond the content words, we find that 10.96% and 
21.31% of unknown words are function words in 
HKSAR and SM data. Such unknown function words 
include the determiner gewei (?everybody?), the con-
junction huoshi (?or?), the preposition liantong
(?with?), the pronoun nali (?where?), and symbols 
used as quotes  ??? and ??? (punctuation). XH 
does contain words with similar function (huozhe
33
?or?, yu ?with?, dajia ?everybody?, quotation marks 
???and ???). Our result thus suggests that each 
Mandarin variety may have characteristic function 
words. 
3.3 Cross language comparison 
A key goal of our work is to understand the way that 
unknown words differ across languages. We thus 
compare Chinese, German, and English. Following 
Brants (2000), we extracted 10% of the data from the 
Penn Treebank Wall Street Journal (WSJ 4 ) and 
NEGRA5 (Brants et al, 1999) as observation samples 
to compare to the rest of the corpora. 
In these observation samples, we found that Chinese 
words are more ambiguous in POS than English and 
German; 29.9% of tokens in CTB have more than 
one POS tag, while only 19.8% and 22.9% of tokens 
are ambiguous in English and German, respectively. 
Table 3 shows that 40.6% of unknown words are 
proper nouns6  in English, while both Chinese and 
German have less than 15% of unknown words as 
proper nouns. Unlike English, 60% of the unknown 
words in Chinese and German are verbs and common 
nouns. In the next section we investigate the cause of 
this similarity between Chinese and German un-
known word distribution. 
Table 3 Comparison of unknown words in English, Ger-
man and Mandarin. The English and German data are ex-
tracted from WSJ and NEGRA. Chinese data is our CTB 
devset.
Language English% German% Chinese%
Proper nouns 40.6 12.2 14.0 
Other nouns 24.0 53.0 41.5 
Verbs 6.8 11.4 19.0 
ALL 100.0 100.0 100.0 
4 Morphological analysis 
In order to understand the causes of the similarity of 
Chinese and German, and to help suggest possible 
features, we turn here to an introduction to Chinese 
morphology and its implications for part-of-speech 
tagging. 
                                                          
4 WSJ unknown words are those in WSJ 19-21 but unseen in WSJ 
0-18; these are the devset and training set from Toutanova et al 
(2003). 
5 The unknown words of NEGRA are words in a 10% randomly 
extracted set that were unseen in the rest of the corpus. 
6 We treat NNP (proper noun) and NNPS(proper noun plural) as 
proper nouns, NN(noun) and NNS(noun plural) as other nouns, 
and V* as verbs in WSJ. We treat NE (Eigennamen) as proper 
nouns, NN (Normales Nomen) as other nouns, and V* as verbs in 
NEGRA. We treat NR as proper nouns, NN and NT as other nouns, 
and V* as verbs in CTB.  
4.1 Chinese morphology 
Chinese words are typically formed by four morpho-
logical processes: affixation, compounding, idiomi-
zation, and reduplication, as shown in Table 4. 
In affixation, a bound morpheme is added to other 
morphemes, forming a larger unit. Chinese has a 
small number of prefixes and infixes7 and numerous 
suffixes (Chao 1968, Li and Thompson 1981). Chi-
nese prefixes include items such as gui (?noble?) in 
guixing (?your name?), bu (?not?) in budaode (?im-
moral?), and  lao (?senior?) in laohu (?tiger?) and 
laoshu (?mouse?). There are a number of Chinese 
suffixes, including zhe (?marks a person who is an 
agent of an action?) in zuozhe (?author?), shi (?mas-
ter?) in laoshi (?teacher?), ran (-ly) in huran (?sud-
denly?), and xin (-ity or ?ness) in kenengxin
(?possibility?). 
Compound words are composed of multiple stem 
morphemes. Chao (1968) describes a few of the dif-
ferent compounding rules in Mandarin, such as coor-
dinate compound, subject predicate compound, noun 
noun compound, adj noun compound and so on. Two 
examples of coordinate compounds are anpai
ARRANGE-ARRANGE (?to arrange, arrangement?) 
and xuexi STUDY-STUDY (?to study?). 
Table 4 Chinese morphological rules and examples 
 Examples 
Prefix lao (?senior?) in laohu ( ?tiger?)  
Suffix shi (?master?) in laoshi (?teacher?) 
Compounding xuexi  (?to study?, ?study?) 
Idiomization wanshiruyi (?everything is fine?) 
Reduplication changchang (?taste a bit?) 
Compounding is extremely common in both Chinese 
and German. The phrase ?income tax? is treated as 
an NP in English, but it is a word in German, Ein-
kommensteuer, and in Chinese, suodesui. We suggest 
that it is this rich use of compounding that causes the 
wide variety of unknown common nouns and verbs 
in Chinese and German. However, there are still dif-
ferences in their compound rules. German com-
pounds can compose with a large number of elements, 
but Chinese compounds normally consist of two 
bases. Most German compounds are nouns, but Chi-
nese has both noun and verb compounds.  
Two final types of Chinese morphological processes 
that we will not focus on are idiomization (in which a 
whole phrase such as wanshiruyi (?everything is 
fine?) functions as a word, and reduplication, in 
which a morpheme or word is repeated to form a new 
word such as the formation of changchang (?taste a 
                                                          
7 Chinese only has two infixes, which are de and bu (not). We do 
not discuss infixes in the paper, because they are handled phrasally 
rather than lexically in CTB. 
34
bit?), from chang ?taste?. (Chao 1968, Li and 
Thompson 1981).  
4.2 Difficulty
The morphological characteristics of Chinese create 
various problems for part-of-speech tagging. First, 
Chinese suffixes are short and sparse. Because of the 
prevalence of compounding and the fact that the mor-
phemes are short (1 character long), there are more 
than 4000 affixes. This means that the identity of an 
affix is often a sparsely-seen feature for predicting 
POS. Second, Chinese affixes are poor cues to POS 
because they are ambiguous; for example 63% of 
Chinese suffix tokens in CTB have more than one 
possible tag, while only 31% of English suffix tokens 
in WSJ have more than one tag. Most English suf-
fixes are derivational and inflectional suffixes like   -
able, -s and -ed. Such functional suffixes are used to 
indicate word classes or syntactic function. Chinese, 
however, has no inflectional suffixes and only a few 
derivational suffixes and so suffixes may not be as 
good a cue for word classes. Finally, since Chinese 
has no derivational morpheme for nominalization, it 
is difficult to distinguish a nominalization and a verb.  
These points suggest that morpheme identity, which 
is the major feature used in previous research on un-
known words in English and German, will be insuffi-
cient in Chinese. This suggests the need for more 
sophisticated features, which we will introduce be-
low.  
5 Experiments
We evaluate our tagger under several experimental 
conditions: after showing the effects of data cleanup 
we show basic results based on features found to be 
useful by previous research. Next, we introduce addi-
tional morphology-based unknown word features, 
and finally, we experiment with training data of vari-
able sizes and different language varieties. 
5.1 Data sets 
To study the significance of training on different 
varieties of data, we created three training sets: train-
ing set I contains data only from one variety, training 
set II contains data from 3 varieties, and is similar in 
total size to training set I. Training set III also con-
tains data from 3 varieties and has twice much data 
as training set I. To facilitate comparison of perform-
ance both between and within Mandarin varieties, 
both the devset and the test set we created are com-
posed of three varieties of data. The XH test data we 
selected was identical to the test set used in previous 
parsing research by Bikel and Chiang (2000). For the 
remaining data, we included HKSAR and SM data 
that is similar in size to the XH test set. Table 5 de-
tails characteristics of the data sets. 
Table 5 Data set splits used. The unknown word tokens are 
with respect to Training I. 
Data set Sect'ns Token Un-
known
Training I 26-270, 600-931 213986 - 
Training II 600-931, 500-527,  
1001-1039
204701 - 
Training III 001-270, 301-527,  
590-593, 600-1039,  
1043-1151
485321 - 
Devset  23839 2849 
XH 001-025 7844 381 
HKSAR 500-527 8202 1168 
SM 590-593, 1001-1002 7793 1300 
Test set  23522 2957 
XH 271-300 8008 358 
HKSAR 528-554 7153 1020 
SM 594-596, 1040-1042 8361 1579 
5.2 The model 
Our model builds on research into loglinear models 
by Ng and Low (2004), Toutanova et al, (2003) and 
Ratnaparkhi (1996). The first research uses inde-
pendent maximum entropy classifiers, with a se-
quence model imposing categorical valid tag 
sequence constraints. The latter two use maximum 
entropy Markov models (MEMM) (McCallum et al, 
2000), that use log-linear  models to obtain the prob-
abilities of a state transition given an observation and 
the previous state, as illustrated in Figure 1 (a).  
Figure 1 Graphical representation of transition probability 
calculation used in maximum entropy Markov models. (a) 
The previous state and the current word are used to calcu-
late the transition probabilities for the next state transition. 
(b) Same as (a), but when model is run right to left. 
Using left-to-right transition probabilities, as in Fig-
ure 1 (a), the equation for the MEMM can be for-
mally stated as the following, where by di represents 
the set of features the transition probabilities are con-
ditioned on: 
( ) ( )iii d|tPwt,P ?=
Maximum entropy is used to calculate the probability 
P(ti| di) using the equation below. Here, fj(ti,di) repre-
sents a feature derived from the available contextual 
information (e.g. current word, previous tag, next 
word, etc.) 
TiTi-1 
Wi
Ti
Wi
(a) (b) 
Ti+1 
35
( )
( )
( )? ?
?
?
=
Tt'
i
j
ii
j
i
)d,t'exp(
)d,t(exp
d|tP i f
f
jj
jj
?
?
We also used Gaussian prior to prevent overfitting. 
This technique allows us to utilize a large number of 
lexical and MEMM state sequence based features 
and also provides an intuitive framework for the use 
of morphological features generated from unknown 
word models. 
5.3 Data cleanup 
Before investigating the effect of our new features, 
we show the effects of data cleanup. Table 6 illus-
trates the .46 (absolute) performance gain obtained 
by cleaning character encoding errors and normaliz-
ing half width to full width.  
We also clustered punctuation symbols, since train-
ing set I has too many (36) variety of punctuations, 
compared to 9 in WSJ. We clustered punctuations, 
for example grouping ??? and ??? together. This 
mapping renders an overall improvement of .08%. 
All models in the following sections are then trained 
on font-normalized and punctuation clustered data. 
Table 6 Improvement of tagging accuracy after data 
cleanup. The features used by all of the models are the 
identity of the two previous words, the current word and 
the two following word. No features based on the sequence 
of tags were used. 
Models Token A8 % ? Token A% Unk A % 
2Rw+2Lw 87.11 - 47.03 
+Cleanup 87.57 0.46 48.54 
+PU 87.65 0.08 49.26 
5.4 Sequence features 
We examined several tag sequence features from 
both left and right side of the current word. We use 
the term lexical features to refer to features derived 
from the identity of a word, and tag sequence fea-
tures refer to features derived from the tags of sur-
rounding words.  
These features have been shown to be useful in pre-
vious research on English (Toutanova et al 2003, 
Brants 2000, Thede and Harper 1999) 
The models9 in Table 7 list the different tag sequence 
features used; they also use the same lexical features 
from the model 2Rw+2Lw shown in Table 6. The ta-
ble shows that Model Lt+LLt conditioning on the 
previous tag and the conjunction of the two previous 
                                                          
8 We abbreviate accuracy as ?A?. 
9 Except where otherwise stated, during training, a count cutoff of 
3 is applied to all features found in the training set. If a feature 
occurs fewer than 3 times, it is simply removed from the training 
data. All models are trained on training set I and evaluated on the 
devset.  
tags yields 88.27%. As such, using the sequence fea-
tures<ti-1, ti-1ti-2> achieves the current best result.  
So far, there are no features specifically tailored to-
ward unknown words in the model. 
Table 7 Tagging accuracy of different sequence feature sets.  
Models Feature sets Token A 
%
Unk A %
Rt+RRt
+2Rw+2Lw
<ti,ti+1>,<ti,ti+1,ti+2>
+ lexical features 
88.10 50.11 
Lt+LLt
+2Rw+2Lw
<ti,ti-1>,<ti,ti-1,ti-2>
+lexical features 
88.27 51.16 
5.5 Unknown word model 
Starting with Model Lt+LLt from the last section, we 
introduce 8 features to improve the performance of 
the tagger on unknown words. In the sections that 
follow, the model using affixation in conjunction 
with the basic lexical features described above is 
considered to be our baseline. 
We considered words that occur less than 7 times in 
the training set I as rare; if Wi is rare, an unknown 
word feature is used in place of a feature based on 
the actual word?s identity. During evaluation, un-
known word features are used for all words that oc-
curred zero to 7 times in the training data. In addition, 
when tagging such rare and unknown words, we re-
strict the set of possible tags to just those tags that 
were associated with one or more rare words in the 
training data. 
5.5.1 Affixation
Our affixation feature is motivated by similar fea-
tures seen in inflectional language models. (Ng and 
Low 2004, Toutanova et al 2003, Brants 2000, Rat-
naparkhi 1996, Samuelsson 1993). Since Chinese 
also has affixation, it is reasonable to incorporate this 
feature into our model. For this feature, we use char-
acter n-gram prefixes and suffixes for n up to 4.10 An 
example is:  
??? INFORMATION-BAG "folder"  
Wi=??? ?a folder? 
FAFFIX={(prefix1,?), (prefix2,??), (prefix3,??
?), (suffix1,?), (suffix2,??), (suffix3,???)} 
5.5.2 CTBMorph (CTBM) 
While affix information can be very informative, we 
showed earlier that affixes in Chinese are sparse, 
short, and ambiguous.  Thus as our first new feature 
we used a POS-vector of the set of tags a given affix 
could have. We used the training set to build a mor-
pheme/POS dictionary with the possible tags for each 
                                                          
10 Despite the short average word length, we found that affixes up 
to size 4 worked better than affixes only up to size 2, perhaps 
mainly because they help with long proper nouns and temporal 
expressions. 
36
morpheme. Thus for each prefix and suffix that oc-
curs with each CTB tag in the training set I, we asso-
ciate a set of binary features corresponding to each 
CTB tag. In the example below the prefix ? oc-
curred in both NN and VV words, but not AD or AS. 
Prefix1=?, suffix1=?
FCTBM-pre= {(AD,0),(AS,0),?(NN,1),?(VV,1)} 
FCTBM-suf= {(AD,0),(AS,0),?(NN,1),?(VV,0)} 
This model smoothes affix identity and the quantity 
of active CTBMorph features for a given affix ex-
presses the degree of ambiguity associated with that 
affix.  
Figure 2 Pseudo-code for CTBMorph
GenCTBMorphFeatureSet (Word W) 
  FeatureSet f; 
  for each t in CTB tag set: 
     for each single-character prefix or suffix k of W 
       if t.affixList contain k f.appendPair(t, 1) 
        else f.appendPair(t, 0)  
5.5.3 ASBC
One way to deal with robustness is to add more var-
ied training data.  For example the Academic Sinica 
Balanced Corpus11 contains POS-tagged data from a 
different variety (Taiwanese Mandarin). But the tags 
in this corpus are not easily converted to the CTB 
tags. This problem of labeled data from very differ-
ent tagsets can happen more generally. We introduce 
two alternative methods for making use of such a 
corpus.
5.5.3.1 ASBCMorph (ASBCM) 
The ASBCMorph feature set is generated in an iden-
tical manner to the CTBMorph feature set, except 
that rather than generating the morpheme table using 
CTB, another corpus is used. The morpheme table is 
generated from the Academic Sinica Balanced Cor-
pus, ASBC (Huang and Chen 1995), a 5 M word 
balanced corpus written in Taiwanese Mandarin. As 
the CTB annotation guide12 states, the mapping be-
tween the tag sets used in the two corpora is non-
trivial. As such, the ASBC data can not be directly 
used to augment the training set. However, using our 
ASBCMorph feature, we are still able to derive some 
benefit out of such an alternative corpus.  
5.5.3.2 ASBCWord (ASBCW) 
The ASBCWord feature is identical to the 
ASBCMorph feature, except that instead of using a 
table of tags that occur with each affix, we use a table 
of tags that a word occurs with in the ASBC data. 
                                                          
11 The ASBC was originally encoded in traditional Big5 character, 
and we converted it to simplified GB. 
12 http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf 
Thus, a rare word in the CTB training/test set is 
augmented with features that correspond to all of the 
tags that the given word occurred with in the ASBC 
corpus, i.e. in this case, the pos tag of the identical 
word in ASBC, ???.
Wi=???
FASBCWord={(A,0),(Caa,0),(Cab,0)?(V_2,0)}  
5.5.4 Verb affix 
This feature set contains only two feature values, 
based on whether a list of verb affixes contains the 
prefix or suffix of an unknown word. We use the 
verb affix list created by the Chinese Knowledge 
Information Processing Group13 at Academia Sinica. 
It contains 735 frequent verb prefixes and 282 fre-
quent verb suffixes. For  example, 
Prefix1=?,  suffix1=?
Fverb={(verb prefix, 1), (verb suffix, 0)} 
5.5.5 Radicals
Radicals are the basic building blocks of Chinese 
characters. There are over 214 radicals, and all Chi-
nese characters contain one or more of them. Some-
times radicals reflect the meaning of a character. For 
example, the characters ?  (monkey), ?  (pig) ?
(kitty cat) all contain the radical ?  that roughly 
means ?something that is an animal?. For our radical 
based feature, we use the radical map from the Uni-
han database.14 The radicals associated with the char-
acters in the prefix and suffix of unknown words 
were incorporated into the model as features, for ex-
ample: 
Prefix1=?, suffix1=?
FRADICAL={(radical prefix, ?), (radical suffix,?)} 
5.5.6 Named Entity Morpheme (NEM) 
There is a convention that the suffix of a named en-
tity points out the essential meaning of the named 
entity. For example, the suffix bao (newspaper) ap-
pears in Chinese translation of ?WSJ?, huaerjierebao.
The suffix he (river) is used to identify rivers, for 
example in huanghe (yellow river).  
To take advantage of this fact, we made 3 tables of 
named entity characters from the Chinese English 
Named Entity Lists (CENEL) (Huang 2002). These 
lists consist of a table of Chinese first name charac-
ters, a table of Chinese last name characters, and a
                                                          
13 http://turing.iis.sinica.edu.tw/affix/ 
14 Unihan database is downloadable from their website: 
http://www.unicode.org/charts/unihan.html. 
37
Table 8 Devset performance of the cumulatively rare word models, starting with the baseline. The second and third columns show the 
change in token accuracies and unknown word accuracies from the baseline for each feature introduced cumulatively. The fourth column 
shows the improvement from each feature set. The six columns on the right side of the table shows the error rate for the 5 most frequent 
tagsets of unknown words and the rest of unknown words.  
 Error analysis: error rate % of unknown words in each POS 
Feature (add one in) Token Unk A% ? Unk A% NN VV NR PU CD Others 
Lt+LLt 88.27 51.16 - 16.67 57.14 68.25 100.00 16.56 60.86 
+Suffix 89.70 60.74 9.58 12.50 41.65 44.75 100.00 5.30 37.25 
  +Prefix ? baseline 90.03 63.66 2.92 10.55 36.62 40.00 100.00 3.97 34.76 
    +CTBM 91.48 76.13 12.47 13.74 31.99 36.00 1.99 0.00 20.58 
       +ASBCM 91.69 77.36 1.23 14.01 28.37 33.75 1.99 0.66 19.57 
         +ASBCW 91.85 78.84 1.48 13.30 23.54 33.50 1.42 0.00 17.93 
           +Verb affix 91.82 79.05 0.21 12.59 24.14 32.75 0.85 0.00 17.76 
              +Radical 91.85 79.09 0.04 11.88 24.75 33.50 0.85 0.00 18.78 
                +NEM 91.91 79.61 0.53 12.23 23.54 31.00 0.85 0.00 18.39 
                   +Length?best 91.97 79.86 0.25 12.15 22.94 30.25 0.85 0.00 18.21 
table of named entity suffixes such as organization, 
place, and company names in CENEL. Our named 
entity feature set contains 3 features, each corre-
sponding to one of the three tables just described. To 
generate these features, first, we check if the prefix 
of an unknown is in the Chinese last name table. Sec-
ond, we check if the suffix is in the Chinese first 
name table. Third, we check if the suffix of an un-
known word is in the table of named entity suffixes. 
In Chinese last names are written in front of a first 
name, and the whole name is considered as a word, 
for example: 
Prefix1=?,  suffix1=?
FNEM={(last name, 0), (first name, 0), (NE suffix, 
1)}
5.5.7 Length of a word 
The length of a word can be a useful feature, because 
the majority of words in CTB have less than 3 char-
acters. Words that have more than 3 characters are 
normally proper nouns, numbers, and idioms. There-
fore, we incorporate this feature into the system. For 
example:  
Wi=???, Flength={(length , 3)} 
5.5.8 Evaluation
Table 8 shows our results using the standard maxi-
mum entropy forward feature selection algorithm; at 
each iteration we add the feature family that most 
significantly improves the log likelihood of the train-
ing data given the model. We seed the feature space 
search with the features in Model Lt+LLt. From this 
model, adding suffix information gives a 9.58% (ab-
solute) gain on unknown word tagging. Subsequently 
adding in prefix makes unknown word accuracy go 
up to 63.66%. Our first result is that Chinese affixes 
are indeed informative for unknown words.  On the 
right side of Table 8, we can see that this perform-
ance gain is derived from better tagging of common 
nouns, verbs, proper nouns, numbers and others. Be-
cause earlier work in many languages including Chi-
nese uses these simple prefix and suffix features 
(Brants 2000, Ng and Low 2004) we consider this 
performance (63.66% on unknown words) as our 
baseline. 
Adding in the feature set CTBM gives another 
12.47% (absolute) improvement on unknown words. 
With this feature, punctuation shows the largest tag-
ging improvement. The CTBM feature helps to iden-
tify punctuation since all other characters have been 
seen in different morpheme table made from the 
training set. That is, for a given word the lack of 
CTBM features cues that the word is a punctuation 
mark. Also, while this feature set generally helps all 
tagsets, it hurts a bit on nouns. 
Adding in the ASBC feature sets yields another 
1.23% and 1.48% (absolute) gains on unknown 
words. These two feature sets generally improve per-
formance on all tagsets. Including the verb affix fea-
ture helps with common nouns and proper nouns, but 
hurts the performance on verbs. Overall, it yields 
0.21% gain on unknown words. Finally, adding the 
radical feature helps the most on nouns, while subse-
quently adding in the name entity morphemes help to 
reduce the error on proper nouns by 2.50%. Finally, 
adding in feature length renders a 0.25% gain on 
unknown words.  Commutatively, applying the fea-
ture sets results in an overall accuracy of 91.97% and 
an unknown word accuracy of 79.86%. 
5.6   Experiments with the training sets of 
variable sizes and varieties 
In this section, we compare our best model with the 
baseline model using different corpora size and lan-
guage varieties in the training set. All the evaluations 
are reported on the test set, which has roughly equal 
amounts of data from XH, HKSAR, and SM. 
The left column of Table 9 shows that when we train 
a model only on a single language variety and test on 
a mixed variety data, our unknown word accuracy is 
79.50%, which is 18.48% (absolute) better than the 
baseline. The middle column shows when the train-
ing set is composed of different varieties and hence 
looks like the test set, performance of both the base-
line and our best model improves. 
38
Table 9 Comparison of the baseline and our best model. 
Using different training sets to evaluate on the test set. 
(McNemar?s Test  p <.001)
 Training  I Training  II Training III 
Token Unk Token Unk  Token Unk 
Base-
line
89.17 61.02 92.54 74.78 93.51 81.11
Best 91.34 79.50 93.00 81.62 93.74 86.33
The right column shows the effect of doubling the 
training set size, using mixed varieties. As expected, 
using more data benefits both models.  
These results show that having training data from 
different varieties is better than having data from one 
source. But crucially, our morphological-based fea-
tures improve the tagging performance on unknown 
words even when the training set includes some data 
that resembles the test set. 
How good are our best numbers, i.e. 93.7% on POS 
tagging in CTB 5.0? Unfortunately, there are no 
clean direct comparisons in the literature. The closest 
result in the literature is Xue et al (2002), who re-
train the Ratnaparkhi (1996) tagger and reach accu-
racies of 93% using CTB-I. However CTB-I contains 
only XH data and furthermore the data split is no 
longer known for this experiment (Xue p.c.) so a 
comparison is not informative. However, our per-
formance on tagging when trained on Training I and 
tested on just the XH part of the test set is 94.44%, 
which might be a more relevant comparison to Xue 
et al (2002).   
6 Conclusion
Previous research in part-of-speech tagging has re-
sulted in taggers that perform well when the training 
set and test set are both drawn from the same corpus. 
Unfortunately, for many potential real world applica-
tions, such an arrangement is just not possible.  
Our results show that using sophisticated morpho-
logical features can help solve this robustness prob-
lem. These features would presumably also be 
applicable to other languages and NLP tasks that 
could benefit from the use of morphological informa-
tion 
Besides these tagging results, our research provides 
valuable analytic results on understanding the nature 
of unknown words cross-linguistically. Our results 
that unknown words in Chinese are not proper nouns 
like in English, but rather common nouns and verbs, 
suggest a similarity to German. We suggest this is 
because both German and Chinese, despite their huge 
differences in genetic, area, and other typological 
characteristics, tend to form unknown words through 
a similar word formation rule, compounding.  
7 Acknowledgement 
Thanks to Kristina Toutanova and Galen Andrew for 
their generous help and to the anonymous reviewers. 
This work was partially funded by ARDA 
AQUAINT and by NSF award IIS-0325646. 
8 References 
Bikel, Daniel and David Chiang. 2000. Two statisti-
cal parsing models applied to the Chinese Tree-
bank. In CLP 2.
Brants, Thorsten. 2000. TnT: a statistical part-of-
speech tagger. In ANLP 6. 
Brants, Thorsten Wojciech Skut, Hans Uszkoreit. 
1999. Syntactic Annotation of a German Newspa-
per Corpus In: Anne Abeill?: ATALA sur le Corpus 
Annot?s pour la Syntaxe Treebanks.
Chao, Yuen Ren. 1968. A Grammar of Spoken Chi-
nese. Berkeley: University of California Press. 
Huang, Chu-ren. and Keh-Jiann Chen. 1995. Aca-
demic Sinica Balanced Corpus. Technical Report 
95-02/98-04. Academic Sinica. 
Huang, Shudong. 2002. Chinese <-> English Name 
Entity Lists Version 1.0 beta. Catalog number: 
LDC2003E01. 
Li, Charles and Sandra A Thompson. 1981. Manda-
rin Chinese: A Functional Reference Grammar.
Berkeley: University of California Press. 
McCallum, Andrew, Dayne Freitag, Fernando 
Pereira. 2000. Maximum Entropy Markov Models 
for Information Extraction and Segmentation. In
ICML 17. 
Marcus, Mitchel, Beatrice Santorini and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: The Penn Treebank. In Compu-
tational Linguistics, 19. 
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese Part-
of-Speech Tagging: One-at-a-Time or All-at-Once? 
Word-Based or Character-Based? In EMNLP 9.  
Martha Palmer, Fu-Dong Chiou, Nianwen Xue, 
Tsan-Kuang Lee. 2005. Chinese Treebank 5.0. 
Catalog number: LDC2005T01. 
Ratnaparkhi, Adwait. 1996. A maximum entropy 
model forpart-of-speech tagging. In EMNLP 1.
Thede, Scott and Mary P. Harper. 1999. Second-
order hidden Markov model for part-of-speech 
tagging. In ACL 37.
Toutanova, Kristina, Dan Klein, Christopher Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In HLT-NAACL 2003. 
Samuelsson, Christer. 1993. Morphological tagging 
based entirely on bayesian inference. In NCCL 9.
Xue, Nianwen, Fu-dong Chiou and Martha Palmer. 
2002. Building a large-scale annotated Chinese 
corpus. In COLING.
39
 A Conditional Random Field Word Segmenter  
for Sighan Bakeoff 2005 
Huihsin Tseng 
Dept. of Linguistics 
University of Colorado 
Boulder, CO 80302 
tseng@colorado.edu
Pichuan Chang, Galen Andrew,  
Daniel Jurafsky, Christopher Manning 
Stanford Natural Language Processing Group 
Stanford University 
Stanford, CA 94309 
{pichuan, pupochik, jurafsky, manning}@stanford.edu 
Abstract
We present a Chinese word seg-
mentation system submitted to the 
closed track of Sighan bakeoff 2005. 
Our segmenter was built using a condi-
tional random field sequence model 
that provides a framework to use a 
large number of linguistic features such 
as character identity, morphological 
and character reduplication features. 
Because our morphological features 
were extracted from the training cor-
pora automatically, our system was not 
biased toward any particular variety of 
Mandarin. Thus, our system does not 
overfit the variety of Mandarin most 
familiar to the system's designers. Our 
final system achieved a F-score of 
0.947 (AS), 0.943 (HK), 0.950 (PK) 
and 0.964 (MSR). 
1 Introduction 
The 2005 Sighan Bakeoff included four dif-
ferent corpora, Academia Sinica (AS), City 
University of Hong Kong (HK), Peking Univer-
sity (PK), and Microsoft Research Asia (MSR), 
each of which has its own definition of a word. 
In the 2003 Sighan Bakeoff (Sproat & Emer-
son 2003), no single model performed well on 
all corpora included in the task. Rather, systems 
tended to do well on corpora largely drawn from 
a set of similar Mandarin varieties to the one 
they were originally developed for. Across cor-
pora, variation is seen in both the lexicons and 
also in the word segmentation standards. We 
concluded that, for future systems, generaliza-
tion across such different Mandarin varieties is 
crucial. To this end, we proposed a new model 
using character identity, morphological and 
character reduplication features in a conditional 
random field modeling framework. 
2 Algorithm
Our system builds on research into condi-
tional random field (CRF), a statistical sequence 
modeling framework first introduced by Lafferty 
et al (2001). Work by Peng et al (2004) first 
used this framework for Chinese word segmen-
tation by treating it as a binary decision task, 
such that each character is labeled either as the 
beginning of a word or the continuation of one. 
Gaussian priors were used to prevent overfitting 
and a quasi-Newton method was used for pa-
rameter optimization.  
The probability assigned to a label sequence 
for a particular sequence of characters by a CRF 
is given by the equation below: 
( ) ( )??
?
??
?
= ??
?Cc k
c cXYkkXZ
XYP f ,,exp)(
1| ??
Y is the label sequence for the sentence, X is 
the sequence of unsegmented characters, Z(X) is 
a normalization term, fk is a feature function, and 
c indexes into characters in the sequence being 
labeled.
A CRF allows us to utilize a large number of 
n-gram features and different state sequence 
168
based features and also provides an intuitive 
framework for the use of morphological features.  
3 Feature engineering 
3.1 Features
The linguistic features used in our model fall 
into three categories: character identity n-grams,
morphological and character reduplication fea-
tures.
For each state, the character identity features 
(Ng & Low 2004, Xue & Shen 2003, Goh et al 
2003) are represented using feature functions 
that key off of the identity of the character in the 
current, proceeding and subsequent positions. 
Specifically, we used four types of unigram fea-
ture functions, designated as C0 (current charac-
ter), C1 (next character), C-1 (previous character), 
C-2 (the character two characters back). Fur-
thermore, four types of bi-gram features were 
used, and are notationally designated here as 
conjunctions of the previously specified unigram 
features, C0C1, C-1C0, C-1C1, C-2C-1, and C2C0.
Given that unknown words are normally 
more than one character long, when representing 
the morphological features as feature functions, 
such feature functions keyed off the morpho-
logical information extracted from both the pro-
ceeding state and the current state. Our morpho-
logical features are based upon the intuition re-
garding unknown word features given in Gao et 
al. (2004). Specifically, their idea was to use 
productive affixes and characters that only oc-
curred independently to predict boundaries of 
unknown words. To construct a table containing 
affixes of unknown words, rather than using 
threshold-filtered affix tables in a separate un-
known word model as was done in Gao et al 
(2004), we first extracted rare words from a cor-
pus and then collected the first and last charac-
ters to construct the prefix and suffix tables. For 
the table of individual character words, we col-
lected an individual character word table for 
each corpus of the characters that always oc-
curred alone as a separate word in the given cor-
pus. We also collected a list of bi-grams from 
each training corpus to distinguish known 
strings from unknown. Adopting all the features 
together in a model and using the automatically 
generated morphological tables prevented our 
system from manually overfitting the Mandarin 
varieties we are most familiar with.  
The tables are used in the following ways: 
1) C-1+C0 unknown word feature functions 
were created for each specific pair of characters 
in the bi-gram tables. Such feature functions are 
active if the characters in the respective states 
match the corresponding feature function?s 
characters. These feature functions are designed 
to distinguish known strings from unknown.  
2) C-1, C0, and C1 individual character feature 
functions were created for each character in the 
individual character word table, and are likewise 
active if the respective character matches the 
feature function?s character. 
3) C-1 prefix feature functions are defined 
over characters in the prefix table, and fire if the 
character in the proceeding state matches the 
feature function?s character. 
4) C0 suffix feature functions are defined 
over suffix table characters, and fire if the char-
acter in the current state matches the feature 
function?s character. 
Additionally, we also use reduplication fea-
ture functions that are active based on the repeti-
tion of a given character. We used two such fea-
ture functions, one that fires if the previous and 
the current character, C-1 and C0, are identical 
and one that does so if the subsequent and the 
previous characters, C-1 and C1, are identical.  
Most features appeared in the first-order tem-
plates with a few of character identity features in 
the both zero-order and first-order templates. 
We also did normalization of punctuations due 
to the fact that Mandarin has a huge variety of 
punctuations.  
Table 1 shows the number of data features 
and lambda weights in each corpus.  
Table 1 The number of features in each corpus 
# of data features # of lambda weights 
AS 2,558,840 8,076,916
HK 2,308,067 7,481,164
PK 1,659,654 5,377,146
MSR 3,634,585 12,468,890
3.2 Experiments 
3.2.1 Results on Sighan bakeoff 2003 
Experiments done while developing this sys-
tem showed that its performance was signifi-
cantly better than that of Peng et al (2004).  
As seen in Table 2, our system?s F-score was 
0.863 on CTB (Chinese Treebank from Univer-
169
sity of Pennsylvania) versus 0.849 F on Peng et 
al. (2004). We do not at present have a good 
understanding of which aspects of our system 
give it superior performance. 
Table 2 Comparisons of Peng et al (2004) and our F-
score on the closed track in Sighan bakeoff 2003 
Sighan  
Bakeoff 2003 
Our F-score F-score 
Peng et al (2004) 
CTB 0.863 0.849 
AS 0.970 0.956 
HK 0.947 0.928 
PK 0.953 0.941 
3.2.2 Results on Sighan bakeoff 2005 
Our final system achieved a F-score of 0.947 
(AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR). 
This shows that our system successfully general-
ized and achieved state of the art performance 
on all four corpora. 
Table 3 Performance of the features cumulatively, 
starting with the n-gram.  
F-score AS HK PK MSR
n-gram 0.943 0.946 0.950 0.961
n-gram (PU fixed)  0.953   
+Unk&redupl 0.947 0.943 0.950 0.964
+Unk&redupl 
(PU fixed) 
 0.952   
Table 3 lists our results on the four corpora. 
We give our results using just character identity 
based features; character identity features plus 
unknown words and reduplication features. Our 
unknown word features only helped on AS and 
MSR. Both of these corpora have words that 
have more characters than HK and PK. This in-
dicates that our unknown word features were 
more useful for corpora with segmentation stan-
dards that tend to result in longer words. 
In the HK corpus, when we added in un-
known word features, our performance dropped. 
However, we found that the testing data uses 
different punctuation than the training set. Our 
system could not distinguish new word charac-
ters from new punctuation, since having a com-
plete punctuation list is considered external 
knowledge for closed track systems. If the new 
punctuation were not unknown to us, our per-
formance on HK data would have gone up to 
0.952 F and the unknown word features would 
have not hurt the system too much. 
Table 4 present recalls (R), precisions (P), f-
scores (F) and recalls on both unknown (Roov)
and known words (Riv).
Table 4 Detailed performances of each corpus 
R P F Roov Riv
AS 0.950 0.943 0.947? 0.718? 0.960
HK 0.941 0.946 0.943? 0.698? 0.961
HK
(PU-fix)
0.952 0.952 0.952 0.791 0.965
PK 0.946 0.954 0.950? 0.787? 0.956
MSR 0.962 0.966 0.964? 0.717? 0.968
3.3 Error analysis 
Our system performed reasonably well on 
morphologically complex new words, such as 
??? (CABLE in AS) and ??? (MUR-
DER CASE in PK), where ? (LINE) and ?
(CASE) are suffixes. However, it over-
generalized to words with frequent suffixes such 
as ?? (it should be ? ? ?to burn some-
one? in PK) and ?? (it should be? ? ?
?to look backward? in PK). For the corpora that 
considered 4 character idioms as a word, our 
system combined most of new idioms together. 
This differs greatly from the results that one 
would likely obtain with a more traditional 
MaxMatch based technique, as such an algo-
rithm would segment novel idioms. 
One short coming of our system is that it is 
not robust enough to distinguish the difference 
between ordinal numbers and numbers with 
measure nouns. For example, ?? (3rd year) 
and ?? (three years) are not distinguishable 
to our system. In order to avoid this problem, it 
might require having more syntactic knowledge 
than was implicitly given in the training data.  
Finally, some errors are due to inconsisten-
cies in the gold segmentation of non-hanzi char-
acter. For example, ?Pentium4? is a word, but 
?PC133? is two words. Sometimes, ?8? is a 
word, but sometimes it is segmented into two 
words.
170
4 Conclusion
Our system used a conditional random field 
sequence model in conjunction with character 
identity features, morphological features and 
character reduplication features. We extracted 
our morphological information automatically to 
prevent overfitting Mandarin from particular 
Mandarin-speaking area. Our final system 
achieved a F-score of 0.947 (AS), 0.943 (HK), 
0.950 (PK) and 0.964 (MSR).  
5 Acknowledgment 
Thanks to Kristina Toutanova for her gener-
ous help and to Jenny Rose Finkel who devel-
oped such a great conditional random field 
package. This work was funded by the Ad-
vanced Research and Development Activity's 
Advanced Question Answering for Intelligence 
Program, National Science Foundation award 
IIS-0325646 and a Stanford Graduate Fellow-
ship.
References
Lafferty, John, A. McCallum, and F. Pereira. 2001. 
Conditional Random Field: Probabilistic Models 
for Segmenting and Labeling Sequence Data. In 
ICML 18. 
Gao, Jianfeng Andi Wu, Mu Li, Chang-Ning Huang, 
Hongqiao Li, Xinsong Xia and Haowei Qin. 2004. 
Adaptive Chinese word segmentation. In ACL-
2004.
Goh, Chooi-Ling, Masayuki Asahara, Yuji Matsu-
moto. 2003. Chinese unknown word identification 
using character-based tagging and chunking. In 
ACL 2003 Interactive Poster/Demo Sessions. 
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese Part-
of-Speech Tagging: One-at-a-Time or All-at-Once? 
Word-Based or Character-Based? In EMNLP 9.
Peng, Fuchun, Fangfang Feng and Andrew 
McCallum. 2004. Chinese segmentation and new 
word detection using conditional random fields. In 
COLING 2004.
Sproat, Richard and Tom Emerson. 2003. The first 
international Chinese word segmentation bakeoff. 
In SIGHAN 2. 
Xue, Nianwen and Libin Shen. 2003. Chinese Word 
Segmentation as LMR Tagging. In SIGHAN 2.
171
Shallow Semantic Parsing using Support Vector Machines?
Sameer Pradhan, Wayne Ward,
Kadri Hacioglu, James H. Martin
Center for Spoken Language Research,
University of Colorado, Boulder, CO 80303
{spradhan,whw,hacioglu,martin}@cslr.colorado.edu
Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
In this paper, we propose a machine learning al-
gorithm for shallow semantic parsing, extend-
ing the work of Gildea and Jurafsky (2002),
Surdeanu et al (2003) and others. Our al-
gorithm is based on Support Vector Machines
which we show give an improvement in perfor-
mance over earlier classifiers. We show perfor-
mance improvements through a number of new
features and measure their ability to general-
ize to a new test set drawn from the AQUAINT
corpus.
1 Introduction
Automatic, accurate and wide-coverage techniques that
can annotate naturally occurring text with semantic argu-
ment structure can play a key role in NLP applications
such as Information Extraction, Question Answering and
Summarization. Shallow semantic parsing ? the process
of assigning a simple WHO did WHAT to WHOM, WHEN,
WHERE, WHY, HOW, etc. structure to sentences in text,
is the process of producing such a markup. When pre-
sented with a sentence, a parser should, for each predicate
in the sentence, identify and label the predicate?s seman-
tic arguments. This process entails identifying groups of
words in a sentence that represent these semantic argu-
ments and assigning specific labels to them.
In recent work, a number of researchers have cast this
problem as a tagging problem and have applied vari-
ous supervised machine learning techniques to it (Gildea
and Jurafsky (2000, 2002); Blaheta and Charniak (2000);
Gildea and Palmer (2002); Surdeanu et al (2003); Gildea
and Hockenmaier (2003); Chen and Rambow (2003);
Fleischman and Hovy (2003); Hacioglu and Ward (2003);
Thompson et al (2003); Pradhan et al (2003)). In this
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IS-9978025
paper, we report on a series of experiments exploring this
approach.
For the initial experiments, we adopted the approach
described by Gildea and Jurafsky (2002) (G&J) and eval-
uated a series of modifications to improve its perfor-
mance. In the experiments reported here, we first re-
placed their statistical classification algorithm with one
that uses Support Vector Machines and then added to the
existing feature set. We evaluate results using both hand-
corrected TreeBank syntactic parses, and actual parses
from the Charniak parser.
2 Semantic Annotation and Corpora
We will be reporting on results using PropBank1 (Kings-
bury et al, 2002), a 300k-word corpus in which predi-
cate argument relations are marked for part of the verbs
in the Wall Street Journal (WSJ) part of the Penn Tree-
Bank (Marcus et al, 1994). The arguments of a verb are
labeled ARG0 to ARG5, where ARG0 is the PROTO-
AGENT (usually the subject of a transitive verb) ARG1
is the PROTO-PATIENT (usually its direct object), etc.
PropBank attempts to treat semantically related verbs
consistently. In addition to these CORE ARGUMENTS,
additional ADJUNCTIVE ARGUMENTS, referred to as
ARGMs are also marked. Some examples are ARGM-
LOC, for locatives, and ARGM-TMP, for temporals. Fig-
ure 1 shows the syntax tree representation along with the
argument labels for an example structure extracted from
the PropBank corpus.
Most of the experiments in this paper, unless speci-
fied otherwise, are performed on the July 2002 release
of PropBank. A larger, cleaner, completely adjudicated
version of PropBank was made available in Feb 2004.
We will also report some final best performance numbers
on this corpus. PropBank was constructed by assigning
semantic arguments to constituents of the hand-corrected
TreeBank parses. The data comprise several sections of
the WSJ, and we follow the standard convention of using
1http://www.cis.upenn.edu/?ace/
Section-23 data as the test set. Section-02 to Section-
21 were used for training. In the July 2002 release, the
training set comprises about 51,000 sentences, instantiat-
ing about 132,000 arguments, and the test set comprises
2,700 sentences instantiating about 7,000 arguments. The
Feb 2004 release training set comprises about 85,000 sen-
tences instantiating about 250,000 arguments and the test
set comprises 5,000 sentences instantiating about 12,000
arguments.
[ARG0 He] [predicate talked] for [ARGM?TMP about
20 minutes].
S
hhhh
((((
NP
PRP
He
ARG0
VP
hhhh
((((
VBD
talked
predicate
PP
hhh
(((
IN
for
NULL
NP
hhhhh
(((((
about 20 minutes
ARGM ? TMP
Figure 1: Syntax tree for a sentence illustrating the Prop-
Bank tags.
3 Problem Description
The problem of shallow semantic parsing can be viewed
as three different tasks.
Argument Identification ? This is the process of identi-
fying parsed constituents in the sentence that represent
semantic arguments of a given predicate.
Argument Classification ? Given constituents known to
represent arguments of a predicate, assign the appropri-
ate argument labels to them.
Argument Identification and Classification ? A combina-
tion of the above two tasks.
Each node in the parse tree can be classified as either
one that represents a semantic argument (i.e., a NON-
NULL node) or one that does not represent any seman-
tic argument (i.e., a NULL node). The NON-NULL nodes
can then be further classified into the set of argument la-
bels. For example, in the tree of Figure 1, the node IN
that encompasses ?for? is a NULL node because it does
not correspond to a semantic argument. The node NP
that encompasses ?about 20 minutes? is a NON-NULL
node, since it does correspond to a semantic argument
? ARGM-TMP.
4 Baseline Features
Our baseline system uses the same set of features in-
troduced by G&J. Some of the features, viz., predicate,
voice and verb sub-categorization are shared by all the
nodes in the tree. All the others change with the con-
stituent under consideration.
? Predicate ? The predicate itself is used as a feature.
? Path ? The syntactic path through the parse tree
from the parse constituent to the predicate being
classified. For example, in Figure 1, the path from
ARG0 ? ?He? to the predicate talked, is represented
with the string NP?S?VP?VBD. ? and ? represent
upward and downward movement in the tree respec-
tively.
? Phrase Type ? This is the syntactic category (NP,
PP, S, etc.) of the phrase/constituent corresponding
to the semantic argument.
? Position ? This is a binary feature identifying
whether the phrase is before or after the predicate.
? Voice ? Whether the predicate is realized as an ac-
tive or passive construction.
? Head Word ? The syntactic head of the phrase. This
is calculated using a head word table described by
(Magerman, 1994) and modified by (Collins, 1999,
Appendix. A).
? Sub-categorization ? This is the phrase struc-
ture rule expanding the predicate?s parent node
in the parse tree. For example, in Figure 1, the
sub-categorization for the predicate talked is
VP?VBD-PP.
5 Classifier and Implementation
We formulate the parsing problem as a multi-class clas-
sification problem and use a Support Vector Machine
(SVM) classifier (Hacioglu et al, 2003; Pradhan et al
2003). Since SVMs are binary classifiers, we have to con-
vert the multi-class problem into a number of binary-class
problems. We use the ONE vs ALL (OVA) formalism,
which involves training n binary classifiers for a n-class
problem.
Since the training time taken by SVMs scales exponen-
tially with the number of examples, and about 80% of the
nodes in a syntactic tree have NULL argument labels, we
found it efficient to divide the training process into two
stages, while maintaining the same accuracy:
1. Filter out the nodes that have a very high probabil-
ity of being NULL. A binary NULL vs NON-NULL
classifier is trained on the entire dataset. A sigmoid
function is fitted to the raw scores to convert the
scores to probabilities as described by (Platt, 2000).
2. The remaining training data is used to train OVA
classifiers, one of which is the NULL-NON-NULL
classifier.
With this strategy only one classifier (NULL vs NON-
NULL) has to be trained on all of the data. The remaining
OVA classifiers are trained on the nodes passed by the
filter (approximately 20% of the total), resulting in a con-
siderable savings in training time.
In the testing stage, we do not perform any filtering
of NULL nodes. All the nodes are classified directly
as NULL or one of the arguments using the classifier
trained in step 2 above. We observe no significant per-
formance improvement even if we filter the most likely
NULL nodes in a first pass.
For our experiments, we used TinySVM2 along with
YamCha3 (Kudo and Matsumoto, 2000)
(Kudo and Matsumoto, 2001) as the SVM training and
test software. The system uses a polynomial kernel with
degree 2; the cost per unit violation of the margin, C=1;
and, tolerance of the termination criterion, e=0.001.
6 Baseline System Performance
Table 1 shows the baseline performance numbers on the
three tasks mentioned earlier; these results are based on
syntactic features computed from hand-corrected Tree-
Bank (hence LDC hand-corrected) parses.
For the argument identification and the combined iden-
tification and classification tasks, we report the precision
(P), recall (R) and the F14 scores, and for the argument
classification task we report the classification accuracy
(A). This test set and all test sets, unless noted otherwise
are Section-23 of PropBank.
Classes Task P R F1 A
(%) (%) (%)
ALL Id. 90.9 89.8 90.4
ARGs Classification - - - 87.9
Id. + Classification 83.3 78.5 80.8
CORE Id. 94.7 90.1 92.3
ARGs Classification - - - 91.4
Id. + Classification 88.4 84.1 86.2
Table 1: Baseline performance on all three tasks using
hand-corrected parses.
7 System Improvements
7.1 Disallowing Overlaps
The system as described above might label two con-
stituents NON-NULL even if they overlap in words. This
is a problem since overlapping arguments are not allowed
in PropBank. Among the overlapping constituents we re-
tain the one for which the SVM has the highest confi-
dence, and label the others NULL. The probabilities ob-
tained by applying the sigmoid function to the raw SVM
scores are used as the measure of confidence. Table 2
shows the performance of the parser on the task of iden-
tifying and labeling semantic arguments using the hand-
corrected parses. On all the system improvements, we
perform a ?2 test of significance at p = 0.05, and all the
2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/
4F1 = 2PRP+R
significant improvements are marked with an ?. In this
system, the overlap-removal decisions are taken indepen-
dently of each other.
P R F1
(%) (%)
Baseline 83.3 78.5 80.8
No Overlaps 85.4 78.1 ?81.6
Table 2: Improvements on the task of argument identi-
fication and classification after disallowing overlapping
constituents.
7.2 New Features
We tested several new features. Two were obtained from
the literature ? named entities in constituents and head
word part of speech. Other are novel features.
1. Named Entities in Constituents ? Following
Surdeanu et al (2003), we tagged 7 named en-
tities (PERSON, ORGANIZATION, LOCATION,
PERCENT, MONEY, TIME, DATE) using Identi-
Finder (Bikel et al, 1999) and added them as 7
binary features.
2. Head Word POS ? Surdeanu et al (2003) showed
that using the part of speech (POS) of the head word
gave a significant performance boost to their system.
Following that, we experimented with the addition
of this feature to our system.
3. Verb Clustering ? Since our training data is rel-
atively limited, any real world test set will con-
tain predicates that have not been seen in training.
In these cases, we can benefit from some informa-
tion about the predicate by using predicate clus-
ter as a feature. The verbs were clustered into 64
classes using the probabilistic co-occurrence model
of Hofmann and Puzicha (1998). The clustering al-
gorithm uses a database of verb-direct-object rela-
tions extracted by Lin (1998). We then use the verb
class of the current predicate as a feature.
4. Partial Path ? For the argument identification task,
path is the most salient feature. However, it is also
the most data sparse feature. To overcome this prob-
lem, we tried generalizing the path by adding a new
feature that contains only the part of the path from
the constituent to the lowest common ancestor of the
predicate and the constituent, which we call ?Partial-
Path?.
5. Verb Sense Information ? The arguments that a
predicate can take depend on the word sense of the
predicate. Each predicate tagged in the PropBank
corpus is assigned a separate set of arguments de-
pending on the sense in which it is used. Table 3
illustrates the argument sets for the predicate talk.
Depending on the sense of the predicate talk, either
ARG1 or ARG2 can identify the hearer. Absence of
this information can be potentially confusing to the
learning mechanism.
Talk sense 1: speak sense 2: persuade/dissuade
Tag Description Tag Description
ARG0 Talker ARG0 Talker
ARG1 Subject ARG1 Talked to
ARG2 Hearer ARG2 Secondary action
Table 3: Argument labels associated with the two senses
of predicate talk in PropBank corpus.
We added the oracle sense information extracted
from PropBank, to our features by treating each
sense of a predicate as a distinct predicate.
6. Head Word of Prepositional Phrases ? Many ad-
junctive arguments, such as temporals and locatives,
occur as prepositional phrases in a sentence, and
it is often the case that the head words of those
phrases, which are always prepositions, are not very
discriminative, eg., ?in the city?, ?in a few minutes?,
both share the same head word ?in? and neither
contain a named entity, but the former is ARGM-
LOC, whereas the latter is ARGM-TMP. Therefore,
we tried replacing the head word of a prepositional
phrase, with that of the first noun phrase inside the
prepositional phrase. We retained the preposition in-
formation by appending it to the phrase type, eg.,
?PP-in? instead of ?PP?.
7. First and Last Word/POS in Constituent ? Some
arguments tend to contain discriminative first and
last words so we tried using them along with their
part of speech as four new features.
8. Ordinal constituent position ? In order to avoid
false positives of the type where constituents far
away from the predicate are spuriously identified as
arguments, we added this feature which is a concate-
nation of the constituent type and its ordinal position
from the predicate.
9. Constituent tree distance ? This is a finer way of
specifying the already present position feature.
10. Constituent relative features ? These are nine fea-
tures representing the phrase type, head word and
head word part of speech of the parent, and left and
right siblings of the constituent in focus. These were
added on the intuition that encoding the tree context
this way might add robustness and improve general-
ization.
11. Temporal cue words ? There are several temporal
cue words that are not captured by the named entity
tagger and were considered for addition as a binary
feature indicating their presence.
12. Dynamic class context ? In the task of argument
classification, these are dynamic features that repre-
sent the hypotheses of at most previous two nodes
belonging to the same tree as the node being classi-
fied.
8 Feature Performance
Table 4 shows the effect each feature has on the ar-
gument classification and argument identification tasks,
when added individually to the baseline. Addition of
named entities improves the F1 score for adjunctive ar-
guments ARGM-LOC from 59% to ?68% and ARGM-
TMP from 78.8% to ?83.4%. But, since these arguments
are small in number compared to the core arguments, the
overall accuracy does not show a significant improve-
ment. We found that adding this feature to the NULL vs
NON-NULL classifier degraded its performance. It also
shows the contribution of replacing the head word and the
head word POS separately in the feature where the head
of a prepositional phrase is replaced by the head word
of the noun phrase inside it. Apparently, a combination
of relative features seem to have a significant improve-
ment on either or both the classification and identification
tasks, and so do the first and last words in the constituent.
Features Class ARGUMENT ID
Acc.
P R F1
Baseline 87.9 93.7 88.9 91.3
+ Named entities 88.1 - - -
+ Head POS ?88.6 94.4 90.1 ?92.2
+ Verb cluster 88.1 94.1 89.0 91.5
+ Partial path 88.2 93.3 88.9 91.1
+ Verb sense 88.1 93.7 89.5 91.5
+ Noun head PP (only POS) ?88.6 94.4 90.0 ?92.2
+ Noun head PP (only head) ?89.8 94.0 89.4 91.7
+ Noun head PP (both) ?89.9 94.7 90.5 ?92.6
+ First word in constituent ?89.0 94.4 91.1 ?92.7
+ Last word in constituent ?89.4 93.8 89.4 91.6
+ First POS in constituent 88.4 94.4 90.6 ?92.5
+ Last POS in constituent 88.3 93.6 89.1 91.3
+ Ordinal const. pos. concat. 87.7 93.7 89.2 91.4
+ Const. tree distance 88.0 93.7 89.5 91.5
+ Parent constituent 87.9 94.2 90.2 ?92.2
+ Parent head 85.8 94.2 90.5 ?92.3
+ Parent head POS ?88.5 94.3 90.3 ?92.3
+ Right sibling constituent 87.9 94.0 89.9 91.9
+ Right sibling head 87.9 94.4 89.9 ?92.1
+ Right sibling head POS 88.1 94.1 89.9 92.0
+ Left sibling constituent ?88.6 93.6 89.6 91.6
+ Left sibling head 86.9 93.9 86.1 89.9
+ Left sibling head POS ?88.8 93.5 89.3 91.4
+ Temporal cue words ?88.6 - - -
+ Dynamic class context 88.4 - - -
Table 4: Effect of each feature on the argument identifi-
cation and classification tasks when added to the baseline
system.
We tried two other ways of generalizing the head word:
i) adding the head word cluster as a feature, and ii) replac-
ing the head word with a named entity if it belonged to
any of the seven named entities mentioned earlier. Nei-
ther method showed any improvement. We also tried gen-
eralizing the path feature by i) compressing sequences of
identical labels, and ii) removing the direction in the path,
but none showed any improvement on the baseline.
8.1 Argument Sequence Information
In order to improve the performance of their statistical ar-
gument tagger, G&J used the fact that a predicate is likely
to instantiate a certain set of arguments. We use a similar
strategy, with some additional constraints: i) argument
ordering information is retained, and ii) the predicate is
considered as an argument and is part of the sequence.
We achieve this by training a trigram language model on
the argument sequences, so unlike G&J, we can also es-
timate the probability of argument sets not seen in the
training data. We first convert the raw SVM scores to
probabilities using a sigmoid function. Then, for each
sentence being parsed, we generate an argument lattice
using the n-best hypotheses for each node in the syn-
tax tree. We then perform a Viterbi search through the
lattice using the probabilities assigned by the sigmoid
as the observation probabilities, along with the language
model probabilities, to find the maximum likelihood path
through the lattice, such that each node is either assigned
a value belonging to the PROPBANK ARGUMENTs, or
NULL.
CORE ARGs/ P R F1
Hand-corrected parses (%) (%)
Baseline w/o overlaps 90.0 86.1 88.0
Common predicate 90.8 86.3 88.5
Specific predicate lemma 90.5 87.4 ?88.9
Table 5: Improvements on the task of argument identifi-
cation and tagging after performing a search through the
argument lattice.
The search is constrained in such a way that no two
NON-NULL nodes overlap with each other. To simplify
the search, we allowed only NULL assignments to nodes
having a NULL likelihood above a threshold. While train-
ing the language model, we can either use the actual pred-
icate to estimate the transition probabilities in and out
of the predicate, or we can perform a joint estimation
over all the predicates. We implemented both cases con-
sidering two best hypotheses, which always includes a
NULL (we add NULL to the list if it is not among the
top two). On performing the search, we found that the
overall performance improvement was not much differ-
ent than that obtained by resolving overlaps as mentioned
earlier. However, we found that there was an improve-
ment in the CORE ARGUMENT accuracy on the combined
task of identifying and assigning semantic arguments,
given hand-corrected parses, whereas the accuracy of the
ADJUNCTIVE ARGUMENTS slightly deteriorated. This
seems to be logical considering the fact that the ADJUNC-
TIVE ARGUMENTS are not linguistically constrained in
any way as to their position in the sequence of argu-
ments, or even the quantity. We therefore decided to
use this strategy only for the CORE ARGUMENTS. Al-
though, there was an increase in F1 score when the lan-
guage model probabilities were jointly estimated over all
the predicates, this improvement is not statistically signif-
icant. However, estimating the same using specific predi-
cate lemmas, showed a significant improvement in accu-
racy. The performance improvement is shown in Table 5.
9 Best System Performance
The best system is trained by first filtering the most
likely nulls using the best NULL vs NON-NULL classi-
fier trained using all the features whose argument identi-
fication F1 score is marked in bold in Table 4, and then
training a ONE vs ALL classifier using the data remain-
ing after performing the filtering and using the features
that contribute positively to the classification task ? ones
whose accuracies are marked in bold in Table 4. Table 6
shows the performance of this system.
Classes Task Hand-corrected parses
P R F1 A
(%) (%) (%)
ALL Id. 95.2 92.5 93.8
ARGs Classification - - - 91.0
Id. + Classification 88.9 84.6 86.7
CORE Id. 96.2 93.0 94.6
ARGs Classification - - - 93.9
Id. + Classification 90.5 87.4 88.9
Table 6: Best system performance on all tasks using
hand-corrected parses.
10 Using Automatic Parses
Thus far, we have reported results using hand-corrected
parses. In real-word applications, the system will have
to extract features from an automatically generated
parse. To evaluate this scenario, we used the Charniak
parser (Chaniak, 2001) to generate parses for PropBank
training and test data. We lemmatized the predicate using
the XTAG morphology database5 (Daniel et al, 1992).
Table 7 shows the performance degradation when
automatically generated parses are used.
11 Using Latest PropBank Data
Owing to the Feb 2004 release of much more and com-
pletely adjudicated PropBank data, we have a chance to
5ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.5/morph-
1.5.tar.gz
Classes Task Automatic parses
P R F1 A
(%) (%) (%)
ALL Id. 89.3 82.9 86.0
ARGs Classification - - - 90.0
Id. + Classification 84.0 75.3 79.4
CORE Id. 92.0 83.3 87.4
ARGs Classification - - - 90.5
Id. + Classification 86.4 78.4 82.2
Table 7: Performance degradation when using automatic
parses instead of hand-corrected ones.
report our performance numbers on this data set. Table 8
shows the same information as in previous Tables 6 and
7, but generated using the new data. Owing to time limi-
tations, we could not get the results on the argument iden-
tification task and the combined argument identification
and classification task using automatic parses.
ALL ARGs Task P R F1 A
(%) (%) (%)
HAND Id. 96.2 95.8 96.0
Classification - - - 93.0
Id. + Classification 89.9 89.0 89.4
AUTOMATIC Classification - - - 90.1
Table 8: Best system performance on all tasks using
hand-corrected parses using the latest PropBank data.
12 Feature Analysis
In analyzing the performance of the system, it is useful
to estimate the relative contribution of the various feature
sets used. Table 9 shows the argument classification ac-
curacies for combinations of features on the training and
test data, using hand-corrected parses, for all PropBank
arguments.
Features Accuracy
(%)
All 91.0
All except Path 90.8
All except Phrase Type 90.8
All except HW and HW -POS 90.7
All except All Phrases ?83.6
All except Predicate ?82.4
All except HW and FW and LW -POS ?75.1
Path, Predicate 74.4
Path, Phrase Type 47.2
Head Word 37.7
Path 28.0
Table 9: Performance of various feature combinations on
the task of argument classification.
In the upper part of Table 9 we see the degradation in
performance by leaving out one feature or a feature fam-
ily at a time. After the addition of all the new features,
it is the case that removal of no individual feature except
predicate degrades the classification performance signifi-
cantly, as there are some other features that provide com-
plimentary information. However, removal of predicate
information hurts performance significantly, so does the
removal of a family of features, eg., all phrase types, or
the head word (HW), first word (FW) and last word (LW)
information. The lower part of the table shows the per-
formance of some feature combinations by themselves.
Table 10 shows the feature salience on the task of ar-
gument identification. One important observation we can
make here is that the path feature is the most salient fea-
ture in the task of argument identification, whereas it is
the least salient in the task of argument classification. We
could not provide the numbers for argument identifica-
tion performance upon removal of the path feature since
that made the SVM training prohibitively slow, indicating
that the SVM had a very hard time separating the NULL
class from the NON-NULL class.
Features P R F1
(%) (%)
All 95.2 92.5 93.8
All except HW 95.1 92.3 93.7
All except Predicate 94.5 91.9 93.2
Table 10: Performance of various feature combinations
on the task of argument identification
13 Comparing Performance with Other
Systems
We compare our system against 4 other shallow semantic
parsers in the literature. In comparing systems, results are
reported for all the three types of tasks mentioned earlier.
13.1 Description of the Systems
The Gildea and Palmer (G&P) System.
The Gildea and Palmer (2002) system uses the same
features and the same classification mechanism used by
G&J. These results are reported on the December 2001
release of PropBank.
The Surdeanu et al System.
Surdeanu et al (2003) report results on two systems
using a decision tree classifier. One that uses exactly the
same features as the G&J system. We call this ?Surdeanu
System I.? They then show improved performance of an-
other system ? ?Surdeanu System II,? which uses some
additional features. These results are are reported on the
July 2002 release of PropBank.
The Gildea and Hockenmaier (G&H) System
The Gildea and Hockenmaier (2003) system uses fea-
tures extracted from Combinatory Categorial Grammar
(CCG) corresponding to the features that were used by
G&J and G&P systems. CCG is a form of dependency
grammar and is hoped to capture long distance relation-
ships better than a phrase structure grammar. The fea-
tures are combined using the same algorithm as in G&J
and G&P. They use a slightly newer ? November 2002 re-
lease of PropBank. We will refer to this as ?G&H System
I?.
The Chen and Rambow (C&R) System
Chen and Rambow report on two different systems,
also using a decision tree classifier. The first ?C&R Sys-
tem I? uses surface syntactic features much like the G&P
system. The second ?C&R System II? uses additional
syntactic and semantic representations that are extracted
from a Tree Adjoining Grammar (TAG) ? another gram-
mar formalism that better captures the syntactic proper-
ties of natural languages.
Classifier Accuracy
(%)
SVM 88
Decision Tree (Surdeanu et al, 2003) 79
Gildea and Palmer (2002) 77
Table 11: Argument classification using same features
but different classifiers.
13.2 Comparing Classifiers
Since two systems, in addition to ours, report results us-
ing the same set of features on the same data, we can
directly assess the influence of the classifiers. G&P sys-
tem estimates the posterior probabilities using several dif-
ferent feature sets and interpolate the estimates, while
Surdeanu et al (2003) use a decision tree classifier. Ta-
ble 11 shows a comparison between the three systems for
the task of argument classification.
13.3 Argument Identification (NULL vs NON-NULL)
Table 12 compares the results of the task of identify-
ing the parse constituents that represent semantic argu-
ments. As expected, the performance degrades consider-
ably when we extract features from an automatic parse as
opposed to a hand-corrected parse. This indicates that the
syntactic parser performance directly influences the argu-
ment boundary identification performance. This could be
attributed to the fact that the two features, viz., Path and
Head Word that have been seen to be good discriminators
of the semantically salient nodes in the syntax tree, are
derived from the syntax tree.
Classes System Hand Automatic
P R F1 P R F1
ALL SVM 95 92 94 89 83 86
ARGs Surdeanu System II - - 89 - - -
Surdeanu System I 85 84 85 - - -
Table 12: Argument identification
13.4 Argument Classification
Table 13 compares the argument classification accuracies
of various systems, and at various levels of classification
granularity, and parse accuracy. It can be seen that the
SVM System performs significantly better than all the
other systems on all PropBank arguments.
Classes System Hand Automatic
Accuracy Accuracy
ALL SVM 91 90
ARGs G&P 77 74
Surdeanu System II 84 -
Surdeanu System I 79 -
CORE SVM 93.9 90.5
ARGs C&R System II 93.5 -
C&R System I 92.4 -
Table 13: Argument classification
13.5 Argument Identification and Classification
Table 14 shows the results for the task where the system
first identifies candidate argument boundaries and then
labels them with the most likely argument. This is the
hardest of the three tasks outlined earlier. SVM does a
very good job of generalizing in both stages of process-
ing.
Classes System Hand Automatic
P R F1 P R F1
ALL SVM 89 85 87 84 75 79
ARGs G&H System I 76 68 72 71 63 67
G&P 71 64 67 58 50 54
CORE SVM System 90 87 89 86 78 82
ARGs G&H System I 82 79 80 76 73 75
C&R System II - - - 65 75 70
Table 14: Identification and classification
14 Generalization to a New Text Source
Thus far, in all experiments our unseen test data was
selected from the same source as the training data.
In order to see how well the features generalize to
texts drawn from a similar source, we used the classifier
trained on PropBank training data to test data drawn from
the AQUAINT corpus (LDC, 2002). We annotated 400
sentences from the AQUAINT corpus with PropBank
arguments. This is a collection of text from the New
York Times Inc., Associated Press Inc., and Xinhua
News Service (PropBank by comparison is drawn from
Wall Street Journal). The results are shown in Table 15.
Task P R F1 A
(%) (%) (%)
ALL Id. 75.8 71.4 73.5 -
ARGs Classification - - - 83.8
Id. + Classification 65.2 61.5 63.3 -
CORE Id. 88.4 74.4 80.8 -
ARGs Classification - - - 84.0
Id. + Classification 75.2 63.3 68.7 -
Table 15: Performance on the AQUAINT test set.
There is a significant drop in the precision and recall
numbers for the AQUAINT test set (compared to the pre-
cision and recall numbers for the PropBank test set which
were 84% and 75% respectively). One possible reason
for the drop in performance is relative coverage of the
features on the two test sets. The head word, path and
predicate features all have a large number of possible val-
ues and could contribute to lower coverage when moving
from one domain to another. Also, being more specific
they might not transfer well across domains.
Features Arguments non-Arguments
(%) (%)
Predicate, Path 87.60 2.91
Predicate, Head Word 48.90 26.55
Cluster, Path 96.31 4.99
Cluster, Head Word 83.85 60.14
Path 99.13 15.15
Head Word 93.02 90.59
Table 16: Feature Coverage on PropBank test set using
parser trained on PropBank training set.
Features Arguments non-Arguments
(%) (%)
Predicate, Path 62.11 4.66
Predicate, Head Word 30.26 17.41
Cluster, Path 87.19 10.68
Cluster, Head Word 65.82 45.43
Path 96.50 29.26
Head Word 84.65 83.54
Table 17: Coverage of features on AQUAINT test set us-
ing parser trained on PropBank training set.
Table 16 shows the coverage for features on the hand-
corrected PropBank test set. The tables show feature
coverage for constituents that were Arguments and con-
stituents that were NULL. About 99% of the predicates in
the AQUAINT test set were seen in the PropBank train-
ing set. Table 17 shows coverage for the same features on
the AQUAINT test set. We believe that the drop in cover-
age of the more predictive feature combinations explains
part of the drop in performance.
15 Conclusions
We have described an algorithm which significantly im-
proves the state-of-the-art in shallow semantic parsing.
Like previous work, our parser is based on a supervised
machine learning approach. Key aspects of our results
include significant improvement via an SVM classifier,
improvement from new features and a series of analytic
experiments on the contributions of the features. Adding
features that are generalizations of the more specific fea-
tures seemed to help. These features were named enti-
ties, head word part of speech and verb clusters. We also
analyzed the transferability of the features to a new text
source.
We would like to thank Ralph Weischedel and Scott Miller of
BBN Inc. for letting us use their named entity tagger ? Iden-
tiFinder; Martha Palmer for providing us with the PropBank
data, Valerie Krugler for tagging the AQUAINT test set with
PropBank arguments, and all the anonymous reviewers for their
helpful comments.
References
[Bikel et al1999] Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel.
1999. An algorithm that learns what?s in a name. Machine Learning, 34:211?
231.
[Blaheta and Charniak2000] Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In NAACL, pages 234?240.
[Chaniak2001] Eugene Chaniak. 2001. Immediate-head parsing for language
models. In ACL-01.
[Chen and Rambow2003] John Chen and Owen Rambow. 2003. Use of deep
linguistics features for the recognition and labeling of semantic arguments.
EMNLP-03.
[Collins1999] Michael John Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania,
Philadelphia.
[Daniel et al1992] K. Daniel, Y. Schabes, M. Zaidel, and D. Egedi. 1992. A freely
available wide coverage morphological analyzer for English. In COLING-92.
[Fleischman and Hovy2003] Michael Fleischman and Eduard Hovy. 2003. A
maximum entropy approach to framenet tagging. In HLT-03.
[Gildea and Hockenmaier2003] Dan Gildea and Julia Hockenmaier. 2003. Identi-
fying semantic roles using combinatory categorial grammar. In EMNLP-03.
[Gildea and Jurafsky2000] Daniel Gildea and Daniel Jurafsky. 2000. Automatic
labeling of semantic roles. In ACL-00, pages 512?520.
[Gildea and Jurafsky2002] Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics, 28(3):245?288.
[Gildea and Palmer2002] Daniel Gildea and Martha Palmer. 2002. The necessity
of syntactic parsing for predicate argument recognition. In ACL-02.
[Hacioglu and Ward2003] Kadri Hacioglu and Wayne Ward. 2003. Target word
detection and semantic role chunking using support vector machines. In HLT-
03.
[Hacioglu et al2003] Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Mar-
tin, and Dan Jurafsky. 2003. Shallow semantic parsing using support vector
machines. Technical Report TR-CSLR-2003-1, Center for Spoken Language
Research, Boulder, Colorado.
[Hofmann and Puzicha1998] Thomas Hofmann and Jan Puzicha. 1998. Statistical
models for co-occurrence data. Memo, MIT AI Laboratory.
[Kingsbury et al2002] Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the Penn Treebank. In HLT-02.
[Kudo and Matsumoto2000] Taku Kudo and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In CoNLL-00.
[Kudo and Matsumoto2001] Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In NAACL-01.
[LDC2002] LDC. 2002. The AQUAINT Corpus of English News Text, Catalog
no. LDC2002t31.
[Lin1998] Dekang Lin. 1998. Automatic retrieval and clustering of similar words.
In COLING-98.
[Magerman1994] David Magerman. 1994. Natural Language Parsing as Statisti-
cal Pattern Recognition. Ph.D. thesis, Stanford University, CA.
[Marcus et al1994] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn TreeBank: Annotating predicate argument structure.
[Platt2000] John Platt. 2000. Probabilities for support vector machines. In
A. Smola, P. Bartlett, B. Scolkopf, and D. Schuurmans, editors, Advances in
Large Margin Classifiers. MIT press.
[Pradhan et al2003] Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Mar-
tin, and Dan Jurafsky. 2003. Semantic role parsing: Adding semantic struc-
ture to unstructured text. In ICDM-03.
[Surdeanu et al2003] Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul
Aarseth. 2003. Using predicate-argument structures for information extrac-
tion. In ACL-03.
[Thompson et al2003] Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for semantic role labeling. In ECML-03.
Parsing Arguments of Nominalizations in English and Chinese?
Sameer Pradhan, Honglin Sun,
Wayne Ward, James H. Martin
Center for Spoken Language Research,
University of Colorado, Boulder, CO 80303
{spradhan,sunh,whw,martin}@cslr.colorado.edu
Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
In this paper, we use a machine learning frame-
work for semantic argument parsing, and apply
it to the task of parsing arguments of eventive
nominalizations in the FrameNet database. We
create a baseline system using a subset of fea-
tures introduced by Gildea and Jurafsky (2002),
which are directly applicable to nominal pred-
icates. We then investigate new features which
are designed to capture the novelties in nom-
inal argument structure and show a significant
performance improvement using these new fea-
tures. We also investigate the parsing perfor-
mance of nominalizations in Chinese and com-
pare the salience of the features for the two lan-
guages.
1 Introduction
The field of NLP had seen a resurgence of research in
shallow semantic analysis. The bulk of this recent work
views semantic analysis as a tagging, or labeling prob-
lem, and has applied various supervised machine learn-
ing techniques to it (Gildea and Jurafsky (2000, 2002);
Gildea and Palmer (2002); Surdeanu et al (2003); Ha-
cioglu and Ward (2003); Thompson et al (2003); Prad-
han et al (2003)). Note that, while all of these systems
are limited to the analysis of verbal predicates, many un-
derlying semantic relations are expressed via nouns, ad-
jectives, and prepositions. This paper presents a prelimi-
nary investigation into the semantic parsing of eventive
nominalizations (Grimshaw, 1990) in English and Chi-
nese.
2 Semantic Annotation and Corpora
For our experiments, we use the FrameNet database
(Baker et al, 1998) which contains frame-specific se-
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IS-9978025
mantic annotation of a number of predicates in English.
Predicates are grouped by the semantic frame that they
instantiate, depending on the sense of their usage, and
their arguments assume one of the frame elements or
roles specific to that frame. The predicate can be a verb,
noun, adjective, prepositional phrase, etc. FrameNet
contains about 500 different frame types and about 700
distinct frame elements. The following example illus-
trates the general idea. Here, the predicate ?complain?
instantiates a ?Statement? frame once as a nominal
predicate and once as a verbal predicate.
Did [Speaker she] make an official [Predicate:nominal com-
plaint] [Addressee to you] [Topic about the attack.]
[Message?Justice has not been done?] [Speaker he]
[Predicate:verbal complained.]
Nominal predicates in FrameNet include ultra-nominals
(Barker and Dowty, 1992), nominals and nominal-
izations. For the purposes of this study, a human analyst
went through the nominal predicates in FrameNet and
selected those that were identified as nominalizations
in NOMLEX (Macleod et al, 1998). Out of those,
the analyst then selected ones that were eventive
nominalizations.
These data comprise 7,333 annotated sentences, with
11,284 roles. There are 105 frames with about 190 dis-
tinct frame role1 types. A stratified sampling over predi-
cates was performed to select 80% of this data for train-
ing, 10% for development and another 10% for testing.
For the Chinese semantic parsing experiments, we se-
lected 22 nominalizations from the Penn Chinese Tree-
bank and tagged all the sentences containing these predi-
cates with PropBank (Kingsbury and Palmer, 2002) style
arguments ? ARG0, ARG1, etc. These consisted of 630
sentences. These are then split into two parts: 503 (80%)
for training and 127 (20%) for testing.
1We will use the terms role and arguments interchangeably
3 Baseline System
The primary assumption in our system is that a seman-
tic argument aligns with some syntactic constituent. The
goal is to identify and label constituents in a syntactic
tree that represent valid semantic arguments of a given
predicate. Unlike PropBank, there are no hand-corrected
parses available for the sentences in FrameNet, so we
cannot quantify the possible mis-alignment of the nomi-
nal arguments with syntactic constituents. The arguments
that do not align with any constituent are simply missed
by the current system.
3.1 Features We created a baseline system using
all and only those features introduced by Gildea and
Jurafsky that are directly applicable to nominal pred-
icates. Most of the features are extracted from the
syntactic parse of a sentence. We used the Charniak
parser (Chaniak, 2001) to parse the sentences in order to
perform feature extraction. The features are listed below:
Predicate ? The predicate lemma is used as a feature.
Path ? The syntactic path through the parse tree from the
parse constituent being classified to the predicate.
Constituent type ? This is the syntactic category (NP, PP,
S, etc.) of the constituent corresponding to the semantic
argument.
Position ? This is a binary feature identifying whether
the constituent is before or after the predicate.
Head word ? The syntactic head of the constituent.
3.2 Classifier and Implementation We formulate the
parsing problem as a multi-class classification problem
and use a Support Vector Machine (SVM) classifier in the
ONE vs ALL (OVA) formalism, which involves training
n classifiers for a n-class problem ? including the NULL
class. We use TinySVM2 along with YamCha3 (Kudo
and Matsumoto (2000, 2001)) as the SVM training and
test software.
3.3 Performance We evaluate our system on three
tasks: i) Argument Identification: Identifying parse con-
stituents that represent arguments of a given predicate, ii)
Argument Classification: Labeling the constituents that
are known to represent arguments with the most likely
roles, and iii) Argument Identification and Classification:
Finding constituents that represent arguments of a pred-
icate, and labeling them with the most likely roles. The
baseline performance on the three tasks is shown in Ta-
ble 1.
4 New Features
To improve the baseline performance we investigated ad-
ditional features that would provide useful information in
identifying arguments of nominalizations. Following is a
2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/
Task P R F?=1 A
(%) (%) (%)
Id. 81.7 65.7 72.8
Classification - - - 70.9
Id. + Classification 65.7 42.1 51.4
Table 1: Baseline performance on all three tasks.
description of each feature along with an intuitive justifi-
cation. Some of these features are not instantiated for a
particular constituent. In those cases, the respective fea-
ture values are set to ?UNK?.
1. Frame ? The frame instantiated by the particular sense
of the predicate in a sentence. This is an oracle feature.
2. Selected words/POS in constituent ? Nominal predi-
cates tend to assign arguments, most commonly through
postnominal of-complements, possessive prenominal
modifiers, etc. We added the values of the first and last
word in the constituent as two separate features. Another
two features represent the part of speech of these words.
3. Ordinal constituent position ? Arguments of nouns
tend to be located closer to the predicate than those
for verbs. This feature captures the ordinal position
of a particular constituent to the left or right of the
predicate on a left or right tree traversal, eg., first PP
from the predicate, second NP from the predicate, etc.
This feature along with the position will encode the
before/after information for the constituent.
4. Constituent tree distance ? Another way of quan-
tifying the position of the constituent is to identify its
index in the list of constituents that are encountered
during linear traversal of the tree from the predicate to
the constituent.
5. Intervening verb features ? Support verbs play an
important role in realizing the arguments of nominal
predicates. We use three classes of intervening verbs:
i) auxiliary verbs ? ones with part of speech AUX, ii)
light verbs ? a small set of known light verbs: took, take,
make, made, give, gave, went and go, and iii) other verbs
? with part of speech VBx. We added three features for
each: i) a binary feature indicating the presence of the
verb in between the predicate and the constituent ii) the
actual word as a feature, and iii) the path through the
tree from the constituent to the verb, as the subject of
intervening verbs sometimes tend to be arguments of
nominalizations. The following example could explain
the intuition behind this feature:
[Speaker Leapor] makes general [Predicate assertions] [Topic
about marriage]
6. Predicate NP expansion rule ? This is the noun
equivalent of the verb sub-categorization feature used by
Gildea and Jurafsky (2002). This is the expansion rule
instantiated by the parser, for the lowermost NP in the
tree, encompassing the predicate. This would tend to
cluster NPs with a similar internal structure and would
thus help finding argumentive modifiers.
7. Noun head of prepositional phrase constituents
? Instead of using the standard head word rule for
prepositional phrases, we use the head word of the first
NP inside the PP as the head of the PP and replace the
constituent type PP with PP-<preposition>.
8. Constituent sibling features ? These are six features
representing the constituent type, head word and part of
speech of the head word of the left and right siblings
of the constituent in consideration. These are used
to capture arguments represented by the modifiers of
nominalizations.
9. Partial-path from constituent to predicate ? This
is the path from the constituent to the lowest common
parent of the constituent and the predicate. This is used
to generalize the path statistics.
10. Is predicate plural ? A binary feature indicating
whether the predicate is singular or plural as they tend to
have different argument selection properties.
11. Genitives in constituent ? This is a binary feature
which is true if there is a genitive word (one with the part
of speech POS, PRP, PRP$ or WP$) in the constituent,
as these tend to be markers for nominal arguments as in
[Speaker Burma ?s] [Phenomenon oil] [Predicate search] hits
virgin forests
12. Constituent parent features ? Same as the sibling
features, except that that these are extracted from the
constituent?s parent.
13. Verb dominating predicate ? The head word of the
first VP ancestor of the predicate.
14. Named Entities in Constituent ? As in Surdeanu
et al (2003), this is represented as seven binary fea-
tures extracted after tagging the sentence with BBN?s
IdentiFinder (Bikel et al, 1999) named entity tagger.
5 Feature Analysis and Best System
Performance
5.1 English For the task of argument identification,
features 2, 3, 4, 5 (the verb itself, path to light-verb and
presence of a light verb), 6, 7, 9, 10 an 13 contributed pos-
itively to the performance. The Frame feature degrades
performance significantly. This could be just an artifact
of the data sparsity. We trained a new classifier using all
the features that contributed positively to the performance
and the F?=1 score increased from the baseline of 72.8%
to 76.3% (?2; p < 0.05).
For the task of argument classification, adding the
Frame feature to the baseline features, provided the most
significant improvement, increasing the classification
accuracy from 70.9% to 79.0% (?2; p < 0.05). All
other features added one-by-one to the baseline did
not bring any significant improvement to the baseline,
which might again be owing to the comparatively small
training and test data sizes. All the features together
produced a classification accuracy of 80.9%. Since the
Frame feature is an oracle, we were interested in finding
out what all the other features combined contributed.
We ran an experiment with all features, except Frame,
added to the baseline, and this produced an accuracy of
73.1%, which however, is not a statistically significant
improvement over the baseline of 70.9%.
For the task of argument identification and classifi-
cation, features 8 and 11 (right sibling head word part
of speech) hurt performance. We trained a classifier
using all the features that contributed positively to the
performance and the resulting system had an improved
F?=1 score of 56.5% compared to the baseline of 51.4%
(?2; p < 0.05).
We found that a significant subset of features that con-
tribute marginally to the classification performance, hurt
the identification task. Therefore, we decided to perform
a two-step process in which we use the set of features that
gave optimum performance for the argument identifica-
tion task and identify all likely argument nodes. Then, for
those nodes, we use all the available features and classify
them into one of the possible classes. This ?two-pass?
system performs slightly better than the ?one-pass? men-
tioned earlier. Again, we performed the second pass of
classification with and without the Frame feature.
Table 2 shows the improved performance numbers.
Task P R F?=1 A
(%) (%) (%)
Id. 83.8 70.0 76.3
Classification (w/o Frame) - - - 73.1
Classification (with Frame) - - - 80.9
Id. + Classification 69.4 47.6 56.5
(one-pass, w/o Frame)
Id. + Classification 62.2 53.1 57.3
(two-pass, w/o Frame)
Id. + Classification 69.4 59.2 63.9
(two-pass, with Frame)
Table 2: Best performance on all three tasks.
5.2 Chinese For the Chinese task, we use the one-pass
algorithm as used for English. A baseline system was
created using the same features as used for English (Sec-
tion 3). We evaluate this system on just the combined task
of argument identification and classification. The base-
line performance is shown in Table 3.
To improve the system?s performance over the base-
line, we added all the features discussed in Section 4, ex-
cept features Frame ? as the data was labeled in a Prop-
Bank fashion, there are no frames involved as in Frame-
Net; Plurals and Genitives ? as they are not realized the
same way morphologically in Chinese, and Named En-
tities ? owing to the unavailability of a Chinese Named
Entity tagger. We found that of these features, 2, 3, 4, 6, 7
and 13 hurt the performance when added to the baseline,
but the other features helped to some degree, although
not significantly. The improved performance is shown in
Table 3
Features P R F?=1
(%) (%)
Baseline 86.2 32.2 46.9
Baseline 83.9 44.1 57.8
+ more features
Table 3: Parsing performance for Chinese on the com-
bined task of identifying and classifying semantic argu-
ments.
An interesting linguistic phenomenon was observed
which explains part of the reason why recall for Chinese
argument parsing is so low. In Chinese, arguments
which are internal to the NP which encompasses the
nominalized predicate, tend to be multi-word, and are
not associated with any node in the parse tree. These
violates our basic assumption of the arguments aligning
with parse tree constituents, and are guaranteed to be
missed. In the case of English however, these tend to be
single word arguments which are represented by a leaf
in the parse tree and stand a chance of getting classified
correctly.
6 Conclusion
In this paper we investigated the task of identifying and
classifying arguments of eventive nominalizations in
FrameNet. The best system generates an F1 score of
57.3% on the combined task of argument identification
and classification using automatically extracted features
on a test set of about 700 sentences using a classifier
trained on about 6,000 sentences.
As noted earlier, the bulk of past research in this area
has focused on verbal predicates. Two notable exceptions
to this include the work of (Hull and Gomez, 1996) ? a
rule based system for identifying the semantic arguments
of nominal predicates, and the work of (Lapata, 2002)
on interpreting the relation between the head of a nom-
inalized compound and its modifier noun. Unfortunately,
meaningful comparisons to these efforts are difficult due
to differing evaluation metrics.
We would like to thank Ralph Weischedel and Scott Miller of
BBN Inc. for letting us use BBN?s named entity tagger ? Iden-
tiFinder; Ashley Thornton for identifying the sentences from
FrameNet with predicates that are eventive nominalizations.
References
[Baker et al1998] Collin F. Baker, Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley FrameNet project. In
COLING/ACL-98, pages 86?90, Montreal.
[Barker and Dowty1992] Chris Barker and David Dowty. 1992.
Non-verbal thematic proto-roles. In NELS-23, Amy Schafer,
ed., GSLA, Amherst, pages 49?62.
[Bikel et al1999] Daniel M. Bikel, Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34:211?231.
[Chaniak2001] Eugene Chaniak. 2001. Immediate-head pars-
ing for language models. In ACL, Toulouse, France.
[Gildea and Jurafsky2000] Daniel Gildea and Daniel Jurafsky.
2000. Automatic labeling of semantic roles. In ACL, pages
512?520, Hong Kong, October.
[Gildea and Jurafsky2002] Daniel Gildea and Daniel Jurafsky.
2002. Automatic labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
[Gildea and Palmer2002] Daniel Gildea and Martha Palmer.
2002. The necessity of syntactic parsing for predicate ar-
gument recognition. In ACL, PA.
[Grimshaw1990] Jane Grimshaw. 1990. Argument Structure.
The MIT Press, US.
[Hacioglu and Ward2003] Kadri Hacioglu and Wayne Ward.
2003. Target word detection and semantic role chunking us-
ing support vector machines. In HLT, Edmonton, Canada.
[Hull and Gomez1996] Richard D. Hull and Fernando Gomez.
1996. Semantic interpretation of nominalizations. In AAAI
Conference, Oregon, pages 1062?1068.
[Kingsbury and Palmer2002] Paul Kingsbury and Martha Pal-
mer. 2002. From Treebank to PropBank. In LREC-2002,
Las Palmas, Canary Islands, Spain.
[Kudo and Matsumoto2000] Taku Kudo and Yuji Matsumoto.
2000. Use of support vector learning for chunk identifica-
tion. In CoNLL-2000, pages 142?144.
[Kudo and Matsumoto2001] Taku Kudo and Yuji Matsumoto.
2001. Chunking with support vector machines. In NAACL.
[Lapata2002] Maria Lapata. 2002. The disambiguation of nom-
inalizations. Computational Linguistics, 28(3):357?388.
[Macleod et al1998] C. Macleod, R. Grishman, A. Meyers,
L. Barrett, and R. Reeves. 1998. Nomlex: A lexicon of
nominalizations.
[Pradhan et al2003] Sameer Pradhan, Kadri Hacioglu, Wayne
Ward, James Martin, and Dan Jurafsky. 2003. Semantic role
parsing: Adding semantic structure to unstructured text. In
ICDM, Melbourne, Florida.
[Surdeanu et al2003] Mihai Surdeanu, Sanda Harabagiu, John
Williams, and Paul Aarseth. 2003. Using predicate-
argument structures for information extraction. In ACL, Sap-
poro, Japan.
[Thompson et al2003] Cynthia A. Thompson, Roger Levy, and
Christopher D. Manning. 2003. A generative model for se-
mantic role labeling. In ECML.
Automatic Tagging of Arabic Text: From Raw Text to Base Phrase Chunks  
Mona Diab
Linguistics Department
Stanford University
mdiab@stanford.edu
Kadri Hacioglu
Center for Spoken Language Research
University of Colorado, Boulder
hacioglu@colorado.edu
Daniel Jurafsky
Linguistics Department
Stanford University
jurafsky@stanford.edu
Abstract
To date, there are no fully automated systems
addressing the community?s need for funda-
mental language processing tools for Arabic
text. In this paper, we present a Support Vector
Machine (SVM) based approach to automati-
cally tokenize (segmenting off clitics), part-of-
speech (POS) tag and annotate base phrases
(BPs) in Arabic text. We adapt highly accu-
rate tools that have been developed for En-
glish text and apply them to Arabic text. Using
standard evaluation metrics, we report that the
SVM-TOK tokenizer achieves an

score
of 99.12, the SVM-POS tagger achieves an ac-
curacy of 95.49%, and the SVM-BP chunker
yields an


score of 92.08.
1 Introduction
Arabic is garnering attention in the NLP community due
to its socio-political importance and its linguistic differ-
ences from Indo-European languages. These linguistic
characteristics, especially dialect differences and com-
plex morphology present interesting challenges for NLP
researchers. But like most non-European languages, Ara-
bic is lacking in annotated resources and tools. Fully au-
tomated fundamental NLP tools such as Tokenizers, Part
Of Speech (POS) Taggers and Base Phrase (BP) Chun-
kers are still not available for Arabic. Meanwhile, these
tools are readily available and have achieved remarkable
accuracy and sophistication for the processing of many
European languages. With the release of the Arabic
Penn TreeBank 1 (v2.0),1 the story is about to
change.
In this paper, we propose solutions to the problems of
Tokenization, POS Tagging and BP Chunking of Arabic
text. By Tokenization we mean the process of segmenting
clitics from stems, since in Arabic, prepositions, conjunc-
tions, and some pronouns are cliticized (orthographically

This work was partially supported by the National
Science Foundation via a KDD Supplement to NSF
CISE/IRI/Interactive Systems Award IIS-9978025.
1http://www.ldc.upenn.edu/
and phonological fused) onto stems. Separating conjunc-
tions from the following noun, for example, is a key first
step in parsing. By POS Tagging, we mean the standard
problem of annotating these segmented words with parts
of speech drawn from the ?collapsed? Arabic Penn
TreeBank POS tagset. Base Phrase (BP) Chunking is
the process of creating non-recursive base phrases such
as noun phrases, adjectival phrases, verb phrases, prepo-
sition phrases, etc. For each of these tasks, we adopt
a supervised machine learning perspective using Sup-
port Vector Machines (SVMs) trained on the Arabic
TreeBank, leveraging off of already existing algorithms
for English. The results are comparable to state-of-the-art
results on English text when trained on similar sized data.
2 Arabic Language and Data
Arabic is a Semitic language with rich templatic mor-
phology. An Arabic word may be composed of a stem
(consisting of a consonantal root and a template), plus
affixes and clitics. The affixes include inflectional mark-
ers for tense, gender, and/or number. The clitics include
some (but not all) prepositions, conjunctions, determin-
ers, possessive pronouns and pronouns. Some are pro-
clitic ( attaching to the beginning of a stem) and some
enclitics (attaching to the end of a stem). The following
is an example of the different morphological segments
in the word 	
  which means and by their virtues.
Arabic is read from right to left hence the directional
switch in the English gloss.
enclitic affix stem proclitic proclitic
Arabic: Proceedings of NAACL HLT 2007, pages 9?16,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
To Memorize or to Predict: Prominence Labeling in Conversational Speech
A. Nenkova, J. Brenier, A. Kothari, S. Calhoun?, L. Whitton, D. Beaver, D. Jurafsky
Stanford University
{anenkova,jbrenier,anubha,lwhitton,dib,jurafsky}@stanford.edu
?University of Edinburgh
Sasha.Calhoun@ed.ac.uk
Abstract
The immense prosodic variation of natural con-
versational speech makes it challenging to pre-
dict which words are prosodically prominent in
this genre. In this paper, we examine a new fea-
ture, accent ratio, which captures how likely it is
that a word will be realized as prominent or not.
We compare this feature with traditional accent-
prediction features (based on part of speech and
N -grams) as well as with several linguistically mo-
tivated and manually labeled information structure
features, such as whether a word is given, new, or
contrastive. Our results show that the linguistic fea-
tures do not lead to significant improvements, while
accent ratio alone can yield prediction performance
almost as good as the combination of any other sub-
set of features. Moreover, this feature is useful even
across genres; an accent-ratio classifier trained only
on conversational speech predicts prominence with
high accuracy in broadcast news. Our results sug-
gest that carefully chosen lexicalized features can
outperform less fine-grained features.
1 Introduction
Being able to predict the prominence or pitch accent
status of a word in conversational speech is impor-
tant for implementing text-to-speech in dialog sys-
tems, as well as in detection of prosody in conversa-
tional speech recognition.
Previous investigations of prominence prediction
from text have primarily relied on robust surface fea-
tures with some deeper information structure fea-
tures. Surface features like a word?s part-of-speech
(POS) (Hirschberg, 1993) and its unigram and bi-
gram probability (Pan and McKeown, 1999; Pan and
0Thanks to the Edinburgh-Stanford Link and ONR (MURI
award N000140510388) for generous support.
Hirschberg, 2000) are quite useful; content words
are much more likely to be accented than function
words, and words with higher probability are less
likely to be prominent. More sophisticated linguis-
tic features have also been used, generally based on
information-structural notions of contrast, focus, or
given-new. (Hirschberg, 1993).
For example, in the Switchboard utterance be-
low, there is an intrinsic contrast between the words
?women? and ?men?, making both terms more
salient (words in all capital letters represent promi-
nent tokens):
you SEE WOMENc GOING off to WARS as WELL as
MENc.
Similarly the givenness of a word may help deter-
mine its prominence. The speaker needs to focus the
hearer?s attention on new entities in the discourse, so
these are likely to be realized as prominent. Old en-
tities, on the other had, need not be prominent; these
tendencies can be seen in the following example.
theyold have all the WATERnew theyold WANT. theyold
can ACTUALLY PUMP waterold.
While previous models have attempted to cap-
ture global properties of words (via POS or unigram
probability), they have not in general used word
identity as a predictive feature, assuming either that
current supervised training sets would be too small
or that word identity would not be robust across gen-
res (Pan et al, 2002). In this paper, we show a way
to capture word identity in a feature, accent ratio,
that works well with current small supervised train-
ing sets, and is robust to genre differences.
We also use a corpus which has been hand-
labeled for information structure features (including
given/new and contrast information) to investigate
the relative usefulness of both linguistic and shallow
features, as well as how well different features com-
bine with each other.
9
2 Data and features
For our experiments we use 12 Switchboard (God-
frey et al, 1992) conversations, 14,555 tokens in to-
tal. Each word was manually labeled for presence
or absence of pitch accent1 , as well as additional
features including information status (or givenness),
contrast and animacy distinctions, (Nissim et al,
2004; Calhoun et al, 2005; Zaenen et al, 2004), fea-
tures that linguistic literature suggests are predictive
of prominence (Bolinger, 1961; Chafe, 1976).
All of the features described in detail below have
been shown to have statistically significant correla-
tion with prominence (Brenier et al, 2006).
Information status The information status (IS),
or givenness, of discourse entities is important for
choosing appropriate reference form (Prince, 1992;
Gundel et al, 1993) and possibly plays a role in
prominence decisions as well (Brown, 1983). No
previous studies have examined the usefulness of
information status in naturally occurring conversa-
tional speech. The annotation in our corpus is based
on the givenness hierarchy of Prince: first mentions
of entities were marked as new and subsequent men-
tions as old. Entities that are not previously men-
tioned, but that are generally known or semantically
related to other entities in the preceding context are
marked as mediated. Obviously, the givenness an-
notation applies only to referring expressions, i.e.
noun phrases the semantic interpretation of which is
a discourse entity. This restriction inherently limits
the power of the feature for prominence prediction,
which has to be performed for all classes of words.
Complete details of the IS annotation can be found
in (Nissim et al, 2004).
Kontrast One reason speakers make entities in
an utterance prominence is because of information
structure considerations (Rooth, 1992; Vallduv?? and
Vilkuna, 1998). That is, parts of an utterance which
distinguish the information the speaker actually says
from the information they could have said, are made
salient, e.g. because that information answers a
question, or contrasts with a similar entity in the
context. Several possible triggers of this sort of
salience were marked in the corpus, with words that
were not kontrastive (in this sense) being marked as
background:
1Of all tokens, 8,429 (or 58%) were not accented.
? contrastive if the word is directly differentiated
from a previous topical or semantically-related
word;
? subset if it refers to a member of a more general
set mentioned in the surrounding context;
? adverbial if a focus-sensitive adverb such as
?only? or ?even? is associated with the word
being annotated;
? correction if the speaker intended to correct or
clarify a previous word or phrase;
? answer if the word completes a question by the
other speaker;
? nonapplic for filler phrases such as ?in fact?, ?I
mean?, etc.
Note that only content words in full sentences
were marked for kontrast, and filler phrases such
as ?in fact? and ?I mean? were excluded. A com-
plete description of the annotation guidelines can be
found in (Calhoun et al, 2005).
Animacy Each noun and pronoun is labeled for the
animacy of its referent (Zaenen et al, 2004). The
categories include concrete, non-concrete, human,
organizations, place, and time.
Dialog act Specifies the function of the utterance
such as statement, opinion, agree, reject, abandon;
or type of question (yes/no, who, rhetoric)
In addition to the above theoretically motivated
features, we used several automatically derivable
word measures.
Part-of-speech Two such features were used, the
full Penn Treebank tagset (called POS) , and a col-
lapsed tagset (called BroadPOS) with six broad cat-
egories (nouns, verbs, function words, pronouns, ad-
jectives and adverbs).
Unigram and bigram probability These features
are defined as log(pw) and log(pwi |pwi?1) respec-
tively and their values were calculated from the
Fisher corpus (Cieri et al, 2004). High probability
words are less likely to be prominent.
TF.IDF This measure captures how central a word is
for a particular conversation. It is a function of the
frequency of occurrence of the word in the conver-
sation (nw), the number of conversations that con-
tain the word in a background corpus (k) and the
number of all conversations in the background cor-
pus (N ). Formally, TF.IDF1 = nw ? log(Nk ). We
10
also used a variant, TF.IDF2, computed by normal-
izing TF.IDF1 by the number of occurrences of the
most frequent word in the conversation. TF.IDF2 =
TF.IDF1/max(nw?conv). Words with high TF.IDF
values are important in the conversation and are
more likely to be prominent.
Stopword This is a binary feature indicating if the
word appears in a high-frequency stopword list from
the Bow toolkit (McCallum, 1996). The list spans
both function and content word classes, though nu-
merals and some nouns and verbs were removed.
Utterance length The number of words.
Length The number of characters in the words. This
feature is correlated with phonetic features that have
been shown to be useful for the task, such as the
number of vowels or phones in the word.
Position from end/beginning The position of the
word in the utterance divided by the number of
words that precede the current word.
Accent ratio This final (new) feature takes the
?memorization? of previous productions of a given
word to the extreme, measuring how likely it is that
a word belongs to a prominence class or not. Our
feature extends an earlier feature proposed by (Yuan
et al, 2005), which was a direct estimate of how
likely it is for the word to be accented as observed
in some corpus. (Yuan et al, 2005) showed that the
original accent ratio feature was not included in the
best set of features for accent prediction. We believe
the reason for this is the fact that the original ac-
cent ratio feature was computed for all words, even
words in which the value was indistinguishable from
chance (.50). Our new feature incorporates the sig-
nificance of the prominence probability, assuming a
default value of 0.5 for those words for which there
is insufficient evidence in the training data. More
specifically,
AccentRatio(w) =
{
k
n if B(k, n, 0.5) ? 0.05
0.5 otherwise
where k is the number of times word w appeared
accented in the corpus, n is the total number of
times the word w appeared, and B(k, n, 0.5) is
the probability (under a binomial distribution) that
there are k successes in n trials if the probabil-
ity of success and failure is equal. Simply put,
the accent ratio of a word is equal to the esti-
mated probability of the word being accented if this
probability is significantly different from 0.5, and
equal to 0.5 otherwise. For example, AccentRa-
tio(you)=0.3407, AccentRatio(education)=0.8666,
and AccentRatio(probably)=0.5.
Many of our features for accent prediction are
based only on the 12 training conversations. Other
features, such as the unigram, bigram, and TF*IDF
features, are computed from larger data sources. Ac-
cent ratio is also computed over a larger corpus,
since the binomial test requires a minimum of six
occurrences of a word in the corpus in order to get
significance and assign an accent ratio value differ-
ent from 0.5. We thus used 60 Switchboard conver-
sations (Ostendorf et al, 2001), annotated for pitch
accent, to compute k and n for each word.
3 Results
For our experiments we used the J48 decision trees
in WEKA (Witten and Frank, 2005). All the results
that we report are from 10-fold cross-validation on
the 12 Switchboard conversations.
Some previous studies have reported results on
prominence prediction in conversational speech with
the Switchboard corpus. Unfortunately these studies
used different parts of the corpus or different label-
ings (Gregory and Altun, 2004; Yuan et al, 2005),
so our results are not directly comparable. Bear-
ing this difference in mind, the best reported results
to our knowledge are those in (Gregory and Altun,
2004), where conditional random fields were used
with both textual, acoustic, and oracle boundary fea-
tures to yield 76.36% accuracy.
Table 1 shows the performance of decision tree
classifiers using a single feature. The majority class
baseline (not accented) has accuracy of 58%. Accent
ratio is the most predictive feature: the accent ratio
classifier has accuracy of 75.59%, which is two per-
cent net improvement above the previously known
best feature (unigram). The accent ratio classifier
assigns a ?no accent? class to all words with accent
ratio lower than 0.38 and ?accent? to all other words.
In Section 4 we discuss in detail the accent ratio dic-
tionary, but it is worth noting that it does correctly
classify even some high-frequency function words
like ?she?, ?he?, ?do? or ?up? as accented.
11
3.1 Combining features
We would expect that a combination of features
would lead to better prediction when compared to
a classifier based on a single feature. Several past
studies have examined classes of features. In order
to quantify the utility of different specific features,
we ran exhaustive experiments producing classifiers
with all possible combinations of two, three, four
and five features.
As we can see from figure 1 and table 2, the clas-
sifiers using accent ratio as a feature perform best,
for all sizes of feature sets. Moreover, the increase
of performance compared to a single-feature classi-
fier is very slight when accent ratio is used as fea-
ture. Kontrast seems to combine well with accent
ratio and all of the best classifiers with more than
one feature use kontrast in addition to accent ratio.
This indicates that automatic detection of kontrast
can potentially help in prominence prediction. But
the gains are small, the best classifiers without kon-
trast but still including accent ratio perform within
0.2 percent of the classifiers that use both.
On the other hand, classifiers that do not use ac-
cent ratio perform poorly compared to those that do,
and even a classifier using five features (unigram,
broad POS, token length, position from beginning
and bigram) performs about as well as a classifier
using solely accent ratio as a feature. Also, when
accent ratio is not used, the overall improvement of
the classifier grows faster with the addition of new
features. This suggest that accent ratio provides rich
information about words beyond that of POS class
and general informativeness.2
Table 2 gives the specific features in (n + 1)-
feature classifiers that lead to better results than the
best n-classifier. The figures are for the classifiers
performing best overall. Interestingly, none of these
best classifiers for all feature set sizes uses POS or
unigram as a feature. We assume that accent ratio
captures all the relevant information that is present
in the unigram and POS features. The best classifier
with five features uses, in addition to accent ratio,
kontrast, tf.idf, information status and distance from
the beginning of the utterance. All of these features
convey somewhat orthogonal information: seman-
2To verify this we will examine the accent ratio dictionary
in closer detail in the next section.
Accent Ratio (AR) 75.59%
AR + Kontrast 76.15%
AR + END/BEG 75.91%
AR + tf.idf2 75.82%
AR + Info Status 75.82%
AR + Length 75.77%
AR + tf.idf1 75.74%
AR + unigram 75.71%
AR + stopword 75.70%
AR + kontrast + length 76.45%
AR + kontrast + BEG 76.24%
AR + kontrast + unigram 76.24%
AR + kontrast + tf.idf1 76.24%
AR + kontrast + length + tfidf1 76.56%
AR + kontrast + length + stopword 76.54%
AR + kontrast + length +tf.idf2 76.52%
AR + kontrast + Status + BEG 76.47%
AR + kontrast + tf.idf1 + Status + BEG 76.65%
AR + kontrast + tf.idf2 + Status + BEG 76.58%
Table 2: Performance increase augmenting the ac-
cent ratio classifier.
tic, topicality, discourse and phrasing information
respectively. Still, all of them in combination im-
prove the performance over accent ratio as a single
feature only by one percent.
Figure 1 shows the overall improvement of clas-
sifiers with the addition of new features in three sce-
narios: overall best, best when kontrast is not used
as a feature and best with neither kontrast nor ac-
cent ratio. The best classifier with five features that
do not include kontrast has accent ratio, broad POS,
word length, stopword and bigram as features and
has accuracy of 76.28%, or just 0.27% worse than
the overall best classifier that uses kontrast and in-
formation status. This indicates that while there is
some benefit to using the two features, they do not
lead to any substantial boost in performance. Strik-
ingly, the best classifier that uses neither accent ra-
tio nor kontrast performs very similarly to a classi-
fier using accent ratio as the only feature: 75.82%
for the classifier using unigram, POS, tf.idf1, word
length and position from end of the utterance.
3.2 The power of linguistic features
One of the objectives of our study was to assess how
useful gold-standard annotations for complex lin-
guistic features are for the task of prominence pre-
diction. The results in this section indicate that an-
imacy distinctions (concrete/non-concrete, person,
time, etc) and dialog act did not have much power
12
AccentRatio unigram stopword POS tf.idf2 tf.idf1 BroadPos Length Kontrast bigram Info Stat
75.59 73.77 70.77 70.28 70.14 69.50 68.64 67.64 67.57 65.87 64.13
Table 1: Single feature classifier performance. Features not in the table (position from end, animacy, utter-
ance length and dialog act) all achieve lower accuracy of around 60%
1 2 3 4 5
73
74
75
76
77
78
Classifier performance
Number of features
Pr
ed
ict
io
n 
ac
cu
ra
cy
Overall best
Without kontrast
Without accent
+ ratio or kontrast
Figure 1: Performance increase with the addition of
new features.
as individual features (table 1) and were never in-
cluded in a model that was best for a given feature
set size (table 2).
Information status is somewhat useful and ap-
pears in the overall best classifier with five features
(table 2). But when compared with other classifiers
with the same number of features, the benefits from
adding information status to the model are small.
For example, the accent ratio + information status
classifier performs 0.23% better than accent ratio
alone, but so does the classifier using accent ratio
and tf.idf. There are two reasons that can explain
why the givenness of the referent is not as helpful
as we might have hoped. First of all, the informa-
tion status distinction applies only to referring ex-
pressions and has undefined values for words such
as verbs, adjectives or function words. Second, in-
formation status of an entity influences the form of
referring expression that is used, with old items be-
ing more likely to be pronominalized. In the numer-
ous cases where pronominalization of old informa-
tion does occur, features such as POS, unigram or
accent ratio will be sensitive to the change of infor-
mation status simply based on the lexical item.
Kontrast is by far the most useful linguistic fea-
ture. It is used in all of the best classifiers for any
feature set size (table 2). It applies to more words
than givenness does, since salience distinctions can
be made for any part-of-speech class. Still, not all
words were annotated for kontrast either, and more-
over kontrast only captures one kind of semantic
salience. This is particularly true of discourse mark-
ers like ?especially? or ?definitely?: these would ei-
ther be in sentence fragments that weren?t marked
for kontrast, or would probably be marked as ?back-
ground? since they are not salience triggers in a se-
mantic sense. As we can see from figure 1, clas-
sifiers that use kontrast perform only slightly better
than others that use only ?cheaper? features.
4 The accent ratio dictionary
Contrary to our initial expectations, both classes in
the accent ratio dictionary (for both low and high
probability of being prominent) cover the full set of
possible POS categories. Tables 3 and 4 list words in
both classes (with words sorted by increasing accent
ratio in each column). The ?no accent? class is dom-
inated by function words, but also includes nouns
and verbs. One of the drawbacks of POS as a fea-
ture for prominence prediction is that normally aux-
iliary verbs will be tagged as ?VB?, the same class
as other more contentful verbs. The informativeness
(unigram probability) of a word would distinguish
between these types of verbs, but so does the accent
ratio measure as well.
Furthermore, some relatively frequent words such
as ?too?, ?now?, ?both?, ?no?, ?yes?, ?else?, ?wow?
have high accent ratio, that is, a high probability for
accenting. Such distinctions within the class of func-
tion words would not be possible on the basis of in-
13
.00?.08 .09?.16 .17?.24 .25?.32 .33?.42
a could you?d being me
uh in because take i?ve
um minutes oh said we?re
uh-huh and since wanna went
the by says been over
an who us those you
of grew where into thing
to cause they?ve little what
were gonna am until some
as about sort they?re out
than their you?re I had
with but didn?t that make
at on her don?t way
for be going this did
from through i?ll should anything
or which will type i?m
you?ve are our we kind
was we?ll just so go
would during though have stuff
it huh like got then
when is your new she
them bit needs mean he
it?s there?s my much do
if any many i?d up
can has they know
him stayed get doesn?t
these supposed there even
Table 3: Accent ratio entries with low prominence
probability.
formativeness, POS, or even information structure
features. Another class like that is words like ?yes?,
?okay?, ?sure? that are mostly accented by virtue of
being the only word in the phrase.
Some rather common words, ?not? for example,
are not included in the accent ratio dictionary be-
cause they do not exhibit a statistically strong pref-
erence for a prominence class. The accent ratio clas-
sifier would thus assign class ?accented? to the word
?not?, which is indeed the class this word occurs in
more often.
Another fact that becomes apparent with the in-
spection of the accent ratio dictionary is that while
certain words have a statistically significant prefer-
ence for deaccenting, there is also a lot of variation
in their observed realization. For example, personal
pronouns such as ?I? and ?you? have accent ratios
near 0.33. This means that every third such pronoun
was actually realized as prominent by the speaker.
In a conversational setting there is an implicit con-
trast between the two speakers, which could partly
explain the phenomenon, but the situations which
prompt the speaker to realize the distinction in their
.58?.74 .75?.79 .80?.86 .87?1.0
lot both sometimes half
time no change topic
now seems child else
kids life young obviously
old tell Texas themselves
too ready town wow
really easy room gosh
three heard pay anyway
work isn?t interesting Dallas
nice again true outside
yeah first mother mostly
two right problems yes
person children agree great
day married war exactly
working may needed especially
job happen told definitely
talking business finally lately
usually still neat thirty
rather daughter sure higher
places gone house forty
government guess okay hey
ten news seven Iowa
parents major best poor
paper fact also glad
actually five older basic
Table 4: Accent ratio values for words with high
probability for being accented.
speech will be the focus of a future linguistic inves-
tigation.
Kontrast is helpful in predicting ?accented? class
for some generally low ratio words. However, even
with its help, production variation in the conversa-
tions cannot be fully explained. The following ex-
amples from our corpus show low accent ratio words
(that, did, and, have, had) that were produced as
prominent.
so i did THAT. and then i, you know, i DID that for SIX
years. AND then i stayed HOME with my SON.
i HAVE NOT, to be honest, HAD much EXPERIENCE
with CHILDREN in that SITUATION.
they?re going to HAVE to WORK it OUT to WORKING
part TIME.
The examples attest to the presence of variation
in production: in the first utterance, for example, we
see the words ?did?, ?and? and ?that? produced both
as prominent and not prominent. Intonational phras-
ing most probably accounts for some of this varia-
tion since it is likely that even words that are typ-
ically not prominent will be accented if they occur
just before or after a longer pause. We come back to
this point in the closing section.
14
5 Robustness of accent ratio
While accent ratio works well for our data (Table
2), a feature based so strongly on memorizing the
status of each word in the training data might lead
to problems. One potential problem, suggested by
Pan et al (2002) for lexicalized features in general,
is whether a lexical feature like accent ratio might
be less robust across genres. Another question is
whether our definition of accent ratio is better than
one that does not use the binomial test: we need to
investigate whether these statistical tests indeed im-
prove performance. We focus on these two issues in
the next two subsections.
Binomial test cut-off
As discussed above, the original accent ratio feature
(Yuan et al, 2005) was based directly on the frac-
tion of accented occurrences in the training set. We
might expect such a use of raw frequencies to be
problematic. Given what we know about word dis-
tributions in text (Baayen, 2001), we would expect
about half of the words in a big corpus to appear only
once. In an accent ratio dictionary without binomial
test cut-off, all such words will have accent ratio of
either exactly 1 or 0, but one or even few occurrences
of a word would not be enough to determine statis-
tical significance. By contrast, our modified accent
ratio feature uses binomial test cut-off to make the
accent ratio more robust to small training sets.
To test if the binomial test cut-off really improved
the accent ratio feature, we compared the perfor-
mance on Switchboard of classifiers using accent
ratio with and without cut-off. The binominal test
improved the performance of the accent ratio fea-
ture from 73.49% (Yuan et al original version) to
75.59% (our version).
Moreover, Yuan et al report that their version of
the feature did not combine well with other features,
while in our experiments best performance was al-
ways achieved by the classifiers that made use of the
accent ratio feature in addition to others.
A cross-genre experiment: broadcast news
In a systematic analysis of the usefulness of differ-
ent informativeness, syntactic and semantic features
for prominence prediction, Pan et al (2002) showed
that word identity is a powerful feature. But they hy-
pothesized that this would not be a useful feature in
a domain independent pitch accent prediction task.
Their hypothesis that word identity cannot be a ro-
bust across genres would obviously carry over to ac-
cent ratio. In order to test the hypothesis, we used
the accent ratio dictionary derived from the Switch-
board corpus to predict prominence in the Boston
University Radio corpus of broadcast news. Using
an accent ratio dictionary from Switchboard and as-
signing class ?not accented? to words with accent ra-
tio less than 0.38 and ?accented? otherwise leads to
82% accuracy of prediction for this broadcast news
corpus. If the accent ratio dictionary is built from
the BU corpus itself, the performance is 83.67%.3
These results indicate that accent ratio is a robust
enough feature and is applicable across genres.
6 Conclusions and future work
In this paper we introduced a new feature for promi-
nence prediction, accent ratio. The accent ratio of
a word is the (maximum likelihood estimate) prob-
ability that a word is accented if there is a signifi-
cant preference for a class, and 0.5 otherwise. Our
experiments demonstrate that the feature is power-
ful both by itself and in combination with other fea-
tures. Moreover, the feature is robust to genre, and
accent ratio dictionaries can be used for prediction
of prominence in read news with very good results.
Of the linguistic features we examined, kontrast
is the only one that is helpful beyond what can be
gained using shallow features such as n-gram prob-
ability, POS or tf.idf. While the improvements from
kontrast are relatively small, the consistency of these
small improvements suggest that developing auto-
matic methods for approximating the gold-standard
annotation we used here, similar to what has been
done for information status in (Nissim, 2006), may
be worthwhile. An automatic predictor for kontrast
may also be helpful in other applications such as
question answering or textual entailment.
All of the features in our study were text-based.
There is a wide variety of research investigating
phonological or acoustic features as well. For exam-
ple Gregory and Altun (2004) used acoustic features
3This result is comparable with the result of (Yuan et al,
2005) who in their experiment with the same corpus report the
best result as 83.9% using three features: unigram, bigram and
backwards bigram probability.
15
such as duration and energy, and phonological fea-
tures such as oracle (hand-labeled) intonation phrase
boundaries, and the number of phones and sylla-
bles in a word. Although acoustic features are not
available in a text-to-speech scenario, we hypothe-
size that in a task where such features are available
(such as in speech recognition applications), acous-
tic or phonological features could improve the per-
formance of our text-only features. To test this hy-
pothesis, we augmented our best 5-feature classifier
which did not include kontrast with hand-labeled in-
tonation phrase boundary information. The resulting
classifier reached an accuracy of 77.45%, more than
one percent net improvement over 76.28% accuracy
of the model based solely on text features and not in-
cluding kontrast. Thus in future work we plan to in-
corporate more acoustic and phonological features.
Finally, prominence prediction classifiers need to
be incorporated in a speech synthesis system and
their performance should be gauged via listening
experiments that test whether the incorporation of
prominence leads to improvement in synthesis.
References
R. H. Baayen. 2001. Word Frequency Distributions.
Kluwer Academic Publishers.
D.L. Bolinger. 1961. Contrastive Accent and Contrastive
Stress. Language, 37(1):83?96.
J. Brenier, A. Nenkova, A. Kothari, L. Whitton,
D. Beaver, and D. Jurafsky. 2006. The (non)utility of
linguistic features for predicting prominence in spon-
taneous speech. In IEEE/ACL 2006 Workshop on Spo-
ken Language Technology.
G. Brown. 1983. Prosodic structure and the given/new
distinction. Prosody: Models and Measurements,
pages 67?77.
S. Calhoun, M. Nissim, M. Steedman, and J.M. Brenier.
2005. A framework for annotating information struc-
ture in discourse. Pie in the Sky: Proceedings of the
workshop, ACL, pages 45?52.
W. Chafe. 1976. Givenness, contrastiveness, definite-
ness, subjects, topics, and point of view. Subject and
Topic, pages 25?55.
C. Cieri, D. Graff, O. Kimball, D. Miller, and Kevin
Walker. 2004. Fisher English training speech part 1
transcripts. LDC.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In IEEE ICASSP-92.
M. Gregory and Y. Altun. 2004. Using conditional ran-
dom fields to predict pitch accents in conversational
speech. Proceedings of ACL, 2004.
J. Gundel, N. Hedberg, and R. Zacharski. 1993. Cog-
nitive status and the form of referring expressions in
discourse. Language, 69:274?307.
J. Hirschberg. 1993. Pitch Accent in Context: Predicting
Intonational Prominence from Text. Artificial Intelli-
gence, 63(1-2):305?340.
A. McCallum. 1996. Bow: A toolkit for statistical lan-
guage modeling, text retrieval, classification and clus-
tering. http://www.cs.cmu.edu/ mccallum/bow.
M. Nissim, S. Dingare, J. Carletta, and M. Steedman.
2004. An annotation scheme for information status in
dialogue. In LREC 2004.
M. Nissim. 2006. Learning information status of dis-
course entities. In Proceedings of EMNLP 2006.
M. Ostendorf, I. Shafran, S. Shattuck-Hufnagel,
L. Carmichael, and W. Byrne. 2001. A prosodically
labeled database of spontaneous speech. Proc. of the
ISCA Workshop on Prosody in Speech Recognition and
Understanding, pages 119?121.
S. Pan and J. Hirschberg. 2000. Modeling local context
for pitch accent prediction. In Proceedings of ACL-00.
S. Pan and K. McKeown. 1999. Word informativeness
and automatic pitch accent modeling. In Proceedings
of EMNLP/VLC-99.
S. Pan, K. McKeown, and J. Hirschberg. 2002. Ex-
ploring features from natural language generation in
prosody modeling. Computer speech and language,
16:457?490.
E. Prince. 1992. The ZPG letter: subject, definiteness,
and information status. In S. Thompson and W. Mann,
editors, Discourse description: diverse analyses of a
fund raising text, pages 295?325. John Benjamins.
Mats Rooth. 1992. A theory of focus interpretation. Nat-
ural Language Semantics, 1(1):75?116.
E. Vallduv?? and M. Vilkuna. 1998. On rheme and kon-
trast. Syntax and Semantics, 29:79?108.
I. H. Witten and E. Frank. 2005. Data Mining: Practical
machine learning tools and techniques. 2nd Edition,
Morgan Kaufmann, San Francisco.
J. Yuan, J. Brenier, and D. Jurafsky. 2005. Pitch Accent
Prediction: Effects of Genre and Speaker. Proceed-
ings of Interspeech.
A. Zaenen, J. Carletta, G. Garretson, J. Bresnan,
A. Koontz-Garboden, T. Nikitina, M.C. O?Connor, and
T. Wasow. 2004. Animacy Encoding in English: why
and how. ACL Workshop on Discourse Annotation.
16
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 638?646,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Extracting Social Meaning: Identifying Interactional Style in Spoken
Conversation
Dan Jurafsky
Linguistics Department
Stanford University
jurafsky@stanford.edu
Rajesh Ranganath
Computer Science Department
Stanford University
rajeshr@cs.stanford.edu
Dan McFarland
School of Education
Stanford University
dmcfarla@stanford.edu
Abstract
Automatically extracting social meaning and
intention from spoken dialogue is an impor-
tant task for dialogue systems and social com-
puting. We describe a system for detecting
elements of interactional style: whether a
speaker is awkward, friendly, or flirtatious.
We create and use a new spoken corpus of 991
4-minute speed-dates. Participants rated their
interlocutors for these elements of style. Us-
ing rich dialogue, lexical, and prosodic fea-
tures, we are able to detect flirtatious, awk-
ward, and friendly styles in noisy natural con-
versational data with up to 75% accuracy,
compared to a 50% baseline. We describe sim-
ple ways to extract relatively rich dialogue fea-
tures, and analyze which features performed
similarly for men and women and which were
gender-specific.
1 Introduction
How can we extract social meaning from speech, de-
ciding if a speaker is particularly engaged in the con-
versation, is uncomfortable or awkward, or is partic-
ularly friendly and flirtatious? Understanding these
meanings and how they are signaled in language is
an important sociolinguistic task in itself. Extracting
them automatically from dialogue speech and text
is crucial for developing socially aware computing
systems for tasks such as detection of interactional
problems or matching conversational style, and will
play an important role in creating more natural dia-
logue agents (Pentland, 2005; Nass and Brave, 2005;
Brave et al, 2005).
Cues for social meaning permeate speech at every
level of linguistic structure. Acoustic cues such as
low and high F0 or energy and spectral tilt are impor-
tant in detecting emotions such as annoyance, anger,
sadness, or boredom (Ang et al, 2002; Lee and
Narayanan, 2002; Liscombe et al, 2003), speaker
characteristics such as charisma (Rosenberg and
Hirschberg, 2005), or personality features like extro-
version (Mairesse et al, 2007; Mairesse and Walker,
2008). Lexical cues to social meaning abound.
Speakers with links to depression or speakers who
are under stress use more first person singular pro-
nouns (Rude et al, 2004; Pennebaker and Lay, 2002;
Cohn et al, 2004), positive emotion words are cues
to agreeableness (Mairesse et al, 2007), and neg-
ative emotion words are useful cues to deceptive
speech (Newman et al, 2003). The number of words
in a sentence can be a useful feature for extroverted
personality (Mairesse et al, 2007). Finally, dia-
log features such as the presence of disfluencies
can inform listeners about speakers? problems in ut-
terance planning or about confidence (Brennan and
Williams, 1995; Brennan and Schober, 2001).
Our goal is to see whether cues of this sort are
useful in detecting particular elements of conversa-
tional style and social intention; whether a speaker
in a speed-dating conversation is judged by the in-
terlocutor as friendly, awkward, or flirtatious.
2 The Corpus
Our experiments make use of a new corpus we have
collected, the SpeedDate Corpus. The corpus is
based on three speed-dating sessions run at an elite
638
private American university in 2005 and inspired
by prior speed-dating research (Madan et al, 2005;
Pentland, 2005). The graduate student participants
volunteered to be in the study and were promised
emails of persons with whom they reported mutual
liking. Each date was conducted in an open setting
where there was substantial background noise. All
participants wore audio recorders on a shoulder sash,
thus resulting in two audio recordings of the approx-
imately 1100 4-minute dates. In addition to the au-
dio, we collected pre-test surveys, event scorecards,
and post-test surveys. This is the largest sample we
know of where audio data and detailed survey infor-
mation were combined in a natural experiment.
The rich survey information included date per-
ceptions and follow-up interest, as well as gen-
eral attitudes, preferences, and demographic infor-
mation. Participants were also asked about the
conversational style and intention of the interlocu-
tor. Each speaker was asked to report how of-
ten their date?s speech reflected different conversa-
tional styles (awkward, friendly, flirtatious, funny,
assertive) on a scale of 1-10 (1=never, 10=con-
stantly): ?How often did the other person behave in
the following ways on this ?date???. We chose three
of these five to focus on in this paper.
We acquired acoustic information by taking the
acoustic wave file from each recorder and manually
segmenting it into a sequence of wavefiles, each cor-
responding to one 4-minute date. Since both speak-
ers wore microphones, most dates had two record-
ings, one from the male recorder and one from the
female recorder. Because of mechanical, opera-
tor, and experimenter errors, some recordings were
lost, and thus some dates had only one recording.
Transcribers at a professional transcription service
used the two recordings to create a transcript for
each date, and time-stamped the start and end time
of each speaker turn. Transcribers were instructed
to mark various disfluencies as well as some non-
verbal elements of the conversation such as laughter.
Because of noise, participants who accidentally
turned off their mikes, and some segmentation and
transcription errors, a number of dates were not pos-
sible to analyze. 19 dates were lost completely, and
for an additional 130 we lost one of the two audio
tracks and had to use the remaining track to extract
features for both interlocutors. The current study fo-
cuses on the 991 remaining clean dates for which
we had usable audio, transcripts, and survey infor-
mation.
3 The Experiments
Our goal is to detect three of the style variables, in
particular awkward, friendly, or flirtatious speakers,
via a machine learning classifier. Recall that each
speaker in a date (each conversation side) was la-
beled by his or her interlocutor with a rating from
1-10 for awkward, friendly, or flirtatious behavior.
For the experiments, the 1-10 Likert scale ratings
were first mean-centered within each respondent so
that the average was 0. Then the top ten percent of
the respondent-centered meaned Likert ratings were
marked as positive for the trait, and the bottom ten
percent were marked as negative for a trait. Thus
each respondent labels the other speaker as either
positive, negative, or NA for each of the three traits.
We run our binary classification experiments to
predict this output variable.
For each speaker side of each 4-minute conversa-
tion, we extracted features from the wavefiles and
the transcript, as described in the next section. We
then trained six separate binary classifiers (for each
gender for the 3 tasks), as described in Section 5.
4 Feature Extraction
In selecting features we drew on previous research
on the use of relatively simple surface features that
cue social meaning, described in the next sections.
Each date was represented by the two 4-minute
wavefiles, one from the recorder worn by each
speaker, and a single transcription. Because of the
very high level of noise, the speaker wearing the
recorder was much clearer on his/her own recording,
and so we extracted the acoustic features for each
speaker from their own microphone (except for the
130 dates for which we only had one audio file). All
lexical and discourse features were extracted from
the transcripts.
All features describe the speaker of the conversa-
tion side being labeled for style. The features for
a conversation side thus indicate whether a speaker
who talks a lot, laughs, is more disfluent, has higher
F0, etc., is more or less likely to be considered flir-
tatious, friendly, or awkward by the interlocutor. We
639
also computed the same features for the alter inter-
locutor. Alter features thus indicate the conversa-
tional behavior of the speaker talking with an inter-
locutor they considered to be flirtatious, friendly, or
awkward.
4.1 Prosodic Features
F0 and RMS amplitude features were extracted us-
ing Praat scripts (Boersma and Weenink, 2005).
Since the start and end of each turn were time-
marked by hand, each feature was easily extracted
over a turn, and then averages and standard devia-
tions were taken over the turns in an entire conversa-
tion side. Thus the feature F0 MIN for a conversation
side was computed by taking the F0 min of each turn
in that conversation side (not counting zero values of
F0), and then averaging these values over all turns in
the side. F0 MIN SD is the standard deviation across
turns of this same measure.
Note that we coded four measures of f0 varia-
tion, not knowing in advance which one was likely
to be the most useful: F0 MEAN SD is the deviation
across turns from the global F0 mean for the con-
versation side, measuring how variable the speakers
mean f0 is across turns. F0 SD is the standard devia-
tion within a turn for the f0 mean, and then averaged
over turns, hence measures how variable the speak-
ers f0 is within a turn. F0 SD SD measures how much
the within-turn f0 variance varies from turn to turn,
and hence is another measure of cross-turn f0 vari-
ation. PITCH RANGE SD measures how much the
speakers pitch range varies from turn to turn, and
hence is another measure of cross-turn f0 variation.
4.2 Lexical Features
Lexical features have been widely explored in the
psychological and computational literature. For
these features we drew mainly on the LIWC lexicons
of Pennebaker et al (2007), the standard for social
psychological analysis of lexical features. From the
large variety of lexical categories in LIWC we se-
lected ten that the previous work of Mairesse et al
(2007) had found to be very significant in detect-
ing personality-related features. The 10 LIWC fea-
tures we used were Anger, Assent, Ingest, Insight,
Negemotion, Sexual, Swear, I, We, and You. We also
added two new lexical features, ?past tense auxil-
iary?, a heuristic for automatically detecting narra-
F0 MIN minimum (non-zero) F0 per turn, av-
eraged over turns
F0 MIN SD standard deviation from F0 min
F0 MAX maximum F0 per turn, averaged over
turns
F0 MAX SD standard deviation from F0 max
F0 MEAN mean F0 per turn, averaged over turns
F0 MEAN SD standard deviation (across turns) from
F0 mean
F0 SD standard deviation (within a turn)
from F0 mean, averaged over turns
F0 SD SD standard deviation from the f0 sd
PITCH RANGE f0 max - f0 min per turn, averaged
over turns
PITCH RANGE
SD
standard deviation from mean pitch
range
RMS MIN minimum amplitude per turn, aver-
aged over turns
RMS MIN SD standard deviation from RMS min
RMS MAX maximum amplitude per turn, aver-
aged over turns
RMS MAX SD standard deviation from RMS max
RMS MEAN mean amplitude per turn, averaged
over turns
RMS MEAN SD standard deviation from RMS mean
TURN DUR duration of turn in seconds, averaged
over turns
TIME total time for a speaker for a conversa-
tion side, in seconds
RATE OF
SPEECH
number of words in turn divided by
duration of turn in seconds, averaged
over turns
Table 1: Prosodic features for each conversation side,
extracted using Praat from the hand-segmented turns of
each side.
tive or story-telling behavior, and Metadate, for dis-
cussion about the speed-date itself. The features are
summarized in Table 2.
4.3 Dialogue Act and Adjacency Pair Features
A number of discourse features were extracted,
drawing from the conversation analysis, disfluency
and dialog act literature (Sacks et al, 1974; Juraf-
sky et al, 1998; Jurafsky, 2001). While discourse
features are clearly important for extracting social
meaning, previous work on social meaning has met
with less success in use of such features (with the
exception of the ?critical segments? work of (Enos
et al, 2007)), presumably because discourse fea-
640
TOTAL WORDS total number of words
PAST TENSE uses of past tense auxiliaries was, were, had
METADATE horn, date, bell, survey, speed, form, questionnaire, rushed, study, research
YOU you, you?d, you?ll, your, you?re, yours, you?ve (not counting you know)
WE lets, let?s, our, ours, ourselves, us, we, we?d, we?ll, we?re, we?ve
I I?d, I?ll, I?m, I?ve, me, mine, my, myself (not counting I mean)
ASSENT yeah, okay, cool, yes, awesome, absolutely, agree
SWEAR hell, sucks, damn, crap, shit, screw, heck, fuck*
INSIGHT think*/thought, feel*/felt, find/found, understand*, figure*, idea*, imagine, wonder
ANGER hate/hated, hell, ridiculous*, stupid, kill*, screwed, blame, sucks, mad, bother, shit
NEGEMOTION bad, weird, hate, crazy, problem*, difficult, tough, awkward, boring, wrong, sad, worry,
SEXUAL love*, passion*, loves, virgin, sex, screw
INGEST food, eat*, water, bar/bars, drink*, cook*, dinner, coffee, wine, beer, restaurant, lunch, dish
Table 2: Lexical features. Each feature value is a total count of the words in that class for each conversation side;
asterisks indicate that suffixed forms were included (e.g., love, loves, loving). All except the first three are from LIWC
(Pennebaker et al, 2007) (modified slightly, for example by removing you know and I mean). The last five classes
include more words in addition to those shown.
tures are expensive to hand-label and hard to auto-
matically extract. We chose a suggestive discourse
features that we felt might still be automatically ex-
tracted.
Four particular dialog acts were chosen as shown
in Table 3. Backchannels (or continuers) and ap-
preciations (a continuer expressing positive affect)
were coded by hand-built regular expressions. The
regular expressions were based on analysis of the
backchannels and appreciations in the hand-labeled
Switchboard corpus of dialog acts (Jurafsky et al,
1997). Questions were coded simply by the pres-
ence of question marks.
Finally, repair questions (also called NTRIs; next
turn repair indicators) are turns in which a speaker
signals lack of hearing or understanding (Schegloff
et al, 1977). To detect these, we used a simple
heuristic: the presence of ?Excuse me? or ?Wait?, as
in the following example:
FEMALE: Okay. Are you excited about that?
MALE: Excuse me?
A collaborative completion is a turn where a
speaker completes the utterance begun by the alter
(Lerner, 1991; Lerner, 1996). Our heuristic for iden-
tifying collaborative completions was to select sen-
tences for which the first word of the speaker was
extremely predictable from the last two words of the
previous speaker. We trained a word trigram model1
1interpolated, with Good Turing smoothing, trained on the
Treebank 3 Switchboard transcripts after stripping punctuation.
and used it to compute the probability p of the first
word of a speaker?s turn given the last two words
of the interlocutor?s turn. We arbitrarily chose the
threshold .01, labeling all turns for which p > .01 as
collaborative completions and used the total number
of collaborative completions in a conversation side
as our variable. This simple heuristic was errorful,
but did tend to find completions beginning with and
or or (1 below) and wh-questions followed by an NP
or PP phrase that is grammatically coherent with the
end of the question (2 and 3):
(1) FEMALE: The driving range.
(1) MALE: And the tennis court, too.
(2) MALE: What year did you graduate?
(2) FEMALE: From high school?
(3) FEMALE: What department are you in?
(3) MALE: The business school.
We also marked aspects of the preference struc-
ture of language. A dispreferred action is one in
which a speaker avoids the face-threat to the inter-
locutor that would be caused by, e.g., refusing a
request or not answering a question, by using spe-
cific strategies such as the use of well, hesitations, or
restarts (Schegloff et al, 1977; Pomerantz, 1984).
Finally, we included the number of instances of
laughter for the side, as well as the total number of
turns a speaker took.
4.4 Disfluency Features
A second group of discourse features relating to re-
pair, disfluency, and speaker overlap are summarized
641
BACKCHANNELS number of backchannel utterances in side (Uh-huh., Yeah., Right., Oh, okay.)
APPRECIATIONS number of appreciations in side (Wow, That?s true, Oh, great)
QUESTIONS number of questions in side
NTRI repair question (Next Turn Repair Indicator) (Wait, Excuse me)
COMPLETION (an approximation to) utterances that were ?collaborative completions?
DISPREFERRED (an approximation to) dispreferred responses, beginning with discourse marker well
LAUGH number of instances of laughter in side
TURNS total number of turns in side
Table 3: Dialog act/adjacency pair features.
in Table 4. Filled pauses (um, uh) were coded by
UH/UM total number of filled pauses (uh or
um) in conversation side
RESTART total number of disfluent restarts in
conversation side
OVERLAP number of turns in side which the two
speakers overlapped
Table 4: Disfluency features
regular expressions (the transcribers had been in-
structed to transcribe all filled pauses). Restarts are
a type of repair in which speakers begin a phrase,
break off, and then restart the syntactic phrase. The
following example shows a restart; the speaker starts
a sentence Uh, I and then restarts, There?s a group...:
Uh, I?there?s a group of us that came in?
Overlaps are cases in which both speakers were
talking at the same time, and were marked by the
transcribers in the transcripts:
MALE: But-and also obviously?
FEMALE: It sounds bigger.
MALE: ?people in the CS school are not
quite as social in general as other?
5 Classifier Training
Before performing the classification task, we prepro-
cessed the data in two ways. First, we standardized
all the variables to have zero mean and unit variance.
We did this to avoid imposing a prior on any of the
features based on their numerical values.2 Second,
2Consider a feature A with mean 100 and a feature B with
mean .1 where A and B are correlated with the output. Since
regularization favors small weights there is a bias to put weight
on feature A because intuitively the weight on feature B would
we removed features correlated greater than .7. One
goal of removing correlated features was to remove
as much colinearity as possible from the regression
so that the regression weights could be ranked for
their importance in the classification. In addition,
we hoped to improve classification because a large
number of features require more training examples
(Ng, 2004). For example for male flirt we removed
f0 range (highly correlated with f0 max), f0 min sd
(highly correlated with f0 min), and Swear (highly
correlated with Anger).
For each classification task due to the small
amounts of data we performed k-fold cross vali-
dation to learn and evaluate our models. We used
a variant of k-fold cross validation with five folds
where three folds are used for training, one fold is
used for validation, and one fold is used as a test set.
This test fold is not used in any training step. This
yields a datasplit of 60% for training, 20% for val-
idation, and 20% for testing, or 120 training exam-
ples, 40 validation examples, and 40 test examples.
To ensure that we were not learning something spe-
cific to our data split, we randomized our data order-
ing and repeated the k-fold cross validation variant
25 times.
We used regularized logistic regression for clas-
sification. Recall that in logistic regression we train
a vector of feature weights ? ? Rn so as to make
the following classification of some output variable
y for an input observation x:3
p(y|x; ?) = 11 + exp(??Tx) (1)
In regularized logistic regression we find the
need to be 1000 times larger to carry the same effect. This ar-
gument holds similarly for the reduction to unit variance.
3Where n is the number of features plus 1 for the intercept.
642
weights ? which maximize the following optimiza-
tion problem:
argmax
?
?
i
log p(yi|xi; ?)? ? ?R(?) (2)
R(?) is a regularization term used to penalize
large weights. We chose R(?), the regularization
function, to be the L1 norm of ?. That is, R(?) =
||?||1 =?ni=1 |?i|.
In our case, given the training set Strain, test set
Stest, and validation set Sval, we trained the weights
? as follows:
argmax
?
accuracy(??, Sval) (3)
where for a given sparsity parameter ?
?? = argmax
?
?
i
log p(yi|xi; ?)? ? ?R(?) (4)
We chose L1-regularization because the number of
training examples to learn well grows logarithmi-
cally with the number of input variables (Ng, 2004),
and to achieve a sparse activation of our features
to find only the most salient explanatory variables.
This choice of regularization was made to avoid the
problems that often plague supervised learning in
situations with large number of features but only a
small number of examples. The search space over
the sparsity parameter ? is bounded around an ex-
pected sparsity to prevent overfitting.
Finally, to evaluate our model on the learned ?
and ?? we used the features X of the test set Stest to
compute the predicted outputs Y using the logistic
regression model. Accuracy is simply computed as
the percent of correct predictions.
To avoid any data ordering bias, we calculated
a ?? for each randomized run. The output of the
runs was a vector of weights for each feature. We
kept any feature if the median of its weight vector
was nonzero.4 A sample boxplot for the highest
weighted ego features for predicting male flirt can
be found in Figure 1.
4We also performed a t-test to find salient feature values
significantly different than zero; the non-zero median method
turned out to be a more conservative measure in practice (intu-
itively, because L1 normed regression pushes weights to 0).
-1   -0.8  -0.6  -0.4  -0.2    0    0.2    0.4   0.6   0.8    1
question
f0 mean std
you
rate
intensity min
backchannel
appreciation
repair quest
intensity max
laugh
I
Figure 1: An illustrative boxplot for flirtation in men
showing the 10 most significant features and one not
significant (?I?). Shown are median values (central red
line), first quartile, third quartile, outliers (red X?s) and
interquartile range (filled box).
6 Results
Results for the 6 binary classifiers are presented in
Table 5.
Awk Flirt Friendly
M F M F M F
Speaker 63% 51% 67% 60% 72% 68%
+other 64% 64% 71% 60% 73% 75%
Table 5: Accuracy of binary classification of each con-
versation side, where chance is 50%. The first row uses
features only from the single speaker; the second adds all
the features from the interlocutor as well. These accu-
racy results were aggregated from 25 randomized runs of
5-fold cross validation.
The first row shows results using features ex-
tracted from the speaker being labeled. Here, all
conversational styles are easiest to detect in men.
The second row of table 5 shows the accuracy
when using features from both speakers. Not sur-
prisingly, adding information about the interlocutor
tends to improve classification, and especially for
women, suggesting that male speaking has greater
sway over perceptions of conversational style. We
discuss below the role of these features.
We first considered the features that helped clas-
sification when considering only the ego (i.e., the re-
sults in the first row of Table 5). Table 6 shows fea-
ture weights for the features (features were normed
so weights are comparable), and is summarized in
the following paragraphs:
? Men who are labeled as friendly use you, col-
643
MALE FRIENDLY MALE FLIRT
backchannel -0.737 question 0.376
you 0.631 f0 mean sd 0.288
intensity min sd 0.552 you 0.214
f0 sd sd -0.446 rate 0.190
intensity min -0.445 intensity min -0.163
completion 0.337 backchannel -0.142
time -0.270 appreciation -0.136
Insight -0.249 repair question 0.128
f0 min -0.226 intensity max -0.121
intensity max -0.221 laugh 0.107
overlap 0.213 time -0.092
laugh 0.192 overlap -0.090
turn dur -0.059 f0 min 0.089
Sexual 0.059 Sexual 0.082
appreciation -0.054 Negemo 0.075
Anger -0.051 metadate -0.041
FEMALE FRIENDLY FEMALE FLIRT
intensity min sd 0.420 f0 max 0.475
intensity max sd -0.367 rate 0.346
completion 0.276 intensity min sd 0.269
repair question 0.255 f0 mean sd 0.21
appreciation 0.253 Swear 0.156
f0 max 0.233 question -0.153
Swear -0.194 Assent -0.127
wordcount 0.165 f0 min -0.111
restart 0.172 intensity max 0.092
uh 0.241 I 0.073
I 0.111 metadate -0.071
past -0.060 wordcount 0.065
laugh 0.048 laugh 0.054
Negemotion -0.021 restart 0.046
intensity min -0.02 overlap -0.036
Ingest -0.017 f0 sd sd -0.025
Assent 0.0087 Ingest -0.024
f0 max sd 0.0089
MALE AWK
restart 0.502 completion -0.141
f0 sd sd 0.371 intensity max -0.135
appreciation -0.354 f0 mean sd -0.091
turns -0.292 Ingest -0.079
uh 0.270 Anger 0.075
you -0.210 repair question -0.067
overlap -0.190 Insight -0.056
past -0.175 rate 0.049
intensity min sd -0.173
Table 6: Feature weights (median weights of the random-
ized runs) for the non-zero predictors for each classifier.
Since our accuracy for detecting awkwardness in women
based solely on ego features is so close to chance, we
didn?t analyze the awkwardness features for women here.
laborative completions, laugh, overlap, but don?t
backchannel or use appreciations. Their utterances
are shorter (in seconds and words) and they are qui-
eter and their (minimum) pitch is lower and some-
what less variable.
? Women labeled as friendly have more collab-
orative completions, repair questions, laughter, and
appreciations. They use more words overall, and use
I more often. They are more disfluent (both restarts
and uh) but less likely to swear. Prosodically their f0
is higher, and there seems to be some pattern involv-
ing quiet speech; more variation in intensity mini-
mum than intensity max.
? Men who are labeled as flirting ask more ques-
tions, including repair questions, and use you. They
don?t use backchannels or appreciations, or overlap
as much. They laugh more, and use more sexual and
negative emotional words. Prosodically they speak
faster, with higher and more variable pitch, but qui-
eter (lower intensity max).
? The strongest features for women who are la-
beled as flirting are prosodic; they speak faster and
louder with higher and more variable pitch. They
also use more words in general, swear more, don?t
ask questions or use Assent, use more I, laugh more,
and are somewhat more disfluent (restarts).
?Men who are labeled as awkward are more dis-
fluent, with increased restarts and filled pauses (uh
and um). They are also not ?collaborative? conversa-
tionalists; they don?t use appreciations, repair ques-
tions, collaborative completions, past-tense, or you,
take fewer turns overall, and don?t overlap. Prosod-
ically the awkward labels are hard to characterize;
there is both an increase in pitch variation (f0 sd sd)
and a decrease (f0 mean sd). They don?t seem to get
quite as loud (intensity max).
The previous analysis showed what features of the
ego help in classification. We next asked about fea-
tures of the alter, based on the results using both
ego and alter features in the second row of Table 5.
Here we are asking about the linguistic behaviors of
a speaker who describes the interlocutor as flirting,
friendly, or awkward.
While we don?t show these values in a table, we
offer here an overview of their tendencies. For
example for women who labeled their male in-
terlocutors as friendly, the women got much qui-
eter, used ?well? much more, laughed, asked more
644
repair questions, used collaborative completions,
and backchanneled more. When a man labeled a
woman as friendly, he used an expanded intensity
range (quieter intensity min, louder intensity max).
laughed more, used more sexual terms, used less
negative emotional terms, and overlapped more.
When women labeled their male interlocutor as
flirting, the women used many more repair ques-
tions, laughed more, and got quieter (lower intensity
min). By contrast, when a man said his female inter-
locutor was flirting, he used more Insight and Anger
words, and raised his pitch.
When women labeled their male interlocutor as
awkward, the women asked a lot of questions, used
well, were disfluent (restarts), had a diminished
pitch range, and didn?t use I. In listening to some
of these conversations, it was clear that the conver-
sation lagged repeatedly, and the women used ques-
tions at these points to restart the conversations.
7 Discussion
The results presented here should be regarded with
some caution. The sample is not a random sample of
English speakers or American adults, and speed dat-
ing is not a natural context for expressing every con-
versational style. Therefore, a wider array of studies
across populations and genres would be required be-
fore a more general theory of conversational styles is
established.
On the other hand, the presented results may
under-reflect the relations being captured. The qual-
ity of recordings and coarse granularity (1 second)
of the time-stamps likely cloud the relations, and as
the data is cleaned and improved, we expect the as-
sociations to only grow stronger.
Caveats aside, we believe the evidence indicates
that the perception of several types of conversational
style have relatively clear signals across genders, but
with some additional gender contextualization.
Both genders convey flirtation by laughing more,
speaking faster, and using higher and more variable
pitch. Both genders convey friendliness by laughing
more, and using collaborative completions.
However, we do find gender differences; men asl
more questions when (labeled as) flirting, women
ask fewer. Men labeled as flirting are softer, but
women labeled as flirting are louder. Women flirt-
ing swear more, while men are more likely to use
sexual vocabulary. Gender differences exist as well
for the other variables. Men labeled as friendly use
you while women labeled as friendly use I. Friendly
women are very disfluent; friendly men are not.
While the features for friendly and flirtatious
speech overlap, there are clear differences. Men
speaker faster and with higher f0 (min) in flirtatious
speech, but not faster and with lower f0 (min) in
friendly speech. For men, flirtatious speech involves
more questions and repair questions, while friendly
speech does not. For women, friendly speech is
more disfluent than flirtatious speech, and has more
collaborative style (completions, repair questions,
appreciations).
We also seem to see a model of collaborative con-
versational style (probably related to the collabo-
rative floor of Edelsky (1981) and Coates (1996)),
cued by the use of more collaborative completions,
repair questions and other questions, you, and laugh-
ter. These collaborative techniques were used by
both women and men who were labeled as friendly,
and occurred less with men labeled as awkward.
Women themselves displayed more of this collab-
orative conversational style when they labeled the
men as friendly. For women only, collaborative style
included appreciations; while for men only, collabo-
rative style included overlaps.
In addition to these implications for social sci-
ence, our work has implications for the extraction of
meaning in general. A key focus of our work was on
ways to extract useful dialog act and disfluency fea-
tures (repair questions, backchannels, appreciations,
restarts, dispreferreds) with very shallow methods.
These features were indeed extractable and proved
to be useful features in classification.
We are currently extending these results to predict
date outcomes including ?liking?, extending work
such as Madan and Pentland (2006).
Acknowledgments
Thanks to three anonymous reviewers, Sonal Nalkur and
Tanzeem Choudhury for assistance and advice on data
collection, Sandy Pentland for a helpful discussion about
feature extraction, and to Google for gift funding.
645
References
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-
cke. 2002. Prosody-Based Automatic Detection of
Annoyance and Frustration in Human-Computer Dia-
log. In INTERSPEECH-02.
P. Boersma and D. Weenink. 2005. Praat: doing pho-
netics by computer (version 4.3.14). [Computer pro-
gram]. Retrieved May 26, 2005, from http://www.
praat.org/.
S. Brave, C. Nass, and K. Hutchinson. 2005. Comput-
ers that care: Investigating the effects of orientation
of emotion exhibited by an embodied conversational
agent. International Journal of Human-Computer
Studies, 62(2):161?178.
S. E. Brennan and M. F. Schober. 2001. How listen-
ers compensate for disfluencies in spontaneous speech.
Journal of Memory and Language, 44:274?296.
S. E. Brennan and M. Williams. 1995. The feeling of
another?s knowing: Prosody and filled pauses as cues
to listeners about the metacognitive states of speakers.
Journal of Memory and Language, 34:383?398.
J. Coates. 1996. Women Talk. Blackwell.
M. A. Cohn, M. R. Mehl, and J. W. Pennebaker. 2004.
Linguistic markers of psychological change surround-
ing September 11, 2001. Psychological Science,
15:687?693.
C. Edelsky. 1981. Who?s got the floor? Language in
Society, 10:383?421.
F. Enos, E. Shriberg, M. Graciarena, J. Hirschberg, and
A. Stolcke. 2007. Detecting Deception Using Critical
Segments. In INTERSPEECH-07.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL Labeling Project Coder?s Man-
ual, Draft 13. Technical Report 97-02, University of
Colorado Institute of Cognitive Science.
D. Jurafsky, E. Shriberg, B. Fox, and T. Curl. 1998. Lex-
ical, prosodic, and syntactic cues for dialog acts. In
Proceedings, COLING-ACL Workshop on Discourse
Relations and Discourse Markers, pages 114?120.
D. Jurafsky. 2001. Pragmatics and computational lin-
guistics. In L. R. Horn and G. Ward, editors, Hand-
book of Pragmatics. Blackwell.
C. M. Lee and S. S. Narayanan. 2002. Combining acous-
tic and language information for emotion recognition.
In ICSLP-02, pages 873?876, Denver, CO.
G. H. Lerner. 1991. On the syntax of sentences-in-
progress. Language in Society, 20(3):441?458.
G. H. Lerner. 1996. On the ?semi-permeable? character
of grammatical units in conversation: Conditional en-
try into the turn space of another speaker. In E. Ochs,
E. A. Schegloff, and S. A. Thompson, editors, Interac-
tion and Grammar, pages 238?276. Cambridge Uni-
versity Press.
J. Liscombe, J. Venditti, and J. Hirschberg. 2003. Clas-
sifying Subject Ratings of Emotional Speech Using
Acoustic Features. In INTERSPEECH-03.
A. Madan and A. Pentland. 2006. Vibefones: Socially
aware mobile phones. In Tenth IEEE International
Symposium on Wearable Computers.
A. Madan, R. Caneel, and A. Pentland. 2005. Voices
of attraction. Presented at Augmented Cognition, HCI
2005, Las Vegas.
F. Mairesse and M. Walker. 2008. Trainable generation
of big-five personality styles through data-driven pa-
rameter estimation. In ACL-08, Columbus.
F. Mairesse, M. Walker, M. Mehl, and R. Moore. 2007.
Using linguistic cues for the automatic recognition of
personality in conversation and text. Journal of Artifi-
cial Intelligence Research (JAIR), 30:457?500.
C. Nass and S. Brave. 2005. Wired for speech: How
voice activates and advances the human-computer re-
lationship. MIT Press, Cambridge, MA.
M. L. Newman, J. W. Pennebaker, D. S. Berry, and J. M.
Richards. 2003. Lying words: Predicting deception
from linguistic style. Personality and Social Psychol-
ogy Bulletin, 29:665?675.
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regulariza-
tion, and rotational invariance. In ICML 2004.
J. W. Pennebaker and T. C. Lay. 2002. Language use and
personality during crises: Analyses of Mayor Rudolph
Giuliani?s press conferences. Journal of Research in
Personality, 36:271?282.
J. W. Pennebaker, R. Booth, and M. Francis. 2007. Lin-
guistic inquiry and word count: LIWC2007 operator?s
manual. Technical report, University of Texas.
A. Pentland. 2005. Socially aware computation and
communication. Computer, pages 63?70.
A. M. Pomerantz. 1984. Agreeing and disagreeing with
assessment: Some features of preferred/dispreferred
turn shapes. In J. M. Atkinson and J. Heritage, edi-
tors, Structure of Social Action: Studies in Conversa-
tion Analysis. Cambridge University Press.
A. Rosenberg and J. Hirschberg. 2005. Acous-
tic/prosodic and lexical correlates of charismatic
speech. In EUROSPEECH-05, pages 513?516, Lis-
bon, Portugal.
S. S. Rude, E. M. Gortner, and J. W. Pennebaker. 2004.
Language use of depressed and depression-vulnerable
college students. Cognition and Emotion, 18:1121?
1133.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696?735.
E. A. Schegloff, G. Jefferson, and H. Sacks. 1977. The
preference for self-correction in the organization of re-
pair in conversation. Language, 53:361?382.
646
Proceedings of the 43rd Annual Meeting of the ACL, pages 581?588,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling Using Different Syntactic Views?
Sameer Pradhan, Wayne Ward,
Kadri Hacioglu, James H. Martin
Center for Spoken Language Research,
University of Colorado,
Boulder, CO 80303
{spradhan,whw,hacioglu,martin}@cslr.colorado.edu
Daniel Jurafsky
Department of Linguistics,
Stanford University,
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
Semantic role labeling is the process of
annotating the predicate-argument struc-
ture in text with semantic labels. In this
paper we present a state-of-the-art base-
line semantic role labeling system based
on Support Vector Machine classifiers.
We show improvements on this system
by: i) adding new features including fea-
tures extracted from dependency parses,
ii) performing feature selection and cali-
bration and iii) combining parses obtained
from semantic parsers trained using dif-
ferent syntactic views. Error analysis of
the baseline system showed that approx-
imately half of the argument identifica-
tion errors resulted from parse errors in
which there was no syntactic constituent
that aligned with the correct argument. In
order to address this problem, we com-
bined semantic parses from a Minipar syn-
tactic parse and from a chunked syntac-
tic representation with our original base-
line system which was based on Charniak
parses. All of the reported techniques re-
sulted in performance improvements.
1 Introduction
Semantic Role Labeling is the process of annotat-
ing the predicate-argument structure in text with se-
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grants IS-9978025 and ITR/HCI 0086132
mantic labels (Gildea and Jurafsky, 2000; Gildea
and Jurafsky, 2002; Gildea and Palmer, 2002; Sur-
deanu et al, 2003; Hacioglu and Ward, 2003; Chen
and Rambow, 2003; Gildea and Hockenmaier, 2003;
Pradhan et al, 2004; Hacioglu, 2004). The architec-
ture underlying all of these systems introduces two
distinct sub-problems: the identification of syntactic
constituents that are semantic roles for a given pred-
icate, and the labeling of the those constituents with
the correct semantic role.
A detailed error analysis of our baseline system
indicates that the identification problem poses a sig-
nificant bottleneck to improving overall system per-
formance. The baseline system?s accuracy on the
task of labeling nodes known to represent semantic
arguments is 90%. On the other hand, the system?s
performance on the identification task is quite a bit
lower, achieving only 80% recall with 86% preci-
sion. There are two sources of these identification
errors: i) failures by the system to identify all and
only those constituents that correspond to semantic
roles, when those constituents are present in the syn-
tactic analysis, and ii) failures by the syntactic ana-
lyzer to provide the constituents that align with cor-
rect arguments. The work we present here is tailored
to address these two sources of error in the identifi-
cation problem.
The remainder of this paper is organized as fol-
lows. We first describe a baseline system based on
the best published techniques. We then report on
two sets of experiments using techniques that im-
prove performance on the problem of finding argu-
ments when they are present in the syntactic analy-
sis. In the first set of experiments we explore new
581
features, including features extracted from a parser
that provides a different syntactic view ? a Combi-
natory Categorial Grammar (CCG) parser (Hocken-
maier and Steedman, 2002). In the second set of
experiments, we explore approaches to identify opti-
mal subsets of features for each argument class, and
to calibrate the classifier probabilities.
We then report on experiments that address the
problem of arguments missing from a given syn-
tactic analysis. We investigate ways to combine
hypotheses generated from semantic role taggers
trained using different syntactic views ? one trained
using the Charniak parser (Charniak, 2000), another
on a rule-based dependency parser ? Minipar (Lin,
1998), and a third based on a flat, shallow syntactic
chunk representation (Hacioglu, 2004a). We show
that these three views complement each other to im-
prove performance.
2 Baseline System
For our experiments, we use Feb 2004 release of
PropBank1 (Kingsbury and Palmer, 2002; Palmer
et al, 2005), a corpus in which predicate argument
relations are marked for verbs in the Wall Street
Journal (WSJ) part of the Penn TreeBank (Marcus
et al, 1994). PropBank was constructed by as-
signing semantic arguments to constituents of hand-
corrected TreeBank parses. Arguments of a verb
are labeled ARG0 to ARG5, where ARG0 is the
PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc.
In addition to these CORE ARGUMENTS, additional
ADJUNCTIVE ARGUMENTS, referred to as ARGMs
are also marked. Some examples are ARGM-LOC,
for locatives; ARGM-TMP, for temporals; ARGM-
MNR, for manner, etc. Figure 1 shows a syntax tree
along with the argument labels for an example ex-
tracted from PropBank. We use Sections 02-21 for
training, Section 00 for development and Section 23
for testing.
We formulate the semantic labeling problem as
a multi-class classification problem using Support
Vector Machine (SVM) classifier (Hacioglu et al,
2003; Pradhan et al, 2003; Pradhan et al, 2004)
TinySVM2 along with YamCha3 (Kudo and Mat-
1http://www.cis.upenn.edu/?ace/
2http://chasen.org/?taku/software/TinySVM/
3http://chasen.org/?taku/software/yamcha/
S
hhhh
((((
NP
hhhh
((((
The acquisition
ARG1
VP
```
   
VBD
was
NULL
VP
XXX
VBN
completed
predicate
PP
```
   
in September
ARGM?TMP
[ARG1 The acquisition] was [predicate completed] [ARGM?TMP in September].
Figure 1: Syntax tree for a sentence illustrating the
PropBank tags.
sumoto, 2000; Kudo and Matsumoto, 2001) are used
to implement the system. Using what is known as
the ONE VS ALL classification strategy, n binary
classifiers are trained, where n is number of seman-
tic classes including a NULL class.
The baseline feature set is a combination of fea-
tures introduced by Gildea and Jurafsky (2002) and
ones proposed in Pradhan et al, (2004), Surdeanu et
al., (2003) and the syntactic-frame feature proposed
in (Xue and Palmer, 2004). Table 1 lists the features
used.
PREDICATE LEMMA
PATH: Path from the constituent to the predicate in the parse tree.
POSITION: Whether the constituent is before or after the predicate.
VOICE
PREDICATE SUB-CATEGORIZATION
PREDICATE CLUSTER
HEAD WORD: Head word of the constituent.
HEAD WORD POS: POS of the head word
NAMED ENTITIES IN CONSTITUENTS: 7 named entities as 7 binary features.
PARTIAL PATH: Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
VERB SENSE INFORMATION: Oracle verb sense information from PropBank
HEAD WORD OF PP: Head of PP replaced by head word of NP inside it,
and PP replaced by PP-preposition
FIRST AND LAST WORD/POS IN CONSTITUENT
ORDINAL CONSTITUENT POSITION
CONSTITUENT TREE DISTANCE
CONSTITUENT RELATIVE FEATURES: Nine features representing
the phrase type, head word and head word part of speech of the
parent, and left and right siblings of the constituent.
TEMPORAL CUE WORDS
DYNAMIC CLASS CONTEXT
SYNTACTIC FRAME
CONTENT WORD FEATURES: Content word, its POS and named entities
in the content word
Table 1: Features used in the Baseline system
As described in (Pradhan et al, 2004), we post-
process the n-best hypotheses using a trigram lan-
guage model of the argument sequence.
We analyze the performance on three tasks:
? Argument Identification ? This is the pro-
cess of identifying the parsed constituents in
the sentence that represent semantic arguments
of a given predicate.
582
? Argument Classification ? Given constituents
known to represent arguments of a predicate,
assign the appropriate argument labels to them.
? Argument Identification and Classification ?
A combination of the above two tasks.
ALL ARGs Task P R F1 A
(%) (%) (%)
HAND Id. 96.2 95.8 96.0
Classification - - - 93.0
Id. + Classification 89.9 89.0 89.4
AUTOMATIC Id. 86.8 80.0 83.3
Classification - - - 90.1
Id. + Classification 80.9 76.8 78.8
Table 2: Baseline system performance on all tasks
using hand-corrected parses and automatic parses on
PropBank data.
Table 2 shows the performance of the system us-
ing the hand corrected, TreeBank parses (HAND)
and using parses produced by a Charniak parser
(AUTOMATIC). Precision (P), Recall (R) and F1
scores are given for the identification and combined
tasks, and Classification Accuracy (A) for the clas-
sification task.
Classification performance using Charniak parses
is about 3% absolute worse than when using Tree-
Bank parses. On the other hand, argument identifi-
cation performance using Charniak parses is about
12.7% absolute worse. Half of these errors ? about
7% are due to missing constituents, and the other
half ? about 6% are due to mis-classifications.
Motivated by this severe degradation in argument
identification performance for automatic parses, we
examined a number of techniques for improving
argument identification. We made a number of
changes to the system which resulted in improved
performance. The changes fell into three categories:
i) new features, ii) feature selection and calibration,
and iii) combining parses from different syntactic
representations.
3 Additional Features
3.1 CCG Parse Features
While the Path feature has been identified to be very
important for the argument identification task, it is
one of the most sparse features and may be diffi-
cult to train or generalize (Pradhan et al, 2004; Xue
and Palmer, 2004). A dependency grammar should
generate shorter paths from the predicate to depen-
dent words in the sentence, and could be a more
robust complement to the phrase structure grammar
paths extracted from the Charniak parse tree. Gildea
and Hockenmaier (2003) report that using features
extracted from a Combinatory Categorial Grammar
(CCG) representation improves semantic labeling
performance on core arguments. We evaluated fea-
tures from a CCG parser combined with our baseline
feature set. We used three features that were intro-
duced by Gildea and Hockenmaier (2003):
? Phrase type ? This is the category of the max-
imal projection between the two words ? the
predicate and the dependent word.
? Categorial Path ? This is a feature formed by
concatenating the following three values: i) cat-
egory to which the dependent word belongs, ii)
the direction of dependence and iii) the slot in
the category filled by the dependent word.
? Tree Path ? This is the categorial analogue of
the path feature in the Charniak parse based
system, which traces the path from the depen-
dent word to the predicate through the binary
CCG tree.
Parallel to the hand-corrected TreeBank parses,
we also had access to correct CCG parses derived
from the TreeBank (Hockenmaier and Steedman,
2002a). We performed two sets of experiments.
One using the correct CCG parses, and the other us-
ing parses obtained using StatCCG4 parser (Hocken-
maier and Steedman, 2002). We incorporated these
features in the systems based on hand-corrected
TreeBank parses and Charniak parses respectively.
For each constituent in the Charniak parse tree, if
there was a dependency between the head word of
the constituent and the predicate, then the corre-
sponding CCG features for those words were added
to the features for that constituent. Table 3 shows the
performance of the system when these features were
added. The corresponding baseline performances
are mentioned in parentheses.
3.2 Other Features
We added several other features to the system. Po-
sition of the clause node (S, SBAR) seems to be
4Many thanks to Julia Hockenmaier for providing us with
the CCG bank as well as the StatCCG parser.
583
ALL ARGs Task P R F1
(%) (%)
HAND Id. 97.5 (96.2) 96.1 (95.8) 96.8 (96.0)
Id. + Class. 91.8 (89.9) 90.5 (89.0) 91.2 (89.4)
AUTOMATIC Id. 87.1 (86.8) 80.7 (80.0) 83.8 (83.3)
Id. + Class. 81.5 (80.9) 77.2 (76.8) 79.3 (78.8)
Table 3: Performance improvement upon adding
CCG features to the Baseline system.
an important feature in argument identification (Ha-
cioglu et al, 2004) therefore we experimented with
four clause-based path feature variations. We added
the predicate context to capture predicate sense vari-
ations. For some adjunctive arguments, punctuation
plays an important role, so we added some punctu-
ation features. All the new features are shown in
Table 4
CLAUSE-BASED PATH VARIATIONS:
I. Replacing all the nodes in a path other than clause nodes with an ?*?.
For example, the path NP?S?VP?SBAR?NP?VP?VBD
becomes NP?S?*S?*?*?VBD
II. Retaining only the clause nodes in the path, which for the above
example would produce NP?S?S?VBD,
III. Adding a binary feature that indicates whether the constituent
is in the same clause as the predicate,
IV. collapsing the nodes between S nodes which gives NP?S?NP?VP?VBD.
PATH N-GRAMS: This feature decomposes a path into a series of trigrams.
For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes:
NP?S?VP, S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, etc. We
used the first ten trigrams as ten features. Shorter paths were padded
with nulls.
SINGLE CHARACTER PHRASE TAGS: Each phrase category is clustered
to a category defined by the first character of the phrase label.
PREDICATE CONTEXT: Two words and two word POS around the
predicate and including the predicate were added as ten new features.
PUNCTUATION: Punctuation before and after the constituent were
added as two new features.
FEATURE CONTEXT: Features for argument bearing constituents
were added as features to the constituent being classified.
Table 4: Other Features
4 Feature Selection and Calibration
In the baseline system, we used the same set of fea-
tures for all the n binary ONE VS ALL classifiers.
Error analysis showed that some features specifi-
cally suited for one argument class, for example,
core arguments, tend to hurt performance on some
adjunctive arguments. Therefore, we thought that
selecting subsets of features for each argument class
might improve performance. To achieve this, we
performed a simple feature selection procedure. For
each argument, we started with the set of features in-
troduced by (Gildea and Jurafsky, 2002). We pruned
this set by training classifiers after leaving out one
feature at a time and checking its performance on
a development set. We used the ?2 significance
while making pruning decisions. Following that, we
added each of the other features one at a time to the
pruned baseline set of features and selected ones that
showed significantly improved performance. Since
the feature selection experiments were computation-
ally intensive, we performed them using 10k training
examples.
SVMs output distances not probabilities. These
distances may not be comparable across classifiers,
especially if different features are used to train each
binary classifier. In the baseline system, we used the
algorithm described by Platt (Platt, 2000) to convert
the SVM scores into probabilities by fitting to a sig-
moid. When all classifiers used the same set of fea-
tures, fitting all scores to a single sigmoid was found
to give the best performance. Since different fea-
ture sets are now used by the classifiers, we trained
a separate sigmoid for each classifier.
Raw Scores Probabilities
After lattice-rescoring
Uncalibrated Calibrated
(%) (%) (%)
Same Feat. same sigmoid 74.7 74.7 75.4
Selected Feat. diff. sigmoids 75.4 75.1 76.2
Table 5: Performance improvement on selecting fea-
tures per argument and calibrating the probabilities
on 10k training data.
Foster and Stine (2004) show that the pool-
adjacent-violators (PAV) algorithm (Barlow et al,
1972) provides a better method for converting raw
classifier scores to probabilities when Platt?s algo-
rithm fails. The probabilities resulting from either
conversions may not be properly calibrated. So, we
binned the probabilities and trained a warping func-
tion to calibrate them. For each argument classifier,
we used both the methods for converting raw SVM
scores into probabilities and calibrated them using
a development set. Then, we visually inspected
the calibrated plots for each classifier and chose the
method that showed better calibration as the calibra-
tion procedure for that classifier. Plots of the pre-
dicted probabilities versus true probabilities for the
ARGM-TMP VS ALL classifier, before and after cal-
ibration are shown in Figure 2. The performance im-
provement over a classifier that is trained using all
the features for all the classes is shown in Table 5.
Table 6 shows the performance of the system af-
ter adding the CCG features, additional features ex-
584
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Predicted Probability
Tr
ue
 P
ro
ba
bi
lity
Before Calibration
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Predicted Probability
Tr
ue
 P
ro
ba
bi
lity
After Calibration
Figure 2: Plots showing true probabilities versus predicted probabilities before and after calibration on the
test set for ARGM-TMP.
tracted from the Charniak parse tree, and performing
feature selection and calibration. Numbers in paren-
theses are the corresponding baseline performances.
TASK P R F1 A
(%) (%) (%)
Id. 86.9 (86.8) 84.2 (80.0) 85.5 (83.3)
Class. - - - 92.0 (90.1)
Id. + Class. 82.1 (80.9) 77.9 (76.8) 79.9 (78.8)
Table 6: Best system performance on all tasks using
automatically generated syntactic parses.
5 Alternative Syntactic Views
Adding new features can improve performance
when the syntactic representation being used for
classification contains the correct constituents. Ad-
ditional features can?t recover from the situation
where the parse tree being used for classification
doesn?t contain the correct constituent representing
an argument. Such parse errors account for about
7% absolute of the errors (or, about half of 12.7%)
for the Charniak parse based system. To address
these errors, we added two additional parse repre-
sentations: i) Minipar dependency parser, and ii)
chunking parser (Hacioglu et al, 2004). The hope is
that these parsers will produce different errors than
the Charniak parser since they represent different
syntactic views. The Charniak parser is trained on
the Penn TreeBank corpus. Minipar is a rule based
dependency parser. The chunking parser is trained
on PropBank and produces a flat syntactic represen-
tation that is very different from the full parse tree
produced by Charniak. A combination of the three
different parses could produce better results than any
single one.
5.1 Minipar-based Semantic Labeler
Minipar (Lin, 1998; Lin and Pantel, 2001) is a rule-
based dependency parser. It outputs dependencies
between a word called head and another called mod-
ifier. Each word can modify at most one word. The
dependency relationships form a dependency tree.
The set of words under each node in Minipar?s
dependency tree form a contiguous segment in the
original sentence and correspond to the constituent
in a constituent tree. We formulate the semantic la-
beling problem in the same way as in a constituent
structure parse, except we classify the nodes that
represent head words of constituents. A similar for-
mulation using dependency trees derived from Tree-
Bank was reported in Hacioglu (Hacioglu, 2004).
In that experiment, the dependency trees were de-
rived from hand-corrected TreeBank trees using
head word rules. Here, an SVM is trained to as-
sign PropBank argument labels to nodes in Minipar
dependency trees using the following features:
Table 8 shows the performance of the Minipar-
based semantic parser.
Minipar performance on the PropBank corpus is
substantially worse than the Charniak based system.
This is understandable from the fact that Minipar
is not designed to produce constituents that would
exactly match the constituent segmentation used in
TreeBank. In the test set, about 37% of the argu-
585
PREDICATE LEMMA
HEAD WORD: The word representing the node in the dependency tree.
HEAD WORD POS: Part of speech of the head word.
POS PATH: This is the path from the predicate to the head word through
the dependency tree connecting the part of speech of each node in the tree.
DEPENDENCY PATH: Each word that is connected to the head
word has a particular dependency relationship to the word. These
are represented as labels on the arc between the words. This
feature is the dependencies along the path that connects two words.
VOICE
POSITION
Table 7: Features used in the Baseline system using
Minipar parses.
Task P R F1
(%) (%)
Id. 73.5 43.8 54.6
Id. + Classification 66.2 36.7 47.2
Table 8: Baseline system performance on all tasks
using Minipar parses.
ments do not have corresponding constituents that
match its boundaries. In experiments reported by
Hacioglu (Hacioglu, 2004), a mismatch of about
8% was introduced in the transformation from hand-
corrected constituent trees to dependency trees. Us-
ing an errorful automatically generated tree, a still
higher mismatch would be expected. In case of
the CCG parses, as reported by Gildea and Hock-
enmaier (2003), the mismatch was about 23%. A
more realistic way to score the performance is to
score tags assigned to head words of constituents,
rather than considering the exact boundaries of the
constituents as reported by Gildea and Hocken-
maier (2003). The results for this system are shown
in Table 9.
Task P R F1
(%) (%)
CHARNIAK Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
MINIPAR Id. 83.3 61.1 70.5
Id. + Classification 72.9 53.5 61.7
Table 9: Head-word based performance using Char-
niak and Minipar parses.
5.2 Chunk-based Semantic Labeler
Hacioglu has previously described a chunk based se-
mantic labeling method (Hacioglu et al, 2004). This
system uses SVM classifiers to first chunk input text
into flat chunks or base phrases, each labeled with
a syntactic tag. A second SVM is trained to assign
semantic labels to the chunks. The system is trained
on the PropBank training data.
WORDS
PREDICATE LEMMAS
PART OF SPEECH TAGS
BP POSITIONS: The position of a token in a BP using the IOB2
representation (e.g. B-NP, I-NP, O, etc.)
CLAUSE TAGS: The tags that mark token positions in a sentence
with respect to clauses.
NAMED ENTITIES: The IOB tags of named entities.
TOKEN POSITION: The position of the phrase with respect to
the predicate. It has three values as ?before?, ?after? and ?-? (for
the predicate)
PATH: It defines a flat path between the token and the predicate
CLAUSE BRACKET PATTERNS
CLAUSE POSITION: A binary feature that identifies whether the
token is inside or outside the clause containing the predicate
HEADWORD SUFFIXES: suffixes of headwords of length 2, 3 and 4.
DISTANCE: Distance of the token from the predicate as a number
of base phrases, and the distance as the number of VP chunks.
LENGTH: the number of words in a token.
PREDICATE POS TAG: the part of speech category of the predicate
PREDICATE FREQUENCY: Frequent or rare using a threshold of 3.
PREDICATE BP CONTEXT: The chain of BPs centered at the predicate
within a window of size -2/+2.
PREDICATE POS CONTEXT: POS tags of words immediately preceding
and following the predicate.
PREDICATE ARGUMENT FRAMES: Left and right core argument patterns
around the predicate.
NUMBER OF PREDICATES: This is the number of predicates in
the sentence.
Table 10: Features used by chunk based classifier.
Table 10 lists the features used by this classifier.
For each token (base phrase) to be tagged, a set of
features is created from a fixed size context that sur-
rounds each token. In addition to the above features,
it also uses previous semantic tags that have already
been assigned to the tokens contained in the linguis-
tic context. A 5-token sliding window is used for the
context.
P R F1
(%) (%)
Id. and Classification 72.6 66.9 69.6
Table 11: Semantic chunker performance on the
combined task of Id. and classification.
SVMs were trained for begin (B) and inside (I)
classes of all arguments and outside (O) class for a
total of 78 one-vs-all classifiers. Again, TinySVM5
along with YamCha6 (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2001) are used as the SVM
training and test software.
Table 11 presents the system performances on the
PropBank test set for the chunk-based system.
5http://chasen.org/?taku/software/TinySVM/
6http://chasen.org/?taku/software/yamcha/
586
6 Combining Semantic Labelers
We combined the semantic parses as follows: i)
scores for arguments were converted to calibrated
probabilities, and arguments with scores below a
threshold value were deleted. Separate thresholds
were used for each parser. ii) For the remaining ar-
guments, the more probable ones among overlap-
ping ones were selected. In the chunked system,
an argument could consist of a sequence of chunks.
The probability assigned to the begin tag of an ar-
gument was used as the probability of the sequence
of chunks forming an argument. Table 12 shows
the performance improvement after the combina-
tion. Again, numbers in parentheses are respective
baseline performances.
TASK P R F1
(%) (%)
Id. 85.9 (86.8) 88.3 (80.0) 87.1 (83.3)
Id. + Class. 81.3 (80.9) 80.7 (76.8) 81.0 (78.8)
Table 12: Constituent-based best system perfor-
mance on argument identification and argument
identification and classification tasks after combin-
ing all three semantic parses.
The main contribution of combining both the
Minipar based and the Charniak-based parsers was
significantly improved performance on ARG1 in ad-
dition to slight improvements to some other argu-
ments. Table 13 shows the effect on selected argu-
ments on sentences that were altered during the the
combination of Charniak-based and Chunk-based
parses.
Number of Propositions 107
Percentage of perfect props before combination 0.00
Percentage of perfect props after combination 45.95
Before After
P R F1 P R F1
(%) (%) (%) (%)
Overall 94.8 53.4 68.3 80.9 73.8 77.2
ARG0 96.0 85.7 90.5 92.5 89.2 90.9
ARG1 71.4 13.5 22.7 59.4 59.4 59.4
ARG2 100.0 20.0 33.3 50.0 20.0 28.5
ARGM-DIS 100.0 40.0 57.1 100.0 100.0 100.0
Table 13: Performance improvement on parses
changed during pair-wise Charniak and Chunk com-
bination.
A marked increase in number of propositions for
which all the arguments were identified correctly
from 0% to about 46% can be seen. Relatively few
predicates, 107 out of 4500, were affected by this
combination.
To give an idea of what the potential improve-
ments of the combinations could be, we performed
an oracle experiment for a combined system that
tags head words instead of exact constituents as we
did in case of Minipar-based and Charniak-based se-
mantic parser earlier. In case of chunks, first word in
prepositional base phrases was selected as the head
word, and for all other chunks, the last word was se-
lected to be the head word. If the correct argument
was found present in either the Charniak, Minipar or
Chunk hypotheses then that was selected. The re-
sults for this are shown in Table 14. It can be seen
that the head word based performance almost ap-
proaches the constituent based performance reported
on the hand-corrected parses in Table 3 and there
seems to be considerable scope for improvement.
Task P R F1
(%) (%)
C Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
C+M Id. 98.4 90.6 94.3
Id. + Classification 93.1 86.0 89.4
C+CH Id. 98.9 88.8 93.6
Id. + Classification 92.5 83.3 87.7
C+M+CH Id. 99.2 92.5 95.7
Id. + Classification 94.6 88.4 91.5
Table 14: Performance improvement on head word
based scoring after oracle combination. Charniak
(C), Minipar (M) and Chunker (CH).
Table 15 shows the performance improvement in
the actual system for pairwise combination of the
parsers and one using all three.
Task P R F1
(%) (%)
C Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
C+M Id. 91.7 89.9 90.8
Id. + Classification 85.0 83.9 84.5
C+CH Id. 91.5 91.1 91.3
Id. + Classification 84.9 84.3 84.7
C+M+CH Id. 91.5 91.9 91.7
Id. + Classification 85.1 85.5 85.2
Table 15: Performance improvement on head word
based scoring after combination. Charniak (C),
Minipar (M) and Chunker (CH).
587
7 Conclusions
We described a state-of-the-art baseline semantic
role labeling system based on Support Vector Ma-
chine classifiers. Experiments were conducted to
evaluate three types of improvements to the sys-
tem: i) adding new features including features ex-
tracted from a Combinatory Categorial Grammar
parse, ii) performing feature selection and calibra-
tion and iii) combining parses obtained from seman-
tic parsers trained using different syntactic views.
We combined semantic parses from a Minipar syn-
tactic parse and from a chunked syntactic repre-
sentation with our original baseline system which
was based on Charniak parses. The belief was that
semantic parses based on different syntactic views
would make different errors and that the combina-
tion would be complimentary. A simple combina-
tion of these representations did lead to improved
performance.
8 Acknowledgements
This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and
by the NSF via grants IS-9978025 and ITR/HCI
0086132. Computer time was provided by NSF
ARI Grant #CDA-9601817, NSF MRI Grant #CNS-
0420873, NASA AIST grant #NAG2-1646, DOE
SciDAC grant #DE-FG02-04ER63870, NSF spon-
sorship of the National Center for Atmospheric Re-
search, and a grant from the IBM Shared University
Research (SUR) program.
We would like to thank Ralph Weischedel and
Scott Miller of BBN Inc. for letting us use their
named entity tagger ? IdentiFinder; Martha Palmer
for providing us with the PropBank data; Dan Gildea
and Julia Hockenmaier for providing the gold stan-
dard CCG parser information, and all the anony-
mous reviewers for their helpful comments.
References
R. E. Barlow, D. J. Bartholomew, J. M. Bremmer, and H. D. Brunk. 1972. Statis-
tical Inference under Order Restrictions. Wiley, New York.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of
NAACL, pages 132?139, Seattle, Washington.
John Chen and Owen Rambow. 2003. Use of deep linguistics features for
the recognition and labeling of semantic arguments. In Proceedings of the
EMNLP, Sapporo, Japan.
Dean P. Foster and Robert A. Stine. 2004. Variable selection in data mining:
building a predictive model for bankruptcy. Journal of American Statistical
Association, 99, pages 303?313.
Dan Gildea and Julia Hockenmaier. 2003. Identifying semantic roles using com-
binatory categorial grammar. In Proceedings of the EMNLP, Sapporo, Japan.
Daniel Gildea and Daniel Jurafsky. 2000. Automatic labeling of semantic roles.
In Proceedings of ACL, pages 512?520, Hong Kong, October.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity of syntactic parsing for
predicate argument recognition. In Proceedings of ACL, Philadelphia, PA.
Kadri Hacioglu. 2004. Semantic role labeling using dependency trees. In Pro-
ceedings of COLING, Geneva, Switzerland.
Kadri Hacioglu and Wayne Ward. 2003. Target word detection and semantic role
chunking using support vector machines. In Proceedings of HLT/NAACL,
Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Shallow semantic parsing using support vector machines. Technical
Report TR-CSLR-2003-1, Center for Spoken Language Research, Boulder,
Colorado.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Juraf-
sky. 2004. Semantic role labeling by tagging syntactic chunks. In Proceed-
ings of CoNLL-2004, Shared Task ? Semantic Role Labeling.
Kadri Hacioglu. 2004a. A lightweight semantic chunking model based on tag-
ging. In Proceedings of HLT/NAACL, Boston, MA.
Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical
parsing with combinatory grammars. In Proceedings of the ACL, pages 335?
342.
Julia Hockenmaier and Mark Steedman. 2002a. Acquiring compact lexicalized
grammars from a cleaner treebank. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation (LREC-2002), Las Pal-
mas, Canary Islands, Spain.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In
Proceedings of LREC, Las Palmas, Canary Islands, Spain.
Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk
identification. In Proceedings of CoNLL and LLL, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings of the NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question
answering. Natural Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In In Workshop
on the Evaluation of Parsing Systems, Granada, Spain.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann
Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument structure.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic roles. To appear Computational Linguistics.
John Platt. 2000. Probabilities for support vector machines. In A. Smola,
P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large
Margin Classifiers. MIT press, Cambridge, MA.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Semantic role parsing: Adding semantic structure to unstructured text.
In Proceedings of ICDM, Melbourne, Florida.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using support vector machines. In Proceed-
ings of HLT/NAACL, Boston, MA.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Us-
ing predicate-argument structures for information extraction. In Proceedings
of ACL, Sapporo, Japan.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role
labeling. In Proceedings of EMNLP, Barcelona, Spain.
588
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 801?808,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic Taxonomy Induction from Heterogenous Evidence
Rion Snow
Computer Science Department
Stanford University
Stanford, CA 94305
rion@cs.stanford.edu
Daniel Jurafsky
Linguistics Department
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Andrew Y. Ng
Computer Science Department
Stanford University
Stanford, CA 94305
ang@cs.stanford.edu
Abstract
We propose a novel algorithm for inducing seman-
tic taxonomies. Previous algorithms for taxonomy
induction have typically focused on independent
classifiers for discovering new single relationships
based on hand-constructed or automatically discov-
ered textual patterns. By contrast, our algorithm
flexibly incorporates evidence from multiple clas-
sifiers over heterogenous relationships to optimize
the entire structure of the taxonomy, using knowl-
edge of a word?s coordinate terms to help in deter-
mining its hypernyms, and vice versa. We apply our
algorithm on the problem of sense-disambiguated
noun hyponym acquisition, where we combine the
predictions of hypernym and coordinate term clas-
sifiers with the knowledge in a preexisting seman-
tic taxonomy (WordNet 2.1). We add 10, 000 novel
synsets to WordNet 2.1 at 84% precision, a rela-
tive error reduction of 70% over a non-joint algo-
rithm using the same component classifiers. Fi-
nally, we show that a taxonomy built using our al-
gorithm shows a 23% relative F-score improvement
over WordNet 2.1 on an independent testset of hy-
pernym pairs.
1 Introduction
The goal of capturing structured relational knowl-
edge about lexical terms has been the motivating
force underlying many projects in lexical acquisi-
tion, information extraction, and the construction
of semantic taxonomies. Broad-coverage seman-
tic taxonomies such as WordNet (Fellbaum, 1998)
and CYC (Lenat, 1995) have been constructed by
hand at great cost; while a crucial source of knowl-
edge about the relations between words, these tax-
onomies still suffer from sparse coverage.
Many algorithms with the potential for auto-
matically extending lexical resources have been
proposed, including work in lexical acquisition
(Riloff and Shepherd, 1997; Roark and Charniak,
1998) and in discovering instances, named enti-
ties, and alternate glosses (Etzioni et al, 2005;
Pasc?a, 2005). Additionally, a wide variety of
relationship-specific classifiers have been pro-
posed, including pattern-based classifiers for hy-
ponyms (Hearst, 1992), meronyms (Girju, 2003),
synonyms (Lin et al, 2003), a variety of verb re-
lations (Chklovski and Pantel, 2004), and general
purpose analogy relations (Turney et al, 2003).
Such classifiers use hand-written or automatically-
induced patterns like Such NPy as NPx or NPy
like NPx to determine, for example that NPy is a
hyponym of NPx (i.e., NPy IS-A NPx). While
such classifiers have achieved some degree of suc-
cess, they frequently lack the global knowledge
necessary to integrate their predictions into a com-
plex taxonomy with multiple relations.
Past work on semantic taxonomy induction in-
cludes the noun hypernym hierarchy created in
(Caraballo, 2001), the part-whole taxonomies in
(Girju, 2003), and a great deal of recent work de-
scribed in (Buitelaar et al, 2005). Such work has
typically either focused on only inferring small
taxonomies over a single relation, or as in (Cara-
ballo, 2001), has used evidence for multiple rela-
tions independently from one another, by for ex-
ample first focusing strictly on inferring clusters
of coordinate terms, and then by inferring hyper-
nyms over those clusters.
Another major shortfall in previous techniques
for taxonomy induction has been the inability to
handle lexical ambiguity. Previous approaches
have typically sidestepped the issue of polysemy
altogether by making the assumption of only a sin-
gle sense per word, and inferring taxonomies ex-
plicitly over words and not senses. Enforcing a
false monosemy has the downside of making po-
tentially erroneous inferences; for example, col-
lapsing the polysemous term Bush into a single
sense might lead one to infer by transitivity that
a rose bush is a kind of U.S. president.
Our approach simultaneously provides a solu-
tion to the problems of jointly considering evi-
dence about multiple relationships as well as lexi-
cal ambiguity within a single probabilistic frame-
work. The key contribution of this work is to offer
a solution to two crucial problems in taxonomy in-
801
duction and hyponym acquisition: the problem of
combining heterogenous sources of evidence in a
flexible way, and the problem of correctly identi-
fying the appropriate word sense of each new word
added to the taxonomy.1
2 A Probabilistic Framework for
Taxonomy Induction
In section 2.1 we introduce our definitions for tax-
onomies, relations, and the taxonomic constraints
that enforce dependencies between relations; in
section 2.2 we give a probabilistic model for defin-
ing the conditional probability of a set of relational
evidence given a taxonomy; in section 2.3 we for-
mulate a local search algorithm to find the taxon-
omy maximizing this conditional probability; and
in section 2.4 we extend our framework to deal
with lexical ambiguity.
2.1 Taxonomies, Relations, and Taxonomic
Constraints
We define a taxonomy T as a set of pairwise re-
lations R over some domain of objects DT. For
example, the relations in WordNet include hyper-
nymy, holonymy, verb entailment, and many oth-
ers; the objects of WordNet between which these
relations hold are its word senses or synsets. We
define that each relation R ? R is a set of ordered
or unordered pairs of objects (i, j) ? DT; we de-
fine Rij ? T if relationship R holds over objects
(i, j) in T.
Relations for Hyponym Acquisition
For the case of hyponym acquisition, the ob-
jects in our taxonomy are WordNet synsets. In
this paper we focus on two of the many possible
relationships between senses: the hypernym rela-
tion and the coordinate term relation. We treat the
hypernym or ISA relation as atomic; we use the
notation Hnij if a sense j is the n-th ancestor of a
sense i in the hypernym hierarchy. We will sim-
ply use Hij to indicate that j is an ancestor of i
at some unspecified level. Two senses are typi-
cally considered to be ?coordinate terms? or ?tax-
onomic sisters? if they share an immediate parent
in the hypernym hierarchy. We generalize this no-
tion of siblinghood to state that two senses i and
j are (m,n)-cousins if their closest least common
1The taxonomies discussed in this paper are available for
download at http://ai.stanford.edu/?rion/swn.
subsumer (LCS)2 is within exactly m and n links,
respectively.3 We use the notation Cmnij to denote
that i and j are (m,n)-cousins. Thus coordinate
terms are (1, 1)-cousins; technically the hypernym
relation may also be seen as a specific case of this
representation; an immediate parent in the hyper-
nym hierarchy is a (1, 0)-cousin, and the k-th an-
cestor is a (k, 0)-cousin.
Taxonomic Constraints
A semantic taxonomy such as WordNet en-
forces certain taxonomic constraints which disal-
low particular taxonomies T. For example, the
ISA transitivity constraint in WordNet requires
that each synset inherits the hypernyms of its hy-
pernym, and the part-inheritance constraint re-
quires that each synset inherits the meronyms of
its hypernyms.
For the case of hyponym acquisition we enforce
the following two taxonomic constraints on the
hypernym and (m,n)-cousin relations:
1. ISA Transitivity:
Hmij ?Hnjk ? Hm+nik .
2. Definition of (m,n)-cousinhood:
Cmnij ? ?k.k = LCS(i, j) ?Hmik ?Hnjk.
Constraint (1) requires that the each synset inherits
the hypernyms of its direct hypernym; constraint
(2) simply defines the (m,n)-cousin relation in
terms of the atomic hypernym relation.
The addition of any new hypernym relation to a
preexisting taxonomy will usually necessitate the
addition of a set of other novel relations as implied
by the taxonomic constraints. We refer to the full
set of novel relations implied by a new link Rij as
I(Rij); we discuss the efficient computation of the
set of implied links for the purpose of hyponym
acquisition in Section 3.4.
2.2 A Probabilistic Formulation
We propose that the event Rij ? T has some
prior probability P (Rij ? T), and P (Rij ?
2A least common subsumer LCS(i, j) is defined as a
synset that is an ancestor in the hypernym hierarchy of both
i and j which has no child that is also an ancestor of both i
and j. When there is more than one LCS (due to multiple
inheritance), we refer to the closest LCS, i.e.,the LCS that
minimizes the maximum distance to i and j.
3An (m,n)-cousin for m ? 2 corresponds to the English
kinship relation ?(m?1)-th cousin |m?n|-times removed.?
802
T) + P (Rij 6? T) = 1. We define the probability
of the taxonomy as a whole as the joint probability
of its component relations; given a partition of all
possible relations R = {A,B} where A ? T and
B 6? T, we define:
P (T) = P (A ? T, B 6? T).
We assume that we have some set of observed evi-
dence E consisting of observed features over pairs
of objects in some domain DE; we?ll begin with
the assumption that our features are over pairs of
words, and that the objects in the taxonomy also
correspond directly to words.4 Given a set of fea-
tures ERij ? E, we assume we have some model
for inferring P (Rij ? T|ERij), i.e., the posterior
probability of the event Rij ? T given the corre-
sponding evidence ERij for that relation. For exam-
ple, evidence for the hypernym relation EHij might
be the set of all observed lexico-syntactic patterns
containing i and j in all sentences in some corpus.
For simplicity we make the following indepen-
dence assumptions: first, we assume that each
item of observed evidence ERij is independent of
all other observed evidence given the taxonomyT,
i.e., P (E|T) = ?ERij?E P (E
R
ij |T).
Further, we assume that each item of observed
evidence ERij depends on the taxonomy T only by
way of the corresponding relation Rij , i.e.,
P (ERij |T) =
{ P (ERij |Rij ? T) if Rij ? T
P (ERij |Rij 6? T) if Rij 6? T
For example, if our evidence EHij is a set of ob-
served lexico-syntactic patterns indicative of hy-
pernymy between two words i and j, we assume
that whatever dependence the relations in T have
on our observations may be explained entirely by
dependence on the existence or non-existence of
the single hypernym relation H(i, j).
Applying these two independence assumptions
we may express the conditional probability of our
evidence given the taxonomy:
P (E|T) =
?
Rij?T
P (ERij |Rij ? T)
?
?
Rij 6?T
P (ERij |Rij 6? T).
Rewriting the conditional probability in terms
of our estimates of the posterior probabilities
4In section 2.4 we drop this assumption, extending our
model to manage lexical ambiguity.
P (Rij |ERij) using Bayes Rule, we obtain:
P (E|T) =
?
Rij?T
P (Rij ? T|ERij)P (ERij)
P (Rij ? T)
?
?
Rij 6?T
P (Rij 6? T|ERij)P (ERij)
P (Rij 6? T) .
Within our model we define the goal of taxon-
omy induction to be to find the taxonomy T? that
maximizes the conditional probability of our ob-
servations E given the relationships of T, i.e., to
find
T? = argmax
T
P (E|T).
2.3 Local Search Over Taxonomies
We propose a search algorithm for finding T? for
the case of hyponym acquisition. We assume we
begin with some initial (possibly empty) taxon-
omy T. We restrict our consideration of possible
new taxonomies to those created by the single op-
eration ADD-RELATION(Rij ,T), which adds the
single relation Rij to T.
We define the multiplicative change ?T(Rij)
to the conditional probability P (E|T) given the
addition of a single relation Rij :
?T(Rij) = P (E|T?)/P (E|T)
= P (Rij ? T|E
R
ij)P (ERij)
P (Rij 6? T|ERij)P (ERij)
? P (Rij 6? T)P (Rij ? T)
= k
?
? P
(
Rij ? T|ERij
)
1? P
(
Rij ? T|ERij
)
?
? .
Here k is the inverse odds of the prior on the event
Rij ? T; we consider this to be a constant inde-
pendent of i, j, and the taxonomy T.
To enforce the taxonomic constraints in T, for
each application of the ADD-RELATION operator
we must add all new relations in the implied set
I(Rij) not already in T.5 Thus we define the mul-
tiplicative change of the full set of implied rela-
tions as the product over all new relations:
?T(I(Rij)) =
?
R?I(Rij)
?T(R).
5For example, in order to add the new synset
microsoft under the noun synset company#n#1
in WordNet 2.1, we must necessarily add the
new relations H2(microsoft, institution#n#1)
C11(microsoft, dotcom#n#1), and so on.
803
This definition leads to the following best-first
search algorithm for hyponym acquisition, which
at each iteration defines the new taxonomy as the
union of the previous taxonomy T and the set of
novel relations implied by the relation Rij that
maximizes ?T(I(Rij)) and thus maximizes the
conditional probability of the evidence over all
possible single relations:
WHILE max
Rij 6?T
?T(I(Rij)) > 1
T ? T ? I(arg max
Rij 6?T
?T(I(Rij))).
2.4 Extending the Model to Manage Lexical
Ambiguity
Since word senses are not directly observable, if
the objects in the taxonomy are word senses (as in
WordNet), we must extend our model to allow for
a many-to-many mapping (e.g., a word-to-sense
mapping) between DE and DT. For this setting
we assume we know the function senses(i), map-
ping from the word i to all of i?s possible corre-
sponding senses.
We assume that each set of word-pair evidence
ERij we possess is in fact sense-pair evidence ERkl
for a specific pair of senses k0 ? senses(i), l0 ?
senses(j). Further, we assume that a new relation
between two words is probable only between the
correct sense pair, i.e.:
P (Rkl|ERij) = 1{k = k0, l = l0} ? P (Rij |ERij).
When computing the conditional probability of a
specific new relation Rkl ? I(Rab), we assume
that the relevant sense pair k0, l0 is the one which
maximizes the probability of the new relation, i.e.
for k ? senses(i), l ? senses(j),
(k0, l0) = argmaxk,l P (Rkl ? T|E
R
ij).
Our independence assumptions for this exten-
sion need only to be changed slightly; we now as-
sume that the evidence ERij depends on the taxon-
omy T via only a single relation between sense-
pairs Rkl. Using this revised independence as-
sumption the derivation for best-first search over
taxonomies for hyponym acquisition remains un-
changed. One side effect of this revised indepen-
dence assumption is that the addition of the single
?sense-collapsed? relation Rkl in the taxonomy T
will explain the evidence ERij for the relation over
words i and j now that such evidence has been re-
vealed to concern only the specific senses k and l.
3 Extending WordNet
We demonstrate the ability of our model to use
evidence from multiple relations to extend Word-
Net with novel noun hyponyms. While in prin-
ciple we could use any number of relations, for
simplicity we consider two primary sources of ev-
idence: the probability of two words in WordNet
being in a hypernym relation, and the probability
of two words in WordNet being in a coordinate re-
lation.
In sections 3.1 and 3.2 we describe the construc-
tion of our hypernym and coordinate classifiers,
respectively; in section 3.3 we outline the efficient
algorithm we use to perform local search over
hyponym-extended WordNets; and in section 3.4
we give an example of the implicit structure-based
word sense disambiguation performed within our
framework.
3.1 Hyponym Classification
Our classifier for the hypernym relation is derived
from the ?hypernym-only? classifier described in
(Snow et al, 2005). The features used for pre-
dicting the hypernym relationship are obtained by
parsing a large corpus of newswire and encyclo-
pedia text with MINIPAR (Lin, 1998). From the
resulting dependency trees the evidence EHij for
each word pair (i, j) is constructed; the evidence
takes the form of a vector of counts of occurrences
that each labeled syntactic dependency path was
found as the shortest path connecting i and j in
some dependency tree. The labeled training set is
constructed by labeling the collected feature vec-
tors as positive ?known hypernym? or negative
?known non-hypernym? examples using WordNet
2.0; 49,922 feature vectors were labeled as pos-
itive training examples, and 800,828 noun pairs
were labeled as negative training examples. The
model for predicting P (Hij |EHij ) is then trained
using logistic regression, predicting the noun-pair
hypernymy label from WordNet from the feature
vector of lexico-syntactic patterns.
The hypernym classifier described above pre-
dicts the probability of the generalized hypernym-
ancestor relation over words P (Hij |EHij ). For
the purposes of taxonomy induction, we would
prefer an ancestor-distance specific set of clas-
sifiers over senses, i.e., for k ? senses(i), l ?
senses(j), the set of classifiers estimating
{P (H1kl|EHij ), P (H2kl|EHij ), . . . }.
804
One problem that arises from directly assign-
ing the probability P (Hnij |EHij ) ? P (Hij |EHij ) for
all n is the possibility of adding a novel hyponym
to an overly-specific hypernym, which might still
satisfy P (Hnij |EHij ) for a very large n. In or-
der to discourage unnecessary overspecification,
we penalize each probability P (Hkij |EHij ) by a
factor ?k?1 for some ? < 1, and renormalize:
P (Hkij |EHij ) ? ?k?1P (Hij |EHij ). In our experi-
ments we set ? = 0.95.
3.2 (m,n)-cousin Classification
The classifier for learning coordinate terms relies
on the notion of distributional similarity, i.e., the
idea that two words with similar meanings will be
used in similar contexts (Hindle, 1990). We ex-
tend this notion to suggest that words with similar
meanings should be near each other in a seman-
tic taxonomy, and in particular will likely share a
hypernym as a near parent.
Our classifier for (m,n)-cousins is derived
from the algorithm and corpus given in (Ravichan-
dran et al, 2005). In that work an efficient ran-
domized algorithm is derived for computing clus-
ters of similar nouns. We use a set of more than
1000 distinct clusters of English nouns collected
by their algorithm over 70 million webpages6,
with each noun i having a score representing its
cosine similarity to the centroid c of the cluster to
which it belongs, cos(?(i, c)).
We use the cluster scores of noun pairs as input
to our own algorithm for predicting the (m,n)-
cousin relationship between the senses of two
words i and j. If two words i and j appear in
a cluster together, with cluster centroid c, we set
our single coordinate input feature to be the mini-
mum cluster score min(cos(?(i, c)), cos(?(j, c))),
and zero otherwise. For each such noun pair fea-
ture, we construct a labeled training set of (m,n)-
cousin relation labels from WordNet 2.1. We de-
fine a noun pair (i, j) to be a ?known (m,n)-
cousin? if for some senses k ? senses(i), l ?
senses(j), Cmnij ? WordNet; if more than one
such relation exists, we assume the relation with
smallest sum m + n, breaking ties by smallest
absolute difference |m ? n|. We consider all
such labeled relationships from WordNet with 0 ?
m,n ? 7; pairs of words that have no correspond-
ing pair of synsets connected in the hypernym hi-
6As a preprocessing step we hand-edit the clusters to re-
move those containing non-English words, terms related to
adult content, and other webpage-specific clusters.
erarchy, or with min(m,n) > 7, are assigned to
a single class C?. Further, due to the symme-
try of the similarity score, we merge each class
Cmn = Cmn ? Cnm; this implies that the result-
ing classifier will predict, as expected given a sym-
metric input, P (Cmnkl |ECij ) = P (Cnmkl |ECij ).
We find 333,473 noun synset pairs in our train-
ing set with similarity score greater than 0.15. We
next apply softmax regression to learn a classifier
that predicts P (Cmnij |ECij ), predicting the Word-
Net class labels from the single similarity score
derived from the noun pair?s cluster similarity.
3.3 Details of our Implementation
Hyponym acquisition is among the simplest and
most straightforward of the possible applications
of our model; here we show how we efficiently
implement our algorithm for this problem. First,
we identify the set of all the word pairs (i, j) over
which we have hypernym and/or coordinate ev-
idence, and which might represent additions of
a novel hyponym to the WordNet 2.1 taxonomy
(i.e., that has a known noun hypernym and an un-
known hyponym, or has a known noun coordi-
nate term and an unknown coordinate term). This
yields a list of 95,000 single links over threshold
P (Rij) > 0.12.
For each unknown hyponym i we may have
several pieces of evidence; for example, for the
unknown term continental we have 21 relevant
pieces of hypernym evidence, with links to possi-
ble hypernyms {carrier, airline, unit, . . .}; and we
have 5 pieces of coordinate evidence, with links to
possible coordinate terms {airline, american ea-
gle, airbus, . . .}.
For each proposed hypernym or coordinate link
involved with the novel hyponym i, we compute
the set of candidate hypernyms for i; in practice
we consider all senses of the immediate hypernym
j for each potential novel hypernym, and all senses
of the coordinate term k and its first two hypernym
ancestors for each potential coordinate.
In the continental example, from the 26 individ-
ual pieces of evidence over words we construct the
set of 99 unique synsets that we will consider as
possible hypernyms; these include the two senses
of the word airline, the ten senses of the word car-
rier, and so forth.
Next, we iterate through each of the possi-
ble hypernym synsets l under which we might
add the new word i; for each synset l we com-
805
pute the change in taxonomy score resulting from
adding the implied relations I(H1il) required by
the taxonomic constraints of T. Since typically
our set of all evidence involving i will be much
smaller than the set of possible relations in I(H1il),
we may efficiently check whether, for each sense
s ? senses(w), for all words where we have
some evidence ERiw, whether s participates in
some relation with i in the set of implied rela-
tions I(H1il).7 If there is more than one sense
s ? senses(w), we add to I(H1il) the single re-
lationship Ris that maximizes the taxonomy like-
lihood, i.e. argmaxs?senses(w) ?T(Ris).
3.4 Hypernym Sense Disambiguation
A major strength of our model is its ability to cor-
rectly choose the sense of a hypernym to which
to add a novel hyponym, despite collecting ev-
idence over untagged word pairs. In our algo-
rithm word sense disambiguation is an implicit
side-effect of our algorithm; since our algorithm
chooses to add the single link which, with its im-
plied links, yields the most likely taxonomy, and
since each distinct synset in WordNet has a differ-
ent immediate neighborhood of relations, our al-
gorithm simply disambiguates each node based on
its surrounding structural information.
As an example of sense disambiguation in prac-
tice, consider our example of continental. Sup-
pose we are iterating through each of the 99 pos-
sible synsets under which we might add conti-
nental as a hyponym, and we come to the synset
airline#n#2 in WordNet 2.1, i.e. ?a commer-
cial organization serving as a common carrier.?
In this case we will iterate through each piece
of hypernym and coordinate evidence; we find
that the relation H(continental, carrier) is satis-
fied with high probability for the specific synset
carrier#n#5, the grandparent of airline#n#2; thus
the factor ?T(H3(continental, carrier#n#5)) is
included in the factor of the set of implied rela-
tions ?T
(I(H1(continental, airline#n#2))).
Suppose we instead evaluate the first synset
of airline, i.e., airline#n#1, with the gloss ?a
hose that carries air under pressure.? For this
synset none of the other 20 relationships di-
rectly implied by hypernym evidence or the
5 relationships implied by the coordinate ev-
7Checking whether or not Ris ? I(H1il) may be effi-
ciently computed by checking whether s is in the hypernym
ancestors of l or if it shares a least common subsumer with l
within 7 steps.
idence are implied by adding the single link
H1(continental,airline#n#1); thus the resulting
change in the set of implied links given by the cor-
rect ?carrier? sense of airline is much higher than
that of the ?hose? sense. In fact it is the largest of
all the 99 considered hypernym links for continen-
tal; H1(continental, airline#n#2) is link #18,736
added to the taxonomy by our algorithm.
4 Evaluation
In order to evaluate our framework for taxonomy
induction, we have applied hyponym acquisition
to construct several distinct taxonomies, starting
with the base of WordNet 2.1 and only adding
novel noun hyponyms. Further, we have con-
structed taxonomies using a baseline algorithm,
which uses the identical hypernym and coordinate
classifiers used in our joint algorithm, but which
does not combine the evidence of the classifiers.
In section 4.1 we describe our evaluation
methodology; in sections 4.2 and 4.3 we analyze
the fine-grained precision and disambiguation pre-
cision of our algorithm compared to the baseline;
in section 4.4 we compare the coarse-grained pre-
cision of our links (motivated by categories de-
fined by the WordNet supersenses) against the
baseline algorithm and against an ?oracle? for
named entity recognition.
Finally, in section 4.5 we evaluate the tax-
onomies inferred by our algorithm directly against
the WordNet 2.1 taxonomy; we perform this eval-
uation by testing each taxonomy on a set of human
judgments of hypernym and non-hypernym noun
pairs sampled from newswire text.
4.1 Methodology
We evaluate the quality of our acquired hy-
ponyms by direct judgment. In four sep-
arate annotation sessions, two judges labeled
{50,100,100,100} samples uniformly generated
from the first {100,1000,10000,20000} single
links added by our algorithm.
For the direct measure of fine-grained precision,
we simply ask for each link H(X,Y ) added by the
system, is X a Y ? In addition to the fine-grained
precision, we give a coarse-grained evaluation, in-
spired by the idea of supersense-tagging in (Cia-
ramita and Johnson, 2003). The 26 supersenses
used in WordNet 2.1 are listed in Table 1; we label
a hyponym link as correct in the coarse-grained
evaluation if the novel hyponym is placed under
the appropriate supersense. This evaluation task
806
1 Tops 8 communication 15 object 22 relation
2 act 9 event 16 person 23 shape
3 animal 10 feeling 17 phenomenon 24 state
4 artifact 11 food 18 plant 25 substance
5 attribute 12 group 19 possession 26 time
6 body 13 location 20 process
7 cognition 14 motive 21 quantity
Table 1: The 26 WordNet supersenses
is similar to a fine-grained Named Entity Recog-
nition (Fleischman and Hovy, 2002) task with 26
categories; for example, if our algorithm mistak-
enly inserts a novel non-capital city under the hy-
ponym state capital, it will inherit the correct su-
persense location. Finally, we evaluate the abil-
ity of our algorithm to correctly choose the ap-
propriate sense of the hypernym under which a
novel hyponym is being added. Our labelers cate-
gorize each candidate sense-disambiguated hyper-
nym synset suggested by our algorithm into the
following categories:
c1: Correct sense-disambiguated hypernym.
c2: Correct hypernym word, but incorrect sense of
that word.
c3: Incorrect hypernym, but correct supersense.
c4: Any other relation is considered incorrect.
A single hyponym/hypernym pair is allowed to be
simultaneously labeled 2 and 3.
4.2 Fine-grained evaluation
Table 2 displays the results of our evaluation of
fine-grained precision for the baseline non-joint
algorithm (Base) and our joint algorithm (Joint),
as well as the relative error reduction (ER) of our
algorithm over the baseline. We use the mini-
mum of the two judges? scores. Here we define
fine-grained precision as c1/total. We see that
our joint algorithm strongly outperforms the base-
line, and has high precision for predicting novel
hyponyms up to 10,000 links.
4.3 Hypernym sense disambiguation
Also in Table 2 we compare the sense dis-
ambiguation precision of our algorithm and the
baseline. Here we measure the precision of
sense-disambiguation among all examples where
each algorithm found a correct hyponym word;
our calculation for disambiguation precision is
c1/ (c1 + c2). Again our joint algorithm outper-
forms the baseline algorithm at all levels of re-
call. Interestingly the baseline disambiguation
precision improves with higher recall; this may
Fine-grained Pre. Disambiguation Pre.
#Links Base Joint ER Base Joint ER
100 0.60 1.00 100% 0.86 1.00 100%
1000 0.52 0.93 85% 0.84 1.00 100%
10000 0.46 0.84 70% 0.90 1.00 100%
20000 0.46 0.68 41% 0.94 0.98 68%
Table 2: Fine-grained and disambiguation preci-
sion and error reduction for hyponym acquisition
# Links NER Base Joint ER vs. ER vs.
Oracle NER Base
100 1.00 0.72 1.00 0% 100%
1000 0.69 0.68 0.99 97% 85%
10000 0.45 0.69 0.96 93% 70%
20000 0.54 0.69 0.92 83% 41%
Table 3: Coarse-grained precision and error reduc-
tion vs. Non-joint baseline and NER Oracle
be attributed to the observation that the highest-
confidence hypernyms predicted by individual
classifiers are likely to be polysemous, whereas
hypernyms of lower confidence are more fre-
quently monosemous (and thus trivially easy to
disambiguate).
4.4 Coarse-grained evaluation
We compute coarse-grained precision as (c1 +
c3)/total. Inferring the correct coarse-grained su-
persense of a novel hyponym can be viewed as a
fine-grained (26-category) Named Entity Recog-
nition task; our algorithm for taxonomy induction
can thus be viewed as performing high-accuracy
fine-grained NER. Here we compare against both
the baseline non-joint algorithm as well as an
?oracle? algorithm for Named Entity Recogni-
tion, which perfectly classifies the supersense of
all nouns that fall under the four supersenses
{person, group, location, quantity}, but works
only for those supersenses. Table 3 shows the
results of this coarse-grained evaluation. We see
that the baseline non-joint algorithm has higher
precision than the NER oracle as 10,000 and
20,000 links; however, both are significantly out-
performed by our joint algorithm, which main-
tains high coarse-grained precision (92%) even at
20,000 links.
4.5 Comparison of inferred taxonomies and
WordNet
For our final evaluation we compare our learned
taxonomies directly against the currently exist-
ing hypernym links in WordNet 2.1. In order to
compare taxonomies we use a hand-labeled test
807
WN +10K +20K +30K +40K
PRE 0.524 0.524 0.574 0.583 0.571
REC 0.165 0.165 0.203 0.211 0.211
F 0.251 0.251 0.300 0.309 0.307
Table 4: Taxonomy hypernym classification vs.
WordNet 2.1 on hand-labeled testset
set of over 5,000 noun pairs, randomly-sampled
from newswire corpora (described in (Snow et al,
2005)). We measured the performance of both our
inferred taxonomies and WordNet against this test
set.8 The performance and comparison of the best
WordNet classifier vs. our taxonomies is given in
Table 4. Our best-performing inferred taxonomy
on this test set is achieved after adding 30,000
novel hyponyms, achieving an 23% relative im-
provement in F-score over the WN2.1 classifier.
5 Conclusions
We have presented an algorithm for inducing se-
mantic taxonomies which attempts to globally
optimize the entire structure of the taxonomy.
Our probabilistic architecture also includes a new
model for learning coordinate terms based on
(m,n)-cousin classification. The model?s ability
to integrate heterogeneous evidence from different
classifiers offers a solution to the key problem of
choosing the correct word sense to which to attach
a new hypernym.
Acknowledgements
Thanks to Christiane Fellbaum, Rajat Raina, Bill
MacCartney, and Allison Buckley for useful dis-
cussions and assistance annotating data. Rion
Snow is supported by an NDSEG Fellowship
sponsored by the DOD and AFOSR. This work
was supported in part by the Disruptive Technol-
ogy Office (DTO)?s Advanced Question Answer-
ing for Intelligence (AQUAINT) Program.
References
P. Buitelaar, P. Cimiano and B. Magnini. 2005. Ontol-
ogy Learning from Text: Methods, Evaluation and
Applications. Volume 123 Frontiers in Artificial In-
telligence and Applications.
S. Caraballo. 2001. Automatic Acquisition of
a Hypernym-Labeled Noun Hierarchy from Text.
Brown University Ph.D. Thesis.
8We found that the WordNet 2.1 model achieving the
highest F-score used only the first sense of each hyponym,
and allowed a maximum distance of 4 edges between each
hyponym and its hypernym.
S. Cederberg and D. Widdows. 2003. Using LSA and
Noun Coordination Information to Improve the Pre-
cision and Recall of Automatic Hyponymy Extrac-
tion. Proc. CoNLL-2003, pp. 111?118.
T. Chklovski and P. Pantel. 2004. VerbOcean: Mining
the Web for Fine-Grained Semantic Verb Relations.
Proc. EMNLP-2004.
M. Ciaramita and M. Johnson. 2003. Supersense
Tagging of Unknown Nouns in WordNet. Proc.
EMNLP-2003.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised Named-Entity Extraction from
the Web: An Experimental Study. Artificial Intelli-
gence, 165(1):91?134.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Cambridge, MA: MIT Press.
R. Girju, A. Badulescu, and D. Moldovan. 2003.
Learning Semantic Constraints for the Automatic
Discovery of Part-Whole Relations. Proc. HLT-03.
M. Fleischman and E. Hovy. 2002. Fine grained clas-
sification of named entities. Proc. COLING-02.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. Proc. COLING-92.
D. Hindle. 1990. Noun classification from predicate-
argument structures. Proc. ACL-90.
D. Lenat. 1995. CYC: A Large-Scale Investment in
Knowledge Infrastructure, Communications of the
ACM, 38:11, 33?35.
D. Lin. 1998. Dependency-based Evaluation of MINI-
PAR. Workshop on the Evaluation of Parsing Sys-
tems, Granada, Spain.
D. Lin, S. Zhao, L. Qin and M. Zhou. 2003. Iden-
tifying Synonyms among Distributionally Similar
Words. Proc. IJCAI-03.
M. Pasc?a. 2005. Finding Instance Names and Alter-
native Glosses on the Web: WordNet Reloaded. CI-
CLing 2005, pp. 280-292.
D. Ravichandran, P. Pantel, and E. Hovy. 2002. Ran-
domized Algorithms and NLP: Using Locality Sen-
sitive Hash Function for High Speed Noun Cluster-
ing. Proc. ACL-2002.
E. Riloff and J. Shepherd. 1997. A Corpus-Based
Approach for Building Semantic Lexicons. Proc
EMNLP-1997.
B. Roark and E. Charniak. 1998. Noun-phrase co-
occurerence statistics for semi-automatic-semantic
lexicon construction. Proc. ACL-1998.
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learn-
ing syntactic patterns for automatic hypernym dis-
covery. NIPS 2005.
P. Turney, M. Littman, J. Bigham, and V. Shnay-
der. 2003. Combining independent modules to
solve multiple-choice synonym and analogy prob-
lems. Proc. RANLP-2003, pp. 482?489.
808
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 105?108,
Prague, June 2007. c?2007 Association for Computational Linguistics
Disambiguating Between Generic and Referential ?You? in Dialog?
Surabhi Gupta
Department of Computer Science
Stanford University
Stanford, CA 94305, US
surabhi@cs.stanford.edu
Matthew Purver
Center for the Study
of Language and Information
Stanford University
Stanford, CA 94305, US
mpurver@stanford.edu
Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305, US
jurafsky@stanford.edu
Abstract
We describe an algorithm for a novel task: disam-
biguating the pronoun you in conversation. You can
be generic or referential; finding referential you is im-
portant for tasks such as addressee identification or
extracting ?owners? of action items. Our classifier
achieves 84% accuracy in two-person conversations;
an initial study shows promising performance even on
more complex multi-party meetings.
1 Introduction and Background
This paper describes an algorithm for disambiguat-
ing the generic and referential senses of the pronoun
you.
Our overall aim is the extraction of action items
from multi-party human-human conversations, con-
crete decisions in which one (or more) individuals
take on a group commitment to perform a given task
(Purver et al, 2006). Besides identifying the task it-
self, it is crucial to determine the owner, or person
responsible. Occasionally, the name of the responsi-
ble party is mentioned explicitly. More usually, the
owner is addressed directly and therefore referred to
using a second-person pronoun, as in example (1).1
(1)
A: and um if you can get that binding point also
maybe with a nice example that would be helpful
for Johno and me.
B: Oh yeah uh O K.
It can also be important to distinguish between
singular and plural reference, as in example (2)
where the task is assigned to more than one person:
(2)
A: So y- so you guys will send to the rest of us um a
version of um, this, and - the - uh, description -
B: With sugge- yeah, suggested improvements and -
Use of ?you? might therefore help us both in de-
?This work was supported by the CALO project
(DARPA grant NBCH-D-03-0010) and ONR (MURI award
N000140510388). The authors also thank John Niekrasz for
annotating our test data.
1(1,2) are taken from the ICSI Meeting Corpus (Shriberg et
al., 2004); (3,4) from Switchboard (Godfrey et al, 1992).
tecting the fact that a task is being assigned, and in
identifying the owner. While there is an increas-
ing body of work concerning addressee identifica-
tion (Katzenmaier et al, 2004; Jovanovic et al,
2006), there is very little investigating the problem
of second-person pronoun resolution, and it is this
that we address here. Most cases of ?you? do not in
fact refer to the addressee but are generic, as in ex-
ample (3); automatic referentiality classification is
therefore very important.
(3)
B: Well, usually what you do is just wait until you
think it?s stopped,
and then you patch them up.
2 Related Work
Previous linguistic work has recognized that ?you?
is not always addressee-referring, differentiating be-
tween generic and referential uses (Holmes, 1998;
Meyers, 1990) as well as idiomatic cases of ?you
know?. For example, (Jurafsky et al, 2002) found
that ?you know? covered 47% of cases, the referen-
tial class 22%, and the generic class 27%, with no
significant differences in surface form (duration or
vowel reduction) between the different cases.
While there seems to be no previous work investi-
gating automatic classification, there is related work
on classifying ?it?, which also takes various referen-
tial and non-referential readings: (Mu?ller, 2006) use
lexical and syntactic features in a rule-based clas-
sifier to detect non-referential uses, achieving raw
accuracies around 74-80% and F-scores 63-69%.
3 Data
We used the Switchboard corpus of two-party tele-
phone conversations (Godfrey et al, 1992), and an-
notated the data with four classes: generic, referen-
tial singular, referential plural and a reported refer-
ential class, for mention in reported speech of an
105
Training Testing
Generic 360 79
Referential singular 287 92
Referential plural 17 3
Reported referential 5 1
Ambiguous 4 1
Total 673 176
Table 1: Number of cases found.
originally referential use (as the original addressee
may not be the current addressee ? see example (4)).
We allowed a separate class for genuinely ambigu-
ous cases. Switchboard explicitly tags ?you know?
when used as a discourse marker; as this (generic)
case is common and seems trivial we removed it
from our data.
(4)
B: Well, uh, I guess probably the last one I went to I
met so many people that I had not seen in proba-
bly ten, over ten years.
It was like, don?t you remember me.
And I am like no.
A: Am I related to you?
To test inter-annotator agreement, two people an-
notated 4 conversations, yielding 85 utterances con-
taining ?you?; the task was reported to be easy, and
the kappa was 100%.
We then annotated a total of 42 conversations for
training and 13 for testing. Different labelers an-
notated the training and test sets; none of the au-
thors were involved in labeling the test set. Table 1
presents information about the number of instances
of each of these classes found.
4 Features
All features used for classifier experiments were
extracted from the Switchboard LDC Treebank 3
release, which includes transcripts, part of speech
information using the Penn tagset (Marcus et al,
1994) and dialog act tags (Jurafsky et al, 1997).
Features fell into four main categories:2 senten-
tial features which capture lexical features of the
utterance itself; part-of-speech features which cap-
ture shallow syntactic patterns; dialog act features
capturing the discourse function of the current ut-
terance and surrounding context; and context fea-
tures which give oracle information (i.e., the cor-
rect generic/referential label) about preceding uses
2Currently, features are all based on perfect transcriptions.
of ?you?. We also investigated using the presence
of a question mark in the transcription as a feature,
as a possible replacement for some dialog act fea-
tures. Table 2 presents our features in detail.
N Features
Sentential Features (Sent)
2 you, you know, you guys
N number of you, your, yourself
2 you (say|said|tell|told|mention(ed)|mean(t)|sound(ed))
2 you (hear|heard)
2 (do|does|did|have|has|had|are|could|should|n?t) you
2 ?if you?
2 (which|what|where|when|how) you
Part of Speech Features (POS)
2 Comparative JJR tag
2 you (VB*)
2 (I|we) (VB*)
2 (PRP*) you
Dialog Act Features (DA)
46 DA tag of current utterance i
46 DA tag of previous utterance i ? 1
46 DA tag of utterance i ? 2
2 Presence of any question DA tag (Q DA)
2 Presence of elaboration DA tag
Oracle Context Features (Ctxt)
3 Class of utterance i ? 1
3 Class of utterance i ? 2
3 Class of previous utterance by same speaker
3 Class of previous labeled utterance
Other Features (QM)
2 Question mark
Table 2: Features investigated. N indicates the num-
ber of possible values (there are 46 DA tags; context
features can be generic, referential or N/A).
5 Experiments and Results
As Table 1 shows, there are very few occurrences
of the referential plural, reported referential and am-
biguous classes. We therefore decided to model our
problem as a two way classification task, predicting
generic versus referential (collapsing referential sin-
gular and plural as one category). Note that we ex-
pect this to be the major useful distinction for our
overall action-item detection task.
Baseline A simple baseline involves predicting the
dominant class (in the test set, referential). This
gives 54.59% accuracy (see Table 1).3
SVM Results We used LIBSVM (Chang and Lin,
2001), a support vector machine classifier trained
using an RBF kernel. Table 3 presents results for
3Precision and recall are of course 54.59% and 100%.
106
Features Accuracy F-Score
Ctxt 45.66% 0%
Baseline 54.59% 70.63%
Sent 67.05% 57.14%
Sent + Ctxt + POS 67.05% 57.14%
Sent + Ctxt + POS + QM 76.30% 72.84%
Sent + Ctxt + POS + Q DA 79.19% 77.50%
DA 80.92% 79.75%
Sent + Ctxt + POS +
QM + DA 84.39% 84.21%
Table 3: SVM results: generic versus referential
various selected sets of features. The best set of fea-
tures gave accuracy of 84.39% and f-score 84.21%.
Discussion Overall performance is respectable;
precision was consistently high (94% for the
highest-accuracy result). Perhaps surprisingly, none
of the context or part-of-speech features were found
to be useful; however, dialog act features proved
very useful ? using these features alone give us
an accuracy of 80.92% ? with the referential class
strongly associated with question dialog acts.
We used manually produced dialog act tags, and
automatic labeling accuracy with this fine-grained
tagset will be low; we would therefore prefer to
use more robust features if possible. We found that
one such heuristic feature, the presence of ques-
tion mark, cannot entirely substitute: accuracy is
reduced to 76.3%. However, using only the binary
Q DA feature (which clusters together all the dif-
ferent kinds of question DAs) does better (79.19%).
Although worse than performance with a full tagset,
this gives hope that using a coarse-grained set of
tags might allow reasonable results. As (Stolcke et
al., 2000) report good accuracy (87%) for statement
vs. question classification on manual Switchboard
transcripts, such coarse-grained information might
be reliably available.
Surprisingly, using the oracle context features (the
correct classification for the previous you) alone per-
forms worse than the baseline; and adding these fea-
tures to sentential features gives no improvement.
This suggests that the generic/referential status of
each you may be independent of previous yous.
Features Accuracy F-Score
Prosodic only 46.66% 44.31%
Baseline 54.59% 70.63%
Sent + Ctxt + POS +
QM + DA + Prosodic 84.39% 84.21%
Table 4: SVM results: prosodic features
Category Referential Generic
Count 294 340
Pitch (Hz) 156.18 143.98
Intensity (dB) 60.06 59.41
Duration (msec) 139.50 136.84
Table 5: Prosodic feature analysis
6 Prosodic Features
We next checked a set of prosodic features, test-
ing the hypothesis that generics are prosodically re-
duced. Mean pitch, intensity and duration were ex-
tracted using Praat, both averaged over the entire
utterance and just for the word ?you?. Classifi-
cation results are shown in Table 4. Using only
prosodic features performs below the baseline; in-
cluding prosodic features with the best-performing
feature set from Table 3 gives identical performance
to that with lexical and contextual features alone.
To see why the prosodic features did not help, we
examined the difference between the average pitch,
intensity and duration for referential versus generic
cases (Table 5). A one-sided t-test shows no signif-
icant differences between the average intensity and
duration (confirming the results of (Jurafsky et al,
2002), who found no significant change in duration).
The difference in the average pitch was found to be
significant (p=0.2) ? but not enough for this feature
alone to cause an increase in overall accuracy.
7 Error Analysis
We performed an error analysis on our best classi-
fier output on the training set; accuracy was 94.53%,
giving a total of 36 errors.
Half of the errors (18 of 36) were ambiguous even
for humans (the authors), if looking at the sentence
alone without the neighboring context from the ac-
tual conversation ? see (5a). Treating these exam-
ples thus needs a detailed model of dialog context.
The other major class of errors requires detailed
107
knowledge about sentential semantics and/or the
world ? see e.g. (5b,c), which we can tell are ref-
erential because they predicate inter-personal com-
parison or communication.
In addition, as questions are such a useful feature
(see above), the classifier tends to label all question
cases as referential. However, generic uses do occur
within questions (5d), especially if rhetorical (5e):
(5) a. so uh and if you don?t have the money then use a
credit card
b. I?m probably older than you
c. although uh I will personally tell you I used to work
at a bank
d. Do they survive longer if you plant them in the winter
time?
e. my question I guess are they really your peers?
8 Initial Multi-Party Experiments
The experiments above used two-person dialog data:
we expect that multi-party data is more complex. We
performed an initial exploratory study, applying the
same classes and features to multi-party meetings.
Two annotators labeled one meeting from the
AMI corpus (Carletta et al, 2006), giving a total of
52 utterances containing ?you? on which to assess
agreement: kappa was 87.18% for two way clas-
sification of generic versus referential. One of the
authors then labeled a testing set of 203 utterances;
104 are generic and 99 referential, giving a baseline
accuracy of 51.23% (and F-score of 67.65%).
We performed experiments for the same task: de-
tecting generic versus referential uses. Due to the
small amount of data, we trained the classifier on the
Switchboard training set from section 3 (i.e. on two-
party rather than multi-party data). Lacking part-of-
speech or dialog act features (since the dialog act
tagset differs from the Switchboard tagset), we used
only the sentential, context and question mark fea-
tures described in Table 2.
However, the classifier still achieves an accuracy
of 73.89% and F-score of 74.15%, comparable to the
results on Switchboard without dialog act features
(accuracy 76.30%). Precision is lower, though (both
precision and recall are 73-75%).
9 Conclusions
We have presented results on two person and multi-
party data for the task of generic versus referential
?you? detection. We have seen that the problem is
a real one: in both datasets the distribution of the
classes is approximately 50/50, and baseline accu-
racy is low. Classifier accuracy on two-party data is
reasonable, and we see promising results on multi-
party data with a basic set of features. We expect the
accuracy to go up once we train and test on same-
genre data and also add features that are more spe-
cific to multi-party data.
References
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot,
T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal,
G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post,
D. Reidsma, and P. Wellner. 2006. The AMI meeting cor-
pus. In MLMI 2005, Revised Selected Papers.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library for
Support Vector Machines. Software available at http:
//www.csie.ntu.edu.tw/?cjlin/libsvm.
J. J. Godfrey, E. Holliman, and J. McDaniel. 1992. SWITCH-
BOARD: Telephone speech corpus for research and devel-
opment. In Proceedings of IEEE ICASSP-92.
J. Holmes. 1998. Generic pronouns in the Wellington corpus
of spoken New Zealand English. Ko?tare, 1(1).
N. Jovanovic, R. op den Akker, and A. Nijholt. 2006. Ad-
dressee identification in face-to-face meetings. In Proceed-
ings of the 11th Conference of the EACL.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL shallow-discourse-function annota-
tion coders manual, draft 13. Technical Report 97-02, Uni-
versity of Colorado, Boulder.
D. Jurafsky, A. Bell, and C. Girand. 2002. The role of the
lemma in form variation. In C. Gussenhoven and N. Warner,
editors, Papers in Laboratory Phonology VII, pages 1?34.
M. Katzenmaier, R. Stiefelhagen, and T. Schultz. 2004. Iden-
tifying the addressee in human-human-robot interactions
based on head pose and speech. In Proceedings of the 6th
International Conference on Multimodal Interfaces.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies,
M. Ferguson, K. Katz, and B. Schasberger. 1994. The Penn
treebank: Annotating predicate argument structure. In ARPA
Human Language Technology Workshop.
M. W. Meyers. 1990. Current generic pronoun usage. Ameri-
can Speech, 65(3):228?237.
C. Mu?ller. 2006. Automatic detection of nonreferential It in
spoken multi-party dialog. In Proceedings of the 11th Con-
ference of the EACL.
M. Purver, P. Ehlen, and J. Niekrasz. 2006. Detecting action
items in multi-party meetings: Annotation and initial exper-
iments. In MLMI 2006, Revised Selected Papers.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004.
The ICSI Meeting Recorder Dialog Act (MRDA) Corpus. In
Proceedings of the 5th SIGdial Workshop.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Juraf-
sky, P. Taylor, C. V. Ess-Dykema, R. Martin, and M. Meteer.
2000. Dialogue act modeling for automatic tagging and
recognition of conversational speech. Computational Lin-
guistics, 26(3):339?373.
108
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 173?176,
Prague, June 2007. c?2007 Association for Computational Linguistics
Classifying Temporal Relations Between Events
Nathanael Chambers and Shan Wang and Dan Jurafsky
Department of Computer Science
Stanford University
Stanford, CA 94305
{natec,shanwang,jurafsky}@stanford.edu
Abstract
This paper describes a fully automatic two-
stage machine learning architecture that
learns temporal relations between pairs of
events. The first stage learns the temporal
attributes of single event descriptions, such
as tense, grammatical aspect, and aspectual
class. These imperfect guesses, combined
with other linguistic features, are then used
in a second stage to classify the temporal re-
lationship between two events. We present
both an analysis of our new features and re-
sults on the TimeBank Corpus that is 3%
higher than previous work that used perfect
human tagged features.
1 Introduction
Temporal information encoded in textual descrip-
tions of events has been of interest since the early
days of natural language processing. Lately, it has
seen renewed interest as Question Answering, Infor-
mation Extraction and Summarization domains find
it critical in order to proceed beyond surface under-
standing. With the recent creation of the Timebank
Corpus (Pustejovsky et al, 2003), the utility of ma-
chine learning techniques can now be tested.
Recent work with the Timebank Corpus has re-
vealed that the six-class classification of temporal
relations is very difficult, even for human annotators.
The highest score reported on Timebank achieved
62.5% accuracy when using gold-standard features
as marked by humans (Mani et al, 2006). This pa-
per describes an approach using features extracted
automatically from raw text that not only dupli-
cates this performance, but surpasses its accuracy
by 3%. We do so through advanced linguistic fea-
tures and a surprising finding that using automatic
rather than hand-labeled tense and aspect knowledge
causes only a slight performance degradation.
We briefly describe current work on temporal or-
dering in section 2. Section 4 describes the first stage
of basic temporal extraction, followed by a full de-
scription of the second stage in 5. The evaluation
and results on Timebank then follow in section 6.
2 Previous Work
Mani et. al (2006) built a MaxEnt classifier that as-
signs each pair of events one of 6 relations from an
augmented Timebank corpus. Their classifier relies
on perfect features that were hand-tagged in the cor-
pus, including tense, aspect, modality, polarity and
event class. Pairwise agreement on tense and aspect
are also included. In a second study, they applied
rules of temporal transitivity to greatly expand the
corpus, providing different results on this enlarged
dataset. We could not duplicate their reported per-
formance on this enlarged data, and instead focus on
performing well on the Timebank data itself.
Lapata and Lascarides (2006) trained an event
classifier for inter-sentential events. They built a cor-
pus by saving sentences that contained two events,
one of which is triggered by a key time word (e.g.
after and before). Their learner was based on syntax
and clausal ordering features. Boguraev and Ando
(2005) evaluated machine learning on related tasks,
but not relevant to event-event classification.
Our work is most similar to Mani?s in that we are
173
learning relations given event pairs, but our work ex-
tends their results both with new features and by us-
ing fully automatic linguistic features from raw text
that are not hand selected from a corpus.
3 Data
We used the Timebank Corpus (v1.1) for evaluation,
186 newswire documents with 3345 event pairs.
Solely for comparison with Mani, we add the 73
document Opinion Corpus (Mani et al, 2006) to cre-
ate a larger dataset called the OTC. We present both
Timebank and OTC results so future work can com-
pare against either. All results below are from 10-
fold cross validation.
4 Stage One: Learning Event Attributes
The task in Stage One is to learn the five tempo-
ral attributes associated with events as tagged in the
Timebank Corpus. (1) Tense and (2) grammatical
aspect are necessary in any approach to temporal
ordering as they define both temporal location and
structure of the event. (3) Modality and (4) polar-
ity indicate hypothetical or non-occuring situations,
and finally, (5) event class is the type of event (e.g.
process, state, etc.). The event class has 7 values in
Timebank, but we believe this paper?s approach is
compatible with other class divisions as well. The
range of values for each event attribute is as follows,
also found in (Pustejovsky et al, 2003):
tense none, present, past, future
aspect none, prog, perfect, prog perfect
class report, aspectual, state, I state
I action, perception, occurrence
modality none, to, should, would, could
can, might
polarity positive, negative
4.1 Machine Learning Classification
We used a machine learning approach to learn each
of the five event attributes. We implemented both
Naive Bayes and Maximum Entropy classifiers, but
found Naive Bayes to perform as well or better than
Maximum Entropy. The results in this paper are
from Naive Bayes with Laplace smoothing.
The features we used on this stage include part of
speech tags (two before the event), lemmas of the
event words, WordNet synsets, and the appearance
tense POS-2-event, POS-1-event
POS-of-event, have word, be word
aspect POS-of-event, modal word, be word
class synset
modality none
polarity none
Figure 1: Features selected for learning each tempo-
ral attribute. POS-2 is two tokens before the event.
Timebank Corpus
tense aspect class
Baseline 52.21 84.34 54.21
Accuracy 88.28 94.24 75.2
Baseline (OTC) 48.52 86.68 59.39
Accuracy (OTC) 87.46 88.15 76.1
Figure 2: Stage One results on classification.
of auxiliaries and modals before the event. This lat-
ter set included all derivations of be and have auxil-
iaries, modal words (e.g. may, might, etc.), and the
presence/absence of not. We performed feature se-
lection on this list of features, learning a different set
of features for each of the five attributes. The list of
selected features for each is shown in figure 1.
Modality and polarity did not select any features
because their majority class baselines were so high
(98%) that learning these attributes does not provide
much utility. A deeper analysis of event interaction
would require a modal analysis, but it seems that a
newswire domain does not provide great variation
in modalities. Consequently, modality and polarity
are not used in Stage Two. Tense, aspect and class
are shown in figure 2 with majority class baselines.
Tense classification achieves 36% absolute improve-
ment, aspect 10% and class 21%. Performance on
the OTC set is similar, although aspect is not as
good. These guesses are then passed to Stage Two.
5 Stage Two: Event-Event Features
The task in this stage is to choose the temporal re-
lation between two events, given the pair of events.
We assume that the events have been extracted and
that there exists some relation between them; the
task is to choose the relation. The Timebank Corpus
uses relations that are based on Allen?s set of thir-
174
teen (Allen, 1984). Six of the relations are inverses
of the other six, and so we condense the set to be-
fore, ibefore, includes, begins, ends and simultane-
ous. We map the thirteenth identity into simultane-
ous. One oddity is that Timebank includes both dur-
ing and included by relations, but during does not
appear in Timebank documentation. While we don?t
know how previous work handles this, we condense
during into included by (invert to includes).
5.1 Features
Event Specific: The five temporal attributes from
Stage One are used for each event in the pair, as well
as the event strings, lemmas and WordNet synsets.
Mani added two other features from these, indica-
tors if the events agree on tense and aspect. We add
a third, event class agreement. Further, to capture
the dependency between events in a discourse, we
create new bigram features of tense, aspect and class
(e.g. ?present past? if the first event is in the present,
and the second past).
Part of Speech: For each event, we include the Penn
Treebank POS tag of the event, the tags for the two
tokens preceding, and one token following. We use
the Stanford Parser1 to extract them. We also extend
previous work and create bigram POS features of the
event and the token before it, as well as the bigram
POS of the first event and the second event.
Event-Event Syntactic Properties: A phrase P is
said to dominate another phrase Q if Q is a daugh-
ter node of P in the syntactic parse tree. We lever-
age the syntactic output of the parser to create the
dominance feature for intra-sentential events. It is
either on or off, depending on the two events? syn-
tactic dominance. Lapata used a similar feature for
subordinate phrases and an indicator before for tex-
tual event ordering. We adopt these features and also
add a same-sentence indicator if the events appear in
the same sentence.
Prepositional Phrase: Since preposition heads are
often indicators of temporal class, we created a new
feature indicating when an event is part of a prepo-
sitional phrase. The feature?s values range over 34
English prepositions. Combined with event dom-
inance (above), these two features capture direct
1http://nlp.stanford.edu/software/lex-parser.shtml
intra-sentential relationships. To our knowledge, we
are the first to use this feature in temporal ordering.
Temporal Discourse: Seeing tense as a type of
anaphora, it is a natural conclusion that the rela-
tionship between two events becomes stronger as
the textual distance draws closer. Because of this,
we adopted the view that intra-sentential events are
generated from a different distribution than inter-
sentential events. We therefore train two models
during learning, one for events in the same sen-
tence, and the other for events crossing sentence
boundaries. It essentially splits the data on the
same sentence feature. As we will see, this turned
out to be a very useful feature. It is called the split
approach in the next section.
Example (require, compromise):
?Their solution required a compromise...?
Features
(lemma1: require) (lemma2: compromise) (dominates: yes)
(tense-bigram: past-none) (aspect-bigram: none-none) (tense-
match: no) (aspect-match: yes) (before: yes) (same-sent: yes)
6 Evaluation and Results
All results are from a 10-fold cross validation us-
ing SVM (Chang and Lin, 2001). We also eval-
uated Naive Bayes and Maximum Entropy. Naive
Bayes (NB) returned similar results to SVM and we
present feature selection results from NB to compare
the added value of our new features.
The input to Stage Two is a list of pairs of events;
the task is to classify each according to one of six
temporal relations. Four sets of results are shown
in figure 3. Mani, Mani+Lapata and All+New cor-
respond to performance on features as listed in the
figure. The three table columns indicate how a gold-
standard Stage One (Gold) compares against imper-
fect guesses (Auto) and the guesses with split distri-
butions (Auto-Split).
A clear improvement is seen in each row, indi-
cating that our new features provide significant im-
provement over previous work. A decrease in per-
formance is seen between columns gold and auto,
as expected, because imperfect data is introduced,
however, the drop is manageable. The auto-split dis-
tributions make significant gains for the Mani and
Lapata features, but less when all new features are
175
Timebank Corpus Gold Auto Auto-Split
Baseline 37.22 37.22 46.58
Mani 50.97 50.19 53.42
Mani+Lapata 52.29 51.57 55.10
All+New 60.45 59.13 59.43
Mani stage one attributes, tense/aspect-match, event strings
Lapata dominance, before, lemma, synset
New prep-phrases, same-sent, class-match, POS uni/bigrams,
tense/aspect/class-bigrams
Figure 3: Incremental accuracy by adding features.
Same Sentence Diff Sentence
POS-1 Ev1 2.5% Tense Pair 1.6%
POS Bigram Ev1 3.5% Aspect Ev1 0.5%
Preposition Ev1 2.0% POS Bigram 0.2%
Tense Ev2 0.7% POS-1 Ev2 0.3%
Preposition Ev2 0.6% Word EV2 0.2%
Figure 4: Top 5 features as added in feature selection
w/ Naive Bayes, with their percentage improvement.
involved. The highest fully-automatic accuracy on
Timebank is 59.43%, a 4.3% gain from our new fea-
tures. We also report 67.57% gold and 65.48% auto-
split on the OTC dataset to compare against Mani?s
reported hand-tagged features of 62.5%, a gain of
3% with our automatic features.
7 Discussion
Previous work on OTC achieved classification accu-
racy of 62.5%, but this result was based on ?perfect
data? from human annotators. A low number from
good data is at first disappointing, however, we show
that performance can be improved through more lin-
guistic features and by isolating the distinct tasks of
ordering inter-sentential and intra-sentential events.
Our new features show a clear improvement over
previous work. The features that capture dependen-
cies between the events, rather than isolated features
provide the greatest utility. Also, the impact of im-
perfect temporal data is surprisingly minimal. Us-
ing Stage One?s results instead of gold values hurts
performance by less than 1.4%. This suggests that
much of the value of the hand-coded information
can be achieved via automatic approaches. Stage
One?s event class shows room for improvement, yet
the negative impact on Event-Event relationships is
manageable. It is conceivable that more advanced
features would better classify the event class, but im-
provement on the event-event task would be slight.
Finally, it is important to note the difference in
classifying events in the same sentence vs. cross-
boundary. Splitting the 3345 pairs of corpus events
into two separate training sets makes our data more
sparse, but we still see a performance improvement
when using Mani/Lapata features. Figure 4 gives a
hint to the difference in distributions as the best fea-
tures of each task are very different. Intra-sentence
events rely on syntax cues (e.g. preposition phrases
and POS), while inter-sentence events use tense and
aspect. However, the differences are minimized as
more advanced features are added. The final row in
figure 3 shows minimal split improvement.
8 Conclusion
We have described a two-stage machine learning
approach to event-event temporal relation classifi-
cation. We have shown that imperfect event at-
tributes can be used effectively, that a range of event-
event dependency features provide added utility to a
classifier, and that events within the same sentence
have distinct characteristics from those across sen-
tence boundaries. This fully automatic raw text ap-
proach achieves a 3% improvement over previous
work based on perfect human tagged features.
Acknowledgement: This work was supported in
part by the DARPA GALE Program and the DTO
AQUAINT Program.
References
James Allen. 1984. Towards a general theory of action and
time. Artificial Intelligence, 23:123?154.
Branimir Boguraev and Rie Kubota Ando. 2005. Timeml-
compliant text analysis for temporal reasoning. In IJCA-05.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a li-
brary for support vector machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Mirella Lapata and Alex Lascarides. 2006. Learning sentence-
internal temporal relations. In Journal of AI Research, vol-
ume 27, pages 85?117.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min Lee,
and James Pustejovsky. 2006. Machine learning of temporal
relations. In ACL-06, July.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See,
David Day, Lisa Ferro, Robert Gaizauskas, Marcia Lazo,
Andrea Setzer, and Beth Sundheim. 2003. The timebank
corpus. Corpus Linguistics, pages 647?656.
176
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 193?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Measuring Importance and Query Relevance in Topic-focused
Multi-document Summarization
Surabhi Gupta and Ani Nenkova and Dan Jurafsky
Stanford University
Stanford, CA 94305
surabhi@cs.stanford.edu, {anenkova,jurafsky}@stanford.edu
Abstract
The increasing complexity of summarization systems
makes it difficult to analyze exactly which mod-
ules make a difference in performance. We carried
out a principled comparison between the two most
commonly used schemes for assigning importance to
words in the context of query focused multi-document
summarization: raw frequency (word probability) and
log-likelihood ratio. We demonstrate that the advan-
tages of log-likelihood ratio come from its known dis-
tributional properties which allow for the identifica-
tion of a set of words that in its entirety defines the
aboutness of the input. We also find that LLR is more
suitable for query-focused summarization since, un-
like raw frequency, it is more sensitive to the integra-
tion of the information need defined by the user.
1 Introduction
Recently the task of multi-document summarization
in response to a complex user query has received
considerable attention. In generic summarization,
the summary is meant to give an overview of the
information in the documents. By contrast, when
the summary is produced in response to a user query
or topic (query-focused, topic-focused, or generally
focused summary), the topic/query determines what
information is appropriate for inclusion in the sum-
mary, making the task potentially more challenging.
In this paper we present an analytical study of two
questions regarding aspects of the topic-focused sce-
nario. First, two estimates of importance on words
have been used very successfully both in generic and
query-focused summarization: frequency (Luhn,
1958; Nenkova et al, 2006; Vanderwende et al,
2006) and loglikelihood ratio (Lin and Hovy, 2000;
Conroy et al, 2006; Lacatusu et al, 2006). While
both schemes have proved to be suitable for sum-
marization, with generally better results from log-
likelihood ratio, no study has investigated in what
respects and by how much they differ. Second, there
are many little-understood aspects of the differences
between generic and query-focused summarization.
For example, we?d like to know if a particular word
weighting scheme is more suitable for focused sum-
marization than others. More significantly, previous
studies show that generic and focused systems per-
form very similarly to each other in query-focused
summarization (Nenkova, 2005) and it is of interest
to find out why.
To address these questions we examine the two
weighting schemes: raw frequency (or word proba-
bility estimated from the input), and log-likelihood
ratio (LLR) and two of its variants. These metrics
are used to assign importance to individual content
words in the input, as we discuss below.
Word probability R(w) = nN , where n is the num-
ber of times the word w appeared in the input and N
is the total number of words in the input.
Log-likelihood ratio (LLR) The likelihood ratio ?
(Manning and Schutze, 1999) uses a background
corpus to estimate the importance of a word and it
is proportional to the mutual information between
a word w and the input to be summarized; ?(w) is
defined as the ratio between the probability (under
a binomial distribution) of observing w in the input
and the background corpus assuming equal proba-
bility of occurrence of w in both and the probability
of the data assuming different probabilities for w in
the input and the background corpus.
LLR with cut-off (LLR(C)) A useful property
of the log-likelihood ratio is that the quantity
193
?2 log(?) is asymptotically well approximated by
?2 distribution. A word appears in the input sig-
nificantly more often than in the background corpus
when ?2 log(?) > 10. Such words are called signa-
ture terms in Lin and Hovy (2000) who were the first
to introduce the log-likelihood weighting scheme for
summarization. Each descriptive word is assigned
an equal weight and the rest of the words have a
weight of zero:
R(w) = 1 if (?2 log(?(w)) > 10), 0 otherwise.
This weighting scheme has been adopted in several
recent generic and topic-focused summarizers (Con-
roy et al, 2006; Lacatusu et al, 2006).
LLR(CQ) The above three weighting schemes as-
sign a weight to words regardless of the user query
and are most appropriate for generic summarization.
When a user query is available, it should inform
the summarizer to make the summary more focused.
In Conroy et al (2006) such query sensititivity is
achieved by augmenting LLR(C) with all content
words from the user query, each assigned a weight
of 1 equal to the weight of words defined by LLR(C)
as topic words from the input to the summarizer.
2 Data
We used the data from the 2005 Document Under-
standing Conference (DUC) for our experiments.
The task is to produce a 250-word summary in re-
sponse to a topic defined by a user for a total of 50
topics with approximately 25 documents for each
marked as relevant by the topic creator. In com-
puting LLR, the remaining 49 topics were used as a
background corpus as is often done by DUC partic-
ipants. A sample topic (d301) shows the complexity
of the queries:
Identify and describe types of organized crime that
crosses borders or involves more than one country. Name
the countries involved. Also identify the perpetrators in-
volved with each type of crime, including both individuals
and organizations if possible.
3 The Experiment
In the summarizers we compare here, the various
weighting methods we describe above are used to
assign importance to individual content words in the
input. The weight or importance of a sentence S in
GENERIC FOCUSED
Frequency 0.11972 0.11795
(0.11168?0.12735) (0.11010?0.12521)
LLR 0.11223 0.11600
(0.10627?0.11873) (0.10915?0.12281)
LLR(C) 0.11949 0.12201
(0.11249?0.12724) (0.11507?0.12950)
LLR(CQ) not app 0.12546
(.11884?.13247)
Table 1: SU4 ROUGE recall (and 95% confidence
intervals) for runs on the entire input (GENERIC) and
on relevant sentences (FOCUSED).
the input is defined as
WeightR(S) =
?
w?S
R(w) (1)
where R(w) assigns a weight for each word w.
For GENERIC summarization, the top scoring sen-
tences in the input are taken to form a generic extrac-
tive summary. In the computation of sentence im-
portance, only nouns, verbs, adjectives and adverbs
are considered and a short list of light verbs are ex-
cluded: ?has, was, have, are, will, were, do, been,
say, said, says?. For FOCUSED summarization, we
modify this algorithm merely by running the sen-
tence selection algorithm on only those sentences
in the input that are relevent to the user query. In
some previous DUC evaluations, relevant sentences
are explicitly marked by annotators and given to sys-
tems. In our version here, a sentence in the input is
considered relevant if it contains at least one word
from the user query.
For evaluation we use ROUGE (Lin, 2004) SU4
recall metric1, which was among the official auto-
matic evaluation metrics for DUC.
4 Results
The results are shown in Table 1. The focused sum-
marizer using LLR(CQ) is the best, and it signif-
icantly outperforms the focused summarizer based
on frequency. Also, LLR (using log-likelihood ra-
tio to assign weights to all words) perfroms signif-
icantly worse than LLR(C). We can observe some
trends even from the results for which there is no
significance. Both LLR and LLR(C) are sensitive to
the introduction of topic relevance, producing some-
what better summaries in the FOCUSED scenario
1
-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -d
194
compared to the GENERIC scenario. This is not the
case for the frequency summarizer, where using only
the relevant sentences has a negative impact.
4.1 Focused summarization: do we need query
expansion?
In the FOCUSED condition there was little (for LLR
weighting) or no (for frequency) improvement over
GENERIC. One possible explanation for the lack of
clear improvement in the FOCUSED setting is that
there are not enough relevant sentences, making it
impossible to get stable estimates of word impor-
tance. Alternatively, it could be the case that many
of the sentences are relevant, so estimates from the
relevant portion of the input are about the same as
those from the entire input.
To distinguish between these two hypotheses, we
conducted an oracle experiment. We modified the
FOCUSED condition by expanding the topic words
from the user query with all content words from any
of the human-written summaries for the topic. This
increases the number of relevant sentences for each
topic. No automatic method for query expansion can
be expected to give more accurate results, since the
content of the human summaries is a direct indica-
tion of what information in the input was important
and relevant and, moreover, the ROUGE evaluation
metric is based on direct n-gram comparison with
these human summaries.
Even under these conditions there was no signif-
icant improvement for the summarizers, each get-
ting better by 0.002: the frequency summarizer gets
R-SU4 of 0.12048 and the LLR(CQ) summarizer
achieves R-SU4 of 0.12717.
These results seem to suggest that considering the
content words in the user topic results in enough rel-
evant sentences. Indeed, Table 2 shows the mini-
mum, maximum and average percentage of relevant
sentences in the input (containing at least one con-
tent words from the user the query), both as defined
by the original query and by the oracle query ex-
pansion. It is clear from the table that, on aver-
age, over half of the input comprises sentences that
are relevant to the user topic. Oracle query expan-
sion makes the number of relevant sentences almost
equivalent to the input size and it is thus not sur-
prising that the corresponding results for content se-
lection are nearly identical to the query independent
Original query Oracle query expansion
Min 13% 52%
Average 57% 86%
Max 82% 98%
Table 2: Percentage of relevant sentences (contain-
ing words from the user query) in the input. The
oracle query expansion considers all content words
form human summaries of the input as query words.
runs of generic summaries for the entire input.
These numbers indictate that rather than finding
ways for query expansion, it might instead be more
important to find techniques for constraining the
query, determining which parts of the input are di-
rectly related to the user questions. Such techniques
have been described in the recent multi-strategy ap-
proach of Lacatusu et al (2006) for example, where
one of the strategies breaks down the user topic
into smaller questions that are answered using ro-
bust question-answering techniques.
4.2 Why is log-likelihood ratio better than
frequency?
Frequency and log-likelihood ratio weighting for
content words produce similar results when applied
to rank all words in the input, while the cut-off
for topicality in LLR(C) does have a positive im-
pact on content selection. A closer look at the
two weighting schemes confirms that when cut-off
is not used, similar weighting of content words is
produced. The Spearman correlation coefficient be-
tween the weights for words assigned by the two
schemes is on average 0.64. At the same time, it is
likely that the weights of sentences are dominated
by only the top most highly weighted words. In
order to see to what extent the two schemes iden-
tify the same or different words as the most impor-
tant ones, we computed the overlap between the 250
most highly weighted words according to LLR and
frequency. The average overlap across the 50 sets
was quite large, 70%.
To illustrate the degree of overlap, we list below
are the most highly weighted words according to
each weighting scheme for our sample topic con-
cerning crimes across borders.
LLR drug, cocaine, traffickers, cartel, police, crime, en-
forcement, u.s., smuggling, trafficking, arrested, government,
seized, year, drugs, organised, heroin, criminal, cartels, last,
195
official, country, law, border, kilos, arrest, more, mexican, laun-
dering, officials, money, accounts, charges, authorities, cor-
ruption, anti-drug, international, banks, operations, seizures,
federal, italian, smugglers, dealers, narcotics, criminals, tons,
most, planes, customs
Frequency drug, cocaine, officials, police, more, last, gov-
ernment, year, cartel, traffickers, u.s., other, drugs, enforce-
ment, crime, money, country, arrested, federal, most, now, traf-
ficking, seized, law, years, new, charges, smuggling, being, of-
ficial, organised, international, former, authorities, only, crimi-
nal, border, people, countries, state, world, trade, first, mexican,
many, accounts, according, bank, heroin, cartels
It becomes clear that the advantage of likelihood
ratio as a weighting scheme does not come from
major differences in overall weights it assigns to
words compared to frequency. It is the signifi-
cance cut-off for the likelihood ratio that leads to
noticeable improvement (see Table 1). When this
weighting scheme is augmented by adding a score
of 1 for content words that appear in the user topic,
the summaries improve even further (LLR(CQ)).
Half of the improvement can be attributed to the
cut-off (LLR(C)), and the other half to focusing
the summary using the information from the user
query (LLR(CQ)). The advantage of likelihood ra-
tio comes from its providing a principled criterion
for deciding which words are truly descriptive of the
input and which are not. Raw frequency provides no
such cut-off.
5 Conclusions
In this paper we examined two weighting schemes
for estimating word importance that have been suc-
cessfully used in current systems but have not to-
date been directly compared. Our analysis con-
firmed that log-likelihood ratio leads to better re-
sults, but not because it defines a more accurate as-
signment of importance than raw frequency. Rather,
its power comes from the use of a known distribution
that makes it possible to determine which words are
truly descriptive of the input. Only when such words
are viewed as equally important in defining the topic
does this weighting scheme show improved perfor-
mance. Using the significance cut-off and consider-
ing all words above it equally important is key.
Log-likelihood ratio summarizer is more sensitive
to topicality or relevance and produces summaries
that are better when it take the user request into ac-
count than when it does not. This is not the case for
a summarizer based on frequency.
At the same time it is noteworthy that the generic
summarizers perform about as well as their focused
counterparts. This may be related to our discovery
that on average 57% of the sentences in the doc-
ument are relevant and that ideal query expansion
leads to a situation in which almost all sentences
in the input become relevant. These facts could
be an unplanned side-effect from the way the test
topics were produced: annotators might have been
influenced by information in the input to be sum-
marizied when defining their topic. Such observa-
tions also suggest that a competitive generic summa-
rizer would be an appropriate baseline for the topic-
focused task in future DUCs. In addition, including
some irrelavant documents in the input might make
the task more challenging and allow more room for
advances in query expansion and other summary fo-
cusing techniques.
References
J. Conroy, J. Schlesinger, and D. O?Leary. 2006. Topic-focused
multi-document summarization using an approximate oracle
score. In Proceedings of the COLING/ACL?06 (Poster Ses-
sion).
F. Lacatusu, A. Hickl, K. Roberts, Y. Shi, J. Bensley, B. Rink,
P. Wang, and L. Taylor. 2006. Lcc?s gistexter at duc 2006:
Multi-strategy multi-document summarization. In Proceed-
ings of DUC?06.
C. Lin and E. Hovy. 2000. The automated acquisition of topic
signatures for text summarization. In Proceedings of COL-
ING?00.
C. Lin. 2004. Rouge: a package for automatic evaluation of
summaries. In Proceedings of the Workshop on Text Sum-
marization Branches Out (WAS 2004).
H. P. Luhn. 1958. The automatic creation of literature abstracts.
IBM Journal of Research and Development, 2(2):159?165.
C. Manning and H. Schutze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press.
A. Nenkova, L. Vanderwende, and K. McKeown. 2006.
A compositional context sensitive multi-document summa-
rizer: Exploring the factors that influence summarization. In
Proceedings of ACM SIGIR?06.
A. Nenkova. 2005. Automatic text summarization of newswire:
lessons learned from the document understanding confer-
ence. In Proceedings of AAAI?05.
L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Microsoft
research at duc 2006: Task-focused summarization with sen-
tence simplification and lexical expansion. In Proceedings of
DUC?06.
196
Proceedings of ACL-08: HLT, pages 380?388,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Which words are hard to recognize?
Prosodic, lexical, and disfluency factors that increase ASR error rates
Sharon Goldwater, Dan Jurafsky and Christopher D. Manning
Department of Linguistics and Computer Science
Stanford University
{sgwater,jurafsky,manning}@stanford.edu
Abstract
Many factors are thought to increase the
chances of misrecognizing a word in ASR,
including low frequency, nearby disfluencies,
short duration, and being at the start of a turn.
However, few of these factors have been for-
mally examined. This paper analyzes a variety
of lexical, prosodic, and disfluency factors to
determine which are likely to increase ASR er-
ror rates. Findings include the following. (1)
For disfluencies, effects depend on the type of
disfluency: errors increase by up to 15% (ab-
solute) for words near fragments, but decrease
by up to 7.2% (absolute) for words near repeti-
tions. This decrease seems to be due to longer
word duration. (2) For prosodic features, there
are more errors for words with extreme values
than words with typical values. (3) Although
our results are based on output from a system
with speaker adaptation, speaker differences
are a major factor influencing error rates, and
the effects of features such as frequency, pitch,
and intensity may vary between speakers.
1 Introduction
In order to improve the performance of automatic
speech recognition (ASR) systems on conversational
speech, it is important to understand the factors
that cause problems in recognizing words. Previous
work on recognition of spontaneous monologues
and dialogues has shown that infrequent words are
more likely to be misrecognized (Fosler-Lussier and
Morgan, 1999; Shinozaki and Furui, 2001) and that
fast speech increases error rates (Siegler and Stern,
1995; Fosler-Lussier and Morgan, 1999; Shinozaki
and Furui, 2001). Siegler and Stern (1995) and
Shinozaki and Furui (2001) also found higher er-
ror rates in very slow speech. Word length (in
phones) has also been found to be a useful pre-
dictor of higher error rates (Shinozaki and Furui,
2001). In Hirschberg et al?s (2004) analysis of
two human-computer dialogue systems, misrecog-
nized turns were found to have (on average) higher
maximum pitch and energy than correctly recog-
nized turns. Results for speech rate were ambiguous:
faster utterances had higher error rates in one corpus,
but lower error rates in the other. Finally, Adda-
Decker and Lamel (2005) demonstrated that both
French and English ASR systems had more trouble
with male speakers than female speakers, and found
several possible explanations, including higher rates
of disfluencies and more reduction.
Many questions are left unanswered by these pre-
vious studies. In the word-level analyses of Fosler-
Lussier and Morgan (1999) and Shinozaki and Fu-
rui (2001), only substitution and deletion errors were
considered, so we do not know how including inser-
tions might affect the results. Moreover, these stud-
ies primarily analyzed lexical, rather than prosodic,
factors. Hirschberg et al?s (2004) work suggests that
prosodic factors can impact error rates, but leaves
open the question of which factors are important at
the word level and how they influence recognition
of natural conversational speech. Adda-Decker and
Lamel?s (2005) suggestion that higher rates of dis-
fluency are a cause of worse recognition for male
speakers presupposes that disfluencies raise error
rates. While this assumption seems natural, it has
yet to be carefully tested, and in particular we do not
380
know whether disfluent words are associated with
errors in adjacent words, or are simply more likely to
be misrecognized themselves. Other factors that are
often thought to affect a word?s recognition, such as
its status as a content or function word, and whether
it starts a turn, also remain unexamined.
The present study is designed to address all of
these questions by analyzing the effects of a wide
range of lexical and prosodic factors on the accu-
racy of an English ASR system for conversational
telephone speech. In the remainder of this paper, we
first describe the data set used in our study and intro-
duce a new measure of error, individual word error
rate (IWER), that allows us to include insertion er-
rors in our analysis, along with deletions and substi-
tutions. Next, we present the features we collected
for each word and the effects of those features indi-
vidually on IWER. Finally, we develop a joint sta-
tistical model to examine the effects of each feature
while controlling for possible correlations.
2 Data
For our analysis, we used the output from the
SRI/ICSI/UW RT-04 CTS system (Stolcke et al,
2006) on the NIST RT-03 development set. This sys-
tem?s performance was state-of-the-art at the time of
the 2004 evaluation. The data set contains 36 tele-
phone conversations (72 speakers, 38477 reference
words), half from the Fisher corpus and half from
the Switchboard corpus.1
The standard measure of error used in ASR is
word error rate (WER), computed as 100(I + D +
S)/R, where I,D and S are the number of inser-
tions, deletions, and substitutions found by align-
ing the ASR hypotheses with the reference tran-
scriptions, and R is the number of reference words.
Since we wish to know what features of a reference
word increase the probability of an error, we need
a way to measure the errors attributable to individ-
ual words ? an individual word error rate (IWER).
We assume that a substitution or deletion error can
be assigned to its corresponding reference word, but
for insertion errors, there may be two adjacent ref-
erence words that could be responsible. Our so-
lution is to assign any insertion errors to each of
1These conversations are not part of the standard Fisher and
Switchboard corpora used to train most ASR systems.
Ins Del Sub Total % data
Full word 1.6 6.9 10.5 19.0 94.2
Filled pause 0.6 ? 16.4 17.0 2.8
Fragment 2.3 ? 17.3 19.6 2.0
Backchannel 0.3 30.7 5.0 36.0 0.6
Guess 1.6 ? 30.6 32.1 0.4
Total 1.6 6.7 10.9 19.7 100
Table 1: Individual word error rates for different word
types, and the proportion of words belonging to each
type. Deletions of filled pauses, fragments, and guesses
are not counted as errors in the standard scoring method.
the adjacent words. We could then define IWER as
100(ni + nd + ns)/R, where ni, nd, and ns are the
insertion, deletion, and substitution counts for indi-
vidual words (with nd = D and ns = S). In general,
however, ni > I , so that the IWER for a given data
set would be larger than the WER. To facilitate com-
parisons with standard WER, we therefore discount
insertions by a factor ?, such that ?ni = I . In this
study, ? = .617.
3 Analysis of individual features
3.1 Features
The reference transcriptions used in our analysis
distinguish between five different types of words:
filled pauses (um, uh), fragments (wh-, redistr-),
backchannels (uh-huh, mm-hm), guesses (where the
transcribers were unsure of the correct words), and
full words (everything else). Error rates for each
of these types can be found in Table 1. The re-
mainder of our analysis considers only the 36159 in-
vocabulary full words in the reference transcriptions
(70 OOV full words are excluded). We collected the
following features for these words:
Speaker sex Male or female.
Broad syntactic class Open class (e.g., nouns and
verbs), closed class (e.g., prepositions and articles),
or discourse marker (e.g., okay, well). Classes were
identified using a POS tagger (Ratnaparkhi, 1996)
trained on the tagged Switchboard corpus.
Log probability The unigram log probability of
each word, as listed in the system?s language model.
Word length The length of each word (in phones),
determined using the most frequent pronunciation
381
BefRep FirRep MidRep LastRep AfRep BefFP AfFP BefFr AfFr
yeah i i i think you should um ask for the ref- recommendation
Figure 1: Example illustrating disfluency features: words occurring before and after repetitions, filled pauses, and
fragments; first, middle, and last words in a repeated sequence.
found for that word in the recognition lattices.
Position near disfluency A collection of features
indicating whether a word occurred before or after a
filled pause, fragment, or repeated word; or whether
the word itself was the first, last, or other word in a
sequence of repetitions. Figure 1 illustrates. Only
identical repeated words with no intervening words
or filled pauses were considered repetitions.
First word of turn Turn boundaries were assigned
automatically at the beginning of any utterance fol-
lowing a pause of at least 100 ms during which the
other speaker spoke.
Speech rate The average speech rate (in phones per
second) was computed for each utterance using the
pronunciation dictionary extracted from the lattices
and the utterance boundary timestamps in the refer-
ence transcriptions.
In addition to the above features, we used Praat
(Boersma and Weenink, 2007) to collect the follow-
ing additional prosodic features on a subset of the
data obtained by excluding all contractions:2
Pitch The minimum, maximum, mean, and range
of pitch for each word.
Intensity The minimum, maximum, mean, and
range of intensity for each word.
Duration The duration of each word.
31017 words (85.8% of the full-word data set) re-
main in the no-contractions data set after removing
words for which pitch and/or intensity features could
not be extracted.
2Contractions were excluded before collecting prosodic fea-
tures for the following reason. In the reference transcriptions
and alignments used for scoring ASR systems, contractions are
treated as two separate words. However, aside from speech rate,
our prosodic features were collected using word-by-word times-
tamps from a forced alignment that used a transcription where
contractions are treated as single words. Thus, the start and end
times for a contraction in the forced alignment correspond to
two words in the alignments used for scoring, and it is not clear
how to assign prosodic features appropriately to those words.
3.2 Results and discussion
Results of our analysis of individual features can be
found in Table 2 (for categorical features) and Figure
2 (for numeric features). Comparing the error rates
for the full-word and the no-contractions data sets in
Table 2 verifies that removing contractions does not
create systematic changes in the patterns of errors,
although it does lower error rates (and significance
values) slightly overall. (First and middle repetitions
are combined as non-final repetitions in the table,
because only 52 words were middle repetitions, and
their error rates were similar to initial repetitions.)
3.2.1 Disfluency features
Perhaps the most interesting result in Table 2 is
that the effects of disfluencies are highly variable de-
pending on the type of disfluency and the position
of a word relative to it. Non-final repetitions and
words next to fragments have an IWER up to 15%
(absolute) higher than the average word, while fi-
nal repetitions and words following repetitions have
an IWER up to 7.2% lower. Words occurring be-
fore repetitions or next to filled pauses do not have
significantly different error rates than words not in
those positions. Our results for repetitions support
Shriberg?s (1995) hypothesis that the final word of a
repeated sequence is in fact fluent.
3.2.2 Other categorical features
Our results support the common wisdom that
open class words have lower error rates than other
words (although the effect we find is small), and that
words at the start of a turn have higher error rates.
Also, like Adda-Decker and Lamel (2005), we find
that male speakers have higher error rates than fe-
males, though in our data set the difference is more
striking (3.6% absolute, compared to their 2.0%).
3.2.3 Word probability and word length
Turning to Figure 2, we find (consistent with pre-
vious results) that low-probability words have dra-
matically higher error rates than high-probability
382
Filled Pau. Fragment Repetition Syntactic Class Sex
Bef Aft Bef Aft Bef Aft NonF Fin Clos Open Disc 1st M F All
(a) IWER 17.6 16.9 33.8 21.6 16.7 13.8 26.0 11.6 19.7 18.0 19.6 21.2 20.6 17.0 18.8
% wds 1.7 1.7 1.6 1.5 0.7 0.9 1.2 1.1 43.8 50.5 5.8 6.2 52.5 47.5 100
(b) IWER 17.6 17.2 32.0 21.5 15.8 14.2 25.1 11.6 18.8 17.8 19.0 20.3 20.0 16.4 18.3
% wds 1.9 1.8 1.6 1.5 0.8 0.8 1.4 1.1 43.9 49.6 6.6 6.4 52.2 47.8 100
Table 2: IWER by feature and percentage of words exhibiting each feature for (a) the full-word data set and (b) the no-
contractions data set. Error rates that are significantly different for words with and without a given feature (computed
using 10,000 samples in a Monte Carlo permutation test) are in bold (p < .05) or bold italics (p < .005). Features
shown are whether a word occurs before or after a filled pause, fragment, or repetition; is a non-final or final repetition;
is open class, closed class, or a discourse marker; is the first word of a turn; or is spoken by a male or female. All is
the IWER for the entire data set. (Overall IWER is slightly lower than in Table 1 due to the removal of OOV words.)
words. More surprising is that word length in
phones does not seem to have a consistent effect on
IWER. Further analysis reveals a possible explana-
tion: word length is correlated with duration, but
anti-correlated to the same degree with log proba-
bility (the Kendall ? statistics are .50 and -.49). Fig-
ure 2 shows that words with longer duration have
lower IWER. Since words with more phones tend to
have longer duration, but lower frequency, there is
no overall effect of length.
3.2.4 Prosodic features
Figure 2 shows that means of pitch and intensity
have relatively little effect except at extreme val-
ues, where more errors occur. In contrast, pitch
and intensity range show clear linear trends, with
greater range of pitch or intensity leading to lower
IWER.3 As noted above, decreased duration is as-
sociated with increased IWER, and (as in previous
work), we find that IWER increases dramatically
for fast speech. We also see a tendency towards
higher IWER for very slow speech, consistent with
Shinozaki and Furui (2001) and Siegler and Stern
(1995). The effects of pitch minimum and maximum
are not shown for reasons of space, but are similar
to pitch mean. Also not shown are intensity mini-
mum (with more errors at higher values) and inten-
sity maximum (with more errors at lower values).
For most of our prosodic features, as well as log
probability, extreme values seem to be associated
3Our decision to use the log transform of pitch range was
originally based on the distribution of pitch range values in the
data set. Exploratory data analysis also indicated that using the
transformed values would likely lead to a better model fit (Sec-
tion 4) than using the raw values.
with worse recognition than average values. We ex-
plore this possibility further in Section 4.
4 Analysis using a joint model
In the previous section, we investigated the effects
of various individual features on ASR error rates.
However, there are many correlations between these
features ? for example, words with longer duration
are likely to have a larger range of pitch and inten-
sity. In this section, we build a single model with all
of our features as potential predictors in order to de-
termine the effects of each feature after controlling
for the others. We use the no-contractions data set so
that we can include prosodic features in our model.
Since only 1% of tokens have an IWER > 1, we
simplify modeling by predicting only whether each
token is responsible for an error or not. That is, our
dependent variable is binary, taking on the value 1 if
IWER > 0 for a given token and 0 otherwise.
4.1 Model
To model data with a binary dependent variable, a
logistic regression model is an appropriate choice.
In logistic regression, we model the log odds as a
linear combination of feature values x0 . . . xn:
log p1 ? p = ?0x0 + ?1x1 + . . . + ?nxn
where p is the probability that the outcome occurs
(here, that a word is misrecognized) and ?0 . . . ?n
are coefficients (feature weights) to be estimated.
Standard logistic regression models assume that all
categorical features are fixed effects, meaning that
all possible values for these features are known in
advance, and each value may have an arbitrarily dif-
ferent effect on the outcome. However, features
383
2 4 6 8 10
0
20
40
Word length (phones)
IW
ER
100 200 300
0
20
40
Pitch mean (Hz)
50 60 70 80
0
20
40
Intensity mean (dB)
0.0 0.2 0.4 0.6 0.8 1.0
0
20
40
Duration (sec)
?5 ?4 ?3 ?2
0
20
40
Log probability
IW
ER
1 2 3 4 5
0
20
40
log(Pitch range) (Hz)
IW
ER
10 30 50
0
20
40
Intensity range (dB)
5 10 15 20
0
20
40
Speech rate (phones/sec)
Figure 2: Effects of numeric features on IWER of the SRI system for the no-contractions data set. All feature values
were binned, and the average IWER for each bin is plotted, with the area of the surrounding circle proportional to the
number of points in the bin. Dotted lines show the average IWER over the entire data set.
such as speaker identity do not fit this pattern. In-
stead, we control for speaker differences by assum-
ing that speaker identity is a random effect, mean-
ing that the speakers observed in the data are a ran-
dom sample from a larger population. The base-
line probability of error for each speaker is therefore
assumed to be a normally distributed random vari-
able, with mean equal to the population mean, and
variance to be estimated by the model. Stated dif-
ferently, a random effect allows us to add a factor
to the model for speaker identity, without allowing
arbitrary variation in error rates between speakers.
Models such as ours, with both fixed and random
effects, are known as mixed-effects models, and are
becoming a standard method for analyzing linguis-
tic data (Baayen, 2008). We fit our models using the
lme4 package (Bates, 2007) of R (R Development
Core Team, 2007).
To analyze the joint effects of all of our features,
we initially built as large a model as possible, and
used backwards elimination to remove features one
at a time whose presence did not contribute signifi-
cantly (at p ? .05) to model fit. All of the features
shown in Table 2 were converted to binary variables
and included as predictors in our initial model, along
with a binary feature controlling for corpus (Fisher
or Switchboard), and all numeric features in Figure
2. We did not include minimum and maximum val-
ues for pitch and intensity because they are highly
correlated with the mean values, making parameter
estimation in the combined model difficult. Prelimi-
nary investigation indicated that using the mean val-
ues would lead to the best overall fit to the data.
In addition to these basic fixed effects, our ini-
tial model included quadratic terms for all of the nu-
meric features, as suggested by our analysis in Sec-
tion 3, as well as random effects for speaker iden-
tity and word identity. All numeric features were
rescaled to values between 0 and 1 so that coeffi-
cients are comparable.
4.2 Results and discussion
Figure 3 shows the estimated coefficients and stan-
dard errors for each of the fixed effect categorical
features remaining in the reduced model (i.e., after
backwards elimination). Since all of the features are
binary, a coefficient of ? indicates that the corre-
sponding feature, when present, adds a weight of ?
to the log odds (i.e., multiplies the odds of an error
by a factor of e?). Thus, features with positive co-
efficients increase the odds of an error, and features
with negative coefficients decrease the odds of an er-
ror. The magnitude of the coefficient corresponds to
the size of the effect.
Interpreting the coefficients for our numeric fea-
tures is less intuitive, since most of these variables
have both linear and quadratic effects. The contribu-
tion to the log odds of a particular numeric feature
384
?1.5 ?1.0 ?0.5 0.0 0.5 1.0
corpus=SW
sex=M
starts turn
before FP
after FP
before frag
after frag
non?final rep
open class
Figure 3: Estimates and standard errors of the coefficients
for the categorical predictors in the reduced model.
xi, with linear and quadratic coefficients a and b, is
axi + bx2i . We plot these curves for each numeric
feature in Figure 4. Values on the x axes with posi-
tive y values indicate increased odds of an error, and
negative y values indicate decreased odds of an er-
ror. The x axes in these plots reflect the rescaled
values of each feature, so that 0 corresponds to the
minimum value in the data set, and 1 to the maxi-
mum value.
4.2.1 Disfluencies
In our analysis of individual features, we found
that different types of disfluencies have different ef-
fects: non-final repeated words and words near frag-
ments have higher error rates, while final repetitions
and words following repetitions have lower error
rates. After controlling for other factors, a differ-
ent picture emerges. There is no longer an effect for
final repetitions or words after repetitions; all other
disfluency features increase the odds of an error by
a factor of 1.3 to 2.9. These differences from Sec-
tion 3 can be explained by noting that words near
filled pauses and repetitions have longer durations
than other words (Bell et al, 2003). Longer duration
lowers IWER, so controlling for duration reveals the
negative effect of the nearby disfluencies. Our re-
sults are also consistent with Shriberg?s (1995) find-
ings on fluency in repeated words, since final rep-
etitions have no significant effect in our combined
model, while non-final repetitions incur a penalty.
4.2.2 Other categorical features
Without controlling for other lexical or prosodic
features, we found that a word is more likely to
be misrecognized at the beginning of a turn, and
less likely to be misrecognized if it is an open class
word. According to our joint model, these effects
still hold even after controlling for other features.
Similarly, male speakers still have higher error rates
than females. This last result sheds some light on
the work of Adda-Decker and Lamel (2005), who
suggested several factors that could explain males?
higher error rates. In particular, they showed that
males have higher rates of disfluency, produce words
with slightly shorter durations, and use more alter-
nate (?sloppy?) pronunciations. Our joint model
controls for the first two of these factors, suggesting
that the third factor or some other explanation must
account for the remaining differences between males
and females. One possibility is that female speech is
more easily recognized because females tend to have
expanded vowel spaces (Diehl et al, 1996), a factor
that is associated with greater intelligibility (Brad-
low et al, 1996) and is characteristic of genres with
lower ASR error rates (Nakamura et al, 2008).
4.2.3 Prosodic features
Examining the effects of pitch and intensity indi-
vidually, we found that increased range for these fea-
tures is associated with lower IWER, while higher
pitch and extremes of intensity are associated with
higher IWER. In the joint model, we see the same
effect of pitch mean and an even stronger effect for
intensity, with the predicted odds of an error dra-
matically higher for extreme intensity values. Mean-
while, we no longer see a benefit for increased pitch
range and intensity; rather, we see small quadratic
effects for both features, i.e. words with average
ranges of pitch and intensity are recognized more
easily than words with extreme values for these fea-
tures. As with disfluencies, we hypothesize that the
linear trends observed in Section 3 are primarily due
to effects of duration, since duration is moderately
correlated with both log pitch range (? = .35) and
intensity range (? = .41).
Our final two prosodic features, duration and
speech rate, showed strong linear and weak
quadratic trends when analyzed individually. Ac-
cording to our model, both duration and speech rate
are still important predictors of error after control-
ling for other features. However, as with the other
prosodic features, predictions of the joint model are
dominated by quadratic trends, i.e., predicted error
rates are lower for average values of duration and
speech rate than for extreme values.
Overall, the results from our joint analysis suggest
385
0.0 0.4 0.8
?
4
0
4
Word length
lo
g 
od
ds
y = ?0.8x
0.0 0.4 0.8
?
4
0
4
Pitch mean
lo
g 
od
ds
y = 1x
0.0 0.4 0.8
?
4
0
4
Intensity mean
lo
g 
od
ds
y = ?13.2x + 11.5x2
0.0 0.4 0.8
?
4
0
4
Duration
lo
g 
od
ds
y = ?12.6x + 14.6x2
0.0 0.4 0.8
?
4
0
4
Log probability
lo
g 
od
ds
y = ?0.6x + 4.1x2
0.0 0.4 0.8
?
4
0
4
log(Pitch range)
lo
g 
od
ds
y = ?2.3x + 2.2x2
0.0 0.4 0.8
?
4
0
4
Intensity range
lo
g 
od
ds
y = ?1x + 1.2x2
0.0 0.4 0.8
?
4
0
4
Speech rate
lo
g 
od
ds
y = ?3.9x + 4.4x2
Figure 4: Predicted effect on the log odds of each numeric feature, including linear and (if applicable) quadratic terms.
Model Neg. log lik. Diff. df
Full 12932 0 32
Reduced 12935 3 26
No lexical 13203 271 16
No prosodic 13387 455 20
No speaker 13432 500 31
No word 13267 335 31
Baseline 14691 1759 1
Table 3: Fit to the data of various models. Degrees of
freedom (df) for each model is the number of fixed ef-
fects plus the number of random effects plus 1 (for the
intercept). Full model contains all predictors; Reduced
contains only predictors contributing significantly to fit;
Baseline contains only intercept. Other models are ob-
tained by removing features from Full. Diff is the differ-
ence in log likelihood between each model and Full.
that, after controlling for other factors, extreme val-
ues for prosodic features are associated with worse
recognition than typical values.
4.2.4 Differences between lexical items
As discussed above, our model contains a random
effect for word identity, to control for the possibil-
ity that certain lexical items have higher error rates
that are not explained by any of the other factors
in the model. It is worth asking whether this ran-
dom effect is really necessary. To address this ques-
tion, we compared the fit to the data of two models,
each containing all of our fixed effects and a ran-
dom effect for speaker identity. One model also con-
tained a random effect for word identity. Results are
shown in Table 3. The model without a random ef-
fect for word identity is significantly worse than the
full model; in fact, this single parameter is more im-
portant than all of the lexical features combined. To
see which lexical items are causing the most diffi-
culty, we examined the items with the highest esti-
mated increases in error. The top 20 items on this
list include yup, yep, yes, buy, then, than, and r., all
of which are acoustically similar to each other or to
other high-frequency words, as well as the words af-
ter, since, now, and though, which occur in many
syntactic contexts, making them difficult to predict
based on the language model.
4.2.5 Differences between speakers
We examined the importance of the random effect
for speaker identity in a similar fashion to the ef-
fect for word identity. As shown in Table 3, speaker
identity is a very important factor in determining the
probability of error. That is, the lexical and prosodic
variables examined here are not sufficient to fully
explain the differences in error rates between speak-
ers. In fact, the speaker effect is the single most im-
portant factor in the model.
Given that the differences in error rates between
speakers are so large (average IWER for different
speakers ranges from 5% to 51%), we wondered
whether our model is sufficient to capture the kinds
of speaker variation that exist. The model assumes
that each speaker has a different baseline error rate,
but that the effects of each variable are the same for
each speaker. Determining the extent to which this
assumption is justified is beyond the scope of this
paper, however we present some suggestive results
in Figure 5. This figure illustrates some of the dif-
386
40 60 80
0.
0
0.
2
0.
4
Intensity mean (dB)
Fi
tte
d 
P(
err
)
100 250 400
0.
0
0.
2
0.
4
Pitch mean (Hz)
0.0 0.5 1.0 1.5
0.
0
0.
2
0.
4
Duration (sec)
?6 ?5 ?4 ?3 ?2
0.
0
0.
2
0.
4
Neg. log prob.
0 5 10 20
0.
0
0.
2
0.
4
Sp. rate (ph/sec)
40 60 80
0.
0
0.
2
0.
4
Intensity mean (dB)
Fi
tte
d 
P(
err
)
100 250 400
0.
0
0.
2
0.
4
Pitch mean (Hz)
0.0 0.5 1.0 1.5
0.
0
0.
2
0.
4
Duration (sec)
?6 ?5 ?4 ?3 ?2
0.
0
0.
2
0.
4
Neg. log prob.
0 5 10 20
0.
0
0.
2
0.
4
Sp. rate (ph/sec)
Figure 5: Estimated effects of various features on the error rates of two different speakers (top and bottom). Dashed
lines illustrate the baseline probability of error for each speaker. Solid lines were obtained by fitting a logistic regres-
sion model to each speaker?s data, with the variable labeled on the x-axis as the only predictor.
ferences between two speakers chosen fairly arbi-
trarily from our data set. Not only are the baseline
error rates different for the two speakers, but the ef-
fects of various features appear to be very different,
in one case even reversed. The rest of our data set
exhibits similar kinds of variability for many of the
features we examined. These differences in ASR be-
havior between speakers are particularly interesting
considering that the system we investigated here al-
ready incorporates speaker adaptation models.
5 Conclusion
In this paper, we introduced the individual word er-
ror rate (IWER) for measuring ASR performance
on individual words, including insertions as well as
deletions and substitutions. Using IWER, we ana-
lyzed the effects of various word-level lexical and
prosodic features, both individually and in a joint
model. Our analysis revealed the following effects.
(1) Words at the start of a turn have slightly higher
IWER than average, and open class (content) words
have slightly lower IWER. These effects persist even
after controlling for other lexical and prosodic fac-
tors. (2) Disfluencies heavily impact error rates:
IWER for non-final repetitions and words adjacent
to fragments rises by up to 15% absolute, while
IWER for final repetitions and words following rep-
etitions decreases by up to 7.2% absolute. Control-
ling for prosodic features eliminates the latter ben-
efit, and reveals a negative effect of adjacent filled
pauses, suggesting that the effects of these disfluen-
cies are normally obscured by the greater duration of
nearby words. (3) For most acoustic-prosodic fea-
tures, words with extreme values have worse recog-
nition than words with average values. This effect
becomes much more pronounced after controlling
for other factors. (4) After controlling for lexical
and prosodic characteristics, the lexical items with
the highest error rates are primarily homophones or
near-homophones (e.g., buy vs. by, then vs. than).
(5) Speaker differences account for much of the vari-
ance in error rates between words. Moreover, the di-
rection and strength of effects of different prosodic
features may vary between speakers.
While we plan to extend our analysis to other
ASR systems in order to determine the generality
of our findings, we have already gained important
insights into a number of factors that increase ASR
error rates. In addition, our results suggest a rich
area for future research in further analyzing the vari-
ability of both lexical and prosodic effects on ASR
behavior for different speakers.
Acknowledgments
This work was supported by the Edinburgh-Stanford
LINK and ONR MURI award N000140510388. We
thank Andreas Stolcke for providing the ASR out-
put, language model, and forced alignments used
here, and Raghunandan Kumaran and Katrin Kirch-
hoff for earlier datasets and additional help.
387
References
M. Adda-Decker and L. Lamel. 2005. Do speech rec-
ognizers prefer female speakers? In Proceedings of
INTERSPEECH, pages 2205?2208.
R. H. Baayen. 2008. Analyzing Linguistic Data. A
Practical Introduction to Statistics. Cambridge
University Press. Prepublication version available at
http://www.mpi.nl/world/persons/private/baayen/pub-
lications.html.
Douglas Bates, 2007. lme4: Linear mixed-effects models
using S4 classes. R package version 0.99875-8.
A. Bell, D. Jurafsky, E. Fosler-Lussier, C. Girand,
M. Gregory, and D. Gildea. 2003. Effects of disflu-
encies, predictability, and utterance position on word
form variation in English conversation. Journal of the
Acoustical Society of America, 113(2):1001?1024.
P. Boersma and D. Weenink. 2007. Praat:
doing phonetics by computer (version 4.5.16).
http://www.praat.org/.
A. Bradlow, G. Torretta, and D. Pisoni. 1996. Intelli-
gibility of normal speech I: Global and fine-grained
acoustic-phonetic talker characteristics. Speech Com-
munication, 20:255?272.
R. Diehl, B. Lindblom, K. Hoemeke, and R. Fahey. 1996.
On explaining certain male-female differences in the
phonetic realization of vowel categories. Journal of
Phonetics, 24:187?208.
E. Fosler-Lussier and N. Morgan. 1999. Effects of
speaking rate and word frequency on pronunciations
in conversational speech. Speech Communication,
29:137? 158.
J. Hirschberg, D. Litman, and M. Swerts. 2004. Prosodic
and other cues to speech recognition failures. Speech
Communication, 43:155? 175.
M. Nakamura, K. Iwano, and S. Furui. 2008. Differ-
ences between acoustic characteristics of spontaneous
and read speech and their effects on speech recogni-
tion performance. Computer Speech and Language,
22:171? 184.
R Development Core Team, 2007. R: A Language and
Environment for Statistical Computing. R Foundation
for Statistical Computing, Vienna, Austria. ISBN 3-
900051-07-0.
A. Ratnaparkhi. 1996. A Maximum Entropy model for
part-of-speech tagging. In Proceedings of the First
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 133?142.
T. Shinozaki and S. Furui. 2001. Error analysis using de-
cision trees in spontaneous presentation speech recog-
nition. In Proceedings of ASRU 2001.
E. Shriberg. 1995. Acoustic properties of disfluent rep-
etitions. In Proceedings of the International Congress
of Phonetic Sciences, volume 4, pages 384?387.
M. Siegler and R. Stern. 1995. On the effects of speech
rate in large vocabulary speech recognition systems.
In Proceedings of ICASSP.
A. Stolcke, B. Chen, H. Franco, V. R. R. Gadde, M. Gra-
ciarena, M.-Y. Hwang, K. Kirchhoff, A. Mandal,
N. Morgan, X. Lin, T. Ng, M. Ostendorf, K. Sonmez,
A. Venkataraman, D. Vergyri, W. Wang, J. Zheng, and
Q. Zhu. 2006. Recent innovations in speech-to-text
transcription at SRI-ICSI-UW. IEEE Transactions on
Audio, Speech and Language Processing, 14(5):1729?
1744.
388
Proceedings of ACL-08: HLT, pages 789?797,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unsupervised Learning of Narrative Event Chains
Nathanael Chambers and Dan Jurafsky
Department of Computer Science
Stanford University
Stanford, CA 94305
{natec,jurafsky}@stanford.edu
Abstract
Hand-coded scripts were used in the 1970-80s
as knowledge backbones that enabled infer-
ence and other NLP tasks requiring deep se-
mantic knowledge. We propose unsupervised
induction of similar schemata called narrative
event chains from raw newswire text.
A narrative event chain is a partially ordered
set of events related by a common protago-
nist. We describe a three step process to learn-
ing narrative event chains. The first uses unsu-
pervised distributional methods to learn narra-
tive relations between events sharing corefer-
ring arguments. The second applies a tempo-
ral classifier to partially order the connected
events. Finally, the third prunes and clusters
self-contained chains from the space of events.
We introduce two evaluations: the narrative
cloze to evaluate event relatedness, and an or-
der coherence task to evaluate narrative order.
We show a 36% improvement over baseline
for narrative prediction and 25% for temporal
coherence.
1 Introduction
This paper induces a new representation of struc-
tured knowledge called narrative event chains (or
narrative chains). Narrative chains are partially or-
dered sets of events centered around a common pro-
tagonist. They are related to structured sequences of
participants and events that have been called scripts
(Schank and Abelson, 1977) or Fillmorean frames.
These participants and events can be filled in and
instantiated in a particular text situation to draw in-
ferences. Chains focus on a single actor to facili-
tate learning, and thus this paper addresses the three
tasks of chain induction: narrative event induction,
temporal ordering of events and structured selection
(pruning the event space into discrete sets).
Learning these prototypical schematic sequences
of events is important for rich understanding of text.
Scripts were central to natural language understand-
ing research in the 1970s and 1980s for proposed
tasks such as summarization, coreference resolu-
tion and question answering. For example, Schank
and Abelson (1977) proposed that understanding
text about restaurants required knowledge about the
Restaurant Script, including the participants (Cus-
tomer, Waiter, Cook, Tables, etc.), the events consti-
tuting the script (entering, sitting down, asking for
menus, etc.), and the various preconditions, order-
ing, and results of each of the constituent actions.
Consider these two distinct narrative chains.
accused X W joined
X claimed W served
X argued W oversaw
dismissed X W resigned
It would be useful for question answering or tex-
tual entailment to know that ?X denied ? is also a
likely event in the left chain, while ? replaces W?
temporally follows the right. Narrative chains (such
as Firing of Employee or Executive Resigns) offer
the structure and power to directly infer these new
subevents by providing critical background knowl-
edge. In part due to its complexity, automatic in-
duction has not been addressed since the early non-
statistical work of Mooney and DeJong (1985).
The first step to narrative induction uses an entity-
based model for learning narrative relations by fol-
789
lowing a protagonist. As a narrative progresses
through a series of events, each event is character-
ized by the grammatical role played by the protag-
onist, and by the protagonist?s shared connection to
surrounding events. Our algorithm is an unsuper-
vised distributional learning approach that uses core-
ferring arguments as evidence of a narrative relation.
We show, using a new evaluation task called narra-
tive cloze, that our protagonist-based method leads
to better induction than a verb-only approach.
The next step is to order events in the same nar-
rative chain. We apply work in the area of temporal
classification to create partial orders of our learned
events. We show, using a coherence-based evalua-
tion of temporal ordering, that our partial orders lead
to better coherence judgements of real narrative in-
stances extracted from documents.
Finally, the space of narrative events and temporal
orders is clustered and pruned to create discrete sets
of narrative chains.
2 Previous Work
While previous work hasn?t focused specifically on
learning narratives1, our work draws from two lines
of research in summarization and anaphora resolu-
tion. In summarization, topic signatures are a set
of terms indicative of a topic (Lin and Hovy, 2000).
They are extracted from hand-sorted (by topic) sets
of documents using log-likelihood ratios. These
terms can capture some narrative relations, but the
model requires topic-sorted training data.
Bean and Riloff (2004) proposed the use of
caseframe networks as a kind of contextual role
knoweldge for anaphora resolution. A case-
frame is a verb/event and a semantic role (e.g.
<patient> kidnapped). Caseframe networks are re-
lations between caseframes that may represent syn-
onymy (<patient> kidnapped and <patient> ab-
ducted) or related events (<patient> kidnapped and
<patient> released). Bean and Riloff learn these
networks from two topic-specific texts and apply
them to the problem of anaphora resolution. Our
work can be seen as an attempt to generalize the in-
tuition of caseframes (finding an entire set of events
1We analyzed FrameNet (Baker et al, 1998) for insight, but
found that very few of the frames are event sequences of the
type characterizing narratives and scripts.
rather than just pairs of related frames) and apply it
to a different task (finding a coherent structured nar-
rative in non-topic-specific text).
More recently, Brody (2007) proposed an ap-
proach similar to caseframes that discovers high-
level relatedness between verbs by grouping verbs
that share the same lexical items in subject/object
positions. He calls these shared arguments anchors.
Brody learns pairwise relations between clusters of
related verbs, similar to the results with caseframes.
A human evaluation of these pairs shows an im-
provement over baseline. This and previous case-
frame work lend credence to learning relations from
verbs with common arguments.
We also draw from lexical chains (Morris and
Hirst, 1991), indicators of text coherence from word
overlap/similarity. We use a related notion of protag-
onist overlap to motivate narrative chain learning.
Work on semantic similarity learning such as
Chklovski and Pantel (2004) also automatically
learns relations between verbs. We use similar dis-
tributional scoring metrics, but differ with our use
of a protagonist as the indicator of relatedness. We
also use typed dependencies and the entire space of
events for similarity judgements, rather than only
pairwise lexical decisions.
Finally, Fujiki et al (2003) investigated script ac-
quisition by extracting the 41 most frequent pairs of
events from the first paragraph of newswire articles,
using the assumption that the paragraph?s textual or-
der follows temporal order. Our model, by contrast,
learns entire event chains, uses more sophisticated
probabilistic measures, and uses temporal ordering
models instead of relying on document order.
3 The Narrative Chain Model
3.1 Definition
Our model is inspired by Centering (Grosz et al,
1995) and other entity-based models of coherence
(Barzilay and Lapata, 2005) in which an entity is in
focus through a sequence of sentences. We propose
to use this same intuition to induce narrative chains.
We assume that although a narrative has several
participants, there is a central actor who character-
izes a narrative chain: the protagonist. Narrative
chains are thus structured by the protagonist?s gram-
matical roles in the events. In addition, narrative
790
events are ordered by some theory of time. This pa-
per describes a partial ordering with the before (no
overlap) relation.
Our task, therefore, is to learn events that consti-
tute narrative chains. Formally, a narrative chain
is a partially ordered set of narrative events that
share a common actor. A narrative event is a tu-
ple of an event (most simply a verb) and its par-
ticipants, represented as typed dependencies. Since
we are focusing on a single actor in this study, a
narrative event is thus a tuple of the event and the
typed dependency of the protagonist: (event, depen-
dency). A narrative chain is a set of narrative events
{e1, e2, ..., en}, where n is the size of the chain, and
a relation B(ei, ej) that is true if narrative event ei
occurs strictly before ej in time.
3.2 The Protagonist
The notion of a protagonist motivates our approach
to narrative learning. We make the following as-
sumption of narrative coherence: verbs sharing
coreferring arguments are semantically connected
by virtue of narrative discourse structure. A single
document may contain more than one narrative (or
topic), but the narrative assumption states that a se-
ries of argument-sharing verbs is more likely to par-
ticipate in a narrative chain than those not sharing.
In addition, the narrative approach captures gram-
matical constraints on narrative coherence. Simple
distributional learning might discover that the verb
push is related to the verb fall, but narrative learning
can capture additional facts about the participants,
specifically, that the object or patient of the push is
the subject or agent of the fall.
Each focused protagonist chain offers one per-
spective on a narrative, similar to the multiple per-
spectives on a commercial transaction event offered
by buy and sell.
3.3 Partial Ordering
A narrative chain, by definition, includes a partial
ordering of events. Early work on scripts included
ordering constraints with more complex precondi-
tions and side effects on the sequence of events. This
paper presents work toward a partial ordering and
leaves logical constraints as future work. We focus
on the before relation, but the model does not pre-
clude advanced theories of temporal order.
4 Learning Narrative Relations
Our first model learns basic information about a
narrative chain: the protagonist and the constituent
subevents, although not their ordering. For this we
need a metric for the relation between an event and
a narrative chain.
Pairwise relations between events are first ex-
tracted unsupervised. A distributional score based
on how often two events share grammatical argu-
ments (using pointwise mutual information) is used
to create this pairwise relation. Finally, a global nar-
rative score is built such that all events in the chain
provide feedback on the event in question (whether
for inclusion or for decisions of inference).
Given a list of observed verb/dependency counts,
we approximate the pointwise mutual information
(PMI) by:
pmi(e(w, d), e(v, g)) = log
P (e(w, d), e(v, g))
P (e(w, d))P (e(v, g))
(1)
where e(w, d) is the verb/dependency pair w and d
(e.g. e(push,subject)). The numerator is defined by:
P (e(w, d), e(v, g)) =
C(e(w, d), e(v, g))
?
x,y
?
d,f C(e(x, d), e(y, f))
(2)
where C(e(x, d), e(y, f)) is the number of times the
two events e(x, d) and e(y, f) had a coreferring en-
tity filling the values of the dependencies d and f .
We also adopt the ?discount score? to penalize low
occuring words (Pantel and Ravichandran, 2004).
Given the debate over appropriate metrics for dis-
tributional learning, we also experimented with the
t-test. Our experiments found that PMI outperforms
the t-test on this task by itself and when interpolated
together using various mixture weights.
Once pairwise relation scores are calculated, a
global narrative score can then be built such that all
events provide feedback on the event in question.
For instance, given all narrative events in a docu-
ment, we can find the next most likely event to occur
by maximizing:
max
j:0<j<m
n?
i=0
pmi(ei, fj) (3)
where n is the number of events in our chain and
ei is the ith event. m is the number of events f in
our training corpus. A ranked list of guesses can be
built from this summation and we hypothesize that
791
Known events:
(pleaded subj), (admits subj), (convicted obj)
Likely Events:
sentenced obj 0.89 indicted obj 0.74
paroled obj 0.76 fined obj 0.73
fired obj 0.75 denied subj 0.73
Figure 1: Three narrative events and the six most likely
events to include in the same chain.
the more events in our chain, the more informed our
ranked output. An example of a chain with 3 events
and the top 6 ranked guesses is given in figure 1.
4.1 Evaluation Metric: Narrative Cloze
The cloze task (Taylor, 1953) is used to evaluate a
system (or human) for language proficiency by re-
moving a random word from a sentence and having
the system attempt to fill in the blank (e.g. I forgot
to the waitress for the good service). Depend-
ing on the type of word removed, the test can evalu-
ate syntactic knowledge as well as semantic. Deyes
(1984) proposed an extended task, discourse cloze,
to evaluate discourse knowledge (removing phrases
that are recoverable from knowledge of discourse re-
lations like contrast and consequence).
We present a new cloze task that requires narra-
tive knowledge to solve, the narrative cloze. The
narrative cloze is a sequence of narrative events in a
document from which one event has been removed.
The task is to predict the missing verb and typed de-
pendency. Take this example text about American
football with McCann as the protagonist:
1. McCann threw two interceptions early.
2. Toledo pulled McCann aside and told him he?d start.
3. McCann quickly completed his first two passes.
These clauses are represented in the narrative model
as five events: (threw subject), (pulled object),
(told object), (start subject), (completed subject).
These verb/dependency events make up a narrative
cloze model. We could remove (threw subject) and
use the remaining four events to rank this missing
event. Removing a single such pair to be filled in au-
tomatically allows us to evaluate a system?s knowl-
edge of narrative relations and coherence. We do not
claim this cloze task to be solvable even by humans,
New York Times Editorial
occupied subj brought subj rejecting subj
projects subj met subj appeared subj
offered subj voted pp for offer subj
thinks subj
Figure 2: One of the 69 test documents, containing 10
narrative events. The protagonist is President Bush.
but rather assert it as a comparative measure to eval-
uate narrative knowledge.
4.2 Narrative Cloze Experiment
We use years 1994-2004 (1,007,227 documents) of
the Gigaword Corpus (Graff, 2002) for training2.
We parse the text into typed dependency graphs
with the Stanford Parser (de Marneffe et al, 2006)3,
recording all verbs with subject, object, or preposi-
tional typed dependencies. We use the OpenNLP4
coreference engine to resolve the entity mentions.
For each document, the verb pairs that share core-
ferring entities are recorded with their dependency
types. Particles are included with the verb.
We used 10 news stories from the 1994 section
of the corpus for development. The stories were
hand chosen to represent a range of topics such as
business, sports, politics, and obituaries. We used
69 news stories from the 2001 (year selected ran-
domly) section of the corpus for testing (also re-
moved from training). The test set documents were
randomly chosen and not preselected for a range of
topics. From each document, the entity involved
in the most events was selected as the protagonist.
For this evaluation, we only look at verbs. All
verb clauses involving the protagonist are manu-
ally extracted and translated into the narrative events
(verb,dependency). Exceptions that are not included
are verbs in headlines, quotations (typically not part
of a narrative), ?be? properties (e.g. john is happy),
modifying verbs (e.g. hurried to leave, only leave is
used), and multiple instances of one event.
The original test set included 100 documents, but
2The document count does not include duplicate news sto-
ries. We found up to 18% of the corpus are duplications, mostly
AP reprints. We automatically found these by matching the first
two paragraphs of each document, removing exact matches.
3http://nlp.stanford.edu/software/lex-parser.shtml
4http://opennlp.sourceforge.net
792
those without a narrative chain at least five events in
length were removed, leaving 69 documents. Most
of the removed documents were not stories, but gen-
res such as interviews and cooking recipes. An ex-
ample of an extracted chain is shown in figure 2.
We evalute with Narrative Cloze using leave-one-
out cross validation, removing one event and using
the rest to generate a ranked list of guesses. The test
dataset produces 740 cloze tests (69 narratives with
740 events). After generating our ranked guesses,
the position of the correct event is averaged over all
740 tests for the final score. We penalize unseen
events by setting their ranked position to the length
of the guess list (ranging from 2k to 15k).
Figure 1 is an example of a ranked guess list for a
short chain of three events. If the original document
contained (fired obj), this cloze test would score 3.
4.2.1 Baseline
We want to measure the utility of the protago-
nist and the narrative coherence assumption, so our
baseline learns relatedness strictly based upon verb
co-occurence. The PMI is then defined as between
all occurrences of two verbs in the same document.
This baseline evaluation is verb only, as dependen-
cies require a protagonist to fill them.
After initial evaluations, the baseline was per-
forming very poorly due to the huge amount of data
involved in counting all possible verb pairs (using a
protagonist vastly reduces the number). We exper-
imented with various count cutoffs to remove rare
occurring pairs of verbs. The final results use a base-
line where all pairs occurring less than 10 times in
the training data are removed.
Since the verb-only baseline does not use typed
dependencies, our narrative model cannot directly
compare to this abstracted approach. We thus mod-
ified the narrative model to ignore typed dependen-
cies, but still count events with shared arguments.
Thus, we calculate the PMI across verbs that share
arguments. This approach is called Protagonist.
The full narrative model that includes the grammat-
ical dependencies is called Typed Deps.
4.2.2 Results
Experiments with varying sizes of training data
are presented in figure 3. Each ranked list of
candidate verbs for the missing event in Base-
1995 1996 1997 1998 1999 2000 2001 2002 2003 20040
500
1000
1500
2000
2500
3000
Training Data from 1994?X
Ran
ked
 Po
sitio
n
Narrative Cloze Test
 
  Baseline Protagonist Typed Deps
Figure 3: Results with varying sizes of training data. Year
2003 is not explicitly shown because it has an unusually
small number of documents compared to other years.
line/Protagonist contained approximately 9 thou-
sand candidates. Of the 740 cloze tests, 714 of the
removed events were present in their respective list
of guesses. This is encouraging as only 3.5% of the
events are unseen (or do not meet cutoff thresholds).
When all training data is used (1994-2004), the
average ranked position is 1826 for Baseline and
1160 for Protagonist (1 being most confident). The
Baseline performs better at first (years 1994-5), but
as more data is seen, the Baseline worsens while
the Protagonist improves. This verb-only narrative
model shows a 36.5% improvement over the base-
line trained on all years. Results from the full Typed
Deps model, not comparable to the baseline, paral-
lel the Protagonist results, improving as more data is
seen (average ranked position of 1908 with all the
training data). We also ran the experiment with-
out OpenNLP coreference, and instead used exact
and substring matching for coreference resolution.
This showed a 5.7% decrease in the verb-only re-
sults. These results show that a protagonist greatly
assists in narrative judgements.
5 Ordering Narrative Events
The model proposed in the previous section is de-
signed to learn the major subevents in a narrative
chain, but not how these events are ordered. In this
section we extend the model to learn a partial tem-
poral ordering of the events.
793
There are a number of algorithms for determining
the temporal relationship between two events (Mani
et al, 2006; Lapata and Lascarides, 2006; Cham-
bers et al, 2007), many of them trained on the Time-
Bank Corpus (Pustejovsky et al, 2003) which codes
events and their temporal relationships. The cur-
rently highest performing of these on raw data is the
model of temporal labeling described in our previ-
ous work (Chambers et al, 2007). Other approaches
have depended on hand tagged features.
Chambers et al (2007) shows 59.4% accuracy on
the classification task for six possible relations be-
tween pairs of events: before, immediately-before,
included-by, simultaneous, begins and ends. We fo-
cus on the before relation because the others are
less relevant to our immediate task. We combine
immediately-before with before, and merge the other
four relations into an other category. At the binary
task of determining if one event is before or other,
we achieve 72.1% accuracy on Timebank.
The above approach is a two-stage machine learn-
ing architecture. In the first stage, the model uses
supervised machine learning to label temporal at-
tributes of events, including tense, grammatical as-
pect, and aspectual class. This first stage classi-
fier relies on features such as neighboring part of
speech tags, neighboring auxiliaries and modals, and
WordNet synsets. We use SVMs (Chambers et al
(2007) uses Naive Bayes) and see minor perfor-
mance boosts on Timebank. These imperfect clas-
sifications, combined with other linguistic features,
are then used in a second stage to classify the tem-
poral relationship between two events. Other fea-
tures include event-event syntactic properties such
as the syntactic dominance relations between the
two events, as well as new bigram features of tense,
aspect and class (e.g. ?present past? if the first event
is in the present, and the second past), and whether
the events occur in the same or different sentences.
5.1 Training a Temporal Classifier
We use the entire Timebank Corpus as super-
vised training data, condensing the before and
immediately-before relations into one before rela-
tion. The remaining relations are merged into other.
The vast majority of potential event pairs in Time-
bank are unlabeled. These are often none relations
(events that have no explicit relation) or as is of-
ten the case, overlap relations where the two events
have no Timebank-defined ordering but overlap in
time. Even worse, many events do have an order-
ing, but they were not tagged by the human annota-
tors. This could be due to the overwhelming task of
temporal annotation, or simply because some event
orderings are deemed more important than others in
understanding the document. We consider all un-
tagged relations as other, and experiment with in-
cluding none, half, and all of them in training.
Taking a cue from Mani et al (2006), we also
increased Timebank?s size by applying transitivity
rules to the hand labeled data. The following is an
example of the applied transitive rule:
if run BEFORE fall and fall BEFORE injured
then run BEFORE injured
This increases the number of relations from 37519
to 45619. Perhaps more importantly for our task,
of all the added relations, the before relation is
added the most. We experimented with original vs.
expanded Timebank and found the expanded per-
formed slightly worse. The decline may be due to
poor transitivity additions, as several Timebank doc-
uments contain inconsistent labelings. All reported
results are from training without transitivity.
5.2 Temporal Classifier in Narrative Chains
We classify the Gigaword Corpus in two stages,
once for the temporal features on each event (tense,
grammatical aspect, aspectual class), and once be-
tween all pairs of events that share arguments. This
allows us to classify the before/other relations be-
tween all potential narrative events.
The first stage is trained on Timebank, and the
second is trained using the approach described
above, varying the size of the none training rela-
tions. Each pair of events in a gigaword document
that share a coreferring argument is treated as a sepa-
rate ordering classification task. We count the result-
ing number of labeled before relations between each
verb/dependency pair. Processing the entire corpus
produces a database of event pair counts where con-
fidence of two generic events A and B can be mea-
sured by comparing how many before labels have
been seen versus their inverted order B and A5.
5Note that we train with the before relation, and so transpos-
ing two events is similar to classifying the after relation.
794
5.3 Temporal Evaluation
We want to evaluate temporal order at the narrative
level, across all events within a chain. We envision
narrative chains being used for tasks of coherence,
among other things, and so it is desired to evaluate
temporal decisions within a coherence framework.
Along these lines, our test set uses actual narrative
chains from documents, hand labeled for a partial
ordering. We evaluate coherence of these true chains
against a random ordering. The task is thus deciding
which of the two chains is most coherent, the orig-
inal or the random (baseline 50%)? We generated
up to 300 random orderings for each test document,
averaging the accuracy across all.
Our evaluation data is the same 69 documents
used in the test set for learning narrative relations.
The chain from each document is hand identified
and labeled for a partial ordering using only the be-
fore relation. Ordering was done by the authors and
all attempts were made to include every before re-
lation that exists in the document, or that could be
deduced through transitivity rules. Figure 4 shows
an example and its full reversal, although the evalu-
ation uses random orderings. Each edge is a distinct
before relation and is used in the judgement score.
The coherence score for a partially ordered nar-
rative chain is the sum of all the relations that our
classified corpus agrees with, weighted by how cer-
tain we are. If the gigaword classifications disagree,
a weighted negative score is given. Confidence is
based on a logarithm scale of the difference between
the counts of before and after classifications. For-
mally, the score is calculated as the following:
?
E:x,y
?
???
???
log(D(x, y)) if x?y and B(x, y) > B(y, x)
?log(D(x, y)) if x?y and B(y, x) > B(x, y)
?log(D(x, y)) if !x?y & !y?x & D(x, y) > 0
0 otherwise
where E is the set of all event pairs, B(i, j) is how
many times we classified events i and j as before in
Gigaword, and D(i, j) = |B(i, j) ? B(j, i)|. The
relation i?j indicates that i is temporally before j.
5.4 Results
Out approach gives higher scores to orders that co-
incide with the pairwise orderings classified in our
gigaword training data. The results are shown in fig-
ure 5. Of the 69 chains, 6 did not have any ordered
events and were removed from the evaluation. We
Figure 4: A narrative chain and its reverse order.
All ? 6 ? 10
correct 8086 75% 7603 78% 6307 89%
incorrect 1738 1493 619
tie 931 627 160
Figure 5: Results for choosing the correct ordered chain.
(? 10) means there were at least 10 pairs of ordered
events in the chain.
generated (up to) 300 random orderings for each of
the remaining 63. We report 75.2% accuracy, but 22
of the 63 had 5 or fewer pairs of ordered events. Fig-
ure 5 therefore shows results from chains with more
than 5 pairs, and also 10 or more. As we would
hope, the accuracy improves the larger the ordered
narrative chain. We achieve 89.0% accuracy on the
24 documents whose chains most progress through
time, rather than chains that are difficult to order
with just the before relation.
Training without none relations resulted in high
recall for before decisions. Perhaps due to data spar-
sity, this produces our best results as reported above.
6 Discrete Narrative Event Chains
Up till this point, we have learned narrative relations
across all possible events, including their temporal
order. However, the discrete lists of events for which
Schank scripts are most famous have not yet been
constructed.
We intentionally did not set out to reproduce ex-
plicit self-contained scripts in the sense that the
?restaurant script? is complete and cannot include
other events. The name narrative was chosen to im-
ply a likely order of events that is common in spoken
and written retelling of world events. Discrete sets
have the drawback of shutting out unseen and un-
795
Figure 6: An automatically learned Prosecution Chain.
Arrows indicate the before relation.
likely events from consideration. It is advantageous
to consider a space of possible narrative events and
the ordering within, not a closed list.
However, it is worthwhile to construct discrete
narrative chains, if only to see whether the combina-
tion of event learning and ordering produce script-
like structures. This is easily achievable by using
the PMI scores from section 4 in an agglomerative
clustering algorithm, and then applying the ordering
relations from section 5 to produce a directed graph.
Figures 6 and 7 show two learned chains after
clustering and ordering. Each arrow indicates a be-
fore relation. Duplicate arrows implied by rules of
transitivity are removed. Figure 6 is remarkably ac-
curate, and figure 7 addresses one of the chains from
our introduction, the employment narrative. The
core employment events are accurate, but cluster-
ing included life events (born, died, graduated) from
obituaries of which some temporal information is in-
correct. The Timebank corpus does not include obit-
uaries, thus we suffer from sparsity in training data.
7 Discussion
We have shown that it is possible to learn narrative
event chains unsupervised from raw text. Not only
do our narrative relations show improvements over
a baseline, but narrative chains offer hope for many
other areas of NLP. Inference, coherence in summa-
rization and generation, slot filling for question an-
swering, and frame induction are all potential areas.
We learned a new measure of similarity, the nar-
Figure 7: An Employment Chain. Dotted lines indicate
incorrect before relations.
rative relation, using the protagonist as a hook to ex-
tract a list of related events from each document.
The 37% improvement over a verb-only baseline
shows that we may not need presorted topics of doc-
uments to learn inferences. In addition, we applied
state of the art temporal classification to show that
sets of events can be partially ordered. Judgements
of coherence can then be made over chains within
documents. Further work in temporal classification
may increase accuracy even further.
Finally, we showed how the event space of narra-
tive relations can be clustered to create discrete sets.
While it is unclear if these are better than an uncon-
strained distribution of events, they do offer insight
into the quality of narratives.
An important area not discussed in this paper is
the possibility of using narrative chains for semantic
role learning. A narrative chain can be viewed as
defining the semantic roles of an event, constraining
it against roles of the other events in the chain. An
argument?s class can then be defined as the set of
narrative arguments in which it appears.
We believe our model provides an important first
step toward learning the rich causal, temporal and
inferential structure of scripts and frames.
Acknowledgment: This work is funded in part
by DARPA through IBM and by the DTO Phase III
Program for AQUAINT through Broad Agency An-
nouncement (BAA) N61339-06-R-0034. Thanks to the
reviewers for helpful comments and the suggestion for a
non-full-coreference baseline.
796
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Christian
Boitet and Pete Whitelock, editors, ACL-98, pages 86?
90, San Francisco, California. Morgan Kaufmann Pub-
lishers.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 141?148.
David Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference reso-
lution. Proc. of HLT/NAACL, pages 297?304.
Samuel Brody. 2007. Clustering Clauses for High-
Level Relation Detection: An Information-theoretic
Approach. Proceedings of the 43rd Annual Meeting
of the Association of Computational Linguistics, pages
448?455.
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying temporal relations between events.
In Proceedings of ACL-07, Prague, Czech Republic.
Timothy Chklovski and Patrick Pantel. 2004. Verbocean:
Mining the web for fine-grained semantic verb rela-
tions. In Proceedings of EMNLP-04.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC-06, pages 449?454.
Tony Deyes. 1984. Towards an authentic ?discourse
cloze?. Applied Linguistics, 5(2).
Toshiaki Fujiki, Hidetsugu Nanba, and Manabu Oku-
mura. 2003. Automatic acquisition of script knowl-
edge from a text collection. In EACL, pages 91?94.
David Graff. 2002. English Gigaword. Linguistic Data
Consortium.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modelling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2).
Mirella Lapata and Alex Lascarides. 2006. Learning
sentence-internal temporal relations. In Journal of AI
Research, volume 27, pages 85?117.
C.Y. Lin and E. Hovy. 2000. The automated acquisi-
tion of topic signatures for text summarization. Pro-
ceedings of the 17th conference on Computational
linguistics-Volume 1, pages 495?501.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In Proceedings of ACL-06, July.
Raymond Mooney and Gerald DeJong. 1985. Learning
schemata for natural language processing. In Ninth In-
ternational Joint Conference on Artificial Intelligence
(IJCAI), pages 681?687.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indicator of the
structure of text. Computational Linguistics, 17:21?
43.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. Proceedings of
HLT/NAACL, 4:321?328.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew
See, David Day, Lisa Ferro, Robert Gaizauskas, Mar-
cia Lazo, Andrea Setzer, and Beth Sundheim. 2003.
The timebank corpus. Corpus Linguistics, pages 647?
656.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding. Lawrence Erlbaum.
Wilson L. Taylor. 1953. Cloze procedure: a new tool for
measuring readability. Journalism Quarterly, 30:415?
433.
797
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 297?305,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Robust Machine Translation Evaluation with Entailment Features?
Sebastian Pado?
Stuttgart University
pado@ims.uni-stuttgart.de
Michel Galley, Dan Jurafsky, Chris Manning
Stanford University
{mgalley,jurafsky,manning}@stanford.edu
Abstract
Existing evaluation metrics for machine translation
lack crucial robustness: their correlations with hu-
man quality judgments vary considerably across lan-
guages and genres. We believe that the main reason
is their inability to properly capture meaning: A good
translation candidate means the same thing as the
reference translation, regardless of formulation. We
propose a metric that evaluates MT output based on
a rich set of features motivated by textual entailment,
such as lexical-semantic (in-)compatibility and ar-
gument structure overlap. We compare this metric
against a combination metric of four state-of-the-
art scores (BLEU, NIST, TER, and METEOR) in
two different settings. The combination metric out-
performs the individual scores, but is bested by the
entailment-based metric. Combining the entailment
and traditional features yields further improvements.
1 Introduction
Constant evaluation is vital to the progress of ma-
chine translation (MT). Since human evaluation is
costly and difficult to do reliably, a major focus of
research has been on automatic measures of MT
quality, pioneered by BLEU (Papineni et al, 2002)
and NIST (Doddington, 2002). BLEU and NIST
measure MT quality by using the strong correla-
tion between human judgments and the degree of
n-gram overlap between a system hypothesis trans-
lation and one or more reference translations. The
resulting scores are cheap and objective.
However, studies such as Callison-Burch et al
(2006) have identified a number of problems with
BLEU and related n-gram-based scores: (1) BLEU-
like metrics are unreliable at the level of individual
sentences due to data sparsity; (2) BLEU metrics
can be ?gamed? by permuting word order; (3) for
some corpora and languages, the correlation to hu-
man ratings is very low even at the system level;
(4) scores are biased towards statistical MT; (5) the
quality gap between MT and human translations is
not reflected in equally large BLEU differences.
?This paper is based on work funded by the Defense Ad-
vanced Research Projects Agency through IBM. The content
does not necessarily reflect the views of the U.S. Government,
and no official endorsement should be inferred.
This is problematic, but not surprising: The met-
rics treat any divergence from the reference as a
negative, while (computational) linguistics has long
dealt with linguistic variation that preserves the
meaning, usually called paraphrase, such as:
(1) HYP: However, this was declared terrorism
by observers and witnesses.
REF: Nevertheless, commentators as well as
eyewitnesses are terming it terrorism.
A number of metrics have been designed to account
for paraphrase, either by making the matching more
intelligent (TER, Snover et al (2006)), or by using
linguistic evidence, mostly lexical similarity (ME-
TEOR, Banerjee and Lavie (2005); MaxSim, Chan
and Ng (2008)), or syntactic overlap (Owczarzak et
al. (2008); Liu and Gildea (2005)). Unfortunately,
each metrics tend to concentrate on one particu-
lar type of linguistic information, none of which
always correlates well with human judgments.
Our paper proposes two strategies. We first ex-
plore the combination of traditional scores into a
more robust ensemble metric with linear regression.
Our second, more fundamental, strategy replaces
the use of loose surrogates of translation quality
with a model that attempts to comprehensively as-
sess meaning equivalence between references and
MT hypotheses. We operationalize meaning equiv-
alence by bidirectional textual entailment (RTE,
Dagan et al (2005)), and thus predict the qual-
ity of MT hypotheses with a rich RTE feature set.
The entailment-based model goes beyond existing
word-level ?semantic? metrics such as METEOR
by integrating phrasal and compositional aspects
of meaning equivalence, such as multiword para-
phrases, (in-)correct argument and modification
relations, and (dis-)allowed phrase reorderings. We
demonstrate that the resulting metric beats both in-
dividual and combined traditional MT metrics. The
complementary features of both metric types can
be combined into a joint, superior metric.
297
HYP: Three aid workers were kidnapped.
REF: Three aid workers were kidnapped by pirates.
no entailment entailment
HYP: The virus did not infect anybody.
REF: No one was infected by the virus.
entailment
entailment
Figure 1: Entailment status between an MT system
hypothesis and a reference translation for equiva-
lent (top) and non-equivalent (bottom) translations.
2 Regression-based MT Quality Prediction
Current MTmetrics tend to focus on a single dimen-
sion of linguistic information. Since the importance
of these dimensions tends not to be stable across
language pairs, genres, and systems, performance
of these metrics varies substantially. A simple strat-
egy to overcome this problem could be to combine
the judgments of different metrics. For example,
Paul et al (2007) train binary classifiers on a fea-
ture set formed by a number of MT metrics. We
follow a similar idea, but use a regularized linear
regression to directly predict human ratings.
Feature combination via regression is a super-
vised approach that requires labeled data. As we
show in Section 5, this data is available, and the
resulting model generalizes well from relatively
small amounts of training data.
3 Textual Entailment vs. MT Evaluation
Our novel approach to MT evaluation exploits the
similarity between MT evaluation and textual en-
tailment (TE). TE was introduced by Dagan et
al. (2005) as a concept that corresponds more
closely to ?common sense? reasoning patterns than
classical, strict logical entailment. Textual entail-
ment is defined informally as a relation between
two natural language sentences (a premise P and
a hypothesis H) that holds if ?a human reading P
would infer that H is most likely true?. Knowledge
about entailment is beneficial for NLP tasks such as
Question Answering (Harabagiu and Hickl, 2006).
The relation between textual entailment and MT
evaluation is shown in Figure 1. Perfect MT output
and the reference translation entail each other (top).
Translation problems that impact semantic equiv-
alence, e.g., deletion or addition of material, can
break entailment in one or both directions (bottom).
On the modelling level, there is common ground
between RTE and MT evaluation: Both have to
distinguish between valid and invalid variation to
determine whether two texts convey the same in-
formation or not. For example, to recognize the
bidirectional entailment in Ex. (1), RTE must ac-
count for the following reformulations: synonymy
(However/Nevertheless), more general semantic
relatedness (observers/commentators), phrasal re-
placements (and/as well as), and an active/passive
alternation that implies structural change (is de-
clared/are terming). This leads us to our main hy-
pothesis: RTE features are designed to distinguish
meaning-preserving variation from true divergence
and are thus also good predictors in MT evaluation.
However, while the original RTE task is asymmet-
ric, MT evaluation needs to determine meaning
equivalence, which is a symmetric relation. We do
this by checking for entailment in both directions
(see Figure 1). Operationally, this ensures we detect
translations which either delete or insert material.
Clearly, there are also differences between the
two tasks. An important one is that RTE assumes
the well-formedness of the two sentences. This is
not generally true in MT, and could lead to de-
graded linguistic analyses. However, entailment
relations are more sensitive to the contribution of
individual words (MacCartney andManning, 2008).
In Example 2, the modal modifiers break the entail-
ment between two otherwise identical sentences:
(2) HYP: Peter is certainly from Lincolnshire.
REF: Peter is possibly from Lincolnshire.
This means that the prediction of TE hinges on
correct semantic analysis and is sensitive to mis-
analyses. In contrast, human MT judgments behave
robustly. Translations that involve individual errors,
like (2), are judged lower than perfect ones, but
usually not crucially so, since most aspects are
still rendered correctly. We thus expect even noisy
RTE features to be predictive for translation quality.
This allows us to use an off-the-shelf RTE system
to obtain features, and to combine them using a
regression model as described in Section 2.
3.1 The Stanford Entailment Recognizer
The Stanford Entailment Recognizer (MacCartney
et al, 2006) is a stochastic model that computes
match and mismatch features for each premise-
hypothesis pair. The three stages of the system
are shown in Figure 2. The system first uses a
robust broad-coverage PCFG parser and a deter-
ministic constituent-dependency converter to con-
struct linguistic representations of the premise and
298
Stage 3: Feature computation (w/ numbers of features)
Premise: India buys 1,000 tanks.
Hypothesis: India acquires arms.
Stage 1: Linguistic analysis
India
buys
1,000 tanks
subj dobj
India
acquires
arms
subj dobj
Stage 2: Alignment
India
buys
1,000 tanks
subj dobj
India
acquires
arms
subj dobj
0.9
1.0
0.7
Alignment (8):
Semantic 
compatibility 
(34): 
Insertions and
deletions (20):
Preservation of 
reference (16):
Structural 
alignment (28):
Overall alignment quality
Modality, Factivity, Polarity, 
Quantification, Lexical-semantic 
relatedness, Tense
Felicity of appositions and adjuncts, 
Types of unaligned material 
Locations, Dates, Entities
Alignment of main verbs and 
syntactically prominent words, 
Argument structure (mis-)matches
Figure 2: The Stanford Entailment Recognizer
the hypothesis. The results are typed dependency
graphs that contain a node for each word and la-
beled edges representing the grammatical relations
between words. Named entities are identified, and
contiguous collocations grouped. Next, it identifies
the highest-scoring alignment from each node in
the hypothesis graph to a single node in the premise
graph, or to null. It uses a locally decomposable
scoring function: The score of an alignment is the
sum of the local word and edge alignment scores.
The computation of these scores make extensive
use of about ten lexical similarity resources, in-
cluding WordNet, InfoMap, and Dekang Lin?s the-
saurus. Since the search space is exponential in
the hypothesis length, the system uses stochastic
(rather than exhaustive) search based on Gibbs sam-
pling (see de Marneffe et al (2007)).
Entailment features. In the third stage, the sys-
tem produces roughly 100 features for each aligned
premise-hypothesis pair. A small number of them
are real-valued (mostly quality scores), but most
are binary implementations of small linguistic the-
ories whose activation indicates syntactic and se-
mantic (mis-)matches of different types. Figure 2
groups the features into five classes. Alignment
features measure the overall quality of the align-
ment as given by the lexical resources. Semantic
compatibility features check to what extent the
aligned material has the same meaning and pre-
serves semantic dimensions such as modality and
factivity, taking a limited amount of context into
account. Insertion/deletion features explicitly ad-
dress material that remains unaligned and assess its
felicity. Reference features ascertain that the two
sentences actually refer to the same events and par-
ticipants. Finally, structural features add structural
considerations by ensuring that argument structure
is preserved in the translation. See MacCartney et
al. (2006) for details on the features, and Sections
5 and 6 for examples of feature firings.
Efficiency considerations. The use of deep lin-
guistic analysis makes our entailment-based met-
ric considerably more heavyweight than traditional
MT metrics. The average total runtime per sentence
pair is 5 seconds on an AMD 2.6GHz Opteron core
? efficient enough to perform regular evaluations on
development and test sets. We are currently investi-
gating caching and optimizations that will enable
the use of our metric for MT parameter tuning in a
Minimum Error Rate Training setup (Och, 2003).
4 Experimental Evaluation
4.1 Experiments
Traditionally, human ratings for MT quality have
been collected in the form of absolute scores on a
five- or seven-point Likert scale, but low reliabil-
ity numbers for this type of annotation have raised
concerns (Callison-Burch et al, 2008). An alter-
native that has been adopted by the yearly WMT
evaluation shared tasks since 2008 is the collection
of pairwise preference judgments between pairs of
MT hypotheses which can be elicited (somewhat)
more reliably. We demonstrate that our approach
works well for both types of annotation and differ-
ent corpora. Experiment 1 models absolute scores
on Asian newswire, and Experiment 2 pairwise
preferences on European speech and news data.
4.2 Evaluation
We evaluate the output of our models both on the
sentence and on the system level. At the sentence
level, we can correlate predictions in Experiment 1
directly with human judgments with Spearman?s ? ,
299
a non-parametric rank correlation coefficient appro-
priate for non-normally distributed data. In Experi-
ment 2, the predictions cannot be pooled between
sentences. Instead of correlation, we compute ?con-
sistency? (i.e., accuracy) with human preferences.
System-level predictions are computed in both
experiments from sentence-level predictions, as the
ratio of sentences for which each system provided
the best translation (Callison-Burch et al, 2008).
We extend this procedure slightly because real-
valued predictions cannot predict ties, while human
raters decide for a significant portion of sentences
(as much as 80% in absolute score annotation) to
?tie? two systems for first place. To simulate this
behavior, we compute ?tie-aware? predictions as
the percentage of sentences where the system?s hy-
pothesis was assigned a score better or at most ?
worse than the best system. ? is set to match the
frequency of ties in the training data.
Finally, the predictions are again correlated with
human judgments using Spearman?s ? . ?Tie aware-
ness? makes a considerable practical difference,
improving correlation figures by 5?10 points.1
4.3 Baseline Metrics
We consider four baselines. They are small regres-
sion models as described in Section 2 over com-
ponent scores of four widely used MT metrics. To
alleviate possible nonlinearity, we add all features
in linear and log space. Each baselines carries the
name of the underlying metric plus the suffix -R.2
BLEUR includes the following 18 sentence-level
scores: BLEU-n and n-gram precision scores
(1? n? 4); BLEU brevity penalty (BP); BLEU
score divided by BP. To counteract BLEU?s brittle-
ness at the sentence level, we also smooth BLEU-n
and n-gram precision as in Lin and Och (2004).
NISTR consists of 16 features. NIST-n scores
(1? n? 10) and information-weighted n-gram
precision scores (1? n? 4); NIST brevity penalty
(BP); and NIST score divided by BP.
1Due to space constraints, we only show results for ?tie-
aware? predictions. See Pado? et al (2009) for a discussion.
2The regression models can simulate the behaviour of each
component by setting the weights appropriately, but are strictly
more powerful. A possible danger is that the parameters over-
fit on the training set. We therefore verified that the three
non-trivial ?baseline? regression models indeed confer a bene-
fit over the default component combination scores: BLEU-1
(which outperformed BLEU-4 in the MetricsMATR 2008 eval-
uation), NIST-4, and TER (with all costs set to 1). We found
higher robustness and improved correlations for the regression
models. An exception is BLEU-1 and NIST-4 on Expt. 1 (Ar,
Ch), which perform 0.5?1 point better at the sentence level.
TERR includes 50 features. We start with the
standard TER score and the number of each of the
four edit operations. Since the default uniform cost
does not always correlate well with human judg-
ment, we duplicate these features for 9 non-uniform
edit costs. We find it effective to set insertion cost
close to 0, as a way of enabling surface variation,
and indeed the new TERp metric uses a similarly
low default insertion cost (Snover et al, 2009).
METEORR consists of METEOR v0.7.
4.4 Combination Metrics
The following three regression models implement
the methods discussed in Sections 2 and 3.
MTR combines the 85 features of the four base-
line models. It uses no entailment features.
RTER uses the 70 entailment features described
in Section 3.1, but no MTR features.
MT+RTER uses all MTR and RTER features,
combining matching and entailment evidence.3
5 Expt. 1: Predicting Absolute Scores
Data. Our first experiment evaluates the models
we have proposed on a corpus with traditional an-
notation on a seven-point scale, namely the NIST
OpenMT 2008 corpus.4 The corpus contains trans-
lations of newswire text into English from three
source languages (Arabic (Ar), Chinese (Ch), Urdu
(Ur)). Each language consists of 1500?2800 sen-
tence pairs produced by 7?15 MT systems.
We use a ?round robin? scheme. We optimize
the weights of our regression models on two lan-
guages and then predict the human scores on the
third language. This gauges performance of our
models when training and test data come from the
same genre, but from different languages, which
we believe to be a setup of practical interest. For
each test set, we set the system-level tie parameter
? so that the relative frequency of ties was equal
to the training set (65?80%). Hypotheses generally
had to receive scores within 0.3?0.5 points to tie.
Results. Table 1 shows the results. We first con-
centrate on the upper half (sentence-level results).
The predictions of all models correlate highly sig-
nificantly with human judgments, but we still see
robustness issues for the individual MT metrics.
3Software for RTER and MT+RTER is available from
http://nlp.stanford.edu/software/mteval.shtml.
4Available from http://www.nist.gov.
300
Evaluation Data Metrics
train test BLEUR METEORR NISTR TERR MTR RTER MT+RTER
Sentence-level
Ar+Ch Ur 49.9 49.1 49.5 50.1 50.1 54.5 55.6
Ar+Ur Ch 53.9 61.1 53.1 50.3 57.3 58.0 62.7
Ch+Ur Ar 52.5 60.1 50.4 54.5 55.2 59.9 61.1
System-level
Ar+Ch Ur 73.9 68.4 50.0 90.0? 92.7? 77.4? 81.0?
Ar+Ur Ch 38.5 44.3 40.0 59.0? 51.8? 47.7 57.3?
Ch+Ur Ar 59.7? 86.3? 61.9? 42.1 48.1 59.7? 61.7?
Table 1: Expt. 1: Spearman?s ? for correlation between human absolute scores and model predictions on
NIST OpenMT 2008. Sentence level: All correlations are highly significant. System level: ?: p<0.05.
METEORR achieves the best correlation for Chi-
nese and Arabic, but fails for Urdu, apparently the
most difficult language. TERR shows the best result
for Urdu, but does worse than METEORR for Ara-
bic and even worse than BLEUR for Chinese. The
MTR combination metric alleviates this problem to
some extent by improving the ?worst-case? perfor-
mance on Urdu to the level of the best individual
metric. The entailment-based RTER system outper-
forms MTR on each language. It particularly im-
proves on MTR?s correlation on Urdu. Even though
METEORR still does somewhat better than MTR
and RTER, we consider this an important confirma-
tion for the usefulness of entailment features in MT
evaluation, and for their robustness.5
In addition, the combined model MT+RTER is
best for all three languages, outperforming METE-
ORR for each language pair. It performs consid-
erably better than either MTR or RTER. This is a
second result: the types of evidence provided by
MTR and RTER appear to be complementary and
can be combined into a superior model.
On the system level (bottom half of Table 1),
there is high variance due to the small number of
predictions per language, and many predictions are
not significantly correlated with human judgments.
BLEUR, METEORR, and NISTR significantly pre-
dict one language each (all Arabic); TERR, MTR,
and RTER predict two languages. MT+RTER is
the only model that shows significance for all three
languages. This result supports the conclusions we
have drawn from the sentence-level analysis.
Further analysis. We decided to conduct a thor-
ough analysis of the Urdu dataset, the most difficult
source language for all metrics. We start with a fea-
5These results are substantially better than the performance
our metric showed in the MetricsMATR 2008 challenge. Be-
yond general enhancement of our model, we attribute the less
good MetricsMATR 2008 results to an infelicitous choice
of training data for the submission, coupled with the large
amount of ASR output in the test data, whose disfluencies
represent an additional layer of problems for deep approaches.
20 40 60 80 1000.4
2
0.46
0.50
0.54
% Training data MT08 Ar+Ch
Spe
arm
an's
 rho
 on 
MT 
08 U
r
l
l
l
l
l l l l l
l l l
l l l l l l
l
l
MetricsMt?RteRRteRMtRMetR
Figure 3: Experiment 1: Learning curve (Urdu).
ture ablation study. Removing any feature group
from RTER results in drops in correlation of at least
three points. The largest drops occur for the struc-
tural (? = ?11) and insertion/deletion (? = ?8)
features. Thus, all feature groups appear to con-
tribute to the good correlation of RTER. However,
there are big differences in the generality of the
feature groups: in isolation, the insertion/deletion
features achieve almost no correlation, and need to
be complemented by more robust features.
Next, we analyze the role of training data. Fig-
ure 3 shows Urdu average correlations for models
trained on increasing subsets of the training data
(10% increments, 10 random draws per step; Ar
and Ch show similar patterns.) METEORR does not
improve, which is to be expected given the model
definition. RTER has a rather flat learning curve
that climbs to within 2 points of the final correla-
tion value for 20% of the training set (about 400
sentence pairs). Apparently, entailment features do
not require a large training set, presumably because
most features of RTER are binary. The remaining
two models, MTR and MT+RTER, show clearer
benefit from more data. With 20% of the total data,
they climb to within 5 points of their final perfor-
mance, but keep slowly improving further.
301
REF: I shall face that fact today.
HYP: Today I will face this reality.
[doc WL-34-174270-7483871, sent 4, system1]
Gold: 6
METEORR: 2.8
RTER: 6.1
? Only function words unaligned (will, this)
? Alignment fact/reality: hypernymy is ok
in upward monotone context
REF: What does BBC?s Haroon Rasheed say after a visit to Lal Masjid Jamia
Hafsa complex? There are no underground tunnels in Lal Masjid or Jamia
Hafsa. The presence of the foreigners could not be confirmed as well. What
became of the extremists like Abuzar?
HYP: BBC Haroon Rasheed Lal Masjid, Jamia Hafsa after his visit to Auob
Medical Complex says Lal Masjid and seminary in under a land mine, not
also been confirmed the presence of foreigners could not be, such as Abu by
the extremist? [doc WL-12-174261-7457007, sent 2, system2]
Gold: 1
METEORR: 4.5
RTER: 1.2
? Hypothesis root node unaligned
? Missing alignments for subjects
? Important entities in hypothesis cannot be
aligned
? Reference, hypothesis differ in polarity
Table 2: Expt. 1: Reference translations and MT output (Urdu). Scores are out of 7 (higher is better).
Finally, we provide a qualitative comparison of
RTER?s performance against the best baseline met-
ric, METEORR. Since the computation of RTER
takes considerably more resources than METEORR,
it is interesting to compare the predictions of RTER
against METEORR. Table 2 shows two classes of
examples with apparent improvements.
The first example (top) shows a good translation
that is erroneously assigned a low score by ME-
TEORR because (a) it cannot align fact and reality
(METEORR aligns only synonyms) and (b) it pun-
ishes the change of word order through its ?penalty?
term. RTER correctly assigns a high score. The
features show that this prediction results from two
semantic judgments. The first is that the lack of
alignments for two function words is unproblem-
atic; the second is that the alignment between fact
and reality, which is established on the basis of
WordNet similarity, is indeed licensed in the cur-
rent context. More generally, we find that RTER
is able to account for more valid variation in good
translations because (a) it judges the validity of
alignments dependent on context; (b) it incorpo-
rates more semantic similarities; and (c) it weighs
mismatches according to the word?s status.
The second example (bottom) shows a very bad
translation that is scored highly by METEORR,
since almost all of the reference words appear either
literally or as synonyms in the hypothesis (marked
in italics). In combination with METEORR?s con-
centration on recall, this is sufficient to yield a
moderately high score. In the case of RTER, a num-
ber of mismatch features have fired. They indicate
problems with the structural well-formedness of
the MT output as well as semantic incompatibil-
ity between hypothesis and reference (argument
structure and reference mismatches).
6 Expt. 2: Predicting Pairwise Preferences
In this experiment, we predict human pairwise pref-
erence judgments (cf. Section 4). We reuse the
linear regression framework from Section 2 and
predict pairwise preferences by predicting two ab-
solute scores (as before) and comparing them.6
Data. This experiment uses the 2006?2008 cor-
pora of the Workshop on Statistical Machine
Translation (WMT).7 It consists of data from EU-
ROPARL (Koehn, 2005) and various news com-
mentaries, with five source languages (French, Ger-
man, Spanish, Czech, and Hungarian). As training
set, we use the portions of WMT 2006 and 2007
that are annotated with absolute scores on a five-
point scale (around 14,000 sentences produced by
40 systems). The test set is formed by the WMT
2008 relative rank annotation task. As in Experi-
ment 1, we set ? so that the incidence of ties in the
training and test set is equal (60%).
Results. Table 4 shows the results. The left result
column shows consistency, i.e., the accuracy on
human pairwise preference judgments.8 The pat-
tern of results matches our observations in Expt. 1:
Among individual metrics, METEORR and TERR
do better than BLEUR and NISTR. MTR and RTER
outperform individual metrics. The best result by a
wide margin, 52.5%, is shown by MT+RTER.
6We also experimented with a logistic regression model
that predicts binary preferences directly. Its performance is
comparable; see Pado? et al (2009) for details.
7Available from http://www.statmt.org/.
8The random baseline is not 50%, but, according to our
experiments, 39.8%. This has two reasons: (1) the judgments
include contradictory and tie annotations that cannot be pre-
dicted correctly (raw inter-annotator agreement on WMT 2008
was 58%); (2) metrics have to submit a total order over the
translations for each sentence, which introduces transitivity
constraints. For details, see Callison-Burch et al (2008).
302
Segment MTR RTER MT+RTER Gold
REF: Scottish NHS boards need to improve criminal records checks for
employees outside Europe, a watchdog has said.
HYP: The Scottish health ministry should improve the controls on extra-
community employees to check whether they have criminal precedents,
said the monitoring committee. [1357, lium-systran]
Rank: 3 Rank: 1 Rank: 2 Rank: 1
REF: Arguments, bullying and fights between the pupils have extended
to the relations between their parents.
HYP: Disputes, chicane and fights between the pupils transposed in
relations between the parents. [686, rbmt4]
Rank: 5 Rank: 2 Rank: 4 Rank: 5
Table 3: Expt. 2: Reference translations and MT output (French). Ranks are out of five (smaller is better).
Feature set Consis-
tency (%)
System-level
correlation (?)
BLEUR 49.6 69.3
METEORR 51.1 72.6
NISTR 50.2 70.4
TERR 51.2 72.5
MTR 51.5 73.1
RTER 51.8 78.3
MT+RTER 52.5 75.8
WMT 08 (worst) 44 37
WMT 08 (best) 56 83
Table 4: Expt. 2: Prediction of pairwise preferences
on the WMT 2008 dataset.
The right column shows Spearman?s ? for the
correlation between human judgments and tie-
aware system-level predictions. All metrics predict
system scores highly significantly, partly due to the
larger number of systems compared (87 systems).
Again, we see better results for METEORR and
TERR than for BLEUR and NISTR, and the indi-
vidual metrics do worse than the combination mod-
els. Among the latter, the order is: MTR (worst),
MT+RTER, and RTER (best at 78.3).
WMT 2009. We submitted the Expt. 2 RTER
metric to the WMT 2009 shared MT evaluation
task (Pado? et al, 2009). The results provide fur-
ther validation for our results and our general ap-
proach. At the system level, RTER made third place
(avg. correlation ? = 0.79), trailing the two top met-
rics closely (? = 0.80, ? = 0.83) and making the
best predictions for Hungarian. It also obtained the
second-best consistency score (53%, best: 54%).
Metric comparison. The pairwise preference an-
notation of WMT 2008 gives us the opportunity to
compare the MTR and RTER models by comput-
ing consistency separately on the ?top? (highest-
ranked) and ?bottom? (lowest-ranked) hypotheses
for each reference. RTER performs about 1.5 per-
cent better on the top than on the bottom hypothe-
ses. The MTR model shows the inverse behavior,
performing 2 percent worse on the top hypothe-
ses. This matches well with our intuitions: We see
some noise-induced degradation for the entailment
features, but not much. In contrast, surface-based
features are better at detecting bad translations than
at discriminating among good ones.
Table 3 further illustrates the difference between
the top models on two example sentences. In the top
example, RTER makes a more accurate prediction
than MTR. The human rater?s favorite translation
deviates considerably from the reference in lexi-
cal choice, syntactic structure, and word order, for
which it is punished by MTR (rank 3/5). In contrast,
RTER determines correctly that the propositional
content of the reference is almost completely pre-
served (rank 1). In the bottom example, RTER?s
prediction is less accurate. This sentence was rated
as bad by the judge, presumably due to the inap-
propriate main verb translation. Together with the
subject mismatch, MTR correctly predicts a low
score (rank 5/5). RTER?s attention to semantic over-
lap leads to an incorrect high score (rank 2/5).
Feature Weights. Finally, we make two observa-
tions about feature weights in the RTER model.
First, the model has learned high weights not
only for the overall alignment score (which be-
haves most similarly to traditional metrics), but also
for a number of binary syntacto-semantic match
and mismatch features. This confirms that these
features systematically confer the benefit we have
shown anecdotally in Table 2. Features with a con-
sistently negative effect include dropping adjuncts,
unaligned or poorly aligned root nodes, incompat-
ible modality between the main clauses, person
and location mismatches (as opposed to general
mismatches) and wrongly handled passives. Con-
303
versely, higher scores result from factors such as
high alignment score, matching embeddings under
factive verbs, and matches between appositions.
Second, good MT evaluation feature weights are
not good weights for RTE. Some differences, par-
ticularly for structural features, are caused by the
low grammaticality of MT data. For example, the
feature that fires for mismatches between depen-
dents of predicates is unreliable on the WMT data.
Other differences do reflect more fundamental dif-
ferences between the two tasks (cf. Section 3). For
example, RTE puts high weights onto quantifier
and polarity features, both of which have the poten-
tial of influencing entailment decisions, but are (at
least currently) unimportant for MT evaluation.
7 Related Work
Researchers have exploited various resources to en-
able the matching between words or n-grams that
are semantically close but not identical. Banerjee
and Lavie (2005) and Chan and Ng (2008) use
WordNet, and Zhou et al (2006) and Kauchak
and Barzilay (2006) exploit large collections of
automatically-extracted paraphrases. These ap-
proaches reduce the risk that a good translation
is rated poorly due to lexical deviation, but do not
address the problem that a translation may contain
many long matches while lacking coherence and
grammaticality (cf. the bottom example in Table 2).
Thus, incorporation of syntactic knowledge has
been the focus of another line of research. Amigo?
et al (2006) use the degree of overlap between the
dependency trees of reference and hypothesis as a
predictor of translation quality. Similar ideas have
been applied by Owczarzak et al (2008) to LFG
parses, and by Liu and Gildea (2005) to features
derived from phrase-structure tress. This approach
has also been successful for the related task of
summarization evaluation (Hovy et al, 2006).
The most comparable work to ours is Gime?nez
and Ma?rquez (2008). Our results agree on the cru-
cial point that the use of a wide range of linguistic
knowledge in MT evaluation is desirable and im-
portant. However, Gime?nez and Ma?rquez advocate
the use of a bottom-up development process that
builds on a set of ?heterogeneous?, independent
metrics each of which measures overlap with re-
spect to one linguistic level. In contrast, our aim
is to provide a ?top-down?, integrated motivation
for the features we integrate through the textual
entailment recognition paradigm.
8 Conclusion and Outlook
In this paper, we have explored a strategy for the
evaluation of MT output that aims at comprehen-
sively assessing the meaning equivalence between
reference and hypothesis. To do so, we exploit the
common ground between MT evaluation and the
Recognition of Textual Entailment (RTE), both of
which have to distinguish valid from invalid lin-
guistic variation. Conceputalizing MT evaluation
as an entailment problem motivates the use of a
rich feature set that covers, unlike almost all earlier
metrics, a wide range of linguistic levels, including
lexical, syntactic, and compositional phenomena.
We have used an off-the-shelf RTE system to
compute these features, and demonstrated that a
regression model over these features can outper-
form an ensemble of traditional MT metrics in two
experiments on different datasets. Even though the
features build on deep linguistic analysis, they are
robust enough to be used in a real-world setting, at
least on written text. A limited amount of training
data is sufficient, and the weights generalize well.
Our data analysis has confirmed that each of the
feature groups contributes to the overall success of
the RTE metric, and that its gains come from its
better success at abstracting away from valid vari-
ation (such as word order or lexical substitution),
while still detecting major semantic divergences.
We have also clarified the relationship between MT
evaluation and textual entailment: The majority of
phenomena (but not all) that are relevant for RTE
are also informative for MT evaluation.
The focus of this study was on the use of an ex-
isting RTE infrastructure for MT evaluation. Future
work will have to assess the effectiveness of individ-
ual features and investigate ways to customize RTE
systems for the MT evaluation task. An interesting
aspect that we could not follow up on in this paper
is that entailment features are linguistically inter-
pretable (cf. Fig. 2) and may find use in uncovering
systematic shortcomings of MT systems.
A limitation of our current metric is that it is
language-dependent and relies on NLP tools in
the target language that are still unavailable for
many languages, such as reliable parsers. To some
extent, of course, this problem holds as well for
state-of-the-art MT systems. Nevertheless, it must
be an important focus of future research to develop
robust meaning-based metrics for other languages
that can cash in the promise that we have shown
for evaluating translation into English.
304
References
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and
Llu??s Ma`rquez. 2006. MT Evaluation: Human-
like vs. human acceptable. In Proceedings of COL-
ING/ACL 2006, pages 17?24, Sydney, Australia.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures, pages 65?72, Ann Ar-
bor, MI.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU
in machine translation research. In Proceedings of
EACL, pages 249?256, Trento, Italy.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the ACL Workshop on Statistical Ma-
chine Translation, pages 70?106, Columbus, OH.
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62, Columbus, Ohio, June.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Southampton, UK.
Marie-Catherine de Marneffe, Trond Grenager, Bill
MacCartney, Daniel Cer, Daniel Ramage, Chloe?
Kiddon, and Christopher D. Manning. 2007. Align-
ing semantic graphs for textual inference and ma-
chine reading. In Proceedings of the AAAI Spring
Symposium, Stanford, CA.
George Doddington. 2002. Automatic evaluation of
machine translation quality using n-gram cooccur-
rence statistics. In Proceedings of HLT, pages 128?
132, San Diego, CA.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2008. Het-
erogeneous automatic MT evaluation through non-
parametric metric combinations. In Proceedings of
IJCNLP, pages 319?326, Hyderabad, India.
Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In Proceedings of ACL, pages 905?
912, Sydney, Australia.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi
Fukumoto. 2006. Automated summarization evalu-
ation with basic elements. In Proceedings of LREC,
Genoa, Italy.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In Proceedings of HLT-
NAACL, pages 455?462.
Phillip Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
MT Summit X, Phuket, Thailand.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation met-
rics for machine translation. In Proceedings of COL-
ING, pages 501?507, Geneva, Switzerland.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures, pages 25?32, Ann Arbor, MI.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in
natural language inference. In Proceedings of COL-
ING, pages 521?528, Manchester, UK.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of
valid textual entailments. In Proceedings of NAACL,
pages 41?48, New York City, NY.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2008. Evaluating machine translation with
LFG dependencies. Machine Translation, 21(2):95?
119.
Sebastian Pado?, Michel Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Textual entailment
features for machine translation evaluation. In Pro-
ceedings of the EACL Workshop on Statistical Ma-
chine Translation, pages 37?41, Athens, Greece.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318, Philadelphia, PA.
Michael Paul, Andrew Finch, and Eiichiro Sumita.
2007. Reducing human assessment of machine
translation quality to binary classifiers. In Proceed-
ings of TMI, pages 154?162, Sko?vde, Sweden.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of AMTA, pages 223?231, Cam-
bridge, MA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proceedings of the EACL
Workshop on Statistical Machine Translation, pages
259?268, Athens, Greece.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating machine translation results with para-
phrase support. In Proceedings of EMNLP, pages
77?84, Sydney, Australia.
305
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 602?610,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Unsupervised Learning of Narrative Schemas and their Participants
Nathanael Chambers and Dan Jurafsky
Stanford University, Stanford, CA 94305
{natec,jurafsky}@stanford.edu
Abstract
We describe an unsupervised system for learn-
ing narrative schemas, coherent sequences or sets
of events (arrested(POLICE,SUSPECT), convicted(
JUDGE, SUSPECT)) whose arguments are filled
with participant semantic roles defined over words
(JUDGE = {judge, jury, court}, POLICE = {police,
agent, authorities}). Unlike most previous work in
event structure or semantic role learning, our sys-
tem does not use supervised techniques, hand-built
knowledge, or predefined classes of events or roles.
Our unsupervised learning algorithm uses corefer-
ring arguments in chains of verbs to learn both rich
narrative event structure and argument roles. By
jointly addressing both tasks, we improve on pre-
vious results in narrative/frame learning and induce
rich frame-specific semantic roles.
1 Introduction
This paper describes a new approach to event se-
mantics that jointly learns event relations and their
participants from unlabeled corpora.
The early years of natural language processing
(NLP) took a ?top-down? approach to language
understanding, using representations like scripts
(Schank and Abelson, 1977) (structured represen-
tations of events, their causal relationships, and
their participants) and frames to drive interpreta-
tion of syntax and word use. Knowledge structures
such as these provided the interpreter rich infor-
mation about many aspects of meaning.
The problem with these rich knowledge struc-
tures is that the need for hand construction, speci-
ficity, and domain dependence prevents robust and
flexible language understanding. Instead, mod-
ern work on understanding has focused on shal-
lower representations like semantic roles, which
express at least one aspect of the semantics of
events and have proved amenable to supervised
learning from corpora like PropBank (Palmer et
al., 2005) and Framenet (Baker et al, 1998). Un-
fortunately, creating these supervised corpora is an
expensive and difficult multi-year effort, requiring
complex decisions about the exact set of roles to
be learned. Even unsupervised attempts to learn
semantic roles have required a pre-defined set of
roles (Grenager and Manning, 2006) and often a
hand-labeled seed corpus (Swier and Stevenson,
2004; He and Gildea, 2006).
In this paper, we describe our attempts to learn
script-like information about the world, including
both event structures and the roles of their partic-
ipants, but without pre-defined frames, roles, or
tagged corpora.
Consider the following Narrative Schema, to be
defined more formally later. The events on the left
follow a set of participants through a series of con-
nected events that constitute a narrative:
A search B
A arrest B
D convict B
B plead C
D acquit B
D sentence B
A = Police
B = Suspect
C = Plea
D = Jury
Events Roles
Being able to robustly learn sets of related
events (left) and frame-specific role information
about the argument types that fill them (right)
could assist a variety of NLP applications, from
question answering to machine translation.
Our previous work (Chambers and Jurafsky,
2008) relied on the intuition that in a coherent text,
any two events that are about the same participants
are likely to be part of the same story or narra-
tive. The model learned simple aspects of nar-
rative structure (?narrative chains?) by extracting
events that share a single participant, the protag-
onist. In this paper we extend this work to rep-
resent sets of situation-specific events not unlike
scripts, caseframes (Bean and Riloff, 2004), and
FrameNet frames (Baker et al, 1998). This paper
shows that verbs in distinct narrative chains can be
merged into an improved single narrative schema,
while the shared arguments across verbs can pro-
vide rich information for inducing semantic roles.
602
2 Background
This paper addresses two areas of work in event
semantics, narrative event chains and semantic
role labeling. We begin by highlighting areas in
both that can mutually inform each other through
a narrative schema model.
2.1 Narrative Event Chains
Narrative Event Chains are partially ordered sets
of events that all involve the same shared par-
ticipant, the protagonist (Chambers and Jurafsky,
2008). A chain contains a set of verbs represent-
ing events, and for each verb, the grammatical role
filled by the shared protagonist.
An event is a verb together with its constellation
of arguments. An event slot is a tuple of an event
and a particular argument slot (grammatical rela-
tion), represented as a pair ?v, d? where v is a verb
and d ? {subject, object, prep}. A chain is a tu-
ple (L,O) where L is a set of event slots and O is
a partial (temporal) ordering. We will write event
slots in shorthand as (X pleads) or (pleads X) for
?pleads, subject? and ?pleads, object?. Below is
an example chain modeling criminal prosecution.
L = (X pleads), (X admits), (convicted X), (sentenced X)
O = {(pleads, convicted), (convicted, sentenced), ...}
A graphical view is often more intuitive:
admits
pleads
sentenced
convicted
(X admits)
(X pleads)
(convicted X)
(sentenced X)
In this example, the protagonist of the chain
is the person being prosecuted and the other un-
specified event slots remain unfilled and uncon-
strained. Chains in the Chambers and Jurafsky
(2008) model are ordered; in this paper rather than
address the ordering task we focus on event and ar-
gument induction, leaving ordering as future work.
The Chambers and Jurafsky (2008) model
learns chains completely unsupervised, (albeit af-
ter parsing and resolving coreference in the text)
by counting pairs of verbs that share corefer-
ring arguments within documents and computing
the pointwise mutual information (PMI) between
these verb-argument pairs. The algorithm creates
chains by clustering event slots using their PMI
scores, and we showed this use of co-referring ar-
guments improves event relatedness.
Our previous work, however, has two major
limitations. First, the model did not express
any information about the protagonist, such as its
type or role. Role information (such as knowing
whether a filler is a location, a person, a particular
class of people, or even an inanimate object) could
crucially inform learning and inference. Second,
the model only represents one participant (the pro-
tagonist). Representing the other entities involved
in all event slots in the narrative could potentially
provide valuable information. We discuss both of
these extensions next.
2.1.1 The Case for Arguments
The Chambers and Jurafsky (2008) narrative
chains do not specify what type of argument fills
the role of protagonist. Chain learning and clus-
tering is based only on the frequency with which
two verbs share arguments, ignoring any features
of the arguments themselves.
Take this example of an actual chain from an
article in our training data. Given this chain of five
events, we want to choose other events most likely
to occur in this scenario.
hunt
use
accuse
suspect
search
fly
charge
?
One of the top scoring event slots is (fly X). Nar-
rative chains incorrectly favor (fly X) because it is
observed during training with all five event slots,
although not frequently with any one of them. An
event slot like (charge X) is much more plausible,
but is unfortunately scored lower by the model.
Representing the types of the arguments can
help solve this problem. Few types of arguments
are shared between the chain and (fly X). How-
ever, (charge X) shares many arguments with (ac-
cuse X), (search X) and (suspect X) (e.g., criminal
and suspect). Even more telling is that these argu-
ments are jointly shared (the same or coreferent)
across all three events. Chains represent coherent
scenarios, not just a set of independent pairs, so we
want to model argument overlap across all pairs.
2.1.2 The Case for Joint Chains
The second problem with narrative chains is that
they make judgments only between protagonist ar-
guments, one slot per event. All entities and slots
603
in the space of events should be jointly considered
when making event relatedness decisions.
As an illustration, consider the verb arrest.
Which verb is more related, convict or capture?
A narrative chain might only look at the objects
of these verbs and choose the one with the high-
est score, usually choosing convict. But in this
case the subjects offer additional information; the
subject of arrest (police) is different from that of
convict (judge). A more informed decision prefers
capture because both the objects (suspect) and
subjects (police) are identical. This joint reason-
ing is absent from the narrative chain model.
2.2 Semantic Role Labeling
The task of semantic role learning and labeling
is to identify classes of entities that fill predicate
slots; semantic roles seem like they?d be a good
model for the kind of argument types we?d like
to learn for narratives. Most work on semantic
role labeling, however, is supervised, using Prop-
bank (Palmer et al, 2005), FrameNet (Baker et
al., 1998) or VerbNet (Kipper et al, 2000) as
gold standard roles and training data. More re-
cent learning work has applied bootstrapping ap-
proaches (Swier and Stevenson, 2004; He and
Gildea, 2006), but these still rely on a hand la-
beled seed corpus as well as a pre-defined set of
roles. Grenegar and Manning (2006) use the EM
algorithm to learn PropBank roles from unlabeled
data, and unlike bootstrapping, they don?t need a
labeled corpus from which to start. However, they
do require a predefined set of roles (arg0, arg1,
etc.) to define the domain of their probabilistic
model.
Green and Dorr (2005) use WordNet?s graph
structure to cluster its verbs into FrameNet frames,
using glosses to name potential slots. We differ in
that we attempt to learn frame-like narrative struc-
ture from untagged newspaper text. Most sim-
ilar to us, Alishahi and Stevenson (2007) learn
verb specific semantic profiles of arguments us-
ing WordNet classes to define the roles. We learn
situation-specific classes of roles shared by multi-
ple verbs.
Thus, two open goals in role learning include
(1) unsupervised learning and (2) learning the
roles themselves rather than relying on pre-defined
role classes. As just described, Chambers and Ju-
rafsky (2008) offers an unsupervised approach to
event learning (goal 1), but lacks semantic role
knowledge (goal 2). The following sections de-
scribe a model that addresses both goals.
3 Narrative Schemas
The next sections introduce typed narrative chains
and chain merging, extensions that allow us to
jointly learn argument roles with event structure.
3.1 Typed Narrative Chains
The first step in describing a narrative schema is to
extend the definition of a narrative chain to include
argument types. We now constrain the protagonist
to be of a certain type or role. A Typed Narrative
Chain is a partially ordered set of event slots that
share an argument, but now the shared argument
is a role defined by being a member of a set of
types R. These types can be lexical units (such as
observed head words), noun clusters, or other se-
mantic representations. We use head words in the
examples below, but we also evaluate with argu-
ment clustering by mapping head words to mem-
ber clusters created with the CBC clustering algo-
rithm (Pantel and Lin, 2002).
We define a typed narrative chain as a tuple
(L,P,O) with L and O the set of event slots
and partial ordering as before. Let P be a set of
argument types (head words) representing a single
role. An example is given here:
L = {(hunt X), (X use), (suspect X), (accuse X), (search X)}
P = {person, government, company, criminal, ...}
O = {(use, hunt), (suspect, search), (suspect, accuse) ... }
3.2 Learning Argument Types
As mentioned above, narrative chains are learned
by parsing the text, resolving coreference, and ex-
tracting chains of events that share participants. In
our new model, argument types are learned simul-
taneously with narrative chains by finding salient
words that represent coreferential arguments. We
record counts of arguments that are observed with
each pair of event slots, build the referential set
for each word from its coreference chain, and then
represent each observed argument by the most fre-
quent head word in its referential set (ignoring pro-
nouns and mapping entity mentions with person
pronouns to a constant PERSON identifier).
As an example, the following contains four
worker mentions:
But for a growing proportion of U.S. workers, the troubles re-
ally set in when they apply for unemployment benefits. Many
workers find their benefits challenged.
604
L = {X arrest, X charge, X raid, X seize,
X confiscate, X detain, X deport }
P = {police, agent, authority, government}
Figure 1: A typed narrative chain. The four top
arguments are given. The orderingO is not shown.
The four bolded terms are coreferential and
(hopefully) identified by coreference. Our algo-
rithm chooses the head word of each phrase and
ignores the pronouns. It then chooses the most
frequent head word as the most salient mention.
In this example, the most salient term is workers.
If any pair of event slots share arguments from this
set, we count workers. In this example, the pair (X
find) and (X apply) shares an argument (they and
workers). The pair ((X find),(X apply)) is counted
once for narrative chain induction, and ((X find),
(X apply), workers) once for argument induction.
Figure 1 shows the top occurring words across
all event slot pairs in a criminal scenario chain.
This chain will be part of a larger narrative
schema, described in section 3.4.
3.3 Event Slot Similarity with Arguments
We now formalize event slot similarity with argu-
ments. Narrative chains as defined in (Chambers
and Jurafsky, 2008) score a new event slot ?f, g?
against a chain of size n by summing over the
scores between all pairs:
chainsim(C, ?f, g?) =
nX
i=1
sim(?ei, di? , ?f, g?) (1)
where C is a narrative chain, f is a verb with
grammatical argument g, and sim(e, e?) is the
pointwise mutual information pmi(e, e?). Grow-
ing a chain by one adds the highest scoring event.
We extend this function to include argument
types by defining similarity in the context of a spe-
cific argument a:
sim(?e, d? ,
?
e?, d?
?
, a) =
pmi(?e, d? ,
?
e?, d?
?
) + ? log freq(?e, d? ,
?
e?, d?
?
, a)
(2)
where ? is a constant weighting factor and
freq(b, b?, a) is the corpus count of a filling the
arguments of events b and b?. We then score the
entire chain for a particular argument:
score(C, a) =
n?1X
i=1
nX
j=i+1
sim(?ei, di? , ?ej , dj? , a) (3)
Using this chain score, we finally extend
chainsim to score a new event slot based on the
argument that maximizes the entire chain?s score:
chainsim?(C, ?f, g?) =
max
a
(score(C, a) +
nX
i=1
sim(?ei, di? , ?f, g? , a))
(4)
The argument is now directly influencing event
slot similarity scores. We will use this definition
in the next section to build Narrative Schemas.
3.4 Narrative Schema: Multiple Chains
Whereas a narrative chain is a set of event slots,
a Narrative Schema is a set of typed narrative
chains. A schema thus models all actors in a set
of events. If (push X) is in one chain, (Y push) is
in another. This allows us to model a document?s
entire narrative, not just one main actor.
3.4.1 The Model
A narrative schema is defined as a 2-tuple N =
(E,C) with E a set of events (here defined as
verbs) and C a set of typed chains over the
event slots. We represent an event as a verb v
and its grammatical argument positions Dv ?
{subject, object, prep}. Thus, each event slot
?v, d? for all d ? Dv belongs to a chain c ? C
in the schema. Further, each c must be unique for
each slot of a single verb. Using the criminal pros-
ecution domain as an example, a narrative schema
in this domain is built as in figure 2.
The three dotted boxes are graphical represen-
tations of the typed chains that are combined in
this schema. The first represents the event slots in
which the criminal is involved, the second the po-
lice, and the third is a court or judge. Although our
representation uses a set of chains, it is equivalent
to represent a schema as a constraint satisfaction
problem between ?e, d? event slots. The next sec-
tion describes how to learn these schemas.
3.4.2 Learning Narrative Schemas
Previous work on narrative chains focused on re-
latedness scores between pairs of verb arguments
(event slots). The clustering step which built
chains depended on these pairwise scores. Narra-
tive schemas use a generalization of the entire verb
with all of its arguments. A joint decision can be
made such that a verb is added to a schema if both
its subject and object are assigned to chains in the
schema with high confidence.
For instance, it may be the case that (Y
pull over) scores well with the ?police? chain in
605
police,
agent
criminal,
suspect
guilty,
innocent
judge,
jury
arrest
charge
convict
sentence
arrest
charge
convict
plead
sentence
police,agent
judge,jury
arrest
charge
convict
plead
sentence
criminal,suspect
Figure 2: Merging typed chains into a single unordered Narrative Schema.
figure 3. However, the object of (pull over A)
is not present in any of the other chains. Police
pull over cars, but this schema does not have a
chain involving cars. In contrast, (Y search) scores
well with the ?police? chain and (search X) scores
well in the ?defendant? chain too. Thus, we want
to favor search instead of pull over because the
schema is already modeling both arguments.
This intuition leads us to our event relatedness
function for the entire narrative schema N , not
just one chain. Instead of asking which event slot
?v, d? is a best fit, we ask if v is best by considering
all slots at once:
narsim(N, v) =
?
d?Dv
max(?, max
c?CN
chainsim?(c, ?v, d?)) (5)
whereCN is the set of chains in our narrativeN . If
?v, d? does not have strong enough similarity with
any chain, it creates a new one with base score ?.
The ? parameter balances this decision of adding
to an existing chain in N or creating a new one.
3.4.3 Building Schemas
We use equation 5 to build schemas from the set
of events as opposed to the set of event slots that
previous work on narrative chains used. In Cham-
bers and Jurafsky (2008), narrative chains add the
best ?e, d? based on the following:
max
j:0<j<m
chainsim(c, ?vj , gj?) (6)
where m is the number of seen event slots in the
corpus and ?vj , gj? is the jth such possible event
slot. Schemas are now learned by adding events
that maximize equation 5:
max
j:0<j<|v|
narsim(N, vj) (7)
where |v| is the number of observed verbs and vj
is the jth such verb. Verbs are incrementally added
to a narrative schema by strength of similarity.
arrest
charge
seize
confiscate
defendant, nichols, 
smith, simpson
police, agent, 
authorities, government
license
immigrant, reporter, 
cavalo, migrant, alien
detain
deport 
raid
Figure 3: Graphical view of an unordered schema
automatically built starting from the verb ?arrest?.
A ? value that encouraged splitting was used.
4 Sample Narrative Schemas
Figures 3 and 4 show two criminal schemas
learned completely automatically from the NYT
portion of the Gigaword Corpus (Graff, 2002).
We parse the text into dependency graphs and re-
solve coreferences. The figures result from learn-
ing over the event slot counts. In addition, figure 5
shows six of the top 20 scoring narrative schemas
learned by our system. We artificially required the
clustering procedure to stop (and sometimes con-
tinue) at six events per schema. Six was chosen
as the size to enable us to compare to FrameNet
in the next section; the mean number of verbs in
FrameNet frames is between five and six. A low
? was chosen to limit chain splitting. We built a
new schema starting from each verb that occurs in
more than 3000 and less than 50,000 documents
in the NYT section. This amounted to approxi-
mately 1800 verbs from which we show the top
20. Not surprisingly, most of the top schemas con-
cern business, politics, crime, or food.
5 Frames and Roles
Most previous work on unsupervised semantic
role labeling assumes that the set of possible
606
A produce B
A sell B
A manufacture B
A *market B
A distribute B
A -develop B
A ? {company, inc, corp, microsoft,
iraq, co, unit, maker, ...}
B ? {drug, product, system, test,
software, funds, movie, ...}
B trade C
B fell C
A *quote B
B fall C
B -slip C
B rise C
A ? {}
B ? {dollar, share, index, mark, currency,
stock, yield, price, pound, ...}
C ? {friday, most, year, percent, thursday
monday, share, week, dollar, ...}
A boil B
A slice B
A -peel B
A saute B
A cook B
A chop B
A ? {wash, heat, thinly, onion, note}
B ? {potato, onion, mushroom, clove,
orange, gnocchi }
A detain B
A confiscate B
A seize B
A raid B
A search B
A arrest B
A ? {police, agent, officer, authorities,
troops, official, investigator, ... }
B ? {suspect, government, journalist,
monday, member, citizen, client, ... }
A *uphold B
A *challenge B
A rule B
A enforce B
A *overturn B
A *strike down B
A ? {court, judge, justice, panel, osteen,
circuit, nicolau, sporkin, majority, ...}
B ? {law, ban, rule, constitutionality,
conviction, ruling, lawmaker, tax, ...}
A own B
A *borrow B
A sell B
A buy back B
A buy B
A *repurchase B
A ? {company, investor, trader, corp,
enron, inc, government, bank, itt, ...}
B ? {share, stock, stocks, bond, company,
security, team, funds, house, ... }
Figure 5: Six of the top 20 scored Narrative Schemas. Events and arguments in italics were marked
misaligned by FrameNet definitions. * indicates verbs not in FrameNet. - indicates verb senses not in
FameNet.
found
convict
acquit
defendant, nichols, 
smith, simpson
jury, juror, court, 
judge, tribunal, senate
sentence
deliberate
deadlocked
Figure 4: Graphical view of an unordered schema
automatically built from the verb ?convict?. Each
node shape is a chain in the schema.
classes is very small (i.e, PropBank roles ARG0
and ARG1) and is known in advance. By con-
trast, our approach induces sets of entities that ap-
pear in the argument positions of verbs in a nar-
rative schema. Our model thus does not assume
the set of roles is known in advance, and it learns
the roles at the same time as clustering verbs into
frame-like schemas. The resulting sets of entities
(such as {police, agent, authorities, government}
or {court, judge, justice}) can be viewed as a kind
of schema-specific semantic role.
How can this unsupervised method of learning
roles be evaluated? In Section 6 we evaluate the
schemas together with their arguments in a cloze
task. In this section we perform a more qualitative
evalation by comparing our schema to FrameNet.
FrameNet (Baker et al, 1998) is a database of
frames, structures that characterize particular sit-
uations. A frame consists of a set of events (the
verbs and nouns that describe them) and a set
of frame-specific semantic roles called frame el-
ements that can be arguments of the lexical units
in the frame. FrameNet frames share commonali-
ties with narrative schemas; both represent aspects
of situations in the world, and both link semanti-
cally related words into frame-like sets in which
each predicate draws its argument roles from a
frame-specific set. They differ in that schemas fo-
cus on events in a narrative, while frames focus on
events that share core participants. Nonetheless,
the fact that FrameNet defines frame-specific ar-
gument roles suggests that comparing our schemas
and roles to FrameNet would be elucidating.
We took the 20 learned narrative schemas de-
scribed in the previous section and used FrameNet
to perform qualitative evaluations on three aspects
of schema: verb groupings, linking structure (the
mapping of each argument role to syntactic sub-
ject or object), and the roles themselves (the set of
entities that constitutes the schema roles).
Verb groupings To compare a schema?s event
selection to a frame?s lexical units, we first map
the top 20 schemas to the FrameNet frames that
have the largest overlap with each schema?s six
verbs. We were able to map 13 of our 20 narra-
tives to FrameNet (for the remaining 7, no frame
contained more than one of the six verbs). The
remaining 13 schemas contained 6 verbs each for
a total of 78 verbs. 26 of these verbs, however,
did not occur in FrameNet, either at all, or with
the correct sense. Of the remaining 52 verb map-
pings, 35 (67%) occurred in the closest FrameNet
frame or in a frame one link away. 17 verbs (33%)
607
occurred in a different frame than the one chosen.
We examined the 33% of verbs that occurred in
a different frame. Most occurred in related frames,
but did not have FrameNet links between them.
For instance, one schema includes the causal verb
trade with unaccusative verbs of change like rise
and fall. FrameNet separates these classes of verbs
into distinct frames, distinguishing motion frames
from caused-motion frames.
Even though trade and rise are in different
FrameNet frames, they do in fact have the narra-
tive relation that our system discovered. Of the 17
misaligned events, we judged all but one to be cor-
rect in a narrative sense. Thus although not exactly
aligned with FrameNet?s notion of event clusters,
our induction algorithm seems to do very well.
Linking structure Next, we compare a
schema?s linking structure, the grammatical
relation chosen for each verb event. We thus
decide, e.g., if the object of the verb arrest (arrest
B) plays the same role as the object of detain
(detain B), or if the subject of detain (B detain)
would have been more appropriate.
We evaluated the clustering decisions of the 13
schemas (78 verbs) that mapped to frames. For
each chain in a schema, we identified the frame
element that could correctly fill the most verb ar-
guments in the chain. The remaining arguments
were considered incorrect. Because we assumed
all verbs to be transitive, there were 156 arguments
(subjects and objects) in the 13 schema. Of these
156 arguments, 151 were correctly clustered to-
gether, achieving 96.8% accuracy.
The schema in figure 5 with events detain, seize,
arrest, etc. shows some of these errors. The object
of all of these verbs is an animate theme, but con-
fiscate B and raid B are incorrect; people cannot
be confiscated/raided. They should have been split
into their own chain within the schema.
Argument Roles Finally, we evaluate the
learned sets of entities that fill the argument slots.
As with the above linking evaluation, we first iden-
tify the best frame element for each argument. For
example, the events in the top left schema of fig-
ure 5 map to the Manufacturing frame. Argument
B was identified as the Product frame element. We
then evaluate the top 10 arguments in the argument
set, judging whether each is a reasonable filler of
the role. In our example, drug and product are cor-
rect Product arguments. An incorrect argument is
test, as it was judged that a test is not a product.
We evaluated all 20 schemas. The 13 mapped
schemas used their assigned frames, and we cre-
ated frame element definitions for the remaining 7
that were consistent with the syntactic positions.
There were 400 possible arguments (20 schemas,
2 chains each), and 289 were judged correct for a
precision of 72%. This number includes Person
and Organization names as correct fillers. A more
conservative metric removing these classes results
in 259 (65%) correct.
Most of the errors appear to be from parsing
mistakes. Several resulted from confusing objects
with adjuncts. Others misattached modifiers, such
as including most as an argument. The cooking
schema appears to have attached verbal arguments
learned from instruction lists (wash, heat, boil).
Two schemas require situations as arguments, but
the dependency graphs chose as arguments the
subjects of the embedded clauses, resulting in 20
incorrect arguments in these schema.
6 Evaluation: Cloze
The previous section compared our learned knowl-
edge to current work in event and role semantics.
We now provide a more formal evaluation against
untyped narrative chains. The two main contribu-
tions of schema are (1) adding typed arguments
and (2) considering joint chains in one model. We
evaluate each using the narrative cloze test as in
(Chambers and Jurafsky, 2008).
6.1 Narrative Cloze
The cloze task (Taylor, 1953) evaluates human un-
derstanding of lexical units by removing a random
word from a sentence and asking the subject to
guess what is missing. The narrative cloze is a
variation on this idea that removes an event slot
from a known narrative chain.Performance is mea-
sured by the position of the missing event slot in a
system?s ranked guess list.
This task is particularly attractive for narrative
schemas (and chains) because it aligns with one
of the original ideas behind Schankian scripts,
namely that scripts help humans ?fill in the blanks?
when language is underspecified.
6.2 Training and Test Data
We count verb pairs and shared arguments over
the NYT portion of the Gigaword Corpus (years
1994-2004), approximately one million articles.
608
1995 1996 1997 1998 1999 2000 2001 2002 2003 20041000
1050
1100
1150
1200
1250
1300
1350
Training Data from 1994?X
Ran
ked
 Po
sitio
n
Narrative Cloze Test
 
  Chain Typed Chain Schema Typed Schema
Figure 6: Results with varying sizes of training
data.
We parse the text into typed dependency graphs
with the Stanford Parser (de Marneffe et al, 2006),
recording all verbs with subject, object, or prepo-
sitional typed dependencies. Unlike in (Chambers
and Jurafsky, 2008), we lemmatize verbs and ar-
gument head words. We use the OpenNLP1 coref-
erence engine to resolve entity mentions.
The test set is the same as in (Chambers and Ju-
rafsky, 2008). 100 random news articles were se-
lected from the 2001 NYT section of the Gigaword
Corpus. Articles that did not contain a protagonist
with five or more events were ignored, leaving a
test set of 69 articles. We used a smaller develop-
ment set of size 17 to tune parameters.
6.3 Typed Chains
The first evaluation compares untyped against
typed narrative event chains. The typed model
uses equation 4 for chain clustering. The dotted
line ?Chain? and solid ?Typed Chain? in figure 6
shows the average ranked position over the test set.
The untyped chains plateau and begin to worsen
as the amount of training data increases, but the
typed model is able to improve for some time af-
ter. We see a 6.9% gain at 2004 when both lines
trend upwards.
6.4 Narrative Schema
The second evaluation compares the performance
of the narrative schema model against single nar-
rative chains. We ignore argument types and use
untyped chains in both (using equation 1 instead
1http://opennlp.sourceforge.net/
of 4). The dotted line ?Chain? and solid ?Schema?
show performance results in figure 6. Narrative
Schemas have better ranked scores in all data sizes
and follow the previous experiment in improving
results as more data is added even though untyped
chains trend upward. We see a 3.3% gain at 2004.
6.5 Typed Narrative Schema
The final evaluation combines schemas with ar-
gument types to measure overall gain. We eval-
uated with both head words and CBC clusters
as argument representations. Not only do typed
chains and schemas outperform untyped chains,
combining the two gives a further performance
boost. Clustered arguments improve the re-
sults further, helping with sparse argument counts
(?Typed Schema? in figure 6 uses CBC argu-
ments). Overall, using all the data (by year 2004)
shows a 10.1% improvement over untyped narra-
tive chains.
7 Discussion
Our significant improvement in the cloze evalua-
tion shows that even though narrative cloze does
not evaluate argument types, jointly modeling the
arguments with events improves event cluster-
ing. Likewise, the FrameNet comparison suggests
that modeling related events helps argument learn-
ing. The tasks mutually inform each other. Our
argument learning algorithm not only performs
unsupervised induction of situation-specific role
classes, but the resulting roles and linking struc-
tures may also offer the possibility of (unsuper-
vised) FrameNet-style semantic role labeling.
Finding the best argument representation is an
important future direction. The performance of
our noun clusters in figure 6 showed that while the
other approaches leveled off, clusters continually
improved with more data. The exact balance be-
tween lexical units, clusters, or more general (tra-
ditional) semantic roles remains to be solved, and
may be application specific.
We hope in the future to show that a range of
NLU applications can benefit from the rich infer-
ential structures that narrative schemas provide.
Acknowledgments
This work is funded in part by NSF (IIS-0811974).
We thank the reviewers and the Stanford NLP
Group for helpful suggestions.
609
References
Afra Alishahi and Suzanne Stevenson. 2007. A com-
putational usage-based model for learning general
properties of semantic roles. In The 2nd European
Cognitive Science Conference, Delphi, Greece.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Christian
Boitet and Pete Whitelock, editors, ACL-98, pages
86?90, San Francisco, California. Morgan Kauf-
mann Publishers.
David Bean and Ellen Riloff. 2004. Unsupervised
learning of contextual role knowledge for corefer-
ence resolution. Proc. of HLT/NAACL, pages 297?
304.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08, Hawaii, USA.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
David Graff. 2002. English Gigaword. Linguistic
Data Consortium.
Rebecca Green and Bonnie J. Dorr. 2005. Frame se-
mantic enhancement of lexical-semantic resources.
In ACL-SIGLEX Workshop on Deep Lexical Acqui-
sition, pages 57?66.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In EMNLP.
Shan He and Daniel Gildea. 2006. Self-training and
co-training for semantic role labeling: Primary re-
port. Technical Report 891, University of Rochester.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In Proceedings of AAAI-2000, Austin, TX.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: A corpus annotated
with semantic roles. Computational Linguistics,
31(1):71?106.
Patrick Pantel and Dekang Lin. 2002. Document clus-
tering with committees. In ACM Conference on Re-
search and Development in Information Retrieval,
pages 199?206, Tampere, Finland.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding. Lawrence Erl-
baum.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In EMNLP.
Wilson L. Taylor. 1953. Cloze procedure: a new tool
for measuring readability. Journalism Quarterly,
30:415?433.
610
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1003?1011,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Distant supervision for relation extraction without labeled data
Mike Mintz, Steven Bills, Rion Snow, Dan Jurafsky
Stanford University / Stanford, CA 94305
{mikemintz,sbills,rion,jurafsky}@cs.stanford.edu
Abstract
Modern models of relation extraction for tasks like
ACE are based on supervised learning of relations
from small hand-labeled corpora. We investigate an
alternative paradigm that does not require labeled
corpora, avoiding the domain dependence of ACE-
style algorithms, and allowing the use of corpora
of any size. Our experiments use Freebase, a large
semantic database of several thousand relations, to
provide distant supervision. For each pair of enti-
ties that appears in some Freebase relation, we find
all sentences containing those entities in a large un-
labeled corpus and extract textual features to train
a relation classifier. Our algorithm combines the
advantages of supervised IE (combining 400,000
noisy pattern features in a probabilistic classifier)
and unsupervised IE (extracting large numbers of
relations from large corpora of any domain). Our
model is able to extract 10,000 instances of 102 re-
lations at a precision of 67.6%. We also analyze
feature performance, showing that syntactic parse
features are particularly helpful for relations that are
ambiguous or lexically distant in their expression.
1 Introduction
At least three learning paradigms have been ap-
plied to the task of extracting relational facts from
text (for example, learning that a person is em-
ployed by a particular organization, or that a ge-
ographic entity is located in a particular region).
In supervised approaches, sentences in a cor-
pus are first hand-labeled for the presence of en-
tities and the relations between them. The NIST
Automatic Content Extraction (ACE) RDC 2003
and 2004 corpora, for example, include over 1,000
documents in which pairs of entities have been la-
beled with 5 to 7 major relation types and 23 to
24 subrelations, totaling 16,771 relation instances.
ACE systems then extract a wide variety of lexi-
cal, syntactic, and semantic features, and use su-
pervised classifiers to label the relation mention
holding between a given pair of entities in a test
set sentence, optionally combining relation men-
tions (Zhou et al, 2005; Zhou et al, 2007; Sur-
deanu and Ciaramita, 2007).
Supervised relation extraction suffers from a
number of problems, however. Labeled training
data is expensive to produce and thus limited in
quantity. Also, because the relations are labeled
on a particular corpus, the resulting classifiers tend
to be biased toward that text domain.
An alternative approach, purely unsupervised
information extraction, extracts strings of words
between entities in large amounts of text, and
clusters and simplifies these word strings to pro-
duce relation-strings (Shinyama and Sekine, 2006;
Banko et al, 2007). Unsupervised approaches can
use very large amounts of data and extract very
large numbers of relations, but the resulting rela-
tions may not be easy to map to relations needed
for a particular knowledge base.
A third approach has been to use a very small
number of seed instances or patterns to do boot-
strap learning (Brin, 1998; Riloff and Jones, 1999;
Agichtein and Gravano, 2000; Ravichandran and
Hovy, 2002; Etzioni et al, 2005; Pennacchiotti
and Pantel, 2006; Bunescu and Mooney, 2007;
Rozenfeld and Feldman, 2008). These seeds are
used with a large corpus to extract a new set of
patterns, which are used to extract more instances,
which are used to extract more patterns, in an it-
erative fashion. The resulting patterns often suffer
from low precision and semantic drift.
We propose an alternative paradigm, distant su-
pervision, that combines some of the advantages
of each of these approaches. Distant supervision
is an extension of the paradigm used by Snow et
al. (2005) for exploiting WordNet to extract hyper-
nym (is-a) relations between entities, and is simi-
lar to the use of weakly labeled data in bioinfor-
matics (Craven and Kumlien, 1999; Morgan et al,
1003
Relation name New instance
/location/location/contains Paris, Montmartre
/location/location/contains Ontario, Fort Erie
/music/artist/origin Mighty Wagon, Cincinnati
/people/deceased person/place of death Fyodor Kamensky, Clearwater
/people/person/nationality Marianne Yvonne Heemskerk, Netherlands
/people/person/place of birth Wavell Wayne Hinds, Kingston
/book/author/works written Upton Sinclair, Lanny Budd
/business/company/founders WWE, Vince McMahon
/people/person/profession Thomas Mellon, judge
Table 1: Ten relation instances extracted by our system that did not appear in Freebase.
2004). Our algorithm uses Freebase (Bollacker et
al., 2008), a large semantic database, to provide
distant supervision for relation extraction. Free-
base contains 116 million instances of 7,300 rela-
tions between 9 million entities. The intuition of
distant supervision is that any sentence that con-
tains a pair of entities that participate in a known
Freebase relation is likely to express that relation
in some way. Since there may be many sentences
containing a given entity pair, we can extract very
large numbers of (potentially noisy) features that
are combined in a logistic regression classifier.
Thus whereas the supervised training paradigm
uses a small labeled corpus of only 17,000 rela-
tion instances as training data, our algorithm can
use much larger amounts of data: more text, more
relations, and more instances. We use 1.2 million
Wikipedia articles and 1.8 million instances of 102
relations connecting 940,000 entities. In addition,
combining vast numbers of features in a large clas-
sifier helps obviate problems with bad features.
Because our algorithm is supervised by a
database, rather than by labeled text, it does
not suffer from the problems of overfitting and
domain-dependence that plague supervised sys-
tems. Supervision by a database also means that,
unlike in unsupervised approaches, the output of
our classifier uses canonical names for relations.
Our paradigm offers a natural way of integrating
data from multiple sentences to decide if a relation
holds between two entities. Because our algorithm
can use large amounts of unlabeled data, a pair of
entities may occur multiple times in the test set.
For each pair of entities, we aggregate the features
from the many different sentences in which that
pair appeared into a single feature vector, allowing
us to provide our classifier with more information,
resulting in more accurate labels.
Table 1 shows examples of relation instances
extracted by our system. We also use this system
to investigate the value of syntactic versus lexi-
cal (word sequence) features in relation extraction.
While syntactic features are known to improve the
performance of supervised IE, at least using clean
hand-labeled ACE data (Zhou et al, 2007; Zhou
et al, 2005), we do not know whether syntactic
features can improve the performance of unsuper-
vised or distantly supervised IE. Most previous
research in bootstrapping or unsupervised IE has
used only simple lexical features, thereby avoid-
ing the computational expense of parsing (Brin,
1998; Agichtein and Gravano, 2000; Etzioni et al,
2005), and the few systems that have used unsu-
pervised IE have not compared the performance
of these two types of feature.
2 Previous work
Except for the unsupervised algorithms discussed
above, previous supervised or bootstrapping ap-
proaches to relation extraction have typically re-
lied on relatively small datasets, or on only a small
number of distinct relations. Approaches based on
WordNet have often only looked at the hypernym
(is-a) or meronym (part-of) relation (Girju et al,
2003; Snow et al, 2005), while those based on the
ACE program (Doddington et al, 2004) have been
restricted in their evaluation to a small number of
relation instances and corpora of less than a mil-
lion words.
Many early algorithms for relation extraction
used little or no syntactic information. For ex-
ample, the DIPRE algorithm by Brin (1998) used
string-based regular expressions in order to rec-
ognize relations such as author-book, while the
SNOWBALL algorithm by Agichtein and Gravano
(2000) learned similar regular expression patterns
over words and named entity tags. Hearst (1992)
used a small number of regular expressions over
words and part-of-speech tags to find examples of
the hypernym relation. The use of these patterns
has been widely replicated in successful systems,
for example by Etzioni et al (2005). Other work
1004
Relation name Size Example
/people/person/nationality 281,107 John Dugard, South Africa
/location/location/contains 253,223 Belgium, Nijlen
/people/person/profession 208,888 Dusa McDuff, Mathematician
/people/person/place of birth 105,799 Edwin Hubble, Marshfield
/dining/restaurant/cuisine 86,213 MacAyo?s Mexican Kitchen, Mexican
/business/business chain/location 66,529 Apple Inc., Apple Inc., South Park, NC
/biology/organism classification rank 42,806 Scorpaeniformes, Order
/film/film/genre 40,658 Where the Sidewalk Ends, Film noir
/film/film/language 31,103 Enter the Phoenix, Cantonese
/biology/organism higher classification 30,052 Calopteryx, Calopterygidae
/film/film/country 27,217 Turtle Diary, United States
/film/writer/film 23,856 Irving Shulman, Rebel Without a Cause
/film/director/film 23,539 Michael Mann, Collateral
/film/producer/film 22,079 Diane Eskenazi, Aladdin
/people/deceased person/place of death 18,814 John W. Kern, Asheville
/music/artist/origin 18,619 The Octopus Project, Austin
/people/person/religion 17,582 Joseph Chartrand, Catholicism
/book/author/works written 17,278 Paul Auster, Travels in the Scriptorium
/soccer/football position/players 17,244 Midfielder, Chen Tao
/people/deceased person/cause of death 16,709 Richard Daintree, Tuberculosis
/book/book/genre 16,431 Pony Soldiers, Science fiction
/film/film/music 14,070 Stavisky, Stephen Sondheim
/business/company/industry 13,805 ATS Medical, Health care
Table 2: The 23 largest Freebase relations we use, with their size and an instance of each relation.
such as Ravichandran and Hovy (2002) and Pan-
tel and Pennacchiotti (2006) use the same formal-
ism of learning regular expressions over words and
part-of-speech tags to discover patterns indicating
a variety of relations.
More recent approaches have used deeper syn-
tactic information derived from parses of the input
sentences, including work exploiting syntactic de-
pendencies by Lin and Pantel (2001) and Snow et
al. (2005), and work in the ACE paradigm such
as Zhou et al (2005) and Zhou et al (2007).
Perhaps most similar to our distant supervision
algorithm is the effective method of Wu and Weld
(2007) who extract relations from a Wikipedia
page by using supervision from the page?s infobox.
Unlike their corpus-specific method, which is spe-
cific to a (single) Wikipedia page, our algorithm
allows us to extract evidence for a relation from
many different documents, and from any genre.
3 Freebase
Following the literature, we use the term ?rela-
tion? to refer to an ordered, binary relation be-
tween entities. We refer to individual ordered pairs
in this relation as ?relation instances?. For ex-
ample, the person-nationality relation holds be-
tween the entities named ?John Steinbeck? and
?United States?, so it has ?John Steinbeck,
United States? as an instance.
We use relations and relation instances from
Freebase, a freely available online database of
structured semantic data. Data in Freebase is
collected from a variety of sources. One major
source is text boxes and other tabular data from
Wikipedia. Data is also taken from NNDB (bio-
graphical information), MusicBrainz (music), the
SEC (financial and corporate data), as well as di-
rect, wiki-style user editing. After some basic
processing of the July 2008 link export to con-
vert Freebase?s data representation into binary re-
lations, we have 116 million instances of 7,300
relations between 9 million entities. We next fil-
ter out nameless and uninteresting entities such as
user profiles and music tracks. Freebase also con-
tains the reverses of many of its relations (book-
author v. author-book), and these are merged. Fil-
tering and removing all but the largest relations
leaves us with 1.8 million instances of 102 rela-
tions connecting 940,000 entities. Examples are
shown in Table 2.
4 Architecture
The intuition of our distant supervision approach
is to use Freebase to give us a training set of rela-
tions and entity pairs that participate in those rela-
tions. In the training step, all entities are identified
1005
in sentences using a named entity tagger that la-
bels persons, organizations and locations. If a sen-
tence contains two entities and those entities are an
instance of one of our Freebase relations, features
are extracted from that sentence and are added to
the feature vector for the relation.
The distant supervision assumption is that if two
entities participate in a relation, any sentence that
contain those two entities might express that rela-
tion. Because any individual sentence may give
an incorrect cue, our algorithm trains a multiclass
logistic regression classifier, learning weights for
each noisy feature. In training, the features for
identical tuples (relation, entity1, entity2) from
different sentences are combined, creating a richer
feature vector.
In the testing step, entities are again identified
using the named entity tagger. This time, every
pair of entities appearing together in a sentence is
considered a potential relation instance, and when-
ever those entities appear together, features are ex-
tracted on the sentence and added to a feature vec-
tor for that entity pair. For example, if a pair of
entities occurs in 10 sentences in the test set, and
each sentence has 3 features extracted from it, the
entity pair will have 30 associated features. Each
entity pair in each sentence in the test corpus is run
through feature extraction, and the regression clas-
sifier predicts a relation name for each entity pair
based on the features from all of the sentences in
which it appeared.
Consider the location-contains relation, imag-
ining that in Freebase we had two instances of
this relation: ?Virginia, Richmond? and
?France, Nantes?. As we encountered sen-
tences like ?Richmond, the capital of Virginia? and
?Henry?s Edict of Nantes helped the Protestants of
France? we would extract features from these sen-
tences. Some features would be very useful, such
as the features from the Richmond sentence, and
some would be less useful, like those from the
Nantes sentence. In testing, if we came across
a sentence like ?Vienna, the capital of Austria?,
one or more of its features would match those of
the Richmond sentence, providing evidence that
?Austria, Vienna? belongs to the location-
contains relation.
Note that one of the main advantages of our
architecture is its ability to combine informa-
tion from many different mentions of the same
relation. Consider the entity pair ?Steven
Spielberg, Saving Private Ryan?
from the following two sentences, as evidence for
the film-director relation.
[Steven Spielberg]?s film [Saving Private
Ryan] is loosely based on the brothers? story.
Allison co-produced the Academy Award-
winning [Saving Private Ryan], directed by
[Steven Spielberg]...
The first sentence, while providing evidence for
film-director, could instead be evidence for film-
writer or film-producer. The second sentence does
not mention that Saving Private Ryan is a film, and
so could instead be evidence for the CEO relation
(consider ?Robert Mueller directed the FBI?). In
isolation, neither of these features is conclusive,
but in combination, they are.
5 Features
Our features are based on standard lexical and syn-
tactic features from the literature. Each feature
describes how two entities are related in a sen-
tence, using either syntactic or non-syntactic in-
formation.
5.1 Lexical features
Our lexical features describe specific words be-
tween and surrounding the two entities in the sen-
tence in which they appear:
? The sequence of words between the two entities
? The part-of-speech tags of these words
? A flag indicating which entity came first in the sentence
? A window of k words to the left of Entity 1 and their
part-of-speech tags
? A window of k words to the right of Entity 2 and their
part-of-speech tags
Each lexical feature consists of the conjunction of
all these components. We generate a conjunctive
feature for each k ? {0, 1, 2}. Thus each lexical
row in Table 3 represents a single lexical feature.
Part-of-speech tags were assigned by a max-
imum entropy tagger trained on the Penn Tree-
bank, and then simplified into seven categories:
nouns, verbs, adverbs, adjectives, numbers, for-
eign words, and everything else.
In an attempt to approximate syntactic features,
we also tested variations on our lexical features:
(1) omitting all words that are not verbs and (2)
omitting all function words. In combination with
the other lexical features, they gave a small boost
to precision, but not large enough to justify the in-
creased demand on our computational resources.
1006
Feature type Left window NE1 Middle NE2 Right window
Lexical [] PER [was/VERB born/VERB in/CLOSED] LOC []
Lexical [Astronomer] PER [was/VERB born/VERB in/CLOSED] LOC [,]
Lexical [#PAD#, Astronomer] PER [was/VERB born/VERB in/CLOSED] LOC [, Missouri]
Syntactic [] PER [?s was ?pred born ?mod in ?pcomp?n] LOC []
Syntactic [Edwin Hubble ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC []
Syntactic [Astronomer ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC []
Syntactic [] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?lex?mod ,]
Syntactic [Edwin Hubble ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?lex?mod ,]
Syntactic [Astronomer ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?lex?mod ,]
Syntactic [] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?inside Missouri]
Syntactic [Edwin Hubble ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?inside Missouri]
Syntactic [Astronomer ?lex?mod] PER [?s was ?pred born ?mod in ?pcomp?n] LOC [?inside Missouri]
Table 3: Features for ?Astronomer Edwin Hubble was born in Marshfield, Missouri?.
Astronomer Edwin Hubble was born in Marshfield , Missouri
lex-mod s pred mod pcomp-n lex-mod
inside
Figure 1: Dependency parse with dependency path from ?Edwin Hubble? to ?Marshfield? highlighted in
boldface.
5.2 Syntactic features
In addition to lexical features we extract a num-
ber of features based on syntax. In order to gener-
ate these features we parse each sentence with the
broad-coverage dependency parser MINIPAR (Lin,
1998).
A dependency parse consists of a set of words
and chunks (e.g. ?Edwin Hubble?, ?Missouri?,
?born?), linked by directional dependencies (e.g.
?pred?, ?lex-mod?), as in Figure 1. For each
sentence we extract a dependency path between
each pair of entities. A dependency path con-
sists of a series of dependencies, directions and
words/chunks representing a traversal of the parse.
Part-of-speech tags are not included in the depen-
dency path.
Our syntactic features are similar to those used
in Snow et al (2005). They consist of the conjunc-
tion of:
? A dependency path between the two entities
? For each entity, one ?window? node that is not part of
the dependency path
A window node is a node connected to one of the
two entities and not part of the dependency path.
We generate one conjunctive feature for each pair
of left and right window nodes, as well as features
which omit one or both of them. Thus each syn-
tactic row in Table 3 represents a single syntactic
feature.
5.3 Named entity tag features
Every feature contains, in addition to the content
described above, named entity tags for the two en-
tities. We perform named entity tagging using the
Stanford four-class named entity tagger (Finkel et
al., 2005). The tagger provides each word with a
label from {person, location, organization, miscel-
laneous, none}.
5.4 Feature conjunction
Rather than use each of the above features in the
classifier independently, we use only conjunctive
features. Each feature consists of the conjunc-
tion of several attributes of the sentence, plus the
named entity tags. For two features to match,
all of their conjuncts must match exactly. This
yields low-recall but high-precision features. With
a small amount of data, this approach would be
problematic, since most features would only be
seen once, rendering them useless to the classifier.
Since we use large amounts of data, even complex
features appear multiple times, allowing our high-
precision features to work as intended. Features
for a sample sentence are shown in Table 3.
6 Implementation
6.1 Text
For unstructured text we use the Freebase
Wikipedia Extraction, a dump of the full text of all
Wikipedia articles (not including discussion and
1007
Relation Feature type Left window NE1 Middle NE2 Right window
/architecture/structure/architect LEXx ORG , the designer of the PER
SYN designed ?s ORG ?s designed ?by?subj by ?pcn PER ?s designed
/book/author/works written LEX PER s novel ORG
SYN PER ?pcn by ?mod story ?pred is ?s ORG
/book/book edition/author editor LEXx ORG s novel PER
SYN PER ?nn series ?gen PER
/business/company/founders LEX ORG co - founder PER
SYN ORG ?nn owner ?person PER
/business/company/place founded LEXx ORG - based LOC
SYN ORG ?s founded ?mod in ?pcn LOC
/film/film/country LEX PER , released in LOC
SYN opened ?s ORG ?s opened ?mod in ?pcn LOC ?s opened
/geography/river/mouth LEX LOC , which flows into the LOC
SYN the ?det LOC ?s is ?pred tributary ?mod of ?pcn LOC ?det the
/government/political party/country LEXx ORG politician of the LOC
SYN candidate ?nn ORG ?nn candidate ?mod for ?pcn LOC ?nn candidate
/influence/influence node/influenced LEXx PER , a student of PER
SYN of ?pcn PER ?pcn of ?mod student ?appo PER ?pcn of
/language/human language/region LEX LOC - speaking areas of LOC
SYN LOC ?lex?mod speaking areas ?mod of ?pcn LOC
/music/artist/origin LEXx ORG based band LOC
SYN is ?s ORG ?s is ?pred band ?mod from ?pcn LOC ?s is
/people/deceased person/place of death LEX PER died in LOC
SYN hanged ?s PER ?s hanged ?mod in ?pcn LOC ?s hanged
/people/person/nationality LEX PER is a citizen of LOC
SYN PER ?mod from ?pcn LOC
/people/person/parents LEX PER , son of PER
SYN father ?gen PER ?gen father ?person PER ?gen father
/people/person/place of birth LEXx PER is the birthplace of PER
SYN PER ?s born ?mod in ?pcn LOC
/people/person/religion LEX PER embraced LOC
SYN convert ?appo PER ?appo convert ?mod to ?pcn LOC ?appo convert
Table 4: Examples of high-weight features for several relations. Key: SYN = syntactic feature; LEX =
lexical feature;x = reversed; NE# = named entity tag of entity.
user pages) which has been sentence-tokenized by
Metaweb Technologies, the developers of Free-
base (Metaweb, 2008). This dump consists of
approximately 1.8 million articles, with an av-
erage of 14.3 sentences per article. The total
number of words (counting punctuation marks) is
601,600,703. For our experiments we use about
half of the articles: 800,000 for training and
400,000 for testing.
We use Wikipedia because it is relatively up-
to-date, and because its sentences tend to make
explicit many facts that might be omitted in
newswire. Much of the information in Freebase is
derived from tabular data from Wikipedia, mean-
ing that Freebase relations are more likely to ap-
pear in sentences in Wikipedia.
6.2 Parsing and chunking
Each sentence of this unstructured text is depen-
dency parsed by MINIPAR to produce a depen-
dency graph.
In preprocessing, consecutive words with the
same named entity tag are ?chunked?, so that
Edwin/PERSON Hubble/PERSON becomes
[Edwin Hubble]/PERSON. This chunking is
restricted by the dependency parse of the sentence,
however, in that chunks must be contiguous in
the parse (i.e., no chunks across subtrees). This
ensures that parse tree structure is preserved, since
the parses must be updated to reflect the chunking.
6.3 Training and testing
For held-out evaluation experiments (see section
7.1), half of the instances of each relation are not
used in training, and are later used to compare
against newly discovered instances. This means
that 900,000 Freebase relation instances are used
in training, and 900,000 are held out. These ex-
periments used 800,000 Wikipedia articles in the
training phase and 400,000 different articles in the
testing phase.
For human evaluation experiments, all 1.8 mil-
lion relation instances are used in training. Again,
we use 800,000 Wikipedia articles in the training
phase and 400,000 different articles in the testing
phase.
For all our experiments, we only extract relation
instances that do not appear in our training data,
i.e., instances that are not already in Freebase.
Our system needs negative training data for the
purposes of constructing the classifier. Towards
this end, we build a feature vector in the train-
ing phase for an ?unrelated? relation by randomly
selecting entity pairs that do not appear in any
Freebase relation and extracting features for them.
While it is possible that some of these entity pairs
1008
0?
0.1?
0.2?
0.3?
0.4?
0.5?
0.6?
0.7?
0.8?
0.9?
1?
0? 0.05? 0.1? 0.15? 0.2? 0.25? 0.3? 0.35? 0.4? 0.45?
Pr
ec
isi
on
?
Oracle?recall?
Both?
Syntax?
Surface?
Figure 2: Automatic evaluation with 50% of Freebase relation data held out and 50% used in training
on the 102 largest relations we use. Precision for three different feature sets (lexical features, syntactic
features, and both) is reported at recall levels from 10 to 100,000. At the 100,000 recall level, we classify
most of the instances into three relations: 60% as location-contains, 13% as person-place-of-birth, and
10% as person-nationality.
are in fact related but are wrongly omitted from
the Freebase data, we expect that on average these
false negatives will have a small effect on the per-
formance of the classifier. For performance rea-
sons, we randomly sample 1% of such entity pairs
for use as negative training examples. By contrast,
in the actual test data, 98.7% of the entity pairs we
extract do not possess any of the top 102 relations
we consider in Freebase.
We use a multi-class logistic classifier opti-
mized using L-BFGS with Gaussian regulariza-
tion. Our classifier takes as input an entity pair
and a feature vector, and returns a relation name
and a confidence score based on the probability of
the entity pair belonging to that relation. Once all
of the entity pairs discovered during testing have
been classified, they can be ranked by confidence
score and used to generate a list of the n most
likely new relation instances.
Table 4 shows some high-weight features
learned by our system. We discuss the results in
the next section.
7 Evaluation
We evaluate labels in two ways: automatically,
by holding out part of the Freebase relation data
during training, and comparing newly discovered
relation instances against this held-out data, and
manually, having humans who look at each posi-
tively labeled entity pair and mark whether the re-
lation indeed holds between the participants. Both
evaluations allow us to calculate the precision of
the system for the best N instances.
7.1 Held-out evaluation
Figure 2 shows the performance of our classifier
on held-out Freebase relation data. While held-out
evaluation suffers from false negatives, it gives a
rough measure of precision without requiring ex-
pensive human evaluation, making it useful for pa-
rameter setting.
At most recall levels, the combination of syn-
tactic and lexical features offers a substantial im-
provement in precision over either of these feature
sets on its own.
7.2 Human evaluation
Human evaluation was performed by evaluators on
Amazon?s Mechanical Turk service, shown to be
effective for natural language annotation in Snow
et al (2008). We ran three experiments: one us-
ing only syntactic features; one using only lexical
features; and one using both syntactic and lexical
features. For each of the 10 relations that appeared
most frequently in our test data (according to our
classifier), we took samples from the first 100 and
1000 instances of this relation generated in each
experiment, and sent these to Mechanical Turk for
1009
Relation name
100 instances 1000 instances
Syn Lex Both Syn Lex Both
/film/director/film 0.49 0.43 0.44 0.49 0.41 0.46
/film/writer/film 0.70 0.60 0.65 0.71 0.61 0.69
/geography/river/basin countries 0.65 0.64 0.67 0.73 0.71 0.64
/location/country/administrative divisions 0.68 0.59 0.70 0.72 0.68 0.72
/location/location/contains 0.81 0.89 0.84 0.85 0.83 0.84
/location/us county/county seat 0.51 0.51 0.53 0.47 0.57 0.42
/music/artist/origin 0.64 0.66 0.71 0.61 0.63 0.60
/people/deceased person/place of death 0.80 0.79 0.81 0.80 0.81 0.78
/people/person/nationality 0.61 0.70 0.72 0.56 0.61 0.63
/people/person/place of birth 0.78 0.77 0.78 0.88 0.85 0.91
Average 0.67 0.66 0.69 0.68 0.67 0.67
Table 5: Estimated precision on human-evaluation experiments of the highest-ranked 100 and 1000
results per relation, using stratified samples. ?Average? gives the mean precision of the 10 relations. Key:
Syn = syntactic features only. Lex = lexical features only. We use stratified samples because of the
overabundance of location-contains instances among our high-confidence results.
human evaluation. Our sample size was 100.
Each predicted relation instance was labeled as
true or false by between 1 and 3 labelers on Me-
chanical Turk. We assigned the truth or falsehood
of each relation according to the majority vote of
the labels; in the case of a tie (one vote each way)
we assigned the relation as true or false with equal
probability. The evaluation of the syntactic, lexi-
cal, and combination of features at a recall of 100
and 1000 instances is presented in Table 5.
At a recall of 100 instances, the combination of
lexical and syntactic features has the best perfor-
mance for a majority of the relations, while at a re-
call level of 1000 instances the results are mixed.
No feature set strongly outperforms any of the oth-
ers across all relations.
8 Discussion
Our results show that the distant supervision algo-
rithm is able to extract high-precision patterns for
a reasonably large number of relations.
The held-out results in Figure 2 suggest that the
combination of syntactic and lexical features pro-
vides better performance than either feature set on
its own. In order to understand the role of syntactic
features, we examine Table 5, the human evalua-
tion of the most frequent 10 relations. For the top-
ranking 100 instances of each relation, most of the
best results use syntactic features, either alone or
in combination with lexical features. For the top-
ranking 1000 instances of each relation, the results
are more mixed, but syntactic features still helped
in most classifications.
We then examine those relations for which syn-
tactic features seem to help. For example, syn-
tactic features consistently outperform lexical fea-
tures for the director-film and writer-film relations.
As discussed in section 4, these two relations are
particularly ambiguous, suggesting that syntactic
features may help tease apart difficult relations.
Perhaps more telling, we noticed many examples
with a long string of words between the director
and the film:
Back Street is a 1932 film made by Univer-
sal Pictures, directed by John M. Stahl, and
produced by Carl Laemmle Jr.
Sentences like this have very long (and thus rare)
lexical features, but relatively short dependency
paths. Syntactic features can more easily abstract
from the syntactic modifiers that comprise the ex-
traneous parts of these strings.
Our results thus suggest that syntactic features
are indeed useful in distantly supervised informa-
tion extraction, and that the benefit of syntax oc-
curs in cases where the individual patterns are par-
ticularly ambiguous, and where they are nearby in
the dependency structure but distant in terms of
words. It remains for future work to see whether
simpler, chunk-based syntactic features might be
able to capture enough of this gain without the
overhead of full parsing, and whether coreference
resolution could improve performance.
Acknowledgments
We would like to acknowledge Sarah Spikes for
her help in developing the relation extraction sys-
tem, Christopher Manning and Mihai Surdeanu
for their invaluable advice, and Fuliang Weng
and Baoshi Yan for their guidance. Our research
was partially funded by the NSF via award IIS-
0811974 and by Robert Bosch LLC.
1010
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM Interna-
tional Conference on Digital Libraries.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In
Manuela M Veloso, editor, IJCAI-07, pages 2670?
2676.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In SIGMOD ?08, pages 1247?
1250, New York, NY. ACM.
Sergei Brin. 1998. Extracting patterns and relations
from the World Wide Web. In Proceedings World
Wide Web and Databases International Workshop,
Number 1590 in LNCS, pages 172?183. Springer.
Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using minimal
supervision. In ACL-07, pages 576?583, Prague,
Czech Republic, June.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Thomas Lengauer, Rein-
hard Schneider, Peer Bork, Douglas L. Brutlag, Jan-
ice I. Glasgow, Hans W. Mewes, and Ralf Zimmer,
editors, ISMB, pages 77?86. AAAI.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) Program?Tasks, Data, and Evaluation.
LREC-04, pages 837?840.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
An experimental study. Artificial Intelligence,
165(1):91?134.
Jenny R. Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL-05, pages 363?370, Ann Arbor,
MI.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In HLT-
NAACL-03, pages 1?8, Edmonton, Canada.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING-92,
Nantes, France.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question-answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Workshop on the Evaluation of Parsing
Systems.
Metaweb. 2008. Freebase data dumps. http://
download.freebase.com/datadumps/.
Alexander A. Morgan, Lynette Hirschman, Marc
Colosimo, Alexander S. Yeh, and Jeff B. Colombe.
2004. Gene name identification and normalization
using a model organism database. J. of Biomedical
Informatics, 37(6):396?410.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for auto-
matically harvesting semantic relations. In COL-
ING/ACL 2006, pages 113?120, Sydney, Australia.
Marco Pennacchiotti and Patrick Pantel. 2006. A boot-
strapping algorithm for automatically harvesting se-
mantic relations. In in Proceedings of Inference in
Computational Semantics (ICoS-06), pages 87?96.
Deepak Ravichandran and Eduard H. Hovy. 2002.
Learning surface text patterns for a question answer-
ing system. In ACL-02, pages 41?47, Philadelphia,
PA.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI-99, pages 474?479.
Benjamin Rozenfeld and Ronen Feldman. 2008. Self-
supervised relation extraction from the web. Knowl-
edge and Information Systems, 17(1):17?33.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In HLT-NAACL-06, pages 304?311,
New York, NY.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
Le?on Bottou, editors, NIPS 17, pages 1297?1304.
MIT Press.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural lan-
guage tasks. In EMNLP 2008, pages 254?263, Hon-
olulu, HI.
Mihai Surdeanu and Massimiliano Ciaramita. 2007.
Robust information extraction with perceptrons. In
Proceedings of the NIST 2007 Automatic Content
Extraction Workshop (ACE07), March.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In CIKM ?07: Proceedings
of the sixteenth ACM conference on Conference on
information and knowledge management, pages 41?
50, Lisbon, Portugal.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In ACL-05, pages 427?434, Ann Arbor,
MI.
Guodong Zhou, Min Zhang, Donghong Ji, and Qiaom-
ing Zhu. 2007. Tree kernel-based relation extrac-
tion with context-sensitive structured parse tree in-
formation. In EMNLP/CoNLL 2007.
1011
Semantic Role Labeling by Tagging Syntactic Chunks?
Kadri Hacioglu1, Sameer Pradhan1, Wayne Ward1, James H. Martin1, Daniel Jurafsky2
1University of Colorado at Boulder, 2Stanford University
{hacioglu,spradhan,whw}@cslr.colorado.edu, martin@cs.colorado.edu, jurafsky@stanford.edu
Abstract
In this paper, we present a semantic role la-
beler (or chunker) that groups syntactic chunks
(i.e. base phrases) into the arguments of a pred-
icate. This is accomplished by casting the se-
mantic labeling as the classification of syntactic
chunks (e.g. NP-chunk, PP-chunk) into one of
several classes such as the beginning of an ar-
gument (B-ARG), inside an argument (I-ARG)
and outside an argument (O). This amounts to
tagging syntactic chunks with semantic labels
using the IOB representation. The chunker is
realized using support vector machines as one-
versus-all classifiers. We describe the represen-
tation of data and information used to accom-
plish the task. We participate in the ?closed
challenge? of the CoNLL-2004 shared task and
report results on both development and test
sets.
1 Introduction
In semantic role labeling the goal is to group sequences
of words together and classify them by using semantic la-
bels. For meaning representation the predicate-argument
structure that exists in most languages is used. In this
structure a word (most frequently a verb) is specified as
a predicate, and a number of word groups are considered
as arguments accompanying the word (or predicate).
In this paper, we select support vector machines
(SVMs) (Vapnik, 1995; Burges, 1998) to implement
the semantic role classifiers, due to their ability to han-
dle an extremely large number of (overlapping) features
with quite strong generalization properties. Support vec-
tor machines for semantic role chunking were first used
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IIS-9978025
in (Hacioglu and Ward, 2003) as word-by-word (W-by-
W) classifiers. The system was then applied to the
constituent-by-constituent (C-by-C) classification in (Ha-
cioglu et al, 2003). In (Pradhan et al, 2003; Prad-
han et al, 2004), several extensions to the basic system
have been proposed, extensively studied and systemati-
cally compared to other systems. In this paper, we imple-
ment a system that classifies syntactic chunks (i.e. base
phrases) instead of words or the constituents derived from
syntactic trees. This system is referred to as the phrase-
by-phrase (P-by-P) semantic role classifier. We partici-
pate in the ?closed challenge? of the CoNLL-2004 shared
task and report results on both development and test sets.
A detailed description of the task, data and related work
can be found in (Carreras and Ma`rquez, 2004).
2 System Description
2.1 Data Representation
In this paper, we change the representation of the original
data as follows:
? Bracketed representation of roles is converted into
IOB2 representation (Ramhsaw and Marcus, 1995;
Sang and Veenstra, 1995)
? Word tokens are collapsed into base phrase (BP) to-
kens.
Since the semantic annotation in the PropBank corpus
does not have any embedded structure there is no loss of
information in the first change. However, this results in
a simpler representation with a reduced set of tagging la-
bels. In the second change, it is possible to miss some
information in cases where the semantic chunks do not
align with the sequence of BPs. However, in Section 3.2
we show that the loss in performance due to the misalign-
ment is much less than the gain in performance that can
be achieved by the change in representation.
from
million
251.2
$
to
declined VBD
CD
NN
IN
CD
$
TO
Sales
%
CD
10
$
278.7
$
CD
NNS
*A2)
*
*
*A4)
*
*
million CD I?NP
O
*
*
B?NP
B?VP
B?NP
I?NP
B?PP
B?NP
I?NP
I?NP
B?PP
B?NP
I?NP
*
(S*
*
*
*
*
*
*
*
*
*
decline
?
?
?
?
?
?
?
?
?
?
?
?
*A3)
(A3*
(A4*
(A2*
(V*V)
(A1*A1)
*S)
O
NP
VP
NP
PP
NP
PP
NP
Sales
declined
NNS
VBD
% NN
TOto
million
from
million
B?NP
CD
CD
O
I?NP
B?PP
I?NP
B?PP
I?NP
?
?
?
?
?
?
?
*
*
*
*
*
* B?V
B?A2
B?A1
B?A3
B?A4
O
O
O
B?VP
IN
*S)
(S*
(b)(a)
. .
. .
decline
Figure 1: Illustration of change in data representation; (a) original word-by-word data representation (b) phrase-by-
phrase data representation used in this paper. Words are collapsed into base phrase types retaining only headwords
with their respective features. Bracketed representation of semantic role labels is converted into IOB2 representation.
See text for details.
The new representation is illustrated in Figure 1 along
with the original representation. Comparing both we note
the following differences and advantages in the new rep-
resentation:
? BPs are being classified instead of words.
? Only the BP headwords (rightmost words) are re-
tained as word information.
? The number of tagging steps is smaller.
? A fixed context spans a larger segment of a sentence.
Therefore, the P-by-P semantic role chunker classifies
larger units, ignores some of the words, uses a relatively
larger context for a given window size and performs the
labeling faster.
2.2 Features
The following features, which we refer to as the base fea-
tures, are provided in the shared task data for each sen-
tence;
? Words
? Predicate lemmas
? Part of Speech tags
? BP Positions: The position of a token in a BP using
the IOB2 representation (e.g. B-NP, I-NP, O etc.)
? Clause tags: The tags that mark token positions in a
sentence with respect to clauses. (e.g *S)*S) marks
a position that two clauses end)
? Named entities: The IOB tags of named entities.
There are four categories; LOC, ORG, PERSON
and MISC.
Using available information we have created the fol-
lowing token level features:
? Token Position: The position of the phrase with re-
spect to the predicate. It has three values as ?be-
fore?, ?after? and ?-? for the predicate.
? Path: It defines a flat path between the token and
the predicate as a chain of base phrases. At both
ends, the chain is terminated with the POS tags of
the predicate and the headword of the token.
? Clause bracket patterns: We use two patterns of
clauses for each token. One is the clause bracket
chain between the token and the predicate, and the
other is from the token to sentence begin or end de-
pending on token?s position with respect to the pred-
icate.
? Clause Position: a binary feature that indicates the
token is inside or outside of the clause which con-
tains the predicate
? Headword suffixes: suffixes of headwords of length
2, 3 and 4.
? Distance: we have two notions of distance; the first
is the distance of the token from the predicate as a
number of base phrases, and the second is the same
distance as the number of VP chunks.
? Length: the number of words in a token.
We also use some sentence level features:
? Predicate POS tag: the part of speech category of
the predicate
? Predicate Frequency; this is a feature which indi-
cates whether the predicate is frequent or rare with
respect to the training set. The threshold on the
counts is currently set to 3.
? Predicate BP Context : The chain of BPs centered
at the predicate within a window of size -2/+2.
? Predicate POS Context : The POS tags of the
words that immediately precede and follow the pred-
icate. The POS tag of a preposition is replaced with
the preposition itself.
? Predicate Argument Frames: We used the left and
right patterns of the core arguments (A0 through A5)
for each predicate . We used the three most frequent
argument frames for both sides depending on the po-
sition of the token in focus with respect to the pred-
icate. (e.g. raise has A0 and A1 AO (A0 being the
most frequent) as its left argument frames, and A1,
A1 A2 and A2 as the three most frequent right argu-
ment frames)
? Number of predicates: This is the number of pred-
icates in the sentence.
For each token (base phrase) to be tagged, a set of or-
dered features is created from a fixed size context that
surrounds each token. In addition to the above features,
we also use previous semantic IOB tags that have already
been assigned to the tokens contained in the context. A
5-token sliding window is used for the context. A greedy
left-to-right tagging is performed.
All of the above features are designed to implicitly cap-
ture the patterns of sentence constructs with respect to
different word/predicate usages and senses. We acknowl-
edge that they significantly overlap and extensive exper-
iments are required to determine the impact of each fea-
ture on the performance.
2.3 Classifier
All SVM classifiers were realized using TinySVM1 with
a polynomial kernel of degree 2 and the general purpose
SVM based chunker YamCha 2. SVMs were trained for
begin (B) and inside (I) classes of all arguments and one
outside (O) class for a total of 78 one-vs-all classifiers
(some arguments do not have an I-tag).
1http://cl.aist-nara.ac.jp/taku-ku/software/TinySVM
2http://cl.aist-nara.ac.jp/taku-ku/software/yamcha
Table 1: Comparison of W-by-W and P-by-P methods.
Both systems use the base features provided (i.e. no fea-
ture engineering is done). Results are on dev set.
Method Precision Recall F?=1
P-by-P 69.04% 54.68% 61.02
W-by-W 68.34% 45.16% 54.39
Table 2: Number of sentences and unique training exam-
ples in each method.
Method Sentences Training Examples
P-by-P 19K 347K
W-by-W 19K 534K
3 Experimental Results
3.1 Data and Evaluation Metrics
The data provided for the shared task is a part of the
February 2004 release of the PropBank corpus. It con-
sists of sections from the Wall Street Journal part of the
Penn Treebank. All experiments were carried out using
Sections 15-18 for training Section-20 for development
and Section-21 for testing. The results were evaluated for
precision, recall and F?=1 numbers using the srl-eval.pl
script provided by the shared task organizers.
3.2 W-by-W and P-by-P Experiments
In these experiments we used only the base features to
compare the two approaches. Table 1 illustrates the over-
all performance on the dev set. Although both systems
were trained using the same number of sentences, the ac-
tual number of training examples in each case were quite
different. Those numbers are presented in Table 2. It is
clear that P-by-P method uses much less data for the same
number of sentences. Despite this we particularly note a
considerable improvement in recall. Actually, the data
reduction was not without a cost. Some arguments have
been missed as they do not align with the base phrase
chunks due to inconsistencies in semantic annotation and
due to errors in automatic base phrase chunking. The per-
centage of this misalignment was around 2.5% (over the
dev set). We observed that nearly 45% of the mismatches
were for the ?outside? chunks. Therefore, sequences of
words with outside tags were not collapsed.
3.3 Best System Results
In these experiments all of the features described earlier
were used with the P-by-P system. Table 3 presents our
best system performance on the development set. Ad-
ditional features have improved the performance from
61.02 to 71.72. The performance of the same system on
the test set is similarly illustrated in Table 4.
Table 3: System results on development set.
Precision Recall F?=1
Overall 74.17% 69.42% 71.72
A0 82.86% 78.50% 80.62
A1 72.82% 73.97% 73.39
A2 60.16% 56.18% 58.10
A3 59.66% 47.65% 52.99
A4 83.21% 74.15% 78.42
A5 100.00% 75.00% 85.71
AM-ADV 52.52% 41.48% 46.35
AM-CAU 61.11% 41.51% 49.44
AM-DIR 47.37% 15.00% 22.78
AM-DIS 76.47% 76.47% 76.47
AM-EXT 74.07% 40.82% 52.63
AM-LOC 51.21% 46.09% 48.51
AM-MNR 51.04% 36.83% 42.78
AM-MOD 99.47% 95.63% 97.51
AM-NEG 99.20% 94.66% 96.88
AM-PNC 70.00% 28.00% 40.00
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 69.33% 58.37% 63.38
R-A0 91.55% 80.25% 85.53
R-A1 72.46% 67.57% 69.93
R-A2 100.00% 52.94% 69.23
R-AM-LOC 100.00% 25.00% 40.00
R-AM-TMP 0.00% 0.00% 0.00
V 99.05% 99.05% 99.05
4 Conclusions
We have described a semantic role chunker using SVMs.
The chunking method has been based on a chunked sen-
tence structure at both syntactic and semantic levels. We
have jointly performed semantic chunk segmentation and
labeling using a set of one-vs-all SVM classifiers on a
phrase-by-phrase basis. It has been argued that the new
representation has several advantages as compared to the
original representation. It yields a semantic role labeler
that classifies larger units, exploits relatively larger con-
text, uses less data (possibly, redundant and noisy data
are filtered out), runs faster and performs better.
References
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the CoNLL-2004 Shared Task: Semantic Role La-
beling in the same volume of Proc. of CoNLL?2004
Shared Task.
Christopher J. C. Burges. 1997. A Tutorial on Support
Vector Machines for Pattern Recognition. Data Min-
ing and Knowledge Discovery, 2(2), pages 1-47.
Kadri Hacioglu and Wayne Ward. 2003. Target word
Table 4: System results on test set.
Precision Recall F?=1
Overall 72.43% 66.77% 69.49
A0 82.93% 79.88% 81.37
A1 71.92% 71.33% 71.63
A2 49.37% 49.30% 49.33
A3 57.50% 46.00% 51.11
A4 87.10% 54.00% 66.67
A5 0.00% 0.00% 0.00
AM-ADV 53.36% 38.76% 44.91
AM-CAU 57.89% 22.45% 32.35
AM-DIR 37.84% 28.00% 32.18
AM-DIS 66.83% 62.44% 64.56
AM-EXT 70.00% 50.00% 58.33
AM-LOC 46.63% 36.40% 40.89
AM-MNR 50.31% 31.76% 38.94
AM-MOD 98.12% 92.88% 95.43
AM-NEG 91.11% 96.85% 93.89
AM-PNC 52.00% 15.29% 23.64
AM-PRD 0.00% 0.00% 0.00
AM-TMP 64.57% 50.74% 56.82
R-A0 90.21% 81.13% 85.43
R-A1 83.02% 62.86% 71.54
R-A2 100.00% 33.33% 50.00
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 60.00% 21.43% 31.58
V 98.46% 98.46% 98.46
Detection and Semantic Role Chunking Using Support
Vector Machines. Proc. of the HLT-NAACL-03.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James
Martin, and Dan Jurafsky. 2003. Shallow Semantic
Parsing Using Support Vector Machines. CSLR Tech.
Report, CSLR-TR-2003-1.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James
Martin, and Dan Jurafsky. 2003. Semantic Role Pars-
ing: Adding Semantic Structure to Unstructured Text.
Proc. of Int. Conf. on Data Mining (ICDM03).
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James
Martin, and Dan Jurafsky. 2004. Support Vector
Learning for Semantic Argument Classification. to ap-
pear in Journal of Machine Learning.
Lance E. Ramhsaw and Mitchell P. Marcus. 1995.
Text Chunking Using Transformation Based Learning.
Proc. of the 3rd ACL Workshop on Very Large Cor-
pora, pages 82-94.
Erik F. T. J. Sang, John Veenstra. 1999. Representing
Text Chunks. Proc. of EACL?99, pages 173-179.
Vladamir Vapnik 1995. The Nature of Statistical Learn-
ing Theory. Springer Verlag, New York, USA.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 217?220, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Chunking Combining Complementary Syntactic Views
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Martin and Daniel Jurafsky?
Center for Spoken Language Research, University of Colorado, Boulder, CO 80303
?Department of Linguistics, Stanford University, Stanford, CA 94305
{spradhan,hacioglu,whw,martin}@cslr.colorado.edu, jurafsky@stanford.edu
Abstract
This paper describes a semantic role la-
beling system that uses features derived
from different syntactic views, and com-
bines them within a phrase-based chunk-
ing paradigm. For an input sentence, syn-
tactic constituent structure parses are gen-
erated by a Charniak parser and a Collins
parser. Semantic role labels are assigned
to the constituents of each parse using
Support Vector Machine classifiers. The
resulting semantic role labels are con-
verted to an IOB representation. These
IOB representations are used as additional
features, along with flat syntactic chunks,
by a chunking SVM classifier that pro-
duces the final SRL output. This strategy
for combining features from three differ-
ent syntactic views gives a significant im-
provement in performance over roles pro-
duced by using any one of the syntactic
views individually.
1 Introduction
The task of Semantic Role Labeling (SRL) involves
tagging groups of words in a sentence with the se-
mantic roles that they play with respect to a particu-
lar predicate in that sentence. Our approach is to use
supervised machine learning classifiers to produce
the role labels based on features extracted from the
input. This approach is neutral to the particular set
of labels used, and will learn to tag input according
to the annotated data that it is trained on. The task
reported on here is to produce PropBank (Kingsbury
and Palmer, 2002) labels, given the features pro-
vided for the CoNLL-2005 closed task (Carreras and
Ma`rquez, 2005).
We have previously reported on using SVM clas-
sifiers for semantic role labeling. In this work, we
formulate the semantic labeling problem as a multi-
class classification problem using Support Vector
Machine (SVM) classifiers. Some of these systems
use features based on syntactic constituents pro-
duced by a Charniak parser (Pradhan et al, 2003;
Pradhan et al, 2004) and others use only a flat syn-
tactic representation produced by a syntactic chun-
ker (Hacioglu et al, 2003; Hacioglu and Ward,
2003; Hacioglu, 2004; Hacioglu et al, 2004). The
latter approach lacks the information provided by
the hierarchical syntactic structure, and the former
imposes a limitation that the possible candidate roles
should be one of the nodes already present in the
syntax tree. We found that, while the chunk based
systems are very efficient and robust, the systems
that use features based on full syntactic parses are
generally more accurate. Analysis of the source
of errors for the parse constituent based systems
showed that incorrect parses were a major source
of error. The syntactic parser did not produce any
constituent that corresponded to the correct segmen-
tation for the semantic argument. In Pradhan et al
(2005), we reported on a first attempt to overcome
this problem by combining semantic role labels pro-
duced from different syntactic parses. The hope is
that the syntactic parsers will make different errors,
and that combining their outputs will improve on
217
either system alone. This initial attempt used fea-
tures from a Charniak parser, a Minipar parser and a
chunk based parser. It did show some improvement
from the combination, but the method for combin-
ing the information was heuristic and sub-optimal.
In this paper, we report on what we believe is an im-
proved framework for combining information from
different syntactic views. Our goal is to preserve the
robustness and flexibility of the segmentation of the
phrase-based chunker, but to take advantage of fea-
tures from full syntactic parses. We also want to
combine features from different syntactic parses to
gain additional robustness. To this end, we use fea-
tures generated from a Charniak parser and a Collins
parser, as supplied for the CoNLL-2005 closed task.
2 System Description
We again formulate the semantic labeling problem
as a multi-class classification problem using Sup-
port Vector Machine (SVM) classifiers. TinySVM1
along with YamCha2 (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2001) are used to implement
the system. Using what is known as the ONE VS
ALL classification strategy, n binary classifiers are
trained, where n is number of semantic classes in-
cluding a NULL class.
The general framework is to train separate seman-
tic role labeling systems for each of the parse tree
views, and then to use the role arguments output by
these systems as additional features in a semantic
role classifier using a flat syntactic view. The con-
stituent based classifiers walk a syntactic parse tree
and classify each node as NULL (no role) or as one
of the set of semantic roles. Chunk based systems
classify each base phrase as being the B(eginning)
of a semantic role, I(nside) a semantic role, or
O(utside) any semantic role (ie. NULL). This
is referred to as an IOB representation (Ramshaw
and Marcus, 1995). The constituent level roles are
mapped to the IOB representation used by the chun-
ker. The IOB tags are then used as features for a
separate base-phase semantic role labeler (chunker),
in addition to the standard set of features used by
the chunker. An n-fold cross-validation paradigm
is used to train the constituent based role classifiers
1http://chasen.org/?taku/software/TinySVM/
2http://chasen.org/?taku/software/yamcha/
and the chunk based classifier.
For the system reported here, two full syntactic
parsers were used, a Charniak parser and a Collins
parser. Features were extracted by first generating
the Collins and Charniak syntax trees from the word-
by-word decomposed trees in the CoNLL data. The
chunking system for combining all features was
trained using a 4-fold paradigm. In each fold, sepa-
rate SVM classifiers were trained for the Collins and
Charniak parses using 75% of the training data. That
is, one system assigned role labels to the nodes in
Charniak based trees and a separate system assigned
roles to nodes in Collins based trees. The other 25%
of the training data was then labeled by each of the
systems. Iterating this process 4 times created the
training set for the chunker. After the chunker was
trained, the Charniak and Collins based semantic la-
belers were then retrained using all of the training
data.
Two pieces of the system have problems scaling
to large training sets ? the final chunk based clas-
sifier and the NULL VS NON-NULL classifier for
the parse tree syntactic views. Two techniques were
used to reduce the amount of training data ? active
sampling and NULL filtering. The active sampling
process was performed as follows. We first train
a system using 10k seed examples from the train-
ing set. We then labeled an additional block of data
using this system. Any sentences containing an er-
ror were added to the seed training set. The sys-
tem was retrained and the procedure repeated until
there were no misclassified sentences remaining in
the training data. The set of examples produced by
this procedure was used to train the final NULL VS
NON-NULL classifier. The same procedure was car-
ried out for the chunking system. After both these
were trained, we tagged the training data using them
and removed all most likely NULLs from the data.
Table 1 lists the features used in the constituent
based systems. They are a combination of features
introduced by Gildea and Jurafsky (2002), ones pro-
posed in Pradhan et al (2004), Surdeanu et al
(2003) and the syntactic-frame feature proposed in
(Xue and Palmer, 2004). These features are ex-
tracted from the parse tree being labeled. In addition
to the features extracted from the parse tree being
labeled, five features were extracted from the other
parse tree (phrase, head word, head word POS, path
218
PREDICATE LEMMA
PATH: Path from the constituent to the predicate in the parse tree.
POSITION: Whether the constituent is before or after the predicate.
PREDICATE SUB-CATEGORIZATION
HEAD WORD: Head word of the constituent.
HEAD WORD POS: POS of the head word
NAMED ENTITIES IN CONSTITUENTS: Person, Organization, Location
and Miscellaneous.
PARTIAL PATH: Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
HEAD WORD OF PP: Head of PP replaced by head word of NP inside it,
and PP replaced by PP-preposition
FIRST AND LAST WORD/POS IN CONSTITUENT
ORDINAL CONSTITUENT POSITION
CONSTITUENT TREE DISTANCE
CONSTITUENT RELATIVE FEATURES: Nine features representing
the phrase type, head word and head word part of speech of the
parent, and left and right siblings of the constituent.
SYNTACTIC FRAME
CONTENT WORD FEATURES: Content word, its POS and named entities
in the content word
CLAUSE-BASED PATH VARIATIONS:
I. Replacing all the nodes in a path other than clause nodes with an ?*?.
For example, the path NP?S?VP?SBAR?NP?VP?VBD
becomes NP?S?*S?*?*?VBD
II. Retaining only the clause nodes in the path, which for the above
example would produce NP?S?S?VBD,
III. Adding a binary feature that indicates whether the constituent
is in the same clause as the predicate,
IV. collapsing the nodes between S nodes which gives NP?S?NP?VP?VBD.
PATH N-GRAMS: This feature decomposes a path into a series of trigrams.
For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes:
NP?S?VP, S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, etc. We
used the first ten trigrams as ten features. Shorter paths were padded
with nulls.
SINGLE CHARACTER PHRASE TAGS: Each phrase category is clustered
to a category defined by the first character of the phrase label.
PREDICATE CONTEXT: Two words and two word POS around the
predicate and including the predicate were added as ten new features.
PUNCTUATION: Punctuation before and after the constituent were
added as two new features.
FEATURE CONTEXT: Features for argument bearing constituents
were added as features to the constituent being classified.
Table 1: Features used by the constituent-based sys-
tem
and predicate sub-categorization). So for example,
when assigning labels to constituents in a Charniak
parse, all of the features in Table 1 were extracted
from the Charniak tree, and in addition phrase, head
word, head word POS, path and sub-categorization
were extracted from the Collins tree. We have pre-
viously determined that using different sets of fea-
tures for each argument (role) achieves better results
than using the same set of features for all argument
classes. A simple feature selection was implemented
by adding features one by one to an initial set of
features and selecting those that contribute signifi-
cantly to the performance. As described in Pradhan
et al (2004), we post-process lattices of n-best de-
cision using a trigram language model of argument
sequences.
Table 2 lists the features used by the chunker.
These are the same set of features that were used
in the CoNLL-2004 semantic role labeling task by
Hacioglu, et al (2004) with the addition of the two
semantic argument (IOB) features. For each token
(base phrase) to be tagged, a set of features is created
from a fixed size context that surrounds each token.
In addition to the features in Table 2, it also uses pre-
vious semantic tags that have already been assigned
to the tokens contained in the linguistic context. A
5-token sliding window is used for the context.
SVMs were trained for begin (B) and inside (I)
classes of all arguments and an outside (O) class.
WORDS
PREDICATE LEMMAS
PART OF SPEECH TAGS
BP POSITIONS: The position of a token in a BP using the IOB2
representation (e.g. B-NP, I-NP, O, etc.)
CLAUSE TAGS: The tags that mark token positions in a sentence
with respect to clauses.
NAMED ENTITIES: The IOB tags of named entities.
TOKEN POSITION: The position of the phrase with respect to
the predicate. It has three values as ?before?, ?after? and ?-? (for
the predicate)
PATH: It defines a flat path between the token and the predicate
HIERARCHICAL PATH: Since we have the syntax tree for the sentences,
we also use the hierarchical path from the phrase being classified to the
base phrase containing the predicate.
CLAUSE BRACKET PATTERNS
CLAUSE POSITION: A binary feature that identifies whether the
token is inside or outside the clause containing the predicate
HEADWORD SUFFIXES: suffixes of headwords of length 2, 3 and 4.
DISTANCE: Distance of the token from the predicate as a number
of base phrases, and the distance as the number of VP chunks.
LENGTH: the number of words in a token.
PREDICATE POS TAG: the part of speech category of the predicate
PREDICATE FREQUENCY: Frequent or rare using a threshold of 3.
PREDICATE BP CONTEXT: The chain of BPs centered at the predicate
within a window of size -2/+2.
PREDICATE POS CONTEXT: POS tags of words immediately preceding
and following the predicate.
PREDICATE ARGUMENT FRAMES: Left and right core argument patterns
around the predicate.
DYNAMIC CLASS CONTEXT: Hypotheses generated for two preceeding
phrases.
NUMBER OF PREDICATES: This is the number of predicates in
the sentence.
CHARNIAK-BASED SEMANTIC IOB TAG: This is the IOB tag generated
using the tagger trained on Charniak trees
COLLINS-BASED SEMANTIC IOB TAG: This is the IOB tag generated
using the tagger trained on Collins? trees
Table 2: Features used by phrase-based chunker.
3 Experimental Results
Table 3 shows the results obtained on the WSJ de-
velopment set (Section 24), the WSJ test set (Section
23) and the Brown test set (Section ck/01-03)
4 Acknowledgments
This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and
by the NSF via grants IS-9978025 and ITR/HCI
219
Precision Recall F?=1
Development 80.90% 75.38% 78.04
Test WSJ 81.97% 73.27% 77.37
Test Brown 73.73% 61.51% 67.07
Test WSJ+Brown 80.93% 71.69% 76.03
Test WSJ Precision Recall F?=1
Overall 81.97% 73.27% 77.37
A0 91.39% 82.23% 86.57
A1 79.80% 76.23% 77.97
A2 68.61% 62.61% 65.47
A3 73.95% 50.87% 60.27
A4 78.65% 68.63% 73.30
A5 75.00% 60.00% 66.67
AM-ADV 61.64% 46.05% 52.71
AM-CAU 76.19% 43.84% 55.65
AM-DIR 53.33% 37.65% 44.14
AM-DIS 80.56% 63.44% 70.98
AM-EXT 100.00% 46.88% 63.83
AM-LOC 64.48% 51.52% 57.27
AM-MNR 62.90% 45.35% 52.70
AM-MOD 98.64% 92.38% 95.41
AM-NEG 98.21% 95.65% 96.92
AM-PNC 56.67% 44.35% 49.76
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 83.37% 71.94% 77.23
R-A0 94.29% 88.39% 91.24
R-A1 85.93% 74.36% 79.73
R-A2 100.00% 37.50% 54.55
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 90.00% 42.86% 58.06
R-AM-MNR 66.67% 33.33% 44.44
R-AM-TMP 75.00% 40.38% 52.50
V 98.86% 98.86% 98.86
Table 3: Overall results (top) and detailed results on
the WSJ test (bottom).
0086132. Computer time was provided by NSF
ARI Grant #CDA-9601817, NSF MRI Grant #CNS-
0420873, NASA AIST grant #NAG2-1646, DOE
SciDAC grant #DE-FG02-04ER63870, NSF spon-
sorship of the National Center for Atmospheric Re-
search, and a grant from the IBM Shared University
Research (SUR) program.
Special thanks to Matthew Woitaszek, Theron Vo-
ran and the other administrative team of the Hemi-
sphere and Occam Beowulf clusters. Without these
the training would never be possible.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. n Introduction to the CoNLL-2005
Shared Task: Semantic Role Labeling. In Proceedings of CoNLL-2005.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Kadri Hacioglu and Wayne Ward. 2003. Target word detection and semantic
role chunking using support vector machines. In Proceedings of the Human
Language Technology Conference, Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Shallow semantic parsing using support vector machines. Technical
Report TR-CSLR-2003-1, Center for Spoken Language Research, Boulder,
Colorado.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Ju-
rafsky. 2004. Semantic role labeling by tagging syntactic chunks. In Pro-
ceedings of the 8th Conference on CoNLL-2004, Shared Task ? Semantic Role
Labeling.
Kadri Hacioglu. 2004. A lightweight semantic chunking model based on tagging.
In Proceedings of the Human Language Technology Conference /North Amer-
ican chapter of the Association of Computational Linguistics (HLT/NAACL),
Boston, MA.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In
Proceedings of the 3rd International Conference on Language Resources and
Evaluation (LREC-2002), Las Palmas, Canary Islands, Spain.
Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk
identification. In Proceedings of the 4th Conference on CoNLL-2000 and
LLL-2000, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings of the 2nd Meeting of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-2001).
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Semantic role parsing: Adding semantic structure to unstructured text.
In Proceedings of the International Conference on Data Mining (ICDM 2003),
Melbourne, Florida.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using support vector machines. In Proceed-
ings of the Human Language Technology Conference/North American chapter
of the Association of Computational Linguistics (HLT/NAACL), Boston, MA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2005. Semantic role labeling using different syntactic views. In Proceedings
of the Association for Computational Linguistics 43rd annual meeting (ACL-
2005), Ann Arbor, MI.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-
based learning. In Proceedings of the Third Annual Workshop on Very Large
Corpora, pages 82?94. ACL.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Us-
ing predicate-argument structures for information extraction. In Proceedings
of the 41st Annual Meeting of the Association for Computational Linguistics,
Sapporo, Japan.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role
labeling. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing, Barcelona, Spain.
220
Proceedings of the Third Workshop on Statistical Machine Translation, pages 26?34,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Regularization and Search for Minimum Error Rate Training
Daniel Cer, Daniel Jurafsky, and Christopher D. Manning
Stanford University
Stanford, CA 94305
cerd,jurafsky,manning@stanford.edu
Abstract
Minimum error rate training (MERT) is a
widely used learning procedure for statistical
machine translation models. We contrast three
search strategies for MERT: Powell?s method,
the variant of coordinate descent found in the
Moses MERT utility, and a novel stochastic
method. It is shown that the stochastic method
obtains test set gains of +0.98 BLEU on MT03
and +0.61 BLEU on MT05. We also present
a method for regularizing the MERT objec-
tive that achieves statistically significant gains
when combined with both Powell?s method
and coordinate descent.
1 Introduction
Och (2003) introduced minimum error rate training
(MERT) as an alternative training regime to the con-
ditional likelihood objective previously used with
log-linear translation models (Och & Ney, 2002).
This approach attempts to improve translation qual-
ity by optimizing an automatic translation evalua-
tion metric, such as the BLEU score (Papineni et al,
2002). This is accomplished by either directly walk-
ing the error surface provided by an evaluation met-
ric w.r.t. the model weights or by using gradient-
based techniques on a continuous approximation of
such a surface. While the former is piecewise con-
stant and thus cannot be optimized using gradient
techniques, Och (2003) provides an approach that
performs such training efficiently.
In this paper we explore a number of variations on
MERT. First, it is shown that performance gains can
be had by making use of a stochastic search strategy
as compare to that obtained by Powell?s method and
coordinate descent. Subsequently, results are pre-
sented for two regularization strategies1. Both allow
coordinate descent and Powell?s method to achieve
performance that is on par with stochastic search.
In what follows, we briefly review minimum er-
ror rate training, introduce our stochastic search and
regularization strategies, and then present experi-
mental results.
2 Minimum Error Rate Training
Let F be a collection of foreign sentences to be
translated, with individual sentences f0, f1, . . . ,
fn. For each fi, the surface form of an indi-
vidual candidate translation is given by ei with
hidden state hi associated with the derivation of
ei from fi. Each ei is drawn from E , which
represents all possible strings our translation sys-
tem can produce. The (ei,hi, fi) triples are con-
verted into vectors of m feature functions by
? : E ?H ?F ? Rm whose dot product with the
weight vector w assigns a score to each triple.
The idealized translation process then is to find the
highest scoring pair (ei,hi) for each fi, or rather
(ei,hi) = argmax(e?E,h?H)w ??(e,h, f).
The aggregate argmax for the entire data set F is
given by equation (1)2. This gives Ew which repre-
sents the set of translations selected by the model for
data set F when parameterized by the weight vec-
tor w. Let?s assume we have an automated mea-
sure of translation quality ` that maps the collec-
1While we prefer the term regularization, the strategies pre-
sented here could also be referred to as smoothing methods.
2Here, the translation of the entire data set is treated as a
single structured prediction problem using the feature function
vector ?(E,H,F) =Pni ?(ei,hi, fi)
26
id Translation log(PTM(f |e)) log(PLM(e)) BLEU-2
e1 This is it -1.2 -0.1 29.64
e2 This is small house -0.2 -1.2 63.59
e3 This is miniscule building -1.6 -0.9 31.79
e4 This is a small house -0.1 -0.9 100.00
ref This is a small house
Table 1: Four hypothetical translations and their corresponding log model scores from a translation model PTM (f |e)
and a language model PLM (e), along with their BLEU-2 scores according to the given reference translation. The
MERT error surface for these translations is given in figure 1.
tion of translations Ew onto some real valued loss,
` : En ? R. For instance, in the experiments that
follow, the loss corresponds to 1 minus the BLEU
score assigned to Ew for a given collection of refer-
ence translations.
(Ew,Hw) = argmax
(E?En,H?Hn)
w ??(E,H,F) (1)
Using n-best lists produced by a decoder to ap-
proximate En and Hn, MERT searches for the
weight vector w? that minimizes the loss `. Let-
ting E?w denote the result of the translation argmax
w.r.t. the approximate hypothesis space, the MERT
search is then expressed by equation (2). Notice the
objective function being optimized is equivalent to
the loss assigned by the automatic measure of trans-
lation quality, i.e. O(w) = `(E?w).
w? = argmin
w
`(E?w) (2)
After performing the parameter search, the de-
coder is then re-run using the weights w? to produce
a new set of n-best lists, which are then concate-
nated with the prior n-best lists in order to obtain a
better approximation of En and Hn. The parameter
search given in (2) can then be performed over the
improved approximation. This process repeats un-
til either no novel entries are produced for the com-
bined n-best lists or the weights change by less than
some ? across iterations.
Unlike the objective functions associated with
other popular learning algorithms, the objective O
is piecewise constant over its entire domain. That
is, while small perturbations in the weights, w, will
change the score assigned by w ??(e,h, f) to each
triple, (e,h, f), such perturbations will generally not
change the ranking between the pair selected by the
argmax, (e?,h?) = argmaxw ??(e,h, f), and any
given competing pair (e?,h?). However, at certain
critical points, the score assigned to some compet-
ing pair (e?,h?) will exceed that assigned to the
prior winner (e?wold ,h?wold). At this point, the pairreturned by argmaxw ??(e,h, f) will change and
loss ` will be evaluated using the newly selected e?.
Figure 1: MERT objective for the translations given
in table 1. Regions are labeled with the translation
that dominates within it, i.e. argmaxw ??(e, f),
and with their corresponding objective values,
1? `(argmaxw ??(e, f)).
This is illustrated in figure (1), which plots the
MERT objective function for a simple model with
two parameters, wtm & wlm, and for which the
space of possible translations, E , consists of the four
sentences given in table 13. Here, the loss ` is de-
3For this example, we ignore the latent variables, h, associ-
27
fined as 1.0?BLEU-2(e). That is, ` is the differ-
ence between a perfect BLEU score and the BLEU
score calculated for each translation using unigram
and bi-gram counts.
The surface can be visualized as a collection of
plateaus that all meet at the origin and then extend
off into infinity. The latter property illustrates that
the objective is scale invariant w.r.t. the weight vec-
tor w. That is, since any vector w? = ?w ??>0 will
still result in the same relative rankings of all pos-
sible translations according to w ??(e,h, f), such
scaling will not change the translation selected by
the argmax. At the boundaries between regions, the
objective is undefined, as 2 or more candidates are
assigned identical scores by the model. Thus, it is
unclear what should be returned by the argmax for
subsequent scoring by `.
Since the objective is piecewise constant, it can-
not be minimized using gradient descent or even the
sub-gradient method. Two applicable methods in-
clude downhill simplex and Powell?s method (Press
et al, 2007). The former attempts to find a lo-
cal minimum in an n dimensional space by itera-
tively shrinking or growing an n+ 1 vertex simplex4
based on the objective values of the current vertex
points and select nearby points. In contrast, Pow-
ell?s method operates by starting with a single point
in weight space, and then performing a series of line
minimizations until no more progress can be made.
In this paper, we focus on line minimization based
techniques, such as Powell?s method.
2.1 Global minimum along a line
Even without gradient information, numerous meth-
ods can be used to find, or approximately find, local
minima along a line. However, by exploiting the fact
that the underlying scores assigned to competing hy-
potheses, w ??(e,h, f), vary linearly w.r.t. changes
in the weight vector, w, Och (2003) proposed a strat-
egy for finding the global minimum along any given
search direction.
The insight behind the algorithm is as follows.
Let?s assume we are examining two competing
ated with the derivation of each e from the foreign sentence f .
If included, such variables would only change the graph in that
multiple different derivations would be possible for each ej . If
present, the graph could then include disjoint regions that all
map to the same ej and thus the same objective value.
4A simplex can be thought of as a generalization of a triangle
to arbitrary dimensional spaces.
Figure 2: Illustration of how the model score assigned
to each candidate translation varies during a line search
along the coordinate direction wlm with a starting point
of (wtm, wlm) = (1.0, 0.5). Each plotted line corre-
sponds to the model score for one of the translation candi-
dates. The vertical bands are labeled with the hypothesis
that dominates in that region. The transitions between
bands result from the dotted intersections between 1-best
lines.
translation/derivation pairs, (e1,h1) & (e2,h2).
Further, let?s say the score assigned by the
model to (e1,h1) is greater than (e2,h2), i.e.
w ??(e1,h1, f) > w ??(e2,h2, f). Since the
scores of the two vary linearly along any search
direction, d, we can find the point at which the
model?s relative preference for the competing
pairs switches as p = w??(e1,h1,f)?w??(e2,h2,f)d??(e2,h2,f)?d??(e1,h1,f) .
At this particular point, we have the equality
(pd+w) ??(e1,h1, f) = (pd+w) ??(e2,h2, f),
or rather the point at which the scores assigned
by the model to the candidates intersect along
search direction d5. Such points correspond to
the boundaries between adjacent plateaus in the
objective, as prior to the boundary the loss function
` is computed using the translation, e1, and after the
boundary it is computed using e2.
To find the global minimum for a search direc-
tion d, we move along d and for each plateau we
5Notice that, this point only exists if the slopes of the
candidates? model scores along d are not equivalent, i.e. if
d ??(e2,h2, f) 6= d ??(e1,h1, f).
28
Translation m b 1-best
e1 -0.1 -1.25 (0.86,+?]
e2 -1.2 -0.8 (-0.83,0.88)
e3 -0.9 -2.05 n/a
e4 -0.9 -0.55 [??,-0.83]
Table 2: Slopes, m, intercepts, b, and 1-best ranges
for the 4 translations given in table 1 during a line
search along the coordinate wlm, with a starting point of
(wtm, wlm) = (1.0, 0.5). This line search in illustrated
in figure(2).
identify all the points at which the score assigned
by the model to the current 1-best translation inter-
sects the score assigned to competing translations.
At the closest such intersection, we have a new 1-
best translation. Moving to the plateau associated
with this new 1-best, we then repeat the search for
the nearest subsequent intersection. This continues
until we know what the 1-best translations are for all
points along d. The global minimum can then be
found by examining ` once for each of these.
Let?s return briefly to our earlier example given in
table 1. Starting at position (wtm, wlm) = (1.0, 0.5)
and searching along the wlm coordinate, i.e.
(dtm, dlm) = (0.0, 1.0), table 2 gives the line
search slopes, m = d ??(e,h, f), and intercepts,
b = w ??(e,h, f), for each of the four candidate
translations. Using the procedure just described, we
can then find what range of values along d each
candidate translation is assigned the highest rela-
tive model score. Figure 2 illustrates how the score
assigned by the model to each of the translations
changes as we move along d. Each of the banded re-
gions corresponds to a plateau in the objective, and
each of the top most line intersections represents the
transition from one plateau to the next. Note that,
while the surface that is defined by the line segments
with the highest classifier score for each region is
convex, this is not a convex optimization problem as
we are optimizing over the loss ` rather than classi-
fier score.
Pseudocode for the line search is given in algo-
rithm 1. Letting n denote the number of foreign sen-
tences, f , in a dataset, and having m denote the size
of the individual n-best lists, |l|, the time complexity
of the algorithm is given by O(nm2). This is seen
in that each time we check for the nearest intersec-
tion to the current 1-best for some n-best list l, we
Algorithm 1 Och (2003)?s line search method to
find the global minimum in the loss, `, when start-
ing at the point w and searching along the direction
d using the candidate translations given in the col-
lection of n-best lists L.
Input: L, w, d, `
I ? {}
for l ? L do
for e ? l do
m{e} ? e.features ? d
b{e} ? e.features ? w
end for
bestn ? argmaxe?l m{e} {b{e} breaks ties}
loop
bestn+1 = argmine?l max
(
0, b{bestn}?b{e}m{e}?m{bestn}
)
intercept ? max
(
0, b{bestn}?b{bestn+1}m{bestn+1}?m{bestn}
)
if intercept > 0 then
add(I, intercept)
else
break
end if
end loop
end for
add(I, max(I) + 2?)
ibest = argmini?I eval`(L,w + (i? ?) ? d)
return w + (ibest ? ?) ? d
must calculate its intersection with all other candi-
date translations that have yet to be selected as the
1-best. And, for each of the n n-best lists, this may
have to be done up to m? 1 times.
2.2 Search Strategies
In this section, we review two search strategies that,
in conjunction with the line search just described,
can be used to drive MERT. The first, Powell?s
method, was advocated by Och (2003) when MERT
was first introduced for statistical machine transla-
tion. The second, which we call Koehn-coordinate
descent (KCD)6, is used by the MERT utility pack-
aged with the popular Moses statistical machine
translation system (Koehn et al, 2007).
6Moses uses David Chiang?s CMERT package. Within the
source file mert.c, the function that implements the overall
search strategy, optimize koehn(), is based on Philipp Koehn?s
Perl script for MERT optimization that was distributed with
Pharaoh.
29
2.2.1 Powell?s Method
Powell?s method (Press et al, 2007) attempts to
efficiently search the objective by constructing a set
of mutually non-interfering search directions. The
basic procedure is as follows: (i) A collection of
search directions is initialized to be the coordinates
of the space being searched; (ii) The objective is
minimized by looping through the search directions
and performing a line minimization for each; (iii) A
new search direction is constructed that summarizes
the cumulative direction of the progress made dur-
ing step (ii) (i.e., dnew = wpreii ?wpostii). After
a line minimization is performed along dnew, it is
used to replace one of the existing search directions.
(iv) The process repeats until no more progress can
be made. For a quadratic function of n variables,
this procedure comes with the guarantee that it will
reach the minimum within n iterations of the outer
loop. However, since Powell?s method is usually ap-
plied to non-quadratic optimization problems, a typ-
ical implementation will forego the quadratic con-
vergence guarantees in favor of a heuristic scheme
that allows for better navigation of complex sur-
faces.
2.2.2 Koehn?s Coordinate Descent
KCD is a variant of coordinate descent that, at
each iteration, moves along the coordinate which al-
lows for the most progress in the objective. In or-
der to determine which coordinate this is, the rou-
tine performs a trial line minimization along each. It
then updates the weight vector with the one that it
found to be most successful. While much less so-
phisticated that Powell, our results indicate that this
method may be marginally more effective at opti-
mizing the MERT objective7.
3 Extensions
In this section we present and motivate two novel
extensions to MERT. The first is a stochastic alterna-
tive to the Powell and KCD search strategies, while
the second is an efficient method for regularizing the
objective.
7While we are not aware of any previously published results
that demonstrate this, it is likely that we were not the first to
make this discovery as even though Moses? MERT implemen-
tation includes a vestigial implementation of Powell?s method,
the code is hardwired to call optimize koehn rather than the rou-
tine for Powell.
3.1 Random Search Directions
One significant advantage of Powell?s algorithm
over coordinate descent is that it can optimize along
diagonal search directions in weight space. That is,
given a model with a dozen or so features, it can
explore gains that are to be had by simultaneously
varying two or more of the feature weights. In gen-
eral, the diagonals that Powell?s method constructs
allow it to walk objective functions more efficiently
than coordinate descent (Press et al, 2007). How-
ever, given that we have a line search algorithm
that will find the global minima along any given
search direction, diagonal search may be of even
more value. That is, similar to ridge phenomenon
that arise in traditional hill climbing search, it is pos-
sible that there are points in the objective that are the
global minimum along any given coordinate direc-
tion, but are not the global minimum along diagonal
directions.
However, one substantial disadvantage for Pow-
ell is that the assumptions it uses to build up the di-
agonal search directions do not hold in the present
context. Specifically, the search directions are built
up under the assumption that near a minimum the
surface looks approximately quadratic and that we
are performing local line minimizations within such
regions. However, since we are performing global
line minimizations, it is possible for the algorithm to
jump from the region around one minima to another.
If Powell?s method has already started to tune its
search directions for the prior minima, it will likely
be less effective in its efforts to search the new re-
gion. To this extent, coordinate descent will be more
robust than Powell as it has no assumptions that are
violated when such a jump occurs.
One way of salvaging Powell?s algorithm in this
context would be to incorporate additional heuris-
tics that detect when the algorithm has jumped from
the region around one minima to another. When
this occurs, the search directions could be reset to
the coordinates of the space. However, we opt for a
simpler solution, which like Powell?s algorithm per-
forms searches along diagonals in the space, but that
like coordinate descent is sufficiently simple that the
algorithm will not be confused by sudden jumps be-
tween regions.
Specifically, the search procedure chooses di-
rections at random such that each component
30
Figure 3: Regularization during line search - using, from left to right: (i) the maximum loss of adjacent plateaus, (ii)
the average loss of adjacent plateaus, (iii) no regularization. Each set of bars represents adjacent plateaus along the line
being searched, with the height of the bars representing their associated loss. The vertical lines indicate the surrogate
loss values used for the center region under each of the schemes (i-iii).
is distributed according to a Gaussian8, d s.t.
di ? N(0, 1). This allows the procedure to mini-
mize along diagonal search directions, while making
essentially no assumptions regarding the character-
istics of the objective or the relationship between a
series of sequential line minimizations. In the results
that follow, we show that, perhaps surprisingly, this
simple procedure outperforms both KCD and Pow-
ell?s method.
3.2 Regularization
One potential drawback of MERT, as it is typically
implemented, is that it attempts to find the best pos-
sible set of parameters for a training set without
making any explicit efforts to find a set of param-
eters that can be expected to generalize well. For
example, let?s say that for some objective there is
a very deep but narrow minima that is surrounded
on all sides by very bad objective values. That
is, the BLEU score at the minima might be 39.1
while all surrounding plateaus have a BLEU score
that is < 10. Intuitively, such a minima would be a
very bad solution, as the resulting parameters would
likely exhibit very poor generalization to other data
sets. This could be avoided by regularizing the sur-
face in order to eliminate such spurious minima.
One candidate for performing such regularization
is the continuous approximation of the MERT objec-
tive, O = Epw(`). Och (2003) claimed that this ap-
proximation achieved essentially equivalent perfor-
mance to that obtained when directly using the loss
as the objective, O = `. However, Zens et al (2007)
found that O = Epw(`) achieved substantially better
test set performance than O = `, even though it per-
forms slightly worse on the data used to train the
parameters. Similarly, Smith and Eisner (2006) re-
ported test set gains for the related technique of min-
imum risk annealing, which incorporates a temper-
8However, we speculate that similar results could be ob-
tained using a uniform distribution over (?1, 1)
ature parameter that trades off between the smooth-
ness of the objective and the degree it reflects the
underlying piecewise constant error surface. How-
ever, the most straightforward implementation of
such methods requires a loss that can be applied at
the sentence level. If the evaluation metric of inter-
est does not have this property (e.g. BLEU), the loss
must be approximated using some surrogate, with
successful learning then being tied to how well the
surrogate captures the critical properties of the un-
derlying loss.
The techniques of Zens et al (2007) & Smith
and Eisner (2006) regularize by implicitly smooth-
ing over nearby plateaus in the error surface. We
propose an alternative scheme that operates directly
on the piecewise constant objective and that miti-
gates the problem of spurious local minima by ex-
plicitly smoothing over adjacent plateaus during the
line search. That is, when assessing the desirabil-
ity of any given plateau, we examine a fixed win-
dow w of adjacent plateaus along the direction be-
ing searched and combine their evaluation scores.
We explore two combination methods, max and
average. The former, max, assigns each plateau an
objective value that is equal to the maximum objec-
tive value in its surrounding window, while average
assigns a plateau an objective value that is equal to
its window?s average. Figure 3 illustrates both meth-
ods for regularizing the plateaus and contrasts them
with the case where no regularization is used. No-
tice that, while both methods discount spurious pits
in the objective, average still does place some value
on isolated deep plateaus, and max discounts them
completely.
Note that one potential weakness of this scheme
is the value assigned by the regularized objective
to any given point differs depending on the direc-
tion being searched. As such, it has the potential to
wreak havoc on methods such as Powell?s, which ef-
fectively attempt to learn about the curvature of the
31
objective from a sequence of line minimizations.
4 Experiments
Three sets of experiments were performed. For the
first set, we compare the performance of Powell?s
method, KCD, and our novel stochastic search strat-
egy. We then evaluate the performance of all three
methods when the objective is regularized using the
average of adjacent plateaus for window sizes vary-
ing from 3 to 7. Finally, we repeat the regularization
experiment, but using the maximum objective value
from the adjacent plateaus. These experiments were
performed using the Chinese English evaluation data
provided for NIST MT eval 2002, 2003, and 2005.
MT02 was used as a dev set for MERT learning,
while MT03 and MT05 were used as our test sets.
For all experiments, MERT training was per-
formed using n-best lists from the decoder of size
100. During each iteration, the MERT search was
performed once with a starting point of the weights
used to generate the most recent set of n-best lists
and then 5 more times using randomly selected start-
ing points9. Of these, we retain the weights from
the search that obtained the lowest objective value.
Training continued until either decoding produced
no novel entries for the combined n-best lists or none
of the parameter values changed by more than 1e-5
across subsequent iterations.
4.1 System
Experiments were run using a right-to-left beam
search decoder that achieves a matching BLEU
score to Moses (Koehn et al, 2007) over a variety
of data sets. Moreover, when using the same under-
lying model, the two decoders only produce transla-
tions that differ by one or more words 0.2% of the
time. We made use of a stack size of 50 as it al-
lowed for faster experiments while only performing
modestly worse than a stack of 200. The distortion
limit was set to 6. And, we retrieved 20 translation
options for each unique source phrase.
Our phrase table was built using 1, 140, 693 sen-
tence pairs sampled from the GALE Y2 training
9Only 5 random restarts were used due to time constraints.
Ideally, a sizable number of random restarts should be used in
order to minimize the degree to which the results are influenced
by some runs receiving starting points that are better in general
or perhaps better/worse w.r.t. their specific optimization strat-
egy.
Method Dev Test Test
MT02 MT03 MT05
KCD 30.967 30.778 29.580
Powell 30.638 30.692 29.780
Random 31.681 31.754 30.191
Table 3: BLEU scores obtained by models trained using
the three different parameter search strategies: Powell?s
method, KCD, and stochastic search.
data. The Chinese data was word segmented us-
ing the GALE Y2 retest release of the Stanford
CRF segmenter (Tseng et al, 2005). Phrases were
extracted using the typical approach described in
Koehn et al (2003) of running GIZA++ (Och &
Ney, 2003) in both directions and then merging
the alignments using the grow-diag-final heuristic.
From the merged alignments we also extracted a bi-
directional lexical reordering model conditioned on
the source and the target phrases (Tillmann, 2004)
(Koehn et al, 2007). A 5-gram language model
was created using the SRI language modeling toolkit
(Stolcke, 2002) and trained using the Gigaword cor-
pus and English sentences from the parallel data.
5 Results
As illustrated in table 3, Powell?s method and KCD
achieve a very similar level of performance, with
KCD modestly outperforming Powell on the MT03
test set while Powell modestly outperforms coordi-
nate descent on the MT05 test set. Moreover, the
fact that Powell?s algorithm did not perform better
than KCD on the training data10, and in fact actually
performed modestly worse, suggests that Powell?s
additional search machinery does not provide much
benefit for MERT objectives.
Similarly, the fact that the stochastic search ob-
tains a much higher dev set score than either Pow-
ell or KCD indicates that it is doing a better job
of optimizing the objective than either of the two
alternatives. These gains suggest that stochastic
search does make better use of the global minimum
line search than the alternative methods. Or, alter-
natively, it strengthens the claim that the method
succeeds at combining one of the critical strengths
10This indicates that Powell failed to find a deeper minima
in the objective, since recall that the unregularized objective is
equivalent to the model?s dev set performance.
32
Method Window Dev Test Test
Avg MT02 MT03 MT05
Coordinate none 30.967 30.778 29.580
3 31.665 31.675 30.266
5 31.317 31.229 30.182
7 31.205 31.824 30.149
Powell none 30.638 30.692 29.780
3 31.333 31.412 29.890
5 31.748 31.777 30.334
7 31.249 31.571 30.161
Random none 31.681 31.754 30.191
3 31.548 31.778 30.263
5 31.336 31.647 30.415
7 30.501 29.336 28.372
Method Window Dev Test Test
Max MT02 MT03 MT05
Coordinate none 30.967 30.778 29.580
3 31.536 31.927 30.334
5 31.484 31.702 29.687
7 31.627 31.294 30.199
Powell none 30.638 30.692 29.780
3 31.428 30.944 29.598
5 31.407 31.596 30.090
7 30.870 30.911 29.620
Random none 31.681 31.754 30.191
3 31.179 30.898 29.529
5 30.903 31.666 29.963
7 31.920 31.906 30.674
Table 4: BLEU scores obtained when regularizing using the average loss of adjacent plateaus, left, and the maximum
loss of adjacent plateaus, right. The none entry for each search strategy represents the baseline where no regularization
is used. Statistically significant test set gains, p < 0.01, over the respective baselines are in bold face.
of Powell?s method, diagonal search, with coordi-
nate descent?s robustness to the sudden jumps be-
tween regions that result from global line minimiza-
tion. Using an approximate randomization test for
statistical significance (Riezler & Maxwell, 2005),
and with KCD as a baseline, the gains obtained
by stochastic search on MT03 are statistically sig-
nificant (p = 0.002), as are the gains on MT05
(p = 0.005).
Table 4 indicates that performing regularization
by either averaging or taking the maximum of adja-
cent plateaus during the line search leads to gains for
both Powell?s method and KCD. However, no reli-
able additional gains appear to be had when stochas-
tic search is combined with regularization.
It may seem surprising that the regularization
gains for Powell & KCD are seen not only in the test
sets but on the dev set as well. That is, in typical ap-
plications, regularization slightly decreases perfor-
mance on the data used to train the model. However,
this trend can in part be accounted for by the fact that
during training, MERT is using n-best lists for objec-
tive evaluations rather than the more expensive pro-
cess of running the decoder for each point that needs
to be checked. As such, during each iteration of
training, the decoding performance of the model ac-
tually represents its generalization performance rel-
ative to what was learned from the n-best lists cre-
ated during prior iterations. Moreover, better gen-
eralization from the prior n-best lists can also help
drive subsequent learning as there will then be more
high quality translations on the n-best lists used for
future iterations of learning. Additionally, regular-
ization can reduce search errors by reducing the risk
of getting stuck in spurious low loss pits that are in
otherwise bad regions of the space.
6 Conclusions
We have presented two methods for improving the
performance of MERT. The first is a novel stochas-
tic search strategy that appears to make better use of
Och (2003)?s algorithm for finding the global min-
imum along any given search direction than either
coordinate descent or Powell?s method. The sec-
ond is a simple regularization scheme that leads to
performance gains for both coordinate descent and
Powell?s method. However, no further gains are ob-
tained by combining the stochastic search with reg-
ularization of the objective.
One quirk of the regularization scheme presented
here is that the regularization applied to any given
point in the objective varies depending upon what
direction the point is approached from. We are
currently looking at other similar regularization
schemes that maintain consistent objective values
regardless of the search direction.
Acknowledgments
We extend our thanks to our three anonymous reviewers,
33
particularly for the depth of analysis provided. This paper
is based on work funded in part by the Defense Advanced
Research Projects Agency through IBM.
References
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C.,
Federico, M., Bertoldi, N., Cowan, B., Shen, W.,
Moran, C., Zens, R., Dyer, C., Bojar, O., Con-
stantin, A., & Herbst, E. (2007). Moses: Open
source toolkit for statistical machine translation.
In ACL.
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical
phrase-based translation. In HLT-NAACL.
Och, F.-J. (2003). Minimum error rate training in
statistical machine translation. In ACL.
Och, F. J., & Ney, H. (2002). Discriminative train-
ing and maximum entropy models for statistical
machine translation. In ACL.
Och, F. J., & Ney, H. (2003). A systematic compari-
son of various statistical alignment models. Com-
putational Linguistics, 29, 19?51.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J.
(2002). Bleu: a method for automatic evaluation
of machine translation. In ACL.
Press, W. H., Teukolsky, S. A., Vetterling, W. T., &
Flannery, B. P. (2007). Numerical recipes 3rd edi-
tion: The art of scientific computing. Cambridge
University Press.
Riezler, S., & Maxwell, J. T. (2005). On some pit-
falls in automatic evaluation and significance test-
ing for mt. In ACL.
Smith, D. A., & Eisner, J. (2006). Minimum risk
annealing for training log-linear models. In ACL.
Stolcke, A. (2002). Srilm ? an extensible language
modeling toolkit. In ICSLP.
Tillmann, C. (2004). A unigram orientation model
for statistical machine translation. In ACL.
Tseng, H., Chang, P., Andrew, G., Jurafsky, D.,
& Manning, C. (2005). A conditional random
field word segmenter for sighan bakeoff 2005. In
SIGHAN Workshop on Chinese Language Pro-
cessing.
Zens, R., Hasan, S., & Ney, H. (2007). A system-
atic comparison of training criteria for statistical
machine translation. In EMNLP.
34
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 37?41,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Textual Entailment Features for Machine Translation Evaluation
Sebastian Pado?, Michel Galley, Dan Jurafsky, Christopher D. Manning?
Stanford University
{pado,mgalley,jurafsky,manning}@stanford.edu
Abstract
We present two regression models for the prediction
of pairwise preference judgments among MT hy-
potheses. Both models are based on feature sets that
are motivated by textual entailment and incorporate
lexical similarity as well as local syntactic features
and specific semantic phenomena. One model pre-
dicts absolute scores; the other one direct pairwise
judgments. We find that both models are compet-
itive with regression models built over the scores
of established MT evaluation metrics. Further data
analysis clarifies the complementary behavior of the
two feature sets.
1 Introduction
Automatic metrics to assess the quality of machine trans-
lations have been a major enabler in improving the per-
formance of MT systems, leading to many varied ap-
proaches to develop such metrics. Initially, most metrics
judged the quality of MT hypotheses by token sequence
match (cf. BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002). These measures rate systems hypothe-
ses by measuring the overlap in surface word sequences
shared between hypothesis and reference translation.
With improvements in the state-of-the-art in machine
translation, the effectiveness of purely surface-oriented
measures has been questioned (see e.g., Callison-Burch
et al (2006)). In response, metrics have been proposed
that attempt to integrate more linguistic information
into the matching process to distinguish linguistically li-
censed from unwanted variation (Gime?nez andMa`rquez,
2008). However, there is little agreement on what types
of knowledge are helpful: Some suggestions concen-
trate on lexical information, e.g., by the integration of
word similarity information as in Meteor (Banerjee and
Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other
proposals use structural information such as dependency
edges (Owczarzak et al, 2007).
In this paper, we investigate an MT evaluation metric
that is inspired by the similarity between this task and
the textual entailment task (Dagan et al, 2005), which
?This paper is based on work funded by the Defense Ad-
vanced Research Projects Agency through IBM. The content
does not necessarily reflect the views of the U.S. Government,
and no official endorsement should be inferred..
HYP: Virus was infected.
REF: No one was infected by the virus.
no entailment
no entailment
HYP: The virus did not infect anybody.
REF: No one was infected by the virus.
entailment
entailment
Figure 1: Entailment status between an MT system hy-
pothesis and a reference translation for good translations
(above) and bad translations (below).
suggests that the quality of an MT hypothesis should be
predictable by a combination of lexical and structural
features that model the matches and mismatches be-
tween system output and reference translation. We use
supervised regression models to combine these features
and analyze feature weights to obtain further insights
into the usefulness of different feature types.
2 Textual Entailment for MT Evaluation
2.1 Textual Entailment vs. MT Evaluation
Textual entailment (TE) was introduced by Dagan et
al. (2005) as a concept that corresponds more closely
to ?common sense? reasoning than classical, categorical
entailment. Textual entailment is defined as a relation
between two natural language sentences (a premise P
and a hypothesis H) that holds if a human reading P
would infer that H is most likely true.
Information about the presence or absence of entail-
ment between two sentences has been found to be ben-
eficial for a range of NLP tasks such as Word Sense
Disambiguation or Question Answering (Dagan et al,
2006; Harabagiu and Hickl, 2006). Our intuition is that
this idea can also be fruitful in MT Evaluation, as illus-
trated in Figure 1. Very good MT output should entail
the reference translation. In contrast, missing hypothesis
material breaks forward entailment; additional material
breaks backward entailment; and for bad translations,
entailment fails in both directions.
Work on the recognition of textual entailment (RTE)
has consistently found that the integration of more syn-
tactic and semantic knowledge can yield gains over
37
surface-based methods, provided that the linguistic anal-
ysis was sufficiently robust. Thus, for RTE, ?deep?
matching outperforms surface matching. The reason is
that linguistic representation makes it considerably eas-
ier to distinguish admissible variation (i.e., paraphrase)
from true, meaning-changing divergence. Admissible
variation may be lexical (synonymy), structural (word
and phrase placement), or both (diathesis alternations).
The working hypothesis of this paper is that the ben-
efits of deeper analysis carry over to MT evaluation.
More specifically, we test whether the features that al-
low good performance on the RTE task can also predict
human judgments for MT output. Analogously to RTE,
these features should help us to differentiate meaning
preserving translation variants from bad translations.
Nevertheless, there are also substantial differences
between TE and MT evaluation. Crucially, TE assumes
the premise and hypothesis to be well-formed sentences,
which is not true in MT evaluation. Thus, a possible crit-
icism to the use of TE methods is that the features could
become unreliable for ill-formed MT output. However,
there is a second difference between the tasks that works
to our advantage. Due to its strict compositional nature,
TE requires an accurate semantic analysis of all sentence
parts, since, for example, one misanalysed negation or
counterfactual embedding can invert the entailment sta-
tus (MacCartney and Manning, 2008). In contrast, hu-
man MT judgments behave more additively: failure of a
translation with respect to a single semantic dimension
(e.g., polarity or tense) degrades its quality, but usually
not crucially so. We therefore expect that even noisy
entailment features can be predictive in MT evaluation.
2.2 Entailment-based prediction of MT quality
Regression-based prediction. Experiences from the
annotation of MT quality judgments show that human
raters have difficulty in consistently assigning absolute
scores to MT system output, due to the number of ways
in which MT output can deviate. Thus, the human an-
notation for the WMT 2008 dataset was collected in
the form of binary pairwise preferences that are con-
siderably easier to make (Callison-Burch et al, 2008).
This section presents two models for the prediction of
pairwise preferences.
The first model (ABS) is a regularized linear regres-
sion model over entailment-motivated features (see be-
low) that predicts an absolute score for each reference-
hypothesis pair. Pairwise preferences are created simply
by comparing the absolute predicted scores. This model
is more general, since it can also be used where absolute
score predictions are desirable; furthermore, the model
is efficient with a runtime linear in the number of sys-
tems and corpus size. On the downside, this model is
not optimized for the prediction of pairwise judgments.
The second model we consider is a regularized logis-
tic regression model (PAIR) that is directly optimized to
predict a weighted binary preference for each hypothe-
sis pair. This model is less efficient since its runtime is
Alignment score(3) Unaligned material (10)
Adjuncts (7) Apposition (2)
Modality (5) Factives (8)
Polarity (5) Quantors (4)
Tense (2) Dates (6)
Root (2) Semantic Relations (4)
Semantic relatedness (7) Structural Match (5)
Compatibility of locations and entities (4)
Table 1: Entailment feature groups provided by the
Stanford RTE system, with number of features
quadratic in the number of systems. On the other hand,
it can be trained on more reliable pairwise preference
judgments. In a second step, we combine the individ-
ual decisions to compute the highest-likelihood total
ordering of hypotheses. The construction of an optimal
ordering from weighted pairwise preferences is an NP-
hard problem (via reduction of CYCLIC-ORDERING;
Barzilay and Elhadad, 2002), but a greedy search yields
a close approximation (Cohen et al, 1999).
Both models can be used to predict system-level
scores from sentence-level scores. Again, we have two
method for doing this. The basic method (BASIC) pre-
dicts the quality of each system directly as the percent-
age of sentences for which its output was rated best
among all systems. However, we noticed that the man-
ual rankings for the WMT 2007 dataset show a tie for
best system for almost 30% of sentences. BASIC is
systematically unable to account for these ties. We
therefore implemented a ?tie-aware? prediction method
(WITHTIES) that uses the same sentence-level output as
BASIC, but computes system-level quality differently,
as the percentage of sentences where the system?s hy-
pothesis was scored better or at most ? worse than the
best system, for some global ?tie interval? ? .
Features. We use the Stanford RTE system (MacCart-
ney et al, 2006) to generate a set of entailment features
(RTE) for each pair of MT hypothesis and reference
translation. Features are generated in both directions
to avoid biases towards short or long translations. The
Stanford RTE system uses a three-stage architecture.
It (a) constructs a robust, dependency-based linguistic
analysis of the two sentences; (b) identifies the best
alignment between the two dependency graphs given
similarity scores from a range of lexical resources, us-
ing a Markov Chain Monte Carlo sampling strategy;
and (c) computes roughly 75 features over the aligned
pair of dependency graphs. The different feature groups
are shown in Table 1. A small number features are
real-valued, measuring different quality aspects of the
alignment. The other features are binary, indicating
matches and mismatches of different types (e.g., align-
ment between predicates embedded under compatible
or incompatible modals, respectively).
To judge to what extent the entailment-based model
delivers improvements that cannot be obtained with es-
tablished methods, we also experiment with a feature set
38
formed from a set of established MT evaluation metrics
(TRADMT). We combine different parametrization of
(smoothed) BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002), and TER (Snover et al, 2006), to give
a total of roughly 100 features. Finally, we consider a
combination of both feature sets (COMB).
3 Experimental Evaluation
Setup. To assess and compare the performance of our
models, we use corpora that were created by past in-
stances of the WMT workshop. We optimize the feature
weights for the ABS models on the WMT 2006 and
2007 absolute score annotations, and correspondingly
for the PAIR models on the WMT 2007 absolute score
and ranking annotations. All models are evaluated on
WMT 2008 to compare against the published results.
Finally, we need to set the tie interval ? . Since we
did not want to optimize ? , we simply assumed that the
percentage of ties observed on WMT 2007 generalizes
to test sets such as the 2008 dataset. We set ? so that
there are ties for first place on 30% of the sentences,
with good practical success (see below).
Results. Table 2 shows our results. The first results
column (Cons) shows consistency, i.e., accuracy in pre-
dicting human pairwise preference judgments. Note that
the performance of a random baseline is not at 50%, but
substantially lower. This is due to (a) the presence of
contradictions and ties in the human judgments, which
cannot be predicted; and (b) WMT?s requirement to
compute a total ordering of all translations for a given
sentence (rather than independent binary judgments),
which introduces transitivity constraints. See Callison-
Burch et al (2008) for details. Among our models, PAIR
shows a somewhat better consistency than ABS, as can
be expected from a model directly optimized on pair-
wise judgments. Across feature sets, COMB works best
with a consistency of 0.53, competitive with published
WMT 2008 results.
The two final columns (BASIC and WITHTIES) show
Spearman?s ? for the correlation between human judg-
ments and the two types of system-level predictions.
For BASIC system-level predictions, we find that
PAIR performs considerably worse than ABS, by a mar-
gin of up to ? = 0.1. Recall that the system-level analy-
sis considers only the top-ranked hypotheses; apparently,
a model optimized on pairwise judgments has a harder
time choosing the best among the top-ranked hypothe-
ses. This interpretation is supported by the large benefit
that PAIR derives from explicit tie modeling. ABS gains
as well, although not as much, so that the correlation of
the tie-aware predictions is similar for ABS and PAIR.
Comparing different feature sets, BASIC show a simi-
lar pattern to the consistency figures. There is no clear
winner between RTE and TRADMT. The performance
of TRADMT is considerably better than the performance
of BLEU and TER in the WMT 2008 evaluation, where
? ? 0.55. RTE is able to match the performance of an
Model Feature set Cons
(Acc.)
BASIC
(?)
WITHTIES
(?)
ABS TRADMT 0.50 0.74 0.74
ABS RTE 0.51 0.72 0.78
ABS COMB 0.51 0.74 0.74
PAIR TRADMT 0.52 0.63 0.73
PAIR RTE 0.51 0.66 0.77
PAIR COMB 0.53 0.70 0.77
WMT 2008 (worst) 0.44 0.37
WMT 2008 (best) 0.56 0.83
Table 2: Evaluation on the WMT 2008 dataset for our
regression models, compared to results fromWMT 2008
ensemble of state-of-the-art metrics, which validates our
hope that linguistically motivated entailment features
are sufficiently robust to make a positive contribution
in MT evaluation. Furthermore, the two individual fea-
ture sets are outperformed by the combined feature set
COMB. We interpret this as support for our regression-
based combination approach.
Moving to WITHTIES, we see the best results from
the RTE model which improves by ?? = 0.06 for ABS
and ?? = 0.11 for PAIR. There is less improvement for
the other feature sets, in particular COMB. We submitted
the two overall best models, ABS-RTE and PAIR-RTE
with tie-aware prediction, to the WMT 2009 challenge.
Data Analysis. We analyzed at the models? predic-
tions to gain a better understanding of the differences in
the behavior of TRADMT-based and RTE-based mod-
els. As a first step, we computed consistency numbers
for the set of ?top? translations (hypotheses that were
ranked highest for a given reference) and for the set
of ?bottom? translations (hypotheses that were ranked
worst for a given reference). We found small but con-
sistent differences between the models: RTE performs
about 1.5 percent better on the top hypotheses than on
the bottom translations. We found the inverse effect for
the TRADMT model, which performs 2 points worse on
the top hypotheses than on the bottom hypotheses. Re-
visiting our initial concern that the entailment features
are too noisy for very bad translations, this finding indi-
cates some ungrammaticality-induced degradation for
the entailment features, but not much. Conversely, these
numbers also provide support for our initial hypothesis
that surface-based features are good at detecting very
deviant translations, but can have trouble dealing with
legitimate linguistic variation.
Next, we analyzed the average size of the score dif-
ferences between the best and second-best hypotheses
for correct and incorrect predictions. We found that the
RTE-based model predicted on average almost twice the
difference for correct predictions (? = 0.30) than for
incorrect predictions (? = 0.16), while the difference
was considerably smaller for the TRADMT-based model
(? = 0.17 for correct vs. ? = 0.13 for incorrect). We
believe it is this better discrimination on the top hypothe-
39
Segment TRADMT RTE COMB Gold
REF: Scottish NHS boards need to improve criminal records checks for
employees outside Europe, a watchdog has said.
HYP: The Scottish health ministry should improve the controls on extra-
community employees to check whether they have criminal precedents,
said the monitoring committee. [1357, lium-systran]
Rank: 3 Rank: 1 Rank: 2 Rank: 1
REF: Arguments, bullying and fights between the pupils have extended
to the relations between their parents.
HYP: Disputes, chicane and fights between the pupils transposed in
relations between the parents. [686, rbmt4]
Rank: 5 Rank: 2 Rank: 4 Rank: 5
Table 3: Examples of reference translations and MT output from the WMT 2008 French-English News dataset.
Rank judgments are out of five (smaller is better).
ses that explains the increased benefit the RTE-based
model obtains from tie-aware predictions: if the best
hypothesis is wrong, chances are much better than for
the TRADMT-based model that counting the second-
best hypothesis as ?best? is correct. Unfortunately, this
property is not shared by COMB to the same degree, and
it does not improve as much as RTE.
Table 3 illustrates the difference between RTE and
TRADMT. In the first example, RTE makes a more ac-
curate prediction than TRADMT. The human rater?s
favorite translation deviates considerably from the ref-
erence translation in lexical choice, syntactic structure,
and word order, for which it is punished by TRADMT.
In contrast, RTE determines correctly that the propo-
sitional content of the reference is almost completely
preserved. The prediction of COMB is between the two
extremes. The second example shows a sentence where
RTE provides a worse prediction. This sentence was
rated as bad by the judge, presumably due to the inap-
propriate translation of the main verb. This problem,
together with the reformulation of the subject, leads
TRADMT to correctly predict a low score (rank 5/5).
RTE?s deeper analysis comes up with a high score (rank
2/5), based on the existing semantic overlap. The com-
bined model is closer to the truth, predicting rank 4.
Feature Weights. Finally, we assessed the impor-
tance of the different entailment feature groups in the
RTE model.1 Since the presence of correlated features
makes the weights difficult to interpret, we restrict our-
selves to two general observations.
First, we find high weights not only for the score of
the alignment between hypothesis and reference, but
also for a number of syntacto-semantic match and mis-
match features. This means that we do get an additional
benefit from the presence of these features. For example,
features with a negative effect include dropping adjuncts,
unaligned root nodes, incompatible modality between
the main clauses, person and location mismatches (as
opposed to general mismatches) and wrongly handled
passives. Conversely, some factors that increase the
prediction are good alignment, matching embeddings
under factive verbs, and matches between appositions.
1The feature weights are similar for the COMB model.
Second, we find clear differences in the usefulness
of feature groups between MT evaluation and the RTE
task. Some of them, in particular structural features,
can be linked to the generally lower grammaticality of
MT hypotheses. A case in point is a feature that fires
for mismatches between dependents of predicates and
which is too unreliable on the SMT data. Other differ-
ences simply reflect that the two tasks have different
profiles, as sketched in Section 2.1. RTE exhibits high
feature weights for quantifier and polarity features, both
of which have the potential to influence entailment deci-
sions, but are relatively unimportant for MT evaluation,
at least at the current state of the art.
4 Conclusion
In this paper, we have investigated an approach to MT
evaluation that is inspired by the similarity between
this task and textual entailment. Our two models ? one
predicting absolute scores and one predicting pairwise
preference judgments ? use entailment features to pre-
dict the quality of MT hypotheses, thus replacing sur-
face matching with syntacto-semantic matching. Both
models perform similarly, showing sufficient robustness
and coverage to attain comparable performance to a
committee of established MT evaluation metrics.
We have described two refinements: (1) combining
the features into a superior joint model; and (2) adding a
confidence interval around the best hypothesis to model
ties for first place. Both strategies improve correlation;
however, unfortunately the benefits do not currently
combine. Our feature weight analysis indicates that
syntacto-semantic features do play an important role in
score prediction in the RTE model. We plan to assess
the additional benefit of the full entailment feature set
against the TRADMT feature set extended by a proper
lexical similarity metric, such as METEOR.
The computation of entailment features is more
heavyweight than traditional MT evaluation metrics.
We found the speed (about 6 s per hypothesis on a cur-
rent PC) to be sufficient for easily judging the quality of
datasets of the size conventionally used for MT evalua-
tion. However, this may still be too expensive as part of
an MT model that directly optimizes some performance
measure, e.g., minimum error rate training (Och, 2003).
40
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and Summarization, pages 65?72, Ann Arbor, MI.
R. Barzilay and N. Elhadad. 2002. Inferring strategies
for sentence ordering in multidocument news summa-
rization. Journal of Artificial Intelligence Research,
17:35?55.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in ma-
chine translation research. In Proceedings of EACL,
pages 249?256, Trento, Italy.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Statistical Machine
Translation, pages 70?106, Columbus, OH.
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62, Columbus, Ohio.
William W. Cohen, Robert E. Schapire, and Yoram
Singer. 1999. Learning to order things. Journal
of Artificial Intelligence Research, 10:243?270.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Southampton, UK.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of ACL, Sydney, Australia.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram cooccurrence
statistics. In Proceedings of HLT, pages 128?132,
San Diego, CA.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. A smorgas-
bord of features for automatic MT evaluation. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 195?198, Columbus, Ohio.
Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain question
answering. In Proceedings of ACL, pages 905?912,
Sydney, Australia.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in nat-
ural language inference. In Proceedings of Coling,
pages 521?528, Manchester, UK.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of NAACL, pages
41?48, New York City, NY.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Dependency-based automatic evalu-
ation for machine translation. In Proceedings of
the NAACL-HLT / AMTA Workshop on Syntax and
Structure in Statistical Translation, pages 80?87,
Rochester, NY.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
ACL, pages 311?318, Philadelphia, PA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA, pages 223?231, Cambridge,
MA.
41
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 215?223,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Disambiguating ?DE? for Chinese-English Machine Translation
Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Manning
Computer Science Department, Stanford University
Stanford, CA 94305
pichuan,jurafsky,manning@stanford.edu
Abstract
Linking constructions involving { (DE) are ubiq-
uitous in Chinese, and can be translated into En-
glish in many different ways. This is a major source
of machine translation error, even when syntax-
sensitive translation models are used. This paper
explores how getting more information about the
syntactic, semantic, and discourse context of uses
of { (DE) can facilitate producing an appropriate
English translation strategy. We describe a finer-
grained classification of { (DE) constructions in
Chinese NPs, construct a corpus of annotated ex-
amples, and then train a log-linear classifier, which
contains linguistically inspired features. We use the
DE classifier to preprocess MT data by explicitly
labeling { (DE) constructions, as well as reorder-
ing phrases, and show that our approach provides
significant BLEU point gains on MT02 (+1.24),
MT03 (+0.88) and MT05 (+1.49) on a phrased-
based system. The improvement persists when a hi-
erarchical reordering model is applied.
1 Introduction
Machine translation (MT) from Chinese to En-
glish has been a difficult problem: structural dif-
ferences between Chinese and English, such as
the different orderings of head nouns and rela-
tive clauses, cause BLEU scores to be consis-
tently lower than for other difficult language pairs
like Arabic-English. Many of these structural
differences are related to the ubiquitous Chinese
{(DE) construction, used for a wide range of
noun modification constructions (both single word
and clausal) and other uses. Part of the solution
to dealing with these ordering issues is hierarchi-
cal decoding, such as the Hiero system (Chiang,
2005), a method motivated by {(DE) examples
like the one in Figure 1. In this case, the transla-
tion goal is to rotate the noun head and the preced-
ing relative clause around{(DE), so that we can
translate to ?[one of few countries]{ [have diplo-
matic relations with North Korea]?. Hiero can
learn this kind of lexicalized synchronous gram-
mar rule.
But use of hierarchical decoders has not solved
the DE construction translation problem. We ana-
lyzed the errors of three state-of-the-art systems
(the 3 DARPA GALE phase 2 teams? systems),
and even though all three use some kind of hier-
archical system, we found many remaining errors
related to reordering. One is shown here:
h? ? ?XProceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 51?59,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Discriminative Reordering with Chinese Grammatical Relations Features
Pi-Chuan Changa, Huihsin Tsengb, Dan Jurafskya, and Christopher D. Manninga
aComputer Science Department, Stanford University, Stanford, CA 94305
bYahoo! Inc., Santa Clara, CA 95054
{pichuan,jurafsky,manning}@stanford.edu, huihui@yahoo-inc.com
Abstract
The prevalence in Chinese of grammatical
structures that translate into English in dif-
ferent word orders is an important cause of
translation difficulty. While previous work has
used phrase-structure parses to deal with such
ordering problems, we introduce a richer set of
Chinese grammatical relations that describes
more semantically abstract relations between
words. Using these Chinese grammatical re-
lations, we improve a phrase orientation clas-
sifier (introduced by Zens and Ney (2006))
that decides the ordering of two phrases when
translated into English by adding path fea-
tures designed over the Chinese typed depen-
dencies. We then apply the log probabil-
ity of the phrase orientation classifier as an
extra feature in a phrase-based MT system,
and get significant BLEU point gains on three
test sets: MT02 (+0.59), MT03 (+1.00) and
MT05 (+0.77). Our Chinese grammatical re-
lations are also likely to be useful for other
NLP tasks.
1 Introduction
Structural differences between Chinese and English
are a major factor in the difficulty of machine trans-
lation from Chinese to English. The wide variety
of such Chinese-English differences include the or-
dering of head nouns and relative clauses, and the
ordering of prepositional phrases and the heads they
modify. Previous studies have shown that using syn-
tactic structures from the source side can help MT
performance on these constructions. Most of the
previous syntactic MT work has used phrase struc-
ture parses in various ways, either by doing syntax-
directed translation to directly translate parse trees
into strings in the target language (Huang et al,
2006), or by using source-side parses to preprocess
the source sentences (Wang et al, 2007).
One intuition for using syntax is to capture dif-
ferent Chinese structures that might have the same
(a) (ROOT  (IP     (LCP       (QP (CD ?)
        (CLP (M ?)))
      (LC ?))
    (PU ?)    (NP       (DP (DT ??))
      (NP (NN ??)))    (VP       (ADVP (AD ??))
      (VP (VV ??)        (NP           (NP             (ADJP (JJ ??))
            (NP (NN ??)))
          (NP (NN ??)))
        (QP (CD ?????)
          (CLP (M ?)))))
    (PU ?)))
(b) (ROOT  (IP     (NP       (DP (DT ??))
      (NP (NN ??)))    (VP       (LCP         (QP (CD ?)
          (CLP (M ?)))
        (LC ?))
      (ADVP (AD ??))
      (VP (VV ??)        (NP           (NP             (ADJP (JJ ??))
            (NP (NN ??)))
          (NP (NN ??)))
        (QP (CD ?????)
          (CLP (M ?)))))
    (PU ?)))
?
?
? ??
??
?? ?? ?
?? ??
??
?????
? (three) 
? (year) 
? (over; in) ?? (city)
??(complete)
??(collectively) ??(invest) ?(yuan)
 (these) ??(asset)
??(fixed)
?????(12 billion)
loc nsubj advmod dobj range
lobj det nn
nummod amod
nummod
Figure 1: Sentences (a) and (b) have the same mean-
ing, but different phrase structure parses. Both sentences,
however, have the same typed dependencies shown at the
bottom of the figure.
meaning and hence the same translation in English.
But it turns out that phrase structure (and linear or-
der) are not sufficient to capture this meaning rela-
tion. Two sentences with the same meaning can have
different phrase structures and linear orders. In the
example in Figure 1, sentences (a) and (b) have the
same meaning, but different linear orders and dif-
ferent phrase structure parses. The translation of
sentence (a) is: ?In the past three years these mu-
nicipalities have collectively put together investment
in fixed assets in the amount of 12 billion yuan.? In
sentence (b), ?in the past three years? has moved its
51
position. The temporal adverbial ??#u? (in the
past three years) has different linear positions in the
sentences. The phrase structures are different too: in
(a) the LCP is immediately under IP while in (b) it
is under VP.
We propose to use typed dependency parses in-
stead of phrase structure parses. Typed dependency
parses give information about grammatical relations
between words, instead of constituency informa-
tion. They capture syntactic relations, such as nsubj
(nominal subject) and dobj (direct object) , but also
encode semantic information such as in the loc (lo-
calizer) relation. For the example in Figure 1, if we
look at the sentence structure from the typed depen-
dency parse (bottom of Figure 1), ??#u? is con-
nected to the main verb q? (finish) by a loc (lo-
calizer) relation, and the structure is the same for
sentences (a) and (b). This suggests that this kind
of semantic and syntactic representation could have
more benefit than phrase structure parses.
Our Chinese typed dependencies are automati-
cally extracted from phrase structure parses. In En-
glish, this kind of typed dependencies has been in-
troduced by de Marneffe and Manning (2008) and
de Marneffe et al (2006). Using typed dependen-
cies, it is easier to read out relations between words,
and thus the typed dependencies have been used in
meaning extraction tasks.
We design features over the Chinese typed depen-
dencies and use them in a phrase-based MT sys-
tem when deciding whether one chunk of Chinese
words (MT system statistical phrase) should appear
before or after another. To achieve this, we train a
discriminative phrase orientation classifier follow-
ing the work by Zens and Ney (2006), and we use
the grammatical relations between words as extra
features to build the classifier. We then apply the
phrase orientation classifier as a feature in a phrase-
based MT system to help reordering.
2 Discriminative Reordering Model
Basic reordering models in phrase-based systems
use linear distance as the cost for phrase move-
ments (Koehn et al, 2003). The disadvantage of
these models is their insensitivity to the content of
the words or phrases. More recent work (Tillman,
2004; Och et al, 2004; Koehn et al, 2007) has in-
troduced lexicalized reordering models which esti-
mate reordering probabilities conditioned on the ac-
tual phrases. Lexicalized reordering models have
brought significant gains over the baseline reorder-
ing models, but one concern is that data sparseness
can make estimation less reliable. Zens and Ney
(2006) proposed a discriminatively trained phrase
orientation model and evaluated its performance as a
classifier and when plugged into a phrase-based MT
system. Their framework allows us to easily add in
extra features. Therefore we use it as a testbed to see
if we can effectively use features from Chinese typed
dependency structures to help reordering in MT.
2.1 Phrase Orientation Classifier
We build up the target language (English) translation
from left to right. The phrase orientation classifier
predicts the start position of the next phrase in the
source sentence. In our work, we use the simplest
class definition where we group the start positions
into two classes: one class for a position to the left of
the previous phrase (reversed) and one for a position
to the right (ordered).
Let c j, j? be the class denoting the movement from
source position j to source position j? of the next
phrase. The definition is:
c j, j? =
{ reversed if j? < j
ordered if j? > j
The phrase orientation classifier model is in the log-
linear form:
p?N1 (c j, j? | f J1 ,eI1, i, j)
= exp
(?Nn=1 ?nhn( f J1 ,eI1, i, j,c j, j?)
)
?c? exp
(?Nn=1 ?nhn( f J1 ,eI1, i, j,c?)
)
i is the target position of the current phrase, and f J1
and eI1 denote the source and target sentences respec-
tively. c? represents possible categories of c j, j? .
We can train this log-linear model on lots of la-
beled examples extracted from all of the aligned MT
training data. Figure 2 is an example of an aligned
sentence pair and the labeled examples that can be
extracted from it. Also, different from conventional
MERT training, we can have a large number of bi-
nary features for the discriminative phrase orienta-
tion classifier. The experimental setting will be de-
scribed in Section 4.1.
52
(21) </s>
(20) .
(19) world
(18) outside
(17) the
(16) to
(15) up
(14) opening
(13) of
(12) policy
(11) 's
(10) China
(9) from
(8) arising
(7) star
(6) bright
(5) a
(4) become
(3) already
(2) has
(1) Beihai
(0) <s>
(15)
</s>
(14)?(13)??(12)?(11)?(10)?(9)??(8)?(7)?5)?(6)?)?(4)??(3)??(2)?(1)??(0)<s>
ordered151420
ordered14618
ordered6516
reversed5715
reversed7810
reversed8109
ordered1098
reversed9137
ordered13126
ordered12115
ordered1134
ordered323
ordered211
ordered100
classj'ji
i j
Figure 2: An illustration of an alignment grid between a Chinese sentence and its English translation along with the
labeled examples for the phrase orientation classifier. Note that the alignment grid in this example is automatically
generated.
The basic feature functions are similar to what
Zens and Ney (2006) used in their MT experiments.
The basic binary features are source words within a
window of size 3 (d ? ?1,0,1) around the current
source position j, and target words within a window
of size 3 around the current target position i. In the
classifier experiments in Zens and Ney (2006) they
also use word classes to introduce generalization ca-
pabilities. In the MT setting it?s harder to incorpo-
rate the part-of-speech information on the target lan-
guage. Zens and Ney (2006) also exclude word class
information in the MT experiments. In our work
we will simply use the word features as basic fea-
tures for the classification experiments as well. As
a concrete example, we look at the labeled example
(i = 4, j = 3, j? = 11) in Figure 2. We include the
word features in a window of size 3 around j and i
as in Zens and Ney (2006), we also include words
around j? as features. So we will have nine word
features for (i = 4, j = 3, j? = 11):
Src?1:. Src0:?? Src1:?)
Src2?1:{ Src20: Src21:(
Tgt?1:already Tgt0:become Tgt1:a
2.2 Path Features Using Typed Dependencies
Assuming we have parsed the Chinese sentence that
we want to translate and have extracted the gram-
matical relations in the sentence, we design features
using the grammatical relations. We use the path be-
tween the two words annotated by the grammatical
relations. Using this feature helps the model learn
about what the relation is between the two chunks
of Chinese words. The feature is defined as follows:
for two words at positions p and q in the Chinese
53
Shared relations Chinese English
nn 15.48% 6.81%
punct 12.71% 9.64%
nsubj 6.87% 4.46%
rcmod 2.74% 0.44%
dobj 6.09% 3.89%
advmod 4.93% 2.73%
conj 6.34% 4.50%
num/nummod 3.36% 1.65%
attr 0.62% 0.01%
tmod 0.79% 0.25%
ccomp 1.30% 0.84%
xsubj 0.22% 0.34%
cop 0.07% 0.85%
cc 2.06% 3.73%
amod 3.14% 7.83%
prep 3.66% 10.73%
det 1.30% 8.57%
pobj 2.82% 10.49%
Table 1: The percentage of typed dependencies in files
1?325 in Chinese (CTB6) and English (English-Chinese
Translation Treebank)
sentence (p < q), we find the shortest path in the
typed dependency parse from p to q, concatenate all
the relations on the path and use that as a feature.
A concrete example is the sentences in Figure 3,
where the alignment grid and labeled examples are
shown in Figure 2. The glosses of the Chinese words
in the sentence are in Figure 3, and the English trans-
lation is ?Beihai has already become a bright star
arising from China?s policy of opening up to the out-
side world.? which is also listed in Figure 2.
For the labeled example (i = 4, j = 3, j? = 11),
we look at the typed dependency parse to find the
path feature between ?? and . The relevant
dependencies are: dobj(??, ?h), clf (?h, ()
and nummod( , ). Therefore the path feature is
PATH:dobjR-clfR-nummodR. We also use the direc-
tionality: we add an R to the dependency name if it?s
going against the direction of the arrow.
3 Chinese Grammatical Relations
Our Chinese grammatical relations are designed to
be very similar to the Stanford English typed depen-
dencies (de Marneffe and Manning, 2008; de Marn-
effe et al, 2006).
3.1 Description
There are 45 named grammatical relations, and a de-
fault 46th relation dep (dependent). If a dependency
matches no patterns, it will have the most generic
relation dep. The descriptions of the 45 grammat-
ical relations are listed in Table 2 ordered by their
frequencies in files 1?325 of CTB6 (LDC2007T36).
The total number of dependencies is 85748, and
other than the ones that fall into the 45 grammatical
relations, there are also 7470 dependencies (8.71%
of all dependencies) that do not match any patterns,
and therefore keep the generic name dep.
3.2 Chinese Specific Structures
Although we designed the typed dependencies to
show structures that exist both in Chinese and En-
glish, there are many other syntactic structures that
only exist in Chinese. The typed dependencies we
designed also cover those Chinese specific struc-
tures. For example, the usage of ?{? (DE) is one
thing that could lead to different English transla-
tions. In the Chinese typed dependencies, there
are relations such as cpm (DE as complementizer)
or assm (DE as associative marker) that are used
to mark these different structures. The Chinese-
specific ??? (BA) construction also has a relation
ba dedicated to it.
The typed dependencies annotate these Chinese
specific relations, but do not directly provide a map-
ping onto how they are translated into English. It
becomes more obvious how those structures affect
the ordering when Chinese sentences are translated
into English when we apply the typed dependencies
as features in the phrase orientation classifier. This
will be further discussed in Section 4.4.
3.3 Comparison with English
To compare the distribution of Chinese typed de-
pendencies with English, we extracted the English
typed dependencies from the translation of files 1?
325 in the English Chinese Translation Treebank
1.0 (LDC2007T02), which correspond to files 1?325
in CTB6. The English typed dependencies are ex-
tracted using the Stanford Parser.
There are 116,799 total English dependencies,
and 85,748 Chinese ones. On the corpus we use,
there are 45 distinct dependency types (not includ-
ing dep) in Chinese, and 50 in English. The cov-
erage of named relations is 91.29% in Chinese and
90.48% in English; the remainder are the unnamed
relation dep. We looked at the 18 shared relations
54
?? ? ?? ?? ? ? ?? ? ?? ? ? ? ?? ?
nsubj nsubjpobj lccomp loc rcmod
dobj
clfnummodadvmod
Beihai already become China to outside open during rising (DE) one measureword brightstar .prep cpm
punct
Figure 3: A Chinese example sentence labeled with typed dependencies
between Chinese and English in Table 1. Chinese
has more nn, punct, nsubj, rcmod, dobj, advmod,
conj, nummod, attr, tmod, and ccomp while English
uses more pobj, det, prep, amod, cc, cop, and xsubj,
due mainly to grammatical differences between Chi-
nese and English. For example, some determiners
in English (e.g., ?the? in (1b)) are not mandatory in
Chinese:
(1a)??=/import and export/total value
(1b) The total value of imports and exports
In another difference, English uses adjectives
(amod) to modify a noun (?financial? in (2b)) where
Chinese can use noun compounds (???/finance?
in (2a)).
(2a)?u/Tibet??/finance?/system??/reform
(2b) the reform in Tibet ?s financial system
We also noticed some larger differences between
the English and Chinese typed dependency distribu-
tions. We looked at specific examples and provide
the following explanations.
prep and pobj English has much more uses of prep
and pobj. We examined the data and found three
major reasons:
1. Chinese uses both prepositions and postposi-
tions while English only has prepositions. ?Af-
ter? is used as a postposition in Chinese exam-
ple (3a), but a preposition in English (3b):
(3a)??/1997??/after
(3b) after 1997
2. Chinese uses noun phrases in some cases where
English uses prepositions. For example, ??
-? (period, or during) is used as a noun phrase
in (4a), but it?s a preposition in English.
(4a)??/1997t/to??/1998?- /period
(4b) during 1997-1998
3. Chinese can use noun phrase modification in
situations where English uses prepositions. In
example (5a), Chinese does not use any prepo-
sitions between ?apple company? and ?new
product?, but English requires use of either
?of? or ?from?.
(5a)?*??/apple companyc??/new product
(5b) the new product of (or from) Apple
The Chinese DE constructions are also often
translated into prepositions in English.
cc and punct The Chinese sentences contain more
punctuation (punct) while the English translation
has more conjunctions (cc), because English uses
conjunctions to link clauses (?and? in (6b)) while
Chinese tends to use only punctuation (?,? in (6a)).
(6a) YJ/these?=/city??/social?/economic
0/development??/rapid??0/local
?/economic"?/strength?/clearly
/enhance
(6b) In these municipalities the social and economic de-
velopment has been rapid, and the local economic
strength has clearly been enhanced
rcmod and ccomp There are more rcmod and
ccomp in the Chinese sentences and less in the En-
glish translation, because of the following reasons:
1. Some English adjectives act as verbs in Chi-
nese. For example, c (new) is an adjectival
predicate in Chinese and the relation between
c (new) and ?? (system) is rcmod. But
?new? is an adjective in English and the En-
glish relation between ?new? and ?system? is
amod. This difference contributes to more rc-
mod in Chinese.
(7a)c/new{/(DE)X=/verify and write off
(7b) a new sales verification system
2. Chinese has two special verbs (VC): 4 (SHI)
and ? (WEI) which English doesn?t use. For
55
abbreviation short description Chinese example typed dependency counts percentagenn noun compound modifier q??e nn(?e,q?) 13278 15.48%punct punctuation 0:,?? punct(,?,?) 10896 12.71%nsubj nominal subject ?? nsubj(,??) 5893 6.87%conj conjunct (links two conjuncts) ??Z?a? conj(?a?,??) 5438 6.34%dobj direct object ???Y??G?G dobj(?Y,?G) 5221 6.09%advmod adverbial modifier \????G advmod(??,) 4231 4.93%prep prepositional modifier ?"B??Zq? prep(q?,?) 3138 3.66%nummod number modifier ?G?G nummod(G,?) 2885 3.36%amod adjectival modifier J-?? amod(??,J-) 2691 3.14%pobj prepositional object ???? pobj(??,?) 2417 2.82%rcmod relative clause modifier X?t,{<Y rcmod(<Y,?t) 2348 2.74%cpm complementizer ??{??? cpm(,{) 2013 2.35%assm associative marker ?{?? assm(?,{) 1969 2.30%assmod associative modifier ?{?? assmod(??,?) 1941 2.26%cc coordinating conjunction ??Z?a? cc(?a?,Z) 1763 2.06%clf classifier modifier ?G?G clf(?G,G) 1558 1.82%ccomp clausal complement Uq??Rzf~?? ccomp(??,Rz) 1113 1.30%det determiner YJ??? det(??,YJ) 1113 1.30%lobj localizer object ?#u lobj(u,?#) 1010 1.18%range dative object that is a quantifier phrase ?b ?7?? range(?b,?) 891 1.04%asp aspect marker ??*~ asp(?,?) 857 1.00%tmod temporal modifier 1X?t, tmod(?t,1) 679 0.79%plmod localizer modifier of a preposition ?Y?yH? plmod(?,?) 630 0.73%attr attributive ?4??7?? attr(?,??) 534 0.62%mmod modal verb modifier ?Czt?F mmod(zt,) 497 0.58%loc localizer 3??1? loc(3,1?) 428 0.50%top topic O?4??? top(4,O?) 380 0.44%pccomp clausal complement of a preposition ??\??? pccomp(?,??) 374 0.44%etc etc modifier )?s? etc(?s,) 295 0.34%lccomp clausal complement of a localizer ?)?i8??{?h lccomp(?,8) 207 0.24%ordmod ordinal number modifier ????? ordmod(?,??) 199 0.23%xsubj controlling subject Uq??Rzf~?? xsubj(Rz,Uq) 192 0.22%neg negative modifier 1X?t, neg(?t,X) 186 0.22%rcomp resultative complement ???? rcomp(??,??) 176 0.21%comod coordinated verb compound modifier ?Y"q comod(?Y,"q) 150 0.17%vmod verb modifier ??|?i??0?{*~ vmod(0?,|?) 133 0.16%prtmod particles such as?,1,u, ????Rz{?? prtmod(Rz,?) 124 0.14%ba ?ba? construction ?????5=? ba(?5,?) 95 0.11%dvpm manner DE(?) modifier ?H?3? dvpm(?H,?) 73 0.09%dvpmod a ?XP+DEV(?)? phrase that modifies VP ?H?3? dvpmod(3?,?H) 69 0.08%prnmod parenthetical modifier ???-? 1990 ? 1995? prnmod(?-, 1995) 67 0.08%cop copular ?4?{? cop(?,4) 59 0.07%pass passive marker ?????b? pass(??,?) 53 0.06%nsubjpass nominal passive subject 1??*S?{?	? nsubjpass(?*,1) 14 0.02%
Table 2: Chinese grammatical relations and examples. The counts are from files 1?325 in CTB6.
example, there is an additional relation, ccomp,
between the verb4/(SHI) and\?/reduce in
(8a). The relation is not necessary in English,
since4/SHI is not translated.
(8a) /second4/(SHI)??#/1996
?)/ChinaLl?/substantially
\?/reduce{/tariff
(8b) Second, China reduced tax substantially in
1996.
conj There are more conj in Chinese than in En-
glish for three major reasons. First, sometimes one
complete Chinese sentence is translated into sev-
eral English sentences. Our conj is defined for two
grammatical roles occurring in the same sentence,
and therefore, when a sentence breaks into multiple
ones, the original relation does not apply. Second,
we define the two grammatical roles linked by the
conj relation to be in the same word class. However,
words which are in the same word class in Chinese
may not be in the same word class in English. For
example, adjective predicates act as verbs in Chi-
nese, but as adjectives in English. Third, certain con-
structions with two verbs are described differently
between the two languages: verb pairs are described
as coordinations in a serial verb construction in Chi-
nese, but as the second verb being the complement
56
of the first verb in English.
4 Experimental Results
4.1 Experimental Setting
We use various Chinese-English parallel corpora1
for both training the phrase orientation classifier, and
for extracting statistical phrases for the phrase-based
MT system. The parallel data contains 1,560,071
sentence pairs from various parallel corpora. There
are 12,259,997 words on the English side. Chi-
nese word segmentation is done by the Stanford Chi-
nese segmenter (Chang et al, 2008). After segmen-
tation, there are 11,061,792 words on the Chinese
side. The alignment is done by the Berkeley word
aligner (Liang et al, 2006) and then we symmetrized
the word alignment using the grow-diag heuristic.
For the phrase orientation classifier experiments,
we extracted labeled examples using the parallel
data and the alignment as in Figure 2. We extracted
9,194,193 total valid examples: 86.09% of them are
ordered and the other 13.91% are reversed. To eval-
uate the classifier performance, we split these exam-
ples into training, dev and test set (8 : 1 : 1). The
phrase orientation classifier used in MT experiments
is trained with all of the available labeled examples.
Our MT experiments use a re-implementation of
Moses (Koehn et al, 2003) called Phrasal, which
provides an easier API for adding features. We
use a 5-gram language model trained on the Xin-
hua and AFP sections of the Gigaword corpus
(LDC2007T40) and also the English side of all the
LDC parallel data permissible under the NIST08
rules. Documents of Gigaword released during the
epochs of MT02, MT03, MT05, and MT06 were
removed. For features in MT experiments, we in-
corporate Moses? standard eight features as well as
the lexicalized reordering features. To have a more
comparable setting with (Zens and Ney, 2006), we
also have a baseline experiment with only the stan-
dard eight features. Parameter tuning is done with
Minimum Error Rate Training (MERT) (Och, 2003).
The tuning set for MERT is the NIST MT06 data
set, which includes 1664 sentences. We evaluate the
result with MT02 (878 sentences), MT03 (919 sen-
1LDC2002E18, LDC2003E07, LDC2003E14,
LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E85,
LDC2002L27 and LDC2005T34.
tences), and MT05 (1082 sentences).
4.2 Phrase Orientation Classifier
Feature Sets #features Train. Acc. Train. Dev DevAcc. (%) Macro-F Acc. (%) Macro-FMajority class - 86.09 - 86.09 -Src 1483696 89.02 71.33 88.14 69.03Src+Tgt 2976108 92.47 82.52 91.29 79.80Src+Src2+Tgt 4440492 95.03 88.76 93.64 85.58Src+Src2+Tgt+PATH 4691887 96.01 91.15 94.27 87.22
Table 3: Feature engineering of the phrase orientation
classifier. Accuracy is defined as (#correctly labeled ex-
amples) divided by (#all examples). The macro-F is an
average of the accuracies of the two classes.
The basic source word features described in Sec-
tion 2 are referred to as Src, and the target word
features as Tgt. The feature set that Zens and Ney
(2006) used in their MT experiments is Src+Tgt. In
addition to that, we also experimented with source
word features Src2 which are similar to Src, but take
a window of 3 around j? instead of j. In Table 3
we can see that adding the Src2 features increased
the total number of features by almost 50%, but also
improved the performance. The PATH features add
fewer total number of features than the lexical fea-
tures, but still provide a 10% error reduction and
1.63 on the macro-F1 on the dev set. We use the best
feature sets from the feature engineering in Table 3
and test it on the test set. We get 94.28% accuracy
and 87.17 macro-F1. The overall improvement of
accuracy over the baseline is 8.19 absolute points.
4.3 MT Experiments
In the MT setting, we use the log probability from
the phrase orientation classifier as an extra feature.
The weight of this discriminative reordering feature
is also tuned by MERT, along with other Moses
features. In order to understand how much the
PATH features add value to the MT experiments, we
trained two phrase orientation classifiers with differ-
ent features: one with the Src+Src2+Tgt feature set,
and the other one with Src+Src2+Tgt+PATH. The re-
sults are listed in Table 4. We compared to two
different baselines: one is Moses8Features which
has a distance-based reordering model, the other is
Baseline which also includes lexicalized reorder-
ing features. From the table we can see that using
the discriminative reordering model with PATH fea-
tures gives significant improvement over both base-
57
Setting #MERT features MT06(tune) MT02 MT03 MT05
Moses8Features 8 31.49 31.63 31.26 30.26Moses8Features+DiscrimRereorderNoPATH 9 31.76(+0.27) 31.86(+0.23) 32.09(+0.83) 31.14(+0.88)Moses8Features+DiscrimRereorderWithPATH 9 32.34(+0.85) 32.59(+0.96) 32.70(+1.44) 31.84(+1.58)
Baseline (Moses with lexicalized reordering) 16 32.55 32.56 32.65 31.89Baseline+DiscrimRereorderNoPATH 17 32.73(+0.18) 32.58(+0.02) 32.99(+0.34) 31.80(?0.09)Baseline+DiscrimRereorderWithPATH 17 32.97(+0.42) 33.15(+0.59) 33.65(+1.00) 32.66(+0.77)
Table 4: MT experiments of different settings on various NIST MT evaluation datasets. All differences marked in bold
are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005).
??? ?? ??
det
every level product
nn
? ? ?? ? ??
products of all level
??? ?? ?? ? ? ?? ? ??whole city this year industry total output value
det nn
gross industrial output value of the whole city this year
Figure 4: Two examples for the feature PATH:det-nn and
how the reordering occurs.
lines. If we use the discriminative reordering model
without PATH features and only with word features,
we still get improvement over the Moses8Features
baseline, but the MT performance is not signifi-
cantly different from Baseline which uses lexical-
ized reordering features. From Table 4 we see that
using the Src+Src2+Tgt+PATH features significantly
outperforms both baselines. Also, if we compare be-
tween Src+Src2+Tgt and Src+Src2+Tgt+PATH, the
differences are also statistically significant, which
shows the effectiveness of the path features.
4.4 Analysis: Highly-weighted Features in the
Phrase Orientation Model
There are a lot of features in the log-linear phrase
orientation model. We looked at some highly-
weighted PATH features to understand what kind
of grammatical constructions were informative for
phrase orientation. We found that many path fea-
tures corresponded to our intuitions. For example,
the feature PATH:prep-dobjR has a high weight for
being reversed. This feature informs the model that
in Chinese a PP usually appears before VP, but in
English they should be reversed. Other features
with high weights include features related to the
DE construction that is more likely to translate to
a relative clause, such as PATH:advmod-rcmod and
PATH:rcmod. They also indicate the phrases are
more likely to be chosen in reversed order. Another
frequent pattern that has not been emphasized in the
previous literature is PATH:det-nn, meaning that a
[DT NP1NP2] in Chinese is translated into English
as [NP2 DT NP1]. Examples with this feature are
in Figure 4. We can see that the important features
decided by the phrase orientation model are also im-
portant from a linguistic perspective.
5 Conclusion
We introduced a set of Chinese typed dependencies
that gives information about grammatical relations
between words, and which may be useful in other
NLP applications as well as MT. We used the typed
dependencies to build path features and used them to
improve a phrase orientation classifier. The path fea-
tures gave a 10% error reduction on the accuracy of
the classifier and 1.63 points on the macro-F1 score.
We applied the log probability as an additional fea-
ture in a phrase-based MT system, which improved
the BLEU score of the three test sets significantly
(0.59 on MT02, 1.00 on MT03 and 0.77 on MT05).
This shows that typed dependencies on the source
side are informative for the reordering component in
a phrase-based system. Whether typed dependen-
cies can lead to improvements in other syntax-based
MT systems remains a question for future research.
Acknowledgments
The authors would like to thank Marie-Catherine de
Marneffe for her help on the typed dependencies,
and Daniel Cer for building the decoder. This work
is funded by a Stanford Graduate Fellowship to the
first author and gift funding from Google for the
project ?Translating Chinese Correctly?.
58
References
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 224?232, Columbus, Ohio, June.
Association for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1?8, Manchester, UK, August. Col-
ing 2008 Organizing Committee.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC-06, pages 449?454.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA, Boston,
MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL-HLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Demonstration Session.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL,
pages 104?111, New York City, USA, June. Associa-
tion for Computational Linguistics.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of HLT-NAACL.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In ACL.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57?
64, Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004: Short Papers, pages 101?104.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 737?745, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
Association for Computational Linguistics.
59
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 492?501,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Multi-Pass Sieve for Coreference Resolution
Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers,
Mihai Surdeanu, Dan Jurafsky, Christopher Manning
Computer Science Department
Stanford University, Stanford, CA 94305
{kr,heeyoung,sudarshn,natec,mihais,jurafsky,manning}@stanford.edu
Abstract
Most coreference resolution models determine
if two mentions are coreferent using a single
function over a set of constraints or features.
This approach can lead to incorrect decisions
as lower precision features often overwhelm
the smaller number of high precision ones. To
overcome this problem, we propose a simple
coreference architecture based on a sieve that
applies tiers of deterministic coreference mod-
els one at a time from highest to lowest preci-
sion. Each tier builds on the previous tier?s
entity cluster output. Further, our model prop-
agates global information by sharing attributes
(e.g., gender and number) across mentions in
the same cluster. This cautious sieve guar-
antees that stronger features are given prece-
dence over weaker ones and that each deci-
sion is made using all of the information avail-
able at the time. The framework is highly
modular: new coreference modules can be
plugged in without any change to the other
modules. In spite of its simplicity, our ap-
proach outperforms many state-of-the-art su-
pervised and unsupervised models on several
standard corpora. This suggests that sieve-
based approaches could be applied to other
NLP tasks.
1 Introduction
Recent work on coreference resolution has shown
that a rich feature space that models lexical, syn-
tactic, semantic, and discourse phenomena is cru-
cial to successfully address the task (Bengston and
Roth, 2008; Haghighi and Klein, 2009; Haghighi
and Klein, 2010). When such a rich representation
is available, even a simple deterministic model can
achieve state-of-the-art performance (Haghighi and
Klein, 2009).
By and large most approaches decide if two men-
tions are coreferent using a single function over all
these features and information local to the two men-
tions.1 This is problematic for two reasons: (1)
lower precision features may overwhelm the smaller
number of high precision ones, and (2) local infor-
mation is often insufficient to make an informed de-
cision. Consider this example:
The second attack occurred after some rocket firings
aimed, apparently, toward [the israelis], apparently in
retaliation. [we]?re checking our facts on that one. ...
the president, quoted by ari fleischer, his spokesman, is
saying he?s concerned the strike will undermine efforts
by palestinian authorities to bring an end to terrorist at-
tacks and does not contribute to the security of [israel].
Most state-of-the-art models will incorrectly link
we to the israelis because of their proximity and
compatibility of attributes (both we and the israelis
are plural). In contrast, a more cautious approach is
to first cluster the israelis with israel because the de-
monymy relation is highly precise. This initial clus-
tering step will assign the correct animacy attribute
(inanimate) to the corresponding geo-political
entity, which will prevent the incorrect merging with
the mention we (animate) in later steps.
We propose an unsupervised sieve-like approach
to coreference resolution that addresses these is-
1As we will discuss below, some approaches use an addi-
tional component to infer the overall best mention clusters for a
document, but this is still based on confidence scores assigned
using local information.
492
sues. The approach applies tiers of coreference
models one at a time from highest to lowest pre-
cision. Each tier builds on the entity clusters con-
structed by previous models in the sieve, guarantee-
ing that stronger features are given precedence over
weaker ones. Furthermore, each model?s decisions
are richly informed by sharing attributes across the
mentions clustered in earlier tiers. This ensures that
each decision uses all of the information available
at the time. We implemented all components in our
approach using only deterministic models. All our
components are unsupervised, in the sense that they
do not require training on gold coreference links.
The contributions of this work are the following:
? We show that a simple scaffolding framework
that deploys strong features through tiers of
models performs significantly better than a
single-pass model. Additionally, we propose
several simple, yet powerful, new features.
? We demonstrate how far one can get with sim-
ple, deterministic coreference systems that do
not require machine learning or detailed se-
mantic information. Our approach outperforms
most other unsupervised coreference models
and several supervised ones on several datasets.
? Our modular framework can be easily extended
with arbitrary models, including statistical or
supervised models. We believe that our ap-
proach also serves as an ideal platform for the
development of future coreference systems.
2 Related Work
This work builds upon the recent observation that
strong features outweigh complex models for coref-
erence resolution, in both supervised and unsuper-
vised learning setups (Bengston and Roth, 2008;
Haghighi and Klein, 2009). Our work reinforces this
observation, and extends it by proposing a novel ar-
chitecture that: (a) allows easy deployment of such
features, and (b) infuses global information that can
be readily exploited by these features or constraints.
Most coreference resolution approaches perform
the task by aggregating local decisions about pairs
of mentions (Bengston and Roth, 2008; Finkel and
Manning, 2008; Haghighi and Klein, 2009; Stoy-
anov, 2010). Two recent works that diverge from
this pattern are Culotta et al (2007) and Poon and
Domingos (2008). They perform coreference reso-
lution jointly for all mentions in a document, using
first-order probabilistic models in either supervised
or unsupervised settings. Haghighi and Klein (2010)
propose a generative approach that models entity
clusters explicitly using a mostly-unsupervised gen-
erative model. As previously mentioned, our work
is not constrained by first-order or Bayesian for-
malisms in how it uses cluster information. Ad-
ditionally, the deterministic models in our tiered
model are significantly simpler, yet perform gener-
ally better than the complex inference models pro-
posed in these works.
From a high level perspective, this work falls un-
der the theory of shaping, defined as a ?method of
successive approximations? for learning (Skinner,
1938). This theory is known by different names in
many NLP applications: Brown et al (1993) used
simple models as ?stepping stones? for more com-
plex word alignment models; Collins (1999) used
?cautious? decision list learning for named entity
classification; Spitkovsky et al (2010) used ?baby
steps? for unsupervised dependency parsing, etc. To
the best of our knowledge, we are the first to apply
this theory to coreference resolution.
3 Description of the Task
Intra-document coreference resolution clusters to-
gether textual mentions within a single document
based on the underlying referent entity. Mentions
are usually noun phrases (NPs) headed by nominal
or pronominal terminals. To facilitate comparison
with most of the recent previous work, we report re-
sults using gold mention boundaries. However, our
approach does not make any assumptions about the
underlying mentions, so it is trivial to adapt it to pre-
dicted mention boundaries (e.g., see Haghighi and
Klein (2010) for a simple mention detection model).
3.1 Corpora
We used the following corpora for development and
evaluation:
? ACE2004-ROTH-DEV2 ? development split
of Bengston and Roth (2008), from the corpus
used in the 2004 Automatic Content Extraction
(ACE) evaluation. It contains 68 documents
and 4,536 mentions.
2We use the same corpus names as (Haghighi and Klein,
2009) to facilitate comparison with previous work.
493
? ACE2004-CULOTTA-TEST ? partition of
ACE 2004 corpus reserved for testing by sev-
eral previous works (Culotta et al, 2007;
Bengston and Roth, 2008; Haghighi and Klein,
2009). It consists of 107 documents and 5,469
mentions.
? ACE2004-NWIRE ? the newswire subset of
the ACE 2004 corpus, utilized by Poon and
Domingos (2008) and Haghighi and Klein
(2009) for testing. It contains 128 documents
and 11,413 mentions.
? MUC6-TEST ? test corpus from the sixth
Message Understanding Conference (MUC-6)
evaluation. It contains 30 documents and 2,068
mentions.
We used the first corpus (ACE2004-ROTH-DEV)
for development. The other corpora are reserved for
testing. We parse all documents using the Stanford
parser (Klein and Manning, 2003). The syntactic in-
formation is used to identify the mention head words
and to define the ordering of mentions in a given
sentence (detailed in the next section). For a fair
comparison with previous work, we do not use gold
named entity labels or mention types but, instead,
take the labels provided by the Stanford named en-
tity recognizer (NER) (Finkel et al, 2005).
3.2 Evaluation Metrics
We use three evaluation metrics widely used in the
literature: (a) pairwise F1 (Ghosh, 2003) ? com-
puted over mention pairs in the same entity clus-
ter; (b) MUC (Vilain et al, 1995) ? which measures
how many predicted clusters need to be merged to
cover the gold clusters; and (c) B3 (Amit and Bald-
win, 1998) ? which uses the intersection between
predicted and gold clusters for a given mention to
mark correct mentions and the sizes of the the pre-
dicted and gold clusters as denominators for preci-
sion and recall, respectively. We refer the interested
reader to (X. Luo, 2005; Finkel and Manning, 2008)
for an analysis of these metrics.
4 Description of the Multi-Pass Sieve
Our sieve framework is implemented as a succes-
sion of independent coreference models. We first de-
scribe how each model selects candidate mentions,
and then describe the models themselves.
4.1 Mention Processing
Given a mention mi, each model may either decline
to propose a solution (in the hope that one of the
subsequent models will solve it) or deterministically
select a single best antecedent from a list of pre-
vious mentions m1, . . . , mi?1. We sort candidate
antecedents using syntactic information provided by
the Stanford parser, as follows:
Same Sentence ? Candidates in the same sentence
are sorted using left-to-right breadth-first traversal
of syntactic trees (Hobbs, 1977). Figure 1 shows an
example of candidate ordering based on this traver-
sal. The left-to-right ordering favors subjects, which
tend to appear closer to the beginning of the sentence
and are more probable antecedents. The breadth-
first traversal promotes syntactic salience by rank-
ing higher noun phrases that are closer to the top of
the parse tree (Haghighi and Klein, 2009). If the
sentence containing the anaphoric mention contains
multiple clauses, we repeat the above heuristic sep-
arately in each S* constituent, starting with the one
containing the mention.
Previous Sentence ? For all nominal mentions we
sort candidates in the previous sentences using right-
to-left breadth-first traversal. This guarantees syn-
tactic salience and also favors document proximity.
For pronominal mentions, we sort candidates in pre-
vious sentences using left-to-right traversal in or-
der to favor subjects. Subjects are more probable
antecedents for pronouns (Kertz et al, 2006). For
example, this ordering favors the correct candidate
(pepsi) for the mention they:
[pepsi] says it expects to double [quaker]?s
snack food growth rate. after a month-long
courtship, [they] agreed to buy quaker oats. . .
In a significant departure from previous work,
each model in our framework gets (possibly incom-
plete) clustering information for each mention from
the earlier coreference models in the multi-pass sys-
tem. In other words, each mention mi may already
be assigned to a cluster Cj containing a set of men-
tions: Cj = {m
j
1, . . . ,m
j
k}; mi ? Cj . Unassigned
mentions are unique members of their own cluster.
We use this information in several ways:
Attribute sharing ? Pronominal coreference reso-
lution (discussed later in this section) is severely af-
494
S	 ?
of	 ?
will	 ?
head	 ?
NP	 ?
Richard	 ?Levin	 ?
the	 ?Globaliza?on	 ?Studies	 ?Center	 ?
NP	 ?
NP	 ?
the	 ?Chancelor	 ?
NP	 ?
,	 ?
VP	 ?
NP	 ?
PP	 ?
this	 ?pres?gious	 ?university	 ?
NP	 ?
VP	 ?
#1	 ?
#2	 ?
#3	 ?
#4	 ?
Figure 1: Example of left-to-right breadth-first tree
traversal. The numbers indicate the order in which the
NPs are visited.
fected by missing attributes (which introduce pre-
cision errors because incorrect antecedents are se-
lected due to missing information) and incorrect at-
tributes (which introduce recall errors because cor-
rect links are not generated due to attribute mismatch
between mention and antecedent). To address this
issue, we perform a union of all mention attributes
(e.g., number, gender, animacy) in a given cluster
and share the result with all cluster mentions. If
attributes from different mentions contradict each
other we maintain all variants. For example, our
naive number detection assigns singular to the
mention a group of students and plural to five stu-
dents. When these mentions end up in the same clus-
ter, the resulting number attributes becomes the set
{singular, plural}. Thus this cluster can later
be merged with both singular and plural pronouns.
Mention selection ? Traditionally, a coreference
model attempts to resolve every mention in the text,
which increases the likelihood of errors. Instead, in
each of our models, we exploit the cluster informa-
tion received from the previous stages by resolving
only mentions that are currently first in textual order
in their cluster. For example, given the following or-
dered list of mentions, {m11, m
2
2, m
2
3, m
3
4, m
1
5, m
2
6},
where the superscript indicates cluster id, our model
will attempt to resolve only m22 and m
3
4. These two
are the only mentions that have potential antecedents
and are currently marked as the first mentions in
their clusters. The intuition behind this heuristic
is two-fold. First, early cluster mentions are usu-
ally better defined than subsequent ones, which are
likely to have fewer modifiers or are pronouns (Fox,
1993). Several of our models use this modifier infor-
mation. Second, by definition, first mentions appear
closer to the beginning of the document, hence there
are fewer antecedent candidates to select from, and
fewer opportunities to make a mistake.
Search Pruning ? Finally, we prune the search
space using discourse salience. We disable coref-
erence for first cluster mentions that: (a) are or start
with indefinite pronouns (e.g., some, other), or (b)
start with indefinite articles (e.g., a, an). One excep-
tion to this rule is the model deployed in the first
pass; it only links mentions if their entire extents
match exactly. This model is triggered for all nom-
inal mentions regardless of discourse salience, be-
cause it is possible that indefinite mentions are re-
peated in a document when concepts are discussed
but not instantiated, e.g., a sports bar below:
Hanlon, a longtime Broncos fan, thinks it is the perfect
place for [a sports bar] and has put up a blue-and-orange
sign reading, ?Wanted Broncos Sports Bar On This Site.?
. . . In a Nov. 28 letter, Proper states ?while we have no
objection to your advertising the property as a location
for [a sports bar], using the Broncos? name and colors
gives the false impression that the bar is or can be affili-
ated with the Broncos.?
4.2 The Modules of the Multi-Pass Sieve
We now describe the coreference models imple-
mented in the sieve. For clarity, we summarize them
in Table 1 and show the cumulative performance as
they are added to the sieve in Table 2.
4.2.1 Pass 1 - Exact Match
This model links two mentions only if they con-
tain exactly the same extent text, including modifiers
and determiners, e.g., the Shahab 3 ground-ground
missile. As expected, this model is extremely pre-
cise, with a pairwise precision over 96%.
4.2.2 Pass 2 - Precise Constructs
This model links two mentions if any of the con-
ditions below are satisfied:
Appositive ? the two nominal mentions are in an
appositive construction, e.g., [Israel?s Deputy De-
fense Minister], [Ephraim Sneh] , said . . . We
use the same syntactic rules to detect appositions as
Haghighi and Klein (2009).
495
Pass Type Features
1 N exact extent match
2 N,P appositive | predicate nominative | role appositive | relative pronoun | acronym | demonym
3 N cluster head match & word inclusion & compatible modifiers only & not i-within-i
4 N cluster head match & word inclusion & not i-within-i
5 N cluster head match & compatible modifiers only & not i-within-i
6 N relaxed cluster head match & word inclusion & not i-within-i
7 P pronoun match
Table 1: Summary of passes implemented in the sieve. The Type column indicates the type of coreference in each
pass: N ? nominal or P ? pronominal. & and | indicate conjunction and disjunction of features, respectively.
Predicate nominative ? the two mentions (nominal
or pronominal) are in a copulative subject-object re-
lation, e.g., [The New York-based College Board] is
[a nonprofit organization that administers the SATs
and promotes higher education] (Poon and Domin-
gos, 2008).
Role appositive ? the candidate antecedent is
headed by a noun and appears as a modifier in an
NP whose head is the current mention, e.g., [[ac-
tress] Rebecca Schaeffer]. This feature is inspired
by Haghighi and Klein (2009), who triggered it only
if the mention is labeled as a person by the NER.
We constrain this heuristic more in our work: we
allow this feature to match only if: (a) the mention
is labeled as a person, (b) the antecedent is animate
(we detail animacy detection in Pass 7), and (c) the
antecedent?s gender is not neutral.
Relative pronoun ? the mention is a relative pro-
noun that modifies the head of the antecedent NP,
e.g., [the finance street [which] has already formed
in the Waitan district].
Acronym ? both mentions are tagged as NNP and
one of them is an acronym of the other, e.g., [Agence
France Presse] . . . [AFP]. We use a simple acronym
detection algorithm, which marks a mention as an
acronym of another if its text equals the sequence
of upper case characters in the other mention. We
will adopt better solutions for acronym detection in
future work (Schwartz, 2003).
Demonym ? one of the mentions is a demonym of
the other, e.g., [Israel] . . . [Israeli]. For demonym
detection we use a static list of countries and their
gentilic forms from Wikipedia.3
All the above features are extremely precise. As
shown in Table 2 the pairwise precision of the sieve
3
http://en.wikipedia.org/wiki/List_of_adjectival_and_
demonymic_forms_of_place_names
after adding these features is over 95% and recall
increases 5 points.
4.2.3 Pass 3 - Strict Head Matching
Linking a mention to an antecedent based on the
naive matching of their head words generates a lot
of spurious links because it completely ignores pos-
sibly incompatible modifiers (Elsner and Charniak,
2010). For example, Yale University and Harvard
University have similar head words, but they are ob-
viously different entities. To address this issue, this
pass implements several features that must all be
matched in order to yield a link:
Cluster head match ? the mention head word
matches any head word in the antecedent clus-
ter. Note that this feature is actually more relaxed
than naive head matching between mention and an-
tecedent candidate because it is satisfied when the
mention?s head matches the head of any entity in the
candidate?s cluster. We constrain this feature by en-
forcing a conjunction with the features below.
Word inclusion ? all the non-stop4 words in the
mention cluster are included in the set of non-stop
words in the cluster of the antecedent candidate.
This heuristic exploits the property of discourse that
it is uncommon to introduce novel information in
later mentions (Fox, 1993). Typically, mentions
of the same entity become shorter and less infor-
mative as the narrative progresses. For example,
the two mentions in . . . intervene in the [Florida
Supreme Court]?s move . . . does look like very dra-
matic change made by [the Florida court] point to
the same entity, but the two mentions in the text be-
low belong to different clusters:
The pilot had confirmed . . . he had turned onto
4Our stop word list includes person titles as well.
496
MUC B3 Pairwise
Passes P R F1 P R F1 P R F1
{1} 95.9 31.8 47.8 99.1 53.4 69.4 96.9 15.4 26.6
{1,2} 95.4 43.7 59.9 98.5 58.4 73.3 95.7 20.6 33.8
{1,2,3} 92.1 51.3 65.9 96.7 62.9 76.3 91.5 26.8 41.5
{1,2,3,4} 91.7 51.9 66.3 96.5 63.5 76.6 91.4 27.8 42.7
{1,2,3,4,5} 91.1 52.6 66.7 96.1 63.9 76.7 90.3 28.4 43.2
{1,2,3,4,5,6} 89.5 53.6 67.1 95.3 64.5 76.9 88.8 29.2 43.9
{1,2,3,4,5,6,7} 83.7 74.1 78.6 88.1 74.2 80.5 80.1 51.0 62.3
Table 2: Cumulative performance on development (ACE2004-ROTH-DEV) as passes are added to the sieve.
[the correct runway] but pilots behind him say
he turned onto [the wrong runway].
Compatible modifiers only ? the mention?s mod-
ifiers are all included in the modifiers of the an-
tecedent candidate. This feature models the same
discourse property as the previous feature, but it fo-
cuses on the two individual mentions to be linked,
rather than their entire clusters. For this feature we
only use modifiers that are nouns or adjectives.
Not i-within-i ? the two mentions are not in an i-
within-i construct, i.e., one cannot be a child NP
in the other?s NP constituent (Haghighi and Klein,
2009).
This pass continues to maintain high precision
(91% pairwise) while improving recall significantly
(over 6 points pairwise and almost 8 points MUC).
4.2.4 Passes 4 and 5 - Variants of Strict Head
Passes 4 and 5 are different relaxations of the
feature conjunction introduced in Pass 3, i.e.,
Pass 4 removes the compatible modifiers
only feature, while Pass 5 removes the word
inclusion constraint. All in all, these two passes
yield an improvement of 1.7 pairwise F1 points,
due to recall improvements. Table 2 shows that the
word inclusion feature is more precise than
compatible modifiers only, but the latter
has better recall.
4.2.5 Pass 6 - Relaxed Head Matching
This pass relaxes the cluster head match heuris-
tic by allowing the mention head to match any word
in the cluster of the candidate antecedent. For ex-
ample, this heuristic matches the mention Sanders
to a cluster containing the mentions {Sauls, the
judge, Circuit Judge N. Sanders Sauls}. To maintain
high precision, this pass requires that both mention
and antecedent be labeled as named entities and the
types coincide. Furthermore, this pass implements
a conjunction of the above features with word
inclusion and not i-within-i. This pass
yields less than 1 point improvement in most met-
rics.
4.2.6 Pass 7 - Pronouns
With one exception (Pass 2), all the previous
coreference models focus on nominal coreference
resolution. However, it would be incorrect to say
that our framework ignores pronominal coreference
in the first six passes. In fact, the previous mod-
els prepare the stage for pronominal coreference by
constructing precise clusters with shared mention at-
tributes. These are crucial factors for pronominal
coreference.
Like previous work, we implement pronominal
coreference resolution by enforcing agreement con-
straints between the coreferent mentions. We use the
following attributes for these constraints:
Number ? we assign number attributes based on:
(a) a static list for pronouns; (b) NER labels: men-
tions marked as a named entity are considered sin-
gular with the exception of organizations, which can
be both singular or plural; (c) part of speech tags:
NN*S tags are plural and all other NN* tags are sin-
gular; and (d) a static dictionary from (Bergsma and
Lin, 2006).
Gender ? we assign gender attributes from static
lexicons from (Bergsma and Lin, 2006; Ji and Lin,
2009).
Person ? we assign person attributes only to pro-
nouns. However, we do not enforce this constraint
when linking two pronouns if one appears within
quotes. This is a simple heuristic for speaker de-
tection, e.g., I and she point to the same person in
497
?[I] voted my conscience,? [she] said.
Animacy ? we set animacy attributes using: (a)
a static list for pronouns; (b) NER labels, e.g.,
PERSON is animate whereas LOCATION is not; and
(c) a dictionary boostrapped from the web (Ji and
Lin, 2009).
NER label ? from the Stanford NER.
If we cannot detect a value, we set attributes to
unknown and treat them as wildcards, i.e., they can
match any other value.
This final model raises the pairwise recall of our
system almost 22 percentage points, with only an 8
point drop in pairwise precision. Table 2 shows that
similar behavior is measured for all other metrics.
After all passes have run, we take the transitive clo-
sure of the generated clusters as the system output.
5 Experimental Results
We present the results of our approach and other rel-
evant prior work in Table 3. We include in the ta-
ble all recent systems that report results under the
same conditions as our experimental setup (i.e., us-
ing gold mentions) and use the same corpora. We
exclude from this analysis two notable works that
report results only on a version of the task that in-
cludes finding mentions (Haghighi and Klein, 2010;
Stoyanov, 2010). The Haghighi and Klein (2009)
numbers have two variants: with semantics (+S)
and without (?S). To measure the contribution of
our multi-pass system, we also present results from a
single-pass variant of our system that uses all appli-
cable features from the multi-pass system (marked
as ?single pass? in the table).
Our sieve model outperforms all systems on
two out of the four evaluation corpora (ACE2004-
ROTH-DEV and ACE2004-NWIRE), on all met-
rics. On the corpora where our model is not best,
it ranks a close second. For example, in ACE2004-
CULOTTA-TEST our system has a B3 F1 score
only .4 points lower than Bengston and Roth (2008)
and it outperforms all unsupervised approaches. In
MUC6-TEST, our sieve?s B3 F1 score is 1.8 points
lower than Haghighi and Klein (2009) +S, but it out-
performs a supervised system that used gold named
entity labels. Finally, the multi-pass architecture al-
ways beats the equivalent single-pass system with
its contribution ranging between 1 and 4 F1 points
depending on the corpus and evaluation metric.
Our approach has the highest precision on all cor-
pora, regardless of evaluation metric. We believe
this is particularly useful for large-scale NLP appli-
cations that use coreference resolution components,
e.g., question answering or information extraction.
These applications can generally function without
coreference information so it is beneficial to provide
such information only when it is highly precise.
6 Discussion
6.1 Comparison to Previous Work
The sieve model outperforms all other systems on
at least two test sets, even though most of the other
models are significantly richer. Amongst the com-
parisons, several are supervised (Bengston and Roth,
2008; Finkel and Manning, 2008; Culotta et al,
2007). The system of Haghighi and Klein (2009)
+S uses a lexicon of semantically-compatible noun
pairs acquired transductively, i.e., with knowledge
of the mentions in the test set. Our system does
not rely on labeled corpora for training (like super-
vised approaches) nor access to corpora during test-
ing (like Haghighi and Klein (2009)).
The system that is closest to ours is Haghighi and
Klein (2009) ?S. Like us, they use a rich set of fea-
tures and deterministic decisions. However, theirs
is a single-pass model with a smaller feature set
(no cluster-level, acronym, demonym, or animacy
information). Table 3 shows that on the two cor-
pora where results for this system are available, we
outperform it considerably on all metrics. To un-
derstand if the difference is due to the multi-pass
architecture or the richer feature set we compared
(Haghighi and Klein, 2009) ?S against both our
multi-pass system and its single-pass variant. The
comparison indicates that both these contributions
help: our single-pass system outperforms Haghighi
and Klein (2009) consistently, and the multi-pass ar-
chitecture further improves the performance of our
single-pass system between 1 and 4 F1 points, de-
pending on the corpus and evaluation metric.
6.2 Semantic Head Matching
Recent unsupervised coreference work from
Haghighi and Klein (2009) included a novel
semantic component that matched related head
words (e.g., AOL is a company) learned from select
498
MUC B3 Pairwise
P R F1 P R F1 P R F1
ACE2004-ROTH-DEV
This work (sieve) 83.7 74.1 78.6 88.1 74.2 80.5 80.1 51.0 62.3
This work (single pass) 82.2 72.6 77.1 86.8 72.6 79.1 76.0 47.6 58.5
Haghighi and Klein (2009) ?S 78.3 70.5 74.2 84.0 71.0 76.9 71.3 45.4 55.5
Haghighi and Klein (2009) +S 77.9 74.1 75.9 81.8 74.3 77.9 68.2 51.2 58.5
ACE2004-CULOTTA-TEST
This work (sieve) 80.4 71.8 75.8 86.3 75.4 80.4 71.6 46.2 56.1
This work (single pass) 78.4 69.2 73.5 85.1 73.9 79.1 69.5 44.1 53.9
Haghighi and Klein (2009) ?S 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3
Haghighi and Klein (2009) +S 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5
Culotta et al (2007) ? ? ? 86.7 73.2 79.3 ? ? ?
Bengston and Roth (2008) 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2
MUC6-TEST
This work (sieve) 90.5 68.0 77.7 91.2 61.2 73.2 90.3 53.3 67.1
This work (single pass) 89.3 65.9 75.8 90.2 58.8 71.1 89.5 50.6 64.7
Haghighi and Klein (2009) +S 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3
Poon and Domingos (2008) 83.0 75.8 79.2 ? ? ? 63.0 57.0 60.0
Finkel and Manning (2008) +G 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5
ACE2004-NWIRE
This work (sieve) 83.8 73.2 78.1 87.5 71.9 78.9 79.6 46.2 58.4
This work (single pass) 82.2 71.5 76.5 86.2 70.0 77.3 76.9 41.9 54.2
Haghighi and Klein (2009) +S 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7
Poon and Domingos (2008) 71.3 70.5 70.9 ? ? ? 62.6 38.9 48.0
Finkel and Manning (2008) +G 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9
Table 3: Results using gold mention boundaries. Where available, we show results for a given corpus grouped in
two blocks: the top block shows results of unsupervised systems and the bottom block contains supervised systems.
Bold numbers indicate best results in a given block. +/-S indicates if the (Haghighi and Klein, 2009) system in-
cludes/excludes their semantic component. +G marks systems that used gold NER labels.
wikipedia articles. They first identified articles
relevant to the entity mentions in the test set, and
then bootstrapped from known syntactic patterns
for apposition and predicate-nominatives in order to
learn a database of related head pairs. They show
impressive gains by using these learned pairs in
coreference decisions. This type of learning using
test set mentions is often described as transductive.
Our work instead focuses on an approach that
does not require access to the dataset beforehand.
We thus did not include a similar semantic compo-
nent in our system, given that running a bootstrap-
ping learner whenever a new data set is encountered
is not practical and, ultimately, reduces the usability
of this NLP component. However, our results show
that our sieve algorithm with minimal semantic in-
formation still performs as well as the Haghighi and
Klein (2009) system with semantics.
6.3 Flexible Architecture
The sieve architecture offers benefits beyond im-
proved accuracy. Its modular design provides a flex-
ibility for features that is not available in most su-
pervised or unsupervised systems. The sieve al-
lows new features to be seamlessly inserted with-
out affecting (or even understanding) the other com-
ponents. For instance, once a new high precision
feature (or group of features) is inserted as its own
stage, it will benefit later stages with more precise
clusters, but it will not interfere with their particu-
499
lar algorithmic decisions. This flexibility is in sharp
contrast to supervised classifiers that require their
models to be retrained on labeled data, and unsu-
pervised systems that do not offer a clear insertion
point for new features. It can be difficult to fully
understand how a system makes a single decision,
but the sieve allows for flexible usage with minimal
effort.
6.4 Error Analysis
Pronominal Nominal Proper Total
Pronominal 49 / 237 116 / 317 104 / 595 269 / 1149
Nominal 79 / 351 129 / 913 61 / 986 269 / 2250
Proper 51 / 518 15 / 730 38 / 595 104 / 1843
Total 179 / 1106 260 / 1960 203 / 2176 642 / 5242
Table 4: Number of pair-wise errors produced by the
sieve after transitive closure in the MUC6-TEST corpus.
Rows indicate mention types; columns are types of an-
tecedent. Each cell shows the number of precision/recall
errors for that configuration. The total number of gold
links in MUC6-TEST is 11,236.
Table 4 shows the number of incorrect pair-wise
links generated by our system on the MUC6-TEST
corpus. The table indicates that most of our er-
rors are for nominal mentions. For example, the
combined (precision plus recall) number of errors
for proper or common noun mentions is three times
larger than the number of errors made for pronom-
inal mentions. The table also highlights that most
of our errors are recall errors. There are eight times
more recall errors than precision errors in our output.
This is a consequence of our decision to prioritize
highly precise features in the sieve.
The above analysis illustrates that our next effort
should focus on improving recall. In order to under-
stand the limitations of our current system, we ran-
domly selected 60 recall errors (20 for each mention
type) and investigated their causes. Not surprisingly,
the causes are unique to each type.
For proper nouns, 50% of recall errors are due to
mention lengthening, mentions that are longer than
their earlier mentions. For example, Washington-
based USAir appears after USAir in the text, so our
head matching components skip it because their high
precision depends on disallowing new modifiers as
the discourse proceeds. When the mentions were re-
versed (as is the usual case), they match.
The common noun recall errors are very differ-
ent from proper nouns: 17 of the 20 random exam-
ples can be classified as semantic knowledge. These
errors are roughly evenly split between recognizing
categories of names (e.g., Gitano is an organization
name hence it should match the nominal antecedent
the company), and understanding hypernym rela-
tions like settlements and agreements.
Pronoun errors come in two forms. Roughly 40%
of these errors are attribute mismatches involving
sometimes ambiguous uses of gender and number
(e.g., she with Pat Carney). Another 40% are not se-
mantic or attribute-based, but rather simply arise due
to the order in which we check potential antecedents.
In all these situations, the correct links are missed
because the system chooses a closer (incorrect) an-
tecedent.
These four highlighted errors (lengthening, se-
mantics, attributes, ordering) add up to 77% of all
recall errors in the selected set. In general, each
error type is particular to a specific mention type.
This suggests that recall improvements can be made
by focusing on one mention type without aversely
affecting the others. Our sieve-based approach to
coreference uniquely allows for such new models to
be seamlessly inserted.
7 Conclusion
We presented a simple deterministic approach to
coreference resolution that incorporates document-
level information, which is typically exploited only
by more complex, joint learning models. Our sieve
architecture applies a battery of deterministic coref-
erence models one at a time from highest to low-
est precision, where each model builds on the pre-
vious model?s cluster output. Despite its simplicity,
our approach outperforms or performs comparably
to the state of the art on several corpora.
An additional benefit of the sieve framework is its
modularity: new features or models can be inserted
in the system with limited understanding of the other
features already deployed. Our code is publicly re-
leased5 and can be used both as a stand-alone coref-
erence system and as a platform for the development
of future systems.
5http://nlp.stanford.edu/software/
dcoref.shtml
500
The strong performance of our system suggests
the use of sieves in other NLP tasks for which a va-
riety of very high-precision features can be designed
and non-local features can be shared; likely candi-
dates include relation and event extraction, template
slot filling, and author name deduplication.
Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the view of DARPA, AFRL, or
the US government.
Many thanks to Jenny Finkel for writing a reim-
plementation of much of Haghighi and Klein (2009),
which served as the starting point for the work re-
ported here. We also thank Nicholas Rizzolo and
Dan Roth for helping us replicate their experimen-
tal setup, and Heng Ji and Dekang Lin for providing
their gender lexicon.
References
B. Amit and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In MUC-7.
E. Bengston and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
S. Bergsma and D. Lin. 2006. Bootstrapping Path-Based
Pronoun Resolution. In ACL-COLING.
P.F. Brown, V.J. Della Pietra, S.A. Della Pietra, and R.L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In EMNLP-VLC.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In NAACL-HLT.
M. Elsner and E. Charniak. 2010. The same-head heuris-
tic for coreference. In ACL.
J. Finkel, T. Grenager, and C. Manning. 2005. Incorpo-
rating non-local information into information extrac-
tion systems by Gibbs sampling. In ACL.
J. Finkel and C. Manning. 2008. Enforcing transitivity
in coreference resolution. In ACL.
B. A. Fox 1993. Discourse structure and anaphora:
written and conversational English. Cambridge Uni-
versity Press.
J. Ghosh. 2003. Scalable clustering methods for data
mining. Handbook of Data Mining, chapter 10, pages
247?277.
A. Haghighi and D. Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In EMNLP.
A. Haghighi and D. Klein. 2010. Coreference resolution
in a modular, entity-centered model. In HLT-NAACL.
J.R. Hobbs. 1977. Resolving pronoun references. Lin-
gua.
H. Ji and D. Lin. 2009. Gender and animacy knowl-
edge discovery from web-scale n-grams for unsuper-
vised person mention detection. In PACLIC.
L. Kertz, A. Kehler, and J. Elman. 2006. Grammatical
and Coherence-Based Factors in Pronoun Interpreta-
tion. In Proceedings of the 28th Annual Conference of
the Cognitive Science Society.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In ACL.
X. Luo. 2005. On coreference resolution performance
metrics. In HTL-EMNLP.
H. Poon and P. Domingos. 2008. Joint unsuper-
vised coreference resolution with Markov Logic. In
EMNLP.
A.S. Schwartz and M.A. Hearst. 2003. A simple
algorithm for identifying abbrevation definitions in
biomedical text. In Pacific Symposium on Biocomput-
ing.
B.F. Skinner. 1938. The behavior of organisms: An ex-
perimental analysis. Appleton-Century-Crofts.
V.I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010.
From baby steps to leapfrog: How ?less is more? in
unsupervised dependency parsing. In NAACL.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2010.
Conundrums in noun phrase coreference resolution:
making sense of the state-of-the-art. In ACL-IJCNLP.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.
Hirschman. 1995. A model-theoretic coreference
scoring scheme. In MUC-6.
501
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1269?1280,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Lateen EM: Unsupervised Training with Multiple Objectives,
Applied to Dependency Grammar Induction
Valentin I. Spitkovsky
Computer Science Department
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc.
Mountain View, CA, 94043, USA
hiyan@google.com
Daniel Jurafsky
Departments of Linguistics and Computer Science
Stanford University, Stanford, CA, 94305, USA
jurafsky@stanford.edu
Abstract
We present new training methods that aim to
mitigate local optima and slow convergence in
unsupervised training by using additional im-
perfect objectives. In its simplest form, lateen
EM alternates between the two objectives of
ordinary ?soft? and ?hard? expectation max-
imization (EM) algorithms. Switching objec-
tives when stuck can help escape local optima.
We find that applying a single such alternation
already yields state-of-the-art results for En-
glish dependency grammar induction. More
elaborate lateen strategies track both objec-
tives, with each validating the moves proposed
by the other. Disagreements can signal earlier
opportunities to switch or terminate, saving it-
erations. De-emphasizing fixed points in these
ways eliminates some guesswork from tuning
EM. An evaluation against a suite of unsu-
pervised dependency parsing tasks, for a vari-
ety of languages, showed that lateen strategies
significantly speed up training of both EM al-
gorithms, and improve accuracy for hard EM.
1 Introduction
Expectation maximization (EM) algorithms (Demp-
ster et al, 1977) play important roles in learning
latent linguistic structure. Unsupervised techniques
from this family excel at core natural language pro-
cessing (NLP) tasks, including segmentation, align-
ment, tagging and parsing. Typical implementations
specify a probabilistic framework, pick an initial
model instance, and iteratively improve parameters
using EM. A key guarantee is that subsequent model
instances are no worse than the previous, according
to training data likelihood in the given framework.
Another attractive feature that helped make EM
instrumental (Meng, 2007) is its initial efficiency:
Training tends to begin with large steps in a param-
eter space, sometimes bypassing many local optima
at once. After a modest number of such iterations,
however, EM lands close to an attractor. Next, its
convergence rate necessarily suffers: Disproportion-
ately many (and ever-smaller) steps are needed to
finally approach this fixed point, which is almost in-
variably a local optimum. Deciding when to termi-
nate EM often involves guesswork; and finding ways
out of local optima requires trial and error. We pro-
pose several strategies that address both limitations.
Unsupervised objectives are, at best, loosely cor-
related with extrinsic performance (Pereira and Sch-
abes, 1992; Merialdo, 1994; Liang and Klein, 2008,
inter alia). This fact justifies (occasionally) devi-
ating from a prescribed training course. For exam-
ple, since multiple equi-plausible objectives are usu-
ally available, a learner could cycle through them,
optimizing alternatives when the primary objective
function gets stuck; or, instead of trying to escape, it
could aim to avoid local optima in the first place, by
halting search early if an improvement to one objec-
tive would come at the expense of harming another.
We test these general ideas by focusing on non-
convex likelihood optimization using EM. This set-
ting is standard and has natural and well-understood
objectives: the classic, ?soft? EM; and Viterbi, or
?hard? EM (Kearns et al, 1997). The name ?la-
teen? comes from the sea ? triangular lateen sails
can take wind on either side, enabling sailing ves-
sels to tack (see Figure 1). As a captain can?t count
on favorable winds, so an unsupervised learner can?t
rely on co-operative gradients: soft EM maximizes
1269
Figure 1: A triangular sail atop a traditional Arab sail-
ing vessel, the dhow (right). Older square sails permitted
sailing only before the wind. But the efficient lateen sail
worked like a wing (with high pressure on one side and
low pressure on the other), allowing a ship to go almost
directly into a headwind. By tacking, in a zig-zag pattern,
it became possible to sail in any direction, provided there
was some wind at all (left). For centuries seafarers ex-
pertly combined both sails to traverse extensive distances,
greatly increasing the reach of medieval navigation.1
likelihoods of observed data across assignments to
hidden variables, whereas hard EM focuses on most
likely completions.2 These objectives are plausible,
yet both can be provably ?wrong? (Spitkovsky et al,
2010a, ?7.3). Thus, it is permissible for lateen EM
to maneuver between their gradients, for example by
tacking around local attractors, in a zig-zag fashion.
2 The Lateen Family of Algorithms
We propose several strategies that use a secondary
objective to improve over standard EM training. For
hard EM, the secondary objective is that of soft EM;
and vice versa if soft EM is the primary algorithm.
2.1 Algorithm #1: Simple Lateen EM
Simple lateen EM begins by running standard EM
to convergence, using a user-supplied initial model,
primary objective and definition of convergence.
Next, the algorithm alternates. A single lateen al-
ternation involves two phases: (i) retraining using
the secondary objective, starting from the previ-
ous converged solution (once again iterating until
convergence, but now of the secondary objective);
1Partially adapted from http://www.britannica.com/
EBchecked/topic/331395, http://allitera.tive.org/
archives/004922.html and http://landscapedvd.com/
desktops/images/ship1280x1024.jpg.
2See Brown et al?s (1993, ?6.2) definition of Viterbi train-
ing for a succinct justification of hard EM; in our case, the cor-
responding objective is Spitkovsky et al?s (2010a, ?7.1) ??VIT.
and (ii) retraining using the primary objective again,
starting from the latest converged solution (once
more to convergence of the primary objective). The
algorithm stops upon failing to sufficiently improve
the primary objective across alternations (applying
the standard convergence criterion end-to-end) and
returns the best of all models re-estimated during
training (as judged by the primary objective).
2.2 Algorithm #2: Shallow Lateen EM
Same as algorithm #1, but switches back to optimiz-
ing the primary objective after a single step with the
secondary, during phase (i) of all lateen alternations.
Thus, the algorithm alternates between optimizing
a primary objective to convergence, then stepping
away, using one iteration of the secondary optimizer.
2.3 Algorithm #3: Early-Stopping Lateen EM
This variant runs standard EM but quits early if
the secondary objective suffers. We redefine con-
vergence by ?or?-ing the user-supplied termination
criterion (i.e., a ?small-enough? change in the pri-
mary objective) with any adverse change of the sec-
ondary (i.e., an increase in its cross-entropy). Early-
stopping lateen EM does not alternate objectives.
2.4 Algorithm #4: Early-Switching Lateen EM
Same as algorithm #1, but with the new definition
of convergence, as in algorithm #3. Early-switching
lateen EM halts primary optimizers as soon as they
hurt the secondary objective and stops secondary op-
timizers once they harm the primary objective. This
algorithm terminates when it fails to sufficiently im-
prove the primary objective across a full alternation.
2.5 Algorithm #5: Partly-Switching Lateen EM
Same as algorithm #4, but again iterating primary
objectives to convergence, as in algorithm #1; sec-
ondary optimizers still continue to terminate early.
3 The Task and Study #1
We chose to test the impact of these five lateen al-
gorithms on unsupervised dependency parsing ? a
task in which EM plays an important role (Paskin,
2001; Klein and Manning, 2004; Gillenwater et al,
2010, inter alia). This entailed two sets of exper-
iments: In study #1, we tested whether single al-
ternations of simple lateen EM (as defined in ?2.1,
1270
System DDA (%)
(Blunsom and Cohn, 2010) 55.7
(Gillenwater et al, 2010) 53.3
(Spitkovsky et al, 2010b) 50.4
+ soft EM + hard EM 52.8 (+2.4)
lexicalized, using hard EM 54.3 (+1.5)
+ soft EM + hard EM 55.6 (+1.3)
Table 1: Directed dependency accuracies (DDA) on Sec-
tion 23 of WSJ (all sentences) for recent state-of-the-art
systems and our two experiments (one unlexicalized and
one lexicalized) with a single alternation of lateen EM.
Algorithm #1) improve our recent publicly-available
system for English dependency grammar induction.
In study #2, we introduced a more sophisticated
methodology that uses factorial designs and regres-
sions to evaluate lateen strategies with unsupervised
dependency parsing in many languages, after also
controlling for other important sources of variation.
For study #1, our base system (Spitkovsky et al,
2010b) is an instance of the popular (unlexicalized)
Dependency Model with Valence (Klein and Man-
ning, 2004). This model was trained using hard EM
on WSJ45 (WSJ sentences up to length 45) until suc-
cessive changes in per-token cross-entropy fell be-
low 2?20 bits (Spitkovsky et al, 2010b; 2010a, ?4).3
We confirmed that the base model had indeed con-
verged, by running 10 steps of hard EM on WSJ45
and verifying that its objective did not change much.
Next, we applied a single alternation of simple la-
teen EM: first running soft EM (this took 101 steps,
using the same termination criterion), followed by
hard EM (again to convergence ? another 23 it-
erations). The result was a decrease in hard EM?s
cross-entropy, from 3.69 to 3.59 bits per token (bpt),
accompanied by a 2.4% jump in accuracy, from 50.4
to 52.8%, on Section 23 of WSJ (see Table 1).4
Our first experiment showed that lateen EM holds
promise for simple models. Next, we tested it in
a more realistic setting, by re-estimating lexicalized
models,5 starting from the unlexicalized model?s
3http://nlp.stanford.edu/pubs/
markup-data.tar.bz2: dp.model.dmv
4It is standard practice to convert gold labeled constituents
from Penn English Treebank?s Wall Street Journal (WSJ) por-
tion (Marcus et al, 1993) into unlabeled reference dependency
parses using deterministic ?head-percolation? rules (Collins,
1999); sentence root symbols (but not punctuation) arcs count
towards accuracies (Paskin, 2001; Klein and Manning, 2004).
5We used Headden et al?s (2009) method (also the approach
parses; this took 24 steps with hard EM. We then
applied another single lateen alternation: This time,
soft EM ran for 37 steps, hard EM took another 14,
and the new model again improved, by 1.3%, from
54.3 to 55.6% (see Table 1); the corresponding drop
in (lexicalized) cross-entropy was from 6.10 to 6.09
bpt. This last model is competitive with the state-of-
the-art; moreover, gains from single applications of
simple lateen alternations (2.4 and 1.3%) are on par
with the increase due to lexicalization alone (1.5%).
4 Methodology for Study #2
Study #1 suggests that lateen EM can improve gram-
mar induction in English. To establish statistical sig-
nificance, however, it is important to test a hypothe-
sis in many settings (Ioannidis, 2005). We therefore
use a factorial experimental design and regression
analyses with a variety of lateen strategies. Two re-
gressions ? one predicting accuracy, the other, the
number of iterations ? capture the effects that la-
teen algorithms have on performance and efficiency,
relative to standard EM training. We controlled for
important dimensions of variation, such as the un-
derlying language: to make sure that our results are
not English-specific, we induced grammars in 19
languages. We also explored the impact from the
quality of an initial model (using both uniform and
ad hoc initializers), the choice of a primary objective
(i.e., soft or hard EM), and the quantity and com-
plexity of training data (shorter versus both short and
long sentences). Appendix A gives the full details.
4.1 Data Sets
We use all 23 train/test splits from the 2006/7
CoNLL shared tasks (Buchholz and Marsi, 2006;
Nivre et al, 2007),6 which cover 19 different lan-
guages.7 We splice out all punctuation labeled in the
data, as is standard practice (Paskin, 2001; Klein and
Manning, 2004), introducing new arcs from grand-
mothers to grand-daughters where necessary, both in
train- and test-sets. Evaluation is always against the
taken by the two stronger state-of-the-art systems): for words
seen at least 100 times in the training corpus, gold part-of-
speech tags are augmented with lexical items.
6These disjoint splits require smoothing; in the WSJ setting,
training and test sets overlapped (Klein and Manning, 2004).
7We down-weigh languages appearing in both years ? Ara-
bic, Chinese, Czech and Turkish ? by 50% in all our analyses.
1271
entire resulting test sets (i.e., all sentence lengths).8
4.2 Grammar Models
In all remaining experiments we model grammars
via the original DMV, which ignores punctuation; all
models are unlexicalized, with gold part-of-speech
tags for word classes (Klein and Manning, 2004).
4.3 Smoothing Mechanism
All unsmoothed models are smoothed immediately
prior to evaluation; some of the baseline models are
also smoothed during training. In both cases, we use
the ?add-one? (a.k.a. Laplace) smoothing algorithm.
4.4 Standard Convergence
We always halt an optimizer once a change in its ob-
jective?s consecutive cross-entropy values falls be-
low 2?20 bpt (at which point we consider it ?stuck?).
4.5 Scoring Function
We report directed accuracies ? fractions of cor-
rectly guessed (unlabeled) dependency arcs, includ-
ing arcs from sentence root symbols, as is standard
practice (Paskin, 2001; Klein and Manning, 2004).
Punctuation does not affect scoring, as it had been
removed from all parse trees in our data (see ?4.1).
5 Experiments
We now summarize our baseline models and briefly
review the proposed lateen algorithms. For details of
the default systems (standard soft and hard EM), all
control variables and both regressions (against final
accuracies and iteration counts) see Appendix A.
5.1 Baseline Models
We tested a total of six baseline models, experiment-
ing with two types of alternatives: (i) strategies that
perturb stuck models directly, by smoothing, ignor-
ing secondary objectives; and (ii) shallow applica-
tions of a single EM step, ignoring convergence.
Baseline B1 alternates running standard EM to
convergence and smoothing. A second baseline, B2,
smooths after every step of EM instead. Another
shallow baseline, B3, alternates single steps of soft
8With the exception of Arabic ?07, from which we discarded
a single sentence containing 145 non-punctuation tokens.
and hard EM.9 Three such baselines begin with hard
EM (marked with the subscript h); and three more
start with soft EM (marked with the subscript s).
5.2 Lateen Models
Ten models, A{1, 2, 3, 4, 5}{h,s}, correspond to our la-
teen algorithms #1?5 (?2), starting with either hard
or soft EM?s objective, to be used as the primary.
6 Results
Soft EM Hard EM
Model ?a ?i ?a ?i
Baselines B3 -2.7 ?0.2 -2.0 ?0.3
B2 +0.6 ?0.7 +0.6 ?1.2
B1 0.0 ?2.0 +0.8 ?3.7
Algorithms A1 0.0 ?1.3 +5.5 ?6.5
A2 -0.0 ?1.3 +1.5 ?3.6
A3 0.0 ?0.7 -0.1 ?0.7
A4 0.0 ?0.8 +3.0 ?2.1
A5 0.0 ?1.2 +2.9 ?3.8
Table 2: Estimated additive changes in directed depen-
dency accuracy (?a) and multiplicative changes in the
number of iterations before terminating (?i) for all base-
line models and lateen algorithms, relative to standard
training: soft EM (left) and hard EM (right). Bold en-
tries are statistically different (p < 0.01) from zero, for
?a, and one, for ?i (details in Table 4 and Appendix A).
Not one baseline attained a statistically significant
performance improvement. Shallow models B3{h,s},
in fact, significantly lowered accuracy: by 2.0%, on
average (p ? 7.8 ? 10?4), for B3h, which began with
hard EM; and down 2.7% on average (p ? 6.4?10?7),
for B3s, started with soft EM. They were, however,
3?5x faster than standard training, on average (see
Table 4 for all estimates and associated p-values;
above, Table 2 shows a preview of the full results).
6.1 A1{h,s} ? Simple Lateen EM
A1h runs 6.5x slower, but scores 5.5% higher, on av-
erage, compared to standard Viterbi training; A1s is
only 30% slower than standard soft EM, but does not
impact its accuracy at all, on average.
Figure 2 depicts a sample training run: Italian ?07
with A1h. Viterbi EM converges after 47 iterations,
9It approximates a mixture (the average of soft and hard
objectives) ? a natural comparison, computable via gradients
and standard optimization algorithms, such as L-BFGS (Liu and
Nocedal, 1989). We did not explore exact interpolations, how-
ever, because replacing EM is itself a significant confounder,
even with unchanged objectives (Berg-Kirkpatrick et al, 2010).
1272
50 100 150 200 250 300
3.0
3.5
4.0
4.5
3.39
3.26
(3.42)
(3.19)
3.33
3.23
(3.39)
(3.18)
3.29
3.21
(3.39)
(3.18)
3.29
3.22
bpt
iteration
cross-entropies (in bits per token)
Figure 2: Cross-entropies for Italian ?07, initialized uni-
formly and trained on sentences up to length 45. The two
curves are primary and secondary objectives (soft EM?s
lies below, as sentence yields are at least as likely as parse
trees): shaded regions indicate iterations of hard EM (pri-
mary); and annotated values are measurements upon each
optimizer?s convergence (soft EM?s are parenthesized).
reducing the primary objective to 3.39 bpt (the sec-
ondary is then at 3.26); accuracy on the held-out set
is 41.8%. Three alternations of lateen EM (totaling
265 iterations) further decrease the primary objec-
tive to 3.29 bpt (the secondary also declines, to 3.22)
and accuracy increases to 56.2% (14.4% higher).
6.2 A2{h,s} ? Shallow Lateen EM
A2h runs 3.6x slower, but scores only 1.5% higher,
on average, compared to standard Viterbi training;
A2s is again 30% slower than standard soft EM and
also has no measurable impact on parsing accuracy.
6.3 A3{h,s} ? Early-Stopping Lateen EM
Both A3h and A3s run 30% faster, on average, than
standard training with hard or soft EM; and neither
heuristic causes a statistical change to accuracy.
Table 3 shows accuracies and iteration counts for
10 (of 23) train/test splits that terminate early with
A3s (in one particular, example setting). These runs
are nearly twice as fast, and only two score (slightly)
lower, compared to standard training using soft EM.
6.4 A4{h,s} ? Early-Switching Lateen EM
A4h runs only 2.1x slower, but scores only 3.0%
higher, on average, compared to standard Viterbi
training; A4s is, in fact, 20% faster than standard soft
EM, but still has no measurable impact on accuracy.
6.5 A5{h,s} ? Partly-Switching Lateen EM
A5h runs 3.8x slower, scoring 2.9% higher, on av-
erage, compared to standard Viterbi training; A5s is
20% slower than soft EM, but, again, no more accu-
rate. Indeed, A4 strictly dominates both A5 variants.
CoNLL Year Soft EM A3s
& Language DDA iters DDA iters
Arabic 2006 28.4 180 28.4 118
Bulgarian ?06 39.1 253 39.6 131
Chinese ?06 49.4 268 49.4 204
Dutch ?06 21.3 246 27.8 35
Hungarian ?07 17.1 366 17.4 213
Italian ?07 39.6 194 39.6 164
Japanese ?06 56.6 113 56.6 93
Portuguese ?06 37.9 180 37.5 102
Slovenian ?06 30.8 234 31.1 118
Spanish ?06 33.3 125 33.1 73
Average: 35.4 216 36.1 125
Table 3: Directed dependency accuracies (DDA) and iter-
ation counts for the 10 (of 23) train/test splits affected by
early termination (setting: soft EM?s primary objective,
trained using shorter sentences and ad-hoc initialization).
7 Discussion
Lateen strategies improve dependency grammar in-
duction in several ways. Early stopping offers a
clear benefit: 30% higher efficiency yet same perfor-
mance as standard training. This technique could be
used to (more) fairly compare learners with radically
different objectives (e.g., lexicalized and unlexical-
ized), requiring quite different numbers of steps (or
magnitude changes in cross-entropy) to converge.
The second benefit is improved performance, but
only starting with hard EM. Initial local optima dis-
covered by soft EM are such that the impact on ac-
curacy of all subsequent heuristics is indistinguish-
able from noise (it?s not even negative). But for hard
EM, lateen strategies consistently improve accuracy
? by 1.5, 3.0 or 5.5% ? as an algorithm follows the
secondary objective longer (a single step, until the
primary objective gets worse, or to convergence).
Our results suggest that soft EM should use early
termination to improve efficiency. Hard EM, by con-
trast, could use any lateen strategy to improve either
efficiency or performance, or to strike a balance.
8 Related Work
8.1 Avoiding and/or Escaping Local Attractors
Simple lateen EM is similar to Dhillon et al?s (2002)
refinement algorithm for text clustering with spher-
ical k-means. Their ?ping-pong? strategy alternates
batch and incremental EM, exploits the strong points
of each, and improves a shared objective at every
1273
step. Unlike generalized (GEM) variants (Neal and
Hinton, 1999), lateen EM uses multiple objectives:
it sacrifices the primary in the short run, to escape
local optima; in the long run, it also does no harm,
by construction (as it returns the best model seen).
Of the meta-heuristics that use more than a stan-
dard, scalar objective, deterministic annealing (DA)
(Rose, 1998) is closest to lateen EM. DA perturbs
objective functions, instead of manipulating solu-
tions directly. As other continuation methods (All-
gower and Georg, 1990), it optimizes an easy (e.g.,
convex) function first, then ?rides? that optimum by
gradually morphing functions towards the difficult
objective; each step reoptimizes from the previous
approximate solution. Smith and Eisner (2004) em-
ployed DA to improve part-of-speech disambigua-
tion, but found that objectives had to be further
?skewed,? using domain knowledge, before it helped
(constituent) grammar induction. (For this reason,
we did not experiment with DA, despite its strong
similarities to lateen EM.) Smith and Eisner (2004)
used a ?temperature? ? to anneal a flat uniform dis-
tribution (? = 0) into soft EM?s non-convex objec-
tive (? = 1). In their framework, hard EM corre-
sponds to ? ?? ?, so the algorithms differ only in
their ?-schedule: DA?s is continuous, from 0 to 1; la-
teen EM?s is a discrete alternation, of 1 and +?.10
8.2 Terminating Early, Before Convergence
EM is rarely run to (even numerical) convergence.
Fixing a modest number of iterations a priori (Klein,
2005, ?5.3.4), running until successive likelihood ra-
tios become small (Spitkovsky et al, 2009, ?4.1) or
using a combination of the two (Ravi and Knight,
2009, ?4, Footnote 5) is standard practice in NLP.
Elworthy?s (1994, ?5, Figure 1) analysis of part-of-
speech tagging showed that, in most cases, a small
number of iterations is actually preferable to conver-
gence, in terms of final accuracies: ?regularization
by early termination? had been suggested for image
deblurring algorithms in statistical astronomy (Lucy,
1974, ?2); and validation against held-out data ? a
strategy proposed much earlier, in psychology (Lar-
son, 1931), has also been used as a halting crite-
rion in NLP (Yessenalina et al, 2010, ?4.2, 5.2).
10One can think of this as a kind of ?beam search? (Lowerre,
1976), with soft EM expanding and hard EM pruning a frontier.
Early-stopping lateen EM tethers termination to a
sign change in the direction of a secondary objective,
similarly to (cross-)validation (Stone, 1974; Geisser,
1975; Arlot and Celisse, 2010), but without splitting
data ? it trains using all examples, at all times.11,12
8.3 Training with Multiple Views
Lateen strategies may seem conceptually related to
co-training (Blum and Mitchell, 1998). However,
bootstrapping methods generally begin with some
labeled data and gradually label the rest (discrimina-
tively) as they grow more confident, but do not opti-
mize an explicit objective function; EM, on the other
hand, can be fully unsupervised, relabels all exam-
ples on each iteration (generatively), and guarantees
not to hurt a well-defined objective, at every step.13
Co-training classically relies on two views of the
data ? redundant feature sets that allow different al-
gorithms to label examples for each other, yielding
?probably approximately correct? (PAC)-style guar-
antees under certain (strong) assumptions. In con-
trast, lateen EM uses the same data, features, model
and essentially the same algorithms, changing only
their objective functions: it makes no assumptions,
but guarantees not to harm the primary objective.
Some of these distinctions have become blurred
with time: Collins and Singer (1999) introduced
an objective function (also based on agreement)
into co-training; Goldman and Zhou (2000), Ng
and Cardie (2003) and Chan et al (2004) made do
without redundant views; Balcan et al (2004) re-
laxed other strong assumptions; and Zhou and Gold-
man (2004) generalized co-training to accommodate
three and more algorithms. Several such methods
have been applied to dependency parsing (S?gaard
and Rish?j, 2010), constituent parsing (Sarkar,
11We see in it a milder contrastive estimation (Smith and Eis-
ner, 2005a; 2005b), agnostic to implicit negative evidence, but
caring whence learners push probability mass towards training
examples: when most likely parse trees begin to benefit at the
expense of their sentence yields (or vice versa), optimizers halt.
12For a recently proposed instance of EM that uses cross-
validation (CV) to optimize smoothed data likelihoods (in learn-
ing synchronous PCFGs, for phrase-based machine translation),
see Mylonakis and Sima?an?s (2010, ?3.1) CV-EM algorithm.
13Some authors (Nigam and Ghani, 2000; Ng and Cardie,
2003; Smith and Eisner, 2005a, ?5.2, 7; ?2; ?6) draw a hard line
between bootstrapping algorithms, such as self- and co-training,
and probabilistic modeling using EM; others (Dasgupta et al,
2001; Chang et al, 2007, ?1; ?5) tend to lump them together.
1274
2001) and parser reranking (Crim, 2002). Funda-
mentally, co-training exploits redundancies in unla-
beled data and/or learning algorithms. Lateen strate-
gies also exploit redundancies: in noisy objectives.
Both approaches use a second vantage point to im-
prove their perception of difficult training terrains.
9 Conclusions and Future Work
Lateen strategies can improve performance and effi-
ciency for dependency grammar induction with the
DMV. Early-stopping lateen EM is 30% faster than
standard training, without affecting accuracy ? it
reduces guesswork in terminating EM. At the other
extreme, simple lateen EM is slower, but signifi-
cantly improves accuracy ? by 5.5%, on average
? for hard EM, escaping some of its local optima.
It would be interesting to apply lateen algorithms
to advanced parsing models (Blunsom and Cohn,
2010; Headden et al, 2009, inter alia) and learn-
ing algorithms (Gillenwater et al, 2010; Cohen and
Smith, 2009, inter alia). Future work could explore
other NLP tasks ? such as clustering, sequence la-
beling, segmentation and alignment ? that often
employ EM. Our meta-heuristics are multi-faceted,
featuring aspects of iterated local search, determin-
istic annealing, cross-validation, contrastive estima-
tion and co-training. They may be generally useful
in machine learning and non-convex optimization.
Appendix A. Experimental Design
Statistical techniques are vital to many aspects of
computational linguistics (Johnson, 2009; Charniak,
1997; Abney, 1996, inter alia). We used factorial
designs,14 which are standard throughout the natu-
ral and social sciences, to assist with experimental
design and statistical analyses. Combined with or-
dinary regressions, these methods provide succinct
and interpretable summaries that explain which set-
tings meaningfully contribute to changes in depen-
dent variables, such as running time and accuracy.
14We used full factorial designs for clarity of exposition. But
many fewer experiments would suffice, especially in regression
models without interaction terms: for the more efficient frac-
tional factorial designs, as well as for randomized block designs
and full factorial designs, see Montgomery (2005, Ch. 4?9).
9.1 Dependent Variables
We constructed two regressions, for two types of de-
pendent variables: to summarize performance, we
predict accuracies; and to summarize efficiency, we
predict (logarithms of) iterations before termination.
In the performance regression, we used four dif-
ferent scores for the dependent variable. These in-
clude both directed accuracies and undirected accu-
racies, each computed in two ways: (i) using a best
parse tree; and (ii) using all parse trees. These four
types of scores provide different kinds of informa-
tion. Undirected scores ignore polarity of parent-
child relations (Paskin, 2001; Klein and Manning,
2004; Schwartz et al, 2011), partially correcting for
some effects of alternate analyses (e.g., systematic
choices between modals and main verbs for heads
of sentences, determiners for noun phrases, etc.).
And integrated scoring, using the inside-outside al-
gorithm (Baker, 1979) to compute expected accu-
racy across all ? not just best ? parse trees, has the
advantage of incorporating probabilities assigned to
individual arcs: This metric is more sensitive to the
margins that separate best from next-best parse trees,
and is not affected by tie-breaking. We tag scores
using two binary predictors in a simple (first order,
multi-linear) regression, where having multiple rel-
evant quality assessments improves goodness-of-fit.
In the efficiency regression, dependent variables
are logarithms of the numbers of iterations. Wrap-
ping EM in an inner loop of a heuristic has a mul-
tiplicative effect on the total number of models re-
estimated prior to termination. Consequently, loga-
rithms of the final counts better fit the observed data.
9.2 Independent Predictors
All of our predictors are binary indicators (a.k.a.
?dummy? variables). The undirected and integrated
factors only affect the regression for accuracies (see
Table 4, left); remaining factors participate also in
the running times regression (see Table 4, right). In a
default run, all factors are zero, corresponding to the
intercept estimated by a regression; other estimates
reflect changes in the dependent variable associated
with having that factor ?on? instead of ?off.?
? adhoc ? This setting controls initialization.
By default, we use the uninformed uniform ini-
tializer (Spitkovsky et al, 2010a); when it is
1275
Regression for Accuracies Regression for ln(Iterations)
Goodness-of-Fit: (R2adj ? 76.2%) (R2adj ? 82.4%)
Indicator Factors coeff. ?? adj. p-value
undirected 18.1 < 2.0 ? 10?16
integrated -0.9 ? 7.0 ? 10?7 coeff. ?? mult. e?? adj. p-value
(intercept) 30.9 < 2.0 ? 10?16 5.5 255.8 < 2.0 ? 10?16
adhoc 1.2 ? 3.1 ? 10?13 -0.0 1.0 ? 1.0
Model sweet 1.0 ? 3.1 ? 10?9 -0.2 0.8 < 2.0 ? 10?16
B3s shallow (soft-first) -2.7 ? 6.4 ? 10?7 -1.5 0.2 < 2.0 ? 10?16
B3h shallow (hard-first) -2.0 ? 7.8 ? 10?4 -1.2 0.3 < 2.0 ? 10?16
B2s shallow smooth 0.6 ? 1.0 -0.4 0.7 ? 1.4 ? 10?12
B1s smooth 0.0 ? 1.0 0.7 2.0 < 2.0 ? 10?16
A1s simple lateen 0.0 ? 1.0 0.2 1.3 ? 4.1 ? 10?4
A2s shallow lateen -0.0 ? 1.0 0.2 1.3 ? 5.8 ? 10?4
A3s early-stopping lateen 0.0 ? 1.0 -0.3 0.7 ? 2.6 ? 10?7
A4s early-switching lateen 0.0 ? 1.0 -0.3 0.8 ? 2.6 ? 10?7
A5s partly-switching lateen 0.0 ? 1.0 0.2 1.2 ? 4.2 ? 10?3
viterbi -4.0 ? 5.7 ? 10?16 -1.7 0.2 < 2.0 ? 10?16
B2h shallow smooth 0.6 ? 1.0 0.2 1.2 ? 5.6 ? 10?2
B1h smooth 0.8 ? 1.0 1.3 3.7 < 2.0 ? 10?16
A1h simple lateen 5.5 < 2.0 ? 10?16 1.9 6.5 < 2.0 ? 10?16
A2h shallow lateen 1.5 ? 5.0 ? 10?2 1.3 3.6 < 2.0 ? 10?16
A3h early-stopping lateen -0.1 ? 1.0 -0.4 0.7 ? 1.7 ? 10?11
A4h early-switching lateen 3.0 ? 1.0 ? 10?8 0.7 2.1 < 2.0 ? 10?16
A5h partly-switching lateen 2.9 ? 7.6 ? 10?8 1.3 3.8 < 2.0 ? 10?16
Table 4: Regressions for accuracies and natural-log-iterations, using 86 binary predictors (all p-values jointly adjusted
for simultaneous hypothesis testing; {langyear} indicators not shown). Accuracies? estimated coefficients ?? that are
statistically different from 0 ? and iteration counts? multipliers e?? significantly different from 1 ? are shown in bold.
on, we use Klein and Manning?s (2004) ?ad-
hoc? harmonic heuristic, bootstrapped using
sentences up to length 10, from the training set.
? sweet ? This setting controls the length cut-
off. By default, we train with all sentences con-
taining up to 45 tokens; when it is on, we use
Spitkovsky et al?s (2009) ?sweet spot? cutoff
of 15 tokens (recommended for English, WSJ).
? viterbi ? This setting controls the primary ob-
jective of the learning algorithm. By default,
we run soft EM; when it is on, we use hard EM.
? {langyeari}22i=1 ? This is a set of 22 mutually-
exclusive selectors for the language/year of a
train/test split; default (all zeros) is English ?07.
Due to space limitations, we exclude langyear pre-
dictors from Table 4. Further, we do not explore
(even two-way) interactions between predictors.15
15This approach may miss some interesting facts, e.g., that
the adhoc initializer is exceptionally good for English, with soft
9.3 Statistical Significance
Our statistical analyses relied on the R package (R
Development Core Team, 2011), which does not,
by default, adjust statistical significance (p-values)
for multiple hypotheses testing.16 We corrected
this using the Holm-Bonferroni method (Holm,
1979), which is uniformly more powerful than the
older (Dunn-)Bonferroni procedure; since we tested
many fewer hypotheses (44 + 42 ? one per inter-
cept/coefficient ??) than settings combinations, its ad-
justments to the p-values are small (see Table 4).17
EM. Instead it yields coarse summaries of regularities supported
by overwhelming evidence across data and training regimes.
16Since we would expect p% of randomly chosen hypotheses
to appear significant at the p% level simply by chance, we must
take precautions against these and other ?data-snooping? biases.
17We adjusted the p-values for all 86 hypotheses jointly, us-
ing http://rss.acs.unt.edu/Rdoc/library/multtest/
html/mt.rawp2adjp.html.
1276
CoNLL Year A3s Soft EM A3h Hard EM A1h
& Language DDA iters DDA iters DDA iters DDA iters DDA iters
Arabic 2006 28.4 118 28.4 162 21.6 19 21.6 21 32.1 200
?7 ? ? 26.9 171 24.7 17 24.8 24 22.0 239
Basque ?7 ? ? 39.9 180 32.0 16 32.2 20 43.6 128
Bulgarian ?6 39.6 131 39.1 253 41.6 22 41.5 25 44.3 140
Catalan ?7 ? ? 58.5 135 50.1 48 50.1 54 63.8 279
Chinese ?6 49.4 204 49.4 268 31.3 24 31.6 55 37.9 378
?7 ? ? 46.0 262 30.0 25 30.2 64 34.5 307
Czech ?6 ? ? 50.5 294 27.8 27 27.7 33 35.2 445
?7 ? ? 49.8 263 29.0 37 29.0 41 31.4 307
Danish ?6 ? ? 43.5 116 43.8 31 43.9 45 44.0 289
Dutch ?6 27.8 35 21.3 246 24.9 44 24.9 49 32.5 241
English ?7 ? ? 38.1 180 34.0 32 33.9 42 34.9 186
German ?6 ? ? 33.3 136 25.4 20 25.4 39 33.5 155
Greek ?7 ? ? 17.5 230 18.3 18 18.3 21 21.4 117
Hungarian ?7 17.4 213 17.1 366 12.3 26 12.4 36 23.0 246
Italian ?7 39.6 164 39.6 194 32.6 25 32.6 27 37.6 273
Japanese ?6 56.6 93 56.6 113 49.6 20 49.7 23 53.5 91
Portuguese ?6 37.5 102 37.9 180 28.6 27 28.9 41 34.4 134
Slovenian ?6 31.1 118 30.8 234 ? ? 23.4 22 33.6 255
Spanish ?6 33.1 73 33.3 125 18.2 29 18.4 36 33.3 235
Swedish ?6 ? ? 41.8 242 36.0 24 36.1 29 42.5 296
Turkish ?6 ? ? 29.8 303 17.8 19 22.2 38 31.9 134
?7 ? ? 28.3 227 14.0 9 10.7 31 33.4 242
Average: 37.4 162 37.0 206 30.0 26 30.0 35 37.1 221
Table 5: Performance (directed dependency accuracies measured against all sentences in the evaluation sets) and
efficiency (numbers of iterations) for standard training (soft and hard EM), early-stopping lateen EM (A3) and simple
lateen EM with hard EM?s primary objective (A1h), for all 23 train/test splits, with adhoc and sweet settings on.
9.4 Interpretation
Table 4 shows the estimated coefficients and their
(adjusted) p-values for both intercepts and most pre-
dictors (excluding the language/year of the data sets)
for all 1,840 experiments. The default (English) sys-
tem uses soft EM, trains with both short and long
sentences, and starts from an uninformed uniform
initializer. It is estimated to score 30.9%, converging
after approximately 256 iterations (both intercepts
are statistically different from zero: p < 2.0 ? 10?16).
As had to be the case, we detect a gain from undi-
rected scoring; integrated scoring is slightly (but
significantly: p ? 7.0 ? 10?7) negative, which is re-
assuring: best parses are scoring higher than the rest
and may be standing out by large margins. The ad-
hoc initializer boosts accuracy by 1.2%, overall (also
significant: p ? 3.1 ? 10?13), without a measurable
impact on running time (p ? 1.0). Training with
fewer, shorter sentences, at the sweet spot gradation,
adds 1.0% and shaves 20% off the total number of it-
erations, on average (both estimates are significant).
We find the viterbi objective harmful ? by 4.0%,
on average (p ? 5.7 ? 10?16) ? for the CoNLL sets.
Spitkovsky et al (2010a) reported that it helps on
WSJ, at least with long sentences and uniform ini-
tializers. Half of our experiments are with shorter
sentences, and half use ad hoc initializers (i.e., three
quarters of settings are not ideal for Viterbi EM),
which may have contributed to this negative result;
still, our estimates do confirm that hard EM is sig-
nificantly (80%, p < 2.0? 10?16) faster than soft EM.
9.5 More on Viterbi Training
The overall negative impact of Viterbi objectives is
a cause for concern: On average, A1h?s estimated
gain of 5.5% should more than offset the expected
4.0% loss from starting with hard EM. But it is, nev-
ertheless, important to make sure that simple lateen
EM with hard EM?s primary objective is in fact an
improvement over both standard EM algorithms.
Table 5 shows performance and efficiency num-
bers for A1h, A3{h,s}, as well as standard soft and
hard EM, using settings that are least favorable for
1277
CoNLL Year A3s Soft EM A3h Hard EM A1h
& Language DDA iters DDA iters DDA iters DDA iters DDA iters
Arabic 2006 ? ? 33.4 317 20.8 8 20.2 32 16.6 269
?7 18.6 60 8.7 252 26.5 9 26.4 14 49.5 171
Basque ?7 ? ? 18.3 245 23.2 16 23.0 23 24.0 162
Bulgarian ?6 27.0 242 27.1 293 40.6 33 40.5 34 43.9 276
Catalan ?7 15.0 74 13.8 159 53.2 30 53.1 31 59.8 176
Chinese ?6 63.5 131 63.6 261 36.8 45 36.8 47 44.5 213
?7 58.5 130 58.5 258 35.2 20 35.0 48 43.2 372
Czech ?6 29.5 125 29.7 224 23.6 18 23.8 41 27.7 179
?7 ? ? 25.9 215 27.1 37 27.2 64 28.4 767
Danish ?6 ? ? 16.6 155 28.7 30 28.7 30 38.3 241
Dutch ?6 20.4 51 21.2 174 25.5 30 25.6 38 27.8 243
English ?7 ? ? 18.0 162 ? ? 38.7 35 45.2 366
German ?6 ? ? 24.4 148 30.1 39 30.1 44 30.4 185
Greek ?7 25.5 133 25.3 156 ? ? 13.2 27 13.2 252
Hungarian ?7 ? ? 18.9 310 28.9 34 28.9 44 34.7 414
Italian ?7 25.4 127 25.3 165 ? ? 52.3 36 52.3 81
Japanese ?6 ? ? 39.3 143 42.2 38 42.4 48 50.2 199
Portuguese ?6 35.2 48 35.6 224 ? ? 34.5 21 36.7 143
Slovenian ?6 24.8 182 25.3 397 28.8 17 28.8 20 32.2 121
Spanish ?6 ? ? 27.7 252 ? ? 28.3 31 50.6 130
Swedish ?6 27.9 49 32.6 287 45.2 22 45.6 52 50.0 314
Turkish ?6 ? ? 30.5 239 30.2 16 30.6 24 29.0 138
?7 ? ? 48.8 254 34.3 24 33.1 34 35.9 269
Average: 27.3 161 27.3 225 33.2 28 33.2 35 38.2 236
Table 6: Performance (directed dependency accuracies measured against all sentences in the evaluation sets) and
efficiency (numbers of iterations) for standard training (soft and hard EM), early-stopping lateen EM (A3) and simple
lateen EM with hard EM?s primary objective (A1h), for all 23 train/test splits, with setting adhoc off and sweet on.
Viterbi training: adhoc and sweet on. Although A1h
scores 7.1% higher than hard EM, on average, it is
only slightly better than soft EM ? up 0.1% (and
worse than A1s). Without adhoc (i.e., using uniform
initializers ? see Table 6), however, hard EM still
improves, by 3.2%, on average, whereas soft EM
drops nearly 10%; here, A1h further improves over
hard EM, scoring 38.2% (up 5.0), higher than soft
EM?s accuracies from both settings (27.3 and 37.0).
This suggests that A1h is indeed better than both
standard EM algorithms. We suspect that our exper-
imental set-up may be disadvantageous for Viterbi
training, since half the settings use ad hoc initializ-
ers, and because CoNLL sets are small. (Viterbi EM
works best with more data and longer sentences.)
Acknowledgments
Partially funded by the Air Force Research Laboratory (AFRL),
under prime contract no. FA8750-09-C-0181, and by NSF, via
award #IIS-0811974. We thank Angel X. Chang, Spence Green,
David McClosky, Fernando Pereira, Slav Petrov and the anony-
mous reviewers, for many helpful comments on draft versions
of this paper, and Andrew Y. Ng, for a stimulating discussion.
First author is grateful to Lynda K. Dunnigan for first introduc-
ing him to lateen sails, among other connections, in Humanities.
1278
References
S. Abney. 1996. Statistical methods and linguistics. In
J. L. Klavans and P. Resnik, editors, The Balancing
Act: Combining Symbolic and Statistical Approaches
to Language. MIT Press.
E. L. Allgower and K. Georg. 1990. Numerical Contin-
uation Methods: An Introduction. Springer-Verlag.
S. Arlot and A. Celisse. 2010. A survey of cross-
validation procedures for model selection. Statistics
Surveys, 4.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. In Speech Communication Papers for the 97th
Meeting of the Acoustical Society of America.
M.-F. Balcan, A. Blum, and K. Yang. 2004. Co-training
and expansion: Towards bridging theory and practice.
In NIPS.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In NAACL-HLT.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In EMNLP.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
J. Chan, I. Koprinska, and J. Poon. 2004. Co-training
with a single natural feature set applied to email clas-
sification. In WI.
M.-W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
ACL.
E. Charniak. 1997. Statistical techniques for natural lan-
guage parsing. AI Magazine, 18.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In NAACL-HLT.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In EMNLP.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Crim. 2002. Co-training re-rankers for improved
parser accuracy.
S. Dasgupta, M. L. Littman, and D. McAllester. 2001.
PAC generalization bounds for co-training. In NIPS.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B, 39.
I. S. Dhillon, Y. Guan, and J. Kogan. 2002. Iterative
clustering of high dimensional text data augmented by
local search. In ICDM.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In ANLP.
S. Geisser. 1975. The predictive sample reuse method
with applications. Journal of the American Statistical
Association, 70.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Technical report, University of
Pennsylvania.
S. Goldman and Y. Zhou. 2000. Enhancing supervised
learning with unlabeled data. In ICML.
W. P. Headden, III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In NAACL-HLT.
S. Holm. 1979. A simple sequentially rejective multiple
test procedure. Scandinavian Journal of Statistics, 6.
J. P. A. Ioannidis. 2005. Why most published research
findings are false. PLoS Medicine, 2.
M. Johnson. 2009. How the statistical revolution
changes (computational) linguistics. In EACL: In-
teraction between Linguistics and Computational Lin-
guistics: Virtuous, Vicious or Vacuous?
M. Kearns, Y. Mansour, and A. Y. Ng. 1997. An
information-theoretic analysis of hard and soft assign-
ment methods for clustering. In UAI.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
D. Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
S. C. Larson. 1931. The shrinkage of the coefficient of
multiple correlation. Journal of Educational Psychol-
ogy, 22.
P. Liang and D. Klein. 2008. Analyzing the errors of
unsupervised learning. In HLT-ACL.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical Programming. Series B, 45.
B. T. Lowerre. 1976. The HARPY Speech Recognition
System. Ph.D. thesis, CMU.
L. B. Lucy. 1974. An iterative technique for the recti-
fication of observed distributions. The Astronomical
Journal, 79.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
1279
X.-L. Meng. 2007. EM and MCMC: Workhorses for sci-
entific computing (thirty years of EM and much more).
Statistica Sinica, 17.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20.
D. C. Montgomery. 2005. Design and Analysis of Exper-
iments. John Wiley & Sons, 6th edition.
M. Mylonakis and K. Sima?an. 2010. Learning prob-
abilistic synchronous CFGs for phrase-based transla-
tion. In CoNLL.
R. M. Neal and G. E. Hinton. 1999. A view of the EM
algorithm that justifies incremental, sparse, and other
variants. In M. I. Jordan, editor, Learning in Graphical
Models. MIT Press.
V. Ng and C. Cardie. 2003. Weakly supervised natural
language learning without redundant views. In HLT-
NAACL.
K. Nigam and R. Ghani. 2000. Analyzing the effective-
ness and applicability of co-training. In CIKM.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In EMNLP-
CoNLL.
M. A. Paskin. 2001. Grammatical bigrams. In NIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In ACL.
R Development Core Team, 2011. R: A Language and
Environment for Statistical Computing. R Foundation
for Statistical Computing.
S. Ravi and K. Knight. 2009. Minimized models for un-
supervised part-of-speech tagging. In ACL-IJCNLP.
K. Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression and related opt-
mization problems. Proceedings of the IEEE, 86.
A. Sarkar. 2001. Applying co-training methods to statis-
tical parsing. In NAACL.
R. Schwartz, O. Abend, R. Reichart, and A. Rappoport.
2011. Neutralizing linguistically problematic annota-
tions in unsupervised dependency parsing evaluation.
In ACL.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In ACL.
N. A. Smith and J. Eisner. 2005a. Contrastive estimation:
Training log-linear models on unlabeled data. In ACL.
N. A. Smith and J. Eisner. 2005b. Guiding unsupervised
grammar induction using contrastive estimation. In IJ-
CAI: Grammatical Inference Applications.
A. S?gaard and C. Rish?j. 2010. Semi-supervised de-
pendency parsing using generalized tri-training. In
COLING.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009.
Baby Steps: How ?Less is More? in unsupervised de-
pendency parsing. In NIPS: Grammar Induction, Rep-
resentation of Language and Language Learning.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010a. Viterbi training improves unsupervised
dependency parsing. In CoNLL.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010b.
Profiting from mark-up: Hyper-text annotations for
guided parsing. In ACL.
M. Stone. 1974. Cross-validatory choice and assessment
of statistical predictions. Journal of the Royal Statisti-
cal Society. Series B, 36.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level sentiment
classification. In EMNLP.
Y. Zhou and S. Goldman. 2004. Democratic co-learning.
In ICTAI.
1280
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1281?1290,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Dependency Parsing without Gold Part-of-Speech Tags
Valentin I. Spitkovsky??
valentin@cs.stanford.edu
Hiyan Alshawi?
hiyan@google.com
Angel X. Chang??
angelx@cs.stanford.edu
Daniel Jurafsky??
jurafsky@stanford.edu
?Computer Science Department
Stanford University
Stanford, CA, 94305
?Google Research
Google Inc.
Mountain View, CA, 94043
?Department of Linguistics
Stanford University
Stanford, CA, 94305
Abstract
We show that categories induced by unsuper-
vised word clustering can surpass the perfor-
mance of gold part-of-speech tags in depen-
dency grammar induction. Unlike classic clus-
tering algorithms, our method allows a word
to have different tags in different contexts.
In an ablative analysis, we first demonstrate
that this context-dependence is crucial to the
superior performance of gold tags ? requir-
ing a word to always have the same part-of-
speech significantly degrades the performance
of manual tags in grammar induction, elim-
inating the advantage that human annotation
has over unsupervised tags. We then introduce
a sequence modeling technique that combines
the output of a word clustering algorithm with
context-colored noise, to allow words to be
tagged differently in different contexts. With
these new induced tags as input, our state-of-
the-art dependency grammar inducer achieves
59.1% directed accuracy on Section 23 (all
sentences) of the Wall Street Journal (WSJ)
corpus ? 0.7% higher than using gold tags.
1 Introduction
Unsupervised learning ? machine learning without
manually-labeled training examples ? is an active
area of scientific research. In natural language pro-
cessing, unsupervised techniques have been success-
fully applied to tasks such as word alignment for ma-
chine translation. And since the advent of the web,
algorithms that induce structure from unlabeled data
have continued to steadily gain importance. In this
paper we focus on unsupervised part-of-speech tag-
ging and dependency parsing ? two related prob-
lems of syntax discovery. Our methods are applica-
ble to vast quantities of unlabeled monolingual text.
Not all research on these problems has been fully
unsupervised. For example, to the best of our knowl-
edge, every new state-of-the-art dependency gram-
mar inducer since Klein and Manning (2004) relied
on gold part-of-speech tags. For some time, multi-
point performance degradations caused by switching
to automatically induced word categories have been
interpreted as indications that ?good enough? parts-
of-speech induction methods exist, justifying the fo-
cus on grammar induction with supervised part-of-
speech tags (Bod, 2006), pace (Cramer, 2007). One
of several drawbacks of this practice is that it weak-
ens any conclusions that could be drawn about how
computers (and possibly humans) learn in the ab-
sence of explicit feedback (McDonald et al, 2011).
In turn, not all unsupervised taggers actually in-
duce word categories: Many systems ? known as
part-of-speech disambiguators (Merialdo, 1994) ?
rely on external dictionaries of possible tags. Our
work builds on two older part-of-speech inducers
? word clustering algorithms of Clark (2000) and
Brown et al (1992) ? that were recently shown to
be more robust than other well-known fully unsuper-
vised techniques (Christodoulopoulos et al, 2010).
We investigate which properties of gold part-of-
speech tags are useful in grammar induction and
parsing, and how these properties could be intro-
duced into induced tags. We also explore the number
of word classes that is good for grammar induction:
in particular, whether categorization is needed at all.
By removing the ?unrealistic simplification? of us-
ing gold tags (Petrov et al, 2011, ?3.2, Footnote 4),
we will go on to demonstrate why grammar induc-
tion from plain text is no longer ?still too difficult.?
1281
NNS VBD IN NN ?
Payrolls fell in September .
P = (1 ?
0? ?? ?
PSTOP(, L, T)) ? PATTACH(, L, VBD)
? (1 ? PSTOP(VBD, L, T)) ? PATTACH(VBD, L, NNS)
? (1 ? PSTOP(VBD, R, T)) ? PATTACH(VBD, R, IN)
? (1 ? PSTOP(IN, R, T)) ? PATTACH(IN, R, NN)
? PSTOP(VBD, L, F) ? PSTOP(VBD, R, F)
? PSTOP(NNS, L, T) ? PSTOP(NNS, R, T)
? PSTOP(IN, L, T) ? PSTOP(IN, R, F)
? PSTOP(NN, L, T) ? PSTOP(NN, R, T)
? PSTOP(, L, F)? ?? ?
1
? PSTOP(, R, T)? ?? ?
1
.
Figure 1: A dependency structure for a short WSJ sen-
tence and its probability, factored by the DMV, using gold
tags, after summing out PORDER (Spitkovsky et al, 2009).
2 Methodology
In all experiments, we model the English grammar
via Klein and Manning?s (2004) Dependency Model
with Valence (DMV), induced from subsets of not-
too-long sentences of the Wall Street Journal (WSJ).
2.1 The Model
The original DMV is a single-state head automata
model (Alshawi, 1996) over lexical word classes
{cw} ? gold part-of-speech tags. Its generative story
for a sub-tree rooted at a head (of class ch) rests on
three types of independent decisions: (i) initial di-
rection dir ? {L, R} in which to attach children, via
probability PORDER(ch); (ii) whether to seal dir, stop-
ping with probability PSTOP(ch, dir, adj), conditioned
on adj ? {T, F} (true iff considering dir?s first, i.e., ad-
jacent, child); and (iii) attachments (of class ca), ac-
cording to PATTACH(ch, dir, ca). This recursive process
produces only projective trees. A root token ? gen-
erates the head of the sentence as its left (and only)
child (see Figure 1 for a simple, concrete example).
2.2 Learning Algorithms
The DMV lends itself to unsupervised learning via
inside-outside re-estimation (Baker, 1979). Klein
and Manning (2004) initialized their system using an
?ad-hoc harmonic? completion, followed by training
using 40 steps of EM (Klein, 2005). We reproduce
this set-up, iterating without actually verifying con-
vergence, in most of our experiments (#1?4, ?3?4).
Experiments #5?6 (?5) employ our new state-of-
the-art grammar inducer (Spitkovsky et al, 2011),
which uses constrained Viterbi EM (details in ?5).
2.3 Training Data
The DMV is usually trained on a customized sub-
set of Penn English Treebank?s Wall Street Jour-
nal portion (Marcus et al, 1993). Following Klein
and Manning (2004), we begin with reference con-
stituent parses, prune out all empty sub-trees and
remove punctuation and terminals (tagged # and $)
that are not pronounced where they appear. We then
train only on the remaining sentence yields consist-
ing of no more than fifteen tokens (WSJ15), in most
of our experiments (#1?4, ?3?4); by contrast, Klein
and Manning?s (2004) original system was trained
using less data: sentences up to length ten (WSJ10).1
Our final experiments (#5?6, ?5) employ a simple
scaffolding strategy (Spitkovsky et al, 2010a) that
follows up initial training at WSJ15 (?less is more?)
with an additional training run (?leapfrog?) that in-
corporates most sentences of the data set, at WSJ45.
2.4 Evaluation Methods
Evaluation is against the training set, as is standard
practice in unsupervised learning, in part because
Klein and Manning (2004, ?3) did not smooth the
DMV (Klein, 2005, ?6.2). For most of our experi-
ments (#1?4, ?3?4), this entails starting with the ref-
erence trees from WSJ15 (as modified in ?2.3), au-
tomatically converting their labeled constituents into
unlabeled dependencies using deterministic ?head-
percolation? rules (Collins, 1999), and then com-
puting (directed) dependency accuracy scores of the
corresponding induced trees. We report overall per-
centages of correctly guessed arcs, including the
arcs from sentence root symbols, as is standard prac-
tice (Paskin, 2001; Klein and Manning, 2004).
For a meaningful comparison with previous work,
we also test some of the models from our earlier ex-
periments (#1,3) ? and both models from final ex-
periments (#5,6) ? against Section 23 of WSJ?, af-
ter applying Laplace (a.k.a. ?add one?) smoothing.
1WSJ15 contains 15,922 sentences up to length fifteen (a to-
tal of 163,715 tokens, not counting punctuation) ? versus 7,422
sentences of at most ten words (only 52,248 tokens) comprising
WSJ10 ? and is a better trade-off between the quantity and
complexity of training data in WSJ (Spitkovsky et al, 2009).
1282
Accuracy Viable
1. manual tags Unsupervised Sky Groups
gold 50.7 78.0 36
mfc 47.2 74.5 34
mfp 40.4 76.4 160
ua 44.3 78.4 328
2. tagless lexicalized models
full 25.8 97.3 49,180
partial 29.3 60.5 176
none 30.7 24.5 1
3. tags from a flat (Clark, 2000) clustering
47.8 83.8 197
4. prefixes of a hierarchical (Brown et al, 1992) clustering
first 7 bits 46.4 73.9 96
8 bits 48.0 77.8 165
9 bits 46.8 82.3 262
Table 1: Directed accuracies for the ?less is more? DMV,
trained on WSJ15 (after 40 steps of EM) and evaluated
also against WSJ15, using various lexical categories in
place of gold part-of-speech tags. For each tag-set, we
include its effective number of (non-empty) categories in
WSJ15 and the oracle skylines (supervised performance).
3 Motivation and Ablative Analyses
The concepts of polysemy and synonymy are of fun-
damental importance in linguistics. For words that
can take on multiple parts of speech, knowing the
gold tag can reduce ambiguity, improving parsing by
limiting the search space. Furthermore, pooling the
statistics of words that play similar syntactic roles,
as signaled by shared gold part-of-speech tags, can
simplify the learning task, improving generalization
by reducing sparsity. We begin with two sets of ex-
periments that explore the impact that each of these
factors has on grammar induction with the DMV.
3.1 Experiment #1: Human-Annotated Tags
Our first set of experiments attempts to isolate the
effect that replacing gold part-of-speech tags with
deterministic one class per word mappings has on
performance, quantifying the cost of switching to a
monosemous clustering (see Table 1: manual; and
Table 4). Grammar induction with gold tags scores
50.7%, while the oracle skyline (an ideal, supervised
instance of the DMV) could attain 78.0% accuracy.
It may be worth noting that only 6,620 (13.5%) of
49,180 unique tokens in WSJ appear with multiple
part-of-speech tags. Most words, like it, are always
tagged the same way (5,768 times PRP). Some words,
token mfc mfp ua
it {PRP} {PRP} {PRP}
gains {NNS} {VBZ, NNS} {VBZ, NNS}
the {DT} {JJ, DT} {VBP, NNP, NN, JJ, DT, CD}
Table 2: Example most frequent class, most frequent pair
and union all reassignments for tokens it, the and gains.
like gains, usually serve as one part of speech (227
times NNS, as in the gains) but are occasionally used
differently (5 times VBZ, as in he gains). Only 1,322
tokens (2.7%) appear with three or more different
gold tags. However, this minority includes the most
frequent word ? the (50,959 times DT, 7 times JJ,
6 times NNP and once as each of CD, NN and VBP).2
We experimented with three natural reassign-
ments of part-of-speech categories (see Table 2).
The first, most frequent class (mfc), simply maps
each token to its most common gold tag in the entire
WSJ (with ties resolved lexicographically). This ap-
proach discards two gold tags (types PDT and RBR are
not most common for any of the tokens in WSJ15)
and costs about three-and-a-half points of accuracy,
in both supervised and unsupervised regimes.
Another reassignment, union all (ua), maps each
token to the set of all of its observed gold tags, again
in the entire WSJ. This inflates the number of group-
ings by nearly a factor of ten (effectively lexicaliz-
ing the most ambiguous words),3 yet improves the
oracle skyline by half-a-point over actual gold tags;
however, learning is harder with this tag-set, losing
more than six points in unsupervised training.
Our last reassignment, most frequent pair (mfp),
allows up to two of the most common tags into
a token?s label set (with ties, once again, resolved
lexicographically). This intermediate approach per-
forms strictly worse than union all, in both regimes.
3.2 Experiment #2: Lexicalization Baselines
Our next set of experiments assesses the benefits of
categorization, turning to lexicalized baselines that
avoid grouping words altogether. All three models
discussed below estimated the DMV without using
the gold tags in any way (see Table 1: lexicalized).
2Some of these are annotation errors in the treebank (Banko
and Moore, 2004, Figure 2): such (mis)taggings can severely
degrade the accuracy of part-of-speech disambiguators, without
additional supervision (Banko and Moore, 2004, ?5, Table 1).
3Kupiec (1992) found that the 50,000-word vocabulary of
the Brown corpus similarly reduces to ?400 ambiguity classes.
1283
First, not surprisingly, a fully-lexicalized model
over nearly 50,000 unique words is able to essen-
tially memorize the training set, supervised. (With-
out smoothing, it is possible to deterministically at-
tach most rare words in a dependency tree correctly,
etc.) Of course, local search is unlikely to find good
instantiations for so many parameters, causing unsu-
pervised accuracy for this model to drop in half.
For our next experiment, we tried an intermediate,
partially-lexicalized approach. We mapped frequent
words ? those seen at least 100 times in the training
corpus (Headden et al, 2009) ? to their own indi-
vidual categories, lumping the rest into a single ?un-
known? cluster, for a total of under 200 groups. This
model is significantly worse for supervised learn-
ing, compared even with the monosemous clusters
derived from gold tags; yet it is only slightly more
learnable than the broken fully-lexicalized variant.
Finally, for completeness, we trained a model that
maps every token to the same one ?unknown? cat-
egory. As expected, such a trivial ?clustering? is
ineffective in supervised training; however, it out-
performs both lexicalized variants unsupervised,4
strongly suggesting that lexicalization alone may be
insufficient for the DMV and hinting that some de-
gree of categorization is essential to its learnability.
Cluster #173 Cluster #188
1. open 1. get
2. free 2. make
3. further 3. take
4. higher 4. find
5. lower 5. give
6. similar 6. keep
7. leading 7. pay
8. present 8. buy
9. growing 9. win
10. increased 10. sell
.
.
.
.
.
.
37. cool 42. improve
.
.
.
.
.
.
1,688. up-wind 2,105. zero-out
Table 3: Representative members for two of the flat word
groupings: cluster #173 (left) contains adjectives, espe-
cially ones that take comparative (or other) complements;
cluster #188 comprises bare-stem verbs (infinitive stems).
(Of course, many of the words have other syntactic uses.)
4Note that it also beats supervised training. That isn?t a bug:
Spitkovsky et al (2010b, ?7.2) explain this paradox in the DMV.
4 Grammars over Induced Word Clusters
We have demonstrated the need for grouping simi-
lar words, estimated a bound on performance losses
due to monosemous clusterings and are now ready
to experiment with induced part-of-speech tags. We
use two sets of established, publicly-available hard
clustering assignments, each computed from a much
larger data set than WSJ (approximately a million
words). The first is a flat mapping (200 clusters)
constructed by training Clark?s (2000) distributional
similarity model over several hundred million words
from the British National and the English Gigaword
corpora.5 The second is a hierarchical clustering ?
binary strings up to eighteen bits long ? constructed
by running Brown et al?s (1992) algorithm over 43
million words from the BLLIP corpus, minus WSJ.6
4.1 Experiment #3: A Flat Word Clustering
Our main purely unsupervised results are with a flat
clustering (Clark, 2000) that groups words having
similar context distributions, according to Kullback-
Leibler divergence. (A word?s context is an ordered
pair: its left- and right-adjacent neighboring words.)
To avoid overfitting, we employed an implemen-
tation from previous literature (Finkel and Manning,
2009). The number of clusters (200) and the suf-
ficient amount of training data (several hundred-
million words) were tuned to a task (NER) that is
not directly related to dependency parsing. (Table 3
shows representative entries for two of the clusters.)
We added one more category (#0) for unknown
words. Now every token in WSJ could again be re-
placed by a coarse identifier (one of at most 201,
instead of just 36), in both supervised and unsuper-
vised training. (Our training code did not change.)
The resulting supervised model, though not as
good as the fully-lexicalized DMV, was more than
five points more accurate than with gold part-of-
speech tags (see Table 1: flat). Unsupervised accu-
racy was lower than with gold tags (see also Table 4)
but higher than with all three derived hard assign-
ments. This suggests that polysemy (i.e., ability to
5http://nlp.stanford.edu/software/
stanford-postagger-2008-09-28.tar.gz:
models/egw.bnc.200
6http://people.csail.mit.edu/maestro/papers/
bllip-clusters.gz
1284
1 4 16 64 256 1,024 (# of clusters) 49,180
20
40
60
80
%
gold
mfc mfp ua
full
partial
none
flat
gold
mfc
mfp
ua
full
partial
none
flat
k = 1 2 3 4 5 6 7 8 9 10 11 12 ? 18 bits
Figure 2: Parsing performance (accuracy on WSJ15) as a ?function? of the number of syntactic categories, for all prefix
lengths ? k ? {1, . . . , 18} ? of a hierarchical (Brown et al, 1992) clustering, connected by solid lines (dependency
grammar induction in blue; supervised oracle skylines in red, above). Tagless lexicalized models (full, partial and
none) connected by dashed lines. Models based on gold part-of-speech tags, and derived monosemous clusters (mfc,
mfp and ua), shown as vertices of gold polygons. Models based on a flat (Clark, 2000) clustering indicated by squares.
tag a word differently in context) may be the primary
advantage of manually constructed categorizations.
4.2 Experiment #4: A Hierarchical Clustering
The purpose of this batch of experiments is to show
that Clark?s (2000) algorithm isn?t unique in its suit-
ability for grammar induction. We found that Brown
et al?s (1992) older information-theoretic approach,
which does not explicitly address the problems of
rare and ambiguous words (Clark, 2000) and was de-
signed to induce large numbers of plausible syntac-
tic and semantic clusters, can perform just as well.
Once again, the sufficient amount of data (43 mil-
lion words) was tuned in earlier work (Koo, 2010).
His task of interest was, in fact, dependency parsing.
But since this algorithm is hierarchical (i.e., there
isn?t a parameter for the number of categories), we
doubt that there was a strong enough risk of overfit-
ting to question the clustering?s unsupervised nature.
As there isn?t a set number of categories, we used
binary prefixes of length k from each word?s address
in the computed hierarchy as cluster labels. Results
for 7 ? k ? 9 bits (approximately 100?250 non-
empty clusters, close to the 200 we used before) are
similar to those of flat clusters (see Table 1: hierar-
chical). Outside of this range, however, performance
can be substantially worse (see Figure 2), consistent
with earlier findings: Headden et al (2008) demon-
strated that (constituent) grammar induction, using
the singular-value decomposition (SVD-based) tag-
ger of Schu?tze (1995), also works best with 100?200
clusters. Important future research directions may
include learning to automatically select a good num-
ber of word categories (in the case of flat clusterings)
and ways of using multiple clustering assignments,
perhaps of different granularities/resolutions, in tan-
dem (e.g., in the case of a hierarchical clustering).
4.3 Further Evaluation
It is important to enable easy comparison with pre-
vious and future work. Since WSJ15 is not a stan-
dard test set, we evaluated two key experiments ?
?less is more? with gold part-of-speech tags (#1, Ta-
ble 1: gold) and with Clark?s (2000) clusters (#3, Ta-
ble 1: flat) ? on all sentences (not just length fifteen
and shorter), in Section 23 of WSJ (see Table 4).
This required smoothing both final models (?2.4).
We showed that two classic unsupervised word
1285
System Description Accuracy
#1 (?3.1) ?less is more? (Spitkovsky et al, 2009) 44.0
#3 (?4.1) ?less is more? with monosemous induced tags 41.4 (-2.6)
Table 4: Directed accuracies on Section 23 of WSJ (all sentences) for two experiments with the base system.
clusterings ? one flat and one hierarchical ? can
be better for dependency grammar induction than
monosemous syntactic categories derived from gold
part-of-speech tags. And we confirmed that the un-
supervised tags are worse than the actual gold tags,
in a simple dependency grammar induction system.
5 State-of-the-Art without Gold Tags
Until now, we have deliberately kept our experimen-
tal methods simple and nearly identical to Klein and
Manning?s (2004), for clarity. Next, we will explore
how our main findings generalize beyond this toy
setting. A preliminary test will simply quantify the
effect of replacing gold part-of-speech tags with the
monosemous flat clustering (as in experiment #3,
?4.1) on a modern grammar inducer. And our last
experiment will gauge the impact of using a polyse-
mous (but still unsupervised) clustering instead, ob-
tained by executing standard sequence labeling tech-
niques to introduce context-sensitivity into the origi-
nal (independent) assignment or words to categories.
These final experiments are with our latest state-
of-the-art system (Spitkovsky et al, 2011) ? a par-
tially lexicalized extension of the DMV that uses
constrained Viterbi EM to train on nearly all of the
data available in WSJ, at WSJ45 (48,418 sentences;
986,830 non-punctuation tokens). The key contribu-
tion that differentiates this model from its predeces-
sors is that it incorporates punctuation into grammar
induction (by turning it into parsing constraints, in-
stead of ignoring punctuation marks altogether). In
training, the model makes a simplifying assumption
? that sentences can be split at punctuation and that
the resulting fragments of text could be parsed inde-
pendently of one another (these parsed fragments are
then reassembled into full sentence trees, by pars-
ing the sequence of their own head words). Fur-
thermore, the model continues to take punctuation
marks into account in inference (using weaker, more
accurate constraints, than in training). This system
scores 58.4% on Section 23 of WSJ? (see Table 5).
5.1 Experiment #5: A Monosemous Clustering
As in experiment #3 (?4.1), we modified the base
system in exactly one way: we swapped out gold
part-of-speech tags and replaced them with a flat dis-
tributional similarity clustering. In contrast to sim-
pler models, which suffer multi-point drops in ac-
curacy from switching to unsupervised tags (e.g.,
2.6%), our new system?s performance degrades only
slightly, by 0.2% (see Tables 4 and 5). This result
improves over substantial performance degradations
previously observed for unsupervised dependency
parsing with induced word categories (Klein and
Manning, 2004; Headden et al, 2008, inter alia).7
One risk that arises from using gold tags is that
newer systems could be finding cleverer ways to ex-
ploit manual labels (i.e., developing an over-reliance
on gold tags) instead of actually learning to acquire
language. Part-of-speech tags are known to contain
significant amounts of information for unlabeled de-
pendency parsing (McDonald et al, 2011, ?3.1), so
we find it reassuring that our latest grammar inducer
is less dependent on gold tags than its predecessors.
5.2 Experiment #6: A Polysemous Clustering
Results of experiments #1 and 3 (?3.1, 4.1) suggest
that grammar induction stands to gain from relaxing
the one class per word assumption. We next test this
conjecture by inducing a polysemous unsupervised
word clustering, then using it to induce a grammar.
Previous work (Headden et al, 2008, ?4) found
that simple bitag hidden Markov models, classically
trained using the Baum-Welch (Baum, 1972) variant
of EM (HMM-EM), perform quite well,8 on aver-
age, across different grammar induction tasks. Such
sequence models incorporate a sensitivity to context
via state transition probabilities PTRAN(ti | ti?1), cap-
turing the likelihood that a tag ti immediately fol-
lows the tag ti?1; emission probabilities PEMIT(wi | ti)
capture the likelihood that a word of type ti is wi.
7We also briefly comment on this result in the ?punctuation?
paper (Spitkovsky et al, 2011, ?7), published concurrently.
8They are also competitive with Bayesian estimators, on
larger data sets, with cross-validation (Gao and Johnson, 2008).
1286
System Description Accuracy
(?5) ?punctuation? (Spitkovsky et al, 2011) 58.4
#5 (?5.1) ?punctuation? with monosemous induced tags 58.2 (-0.2)
#6 (?5.2) ?punctuation? with context-sensitive induced tags 59.1 (+0.7)
Table 5: Directed accuracies on Section 23 of WSJ (all sentences) for experiments with the state-of-the-art system.
We need a context-sensitive tagger, and HMM
models are good ? relative to other tag-inducers.
However, they are not better than gold tags, at least
when trained using a modest amount of data.9 For
this reason, we decided to relax the monosemous
flat clustering, plugging it in as an initializer for the
HMM. The main problem with this approach is that,
at least without smoothing, every monosemous la-
beling is trivially at a local optimum, since P(ti | wi)
is deterministic. To escape the initial assignment,
we used a ?noise injection? technique (Selman et
al., 1994), inspired by the contexts of Clark (2000).
First, we collected the MLE statistics for PR(ti+1 | ti)
and PL(ti | ti+1) in WSJ, using the flat monosemous
tags. Next, we replicated the text of WSJ 100-fold.
Finally, we retagged this larger data set, as follows:
with probability 80%, a word kept its monosemous
tag; with probability 10%, we sampled a new tag
from the left context (PL) associated with the origi-
nal (monosemous) tag of its rightmost neighbor; and
with probability 10%, we drew a tag from the right
context (PR) of its leftmost neighbor.10 Given that
our initializer ? and later the input to the grammar
inducer ? are hard assignments of tags to words, we
opted for (the faster and simpler) Viterbi training.
In the spirit of reproducibility, we again used an
off-the-shelf component for tagging-related work.11
Viterbi training converged after just 17 steps, re-
placing the original monosemous tags for 22,280 (of
1,028,348 non-punctuation) tokens in WSJ. For ex-
9All of Headden et al?s (2008) grammar induction experi-
ments with induced parts-of-speech were worse than their best
results using gold part-of-speech tags, most likely because they
used a very small corpus (half of WSJ10) to cluster words.
10We chose the sampling split (80:10:10) and replication pa-
rameter (100) somewhat arbitrarily, so better results could likely
be obtained with tuning. However, we suspect that the real gains
would come from using soft clustering techniques (Hinton and
Roweis, 2003; Pereira et al, 1993, inter alia) and propagating
(joint) estimates of tag distributions into a parser. Our ad-hoc
approach is intended to serve solely as a proof of concept.
11David Elworthy?s C+ tagger, with options -i t -G -l,
available from http://friendly-moose.appspot.com/
code/NewCpTag.zip.
ample, the first changed sentence is #3 (of 49,208):
Some ?circuit breakers? installed after
the October 1987 crash failed their first
test, traders say, unable to cool the selling
panic in both stocks and futures.
Above, the word cool gets relabeled as #188 (from
#173 ? see Table 3), since its context is more
suggestive of an infinitive verb than of its usual
grouping with adjectives. (A proper analysis of all
changes, however, is beyond the scope of this work.)
Using this new context-sensitive hard assignment
of tokens to unsupervised categories our gram-
mar inducer attained a directed accuracy of 59.1%,
nearly a full point better than with the monosemous
hard assignment (see Table 5). To the best of our
knowledge it is also the first state-of-the-art unsuper-
vised dependency parser to perform better with in-
duced categories than with gold part-of-speech tags.
6 Related Work
Early work in dependency grammar induction al-
ready relied on gold part-of-speech tags (Carroll and
Charniak, 1992). Some later models (Yuret, 1998;
Paskin, 2001, inter alia) attempted full lexicaliza-
tion. However, Klein and Manning (2004) demon-
strated that effort to be worse at recovering depen-
dency arcs than choosing parse structures at random,
leading them to incorporate gold tags into the DMV.
Klein and Manning (2004, ?5, Figure 6) had also
tested their own models with induced word classes,
constructed using a distributional similarity cluster-
ing method (Schu?tze, 1995). Without gold part-of-
speech tags, their combined DMV+CCM model was
about five points worse, both in (directed) unlabeled
dependency accuracy (42.3% vs. 47.5%)12 and unla-
beled bracketing F1 (72.9% vs. 77.6%), on WSJ10.
In constituent parsing, earlier Seginer (2007a, ?6,
Table 1) built a fully-lexicalized grammar inducer
12On the same evaluation set (WSJ10), our context-sensitive
system without gold tags (Experiment #6, ?5.2) scores 66.8%.
1287
that was competitive with DMV+CCM despite not
using gold tags. His CCL parser has since been
improved via a ?zoomed learning? technique (Re-
ichart and Rappoport, 2010). Moreover, Abend et
al. (2010) reused CCL?s internal distributional rep-
resentation of words in a cognitively-motivated part-
of-speech inducer. Unfortunately their tagger did
not make it into Christodoulopoulos et al?s (2010)
excellent and otherwise comprehensive evaluation.
Outside monolingual grammar induction, fully-
lexicalized statistical dependency transduction mod-
els have been trained from unannotated parallel bi-
texts for machine translation (Alshawi et al, 2000).
More recently, McDonald et al (2011) demonstrated
an impressive alternative to grammar induction by
projecting reference parse trees from languages that
have annotations to ones that are resource-poor.13 It
uses graph-based label propagation over a bilingual
similarity graph for a sentence-aligned parallel cor-
pus (Das and Petrov, 2011), inducing part-of-speech
tags from a universal tag-set (Petrov et al, 2011).
Even in supervised parsing we are starting to see
a shift away from using gold tags. For example,
Alshawi et al (2011) demonstrated good results for
mapping text to underspecified semantics via depen-
dencies without resorting to gold tags. And Petrov et
al. (2010, ?4.4, Table 4) observed only a small per-
formance loss ?going POS-less? in question parsing.
We are not aware of any systems that induce both
syntactic trees and their part-of-speech categories.
However, aside from the many systems that induce
trees from gold tags, there are also unsupervised
methods for inducing syntactic categories from gold
trees (Finkel et al, 2007; Pereira et al, 1993), as
well as for inducing dependencies from gold con-
stituent annotations (Sangati and Zuidema, 2009;
Chiang and Bikel, 2002). Considering that Headden
et al?s (2008) study of part-of-speech taggers found
no correlation between standard tagging metrics and
the quality of induced grammars, it may be time for
a unified treatment of these very related syntax tasks.
13When the target language is English, however, their best ac-
curacy (projected from Greek) is low: 45.7% (McDonald et al,
2011, ?4, Table 2); tested on the same CoNLL 2007 evaluation
set (Nivre et al, 2007), our ?punctuation? system with context-
sensitive induced tags (trained on WSJ45, without gold tags)
performs substantially better, scoring 51.6%. Note that this is
also an improvement over our system trained on the CoNLL set
using gold tags: 50.3% (Spitkovsky et al, 2011, ?8, Table 6).
7 Discussion and Conclusions
Unsupervised word clustering techniques of Brown
et al (1992) and Clark (2000) are well-suited to de-
pendency parsing with the DMV. Both methods out-
perform gold parts-of-speech in supervised modes.
And both can do better than monosemous clusters
derived from gold tags in unsupervised training. We
showed how Clark?s (2000) flat tags can be relaxed,
using context, with the resulting polysemous cluster-
ing outperforming gold part-of-speech tags for the
English dependency grammar induction task.
Monolingual evaluation is a significant flaw in our
methodology, however. One (of many) take-home
points made in Christodoulopoulos et al?s (2010)
study is that results on one language do not neces-
sarily correlate with other languages.14 Assuming
that our results do generalize, it will still remain to
remove the present reliance on gold tokenization and
sentence boundary labels. Nevertheless, we feel that
eliminating gold tags is an important step towards
the goal of fully-unsupervised dependency parsing.
We have cast the utility of a categorization scheme
as a combination of two effects on parsing accuracy:
a synonymy effect and a polysemy effect. Results
of our experiments with both full and partial lexi-
calization suggest that grouping similar words (i.e.,
synonymy) is vital to grammar induction with the
DMV. This is consistent with an established view-
point, that simple tabulation of frequencies of words
participating in certain configurations cannot be reli-
ably used for comparing their likelihoods (Pereira et
al., 1993, ?4.2): ?The statistics of natural languages
is inherently ill defined. Because of Zipf?s law, there
is never enough data for a reasonable estimation of
joint object distributions.? Seginer?s (2007b, ?1.4.4)
argument, however, is that the Zipfian distribution
? a property of words, not parts-of-speech ?
should allow frequent words to successfully guide
14Furthermore, it would be interesting to know how sensitive
different head-percolation schemes (Yamada and Matsumoto,
2003; Johansson and Nugues, 2007) would be to gold versus
unsupervised tags, since the Magerman-Collins rules (Mager-
man, 1995; Collins, 1999) agree with gold dependency annota-
tions only 85% of the time, even for WSJ (Sangati and Zuidema,
2009). Proper intrinsic evaluation of dependency grammar in-
ducers is not yet a solved problem (Schwartz et al, 2011).
1288
parsing and learning: ?A relatively small number of
frequent words appears almost everywhere and most
words are never too far from such a frequent word
(this is also the principle behind successful part-of-
speech induction).? We believe that it is important to
thoroughly understand how to reconcile these only
seemingly conflicting insights, balancing them both
in theory and in practice. A useful starting point may
be to incorporate frequency information in the pars-
ing models directly ? in particular, capturing the
relationships between words of various frequencies.
The polysemy effect appears smaller but is less
controversial: Our experiments suggest that the pri-
mary drawback of the classic clustering schemes
stems from their one class per word nature ? and
not a lack of supervision, as may be widely believed.
Monosemous groupings, even if they are themselves
derived from human-annotated syntactic categories,
simply cannot disambiguate words the way gold tags
can. By relaxing Clark?s (2000) flat clustering, us-
ing contextual cues, we improved dependency gram-
mar induction: directed accuracy on Section 23 (all
sentences) of the WSJ benchmark increased from
58.2% to 59.1% ? from slightly worse to better than
with gold tags (58.4%, previous state-of-the-art).
Since Clark?s (2000) word clustering algorithm is
already context-sensitive in training, we suspect that
one could do better simply by preserving the polyse-
mous nature of its internal representation. Importing
the relevant distributions into a sequence tagger di-
rectly would make more sense than going through an
intermediate monosemous summary. And exploring
other uses of soft clustering algorithms ? perhaps as
inputs to part-of-speech disambiguators ? may be
another fruitful research direction. We believe that
a joint treatment of grammar and parts-of-speech in-
duction could fuel major advances in both tasks.
Acknowledgments
Partially funded by the Air Force Research Laboratory (AFRL),
under prime contract no. FA8750-09-C-0181, and by NSF, via
award #IIS-0811974. We thank Omri Abend, Spence Green,
David McClosky and the anonymous reviewers for many help-
ful comments on draft versions of this paper.
References
O. Abend, R. Reichart, and A. Rappoport. 2010. Im-
proved unsupervised POS induction through prototype
discovery. In ACL.
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learn-
ing dependency translation models as collections of
finite-state head transducers. Computational Linguis-
tics, 26.
H. Alshawi, P.-C. Chang, and M. Ringgaard. 2011. De-
terministic statistical mapping of sentences to under-
specied semantics. In IWCS.
H. Alshawi. 1996. Head automata for speech translation.
In ICSLP.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. In Speech Communication Papers for the 97th
Meeting of the Acoustical Society of America.
M. Banko and R. C. Moore. 2004. Part of speech tagging
in context. In COLING.
L. E. Baum. 1972. An inequality and associated maxi-
mization technique in statistical estimation for proba-
bilistic functions of Markov processes. In Inequalities.
R. Bod. 2006. An all-subtrees approach to unsupervised
parsing. In COLING-ACL.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-based n-gram models
of natural language. Computational Linguistics, 18.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. Technical report, Brown University.
D. Chiang and D. M. Bikel. 2002. Recovering latent
information in treebanks. In COLING.
C. Christodoulopoulos, S. Goldwater, and M. Steedman.
2010. Two decades of unsupervised POS induction:
How far have we come? In EMNLP.
A. Clark. 2000. Inducing syntactic categories by context
distribution clustering. In CoNLL-LLL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
B. Cramer. 2007. Limitations of current grammar induc-
tion algorithms. In ACL: Student Research.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In ACL.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL-HLT.
J. R. Finkel, T. Grenager, and C. D. Manning. 2007. The
infinite tree. In ACL.
J. Gao and M. Johnson. 2008. A comparison of Bayesian
estimators for unsupervised Hidden Markov Model
POS taggers. In EMNLP.
1289
W. P. Headden, III, D. McClosky, and E. Charniak.
2008. Evaluating unsupervised part-of-speech tagging
for grammar induction. In COLING.
W. P. Headden, III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In NAACL-HLT.
G. Hinton and S. Roweis. 2003. Stochastic neighbor
embedding. In NIPS.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
D. Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
T. Koo. 2010. Advances in Discriminative Dependency
Parsing. Ph.D. thesis, MIT.
J. Kupiec. 1992. Robust part-of-speech tagging using
a hidden Markov model. Computer Speech and Lan-
guage, 6.
D. M. Magerman. 1995. Statistical decision-tree models
for parsing. In ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-
source transfer of delexicalized dependency parsers.
In EMNLP.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In EMNLP-
CoNLL.
M. A. Paskin. 2001. Grammatical bigrams. In NIPS.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In ACL.
S. Petrov, P.-C. Chang, M. Ringgaard, and H. Alshawi.
2010. Uptraining for accurate deterministic question
parsing. In EMNLP.
S. Petrov, D. Das, and R. McDonald. 2011. A universal
part-of-speech tagset. In ArXiv.
R. Reichart and A. Rappoport. 2010. Improved fully un-
supervised parsing with zoomed learning. In EMNLP.
F. Sangati and W. Zuidema. 2009. Unsupervised meth-
ods for head assignments. In EACL.
H. Schu?tze. 1995. Distributional part-of-speech tagging.
In EACL.
R. Schwartz, O. Abend, R. Reichart, and A. Rappoport.
2011. Neutralizing linguistically problematic annota-
tions in unsupervised dependency parsing evaluation.
In ACL.
Y. Seginer. 2007a. Fast unsupervised incremental pars-
ing. In ACL.
Y. Seginer. 2007b. Learning Syntactic Structure. Ph.D.
thesis, University of Amsterdam.
B. Selman, H. A. Kautz, and B. Cohen. 1994. Noise
strategies for improving local search. In AAAI.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009.
Baby Steps: How ?Less is More? in unsupervised de-
pendency parsing. In NIPS: Grammar Induction, Rep-
resentation of Language and Language Learning.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a.
From Baby Steps to Leapfrog: How ?Less is More? in
unsupervised dependency parsing. In NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010b. Viterbi training improves unsupervised
dependency parsing. In CoNLL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011.
Punctuation: Making a point in unsupervised depen-
dency parsing. In CoNLL.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT.
D. Yuret. 1998. Discovery of Linguistic Relations Using
Lexical Attraction. Ph.D. thesis, MIT.
1290
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 489?500, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Entity and Event Coreference Resolution across Documents
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, Dan Jurafsky
Stanford University, Stanford, CA 94305
{heeyoung,recasens,angelx,mihais,jurafsky}@stanford.edu
Abstract
We introduce a novel coreference resolution
system that models entities and events jointly.
Our iterative method cautiously constructs
clusters of entity and event mentions using lin-
ear regression to model cluster merge opera-
tions. As clusters are built, information flows
between entity and event clusters through fea-
tures that model semantic role dependencies.
Our system handles nominal and verbal events
as well as entities, and our joint formulation
allows information from event coreference to
help entity coreference, and vice versa. In a
cross-document domain with comparable doc-
uments, joint coreference resolution performs
significantly better (over 3 CoNLL F1 points)
than two strong baselines that resolve entities
and events separately.
1 Introduction
Most coreference resolution systems focus on enti-
ties and tacitly assume a correspondence between
entities and noun phrases (NPs). Focusing on NPs
is a way to restrict the challenging problem of coref-
erence resolution, but misses coreference relations
like the one between hanged and his suicide in (1),
and between placed and put in (2).
1. (a) One of the key suspected Mafia bosses ar-
rested yesterday has hanged himself.
(b) Police said Lo Presti had hanged himself.
(c) His suicide appeared to be related to clan feuds.
2. (a) The New Orleans Saints placed Reggie Bush
on the injured list on Wednesday.
(b) Saints put Bush on I.R.
As (1c) shows, NPs can also refer to events, and
so corefer with phrases other than NPs (Webber,
1988). By being anchored in spatio-temporal dimen-
sions, events represent the most frequent referent of
verbal elements. In addition to time and location,
events are characterized by their participants or ar-
guments, which often correspond with discourse en-
tities. This two-way feedback between events and
their arguments (or entities) is the core of our ap-
proach. Since arguments play a key role in describ-
ing an event, knowing that two arguments corefer
is useful for finding coreference relations between
events, and knowing that two events corefer is use-
ful for finding coreference relations between enti-
ties. In (1), the coreference relation between One
of the key suspected Mafia bosses arrested yesterday
and Lo Presti can be found by knowing that their
predicates (i.e., has hanged and had hanged) core-
fer. On the other hand, the coreference relations be-
tween the arguments Saints and Bush in (2) helps
to determine the coreference relation between their
predicates placed and put.
In this paper, we take a holistic approach to coref-
erence. We annotate a corpus with cross-document
coreference relations for nominal and verbal men-
tions. We focus on both intra and inter-document
coreference because this scenario is at the same time
more challenging and more relevant to real-world
applications such as news aggregation. We use this
corpus to train a model that jointly addresses refer-
ences to both entities and events across documents.
The contributions of this work are the following:
? We introduce a novel approach for entity and
event coreference resolution. At the core of
489
our approach is an iterative algorithm that cau-
tiously constructs clusters of entity and event
mentions using linear regression to model clus-
ter merge operations. Importantly, our model
allows information to flow between clusters of
both types through features that model context
using semantic role dependencies.
? We annotate and release a new corpus with
coreference relations between both entities and
events across documents. The relations anno-
tated are both intra and inter-document, which
more accurately models real-world scenarios.
? We evaluate our cross-document coreference
resolution system on this corpus and show that
our joint approach significantly outperforms
two strong baselines that resolve entities and
events separately.
2 Related Work
Entity coreference resolution is a well studied prob-
lem with many successful techniques for identify-
ing mention clusters (Ponzetto and Strube, 2006;
Haghighi and Klein, 2009; Stoyanov et al2009;
Haghighi and Klein, 2010; Raghunathan et al2010;
Rahman and Ng, 2011, inter alia). Most of these
techniques focus on matching compatible noun pairs
using various syntactic and semantic features, with
efforts targeted toward improving features and clus-
tering models.
Prior work showed that models that jointly resolve
mentions across multiple entities result in better per-
formance than simply resolving mentions in a pair-
wise fashion (Denis and Baldridge, 2007; Poon and
Domingos, 2008; Wick et al2008; Lee et al2011,
inter alia). A natural extension is to perform coref-
erence jointly across both entities and events. Yet
there has been little attempt in this direction.
We know of only limited work that incorporates
event-related information in entity coreference, typ-
ically by incorporating the verbs in context as fea-
tures. For instance, Haghighi and Klein (2010) in-
clude the governor of the head of nominal mentions
as features in their model. Rahman and Ng (2011)
also used event-related information by looking at
which semantic role the entity mentions can have
and the verb pairs of their predicates. We confirm
that such features are useful but also show that the
complementary features for verbal mentions lead to
even better performance, especially when event and
entity clusters are jointly modeled.
Compared to the extensive work on entity coref-
erence, the related problem of event coreference re-
mains relatively under-explored, with minimal work
on how entity and event coreference can be con-
sidered jointly on an open domain. Early work on
event coreference for MUC (Humphreys et al1997;
Bagga and Baldwin, 1999) focused on scenario-
specific events. More recently, there have been
approaches that looked at event coreference for
wider domains. Chen and Ji (2009) proposed us-
ing spectral graph clustering to cluster events. Be-
jan and Harabagiu (2010) proposed a nonparamet-
ric Bayesian model for open-domain event resolu-
tion. However, most of this prior work focused only
on event coreference, whereas we address both en-
tities and events with a single model. Humphreys
et al1997) considered entities as well as events,
but due to the lack of a corpus annotated with event
coreference, their approach was only evaluated im-
plicitly in the MUC-6 template filling task. To our
knowledge, the only previous work that considered
entity and event coreference resolution jointly is
He (2007), but limited to the medical domain and
focused on just five semantic categories.
3 Architecture
Following the intuition introduced in Section 1, our
approach iteratively builds clusters of event and en-
tity mentions jointly. As more information becomes
available (e.g., finding out that two verbal mentions
have arguments that belong to the same entity clus-
ter), the features of both entity and event mentions
are re-generated, which prompts future clustering
operations. Our model follows a cautious (or ?baby
steps?) approach, which we previously showed to be
successful for entity coreference resolution (Raghu-
nathan et al2010; Lee et al2011). However,
unlike our previous work, which used deterministic
rules, in this paper we learn a coreference resolution
model using linear regression. Algorithm 1 summa-
rizes the flow of the proposed algorithm. We detail
its steps next. We describe the training procedure in
Section 4 and the features used in Section 5.
490
Algorithm 1: Joint Coreference Resolution
input : set of documents D
input : coreference model ?
// clusters of mentions:
E= {}1
// clusters of documents:
C = clusterDocuments(D)2
foreach document cluster c in C do3
// all mentions in one doc cluster:
M = extractMentions(c)4
// singleton mention clusters:
E ? = buildSingletonClusters(M)5
// high-precision deterministic sieves:
E ? = applyHighPrecisionSieves(E ?)6
// iterative event/entity coreference:
while ? e1, e2 ? E ?s.t. score(e1, e2,?) > 0.5 do7
(e1, e2) = arg max e1,e2?E? score(e1, e2,?)8
E ? = merge(e1, e2, E ?)9
// pronoun sieve:
E ? = applyPronounSieve(E ?)10
// append to global output:
E = E + E ?11
output : E
3.1 Document Clustering
Our approach starts with several steps that reduce
the search space for the actual coreference resolution
task. The first is document clustering, which clusters
the set of input documents (D) into a set of docu-
ment clusters (C). In the subsequent steps we only
cluster mentions that appear in the same document
cluster. We found this to be very useful in practice
because, in addition to reducing the search space, it
provides a word sense disambiguation mechanism
based on corpus-wide topics. For example, with-
out document clustering, our algorithm may decide
to cluster two mentions of the verb hit, but know-
ing that one belongs to a cluster containing earth-
quake reports and the other to a cluster with reports
on criminal activities, this decision can be avoided.1
Any non-parametric clustering algorithm can be
used in this step. In this paper, we used the algo-
rithm proposed by Surdeanu et al2005). This algo-
rithm is an Expectation Maximization (EM) variant
where the initial points (and the number of clusters)
are selected from the clusters generated by a hierar-
chical agglomerative clustering algorithm using ge-
1Since different mentions of the verb say in the same topic
might refer to different events, they are only merged if they have
coreferent arguments.
ometric heuristics. This algorithm performs well on
our data. For example, in the training dataset, only
two topics (handling different earthquake events) are
incorrectly merged into the same cluster.
3.2 Mention Extraction
In this step (4 in Algorithm 1) we extract nominal,
pronominal, and verbal mentions. We extract nom-
inal and pronominal mentions using the mention
identification component in the publicly download-
able Stanford coreference resolution system (Raghu-
nathan et al2010; Lee et al2011). We consider
as verbal mentions all words whose part of speech
starts with VB, with the exception of some auxil-
iary/copulative verbs (have, be and seem). For each
of the identified mentions we build a singleton clus-
ter (step 5 in Algorithm 1).
Crucially, we do not make a formal distinction be-
tween entity and event mentions. This distinction is
not trivial to implement (e.g., is the noun earthquake
an entity or an event mention?) and an imperfect
classification would negatively affect the following
coreference resolution. Instead, we simply classify
mentions into verbal or nominal, and use this dis-
tinction later during feature generation (Section 5).
To compare event nouns (e.g., development) with
verbal mentions, the ?derivationally related form?
relation in WordNet is used.
3.3 High-precision Entity Resolution Sieves
To further reduce the problem?s search space, in
step 6 of Algorithm 1 we apply a set of high-
precision filters from the Stanford coreference res-
olution system. This system is a collection of deter-
ministic models (or ?sieves?) for entity coreference
resolution that incorporate lexical, syntactic, seman-
tic, and discourse information. These sieves are ap-
plied from higher to lower precision. As clusters are
built, information such as mention gender and num-
ber is propagated across mentions in the same clus-
ter, which helps subsequent decisions. The Stanford
system obtained the highest score at the CoNLL-
2011 shared task on English coreference resolution.
For this step, we selected all the sieves from the
Stanford system with the exception of the pronoun
resolution sieve. All the remaining sieves (listed
in Table 1) have high precision because they em-
ploy linguistic heuristics with little ambiguity, e.g.,
491
High-precision sieves
Discourse processing sieve
Exact string match sieve
Relaxed string match sieve
Precise constructs sieve (e.g., appositives)
Strict head match sieves
Proper head noun match sieve
Relaxed head matching sieve
Table 1: Deterministic sieves in step 6 of Algorithm 1.
one sieve clusters together two entity mentions only
when they have the same head word. Note that all
these heuristics were designed for within-document
coreference. They work well in our context be-
cause we apply them in individual document clus-
ters, where the one-sense-per-discourse principle
still holds (Yarowsky, 1995).
Importantly, these sieves do not address verbal
mentions. That is, all verbal mentions are still in sin-
gleton clusters after this step. Furthermore, none of
these sieves use features that facilitate the joint reso-
lution of nominal and verbal mentions (e.g., features
from semantic role frames). All these limitations are
addressed next.
3.4 Iterative Entity/Event Resolution
In this stage (steps 7 ? 9 in Algorithm 1), we con-
struct entity and event clusters using a cautious or
?baby steps? approach. We use a single linear re-
gressor (?) to model cluster merge operations be-
tween both verbal and nominal clusters. Intuitively,
the linear regressor models the quality of the merge
operation, i.e., a score larger than 0.5 indicates that
more than half of the mention pairs introduced by
this merge are correct. We discuss the training pro-
cedure that yields this scoring function in Section 4.
In each iteration, we perform the merge operation
that has the highest score. Once two clusters are
merged (step 9) we regenerate all the mention fea-
tures to reflect the current clusters. We stop when no
merging operation with an overall benefit is found.
This iterative procedure is the core of our joint
coreference resolution approach. This algorithm
transparently merges both entity and event men-
tions and, importantly, allows information to flow
between clusters of both types as merge operations
take place. For example, assume that during iter-
ation i we merge the two hanged verbs in the first
example in Section 1 (because they have the same
lemma). Because of this merge, in iteration i+ 1 the
nominal mentions Lo Presti and One of the key sus-
pected Mafia bosses have the same semantic role for
verbs assigned to the same cluster. This is a strong
hint that these two nominal mentions belong to the
same cluster. Indeed, the feature that models this
structure received one of the highest weights in our
linear regression model (see Section 7).
3.5 Pronoun Sieve
Our approach concludes with the pronominal coref-
erence resolution sieve from the Stanford system.
This sieve is necessary because our current reso-
lution algorithm ignores mention ordering and dis-
tance (i.e., in step 7 we compare all clusters regard-
less of where their mentions appear in the text). As
previous work has proved, the structure of the text is
crucial for pronominal coreference (Hobbs, 1978).
For this reason, we handle pronouns outside of the
main algorithm block.
4 Training the Cluster Merging Model
Two observations drove our choice of model and
training algorithm. First, modeling the merge op-
eration as a classification task is not ideal, because
only a few of the resulting clusters are entirely cor-
rect or incorrect. In practice, most of the clusters
will contain some mention pairs that are correct and
some that are not. Second, generating training data
for the merging model is not trivial: a brute force
approach that looks at all the possible combinations
is exponential in the number of mentions. This is
both impractical and unnecessary, as some of these
combinations are unlikely to be seen in practice.
We address these observations with Algorithm 2.
The algorithm uses gold coreference labels to train a
linear regressor that models the quality of the clus-
ters produced by merge operations. We define the
quality score q of a new cluster as the percentage of
new mention pairs (i.e., not present in either one of
the clusters to be merged) that are correct:
q =
linkscorrect
linkscorrect + linksincorrect
(1)
where links(in)correct is the number of newly intro-
duced (in)correct pairwise mention links when two
clusters are merged.
492
Algorithm 2: Training Procedure
input : set of documents D
input : correct mention clusters G
C = clusterDocuments(D)1
// linear regression coreference model:
? = assignInitialWeights(C,G)2
// repeat for T epochs:
for t = 1 to T do3
// training data for linear regressor:
? = {}4
foreach document cluster c in C do5
M = extractMentions(c)6
E = buildSingletonClusters(M)7
E = applyHighPrecisionSieves(E)8
// gather training examples
// as clusters are built:
while ? e1, e2 ? Es.t. sco(e1, e2,?) > 0.5 do9
forall e?1, e
?
2 ? E do10
q = qualityOfMerge(e?1, e
?
2,G)11
? = append(e?1, e
?
2, q,?)12
(e1, e2) = arg max e1,e2?E sco(e1, e2,?)13
E = merge(e1, e2, E)14
// train using data from last epoch:
?? = trainLinearRegressor(?)15
// interpolate with older model:
? = ?? + (1? ?)??16
output : ?
We address the potential explosion in training data
size by considering only merge operations that are
likely to be inspected by the algorithm as it runs.
To achieve this, Algorithm 2 repeatedly runs the ac-
tual clustering algorithm (as given by the current
model ?) over the training dataset (steps 5 ? 14).2
When the algorithm iteratively constructs its clus-
ters (steps 9 ? 14), we generate training data from
all possible cluster pairs available during a particular
iteration (steps 10 ? 12). For each pair, we compute
its score using Equation 1 (step 11) and add it to the
training corpus ? (step 12). Note that this avoids in-
specting many of the possible cluster combinations:
once a cluster is built (e.g., during the previous iter-
ations or by the deterministic sieves in step 8), we
do not generate training data from its members, but
rather treat it as an atomic unit. On the other hand,
our approach generates more training data than on-
line learning, which trains using only the actual de-
cisions taken during inference in each iteration (i.e.,
2We skip the pronoun sieve here because it does not affect
the decisions taken during the iterative resolution steps.
the pair (e1, e2) in step 13).
After each epoch we have a new training cor-
pus ?, which we use to train the new linear regres-
sion model ?? (step 15), which is then interpolated
with the old one (step 16).
Our training procedure is similar in spirit to trans-
formation based learning (TBL) (Brill, 1995). Sim-
ilarly to TBL, our approach repeatedly applies the
model over the training data and attempts to mini-
mize the error rate of the current model. However,
while TBL learns rules that directly minimize the
current error rate, our approach achieves this indi-
rectly, by incorporating the reduction in error rate in
the score of the generated datums. This allows us
to fit a linear regression to this task, which, as dis-
cussed before, is a better model for this task.
Just like any hill-climbing algorithm, our ap-
proach has the risk of converging to a local max-
imum. To mitigate this risk, we do not initialize
our model ? with random weights, but rather use
hints from the deterministic sieves. This procedure
(listed in step 2) runs the high-precision sieves in-
troduced in Section 3.3 and, just like the data gen-
eration loop in Algorithm 2, creates training exam-
ples from the clusters available after every merge
operation. Since these deterministic models address
only nominal clusters, at the end we generate train-
ing data for events by inspecting all the pairs of sin-
gleton verbal clusters. Using this data, we train the
initial linear regression model.
We trained our model using L2 regularized linear
regression with a regularization coefficient of 1.0.
We did not tune the regularization coefficient. We
ran the training algorithm for 10 epochs, although
we observed minimal changes after three epochs.
We tuned the interpolation weight (?) to a value
of 0.7 using our development corpus.
5 Features
We list in Table 2 the features used by the lin-
ear regression model. As the table indicates, our
feature set relies heavily on semantic roles, which
were extracted using the SwiRL semantic role la-
beling (SRL) system (Surdeanu et al2007).3 Be-
cause SwiRL addresses only verbal predicates, we
extended it to handle nominal predicates. In this
3http://www.surdeanu.name/mihai/swirl/
493
Feature Name
Applies to
Entities (E)
or Events (V)
Description and Example
Entity Heads E
Cosine similarity of the head-word vectors of two clusters. The head-word vector
stores the head words of all mentions in a cluster and their frequencies. For example,
the vector for the three-mention cluster {Barack Obama, President Obama, US
president}, is {Obama:2, president:1}.
Event Lemmas V
Cosine similarity of the lemma vectors of two clusters. For example, the lemma
vector for the cluster {murdered, murders, hitting} is {murder:2, hit:1}.
Links between
Synonyms
E, V
The percentage of newly-introduced mention links after the merge that are WordNet
synonyms (Fellbaum, 1998). For example, when merging the following two clus-
ters, {hit, strike} and {strike, join, say}, two out of the six new links are between
words that belong to the same WordNet synset: (hit ? strike) and (strike ? strike).
Number of Coreferent
Arguments or
Predicates
E, V
The total number of shared arguments and predicates between mentions in the
two clusters. We use the cluster IDs of the corresponding arguments/predicates
to check for identity. For example, when comparing the event clusters {bought}
and {acquired}, extracted from the sentences [AMD]Arg0 bought [ATI]Arg1 and
[AMD]Arg0 acquired [ATI]Arg1, the value of this feature is 2 because the two men-
tions share one Arg0 and one Arg1 argument (assuming that the clusters {AMD,
AMD} and {ATI, ATI} were previously created). For entity clusters, this feature
counts the number of coreferent predicates. In addition to PropBank-style roles, for
event mentions we also include the closest left and right entity mentions in order to
capture any arguments missed by the SRL system.
Coreferent Arguments
in a Specific Role?
E, V
Indicator feature set to 1 if the two clusters have at least one coreferent argument in
a given role. We generate one variant of this feature for each argument label, e.g.,
Arg0, Arg1, etc. For example, the value of this feature for Arg0 for the clusters
{bought} and {acquired} in the above example is 1.
Coreferent Predicate in
a Specific Role?
E
Indicator feature set to 1 if the two clusters have at least one coreferent predicate for
a given role. For example, for the clusters {the man} and {the person}, extracted
from the sentences helped [the man]Arg1 and helped [the person]Arg1, the value of
this feature is 1 if the two helped verbs were previously clustered together.
2nd Order Similarity of
Mention Words
E
Cosine similarity of vectors containing words that are distributionally similar to
words in the cluster mentions. We built these vectors by extracting the top-ten
most-similar words in Dekang Lin?s similarity thesaurus (Lin, 1998) for all the
nouns/adjectives/verbs in a cluster. For example, for the singleton cluster {a new
home}, we construct this vector by expanding new and home to: {new:1, original:1,
old:1, existing:1, current:1, unique:1, modern:1, different:1, special:1, major:1,
small:1, home:1, house:1, apartment:1, building:1, hotel:1, residence:1, office:1,
mansion:1, school:1, restaurant:1, hospital:1 }.
Number; Animacy;
Gender; NE Label
E
Cosine similarity of number, gender, animacy, and NE label vectors. For example,
the number and gender vectors for the two-mention cluster {systems, a pen} are
Number = {singular:1, plural:1}, Gender = {neutral:2}.
Table 2: List of features used when comparing two clusters. If any of the two clusters contains a verbal mention we
consider the merge an operation between event (V) clusters; otherwise it is a merge between entity (E) clusters. We
append to all entity features the suffix Proper or Common based on the type of the head word of the first mention in
each of the two clusters. We use the suffix Proper only if both head words are proper nouns.
paper we used a single heuristic: the possessor of
a nominal event?s predicate is marked as its Arg0,
e.g., Logan is the Arg0 to run in Logan?s run.4
4A principled solution to this problem is to use an SRL sys-
tem for nominal predicates trained using NomBank (Meyers et
al., 2004). We will address this in future work.
494
We extracted named entity labels using the named
entity recognizer from the Stanford CoreNLP suite.
6 Evaluation
6.1 Corpus
The training and test data sets were derived from
the EventCorefBank (ECB) corpus5 created by Be-
jan and Harabagiu (2010) to study event coreference
since standard corpora such as OntoNotes (Pradhan
et al2007) contain a small number of annotated
event clusters. The ECB corpus consists of 482 doc-
uments from Google News clustered into 43 topics,
where a topic is described as a seminal event. The
reason for including comparable documents was to
increase the number of cross-document coreference
relations. Bejan and Harabagiu (2010) only anno-
tated a selection of events.
For the purpose of our study, we extended the
original corpus in two directions: (i) fully anno-
tated sentences, and (ii) entity coreference relations.
In addition, we removed relations other than coref-
erence (e.g., subevent, purpose, related, etc.) that
had been originally annotated. We revised and com-
pleted the original annotation by annotating every
entity and event in the sentences that were (partially)
annotated. The annotation was performed by four
experts, using the Callisto annotation tool.6 The
annotation guidelines and the generated corpus are
available here.7
Our annotation of the ECB corpus followed the
OntoNotes (Pradhan et al2007) standard for coref-
erence annotation, with a few extensions to handle
events. For nouns, we annotated full NPs (with all
modifiers), excluding appositive phrases and nomi-
nal predicates. Only premodifiers that were proper
nouns or possessive phrases were annotated. For
events, we annotated the semantic head of the verb
phrase. We extended the OntoNotes guidelines by
also annotating singletons (but we do not score
them; see below), and by including all events men-
tions (not only those mentioned at least once with an
NP). This required us to be specific with respect to:
5http://faculty.washington.edu/bejan/
data/ECB1.0.tar.gz
6http://callisto.mitre.org
7http://nlp.stanford.edu/pubs/
jcoref-corpus.zip
Training Dev Test Total
# Topics 12 3 28 43
# Documents 112 39 331 482
# Entities 459 46 563 1068
# Entity Mentions 1723 259 3465 5447
# Events 300 30 444 774
# Event Mentions 751 140 1642 2533
Table 3: Corpus statistics.
?ENTITY COREFID=?26?? A publicist ?/ENTITY? ?EVENT
COREFID=?4?? says ?/EVENT? ?ENTITY COREFID=?23??
Tara Reid ?/ENTITY? has ?EVENT COREFID=?3?? checked
?/EVENT? ?ENTITY COREFID=?23?? herself ?/ENTITY? ?EVENT
COREFID=?3*?? into ?/EVENT? ?ENTITY COREFID=?28?? rehab
?/ENTITY?.
Figure 1: Annotation example.
Light verbs Verbs such as give and make followed
by a noun (e.g., make an offer) were not anno-
tated, but the noun was.
Phrasal verbs We annotated the verb together with
the preposition or adverb (e.g., check in).
Idioms They were annotated with all their elements
(e.g., booze it up).
The first topic was annotated by all four anno-
tators as burn-in. Afterwards, annotation disagree-
ments were resolved between all annotators and the
next three topics were annotated again by all four an-
notators to measure agreement. Following Passon-
neau (2004), we computed an inter-annotator agree-
ment of ? = 0.55 (Krippendorff, 2004) on these
three topics, indicating moderate agreement among
the annotators. Given the complexity of the task, we
consider this to be a good score. For example, the
average of the CoNLL F1 between any two annota-
tors is 73.58, which is much higher than the system
scores reported in the literature.
After annotating the four topics, disagreements
were resolved again and all the documents in the
four topics were corrected to match the consensus.
The rest of the corpus was split between the four an-
notators, and each document was annotated by a sin-
gle annotator. Figure 1 shows an example. Table 3
shows the corpus statistics, including the training,
development (dev) and test set splits. The dev topics
were used for tuning the interpolation parameter ?
from Section 4.
495
MUC B3 CEAF-?4 BLANC
System R P F1 R P F1 R P F1 R P F1 CoNLL F1
Baseline 1
Wo/ SRL
Entity 47.4 72.3 57.2 44.1 82.7 57.5 42.5 21.9 28.9 60.1 78.3 64.8 47.9
Event 56.0 56.8 56.4 59.8 71.9 65.3 32.2 31.6 31.9 63.5 68.8 65.7 51.2
Both 49.9 75.4 60.0 44.9 83.9 58.5 46.2 23.3 31.0 60.9 81.2 66.1 49.8
Baseline 2
With SRL
Entity 52.7 73.0 61.2 48.6 80.8 60.7 41.8 24.1 30.6 63.4 78.4 68.2 50.8
Event 59.2 57.0 58.1 62.3 70.8 66.3 31.5 33.2 32.3 65.4 68.0 66.6 52.2
Both 54.5 76.4 63.7 48.7 82.6 61.3 46.3 25.5 32.9 63.9 81.1 69.2 52.6
This paper
Entity 60.7 70.6 65.2 55.5 74.9 63.7 39.3 29.5 33.7 66.9 79.6 71.5 54.2
Event 62.7 62.8 62.7 62.5 73.9 67.7 34.0 33.9 33.9 67.6 78.5 71.7 54.8
Both 61.2 75.9 67.8 53.9 79.0 64.1 45.2 30.0 35.8 67.1 82.2 72.3 55.9
Table 4: Performance of the two baselines and our model. We report scores for entity clusters, event clusters and the
complete task using five metrics.
6.2 Evaluation
We use five coreference evaluation metrics widely
used in the literature:
MUC (Vilain et al1995) Link-based metric which
measures how many predicted and gold clus-
ters need to be merged to cover the gold and
predicted clusters, respectively.
B3 (Bagga and Baldwin, 1998) Mention-based
metric which measures the proportion of over-
lap between predicted and gold clusters for a
given mention.
CEAF (Luo, 2005) Entity-based metric that, unlike
B3, enforces a one-to-one alignment between
gold and predicted clusters. We employ the
entity-based version of CEAF.
BLANC (Recasens and Hovy, 2011) Metric based
on the Rand index (Rand, 1971) that consid-
ers both coreference and non-coreference links
to address the imbalance between singleton and
coreferent mentions.
CoNLL F1 Average of MUC, B3, and CEAF-?4.
This was the official metric in the CoNLL-2011
shared task (Pradhan et al2011).
We followed the CoNLL-2011 evaluation methodol-
ogy, that is, we removed all singleton clusters, and
apposition/copular relations before scoring.
We evaluated the systems on three different set-
tings: only on entity clusters, only on event clus-
ters, and on the complete task, i.e., both entities and
events. Note that the gold corpus separates clusters
into entity and event clusters (see Table 3), but our
system does not make this distinction at runtime.
In order to compute the entity-only and event-only
scores in Table 4, we implemented the following
procedure: (a) when scoring entity clusters, we re-
moved all mentions that were found to be coreferent
with at least one gold event mention and not coref-
erent with any gold entity mentions; and (b) we per-
formed the opposite action when scoring event clus-
ters. This procedure is necessary because our men-
tion identification component is not perfect, i.e., it
generates mentions that do not exist in the gold an-
notation. Furthermore, this procedure is conserva-
tive with respect to the clustering errors of our sys-
tem, e.g., all spurious mentions that our system in-
cludes in a cluster with a gold entity mention are
considered for the entity score, regardless of their
gold type (event or entity).
6.3 Results
Table 4 compares the performance of our system
against two strong baselines that resolve entities and
events separately. Baseline 1 uses a modified Stan-
ford coreference resolution system after our doc-
ument clustering and mention identification steps.
Because the original Stanford system implements
only entity coreference, we extended it with an extra
sieve that implements lemma matching for events.
This additional sieve merges two verbal clusters
(i.e., clusters that contain at least one verbal men-
tion) or a verbal and a nominal cluster when at least
two lemmas of mention head words are the same be-
tween clusters, e.g., helped and the help.
The second baseline adds two more sieves to
Baseline 1. Both these sieves model entity and event
496
contextual information using semantic roles. The
first sieve merges two nominal clusters when two
mentions in the respective clusters have the same
head words and two mentions (possibly with dif-
ferent heads) modify with the same role label two
predicates that have the same lemma. For exam-
ple, this sieve merges the clusters {Obama, the pres-
ident} (seen in the text [Obama]Arg0 attended and
[the president]Arg1 was elected) and {Obama} (seen
in the text [Obama]Arg1 was elected), because they
share a mention with the same head word (Obama)
and two mentions modify with the same role (Arg1)
predicates with the same lemma (elect). The sec-
ond sieve implements the complementary action for
event clusters. That is, it merges two verbal clusters
when at least two mentions have the same lemma
and at least two mentions have semantic arguments
with the same role label and the same lemma.
7 Discussion
The first block in Table 4 indicates that lemma
matching is a strong baseline for event resolution.
Most of the event scores for Baseline 1 are actually
higher than the corresponding entity scores, which
were obtained using the highest ranked system at the
CoNLL-2011 shared task (Lee et al2011). Adding
contextual information using semantic roles (Base-
line 2) helps both entities and events. The CoNLL
F1 for Baseline 2 increases almost 3 points for enti-
ties and 1 point for events. This demonstrates that
local syntactico-semantic context is important for
coreference resolution even in a cross-document set-
ting and that the current state-of-the-art in SRL can
model this context accurately.
The best scores (almost unanimously) are ob-
tained by the model proposed in this paper, which
scores 3.4 CoNLL F1 points higher than Baseline 2
for entities, and 2.6 points higher for events. For the
complete task, our approach scores 3.3 CoNLL F1
points higher than Baseline 2, and 6.1 points higher
than Baseline 1. This demonstrates that a holistic
approach to coreference resolution improves the res-
olution of both entities and events more than models
that address aspects of the task separately. To fur-
ther understand our experiments, we listed the top
five entity/event features with the highest weights in
our model in Table 5. The table indicates that six out
of the ten features serve the purpose of passing infor-
Entity Feature Weight
Entity Heads ? Proper 1.10
Coreferent Predicate for ArgM-LOC ? Common 0.45
Entity Heads ? Common 0.36
Coreferent Predicate for Arg0 ? Proper 0.29
Coreferent Predicate for Arg2 ? Common 0.28
Event Feature Weight
Event Lemmas 0.45
Coreferent Argument for Arg1 0.19
Links between Synonym 0.16
Coreferent Argument for Arg2 0.13
Number of Coreferent Arguments 0.07
Table 5: Top five features with the highest weights.
mation between entity and event clusters. For exam-
ple, the ?Coreferent Argument for Arg1? feature is
triggered when two event clusters have Arg1 argu-
ments that already belong to the same entity cluster.
This allows information from previous entity coref-
erence operations to impact future merges of event
clusters. This is the crux of our iterative approach to
joint coreference resolution.
Finally, we performed an error analysis by man-
ually evaluating 100 errors. We distinguished nine
major types of errors. Their ratios together with a
description and an example are given in Table 6.
This work demonstrates that an approach that
jointly models entities and events is better for cross-
document coreference resolution. However, our
model can be improved. For example, document
clustering and coreference resolution can be solved
jointly, which we expect would improve both tasks.
Furthermore, our iterative coreference resolution
procedure (Algorithm 1) could be modified to ac-
count for mention ordering and distance, which
would allow us to include pronominal resolution in
our joint model, rather than addressing it with a sep-
arate deterministic sieve.
8 Conclusion
We have presented a holistic model for cross-
document coreference resolution that jointly solves
references to events and entities by handling both
nominal and verbal mentions. Our joint resolution
algorithm allows event coreference to help improve
entity coreference, and vice versa. In addition, our
iterative procedure, based on a linear regressor that
models the quality of cluster merges, allows each
497
Error Type (Ratio)
Description
Example
Pronoun resolution
(36%)
The pronoun is incorrectly resolved by the pronominal sieve of the Stanford deterministic entity
system. These errors include (only a small number of) event pronouns.
He said Timmons aimed and missed his target.
Semantics beyond
role frames
(20%)
The semantics of the coreference relation cannot be captured by role frames or WordNet.
Israeli forces on Tuesday killed at least 40 people . . . The Israeli army said the UN school in the
Jabaliya refugee camp was hit . . . and that the dead included a number of Hamas militants.
Arguments of
nominal events
(17%)
The arguments of two nominal events are not detected and thus not coreferred.
The attack on the school has caused widespread shock across Israel . . . while Israeli forces on
Tuesday killed at least 40 people during an attack on a United Nations-run school in Gaza.
Cascaded errors
(7%)
Entities or events are not coreferred due to errors in a previous merge iteration in the same
semantic frame. In the example below, we failed to link the two die verbs, which leads to the
listed entity error.
An Australian climber who survived two nights stuck on Mount Cook after seeing his brother
die . . . Dr Mark Vinar, 43, is presumed dead . . .
Initial high-precision
sieves
(6%)
An error made by the initial high-precision entity resolution sieves is propagated to our model.
Timmons told police he fired when he thought he saw someone in the other group reach for
a gun . . . 15-year-old Timmons was at the scene of the shooting and had a gun.
Phrasal verbs
(6%)
The meaning of a phrasal verb is not captured.
A relative unknown will take over the title role of Doctor Who . . . But the casting of Smith is
a stroke of genius.
Linear regression
(4%)
Recall error made by the regression model when the features are otherwise correct.
The Interior Department on Thursday issued ?revised? regulations . . . Interior Secretary Dirk
Kempthorne announced major changes . . .
Mention detection
(3%)
The mention detection module detects a spurious mention.
Police have arrested a man . . . in the parking lot crosswalk at Sam?s Club in Bloomington.
SRL
(1%)
The SRL system fails to label the semantic role. In this example, jail is detected as the ArgM-
MNR of hanged instead of ArgM-LOC.
A Mafia boss in Palermo hanged himself in jail.
Table 6: Error analysis. Mentions to be resolved are in bold face, correct antecedents are in italics, and our system?s
predictions are underlined.
merging state to benefit from the previous merged
entity and event mentions. This approach allows us
to start with a set of high-precision coreference rela-
tions and gradually add new ones to increase recall.
The experimental evaluation shows that our coref-
erence algorithm gives markedly better F1 for both
entities and events, outperforming two strong base-
lines that handle entities and events separately, mea-
sured by all the standard measures: MUC, B3,
CEAF-?4, BLANC and the official CoNLL-2011
metric. This is noteworthy since each measure has
been shown to place primary emphasis in evaluating
a different aspect of the coreference resolution task.
Our system is tailored for cross-document coref-
erence resolution on a corpus that contains news ar-
ticles that repeatedly report on a smaller number of
topics. This makes it particularly suitable for real-
world applications such as multi-document summa-
rization and cross-document information extraction.
We also release our labeled corpus to facilitate ex-
tensions and comparisons to our work.
Acknowledgements
We acknowledge the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusion or recommendations expressed
in this material are those of the author(s) and do not nec-
essarily reflect the view of the DARPA, AFRL, or the US
government. MR is supported by a Beatriu de Pino?s post-
doctoral scholarship (2010 BP-A 00149) from Generali-
tat de Catalunya. AC is supported by a SAP Stanford
Graduate Fellowship. We also gratefully thank Cosmin
Bejan for sharing his code and the useful discussions.
498
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC 1998 Workshop on Linguistic Coreference,
pages 563?566.
Amit Bagga and Breck Baldwin. 1999. Cross-document
event coreference: Annotations, experiments, and ob-
servations. In Proceedings of the ACL 1999 Workshop
on Coreference and Its Applications, pages 1?8.
Cosmin Bejan and Sanda Harabagiu. 2010. Unsuper-
vised Event Coreference Resolution with Rich Lin-
guistic Features. In Proceedings of ACL 2010, pages
1412?1422.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in part of speech tagging. Computational Linguistics,
21(4):543?565.
Zheng Chen and Heng Ji. 2009. Graph-based event
coreference resolution. In Proceedings of the ACL-
IJCNLP 2009 Workshop on Graph-based Methods for
Natural Language Processing, pages 54?57.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of NAACL-
HLT 2007.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP 2009, pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of HLT-NAACL 2010, pages 385?393.
Tian He. 2007. Coreference Resolution on Entities and
Events for Hospital Discharge Summaries. Thesis,
Massachusetts Institute of Technology.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44(4):311?338.
Kevin Humphreys, Robert Gaizauskas, and Saliha Az-
zam. 1997. Event coreference for information extrac-
tion. In Proceedings of the Workshop On Operational
Factors In Practical Robust Anaphora Resolution For
Unrestricted Texts, pages 75?81.
Klaus Krippendorff. 2004. Content Analysis: An In-
troduction to its Methodology. Sage, Thousand Oaks,
CA, second edition.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of CoNLL 2011: Shared Task, pages 28?34.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL 1998,
pages 768?774.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT-EMNLP 2005,
pages 25?32.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank project: an interim report. In Proceedings of the
HLT-NAACL 2004 Workshop on Frontiers in Corpus
Annotation, pages 24?31.
Rebecca Passonneau. 2004. Computing reliability for
coreference annotation. In Proceedings of LREC
2004, pages 1503?1506.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of HLT-NAACL 2006, pages 192?199.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic. In
Proceedings of EMNLP 2008, pages 650?659.
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Un-
restricted coreference: Identifying entities and events
in OntoNotes. In Proceedings of ICSC 2007, pages
446?453.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
CoNLL 2011: Shared Task, pages 1?27.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Chris Manning. 2010. A multi-pass
sieve for coreference resolution. In Proceedings of
EMNLP 2010, pages 492?501.
Altaf Rahman and Vincent Ng. 2011. Coreference
resolution with world knowledge. In Proceedings of
ACL 2011, pages 814?824.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical Association, 66(336):846?850.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(4):485?510.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-art.
In Proceedings of ACL-IJCNLP 2009, pages 656?664.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2005.
A hybrid unsupervised approach for document cluster-
ing. In Proceedings of KDD 2005, pages 685?690.
499
Mihai Surdeanu, Llu??s Ma`rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence
Research, 29:105?151.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45?52.
Bonnie Lynn Webber. 1988. Discourse deixis: reference
to discourse segments. In Proceedings of ACL 1988,
pages 113?122.
Michael L. Wick, Khashayar Rohanimanesh, Karl
Schultz, and Andrew McCallum. 2008. A unified ap-
proach for schema matching, coreference and canoni-
calization. In Proceedings of KDD 2008, pages 722?
730.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of ACL 1995, pages 189?196.
500
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 688?698, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Three Dependency-and-Boundary Models for Grammar Induction
Valentin I. Spitkovsky
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc., Mountain View, CA, 94043
hiyan@google.com
Daniel Jurafsky
Stanford University, Stanford, CA, 94305
jurafsky@stanford.edu
Abstract
We present a new family of models for unsu-
pervised parsing, Dependency and Boundary
models, that use cues at constituent bound-
aries to inform head-outward dependency tree
generation. We build on three intuitions that
are explicit in phrase-structure grammars but
only implicit in standard dependency formu-
lations: (i) Distributions of words that oc-
cur at sentence boundaries ? such as English
determiners ? resemble constituent edges.
(ii) Punctuation at sentence boundaries fur-
ther helps distinguish full sentences from
fragments like headlines and titles, allow-
ing us to model grammatical differences be-
tween complete and incomplete sentences.
(iii) Sentence-internal punctuation boundaries
help with longer-distance dependencies, since
punctuation correlates with constituent edges.
Our models induce state-of-the-art depen-
dency grammars for many languages without
special knowledge of optimal input sentence
lengths or biased, manually-tuned initializers.
1 Introduction
Natural language is ripe with all manner of bound-
aries at the surface level that align with hierarchical
syntactic structure. From the significance of func-
tion words (Berant et al2006) and punctuation
marks (Seginer, 2007; Ponvert et al2010) as sepa-
rators between constituents in longer sentences ? to
the importance of isolated words in children?s early
vocabulary acquisition (Brent and Siskind, 2001)
? word boundaries play a crucial role in language
learning. We will show that boundary information
can also be useful in dependency grammar induc-
tion models, which traditionally focus on head rather
than fringe words (Carroll and Charniak, 1992).
DT NN VBZ IN DT NN
[The check] is in [the mail].
? ?? ?
Subject
? ?? ?
Object
Figure 1: A partial analysis of our running example.
Consider the example in Figure 1. Because the
determiner (DT) appears at the left edge of the sen-
tence, it should be possible to learn that determiners
may generally be present at left edges of phrases.
This information could then be used to correctly
parse the sentence-internal determiner in the mail.
Similarly, the fact that the noun head (NN) of the ob-
ject the mail appears at the right edge of the sentence
could help identify the noun check as the right edge
of the subject NP. As with jigsaw puzzles, working
inwards from boundaries helps determine sentence-
internal structures of both noun phrases, neither of
which would be quite so clear if viewed separately.
Furthermore, properties of noun-phrase edges are
partially shared with prepositional- and verb-phrase
units that contain these nouns. Because typical head-
driven grammars model valence separately for each
class of head, however, they cannot see that the left
fringe boundary, The check, of the verb-phrase is
shared with its daughter?s, check. Neither of these
insights is available to traditional dependency for-
mulations, which could learn from the boundaries
of this sentence only that determiners might have no
left- and that nouns might have no right-dependents.
We propose a family of dependency parsing mod-
els that are capable of inducing longer-range im-
plications from sentence edges than just fertilities
of their fringe words. Our ideas conveniently lend
themselves to implementations that can reuse much
of the standard grammar induction machinery, in-
cluding efficient dynamic programming routines for
the relevant expectation-maximization algorithms.
688
2 The Dependency and Boundary Models
Our models follow a standard generative story for
head-outward automata (Alshawi, 1996a), restricted
to the split-head case (see below),1 over lexical word
classes {cw}: first, a sentence root cr is chosen, with
probability PATTACH(cr | ?; L); ? is a special start
symbol that, by convention (Klein and Manning,
2004; Eisner, 1996), produces exactly one child, to
its left. Next, the process recurses. Each (head)
word ch generates a left-dependent with probability
1 ? PSTOP( ? | L; ? ? ? ), where dots represent additional
parameterization on which it may be conditioned. If
the child is indeed generated, its identity cd is cho-
sen with probability PATTACH(cd | ch; ? ? ? ), influenced
by the identity of the parent ch and possibly other pa-
rameters (again represented by dots). The child then
generates its own subtree recursively and the whole
process continues, moving away from the head, un-
til ch fails to generate a left-dependent. At that point,
an analogous procedure is repeated to ch?s right, this
time using stopping factors PSTOP( ? | R; ? ? ? ). All parse
trees derived in this way are guaranteed to be projec-
tive and can be described by split-head grammars.
Instances of these split-head automata have been
heavily used in grammar induction (Paskin, 2001b;
Klein and Manning, 2004; Headden et al2009,
inter alia), in part because they allow for efficient
implementations (Eisner and Satta, 1999, ?8) of
the inside-outside re-estimation algorithm (Baker,
1979). The basic tenet of split-head grammars is
that every head word generates its left-dependents
independently of its right-dependents. This as-
sumption implies, for instance, that words? left-
and right-valences ? their numbers of children
to each side ? are also independent. But it does
not imply that descendants that are closer to the
head cannot influence the generation of farther
dependents on the same side. Nevertheless, many
popular grammars for unsupervised parsing behave
as if a word had to generate all of its children
(to one side) ? or at least their count ? before
allowing any of these children themselves to recurse.
For example, Klein and Manning?s (2004) depen-
dency model with valence (DMV) could be imple-
1Unrestricted head-outward automata are strictly more pow-
erful (e.g., they recognize the language anbn in finite state) than
the split-head variants, which process one side before the other.
mented as both head-outward and head-inward au-
tomata. (In fact, arbitrary permutations of siblings
to a given side of their parent would not affect the
likelihood of the modified tree, with the DMV.) We
propose to make fuller use of split-head automata?s
head-outward nature by drawing on information in
partially-generated parses, which contain useful pre-
dictors that, until now, had not been exploited even
in featurized systems for grammar induction (Cohen
and Smith, 2009; Berg-Kirkpatrick et al2010).
Some of these predictors, including the identity
? or even number (McClosky, 2008) ? of already-
generated siblings, can be prohibitively expensive in
sentences above a short length k. For example, they
break certain modularity constraints imposed by the
charts used in O(k3)-optimized algorithms (Paskin,
2001a; Eisner, 2000). However, in bottom-up pars-
ing and training from text, everything about the yield
? i.e., the ordered sequence of all already-generated
descendants, on the side of the head that is in the
process of spawning off an additional child ? is not
only known but also readily accessible. Taking ad-
vantage of this availability, we designed three new
models for dependency grammar induction.
2.1 Dependency and Boundary Model One
DBM-1 conditions all stopping decisions on adja-
cency and the identity of the fringe word ce ? the
currently-farthest descendant (edge) derived by head
ch in the given head-outward direction (dir ? {L, R}):
PSTOP( ? | dir; adj, ce).
In the adjacent case (adj = T), ch is deciding whether
to have any children on a given side: a first child?s
subtree would be right next to the head, so the head
and the fringe words coincide (ch = ce). In the non-
adjacent case (adj = F), these will be different words
and their classes will, in general, not be the same.2
Thus, non-adjacent stopping decisions will be made
independently of a head word?s identity. Therefore,
all word classes will be equally likely to continue to
grow or not, for a specific proposed fringe boundary.
For example, production of The check is involves
two non-adjacent stopping decisions on the left: one
by the noun check and one by the verb is, both of
which stop after generating a first child. In DBM-1,
2Fringe words differ also from other standard dependency
features (Eisner, 1996, ?2.3): parse siblings and adjacent words.
689
DT NN VBZ IN DT NN ?
The check is in the mail .
P = (1?
0
? ?? ?
PSTOP(? | L; T)) ? PATTACH(VBZ | ?; L)
? (1? PSTOP( ? | L; T, VBZ)) ? PATTACH(NN | VBZ; L)
? (1? PSTOP( ? | R; T, VBZ)) ? PATTACH(IN | VBZ; R)
? PSTOP( ? | L; F, DT) // VBZ ? PSTOP( ? | R; F, NN) // VBZ
? (1? PSTOP( ? | L; T, NN))2 ? P2ATTACH(DT | NN; L)
? (1? PSTOP( ? | R; T, IN)) ? PATTACH(NN | IN; R)
? P2STOP( ? | R; T, NN) ? P2STOP( ? | L; F, DT) // NN
? PSTOP( ? | L; T, IN) ? PSTOP( ? | R; F, NN) // IN
? P2STOP( ? | L; T, DT) ? P2STOP( ? | R; T, DT)
? PSTOP(? | L; F)
? ?? ?
1
? PSTOP(? | R; T)
? ?? ?
1
.
Figure 2: Our running example ? a simple sentence and
its unlabeled dependency parse structure?s probability, as
factored by DBM-1; highlighted comments specify heads
associated to non-adjacent stopping probability factors.
this outcome is captured by squaring a shared pa-
rameter belonging to the left-fringe determiner The:
PSTOP( ? | L; F, DT)2 ? instead of by a product of two
factors, such as PSTOP( ? | L; F, NN) ? PSTOP( ? | L; F, VBZ).
In these grammars, dependents? attachment prob-
abilities, given heads, are additionally conditioned
only on their relative positions ? as in traditional
models (Klein and Manning, 2004; Paskin, 2001b):
PATTACH(cd | ch; dir).
Figure 2 shows a completely factored example.
2.2 Dependency and Boundary Model Two
DBM-2 allows different but related grammars to co-
exist in a single model. Specifically, we presuppose
that all sentences are assigned to one of two classes:
complete and incomplete (comp ? {T, F}, for now
taken as exogenous). This model assumes that word-
word (i.e., head-dependent) interactions in the two
domains are the same. However, sentence lengths
? for which stopping probabilities are responsible
? and distributions of root words may be different.
Consequently, an additional comp parameter is
added to the context of two relevant types of factors:
PSTOP( ? | dir; adj, ce, comp);
and PATTACH(cr | ?; L, comp).
For example, the new stopping factors could capture
the fact that incomplete fragments ? such as the
noun-phrases George Morton, headlines Energy and
Odds and Ends, a line item c - Domestic car, dollar
quantity Revenue: $3.57 billion, the time 1:11am,
and the like ? tend to be much shorter than com-
plete sentences. The new root-attachment factors
could further track that incomplete sentences gener-
ally lack verbs, in contrast to other short sentences,
e.g., Excerpts follow:, Are you kidding?, Yes, he
did., It?s huge., Indeed it is., I said, ?NOW??, ?Ab-
solutely,? he said., I am waiting., Mrs. Yeargin de-
clined., McGraw-Hill was outraged., ?It happens.?,
I?m OK, Jack., Who cares?, Never mind. and so on.
All other attachment probabilities PATTACH(cd | ch; dir)
remain unchanged, as in DBM-1. In practice, comp
can indicate presence of sentence-final punctuation.
2.3 Dependency and Boundary Model Three
DBM-3 adds further conditioning on punctuation
context. We introduce another boolean parameter,
cross, which indicates the presence of intervening
punctuation between a proposed head word ch and
its dependent cd. Using this information, longer-
distance punctuation-crossing arcs can be modeled
separately from other, lower-level dependencies, via
PATTACH(cd | ch; dir, cross).
For instance, in Continentals believe that the
strongest growth area will be southern Europe., four
words appear between that and will. Conditioning
on (the absence of) intervening punctuation could
help tell true long-distance relations from impostors.
All other probabilities, PSTOP( ? | dir; adj, ce, comp) and
PATTACH(cr | ?; L, comp), remain the same as in DBM-2.
2.4 Summary of DBMs and Related Models
Head-outward automata (Alshawi, 1996a; Alshawi,
1996b; Alshawi et al2000) played a central part as
generative models for probabilistic grammars, start-
ing with their early adoption in supervised split-head
constituent parsers (Collins, 1997; Collins, 2003).
Table 1 lists some parameterizations that have since
been used by unsupervised dependency grammar in-
ducers sharing their backbone split-head process.
3 Experimental Set-Up and Methodology
We first motivate each model by analyzing the Wall
Street Journal (WSJ) portion of the Penn English
Treebank (Marcus et al1993),3 before delving into
3We converted labeled constituents into unlabeled depen-
dencies using deterministic ?head-percolation? rules (Collins,
690
Split-Head Dependency Grammar PATTACH (head-root) PATTACH (dependent-head) PSTOP (adjacent and not)
GB (Paskin, 2001b) 1 / |{w}| d | h; dir 1 / 2
DMV (Klein and Manning, 2004) cr | ?; L cd | ch; dir ? | dir; adj, ch
EVG (Headden et al2009) cr | ?; L cd | ch; dir, adj ? | dir; adj, ch
DBM-1 (?2.1) cr | ?; L cd | ch; dir ? | dir; adj, ce
DBM-2 (?2.2) cr | ?; L, comp cd | ch; dir ? | dir; adj, ce, comp
DBM-3 (?2.3) cr | ?; L, comp cd | ch; dir, cross ? | dir; adj, ce, comp
Table 1: Parameterizations of the split-head-outward generative process used by DBMs and in previous models.
grammar induction experiments. Although motivat-
ing solely from this treebank biases our discussion
towards a very specific genre of just one language,
it has the advantage of allowing us to make concrete
claims that are backed up by significant statistics.
In the grammar induction experiments that follow,
we will test each model?s incremental contribution
to accuracies empirically, across many disparate lan-
guages. We worked with all 23 (disjoint) train/test
splits from the 2006/7 CoNLL shared tasks (Buch-
holz and Marsi, 2006; Nivre et al2007), span-
ning 19 languages.4 For each data set, we induced
a baseline grammar using the DMV. We excluded
all training sentences with more than 15 tokens to
create a conservative bias, because in this set-up the
baseline is known to excel (Spitkovsky et al2009).
Grammar inducers were initialized using (the same)
uniformly-at-random chosen parse trees of training
sentences (Cohen and Smith, 2010); thereafter, we
applied ?add one? smoothing at every training step.
To fairly compare the models under considera-
tion ? which could have quite different starting
perplexities and ensuing consecutive relative like-
lihoods ? we experimented with two termination
strategies. In one case, we blindly ran each learner
through 40 steps of inside-outside re-estimation, ig-
noring any convergence criteria; in the other case,
we ran until numerical convergence of soft EM?s ob-
jective function or until the likelihood of resulting
Viterbi parse trees suffered ? an ?early-stopping la-
teen EM? strategy (Spitkovsky et al2011a, ?2.3).
We evaluated against all sentences of the blind test
sets (except one 145-token item in Arabic ?07 data).
Table 2 shows experimental results, averaged over
1999), discarding any empty nodes, etc., as is standard practice.
4We did not test on WSJ data because such evaluation would
not be blind, as parse trees from the PTB are our motivating ex-
amples; instead, performance on WSJ serves as a strong base-
line in a separate study (Spitkovsky et al2012a): bootstrapping
of DBMs from mostly incomplete inter-punctuation fragments.
all 19 languages, for the DMV baselines and DBM-1
and 2. We did not test DBM-3 in this set-up because
most sentence-internal punctuation occurs in longer
sentences; instead, DBM-3 will be tested later (see
?7), using most sentences,5 in the final training step
of a curriculum strategy (Bengio et al2009) that we
will propose for DBMs. For the three models tested
on shorter inputs (up to 15 tokens) both terminating
criteria exhibited the same trend; lateen EM consis-
tently scored slightly higher than 40 EM iterations.
Termination Criterion DMV DBM-1 DBM-2
40 steps of EM 33.5 38.8 40.7
early-stopping lateen EM 34.0 39.0 40.9
Table 2: Directed dependency accuracies, averaged over
all 2006/7 CoNLL evaluation sets (all sentences), for the
DMV and two new dependency-and-boundary grammar
inducers (DBM-1,2) ? using two termination strategies.6
4 Dependency and Boundary Model One
The primary difference between DBM-1 and tradi-
tional models, such as the DMV, is that DBM-1 con-
ditions non-adjacent stopping decisions on the iden-
tities of fringe words in partial yields (see ?2.1).
4.1 Analytical Motivation
Treebank data suggests that the class of the fringe
word ? its part-of-speech, ce ? is a better predic-
tor of (non-adjacent) stopping decisions, in a given
direction dir, than the head?s own class ch. A statis-
tical analysis of logistic regressions fitted to the data
shows that the (ch, dir) predictor explains only about
7% of the total variation (see Table 3). This seems
low, although it is much better compared to direction
alone (which explains less than 2%) and slightly bet-
ter than using the (current) number of the head?s de-
5Results for DBM-3 ? given only standard input sentences,
up to length fifteen ? would be nearly identical to DBM-2?s.
6We down-weighed the four languages appearing in both
CoNLL years (see Table 8) by 50% in all reported averages.
691
Non-Adjacent Stop Predictor R2adj AICc
(dir) 0.0149 1,120,200
(n, dir) 0.0726 1,049,175
(ch, dir) 0.0728 1,047,157
(ce, dir) 0.2361 904,102.4
(ch, ce, dir) 0.3320 789,594.3
Table 3: Coefficients of determination (R2) and Akaike
information criteria (AIC), both adjusted for the number
of parameters, for several single-predictor logistic models
of non-adjacent stops, given direction dir; ch is the class
of the head, n is its number of descendants (so far) to that
side, and ce represents the farthest descendant (the edge).
scendants on that side, n, instead of the head?s class.
In contrast, using ce in place of ch boosts explanatory
power to 24%, keeping the number of parameters the
same. If one were willing to roughly square the size
of the model, explanatory power could be improved
further, to 33% (see Table 3), using both ce and ch.
Fringe boundaries thus appear to be informative
even in the supervised case, which is not surprising,
since using just one probability factor (and its com-
plement) to generate very short (geometric coin-flip)
sequences is a recipe for high entropy. But as sug-
gested earlier, fringes should be extra attractive in
unsupervised settings because yields are observable,
whereas heads almost always remain hidden. More-
over, every sentence exposes two true edges (Ha?nig,
2010): integrated over many sample sentence begin-
nings and ends, cumulative knowledge about such
markers can guide a grammar inducer inside long in-
puts, where structure is murky. Table 4 shows distri-
butions of all part-of-speech (POS) tags in the tree-
bank versus in sentence-initial, sentence-final and
sentence-root positions. WSJ often leads with deter-
miners, proper nouns, prepositions and pronouns ?
all good candidates for starting English phrases; and
its sentences usually end with various noun types,
again consistent with our running example.
4.2 Experimental Results
Table 2 shows DBM-1 to be substantially more ac-
curate than the DMV, on average: 38.8 versus 33.5%
after 40 steps of EM.7 Lateen termination improved
both models? accuracies slightly, to 39.0 and 34.0%,
respectively, with DBM-1 scoring five points higher.
7DBM-1?s 39% average accuracy with uniform-at-random
initialization is two points above DMV?s scores with the ?ad-
hoc harmonic? strategy, 37% (Spitkovsky et al2011a, Table 5).
% of All First Last Sent. Frag.
POS Tokens Tokens Tokens Roots Roots
NN 15.94 4.31 36.67 0.10 23.40
IN 11.85 13.54 0.57 0.24 4.33
NNP 11.09 20.49 12.85 0.02 32.02
DT 9.84 23.34 0.34 0.00 0.04
JJ 7.32 4.33 3.74 0.01 1.15
NNS 7.19 4.49 20.64 0.15 17.12
CD 4.37 1.29 6.92 0.00 3.27
RB 3.71 5.96 3.88 0.00 1.50
VBD 3.65 0.09 3.52 46.65 0.93
VB 3.17 0.44 1.67 0.48 6.81
CC 2.86 5.93 0.00 0.00 0.00
TO 2.67 0.37 0.05 0.02 0.44
VBZ 2.57 0.17 1.65 28.31 0.93
VBN 2.42 0.61 2.57 0.65 1.28
PRP 2.08 9.04 1.34 0.00 0.00
VBG 1.77 1.26 0.64 0.10 0.97
VBP 1.50 0.05 0.61 14.33 0.71
MD 1.17 0.07 0.05 8.88 0.57
POS 1.05 0.00 0.11 0.01 0.04
PRP$ 1.00 0.90 0.00 0.00 0.00
WDT 0.52 0.08 0.00 0.01 0.13
JJR 0.39 0.18 0.43 0.00 0.09
RP 0.32 0.00 0.42 0.00 0.00
NNPS 0.30 0.20 0.56 0.00 2.96
WP 0.28 0.42 0.01 0.01 0.04
WRB 0.26 0.78 0.02 0.01 0.31
JJS 0.23 0.27 0.06 0.00 0.00
RBR 0.21 0.20 0.54 0.00 0.04
EX 0.10 0.75 0.00 0.00 0.00
RBS 0.05 0.06 0.01 0.00 0.00
PDT 0.04 0.08 0.00 0.00 0.00
FW 0.03 0.01 0.05 0.00 0.09
WP$ 0.02 0.00 0.00 0.00 0.00
UH 0.01 0.08 0.05 0.00 0.62
SYM 0.01 0.11 0.01 0.00 0.18
LS 0.01 0.09 0.00 0.00 0.00
Table 4: Empirical distributions for non-punctuation part-
of-speech tags in WSJ, ordered by overall frequency, as
well as distributions for sentence boundaries and for the
roots of complete and incomplete sentences. (A uniform
distribution would have 1/36 = 2.7% for all POS-tags.)
?
1?
?
x
?pxqx All First Last Sent. Frag.
Uniform 0.48 0.58 0.64 0.79 0.65
All 0.35 0.40 0.79 0.42
First 0.59 0.94 0.57
Last 0.83 0.29
Sent. 0.86
Table 5: A distance matrix for all pairs of probability dis-
tributions over POS-tags shown in Table 4 and the uni-
form distribution; the BC- (or Hellinger) distance (Bhat-
tacharyya, 1943; Nikulin, 2002) between discrete distri-
butions p and q (over x ? X ) ranges from zero (iff p = q)
to one (iff p ? q = 0, i.e., when they do not overlap at all).
692
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75
250
500
750
1,000
1,250
1,500
1,750
2,000
(Box-and-whiskers quartile diagrams.)
1
3
7
17
76
1
14
20
27
171
l
Distributions of Sentence Lengths (l) in WSJ
Figure 3: Histograms of lengths (in tokens) for 2,261 non-clausal fragments (red) and other sentences (blue) in WSJ.
5 Dependency and Boundary Model Two
DBM-2 adapts DBM-1 grammars to two classes
of inputs (complete sentences and incomplete frag-
ments) by forking off new, separate multinomials for
stopping decisions and root-distributions (see ?2.2).
5.1 Analytical Motivation
Unrepresentative short sentences ? such as head-
lines and titles ? are common in news-style data
and pose a known nuisance to grammar inducers.
Previous research sometimes took radical measures
to combat the problem: for example, Gillenwater
et al2009) excluded all sentences with three or
fewer tokens from their experiments; and Marec?ek
and Zabokrtsky? (2011) enforced an ?anti-noun-root?
policy to steer their Gibbs sampler away from the
undercurrents caused by the many short noun-phrase
fragments (among sentences up to length 15, in
Czech data). We refer to such snippets of text as
?incomplete sentences? and focus our study of WSJ
on non-clausal data (as signaled by top-level con-
stituent annotations whose first character is not S).8
Table 4 shows that roots of incomplete sentences,
which are dominated by nouns, barely resemble the
other roots, drawn from more traditional verb and
modal types. In fact, these two empirical root dis-
tributions are more distant from one another than ei-
ther is from the uniform distribution, in the space of
discrete probability distributions over POS-tags (see
Table 5). Of the distributions we considered, only
sentence boundaries are as or more different from
8I.e., separating top-level types {S, SINV, SBARQ, SQ, SBAR}
from the rest (ordered by frequency): {NP, FRAG, X, PP, . . .}.
(complete) roots, suggesting that heads of fragments
too may warrant their own multinomial in the model.
Further, incomplete sentences are uncharacteris-
tically short (see Figure 3). It is this property that
makes them particularly treacherous to grammar in-
ducers, since by offering few options of root posi-
tions they increase the chances that a learner will
incorrectly induce nouns to be heads. Given that ex-
pected lengths are directly related to stopping deci-
sions, it could make sense to also model the stopping
probabilities of incomplete sentences separately.
5.2 Experimental Results
Since it is not possible to consult parse trees during
grammar induction (to check whether an input sen-
tence is clausal), we opted for a proxy: presence of
sentence-final punctuation. Using punctuation to di-
vide input sentences into two groups, DBM-2 scored
higher: 40.9, up from 39.0% accuracy (see Table 2).
After evaluating these multi-lingual experiments,
we checked how well our proxy corresponds to ac-
tual clausal sentences in WSJ. Table 6 shows the bi-
nary confusion matrix having a fairly low (but posi-
tive) Pearson correlation coefficient. False positives
r? ? 0.31 Clausal non-Clausal Total
Punctuation 46,829 1,936 48,765
no Punctuation 118 325 443
Total 46,947 2,261 49,208
Table 6: A contingency table for clausal sentences and
trailing punctuation in WSJ; the mean square contingency
coefficient r? signifies a low degree of correlation. (For
two binary variables, r? is equivalent to Karl Pearson?s
better-known product-moment correlation coefficient, ?.)
693
include parenthesized expressions that are marked
as noun-phrases, such as (See related story: ?Fed
Ready to Inject Big Funds?: WSJ Oct. 16, 1989);
false negatives can be headlines having a main verb,
e.g., Population Drain Ends For Midwestern States.
Thus, our proxy is not perfect but seems to be toler-
able in practice. We suspect that identities of punc-
tuation marks (Collins, 2003, Footnote 13) ? both
sentence-final and sentence-initial ? could be of ex-
tra assistance in grammar induction, specifically for
grouping imperatives, questions, and so forth.
6 Dependency and Boundary Model Three
DBM-3 exploits sentence-internal punctuation con-
texts by modeling punctuation-crossing dependency
arcs separately from other attachments (see ?2.3).
6.1 Analytical Motivation
Many common syntactic relations, such as between
a determiner and a noun, are unlikely to hold over
long distances. (In fact, 45% of all head-percolated
dependencies in WSJ are between adjacent words.)
However, some common constructions are more re-
mote: e.g., subordinating conjunctions are, on av-
erage, 4.8 tokens away from their dependent modal
verbs. Sometimes longer-distance dependencies can
be vetted using sentence-internal punctuation marks.
It happens that the presence of punctuation be-
tween such conjunction (IN) and verb (MD) types
serves as a clue that they are not connected (see Ta-
ble 7a); by contrast, a simpler cue ? whether these
words are adjacent ? is, in this case, hardly of any
use (see Table 7b). Conditioning on crossing punc-
tuation could be of help then, playing a role simi-
lar to that of comma-counting (Collins, 1997, ?2.1)
? and ?verb intervening? (Bikel, 2004, ?5.1) ? in
early head-outward models for supervised parsing.
a) r? ? ?0.40 Attached not Attached Total
Punctuation 337 7,645 7,982
no Punctuation 2,144 4,040 6,184
Total 2,481 11,685 14,166
non-Adjacent 2,478 11,673 14,151
Adjacent 3 12 15
b) r? ? +0.00 Attached not Attached Total
Table 7: Contingency tables for IN right-attaching MD,
among closest ordered pairs of these tokens in WSJ sen-
tences with punctuation, versus: (a) presence of interven-
ing punctuation; and (b) presence of intermediate words.
6.2 Experimental Results Postponed
As we mentioned earlier (see ?3), there is little point
in testing DBM-3 with shorter sentences, since most
sentence-internal punctuation occurs in longer in-
puts. Instead, we will test this model in a final step of
a staged training strategy, with more data (see ?7.3).
7 A Curriculum Strategy for DBMs
We propose to train up to DBM-3 iteratively ?
by beginning with DBM-1 and gradually increasing
model complexity through DBM-2, drawing on the
intuitions of IBM translation models 1?4 (Brown et
al., 1993). Instead of using sentences of up to 15 to-
kens, as in all previous experiments (?4?5), we will
now make use of nearly all available training data:
up to length 45 (out of concern for efficiency), dur-
ing later stages. In the first stage, however, we will
use only a subset of the data with DBM-1, in a pro-
cess sometimes called curriculum learning (Bengio
et al2009; Krueger and Dayan, 2009, inter alia).
Our grammar inducers will thus be ?starting small?
in both senses suggested by Elman (1993): simulta-
neously scaffolding on model- and data-complexity.
7.1 Scaffolding Stage #1: DBM-1
We begin by training DBM-1 on sentences with-
out sentence-internal punctuation but with at least
one trailing punctuation mark. Our goal is to avoid,
when possible, overly specific arbitrary parameters
like the ?15 tokens or less? threshold used to select
training sentences. Unlike DBM-2 and 3, DBM-1
does not model punctuation or sentence fragments,
so we instead explicitly restrict its attention to this
cleaner subset of the training data, which takes ad-
vantage of the fact that punctuation may generally
correlate with sentence complexity (Frank, 2000).9
Aside from input sentence selection, our exper-
imental set-up here remained identical to previous
training of DBMs (?4?5). Using this new input data,
DBM-1 averaged 40.7% accuracy (see Table 8).
This is slightly higher than the 39.0% when using
sentences up to length 15, suggesting that our heuris-
tic for clean, simple sentences may be a useful one.
9More incremental training strategies are the subject of an
unpublished companion manuscript (Spitkovsky et al2012a).
694
Directed Dependency Accuracies for: Best of State-of-the-Art Systems
CoNLL Year this Work (@10) Monolingual; POS- Cross-Lingual
& Language DMV DBM-1 DBM-2 DBM-3 +inference (i) Agnostic (ii) Identified (iii) Transfer
Arabic 2006 12.9 10.6 11.0 11.1 10.9 (34.5) 33.4 SCAJ6 ? 50.2 Sbg
?7 36.6 43.9 44.0 44.4 44.9 (48.8) 55.6 RF 54.6 RFH1 ?
Basque ?7 32.7 34.1 33.0 32.7 33.3 (36.5) 43.6 SCAJ5 34.7 MZNR ?
Bulgarian ?7 24.7 59.4 63.6 64.6 65.2 (70.4) 44.3 SCAJ5 53.9 RFH1&2 70.3 Spt
Catalan ?7 41.1 61.3 61.1 61.1 62.1 (78.1) 63.8 SCAJ5 56.3 MZNR ?
Chinese ?6 50.4 63.1 63.0 63.2 63.2 (65.7) 63.6 SCAJ6 ? ?
?7 55.3 56.8 57.0 57.1 57.0 (59.8) 58.5 SCAJ6 34.6 MZNR ?
Czech ?6 31.5 51.3 52.8 53.0 55.1 (61.8) 50.5 SCAJ5 ? ?
?7 34.5 50.5 51.2 53.3 54.2 (67.3) 49.8 SCAJ5 42.4 RFH1&2 ?
Danish ?6 22.4 21.3 19.9 21.8 22.2 (27.4) 46.0 RF 53.1 RFH1&2 56.5 Sar
Dutch ?6 44.9 45.9 46.5 46.0 46.6 (48.6) 32.5 SCAJ5 48.8 RFH1&2 65.7 MPHm:p
English ?7 32.3 29.2 28.6 29.0 29.6 (51.4) 50.3 SAJ 23.8 MZNR 45.7 MPHel
German ?6 27.7 36.3 37.9 38.4 39.1 (52.1) 33.5 SCAJ5 21.8 MZNR 56.7 MPHm:d
Greek ?6 36.3 28.1 26.1 26.1 26.9 (36.8) 39.0 MZ 33.4 MZNR 65.1 MPHm:p
Hungarian ?7 23.6 43.2 52.1 57.4 58.2 (68.4) 48.0 MZ 48.1 MZNR ?
Italian ?7 25.5 41.7 39.8 39.9 40.7 (41.8) 57.5 MZ 60.6 MZNR 69.1 MPHpt
Japanese ?6 42.2 22.8 22.7 22.7 22.7 (32.5) 56.6 SCAJ5 53.5 MZNR ?
Portuguese ?6 37.1 68.9 72.3 71.1 72.4 (80.6) 43.2 MZ 55.8 RFH1&2 76.9 Sbg
Slovenian ?6 33.4 30.4 33.0 34.1 35.2 (36.8) 33.6 SCAJ5 34.6 MZNR ?
Spanish ?6 22.0 25.0 26.7 27.1 28.2 (51.8) 53.0 MZ 54.6 MZNR 68.4 MPHit
Swedish ?6 30.7 48.6 50.3 50.0 50.7 (63.2) 50.0 SCAJ6 34.3 RFH1&2 68.0 MPHm:p
Turkish ?6 43.4 32.9 33.7 33.4 34.4 (38.1) 40.9 SAJ 61.3 RFH1 ?
?7 58.5 44.6 44.2 43.7 44.8 (44.4) 48.8 SCAJ6 ? ?
Average: 33.6 40.7 41.7 42.2 42.9 (51.9) 38.2 SCAJ6 (best average, not an average of bests)
Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1?3 trained
with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual,
including SCAJ (Spitkovsky et al2011a, Tables 5?6) and SAJ (Spitkovsky et al2011b); (ii) rely on gold POS-tag
identities to discourage noun roots (Marec?ek and Zabokrtsky?, 2011, MZ) or to encourage verbs (Rasooli and Faili,
2012, RF); and (iii) transfer delexicalized parsers (S?gaard, 2011a, S) from resource-rich languages with transla-
tions (McDonald et al2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3
trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints.
7.2 Scaffolding Stage #2: DBM-2? DBM-1
Next, we trained on all sentences up to length 45.
Since these inputs are punctuation-rich, in both re-
maining stages we used the constrained Viterbi EM
set-up suggested by Spitkovsky et al2011b) in-
stead of plain soft EM; we employ an early termina-
tion strategy, quitting hard EM as soon as soft EM?s
objective suffers (Spitkovsky et al2011a). Punc-
tuation was converted into Viterbi-decoding con-
straints during training using the so-called loose
method, which stipulates that all words in an inter-
punctuation fragment must be dominated by a single
(head) word, also from that fragment ? with only
these head words allowed to attach the head words
of other fragments, across punctuation boundaries.
To adapt to full data, we initialized DBM-2 using
Viterbi parses from the previous stage (?7.1), plus
uniformly-at-random chosen dependency trees for
the new complex and incomplete sentences, subject
to punctuation-induced constraints. This approach
improved parsing accuracies to 41.7% (see Table 8).
7.3 Scaffolding Stage #3: DBM-3? DBM-2
Next, we repeated the training process of the pre-
vious stage (?7.2) using DBM-3. To initialize this
model, we combined the final instance of DBM-2
with uniform multinomials for punctuation-crossing
attachment probabilities (see ?2.3). As a result, av-
erage performance improved to 42.2% (see Table 8).
Lastly, we applied punctuation constraints also in
inference. Here we used the sprawl method ? a
more relaxed approach than in training, allowing ar-
bitrary words to attach inter-punctuation fragments
(provided that each entire fragment still be derived
695
by one of its words) ? as suggested by Spitkovsky
et al2011b). This technique increased DBM-3?s
average accuracy to 42.9% (see Table 8). Our fi-
nal result substantially improves over the baseline?s
33.6% and compares favorably to previous work.10
8 Discussion and the State-of-the-Art
DBMs come from a long line of head-outward mod-
els for dependency grammar induction yet their gen-
erative processes feature important novelties. One
is conditioning on more observable state ? specifi-
cally, the left and right end words of a phrase being
constructed ? than in previous work. Another is al-
lowing multiple grammars ? e.g., of complete and
incomplete sentences ? to coexist in a single model.
These improvements could make DBMs quick-and-
easy to bootstrap directly from any available partial
bracketings (Pereira and Schabes, 1992), for exam-
ple capitalized phrases (Spitkovsky et al2012b).
The second part of our work ? the use of a cur-
riculum strategy to train DBM-1 through 3 ? elim-
inates having to know tuned cut-offs, such as sen-
tences with up to a predetermined number of tokens.
Although this approach adds some complexity, we
chose conservatively, to avoid overfitting settings
of sentence length, convergence criteria, etc.: stage
one?s data is dictated by DBM-1 (which ignores
punctuation); subsequent stages initialize additional
pieces uniformly: uniform-at-random parses for new
data and uniform multinomials for new parameters.
Even without curriculum learning ? trained with
vanilla EM ? DBM-2 and 1 are already strong.
Further boosts to accuracy could come from em-
ploying more sophisticated optimization algorithms,
e.g., better EM (Samdani et al2012), constrained
Gibbs sampling (Marec?ek and Zabokrtsky?, 2011) or
locally-normalized features (Berg-Kirkpatrick et al
2010). Other orthogonal dependency grammar in-
duction techniques ? including ones based on uni-
versal rules (Naseem et al2010) ? may also ben-
efit in combination with DBMs. Direct comparisons
to previous work require some care, however, as
there are several classes of systems that make dif-
ferent assumptions about training data (see Table 8).
10Note that DBM-1?s 39% average accuracy with standard
training (see Table 2) was already nearly a full point higher than
that of any single previous best system (SCAJ6 ? see Table 8).
8.1 Monolingual POS-Agnostic Inducers
The first type of grammar inducers, including our
own approach, uses standard training and test data
sets for each language, with gold part-of-speech tags
as anonymized word classes. For the purposes of
this discussion, we also include in this group trans-
ductive learners that may train on data from the test
sets. Our DBM-3 (decoded with punctuation con-
straints) does well among such systems ? for which
accuracies on all sentence lengths of the evaluation
sets are reported ? attaining highest scores for 8 of
19 languages; the DMV baseline is still state-of-the-
art for one language; and the remaining 10 bests are
split among five other recent systems (see Table 8).11
Half of the five came from various lateen EM strate-
gies (Spitkovsky et al2011a) for escaping and/or
avoiding local optima. These heuristics are compat-
ible with how we trained our DBMs and could po-
tentially provide further improvement to accuracies.
Overall, the final scores of DBM-3 were better, on
average, than those of any other single system: 42.9
versus 38.2% (Spitkovsky et al2011a, Table 6).
The progression of scores for DBM-1 through 3
without using punctuation constraints in inference
? 40.7, 41.7 and 42.2% ? fell entirely above this
previous state-of-the-art result as well; the DMV
baseline ? also trained on sentences without inter-
nal but with final punctuation ? averaged 33.6%.
8.2 Monolingual POS-Identified Inducers
The second class of techniques assumes knowledge
about identities of part-of-speech tags (Naseem et
al., 2010), i.e., which word tokens are verbs, which
ones are nouns, etc. Such grammar inducers gener-
ally do better than the first kind ? e.g., by encour-
aging verbocentricity (Gimpel and Smith, 2011) ?
though even here our results appear to be compet-
itive. In fact, to our surprise, only in 5 of 19 lan-
guages a ?POS-identified? system performed better
than all of the ?POS-agnostic? ones (see Table 8).
8.3 Multi-Lingual Semi-Supervised Parsers
The final broad class of related algorithms we con-
sidered extends beyond monolingual data and uses
11For Turkish ?06, the ?right-attach? baseline outperforms
even the DMV, at 65.4% (Rasooli and Faili, 2012, Table 1); an
important difference between 2006 and 2007 CoNLL data sets
has to do with segmentation of morphologically-rich languages.
696
both identities of POS-tags and/or parallel bitexts
to transfer (supervised) delexicalized parsers across
languages. Parser projection is by far the most suc-
cessful approach to date and we hope that it too
may stand to gain from our modeling improvements.
Of the 10 languages for which we found results
in the literature, transferred parsers underperformed
the grammar inducers in only one case: on En-
glish (see Table 8). The unsupervised system that
performed better used a special ?weighted? initial-
izer (Spitkovsky et al2011b, ?3.1) that worked well
for English (but less so for many other languages).
DBMs may be able to improve initialization. For
example, modeling of incomplete sentences could
help in incremental initialization strategies like baby
steps (Spitkovsky et al2009), which are likely sen-
sitive to the proverbial ?bum steer? from unrepresen-
tative short fragments, pace Tu and Honavar (2011).
8.4 Miscellaneous Systems on Short Sentences
Several recent systems (Cohen et al2011; S?gaard,
2011b; Naseem et al2010; Gillenwater et al2010;
Berg-Kirkpatrick and Klein, 2010, inter alia) are ab-
sent from Table 8 because they do not report perfor-
mance for all sentence lengths. To facilitate com-
parison with this body of important previous work,
we also tabulated final accuracies for the ?up-to-ten
words? task under heading @10: 51.9%, on average.
9 Conclusion
Although a dependency parse for a sentence can be
mapped to a constituency parse (Xia and Palmer,
2001), the probabilistic models generating them use
different conditioning: dependency grammars focus
on the relationship between arguments and heads,
constituency grammars on the coherence of chunks
covered by non-terminals. Since redundant views of
data can make learning easier (Blum and Mitchell,
1998), integrating aspects of both constituency and
dependency ought to be able to help grammar in-
duction. We have shown that this insight is correct:
dependency grammar inducers can gain from mod-
eling boundary information that is fundamental to
constituency (i.e., phrase-structure) formalisms.
DBMs are a step in the direction towards mod-
eling constituent boundaries jointly with head de-
pendencies. Further steps must involve more tightly
coupling the two frameworks, as well as showing
ways to incorporate both kinds of information in
other state-of-the art grammar induction paradigms.
Acknowledgments
We thank Roi Reichart and Marta Recasens, for many helpful
comments on draft versions of this paper, and Marie-Catherine
de Marneffe, Roy Schwartz, Mengqiu Wang and the anonymous
reviewers, for their apt recommendations. Funded, in part, by
Defense Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program, under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusion or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the view of the DARPA, AFRL, or the US government.
First author is grateful to Cindy Chan for her friendship and
support over many long months leading up to this publication.
References
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning
dependency translation models as collections of finite-state
head transducers. Computational Linguistics, 26.
H. Alshawi. 1996a. Head automata for speech translation. In
ICSLP.
H. Alshawi. 1996b. Method and apparatus for an improved
language recognition system. US Patent 1999/5870706.
J. K. Baker. 1979. Trainable grammars for speech recognition.
In Speech Communication Papers for the 97th Meeting of the
Acoustical Society of America.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009.
Curriculum learning. In ICML.
J. Berant, Y. Gross, M. Mussel, B. Sandbank, E. Ruppin, and
S. Edelman. 2006. Boosting unsupervised grammar induc-
tion by splitting complex sentences on function words. In
BUCLD.
T. Berg-Kirkpatrick and D. Klein. 2010. Phylogenetic gram-
mar induction. In ACL.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with fea-
tures. In NAACL-HLT.
A. Bhattacharyya. 1943. On a measure of divergence between
two statistical populations defined by their probability distri-
butions. BCMS, 35.
D. M. Bikel. 2004. Intricacies of Collins? parsing model. Com-
putational Linguistics, 30.
A. Blum and T. Mitchell. 1998. Combining labeled and unla-
beled data with co-training. In COLT.
M. R. Brent and J. M. Siskind. 2001. The role of exposure to
isolated words in early vocabulary development. Cognition,
81.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguistics, 19.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In CoNLL.
697
G. Carroll and E. Charniak. 1992. Two experiments on learning
probabilistic dependency grammars from corpora. Technical
report, Brown University.
S. B. Cohen and N. A. Smith. 2009. Shared logistic normal dis-
tributions for soft parameter tying in unsupervised grammar
induction. In NAACL-HLT.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs:
Hardness results and competitiveness of uniform initializa-
tion. In ACL.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsupervised
structure prediction with non-parallel multilingual guidance.
In EMNLP.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In ACL.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
M. Collins. 2003. Head-driven statistical models for natural
language parsing. Computational Linguistics, 29.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head-automaton grammars. In
ACL.
J. M. Eisner. 1996. An empirical comparison of probability
models for dependency grammar. Technical report, IRCS.
J. Eisner. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In H. C. Bunt and A. Nijholt, editors,
Advances in Probabilistic and Other Parsing Technologies.
Kluwer Academic Publishers.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
R. Frank. 2000. From regular to context-free to mildly context-
sensitive tree rewriting systems: The path of child language
acquisition. In A. Abeille? and O. Rambow, editors, Tree
Adjoining Grammars: Formalisms, Linguistic Analysis and
Processing. CSLI Publications.
J. Gillenwater, K. Ganchev, J. Grac?a, B. Taskar, and F. Pereira.
2009. Sparsity in grammar induction. In NIPS: Gram-
mar Induction, Representation of Language and Language
Learning.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and B. Taskar.
2010. Posterior sparsity in unsupervised dependency pars-
ing. Technical report, University of Pennsylvania.
K. Gimpel and N. A. Smith. 2011. Concavity and initialization
for unsupervised dependency grammar induction. Technical
report, CMU.
C. Ha?nig. 2010. Improvements in unsupervised co-occurrence
based parsing. In CoNLL.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with richer con-
texts and smoothing. In NAACL-HLT.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In ACL.
K. A. Krueger and P. Dayan. 2009. Flexible shaping: How
learning in small steps helps. Cognition, 110.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. Marec?ek and Z. Zabokrtsky?. 2011. Gibbs sampling with
treeness constraint in unsupervised dependency parsing. In
ROBUS.
D. McClosky. 2008. Modeling valence effects in unsupervised
grammar induction. Technical report, Brown University.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-source trans-
fer of delexicalized dependency parsers. In EMNLP.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010. Using
universal linguistic knowledge to guide grammar induction.
In EMNLP.
M. S. Nikulin. 2002. Hellinger distance. In M. Hazewinkel,
editor, Encyclopaedia of Mathematics. Kluwer Academic
Publishers.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In EMNLP-CoNLL.
M. A. Paskin. 2001a. Cubic-time parsing and learning algo-
rithms for grammatical bigram models. Technical report,
UCB.
M. A. Paskin. 2001b. Grammatical bigrams. In NIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
E. Ponvert, J. Baldridge, and K. Erk. 2010. Simple unsuper-
vised identification of low-level constituents. In ICSC.
M. S. Rasooli and H. Faili. 2012. Fast unsupervised depen-
dency parsing with arc-standard transitions. In ROBUS-
UNSUP.
R. Samdani, M.-W. Chang, and D. Roth. 2012. Unified expec-
tation maximization. In NAACL-HLT.
Y. Seginer. 2007. Learning Syntactic Structure. Ph.D. thesis,
University of Amsterdam.
A. S?gaard. 2011a. Data point selection for cross-language
adaptation of dependency parsers. In ACL.
A. S?gaard. 2011b. From ranked words to dependency trees:
two-stage unsupervised non-projective dependency parsing.
In TextGraphs.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby
Steps: How ?Less is More? in unsupervised dependency
parsing. In NIPS: Grammar Induction, Representation of
Language and Language Learning.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen
EM: Unsupervised training with multiple objectives, applied
to dependency grammar induction. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctu-
ation: Making a point in unsupervised dependency parsing.
In CoNLL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012a. Boot-
strapping dependency grammar inducers from incomplete
sentence fragments via austere models ? the ?wabi-sabi? of
unsupervised parsing. In submission.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012b. Capi-
talization cues improve dependency grammar induction. In
WILS.
K. Tu and V. Honavar. 2011. On the utility of curricula in
unsupervised learning of probabilistic grammars. In IJCAI.
F. Xia and M. Palmer. 2001. Converting dependency structures
to phrase structures. In HLT.
698
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1983?1995,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Breaking Out of Local Optima with Count Transforms
and Model Recombination: A Study in Grammar Induction
Valentin I. Spitkovsky
valentin@cs.stanford.edu
Hiyan Alshawi
hiyan@google.com
Daniel Jurafsky
jurafsky@stanford.edu
Abstract
Many statistical learning problems in NLP call
for local model search methods. But accu-
racy tends to suffer with current techniques,
which often explore either too narrowly or too
broadly: hill-climbers can get stuck in local
optima, whereas samplers may be inefficient.
We propose to arrange individual local opti-
mizers into organized networks. Our building
blocks are operators of two types: (i) trans-
form, which suggests new places to search, via
non-random restarts from already-found local
optima; and (ii) join, which merges candidate
solutions to find better optima. Experiments
on grammar induction show that pursuing dif-
ferent transforms (e.g., discarding parts of a
learned model or ignoring portions of train-
ing data) results in improvements. Groups of
locally-optimal solutions can be further per-
turbed jointly, by constructing mixtures. Us-
ing these tools, we designed several modu-
lar dependency grammar induction networks
of increasing complexity. Our complete sys-
tem achieves 48.6% accuracy (directed depen-
dency macro-average over all 19 languages in
the 2006/7 CoNLL data) ? more than 5%
higher than the previous state-of-the-art.
1 Introduction
Statistical methods for grammar induction often boil
down to solving non-convex optimization problems.
Early work attempted to locally maximize the likeli-
hood of a corpus, using EM to estimate probabilities
of dependency arcs between word bigrams (Paskin
2001a; 2001b). That parsing model has since been
extended to make unsupervised learning more feasi-
ble (Klein and Manning, 2004; Headden et al, 2009;
Spitkovsky et al, 2012b). But even the latest tech-
niques can be quite error-prone and sensitive to ini-
tialization, because of approximate, local search.
In theory, global optima can be found by enumer-
ating all parse forests that derive a corpus, though
this is usually prohibitively expensive in practice. A
preferable brute force approach is sampling, as in
Markov-chain Monte Carlo (MCMC) and random
restarts (Hu et al, 1994), which hit exact solutions
eventually. Restarts can be giant steps in a parameter
space that undo all previous work. At the other ex-
treme, MCMC may cling to a neighborhood, reject-
ing most proposed moves that would escape a local
attractor. Sampling methods thus take unbounded
time to solve a problem (and can?t certify optimal-
ity) but are useful for finding approximate solutions
to grammar induction (Cohn et al, 2011; Marec?ek
and ?Zabokrtsky?, 2011; Naseem and Barzilay, 2011).
We propose an alternative (deterministic) search
heuristic that combines local optimization via EM
with non-random restarts. Its new starting places are
informed by previously found solutions, unlike con-
ventional restarts, but may not resemble their prede-
cessors, unlike typical MCMC moves. We show that
one good way to construct such steps in a parame-
ter space is by forgetting some aspects of a learned
model. Another is by merging promising solutions,
since even simple interpolation (Jelinek and Mercer,
1980) of local optima may be superior to all of the
originals. Informed restarts can make it possible to
explore a combinatorial search space more rapidly
and thoroughly than with traditional methods alone.
2 Abstract Operators
Let C be a collection of counts ? the sufficient
statistics from which a candidate solution to an
optimization problem could be computed, e.g., by
smoothing and normalizing to yield probabilities.
The counts may be fractional and solutions could
take the form of multinomial distributions. A local
optimizer L will convert C into C? = LD(C) ? an
updated collection of counts, resulting in a proba-
bilistic model that is no less (and hopefully more)
consistent with a data set D than the original C:
(1)
LDC C?
1983
Unless C? is a global optimum, we should be able
to make further improvements. But if L is idempo-
tent (and ran to convergence) then L(L(C)) = L(C).
Given only C and LD, the single-node optimization
network above would be the minimal search pattern
worth considering. However, if we had another opti-
mizer L? ? or a fresh starting point C? ? then more
complicated networks could become useful.
2.1 Transforms (Unary)
New starts could be chosen by perturbing an existing
solution, as in MCMC, or independently of previous
results, as in random restarts. We focus on interme-
diate changes to C, without injecting randomness.
All of our transforms involve selective forgetting
or filtering. For example, if the probabilistic model
that is being estimated decomposes into independent
constituents (e.g., several multinomials) then a sub-
set of them can be reset to uniform distributions, by
discarding associated counts from C. In text classifi-
cation, this could correspond to eliminating frequent
or rare tokens from bags-of-words. We use circular
shapes to represent such model ablation operators:
(2)C
An orthogonal approach might separate out vari-
ous counts in C by their provenance. For instance,
if D consisted of several heterogeneous data sources,
then the counts from some of them could be ignored:
a classifier might be estimated from just news text.
We will use squares to represent data-set filtering:
(3)C
Finally, if C represents a mixture of possible inter-
pretations over D ? e.g., because it captures the out-
put of a ?soft? EM algorithm ? contributions from
less likely, noisier completions could also be sup-
pressed (and their weights redistributed to the more
likely ones), as in ?hard? EM. Diamonds will repre-
sent plain (single) steps of Viterbi training:
(4)C
2.2 Joins (Binary)
Starting from different initializers, say C1 and C2,
it may be possible for L to arrive at distinct local
optima, C?1 6= C?2 . The better of the two solutions,
according to likelihood LD of D, could then be se-
lected ? as is standard practice when sampling.
Our joining technique could do better than either
C?1 or C
?
2 , by entertaining also a third possibility,
which combines the two candidates. We construct
a mixture model by adding together all counts from
C?1 and C?2 into C+ = C?1 + C?2 . Original initializers
C1, C2 will, this way, have equal pull on the merged
model,1 regardless of nominal size (because C?1 , C?2
will have converged using a shared training set, D).
We return the best of C?1 , C?2 and C?+ = L(C+). This
approach may uncover more (and never returns less)
likely solutions than choosing among C?1 , C?2 alone:
(5)
LD
LD
LD
+
arg
M
A
X
L
D
C1
C?1 = L(C1)
C2
C?2 = L(C2)
C?1 + C?2 = C+
We will use a short-hand notation to represent the
combiner network diagrammed above, less clutter:
(6)
LDC2
C1
3 The Task and Methodology
We apply transform and join paradigms to grammar
induction, an important problem of computational
linguistics that involves notoriously difficult objec-
tives (Pereira and Schabes, 1992; de Marcken, 1995;
Gimpel and Smith, 2012, inter alia). The goal is to
induce grammars capable of parsing unseen text. In-
put, in both training and testing, is a sequence of to-
kens labeled as: (i) a lexical item and its category,
(w, cw); (ii) a punctuation mark; or (iii) a sentence
boundary. Output is unlabeled dependency trees.
3.1 Models and Data
We constrain all parse structures to be projective, via
dependency-and-boundary grammars (Spitkovsky et
al., 2012a; 2012b): DBMs 0?3 are head-outward
generative parsing models (Alshawi, 1996) that dis-
tinguish complete sentences from incomplete frag-
ments in a corpus D: Dcomp comprises inputs ending
with punctuation; Dfrag = D ? Dcomp is everything
1If desired, a scaling factor could be used to bias C+ towards
either C?1 or C?2 , for example based on their likelihood ratio.
1984
else. The ?complete? subset is further partitioned
into simple sentences, Dsimp ? Dcomp, with no inter-
nal punctuation, and others, which may be complex.
As an example, consider the beginning of an arti-
cle from (simple) Wikipedia: (i) Linguistics (ii) Lin-
guistics (sometimes called philology) is the science
that studies language. (iii) Scientists who study lan-
guage are called linguists. Since the title does not
end with punctuation, it would be relegated to Dfrag.
But two complete sentences would be in Dcomp, with
the last also filed under Dsimp, as it has only a trail-
ing punctuation mark. Spitkovsky et al suggested
two curriculum learning strategies: (i) one in which
induction begins with clean, simple data, Dsimp, and
a basic model, DBM-1 (2012b); and (ii) an alterna-
tive bootstrapping approach: starting with still more,
simpler data ? namely, short inter-punctuation frag-
ments up to length l = 15, Dlsplit ? Dlsimp ? and a
bare-bones model, DBM-0 (2012a). In our example,
Dsplit would hold five text snippets: (i) Linguistics;
(ii) Linguistics; (iii) sometimes called philology;
(iv) is the science that studies language; and (v) Sci-
entists who study language are called linguists.
Only the last piece of text would still be considered
complete, isolating its contribution to sentence root
and boundary word distributions from those of in-
complete fragments. The sparse model, DBM-0, as-
sumes a uniform distribution for roots of incomplete
inputs and reduces conditioning contexts of stopping
probabilities, which works well with split data. We
will exploit both DBM-0 and the full DBM,2 draw-
ing also on split, simple and raw views of input text.
All experiments prior to final multi-lingual eval-
uation will use the Penn English Treebank?s Wall
Street Journal (WSJ) portion (Marcus et al, 1993) as
the underlying tokenized and sentence-broken cor-
pus D. Instead of gold parts-of-speech, we plugged
in 200 context-sensitive unsupervised tags, from
Spitkovsky et al (2011c),3 for the word categories.
3.2 Smoothing and Lexicalization
All unlexicalized instances of DBMs will be esti-
mated with ?add one? (a.k.a. Laplace) smoothing,
2We use the short-hand DBM to refer to DBM-3, which is
equivalent to DBM-2 if D has no internally-punctuated sen-
tences (D=Dsplit), and DBM-1 if all inputs also have trailing
punctuation (D=Dsimp); DBM0 is our short-hand for DBM-0.
3http://nlp.stanford.edu/pubs/goldtags-data.tar.bz2
using only the word category cw to represent a token.
Fully-lexicalized grammars (L-DBM) are left un-
smoothed, and represent each token as both a word
and its category, i.e., the whole pair (w, cw). To eval-
uate a lexicalized parsing model, we will always ob-
tain a delexicalized-and-smoothed instance first.
3.3 Optimization and Viterbi Decoding
We use ?early-switching lateen? EM (Spitkovsky et
al., 2011a, ?2.4) to train unlexicalized models, alter-
nating between the objectives of ordinary (soft) and
hard EM algorithms, until neither can improve its
own objective without harming the other?s. This ap-
proach does not require tuning termination thresh-
olds, allowing optimizers to run to numerical con-
vergence if necessary, and handles only our shorter
inputs (l ? 15), starting with soft EM (L = SL, for
?soft lateen?). Lexicalized models will cover full
data (l ? 45) and employ ?early-stopping lateen?
EM (2011a, ?2.3), re-estimating via hard EM until
soft EM?s objective suffers. Alternating EMs would
be expensive here, since updates take (at least) O(l3)
time, and hard EM?s objective (L = H) is the one
better suited to long inputs (Spitkovsky et al, 2010).
Our decoders always force an inter-punctuation
fragment to derive itself (Spitkovsky et al, 2011b,
?2.2).4 In evaluation, such (loose) constraints may
help attach sometimes and philology to called (and
the science... to is). In training, stronger (strict)
constraints also disallow attachment of fragments?
heads by non-heads, to connect Linguistics, called
and is (assuming each piece got parsed correctly).
3.4 Final Evaluation and Metrics
Evaluation is against held-out CoNLL shared task
data (Buchholz and Marsi, 2006; Nivre et al, 2007),
spanning 19 languages. We compute performance
as directed dependency accuracies (DDA), fractions
of correct unlabeled arcs in parsed output (an extrin-
sic metric).5 For most WSJ experiments we include
also sentence and parse tree cross-entropies (soft and
hard EMs? intrinsic metrics), in bits per token (bpt).
4But these constraints do not impact training with shorter
inputs, since there is no internal punctuation in Dsplit or Dsimp.
5We converted gold labeled constituents in WSJ to unlabeled
reference dependencies using deterministic ?head-percolation?
rules (Collins, 1999); sentence root symbols, though not punc-
tuation arcs, contribute to scores, as is standard (Paskin, 2001b).
1985
4 Concrete Operators
We will now instantiate the operators sketched out
in ?2 specifically for the grammar induction task.
Throughout, we repeatedly employ single steps of
Viterbi training to transfer information between sub-
networks in a model-independent way: when a mod-
ule?s output is a set of (Viterbi) parse trees, it neces-
sarily contains sufficient information required to es-
timate an arbitrarily-factored model down-stream.6
4.1 Transform #1: A Simple Filter
Given a model that was estimated from (and there-
fore parses) a data set D, the simple filter (F ) at-
tempts to extract a cleaner model, based on the sim-
pler complete sentences of Dsimp. It is implemented
as a single (unlexicalized) step of Viterbi training:
(7)C F
The idea here is to focus on sentences that are not
too complicated yet grammatical. This punctuation-
sensitive heuristic may steer a learner towards easy
but representative training text and, we showed, aids
grammar induction (Spitkovsky et al, 2012b, ?7.1).
4.2 Transform #2: A Symmetrizer
The symmetrizer (S) reduces input models to sets of
word association scores. It blurs all details of in-
duced parses in a data set D, except the number of
times each (ordered) word pair participates in a de-
pendency relation. We implemented symmetrization
also as a single unlexicalized Viterbi training step,
but now with proposed parse trees? scores, for a sen-
tence in D, proportional to a product over non-root
dependency arcs of one plus how often the left and
right tokens (are expected to) appear connected:
(8)C S
The idea behind the symmetrizer is to glean infor-
mation from skeleton parses. Grammar inducers can
sometimes make good progress in resolving undi-
rected parse structures despite being wrong about
the polarities of most arcs (Spitkovsky et al, 2009,
Figure 3: Uninformed). Symmetrization offers an
extra chance to make heads or tails of syntactic rela-
tions, after learning which words tend to go together.
6A related approach ? initializing EM training with an
M-step ? was advocated by Klein and Manning (2004, ?3).
At each instance where a word a? attaches z? on
(say) the right, our implementation attributes half its
weight to the intended construction, ya? z?, reserving
the other half for the symmetric structure, z? attach-
ing a? to its left: xa? z?. For the desired effect, these
aggregated counts are left unnormalized, while all
other counts (of word fertilities and sentence roots)
get discarded. To see why we don?t turn word attach-
ment scores into probabilities, consider sentences
a? z? and c? z?. The fact that z? co-occurs with a?
introduces an asymmetry into z??s relation with c?:
P( z? | c?) = 1 differs from P( c? | z?) = 1/2. Normal-
izing might force the interpretation yc? z? (and also
y
a? z?), not because there is evidence in the data, but
as a side-effect of a model?s head-driven nature (i.e.,
factored with dependents conditioned on heads). Al-
ways branching right would be a mistake, however,
for example if z? is a noun, since either of a? or c?
could be a determiner, with the other a verb.
4.3 Join: A Combiner
The combiner must admit arbitrary inputs, includ-
ing models not estimated from D, unlike the trans-
forms. Consequently, as a preliminary step, we con-
vert each input Ci into parse trees of D, with counts
C?i, via Viterbi-decoding with a smoothed, unlexical-
ized version of the corresponding incoming model.
Actual combination is then performed in a more pre-
cise (unsmoothed) fashion: C?i are the (lexicalized)
solutions starting from C?i; and C?+ is initialized with
their sum,
?
iC
?
i . Counts of the lexicalized model
with lowest cross-entropy on D become the output:7
(9)
LDC2
C1
5 Basic Networks
We are ready to propose a non-trivial subnetwork for
grammar induction, based on the transform and join
operators, which we will reuse in larger networks.
5.1 Fork/Join (FJ)
Given a model that parses a base data set D0, the
fork/join subnetwork will output an adaptation of
that model for D. It could facilitate a grammar in-
duction process, e.g., by advancing it from smaller
7In our diagrams, lexicalized modules are shaded black.
1986
to larger ? or possibly more complex ? data sets.
We first fork off two variations of the incoming
model based on D0: (i) a filtered view, which fo-
cuses on cleaner, simpler data (transform #1); and
(ii) a symmetrized view that backs off to word asso-
ciations (transform #2). Next is grammar induction
over D. We optimize a full DBM instance starting
from the first fork, and bootstrap a reduced DBM0
from the second. Finally, the two new induced sets
of parse trees, for D, are merged (lexicalized join):
(10)
HL?DBMD
SLDBMD
SLDBM0D
C
F
S
D0
C1
C2
C?1
C?2
The idea here is to prepare for two scenarios: an
incoming grammar that is either good or bad for D.
If the model is good, DBM should be able to hang
on to it and make improvements. But if it is bad,
DBM could get stuck fitting noise, whereas DBM0
might be more likely to ramp up to a good alterna-
tive. Since we can?t know ahead of time which is the
true case, we pursue both optimization paths simul-
taneously and let a combiner later decide for us.
Note that the forks start (and end) optimizing with
soft EM. This is because soft EM integrates previ-
ously unseen tokens into new grammars better than
hard EM, as evidenced by our failed attempt to re-
produce the ?baby steps? strategy with Viterbi train-
ing (Spitkovsky et al, 2010, Figure 4). A combiner
then executes hard EM, and since outputs of trans-
forms are trees, the end-to-end process is a chain of
lateen alternations that starts and ends with hard EM.
We will use a ?grammar inductor? to represent
subnetworks that transition from Dlsplit to Dl+1split, by
taking transformed parse trees of inter-punctuation
fragments up to length l (base data set, D0) to ini-
tialize training over fragments up to length l + 1:
(11)C l+1
The FJ network instantiates a grammar inductor
with l = 14, thus training on inter-punctuation frag-
ments up to length 15, as in previous work, starting
from an empty set of counts, C = ?. Smoothing
causes initial parse trees to be chosen uniformly at
random, as suggested by Cohen and Smith (2010):
(12)? 15
5.2 Iterated Fork/Join (IFJ)
Our second network daisy-chains grammar induc-
tors, starting from the single-word inter-punctuation
fragments in D1split, then retraining on D2split, and so
forth, until finally stopping at D15split, as before:
(13)1 2 14 15
We diagrammed this system as not taking an input,
since the first inductor?s output is fully determined
by unique parse trees of single-token strings. This
iterative approach to optimization is akin to deter-
ministic annealing (Rose, 1998), and is patterned af-
ter ?baby steps? (Spitkovsky et al, 2009, ?4.2).
Unlike the basic FJ, where symmetrization was a
no-op (since there were no counts in C = ?), IFJ
makes use of symmetrizers ? e.g., in the third in-
ductor, whose input is based on strings with up to
two tokens. Although it should be easy to learn
words that go together from very short fragments,
extracting correct polarities of their relations could
be a challenge: to a large extent, outputs of early in-
ductors may be artifacts of how our generative mod-
els factor (see ?4.2) or how ties are broken in opti-
mization (Spitkovsky et al, 2012a, Appendix B). We
therefore expect symmetrization to be crucial in ear-
lier stages but to weaken any high quality grammars,
nearer the end; it will be up to combiners to handle
such phase transitions correctly (or gracefully).
5.3 Grounded Iterated Fork/Join (GIFJ)
So far, our networks have been either purely itera-
tive (IFJ) or static (FJ). These two approaches can
also be combined, by injecting FJ?s solutions into
IFJ?s more dynamic stream. Our new transition sub-
network will join outputs of grammar inductors that
either (i) continue a previous solution (as in IFJ); or
(ii) start over from scratch (?grounding? to an FJ):
(14)
HL?DBMDl+1split?
Cl Cl+1l+1
l+1
The full GIFJ network can then be obtained by un-
rolling the above template from l = 14 back to one.
1987
WSJ15split WSJ
15
simp
Instance Label Model hsents htrees DDA hsents htrees DDA TA Description
DBM 6.54 6.75 83.7 6.05 6.21 85.1 42.7 Supervised (MLE of WSJ45)
? = C ? 8.76 10.46 21.4 8.58 10.52 20.7 3.9 Random Projective Parses
SL(S(C)) = C2 DBM0 6.18 6.39 57.0 5.90 6.11 57.5 10.4 B
A
}
Unlexicalized
BaselinesSL(F (C)) = C1 DBM 5.89 5.99 62.2 5.79 5.90 60.9 12.0
H(C?2) = C?2 L-DBM 7.28 7.30 59.2 6.87 6.88 58.6 10.4
Fork/Join
?
?
?
?
?
Baseline
Combination
H(C?1) = C?1 L-DBM 7.07 7.08 62.3 6.72 6.73 60.8 12.0
C?1 + C?2 = C+ L-DBM 7.20 7.27 64.0 6.82 6.88 62.5 12.3
H(C+) = C?+ L-DBM 7.02 7.04 64.2 6.64 6.65 62.7 12.8
L-DBM 6.95 6.96 70.5 6.55 6.56 68.2 14.9 Iterated Fork/Join (IFJ)
L-DBM 6.91 6.92 71.4 6.52 6.52 69.2 15.6 Grounded Iterated Fork/Join
L-DBM 6.83 6.83 72.3 6.41 6.41 70.2 17.9 Grammar Transformer (GT)
L-DBM 6.92 6.93 71.9 6.53 6.53 69.8 16.7 IFJ
GT
}
w/Iterated
CombinersL-DBM 6.83 6.83 72.9 6.41 6.41 70.6 18.0
Table 1: Sentence string and parse tree cross-entropies (in bpt), and accuracies (DDA), on inter-punctuation fragments
up to length 15 (WSJ15split) and its subset of simple, complete sentences (WSJ15simp, with exact tree accuracies ? TA).
6 Performance of Basic Networks
We compared our three networks? performance on
their final training sets, WSJ15split (see Table 1, which
also tabulates results for a cleaner subset, WSJ15simp).
The first network starts from C = ?, helping us es-
tablish several straw-man baselines. Its empty ini-
tializer corresponds to guessing (projective) parse
trees uniformly at random, which has 21.4% accu-
racy and sentence string cross-entropy of 8.76bpt.
6.1 Fork/Join (FJ)
FJ?s symmetrizer yields random parses of WSJ14split,
which initialize training of DBM0. This baseline (B)
lowers cross-entropy to 6.18bpt and scores 57.0%.
FJ?s filter starts from parse trees of WSJ14simp only, and
trains up a full DBM. This choice makes a stronger
baseline (A), with 5.89bpt cross-entropy, at 62.2%.
The join operator uses counts from A and B, C1
and C2, to obtain parse trees whose own counts C?1
and C?2 initialize lexicalized training. From each C?i,
an optimizer arrives at C?i . Grammars corresponding
to these counts have higher cross-entropies, because
of vastly larger vocabularies, but also better accura-
cies: 59.2 and 62.3%. Their mixture C+ is a simple
sum of counts in C?1 and C?2 : it is not expected to be
an improvement but happens to be a good move, re-
sulting in a grammar with higher accuracy (64.0%),
though not better Viterbi cross-entropy (7.27 falls
between 7.08 and 7.30bpt) than both sources. The
combiner?s third alternative, a locally optimal C?+, is
then obtained by re-optimizing from C+. This so-
lution performs slightly better (64.2%) and will be
the local optimum returned by FJ?s join operator, be-
cause it attains the lowest cross-entropy (7.04bpt).
6.2 Iterated Fork/Join (IFJ)
IFJ?s iterative approach results in an improvement:
70.5% accuracy and 6.96bpt cross-entropy. To test
how much of this performance could be obtained by
a simpler iterated network, we experimented with
ablated systems that don?t fork or join, i.e., our clas-
sic ?baby steps? schema (chaining together 15 op-
timizers), using both DBM and DBM0, with and
without a transform in-between. However, all such
?linear? networks scored well below 50%. We con-
clude from these results that an ability to branch out
into different promising regions of a solution space,
and to merge solutions of varying quality into better
models, are important properties of FJ subnetworks.
6.3 Grounded Iterated Fork/Join (GIFJ)
Grounding improves GIFJ?s performance further, to
71.4% accuracy and 6.92bpt cross-entropy. This re-
sult shows that fresh perspectives from optimizers
that start over can make search efforts more fruitful.
7 Enhanced Subnetworks
Modularity and abstraction allow for compact repre-
sentations of complex systems. Another key benefit
is that individual components can be understood and
improved in isolation, as we will demonstrate next.
1988
7.1 An Iterative Combiner (IC)
Our basic combiner introduced a third option, C?+,
into a pool of candidate solutions, {C?1 , C?2}. This
new entry may not be a simple mixture of the orig-
inals, because of non-linear effects from applying L
to C?1 + C?2 , but could most likely still be improved.
Rather than stop at C?+, when it is better than both
originals, we could recombine it with a next best so-
lution, continuing until no further improvement is
made. Iterating can?t harm a given combiner?s cross-
entropy (e.g., it lowers FJ?s from 7.04 to 7.00bpt),
and its advantages can be realized more fully in the
larger networks (albeit without any end-to-end guar-
antees): upgrading all 15 combiners in IFJ would
improve performance (slightly) more than ground-
ing (71.5 vs. 71.4%), and lower cross-entropy (from
6.96 to 6.93bpt). But this approach is still a bit timid.
A more greedy way is to proceed so long as C?+
is not worse than both predecessors. We shall now
state our most general iterative combiner (IC) algo-
rithm: Start with a solution pool p = {C?i }ni=1. Next,
construct p? by adding C?+ = L(
?n
i=1 C
?
i ) to p and re-
moving the worst of n+ 1 candidates in the new set.
Finally, if p = p?, return the best of the solutions in p;
otherwise, repeat from p := p?. At n = 2, one could
think of taking L(C?1 + C?2 ) as performing a kind of
bisection search in some (strange) space. With these
new and improved combiners, the IFJ network per-
forms better: 71.9% (up from 70.5 ? see Table 1),
lowering cross-entropy (down from 6.96 to 6.93bpt).
We propose a distinguished notation for the ICs:
(15)
*C2
C1
7.2 A Grammar Transformer (GT)
The levels of our systems? performance at grammar
induction thus far suggest that the space of possible
networks (say, with up to k components) may itself
be worth exploring more thoroughly. We leave this
exercise to future work, ending with two relatively
straight-forward extensions for grounded systems.
Our static bootstrapping mechanism (?ground? of
GIFJ) can be improved by pretraining with simple
sentences first ? as in the curriculum for learning
DBM-1 (Spitkovsky et al, 2012b, ?7.1), but now
with a variable length cut-off l (much lower than the
original 45) ? instead of starting from ? directly:
(16)
SDBMDlsimp?
l+1
?
?
?
l
The output of this subnetwork can then be refined,
by reconciling it with a previous dynamic solution.
We perform a mini-join of a new ground?s counts
with Cl, using the filter transform (single steps of
lexicalized Viterbi training on clean, simple data),
ahead of the main join (over more training data):
(17)
HL?DBMDl+1splitCl Cl+1
l+1
F
l
This template can be unrolled, as before, to obtain
our last network (GT), which achieves 72.9% accu-
racy and 6.83bpt cross-entropy (slightly less accu-
rate with basic combiners, at 72.3% ? see Table 1).
8 Full Training and System Combination
All systems that we described so far stop training at
D15split. We will use a two-stage adaptor network to
transition their grammars to a full data set, D45:
(18)
HL?DBMD45split H
L?DBM
D45C
The first stage exposes grammar inducers to longer
inputs (inter-punctuation fragments with up to 45
tokens); the second stage, at last, reassembles text
snippets into actual sentences (also up to l = 45).8
After full training, our IFJ and GT systems parse
Section 23 of WSJ at 62.7 and 63.4% accuracy, bet-
ter than the previous state-of-the-art (61.2% ? see
Table 2). To test the generalized IC algorithm, we
merged our implementations of these three strong
grammar induction pipelines into a combined sys-
tem (CS). It scored highest: 64.4%.
(19)
HL?DBMD45(GT) #1
(IFJ) #2
#3
CS
The quality of bracketings corresponding to (non-
trivial) spans derived by heads of our dependency
structures is competitive with the state-of-the-art in
unsupervised constituent parsing. On the WSJ sen-
tences up to length 40 in Section 23, CS attains sim-
ilar F1-measure (54.2 vs. 54.6, with higher recall) to
8Note that smoothing in the final (unlexicalized) Viterbi step
masks the fact that model parts that could not be properly es-
timated in the first stage (e.g., probabilities of punctuation-
crossing arcs) are being initialized to uniform multinomials.
1989
System DDA (@10)
(Gimpel and Smith, 2012) 53.1 (64.3)
(Gillenwater et al, 2010) 53.3 (64.3)
(Bisk and Hockenmaier, 2012) 53.3 (71.5)
(Blunsom and Cohn, 2010) 55.7 (67.7)
(Tu and Honavar, 2012) 57.0 (71.4)
(Spitkovsky et al, 2011b) 58.4 (71.4)
(Spitkovsky et al, 2011c) 59.1 (71.4)
#3 (Spitkovsky et al, 2012a) 61.2 (71.4)
#2
w/Full Training
{
IFJ
GT
62.7 (70.3)
#1 63.4 (70.3)
#1 + #2 + #3 System Combination CS 64.4 (72.0)
Supervised DBM (also with loose decoding) 76.3 (85.4)
Table 2: Directed dependency accuracies (DDA) on Sec-
tion 23 of WSJ (all sentences and up to length ten) for
recent systems, our full networks (IFJ and GT), and three-
way combination (CS) with the previous state-of-the-art.
PRLG (Ponvert et al, 2011), which is the strongest
system of which we are aware (see Table 3).9
9 Multi-Lingual Evaluation
Last, we checked how our algorithms generalize out-
side English WSJ, by testing in 23 more set-ups: all
2006/7 CoNLL test sets (Buchholz and Marsi, 2006;
Nivre et al, 2007), spanning 19 languages. Most re-
cent work evaluates against this multi-lingual data,
with the unrealistic assumption of part-of-speech
tags. But since inducing high quality word clusters
for many languages would be beyond the scope of
our paper, here we too plugged in gold tags for word
categories (instead of unsupervised tags, as in ?3?8).
We compared to the two strongest systems we
knew:10 MZ (Marec?ek and ?Zabokrtsky?, 2012) and
SAJ (Spitkovsky et al, 2012b), which report average
accuracies of 40.0 and 42.9% for CoNLL data (see
Table 4). Our fully-trained IFJ and GT systems score
40.0 and 47.6%. As before, combining these net-
works with our own implementation of the best pre-
vious state-of-the-art system (SAJ) yields a further
improvement, increasing final accuracy to 48.6%.
9These numbers differ from Ponvert et al?s (2011, Table 6)
for the full Section 23 because we restricted their eval-ps.py
script to a maximum length of 40 words, in our evaluation, to
match other previous work: Golland et al?s (2012, Figure 1) for
CCM and LLCCM; Huang et al?s (2012, Table 2) for the rest.
10During review, another strong system (Marec?ek and Straka,
2013, scoring 48.7%) of possible interest to the reader came out,
exploiting prior knowledge of stopping probabilities (estimated
from large POS-tagged corpora, via reducibility principles).
System F1
Binary-Branching Upper Bound 85.7
Left-Branching Baseline 12.0
CCM (Klein and Manning, 2002) 33.7
Right-Branching Baseline 40.7
F-CCM (Huang et al, 2012) 45.1
HMM (Ponvert et al, 2011) 46.3
LLCCM (Golland et al, 2012) 47.6 P R
CCL (Seginer, 2007) 52.8 54.6 51.1
PRLG (Ponvert et al, 2011) 54.6 60.4 49.8
CS System Combination 54.2 55.6 52.8
Supervised DBM Skyline 59.3 65.7 54.1
Dependency-Based Upper Bound 87.2 100 77.3
Table 3: Harmonic mean (F1) of precision (P) and re-
call (R) for unlabeled constituent bracketings on Section
23 of WSJ (sentences up to length 40) for our combined
system (CS), recent state-of-the-art and the baselines.
10 Discussion
CoNLL training sets were intended for comparing
supervised systems, and aren?t all suitable for unsu-
pervised learning: 12 languages have under 10,000
sentences (with Arabic, Basque, Danish, Greek, Ital-
ian, Slovenian, Spanish and Turkish particularly
small), compared to WSJ?s nearly 50,000. In some
treebanks sentences are very short (e.g., Chinese and
Japanese, which appear to have been split on punc-
tuation), and in others extremely long (e.g., Arabic).
Even gold tags aren?t always helpful, as their num-
ber is rarely ideal for grammar induction (e.g., 42 vs.
200 for English). These factors contribute to high
variances of our (and previous) results (see Table 4).
Nevertheless, if we look at the more stable aver-
age accuracies, we see a positive trend as we move
from a simpler fully-trained system (IFJ, 40.0%),
to a more complex system (GT, 47.6%), to system
combination (CS, 48.6%). Grounding seems to be
more important for the CoNLL sets, possibly be-
cause of data sparsity or availability of gold tags.
11 Related Work
The surest way to avoid local optima is to craft
an objective that doesn?t have them. For example,
Wang et al (2008) demonstrated a convex train-
ing method for semi-supervised dependency pars-
ing; Lashkari and Golland (2008) introduced a con-
vex reformulation of likelihood functions for clus-
tering tasks; and Corlett and Penn (2010) designed
1990
Directed Dependency Accuracies (DDA) (@10)
CoNLL Data MZ SAJ IFJ GT CS
Arabic 2006 26.5 10.9 33.3 8.3 9.3 (30.2)
?7 27.9 44.9 26.1 25.6 26.8 (45.6)
Basque ?7 26.8 33.3 23.5 24.2 24.4 (32.8)
Bulgarian ?7 46.0 65.2 35.8 64.2 63.4 (69.1)
Catalan ?7 47.0 62.1 65.0 68.4 68.0 (79.2)
Chinese ?6 ? 63.2 56.0 55.8 58.4 (60.8)
?7 ? 57.0 49.0 48.6 52.5 (56.0)
Czech ?6 49.5 55.1 44.5 43.9 44.0 (52.3)
?7 48.0 54.2 42.9 24.5 34.3 (51.1)
Danish ?6 38.6 22.2 37.8 17.1 21.4 (29.8)
Dutch ?6 44.2 46.6 40.8 51.3 48.0 (48.7)
English ?7 49.2 29.6 39.3 57.6 58.2 (75.0)
German ?6 44.8 39.1 34.1 54.5 56.2 (71.2)
Greek ?6 20.2 26.9 23.7 45.0 45.4 (52.2)
Hungarian ?7 51.8 58.2 24.8 52.9 58.3 (67.6)
Italian ?7 43.3 40.7 56.8 31.1 34.9 (44.9)
Japanese ?6 50.8 22.7 32.6 63.7 63.0 (68.9)
Portuguese ?6 50.6 72.4 38.0 72.7 74.5 (81.1)
Slovenian ?6 18.1 35.2 42.1 50.8 50.9 (57.3)
Spanish ?6 51.9 28.2 57.0 61.7 61.4 (73.2)
Swedish ?6 48.2 50.7 46.6 48.6 49.7 (62.1)
Turkish ?6 ? 34.4 28.0 32.9 29.2 (33.2)
?7 15.7 44.8 42.1 41.7 37.9 (42.4)
Average: 40.0 42.9 40.0 47.6 48.6 (57.8)
Table 4: Blind evaluation on 2006/7 CoNLL test sets (all
sentences) for our full networks (IFJ and GT), previous
state-of-the-art systems of Spitkovsky et al (2012b) and
Marec?ek and ?Zabokrtsky? (2012), and three-way combi-
nation with SAJ (CS, including results up to length ten).
a search algorithm for encoding decipherment prob-
lems that guarantees to quickly converge on optimal
solutions. Convexity can be ideal for comparative
analyses, by eliminating dependence on initial con-
ditions. But for many NLP tasks, including grammar
induction, the most relevant known objective func-
tions are still riddled with local optima. Renewed ef-
forts to find exact solutions (Eisner, 2012; Gormley
and Eisner, 2013) may be a good fit for the smaller
and simpler, earlier stages of our iterative networks.
Multi-start methods (Solis and Wets, 1981) can
recover certain global extrema almost surely (i.e.,
with probability approaching one). Moreover, ran-
dom restarts via uniform probability measures can
be optimal, in a worst-case-analysis sense, with par-
allel processing sometimes leading to exponential
speed-ups (Hu et al, 1994). This approach is rarely
emphasized in NLP literature. For instance, Moore
and Quirk (2008) demonstrated consistent, substan-
tial gains from random restarts in statistical machine
translation (but also suggested better and faster re-
placements ? see below); Ravi and Knight (2009,
?5, Figure 8) found random restarts for EM to be
crucial in parts-of-speech disambiguation. However,
other reviews are few and generally negative (Kim
and Mooney, 2010; Martin-Brualla et al, 2010).
Iterated local search methods (Hoos and Stu?tzle,
2004; Johnson et al, 1988, inter alia) escape lo-
cal basins of attraction by perturbing candidate so-
lutions, without undoing all previous work. ?Large-
step? moves can come from jittering (Hinton and
Roweis, 2003), dithering (Price et al, 2005, Ch. 2)
or smoothing (Bhargava and Kondrak, 2009). Non-
improving ?sideways? moves offer substantial help
with hard satisfiability problems (Selman et al,
1992); and injecting non-random noise (Selman et
al., 1994), by introducing ?uphill? moves via mix-
tures of random walks and greedy search strate-
gies, does better than random noise alone or simu-
lated annealing (Kirkpatrick et al, 1983). In NLP,
Moore and Quirk?s (2008) random walks from pre-
vious local optima were faster than uniform sam-
pling and also increased BLEU scores; Elsner and
Schudy (2009) showed that local search can outper-
form greedy solutions for document clustering and
chat disentanglement tasks; and Mei et al (2001)
incorporated tabu search (Glover, 1989; Glover and
Laguna, 1993, Ch. 3) into HMM training for ASR.
Genetic algorithms are a fusion of what?s best in
local search and multi-start methods (Houck et al,
1996), exploiting a problem?s structure to combine
valid parts of any partial solutions (Holland, 1975;
Goldberg, 1989). Evolutionary heuristics proved
useful in the induction of phonotactics (Belz, 1998),
text planning (Mellish et al, 1998), factored mod-
eling of morphologically-rich languages (Duh and
Kirchhoff, 2004) and plot induction for story gener-
ation (McIntyre and Lapata, 2010). Multi-objective
genetic algorithms (Fonseca and Fleming, 1993) can
handle problems with equally important but con-
flicting criteria (Stadler, 1988), using Pareto-optimal
ensembles. They are especially well-suited to lan-
guage, which evolves under pressures from compet-
ing (e.g., speaker, listener and learner) constraints,
and have been used to model configurations of vow-
els and tone systems (Ke et al, 2003). Our transform
and join mechanisms also exhibit some features of
genetic search, and make use of competing objec-
1991
tives: good sets of parse trees must make sense both
lexicalized and with word categories, to rich and im-
poverished models of grammar, and for both long,
complex sentences and short, simple text fragments.
This selection of text filters is a specialized case
of more general ?data perturbation? techniques ?
even cycling over randomly chosen mini-batches
that partition a data set helps avoid some local op-
tima (Liang and Klein, 2009). Elidan et al (2002)
suggested how example-reweighing could cause ?in-
formed? changes, rather than arbitrary damage, to
a hypothesis. Their (adversarial) training scheme
guided learning toward improved generalizations,
robust against input fluctuations. Language learn-
ing has a rich history of reweighing data via (co-
operative) ?starting small? strategies (Elman, 1993),
beginning from simpler or more certain cases. This
family of techniques has met with success in semi-
supervised named entity classification (Collins and
Singer, 1999; Yarowsky, 1995),11 parts-of-speech
induction (Clark, 2000; 2003), and language model-
ing (Krueger and Dayan, 2009; Bengio et al, 2009),
in addition to unsupervised parsing (Spitkovsky et
al., 2009; Tu and Honavar, 2011; Cohn et al, 2011).
12 Conclusion
We proposed several simple algorithms for combin-
ing grammars and showed their usefulness in merg-
ing the outputs of iterative and static grammar in-
duction systems. Unlike conventional system com-
bination methods, e.g., in machine translation (Xiao
et al, 2010), ours do not require incoming mod-
els to be of similar quality to make improvements.
We exploited these properties of the combiners to
reconcile grammars induced by different views of
data (Blum and Mitchell, 1998). One such view re-
tains just the simple sentences, making it easier to
recognize root words. Another splits text into many
inter-punctuation fragments, helping learn word as-
sociations. The induced dependency trees can them-
selves also be viewed not only as directed structures
but also as skeleton parses, facilitating the recovery
of correct polarities for unlabeled dependency arcs.
By reusing templates, as in dynamic Bayesian
network (DBN) frameworks (Koller and Friedman,
11The so-called Yarowsky-cautious modification of the orig-
inal algorithm for unsupervised word-sense disambiguation.
2009, ?6.2.2), we managed to specify relatively
?deep? learning architectures without sacrificing
(too much) clarity or simplicity. On a still more
speculative note, we see two (admittedly, tenuous)
connections to human cognition. First, the benefits
of not normalizing probabilities, when symmetriz-
ing, might be related to human language process-
ing through the base-rate fallacy (Bar-Hillel, 1980;
Kahneman and Tversky, 1982) and the availability
heuristic (Chapman, 1967; Tversky and Kahneman,
1973), since people are notoriously bad at probabil-
ity (Attneave, 1953; Kahneman and Tversky, 1972;
Kahneman and Tversky, 1973). And second, inter-
mittent ?unlearning? ? though perhaps not of the
kind that takes place inside of our transforms ?
is an adaptation that can be essential to cognitive
development in general, as evidenced by neuronal
pruning in mammals (Craik and Bialystok, 2006;
Low and Cheng, 2006). ?Forgetful EM? strategies
that reset subsets of parameters may thus, possibly,
be no less relevant to unsupervised learning than is
?partial EM,? which only suppresses updates, other
EM variants (Neal and Hinton, 1999), or ?dropout
training? (Hinton et al, 2012; Wang and Manning,
2013), which is important in supervised settings.
Future parsing models, in grammar induction,
may benefit by modeling head-dependent relations
separately from direction. As frequently employed
in tasks like semantic role labeling (Carreras and
Ma`rquez, 2005) and relation extraction (Sun et al,
2011), it may be easier to first establish existence,
before trying to understand its nature. Other key
next steps may include exploring more intelligent
ways of combining systems (Surdeanu and Man-
ning, 2010; Petrov, 2010) and automating the op-
erator discovery process. Furthermore, we are opti-
mistic that both count transforms and model recom-
bination could be usefully incorporated into sam-
pling methods: although symmetrized models may
have higher cross-entropies, hence prone to rejection
in vanilla MCMC, they could work well as seeds
in multi-chain designs; existing algorithms, such as
MCMCMC (Geyer, 1991), which switch contents
of adjacent chains running at different temperatures,
may also benefit from introducing the option to com-
bine solutions, in addition to just swapping them.
1992
Acknowledgments
We thank Yun-Hsuan Sung, for early-stage discussions
on ways of extending ?baby steps,? Elias Ponvert, for
sharing all of the relevant experimental results and eval-
uation scripts from his work with Jason Baldridge and
Katrin Erk, and the anonymous reviewers, for their
helpful comments on the draft version of this paper.
Funded, in part, by Defense Advanced Research Projects
Agency (DARPA) Deep Exploration and Filtering of
Text (DEFT) Program, under Air Force Research Lab-
oratory (AFRL) prime contract no. FA8750-13-2-0040.
Any opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
DARPA, AFRL, or the US government. Once again, the
first author thanks Moofus.
References
H. Alshawi. 1996. Head automata for speech translation. In
ICSLP.
F. Attneave. 1953. Psychological probability as a function of
experienced frequency. Experimental Psychology, 46.
M. Bar-Hillel. 1980. The base-rate fallacy in probability judg-
ments. Acta Psychologica, 44.
A. Belz. 1998. Discovering phonotactic finite-state automata
by genetic search. In COLING-ACL.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009.
Curriculum learning. In ICML.
A. Bhargava and G. Kondrak. 2009. Multiple word alignment
with profile hidden Markov models. In NAACL-HLT: Stu-
dent Research and Doctoral Consortium.
Y. Bisk and J. Hockenmaier. 2012. Simple robust grammar
induction with combinatory categorial grammars. In AAAI.
A. Blum and T. Mitchell. 1998. Combining labeled and unla-
beled data with co-training. In COLT.
P. Blunsom and T. Cohn. 2010. Unsupervised induction of tree
substitution grammars for dependency parsing. In EMNLP.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In CoNLL.
X. Carreras and L. Ma`rquez. 2005. Introduction to the CoNLL-
2005 shared task: Semantic role labeling. In CoNLL.
L. J. Chapman. 1967. Illusory correlation in observational re-
port. Verbal Learning and Verbal Behavior, 6.
A. Clark. 2000. Inducing syntactic categories by context distri-
bution clustering. In CoNLL-LLL.
A. Clark. 2003. Combining distributional and morphological
information for part of speech induction. In EACL.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs:
Hardness results and competitiveness of uniform initializa-
tion. In ACL.
T. Cohn, P. Blunsom, and S. Goldwater. 2011. Inducing tree-
substitution grammars. JMLR.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In EMNLP.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
E. Corlett and G. Penn. 2010. An exact A? method for deci-
phering letter-substitution ciphers. In ACL.
F. I. M. Craik and E. Bialystok. 2006. Cognition through the
lifespan: mechanisms of change. TRENDS in Cognitive Sci-
ences, 10.
C. de Marcken. 1995. Lexical heads, phrase structure and the
induction of grammar. In WVLC.
K. Duh and K. Kirchhoff. 2004. Automatic learning of lan-
guage model structure. In COLING.
J. Eisner. 2012. Grammar induction: Beyond local search. In
ICGI.
G. Elidan, M. Ninio, N. Friedman, and D. Schuurmans. 2002.
Data perturbation for escaping local maxima in learning. In
AAAI.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
M. Elsner and W. Schudy. 2009. Bounding and comparing
methods for correlation clustering beyond ILP. In NAACL-
HLT: Integer Linear Programming for NLP.
C. M. Fonseca and P. J. Fleming. 1993. Genetic algorithms for
multiobjective optimization: Formulation, discussion and
generalization. In ICGA.
C. J. Geyer. 1991. Markov chain Monte Carlo maximum like-
lihood. In Interface Symposium.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and B. Taskar.
2010. Posterior sparsity in unsupervised dependency pars-
ing. Technical report, University of Pennsylvania.
K. Gimpel and N. A. Smith. 2012. Concavity and initialization
for unsupervised dependency parsing. In NAACL-HLT.
F. Glover and M. Laguna. 1993. Tabu search. In C. R.
Reeves, editor, Modern Heuristic Techniques for Combina-
torial Problems. Blackwell Scientific Publications.
F. Glover. 1989. Tabu search ? Part I. ORSA Journal on
Computing, 1.
D. E. Goldberg. 1989. Genetic Algorithms in Search, Opti-
mization & Machine Learning. Addison-Wesley.
D. Golland, J. DeNero, and J. Uszkoreit. 2012. A feature-
rich constituent context model for grammar induction. In
EMNLP-CoNLL.
M. R. Gormley and J. Eisner. 2013. Nonconvex global opti-
mization for latent-variable models. In ACL.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with richer con-
texts and smoothing. In NAACL-HLT.
G. Hinton and S. Roweis. 2003. Stochastic neighbor embed-
ding. In NIPS.
G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and
R. R. Salakhutdinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors. In ArXiv.
J. H. Holland. 1975. Adaptation in Natural and Artificial Sys-
tems: An Introductory Analysis with Applications to Biology,
Control, and Artificial Intelligence. University of Michigan
Press.
H. H. Hoos and T. Stu?tzle. 2004. Stochastic Local Search:
Foundations and Applications. Morgan Kaufmann.
1993
C. R. Houck, J. A. Joines, and M. G. Kay. 1996. Comparison
of genetic algorithms, random restart, and two-opt switching
for solving large location-allocation problems. Computers
& Operations Research, 23.
X. Hu, R. Shonkwiler, and M. C. Spruill. 1994. Random
restarts in global optimization. Technical report, GT.
Y. Huang, M. Zhang, and C. L. Tan. 2012. Improved con-
stituent context model with features. In PACLIC.
F. Jelinek and R. L. Mercer. 1980. Interpolated estimation
of Markov source parameters from sparse data. In Pattern
Recognition in Practice.
D. S. Johnson, C. H. Papadimitriou, and M. Yannakakis. 1988.
How easy is local search? Journal of Computer and System
Sciences, 37.
D. Kahneman and A. Tversky. 1972. Subjective probability: A
judgment of representativeness. Cognitive Psychology, 3.
D. Kahneman and A. Tversky. 1973. On the psychology of
prediction. Psychological Review, 80.
D. Kahneman and A. Tversky. 1982. Evidential impact of base
rates. In D. Kahneman, P. Slovic, and A. Tversky, editors,
Judgment under uncertainty: Heuristics and biases. Cam-
bridge University Press.
J. Ke, M. Ogura, and W. S.-Y. Wang. 2003. Optimization mod-
els of sound systems using genetic algorithms. Computa-
tional Linguistics, 29.
J. Kim and R. J. Mooney. 2010. Generative alignment and
semantic parsing for learning from ambiguous supervision.
In COLING.
S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi. 1983. Opti-
mization by simulated annealing. Science, 220.
D. Klein and C. D. Manning. 2002. A generative constituent-
context model for improved grammar induction. In ACL.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In ACL.
D. Koller and N. Friedman. 2009. Probabilistic Graphical
Models: Principles and Techniques. MIT Press.
K. A. Krueger and P. Dayan. 2009. Flexible shaping: How
learning in small steps helps. Cognition, 110.
D. Lashkari and P. Golland. 2008. Convex clustering with
exemplar-based models. In NIPS.
P. Liang and D. Klein. 2009. Online EM for unsupervised
models. In NAACL-HLT.
L. K. Low and H.-J. Cheng. 2006. Axon pruning: an essen-
tial step underlying the developmental plasticity of neuronal
connections. Royal Society of London Philosophical Trans-
actions Series B, 361.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. Marec?ek and M. Straka. 2013. Stop-probability estimates
computed on a large corpus improve unsupervised depen-
dency parsing. In ACL.
D. Marec?ek and Z. ?Zabokrtsky?. 2011. Gibbs sampling with
treeness constraint in unsupervised dependency parsing. In
ROBUS.
D. Marec?ek and Z. ?Zabokrtsky?. 2012. Exploiting reducibility
in unsupervised dependency parsing. In EMNLP-CoNLL.
R. Martin-Brualla, E. Alfonseca, M. Pasca, K. Hall, E. Robledo-
Arnuncio, and M. Ciaramita. 2010. Instance sense induction
from attribute sets. In COLING.
N. McIntyre and M. Lapata. 2010. Plot induction and evolu-
tionary search for story generation. In ACL.
X.-d. Mei, S.-h. Sun, J.-s. Pan, and T.-Y. Chen. 2001. Op-
timization of HMM by the tabu search algorithm. In RO-
CLING.
C. Mellish, A. Knott, J. Oberlander, and M. O?Donnell. 1998.
Experiments using stochastic search for text planning. In
INLG.
R. C. Moore and C. Quirk. 2008. Random restarts in min-
imum error rate training for statistical machine translation.
In COLING.
T. Naseem and R. Barzilay. 2011. Using semantic cues to learn
syntax. In AAAI.
R. M. Neal and G. E. Hinton. 1999. A view of the EM al-
gorithm that justifies incremental, sparse, and other variants.
In M. I. Jordan, editor, Learning in Graphical Models. MIT
Press.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In EMNLP-CoNLL.
M. A. Paskin. 2001a. Cubic-time parsing and learning algo-
rithms for grammatical bigram models. Technical report,
UCB.
M. A. Paskin. 2001b. Grammatical bigrams. In NIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
S. Petrov. 2010. Products of random latent variable grammars.
In NAACL-HLT.
E. Ponvert, J. Baldridge, and K. Erk. 2011. Simple unsuper-
vised grammar induction from raw text with cascaded finite
state models. In ACL-HLT.
K. V. Price, R. M. Storn, and J. A. Lampinen. 2005. Differ-
ential Evolution: A Practical Approach to Global Optimiza-
tion. Springer.
S. Ravi and K. Knight. 2009. Minimized models for unsuper-
vised part-of-speech tagging. In ACL-IJCNLP.
K. Rose. 1998. Deterministic annealing for clustering, com-
pression, classification, regression and related optmization
problems. Proceedings of the IEEE, 86.
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
ACL.
B. Selman, H. Levesque, and D. Mitchell. 1992. A new method
for solving hard satisfiability problems. In AAAI.
B. Selman, H. A. Kautz, and B. Cohen. 1994. Noise strategies
for improving local search. In AAAI.
F. J. Solis and R. J.-B. Wets. 1981. Minimization by random
search techniques. Mathematics of Operations Research, 6.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby
Steps: How ?Less is More? in unsupervised dependency
parsing. In GRLL.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning.
2010. Viterbi training improves unsupervised dependency
parsing. In CoNLL.
1994
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen
EM: Unsupervised training with multiple objectives, applied
to dependency grammar induction. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctu-
ation: Making a point in unsupervised dependency parsing.
In CoNLL.
V. I. Spitkovsky, A. X. Chang, H. Alshawi, and D. Jurafsky.
2011c. Unsupervised dependency parsing without gold part-
of-speech tags. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012a. Boot-
strapping dependency grammar inducers from incomplete
sentence fragments via austere models. In ICGI.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012b. Three
dependency-and-boundary models for grammar induction.
In EMNLP-CoNLL.
W. Stadler, editor. 1988. Multicriteria Optimization in Engi-
neering and in the Sciences. Plenum Press.
A. Sun, R. Grishman, and S. Sekine. 2011. Semi-supervised
relation extraction with large-scale word clustering. In ACL.
M. Surdeanu and C. D. Manning. 2010. Ensemble models for
dependency parsing: Cheap and good? In NAACL-HLT.
K. Tu and V. Honavar. 2011. On the utility of curricula in
unsupervised learning of probabilistic grammars. In IJCAI.
K. Tu and V. Honavar. 2012. Unambiguity regularization
for unsupervised learning of probabilistic grammars. In
EMNLP-CoNLL.
A. Tversky and D. Kahneman. 1973. Availability: A heuristic
for judging frequency and probability. Cognitive Psychol-
ogy, 5.
S. I. Wang and C. D. Manning. 2013. Fast dropout training. In
ICML.
Q. I. Wang, D. Schuurmans, and D. Lin. 2008. Semi-
supervised convex training for dependency parsing. In HLT-
ACL.
T. Xiao, J. Zhu, M. Zhu, and H. Wang. 2010. Boosting-based
system combination for machine translation. In ACL.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In ACL.
1995
Obituary
Charles J. Fillmore
Dan Jurafsky
Stanford University
Charles J. Fillmore died at his home in San Francisco on February 13, 2014, of brain
cancer. He was 84 years old. Fillmore was one of the world?s pre-eminent scholars of
lexical meaning and its relationship with context, grammar, corpora, and computation,
and his work had an enormous impact on computational linguistics. His early theoret-
ical work in the 1960s, 1970s, and 1980s on case grammar and then frame semantics
significantly influenced computational linguistics, AI, and knowledge representation.
More recent work in the last two decades on FrameNet, a computational lexicon and
annotated corpus, influenced corpus linguistics and computational lexicography, and
led to modern natural language understanding tasks like semantic role labeling.
Fillmore was born and raised in St. Paul, Minnesota, and studied linguistics at the
University of Minnesota. As an undergraduate he worked on a pre-computational Latin
corpus linguistics project, alphabetizing index cards and building concordances. During
his service in the Army in the early 1950s he was stationed for three years in Japan.
After his service he became the first US soldier to be discharged locally in Japan, and
stayed for three years studying Japanese. He supported himself by teaching English,
pioneering a way to make ends meet that afterwards became popular with generations
of young Americans abroad. In 1957 he moved back to the United States to attend
graduate school at the University of Michigan.
At Michigan, Fillmore worked on phonetics, phonology, and syntax, first in the
American Structuralist tradition of developing what were called ?discovery proce-
dures? for linguistic analysis, algorithms for inducing phones or parts of speech. Dis-
covery procedures were thought of as a methodological tool, a formal procedure that
linguists could apply to data to discover linguistic structure, for example inducing parts
of speech from the slots in ?sentence frames? informed by the distribution of surround-
ing words. Like many linguistic graduate students of the period, he also worked partly
on machine translation, and was interviewed at the time by Yehoshua Bar-Hillel, who
was touring US machine translation laboratories in preparation for his famous report
on the state of MT (Bar-Hillel 1960).
Early in his graduate career, however, Fillmore read Noam Chomsky?s Syntactic
Structures and became an immediate proponent of the new transformational grammar.
He graduated with his PhD in 1962 and moved to the linguistics department at Ohio
State University. In his early work there Fillmore developed a number of early formal
properties of generative grammar, such as the idea that rules would re-apply to repre-
sentations in iterative stages called cycles (Fillmore 1963), a formal mechanism that still
plays a role in modern theories of generative grammar.
doi:10.1162/COLI a 00201
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
But his greatest impact on computational linguistics came from the line of research
that began with his early work on case grammar (Fillmore 1966, 1968, 1971, 1977a).
Fillmore had become interested in argument structure by studying Lucien Tesnie`re?s
groundbreaking E?le?ments de Syntaxe Structurale (Tesnie`re 1959) in which the term
?dependency? was introduced and the foundations were laid for dependency grammar.
Like many transformational grammarians of the time, Fillmore began by trying to
capture the relationships between distinct formal patterns with systematically related
meanings; and he became interested in the different ways of expressing the object
and recipient of transfer in sentences like ?He gave a book to me? and ?He gave me
a book? (Fillmore 1962, 1965), a phenomenon that became known as dative movement.
He then expanded to the more general goal of representing how the participants in
an event are expressed syntactically, as in these two sentences about an event of
opening:
a. The janitor will open the door with this key
b. This key will open the door
Fillmore noticed that despite the differing syntactic structure, in both sentences key
plays the role of the instrument of the action and door the role of the object, patient,
or theme, and suggested that such abstract roles could constitute a shallow level of
meaning representation. Following Tesnie`re?s terminology, Fillmore first referred to
these argument roles as actants (Fillmore 1966) but quickly switched to the term case,
(see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent,
Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs
would be listed in the lexicon with their ?case frame?, the list of obligatory (or optional)
case arguments.
The idea that semantic roles could provide an intermediate level of semantic
representation that could help map from syntactic parse structures to deeper,
more fully-specified representations of meaning was quickly adopted in natural
language processing, and systems for extracting case frames were created for machine
translation (Wilks 1973), question-answering (Hendrix, Thompson, and Slocum 1973),
spoken-language understanding (Nash-Webber 1975), and dialogue systems (Bobrow
et al. 1977). General-purpose semantic role labelers were developed to map to case
representations via ATNs (Simmons 1973) or, from parse trees, by using dictionaries
with verb-specific case frames (Levin 1977; Marcus 1980). By 1977 case representation
was widely used and taught in natural language processing and artificial intelligence,
and was described as a standard component of natural language understanding in the
first edition of Winston?s (1977) textbook Artificial Intelligence.
In 1971 Fillmore joined the linguistics faculty at the University of California,
Berkeley, and by the mid-1970s he began to expand his ideas on case. He arrived at
a more general model of semantic representation, one that expressed the background
contexts or perspectives by which a word or a case role could be defined. He called this
new representation a frame, and later described the intuition as follows:
?The idea behind frame semantics is that speakers are aware of possibly quite complex
situation types, packages of connected expectations, that go by various names?frames,
schemas, scenarios, scripts, cultural narratives, memes?and the words in our language
are understood with such frames as their presupposed background.? (Fillmore 2012,
p. 712)
726
Jurafsky Obituary
He described the name as coming from ?the pre-transformationalist view of sentence
structure as consisting of a frame and a substitution list,? but the word frame seemed to
be in the air for a suite of related notions proposed at about the same time by Minsky
(1974), Hymes (1974), and Goffman (1974), as well as related notions with other names
like scripts (Schank and Abelson 1975) and schemata (Bobrow and Norman 1975) (see
Tannen [1979] for a comparison). Fillmore was also influenced by the semantic field
theorists and by a visit to the Yale AI lab where he took notice of the lists of slots and
fillers used by early information extraction systems like DeJong (1982) and Schank and
Abelson (1977).
Fillmore?s version of this new idea?more linguistic than other manifestations,
focusing on the way that words are associated with frames?was expressed in a series
of papers starting in the mid-1970?s (Fillmore 1975a, 1976, 1977b, 1982, 1985). His
motivating example was the Commercial Event frame, in which a seller sells goods
to a buyer, the buyer thus buying the goods that cost a certain amount by paying a
price charged by the seller. The definition of each of these verbs (buy, sell, cost, pay,
charge), is interrelated by virtue of their joint association with a single kind of event
or scenario. The meaning of each word draws in the entire frame, and by using (or
hearing) the word, a language user necessarily activates the entire frame. As Fillmore
put it:
If I tell you that I bought a new pair of shoes, you do not know where I bought them or
how much they cost, but you know, by virtue of the frame I have introduced into our
discourse, that there have got to be answers to those questions. (Fillmore 1976, p. 29)
Fillmore also emphasized the way that frames could represent perspectives on events,
such that verbs like sell or pay emphasize different aspects of the same event, or that the
differences between alternative senses of the same word might come from their drawing
on different frames. Fillmore?s linguistic interpretation of frames influenced work in
artificial intelligence on knowledge representation like KRL (Bobrow and Winograd
1977), and the perspective-taking aspect of frames had a strong influence on work on
framing in linguistics and politics (Lakoff 2010).
In 1988 Fillmore taught at the computational linguistics summer school in Pisa run
by the late Antonio Zampolli and met the lexicographer Beryl T. Atkins. The two began
a collaboration to produce a frame description for the verb risk based on corpus evidence
(Fillmore and Atkins 1992). This work, including an invited talk at ACL 1991 (Fillmore
and Atkins 1991), influenced the development of other projects in corpus-based lexical
semantics (Kipper, Dang, and Palmer 2000; Kipper et al. 2008).
Fillmore became interested in this idea that corpus linguistics, lexicography, and
lexical semantics could fruitfully be combined (Fillmore 1992) and when he officially
retired from UC Berkeley in 1995 he moved to the International Computer Science
Institute (ICSI) in Berkeley (although still teaching at UC Berkeley part-time) and began
work on the FrameNet project of computational corpus lexicography that combined his
early ideas on semantic roles with his later work on frames and his recent interest in
corpus lexicography.
The idea of FrameNet was to build a large set of frames, each of which consisted
of lists of constitutive roles or ?frame elements?: sets of words that evoke the frame,
grammatical information expressing how each frame element is realized in the sentence,
and semantic relations between frames and between frame elements. Corpora were
annotated with the evoking words, frames, and frame elements (Baker, Fillmore, and
Lowe 1998; Fillmore, Johnson, and Petruck 2003; Fillmore and Baker 2009).
727
Computational Linguistics Volume 40, Number 3
Over the next 20 years until his death, Fillmore and his students and colleagues,
especially under the direction of Collin Baker, proceeded to create the frames and hand-
annotate the corpora. This period of his career was a productive and enjoyable one for
Fillmore. In an interview for the ICSI Newsletter, he said
?The happiest time of my career has been here at ICSI, where FrameNet has made it
possible for me to work with a team of bright young people on a continuing basis
doing work that I?ll never lose interest in.?
The combination of rich linguistic annotation and corpus-based approach
instantiated in FrameNet, together with the PropBank semantic-role-labeled corpus
created soon afterwards by Martha Palmer and colleagues (Palmer, Kingsbury, and
Gildea 2005), led to a revival of automatic approaches to semantic role labeling, first
on FrameNet (Gildea and Jurafsky 2000) and then on PropBank data (Gildea and
Palmer 2002, inter alia). The problem first addressed in the 1970s by hand-written rules
was thus now generally recast as one of supervised machine learning. The resulting
plethora of systems for performing automatic semantic role labeling (see the surveys
in Palmer, Gildea, and Xue (2010) and Ma`rquez et al. (2008)) have been applied widely
to improve the state of the art in tasks across NLP such as question answering (Shen
and Lapata 2007; Surdeanu, Ciaramita, and Zaragoza 2011) and machine translation
(Liu and Gildea 2010; Lo et al. 2013). Fillmore?s FrameNet project also led to the
development of FrameNets for many other languages including Spanish, German,
Japanese, Portuguese, Italian, and Chinese. And in a perhaps appropriate return to the
discovery procedures that first inspired Fillmore, modern work has focused on ways
to induce semantic roles from corpora without role annotation (Swier and Stevenson
2004; Chambers and Jurafsky 2009, 2011; Lang and Lapata 2014).
In addition to his work in semantics, Fillmore had significant contributions to
syntax and pragmatics, including the influential Santa Cruz Lectures on Deixis (Fillmore
1975b) and a long-standing research project in developing Construction Grammar, a
theory?or perhaps more accurately family of theories?that represented a grammar
as a collection of constructions, pairings of meaning, and form (Fillmore, Kay, and
O?Connor 1988). He also contributed to the application of linguistics to other disciplines
including cognitive science, education, and law. Ackerman, Kay, and O?Connor (2014)
offer more discussion of these aspects of Fillmore?s work.
Fillmore was much honored during his career; he was a fellow of the American
Academy of Arts and Sciences, served as president of the Linguistic Society of America,
was awarded an honorary doctorate from the University of Chicago, had festschrifts
and conferences in his honor, received the ACL lifetime achievement award in 2012 (see
the text of his acceptance speech in Fillmore [2012]) and, together with Collin Baker,
the Antonio Zampolli Prize from ELRA in 2012. Nonetheless, he was unpretentious
(universally referred to even by his undergraduates as ?Chuck?), modest, embarrassed
by compliments, and generally referred to himself light-heartedly as an Ordinary Work-
ing Linguist. His Minnesota background (he was Norwegian on his mother?s side)
always led to Lake Wobegon comparisons, especially given his often bemused smile and
wry deadpan wit. His colleague George Lakoff tells the story: ?When he first came to
Berkeley in 1971, he encountered a culture defined by the then-commonplace expres-
sion, ?Let it all hang out.? His response was to wear a button saying, ?Tuck it all back in.??
Fillmore was also a favorite teacher and mentor who enjoyed working with what
he often capitalized as ?Young People?; and was deeply respected for his brilliance,
careful attention to detail, and encyclopedic knowledge of language, and universally
728
Jurafsky Obituary
beloved for his warmth, generosity, and patience. He is survived by his beloved wife
Lily Wong Fillmore, a retired Berkeley linguist and Education professor, their children
and grandchildren, and a wide community of fond former colleagues, students, and
collaborators, among whom I am proud to include myself.
References
Ackerman, Farrell, Paul Kay, and
Mary Catherine O?Connor. 2014.
Charles J. Fillmore. Language, 90(3).
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings of
ACL-COLING 1998, pages 86?90.
Bar-Hillel, Yehoshua. 1960. The present
status of automatic translation of
languages. Advances in computers,
1(1):91?163.
Bobrow, Daniel G., Ronald M. Kaplan,
Martin Kay, Donald A. Norman,
Henry Thompson, and Terry Winograd.
1977. GUS-1, a frame driven dialog
system. Artificial Intelligence,
8(2):155?173.
Bobrow, Daniel G. and Donald A. Norman.
1975. Some principles of memory
schemata. In Daniel G. Bobrow and
Allan Collins, editors, Representation
and Understanding. Academic Press.
Bobrow, Daniel G. and Terry Winograd.
1977. An overview of KRL: A knowledge
representation language. Cognitive Science,
1:3?46.
Chambers, Nathanael and Dan Jurafsky.
2009. Unsupervised learning of narrative
schemas and their participants. In ACL
IJCNLP 2009.
Chambers, Nathanael and Dan Jurafsky.
2011. Template-based information
extraction without the templates.
In Proceedings of ACL 2011.
DeJong, Gerald F. 1982. An overview
of the FRUMP system. In Wendy G.
Lehnert and Martin H. Ringle,
editors, Strategies for Natural Language
Processing. Lawrence Erlbaum,
pages 149?176.
Fillmore, Charles J. 1962. Indirect object
constructions in English and the ordering
of transformations. Technical Report
Report No. 1, The Ohio State University
Research Foundation Project on Linguistic
Analysis.
Fillmore, Charles J. 1963. The position of
embedding transformations in a grammar.
Word, 19(2):208?231.
Fillmore, Charles J. 1965. Indirect Object
Constructions in English and the Ordering
of Transformations. Mouton.
Fillmore, Charles J. 1966. A proposal
concerning English prepositions. In
Francis P. Dinneen, editor, 17th annual
Round Table., volume 17 of Monograph
Series on Language and Linguistics.
Georgetown University Press,
Washington D.C., pages 19?34.
Fillmore, Charles J. 1968. The case for
case. In Emmon W. Bach and Robert T.
Harms, editors, Universals in Linguistic
Theory. Holt, Rinehart & Winston,
pages 1?88.
Fillmore, Charles J. 1971. Some problems
for case grammar. In R. J. O?Brien, editor,
22nd annual Round Table. Linguistics:
developments of the sixties ? viewpoints of the
seventies, volume 24 of Monograph Series
on Language and Linguistics. Georgetown
University Press, Washington D.C.,
pages 35?56.
Fillmore, Charles J. 1975a. An alternative to
checklist theories of meaning. In BLS-75,
Berkeley, CA.
Fillmore, Charles J. 1975b. Lectures on deixis.
Indiana University Linguistics Club,
Bloomington, IN.
Fillmore, Charles J. 1976. Frame semantics
and the nature of language. In Annals
of the New York Academy of Sciences:
Conference on the Origin and Development
of Language and Speech, volume 280,
pages 20?32.
Fillmore, Charles J. 1977a. The case for case
reopened. In Peter Cole and Jerrold M.
Sadock, editors, Syntax and Semantics
Volume 8: Grammatical Relations.
Academic Press.
Fillmore, Charles J. 1977b. Scenes-and-frames
semantics. In Antonio Zampolli, editor,
Linguistic Structures Processing. North
Holland, pages 55?79.
Fillmore, Charles J. 1982. Frame semantics. In
Linguistics in the Morning Calm. Hanshin,
Seoul, pages 111?138. Linguistics Society
of Korea.
Fillmore, Charles J. 1985. Frames and the
semantics of understanding. Quaderni di
Semantica, VI(2):222?254.
Fillmore, Charles J. 1992. ?Corpus
linguistics? vs. ?computer-aided
armchair linguistics?. In Directions in
Corpus Linguistics: Proceedings from
a 1991 Nobel Symposium on Corpus
729
Computational Linguistics Volume 40, Number 3
Linguistics, pages 35?66. Mouton de
Gruyter.
Fillmore, Charles J. 2003. Valency and
semantic roles: the concept of deep
structure case. In Vilmos A?gel,
Ludwig M. Eichinger, Hans Werner
Eroms, Peter Hellwig, Hans Ju?rgen
Heringer, and Henning Lobin, editors,
Dependenz und Valenz: Ein internationales
Handbuch der zeitgeno?ssischen Forschung.
Walter de Gruyter, chapter 36,
pages 457?475.
Fillmore, Charles J. 2012. Encounters with
language. Computational Linguistics,
38(4):701?718.
Fillmore, Charles J. and Beryl T. Atkins.
1991. Word meaning: Starting where
the MRDs stop. Invited talk at
ACL 1991.
Fillmore, Charles J. and Beryl T. Atkins.
1992. Towards a frame-based lexicon: The
semantics of RISK and its neighbors.
In Adrienne Lehrer and E. Feder
Kittay, editors, Frames, Fields, and
Contrasts. Lawrence Erlbaum,
pages 75?102.
Fillmore, Charles J. and Collin F. Baker.
2009. A frames approach to semantic
analysis. In Bernd Heine and Heiko
Narrog, editors, The Oxford Handbook of
Linguistic Analysis. Oxford University
Press, pages 313?340.
Fillmore, Charles J., Christopher R.
Johnson, and Miriam R. L. Petruck.
2003. Background to FrameNet.
International journal of lexicography,
16(3):235?250.
Fillmore, Charles J., Paul Kay, and
Mary Catherine O?Connor. 1988.
Regularity and idiomaticity in
grammatical constructions: The
case of Let Alone. Language,
64(3):501?538.
Gildea, Daniel and Daniel Jurafsky.
2000. Automatic labeling of semantic
roles. In ACL-00, pages 512?520,
Hong Kong.
Gildea, Daniel and Martha Palmer. 2002. The
necessity of syntactic parsing for predicate
argument recognition. In ACL-02,
Philadelphia, PA.
Goffman, Erving. 1974. Frame analysis: An
essay on the organization of experience.
Harvard University Press.
Hendrix, Gary G., Craig W. Thompson,
and Jonathan Slocum. 1973. Language
processing via canonical verbs and
semantic models. In Proceedings of
IJCAI-73.
Hymes, Dell. 1974. Ways of speaking.
In Richard Bauman and Joel Sherzer,
editors, Explorations in the ethnography of
speaking. Cambridge University Press,
pages 433?451.
Kipper, Karin, Hoa T. Dang, and Martha
Palmer. 2000. Class-based construction
of a verb lexicon. In AAAI/IAAI,
pages 691?696.
Kipper, Karin, Anna Korhonen, Neville
Ryant, and Martha Palmer. 2008. A
large-scale classification of English
verbs. Language Resources and Evaluation,
42(1):21?40.
Lakoff, George. 2010. Moral politics: How
liberals and conservatives think. University
of Chicago Press.
Lang, Joel and Mirella Lapata. 2014.
Similarity-driven semantic role induction
via graph partitioning. Computational
Linguistics, 40(3):633?669.
Levin, Beth. 1977. Mapping sentences
to case frames. Technical Report 167,
MIT AI Laboratory. AI Working
Paper 143.
Liu, Ding and Daniel Gildea. 2010. Semantic
role features for machine translation.
In Proceedings of COLING 2010,
pages 716?724.
Lo, Chi-kiu, Karteek Addanki, Markus
Saers, and Dekai Wu. 2013. Improving
machine translation by training against
an automatic semantic frame based
evaluation metric. In Proceedings of
ACL 2013.
Marcus, Mitchell P. 1980. A Theory of
Syntactic Recognition for Natural Language.
MIT Press.
Ma`rquez, Llu??s, Xavier Carreras, Kenneth C
Litkowski, and Suzanne Stevenson. 2008.
Semantic role labeling: An introduction to
the special issue. Computational linguistics,
34(2):145?159.
Minsky, Marvin. 1974. A framework for
representing knowledge. Technical
Report 306, MIT AI Laboratory.
Memo 306.
Nash-Webber, Bonnie L. 1975. The role
of semantics in automatic speech
understanding. In Daniel G. Bobrow and
Allan Collins, editors, Representation
and Understanding. Academic Press,
pages 351?382.
Palmer, Martha, Daniel Gildea, and
Nianwen Xue. 2010. Semantic role
labeling. Synthesis Lectures on Human
Language Technologies, 3(1):1?103.
Palmer, Martha, Paul Kingsbury, and
Daniel Gildea. 2005. The proposition
730
Jurafsky Obituary
bank: An annotated corpus of semantic
roles. Computational Linguistics,
31(1):71?106.
Schank, Roger C. and Robert P. Abelson.
1975. Scripts, plans, and knowledge. In
Proceedings of IJCAI-75, pages 151?157.
Schank, Roger C. and Robert P. Abelson.
1977. Scripts, Plans, Goals and
Understanding. Lawrence Erlbaum.
Shen, Dan and Mirella Lapata. 2007.
Using semantic roles to improve
question answering. In EMNLP-CoNLL,
pages 12?21.
Simmons, Robert F. 1973. Semantic
networks: Their computation and use
for understanding English sentences.
In Roger C. Schank and Kenneth Mark
Colby, editors, Computer Models of Thought
and Language. W.H. Freeman and Co.,
pages 61?113.
Surdeanu, Mihai, Massimiliano Ciaramita,
and Hugo Zaragoza. 2011. Learning to
rank answers to non-factoid questions
from web collections. Computational
Linguistics, 37(2):351?383.
Swier, R. and S. Stevenson. 2004.
Unsupervised semantic role labelling.
In EMNLP 2004, pages 95?102.
Tannen, Deborah. 1979. What?s in a frame?
Surface evidence for underlying
expectations. In Roy Freedle, editor,
New Directions in Discourse Processing.
Ablex, pages 137?181.
Tesnie`re, Lucien. 1959. E?le?ments de Syntaxe
Structurale. Librairie C. Klincksieck,
Paris.
Wilks, Yorick. 1973. An artificial intelligence
approach to machine translation.
In Roger C. Schank and Kenneth Mark
Colby, editors, Computer Models of
Thought and Language. W.H. Freeman,
pages 114?151.
Winston, Patrick H. 1977. Artificial
Intelligence. Addison Wesley.
731

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 555?563,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
The Best Lexical Metric for
Phrase-Based Statistical MT System Optimization
Daniel Cer, Christopher D. Manning and Daniel Jurafsky
Stanford University
Stanford, CA 94305, USA
Abstract
Translation systems are generally trained to
optimize BLEU, but many alternative metrics
are available. We explore how optimizing
toward various automatic evaluation metrics
(BLEU, METEOR, NIST, TER) affects the re-
sulting model. We train a state-of-the-art MT
system using MERT on many parameteriza-
tions of each metric and evaluate the result-
ing models on the other metrics and also us-
ing human judges. In accordance with popular
wisdom, we find that it?s important to train on
the same metric used in testing. However, we
also find that training to a newer metric is only
useful to the extent that the MT model?s struc-
ture and features allow it to take advantage of
the metric. Contrasting with TER?s good cor-
relation with human judgments, we show that
people tend to prefer BLEU and NIST trained
models to those trained on edit distance based
metrics like TER or WER. Human prefer-
ences for METEOR trained models varies de-
pending on the source language. Since using
BLEU or NIST produces models that are more
robust to evaluation by other metrics and per-
form well in human judgments, we conclude
they are still the best choice for training.
1 Introduction
Since their introduction, automated measures of ma-
chine translation quality have played a critical role
in the development and evolution of SMT systems.
While such metrics were initially intended for eval-
uation, popular training methods such as minimum
error rate training (MERT) (Och, 2003) and mar-
gin infused relaxed algorithm (MIRA) (Crammer
and Singer, 2003; Watanabe et al, 2007; Chiang et
al., 2008) train translation models toward a specific
evaluation metric. This makes the quality of the re-
sulting model dependent on how accurately the au-
tomatic metric actually reflects human preferences.
The most popular metric for both comparing sys-
tems and tuning MT models has been BLEU. While
BLEU (Papineni et al, 2002) is relatively simple,
scoring translations according to their n-gram over-
lap with reference translations, it still achieves a rea-
sonable correlation with human judgments of trans-
lation quality. It is also robust enough to use for au-
tomatic optimization. However, BLEU does have a
number of shortcomings. It doesn?t penalize n-gram
scrambling (Callison-Burch et al, 2006), and since
it isn?t aware of synonymous words or phrases, it can
inappropriately penalize translations that use them.
Recently, there have been efforts to develop bet-
ter evaluation metrics. Metrics such as Translation
Edit Rate (TER) (Snover et al, 2006; Snover et al,
2009) and METEOR1 (Lavie and Denkowski, 2009)
perform a more sophisticated analysis of the trans-
lations being evaluated and the scores they produce
tend to achieve a better correlation with human judg-
ments than those produced by BLEU (Snover et al,
2009; Lavie and Denkowski, 2009; Przybocki et al,
2008; Snover et al, 2006).
Their better correlations suggest that we might
obtain higher quality translations by making use of
these new metrics when training our models. We ex-
pect that training on a specific metric will produce
the best performing model according to that met-
1METEOR: Metric for Evaluation of Translation with Ex-
plicit ORdering.
555
ric. Doing better on metrics that better reflect human
judgments seems to imply the translations produced
by the model would be preferred by human judges.
However, there are four potential problems. First,
some metrics could be susceptible to systematic ex-
ploitation by the training algorithm and result in
model translations that have a high score according
to the evaluation metric but that are of low qual-
ity.2 Second, other metrics may result in objective
functions that are harder to optimize. Third, some
may result in better generalization performance at
test time by not encouraging overfitting of the train-
ing data. Finally, as a practical concern, metrics used
for training cannot be too slow.
In this paper, we systematically explore these four
issues for the most popular metrics available to the
MT community. We examine how well models per-
form both on the metrics on which they were trained
and on the other alternative metrics. Multiple mod-
els are trained using each metric in order to deter-
mine the stability of the resulting models. Select
models are scored by human judges in order to deter-
mine how performance differences obtained by tun-
ing to different automated metrics relates to actual
human preferences.
The next sections introduce the metrics and our
training procedure. We follow with two sets of core
results, machine evaluation in section 5, and human
evaluation in section 6.
2 Evaluation Metrics
Designing good automated metrics for evaluating
machine translations is challenging due to the vari-
ety of acceptable translations for each foreign sen-
tence. Popular metrics produce scores primarily
based on matching sequences of words in the system
translation to those in one or more reference trans-
lations. The metrics primarily differ in how they ac-
count for reorderings and synonyms.
2.1 BLEU
BLEU (Papineni et al, 2002) uses the percentage
of n-grams found in machine translations that also
occur in the reference translations. These n-gram
precisions are calculated separately for different n-
2For example, BLEU computed without the brevity penalty
would likely result in models that have a strong preference for
generating pathologically short translations.
gram lengths and then combined using a geometric
mean. The score is then scaled by a brevity penalty
if the candidate translations are shorter than the ref-
erences, BP = min(1.0, e1?len(R)/len(T )). Equa-
tion 1 gives BLEU using n-grams up to lengthN for
a corpus of candidate translations T and reference
translations R. A variant of BLEU called the NIST
metric (Doddington, 2002) weights n-gram matches
by how informative they are.
BLEU:N =
(
N?
n=1
n-grams(T
?
R)
n-grams(T )
) 1
N
BP (1)
While easy to compute, BLEU has a number of
shortcomings. Since the order of matching n-grams
is ignored, n-grams in a translation can be randomly
rearranged around non-matching material or other
n-gram breaks without harming the score. BLEU
also does not explicitly check whether information
is missing from the candidate translations, as it only
examines what fraction of candidate translation n-
grams are in the references and not what fraction
of references n-grams are in the candidates (i.e.,
BLEU ignores n-gram recall). Finally, the metric
does not account for words and phrases that have
similar meanings.
2.2 METEOR
METEOR (Lavie and Denkowski, 2009) computes
a one-to-one alignment between matching words in
a candidate translation and a reference. If a word
matches multiple other words, preference is given to
the alignment that reorders the words the least, with
the amount of reordering measured by the number of
crossing alignments. Alignments are first generated
for exact matches between words. Additional align-
ments are created by repeatedly running the align-
ment procedure over unaligned words, first allowing
for matches between word stems, and then allow-
ing matches between words listed as synonyms in
WordNet. From the final alignment, the candidate
translation?s unigram precision and recall is calcu-
lated, P = matcheslength trans andR =
matches
length ref . These two
are then combined into a weighted harmonic mean
(2). To penalize reorderings, this value is then scaled
by a fragmentation penalty based on the number of
chunks the two sentences would need to be broken
556
into to allow them to be rearranged with no crossing
alignments, P?,? = 1 ? ?
(
chunks
matches
)?
.
F? =
PR
?P + (1 ? ?)R
(2)
METEOR?,?,? = F? ? P?,? (3)
Equation 3 gives the final METEOR score as the
product of the unigram harmonic mean, F?, and the
fragmentation penalty, P?,? . The free parameters ?,
?, and ? can be used to tune the metric to human
judgments on a specific language and variation of
the evaluation task (e.g., ranking candidate transla-
tions vs. reproducing judgments of translations ade-
quacy and fluency).
2.3 Translation Edit Rate
TER (Snover et al, 2006) searches for the shortest
sequence of edit operations needed to turn a can-
didate translation into one of the reference transla-
tions. The allowable edits are the insertion, dele-
tion, and substitution of individual words and swaps
of adjacent sequences of words. The swap opera-
tion differentiates TER from the simpler word error
rate (WER) metric (Nie?en et al, 2000), which only
makes use of insertions, deletions, and substitutions.
Swaps prevent phrase reorderings from being exces-
sively penalized. Once the shortest sequence of op-
erations is found,3 TER is calculated simply as the
number of required edits divided by the reference
translation length, or average reference translation
length when multiple are available (4).
TER =
min edits
avg ref length
(4)
TER-Plus (TERp) (Snover et al, 2009) extends
TER by allowing the cost of edit operations to be
tuned in order to maximize the metric?s agreement
with human judgments. TERp also introduces three
new edit opertions: word stem matches, WordNet
synonym matches, and multiword matches using a
table of scored paraphrases.
3Since swaps prevent TER from being calculated exactly us-
ing dynamic programming, a beam search is used and this can
overestimate the number of required edits.
3 MERT
MERT is the standard technique for obtaining a ma-
chine translation model fit to a specific evaluation
metric (Och, 2003). Learning such a model cannot
be done using gradient methods since the value of
the objective function only depends on the transla-
tion model?s argmax for each sentence in the tun-
ing set. Typically, this optimization is performed as
a series of line searches that examines the value of
the evaluation metric at critical points where a new
translation argmax becomes preferred by the model.
Since the model score assigned to each candidate
translation varies linearly with changes to the model
parameters, it is possible to efficiently find the global
minimum along any given search direction with only
O(n2) operations when n-best lists are used.
Using our implementation of MERT that allows
for pluggable optimization metrics, we tune mod-
els to BLEU:N for N = 1 . . . 5, TER, two con-
figurations of TERp, WER, several configurations
of METEOR, as well as additive combinations of
these metrics. The TERp configurations include
the default configuration of TERp and TERpA:
the configuration of TERp that was trained to
match human judgments for NIST Metrics MATR
(Matthew Snover and Schwartz, 2008; Przybocki et
al., 2008). For METEOR, we used the standard ME-
TEOR English parameters (? = 0.8, ? = 2.5, ? =
0.4), and the English parameters for the rankingME-
TEOR (? = 0.95, ? = 0.5, ? = 0.5),4 which
was tuned to maximize the metric?s correlation with
WMT-07 human ranking judgements (Agarwal and
Lavie, 2008). The default METEOR parameters fa-
vor longer translations than the other metrics, since
high ? values place much more weight on unigram
recall than precision. Since this may put models
tuned to METEOR at a disadvantage when being
evaluated by the other metrics, we also use a variant
of the standard English model and of ranking ME-
TEOR with ? set to 0.5, as this weights both recall
and precision equally.
For each iteration of MERT, 20 random restarts
were used in addition to the best performing point
discovered during earlier iterations of training.5
4Agarwal and Lavie (2008) report ? = 0.45, however the
0.8.2 release of METEOR uses ? = 0.5 for ranking English.
5This is not necessarily identical with the point returned by
the most recent MERT iteration, but rather can be any point
557
Since MERT is known to be sensitive to what restart
points are provided, we use the same series of ran-
dom restart points for each model. During each it-
eration of MERT, the random seed is based on the
MERT iteration number. Thus, while a different set
of random points is selected during each MERT iter-
ation, on any given iteration all models use the same
set of points. This prevents models from doing better
or worse just because they received different starting
points. However, it is still possible that certain ran-
dom starting points are better for some evaluation
metrics than others.
4 Experiments
Experiments were run using Phrasal (Cer et al,
2010), a left-to-right beam search decoder that
achieves a matching BLEU score to Moses (Koehn
et al, 2007) on a variety of data sets. During de-
coding we made use of a stack size of 100, set the
distortion limit to 6, and retrieved 20 translation op-
tions for each unique source phrase.
Using the selected metrics, we train both Chi-
nese to English and Arabic to English models.6 The
Chinese to English models are trained using NIST
MT02 and evaluated on NIST MT03. The Arabic
to English experiments use NIST MT06 for train-
ing and GALE dev07 for evaluation. The resulting
models are scored using all of the standalone metrics
used during training.
4.1 Arabic to English
Our Arabic to English system was based on a well
ranking 2009 NIST submission (Galley et al, 2009).
The phrase table was extracted using all of the al-
lowed resources for the constrained Arabic to En-
glish track. Word alignment was performed using
the Berkeley cross-EM aligner (Liang et al, 2006).
Phrases were extracted using the grow heuristic
(Koehn et al, 2003). However, we threw away all
phrases that have a P (e|f) < 0.0001 in order to re-
duce the size of the phrase table. From the aligned
data, we also extracted a hierarchical reordering
model that is similar to popular lexical reordering
models (Koehn et al, 2007) but that models swaps
containing more than just one phrase (Galley and
returned during an earlier iteration of MERT.
6Given the amount of time required to train a TERpAmodel,
we only present TERpA results for Chinese to English.
Manning, 2008). A 5-gram language model was cre-
ated with the SRI language modeling toolkit (Stol-
cke, 2002) using all of the English material from
the parallel data employed to train the phrase table
as well as Xinhua Chinese English Parallel News
(LDC2002E18).7 The resulting decoding model has
16 features that are optimized during MERT.
4.2 Chinese to English
For our Chinese to English system, our phrase ta-
ble was built using 1,140,693 sentence pairs sam-
pled from the GALE Y2 training data. The Chinese
sentences were word segmented using the 2008 ver-
sion of Stanford Chinese Word Segmenter (Chang et
al., 2008; Tseng et al, 2005). Phrases were extracted
by running GIZA++ (Och and Ney, 2003) in both
directions and then merging the alignments using
the grow-diag-final heuristic (Koehn et al, 2003).
From the merged alignments we also extracted a
bidirectional lexical reordering model conditioned
on the source and the target phrases (Koehn et al,
2007). A 5-gram language model was created with
the SRI language modeling toolkit (Stolcke, 2002)
and trained using the Gigaword corpus and English
sentences from the parallel data. The resulting de-
coding model has 14 features to be trained.
5 Results
As seen in tables 1 and 2, the evaluation metric we
use during training has a substantial impact on per-
formance as measured by the various other metrics.
There is a clear block structure where the best class
of metrics to train on is the same class that is used
during evaluation. Within this block structure, we
make three primary observations. First, the best
performing model according to any specific metric
configuration is usually not the model we trained to
that configuration. In the Chinese results, the model
trained on BLEU:3 scores 0.74 points higher on
BLEU:4 than the model actually trained to BLEU:4.
In fact, the BLEU:3 trained model outperforms all
other models on BLEU:N metrics. For the Arabic
results, training on NIST scores 0.27 points higher
7In order to run multiple experiments in parallel on the com-
puters available to us, the system we use for this work differs
from our NIST submission in that we remove the Google n-
gram language model. This results in a performance drop of
less than 1.0 BLEU point on our dev data.
558
Train\Eval BLEU:1 BLEU:2 BLEU:3 BLEU:4 BLEU:5 NIST TER TERp WER TERpA METR METR-r METR METR-r
? = 0.5 ? = 0.5
BLEU:1 75.98 55.39 40.41 29.64 21.60 11.94 78.07 78.71 68.28 73.63 41.98 59.63 42.46 60.02
BLEU:2 76.58 57.24 42.84 32.21 24.09 12.20 77.09 77.63 67.16 72.54 43.20 60.91 43.59 61.56
BLEU:3 76.74 57.46 43.13 32.52 24.44 12.22 76.53 77.07 66.81 72.01 42.94 60.57 43.40 60.88
BLEU:4 76.24 56.86 42.43 31.80 23.77 12.14 76.75 77.25 66.78 72.01 43.29 60.94 43.10 61.27
BLEU:5 76.39 57.14 42.93 32.38 24.33 12.40 75.42 75.77 65.86 70.29 43.02 61.22 43.57 61.43
NIST 76.41 56.86 42.34 31.67 23.57 12.38 75.20 75.72 65.78 70.11 43.11 61.04 43.78 61.84
TER 73.23 53.39 39.09 28.81 21.18 12.73 71.33 71.70 63.92 66.58 38.65 55.49 41.76 59.07
TERp 72.78 52.90 38.57 28.32 20.76 12.68 71.76 72.16 64.26 66.96 38.51 56.13 41.48 58.73
TERpA 71.79 51.58 37.36 27.23 19.80 12.54 72.26 72.56 64.58 67.30 37.86 55.10 41.16 58.04
WER 74.49 54.59 40.30 29.88 22.14 12.64 71.85 72.34 63.82 67.11 39.76 57.29 42.37 59.97
METR 73.33 54.35 40.28 30.04 22.39 11.53 84.74 85.30 71.49 79.47 44.68 62.14 42.99 60.73
METR-r 74.20 54.99 40.91 30.66 22.98 11.74 82.69 83.23 70.49 77.77 44.64 62.25 43.44 61.32
METR:0.5 76.36 56.75 42.48 31.98 24.00 12.44 74.94 75.32 66.09 70.14 42.75 60.98 43.86 61.38
METR-r:0.5 76.49 56.93 42.36 31.70 23.68 12.21 77.04 77.58 67.12 72.23 43.26 61.03 43.63 61.67
Combined Models
BLEU:4-TER 75.32 55.98 41.87 31.42 23.50 12.62 72.97 73.38 64.46 67.95 41.50 59.11 43.50 60.82
BLEU:4-2TERp 75.22 55.76 41.57 31.11 23.25 12.64 72.48 72.89 64.17 67.43 41.12 58.82 42.73 60.86
BLEU:4+2MTR 75.77 56.45 42.04 31.47 23.48 11.98 79.96 80.65 68.85 74.84 44.06 61.78 43.70 61.48
Table 1: Chinese to English test set performance on MT03 using models trained using MERT on MT02. In each column,
cells shaded blue are better than average and those shaded red are below average. The intensity of the shading indicates
the degree of deviation from average. For BLEU, NIST, and METEOR, higher is better. For edit distance metrics like
TER and WER, lower is better.
Train\Eval BLEU:1 BLEU:2 BLEU:3 BLEU:4 BLEU:5 NIST TER TERp WER METR METR-r METR METR-r
? = 0.5 ? = 0.5
BLEU:1 79.90 65.35 54.08 45.14 37.81 10.68 46.19 61.04 49.98 49.74 67.79 49.19 68.12
BLEU:2 80.03 65.84 54.70 45.80 38.47 10.75 45.74 60.63 49.24 50.02 68.00 49.71 68.27
BLEU:3 79.87 65.71 54.59 45.67 38.34 10.72 45.86 60.80 49.18 49.87 68.32 49.61 67.67
BLEU:4 80.39 66.14 54.99 46.05 38.70 10.82 45.25 59.83 48.69 49.65 68.13 49.66 67.92
BLEU:5 79.97 65.77 54.64 45.76 38.44 10.75 45.66 60.55 49.11 49.89 68.33 49.64 68.19
NIST 80.41 66.27 55.22 46.32 38.98 10.96 44.11 57.92 47.74 48.88 67.85 49.88 68.52
TER 79.69 65.52 54.44 45.55 38.23 10.75 43.36 56.12 47.11 47.90 66.49 49.55 68.12
TERp 79.27 65.11 54.13 45.35 38.12 10.75 43.36 55.92 47.14 47.83 66.34 49.43 67.94
WER 79.42 65.28 54.30 45.51 38.27 10.78 43.44 56.13 47.13 47.82 66.33 49.38 67.88
METR 75.52 60.94 49.84 41.17 34.12 9.93 52.81 70.08 55.72 50.92 68.55 48.47 66.89
METR-r 77.42 62.91 51.67 42.81 35.61 10.24 49.87 66.26 53.17 50.95 69.29 49.29 67.89
METR:0.5 79.69 65.14 53.94 45.03 37.72 10.72 45.80 60.44 49.34 49.78 68.31 49.23 67.72
METR-r:0.5 79.76 65.12 53.82 44.88 37.57 10.67 46.53 61.55 50.17 49.66 68.57 49.58 68.25
Combined Models
BLEU:4-TER 80.37 66.31 55.27 46.36 39.00 10.96 43.94 57.46 47.46 49.00 67.10 49.85 68.41
BLEU:4-2TERp 79.65 65.53 54.54 45.75 38.48 10.80 43.42 56.16 47.15 47.90 65.93 49.09 67.90
BLEU:4+2METR 79.43 64.97 53.75 44.87 37.58 10.63 46.74 62.03 50.35 50.42 68.92 49.70 68.37
Table 2: Arabic to English test set performance on dev07 using models trained using MERT on MT06. As above, in each
column, cells shaded blue are better than average and those shaded red are below average. The intensity of the shading
indicates the degree of deviation from average.
on BLEU:4 than training on BLEU:4, and outper-
forms all other models on BLEU:N metrics.
Second, the edit distance based metrics (WER,
TER, TERp, TERpA)8 seem to be nearly inter-
changeable. While the introduction of swaps al-
lows the scores produced by the TER metrics to
achieve better correlation with human judgments,
our models are apparently unable to exploit this dur-
ing training. This maybe due to the monotone na-
8In our implementation of multi-reference WER, we use the
length of the references that result in the lowest sentence level
WER to divide the edit costs. In contrast, TER divides by the
average reference length. This difference can sometimes result
in WER being lower than the corresponding TER. Also, as can
be seen in the Arabic to English results, TERp scores sometimes
differ dramatically from TER scores due to normalization and
tokenization differences (e.g., TERp removes punctuation prior
to scoring, while TER does not).
ture of the reference translations and the fact that
having multiple references reduces the need for re-
orderings. However, it is possible that differences
between training to WER and TER would become
more apparent using models that allow for longer
distance reorderings or that do a better job of cap-
turing what reorderings are acceptable.
Third, with the exception of BLEU:1, the perfor-
mance of the BLEU, NIST, and the METEOR ?=.5
models appears to be more robust across the other
evaluation metrics than the standardMETEOR,ME-
TEOR ranking, and edit distance based models
(WER, TER, TERp, an TERpA). The latter mod-
els tend to do quite well on metrics similar to what
they were trained on, while performing particularly
poorly on the other metrics. For example, on Chi-
nese, the TER and WER models perform very well
559
on other edit distance based metrics, while perform-
ing poorly on all the other metrics except NIST.
While less pronounced, the same trend is also seen
in the Arabic data. Interestingly enough, while the
TER, TERp and standard METEOR metrics achieve
good correlations with human judgments, models
trained to them are particularly mismatched in our
results. The edit distance models do terribly on ME-
TEOR and METEOR ranking, while METEOR and
METEOR ranking models do poorly on TER, TERp,
and TERpA.
Training Itr MERT Training Itr MERT
Metric Time Metric Time
BLEU:1 13 21:57 NIST 15 78:15
BLEU:2 15 32:40 TER 7 21:00
BLEU:3 19 45:08 TERp 9 19:19
BLEU:4 10 24:13 TERpA 8 393:16
BLEU:5 16 46:12 WER 13 33:53
BL:4-TR 9 21:07 BL:4-2TRp 8 22:03
METR 12 39:16 METR 0.5 18 42:04
METR R 12 47:19 METR R:0.5 13 25:44
Table 3: Chinese to English MERT iterations and training
times, given in hours:mins and excluding decoder time.
5.1 Other Results
On the training data, we see a similar block struc-
ture within the results, but there is a different pattern
among the top performers. The tables are omitted,
but we observe that, for Chinese, the BLEU:5 model
performs best on the training data according to all
higher order BLEU metrics (4-7). On Arabic, the
BLEU:6 model does best on the same higher order
BLEU metrics (4-7). By rewarding higher order n-
gram matches, these objectives actually find minima
that result in more 4-gram matches than the mod-
els optimized directly to BLEU:4. However, the fact
that this performance advantage disappears on the
evaluation data suggests these higher order models
also promote overfitting.
Models trained on additive metric blends tend
to smooth out performance differences between
the classes of metrics they contain. As expected,
weighting the metrics used in the additive blends re-
sults in models that perform slightly better on the
type of metric with the highest weight.
Table 3 reports training times for select Chinese
to English models. Training to TERpA is very com-
putationally expensive due to the implementation of
the paraphrase table. The TER family of metrics
tends to converge in fewer MERT iterations than
those trained on other metrics such as BLEU, ME-
TEOR or even WER. This suggests that the learning
objective provided by these metrics is either easier to
optimize or they more easily trap the search in local
minima.
5.2 Model Variance
One potential problem with interpreting the results
above is that learning with MERT is generally as-
sumed to be noisy, with different runs of the al-
gorithm possibly producing very different models.
We explore to what extent the results just presented
were affected by noise in the training procedure. We
perform multiple training runs using select evalua-
tion metrics and examining how consistent the re-
sulting models are. This also allows us to deter-
mine whether the metric used as a learning criteria
influences the stability of learning. For these experi-
ments, Chinese to English models are trained 5 times
using a different series of random starting points. As
before, 20 random restarts were used during each
MERT iteration.
In table 4, models trained to BLEU andMETEOR
are relatively stable, with the METEOR:0.5 trained
models being the most stable. The edit distance
models, WER and TERp, vary more across train-
ing runs, but still do not exceed the interesting cross
metric differences seen in table 1. The instability of
WER and TERp, with TERp models having a stan-
dard deviation of 1.3 in TERp and 2.5 in BLEU:4,
make them risky metrics to use for training.
6 Human Evaluation
The best evaluation metric to use during training is
the one that ultimately leads to the best translations
according to human judges. We perform a human
evaluation of select models using Amazon Mechan-
ical Turk, an online service for cheaply performing
simple tasks that require human intelligence. To use
the service, tasks are broken down into individual
units of work known as human intelligence tasks
(HITs). HITs are assigned a small amount of money
that is paid out to the workers that complete them.
For many natural language annotation tasks, includ-
ing machine translation evaluation, it is possible to
obtain annotations that are as good as those pro-
560
Train\Eval ? BLEU:1 BLEU:3 BLEU:4 BLEU:5 TERp WER METEOR METEOR:0.5
BLEU:1 0.17 0.56 0.59 0.59 0.36 0.58 0.42 0.24
BLEU:3 0.38 0.41 0.38 0.32 0.70 0.49 0.44 0.33
BLEU:4 0.27 0.29 0.29 0.27 0.67 0.50 0.41 0.29
BLEU:5 0.17 0.14 0.19 0.21 0.67 0.75 0.34 0.17
TERp 1.38 2.66 2.53 2.20 1.31 1.39 1.95 1.82
WER 0.62 1.37 1.37 1.25 1.31 1.21 1.10 1.01
METEOR 0.80 0.56 0.48 0.44 3.71 2.69 0.69 1.10
METEOR:0.5 0.32 0.11 0.09 0.11 0.23 0.12 0.07 0.11
Table 4: MERT model variation for Chinese to English. We train five models to each metric listed above. The
collection of models trained to a given metric is then evaluated using the other metrics. We report the resulting
standard devation for the collection on each of the metrics. The collection with the lowest varience is bolded.
Model Pair % Preferred p-value
Chinese
METR R vs. TERp 60.0 0.0028
BLEU:4 vs. TERp 57.5 0.02
NIST vs. TERp 55.0 0.089
NIST vs. TERpA 55.0 0.089
BLEU:4 vs. TERpA 54.5 0.11
BLEU:4 vs. METR R 54.5 0.11
METR:0.5 vs. METR 54.5 0.11
METR:0.5 vs. METR R 53.0 0.22
METR vs. BLEU:4 52.5 0.26
BLEU:4 vs. METR:0.5 52.5 0.26
METR vs. TERp 52.0 0.31
NIST vs. BLEU:4 52.0 0.31
BLEU:4 vs. METR R:0.5 51.5 0.36
WER vs. TERp 51.5 0.36
TERpA vs. TERp 50.5 0.47
Arabic
BLEU:4 vs. METR R 62.0 < 0.001
NIST vs. TERp 56.0 0.052
BLEU:4 vs. METR:0.5 55.5 0.069
BLEU:4 vs. METR 54.5 0.11
METR R:0.5 vs METR R 54.0 0.14
NIST vs. BLEU:4 51.5 0.36
WER vs. TERp 51.5 0.36
METR:0.5 vs METR 51.5 0.36
TERp vs. BLEU:4 51.0 0.42
BLEU:4 vs. METR R:0.5 50.5 0.47
Table 5: Select pairwise preference for models trained to
different evaluation metrics. For A vs. B, preferred indi-
cates how often A was preferred to B. We bold the better
training metric for statistically significant differences.
duced by experts by having multiple workers com-
plete each HIT and then combining their answers
(Snow et al, 2008; Callison-Burch, 2009).
We perform a pairwise comparison of the trans-
lations produced for the first 200 sentences of our
Chinese to English test data (MT03) and our Arabic
to English test data (dev07). The HITs consist of a
pair of machine translated sentences and a single hu-
man generated reference translation. The reference
is chosen at random from those available for each
sentence. Capitalization of the translated sentences
is restored using an HMM based truecaser (Lita et
al., 2003). Turkers are instructed to ?. . . select the
machine translation generated sentence that is eas-
iest to read and best conveys what is stated in the
reference?. Differences between the two machine
translations are emphasized by being underlined and
bold faced.9 The resulting HITs are made available
only to workers in the United States, as pilot experi-
ments indicated this results in more consistent pref-
erence judgments. Three preference judgments are
obtained for each pair of translations and are com-
bined using weighted majority vote.
As shown in table 5, in many cases the quality of
the translations produced by models trained to dif-
ferent metrics is remarkably similar. Training to the
simpler edit distance metric WER produces transla-
tions that are as good as those from models tuned to
the similar but more advanced TERp metric that al-
lows for swaps. Similarly, training to TERpA, which
makes use of both a paraphrase table and edit costs
9We emphasize relative differences between the two trans-
lations rather than the difference between each translation and
the reference in order to avoid biasing evaluations toward edit
distance metrics.
561
tuned to human judgments, is no better than TERp.
For the Chinese to English results, there is a sta-
tistically significant human preference for transla-
tions that are produced by training to BLEU:4 and
a marginally significant preferences for training to
NIST over the default configuration of TERp. This
contrasts sharply with earlier work showing that
TER and TERp correlate better with human judge-
ments than BLEU (Snover et al, 2009; Przybocki
et al, 2008; Snover et al, 2006). While it is as-
sumed that, by using MERT, ?improved evaluation
measures lead directly to improved machine trans-
lation quality? (Och, 2003), these results show im-
proved correlations with human judgments are not
always sufficient to establish that tuning to a metric
will result in higher quality translations. In the Ara-
bic results, we see a similar pattern where NIST is
preferred to TERp, again with marginal signficance.
Strangely, however, there is no real difference be-
tween TERp vs. BLEU:4.
For Arabic, training to rankingMETEOR is worse
than BLEU:4, with the differences being very sig-
nificant. The Arabic results also trend toward sug-
gesting that BLEU:4 is better than either standard
METEOR and METEOR ? 0.5. However, for the
Chinese models, training to standard METEOR and
METEOR ? 0.5 is about as good as training to
BLEU:4. In both the Chinese and Arabic results, the
METEOR ? 0.5 models are at least as good as those
trained to standard METEOR and METEOR rank-
ing. In contrast to the cross evaluation metric results,
where the differences between the ? 0.5 models and
the standard METEOR models were always fairly
dramatic, the human preferences suggest there is of-
ten not much of a difference in the true quality of the
translations produced by these models.
7 Conclusion
Training to different evaluation metrics follows the
expected pattern whereby models perform best on
the same type of metric used to train them. How-
ever, models trained using the n-gram based metrics,
BLEU and NIST, are more robust to being evaluated
using the other metrics.
Edit distance models tend to do poorly when eval-
uated on other metrics, as do models trained using
METEOR. However, training models to METEOR
can be made more robust by setting ? to 0.5, which
balances the importance the metric assigns to preci-
sion and recall.
The fact that the WER, TER and TERp models
perform very similarly suggests that current phrase-
based translation systems lack either the features or
the model structure to take advantage of swap edit
operations. The situation might be improved by us-
ing a model that does a better job of both captur-
ing the structure of the source and target sentences
and their allowable reorderings, such as a syntac-
tic tree-to-string system that uses contextually rich
rewrite rules (Galley et al, 2006), or by making use
of larger more fine grained feature sets (Chiang et
al., 2009) that allow for better discrimination be-
tween hypotheses.
Human results indicate that edit distance trained
models such as WER and TERp tend to pro-
duce lower quality translations than BLEU or NIST
trained models. Tuning to METEOR works reason-
ably well for Chinese, but is not a good choice for
Arabic. We suspect that the newer RYPT metric
(Zaidan and Callison-Burch, 2009), which directly
makes use of human adequacy judgements of sub-
strings, would obtain better human results than the
automated metrics presented here. However, like
other metrics, we expect performance gains still will
be sensitive to how the mechanics of the metric inter-
act with the structure and feature set of the decoding
model being used.
BLEU and NIST?s strong showing in both the ma-
chine and human evaluation results indicates that
they are still the best general choice for training
model parameters. We emphasize that improved
metric correlations with human judgments do not
imply that models trained to a metric will result in
higher quality translations. We hope future work
on developing new evaluation metrics will explicitly
explore the translation quality of models trained to
them.
Acknowledgements
The authors thank Alon Lavie for suggesting set-
ting ? to 0.5 when training to METEOR. This work
was supported by the Defense Advanced Research
Projects Agency through IBM. The content does
not necessarily reflect the views of the U.S. Gov-
ernment, and no official endorsement should be in-
ferred.
562
References
Abhaya Agarwal and Alon Lavie. 2008. METEOR,
M-BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In StatMT workshop at ACL.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in ma-
chine translation research. In EACL.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP.
Daniel Cer, Michel Galley, Christopher D. Manning, and
Dan Jurafsky. 2010. Phrasal: A statistical machine
translation toolkit for exploring new model features.
In NAACL.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In StatMT
workshop at ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. JMLR,
3:951?991.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In HLT.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL.
Michel Galley, Spence Green, Daniel Cer, Pi-Chuan
Chang, and Christopher D. Manning. 2009. Stanford
university?s arabic-to-english statistical machine trans-
lation system for the 2009 NIST evaluation. In NIST
Open Machine Translation Evaluation Meeting.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In NAACL.
Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and
Nanda Kambhatla. 2003. tRuEcasIng. In ACL.
Bonnie Dorr Matthew Snover, Nitin Madnani and
Richard Schwartz. 2008. TERp system description.
In MetricsMATR workshop at AMTA.
Sonja Nie?en, Franz Josef Och, and Hermann Ney. 2000.
An evaluation tool for machine translation: Fast eval-
uation for MT research. In LREC.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL.
M. Przybocki, K. Peterson, and S. Bronsart. 2008.
Official results of the ?Metrics for MAchine TRans-
lation? Challenge (MetricsMATR08). Techni-
cal report, NIST, http://nist.gov/speech/
tests/metricsmatr/2008/results/.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER?: exploring different human judgments with a
tunable MT metric. In StatMT workshop at EACL).
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good? Eval-
uating non-expert annotations for natural language
tasks. In EMNLP.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In ICSLP.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher D. Manning. 2005. A con-
ditional random field word segmenter. In SIGHAN.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In EMNLP-CoNLL.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
bility of human-in-the-loop minimum error rate train-
ing. In EMNLP, pages 52?61, August.
563
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 751?759,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
From Baby Steps to Leapfrog: How ?Less is More?
in Unsupervised Dependency Parsing?
Valentin I. Spitkovsky
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc., Mountain View, CA, 94043
hiyan@google.com
Daniel Jurafsky
Stanford University, Stanford, CA, 94305
jurafsky@stanford.edu
Abstract
We present three approaches for unsupervised
grammar induction that are sensitive to data
complexity and apply them to Klein and Man-
ning?s Dependency Model with Valence. The
first, Baby Steps, bootstraps itself via iterated
learning of increasingly longer sentences and
requires no initialization. This method sub-
stantially exceeds Klein and Manning?s pub-
lished scores and achieves 39.4% accuracy on
Section 23 (all sentences) of the Wall Street
Journal corpus. The second, Less is More,
uses a low-complexity subset of the avail-
able data: sentences up to length 15. Focus-
ing on fewer but simpler examples trades off
quantity against ambiguity; it attains 44.1%
accuracy, using the standard linguistically-
informed prior and batch training, beating
state-of-the-art. Leapfrog, our third heuristic,
combines Less is More with Baby Steps by
mixing their models of shorter sentences, then
rapidly ramping up exposure to the full train-
ing set, driving up accuracy to 45.0%. These
trends generalize to the Brown corpus; aware-
ness of data complexity may improve other
parsing models and unsupervised algorithms.
1 Introduction
Unsupervised learning of hierarchical syntactic
structure from free-form natural language text is a
hard problem whose eventual solution promises to
benefit applications ranging from question answer-
ing to speech recognition and machine translation.
A restricted version that targets dependencies and
?Partially funded by NSF award IIS-0811974; first author
supported by the Fannie & John Hertz Foundation Fellowship.
assumes partial annotation, e.g., sentence bound-
aries, tokenization and typically even part-of-speech
(POS) tagging, has received much attention, elicit-
ing a diverse array of techniques (Smith and Eis-
ner, 2005; Seginer, 2007; Cohen et al, 2008). Klein
and Manning?s (2004) Dependency Model with Va-
lence (DMV) was the first to beat a simple parsing
heuristic ? the right-branching baseline. Today?s
state-of-the-art systems (Headden et al, 2009; Co-
hen and Smith, 2009) are still rooted in the DMV.
Despite recent advances, unsupervised parsers lag
far behind their supervised counterparts. Although
large amounts of unlabeled data are known to im-
prove semi-supervised parsing (Suzuki et al, 2009),
the best unsupervised systems use less data than is
available for supervised training, relying on complex
models instead: Headden et al?s (2009) Extended
Valence Grammar (EVG) combats data sparsity with
smoothing alone, training on the same small subset
of the tree-bank as the classic implementation of the
DMV; Cohen and Smith (2009) use more compli-
cated algorithms (variational EM and MBR decod-
ing) and stronger linguistic hints (tying related parts
of speech and syntactically similar bilingual data).
We explore what can be achieved through judi-
cious use of data and simple, scalable techniques.
Our first approach iterates over a series of training
sets that gradually increase in size and complex-
ity, forming an initialization-independent scaffold-
ing for learning a grammar. It works with Klein and
Manning?s simple model (the original DMV) and
training algorithm (classic EM) but eliminates their
crucial dependence on manually-tuned priors. The
second technique is consistent with the intuition that
learning is most successful within a band of the size-
complexity spectrum. Both could be applied to more
751
intricate models and advanced learning algorithms.
We combine them in a third, efficient hybrid method.
2 Intuition
Focusing on simple examples helps guide unsuper-
vised learning,1 as blindly added confusing data can
easily mislead training. We suggest that unless it is
increased gradually, unbridled, complexity can over-
whelm a system. How to grade an example?s diffi-
culty? The cardinality of its solution space presents
a natural proxy. In the case of parsing, the num-
ber of possible syntactic trees grows exponentially
with sentence length. For longer sentences, the un-
supervised optimization problem becomes severely
under-constrained, whereas for shorter sentences,
learning is tightly reined in by data. In the extreme
case of a single-word sentence, there is no choice
but to parse it correctly. At two words, a raw 50%
chance of telling the head from its dependent is still
high, but as length increases, the accuracy of even
educated guessing rapidly plummets. In model re-
estimation, long sentences amplify ambiguity and
pollute fractional counts with noise. At times, batch
systems are better off using less data.
Baby Steps: Global non-convex optimization is
hard. We propose a meta-heuristic that takes the
guesswork out of initializing local search. Begin-
ning with an easy (convex) case, it slowly extends it
to the fully complex target task by taking tiny steps
in the problem space, trying not to stray far from
the relevant neighborhoods of the solution space. A
series of nested subsets of increasingly longer sen-
tences that culminates in the complete data set offers
a natural progression. Its base case ? sentences of
length one ? has a trivial solution that requires nei-
ther initialization nor search yet reveals something
of sentence heads. The next step ? sentences of
length one and two ? refines initial impressions
of heads, introduces dependents, and exposes their
identities and relative positions. Although not rep-
resentative of the full grammar, short sentences cap-
ture enough information to paint most of the picture
needed by slightly longer sentences. They set up an
easier, incremental subsequent learning task. Step
k + 1 augments training input to include lengths
1It mirrors the effect that boosting hard examples has for
supervised training (Freund and Schapire, 1997).
1, 2, . . . , k, k + 1 of the full data set and executes
local search starting from the (smoothed) model es-
timated by step k. This truly is grammar induction.
Less is More: For standard batch training, just us-
ing simple, short sentences is not enough. They are
rare and do not reveal the full grammar. We find a
?sweet spot? ? sentence lengths that are neither too
long (excluding the truly daunting examples) nor too
few (supplying enough accessible information), us-
ing Baby Steps? learning curve as a guide. We train
where it flattens out, since remaining sentences con-
tribute little (incremental) educational value.2
Leapfrog: As an alternative to discarding data, a
better use of resources is to combine the results of
batch and iterative training up to the sweet spot data
gradation, then iterate with a large step size.
3 Related Work
Two types of scaffolding for guiding language learn-
ing debuted in Elman?s (1993) experiments with
?starting small?: data complexity (restricting input)
and model complexity (restricting memory). In both
cases, gradually increasing complexity allowed ar-
tificial neural networks to master a pseudo-natural
grammar they otherwise failed to learn. Initially-
limited capacity resembled maturational changes in
working memory and attention span that occur over
time in children (Kail, 1984), in line with the ?less
is more? proposal (Newport, 1988; 1990). Although
Rohde and Plaut (1999) failed to replicate this3 re-
sult with simple recurrent networks, other machine
learning techniques reliably benefit from scaffolded
model complexity on a variety of language tasks.
In word-alignment, Brown et al (1993) used IBM
Models 1-4 as ?stepping stones? to training Model 5.
Other prominent examples include ?coarse-to-fine?
2This is akin to McClosky et al?s (2006) ?Goldilocks effect.?
3Worse, they found that limiting input hindered language
acquisition. And making the grammar more English-like (by
introducing and strengthening semantic constraints), increased
the already significant advantage for ?starting large!? With it-
erative training invoking the optimizer multiple times, creating
extra opportunities to converge, Rohde and Plaut (1999) sus-
pected that Elman?s (1993) simulations simply did not allow
networks exposed exclusively to complex inputs sufficient train-
ing time. Our extremely generous, low termination threshold
for EM (see ?5.1) addresses this concern. However, given the
DMV?s purely syntactic POS tag-based approach (see ?5), it
would be prudent to re-test Baby Steps with a lexicalized model.
752
approaches to parsing, translation and speech recog-
nition (Charniak and Johnson, 2005; Charniak et al,
2006; Petrov et al, 2008; Petrov, 2009), and re-
cently unsupervised POS tagging (Ravi and Knight,
2009). Initial models tend to be particularly simple,4
and each refinement towards a full model introduces
only limited complexity, supporting incrementality.
Filtering complex data, the focus of our work,
is unconventional in natural language processing.
Such scaffolding qualifies as shaping ? a method
of instruction (routinely exploited in animal train-
ing) in which the teacher decomposes a complete
task into sub-components, providing an easier path
to learning. When Skinner (1938) coined the term,
he described it as a ?method of successive approx-
imations.? Ideas that gradually make a task more
difficult have been explored in robotics (typically,
for navigation), with reinforcement learning (Singh,
1992; Sanger, 1994; Saksida et al, 1997; Dorigo
and Colombetti, 1998; Savage, 1998; Savage, 2001).
Recently, Krueger and Dayan (2009) showed that
shaping speeds up language acquisition and leads
to better generalization in abstract neural networks.
Bengio et al (2009) confirmed this for deep de-
terministic and stochastic networks, using simple
multi-stage curriculum strategies. They conjectured
that a well-chosen sequence of training criteria ?
different sets of weights on the examples ? could
act as a continuation method (Allgower and Georg,
1990), helping find better local optima for non-
convex objectives. Elman?s learners constrained the
peaky solution space by focusing on just the right
data (simple sentences that introduced basic repre-
sentational categories) at just the right time (early
on, when their plasticity was greatest). Self-shaping,
they simplified tasks through deliberate omission (or
misunderstanding). Analogously, Baby Steps in-
duces an early structural locality bias (Smith and
Eisner, 2006), then relaxes it, as if annealing (Smith
and Eisner, 2004). Its curriculum of binary weights
initially discards complex examples responsible for
?high-frequency noise,? with earlier, ?smoothed?
objectives revealing more of the global picture.
There are important differences between our re-
sults and prior work. In contrast to Elman, we use a
4Brown et al?s (1993) Model 1 (and, similarly, the first baby
step) has a global optimum that can be computed exactly, so that
no initial or subsequent parameters depend on initialization.
large data set (WSJ) of real English. Unlike Bengio
et al and Krueger and Dayan, we shape a parser, not
a language model. Baby Steps is similar, in spirit, to
Smith and Eisner?s methods. Deterministic anneal-
ing (DA) shares nice properties with Baby Steps,
but performs worse than EM for (constituent) pars-
ing; Baby Steps handedly defeats standard training.
Structural annealing works well, but requires a hand-
tuned annealing schedule and direct manipulation of
the objective function; Baby Steps works ?out of the
box,? its locality biases a natural consequence of a
complexity/data-guided tour of optimization prob-
lems. Skewed DA incorporates a good initializer
by interpolating between two probability distribu-
tions, whereas our hybrid, Leapfrog, admits multi-
ple initializers by mixing structures instead. ?Less
is More? is novel and confirms the tacit consensus
implicit in training on small data sets (e.g., WSJ10).
4 Data Sets and Metrics
Klein and Manning (2004) both trained and tested
the DMV on the same customized subset (WSJ10)
of Penn English Treebank?s Wall Street Journal por-
tion (Marcus et al, 1993). Its 49,208 annotated
parse trees were pruned5 down to 7,422 sentences
of at most 10 terminals, spanning 35 unique POS
tags. Following standard practice, automatic ?head-
percolation? rules (Collins, 1999) were used to con-
vert the remaining trees into dependencies. Forced
to produce a single ?best? parse, their algorithm
was judged on accuracy: its directed score was the
fraction of correct dependencies; a more flattering6
undirected score was also used. We employ the
same metrics, emphasizing directed scores, and gen-
eralize WSJk to be the subset of pre-processed sen-
tences with at most k terminals. Our experiments fo-
cus on k ? {1, . . . , 45}, but we also test on WSJ100
and Section 23 of WSJ? (the entire WSJ), as well as
the held-out Brown100 (similarly derived from the
Brown corpus (Francis and Kucera, 1979)). See Fig-
ure 1 for these corpora?s sentence and token counts.
5Stripped of all empty sub-trees, punctuation, and terminals
(tagged # and $) not pronounced where they appear, those sen-
tences still containing more than ten tokens were thrown out.
6Ignoring polarity of parent-child relations partially ob-
scured effects of alternate analyses (systematic choices between
modals and main verbs for heads of sentences, determiners for
noun phrases, etc.) and facilitated comparison with prior work.
753
Corpus Sentences POS Tokens Corpus Sentences POS Tokens
WSJ1 159 159 WSJ13 12,270 110,760
WSJ2 499 839 WSJ14 14,095 136,310
WSJ3 876 1,970 WSJ15 15,922 163,715
WSJ4 1,394 4,042 WSJ20 25,523 336,555
WSJ5 2,008 7,112 WSJ25 34,431 540,895
WSJ6 2,745 11,534 WSJ30 41,227 730,099
WSJ7 3,623 17,680 WSJ35 45,191 860,053
WSJ8 4,730 26,536 WSJ40 47,385 942,801
WSJ9 5,938 37,408 WSJ45 48,418 986,830
WSJ10 7,422 52,248 WSJ100 49,206 1,028,054
WSJ11 8,856 68,022 Section 23 2,353 48,201
WSJ12 10,500 87,750 Brown100 24,208 391,796 5 10 15 20 25 30 35 40 45
5
10
15
20
25
30
35
40
45
Thousands
of Sentences
Thousands
of Tokens 100
200
300
400
500
600
700
800
900
WSJk
Figure 1: Sizes of WSJ{1, . . . , 45, 100}, Section 23 of WSJ? and Brown100.
NNS VBD IN NN ?
Payrolls fell in September .
P = (1 ?
0
z }| {
PSTOP(?, L, T)) ? PATTACH(?, L, VBD)
? (1 ? PSTOP(VBD, L, T)) ? PATTACH(VBD, L, NNS)
? (1 ? PSTOP(VBD, R, T)) ? PATTACH(VBD, R, IN)
? (1 ? PSTOP(IN, R, T)) ? PATTACH(IN, R, NN)
? PSTOP(VBD, L, F) ? PSTOP(VBD, R, F)
? PSTOP(NNS, L, T) ? PSTOP(NNS, R, T)
? PSTOP(IN, L, T) ? PSTOP(IN, R, F)
? PSTOP(NN, L, T) ? PSTOP(NN, R, T)
? PSTOP(?, L, F)
| {z }
1
? PSTOP(?, R, T)
| {z }
1
.
Figure 2: A simple dependency structure for a short sen-
tence and its probability, as factored by the DMV.
5 New Algorithms for the Classic Model
The DMV (Klein and Manning, 2004) is a single-
state head automata model (Alshawi, 1996) over lex-
ical word classes {cw} ? POS tags. Its generative
story for a sub-tree rooted at a head (of class ch) rests
on three types of independent decisions: (i) initial
direction dir ? {L, R} in which to attach children, via
probability PORDER(ch); (ii) whether to seal dir, stop-
ping with probability PSTOP(ch, dir, adj), conditioned
on adj ? {T, F} (true iff considering dir?s first, i.e.,
adjacent, child); and (iii) attachments (of class ca),
according to PATTACH(ch, dir, ca). This produces only
projective trees.7 A root token ? generates the head
of a sentence as its left (and only) child. Figure 2
displays an example that ignores (sums out) PORDER.
The DMV lends itself to unsupervised learn-
7Unlike spanning tree algorithms (McDonald et al, 2005),
DMV?s chart-based method disallows crossing dependencies.
ing via inside-outside re-estimation (Baker, 1979).
Klein and Manning did not use smoothing and
started with an ?ad-hoc harmonic? completion: aim-
ing for balanced trees, non-root heads attached de-
pendents in inverse proportion to (a constant plus)
their distance; ? generated heads uniformly at ran-
dom. This non-distributional heuristic created favor-
able initial conditions that nudged EM towards typi-
cal linguistic dependency structures.
5.1 Algorithm #0: Ad-Hoc?
? A Variation on Original Ad-Hoc Initialization
Since some of the important implementation details
are not available in the literature (Klein and Man-
ning, 2004; Klein, 2005), we had to improvise ini-
tialization and terminating conditions. We suspect
that our choices throughout this section do not match
Klein and Manning?s actual training of the DMV.
We use the following ad-hoc harmonic scores (for
all tokens other than ?): P?ORDER ? 1/2;
P?STOP ? (ds + ?s)?1 = (ds + 3)?1, ds ? 0;
P?ATTACH ? (da + ?a)?1 = (da + 2)?1, da ? 1.
Integers d{s,a} are distances from heads to stopping
boundaries and dependents.8 We initialize train-
ing by producing best-scoring parses of all input
sentences and converting them into proper proba-
bility distributions PSTOP and PATTACH via maximum-
likelihood estimation (a single step of Viterbi train-
ing (Brown et al, 1993)). Since left and right chil-
dren are independent, we drop PORDER altogether, mak-
8Constants ?{s,a} come from personal communication.
Note that ?s is one higher than is strictly necessary to avoid both
division by zero and determinism; ?a could have been safely ze-
roed out, since we never compute 1 ? PATTACH (see Figure 2).
754
ing ?headedness? deterministic. Our parser care-
fully randomizes tie-breaking, so that all parse trees
having the same score get an equal shot at being
selected (both during initialization and evaluation).
We terminate EM when a successive change in over-
all per-token cross-entropy drops below 2?20 bits.
5.2 Algorithm #1: Baby Steps
? An Initialization-Independent Scaffolding
We eliminate the need for initialization by first train-
ing on a trivial subset of the data ? WSJ1; this
works, since there is only one (the correct) way to
parse a single-token sentence. We plug the resulting
model into training on WSJ2 (sentences of up to two
tokens), and so forth, building up to WSJ45.9 This
algorithm is otherwise identical to Ad-Hoc?, with
the exception that it re-estimates each model using
Laplace smoothing, so that earlier solutions could
be passed to next levels, which sometimes contain
previously unseen dependent and head POS tags.
5.3 Algorithm #2: Less is More
? Ad-Hoc? where Baby Steps Flatlines
We jettison long, complex sentences and deploy Ad-
Hoc??s initializer and batch training at WSJk?? ? an
estimate of the sweet spot data gradation. To find
it, we track Baby Steps? successive models? cross-
entropies on the complete data set, WSJ45. An ini-
tial segment of rapid improvement is separated from
the final region of convergence by a knee ? points
of maximum curvature (see Figure 3). We use an
improved10 L method (Salvador and Chan, 2004) to
automatically locate this area of diminishing returns.
Specifically, we determine its end-points [k0, k?] by
minimizing squared error, estimating k?0 = 7 and
k?? = 15. Training at WSJ15 just misses the plateau.
5.4 Algorithm #3: Leapfrog
? A Practical and Efficient Hybrid Mixture
Cherry-picking the best features of ?Less is More?
and Baby Steps, we begin by combining their mod-
9Its 48,418 sentences (see Figure 1) cover 94.4% of all sen-
tences in WSJ; the longest of the missing 790 has length 171.
10Instead of iteratively fitting a two-segment form and adap-
tively discarding its tail, we use three line segments, applying
ordinary least squares to the first two, but requiring the third to
be horizontal and tangent to a minimum. The result is a batch
optimization routine that returns an interval for the knee, rather
than a point estimate (see Figure 3 for details).
5 10 15 20 25 30 35 40 45
3.0
3.5
4.0
4.5
5.0
WSJk
bpt
Cross-entropy h (in bits per token) on WSJ45
Knee
[7, 15] Tight, Flat, Asymptotic Bound
min
b0,m0,b1,m1
2<k0<k?<45
8
>>
>>
>
>
<
>
>
>
>>
>:
k0?1X
k=1
(hk ? b0 ? m0k)2 +
k?X
k=k0
(hk ? b1 ? m1k)2 +
45X
k=k?+1
?
hk ?
45
min
j=k?+1
hj
?2
Figure 3: Cross-entropy on WSJ45 after each baby step, a
piece-wise linear fit, and an estimated region for the knee.
els at WSJk??. Using one best parse from each,
for every sentence in WSJk??, the base case re-
estimates a new model from a mixture of twice the
normal number of trees; inductive steps leap over k??
lengths, conveniently ending at WSJ45, and estimate
their initial models by applying a previous solution
to a new input set. Both follow up the single step of
Viterbi training with at most five iterations of EM.
Our hybrid makes use of two good (condition-
ally) independent initialization strategies and exe-
cutes many iterations of EM where that is cheap ?
at shorter sentences (WSJ15 and below). It then in-
creases the step size, training just three more times
(at WSJ{15, 30, 45}) and allowing only a few (more
expensive) iterations of EM. Early termination im-
proves efficiency and regularizes these final models.
5.5 Reference Algorithms
? Baselines, a Skyline and Published Art
We carve out the problem space using two extreme
initialization strategies: (i) the uninformed uniform
prior, which serves as a fair ?zero-knowledge? base-
line for comparing uninitialized models; and (ii) the
maximum-likelihood ?oracle? prior, computed from
reference parses, which yields a skyline (a reverse
baseline) ? how well any algorithm that stumbled
on the true solution would fare at EM?s convergence.
In addition to citing Klein and Manning?s (2004)
results, we compare our accuracies on Section 23
of WSJ? to two state-of-the-art systems and past
baselines (see Table 2). Headden et al?s (2009)
lexicalized EVG is the best on short sentences, but
755
5 10 15 20 25 30 35 40
20
30
40
50
60
70
80
90
Oracle
Baby StepsAd-Hoc
Uninformed
WSJk
(a) Directed Accuracy (%) on WSJk
5 10 15 20 25 30 35 40 45
(b) Undirected Accuracy (%) on WSJk
Oracle
Baby Steps
Ad-Hoc
Uninformed
Figure 4: Directed and undirected accuracy scores attained by the DMV, when trained and tested on the same gradation
of WSJ, for several different initialization strategies. Green circles mark Klein and Manning?s (2004) published scores;
red, violet and blue curves represent the supervised (maximum-likelihood oracle) initialization, Baby Steps, and the
uninformed uniform prior. Dotted curves reflect starting performance, solid curves register performance at EM?s
convergence, and the arrows connecting them emphasize the impact of learning.
5 10 15 20 25 30 35 40 45
20
30
40
50
60
WSJk
Oracle
Leapfrog
Baby Steps
Ad-Hoc?
Uninformed
Ad-Hoc
Directed Accuracy (%) on WSJk
Figure 5: Directed accuracies for Ad-Hoc? (shown in
green) and Leapfrog (in gold); all else as in Figure 4(a).
its performance is unreported for longer sentences,
for which Cohen and Smith?s (2009) seem to be
the highest published scores; we include their in-
termediate results that preceded parameter-tying ?
Bayesian models with Dirichlet and log-normal pri-
ors, coupled with both Viterbi and minimum Bayes-
risk (MBR) decoding (Cohen et al, 2008).
6 Experimental Results
We packed thousands of empirical outcomes into the
space of several graphs (Figures 4, 5 and 6). The col-
ors (also in Tables 1 and 2) correspond to different
initialization strategies ? to a first approximation,
the learning algorithm was held constant (see ?5).
Figures 4 and 5 tell one part of our story. As data
sets increase in size, training algorithms gain access
to more information; however, since in this unsu-
pervised setting training and test sets are the same,
additional longer sentences make for substantially
more challenging evaluation. To control for these
dynamics, we applied Laplace smoothing to all (oth-
erwise unsmoothed) models and re-plotted their per-
formance, holding several test sets fixed, in Figure 6.
We report undirected accuracies parenthetically.
6.1 Result #1: Baby Steps
Figure 4 traces out performance on the training set.
Klein and Manning?s (2004) published scores ap-
pear as dots (Ad-Hoc) at WSJ10: 43.2% (63.7%).
Baby Steps achieves 53.0% (65.7%) by WSJ10;
trained and tested on WSJ45, it gets 39.7% (54.3%).
Uninformed, classic EM learns little about directed
dependencies: it improves only slightly, e.g., from
17.3% (34.2%) to 19.1% (46.5%) on WSJ45 (learn-
ing some of the structure, as evidenced by its undi-
rected scores), but degrades with shorter sentences,
where its initial guessing rate is high. In the case
of oracle training, we expected EM to walk away
from supervised solutions (Elworthy, 1994; Meri-
756
5 10 15 20 25 30 35 40
20
30
40
50
60
70
80
(a) Directed Accuracy (%) on WSJ10
WSJk
Oracle
Leapfrog
Baby Steps
Less is More
| {z }
Ad-Hoc?
Ad-Hoc
Uninformed
5 10 15 20 25 30 35 40 45
(b) Directed Accuracy (%) on WSJ40
Oracle
Leapfrog
Baby Steps
Less is More
| {z }
Ad-Hoc?
Uninformed
Figure 6: Directed accuracies attained by the DMV, when trained at various gradations of WSJ, smoothed, then tested
against fixed evaluation sets ? WSJ{10, 40}; graphs for WSJ{20, 30}, not shown, are qualitatively similar to WSJ40.
aldo, 1994; Liang and Klein, 2008), but the ex-
tent of its drops is alarming, e.g., from the super-
vised 69.8% (72.2%) to the skyline?s 50.6% (59.5%)
on WSJ45. In contrast, Baby Steps? scores usu-
ally do not change much from one step to the
next, and where its impact of learning is big (at
WSJ{4, 5, 14}), it is invariably positive.
6.2 Result #2: Less is More
Ad-Hoc??s curve (see Figure 5) suggests how Klein
and Manning?s Ad-Hoc initializer may have scaled
with different gradations of WSJ. Strangely, our im-
plementation performs significantly above their re-
ported numbers at WSJ10: 54.5% (68.3%) is even
slightly higher than Baby Steps; nevertheless, given
enough data (from WSJ22 onwards), Baby Steps
overtakes Ad-Hoc?, whose ability to learn takes a se-
rious dive once the inputs become sufficiently com-
plex (at WSJ23), and never recovers. Note that Ad-
Hoc??s biased prior peaks early (at WSJ6), eventu-
ally falls below the guessing rate (by WSJ24), yet
still remains well-positioned to climb, outperform-
ing uninformed learning.
Figure 6 shows that Baby Steps scales better with
more (complex) data ? its curves do not trend
downwards. However, a good initializer induces a
sweet spot at WSJ15, where the DMV is learned
best using Ad-Hoc?. This mode is ?Less is More,?
scoring 44.1% (58.9%) on WSJ45. Curiously, even
oracle training exhibits a bump at WSJ15: once sen-
tences get long enough (at WSJ36), its performance
degrades below that of oracle training with virtually
no supervision (at the hardly representative WSJ3).
6.3 Result #3: Leapfrog
Mixing Ad-Hoc? with Baby Steps at WSJ15 yields
a model whose performance initially falls between
its two parents but surpasses both with a little train-
ing (see Figure 5). Leaping to WSJ45, via WSJ30,
results in our strongest model: its 45.0% (58.4%) ac-
curacy bridges half of the gap between Baby Steps
and the skyline, and at a tiny fraction of the cost.
6.4 Result #4: Generalization
Our models carry over to the larger WSJ100, Section
23 of WSJ?, and the independent Brown100 (see
Table 1). Baby Steps improves out of domain, con-
firming that shaping generalizes well (Krueger and
Dayan, 2009; Bengio et al, 2009). Leapfrog does
best across the board but dips on Brown100, despite
its safe-guards against over-fitting.
Section 23 (see Table 2) reveals, unexpectedly,
that Baby Steps would have been state-of-the-art in
2008, whereas ?Less is More? outperforms all prior
work on longer sentences. Baby Steps is competi-
tive with log-normal families (Cohen et al, 2008),
scoring slightly better on longer sentences against
Viterbi decoding, though worse against MBR. ?Less
is More? beats state-of-the-art on longer sentences
by close to 2%; Leapfrog gains another 1%.
757
Ad-Hoc? Baby Steps Leapfrog Ad-Hoc? Baby Steps Leapfrog
Section 23 44.1 (58.8) 39.2 (53.8) 43.3 (55.7) 31.5 (51.6) 39.4 (54.0) 45.0 (58.4)
WSJ100 43.8 (58.6) 39.2 (53.8) 43.3 (55.6) @15 31.3 (51.5) 39.4 (54.1) 44.7 (58.1) @45
Brown100 43.3 (59.2) 42.3 (55.1) 42.8 (56.5) 32.0 (52.4) 42.5 (55.5) 43.6 (59.1)
Table 1: Directed and undirected accuracies on Section 23 of WSJ?, WSJ100 and Brown100 for Ad-Hoc?, Baby
Steps and Leapfrog, trained at WSJ15 and WSJ45.
Decoding WSJ10 WSJ20 WSJ?
Attach-Right (Klein and Manning, 2004) ? 38.4 33.4 31.7
DMV Ad-Hoc (Klein and Manning, 2004) Viterbi 45.8 39.1 34.2
Dirichlet (Cohen et al, 2008) Viterbi 45.9 39.4 34.9
Ad-Hoc (Cohen et al, 2008) MBR 46.1 39.9 35.9
Dirichlet (Cohen et al, 2008) MBR 46.1 40.6 36.9
Log-Normal Families (Cohen et al, 2008) Viterbi 59.3 45.1 39.0
Baby Steps (@15) Viterbi 55.5 44.3 39.2
Baby Steps (@45) Viterbi 55.1 44.4 39.4
Log-Normal Families (Cohen et al, 2008) MBR 59.4 45.9 40.5
Shared Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 61.3 47.4 41.4
Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 62.0 48.0 42.2
Less is More (Ad-Hoc? @15) Viterbi 56.2 48.2 44.1
Leapfrog (Hybrid @45) Viterbi 57.1 48.7 45.0
EVG Smoothed (skip-val) (Headden et al, 2009) Viterbi 62.1
Smoothed (skip-head) (Headden et al, 2009) Viterbi 65.0
Smoothed (skip-head), Lexicalized (Headden et al, 2009) Viterbi 68.8
Table 2: Directed accuracies on Section 23 of WSJ{10, 20,? } for several baselines and recent state-of-the-art systems.
7 Conclusion
We explored three simple ideas for unsupervised de-
pendency parsing. Pace Halevy et al (2009), we
find ?Less is More? ? the paradoxical result that
better performance can be attained by training with
less data, even when removing samples from the true
(test) distribution. Our small tweaks to Klein and
Manning?s approach of 2004 break through the 2009
state-of-the-art on longer sentences, when trained at
WSJ15 (the auto-detected sweet spot gradation).
The second, Baby Steps, is an elegant meta-
heuristic for optimizing non-convex training crite-
ria. It eliminates the need for linguistically-biased
manually-tuned initializers, particularly if the loca-
tion of the sweet spot is not known. This tech-
nique scales gracefully with more (complex) data
and should easily carry over to more powerful pars-
ing models and learning algorithms.
Finally, Leapfrog forgoes the elegance and metic-
ulousness of Baby Steps in favor of pragmatism.
Employing both good initialization strategies at
its disposal, and spending CPU cycles wisely, it
achieves better performance than both ?Less is
More? and Baby Steps.
Future work could explore unifying these tech-
niques with other state-of-the-art approaches. It may
be useful to scaffold on both data and model com-
plexity, e.g., by increasing head automata?s number
of states (Alshawi and Douglas, 2000). We see many
opportunities for improvement, considering the poor
performance of oracle training relative to the super-
vised state-of-the-art, and in turn the poor perfor-
mance of unsupervised state-of-the-art relative to the
oracle models.11 To this end, it would be instructive
to understand both the linguistic and statistical na-
ture of the sweet spot, and to test its universality.
Acknowledgments
We thank Angel X. Chang, Pi-Chuan Chang, David L.W. Hall,
Christopher D. Manning, David McClosky, Daniel Ramage and
the anonymous reviewers for many helpful comments on draft
versions of this paper.
References
E. L. Allgower and K. Georg. 1990. Numerical Continuation
Methods: An Introduction. Springer-Verlag.
11To facilitate future work, all of our models are publicly
available at http://cs.stanford.edu/?valentin/.
758
H. Alshawi and S. Douglas. 2000. Learning dependency trans-
duction models from unannotated examples. In Royal Soci-
ety of London Philosophical Transactions Series A, volume
358.
H. Alshawi. 1996. Head automata for speech translation. In
Proc. of ICSLP.
J. K. Baker. 1979. Trainable grammars for speech recognition.
In Speech Communication Papers for the 97th Meeting of the
Acoustical Society of America.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009.
Curriculum learning. In ICML.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguistics, 19.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In Proc. of ACL.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil, D. Ellis,
I. Haxton, C. Hill, R. Shrivaths, J. Moore, M. Pozar, and
T. Vu. 2006. Multilevel coarse-to-fine PCFG parsing. In
HLT-NAACL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic normal dis-
tributions for soft parameter tying in unsupervised grammar
induction. In Proc. of NAACL-HLT.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic nor-
mal priors for unsupervised probabilistic grammar induction.
In NIPS.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
M. Dorigo and M. Colombetti. 1998. Robot Shaping: An
Experiment in Behavior Engineering. MIT Press/Bradford
Books.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
D. Elworthy. 1994. Does Baum-Welch re-estimation help tag-
gers? In Proc. of ANLP.
W. N. Francis and H. Kucera, 1979. Manual of Information to
Accompany a Standard Corpus of Present-Day Edited Amer-
ican English, for use with Digital Computers. Department of
Linguistic, Brown University.
Y. Freund and R. E. Schapire. 1997. A decision-theoretic gen-
eralization of on-line learning and an application to boosting.
Journal of Computer and System Sciences, 55(1).
A. Halevy, P. Norvig, and F. Pereira. 2009. The unreasonable
effectiveness of data. IEEE Intelligent Systems, 24(2).
W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with richer con-
texts and smoothing. In Proc. of NAACL-HLT.
R. Kail. 1984. The development of memory in children. W. H.
Freeman and Company, 2nd edition.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In Proc. of ACL.
D. Klein. 2005. The Unsupervised Learning of Natural Lan-
guage Structure. Ph.D. thesis, Stanford University.
K. A. Krueger and P. Dayan. 2009. Flexible shaping: How
learning in small steps helps. Cognition, 110.
P. Liang and D. Klein. 2008. Analyzing the errors of unsuper-
vised learning. In Proc. of HLT-ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2).
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In Proc. of NAACL-HLT.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning tree al-
gorithms. In Proc. of HLT-EMNLP.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2):155?172.
E. L. Newport. 1988. Constraints on learning and their role in
language acquisition: Studies of the acquisition of American
Sign Language. Language Sciences, 10(1).
E. L. Newport. 1990. Maturational constraints on language
learning. Cognitive Science, 14(1).
S. Petrov, A. Haghighi, and D. Klein. 2008. Coarse-to-fine
syntactic machine translation using language projections. In
Proc. of EMNLP.
S. O. Petrov. 2009. Coarse-to-Fine Natural Language Process-
ing. Ph.D. thesis, University of California, Berkeley.
S. Ravi and K. Knight. 2009. Minimized models for unsuper-
vised part-of-speech tagging. In Proc. of ACL-IJCNLP.
D. L. T. Rohde and D. C. Plaut. 1999. Language acquisition in
the absence of explicit negative evidence: How important is
starting small? Cognition, 72(1).
L. M. Saksida, S. M. Raymond, and D. S. Touretzky. 1997.
Shaping robot behavior using principles from instrumental
conditioning. Robotics and Autonomous Systems, 22(3).
S. Salvador and P. Chan. 2004. Determining the number of
clusters/segments in hierarchical clustering/segmentation al-
gorithms. In Proc. of ICTAI.
T. D. Sanger. 1994. Neural network learning control of
robot manipulators using gradually increasing task difficulty.
IEEE Trans. on Robotics and Automation, 10.
T. Savage. 1998. Shaping: The link between rats and robots.
Connection Science, 10(3).
T. Savage. 2001. Shaping: A multiple contingencies analysis
and its relevance to behaviour-based robotics. Connection
Science, 13(3).
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
Proc. of ACL.
S. P. Singh. 1992. Transfer of learning by composing solutions
of elemental squential tasks. Machine Learning, 8.
B. F. Skinner. 1938. The behavior of organisms: An experi-
mental analysis. Appleton-Century-Crofts.
N. A. Smith and J. Eisner. 2004. Annealing techniques for
unsupervised statistical language learning. In Proc. of ACL.
N. A. Smith and J. Eisner. 2005. Guiding unsupervised gram-
mar induction using contrastive estimation. In Proc. of the
IJCAI Workshop on Grammatical Inference Applications.
N. A. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In Proc. of
COLING-ACL.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009. An
empirical study of semi-supervised structured conditional
models for dependency parsing. In Proc. of EMNLP.
759
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 9?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Phrasal: A Toolkit for Statistical Machine Translation
with Facilities for Extraction and Incorporation of Arbitrary Model Features
Daniel Cer, Michel Galley, Daniel Jurafsky and Christopher D. Manning
Stanford University
Stanford, CA 94305, USA
Abstract
We present a new Java-based open source
toolkit for phrase-based machine translation.
The key innovation provided by the toolkit
is to use APIs for integrating new fea-
tures (/knowledge sources) into the decod-
ing model and for extracting feature statis-
tics from aligned bitexts. The package in-
cludes a number of useful features written to
these APIs including features for hierarchi-
cal reordering, discriminatively trained linear
distortion, and syntax based language models.
Other useful utilities packaged with the toolkit
include: a conditional phrase extraction sys-
tem that builds a phrase table just for a spe-
cific dataset; and an implementation of MERT
that allows for pluggable evaluation metrics
for both training and evaluation with built in
support for a variety of metrics (e.g., TERp,
BLEU, METEOR).
1 Motivation
Progress in machine translation (MT) depends crit-
ically on the development of new and better model
features that allow translation systems to better iden-
tify and construct high quality machine translations.
The popular Moses decoder (Koehn et al, 2007)
was designed to allow new features to be defined us-
ing factored translation models. In such models, the
individual phrases being translated can be factored
into two or more abstract phrases (e.g., lemma, POS-
tags) that can be translated individually and then
combined in a seperate generation stage to arrive at
the final target translation. While greatly enriching
the space of models that can be used for phrase-
based machine translation, Moses only allows fea-
tures that can be defined at the level of individual
words and phrases.
The Phrasal toolkit provides easy-to-use APIs
for the development of arbitrary new model fea-
tures. It includes an API for extracting feature
statistics from aligned bitexts and for incor-
porating the new features into the decoding
model. The system has already been used to
develop a number of innovative new features
(Chang et al, 2009; Galley and Manning, 2008;
Galley and Manning, 2009; Green et al, 2010) and
to build translation systems that have placed well
at recent competitive evaluations, achieving second
place for Arabic to English translation on the NIST
2009 constrained data track.1
We implemented the toolkit in Java because it of-
fers a good balance between performance and de-
veloper productivity. Compared to C++, develop-
ers using Java are 30 to 200% faster, produce fewer
defects, and correct defects up to 6 times faster
(Phipps, 1999). While Java programs were histori-
cally much slower than similar programs written in
C or C++, modern Java virtual machines (JVMs) re-
sult in Java programs being nearly as fast as C++
programs (Bruckschlegel, 2005). Java also allows
for trivial code portability across different platforms.
In the remainder of the paper, we will highlight
various useful capabilities, components and model-
ing features included in the toolkit.
2 Toolkit
The toolkit provides end-to-end support for the cre-
ation and evaluation of machine translation models.
Given sentence-aligned parallel text, a new transla-
tion system can be built using a single command:
java edu.stanford.nlp.mt.CreateModel \
(source.txt) (target.txt) \
(dev.source.txt) (dev.ref) (model_name)
Running this command will first create word
level alignments for the sentences in source.txt
and target.txt using the Berkeley cross-EM aligner
1http://www.itl.nist.gov/iad/mig/tests
/mt/2009/ResultsRelease/currentArabic.html
9
Figure 1: Chinese-to-English translation using discontinuous phrases.
(Liang et al, 2006).2 From the word-to-word
alignments, the system extracts a phrase ta-
ble (Koehn et al, 2003) and hierarchical reorder-
ing model (Galley and Manning, 2008). Two n-
gram language models are trained on the tar-
get.txt sentences: one over lowercased target sen-
tences that will be used by the Phrasal decoder
and one over the original source sentences that
will be used for truecasing the MT output. Fi-
nally, the system trains the feature weights for the
decoding model using minimum error rate train-
ing (Och, 2003) to maximize the system?s BLEU
score (Papineni et al, 2002) on the development
data given by dev.source.txt and dev.ref. The toolkit
is distributed under the GNU general public license
(GPL) and can be downloaded from http://
nlp.stanford.edu/software/phrasal.
3 Decoder
Decoding Engines The package includes two de-
coding engines, one that implements the left-to-
right beam search algorithm that was first intro-
duced with the Pharaoh machine translation system
(Koehn, 2004), and another that provides a recently
developed decoding algorithm for translating with
discontinuous phrases (Galley and Manning, 2010).
Both engines use features written to a common but
extensible feature API, which allows features to be
written once and then loaded into either engine.
Discontinuous phrases provide a mechanism for
systematically translating grammatical construc-
tions. As seen in Fig. 1, using discontinuous phrases
allows us to successfully capture that the Chinese
construction? X? can be translated as when X.
Multithreading The decoder has robust support
for multithreading, allowing it to take full advantage
of modern hardware that provides multiple CPU
cores. As shown in Fig. 2, decoding speed scales
well when the number of threads being used is in-
creased from one to four. However, increasing the
2Optionally, GIZA++ (Och and Ney, 2003) can also be used
to create the word-to-word alignments.
1 2 3 4 5 6 7 8
15
25
35
Cores
tra
n
la
tio
ns
 p
er
 m
in
u
te
Figure 2: Multicore translations per minute on a sys-
tem with two Intel Xeon L5530 processors running at
2.40GHz.
threads past four results in only marginal additional
gains as the cost of managing the resources shared
between the threads is starting to overwhelm the
value provided by each additional thread. Moses
also does not run faster with more than 4-5 threads.3
Feature API The feature API was designed to
abstract away complex implementation details of
the underlying decoding engine and provide a sim-
ple consistent framework for creating new decoding
model features. During decoding, as each phrase
that is translated, the system constructs a Featuriz-
able object. As seen in Table 1, Featurizable objects
specify what phrase was just translated and an over-
all summary of the translation being built. Code that
implements a feature inspects the Featurizable and
returns one or more named feature values. Prior to
translating a new sentence, the sentence is passed to
the active features for a decoding model, so that they
can perform any necessary preliminary analysis.
Comparison with Moses Credible research into
new features requires baseline system performance
that is on par with existing state-of-the-art systems.
Seen in Table 2, Phrasal meets the performance of
Moses when using the exact same decoding model
feature set as Moses and outperforms Moses signifi-
cantly when using its own default feature set.4
3http://statmt.org/moses
/?n=Moses.AdvancedFeatures (April 6, 2010)
4Phrasal was originally written to replicate Moses as it was
implemented in 2007 (release 2007-05-29), and the current ver-
10
Featurizable
Last Translated Phrase Pair
Source and Target Alignments
Partial Translation
Source Sentence
Current Source Coverage
Pointer to Prior Featurizable
Table 1: Information passed to features in the form of a
Featurizable object for each translated phrase.
System Features MT06 (tune) MT03 MT05
Moses Moses 34.23 33.72 32.51
Phrasal Moses 34.25 33.72 32.49
Phrasal Default 35.02 34.98 33.21
Table 2: Comparison of two configurations of Phrasal
to Moses on Chinese-to-English. One Phrasal configura-
tion uses the standard Moses feature set for single factor
phrase-based translation with distance and phrase level
msd-bidirectional-fe reordering features. The other uses
the default configuration of Phrasal, which replaces the
phrase level msd-bidirectional-fe feature with a heirarchi-
cal reordering feature.
4 Features
The toolkit includes the basic eight phrase-based
translation features available in Moses as well as
Moses? implementation of lexical reordering fea-
tures. In addition to the common Moses features, we
also include innovative new features that improve
translation quality. One of these features is a hier-
archical generalization of the Moses lexical reorder-
ing model. Instead of just looking at the reorder-
ing relationship between individual phrases, the new
feature examines the reordering of blocks of ad-
jacent phrases (Galley and Manning, 2008) and im-
proves translation quality when the material being
reordered cannot be captured by single phrase. This
hierarchical lexicalized reordering model is used by
default in Phrasal and is responsible for the gains
shown in Table 2 using the default features.
To illustrate how Phrasal can effectively be used
to design rich feature sets, we present an overview
of various extensions that have been built upon the
sion still almost exactly replicates this implementation when
using only the baseline Moses features. To ensure this con-
figuration of the decoder is still competitive, we compared it
against the current Moses implementation (release 2009-04-
13) and found that the performance of the two systems is still
close. Tthe current Moses implementation obtains slightly
lower BLEU scores, respectively 33.98 and 32.39 on MT06 and
MT05.
Phrasal feature API. These extensions are currently
not included in the release:
Target Side Dependency Language Model The
n-gram language models that are traditionally used
to capture the syntax of the target language do a
poor job of modeling long distance syntactic rela-
tionships. For example, if there are a number of
intervening words between a verb and its subject,
n-gram language models will often not be of much
help in selecting the verb form that agrees with the
subject. The target side dependency language model
feature captures these long distance relationships by
providing a dependency score for the target transla-
tions produced by the decoder. This is done using
an efficient quadratic time algorithm that operates
within the main decoding loop rather than in a sepa-
rate reranking stage (Galley and Manning, 2009).
Discriminative Distortion The standard distor-
tion cost model used in phrase-based MT systems
such as Moses has two problems. First, it does not
estimate the future cost of known required moves,
thus increasing search errors. Second, the model pe-
nalizes distortion linearly, even when appropriate re-
orderings are performed. To address these problems,
we used the Phrasal feature API to design a new
discriminative distortion model that predicts word
movement during translation and that estimates fu-
ture cost. These extensions allow us to triple the
distortion limit and provide a statistically significant
improvement over the baseline (Green et al, 2010).
Discriminative Reordering with Chinese Gram-
matical Relations During translation, a source
sentence can be more accurately reordered if the
system knows something about the syntactic rela-
tionship between the words in the phrases being re-
ordered. The discriminative reordering with Chinese
grammatical relations feature examines the path be-
tween words in a source-side dependency tree and
uses it to evaluate the appropriateness of candidate
phrase reorderings (Chang et al, 2009).
5 Other components
Training Decoding Models The package includes
a comprehensive toolset for training decoding mod-
els. It supports MERT training using coordinate de-
scent, Powell?s method, line search along random
search directions, and downhill Simplex. In addi-
tion to the BLEU metric, models can be trained
11
to optimize other popular evaluation metrics such
as METEOR (Lavie and Denkowski, 2009), TERp
(Snover et al, 2009), mWER (Nie?en et al, 2000),
and PER (Tillmann et al, 1997). It is also possible
to plug in other new user-created evaluation metrics.
Conditional Phrase Table Extraction Rather
than first building a massive phrase table from a par-
allel corpus and then filtering it down to just what
is needed for a specific data set, our toolkit sup-
ports the extraction of just those phrases that might
be used on a given evaluation set. In doing so, it
dramatically reduces the time required to build the
phrase table and related data structures such as re-
ordering models.
Feature Extraction API In order to assist in the
development of new features, the toolkit provides
an API for extracting feature statistics from a word-
aligned parallel corpus. This API ties into the condi-
tional phrase table extraction utility, and thus allows
for the extraction of just those feature statistics that
are relevant to a given data set.
6 Conclusion
Phrasal is an open source state-of-the-art Java-
based machine translation system that was designed
specifically for research into new decoding model
features. The system supports traditional phrase-
based translation as well as translation using discon-
tinuous phrases. It includes a number of new and
innovative model features in addition to those typi-
cally found in phrase-based translation systems. It is
also packaged with other useful components such as
tools for extracting feature statistics, building phrase
tables for specific data sets, and MERT training rou-
tines that support a number of optimization tech-
niques and evaluation metrics.
Acknowledgements
The Phrasal decoder has benefited from the help-
ful comments and code contributions of Pi-Chuan
Chang, Spence Green, Karthik Raghunathan,
Ankush Singla, and Huihsin Tseng. The software
presented in this paper is based on work work was
funded by the Defense Advanced Research Projects
Agency through IBM. The content does not neces-
sarily reflect the views of the U.S. Government, and
no official endorsement should be inferred.
References
Thomas Bruckschlegel. 2005. Microbenchmarking C++,
C#, and Java. C/C++ Users Journal.
P. Chang, H. Tseng, D. Jurafsky, and C.D. Manning.
2009. Discriminative reordering with Chinese gram-
matical relations features. In SSST Workshop at
NAACL.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP.
Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In ACL.
Michel Galley and Christopher Manning. 2010. Improv-
ing phrase-based machine translation with discontigu-
ous phrases. In NAACL.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In In NAACL.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In NAACL.
Sonja Nie?en, Franz Josef Och, and Hermann Ney. 2000.
An evaluation tool for machine translation: Fast eval-
uation for MT research. In LREC.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL.
Geoffrey Phipps. 1999. Comparing observed bug and
productivity rates for java and C++. Softw. Pract. Ex-
per., 29(4):345?358.
M. Snover, N. Madnani, B.J. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or HTER?: exploring dif-
ferent human judgments with a tunable MT metric. In
SMT workshop at EACL.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based search for statistical
translation. In In Eurospeech.
12
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 446?455,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Parsing Time: Learning to Interpret Time Expressions
Gabor Angeli
Stanford University
Stanford, CA 94305
angeli@stanford.edu
Christopher D. Manning
Stanford University
Stanford, CA 94305
manning@stanford.edu
Daniel Jurafsky
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
We present a probabilistic approach for learn-
ing to interpret temporal phrases given only a
corpus of utterances and the times they ref-
erence. While most approaches to the task
have used regular expressions and similar lin-
ear pattern interpretation rules, the possibil-
ity of phrasal embedding and modification in
time expressions motivates our use of a com-
positional grammar of time expressions. This
grammar is used to construct a latent parse
which evaluates to the time the phrase would
represent, as a logical parse might evaluate to
a concrete entity. In this way, we can employ
a loosely supervised EM-style bootstrapping
approach to learn these latent parses while
capturing both syntactic uncertainty and prag-
matic ambiguity in a probabilistic framework.
We achieve an accuracy of 72% on an adapted
TempEval-2 task ? comparable to state of the
art systems.
1 Introduction
Temporal resolution is the task of mapping from
a textual phrase describing a potentially complex
time, date, or duration to a normalized (grounded)
temporal representation. For example, possibly
complex phrases such as the week before last are
often more useful in their grounded form ? e.g.,
January 1 - January 7.
The dominant approach to this problem in previ-
ous work has been to use rule-based methods, gen-
erally a combination of regular-expression matching
followed by hand-written interpretation functions.
In general, it is appealing to learn the interpre-
tation of temporal expressions, rather than hand-
building systems. Moreover, complex hierarchical
temporal expressions, such as the Tuesday before
last or the third Wednesday of each month, and am-
biguous expressions, such as last Friday, are diffi-
cult to handle using deterministic rules and would
benefit from a recursive and probabilistic phrase
structure representation. Therefore, we attempt to
learn a temporal interpretation system where tempo-
ral phrases are parsed by a grammar, but this gram-
mar and its semantic interpretation rules are latent,
with only the input phrase and its grounded interpre-
tation given to the learning system.
Employing probabilistic techniques allows us to
capture ambiguity in temporal phrases in two impor-
tant respects. In part, it captures syntactic ambigu-
ity ? e.g., last Friday the 13th bracketing as either
[last Friday] [the 13th], or last [Friday the 13th].
This also includes examples of lexical ambiguity ?
e.g., two meanings of last in last week of November
versus last week. In addition, temporal expressions
often carry a pragmatic ambiguity. For instance, a
speaker may refer to either the next or previous Fri-
day when he utters Friday on a Sunday. Similarly,
next week can refer to either the coming week or the
week thereafter.
Probabilistic systems furthermore allow propaga-
tion of uncertainty to higher-level components ? for
example recognizing that May could have a num-
ber of non-temporal meanings and allowing a sys-
tem with a broader contextual scope to make the fi-
nal judgment. We implement a CRF to detect tem-
poral expressions, and show our model?s ability to
act as a component in such a system.
We describe our temporal representation, fol-
lowed by the learning algorithm; we conclude with
experimental results showing our approach to be
competitive with state of the art systems.
446
2 Related Work
Our approach draws inspiration from a large body of
work on parsing expressions into a logical form. The
latent parse parallels the formal semantics in previ-
ous work, e.g., Montague semantics. Like these rep-
resentations, a parse ? in conjunction with the refer-
ence time ? defines a set of matching entities, in this
case the grounded time. The matching times can be
thought of as analogous to the entities in a logical
model which satisfy a given expression.
Supervised approaches to logical parsing promi-
nently include Zelle and Mooney (1996), Zettle-
moyer and Collins (2005), Kate et al (2005), Zettle-
moyer and Collins (2007), inter alia. For exam-
ple, Zettlemoyer and Collins (2007) learn a mapping
from textual queries to a logical form. This logical
form importantly contains all the predicates and en-
tities used in their parse. We loosen the supervision
required in these systems by allowing the parse to be
entirely latent; the annotation of the grounded time
neither defines, nor gives any direct cues about the
elements of the parse, since many parses evaluate to
the same grounding. To demonstrate, the grounding
for a week ago could be described by specifying a
month and day, or as a week ago, or as last x ? sub-
stituting today?s day of the week for x. Each of these
correspond to a completely different parse.
Recent work by Clarke et al (2010) and Liang et
al. (2011) similarly relax supervision to require only
annotated answers rather than full logical forms. For
example, Liang et al (2011) constructs a latent parse
similar in structure to a dependency grammar, but
representing a logical form. Our proposed lexi-
cal entries and grammar combination rules can be
thought of as paralleling the lexical entries and pred-
icates, and the implicit combination rules respec-
tively in this framework. Rather than querying from
a finite database, however, our system must com-
pare temporal expression within an infinite timeline.
Furthermore, our system is run using neither lexical
cues nor intelligent initialization.
Related work on interpreting temporal expres-
sions has focused on constructing hand-crafted in-
terpretation rules (Mani and Wilson, 2000; Saquete
et al, 2003; Puscasu, 2004; Grover et al, 2010). Of
these, HeidelTime (Stro?tgen and Gertz, 2010) and
SUTime (Chang and Manning, 2012) provide par-
ticularly strong competition.
Recent probabilistic approaches to temporal reso-
lution include UzZaman and Allen (2010), who em-
ploy a parser to produce deep logical forms, in con-
junction with a CRF classifier. In a similar vein,
Kolomiyets and Moens (2010) employ a maximum
entropy classifier to detect the location and temporal
type of expressions; the grounding is then done via
deterministic rules.
3 Representation
We define a compositional representation of time;
a type system is described in Section 3.1 while the
grammar is outlined in Section 3.2 and described in
detail in Sections 3.3 and 3.4.
3.1 Temporal Expression Types
We represent temporal expressions as either a
Range, Sequence, or Duration. We describe these,
the Function type, and the miscellaneous Number
and Nil types below:
Range [and Instant] A period between two dates
(or times). This includes entities such as Today,
1987, or Now. We denote a range by the variable
r. We maintain a consistent interval-based theory of
time (Allen, 1981) and represent instants as intervals
with zero span.
Sequence A sequence of Ranges, not necessarily
occurring at regular intervals. This includes enti-
ties such as Friday, November 27th, or last
Friday. A Sequence is a tuple of three elements
s = (rs,?s, ?s):
1. rs(i): The ith element of a sequence, of type
Range. In the case of the sequence Friday,
rs(0) corresponds to the Friday in the current
week; rs(1) is the Friday in the following week,
etc.
2. ?s: The distance between two elements in the
sequence ? approximated if this distance is not
constant. In the case of Friday, this distance
would be a week.
3. ?s: The containing unit of an element of a se-
quence. For example, ?Friday would be the
Range corresponding to the current week. The
sequence index i ? Z, from rs(i), is defined
447
relative to rs(0) ? the element in the same con-
taining unit as the reference time.
We define the reference time t (Reichenbach,
1947) to be the instant relative to which times are
evaluated. For the TempEval-2 corpus, we approxi-
mate this as the publication time of the article. While
this is conflating Reichenbach?s reference time with
speech time, it is a useful approximation.
To contrast with Ranges, a Sequence can rep-
resent a number of grounded times. Nonetheless,
pragmatically, not all of these are given equal weight
? an utterance of last Friday may mean either of the
previous two Fridays, but is unlikely to ground to
anything else. We represent this ambiguity by defin-
ing a distribution over the elements of the Sequence.
While this could be any distribution, we chose to ap-
proximate it as a Gaussian.
In order to allow sharing parameters between any
sequence, we define the domain in terms of the index
of the sequence rather than of a constant unit of time
(e.g., seconds). To illustrate, the distribution over
April would have a much larger variance than the
distribution over Sunday, were the domains fixed.
The probability of the ith element of a sequence thus
depends on the beginning of the range rs(i), the ref-
erence time t, and the distance between elements of
the sequence ?s. We summarize this in the equation
below, with learned parameters ? and ?:
Pt(i) =
? 0.5
?=?0.5
N?,?
(
rs(i)? t
?s
+ ?
)
(1)
Figure 1 shows an example of such a distribution;
importantly, note that moving the reference time be-
tween two elements dynamically changes the prob-
ability assigned to each.
Duration A period of time. This includes entities
like Week, Month, and 7 days. We denote a du-
ration with the variable d.
We define a special case of the Duration type to
represent approximate durations, identified by their
canonical unit (week, month, etc). These are used
to represent expressions such as a few years or some
days.
Function A function of arity less than or equal to
two representing some general modification to one
-2
11/13
-1
11/20
-0.3
t
?s
1
12/4
2
12/11
Reference time
P (11/20) = ? ?0.5?1.5 f(x)
0
11/27
ff
ff
Figure 1: An illustration of a temporal distribution, e.g.,
Sunday. The reference time is labeled as time t between
Nov 20 and Nov 27; the probability that this sequence
is referring to Nov 20 is the integral of the marked area.
The domain of the graph are the indices of the sequence;
the distribution is overlaid with mean at the (normalized)
reference time t/?s; in our case ?s is a week. Note
that the probability of an index changes depending on the
exact location of the reference time.
of the above types. This captures semantic entities
such as those implied in last x, the third x [of y],
or x days ago. The particular functions and their
application are enumerated in Table 2.
Other Types Two other types bear auxiliary roles
in representing temporal expressions, though they
are not directly temporal concepts. In the grammar,
these appear as preterminals only.
The first of these types is Number ? denoting
a number without any temporal meaning attached.
This comes into play representing expressions such
as 2 weeks. The other is the Nil type ? denoting
terms which are not directly contributing to the se-
mantic meaning of the expression. This is intended
for words such as a or the, which serve as cues with-
out bearing temporal content themselves. The Nil
type is lexicalized with the word it generates.
Omitted Phenomena The representation de-
scribed is a simplification of the complexities of
time. Notably, a body of work has focused on
reasoning about events or states relative to temporal
expressions. Moens and Steedman (1988) describes
temporal expressions relating to changes of state;
Condoravdi (2010) explores NPI licensing in
temporal expressions. Broader context is also not
448
Range
f(Duration) : Range
catRight
next
Duration
Number
Numn?100
2
Duration
Day
days(a)
catRight(t, 2D )
catRight(t,?)
next
2D
Num(2)
2
1D
days(b)
Figure 2: The grammar ? (a) describes the CFG parse of
the temporal types. Words are tagged with their nontermi-
nal entry, above which only the types of the expressions
are maintained; (b) describes the corresponding combi-
nation of the temporal instances. The parse in (b) is de-
terministic given the grammar combination rules in (a).
directly modeled, but rather left to systems in which
the model would be embedded. Furthermore, vague
times (e.g., in the 90?s) represent a notable chunk
of temporal expressions uttered. In contrast, NLP
evaluations have generally not handled such vague
time expressions.
3.2 Grammar Formalism
Our approach builds on the assumption that natural
language descriptions of time are compositional in
nature. Each word attached to a temporal phrase is
usually compositionally modifying the meaning of
the phrase. To demonstrate, we consider the expres-
sion the week before last week. We can construct a
meaning by applying the modifier last to week ? cre-
ating the previous week; and then applying before to
week and last week.
We construct a paradigm for parsing temporal
phrases consisting of a standard PCFG over tempo-
ral types with each parse rule defining a function to
apply to the child nodes, or the word being gener-
ated. At the root of the tree, we recursively apply
the functions in the parse tree to obtain a final tem-
poral value. One can view this formalism as a rule-
to-rule translation (Bach, 1976; Allen, 1995, p. 263),
or a constrained Synchronous PCFG (Yamada and
Knight, 2001).
Our approach contrasts with common approaches,
such as CCG grammars (Steedman, 2000; Bos et
al., 2004; Kwiatkowski et al, 2011), giving us more
flexibility in the composition rules. Figure 2 shows
an example of the grammar.
Formally, we define our temporal grammar
G = (?, S,V,W,R, ?). The alphabet ? and start
symbol S retain their usual interpretations. We de-
fine a set V to be the set of types, as described in
Section 3.1 ? these act as our nonterminals. For each
v ? V we define an (infinite) set Wv corresponding
to the possible instances of type v. Concretely, if
v = Sequence, our set Wv ? W could contain el-
ements corresponding to Friday, last Friday, Nov.
27th, etc. Each node in the tree defines a pair (v, w)
such that w ? Wv, with combination rules defined
over v and function applications performed on w.
A rule R ? R is defined as a pair
R =
(
vi ? vjvk, f : (Wvj ,Wvk)?Wvi
)
. The
first term is our conventional PCFG rule over the
types V . The second term defines the function to
apply to the values returned recursively by the child
nodes. Note that this definition is trivially adapted
for the case of unary rules.
The last term in our grammar formalism denotes
the rule probabilities ?. In line with the usual in-
terpretation, this defines a probability of applying a
particular rule r ? R. Importantly, note that the
distribution over possible groundings of a temporal
expression are not included in the grammar formal-
ism. The learning of these probabilities is detailed
in Section 4.
3.3 Preterminals
We define a set of preterminals, specifying their
eventual type, as well as the temporal instance it pro-
duces when its function is evaluated on the word it
generates (e.g., f(day) = Day). A distinction is
made in our description between entities with con-
tent roles versus entities with a functional role.
The first ? consisting of Ranges, Sequences, and
Durations ? are listed in Table 1. A total of 62 such
preterminals are defined in the implemented system,
corresponding to primitive entities often appearing
in newswire, although this list is easily adaptable to
449
Function Description Signature(s)
shiftLeft Shift a Range or Sequence left by a Duration f : S,D? S; f : R,D? R
shiftRight Shift a Range or Sequence right by a Duration f : S,D? S; f : R,D? R
shrinkBegin Take the first Duration of a Range/Sequence f : S,D? S; f : R,D? R
shrinkEnd Take the last Duration of a Range/Sequence f : S,D? S; f : R,D? R
catLeft Take Duration units after the end of a Range f : R,D? R
catRight Take Duration units before the start of a Range f : R,D? R
moveLeft1 Move the origin of a sequence left by 1 f : S? S
moveRight1 Move the origin of a sequence right by 1 f : S? S
nth x of y Take the nth Sequence in y (Day of Week, etc) f : Number? S
approximate Make a Duration approximate f : D? D
Table 2: The functional preterminals of the grammar; R, S, and D denote Ranges Sequences and Durations respec-
tively. The name, a brief description, and the type signature of the function (as used in parsing) are given. Described
in more detail in Section 3.4, the functions are most easily interpreted as operations on either an interval or sequence.
Type Instances
Range Past, Future, Yesterday,
Tomorrow, Today, Reference,
Year(n), Century(n)
Sequence Friday, January, . . .
DayOfMonth, DayOfWeek, . . .
EveryDay, EveryWeek, . . .
Duration Second, Minute, Hour,
Day, Week, Month, Quarter,
Year, Decade, Century
Table 1: The content-bearing preterminals of the gram-
mar, arranged by their types. Note that the Sequence
type contains more elements than enumerated here; how-
ever, only a few of each characteristic type are shown here
for brevity.
fit other domains. It should be noted that the expres-
sions, represented in Typewriter, have no a pri-
ori association with words, denoted by italics; this
correspondence must be learned. Furthermore, enti-
ties which are subject to interpretation ? for example
Quarter or Season ? are given a concrete inter-
pretation. The nth quarter is defined by evenly split-
ting a year into four; the seasons are defined in the
same way but with winter beginning in December.
The functional entities are described in Table 2,
and correspond to the Function type. The majority
of these mirror generic operations on intervals on a
timeline, or manipulations of a sequence. Notably,
like intervals, times can be moved (3 weeks ago) or
their size changed (the first two days of the month),
or a new interval can be started from one of the end-
points (the last 2 days). Additionally, a sequence can
be modified by shifting its origin (last Friday), or
taking the nth element of the sequence within some
bound (fourth Sunday in November).
The lexical entry for the Nil type is tagged with the
word it generates, producing entries such as Nil(a),
Nil(November), etc. The lexical entry for the Num-
ber type is parameterized by the order of magnitude
and ordinality of the number; e.g., 27th becomes
Number(101,ordinal).
3.4 Combination Rules
As mentioned earlier, our grammar defines both
combination rules over types (in V) as well as a
method for combining temporal instances (in Wv ?
W). This method is either a function application of
one of the functions in Table 2, a function which is
implicit in the text (intersection and multiplication),
or an identity operation (for Nils). These cases are
detailed below:
? Function application, e.g., last week. We apply
(or partially apply) a function to an argument
on either the left or the right: f(x, y)x or x
f(x, y). Furthermore, for functions of arity 2
taking a Range as an argument, we define a rule
treating it as a unary function with the reference
time taking the place of the second argument.
? Intersecting two ranges or sequences, e.g.,
450
Input (w,t) ( Last Friday the 13 th , May 16 2011 )
LatentparseR
moveLeft1( FRI ) ? 13th
moveLeft1( FRI )
moveLeft1(?)
last
FRI
friday
13th
Nilthe
the
13th
13th
Output ?? May 13 2011
Figure 3: An overview of the system architecture. Note
that the parse is latent ? that is, it is not annotated in the
training data.
November 27th. The intersect function treats
both arguments as intervals, and will return an
interval (Range or Sequence) corresponding to
the overlap between the two.1
? Multiplying a Number with a Duration, e.g., 5
weeks.
? Combining a non-Nil and Nil element with no
change to the temporal expression, e.g., a week.
The lexicalization of the Nil type allows the
algorithm to take hints from these supporting
words.
We proceed to describe learning the parameters of
this grammar.
4 Learning
We present a system architecture, described in Fig-
ure 3. We detail the inference procedure in Sec-
tion 4.1 and training in Section 4.2.
4.1 Inference
To provide a list of candidate expressions with their
associated probabilities, we employ a k-best CKY
parser. Specifically, we implement Algorithm 3 de-
scribed in Huang and Chiang (2005), providing an
O(Gn3k log k) algorithm with respect to the gram-
mar size G, phrase length n, and beam size k. We
set the beam size to 2000.
1In the case of complex sequences (e.g., Friday the 13th) an
A? search is performed to find overlapping ranges in the two
sequences; the origin rs(0) is updated to refer to the closest
such match to the reference time.
Revisiting the notion of pragmatic ambiguity, in
a sense the most semantically complete output of
the system would be a distribution ? an utterance of
Friday would give a distribution over Fridays rather
than a best guess of its grounding. However, it is of-
ten advantageous to ground to a concrete expression
with a corresponding probability. The CKY k-best
beam and the temporal distribution ? capturing syn-
tactic and pragmatic ambiguity ? can be combined to
provide a Viterbi decoding, as well as its associated
probability.
We define the probability of a syntactic parse
y making use of rules R ? R as P (y) =
P (w1, . . . wn;R) =
?
i?j,k?R P (j, k | i). As de-
scribed in Section 3.1, we define the probability of
a grounding relative to reference time t and a par-
ticular syntactic interpretation Pt(i|y). The prod-
uct of these two terms provides the probability of
a grounded temporal interpretation; we can obtain a
Viterbi decoding by maximizing this joint probabil-
ity:
Pt(i, y) = P (y)? Pt(i|y) (2)
This provides us with a framework for obtaining
grounded times from a temporal phrase ? in line with
the annotations provided during training time.
4.2 Training
We present an EM-style bootstrapping approach to
training the parameters of our grammar jointly with
the parameters of our Gaussian temporal distribu-
tion.
Our TimEM algorithm for learning the parame-
ters for the grammar (?), jointly with the temporal
distribution (? and ?) is given in Algorithm 1. The
inputs to the algorithm are the initial parameters ?,
?, and ?, and a set of training instances D. Further-
more, the algorithm makes use of a Dirichlet prior ?
on the grammar parameters ?, as well as a Gaussian
prior N on the mean of the temporal distribution ?.
The algorithm outputs the final parameters ??, ??
and ??.
Each training instance is a tuple consisting of
the words in the temporal phrase w, the annotated
grounded time ??, and the reference time of the ut-
terance t. The input phrase is tokenized according
to Penn Treebank guidelines, except we additionally
451
Algorithm 1: TimEM
Input: Initial parameters ?, ?, ?; data
D = {(w, ??, t)}; Dirichlet prior ?,
Gaussian prior N
Output: Optimal parameters ??, ??, ??
while not converged do1
(M??, M??,?) := E-Step (D,?,?,?)2
(?, ?, ?) := M-Step (M??, M??,?)3
end4
return (?s, ?, ?)5
begin E-Step(D,?,?,?)6
M?? = []; M??,? = []7
for (w, ??, t) ? D do8
m?? = []; m??,? = []9
for y ? k-bestCKY(w, ?) do10
if p = P?,?(?? | y, t) > 0 then11
m?? += (y, p); m??,? += (i, p)12
end13
end14
M? += normalize(m??)15
M??,? += normalize(m??,?)16
end17
return M?18
end19
begin M-Step(M??,M??,?)20
?? := bayesianPosterior(M??, ?)21
?? := mlePosterior(M??,?)22
?? := bayesianPosterior(M??,?, ??, N )23
return (??, ??, ??)24
end25
split on the characters ?-? and ?/,? which often de-
limit a boundary between temporal entities. Beyond
this preprocessing, no language-specific information
about the meanings of the words are introduced, in-
cluding syntactic parses, POS tags, etc.
The algorithm operates similarly to the EM algo-
rithms used for grammar induction (Klein and Man-
ning, 2004; Carroll and Charniak, 1992). How-
ever, unlike grammar induction, we are allowed a
certain amount of supervision by requiring that the
predicted temporal expression match the annotation.
Our expected statistics are therefore more accurately
our normalized expected counts of valid parses.
Note that in conventional grammar induction, the
expected sufficient statistics can be gathered analyt-
ically from reading off the chart scores of a parse.
This does not work in our case for two reasons. In
part, we would like to incorporate the probability
of the temporal grounding in our feedback probabil-
ity. Additionally, we are only using parses which are
valid candidates ? that is, the parses which ground to
the correct time ?? ? which we cannot establish until
the entire expression is parsed. The expected statis-
tics are thus computed non-analytically via a beam
on both the possible parses (line 10) and the pos-
sible temporal groundings of a given interpretation
(line 11).
The particular EM updates are the standard up-
dates for multinomial and Gaussian distributions
given fully observed data. In the multinomial case,
our (unnormalized) parameter updates, with Dirich-
let prior ?, are:
??mn|l = ?+
?
(y,p)?M??
?
vjk|i?y
1
(
vjk|i = vmn|l
)
p (3)
In the Gaussian case, the parameter update for ?
is the maximum likelihood update; while the update
for ? incorporates a Bayesian prior N (?0, ?0):
?? =
?
?
?
?
1
?
(i,p)?M??,?
p
?
(i,p)?M??,?
(i? ??)2 ? p (4)
?? =
??2?0 + ?20
?
(i,p)?M??,? i ? p
??2 + ?20
?
(i,p)?M??,? p
(5)
As the parameters improve, the parser more effi-
ciently prunes incorrect parses and the beam incor-
porates valid parses for longer and longer phrases.
For instance, in the first iteration the model must
learn the meaning of both words in last Friday; once
the parser learns the meaning of one of them ? e.g.,
Friday appears elsewhere in the corpus ? subsequent
iterations focus on proposing candidate meanings
for last. In this way, a progressively larger percent-
age of the data is available to be learned from at each
iteration.
5 Evaluation
We evaluate our model against current state-of-the
art systems for temporal resolution on the English
452
Train Test
System Type Value Type Value
GUTime 0.72 0.46 0.80 0.42
SUTime 0.85 0.69 0.94 0.71
HeidelTime 0.80 0.67 0.85 0.71
OurSystem 0.90 0.72 0.88 0.72
Table 3: TempEval-2 Attribute scores for our system and
three previous systems. The scores are calculated us-
ing gold extents, forcing a guessed interpretation for each
parse.
portion of the TempEval-2 Task A dataset (Verhagen
et al, 2010).
5.1 Dataset
The TempEval-2 dataset is relatively small, contain-
ing 162 documents and 1052 temporal phrases in the
training set and an additional 20 documents and 156
phrases in the evaluation set. Each temporal phrase
was annotated as a TIMEX32 tag around an adver-
bial or prepositional phrase
5.2 Results
In the TempEval-2 A Task, system performance is
evaluated on detection and resolution of expressions.
Since we perform only the second of these, we eval-
uate our system assuming gold detection.
Similarly, the original TempEval-2 scoring
scheme gave a precision and recall for detection,
and an accuracy for only the temporal expressions
attempted. Since our system is able to produce a
guess for every expression, we produce a precision-
recall curve on which competing systems are plotted
(see Figure 4). Note that the downward slope of the
curve indicates that the probabilities returned by the
system are indicative of its confidence ? the prob-
ability of a parse correlates with the probability of
that parse being correct.
Additionally, and perhaps more accurately, we
compare to previous system scores when con-
strained to make a prediction on every example; if
no guess is made, the output is considered incorrect.
This in general yields lower results, as the system
is not allowed to abstain on expressions it does not
2See http://www.timeml.org for details on the
TimeML format and TIMEX3 tag.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Valu
e ac
cura
cy
Extent recall
HeidelTime1
HeidelTime2
SUTime
OurSystem
Figure 4: A precision-recall curve for our system, com-
pared to prior work. The data points are obtained by set-
ting a threshold minimum probability at which to guess
a time creating different extent recall values. The curve
falls below HeidelTime1 and SUTime in part from lack
of context, and in part since our system was not trained
to optimize this curve.
recognize. Results are summarized in Table 3.
We compare to three previous rule-based sys-
tems. GUTime (Mani and Wilson, 2000) presents an
older but widely used baseline.3 More recently, SU-
Time (Chang and Manning, 2012) provides a much
stronger comparison. We also compare to Heidel-
Time (Stro?tgen and Gertz, 2010), which represents
the state-of-the-art system at the TempEval-2 task.
5.3 Detection
One of the advantages of our model is that it can pro-
vide candidate groundings for any expression. We
explore this ability by building a detection model to
find candidate temporal expressions, which we then
ground. The detection model is implemented as a
Conditional Random Field (Lafferty et al, 2001),
with features over the morphology and context. Par-
ticularly, we define the following features:
? The word and lemma within 2 of the current
word.
? The word shape4 and part of speech of the cur-
rent word.
3Due to discrepancies in output formats, the output of
GUTime was heuristically patched and manually checked to
conform to the expected format.
4Word shape is calculated by mapping each character to one
of uppercase, lowercase, number, or punctuation. The first four
characters are mapped verbatim; subsequent sequences of sim-
ilar characters are collapsed.
453
Extent Attribute
System P R F1 Typ Val
GUTime 0.89 0.79 0.84 0.95 0.68
SUTime 0.88 0.96 0.92 0.96 0.82
HeidelTime1 0.90 0.82 0.86 0.96 0.85
HeidelTime2 0.82 0.91 0.86 0.92 0.77
OurSystem 0.89 0.84 0.86 0.91 0.72
Table 4: TempEval-2 Extent scores for our system and
three previous systems. Note that the attribute scores are
now relatively low compared to previous work; unlike
rule-based approaches, our model can guess a temporal
interpretation for any phrase, meaning that a good pro-
portion of the phrases not detected would have been in-
terpreted correctly.
? Whether the current word is a number, along
with its ordinality and order of magnitude
? Prefixes and suffixes up to length 5, along with
their word shape.
We summarize our results in Table 4, noting that
the performance indicates that the CRF and interpre-
tation model find somewhat different phrases hard to
detect and interpret respectively. Many errors made
in detection are attributable to the small size of the
training corpus (63,000 tokens).
5.4 Discussion
Our system performs well above the GUTime base-
line and is competitive with both of the more recent
systems. In part, this is from more sophisticated
modeling of syntactic ambiguity: e.g., the past few
weeks has a clause the past ? which, alone, should
be parsed as PAST ? yet the system correctly dis-
prefers incorporating this interpretation and returns
the approximate duration 1 week. Furthermore,
we often capture cases of pragmatic ambiguity ? for
example, empirically, August tends to refers to the
previous August when mentioned in February.
Compared to rule-based systems, we attribute
most errors the system makes to either data spar-
sity or missing lexical primitives. For example ?
illustrating sparsity ? we have trouble recognizing
Nov. as corresponding to November (e.g., Nov. 13),
since the publication time of the articles happen to
often be near November and we prefer tagging the
word as Nil (analogous to the 13th). Missing lexi-
cal primitives, in turn, include tags for 1990s, or half
(in minute and a half ); as well as missing functions,
such as or (in weeks or months).
Remaining errors can be attributed to causes such
as providing the wrong Viterbi grounding to the
evaluation script (e.g., last rather than this Friday),
differences in annotation (e.g., 24 hours is marked
wrong against a day), or missing context (e.g., the
publication time is not the true reference time),
among others.
6 Conclusion
We present a new approach to resolving temporal ex-
pressions, based on synchronous parsing of a fixed
grammar with learned parameters and a composi-
tional representation of time. The system allows
for output which captures uncertainty both with re-
spect to the syntactic structure of the phrase and the
pragmatic ambiguity of temporal utterances. We
also note that the approach is theoretically better
adapted for phrases more complex than those found
in TempEval-2.
Furthermore, the system makes very few
language-specific assumptions, and the algorithm
could be adapted to domains beyond temporal
resolution. We hope to improve detection and
explore system performance on multilingual and
complex datasets in future work.
Acknowledgements The authors would like to thank Valentin
Spitkovsky, David McClosky, and Angel Chang for valuable
discussion and insights. We gratefully acknowledge the support
of the Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the view of DARPA, AFRL, or the US government.
References
James F. Allen. 1981. An interval-based representa-
tion of temporal knowledge. In Proceedings of the
7th international joint conference on Artificial intelli-
gence, pages 221?226, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
James Allen. 1995. Natural Language Understanding.
Benjamin/Cummings, Redwood City, CA.
454
E. Bach. 1976. An extension of classical transforma-
tional grammar. In Problems of Linguistic Metatheory
(Proceedings of the 1976 Conference), Michigan State
University.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In
Proceedings of Coling, pages 1240?1246, Geneva,
Switzerland. COLING.
Glenn Carroll and Eugene Charniak. 1992. Two experi-
ments on learning probabilistic dependency grammars
from corpora. Technical report, Providence, RI, USA.
Angel Chang and Chris Manning. 2012. SUTIME: a
library for recognizing and normalizing time expres-
sions. In Language Resources and Evaluation.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In CoNLL, pages 18?27, Uppsala,
Sweden.
Cleo Condoravdi. 2010. NPI licensing in temporal
clauses. Natural Language and Linguistic Theory,
28:877?910.
Claire Grover, Richard Tobin, Beatrice Alex, and Kate
Byrne. 2010. Edinburgh-LTG: TempEval-2 system
description. In Proceedings of the 5th International
Workshop on Semantic Evaluation, Sem-Eval, pages
333?336.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technology, Parsing, pages 53?
64.
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney.
2005. Learning to transform natural to formal lan-
guages. In AAAI, pages 1062?1068, Pittsburgh, PA.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL.
Oleksandr Kolomiyets and Marie-Francine Moens. 2010.
KUL: recognition and normalization of temporal ex-
pressions. In Proceedings of the 5th International
Workshop on Semantic Evaluation, Sem-Eval ?10,
pages 325?328.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generalization
in CCG grammar induction for semantic parsing. In
EMNLP, pages 1512?1523, Edinburgh, Scotland, UK.
J. Lafferty, A. McCallum, and F Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In International
Conference on Machine Learning (ICML).
P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In ACL.
Inderjeet Mani and George Wilson. 2000. Robust tem-
poral processing of news. In ACL, pages 69?76, Hong
Kong.
Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational Lin-
guistics, 14:15?28.
G. Puscasu. 2004. A framework for temporal resolution.
In LREC, pages 1901?1904.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
Macmillan, New York.
E. Saquete, R. Muoz, and P. Martnez-Barco. 2003.
Terseo: Temporal expression resolution system ap-
plied to event ordering. In Text, Speech and Dialogue,
pages 220?228.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Jannik Stro?tgen and Michael Gertz. 2010. Heideltime:
High quality rule-based extraction and normalization
of temporal expressions. In Proceedings of the 5th In-
ternational Workshop on Semantic Evaluation, Sem-
Eval, pages 321?324.
Naushad UzZaman and James F. Allen. 2010. TRIPS
and TRIOS system for TempEval-2: Extracting tem-
poral information from text. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Sem-
Eval, pages 276?283.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
TempEval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62, Up-
psala, Sweden.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL, pages 523?530.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic pro-
gramming. In AAAI/IAAI, pages 1050?1055, Portland,
OR.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
UAI, pages 658?666. AUAI Press.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing to
logical form. In EMNLP-CoNLL, pages 678?687.
455
Proceedings of NAACL-HLT 2013, pages 897?906,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Same Referent, Different Words:
Unsupervised Mining of Opaque Coreferent Mentions
Marta Recasens*, Matthew Can?, and Dan Jurafsky*
*Linguistics Department, Stanford University, Stanford, CA 94305
?Computer Science Department, Stanford University, Stanford, CA 94305
recasens@google.com, {mattcan,jurafsky}@stanford.edu
Abstract
Coreference resolution systems rely heav-
ily on string overlap (e.g., Google Inc. and
Google), performing badly on mentions with
very different words (opaque mentions) like
Google and the search giant. Yet prior at-
tempts to resolve opaque pairs using ontolo-
gies or distributional semantics hurt precision
more than improved recall. We present a new
unsupervised method for mining opaque pairs.
Our intuition is to restrict distributional se-
mantics to articles about the same event, thus
promoting referential match. Using an En-
glish comparable corpus of tech news, we built
a dictionary of opaque coreferent mentions
(only 3% are in WordNet). Our dictionary can
be integrated into any coreference system (it
increases the performance of a state-of-the-art
system by 1% F1 on all measures) and is eas-
ily extendable by using news aggregators.
1 Introduction
Repetition is one of the most common coreferential
devices in written text, making string-match features
important to all coreference resolution systems. In
fact, the scores achieved by just head match and a
rudimentary form of pronominal resolution1 are not
far from that of state-of-the-art systems (Recasens
and Hovy, 2010). This suggests that opaque men-
tions (i.e., lexically different) such as iPad and the
Cupertino slate are a serious problem for modern
systems: they comprise 65% of the non-pronominal
1Closest NP with the same gender and number.
errors made by the Stanford system on the CoNLL-
2011 data. Solving this problem is critical for over-
coming the recall gap of state-of-the-art systems
(Haghighi and Klein, 2010; Stoyanov et al, 2009).
Previous systems have turned either to ontologies
(Ponzetto and Strube, 2006; Uryupina et al, 2011;
Rahman and Ng, 2011) or distributional semantics
(Yang and Su, 2007; Kobdani et al, 2011; Bansal
and Klein, 2012) to help solve these errors. But nei-
ther semantic similarity nor hypernymy are the same
as coreference: Microsoft and Google are distribu-
tionally similar but not coreferent; people is a hy-
pernym of both voters and scientists, but the peo-
ple can corefer with the voters, but is less likely
to corefer with the scientists. Thus ontologies lead
to precision problems, and to recall problems like
missing NE descriptions (e.g., Apple and the iPhone
maker) and metonymies (e.g., agreement and word-
ing), while distributional systems lead to precision
problems like coreferring Microsoft and the Moun-
tain View giant because of their similar vector rep-
resentation (release, software, update).
We increase precision by drawing on the intuition
that referents that are both similar and participate in
the same event are likely to corefer. We restrict dis-
tributional similarity to collections of articles that
discuss the same event. In the following two doc-
uments on the Nexus One from different sources,
we take the subjects of the identical verb release?
Google and the Mountain View giant?as coreferent.
Document 1: Google has released a software update.
Document 2: The Mountain View giant released an update.
Based on this idea, we introduce a new unsuper-
vised method that uses verbs in comparable corpora
897
as pivots for extracting the hard cases of corefer-
ence resolution, and build a dictionary of opaque
coreferent mentions (i.e., the dictionary entries are
pairs of mentions). This dictionary is then inte-
grated into the Stanford coreference system (Lee et
al., 2011), resulting in an average 1% improvement
in the F1 score of all the evaluation measures.
Our work points out the importance of context to
decide whether a specific mention pair is coreferent.
On the one hand, we need to know what semantic
relations are potentially coreferent (e.g., content and
video). On the other, we need to distinguish contexts
that are compatible for coreference?(1) and (2-a)?
from those that are not?(1) and (2-b).
(1) Elemental helps those big media entities process
content across a full slate of mobile devices.
(2) a. Elemental provides the picks and shovels to
make video work across multiple devices.
b. Elemental is powering the video for HBO Go.
Our dictionary of opaque coreferent pairs is our so-
lution to the first problem, and we report on some
preliminary work on context compatibility to ad-
dress the second problem.
2 Building a Dictionary for Coreference
To build a dictionary of semantic relations that are
appropriate for coreference we will use a cluster of
documents about the same news event, which we
call a story. Consider as an example the story Sprint
blocks out vacation days for employees. We deter-
mine using tf-idf the representative verbs for this
story, the main actions and events of the story (e.g.,
block out). Since these verbs are representative of
the story, different instances across documents in the
cluster are likely to refer to the same events (Sprint
blocks out. . . and the carrier blocks out. . . ). By the
same logic, the subjects and objects of the verbs are
also likely to be coreferent (Sprint and the carrier).
2.1 Comparable corpus
To build our dictionary, we require a monolingual
comparable corpus, containing clusters of docu-
ments from different sources that discuss the same
story. To ensure likely coreference, the story must
be the very same; documents that are merely clus-
tered by (general) topic do not suffice. The corpus
does not need to be parallel in the sense that docu-
ments in the same cluster do not need to be sentence
aligned.
We used Techmeme,2 a news aggregator for tech-
nology news, to construct a comparable corpus. Its
website lists the major tech stories, each with links
to several articles from different sources. We used
the Readability API3 to download and extract the ar-
ticle text for each document. We scraped two years
worth of data from Techmeme and only took stories
containing at least 5 documents. Our corpus con-
tains approximately 160 million words, 25k stories,
and 375k documents. Using a corpus from Tech-
meme means that our current coreference dictionary
is focused on the technological domain. Our method
can be easily extended to other domains, however,
since getting comparable corpora is relatively sim-
ple from the many similar news aggregator sites.
2.2 Extraction
After building our corpus, we used Stanford?s
CoreNLP tools4 to tokenize the text and annotate it
with POS tags and named entity types. We parsed
the text using the MaltParser 1.7, a linear time de-
pendency parser (Nivre et al, 2004).5
We then extracted the representative verbs of each
story by ranking the verbs in each story according
to their tf-idf scores. We took the top ten to be the
representative set. For each of these verbs, we clus-
tered together its subjects and objects (separately)
across instances of the verb in the document clus-
ter, excluding pronouns and NPs headed by the same
noun. For example, suppose that crawl is a represen-
tative verb and that in one document we have Google
crawls web pages and The search giant crawls sites
in another document. We will create the clusters
{Google, the search giant} and {web pages, sites}.
When detecting representative verbs, we kept
phrasal verbs as a unit (e.g., give up) and excluded
auxiliary and copular verbs,6 light verbs,7 and report
2http://www.techmeme.com
3http://www.readability.com/developers/api
4http://nlp.stanford.edu/software/corenlp.shtml
5http://www.maltparser.org
6Auxiliary and copular verbs include appear, be, become,
do, have, seem.
7Light verbs include do, get, give, go, have, keep, make, put,
set, take.
898
verbs,8 as they are rarely representative of a story
and tend to add noise to our dictionary. To increase
recall, we also considered the synonyms from Word-
Net and nominalizations from NomBank of the rep-
resentative verbs, thus clustering together the sub-
jects and objects of any synonym as well as the ar-
guments of nominalizations.9 We used syntactic re-
lations instead of semantic roles because the Malt-
Parser is faster than any SRL system, but we checked
for frequent syntactic structures in which the agent
and patient are inverted, such as passive and ergative
constructions.10
From each cluster of subject or object mentions,
we generated all pairs of mentions. This forms the
initial version of our dictionary. The next sections
describe how we filter and generalize these pairs.
2.3 Filtering
We manually analyzed 200 random pairs and clas-
sified them into coreference and spurious relations.
The spurious relations were caused by errors due to
the parser, the text extraction, and violations of our
algorithm assumption (i.e., the representative verb
does not refer to a unique event). We employed a fil-
tering strategy to improve the precision of the dictio-
nary. We used a total of thirteen simple rules, which
are shown in Table 1. For instance, we sometimes
get the same verb with non-coreferent arguments,
especially in tech news that compare companies or
products. In these cases, NEs are often used, and so
we can get rid of a large number of errors by auto-
matically removing pairs in which both mentions are
NEs (e.g., Google and Samsung).
Before filtering, 53% of all relations were good
coreference relations versus 47% spurious ones. Of
the relations that remained after filtering, 74% were
8Report verbs include argue, claim, say, suggest, tell, etc.
9As a general rule, we extract possessive phrases as subjects
(e.g. Samsung?s plan) and of -phrases as objects (e.g. develop-
ment of the new logo).
10We can easily detect passive subjects (i-b) as they have their
own dependency label, and ergative subjects (ii-b) using a list
of ergative verbs extracted from Levin (1993).
(i) a. Developers hacked the device.
b. The device was hacked.
(ii) a. Police scattered the crowds.
b. The crowds scattered.
Both mentions are NEs
Both mentions appear in the same document
Object of a negated verb
Enumeration or list environment
Sentence is ill-formed
Number NE
Temporal NE
Quantifying noun
Coordinated
Verb is preceded by a determiner or an adjective
Head is not nominal
Sentence length ? 100
Mention length ? 70% of sentence length
Table 1: Filters to improve the dictionary precision. Un-
less otherwise noted, the filter was applied if either men-
tion in the relation satisfied the condition.
coreferent and only 26% were spurious. In total,
about half of the dictionary relations were removed
in the filtering process, resulting in a total of 128,492
coreferent pairs.
2.4 Generalization
The final step of generating our dictionary is to pro-
cess the opaque mention pairs so that they gener-
alize better. We strip mentions of any determiners,
relative clauses, and -ing and -ed clauses. However,
we retain adjectives and prepositional modifiers be-
cause they are sometimes necessary for corefer-
ence to hold (e.g., online piracy and distribution
of pirated material). We also generalize NEs to
their types so that our dictionary entries can func-
tion as templates (e.g., Cook?s departure becomes
<person>?s departure), but we keep NE tokens that
are in the head position as these are pairs containing
world knowledge (e.g., iPad and slate). Finally, we
replace all tokens with their lemmas. Table 2 shows
a snapshot of the dictionary.
2.5 Semantics of coreference
From manually classifying a sample of 200 dictio-
nary pairs (e.g., Table 2), we find that our dictio-
nary includes many synonymy (e.g., IPO and offer-
ing) and hypernymy relations (e.g., phone and de-
vice), which are the relations that are typically ex-
tracted from ontologies for coreference resolution.
However, not all synonyms and hypernyms are valid
for coreference (recall the voters-people vs. scien-
tists-people example in the introduction), so our dic-
899
Mention 1 Mention 2
offering IPO
user consumer
phone device
Apple company
hardware key digital lock
iPad slate
content photo
bug issue
password login information
Google search giant
site company
filing complaint
company government
TouchPad tablet
medical record medical file
version handset
information credit card
government chairman
app software
Android platform
the leadership change <person>?s departure
change update
Table 2: Coreference relations in our dictionary.
tionary only includes the ones that are relevant for
coreference (e.g., update and change). Furthermore,
only 3% of our 128,492 opaque pairs are related in
WordNet, confirming that our method is introducing
a large number of new semantic relations.
We also discover other semantic relations that are
relevant for coreference, such as various metonymy
relations like mentioning the part for the whole.
Again though, we can use some part-whole rela-
tions coreferentially (e.g., car and engine) but not
others (e.g., car and window). Our dictionary in-
cludes part-whole relations that have been observed
as coreferent at least once (e.g., company and site).
We also extract world-knowledge descriptions for
NEs (e.g., Google and the Internet giant).
3 Integration into a Coreference System
We next integrated our dictionary into an existing
coreference resolution system to see if it improves
resolution.
3.1 Stanford coreference resolution system
Our baseline is the Stanford coreference resolution
system (Lee et al, 2011) which was the highest-
scoring system in the CoNLL-2011 Shared Task,
Sieve number Sieve name
1 Discourse processing
2 Exact string match
3 Relaxed string match
4 Precise constructs
5?7 Strict head match
8 Proper head noun match
9 Relaxed head match
10 Pronoun match
Table 3: Rules of the baseline system.
and was also part of the highest-scoring system in
the CoNLL-2012 Shared Task (Fernandes et al,
2012). It is a rule-based system that includes a to-
tal of ten rules (or ?sieves?) for entity coreference,
shown in Table 3. The sieves are applied from high-
est to lowest precision, each rule extending entities
(i.e., mention clusters) built by the previous tiers, but
never modifying links previously made. The major-
ity of the sieves rely on string overlap.11
The highly modular architecture made it easy for
us to integrate additional sieves using our dictionary
to increase recall.
3.2 Dictionary sieves
We propose four new sieves, each one using a differ-
ent granularity level from our dictionary, with each
consecutive sieve using higher precision relations
than the previous one. The Dict 1 sieve uses only
the heads of mentions in each relation (e.g., devices).
Dict 2 uses the heads and one premodifier, if it ex-
ists (e.g., iOS devices). Dict 3 uses the heads and up
to two premodifiers (e.g., new iOS devices). Dict 4
uses the full mentions, including any postmodifiers
(e.g., new iOS devices for businesses).
We take advantage of frequency counts to get rid
of low-precision coreference pairs and only keep
(i) pairs that have been seen more than 75 times
(Dict 1) or 15 times (Dict 2, Dict 3, Dict 4);
and (ii) pairs with a frequency count larger than 8
(Dict 1) or 2 (Dict 2, Dict 3, Dict 4) and a normal-
ized PMI score larger than 0.18. We use the nor-
malized PMI score (Bouma, 2009) as a measure of
association between the mentions mi and mj of a
11Exceptions: sieve 1 links first-person pronouns inside a
quotation with the speaker; sieve 4 links mention pairs that ap-
pear in an appositive, copular, acronym, etc., construction; sieve
10 implements generic pronominal coreference resolution.
900
dictionary pair, computed as
(ln p(mi,mj)p(mi)p(mj)) /? ln p(mi,mj)
These thresholds were set on the development set.
Since the different coreference rules in the Stan-
ford system are arranged in decreasing order of pre-
cision, we start by applying the sieve that uses the
highest-precision relations in the dictionary (Dict 4),
followed by Dict 3, Dict 2, and Dict 1. We add
these new sieves right before the last sieve, as the
pronominal sieve can perform better if opaque men-
tions have been successfully linked. The current
sieves only use the dictionary for linking singular
mentions, as the experiments on the dev showed that
plural mentions brought too much noise.
For any mention pair under analysis, each sieve
checks whether it is supported by the dictionary as
well as whether basic constraints are satisfied, such
as number, animacy and NE-type agreement, and
NE?common noun order (not the opposite).
4 Experiments
4.1 Data
Although our dictionary creation technology can ap-
ply across domains, our current coreference dictio-
nary is focused on the technical domain, so we cre-
ated a coreference labeled corpus in this domain for
evaluation. We extracted new data from Techmeme
(different from that used to extract the dictionary) to
create a development and a test set. It is important
to note that we do not need comparable data at this
stage. A massive comparable corpus is only needed
for mining the coreference dictionary (Section 2);
once it is built, it can be used for solving corefer-
ence within and across documents.
The annotation was performed by two experts, us-
ing the Callisto annotation tool. The development
and test sets were annotated with coreference rela-
tions following the OntoNotes guidelines (Pradhan
et al, 2007). We annotated full NPs (with all mod-
ifiers), excluding appositive phrases and predicate
nominals. Only premodifiers that were proper nouns
or possessive phrases were annotated. We extended
the OntoNotes guidelines by also annotating single-
tons. Table 4 shows the dataset statistics.
Dataset Stories Docs Tokens Entities Mentions
Dev 4 27 7837 1360 2279
Test 24 24 8547 1341 2452
Table 4: Dataset statistics: development (dev) and test.
4.2 Evaluation measures
We evaluated using six coreference measures, as
they sometimes provide different results and there is
no agreement on a standard. We used the scorer of
the CoNLL-2011 Shared Task (Pradhan et al, 2011).
? MUC (Vilain et al, 1995). Link-based metric
that measures how many links the true and sys-
tem partitions have in common.
? B3 (Bagga and Baldwin, 1998). Mention-based
metric that measures the proportion of mention
overlap between gold and predicted entities.
? CEAF-?3 (Luo, 2005). Mention-based metric
that, unlike B3, enforces a one-to-one align-
ment between gold and predicted entities.
? CEAF-?4 (Luo, 2005). The entity-based ver-
sion of the above metric.
? BLANC (Recasens and Hovy, 2011). Link-
based metric that considers both coreference
and non-coreference links.
? CoNLL (Denis and Baldridge, 2009). Average
of MUC, B3 and CEAF-?4. It was the official
metric of the CoNLL-2011 Shared Task.
4.3 Results
We always start from the baseline, which corre-
sponds to the Stanford system with the sieves listed
in Table 3. This is the set of sieves that won the
CoNLL-2011 Shared Task (Pradhan et al, 2011),
and they exclude WordNet.
Table 5 shows the incremental scores, on the de-
velopment set, for the four sieves that use the dictio-
nary, corresponding to the different granularity lev-
els, from the highest precision one (Dict 4) to the
lowest one (Dict 1). The largest improvement is
achieved by Dict 4 and Dict 3, as they improve re-
call (R) without hurting precision (P). R is equiva-
lent to P for CEAF-?4, and vice versa. The other
two sieves increase R further, especially Dict 1,
but also decrease P, although the trade-off for the
F-score (F1) is still positive. It is the best score, with
the exception of B3.
901
MUC B3 CEAF-?3 CEAF-?4 BLANC CoNLL
System R P F1 R P F1 R / P / F1 R P F1 R P B F1
Baseline 55.9 72.8 63.3 74.1 89.8 81.2 74.6 85.2 73.6 79.0 66.6 87.1 72.6 74.5
+Dict 4 57.0 72.8 63.9 75.1 89.4 81.6 75.3 85.2 74.3 79.4 68.2 87.3 74.2 75.0
+Dict 3 57.6 72.8 64.3 75.4 89.3 81.7 75.5 85.1 74.6 79.5 68.4 87.2 74.4 75.2
+Dict 2 57.6 72.5 64.2 75.4 89.1 81.7 75.4 85.0 74.6 79.5 68.4 87.0 74.3 75.1
+Dict 1 58.4 71.9 64.5 75.7 88.5 81.6 75.5 84.6 75.1 79.6 68.6 86.6 74.4 75.2
Table 5: Incremental results for the four sieves using our dictionary on the development set. Baseline is the Stanford
system without the WordNet sieves. Scores are on gold mentions.
MUC B3 CEAF-?3 CEAF-?4 BLANC CoNLL
System R P F1 R P F1 R / P / F1 R P F1 R P B F1
Baseline 62.4 78.2 69.4 73.7 89.5 80.8 75.1 86.2 73.8 79.5 71.4 88.6 77.3 76.6
w/ WN 63.5 75.3 68.9 74.2 87.5 80.3 74.1 83.7 74.1 78.6 71.8 87.3 77.3 75.9
w/ Dict 64.7* 77.6* 70.6* 75.7* 88.5* 81.6* 76.5* 85.3* 75.0* 79.9* 74.6* 88.6 79.9* 77.3*
w/ Dict +
Context
64.8* 77.8* 70.7* 75.7* 88.6* 81.7* 76.5* 85.5* 75.1* 80.0* 74.6* 88.7 79.9* 77.5*
Table 6: Performance on the test set. Scores are on gold mentions. Stars indicate a statistically significant difference
with respect to the baseline.
Table 6 reports the scores on the test set and com-
pares the scores obtained by adding the WordNet
sieves to the baseline (w/ WN) with those obtained
by adding the dictionary sieves (w/ Dict). Whereas
adding WordNet only brings a small improvement
in R that is much lower than the loss in P, the dic-
tionary sieves succeed in increasing R by a larger
amount and at a smaller cost to P, resulting in a sig-
nificant improvement in F1: 1.2 points according to
MUC, 0.8 points according to B3, 1.4 points accord-
ing to CEAF-?3, 0.4 points according to CEAF-?4,
2.6 points according to BLANC, and 0.7 points ac-
cording to CoNLL. Section 5.2 presents the last line
(w/ Dict + Context).
5 Discussion
5.1 Error analysis
Thanks to the dictionary, the coreference system im-
proves the baseline by establishing coreference links
between the bolded mentions in (3) and (4).
(3) With Groupon Inc.?s stock down by half from its IPO
price and the company heading into its first earnings
report since an accounting blowup [...] outlining op-
portunity ahead and the promise of new products for
the daily-deals company.
(4) Thompson revealed the diagnosis as evidence arose
that seemed to contradict his story about why he was
not responsible for a degree listed on his resume that
he does not have, the newspaper reports, citing anony-
mous sources familiar with the situation [...] a Yahoo
board committee appointed to investigate the matter.
The first case requires world knowledge and the sec-
ond case, semantic knowledge.
We manually analyzed 40 false positive errors
caused by the dictionary sieves. Only a small num-
ber of them were due to noise in the dictionary. The
majority of errors were due to the discourse context:
the two mentions could be coreferent, but not in the
given context. For example, Apple and company are
potentially coreferent?which is successfully cap-
tured by our dictionary?and while they are coref-
erent in (5), they are not in (6).12
(5) It will only get better as Apple will be updating it
with iOS6, an operating system that the company will
likely be showing off this summer.
(6) Since Apple reinvented the segment, Microsoft is the
latest entrant into the tablet market, banking on its
Windows 8 products to bridge the gap between PCs
and tablets. [...] The company showed off Windows 8
last September.
12Examples in this section show gold coreference relations in
bold and incorrectly predicted coreferent mentions in italics.
902
In these cases it does not suffice to check whether
the opaque mention pair is included in the corefer-
ence dictionary, but we need a method for taking the
surrounding context into account. In the next section
we present our preliminary work in this direction.
5.2 Context fit
To help the coreference system choose the right an-
tecedent in examples like (6), we exploit the fact
that the company is closely followed by Windows 8,
which is a clue for selecting Microsoft instead of Ap-
ple as the antecedent. We devise a contextual con-
straint that rules out a mention pair if the contexts are
incompatible. To check for context compatibility,
we borrow the idea of topic signatures from Lin and
Hovy (2000) and that Agirre et al (2001) used for
Word Sense Disambiguation. Instead of identifying
the keywords of a topic, we find the NEs that tend
to co-occur with another NE. For example, the sig-
nature for Apple should include terms like iPhone,
MacBook, iOS, Steve Jobs, etc. This is what we call
the NE signature for Apple.
To construct NE signatures, we first compute the
log-likelihood ratio (LLR) statistic between NEs in
our corpus (the same one used to build the dictio-
nary). Then, the signature for a NE, w, is the list of
k other NEs that have the highest LLR with w. The
LLR between two NEs, w1 and w2, is ?2 ln
L(H1)
L(H2)
,
where H1 is the hypothesis that
P (w1 ? sent|w2 ? sent) = P (w1 ? sent|w2 /? sent),
H2 is the hypothesis that
P (w1 ? sent|w2 ? sent) 6= P (w1 ? sent|w2 /? sent),
and L(?) is the likelihood. We assume a binomial
distribution for the likelihood.
Once we have NE signatures, we determine the
context fit as follows. When the system compares a
NE antecedent with a (non-NE) anaphor, we check
whether any NEs in the anaphor?s sentence are in
the antecedent?s signature. We also check whether
the antecedent is in the signature list of any NE?s in
the anaphor?s sentence. If neither of these is true,
we do not allow the system to link the antecedent
and the anaphor. In (6), Apple is not linked with the
company because it is not in Windows? signature,
and Windows is not in Apple?s signature either (but
Microsoft is in Windows? signature).
The last two lines in Table 6 compare the scores
without using this contextual feature (w/ Dict) with
those using context (w/ Dict + Context). Our feature
for context compatibility leads to a small but posi-
tive improvement, taking the final improvement of
the dictionary sieves to be about 1 percentage point
above the baseline according to all six evaluation
measures. We leave as future work to test this idea
on a larger test set and refine it further so as to ad-
dress more challenging cases where comparing NEs
is not enough, like in (7).
(7) Snapchat will notify users [...] The program is avail-
able for free in Apple?s App Store [...] While the com-
pany ?attempts to delete image data as soon as possi-
ble after the message is transmitted,? it cannot guaran-
tee messages will always be deleted.
To resolve (7), it would be helpful to know that
Snapchat is a picture messaging platform, as the
context mentions image data and messages.
6 Related Work
Existing ontologies are not optimal for solving
opaque coreferent mentions because of both a preci-
sion and a recall problem (Lee et al, 2011; Uryupina
et al, 2011). On the other hand, using data-driven
methods such as distributional semantics for coref-
erence resolution suffers especially from a precision
problem (Ng, 2007). Our work combines ideas from
distributional semantics and paraphrase acquisition
methods in order to efficiently use contextual infor-
mation to extract coreference relations.
The main idea that we borrow from paraphrase
acquisition is the use of monolingual (non-parallel)
comparable corpora, which have been exploited
to extract both sentence-level (Barzilay and McK-
eown, 2001) and sub-sentential-level paraphrases
(Shinyama and Sekine, 2003; Wang and Callison-
Burch, 2011). To ensure that the NPs are coreferent,
we limit the meaning of comparable corpora to col-
lections of documents that report on the very same
story, as opposed to collections of documents that
are about the same (general) topic. However, the
distinguishing factor is that while most paraphrasing
studies, including Lin and Pantel (2001), use NEs?
or nouns in general?as pivots to learn paraphrases
of their surrounding context, we use verbs as pivots
to learn coreference relations at the NP level.
There are many similarities between paraphrase
and coreference, and our work is most similar to
903
that by Wang and Callison-Burch (2011). However,
some paraphrases that might not be considered to
be valid (e.g., under $200 and around $200) can
be acceptable coreference relations. Unlike Wang
and Callison-Burch (2011), we do not work on doc-
ument pairs but on sets of at least five (comparable)
documents, and we do not require sentence align-
ment, but just verb alignment.
Another source of inspiration is the work by Bean
and Riloff (2004). They use contextual roles (i.e.,
the role that an NP plays in an event) for extract-
ing patterns that can be used in coreference reso-
lution, showing the relevance of verbs in deciding
on coreference between their arguments. However,
they use a very small corpus (two domains) and do
not aim to build a dictionary. The idea of creating
a repository of extracted concept-instance relations
appears in Fleischman et al (2003), but restricted
to person-role pairs, e.g. Yasser Arafat and leader.
Although it was originally designed for answering
who-is questions, Daume? III and Marcu (2005) suc-
cessfully used it for coreference resolution.
The coreference relations that we extract might
overlap but go beyond those detected by Bansal and
Klein (2012)?s Web-based features. First, they focus
on NP headwords, while we extract full NPs, includ-
ing multi-word mentions. Second, the fact that they
use the Google n-gram corpus means that the two
headwords must appear at most four words apart,
thus ruling out coreferent mentions that can only ap-
pear far from each other. Finally, while their extrac-
tion patterns focus on synonymy and hypernymy re-
lations, we discover other types of semantic relations
that are relevant for coreference (Section 2.5).
7 Conclusions
We have pointed out an important problem with cur-
rent coreference resolution systems: their heavy re-
liance on string overlap. Pronouns aside, opaque
mentions account for 65% of the errors made by
state-of-the-art systems. To improve coreference
scores beyond 60-70%, we therefore need to make
better use of semantic and world knowledge to deal
with non-identical-string coreference. But, as we
have also shown, coreference is not the same as se-
mantic similarity or hypernymy. Only certain se-
mantic relations in certain contexts are good cues for
coreference. We therefore need semantic resources
specifically targeted at coreference.
We proposed a new solution for detecting opaque
mention pairs: restricting distributional similarity to
a comparable corpus of articles about the very same
story, thus ensuring that similar mentions will also
likely be coreferent. We used this corpus to build a
dictionary focused on coreference, and successfully
extracted the specific semantic and world knowledge
relevant for coreference. The resulting dictionary
can be added on top of any coreference system to
increase recall at a minimum cost to precision. Inte-
grated into the Stanford coreference resolution sys-
tem, which won the CoNLL-2011 shared task, the
F-score increases about 1 percentage point accord-
ing to all of the six evaluation measures. The dictio-
nary and NE signatures are available on the Web.13
We showed that apart from the need for extracting
coreference-specific semantic and world knowledge,
we need to take into account the context surrounding
the mentions. The results from our preliminary work
for identifying incompatible contexts is promising.
Our unsupervised method for extracting opaque
coreference relations can be easily extended to other
domains by using online news aggregators, and
trained on more data to build a more comprehensive
dictionary that can increase recall even further. We
integrated the dictionary into a rule-based corefer-
ence system, but it remains for future work to in-
tegrate it into a learning-based architecture, where
the system can combine the dictionary features with
other features. This can also make it easier to in-
clude contextual features that take into account how
well a dictionary pair fits in a specific context.
Acknowledgments
We would like to thank the members of the Stanford
NLP Group, Valentin Spitkovsky, and Ed Hovy for
valuable comments at various stages of the project.
The first author was supported by a Beatriu de
Pino?s postdoctoral scholarship (2010 BP-A 00149)
from Generalitat de Catalunya. We also gratefully
acknowledge the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181.
13http://nlp.stanford.edu/pubs/coref-dictionary.zip
904
References
Eneko Agirre, Olatz Ansa, David Martinez, and Eduard
Hovy. 2001. Enriching wordnet concepts with topic
signatures. In Proceedings of the NAACLWorkshop on
WordNet and Other Lexical Resources: Applications,
Extensions and Customizations, pages 23?28.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC 1998 Workshop on Linguistic Coreference,
pages 563?566.
Mohit Bansal and Dan Klein. 2012. Coreference seman-
tics from web features. In Proceedings of ACL, pages
389?398.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL, pages 50?57.
David Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference reso-
lution. In Proceedings of NAACL-HTL.
Geolof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceedings
of the Biennial GSCL Conference, pages 31?40.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
HLT-EMNLP, pages 97?104.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
42:87?96.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidiu?.
2012. Latent structure perceptron with feature induc-
tion for unrestricted coreference resolution. In Pro-
ceedings of CoNLL - Shared Task, pages 41?48.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online question
answering: answering questions before they are asked.
In Proceedings of ACL, pages 1?7.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of HLT-NAACL, pages 385?393.
Hamidreza Kobdani, Hinrich Schu?tze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrap-
ping coreference resolution using word associations.
In Proceedings of ACL, pages 783?792.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 Shared Task. In Proceedings
of CoNLL - Shared Task, pages 28?34.
Beth Levin. 1993. English Verb Class and Alternations:
A Preliminary Investigation. University of Chicago
Press, Chicago.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summarization.
In Proceedings of COLING, pages 495?501.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discov-
ery of inference rules from text. In Proceedings of the
ACM SIGKDD, pages 323?328.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT-EMNLP, pages
25?32.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of IJCAI, pages 1689?
1694.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL, pages 49?56.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of HLT-NAACL, pages 192?199.
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events in
OntoNotes. In Proceedings of ICSC, pages 446?453.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
CoNLL - Shared Task, pages 1?27.
Altaf Rahman and Vincent Ng. 2011. Coreference reso-
lution with world knowledge. In Proceedings of ACL,
pages 814?824.
Marta Recasens and Eduard Hovy. 2010. Corefer-
ence resolution across corpora: Languages, coding
schemes, and preprocessing information. In Proceed-
ings of ACL, pages 1423?1432.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(4):485?510.
Yusuke Shinyama and Satoshi Sekine. 2003. Paraphrase
acquisition for information extraction. In Proceedings
of ACL, pages 65?71.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-art.
In Proceedings of ACL-IJCNLP, pages 656?664.
Olga Uryupina, Massimo Poesio, Claudio Giuliano, and
Kateryna Tymoshenko. 2011. Disambiguation and
filtering methods in using web knowledge for coref-
erence resolution. In Proceedings of FLAIRS, pages
317?322.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45?52.
905
Rui Wang and Chris Callison-Burch. 2011. Para-
phrase fragment extraction from monolingual compa-
rable corpora. In Proceedings of the 4th ACL Work-
shop on Building and Using Comparable Corpora,
pages 52?60.
Xiaofeng Yang and Jian Su. 2007. Coreference resolu-
tion using semantic relatedness information from auto-
matically discovered patterns. In Proceedings of ACL,
pages 528?535.
906
Proceedings of NAACL-HLT 2013, pages 1072?1081,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Emergence of Gricean Maxims from Multi-Agent Decision Theory
Adam Vogel, Max Bodoia, Christopher Potts, and Dan Jurafsky
Stanford University
Stanford, CA, USA
{acvogel,mbodoia,cgpotts,jurafsky}@stanford.edu
Abstract
Grice characterized communication in terms
of the cooperative principle, which enjoins
speakers to make only contributions that serve
the evolving conversational goals. We show
that the cooperative principle and the associ-
ated maxims of relevance, quality, and quan-
tity emerge from multi-agent decision theory.
We utilize the Decentralized Partially Observ-
able Markov Decision Process (Dec-POMDP)
model of multi-agent decision making which
relies only on basic definitions of rationality
and the ability of agents to reason about each
other?s beliefs in maximizing joint utility. Our
model uses cognitively-inspired heuristics to
simplify the otherwise intractable task of rea-
soning jointly about actions, the environment,
and the nested beliefs of other actors. Our
experiments on a cooperative language task
show that reasoning about others? belief states,
and the resulting emergent Gricean commu-
nicative behavior, leads to significantly im-
proved task performance.
1 Introduction
Grice (1975) famously characterized communica-
tion among rational agents in terms of an overarch-
ing cooperative principle and a set of more specific
maxims, which enjoin speakers to make contribu-
tions that are truthful, informative, relevant, clear,
and concise. Since then, there have been many at-
tempts to derive the maxims (or perhaps just their ef-
fects) from more basic cognitive principles concern-
ing how people make decisions, formulate plans,
and collaborate to achieve goals. This research
traces to early work by Lewis (1969) on signaling
systems. It has recently been the subject of ex-
tensive theoretical discussion (Clark, 1996; Merin,
1997; Blutner, 1998; Parikh, 2001; Beaver, 2002;
van Rooy, 2003; Benz et al, 2005; Franke, 2009)
and has been tested experimentally using one-step
games in which the speaker produces a message and
the hearer ventures a guess as to its intended refer-
ent (Rosenberg and Cohen, 1964; Dale and Reiter,
1995; Golland et al, 2010; Stiller et al, 2011; Frank
and Goodman, 2012; Krahmer and van Deemter,
2012; Degen and Franke, 2012; Rohde et al, 2012).
To date, however, these theoretical models and ex-
periments have not been extended to multi-step in-
teractions extending over time and involving both
language and action together, which leaves this work
relatively disconnected from research on planning
and goal-orientation in artificial agents (Perrault
and Allen, 1980; Allen, 1991; Grosz and Sidner,
1986; Bratman, 1987; Hobbs et al, 1993; Allen
et al, 2007; DeVault et al, 2005; Stone et al,
2007; DeVault, 2008). We attribute this in large
part to the complexity of Gricean reasoning itself,
which requires agents to model each other?s belief
states. Tracking these as they evolve over time in re-
sponse to experiences is extremely demanding. Our
approach complements slot-filling dialog systems,
where the focus is on managing speech recogni-
tion uncertainty (Young et al, 2010; Thomson and
Young, 2010).
However, recent years have seen significant ad-
vances in multi-agent decision-theoretic models and
their efficient implementation. With the current pa-
per, we seek to show that the Decentralized Par-
1072
tially Observable Markov Decision Process (Dec-
POMDP) provides a robust, flexible foundation for
implementing agents that communicate in a Gricean
manner. Dec-POMDPs are multi-agent, partially-
observable models in which agents maintain be-
lief distributions over the underlying, hidden world
state, including the beliefs of the other players, and
speech actions change those beliefs. In this setting,
informative, relevant communication emerges as the
best way to maximize joint utility.
The complexity of pragmatic reasoning is still
forbidding, though. Correspondingly, optimal de-
cision making in Dec-POMDPs is NEXP complete
(Bernstein et al, 2002). To manage this issue, we
introduce several cognitively-plausible approxima-
tions which allow us to simplify the Dec-POMDP to
a single-agent POMDP, for which relatively efficient
solvers exist (Spaan and Vlassis, 2005). We demon-
strate our algorithms on a variation of the Cards task,
a partially-observable collaborative search problem
(Potts, 2012). Spatial language comprises the bulk
of communication in the Cards task, and we dis-
cuss a model of spatial semantics in Section 3. Us-
ing this task and a model of the meaning of spatial
language, we next discuss two agents that play the
game: ListenerBot (Section 4) makes decisions us-
ing a single-agent POMDP that does not take into
account the beliefs or actions of its partner, whereas
DialogBot (Section 5) maintains a model of its part-
ner?s beliefs. As a result of the cooperative structure
of the underlying model and the effects of commu-
nication within it, DialogBot?s contributions are rel-
evant, truthful, and informative, which leads to sig-
nificantly improved task performance.
2 The Cards Task and Corpus
The Cards corpus consists of 1,266 transcripts1 from
an online, two-person collaborative game in which
two players explore a maze-like environment, com-
municating with each other via a text chat window
(Figure 1). A deck of playing cards has been dis-
tributed randomly around the environment, and the
players? task is to find six consecutive cards of the
same suit. Our implemented agents solve a sim-
plified version of this task in which the two agents
1Released by Potts (2012) at http://cardscorpus.
christopherpotts.net
Figure 1: The Cards corpus gameboard. Player 1?s
location is marked ?P1?. The nearby yellow boxes
mark card locations. The dialogue history and chat
window are at the top. This board, the one we use
throughout, consists of 231 open grid squares.
must both end up co-located with a single card, the
Ace of Spades (AS). This is much simpler than the
six-card version from the human?human corpus, but
it involves the same kind of collaborative goal and
forces our agents to deal with the same kind of par-
tial knowledge about the world as the humans did.
Each agent knows its own location, but not his part-
ner?s, and a player can see the AS only when co-
located with it. The agents use (simplified) English
to communicate with each other.
3 Spatial Semantics
Much of the communication in the Cards task in-
volves referring to spatial locations on the board.
Accordingly, we focus on spatial language for our
artificial agents. In this section, we present a model
of spatial semantics, which we create by leveraging
the human?human Cards transcripts. We discuss the
spatial semantic representation, how we classify the
semantics of new locative expressions, and our use
of spatial semantics to form a high-level state space
for decision making.
3.1 Semantic Representation
Potts (2012) released annotations, derived from the
Cards corpus, which reduce 599 of the players?
statements about their locations to formulae of the
form ? (?1 ? ?? ? ? ?k), where ? is a domain and
?1, . . . ,?k are semantic literals. For example, the ut-
terance ?(I?m) at the top right of the board? is anno-
tated as BOARD(top? right), and ?(I?m) in bottom
1073
of the C room? is annotated as C room(bottom). Ta-
ble 1 lists the full set of semantic primitives that ap-
pear as domain expressions and literals.
Because the Cards transcripts are so highly struc-
tured, we can interpret these expressions in terms
of the Cards world itself. For a given formula
? = ? (?1 ? ?? ? ? ?k), we compute the number of
times that a player identified its location with (an
utterance translated as) ? while standing on grid
square (x,y). These counts are smoothed using
a simple 2D-smoothing scheme, detailed in (Potts,
2012), and normalized in the usual manner to form a
distribution over board squares Pr((x,y)|?). These
grounded interpretations are the basis for commu-
nication between the artificial agents we define in
Section 4.
BOARD, SQUARE, right, middle, top, left, bot-
tom, corner, approx, precise, entrance, C room,
hall, room, sideways C, loop, reverse C,
U room, T room, deadend, wall, sideways F
Table 1: The spatial semantic primitives.
3.2 Semantics Classifier
Using the corpus examples of utterances paired with
their spatial semantic representations, we learn a set
of classifiers to predict a spatial utterance?s semantic
representation. We train a binary classifier for each
semantic primitive ?i using a log-linear model with
simple bag of words features. The words are not
normalized or stemmed and we use whitespace tok-
enization. We additionally train a multi-class clas-
sifier for all possible domains ? . At test time, we
use the domain classifier and each primitive binary
classifier to produce a semantic representation.
3.3 Semantic State Space
The decision making algorithms that we discuss in
Section 4 are highly sensitive to the size of the state
space. The full representation of the game board
consists of 231 squares. Representing the location
of both players and the location of the card requires
3233 = 12,326,391 states, well beyond the capabil-
ities of current decision-making algorithms.
To ameliorate this difficulty, we cluster squares
together using the spatial referring expression cor-
Figure 2: Semantic state space clusters with k = 16.
pus. This approach follows from research that
shows that humans? mental spatial representations
are influenced by their language (Hayward and Tarr,
1995). Our intuition is that human players do not
consider all possible locations of the card and play-
ers, but instead lump them into semantically coher-
ent states, such as ?the card is in the top right cor-
ner.? Following this intuition, we cluster states to-
gether which have similar referring expressions, al-
lowing our agents to use language as a cognitive
technology and not just a tool for communication.
For each board square (x,y) we form a vector
?(x,y) with ?i(x,y) = Pr((x,y)|?i), where ?i is the
ith distinct semantic representation in the corpus.
This forms a 136-dimensional vector for each board
square. We then use k-means clustering with a Eu-
clidean distance metric in this semantic space to
cluster states which are referred to similarly.
Figure 2 shows a clustering for k = 16 which we
utilize for the remainder of the paper. Denoting the
board regions by {1, . . . ,Nregions}, we compute the
probability of an expression ? referring to a region
r by averaging over the squares in the region:
Pr(r|?i) ? ?
(x,y)? region r
Pr((x,y)|?i)
|{(x,y)|(x,y) ? region r}|
4 ListenerBot
We first introduce ListenerBot, an agent that does
not take into account the actions or beliefs of its
partner. ListenerBot decides what actions to take
using a Partially Observable Markov Decision Pro-
cess (POMDP). This allows ListenerBot to track its
beliefs about the location of the card and to incor-
porate linguistic advice. However, ListenerBot does
not produce utterances.
1074
A POMDP is defined by a tuple
(S,A,T,O,?,R,b0,?). We explicate each com-
ponent with examples from our task. Figure 3(a)
provides the POMDP influence diagram.
States S is the finite state space of the world. The
state space S of ListenerBot consists of the location
of the player p and the location of the card c. As
discussed above in Section 3.3, we cluster squares of
the board into Nregions semantically coherent regions,
denoted by {1, . . . ,Nregions}. The state space over
these regions is defined as
S := {(p,c)|p,c ? {1, . . . ,Nregions}}
Two regions r1 and r2 are called adjacent, written
adj(r1,r2), if any of their constituent squares touch.
Actions A is the set of actions available to the
agent. ListenerBot can only take physical actions
and has no communicative ability. Physical actions
in our region-based state space are composed of two
types: traveling to a region and searching a region.
? travel(r): travel to region r
? search: player exhaustively searches the cur-
rent region
Transition Distributions The transition distribu-
tion T (s?|a,s) models the dynamics of the world.
This represents the ramifications of physical actions
such as moving around the map. For a state s =
(p,c) and action a = travel(r), the player moves to
region r if it is adjacent to p, and otherwise stays in
the same place:
T ((p?,c?)|travel(r),(p,c))=
?
???????
???????
1 adj(r, p)? p? = r
?c = c?
1 ?adj(r, p)? p = p?
?c = c?
0 otherwise
Search actions are only concerned with observations
and do not change the state of the world:2
T ((p?,c?)|search,(p,c)) = 1
[
p? = p? c? = c
]
The travel and search high-level actions are trans-
lated into low-level (up, down, left, right) actions
using a simple A? path planner.
Observations Agents receive observations from
a set O according to an observation distribution
2
1[Q] is the indicator function, which is 1 if proposition Q
is true and 0 otherwise.
?(o|s?,a). Observations include properties of the
physical world, such as the location of the card, and
also natural language utterances, which serve to in-
directly change agents? beliefs about the world and
the beliefs of their interlocutors.
Search actions generate two possible observa-
tions: ohere and o?here, which denote the presence
or absence of the card from the current region.
?(ohere|(p
?,c?),search) = 1
[
p? = c?
]
?(o?here|(p
?,c?),search) = 1
[
p? 6= c?
]
Travel actions do not generate meaningful observa-
tions:
?(o?here|(p
?,c?), travel) = 1
Linguistic Advice We model linguistic advice as
another form of observation. Agents receive mes-
sages from a finite set ?, and each message ? ? ?
has a semantics, or distribution over the state space
Pr(s|?). In the Cards task, we use the semantic dis-
tributions defined in Section 3. To combine the se-
mantics of language with the standard POMDP ob-
servation model, we apply Bayes? rule:
Pr(? |s) = Pr(s|?)Pr(?)
?? ? Pr(s|? ?)Pr(? ?)
(1)
The prior, Pr(?), can be derived from corpus data.
By treating language as just another form of ob-
servation, we are able to leverage existing POMDP
solution algorithms. This approach contrasts with
previous work on communication in Dec-POMDPs,
where agents directly share their perceptual obser-
vations (Pynadath and Tambe, 2002; Spaan et al,
2008), an assumption which does not fit natural lan-
guage.
Reward The reward function R(s,a) : S? R rep-
resents the goals of the agent, who chooses actions
to maximize reward. The goal of the Cards task is
for both players to be on top of the card, so any ac-
tion that leads to this state receives a high reward R+.
All other actions receive a small negative reward R?,
which gives agents an incentive to finish the task as
quickly as possible.
R((p,c),a) =
{
R+ p = c
R? p 6= c
Lastly, ? ? [0,1) is the discount factor, specifying
the trade-off between immediate and future rewards.
1075
s s?
o o?a
R
(a) ListenerBot POMDP
s s?
o1 o?1
o2 o?2
a1
a2
R
(b) Full Dec-POMDP
s s?
o o?a
R
s? s??
(c) DialogBot POMDP
Figure 3: The decision diagram for the ListenerBot POMDP, the full Dec-POMDP, and the DialogBot ap-
proximation POMDP. The ListenerBot (a) only considers his own location p and the card location c. In the
full Dec-POMDP (b), both agents receive individual observations and choose actions independently. Opti-
mal decision making requires tracking all possible histories of beliefs of the other agent. In diagram (c), Di-
alogBot approximates the full Dec-POMDP as single-agent POMDP. At each time step, DialogBot marginal-
izes out the possible observations o? that ListenerBot received, yielding an expected belief state b?.
Initial Belief State The initial belief state, b0 ?
?(S), is a distribution over the state space S. Lis-
tenerBot begins each game with a known initial lo-
cation p0 but a uniform distribution over the location
of the card c:
b0(p,c) ?
{
1
Nregions
p = p0
0 otherwise
Belief Update and Decision Making The key de-
cision making problem in POMDPs is the construc-
tion of a policy pi : ?(S)? A, a function from beliefs
to actions which dictates how the agent acts. Deci-
sion making in POMDPs proceeds as follows. The
world starts in a hidden state s0 ? b0. The agent
executes action a0 = pi(b0). The underlying hid-
den world state transitions to s1 ? T (s?|a0,s0), the
world generates observation o0 ? ?(o|s1,a0), and
the agent receives reward R(s0,a0). Using the obser-
vation o0, the agent constructs a new belief b1 ??(S)
using Bayes? rule:
bat ,ott+1 (s
?) = Pr(s?|at ,ot ,bt)
=
Pr(ot |at ,s?,bt)Pr(s?|at ,bt)
Pr(ot |bt ,at)
=
?(ot |s?,at)?s?S T (s
?|at ,s)bt(s)
?s???(ot |s??,at)?s?S T (s??|at ,s)bt(s)
This process is referred to as belief update and is
analogous to the forward algorithm in HMMs. To in-
corporate communication into the standard POMDP
model, we consider observations (o,?) ? O ? ?
which are a combination of a perceptual observation
o and a received message ? . The semantics of the
message ? is included in the belief update equation
using Pr(s|?), derived in Equation 1:
bat ,ot ,?tt+1 (s
?) =
?(o|s?,a) Pr(s
?|?)Pr(?)
?? ??? Pr(s
?|? ?)Pr(? ?) ?s?S T (s
?|a,s)bt(s)
?s???S?(o|s??,a)
Pr(s??|?)Pr(?)
?? ??? Pr(s
??|? ?)Pr(? ?) ?s?S T (s
??|a,s)bt(s)
Using this new belief state b1, the agent selects an
action a1 = pi(b1), and the process continues.
An initial belief state b0 and a policy pi to-
gether define a Markov chain over pairs of states
and actions. For a given policy pi , we define a
value function V pi : ?(S)? R which represents the
expected discounted reward with respect to that
Markov chain:
V pi(b0) =
?
?
t=0
? t E[R(bt ,at)|b0,pi]
The goal of the agent is find a policy pi? which max-
imizes the value of the initial belief state:
pi? = argmax
pi
V pi(b0)
Exact computation of pi? is PSPACE-complete (Pa-
padimitriou and Tsitsiklis, 1987), making approx-
imation algorithms necessary for all but the sim-
plest problems. We use Perseus (Spaan and Vlassis,
2005), an anytime approximate point-based value it-
1076
eration algorithm.
5 DialogBot
We now introduce DialogBot, a Cards agent which
is capable of producing linguistic advice. To decide
when and how to speak, DialogBot maintains a dis-
tribution over its partner?s beliefs and reasons about
the effects his utterances will have on those beliefs.
To handle these complexities, DialogBot models
the world as a Decentralized Partially Observable
Markov Decision Process (Dec-POMDP) (Bernstein
et al, 2002). See Figure 3(b) for the influence dia-
gram. The definition of Dec-POMDPs mirrors that
of the POMDP, with the following changes.
There is a finite set I of agents, which we re-
strict to two. Each agent takes an action ai at
each time step, forming a joint action ~a = (a1,a2).
Each agent receives its own observation oi accord-
ing to ?(o1,o2|a1,a2,s?). The transition distribu-
tions T (s?|a1,a2,s) and the reward R(s,a1,a2) both
depend on both agents? actions.
Optimal decision making in Dec-POMDPs re-
quires maintaining a probability distribution over
all possible sequences of actions and observations
(a?1, o?1, . . . , a?t , o?t) that the other player might have
received. As t increases, we have an exponential in-
crease in the belief states an agent must consider.
Confirming this informal intuition, decision mak-
ing in Dec-POMDPs is NEXP-complete, a complex-
ity class above P-SPACE (Bernstein et al, 2002).
This computational complexity limits the applica-
tion of Dec-POMDPs to very small problems. To
address this difficulty we make several simplifying
assumptions, allowing us to construct a single-agent
POMDP which approximates the full Dec-POMDP.
Firstly, we assume that other agents do not take
into account our own beliefs, i.e., the other agent
acts like a ListenerBot. This bypasses the infinitely
nested belief problem by assuming that other agents
track one less level of nested beliefs, a common
approach (Goodman and Stuhlmu?ller, 2012; Gmy-
trasiewicz and Doshi, 2005).
Secondly, instead of tracking the full tree of pos-
sible observation histories, we maintain a point es-
timate b? of the other agent?s beliefs, which we
term the expected belief state. Rather than track-
ing each possible observation/action history of the
other agent, at each time step we marginalize out
the observations they could have received. Figure 4
compares this approach with exact belief update.
Thirdly, we assume that the other agent acts ac-
cording to a variant of the QMDP approximation
(Littman et al, 1995). Under this approximation, the
other agent solves a fully-observable MDP version
of the ListenerBot POMDP, yielding an MDP pol-
icy p?i : S? A. This critically allows us to approxi-
mate the other agent?s belief update using a specially
formed POMDP, which we detail next.
State Space To construct the approximate single-
agent POMDP from the full Dec-POMDP problem,
we formulate the state space as S? S. (See Figure
3(c) for the influence diagram.) We write a state
(s, s?) ? S? S, where s is DialogBot?s beliefs about
the true state of the world, and s? is DialogBot?s esti-
mate of the other agent?s beliefs.
Transition Distribution The main difficulty
in constructing the approximate single-agent
POMDP is specifying the transition distribu-
tion T ((s?, s??)|a,(s, s?)). To address this, we
break this distribution into two components:
T ((s?, s??)|a,(s, s?)) = T? (s??|s?,a,(s, s?))T (s?|a,s, s?).
The first term dictates how DialogBot updates its
beliefs about the other agent?s beliefs:
T? (s??|s?,a,(s, s?)) = Pr(s??|s?,a,(s, s?))
=?
o??O
Pr(s??|a, o?, s?,s)Pr(o?|s?,a, p?i(s?))
=?
o??O
(
?(o?|s??,a, p?i(s?))T (s??|a, p?i(s?), s?)
?s????(o?|s???,a, p?i(s?))T (s???|a, p?i(s?), s?)
??(o?|s?,a, p?i(s?))
)
We sum over all observations o? the other agent could
have received, updating our probability of s?? as Lis-
tenerBot would have, multiplied by the probability
that ListenerBot would have received that observa-
tion, ?(o?|s?, p?i(s?)). The QMDP approximation al-
lows us to simulate ListenerBot?s belief update in
T? (s??|s?,a,(s, s?)). Exact belief update would require
access to b?: by using p?i(s?) we can estimate the action
that ListenerBot would have taken.
In cases where s? contradicts s such that for all o? ei-
ther ?(o?|s?, p?i(s?)) = 0 or ?(o?|s??, p?i(s?)) = 0, we redis-
tribute the belief mass uniformly: T? (s??|s?,a,(s, s?))?
1077
b?t
b?o1t+1
o1
b?o2t+1
o2
b?o1,o1t+2
o1
b?o1,o2t+2
o2
b?o2,o1t+2
o1
b?o2,o2t+2
o2
(a) Exact multi-agent belief tracking
b?t
o1
o2
o1
o2
b?t+1
o1
o2
o1
o2
b?t+2
(b) Approximate multi-agent belief tracking
Figure 4: Exact multi-agent belief tracking compared with our approximate approach. Each node represents
a belief state. In exact tracking (a), the agent tracks every possible history of observations that its partner
could have received, which grows exponentially in time. In approximate update (b), the agent considers each
possible observation and then averages the resulting belief states, weighted by the probability the other agent
received that observation, resulting in a single summary belief state b?t+1. Under the QMDP approximation,
the agent considers what action the other agent would have taken if it completely believed the world was in
a certain state. Thus, there are four belief states resulting from b?t , as opposed to two in the exact case.
1 ?s?? 6= s?. This approach to managing contradiction
is analogous to logical belief revision (Alchourrono?n
et al, 1985; Ga?rdenfors, 1988; Ferme? and Hansson,
2011).
Speech Actions Speech actions are modeled by
how they change the beliefs of the other agent.
The effects of a speech actions are modeled in
T? (s??|s?,a,(s, s?)), our model of how ListenerBot?s be-
liefs change. For a speech action a = say(?) with
? ? ?,
T? (s??|s?,a,(s, s?)) =
?
o??O
(
?(o?|s??,a, p?i(s?))Pr(? |s??)T (s??|a, p?i(s?), s?)
?s????(o?|s???,a, p?i(s?))Pr(? |s???)T (s???|a, p?i(s?), s?)
??(o?|s?,a, p?i(s?))
)
DialogBot is equipped with the five most
frequent speech actions: BOARD(middle),
BOARD(top), BOARD(bottom), BOARD(left),
and BOARD(right). It produces concrete utterances
by selecting a sentence from the training corpus
with the desired semantics.
Reward DialogBot receives a large reward when
both it and its partner are located on the card, and a
negative cost when moving or speaking:
R((p,c, p?, c?),a) =
{
R+ p = c? p? = c
R? p 6= c? p? 6= c
DialogBot?s reward is not dependent on the beliefs
of the other player, only the true underlying state of
the world.
6 Experimental Results
We now experimentally evaluate our semantic clas-
sifiers and the agents? task performance.
6.1 Spatial Semantics Classifiers
We report the performance of our spatial seman-
tics classifiers, although their accuracy is not the fo-
cus of this paper. We use 10-fold cross validation
on a corpus of 577 annotated utterances. We used
simple bag-of-words features, so overfitting the data
with cross validation is not a pressing concern. Of
the 577 utterances, our classifiers perfectly labeled
325 (56.3% accuracy). The classifiers correctly pre-
dicted the domain ? of 515 (89.3%) utterances. The
1078
precision of our binary semantic primitive classifiers
was 9691126 = .861 and recall
969
1242 = .780, yielding F1
measure .818.
6.2 Cards Task Evaluation
We evaluated our ListenerBot and DialogBot agents
in the Cards task. Using 500 randomly generated
initial player and card locations, we tested each
combination of ListenerBot and DialogBot partners.
Agents succeeded at a given initial position if they
both reached the card within 50 moves. Table 2
shows how many trials each dyad won and how
many high-level actions they took to do so.
Agents % Success Moves
LB & LB 84.4% 19.8
LB & DB 87.2% 17.5
DB & DB 90.6% 16.6
Table 2: The evaluation for each combination of
agents. LB = ListenerBot; DB = DialogBot.
Collaborating DialogBots performed the best,
completing more trials and using fewer moves than
the ListenerBots. The DialogBots initially explore
the space in a similar manner to the ListenerBots,
but then share card location information. This leads
to shorter interactions, as once the DialogBot finds
the card, the other player can find it more quickly.
In the combination of ListenerBot and DialogBot,
we see about half of the improvement over two Lis-
tenerBots. Roughly 50% of the time, the Listener-
Bot finds the card first, which doesn?t help the Di-
alogBot find the card any faster.
7 Emergent Pragmatics
Grice?s original model of pragmatics (Grice, 1975)
involves the cooperative principle and four maxims:
quality (?say only what you know to be true?), rela-
tion (?be relevant?), quantity (?be as informative as
is required; do not say more than is required?), and
manner (roughly, be clear and concise).
In most interactions, DialogBot searches for the
card and then reports its location to the other agent.
These reports obey quality in that they are made only
when based on actual observations. The behavior
is not hard-coded, but rather emerges, because only
accurate information serves the agents? goals. In
contrast, sub-optimal policies generated early in the
POMDP solving process sometimes lie about card
locations. Since this behavior confuses the other
agent and thus has a lower utility, it gets replaced
by truthful communication as the policies improve.
We also capture the effects of relation and the first
clause of quantity, because the nature of the reward
function and the nested belief structures ensure that
DialogBot offers only relevant, informative informa-
tion. For instance, when DialogBot finds the card in
the lower left corner, it alternates saying ?left? and
?bottom?, effectively overcoming its limited gener-
ation capabilities. Again, early sub-optimal policies
sometimes do not report the location of the card at
all, thereby failing to fulfill these maxims.
We expect these models to produce behavior con-
sistent with manner and the second clause of quan-
tity, but evaluating this claim will require a richer ex-
perimental paradigm. For example, if DialogBot had
a larger and more structured vocabulary, it would
have to choose between levels of specificity as well
as more or less economical forms.
8 Conclusion
We have shown that cooperative pragmatic behavior
can arise from multi-agent decision-theoretic mod-
els in which the agents share a joint utility func-
tion and reason about each other?s belief states.
Decision-making in these models is intractable,
which has been a major obstacle to achieving exper-
imental results in this area. We introduced a series
of approximations to manage this intractability: (i)
combining low-level states into semantically coher-
ent high-level ones; (ii) tracking only an averaged
summary of the other agent?s potential beliefs; (iii)
limiting belief state nesting to one level, and (iv)
simplifying each agent?s model of the other?s be-
liefs so as to reduce uncertainty. These approxima-
tions bring the problems under sufficient control that
they can be solved with current POMDP approxi-
mation algorithms. Our experimental results high-
light the rich pragmatic behavior this gives rise to
and quantify the communicative value of such be-
havior. While there remain insights from earlier the-
oretical proposals and logic-based methods that we
have not fully captured, our current results support
1079
the notion that probabilistic decision-making meth-
ods can yield robust, widely applicable models that
address the real-world difficulties of partial observ-
ability and uncertainty.
Acknowledgments
This research was supported in part by ONR
grants N00014-10-1-0109 and N00014-13-1-0287
and ARO grant W911NF-07-1-0216.
References
Carlos E. Alchourrono?n, Peter Ga?rdenfors, and David
Makinson. 1985. On the logic of theory change: Par-
tial meets contradiction and revision functions. Jour-
nal of Symbolic Logic, 50(2):510?530.
James F. Allen, Nathanael Chambers, George Ferguson,
Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Taysom. 2007. PLOW: A collaborative
task learning agent. In Proceedings of the Twenty-
Second AAAI Conference on Artificial Intelligence,
pages 1514?1519. AAAI Press, Vancouver, British
Columbia, Canada.
James F. Allen. 1991. Reasoning About Plans. Morgan
Kaufmann, San Francisco.
David Beaver. 2002. Pragmatics, and that?s an order. In
David Barker-Plummer, David Beaver, Johan van Ben-
them, and Patrick Scotto di Luzio, editors, Logic, Lan-
guage, and Visual Information, pages 192?215. CSLI,
Stanford, CA.
Anton Benz, Gerhard Ja?ger, and Robert van Rooij, edi-
tors. 2005. Game Theory and Pragmatics. Palgrave
McMillan, Basingstoke, Hampshire.
Daniel S. Bernstein, Robert Givan, Neil Immerman, and
Shlomo Zilberstein. 2002. The complexity of decen-
tralized control of Markov decision processes. Mathe-
matics of Operations Research, 27(4):819?840.
Reinhard Blutner. 1998. Lexical pragmatics. Journal of
Semantics, 15(2):115?162.
Michael Bratman. 1987. Intentions, Plans, and Practical
Reason. Harvard University Press.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233?263.
Judith Degen and Michael Franke. 2012. Optimal rea-
soning about referential expressions. In Proceedings
of SemDIAL 2012, Paris, September.
David DeVault, Natalia Kariaeva, Anubha Kothari, Iris
Oved, and Matthew Stone. 2005. An information-
state approach to collaborative reference. In Proceed-
ings of the ACL Interactive Poster and Demonstration
Sessions, pages 1?4, Ann Arbor, MI, June. Association
for Computational Linguistics.
David DeVault. 2008. Contribution Tracking: Partici-
pating in Task-Oriented Dialogue under Uncertainty.
Ph.D. thesis, Rutgers University, New Brunswick, NJ.
Eduardo Ferme? and Sven Ove Hansson. 2011. AGM 25
years: Twenty-five years of research in belief change.
Journal of Philosophical Logic, 40(2):295?331.
Michael C. Frank and Noah D. Goodman. 2012. Predict-
ing pragmatic reasoning in language games. Science,
336(6084):998.
Michael Franke. 2009. Signal to Act: Game Theory
in Pragmatics. ILLC Dissertation Series. Institute for
Logic, Language and Computation, University of Am-
sterdam.
Peter Ga?rdenfors. 1988. Knowledge in Flux: Modeling
the Dynamics of Epistemic States. MIT Press.
Piotr J. Gmytrasiewicz and Prashant Doshi. 2005. A
framework for sequential planning in multi-agent set-
tings. Journal of Artificial Intelligence Research,
24:24?49.
Dave Golland, Percy Liang, and Dan Klein. 2010. A
game-theoretic approach to generating spatial descrip-
tions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 410?419, Cambridge, MA, October. ACL.
Noah D. Goodman and Andreas Stuhlmu?ller. 2012.
Knowledge and implicature: Modeling language un-
derstanding as social cognition. In Proceedings of the
Thirty-Fourth Annual Conference of the Cognitive Sci-
ence Society.
H. Paul Grice. 1975. Logic and conversation. In Peter
Cole and Jerry Morgan, editors, Syntax and Semantics,
volume 3: Speech Acts, pages 43?58. Academic Press,
New York.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
put. Linguist., 12(3):175?204, July.
William G. Hayward and Michael J. Tarr. 1995. Spa-
tial language and spatial representation. Cognition,
55:39?84.
Jerry Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1?2):69?142.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
Computational Linguistics, 38(1):173?218.
David Lewis. 1969. Convention. Harvard University
Press, Cambridge, MA. Reprinted 2002 by Blackwell.
1080
Michael L. Littman, Anthony R. Cassandra, and
Leslie Pack Kaelbling. 1995. Learning policies for
partially observable environments: Scaling up. In Ar-
mand Prieditis and Stuart J. Russell, editors, ICML,
pages 362?370. Morgan Kaufmann.
Arthur Merin. 1997. If all our arguments had to be con-
clusive, there would be few of them. Arbeitspapiere
SFB 340 101, University of Stuttgart, Stuttgart.
Christos Papadimitriou and John N. Tsitsiklis. 1987. The
complexity of markov decision processes. Math. Oper.
Res., 12(3):441?450, August.
Prashant Parikh. 2001. The Use of Language. CSLI,
Stanford, CA.
C. Raymond Perrault and James F. Allen. 1980. A plan-
based analysis of indirect speech acts. American Jour-
nal of Computational Linguistics, 6(3?4):167?182.
Christopher Potts. 2012. Goal-driven answers in the
Cards dialogue corpus. In Nathan Arnett and Ryan
Bennett, editors, Proceedings of the 30th West Coast
Conference on Formal Linguistics, Somerville, MA.
Cascadilla Press.
David V. Pynadath and Milind Tambe. 2002. The com-
municative multiagent team decision problem: Ana-
lyzing teamwork theories and models. Journal of Ar-
tificial Intelligence Research, 16:2002.
Hannah Rohde, Scott Seyfarth, Brady Clark, Gerhard
Ja?ger, and Stefan Kaufmann. 2012. Communicat-
ing with cost-based implicature: A game-theoretic ap-
proach to ambiguity. In The 16th Workshop on the Se-
mantics and Pragmatics of Dialogue, Paris, Septem-
ber.
Robert van Rooy. 2003. Questioning to resolve decision
problems. Linguistics and Philosophy, 26(6):727?
763.
Seymour Rosenberg and Bertram D. Cohen. 1964.
Speakers? and listeners? processes in a word commu-
nication task. Science, 145:1201?1203.
Matthijs T. J. Spaan and Nikos Vlassis. 2005. Perseus:
Randomized point-based value iteration for POMDPs.
Journal of Artificial Intelligence Research, 24(1):195?
220, August.
Matthijs T. J. Spaan, Frans A. Oliehoek, and Nikos Vlas-
sis. 2008. Multiagent planning under uncertainty with
stochastic communication delays. In In Proc. of the
18th Int. Conf. on Automated Planning and Schedul-
ing, pages 338?345.
Alex Stiller, Noah D. Goodman, and Michael C. Frank.
2011. Ad-hoc scalar implicature in adults and chil-
dren. In Proceedings of the 33rd Annual Meeting of
the Cognitive Science Society, Boston, July.
Matthew Stone, Richmond Thomason, and David De-
Vault. 2007. Enlightened update: A computational
architecture for presupposition and other pragmatic
phenomena. To appear in Donna K. Byron; Craige
Roberts; and Scott Schwenter, Presupposition Accom-
modation.
Blaise Thomson and Steve Young. 2010. Bayesian up-
date of dialogue state: A pomdp framework for spoken
dialogue systems. Comput. Speech Lang., 24(4):562?
588, October.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and Kai
Yu. 2010. The hidden information state model: A
practical framework for pomdp-based spoken dialogue
management. Comput. Speech Lang., 24(2):150?174,
April.
1081
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 445?453,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Improving the Use of Pseudo-Words for Evaluating
Selectional Preferences
Nathanael Chambers and Dan Jurafsky
Department of Computer Science
Stanford University
{natec,jurafsky}@stanford.edu
Abstract
This paper improves the use of pseudo-
words as an evaluation framework for
selectional preferences. While pseudo-
words originally evaluated word sense
disambiguation, they are now commonly
used to evaluate selectional preferences. A
selectional preference model ranks a set of
possible arguments for a verb by their se-
mantic fit to the verb. Pseudo-words serve
as a proxy evaluation for these decisions.
The evaluation takes an argument of a verb
like drive (e.g. car), pairs it with an al-
ternative word (e.g. car/rock), and asks a
model to identify the original. This pa-
per studies two main aspects of pseudo-
word creation that affect performance re-
sults. (1) Pseudo-word evaluations often
evaluate only a subset of the words. We
show that selectional preferences should
instead be evaluated on the data in its en-
tirety. (2) Different approaches to select-
ing partner words can produce overly op-
timistic evaluations. We offer suggestions
to address these factors and present a sim-
ple baseline that outperforms the state-of-
the-art by 13% absolute on a newspaper
domain.
1 Introduction
For many natural language processing (NLP)
tasks, particularly those involving meaning, cre-
ating labeled test data is difficult or expensive.
One way to mitigate this problem is with pseudo-
words, a method for automatically creating test
corpora without human labeling, originally pro-
posed for word sense disambiguation (Gale et al,
1992; Schutze, 1992). While pseudo-words are
now less often used for word sense disambigation,
they are a common way to evaluate selectional
preferences, models that measure the strength of
association between a predicate and its argument
filler, e.g., that the noun lunch is a likely object
of eat. Selectional preferences are useful for NLP
tasks such as parsing and semantic role labeling
(Zapirain et al, 2009). Since evaluating them in
isolation is difficult without labeled data, pseudo-
word evaluations can be an attractive evaluation
framework.
Pseudo-word evaluations are currently used to
evaluate a variety of language modeling tasks
(Erk, 2007; Bergsma et al, 2008). However,
evaluation design varies across research groups.
This paper studies the evaluation itself, showing
how choices can lead to overly optimistic results
if the evaluation is not designed carefully. We
show in this paper that current methods of apply-
ing pseudo-words to selectional preferences vary
greatly, and suggest improvements.
A pseudo-word is the concatenation of two
words (e.g. house/car). One word is the orig-
inal in a document, and the second is the con-
founder. Consider the following example of ap-
plying pseudo-words to the selectional restrictions
of the verb focus:
Original: This story focuses on the campaign.
Test: This story/part focuses on the campaign/meeting.
In the original sentence, focus has two arguments:
a subject story and an object campaign. In the test
sentence, each argument of the verb is replaced by
pseudo-words. A model is evaluated by its success
at determining which of the two arguments is the
original word.
Two problems exist in the current use of
445
pseudo-words to evaluate selectional preferences.
First, selectional preferences historically focus on
subsets of data such as unseen words or words in
certain frequency ranges. While work on unseen
data is important, evaluating on the entire dataset
provides an accurate picture of a model?s overall
performance. Most other NLP tasks today evalu-
ate all test examples in a corpus. We will show
that seen arguments actually dominate newspaper
articles, and thus propose creating test sets that in-
clude all verb-argument examples to avoid artifi-
cial evaluations.
Second, pseudo-word evaluations vary in how
they choose confounders. Previous work has at-
tempted to maintain a similar corpus frequency
to the original, but it is not clear how best to do
this, nor how it affects the task?s difficulty. We
argue in favor of using nearest-neighbor frequen-
cies and show how using random confounders pro-
duces overly optimistic results.
Finally, we present a surprisingly simple base-
line that outperforms the state-of-the-art and is far
less memory and computationally intensive. It
outperforms current similarity-based approaches
by over 13% when the test set includes all of the
data. We conclude with a suggested backoff model
based on this baseline.
2 History of Pseudo-Word
Disambiguation
Pseudo-words were introduced simultaneously by
two papers studying statistical approaches to word
sense disambiguation (WSD). Schu?tze (1992)
simply called the words, ?artificial ambiguous
words?, but Gale et al (1992) proposed the suc-
cinct name, pseudo-word. Both papers cited the
sparsity and difficulty of creating large labeled
datasets as the motivation behind pseudo-words.
Gale et al selected unambiguous words from the
corpus and paired them with random words from
different thesaurus categories. Schu?tze paired his
words with confounders that were ?comparable in
frequency? and ?distinct semantically?. Gale et
al.?s pseudo-word term continues today, as does
Schu?tze?s frequency approach to selecting the con-
founder.
Pereira et al (1993) soon followed with a selec-
tional preference proposal that focused on a lan-
guage model?s effectiveness on unseen data. The
work studied clustering approaches to assist in
similarity decisions, predicting which of two verbs
was the correct predicate for a given noun object.
One verb v was the original from the source doc-
ument, and the other v? was randomly generated.
This was the first use of such verb-noun pairs, as
well as the first to test only on unseen pairs.
Several papers followed with differing methods
of choosing a test pair (v, n) and its confounder
v?. Dagan et al (1999) tested all unseen (v, n)
occurrences of the most frequent 1000 verbs in
his corpus. They then sorted verbs by corpus fre-
quency and chose the neighboring verb v? of v
as the confounder to ensure the closest frequency
match possible. Rooth et al (1999) tested 3000
random (v, n) pairs, but required the verbs and
nouns to appear between 30 and 3000 times in
training. They also chose confounders randomly
so that the new pair was unseen.
Keller and Lapata (2003) specifically addressed
the impact of unseen data by using the web to first
?see? the data. They evaluated unseen pseudo-
words by attempting to first observe them in a
larger corpus (the Web). One modeling difference
was to disambiguate the nouns as selectional pref-
erences instead of the verbs. Given a test pair
(v, n) and its confounder (v, n?), they used web
searches such as ?v Det n? to make the decision.
Results beat or matched current results at the time.
We present a similarly motivated, but new web-
based approach later.
Very recent work with pseudo-words (Erk,
2007; Bergsma et al, 2008) further blurs the lines
between what is included in training and test data,
using frequency-based and semantic-based rea-
sons for deciding what is included. We discuss
this further in section 5.
As can be seen, there are two main factors when
devising a pseudo-word evaluation for selectional
preferences: (1) choosing (v, n) pairs from the test
set, and (2) choosing the confounding n? (or v?).
The confounder has not been looked at in detail
and as best we can tell, these factors have var-
ied significantly. Many times the choices are well
motivated based on the paper?s goals, but in other
cases the motivation is unclear.
3 How Frequent is Unseen Data?
Most NLP tasks evaluate their entire datasets, but
as described above, most selectional preference
evaluations have focused only on unseen data.
This section investigates the extent of unseen ex-
amples in a typical training/testing environment
446
of newspaper articles. The results show that even
with a small training size, seen examples dominate
the data. We argue that, absent a system?s need for
specialized performance on unseen data, a repre-
sentative test set should include the dataset in its
entirety.
3.1 Unseen Data Experiment
We use the New York Times (NYT) and Associ-
ated Press (APW) sections of the Gigaword Cor-
pus (Graff, 2002), as well as the British National
Corpus (BNC) (Burnard, 1995) for our analysis.
Parsing and SRL evaluations often focus on news-
paper articles and Gigaword is large enough to
facilitate analysis over varying amounts of train-
ing data. We parsed the data with the Stan-
ford Parser1 into dependency graphs. Let (vd, n)
be a verb v with grammatical dependency d ?
{subject, object, prep} filled by noun n. Pairs
(vd, n) are chosen by extracting every such depen-
dency in the graphs, setting the head predicate as
v and the head word of the dependent d as n. All
prepositions are condensed into prep.
We randomly selected documents from the year
2001 in the NYT portion of the corpus as devel-
opment and test sets. Training data for APW and
NYT include all years 1994-2006 (minus NYT de-
velopment and test documents). We also identified
and removed duplicate documents2. The BNC in
its entirety is also used for training as a single data
point. We then record every seen (vd, n) pair dur-
ing training that is seen two or more times3 and
then count the number of unseen pairs in the NYT
development set (1455 tests).
Figure 1 plots the percentage of unseen argu-
ments against training size when trained on either
NYT or APW (the APW portion is smaller in total
size, and the smaller BNC is provided for com-
parison). The first point on each line (the high-
est points) contains approximately the same num-
ber of words as the BNC (100 million). Initially,
about one third of the arguments are unseen, but
that percentage quickly falls close to 10% as ad-
ditional training is included. This suggests that an
evaluation focusing only on unseen data is not rep-
resentative, potentially missing up to 90% of the
data.
1http://nlp.stanford.edu/software/lex-parser.shtml
2Any two documents whose first two paragraphs in the
corpus files are identical.
3Our results are thus conservative, as including all single
occurrences would achieve even smaller unseen percentages.
0 2 4 6 8 10 120
5
10
15
20
25
30
35
40
45
Number of Tokens in Training (hundred millions)
Per
cen
t Un
see
n
Unseen Arguments in NYT Dev
 
  BNC AP NYT Google
Figure 1: Percentage of NYT development set
that is unseen when trained on varying amounts of
data. The two lines represent training with NYT or
APW data. The APW set is smaller in size from
the NYT. The dotted line uses Google n-grams as
training. The x-axis represents tokens ? 108.
0 2 4 6 8 10 120
5
10
15
20
25
30
35
40
Number of Tokens in Training (hundred millions)
Per
cen
t Un
see
n
Unseen Arguments by Type
 
  Preps Subjects Objects
Figure 2: Percentage of subject/object/preposition
arguments in the NYT development set that is un-
seen when trained on varying amounts of NYT
data. The x-axis represents tokens ? 108.
447
The third line across the bottom of the figure is
the number of unseen pairs using Google n-gram
data as proxy argument counts. Creating argu-
ment counts from n-gram counts is described in
detail below in section 5.2. We include these Web
counts to illustrate how an openly available source
of counts affects unseen arguments. Finally, fig-
ure 2 compares which dependency types are seen
the least in training. Prepositions have the largest
unseen percentage, but not surprisingly, also make
up less of the training examples overall.
In order to analyze why pairs are unseen, we an-
alyzed the distribution of rare words across unseen
and seen examples. To define rare nouns, we order
head words by their individual corpus frequencies.
A noun is rare if it occurs in the lowest 10% of the
list. We similarly define rare verbs over their or-
dered frequencies (we count verb lemmas, and do
not include the syntactic relations). Corpus counts
covered 2 years of the AP section, and we used
the development set of the NYT section to extract
the seen and unseen pairs. Figure 3 shows the per-
centage of rare nouns and verbs that occur in un-
seen and seen pairs. 24.6% of the verbs in un-
seen pairs are rare, compared to only 4.5% in seen
pairs. The distribution of rare nouns is less con-
trastive: 13.3% vs 8.9%. This suggests that many
unseen pairs are unseen mainly because they con-
tain low-frequency verbs, rather than because of
containing low-frequency argument heads.
Given the large amount of seen data, we be-
lieve evaluations should include all data examples
to best represent the corpus. We describe our full
evaluation results and include a comparison of dif-
ferent training sizes below.
4 How to Select a Confounder
Given a test set S of pairs (vd, n) ? S, we now ad-
dress how best to select a confounder n?. Work in
WSD has shown that confounder choice can make
the pseudo-disambiguation task significantly eas-
ier. Gaustad (2001) showed that human-generated
pseudo-words are more difficult to classify than
random choices. Nakov and Hearst (2003) further
illustrated how random confounders are easier to
identify than those selected from semantically am-
biguous, yet related concepts. Our approach eval-
uates selectional preferences, not WSD, but our re-
sults complement these findings.
We identified three methods of confounder se-
lection based on varying levels of corpus fre-
verbs nouns
Unseen Tests
Seen Tests
Distribution of Rare Verbs and Nouns in Tests
Per
cen
t Ra
re W
ord
s
0
5
10
15
20
25
30
Figure 3: Comparison between seen and unseen
tests (verb,relation,noun). 24.6% of unseen tests
have rare verbs, compared to just 4.5% in seen
tests. The rare nouns are more evenly distributed
across the tests.
quency: (1) choose a random noun, (2) choose a
random noun from a frequency bucket similar to
the original noun?s frequency, and (3) select the
nearest neighbor, the noun with frequency clos-
est to the original. These methods evaluate the
range of choices used in previous work. Our ex-
periments compare the three.
5 Models
5.1 A New Baseline
The analysis of unseen slots suggests a baseline
that is surprisingly obvious, yet to our knowledge,
has not yet been evaluated. Part of the reason
is that early work in pseudo-word disambiguation
explicitly tested only unseen pairs4. Our evalua-
tion will include seen data, and since our analysis
suggests that up to 90% is seen, a strong baseline
should address this seen portion.
4Recent work does include some seen data. Bergsma et
al. (2008) test pairs that fall below a mutual information
threshold (might include some seen pairs), and Erk (2007)
selects a subset of roles in FrameNet (Baker et al, 1998) to
test and uses all labeled instances within this subset (unclear
what portion of subset of data is seen). Neither evaluates all
of the seen data, however.
448
We propose a conditional probability baseline:
P (n|vd) =
{
C(vd,n)
C(vd,?)
if C(vd, n) > 0
0 otherwise
where C(vd, n) is the number of times the head
word n was seen as an argument to the pred-
icate v, and C(vd, ?) is the number of times
vd was seen with any argument. Given a test
(vd, n) and its confounder (vd, n?), choose n if
P (n|vd) > P (n?|vd), and n? otherwise. If
P (n|vd) = P (n?|vd), randomly choose one.
Lapata et al (1999) showed that corpus fre-
quency and conditional probability correlate with
human decisions of adjective-noun plausibility,
and Dagan et al (1999) appear to propose a very
similar baseline for verb-noun selectional prefer-
ences, but the paper evaluates unseen data, and so
the conditional probability model is not studied.
We later analyze this baseline against a more
complicated smoothing approach.
5.2 A Web Baseline
If conditional probability is a reasonable baseline,
better performance may just require more data.
Keller and Lapata (2003) proposed using the web
for this task, querying for specific phrases like
?Verb Det N? to find syntactic objects. Such a web
corpus would be attractive, but we?d like to find
subjects and prepositional objects as well as ob-
jects, and also ideally we don?t want to limit our-
selves to patterns. Since parsing the web is unre-
alistic, a reasonable compromise is to make rough
counts when pairs of words occur in close proxim-
ity to each other.
Using the Google n-gram corpus, we recorded
all verb-noun co-occurrences, defined by appear-
ing in any order in the same n-gram, up to and
including 5-grams. For instance, the test pair
(throwsubject, ball) is considered seen if there ex-
ists an n-gram such that throw and ball are both
included. We count all such occurrences for all
verb-noun pairs. We also avoided over-counting
co-occurrences in lower order n-grams that appear
again in 4 or 5-grams. This crude method of count-
ing has obvious drawbacks. Subjects are not dis-
tinguished from objects and nouns may not be ac-
tual arguments of the verb. However, it is a simple
baseline to implement with these freely available
counts.
Thus, we use conditional probability as de-
fined in the previous section, but define the count
C(vd, n) as the number of times v and n (ignoring
d) appear in the same n-gram.
5.3 Smoothing Model
We implemented the current state-of-the-art
smoothing model of Erk (2007). The model is
based on the idea that the arguments of a particular
verb slot tend to be similar to each other. Given
two potential arguments for a verb, the correct
one should correlate higher with the arguments ob-
served with the verb during training.
Formally, given a verb v and a grammatical de-
pendency d, the score for a noun n is defined:
Svd(n) =
?
w?Seen(vd)
sim(n,w) ? C(vd, w) (1)
where sim(n,w) is a noun-noun similarity score,
Seen(vd) is the set of seen head words filling the
slot vd during training, and C(vd, n) is the num-
ber of times the noun n was seen filling the slot vd
The similarity score sim(n,w) can thus be one of
many vector-based similarity metrics5. We eval-
uate both Jaccard and Cosine similarity scores in
this paper, but the difference between the two is
small.
6 Experiments
Our training data is the NYT section of the Gi-
gaword Corpus, parsed into dependency graphs.
We extract all (vd, n) pairs from the graph, as de-
scribed in section 3. We randomly chose 9 docu-
ments from the year 2001 for a development set,
and 41 documents for testing. The test set con-
sisted of 6767 (vd, n) pairs. All verbs and nouns
are stemmed, and the development and test docu-
ments were isolated from training.
6.1 Varying Training Size
We repeated the experiments with three different
training sizes to analyze the effect data size has on
performance:
? Train x1: Year 2001 of the NYT portion of
the Gigaword Corpus. After removing du-
plicate documents, it contains approximately
110 million tokens, comparable to the 100
million tokens in the BNC corpus.
5A similar type of smoothing was proposed in earlier
work by Dagan et al (1999). A noun is represented by a
vector of verb slots and the number of times it is observed
filling each slot.
449
? Train x2: Years 2001 and 2002 of the NYT
portion of the Gigaword Corpus, containing
approximately 225 million tokens.
? Train x10: The entire NYT portion of Giga-
word (approximately 1.2 billion tokens). It is
an order of magnitude larger than Train x1.
6.2 Varying the Confounder
We generated three different confounder sets
based on word corpus frequency from the 41 test
documents. Frequency was determined by count-
ing all tokens with noun POS tags. As motivated
in section 4, we use the following approaches:
? Random: choose a random confounder from
the set of nouns that fall within some broad
corpus frequency range. We set our range to
eliminate (approximately) the top 100 most
frequent nouns, but otherwise arbitrarily set
the lower range as previous work seems to
do. The final range was [30, 400000].
? Buckets: all nouns are bucketed based on
their corpus frequencies6. Given a test pair
(vd, n), choose the bucket in which n belongs
and randomly select a confounder n? from
that bucket.
? Neighbor: sort all seen nouns by frequency
and choose the confounder n? that is the near-
est neighbor of n with greater frequency.
6.3 Model Implementation
None of the models can make a decision if they
identically score both potential arguments (most
often true when both arguments were not seen with
the verb in training). As a result, we extend all
models to randomly guess (50% performance) on
pairs they cannot answer.
The conditional probability is reported as Base-
line. For the web baseline (reported as Google),
we stemmed all words in the Google n-grams and
counted every verb v and noun n that appear in
Gigaword. Given two nouns, the noun with the
higher co-occurrence count with the verb is cho-
sen. As with the other models, if the two nouns
have the same counts, it randomly guesses.
The smoothing model is named Erk in the re-
sults with both Jaccard and Cosine as the simi-
larity metric. Due to the large vector representa-
tions of the nouns, it is computationally wise to
6We used frequency buckets of 4, 10, 25, 200, 1000,
>1000. Adding more buckets moves the evaluation closer
to Neighbor, less is closer to Random.
trim their vectors, but also important to do so for
best performance. A noun?s representative vector
consists of verb slots and the number of times the
noun was seen in each slot. We removed any verb
slot not seen more than x times, where x varied
based on all three factors: the dataset, confounder
choice, and similarity metric. We optimized x
on the development data with a linear search, and
used that cutoff on each test. Finally, we trimmed
any vectors over 2000 in size to reduce the com-
putational complexity. Removing this strict cutoff
appears to have little effect on the results.
Finally, we report backoff scores for Google and
Erk. These consist of always choosing the Base-
line if it returns an answer (not a guessed unseen
answer), and then backing off to the Google/Erk
result for Baseline unknowns. These are labeled
Backoff Google and Backoff Erk.
7 Results
Results are given for the two dimensions: con-
founder choice and training size. Statistical sig-
nificance tests were calculated using the approx-
imate randomization test (Yeh, 2000) with 1000
iterations.
Figure 4 shows the performance change over the
different confounder methods. Train x2 was used
for training. Each model follows the same pro-
gression: it performs extremely well on the ran-
dom test set, worse on buckets, and the lowest on
the nearest neighbor. The conditional probability
Baseline falls from 91.5 to 79.5, a 12% absolute
drop from completely random to neighboring fre-
quency. The Erk smoothing model falls 27% from
93.9 to 68.1. The Google model generally per-
forms the worst on all sets, but its 74.3% perfor-
mance with random confounders is significantly
better than a 50-50 random choice. This is no-
table since the Google model only requires n-gram
counts to implement. The Backoff Erk model is
the best, using the Baseline for the majority of
decisions and backing off to the Erk smoothing
model when the Baseline cannot answer.
Figure 5 (shown on the next page) varies the
training size. We show results for both Bucket Fre-
quencies and Neighbor Frequencies. The only dif-
ference between columns is the amount of training
data. As expected, the Baseline improves as the
training size is increased. The Erk model, some-
what surprisingly, shows no continual gain with
more training data. The Jaccard and Cosine simi-
450
Varying the Confounder Frequency
Random Buckets Neighbor
Baseline 91.5 89.1 79.5
Erk-Jaccard 93.9* 82.7* 68.1*
Erk-Cosine 91.2 81.8* 65.3*
Google 74.3* 70.4* 59.4*
Backoff Erk 96.6* 91.8* 80.8*
Backoff Goog 92.7? 89.7 79.8
Figure 4: Trained on two years of NYT data (Train
x2). Accuracy of the models on the same NYT test
documents, but with three different ways of choos-
ing the confounders. * indicates statistical signifi-
cance with the column?s Baseline at the p < 0.01
level, ? at p < 0.05. Random is overly optimistic,
reporting performance far above more conserva-
tive (selective) confounder choices.
Baseline Details
Train Train x2 Train x10
Precision 96.1 95.5* 95.0?
Accuracy 78.2 82.0* 88.1*
Accuracy +50% 87.5 89.1* 91.7*
Figure 6: Results from the buckets confounder test
set. Baseline precision, accuracy (the same as re-
call), and accuracy when you randomly guess the
tests that Baseline does not answer. All numbers
are statistically significant * with p-value < 0.01
from the number to their left.
larity scores perform similarly in their model. The
Baseline achieves the highest accuracies (91.7%
and 81.2%) with Train x10, outperforming the best
Erk model by 5.2% and 13.1% absolute on buck-
ets and nearest neighbor respectively. The back-
off models improve the baseline by just under 1%.
The Google n-gram backoff model is almost as
good as backing off to the Erk smoothing model.
Finally, figure 6 shows the Baseline?s precision
and overall accuracy. Accuracy is the same as
recall when the model does not guess between
pseudo words that have the same conditional prob-
abilities. Accuracy +50% (the full Baseline in
all other figures) shows the gain from randomly
choosing one of the two words when uncertain.
Precision is extremely high.
8 Discussion
Confounder Choice: Performance is strongly in-
fluenced by the method used when choosing con-
founders. This is consistent with findings for
WSD that corpus frequency choices alter the task
(Gaustad, 2001; Nakov and Hearst, 2003). Our
results show the gradation of performance as one
moves across the spectrum from completely ran-
dom to closest in frequency. The Erk model
dropped 27%, Google 15%, and our baseline 12%.
The overly optimistic performance on random data
suggests using the nearest neighbor approach for
experiments. Nearest neighbor avoids evaluating
on ?easy? datasets, and our baseline (at 79.5%)
still provides room for improvement. But perhaps
just as important, the nearest neighbor approach
facilitates the most reproducibile results in exper-
iments since there is little ambiguity in how the
confounder is selected.
Realistic Confounders: Despite its over-
optimism, the random approach to confounder se-
lection may be the correct approach in some cir-
cumstances. For some tasks that need selectional
preferences, random confounders may be more re-
alistic. It?s possible, for example, that the options
in a PP-attachment task might be distributed more
like the random rather than nearest neighbor mod-
els. In any case, this is difficult to decide without
a specific application in mind. Absent such spe-
cific motiviation, a nearest neighbor approach is
the most conservative, and has the advantage of
creating a reproducible experiment, whereas ran-
dom choice can vary across design.
Training Size: Training data improves the con-
ditional probability baseline, but does not help the
smoothing model. Figure 5 shows a lack of im-
provement across training sizes for both jaccard
and cosine implementations of the Erk model. The
Train x1 size is approximately the same size used
in Erk (2007), although on a different corpus. We
optimized argument cutoffs for each training size,
but the model still appears to suffer from addi-
tional noise that the conditional probability base-
line does not. This may suggest that observing a
test argument with a verb in training is more re-
liable than a smoothing model that compares all
training arguments against that test example.
High Precision Baseline: Our conditional
probability baseline is very precise. It outper-
forms the smoothed similarity based Erk model
and gives high results across tests. The only com-
bination when Erk is better is when the training
data includes just one year (one twelfth of the
NYT section) and the confounder is chosen com-
451
Varying the Training Size
Bucket Frequency Neighbor Frequency
Train x1 Train x2 Train x10 Train x1 Train x2 Train x10
Baseline 87.5 89.1 91.7 78.4 79.5 81.2
Erk-Jaccard 86.5* 82.7* 83.1* 66.8* 68.1* 65.5*
Erk-Cosine 82.1* 81.8* 81.1* 66.1* 65.3* 65.7*
Google - - 70.4* - - 59.4*
Backoff Erk 92.6* 91.8* 92.6* 79.4* 80.8* 81.7*
Backoff Google 88.6 89.7 91.9? 78.7 79.8 81.2
Figure 5: Accuracy of varying NYT training sizes. The left and right tables represent two confounder
choices: choose the confounder with frequency buckets, and choose by nearest frequency neighbor.
Trainx1 starts with year 2001 of NYT data, Trainx2 doubles the size, and Trainx10 is 10 times larger. *
indicates statistical significance with the column?s Baseline at the p < 0.01 level, ? at p < 0.05.
pletely randomly. These results appear consistent
with Erk (2007) because that work used the BNC
corpus (the same size as one year of our data) and
Erk chose confounders randomly within a broad
frequency range. Our reported results include ev-
ery (vd, n) in the data, not a subset of particu-
lar semantic roles. Our reported 93.9% for Erk-
Jaccard is also significantly higher than their re-
ported 81.4%, but this could be due to the random
choices we made for confounders, or most likely
corpus differences between Gigaword and the sub-
set of FrameNet they evaluated.
Ultimately we have found that complex models
for selectional preferences may not be necessary,
depending on the task. The higher computational
needs of smoothing approaches are best for back-
ing off when unseen data is encountered. Condi-
tional probability is the best choice for seen exam-
ples. Further, analysis of the data shows that as
more training data is made available, the seen ex-
amples make up a much larger portion of the test
data. Conditional probability is thus a very strong
starting point if selectional preferences are an in-
ternal piece to a larger application, such as seman-
tic role labeling or parsing.
Perhaps most important, these results illustrate
the disparity in performance that can come about
when designing a pseudo-word disambiguation
evaluation. It is crucially important to be clear
during evaluations about how the confounder was
generated. We suggest the approach of sorting
nouns by frequency and using a neighbor as the
confounder. This will also help avoid evaluations
that produce overly optimistic results.
9 Conclusion
Current performance on various natural language
tasks is being judged and published based on
pseudo-word evaluations. It is thus important
to have a clear understanding of the evaluation?s
characteristics. We have shown that the evalu-
ation is strongly affected by confounder choice,
suggesting a nearest frequency neighbor approach
to provide the most reproducible performance and
avoid overly optimistic results. We have shown
that evaluating entire documents instead of sub-
sets of the data produces vastly different results.
We presented a conditional probability baseline
that is both novel to the pseudo-word disambigua-
tion task and strongly outperforms state-of-the-art
models on entire documents. We hope this pro-
vides a new reference point to the pseudo-word
disambiguation task, and enables selectional pref-
erence models whose performance on the task
similarly transfers to larger NLP applications.
Acknowledgments
This work was supported by the National Science
Foundation IIS-0811974, and the Air Force Re-
search Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181. Any opinions, ndings,
and conclusion or recommendations expressed in
this material are those of the authors and do not
necessarily reect the view of the AFRL. Thanks
to Sebastian Pado?, the Stanford NLP Group, and
the anonymous reviewers for very helpful sugges-
tions.
452
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Christian
Boitet and Pete Whitelock, editors, ACL-98, pages
86?90, San Francisco, California. Morgan Kauf-
mann Publishers.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional prefer-
ence from unlabeled text. In Empirical Methods in
Natural Language Processing, pages 59?68, Hon-
olulu, Hawaii.
Lou Burnard. 1995. User Reference Guide for the
British National Corpus. Oxford University Press,
Oxford.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. Machine Learning, 34(1):43?
69.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, Prague, Czech Republic.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Work on statistical methods for
word sense disambiguation. In AAAI Fall Sympo-
sium on Probabilistic Approaches to Natural Lan-
guage, pages 54?60.
Tanja Gaustad. 2001. Statistical corpus-based word
sense disambiguation: Pseudowords vs. real am-
biguous words. In 39th Annual Meeting of the Asso-
ciation for Computational Linguistics - Student Re-
search Workshop.
David Graff. 2002. English Gigaword. Linguistic
Data Consortium.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3):459?484.
Maria Lapata, Scott McDonald, and Frank Keller.
1999. Determinants of adjective-noun plausibility.
In European Chapter of the Association for Compu-
tational Linguistics (EACL).
Preslav I. Nakov and Marti A. Hearst. 2003. Category-
based pseudowords. In Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 67?69, Edmonton, Canada.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
31st Annual Meeting of the Association for Com-
putational Linguistics, pages 183?190, Columbus,
Ohio.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
37th Annual Meeting of the Association for Compu-
tational Linguistics, pages 104?111.
Hinrich Schutze. 1992. Context space. In AAAI Fall
Symposium on Probabilistic Approaches to Natural
Language, pages 113?120.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Inter-
national Conference on Computational Linguistics
(COLING).
Beat Zapirain, Eneko Agirre, and Llus Mrquez. 2009.
Generalizing over lexical features: Selectional pref-
erences for semantic role classification. In Joint
Conference of the 47th Annual Meeting of the As-
sociation for Computational Linguistics and the
4th International Joint Conference on Natural Lan-
guage Processing, Singapore.
453
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 806?814,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning to Follow Navigational Directions
Adam Vogel and Dan Jurafsky
Department of Computer Science
Stanford University
{acvogel,jurafsky}@stanford.edu
Abstract
We present a system that learns to fol-
low navigational natural language direc-
tions. Where traditional models learn
from linguistic annotation or word distri-
butions, our approach is grounded in the
world, learning by apprenticeship from
routes through a map paired with English
descriptions. Lacking an explicit align-
ment between the text and the reference
path makes it difficult to determine what
portions of the language describe which
aspects of the route. We learn this corre-
spondence with a reinforcement learning
algorithm, using the deviation of the route
we follow from the intended path as a re-
ward signal. We demonstrate that our sys-
tem successfully grounds the meaning of
spatial terms like above and south into ge-
ometric properties of paths.
1 Introduction
Spatial language usage is a vital component for
physically grounded language understanding sys-
tems. Spoken language interfaces to robotic assis-
tants (Wei et al, 2009) and Geographic Informa-
tion Systems (Wang et al, 2004) must cope with
the inherent ambiguity in spatial descriptions.
The semantics of imperative and spatial lan-
guage is heavily dependent on the physical set-
ting it is situated in, motivating automated learn-
ing approaches to acquiring meaning. Tradi-
tional accounts of learning typically rely on lin-
guistic annotation (Zettlemoyer and Collins, 2009)
or word distributions (Curran, 2003). In con-
trast, we present an apprenticeship learning sys-
tem which learns to imitate human instruction fol-
lowing, without linguistic annotation. Solved us-
ing a reinforcement learning algorithm, our sys-
tem acquires the meaning of spatial words through
1. go vertically down until you?re underneath eh
diamond mine
2. then eh go right until you?re
3. you?re between springbok and highest view-
point
Figure 1: A path appears on the instruction giver?s
map, who describes it to the instruction follower.
grounded interaction with the world. This draws
on the intuition that children learn to use spatial
language through a mixture of observing adult lan-
guage usage and situated interaction in the world,
usually without explicit definitions (Tanz, 1980).
Our system learns to follow navigational direc-
tions in a route following task. We evaluate our
approach on the HCRC Map Task corpus (Ander-
son et al, 1991), a collection of spoken dialogs
describing paths to take through a map. In this
setting, two participants, the instruction giver and
instruction follower, each have a map composed
of named landmarks. Furthermore, the instruc-
tion giver has a route drawn on her map, and it
is her task to describe the path to the instruction
follower, who cannot see the reference path. Our
system learns to interpret these navigational direc-
tions, without access to explicit linguistic annota-
tion.
We frame direction following as an apprentice-
ship learning problem and solve it with a rein-
forcement learning algorithm, extending previous
work on interpreting instructions by Branavan et
al. (2009). Our task is to learn a policy, or mapping
806
from world state to action, which most closely fol-
lows the reference route. Our state space com-
bines world and linguistic features, representing
both our current position on the map and the com-
municative content of the utterances we are inter-
preting. During training we have access to the ref-
erence path, which allows us to measure the util-
ity, or reward, for each step of interpretation. Us-
ing this reward signal as a form of supervision, we
learn a policy to maximize the expected reward on
unseen examples.
2 Related Work
Levit and Roy (2007) developed a spatial seman-
tics for the Map Task corpus. They represent
instructions as Navigational Information Units,
which decompose the meaning of an instruction
into orthogonal constituents such as the reference
object, the type of movement, and quantitative as-
pect. For example, they represent the meaning of
?move two inches toward the house? as a reference
object (the house), a path descriptor (towards), and
a quantitative aspect (two inches). These represen-
tations are then combined to form a path through
the map. However, they do not learn these rep-
resentations from text, leaving natural language
processing as an open problem. The semantics
in our paper is simpler, eschewing quantitative as-
pects and path descriptors, and instead focusing
on reference objects and frames of reference. This
simplifies the learning task, without sacrificing the
core of their representation.
Learning to follow instructions by interacting
with the world was recently introduced by Brana-
van et al (2009), who developed a system which
learns to follow Windows Help guides. Our re-
inforcement learning formulation follows closely
from their work. Their approach can incorpo-
rate expert supervision into the reward function
in a similar manner to this paper, but is also able
to learn effectively from environment feedback
alone. The Map Task corpus is free form conversa-
tional English, whereas the Windows instructions
are written by a professional. In the Map Task cor-
pus we only observe expert route following behav-
ior, but are not told how portions of the text cor-
respond to parts of the path, leading to a difficult
learning problem.
The semantics of spatial language has been
studied for some time in the linguistics literature.
Talmy (1983) classifies the way spatial meaning is
Figure 2: The instruction giver and instruction fol-
lower face each other, and cannot see each others
maps.
encoded syntactically, and Fillmore (1997) studies
spatial terms as a subset of deictic language, which
depends heavily on non-linguistic context. Levin-
son (2003) conducted a cross-linguistic semantic
typology of spatial systems. Levinson categorizes
the frames of reference, or spatial coordinate sys-
tems1, into
1. Egocentric: Speaker/hearer centered frame
of reference. Ex: ?the ball to your left?.
2. Allocentric: Speaker independent. Ex: ?the
road to the north of the house?
Levinson further classifies allocentric frames of
reference into absolute, which includes the cardi-
nal directions, and intrinsic, which refers to a fea-
tured side of an object, such as ?the front of the
car?. Our spatial feature representation follows
this egocentric/allocentric distinction. The intrin-
sic frame of reference occurs rarely in the Map
Task corpus and is ignored, as speakers tend not
to mention features of the landmarks beyond their
names.
Regier (1996) studied the learning of spatial
language from static 2-D diagrams, learning to
distinguish between terms with a connectionist
model. He focused on the meaning of individual
terms, pairing a diagram with a given word. In
contrast, we learn from whole texts paired with a
1Not all languages exhibit all frames of reference. Terms
for ?up? and ?down? are exhibited in most all languages, while
?left? and ?right? are absent in some. Gravity breaks the sym-
metry between ?up? and ?down? but no such physical distinc-
tion exists for ?left? and ?right?, which contributes to the dif-
ficulty children have learning them.
807
path, which requires learning the correspondence
between text and world. We use similar geometric
features as Regier, capturing the allocentric frame
of reference.
Spatial semantics have also been explored in
physically grounded systems. Kuipers (2000) de-
veloped the Spatial Semantic Hierarchy, a knowl-
edge representation formalism for representing
different levels of granularity in spatial knowl-
edge. It combines sensory, metrical, and topolog-
ical information in a single framework. Kuipers
et al demonstrate its effectiveness on a physical
robot, but did not address the learning problem.
More generally, apprenticeship learning is well
studied in the reinforcement learning literature,
where the goal is to mimic the behavior of an ex-
pert in some decision making domain. Notable ex-
amples include (Abbeel and Ng, 2004), who train
a helicopter controller from pilot demonstration.
3 The Map Task Corpus
The HCRC Map Task Corpus (Anderson et al,
1991) is a set of dialogs between an instruction
giver and an instruction follower. Each participant
has a map with small named landmarks. Addition-
ally, the instruction giver has a path drawn on her
map, and must communicate this path to the in-
struction follower in natural language. Figure 1
shows a portion of the instruction giver?s map and
a sample of the instruction giver language which
describes part of the path.
The Map Task Corpus consists of 128 dialogs,
together with 16 different maps. The speech has
been transcribed and segmented into utterances,
based on the length of pauses. We restrict our
attention to just the utterances of the instruction
giver, ignoring the instruction follower. This is to
reduce redundancy and noise in the data - the in-
struction follower rarely introduces new informa-
tion, instead asking for clarification or giving con-
firmation. The landmarks on the instruction fol-
lower map sometimes differ in location from the
instruction giver?s. We ignore this caveat, giving
the system access to the instruction giver?s land-
marks, without the reference path.
Our task is to build an automated instruction
follower. Whereas the original participants could
speak freely, our system does not have the ability
to query the instruction giver and must instead rely
only on the previously recorded dialogs.
Figure 3: Sample state transition. Both actions get
credit for visiting the great rock after the indian
country. Action a1 also gets credit for passing the
great rock on the correct side.
4 Reinforcement Learning Formulation
We frame the direction following task as a sequen-
tial decision making problem. We interpret ut-
terances in order, where our interpretation is ex-
pressed by moving on the map. Our goal is to
construct a series of moves in the map which most
closely matches the expert path.
We define intermediate steps in our interpreta-
tion as states in a set S, and interpretive steps as
actions drawn from a set A. To measure the fi-
delity of our path with respect to the expert, we
define a reward function R : S ? A? R+ which
measures the utility of choosing a particular action
in a particular state. Executing action a in state s
carries us to a new state s?, and we denote this tran-
sition function by s? = T (s, a). All transitions are
deterministic in this paper.2
For training we are given a set of dialogs D.
Each dialog d ? D is segmented into utter-
ances (u1, . . . , um) and is paired with a map,
which is composed of a set of named landmarks
(l1, . . . , ln).
4.1 State
The states of our decision making problem com-
bine both our position in the dialog d and the path
we have taken so far on the map. A state s ? S is
composed of s = (ui, l, c), where l is the named
landmark we are located next to and c is a cardinal
direction drawn from {North,South,East,West}
which determines which side of l we are on.
Lastly, ui is the utterance in d we are currently
interpreting.
2Our learning algorithm is not dependent on a determin-
istic transition function and can be applied to domains with
stochastic transitions, such as robot locomotion.
808
4.2 Action
An action a ? A is composed of a named land-
mark l, the target of the action, together with a
cardinal direction c which determines which side
to pass l on. Additionally, a can be the null action,
with l = l? and c = c?. In this case, we interpret
an utterance without moving on the map. A target
l together with a cardinal direction c determine a
point on the map, which is a fixed distance from l
in the direction of c.
We make the assumption that at most one in-
struction occurs in a given utterance. This does not
always hold true - the instruction giver sometimes
chains commands together in a single utterance.
4.3 Transition
Executing action a = (l?, c?) in state s = (ui, l, c)
leads us to a new state s? = T (s, a). This tran-
sition moves us to the next utterance to interpret,
and moves our location to the target of the action.
If a is the null action, s = (ui+1, l, c), otherwise
s? = (ui+1, l?, c?). Figure 3 displays the state tran-
sitions two different actions.
To form a path through the map, we connect
these state waypoints with a path planner3 based
on A?, where the landmarks are obstacles. In a
physical system, this would be replaced with a
robot motion planner.
4.4 Reward
We define a reward function R(s, a) which mea-
sures the utility of executing action a in state s.
We wish to construct a route which follows the
expert path as closely as possible. We consider a
proposed route P close to the expert path Pe if P
visits landmarks in the same order as Pe, and also
passes them on the correct side.
For a given transition s = (ui, l, c), a = (l?, c?),
we have a binary feature indicating if the expert
path moves from l to l?. In Figure 3, both a1 and
a2 visit the next landmark in the correct order.
To measure if an action is to the correct side of
a landmark, we have another binary feature indi-
cating if Pe passes l? on side c. In Figure 3, only
a1 passes l? on the correct side.
In addition, we have a feature which counts the
number of words in ui which also occur in the
name of l?. This encourages us to choose poli-
cies which interpret language relevant to a given
3We used the Java Path Planning Library, available at
http://www.cs.cmu.edu/?ggordon/PathPlan/.
landmark.
Our reward function is a linear combination of
these features.
4.5 Policy
We formally define an interpretive strategy as a
policy pi : S ? A, a mapping from states to ac-
tions. Our goal is to find a policy pi which max-
imizes the expected reward Epi[R(s, pi(s))]. The
expected reward of following policy pi from state
s is referred to as the value of s, expressed as
V pi(s) = Epi[R(s, pi(s))] (1)
When comparing the utilities of executing an ac-
tion a in a state s, it is useful to define a function
Qpi(s, a) = R(s, a) + V pi(T (s, a))
= R(s, a) +Qpi(T (s, a), pi(s)) (2)
which measures the utility of executing a, and fol-
lowing the policy pi for the remainder. A given Q
function implicitly defines a policy pi by
pi(s) = max
a
Q(s, a). (3)
Basic reinforcement learning methods treat
states as atomic entities, in essence estimating V pi
as a table. However, at test time we are following
new directions for a map we haven?t previously
seen. Thus, we represent state/action pairs with a
feature vector ?(s, a) ? RK . We then represent
the Q function as a linear combination of the fea-
tures,
Q(s, a) = ?T?(s, a) (4)
and learn weights ? which most closely approxi-
mate the true expected reward.
4.6 Features
Our features ?(s, a) are a mixture of world and
linguistic information. The linguistic information
in our feature representation includes the instruc-
tion giver utterance and the names of landmarks
on the map. Additionally, we furnish our algo-
rithm with a list of English spatial terms, shown
in Table 1. Our feature set includes approximately
200 features. Learning exactly which words in-
fluence decision making is difficult; reinforcement
learning algorithms have problems with the large,
sparse feature vectors common in natural language
processing.
For a given state s = (u, l, c) and action a =
(l?, c?), our feature vector ?(s, a) is composed of
the following:
809
above, below, under, underneath, over, bottom,
top, up, down, left, right, north, south, east, west,
on
Table 1: The list of given spatial terms.
? Coherence: The number of wordsw ? u that
occur in the name of l?
? Landmark Locality: Binary feature indicat-
ing if l? is the closest landmark to l
? Direction Locality: Binary feature indicat-
ing if cardinal direction c? is the side of l?
closest to (l, c)
? Null Action: Binary feature indicating if l? =
NULL
? Allocentric Spatial: Binary feature which
conjoins the side c we pass the landmark on
with each spatial term w ? u. This allows us
to capture that the word above tends to indi-
cate passing to the north of the landmark.
? Egocentric Spatial: Binary feature which
conjoins the cardinal direction we move in
with each spatial term w ? u. For instance, if
(l, c) is above (l?, c?), the direction from our
current position is south. We conjoin this di-
rection with each spatial term, giving binary
features such as ?the word down appears in
the utterance and we move to the south?.
5 Approximate Dynamic Programming
Given this feature representation, our problem is
to find a parameter vector ? ? RK for which
Q(s, a) = ?T?(s, a) most closely approximates
E[R(s, a)]. To learn these weights ? we use
SARSA (Sutton and Barto, 1998), an online learn-
ing algorithm similar to Q-learning (Watkins and
Dayan, 1992).
Algorithm 1 details the learning algorithm,
which we follow here. We iterate over training
documents d ? D. In a given state st, we act ac-
cording to a probabilistic policy defined in terms
of the Q function. After every transition we up-
date ?, which changes how we act in subsequent
steps.
Exploration is a key issue in any RL algorithm.
If we act greedily with respect to our current Q
function, we might never visit states which are ac-
Input: Dialog set D
Reward function R
Feature function ?
Transition function T
Learning rate ?t
Output: Feature weights ?
1 Initialize ? to small random values
2 until ? converges do
3 foreach Dialog d ? D do
4 Initialize s0 = (l1, u1, ?),
a0 ? Pr(a0|s0; ?)
5 for t = 0; st non-terminal; t++ do
6 Act: st+1 = T (st, at)
7 Decide: at+1 ? Pr(at+1|st+1; ?)
8 Update:
9 ?? R(st, at) + ?T?(st+1, at+1)
10 ? ?T?(st, at)
11 ? ? ? + ?t?(st, at)?
12 end
13 end
14 end
15 return ?
Algorithm 1: The SARSA learning algorithm.
tually higher in value. We utilize Boltzmann ex-
ploration, for which
Pr(at|st; ?) =
exp( 1? ?
T?(st, at))
?
a? exp(
1
? ?
T?(st, a?))
(5)
The parameter ? is referred to as the tempera-
ture, with a higher temperature causing more ex-
ploration, and a lower temperature causing more
exploitation. In our experiments ? = 2.
Acting with this exploration policy, we iterate
through the training dialogs, updating our fea-
ture weights ? as we go. The update step looks
at two successive state transitions. Suppose we
are in state st, execute action at, receive reward
rt = R(st, at), transition to state st+1, and there
choose action at+1. The variables of interest are
(st, at, rt, st+1, at+1), which motivates the name
SARSA.
Our current estimate of the Q function is
Q(s, a) = ?T?(s, a). By the Bellman equation,
for the true Q function
Q(st, at) = R(st, at) + max
a?
Q(st+1, a
?) (6)
After each action, we want to move ? to minimize
the temporal difference,
R(st, at) +Q(st+1, at+1)?Q(st, at) (7)
810
Map 4g Map 10g
Figure 4: Sample output from the SARSA policy. The dashed black line is the reference path and the
solid red line is the path the system follows.
For each feature ?i(st, at), we change ?i propor-
tional to this temporal difference, tempered by a
learning rate ?t. We update ? according to
? = ?+?t?(st, at)(R(st, at)
+ ?T?(st+1, at+1)? ?
T?(st, at)) (8)
Here ?t is the learning rate, which decays over
time4. In our case, ?t = 1010+t , which was tuned on
the training set. We determine convergence of the
algorithm by examining the magnitude of updates
to ?. We stop the algorithm when
||?t+1 ? ?t||? <  (9)
6 Experimental Design
We evaluate our system on the Map Task corpus,
splitting the corpus into 96 training dialogs and 32
test dialogs. The whole corpus consists of approx-
imately 105,000 word tokens. The maps seen at
test time do not occur in the training set, but some
of the human participants are present in both.
4To guarantee convergence, we require
P
t ?t = ? andP
t ?
2
t < ?. Intuitively, the sum diverging guarantees we
can still learn arbitrarily far into the future, and the sum of
squares converging guarantees that our updates will converge
at some point.
6.1 Evaluation
We evaluate how closely the path P generated by
our system follows the expert path Pe. We mea-
sure this with respect to two metrics: the order
in which we visit landmarks and the side we pass
them on.
To determine the order Pe visits landmarks we
compute the minimum distance from Pe to each
landmark, and threshold it at a fixed value.
To score path P , we compare the order it visits
landmarks to the expert path. A transition l ? l?
which occurs in P counts as correct if the same
transition occurs in Pe. Let |P | be the number
of landmark transitions in a path P , and N the
number of correct transitions in P . We define the
order precision as N/|P |, and the order recall as
N/|Pe|.
We also evaluate how well we are at passing
landmarks on the correct side. We calculate the
distance of Pe to each side of the landmark, con-
sidering the path to visit a side of the landmark
if the distance is below a threshold. This means
that a path might be considered to visit multiple
sides of a landmark, although in practice it is usu-
811
Figure 5: This figure shows the relative weights of spatial features organized by spatial word. The top
row shows the weights of allocentric (landmark-centered) features. For example, the top left figure shows
that when the word above occurs, our policy prefers to go to the north of the target landmark. The bottom
row shows the weights of egocentric (absolute) spatial features. The bottom left figure shows that given
the word above, our policy prefers to move in a southerly cardinal direction.
ally one. If C is the number of landmarks we pass
on the correct side, define the side precision as
C/|P |, and the side recall as C/|Pe|.
6.2 Comparison Systems
The baseline policy simply visits the closest land-
mark at each step, taking the side of the landmark
which is closest. It pays no attention to the direc-
tion language.
We also compare against the policy gradient
learning algorithm of Branavan et al (2009). They
parametrize a probabilistic policy Pr(s|a; ?) as a
log-linear model, in a similar fashion to our explo-
ration policy. During training, the learning algo-
rithm adjusts the weights ? according to the gradi-
ent of the value function defined by this distribu-
tion.
Reinforcement learning algorithms can be clas-
sified into value based and policy based. Value
methods estimate a value function V for each
state, then act greedily with respect to it. Pol-
icy learning algorithms directly search through
the space of policies. SARSA is a value based
method, and the policy gradient algorithm is pol-
icy based.
Visit Order Side
P R F1 P R F1
Baseline 28.4 37.2 32.2 46.1 60.3 52.2
PG 31.1 43.9 36.4 49.5 69.9 57.9
SARSA 45.7 51.0 48.2 58.0 64.7 61.2
Table 2: Experimental results. Visit order shows
how well we follow the order in which the answer
path visits landmarks. ?Side? shows how success-
fully we pass on the correct side of landmarks.
7 Results
Table 2 details the quantitative performance of the
different algorithms. Both SARSA and the policy
gradient method outperform the baseline, but still
fall significantly short of expert performance. The
baseline policy performs surprisingly well, espe-
cially at selecting the correct side to visit a land-
mark.
The disparity between learning approaches and
gold standard performance can be attributed to
several factors. The language in this corpus is con-
versational, frequently ungrammatical, and con-
tains troublesome aspects of dialog such as con-
versational repairs and repetition. Secondly, our
action and feature space are relatively primitive,
and don?t capture the full range of spatial expres-
sion. Path descriptors, such as the difference be-
tween around and past are absent, and our feature
812
representation is relatively simple.
The SARSA learning algorithm accrues more
reward than the policy gradient algorithm. Like
most gradient based optimization methods, policy
gradient algorithms oftentimes get stuck in local
maxima, and are sensitive to the initial conditions.
Furthermore, as the size of the feature vectorK in-
creases, the space becomes even more difficult to
search. There are no guarantees that SARSA has
reached the best policy under our feature space,
and this is difficult to determine empirically. Thus,
some accuracy might be gained by considering
different RL algorithms.
8 Discussion
Examining the feature weights ? sheds some light
on our performance. Figure 5 shows the relative
strength of weights for several spatial terms. Re-
call that the two main classes of spatial features in
? are egocentric (what direction we move in) and
allocentric (on which side we pass a landmark),
combined with each spatial word.
Allocentric terms such as above and below tend
to be interpreted as going to the north and south
of landmarks, respectively. Interestingly, our sys-
tem tends to move in the opposite cardinal direc-
tion, i.e. the agent moves south in the egocen-
tric frame of reference. This suggests that people
use above when we are already above a landmark.
South slightly favors passing on the south side of
landmarks, and has a heavy tendency to move in
a southerly direction. This suggests that south is
used more frequently in an egocentric reference
frame.
Our system has difficulty learning the meaning
of right. Right is often used as a conversational
filler, and also for dialog alignment, such as
?right okay right go vertically up then
between the springboks and the highest
viewpoint.?
Furthermore, right can be used in both an egocen-
tric or allocentric reference frame. Compare
?go to the uh right of the mine?
which utilizes an allocentric frame, with
?right then go eh uh to your right hori-
zontally?
which uses an egocentric frame of reference. It
is difficult to distinguish between these meanings
without syntactic features.
9 Conclusion
We presented a reinforcement learning system
which learns to interpret natural language direc-
tions. Critically, our approach uses no semantic
annotation, instead learning directly from human
demonstration. It successfully acquires a subset
of spatial semantics, using reinforcement learning
to derive the correspondence between instruction
language and features of paths. While our results
are still preliminary, we believe our model repre-
sents a significant advance in learning natural lan-
guage meaning, drawing its supervision from hu-
man demonstration rather than word distributions
or hand-labeled semantic tags. Framing language
acquisition as apprenticeship learning is a fruitful
research direction which has the potential to con-
nect the symbolic, linguistic domain to the non-
symbolic, sensory aspects of cognition.
Acknowledgments
This research was partially supported by the Na-
tional Science Foundation via a Graduate Re-
search Fellowship to the first author and award
IIS-0811974 to the second author and by the Air
Force Research Laboratory (AFRL), under prime
contract no. FA8750-09-C-0181. Thanks to
Michael Levit and Deb Roy for providing digital
representations of the maps and a subset of the cor-
pus annotated with their spatial representation.
References
Pieter Abbeel and Andrew Y. Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proceedings of the Twenty-first International Con-
ference on Machine Learning. ACM Press.
A. Anderson, M. Bader, E. Bard, E. Boyle, G. Do-
herty, S. Garrod, S. Isard, J. Kowtko, J. Mcallister,
J. Miller, C. Sotillo, H. Thompson, and R. Weinert.
1991. The HCRC map task corpus. Language and
Speech, 34, pages 351?366.
S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In ACL-IJCNLP
?09.
James Richard Curran. 2003. From Distributional to
Semantic Similarity. Ph.D. thesis, University of Ed-
inburgh.
Charles Fillmore. 1997. Lectures on Deixis. Stanford:
CSLI Publications.
Benjamin Kuipers. 2000. The spatial semantic hierar-
chy. Artificial Intelligence, 119(1-2):191?233.
813
Stephen Levinson. 2003. Space In Language And
Cognition: Explorations In Cognitive Diversity.
Cambridge University Press.
Michael Levit and Deb Roy. 2007. Interpretation
of spatial language in a map navigation task. In
IEEE Transactions on Systems, Man, and Cybernet-
ics, Part B, 37(3), pages 667?679.
Terry Regier. 1996. The Human Semantic Potential:
Spatial Language and Constrained Connectionism.
The MIT Press.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. MIT Press.
Leonard Talmy. 1983. How language structures space.
In Spatial Orientation: Theory, Research, and Ap-
plication.
Christine Tanz. 1980. Studies in the acquisition of de-
ictic terms. Cambridge University Press.
Hongmei Wang, Alan M. Maceachren, and Guoray
Cai. 2004. Design of human-GIS dialogue for com-
munication of vague spatial concepts. In GIScience.
C. J. C. H. Watkins and P. Dayan. 1992. Q-learning.
Machine Learning, pages 8:279?292.
Yuan Wei, Emma Brunskill, Thomas Kollar, and
Nicholas Roy. 2009. Where to go: interpreting nat-
ural directions using global inference. In ICRA?09:
Proceedings of the 2009 IEEE international con-
ference on Robotics and Automation, pages 3761?
3767, Piscataway, NJ, USA. IEEE Press.
Luke S. Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In ACL-IJCNLP ?09, pages
976?984.
814
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1278?1287,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing
Valentin I. Spitkovsky
Computer Science Department
Stanford University and Google Inc.
valentin@google.com
Daniel Jurafsky
Departments of Linguistics and
Computer Science, Stanford University
jurafsky@stanford.edu
Hiyan Alshawi
Google Inc.
hiyan@google.com
Abstract
We show how web mark-up can be used
to improve unsupervised dependency pars-
ing. Starting from raw bracketings of four
common HTML tags (anchors, bold, ital-
ics and underlines), we refine approximate
partial phrase boundaries to yield accurate
parsing constraints. Conversion proce-
dures fall out of our linguistic analysis of
a newly available million-word hyper-text
corpus. We demonstrate that derived con-
straints aid grammar induction by training
Klein and Manning?s Dependency Model
with Valence (DMV) on this data set: pars-
ing accuracy on Section 23 (all sentences)
of the Wall Street Journal corpus jumps
to 50.4%, beating previous state-of-the-
art by more than 5%. Web-scale exper-
iments show that the DMV, perhaps be-
cause it is unlexicalized, does not benefit
from orders of magnitude more annotated
but noisier data. Our model, trained on a
single blog, generalizes to 53.3% accuracy
out-of-domain, against the Brown corpus
? nearly 10% higher than the previous
published best. The fact that web mark-up
strongly correlates with syntactic structure
may have broad applicability in NLP.
1 Introduction
Unsupervised learning of hierarchical syntactic
structure from free-form natural language text is
a hard problem whose eventual solution promises
to benefit applications ranging from question an-
swering to speech recognition and machine trans-
lation. A restricted version of this problem that tar-
gets dependencies and assumes partial annotation
? sentence boundaries and part-of-speech (POS)
tagging ? has received much attention. Klein
and Manning (2004) were the first to beat a sim-
ple parsing heuristic, the right-branching baseline;
today?s state-of-the-art systems (Headden et al,
2009; Cohen and Smith, 2009; Spitkovsky et al,
2010a) are rooted in their Dependency Model with
Valence (DMV), still trained using variants of EM.
Pereira and Schabes (1992) outlined three ma-
jor problems with classic EM, applied to a related
problem, constituent parsing. They extended clas-
sic inside-outside re-estimation (Baker, 1979) to
respect any bracketing constraints included with
a training corpus. This conditioning on partial
parses addressed all three problems, leading to:
(i) linguistically reasonable constituent boundaries
and induced grammars more likely to agree with
qualitative judgments of sentence structure, which
is underdetermined by unannotated text; (ii) fewer
iterations needed to reach a good grammar, coun-
tering convergence properties that sharply deterio-
rate with the number of non-terminal symbols, due
to a proliferation of local maxima; and (iii) better
(in the best case, linear) time complexity per it-
eration, versus running time that is ordinarily cu-
bic in both sentence length and the total num-
ber of non-terminals, rendering sufficiently large
grammars computationally impractical. Their al-
gorithm sometimes found good solutions from
bracketed corpora but not from raw text, sup-
porting the view that purely unsupervised, self-
organizing inference methods can miss the trees
for the forest of distributional regularities. This
was a promising break-through, but the problem
of whence to get partial bracketings was left open.
We suggest mining partial bracketings from a
cheap and abundant natural language resource: the
hyper-text mark-up that annotates web-pages. For
example, consider that anchor text can match lin-
guistic constituents, such as verb phrases, exactly:
..., whereas McCain is secure on the topic, Obama
<a>[VP worries about winning the pro-Israel vote]</a>.
To validate this idea, we created a new data set,
novel in combining a real blog?s raw HTML with
tree-bank-like constituent structure parses, gener-
1278
ated automatically. Our linguistic analysis of the
most prevalent tags (anchors, bold, italics and un-
derlines) over its 1M+ words reveals a strong con-
nection between syntax and mark-up (all of our
examples draw from this corpus), inspiring several
simple techniques for automatically deriving pars-
ing constraints. Experiments with both hard and
more flexible constraints, as well as with different
styles and quantities of annotated training data ?
the blog, web news and the web itself, confirm that
mark-up-induced constraints consistently improve
(otherwise unsupervised) dependency parsing.
2 Intuition and Motivating Examples
It is natural to expect hidden structure to seep
through when a person annotates a sentence. As it
happens, a non-trivial fraction of the world?s pop-
ulation routinely annotates text diligently, if only
partially and informally.1 They inject hyper-links,
vary font sizes, and toggle colors and styles, using
mark-up technologies such as HTML and XML.
As noted, web annotations can be indicative of
phrase boundaries, e.g., in a complicated sentence:
In 1998, however, as I <a>[VP established in
<i>[NP The New Republic]</i>]</a> and Bill
Clinton just <a>[VP confirmed in his memoirs]</a>,
Netanyahu changed his mind and ...
In doing so, mark-up sometimes offers useful cues
even for low-level tokenization decisions:
[NP [NP Libyan ruler]
<a>[NP Mu?ammar al-Qaddafi]</a>] referred to ...
(NP (ADJP (NP (JJ Libyan) (NN ruler))
(JJ Mu))
(?? ?) (NN ammar) (NNS al-Qaddafi))
Above, a backward quote in an Arabic name con-
fuses the Stanford parser.2 Yet mark-up lines up
with the broken noun phrase, signals cohesion, and
moreover sheds light on the internal structure of
a compound. As Vadas and Curran (2007) point
out, such details are frequently omitted even from
manually compiled tree-banks that err on the side
of flat annotations of base-NPs.
Admittedly, not all boundaries between HTML
tags and syntactic constituents match up nicely:
..., but [S [NP the <a><i>Toronto
Star</i>][VP reports [NP this][PP in the
softest possible way]</a>,[S stating only that ...]]]
Combining parsing with mark-up may not be
straight-forward, but there is hope: even above,
1Even when (American) grammar schools lived up to their
name, they only taught dependencies. This was back in the
days before constituent grammars were invented.
2http://nlp.stanford.edu:8080/parser/
one of each nested tag?s boundaries aligns; and
Toronto Star?s neglected determiner could be for-
given, certainly within a dependency formulation.
3 A High-Level Outline of Our Approach
Our idea is to implement the DMV (Klein and
Manning, 2004) ? a standard unsupervised gram-
mar inducer. But instead of learning the unan-
notated test set, we train with text that contains
web mark-up, using various ways of converting
HTML into parsing constraints. We still test on
WSJ (Marcus et al, 1993), in the standard way,
and also check generalization against a hidden
data set ? the Brown corpus (Francis and Kucera,
1979). Our parsing constraints come from a blog
? a new corpus we created, the web and news (see
Table 1 for corpora?s sentence and token counts).
To facilitate future work, we make the final
models and our manually-constructed blog data
publicly available.3 Although we are unable
to share larger-scale resources, our main results
should be reproducible, as both linguistic analysis
and our best model rely exclusively on the blog.
Corpus Sentences POS Tokens
WSJ? 49,208 1,028,347
Section 23 2,353 48,201
WSJ45 48,418 986,830
WSJ15 15,922 163,715
Brown100 24,208 391,796
BLOGp 57,809 1,136,659
BLOGt45 56,191 1,048,404
BLOGt15 23,214 212,872
NEWS45 2,263,563,078 32,119,123,561
NEWS15 1,433,779,438 11,786,164,503
WEB45 8,903,458,234 87,269,385,640
WEB15 7,488,669,239 55,014,582,024
Table 1: Sizes of corpora derived from WSJ and
Brown, as well as those we collected from the web.
4 Data Sets for Evaluation and Training
The appeal of unsupervised parsing lies in its abil-
ity to learn from surface text alone; but (intrinsic)
evaluation still requires parsed sentences. Follow-
ing Klein and Manning (2004), we begin with ref-
erence constituent parses and compare against de-
terministically derived dependencies: after prun-
ing out all empty subtrees, punctuation and ter-
minals (tagged # and $) not pronounced where
they appear, we drop all sentences with more
than a prescribed number of tokens remaining and
use automatic ?head-percolation? rules (Collins,
1999) to convert the rest, as is standard practice.
3http://cs.stanford.edu/?valentin/
1279
Length Marked POS Bracketings Length Marked POS Bracketings
Cutoff Sentences Tokens All Multi-Token Cutoff Sentences Tokens All Multi-Token
0 6,047 1,136,659 7,731 6,015 8 485 14,528 710 684
1 of 57,809 149,483 7,731 6,015 9 333 10,484 499 479
2 4,934 124,527 6,482 6,015 10 245 7,887 365 352
3 3,295 85,423 4,476 4,212 15 42 1,519 65 63
4 2,103 56,390 2,952 2,789 20 13 466 20 20
5 1,402 38,265 1,988 1,874 25 6 235 10 10
6 960 27,285 1,365 1,302 30 3 136 6 6
7 692 19,894 992 952 40 0 0 0 0
Table 2: Counts of sentences, tokens and (unique) bracketings for BLOGp, restricted to only those
sentences having at least one bracketing no shorter than the length cutoff (but shorter than the sentence).
Our primary reference sets are derived from the
Penn English Treebank?s Wall Street Journal por-
tion (Marcus et al, 1993): WSJ45 (sentences with
fewer than 46 tokens) and Section 23 of WSJ? (all
sentence lengths). We also evaluate on Brown100,
similarly derived from the parsed portion of the
Brown corpus (Francis and Kucera, 1979). While
we use WSJ45 and WSJ15 to train baseline mod-
els, the bulk of our experiments is with web data.
4.1 A News-Style Blog: Daniel Pipes
Since there was no corpus overlaying syntactic
structure with mark-up, we began constructing a
new one by downloading articles4 from a news-
style blog. Although limited to a single genre ?
political opinion, danielpipes.org is clean, consis-
tently formatted, carefully edited and larger than
WSJ (see Table 1). Spanning decades, Pipes?
editorials are mostly in-domain for POS taggers
and tree-bank-trained parsers; his recent (internet-
era) entries are thoroughly cross-referenced, con-
veniently providing just the mark-up we hoped to
study via uncluttered (printer-friendly) HTML.5
After extracting moderately clean text and
mark-up locations, we used MxTerminator (Rey-
nar and Ratnaparkhi, 1997) to detect sentence
boundaries. This initial automated pass begot mul-
tiple rounds of various semi-automated clean-ups
that involved fixing sentence breaking, modifying
parser-unfriendly tokens, converting HTML enti-
ties and non-ASCII text, correcting typos, and so
on. After throwing away annotations of fractional
words (e.g., <i>basmachi</i>s) and tokens (e.g.,
<i>Sesame Street</i>-like), we broke up all mark-
up that crossed sentence boundaries (i.e., loosely
speaking, replaced constructs like <u>...][S...</u>
with <u>...</u> ][S <u>...</u>) and discarded any
4http://danielpipes.org/art/year/all
5http://danielpipes.org/article print.php?
id=. . .
tags left covering entire sentences.
We finalized two versions of the data: BLOGt,
tagged with the Stanford tagger (Toutanova and
Manning, 2000; Toutanova et al, 2003),6 and
BLOGp, parsed with Charniak?s parser (Charniak,
2001; Charniak and Johnson, 2005).7 The rea-
son for this dichotomy was to use state-of-the-art
parses to analyze the relationship between syntax
and mark-up, yet to prevent jointly tagged (and
non-standard AUX[G]) POS sequences from interfer-
ing with our (otherwise unsupervised) training.8
4.2 Scaled up Quantity: The (English) Web
We built a large (see Table 1) but messy data set,
WEB ? English-looking web-pages, pre-crawled
by a search engine. To avoid machine-generated
spam, we excluded low quality sites flagged by the
indexing system. We kept only sentence-like runs
of words (satisfying punctuation and capitalization
constraints), POS-tagged with TnT (Brants, 2000).
4.3 Scaled up Quality: (English) Web News
In an effort to trade quantity for quality, we con-
structed a smaller, potentially cleaner data set,
NEWS. We reckoned editorialized content would
lead to fewer extracted non-sentences. Perhaps
surprisingly, NEWS is less than an order of magni-
tude smaller than WEB (see Table 1); in part, this
is due to less aggressive filtering ? we trust sites
approved by the human editors at Google News.9
In all other respects, our pre-processing of NEWS
pages was identical to our handling of WEB data.
6http://nlp.stanford.edu/software/
stanford-postagger-2008-09-28.tar.gz
7ftp://ftp.cs.brown.edu/pub/nlparser/
parser05Aug16.tar.gz
8However, since many taggers are themselves trained on
manually parsed corpora, such as WSJ, no parser that relies
on external POS tags could be considered truly unsupervised;
for a fully unsupervised example, see Seginer?s (2007) CCL
parser, available at http://www.seggu.net/ccl/
9http://news.google.com/
1280
5 Linguistic Analysis of Mark-Up
Is there a connection between mark-up and syn-
tactic structure? Previous work (Barr et al, 2008)
has only examined search engine queries, show-
ing that they consist predominantly of short noun
phrases. If web mark-up shared a similar char-
acteristic, it might not provide sufficiently dis-
ambiguating cues to syntactic structure: HTML
tags could be too short (e.g., singletons like
?click <a>here</a>?) or otherwise unhelpful in re-
solving truly difficult ambiguities (such as PP-
attachment). We began simply by counting vari-
ous basic events in BLOGp.
Count POS Sequence Frac Sum
1 1,242 NNP NNP 16.1%
2 643 NNP 8.3 24.4
3 419 NNP NNP NNP 5.4 29.8
4 414 NN 5.4 35.2
5 201 JJ NN 2.6 37.8
6 138 DT NNP NNP 1.8 39.5
7 138 NNS 1.8 41.3
8 112 JJ 1.5 42.8
9 102 VBD 1.3 44.1
10 92 DT NNP NNP NNP 1.2 45.3
11 85 JJ NNS 1.1 46.4
12 79 NNP NN 1.0 47.4
13 76 NN NN 1.0 48.4
14 61 VBN 0.8 49.2
15 60 NNP NNP NNP NNP 0.8 50.0
BLOGp +3,869 more with Count ? 49 50.0%
Table 3: Top 50% of marked POS tag sequences.
Count Non-Terminal Frac Sum
1 5,759 NP 74.5%
2 997 VP 12.9 87.4
3 524 S 6.8 94.2
4 120 PP 1.6 95.7
5 72 ADJP 0.9 96.7
6 61 FRAG 0.8 97.4
7 41 ADVP 0.5 98.0
8 39 SBAR 0.5 98.5
9 19 PRN 0.2 98.7
10 18 NX 0.2 99.0
BLOGp +81 more with Count ? 16 1.0%
Table 4: Top 99% of dominating non-terminals.
5.1 Surface Text Statistics
Out of 57,809 sentences, 6,047 (10.5%) are anno-
tated (see Table 2); and 4,934 (8.5%) have multi-
token bracketings. We do not distinguish HTML
tags and track only unique bracketing end-points
within a sentence. Of these, 6,015 are multi-token
? an average per-sentence yield of 10.4%.10
10A non-trivial fraction of our corpus is older (pre-internet)
unannotated articles, so this estimate may be conservative.
As expected, many of the annotated words are
nouns, but there are adjectives, verbs and other
parts of speech too (see Table 3). Mark-up is short,
typically under five words, yet (by far) the most
frequently marked sequence of POS tags is a pair.
5.2 Common Syntactic Subtrees
For three-quarters of all mark-up, the lowest domi-
nating non-terminal is a noun phrase (see Table 4);
there are also non-trace quantities of verb phrases
(12.9%) and other phrases, clauses and fragments.
Of the top fifteen ? 35.2% of all ? annotated
productions, only one is not a noun phrase (see Ta-
ble 5, left). Four of the fifteen lowest dominating
non-terminals do not match the entire bracketing
? all four miss the leading determiner, as we saw
earlier. In such cases, we recursively split internal
nodes until the bracketing aligned, as follows:
[S [NP the <a>Toronto Star][VP reports [NP this]
[PP in the softest possible way]</a>,[S stating ...]]]
S? NP VP? DT NNP NNP VBZ NP PP S
We can summarize productions more compactly
by using a dependency framework and clipping
off any dependents whose subtrees do not cross a
bracketing boundary, relative to the parent. Thus,
DT NNP NNP VBZ DT IN DT JJS JJ NN
becomes DT NNP VBZ, ?the <a>Star reports</a>.?
Viewed this way, the top fifteen (now collapsed)
productions cover 59.4% of all cases and include
four verb heads, in addition to a preposition and
an adjective (see Table 5, right). This exposes five
cases of inexact matches, three of which involve
neglected determiners or adjectives to the left of
the head. In fact, the only case that cannot be ex-
plained by dropped dependents is #8, where the
daughters are marked but the parent is left out.
Most instances contributing to this pattern are flat
NPs that end with a noun, incorrectly assumed to
be the head of all other words in the phrase, e.g.,
... [NP a 1994 <i>New Yorker</i> article] ...
As this example shows, disagreements (as well
as agreements) between mark-up and machine-
generated parse trees with automatically perco-
lated heads should be taken with a grain of salt.11
11In a relatively recent study, Ravi et al (2008) report
that Charniak?s re-ranking parser (Charniak and Johnson,
2005) ? reranking-parserAug06.tar.gz, also available
from ftp://ftp.cs.brown.edu/pub/nlparser/ ? at-
tains 86.3% accuracy when trained on WSJ and tested against
Brown; its nearly 5% performance loss out-of-domain is con-
sistent with the numbers originally reported by Gildea (2001).
1281
Count Constituent Production Frac Sum
1 746 NP? NNP NNP 9.6%
2 357 NP? NNP 4.6 14.3
3 266 NP? NP PP 3.4 17.7
4 183 NP? NNP NNP NNP 2.4 20.1
5 165 NP? DT NNP NNP 2.1 22.2
6 140 NP? NN 1.8 24.0
7 131 NP? DT NNP NNP NNP 1.7 25.7
8 130 NP? DT NN 1.7 27.4
9 127 NP? DT NNP NNP 1.6 29.0
10 109 S ? NP VP 1.4 30.4
11 91 NP? DT NNP NNP NNP 1.2 31.6
12 82 NP? DT JJ NN 1.1 32.7
13 79 NP? NNS 1.0 33.7
14 65 NP? JJ NN 0.8 34.5
15 60 NP? NP NP 0.8 35.3
BLOGp +5,000 more with Count ? 60 64.7%
Count Head-Outward Spawn Frac Sum
1 1,889 NNP 24.4%
2 623 NN 8.1 32.5
3 470 DT NNP 6.1 38.6
4 458 DT NN 5.9 44.5
5 345 NNS 4.5 49.0
6 109 NNPS 1.4 50.4
7 98 VBG 1.3 51.6
8 96 NNP NNP NN 1.2 52.9
9 80 VBD 1.0 53.9
10 77 IN 1.0 54.9
11 74 VBN 1.0 55.9
12 73 DT JJ NN 0.9 56.8
13 71 VBZ 0.9 57.7
14 69 POS NNP 0.9 58.6
15 63 JJ 0.8 59.4
BLOGp +3,136 more with Count ? 62 40.6%
Table 5: Top 15 marked productions, viewed as constituents (left) and as dependencies (right), after
recursively expanding any internal nodes that did not align with the bracketing (underlined). Tabulated
dependencies were collapsed, dropping any dependents that fell entirely in the same region as their parent
(i.e., both inside the bracketing, both to its left or both to its right), keeping only crossing attachments.
5.3 Proposed Parsing Constraints
The straight-forward approach ? forcing mark-up
to correspond to constituents ? agrees with Char-
niak?s parse trees only 48.0% of the time, e.g.,
... in [NP<a>[NP an analysis]</a>[PP of perhaps the
most astonishing PC item I have yet stumbled upon]].
This number should be higher, as the vast major-
ity of disagreements are due to tree-bank idiosyn-
crasies (e.g., bare NPs). Earlier examples of in-
complete constituents (e.g., legitimately missing
determiners) would also be fine in many linguistic
theories (e.g., as N-bars). A dependency formula-
tion is less sensitive to such stylistic differences.
We begin with the hardest possible constraint on
dependencies, then slowly relax it. Every example
used to demonstrate a softer constraint doubles
as a counter-example against all previous versions.
? strict ? seals mark-up into attachments, i.e.,
inside a bracketing, enforces exactly one external
arc ? into the overall head. This agrees with
head-percolated trees just 35.6% of the time, e.g.,
As author of <i>The Satanic Verses</i>, I ...
? loose ? same as strict, but allows the bracket-
ing?s head word to have external dependents. This
relaxation already agrees with head-percolated de-
pendencies 87.5% of the time, catching many
(though far from all) dropped dependents, e.g.,
. . . the <i>Toronto Star</i> reports . . .
? sprawl ? same as loose, but now allows all
words inside a bracketing to attach external de-
pendents.12 This boosts agreement with head-
percolated trees to 95.1%, handling new cases,
e.g., where ?Toronto Star? is embedded in longer
mark-up that includes its own parent ? a verb:
. . . the <a>Toronto Star reports . . .</a> . . .
? tear ? allows mark-up to fracture after all,
requiring only that the external heads attaching the
pieces lie to the same side of the bracketing. This
propels agreement with percolated dependencies
to 98.9%, fixing previously broken PP-attachment
ambiguities, e.g., a fused phrase like ?Fox News in
Canada? that detached a preposition from its verb:
... concession ... has raised eyebrows among those
waiting [PP for <a>Fox News][PP in Canada]</a>.
Most of the remaining 1.1% of disagreements are
due to parser errors. Nevertheless, it is possible for
mark-up to be torn apart by external heads from
both sides. We leave this section with a (very rare)
true negative example. Below, ?CSA? modifies
?authority? (to its left), appositively, while ?Al-
Manar? modifies ?television? (to its right):13
The French broadcasting authority, <a>CSA, banned
... Al-Manar</a> satellite television from ...
12This view evokes the trapezoids of the O(n3) recognizer
for split head automaton grammars (Eisner and Satta, 1999).
13But this is a stretch, since the comma after ?CSA? ren-
ders the marked phrase ungrammatical even out of context.
1282
6 Experimental Methods and Metrics
We implemented the DMV (Klein and Manning,
2004), consulting the details of (Spitkovsky et al,
2010a). Crucially, we swapped out inside-outside
re-estimation in favor of Viterbi training. Not only
is it better-suited to the general problem (see ?7.1),
but it also admits a trivial implementation of (most
of) the dependency constraints we proposed.14
5 10 15 20 25 30 35 40 45
4.5
5.0
5.5
WSJk
bpt
lowest cross-entropy (4.32bpt) attained at WSJ8
x-Entropy h (in bits per token) on WSJ15
Figure 1: Sentence-level cross-entropy on WSJ15
for Ad-Hoc? initializers of WSJ{1, . . . , 45}.
Six settings parameterized each run:
? INIT: 0? default, uniform initialization; or
1 ? a high quality initializer, pre-trained using
Ad-Hoc? (Spitkovsky et al, 2010a): we chose the
Laplace-smoothed model trained at WSJ15 (the
?sweet spot? data gradation) but initialized off
WSJ8, since that ad-hoc harmonic initializer has
the best cross-entropy on WSJ15 (see Figure 1).
? GENRE: 0? default, baseline training on WSJ;
else, uses 1? BLOGt; 2? NEWS; or 3? WEB.
? SCOPE: 0 ? default, uses all sentences up to
length 45; if 1, trains using sentences up to length
15; if 2, re-trains on sentences up to length 45,
starting from the solution to sentences up to length
15, as recommended by Spitkovsky et al (2010a).
? CONSTR: if 4, strict; if 3, loose; and if 2,
sprawl. We did not implement level 1, tear. Over-
constrained sentences are re-attempted at succes-
sively lower levels until they become possible to
parse, if necessary at the lowest (default) level 0.15
? TRIM: if 1, discards any sentence without a sin-
gle multi-token mark-up (shorter than its length).
? ADAPT: if 1, upon convergence, initializes re-
training on WSJ45 using the solution to <GENRE>,
attempting domain adaptation (Lee et al, 1991).
These make for 294 meaningful combinations. We
judged each one by its accuracy on WSJ45, using
standard directed scoring ? the fraction of correct
dependencies over randomized ?best? parse trees.
14We analyze the benefits of Viterbi training in a compan-
ion paper (Spitkovsky et al, 2010b), which dedicates more
space to implementation and to the WSJ baselines used here.
15At level 4, <b> X<u> Y</b> Z</u> is over-constrained.
7 Discussion of Experimental Results
Evaluation on Section 23 of WSJ and Brown re-
veals that blog-training beats all published state-
of-the-art numbers in every traditionally-reported
length cutoff category, with news-training not far
behind. Here is a mini-preview of these results, for
Section 23 of WSJ10 and WSJ? (from Table 8):
WSJ10 WSJ?
(Cohen and Smith, 2009) 62.0 42.2
(Spitkovsky et al, 2010a) 57.1 45.0
NEWS-best 67.3 50.1
BLOGt-best 69.3 50.4
(Headden et al, 2009) 68.8
Table 6: Directed accuracies on Section 23 of
WSJ{10,? } for three recent state-of-the-art sys-
tems and our best runs (as judged against WSJ45)
for NEWS and BLOGt (more details in Table 8).
Since our experimental setup involved testing
nearly three hundred models simultaneously, we
must take extreme care in analyzing and interpret-
ing these results, to avoid falling prey to any loom-
ing ?data-snooping? biases.16 In a sufficiently
large pool of models, where each is trained using
a randomized and/or chaotic procedure (such as
ours), the best may look good due to pure chance.
We appealed to three separate diagnostics to con-
vince ourselves that our best results are not noise.
The most radical approach would be to write off
WSJ as a development set and to focus only on the
results from the held-out Brown corpus. It was ini-
tially intended as a test of out-of-domain general-
ization, but since Brown was in no way involved
in selecting the best models, it also qualifies as
a blind evaluation set. We observe that our best
models perform even better (and gain more ? see
Table 8) on Brown than on WSJ ? a strong indi-
cation that our selection process has not overfitted.
Our second diagnostic is a closer look at WSJ.
Since we cannot graph the full (six-dimensional)
set of results, we begin with a simple linear re-
gression, using accuracy on WSJ45 as the depen-
dent variable. We prefer this full factorial design
to the more traditional ablation studies because it
allows us to account for and to incorporate every
single experimental data point incurred along the
16In the standard statistical hypothesis testing setting, it
is reasonable to expect that p% of randomly chosen hy-
potheses will appear significant at the p% level simply by
chance. Consequently, multiple hypothesis testing requires
re-evaluating significance levels ? adjusting raw p-values,
e.g., using the Holm-Bonferroni method (Holm, 1979).
1283
Corpus Marked Sentences All Sentences POS Tokens All Bracketings Multi-Token Bracketings
BLOGt45 5,641 56,191 1,048,404 7,021 5,346
BLOG?t45 4,516 4,516 104,267 5,771 5,346
BLOGt15 1,562 23,214 212,872 1,714 1,240
BLOG?t15 1,171 1,171 11,954 1,288 1,240
NEWS45 304,129,910 2,263,563,078 32,119,123,561 611,644,606 477,362,150
NEWS?45 205,671,761 205,671,761 2,740,258,972 453,781,081 392,600,070
NEWS15 211,659,549 1,433,779,438 11,786,164,503 365,145,549 274,791,675
NEWS?15 147,848,358 147,848,358 1,397,562,474 272,223,918 231,029,921
WEB45 1,577,208,680 8,903,458,234 87,269,385,640 3,309,897,461 2,459,337,571
WEB?45 933,115,032 933,115,032 11,552,983,379 2,084,359,555 1,793,238,913
WEB15 1,181,696,194 7,488,669,239 55,014,582,024 2,071,743,595 1,494,675,520
WEB?15 681,087,020 681,087,020 5,813,555,341 1,200,980,738 1,072,910,682
Table 7: Counts of sentences, tokens and (unique) bracketings for web-based data sets; trimmed versions,
restricted to only those sentences having at least one multi-token bracketing, are indicated by a prime (?).
way. Its output is a coarse, high-level summary of
our runs, showing which factors significantly con-
tribute to changes in error rate on WSJ45:
Parameter (Indicator) Setting ?? p-value
INIT 1 ad-hoc @WSJ8,15 11.8 ***
GENRE 1 BLOGt -3.7 0.06
2 NEWS -5.3 **
3 WEB -7.7 ***
SCOPE 1 @15 -0.5 0.40
2 @15?45 -0.4 0.53
CONSTR 2 sprawl 0.9 0.23
3 loose 1.0 0.15
4 strict 1.8 *
TRIM 1 drop unmarked -7.4 ***
ADAPT 1 WSJ re-training 1.5 **
Intercept (R2Adjusted = 73.6%) 39.9 ***
We use a standard convention: *** for p < 0.001;
** for p < 0.01 (very signif.); and * for p < 0.05 (signif.).
The default training mode (all parameters zero) is
estimated to score 39.9%. A good initializer gives
the biggest (double-digit) gain; both domain adap-
tation and constraints also make a positive impact.
Throwing away unannotated data hurts, as does
training out-of-domain (the blog is least bad; the
web is worst). Of course, this overview should not
be taken too seriously. Overly simplistic, a first
order model ignores interactions between parame-
ters. Furthermore, a least squares fit aims to cap-
ture central tendencies, whereas we are more in-
terested in outliers ? the best-performing runs.
A major imperfection of the simple regression
model is that helpful factors that require an in-
teraction to ?kick in? may not, on their own, ap-
pear statistically significant. Our third diagnostic
is to examine parameter settings that give rise to
the best-performing models, looking out for com-
binations that consistently deliver superior results.
7.1 WSJ Baselines
Just two parameters apply to learning from WSJ.
Five of their six combinations are state-of-the-art,
demonstrating the power of Viterbi training; only
the default run scores worse than 45.0%, attained
by Leapfrog (Spitkovsky et al, 2010a), on WSJ45:
Settings SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 41.3 45.0 45.2
1 46.6 47.5 47.6
@45 @15 @15?45
7.2 Blog
Simply training on BLOGt instead of WSJ hurts:
GENRE=1 SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 39.6 36.9 36.9
1 46.5 46.3 46.4
@45 @15 @15?45
The best runs use a good initializer, discard unan-
notated sentences, enforce the loose constraint on
the rest, follow up with domain adaptation and
benefit from re-training ? GENRE=TRIM=ADAPT=1:
INIT=1 SCOPE=0 SCOPE=1 SCOPE=2
CONSTR=0 45.8 48.3 49.6
(sprawl) 2 46.3 49.2 49.2
(loose) 3 41.3 50.2 50.4
(strict) 4 40.7 49.9 48.7
@45 @15 @15?45
The contrast between unconstrained learning and
annotation-guided parsing is higher for the default
initializer, still using trimmed data sets (just over a
thousand sentences for BLOG?t15 ? see Table 7):
INIT=0 SCOPE=0 SCOPE=1 SCOPE=2
CONSTR=0 25.6 19.4 19.3
(sprawl) 2 25.2 22.7 22.5
(loose) 3 32.4 26.3 27.3
(strict) 4 36.2 38.7 40.1
@45 @15 @15?45
Above, we see a clearer benefit to our constraints.
1284
7.3 News
Training on WSJ is also better than using NEWS:
GENRE=2 SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 40.2 38.8 38.7
1 43.4 44.0 43.8
@45 @15 @15?45
As with the blog, the best runs use the good initial-
izer, discard unannotated sentences, enforce the
loose constraint and follow up with domain adap-
tation ? GENRE=2; INIT=TRIM=ADAPT=1:
Settings SCOPE=0 SCOPE=1 SCOPE=2
CONSTR=0 46.6 45.4 45.2
(sprawl) 2 46.1 44.9 44.9
(loose) 3 49.5 48.1 48.3
(strict) 4 37.7 36.8 37.6
@45 @15 @15?45
With all the extra training data, the best new score
is just 49.5%. On the one hand, we are disap-
pointed by the lack of dividends to orders of mag-
nitude more data. On the other, we are comforted
that the system arrives within 1% of its best result
? 50.4%, obtained with a manually cleaned up
corpus ? now using an auto-generated data set.
7.4 Web
The WEB-side story is more discouraging:
GENRE=3 SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 38.3 35.1 35.2
1 42.8 43.6 43.4
@45 @15 @15?45
Our best run again uses a good initializer, keeps
all sentences, still enforces the loose constraint
and follows up with domain adaptation, but per-
forms worse than all well-initialized WSJ base-
lines, scoring only 45.9% (trained at WEB15).
We suspect that the web is just too messy for
us. On top of the challenges of language iden-
tification and sentence-breaking, there is a lot of
boiler-plate; furthermore, web text can be difficult
for news-trained POS taggers. For example, note
that the verb ?sign? is twice mistagged as a noun
and that ?YouTube? is classified as a verb, in the
top four POS sequences of web sentences:17
POS Sequence WEB Count
Sample web sentence, chosen uniformly at random.
1 DT NNS VBN 82,858,487
All rights reserved.
2 NNP NNP NNP 65,889,181
Yuasa et al
3 NN IN TO VB RB 31,007,783
Sign in to YouTube now!
4 NN IN IN PRP$ JJ NN 31,007,471
Sign in with your Google Account!
17Further evidence: TnT tags the ubiquitous but ambigu-
ous fragments ?click here? and ?print post? as noun phrases.
7.5 The State of the Art
Our best model gains more than 5% over previ-
ous state-of-the-art accuracy across all sentences
of WSJ?s Section 23, more than 8% on WSJ20 and
rivals the oracle skyline (Spitkovsky et al, 2010a)
on WSJ10; these gains generalize to Brown100,
where it improves by nearly 10% (see Table 8).
We take solace in the fact that our best mod-
els agree in using loose constraints. Of these,
the models trained with less data perform better,
with the best two using trimmed data sets, echo-
ing that ?less is more? (Spitkovsky et al, 2010a),
pace Halevy et al (2009). We note that orders of
magnitude more data did not improve parsing per-
formance further and suspect a different outcome
from lexicalized models: The primary benefit of
additional lower-quality data is in improved cover-
age. But with only 35 unique POS tags, data spar-
sity is hardly an issue. Extra examples of lexical
items help little and hurt when they are mistagged.
8 Related Work
The wealth of new annotations produced in many
languages every day already fuels a number of
NLP applications. Following their early and
wide-spread use by search engines, in service of
spam-fighting and retrieval, anchor text and link
data enhanced a variety of traditional NLP tech-
niques: cross-lingual information retrieval (Nie
and Chen, 2002), translation (Lu et al, 2004), both
named-entity recognition (Mihalcea and Csomai,
2007) and categorization (Watanabe et al, 2007),
query segmentation (Tan and Peng, 2008), plus
semantic relatedness and word-sense disambigua-
tion (Gabrilovich and Markovitch, 2007; Yeh et
al., 2009). Yet several, seemingly natural, can-
didate core NLP tasks ? tokenization, CJK seg-
mentation, noun-phrase chunking, and (until now)
parsing ? remained conspicuously uninvolved.
Approaches related to ours arise in applications
that combine parsing with named-entity recogni-
tion (NER). For example, constraining a parser to
respect the boundaries of known entities is stan-
dard practice not only in joint modeling of (con-
stituent) parsing and NER (Finkel and Manning,
2009), but also in higher-level NLP tasks, such as
relation extraction (Mintz et al, 2009), that couple
chunking with (dependency) parsing. Although
restricted to proper noun phrases, dates, times and
quantities, we suspect that constituents identified
by trained (supervised) NER systems would also
1285
Model Incarnation WSJ10 WSJ20 WSJ?
DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100
Leapfrog (Spitkovsky et al, 2010a) 57.1 48.7 45.0 43.6
default INIT=0,GENRE=0,SCOPE=0,CONSTR=0,TRIM=0,ADAPT=0 55.9 45.8 41.6 40.5
WSJ-best INIT=1,GENRE=0,SCOPE=2,CONSTR=0,TRIM=0,ADAPT=0 65.3 53.8 47.9 50.8
BLOGt-best INIT=1,GENRE=1,SCOPE=2,CONSTR=3,TRIM=1,ADAPT=1 69.3 56.8 50.4 53.3
NEWS-best INIT=1,GENRE=2,SCOPE=0,CONSTR=3,TRIM=1,ADAPT=1 67.3 56.2 50.1 51.6
WEB-best INIT=1,GENRE=3,SCOPE=1,CONSTR=3,TRIM=0,ADAPT=1 64.1 52.7 46.3 46.9
EVG Smoothed (skip-head), Lexicalized (Headden et al, 2009) 68.8
Table 8: Accuracies on Section 23 of WSJ{10, 20,? } and Brown100 for three recent state-of-the-art
systems, our default run, and our best runs (judged by accuracy on WSJ45) for each of four training sets.
be helpful in constraining grammar induction.
Following Pereira and Schabes? (1992) success
with partial annotations in training a model of
(English) constituents generatively, their idea has
been extended to discriminative estimation (Rie-
zler et al, 2002) and also proved useful in mod-
eling (Japanese) dependencies (Sassano, 2005).
There was demand for partially bracketed corpora.
Chen and Lee (1995) constructed one such corpus
by learning to partition (English) POS sequences
into chunks (Abney, 1991); Inui and Kotani (2001)
used n-gram statistics to split (Japanese) clauses.
We combine the two intuitions, using the web
to build a partially parsed corpus. Our approach
could be called lightly-supervised, since it does
not require manual annotation of a single complete
parse tree. In contrast, traditional semi-supervised
methods rely on fully-annotated seed corpora.18
9 Conclusion
We explored novel ways of training dependency
parsing models, the best of which attains 50.4%
accuracy on Section 23 (all sentences) of WSJ,
beating all previous unsupervised state-of-the-art
by more than 5%. Extra gains stem from guid-
ing Viterbi training with web mark-up, the loose
constraint consistently delivering best results. Our
linguistic analysis of a blog reveals that web an-
notations can be converted into accurate parsing
constraints (loose: 88%; sprawl: 95%; tear: 99%)
that could be helpful to supervised methods, e.g.,
by boosting an initial parser via self-training (Mc-
Closky et al, 2006) on sentences with mark-up.
Similar techniques may apply to standard word-
processing annotations, such as font changes, and
to certain (balanced) punctuation (Briscoe, 1994).
We make our blog data set, overlaying mark-up
and syntax, publicly available. Its annotations are
18A significant effort expended in building a tree-bank
comes with the first batch of sentences (Druck et al, 2009).
75% noun phrases, 13% verb phrases, 7% simple
declarative clauses and 2% prepositional phrases,
with traces of other phrases, clauses and frag-
ments. The type of mark-up, combined with POS
tags, could make for valuable features in discrimi-
native models of parsing (Ratnaparkhi, 1999).
A logical next step would be to explore the con-
nection between syntax and mark-up for genres
other than a news-style blog and for languages
other than English. We are excited by the possi-
bilities, as unsupervised parsers are on the cusp
of becoming useful in their own right ? re-
cently, Davidov et al (2009) successfully applied
Seginer?s (2007) fully unsupervised grammar in-
ducer to the problems of pattern-acquisition and
extraction of semantic data. If the strength of the
connection between web mark-up and syntactic
structure is universal across languages and genres,
this fact could have broad implications for NLP,
with applications extending well beyond parsing.
Acknowledgments
Partially funded by NSF award IIS-0811974 and by the Air
Force Research Laboratory (AFRL), under prime contract
no. FA8750-09-C-0181; first author supported by the Fannie
& John Hertz Foundation Fellowship. We thank Angel X.
Chang, Spence Green, Christopher D. Manning, Richard
Socher, Mihai Surdeanu and the anonymous reviewers for
many helpful suggestions, and we are especially grateful to
Andy Golding, for pointing us to his sample Map-Reduce
over the Google News crawl, and to Daniel Pipes, for allow-
ing us to distribute the data set derived from his blog entries.
References
S. Abney. 1991. Parsing by chunks. Principle-Based Pars-
ing: Computation and Psycholinguistics.
J. K. Baker. 1979. Trainable grammars for speech recogni-
tion. In Speech Communication Papers for the 97th Meet-
ing of the Acoustical Society of America.
C. Barr, R. Jones, and M. Regelson. 2008. The linguistic
structure of English web-search queries. In EMNLP.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP.
1286
T. Briscoe. 1994. Parsing (with) punctuation, etc. Technical
report, Xerox European Research Laboratory.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In ACL.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL.
H.-H. Chen and Y.-S. Lee. 1995. Development of a partially
bracketed corpus with part-of-speech information only. In
WVLC.
S. B. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsupervised
grammar induction. In NAACL-HLT.
M. Collins. 1999. Head-Driven Statistical Models for Nat-
ural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
D. Davidov, R. Reichart, and A. Rappoport. 2009. Supe-
rior and efficient fully unsupervised pattern-based concept
acquisition using an unsupervised parser. In CoNLL.
G. Druck, G. Mann, and A. McCallum. 2009. Semi-
supervised learning of dependency parsers using general-
ized expectation criteria. In ACL-IJCNLP.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head-automaton grammars. In
ACL.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL-HLT.
W. N. Francis and H. Kucera, 1979. Manual of Information
to Accompany a Standard Corpus of Present-Day Edited
American English, for use with Digital Computers. De-
partment of Linguistic, Brown University.
E. Gabrilovich and S. Markovitch. 2007. Computing seman-
tic relatedness using Wikipedia-based Explicit Semantic
Analysis. In IJCAI.
D. Gildea. 2001. Corpus variation and parser performance.
In EMNLP.
A. Halevy, P. Norvig, and F. Pereira. 2009. The unreasonable
effectiveness of data. IEEE Intelligent Systems, 24.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with richer
contexts and smoothing. In NAACL-HLT.
S. Holm. 1979. A simple sequentially rejective multiple test
procedure. Scandinavian Journal of Statistics, 6.
N. Inui and Y. Kotani. 2001. Robust N -gram based syntactic
analysis using segmentation words. In PACLIC.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In ACL.
C.-H. Lee, C.-H. Lin, and B.-H. Juang. 1991. A study on
speaker adaptation of the parameters of continuous den-
sity Hidden Markov Models. IEEE Trans. on Signal Pro-
cessing, 39.
W.-H. Lu, L.-F. Chien, and H.-J. Lee. 2004. Anchor text
mining for translation of Web queries: A transitive trans-
lation approach. ACM Trans. on Information Systems, 22.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In NAACL-HLT.
R. Mihalcea and A. Csomai. 2007. Wikify!: Linking docu-
ments to encyclopedic knowledge. In CIKM.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Distant
supervision for relation extraction without labeled data. In
ACL-IJCNLP.
J.-Y. Nie and J. Chen. 2002. Exploiting the Web as paral-
lel corpora for cross-language information retrieval. Web
Intelligence.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
A. Ratnaparkhi. 1999. Learning to parse natural language
with maximum entropy models. Machine Learning, 34.
S. Ravi, K. Knight, and R. Soricut. 2008. Automatic predic-
tion of parser accuracy. In EMNLP.
J. C. Reynar and A. Ratnaparkhi. 1997. A maximum entropy
approach to identifying sentence boundaries. In ANLP.
S. Riezler, T. H. King, R. M. Kaplan, R. Crouch, J. T.
Maxwell, III, and M. Johnson. 2002. Parsing the Wall
Street Journal using a lexical-functional grammar and dis-
criminative estimation techniques. In ACL.
M. Sassano. 2005. Using a partially annotated corpus to
build a dependency parser for Japanese. In IJCNLP.
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a. From
Baby Steps to Leapfrog: How ?Less is More? in unsuper-
vised dependency parsing. In NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010b. Viterbi training improves unsupervised de-
pendency parsing. In CoNLL.
B. Tan and F. Peng. 2008. Unsupervised query segmenta-
tion using generative language models and Wikipedia. In
WWW.
K. Toutanova and C. D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-of-
speech tagger. In EMNLP-VLC.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic depen-
dency network. In HLT-NAACL.
D. Vadas and J. R. Curran. 2007. Adding noun phrase struc-
ture to the Penn Treebank. In ACL.
Y. Watanabe, M. Asahara, and Y. Matsumoto. 2007. A
graph-based approach to named entity categorization in
Wikipedia using conditional random fields. In EMNLP-
CoNLL.
E. Yeh, D. Ramage, C. D. Manning, E. Agirre, and A. Soroa.
2009. WikiWalk: Random walks on Wikipedia for se-
mantic relatedness. In TextGraphs.
1287
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 976?986,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Template-Based Information Extraction without the Templates
Nathanael Chambers and Dan Jurafsky
Department of Computer Science
Stanford University
{natec,jurafsky}@stanford.edu
Abstract
Standard algorithms for template-based in-
formation extraction (IE) require predefined
template schemas, and often labeled data,
to learn to extract their slot fillers (e.g., an
embassy is the Target of a Bombing tem-
plate). This paper describes an approach to
template-based IE that removes this require-
ment and performs extraction without know-
ing the template structure in advance. Our al-
gorithm instead learns the template structure
automatically from raw text, inducing tem-
plate schemas as sets of linked events (e.g.,
bombings include detonate, set off, and de-
stroy events) associated with semantic roles.
We also solve the standard IE task, using the
induced syntactic patterns to extract role fillers
from specific documents. We evaluate on the
MUC-4 terrorism dataset and show that we in-
duce template structure very similar to hand-
created gold structure, and we extract role
fillers with an F1 score of .40, approaching
the performance of algorithms that require full
knowledge of the templates.
1 Introduction
A template defines a specific type of event (e.g.,
a bombing) with a set of semantic roles (or slots)
for the typical entities involved in such an event
(e.g., perpetrator, target, instrument). In contrast to
work in relation discovery that focuses on learning
atomic facts (Banko et al, 2007a; Carlson et al,
2010), templates can extract a richer representation
of a particular domain. However, unlike relation dis-
covery, most template-based IE approaches assume
foreknowledge of the domain?s templates. Very little
work addresses how to learn the template structure
itself. Our goal in this paper is to perform the stan-
dard template filling task, but to first automatically
induce the templates from an unlabeled corpus.
There are many ways to represent events, rang-
ing from role-based representations such as frames
(Baker et al, 1998) to sequential events in scripts
(Schank and Abelson, 1977) and narrative schemas
(Chambers and Jurafsky, 2009; Kasch and Oates,
2010). Our approach learns narrative-like knowl-
edge in the form of IE templates; we learn sets of
related events and semantic roles, as shown in this
sample output from our system:
Bombing Template
{detonate, blow up, plant, explode, defuse, destroy}
Perpetrator: Person who detonates, plants, blows up
Instrument: Object that is planted, detonated, defused
Target: Object that is destroyed, is blown up
A semantic role, such as target, is a cluster of syn-
tactic functions of the template?s event words (e.g.,
the objects of detonate and explode). Our goal is
to characterize a domain by learning this template
structure completely automatically. We learn tem-
plates by first clustering event words based on their
proximity in a training corpus. We then use a novel
approach to role induction that clusters the syntactic
functions of these events based on selectional prefer-
ences and coreferring arguments. The induced roles
are template-specific (e.g., perpetrator), not univer-
sal (e.g., agent or patient) or verb-specific.
After learning a domain?s template schemas, we
perform the standard IE task of role filling from in-
dividual documents, for example:
Perpetrator: guerrillas
Instrument: dynamite
Target: embassy
976
This extraction stage identifies entities using the
learned syntactic functions of our roles. We evalu-
ate on the MUC-4 terrorism corpus with results ap-
proaching those of supervised systems.
The core of this paper focuses on how to char-
acterize a domain-specific corpus by learning rich
template structure. We describe how to first expand
the small corpus? size, how to cluster its events, and
finally how to induce semantic roles. Section 5 then
describes the extraction algorithm, followed by eval-
uations against previous work in section 6 and 7.
2 Previous Work
Many template extraction algorithms require full
knowledge of the templates and labeled corpora,
such as in rule-based systems (Chinchor et al, 1993;
Rau et al, 1992) and modern supervised classi-
fiers (Freitag, 1998; Chieu et al, 2003; Bunescu
and Mooney, 2004; Patwardhan and Riloff, 2009).
Classifiers rely on the labeled examples? surround-
ing context for features such as nearby tokens, doc-
ument position, syntax, named entities, semantic
classes, and discourse relations (Maslennikov and
Chua, 2007). Ji and Grishman (2008) also supple-
mented labeled with unlabeled data.
Weakly supervised approaches remove some of
the need for fully labeled data. Most still require the
templates and their slots. One common approach is
to begin with unlabeled, but clustered event-specific
documents, and extract common word patterns as
extractors (Riloff and Schmelzenbach, 1998; Sudo
et al, 2003; Riloff et al, 2005; Patwardhan and
Riloff, 2007). Filatova et al (2006) integrate named
entities into pattern learning (PERSON won) to ap-
proximate unknown semantic roles. Bootstrapping
with seed examples of known slot fillers has been
shown to be effective (Surdeanu et al, 2006; Yan-
garber et al, 2000). In contrast, this paper removes
these data assumptions, learning instead from a cor-
pus of unknown events and unclustered documents,
without seed examples.
Shinyama and Sekine (2006) describe an ap-
proach to template learning without labeled data.
They present unrestricted relation discovery as a
means of discovering relations in unlabeled docu-
ments, and extract their fillers. Central to the al-
gorithm is collecting multiple documents describ-
ing the same exact event (e.g. Hurricane Ivan), and
observing repeated word patterns across documents
connecting the same proper nouns. Learned patterns
represent binary relations, and they show how to
construct tables of extracted entities for these rela-
tions. Our approach draws on this idea of using un-
labeled documents to discover relations in text, and
of defining semantic roles by sets of entities. How-
ever, the limitations to their approach are that (1)
redundant documents about specific events are re-
quired, (2) relations are binary, and (3) only slots
with named entities are learned. We will extend
their work by showing how to learn without these
assumptions, obviating the need for redundant doc-
uments, and learning templates with any type and
any number of slots.
Large-scale learning of scripts and narrative
schemas also captures template-like knowledge
from unlabeled text (Chambers and Jurafsky, 2008;
Kasch and Oates, 2010). Scripts are sets of re-
lated event words and semantic roles learned by
linking syntactic functions with coreferring argu-
ments. While they learn interesting event structure,
the structures are limited to frequent topics in a large
corpus. We borrow ideas from this work as well, but
our goal is to instead characterize a specific domain
with limited data. Further, we are the first to apply
this knowledge to the IE task of filling in template
mentions in documents.
In summary, our work extends previous work on
unsupervised IE in a number of ways. We are the
first to learn MUC-4 templates, and we are the first
to extract entities without knowing how many tem-
plates exist, without examples of slot fillers, and
without event-clustered documents.
3 The Domain and its Templates
Our goal is to learn the general event structure of
a domain, and then extract the instances of each
learned event. In order to measure performance
in both tasks (learning structure and extracting in-
stances), we use the terrorism corpus of MUC-4
(Sundheim, 1991) as our target domain. This cor-
pus was chosen because it is annotated with tem-
plates that describe all of the entities involved in
each event. An example snippet from a bombing
document is given here:
977
The terrorists used explosives against the
town hall. El Comercio reported that alleged
Shining Path members also attacked public fa-
cilities in huarpacha, Ambo, tomayquichua,
and kichki. Municipal official Sergio Horna
was seriously wounded in an explosion in
Ambo.
The entities from this document fill the following
slots in a MUC-4 bombing template.
Perp: Shining Path members Victim: Sergio Horna
Target: public facilities Instrument: explosives
We focus on these four string-based slots1 from
the MUC-4 corpus, as is standard in this task. The
corpus consists of 1300 documents, 733 of which
are labeled with at least one template. There are six
types of templates, but only four are modestly fre-
quent: bombing (208 docs), kidnap (83 docs), attack
(479 docs), and arson (40 docs). 567 documents do
not have any templates. Our learning algorithm does
not know which documents contain (or do not con-
tain) which templates. After learning event words
that represent templates, we induce their slots, not
knowing a priori how many there are, and then fill
them in by extracting entities as in the standard task.
In our example above, the three bold verbs (use, at-
tack, wound) indicate the Bombing template, and
their syntactic arguments fill its slots.
4 Learning Templates from Raw Text
Our goal is to learn templates that characterize a
domain as described in unclustered, unlabeled doc-
uments. This presents a two-fold problem to the
learner: it does not know how many events exist, and
it does not know which documents describe which
event (some may describe multiple events). We ap-
proach this problem with a three step process: (1)
cluster the domain?s event patterns to approximate
the template topics, (2) build a new corpus specific to
each cluster by retrieving documents from a larger
unrelated corpus, (3) induce each template?s slots
using its new (larger) corpus of documents.
4.1 Clustering Events to Learn Templates
We cluster event patterns to create templates. An
event pattern is either (1) a verb, (2) a noun in Word-
1There are two Perpetrator slots in MUC-4: Organization
and Individual. We consider their union as a single slot.
Net under the Event synset, or (3) a verb and the
head word of its syntactic object. Examples of each
include (1) ?explode?, (2) ?explosion?, and (3) ?ex-
plode:bomb?. We also tag the corpus with an NER
system and allow patterns to include named entity
types, e.g., ?kidnap:PERSON?. These patterns are
crucially needed later to learn a template?s slots.
However, we first need an algorithm to cluster these
patterns to learn the domain?s core events. We con-
sider two unsupervised algorithms: Latent Dirichlet
Allocation (LDA) (Blei et al, 2003), and agglomer-
ative clustering based on word distance.
4.1.1 LDA for Unknown Data
LDA is a probabilistic model that treats documents
as mixtures of topics. It learns topics as discrete
distributions (multinomials) over the event patterns,
and thus meets our needs as it clusters patterns based
on co-occurrence in documents. The algorithm re-
quires the number of topics to be known ahead of
time, but in practice this number is set relatively high
and the resulting topics are still useful. Our best per-
forming LDA model used 200 topics. We had mixed
success with LDA though, and ultimately found our
next approach performed slightly better on the doc-
ument classification evaluation.
4.1.2 Clustering on Event Distance
Agglomerative clustering does not require fore-
knowledge of the templates, but its success relies on
how event pattern similarity is determined.
Ideally, we want to learn that detonate and destroy
belong in the same cluster representing a bombing.
Vector-based approaches are often adopted to rep-
resent words as feature vectors and compute their
distance with cosine similarity. Unfortunately, these
approaches typically learn clusters of synonymous
words that can miss detonate and destroy. Our
goal is to instead capture world knowledge of co-
occuring events. We thus adopt an assumption that
closeness in the world is reflected by closeness in a
text?s discourse. We hypothesize that two patterns
are related if they occur near each other in a docu-
ment more often than chance.
Let g(wi, wj) be the distance between two events
(1 if in the same sentence, 2 in neighboring, etc). Let
Cdist(wi, wj) be the distance-weighted frequency of
978
kidnap: kidnap, kidnap:PER, abduct, release, kidnap-
ping, ransom, robbery, registration
bombing: explode, blow up, locate, place:bomb, det-
onate, damage, explosion, cause, damage, ...
attack: kill, shoot down, down, kill:civilian, kill:PER,
kill:soldier, kill:member, killing, shoot:PER, wave, ...
arson: burn, search, burning, clip, collaborate, ...
Figure 1: The 4 clusters mapped to MUC-4 templates.
two events occurring together:
Cdist(wi, wj) =
?
d?D
?
wi,wj?d
1? log4(g(wi, wj)) (1)
where d is a document in the set of all documents
D. The base 4 logarithm discounts neighboring sen-
tences by 0.5 and within the same sentence scores 1.
Using this definition of distance, pointwise mutual
information measures our similarity of two events:
pmi(wi, wj) = Pdist(wi, wj)/(P (wi)P (wj)) (2)
P (wi) =
C(wi)
?
j C(wj)
(3)
Pdist(wi, wj) =
Cdist(wi, wj)
?
k
?
l Cdist(wk, wl)
(4)
We run agglomerative clustering with pmi over
all event patterns. Merging decisions use the average
link score between all new links across two clusters.
As with all clustering algorithms, a stopping crite-
rion is needed. We continue merging clusters un-
til any single cluster grows beyond m patterns. We
briefly inspected the clustering process and chose
m = 40 to prevent learned scenarios from intuitively
growing too large and ambiguous. Post-evaluation
analysis shows that this value has wide flexibility.
For example, the Kidnap and Arson clusters are un-
changed in 30 < m < 80, and Bombing unchanged
in 30 < m < 50. Figure 1 shows 3 clusters (of 77
learned) that characterize the main template types.
4.2 Information Retrieval for Templates
Learning a domain often suffers from a lack of train-
ing data. The previous section clustered events from
the MUC-4 corpus, but its 1300 documents do not
provide enough examples of verbs and argument
counts to further learn the semantic roles in each
cluster. Our solution is to assemble a larger IR-
corpus of documents for each cluster. For exam-
ple, MUC-4 labels 83 documents with Kidnap, but
our learned cluster (kidnap, abduct, release, ...) re-
trieved 3954 documents from a general corpus.
We use the Associated Press and New York Times
sections of the Gigaword Corpus (Graff, 2002) as
our general corpus. These sections include approxi-
mately 3.5 million news articles spanning 12 years.
Our retrieval algorithm retrieves documents that
score highly with a cluster?s tokens. The docu-
ment score is defined by two common metrics: word
match, and word coverage. A document?s match
score is defined as the average number of times the
words in cluster c appear in document d:
avgm(d, c) =
?
w?c
?
t?d 1{w = t}
|c|
(5)
We define word coverage as the number of seen
cluster words. Coverage penalizes documents that
score highly by repeating a single cluster word a lot.
We only score a document if its coverage, cvg(d, c),
is at least 3 words (or less for tiny clusters):
ir(d, c) =
{
avgm(d, c) if cvg(d, c) > min(3, |c|/4)
0 otherwise
A document d is retrieved for a cluster c if
ir(d, c) > 0.4. Finally, we emphasize precision
by pruning away 50% of a cluster?s retrieved doc-
uments that are farthest in distance from the mean
document of the retrieved set. Distance is the co-
sine similarity between bag-of-words vector repre-
sentations. The confidence value of 0.4 was chosen
from a manual inspection among a single cluster?s
retrieved documents. Pruning 50% was arbitrarily
chosen to improve precision, and we did not exper-
iment with other quantities. A search for optimum
parameter values may lead to better results.
4.3 Inducing Semantic Roles (Slots)
Having successfully clustered event words and re-
trieved an IR-corpus for each cluster, we now ad-
dress the problem of inducing semantic roles. Our
learned roles will then extract entities in the next sec-
tion and we will evaluate their per-role accuracy.
Most work on unsupervised role induction fo-
cuses on learning verb-specific roles, starting with
seed examples (Swier and Stevenson, 2004; He and
979
Gildea, 2006) and/or knowing the number of roles
(Grenager and Manning, 2006; Lang and Lapata,
2010). Our previous work (Chambers and Juraf-
sky, 2009) learned situation-specific roles over nar-
rative schemas, similar to frame roles in FrameNet
(Baker et al, 1998). Schemas link the syntactic rela-
tions of verbs by clustering them based on observing
coreferring arguments in those positions. This paper
extends this intuition by introducing a new vector-
based approach to coreference similarity.
4.3.1 Syntactic Relations as Roles
We learn the roles of cluster C by clustering the syn-
tactic relations RC of its words. Consider the fol-
lowing example:
C = {go off, explode, set off, damage, destroy}
RC = {go off:s, go off:p in, explode:s, set off:s}
where verb:s is the verb?s subject, :o the object, and
p in a preposition. We ideally want to cluster RC as:
bomb = {go off:s, explode:s, set off:o, destroy:s}
suspect = {set off:s}
target = {go off:p in, destroy:o}
We want to cluster all subjects, objects, and
prepositions. Passive voice is normalized to active2.
We adopt two views of relation similarity:
coreferring arguments and selectional preferences.
Chambers and Jurafsky (2008) observed that core-
ferring arguments suggest a semantic relation be-
tween two predicates. In the sentence, he ran and
then he fell, the subjects of run and fall corefer, and
so they likely belong to the same scenario-specific
semantic role. We applied this idea to a new vec-
tor similarity framework. We represent a relation
as a vector of all relations with which their argu-
ments coreferred. For instance, arguments of the
relation go off:s were seen coreferring with men-
tions in plant:o, set off:o and injure:s. We represent
go off:s as a vector of these relation counts, calling
this its coref vector representation.
Selectional preferences (SPs) are also useful in
measuring similarity (Erk and Pado, 2008). A re-
lation can be represented as a vector of its observed
arguments during training. The SPs for go off:s in
our data include {bomb, device, charge, explosion}.
We measure similarity using cosine similarity be-
tween the vectors in both approaches. However,
2We use the Stanford Parser at nlp.stanford.edu/software
coreference and SPs measure different types of sim-
ilarity. Coreference is a looser narrative similarity
(bombings cause injuries), while SPs capture syn-
onymy (plant and place have similar arguments). We
observed that many narrative relations are not syn-
onymous, and vice versa. We thus take the max-
imum of either cosine score as our final similarity
metric between two relations. We then back off to
the average of the two cosine scores if the max is not
confident (less than 0.7); the average penalizes the
pair. We chose the value of 0.7 from a grid search to
optimize extraction results on the training set.
4.3.2 Clustering Syntactic Functions
We use agglomerative clustering with the above
pairwise similarity metric. Cluster similarity is the
average link score over all new links crossing two
clusters. We include the following sparsity penalty
r(ca, cb) if there are too few links between clusters
ca and cb.
score(ca, cb) =
?
wi?ca
?
wj?cb
sim(wi, wj)?r(ca, cb) (6)
r(ca, cb) =
?
wi?ca
?
wj?cb
1{sim(wi, wj) > 0}
?
wi?ca
?
wj?cb
1
(7)
This penalizes clusters from merging when they
share only a few high scoring edges. Clustering
stops when the merged cluster scores drop below
a threshold optimized to extraction performance on
the training data.
We also begin with two assumptions about syntac-
tic functions and semantic roles. The first assumes
that the subject and object of a verb carry different
semantic roles. For instance, the subject of sell fills
a different role (Seller) than the object (Good). The
second assumption is that each semantic role has a
high-level entity type. For instance, the subject of
sell is a Person or Organization, and the object is a
Physical Object.
We implement the first assumption as a constraint
in the clustering algorithm, preventing two clusters
from merging if their union contains the same verb?s
subject and object.
We implement the second assumption by auto-
matically labeling each syntactic function with a role
type based on its observed arguments. The role types
are broad general classes: Person/Org, Physical Ob-
ject, or Other. A syntactic function is labeled as a
980
Bombing Template (MUC-4)
Perpetrator Person/Org who detonates, blows up, plants,
hurls, stages, is detained, is suspected, is blamed on,
launches
Instrument A physical object that is exploded, explodes, is
hurled, causes, goes off, is planted, damages, is set off, is
defused
Target A physical object that is damaged, is destroyed, is
exploded at, is damaged, is thrown at, is hit, is struck
Police Person/Org who raids, questions, discovers, investi-
gates, defuses, arrests
N/A A physical object that is blown up, destroys
Attack/Shooting Template (MUC-4)
Perpetrator Person/Org who assassinates, patrols, am-
bushes, raids, shoots, is linked to
Victim Person/Org who is assassinated, is toppled, is gunned
down, is executed, is evacuated
Target Person/Org who is hit, is struck, is downed, is set fire
to, is blown up, surrounded
Instrument A physical object that is fired, injures, downs, is
set off, is exploded
Kidnap Template (MUC-4)
Perpetrator Person/Org who releases, abducts, kidnaps,
ambushes, holds, forces, captures, is imprisoned, frees
Target Person/Org who is kidnapped, is released, is freed,
escapes, disappears, travels, is harmed, is threatened
Police Person/Org who rules out, negotiates, condemns, is
pressured, finds, arrests, combs
Weapons Smuggling Template (NEW)
Perpetrator Person/Org who smuggles, is seized from, is
captured, is detained
Police Person/Org who raids, seizes, captures, confiscates,
detains, investigates
Instrument A physical object that is smuggled, is seized, is
confiscated, is transported
Election Template (NEW)
Voter Person/Org who chooses, is intimidated, favors, is ap-
pealed to, turns out
Government Person/Org who authorizes, is chosen, blames,
authorizes, denies
Candidate Person/Org who resigns, unites, advocates, ma-
nipulates, pledges, is blamed
Figure 2: Five learned example templates. All knowledge except the template/role names (e.g., ?Victim?) is learned.
class if 20% of its arguments appear under the cor-
responding WordNet synset3, or if the NER system
labels them as such. Once labeled by type, we sep-
arately cluster the syntactic functions for each role
type. For instance, Person functions are clustered
separate from Physical Object functions. Figure 2
shows some of the resulting roles.
Finally, since agglomerative clustering makes
hard decisions, related events to a template may have
been excluded in the initial event clustering stage.
To address this problem, we identify the 200 nearby
events to each event cluster. These are simply the
top scoring event patterns with the cluster?s original
events. We add their syntactic functions to their best
matching roles. This expands the coverage of each
learned role. Varying the 200 amount does not lead
to wide variation in extraction performance. Once
induced, the roles are evaluated by their entity ex-
traction performance in Section 5.
4.4 Template Evaluation
We now compare our learned templates to those
hand-created by human annotators for the MUC-4
terrorism corpus. The corpus contains 6 template
3Physical objects are defined as non-person physical objects
Bombing Kidnap Attack Arson
Perpetrator x x x x
Victim x x x x
Target x x x
Instrument x x
Figure 3: Slots in the hand-crafted MUC-4 templates.
types, but two of them occur in only 4 and 14 of the
1300 training documents. We thus only evaluate the
4 main templates (bombing, kidnapping, attack, and
arson). The gold slots are shown in figure 3.
We evaluate the four learned templates that score
highest in the document classification evaluation
(to be described in section 5.1), aligned with their
MUC-4 types. Figure 2 shows three of our four tem-
plates, and two brand new ones that our algorithm
learned. Of the four templates, we learned 12 of the
13 semantic roles as created for MUC. In addition,
we learned a new role not in MUC for bombings,
kidnappings, and arson: the Police or Authorities
role. The annotators chose not to include this in their
labeling, but this knowledge is clearly relevant when
understanding such events, so we consider it correct.
There is one additional Bombing and one Arson role
that does not align with MUC-4, marked incorrect.
981
We thus report 92% slot recall, and precision as 14
of 16 (88%) learned slots.
We only measure agreement with the MUC tem-
plate schemas, but our system learns other events as
well. We show two such examples in figure 2: the
Weapons Smuggling and Election Templates.
5 Information Extraction: Slot Filling
We now present how to apply our learned templates
to information extraction. This section will describe
how to extract slot fillers using our templates, but
without knowing which templates are correct.
We could simply use a standard IE approach, for
example, creating seed words for our new learned
templates. But instead, we propose a new method
that obviates the need for even a limited human la-
beling of seed sets. We consider each learned se-
mantic role as a potential slot, and we extract slot
fillers using the syntactic functions that were previ-
ously learned. Thus, the learned syntactic patterns
(e.g., the subject of release) serve the dual purpose
of both inducing the template slots, and extracting
appropriate slot fillers from text.
5.1 Document Classification
A document is labeled for a template if two different
conditions are met: (1) it contains at least one trig-
ger phrase, and (2) its average per-token conditional
probability meets a strict threshold.
Both conditions require a definition of the condi-
tional probability of a template given a token. The
conditional is defined as the token?s importance rel-
ative to its uniqueness across all templates. This
is not the usual conditional probability definition as
IR-corpora are different sizes.
P (t|w) =
PIRt(w)?
s?T PIRs(w)
(8)
where PIRt(w) is the probability of pattern w in the
IR-corpus of template t.
PIRt(w) =
Ct(w)
?
v Ct(v)
(9)
where Ct(w) is the number of times word w appears
in the IR-corpus of template t. A template?s trigger
words are defined as words satisfying P (t|w) > 0.2.
Kidnap Bomb Attack Arson
Precision .64 .83 .66 .30
Recall .54 .63 .35 1.0
F1 .58 .72 .46 .46
Figure 4: Document classification results on test.
Trigger phrases are thus template-specific patterns
that are highly indicative of that template.
After identifying triggers, we use the above defi-
nition to score a document with a template. A doc-
ument is labeled with a template if it contains at
least one trigger, and its average word probability
is greater than a parameter optimized on the training
set. A document can be (and often is) labeled with
multiple templates.
Finally, we label the sentences that contain trig-
gers and use them for extraction in section 5.2.
5.1.1 Experiment: Document Classification
The MUC-4 corpus links templates to documents,
allowing us to evaluate our document labels. We
treat each link as a gold label (kidnap, bomb, or
attack) for that document, and documents can have
multiple labels. Our learned clusters naturally do not
have MUC labels, so we report results on the four
clusters that score highest with each label.
Figure 4 shows the document classification
scores. The bombing template performs best with
an F1 score of .72. Arson occurs very few times,
and Attack is lower because it is essentially an ag-
glomeration of diverse events (discussed later).
5.2 Entity Extraction
Once documents are labeled with templates, we next
extract entities into the template slots. Extraction oc-
curs in the trigger sentences from the previous sec-
tion. The extraction process is two-fold:
1. Extract all NPs that are arguments of patterns in the
template?s induced roles.
2. Extract NPs whose heads are observed frequently
with one of the roles (e.g., ?bomb? is seen with In-
strument relations in figure 2).
Take the following MUC-4 sentence as an example:
The two bombs were planted with the exclusive
purpose of intimidating the owners of...
982
The verb plant is in our learned bombing cluster, so
step (1) will extract its passive subject bombs and
map it to the correct instrument role (see figure 2).
The human target, owners, is missed because intim-
idate was not learned. However, if owner is in the
selectional preferences of the learned ?human target?
role, step (2) correctly extracts it into that role.
These are two different, but complementary,
views of semantic roles. The first is that a role is de-
fined by the set of syntactic relations that describe it.
Thus, we find all role relations and save their argu-
ments (pattern extraction). The second view is that
a role is defined by the arguments that fill it. Thus,
we extract all arguments that filled a role in training,
regardless of their current syntactic environment.
Finally, we filter extractions whose WordNet or
named entity label does not match the learned slot?s
type (e.g., a Location does not match a Person).
6 Standard Evaluation
We trained on the 1300 documents in the MUC-4
corpus and tested on the 200 document TST3 and
TST4 test set. We evaluate the four string-based
slots: perpetrator, physical target, human target, and
instrument. We merge MUC?s two perpetrator slots
(individuals and orgs) into one gold Perpetrator slot.
As in Patwardhan and Riloff (2007; 2009), we ig-
nore missed optional slots in computing recall. We
induced clusters in training, performed IR, and in-
duced the slots. We then extracted entities from the
test documents as described in section 5.2.
The standard evaluation for this corpus is to report
the F1 score for slot type accuracy, ignoring the tem-
plate type. For instance, a perpetrator of a bombing
and a perpetrator of an attack are treated the same.
This allows supervised classifiers to train on all per-
petrators at once, rather than template-specific learn-
ers. Although not ideal for our learning goals, we
report it for comparison against previous work.
Several supervised approaches have presented re-
sults on MUC-4, but unfortunately we cannot com-
pare against them. Maslennikov and Chua (2006;
2007) evaluated a random subset of test (they report
.60 and .63 F1), and Xiao et al (2004) did not eval-
uate all slot types (they report .57 F1).
Figure 5 thus shows our results with previous
work that is comparable: the fully supervised and
P R F1
Patwardhan & Riloff-09 : Supervised 48 59 53
Patwardhan & Riloff-07 : Weak-Sup 42 48 44
Our Results (1 attack) 48 25 33
Our Results (5 attack) 44 36 40
Figure 5: MUC-4 extraction, ignoring template type.
F1 Score Kidnap Bomb Arson Attack
Results .53 .43 .42 .16 / .25
Figure 6: Performance of individual templates. Attack
compares our 1 vs 5 best templates.
weakly supervised approaches of Patwardhan and
Riloff (2009; 2007). We give two numbers for our
system: mapping one learned template to Attack,
and mapping five. Our learned templates for Attack
have a different granularity than MUC-4. Rather
than one broad Attack type, we learn several: Shoot-
ing, Murder, Coup, General Injury, and Pipeline At-
tack. We see these subtypes as strengths of our al-
gorithm, but it misses the MUC-4 granularity of At-
tack. We thus show results when we apply the best
five learned templates to Attack, rather than just one.
The final F1 with these Attack subtypes is .40.
Our precision is as good as (and our F1 score near)
two algorithms that require knowledge of the tem-
plates and/or labeled data. Our algorithm instead
learned this knowledge without such supervision.
7 Specific Evaluation
In order to more precisely evaluate each learned
template, we also evaluated per-template perfor-
mance. Instead of merging all slots across all tem-
plate types, we score the slots within each template
type. This is a stricter evaluation than Section 6; for
example, bombing victims assigned to attacks were
previously deemed correct4.
Figure 6 gives our results. Three of the four tem-
plates score at or above .42 F1, showing that our
lower score from the previous section is mainly due
to the Attack template. Arson also unexpectedly
4We do not address the task of template instance identifica-
tion (e.g., splitting two bombings into separate instances). This
requires deeper discourse analysis not addressed by this paper.
983
Precision Recall F1
Kidnap .82 .47 .60 (+.07)
Bomb .60 .36 .45 (+.02)
Arson 1.0 .29 .44 (+.02)
Attack .36 .09 .15 (0.0)
Figure 7: Performance of each template type, but only
evaluated on documents labeled with each type. All oth-
ers are removed from test. The parentheses indicate F1
gain over evaluating on all test documents (figure 6).
scored well. It only occurs in 40 documents overall,
suggesting our algorithm works with little evidence.
Per-template performace is good, and our .40
overall score from the previous section illustrates
that we perform quite well in comparison to the .44-
.53 range of weakly and fully supervised results.
These evaluations use the standard TST3 and
TST4 test sets, including the documents that are not
labeled with any templates. 74 of the 200 test doc-
uments are unlabeled. In order to determine where
the system?s false positives originate, we also mea-
sure performance only on the 126 test documents
that have at least one template. Figure 7 presents the
results on this subset. Kidnap improves most signifi-
cantly in F1 score (7 F1 points absolute), but the oth-
ers only change slightly. Most of the false positives
in the system thus do not originate from the unla-
beled documents (the 74 unlabeled), but rather from
extracting incorrect entities from correctly identified
documents (the 126 labeled).
8 Discussion
Template-based IE systems typically assume knowl-
edge of the domain and its templates. We began
by showing that domain knowledge isn?t necessar-
ily required; we learned the MUC-4 template struc-
ture with surprising accuracy, learning new seman-
tic roles and several new template structures. We
are the first to our knowledge to automatically in-
duce MUC-4 templates. It is possible to take these
learned slots and use a previous approach to IE (such
as seed-based bootstrapping), but we presented an
algorithm that instead uses our learned syntactic pat-
terns. We achieved results with comparable preci-
sion, and an F1 score of .40 that approaches prior
algorithms that rely on hand-crafted knowledge.
The extraction results are encouraging, but the
template induction itself is a central contribution of
this work. Knowledge induction plays an important
role in moving to new domains and assisting users
who may not know what a corpus contains. Re-
cent work in Open IE learns atomic relations (Banko
et al, 2007b), but little work focuses on structured
scenarios. We learned more templates than just the
main MUC-4 templates. A user who seeks to know
what information is in a body of text would instantly
recognize these as key templates, and could then ex-
tract the central entities.
We hope to address in the future how the al-
gorithm?s unsupervised nature hurts recall. With-
out labeled or seed examples, it does not learn as
many patterns or robust classifiers as supervised ap-
proaches. We will investigate new text sources and
algorithms to try and capture more knowledge. The
final experiment in figure 7 shows that perhaps new
work should first focus on pattern learning and entity
extraction, rather than document identification.
Finally, while our pipelined approach (template
induction with an IR stage followed by entity ex-
traction) has the advantages of flexibility in devel-
opment and efficiency, it does involve a number
of parameters. We believe the IR parameters are
quite robust, and did not heavily focus on improving
this stage, but the two clustering steps during tem-
plate induction require parameters to control stop-
ping conditions and word filtering. While all learn-
ing algorithms require parameters, we think it is im-
portant for future work to focus on removing some
of these to help the algorithm be even more robust to
new domains and genres.
Acknowledgments
This work was supported by the National Science
Foundation IIS-0811974, and this material is also
based upon work supported by the Air Force Re-
search Laboratory (AFRL) under prime contract no.
FA8750-09-C-0181. Any opinions, findings, and
conclusion or recommendations expressed in this
material are those of the authors and do not necessar-
ily reflect the view of the Air Force Research Labo-
ratory (AFRL). Thanks to the Stanford NLP Group
and reviewers for helpful suggestions.
984
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Christian
Boitet and Pete Whitelock, editors, ACL-98, pages 86?
90, San Francisco, California. Morgan Kaufmann Pub-
lishers.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007a. Learning
relations from the web. In Proceedings of the Interna-
tional Joint Conferences on Artificial Intelligence (IJ-
CAI).
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007b. Open in-
formation extraction from the web. In Proceedings of
the International Joint Conferences on Artificial Intel-
ligence (IJCAI).
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent dirichlet alocation. Journal of Machine Learning
Research.
Razvan Bunescu and Raymond Mooney. 2004. Collec-
tive information extraction with relational markov net-
works. In Proceedings of the Association of Computa-
tional Linguistics (ACL), pages 438?445.
Andrew Carlson, J. Betteridge, R.C. Wang, E.R. Hr-
uschka Jr., and T.M. Mitchell. 2010. Coupled semi-
supervised learning for information extraction. In Pro-
ceedings of the ACM International Conference on Web
Search and Data Mining (WSDM).
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings of the Association of Computational Linguistics
(ACL), Hawaii, USA.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proceedings of the Association of Computa-
tional Linguistics (ACL), Columbus, Ohio.
Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.
2003. Closing the gap: Learning-based information
extraction rivaling knowledge-engineering methods.
In Proceedings of the Association of Computational
Linguistics (ACL).
Nancy Chinchor, David Lewis, and Lynette Hirschman.
1993. Evaluating message understanding systems: an
analysis of the third message understanding confer-
ence. Computational Linguistics, 19:3:409?449.
Katrin Erk and Sebastian Pado. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods on Natural Language Processing (EMNLP).
Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen
McKeown. 2006. Automatic creation of domain tem-
plates. In Proceedings of the Association of Computa-
tional Linguistics (ACL).
Dayne Freitag. 1998. Toward general-purpose learning
for information extraction. In Proceedings of the As-
sociation of Computational Linguistics (ACL), pages
404?408.
David Graff. 2002. English gigaword. Linguistic Data
Consortium.
Trond Grenager and Christopher D. Manning. 2006. Un-
supervised discovery of a statistical verb lexicon. In
Proceedings of the the 2006 Conference on Empirical
Methods on Natural Language Processing (EMNLP).
Shan He and Daniel Gildea. 2006. Self-training and
co-training for semantic role labeling: Primary report.
Technical Report 891, University of Rochester.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through unsupervised cross-document infer-
ence. In Proceedings of the Association of Compu-
tational Linguistics (ACL).
Niels Kasch and Tim Oates. 2010. Mining script-like
structures from the web. In Proceedings of NAACL
HLT, pages 34?42.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In Proceedings of the North
American Association of Computational Linguistics.
Mstislav Maslennikov and Tat-Seng Chua. 2007. Auto-
matic acquisition of domain knowledge for informa-
tion extraction. In Proceedings of the Association of
Computational Linguistics (ACL).
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
ie with semantic affinity patterns and relevant regions.
In Proceedings of the 2007 Conference on Empirical
Methods on Natural Language Processing (EMNLP).
Siddharth Patwardhan and Ellen Riloff. 2009. A unified
model of phrasal and sentential evidence for informa-
tion extraction. In Proceedings of the 2009 Conference
on Empirical Methods on Natural Language Process-
ing (EMNLP).
Lisa Rau, George Krupka, Paul Jacobs, Ira Sider, and
Lois Childs. 1992. Ge nltoolset: Muc-4 test results
and analysis. In Proceedings of the Message Under-
standing Conference (MUC-4), pages 94?99.
Ellen Riloff and Mark Schmelzenbach. 1998. An em-
pirical approach to conceptual case frame acquisition.
In Proceedings of the Sixth Workshop on Very Large
Corpora.
Ellen Riloff, Janyce Wiebe, and William Phillips. 2005.
Exploiting subjectivity classification to improve infor-
mation extraction. In Proceedings of AAAI-05.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding. Lawrence Erlbaum.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
ie using unrestricted relation discovery. In Proceed-
ings of NAACL.
985
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of the Association of Computational Linguis-
tics (ACL), pages 224?231.
Beth M. Sundheim. 1991. Third message understand-
ing evaluation and conference (muc-3): Phase 1 status
report. In Proceedings of the Message Understanding
Conference.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006.
A hybrid approach for the acquisition of information
extraction patterns. In Proceedings of the EACL Work-
shop on Adaptive Text Extraction and Mining.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In Proceedings of
the 2004 Conference on Empirical Methods on Nat-
ural Language Processing (EMNLP).
Jing Xiao, Tat-Seng Chua, and Hang Cui. 2004. Cas-
cading use of soft and hard matching pattern rules
for weakly supervised information extraction. In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING).
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
COLING, pages 940?946.
986
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 250?259,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A computational approach to politeness with application to social factors
Cristian Danescu-Niculescu-Mizil??, Moritz Sudhof?, Dan Jurafsky?,
Jure Leskovec?, and Christopher Potts?
?Computer Science Department, ?Linguistics Department
??Stanford University, ?Max Planck Institute SWS
cristiand|jure@cs.stanford.edu, sudhof|jurafsky|cgpotts@stanford.edu
Abstract
We propose a computational framework
for identifying linguistic aspects of polite-
ness. Our starting point is a new corpus
of requests annotated for politeness, which
we use to evaluate aspects of politeness
theory and to uncover new interactions
between politeness markers and context.
These findings guide our construction of
a classifier with domain-independent lexi-
cal and syntactic features operationalizing
key components of politeness theory, such
as indirection, deference, impersonaliza-
tion and modality. Our classifier achieves
close to human performance and is effec-
tive across domains. We use our frame-
work to study the relationship between po-
liteness and social power, showing that po-
lite Wikipedia editors are more likely to
achieve high status through elections, but,
once elevated, they become less polite. We
see a similar negative correlation between
politeness and power on Stack Exchange,
where users at the top of the reputation
scale are less polite than those at the bot-
tom. Finally, we apply our classifier to
a preliminary analysis of politeness vari-
ation by gender and community.
1 Introduction
Politeness is a central force in communication, ar-
guably as basic as the pressure to be truthful, in-
formative, relevant, and clear (Grice, 1975; Leech,
1983; Brown and Levinson, 1978). Natural lan-
guages provide numerous and diverse means for
encoding politeness and, in conversation, we con-
stantly make choices about where and how to use
these devices. Kaplan (1999) observes that ?peo-
ple desire to be paid respect? and identifies hon-
orifics and other politeness markers, like please,
as ?the coin of that payment?. In turn, polite-
ness markers are intimately related to the power
dynamics of social interactions and are often a
decisive factor in whether those interactions go
well or poorly (Gyasi Obeng, 1997; Chilton, 1990;
Andersson and Pearson, 1999; Rogers and Lee-
Wong, 2003; Holmes and Stubbe, 2005).
The present paper develops a computational
framework for identifying and characterizing po-
liteness marking in requests. We focus on re-
quests because they involve the speaker imposing
on the addressee, making them ideal for exploring
the social value of politeness strategies (Clark and
Schunk, 1980; Francik and Clark, 1985). Requests
also stimulate extensive use of what Brown and
Levinson (1987) call negative politeness: speaker
strategies for minimizing (or appearing to mini-
mize) the imposition on the addressee, for exam-
ple, by being indirect (Would you mind) or apolo-
gizing for the imposition (I?m terribly sorry, but)
(Lakoff, 1973; Lakoff, 1977; Brown and Levin-
son, 1978).
Our investigation is guided by a new corpus
of requests annotated for politeness. The data
come from two large online communities in which
members frequently make requests of other mem-
bers: Wikipedia, where the requests involve edit-
ing and other administrative functions, and Stack
Exchange, where the requests center around a di-
verse range of topics (e.g., programming, garden-
ing, cycling). The corpus confirms the broad out-
lines of linguistic theories of politeness pioneered
by Brown and Levinson (1987), but it also reveals
new interactions between politeness markings and
the morphosyntactic context. For example, the po-
liteness of please depends on its syntactic position
and the politeness markers it co-occurs with.
Using this corpus, we construct a polite-
ness classifier with a wide range of domain-
independent lexical, sentiment, and dependency
features operationalizing key components of po-
250
liteness theory, including not only the negative
politeness markers mentioned above but also el-
ements of positive politeness (gratitude, positive
and optimistic sentiment, solidarity, and inclusive-
ness). The classifier achieves near human-level ac-
curacy across domains, which highlights the con-
sistent nature of politeness strategies and paves the
way to using the classifier to study new data.
Politeness theory predicts a negative correlation
between politeness and the power of the requester,
where power is broadly construed to include so-
cial status, authority, and autonomy (Brown and
Levinson, 1987). The greater the speaker?s power
relative to her addressee, the less polite her re-
quests are expected to be: there is no need for her
to incur the expense of paying respect, and failing
to make such payments can invoke, and hence re-
inforce, her power. We support this prediction by
applying our politeness framework to Wikipedia
and Stack Exchange, both of which provide in-
dependent measures of social status. We show
that polite Wikipedia editors are more likely to
achieve high status through elections; however,
once elected, they become less polite. Similarly,
on Stack Exchange, we find that users at the top of
the reputation scale are less polite than those at the
bottom.
Finally, we briefly address the question of how
politeness norms vary across communities and so-
cial groups. Our findings confirm established re-
sults about the relationship between politeness and
gender, and they identify substantial variation in
politeness across different programming language
subcommunities on Stack Exchange.
2 Politeness data
Requests involve an imposition on the addressee,
making them a natural domain for studying the
inter-connections between linguistic aspects of po-
liteness and social variables.
Requests in online communities We base our
analysis on two online communities where re-
quests have an important role: the Wikipedia
community of editors and the Stack Exchange
question-answer community.1 On Wikipedia, to
coordinate on the creation and maintenance of
the collaborative encyclopedia, editors can in-
teract with each other on user talk-pages;2 re-
1http://stackexchange.com/about
2http://en.wikipedia.org/wiki/
Wikipedia:User_pages
quests posted on a user talk-page, although pub-
lic, are generally directed to the owner of the talk-
page. On Stack Exchange, users often comment
on existing posts requesting further information or
proposing edits; these requests are generally di-
rected to the authors of the original posts.
Both communities are not only rich in user-
to-user requests, but these requests are also part
of consequential conversations, not empty social
banter; they solicit specific information or con-
crete actions, and they expect a response.
Politeness annotation Computational studies of
politeness, or indeed any aspect of linguistic prag-
matics, demand richly labeled data. We there-
fore label a large portion of our request data
(over 10,000 utterances) using Amazon Mechan-
ical Turk (AMT), creating the largest corpus with
politeness annotations (see Table 1 for details).3
We choose to annotate requests containing ex-
actly two sentences, where the second sentence
is the actual request (and ends with a question
mark). This provides enough context to the an-
notators while also controlling for length effects.
Each annotator was instructed to read a batch of
13 requests and consider them as originating from
a co-worker by email. For each request, the anno-
tator had to indicate how polite she perceived the
request to be by using a slider with values rang-
ing from ?very impolite? to ?very polite?.4 Each
request was labeled by five different annotators.
We vetted annotators by restricting their resi-
dence to be in the U.S. and by conducting a lin-
guistic background questionnaire. We also gave
them a paraphrasing task shown to be effective
for verifying and eliciting linguistic attentiveness
(Munro et al, 2010), and we monitored the an-
notation job and manually filtered out annotators
who submitted uniform or seemingly random an-
notations.
Because politeness is highly subjective and an-
notators may have inconsistent scales, we ap-
plied the standard z-score normalization to each
worker?s scores. Finally, we define the politeness
score (henceforth politeness) of a request as the
average of the five scores assigned by the annota-
tors. The distribution of resulting request scores
(shown in Figure 1) has an average of 0 and stan-
3Publicly available at http://www.mpi-sws.org/
?cristian/Politeness.html4We used non-categorical ratings for finer granularity and
to help account for annotators? different perception scales.
251
domain #requests #annotated #annotators
Wiki 35,661 4,353 219
SE 373,519 6,604 212
Table 1: Summary of the request data and its po-
liteness annotations.
Figure 1: Distribution of politeness scores. Posi-
tive scores indicate requests perceived as polite.
dard deviation of 0.7 for both domains; positive
values correspond to polite requests (i.e., requests
with normalized annotations towards the ?very po-
lite? extreme) and negative values to impolite re-
quests. A summary of all our request data is shown
in Table 1.
Inter-annotator agreement To evaluate the re-
liability of the annotations we measure the inter-
annotator agreement by computing, for each batch
of 13 documents that were annotated by the same
set of 5 users, the mean pairwise correlation of the
respective scores. For reference, we compute the
same quantities after randomizing the scores by
sampling from the observed distribution of polite-
ness scores. As shown in Figure 2, the labels are
coherent and significantly different from the ran-
domized procedure (p < 0.0001 according to a
Wilcoxon signed rank test).5
Binary perception Although we did not im-
pose a discrete categorization of politeness, we
acknowledge an implicit binary perception of the
phenomenon: whenever an annotator moved a
slider in one direction or the other, she made a
binary politeness judgment. However, the bound-
5The commonly used Cohen/Fleiss Kappa agreement
measures are not suitable for this type of annotation, in which
labels are continuous rather than categorical.
Figure 2: Inter-annotator pairwise correlation,
compared to the same measure after randomizing
the scores.
Quartile: 1st 2nd 3rd 4th
Wiki 62% 8% 3% 51%
SE 37% 4% 6% 46%
Table 2: The percentage of requests for which all
five annotators agree on binary politeness. The
4th quartile contains the requests with the top 25%
politeness scores in the data. (For reference, ran-
domized scoring yields agreement percentages of
<20% for all quartiles.)
ary between somewhat polite and somewhat im-
polite requests can be blurry. To test this intuition,
we break the set of annotated requests into four
groups, each corresponding to a politeness score
quartile. For each quartile, we compute the per-
centage of requests for which all five annotators
made the same binary politeness judgment. As
shown in Table 2, full agreement is much more
common in the 1st (bottom) and 4th (top) quar-
tiles than in the middle quartiles. This suggests
that the politeness scores assigned to requests that
are only somewhat polite or somewhat impolite
are less reliable and less tied to an intuitive notion
of binary politeness. This discrepancy motivates
our choice of classes in the prediction experiments
(Section 4) and our use of the top politeness quar-
tile (the 25% most polite requests) as a reference
in our subsequent discussion.
3 Politeness strategies
As we mentioned earlier, requests impose on the
addressee, potentially placing her in social peril if
she is unwilling or unable to comply. Requests
therefore naturally give rise to the negative po-
252
liteness strategies of Brown and Levinson (1987),
which are attempts to mitigate these social threats.
These strategies are prominent in Table 3, which
describes the core politeness markers we analyzed
in our corpus of Wikipedia requests. We do not
include the Stack Exchange data in this analysis,
reserving it as a ?test community? for our predic-
tion task (Section 4).
Requests exhibiting politeness markers are au-
tomatically extracted using regular expression
matching on the dependency parse obtained by the
Stanford Dependency Parser (de Marneffe et al,
2006), together with specialized lexicons. For ex-
ample, for the hedges marker (Table 3, line 19),
we match all requests containing a nominal subject
dependency edge pointing out from a hedge verb
from the hedge list created by Hyland (2005). For
each politeness strategy, Table 3 shows the aver-
age politeness score of the respective requests (as
described in Section 2; positive numbers indicate
polite requests), and their top politeness quartile
membership (i.e., what percentage fall within the
top quartile of politeness scores). As discussed at
the end of Section 2, the top politeness quartile
gives a more robust and more intuitive measure of
politeness. For reference, a random sample of re-
quests will have a 0 politeness score and a 25% top
quartile membership; in both cases, larger num-
bers indicate higher politeness.
Gratitude and deference (lines 1?2) are ways
for the speaker to incur a social cost, helping to
balance out the burden the request places on the
addressee. Adopting Kaplan (1999)?s metaphor,
these are the coin of the realm when it comes to
paying the addressee respect. Thus, they are indi-
cators of positive politeness.
Terms from the sentiment lexicon (Liu et al,
2005) are also tools for positive politeness, either
by emphasizing a positive relationship with the ad-
dressee (line 4), or being impolite by using nega-
tive sentiment that damages this positive relation-
ship (line 5). Greetings (line 3) are another way to
build a positive relationship with the addressee.
The remainder of the cues in Table 3 are neg-
ative politeness strategies, serving the purpose of
minimizing, at least in appearance, the imposition
on the addressee. Apologizing (line 6) deflects the
social threat of the request by attuning to the impo-
sition itself. Being indirect (line 9) is another way
to minimize social threat. This strategy allows the
speaker to avoid words and phrases convention-
ally associated with requests. First-person plural
forms like we and our (line 15) are also ways of
being indirect, as they create the sense that the
burden of the request is shared between speaker
and addressee (We really should . . . ). Though in-
directness is not invariably interpreted as polite-
ness marking (Blum-Kulka, 2003), it is nonethe-
less a reliable marker of it, as our scores indicate.
What?s more, direct variants (imperatives, state-
ments about the addressee?s obligations) are less
polite (lines 10?11).
Indirect strategies also combine with hedges
(line 19) conveying that the addressee is unlikely
to accept the burden (Would you by any chance
. . . ?, Would it be at all possible . . . ?). These too
serve to provide the addressee with a face-saving
way to deny the request. We even see subtle effects
of modality at work here: the irrealis, counterfac-
tual forms would and could are more polite than
their ability (dispositional) or future-oriented vari-
ants can and will; compare lines 12 and 13. This
parallels the contrast between factuality markers
(impolite; line 20) and hedging (polite; line 19).
Many of these features are correlated with each
other, in keeping with the insight of Brown and
Levinson (1987) that politeness markers are of-
ten combined to create a cumulative effect of in-
creased politeness. Our corpora also highlight in-
teractions that are unexpected (or at least unac-
counted for) on existing theories of politeness. For
example, sentence-medial please is polite (line 7),
presumably because of its freedom to combine
with other negative politeness strategies (Could
you please . . . ). In contrast, sentence-initial please
is impolite (line 8), because it typically signals a
more direct strategy (Please do this), which can
make the politeness marker itself seem insincere.
We see similar interactions between pronominal
forms and syntactic structure: sentence-initial you
is impolite (You need to . . . ), whereas sentence-
medial you is often part of the indirect strategies
we discussed above (Would/Could you . . . ).
4 Predicting politeness
We now show how our linguistic analysis can be
used in a machine learning model for automati-
cally classifying requests according to politeness.
A classifier can help verify the predictive power,
robustness, and domain-independent generality of
the linguistic strategies of Section 3. Also, by pro-
viding automatic politeness judgments for large
253
Strategy Politeness In top quartile Example
1. Gratitude 0.87*** 78%*** I really appreciate that you?ve done them.
2. Deference 0.78*** 70%*** Nice work so far on your rewrite.
3. Greeting 0.43*** 45%*** Hey, I just tried to . . .
4. Positive lexicon 0.12*** 32%*** Wow! / This is a great way to deal. . .
5. Negative lexicon -0.13*** 22%** If you?re going to accuse me . . .
6. Apologizing 0.36*** 53%*** Sorry to bother you . . .
7. Please 0.49*** 57%*** Could you please say more. . .
8. Please start ?0.30* 22% Please do not remove warnings . . .
9. Indirect (btw) 0.63*** 58%** By the way, where did you find . . .
10. Direct question ?0.27*** 15%*** What is your native language?
11. Direct start ?0.43*** 9%*** So can you retrieve it or not?
12. Counterfactual modal 0.47*** 52%*** Could/Would you . . .
13. Indicative modal 0.09 27% Can/Will you . . .
14. 1st person start 0.12*** 29%** I have just put the article . . .
15. 1st person pl. 0.08* 27% Could we find a less complex name . . .
16. 1st person 0.08*** 28%*** It is my view that ...
17. 2nd person 0.05*** 30%*** But what?s the good source you have in mind?
18. 2nd person start ?0.30*** 17%** You?ve reverted yourself . . .
19. Hedges 0.14*** 28% I suggest we start with . . .
20. Factuality ?0.38*** 13%*** In fact you did link, . . .
Table 3: Positive (1-5) and negative (6?20) politeness strategies and their relation to human perception of
politeness. For each strategy we show the average (human annotated) politeness scores for the requests
exhibiting that strategy (compare with 0 for a random sample of requests; a positive number indicates
the strategy is perceived as being polite), as well as the percentage of requests exhibiting the respective
strategy that fall in the top quartile of politeness scores (compare with 25% for a random sample of
requests). Throughout the paper: for politeness scores, statistical significance is calculated by comparing
the set of requests exhibiting the strategy with the rest using a Mann-Whitney-Wilcoxon U test; for top
quartile membership a binomial test is used.
amounts of new data on a scale unfeasible for hu-
man annotation, it can also enable a detailed anal-
ysis of the relation between politeness and social
factors (Section 5).
Task setup To evaluate the robustness and
domain-independence of the analysis from Sec-
tion 3, we run our prediction experiments on two
very different domains. We treat Wikipedia as a
?development domain? since we used it for de-
veloping and identifying features and for training
our models. Stack Exchange is our ?test domain?
since it was not used for identifying features. We
take the model (features and weights) trained on
Wikipedia and use them to classify requests from
Stack Exchange.
We consider two classes of requests: polite
and impolite, defined as the top and, respectively,
bottom quartile of requests when sorted by their
politeness score (based on the binary notion of
politeness discussed in Section 2). The classes
are therefore balanced, with each class consisting
of 1,089 requests for the Wikipedia domain and
1,651 requests for the Stack Exchange domain.
We compare two classifiers ? a bag of words
classifier (BOW) and a linguistically informed
classifier (Ling.) ? and use human labelers as a
reference point. The BOW classifier is an SVM
using a unigram feature representation.6 We con-
sider this to be a strong baseline for this new
6Unigrams appearing less than 10 times are excluded.
254
classification task, especially considering the large
amount of training data available. The linguisti-
cally informed classifier (Ling.) is an SVM using
the linguistic features listed in Table 3 in addition
to the unigram features. Finally, to obtain a ref-
erence point for the prediction task we also collect
three new politeness annotations for each of the re-
quests in our dataset using the same methodology
described in Section 2. We then calculate human
performance on the task (Human) as the percent-
age of requests for which the average score from
the additional annotations matches the binary po-
liteness class of the original annotations (e.g., a
positive score corresponds to the polite class).
Classification results We evaluate the classi-
fiers both in an in-domain setting, with a standard
leave-one-out cross validation procedure, and in a
cross-domain setting, where we train on one do-
main and test on the other (Table 4). For both our
development and our test domains, and in both the
in-domain and cross-domain settings, the linguis-
tically informed features give 3-4% absolute im-
provement over the bag of words model. While
the in-domain results are within 3% of human per-
formance, the greater room for improvement in the
cross-domain setting motivates further research on
linguistic cues of politeness.
The experiments in this section confirm that
our theory-inspired features are indeed effective in
practice, and generalize well to new domains. In
the next section we exploit this insight to automat-
ically annotate a much larger set of requests (about
400,000) with politeness labels, enabling us to re-
late politeness to several social variables and out-
comes. For new requests, we use class probabil-
ity estimates obtained by fitting a logistic regres-
sion model to the output of the SVM (Witten and
Frank, 2005) as predicted politeness scores (with
values between 0 and 1; henceforth politeness, by
abuse of language).
5 Relation to social factors
We now apply our framework to studying the rela-
tionship between politeness and social variables,
focussing on social power dynamics. Encour-
aged by the close-to-human performance of our
in-domain classifiers, we use them to assign po-
liteness labels to our full dataset and then compare
these labels to independent measures of power and
status in our data. The results closely match those
obtained with human-labeled data alone, thereby
In-domain Cross-domain
Train Wiki SE Wiki SE
Test Wiki SE SE Wiki
BOW 79.84% 74.47% 64.23% 72.17%
Ling. 83.79% 78.19% 67.53% 75.43%
Human 86.72% 80.89% 80.89% 86.72%
Table 4: Accuracies of our two classifiers for
Wikipedia (Wiki) and Stack Exchange (SE), for
in-domain and cross-domain settings. Human per-
formance is included as a reference point. The ran-
dom baseline performance is 50%.
supporting the use of computational methods to
pursue questions about social variables.
5.1 Relation to social outcome
Earlier, we characterized politeness markings as
currency used to pay respect. Such language is
therefore costly in a social sense, and, relatedly,
tends to incur costs in terms of communicative ef-
ficiency (Van Rooy, 2003). Are these costs worth
paying? We now address this question by studying
politeness in the context of the electoral system of
the Wikipedia community of editors.
Among Wikipedia editors, status is a salient so-
cial variable (Anderson et al, 2012). Administra-
tors (admins) are editors who have been granted
certain rights, including the ability to block other
editors and to protect or delete articles.7 Ad-
mins have a higher status than common editors
(non-admins), and this distinction seems to be
widely acknowledged by the community (Burke
and Kraut, 2008b; Leskovec et al, 2010; Danescu-
Niculescu-Mizil et al, 2012). Aspiring editors
become admins through public elections,8 so we
know when the status change from non-admin to
admins occurred and can study users? language
use in relation to that time.
To see whether politeness correlates with even-
tual high status, we compare, in Table 5, the po-
liteness levels of requests made by users who will
eventually succeed in becoming administrators
(Eventual status: Admins) with requests made by
users who are not admins (Non-admins).9 We ob-
serve that admins-to-be are significantly more po-
7http://en.wikipedia.org/wiki/
Wikipedia:Administrators
8http://en.wikipedia.org/wiki/
Wikipedia:Requests_for_adminship
9We consider only requests made up to one month before
the election, to avoid confusion with pre-election behavior.
255
Eventual status Politeness Top quart.
Admins 0.46** 30%***
Non-admins 0.39*** 25%
Failed 0.37** 22%
Table 5: Politeness and status. Editors who
will eventually become admins are more polite
than non-admins (p<0.001 according to a Mann-
Whitney-Wilcoxon U test) and than editors who
will eventually fail to become admins (p<0.001).
Out of their requests, 30% are rated in the top po-
liteness quartile (significantly more than the 25%
of a random sample; p<0.001 according to a bi-
nomial test). This analysis was conducted on 31k
requests (1.4k for Admins, 28.9k for Non-admins,
652 for Failed).
lite than non-admins. One might wonder whether
this merely reflects the fact that not all users aspire
to become admins, and those that do are more po-
lite. To address this, we also consider users who
ran for adminship but did not earn community ap-
proval (Eventual status: Failed). These users are
also significantly less polite than their successful
counterparts, indicating that politeness indeed cor-
relates with a positive social outcome here.
5.2 Politeness and power
We expect a rise in status to correlate with a de-
cline in politeness (as predicted by politeness the-
ory, and discussed in Section 1). The previous sec-
tion does not test this hypothesis, since all editors
compared in Table 5 had the same (non-admin)
status when writing the requests. However, our
data does provide three ways of testing this hy-
pothesis.
First, after the adminship elections, successful
editors get a boost in power by receiving admin
privileges. Figure 3 shows that this boost is mir-
rored by a significant decrease in politeness (blue,
diamond markers). Losing an election has the op-
posite effect on politeness (red, circle markers),
perhaps as a consequence of reinforced low status.
Second, Stack Exchange allows us to test more
situational power effects.10 On the site, users re-
quest, from the community, information they are
lacking. This informational asymmetry between
the question-asker and his audience puts him at
10We restrict all experiments in this section to the largest
subcommunity of Stack Exchange, namely Stack Overflow.
Before election Election After election
0.41
0.37
0.39
0.46
Pred
icte
d po
liten
ess 
sco
res
Successful candidatesFailed candidates
Figure 3: Successful and failed candidates be-
fore and after elections. Editors that will even-
tually succeed (diamond marker) are significantly
more polite than those that will fail (circle mark-
ers). Following the elections, successful editors
become less polite while unsuccessful editors be-
come more polite.
a social disadvantage. We therefore expect the
question-asker to be more polite than the people
who respond. Table 6 shows that this expectation
is born out: comments posted to a thread by the
original question-asker are more polite than those
posted by other users.
Role Politeness Top quart.
Question-asker 0.65*** 32%***
Answer-givers 0.52*** 20%***
Table 6: Politeness and dependence. Requests
made in comments posted by the question-asker
are significantly more polite than the other re-
quests. Analysis conducted on 181k requests
(106k for question-askers, 75k for answer-givers).
Third, Stack Exchange allows us to examine
power in the form of authority, through the com-
munity?s reputation system. Again, we see a neg-
ative correlation between politeness and power,
even after controlling for the role of the user mak-
ing the requests (i.e., Question-asker or Answer-
giver). Table 7 summarizes the results.11
Human validation The above analyses are
based on predicted politeness from our classifier.
This allows us to use the entire request data cor-
11Since our data does not contain time stamps for reputa-
tion scores, we only consider requests that were issued in the
six months prior to the available snapshot.
256
Reputation level Politeness Top quart.
Low reputation 0.68*** 27%***
Middle reputation 0.66*** 25%
High reputation 0.64*** 23%***
Table 7: Politeness and Stack Exchange reputation
(texts by question-askers only). High-reputation
users are less polite. Analysis conducted on 25k
requests (4.5k low, 12.5k middle, 8.4k high).
pus to test our hypotheses and to apply precise
controls to our experiments (such as restricting
our analysis to question-askers in the reputation
experiment). In order to validate this methodol-
ogy, we turned again to human annotation: we
collected additional politeness annotation for the
types of requests involved in the newly designed
experiments. When we re-ran our experiments on
human-labeled data alone we obtained the same
qualitative results, with statistical significance al-
ways lower than 0.01.12
Prediction-based interactions The human val-
idation of classifier-based results suggests that
our prediction framework can be used to explore
differences in politeness levels across factors of
interest, such as communities, geographical re-
gions and gender, even where gathering suffi-
cient human-annotated data is infeasible. We
mention just a few such preliminary results here:
(i) Wikipedians from the U.S. Midwest are most
polite (when compared to other census-defined
regions), (ii) female Wikipedians are generally
more polite (consistent with prior studies in which
women are more polite in a variety of domains;
(Herring, 1994)), and (iii) programming language
communities on Stack Exchange vary significantly
by politeness (Table 8; full disclosure: our analy-
ses were conducted in Python).
6 Related work
Politeness has been a central concern of modern
pragmatic theory since its inception (Grice, 1975;
Lakoff, 1973; Lakoff, 1977; Leech, 1983; Brown
and Levinson, 1978), because it is a source of
pragmatic enrichment, social meaning, and cul-
tural variation (Harada, 1976; Matsumoto, 1988;
12However, due to the limited size of the human-labeled
data, we could not control for the role of the user in the Stack
Exchange reputation experiment.
PL name Politeness Top quartile
Python 0.47*** 23%
Perl 0.49 24%
PHP 0.51 24%
Javascript 0.53** 26%**
Ruby 0.59*** 28%*
Table 8: Politeness of requests from different lan-
guage communities on Stack Exchange.
Ide, 1989; Blum-Kulka and Kasper, 1990; Blum-
Kulka, 2003; Watts, 2003; Byon, 2006). The start-
ing point for most research is the theory of Brown
and Levinson (1987). Aspects of this theory
have been explored from game-theoretic perspec-
tives (Van Rooy, 2003) and implemented in lan-
guage generation systems for interactive narratives
(Walker et al, 1997), cooking instructions, (Gupta
et al, 2007), translation (Faruqui and Pado, 2012),
spoken dialog (Wang et al, 2012), and subjectivity
analysis (Abdul-Mageed and Diab, 2012), among
others.
In recent years, politeness has been studied in
online settings. Researchers have identified vari-
ation in politeness marking across different con-
texts and media types (Herring, 1994; Brennan
and Ohaeri, 1999; Duthler, 2006) and between
different social groups (Burke and Kraut, 2008a).
The present paper pursues similar goals using or-
ders of magnitude more data, which facilitates a
fuller survey of different politeness strategies.
Politeness marking is one aspect of the broader
issue of how language relates to power and status,
which has been studied in the context of workplace
discourse (Bramsen et al, ; Diehl et al, 2007;
Peterson et al, 2011; Prabhakaran et al, 2012;
Gilbert, 2012; McCallum et al, 2007) and so-
cial networking (Scholand et al, 2010). However,
this research focusses on domain-specific textual
cues, whereas the present work seeks to lever-
age domain-independent politeness cues, build-
ing on the literature on how politeness affects
worksplace social dynamics and power structures
(Gyasi Obeng, 1997; Chilton, 1990; Andersson
and Pearson, 1999; Rogers and Lee-Wong, 2003;
Holmes and Stubbe, 2005). Burke and Kraut
(2008b) study the question of how and why spe-
cific individuals rise to administrative positions
on Wikipedia, and Danescu-Niculescu-Mizil et al
(2012) show that power differences on Wikipedia
257
are revealed through aspects of linguistic accom-
modation. The present paper complements this
work by revealing the role of politeness in social
outcomes and power relations.
7 Conclusion
We construct and release a large collection of
politeness-annotated requests and use it to evalu-
ate key aspects of politeness theory. We build a
politeness classifier that achieves near-human per-
formance and use it to explore the relation between
politeness and social factors such as power, status,
gender, and community membership. We hope the
publicly available collection of annotated requests
enables further study of politeness and its relation
to social factors, as this paper has only begun to
explore this area.
Acknowledgments
We thank Jean Wu for running the AMT an-
notation task, and all the participating turkers.
We thank Diana Minculescu and the anonymous
reviewers for their helpful comments. This
work was supported in part by NSF IIS-1016909,
CNS-1010921, IIS-1149837, IIS-1159679, ARO
MURI, DARPA SMISC, Okawa Foundation, Do-
como, Boeing, Allyes, Volkswagen, Intel, Alfred
P. Sloan Fellowship, the Microsoft Faculty Fel-
lowship, the Gordon and Dailey Pattee Faculty
Fellowship, and the Center for Advanced Study in
the Behavioral Sciences at Stanford.
References
Muhammad Abdul-Mageed and Mona Diab. 2012.
AWATIF: A multi-genre corpus for Modern Stan-
dard Arabic subjectivity and sentiment analysis. In
Proceedings of LREC, pages 3907?3914.
Ashton Anderson, Daniel Huttenlocher, Jon Kleinberg,
and Jure Leskovec. 2012. Effects of user similarity
in social media. In Proceedings of WSDM, pages
703?712.
Lynne M. Andersson and Christine M. Pearson. 1999.
Tit for tat? the spiraling effect of incivility in the
workplace. The Academy of Management Review,
24(3):452?471.
Shoshana Blum-Kulka and Gabriele Kasper. 1990.
Special issue on politeness. Journal of Pragmatics,
144(2).
Shoshana Blum-Kulka. 2003. Indirectness and po-
liteness in requests: Same or different? Journal of
Pragmatics, 11(2):131?146.
Philip Bramsen, Martha Escobar-Molana, Ami Patel,
and Rafael Alonso. Extracting social power rela-
tionships from natural language. In Proceedings of
ACL, pages 773?782.
Susan E Brennan and Justina O Ohaeri. 1999. Why
do electronic conversations seem less polite? the
costs and benefits of hedging. SIGSOFT Softw. Eng.
Notes, 24(2):227?235.
Penelope Brown and Stephen C. Levinson. 1978.
Universals in language use: Politeness phenomena.
In Esther N. Goody, editor, Questions and Polite-
ness: Strategies in Social Interaction, pages 56?311,
Cambridge. Cambridge University Press.
Penelope Brown and Stephen C Levinson. 1987. Po-
liteness: some universals in language usage. Cam-
bridge University Press.
Moira Burke and Robert Kraut. 2008a. Mind your
Ps and Qs: the impact of politeness and rudeness
in online communities. In Proceedings of CSCW,
pages 281?284.
Moira Burke and Robert Kraut. 2008b. Taking up the
mop: identifying future wikipedia administrators. In
CHI ?08 extended abstracts on Human factors in
computing systems, pages 3441?3446.
Andrew Sangpil Byon. 2006. The role of linguistic in-
directness and honorifics in achieving linguistic po-
liteness in Korean requests. Journal of Politeness
Research, 2(2):247?276.
Paul Chilton. 1990. Politeness, politics, and diplo-
macy. Discourse and Society, 1(2):201?224.
Herbert H. Clark and Dale H. Schunk. 1980. Polite
responses to polite requests. Cognition, 8(1):111?
143.
Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: Language effects and power differences in
social interaction. In Proceedings of WWW, pages
699?708.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, pages 449?454.
Christopher P. Diehl, Galileo Namata, and Lise Getoor.
2007. Relationship identification for social network
discovery. In Proceedings of the AAAI Workshop on
Enhanced Messaging, pages 546?552.
Kirk W Duthler. 2006. The Politeness of Requests
Made Via Email and Voicemail: Support for the Hy-
perpersonal Model. Journal of Computer-Mediated
Communication, 11(2):500?521.
Manaal Faruqui and Sebastian Pado. 2012. Towards a
model of formal and informal address in english. In
Proceedings of EACL, pages 623?633.
258
Elen P. Francik and Herbert H. Clark. 1985. How to
make requests that overcome obstacles to compli-
ance. Journal of Memory and Language, 24:560?
568.
Eric Gilbert. 2012. Phrases that signal workplace hier-
archy. In Proceedings of CSCW, pages 1037?1046.
H. Paul Grice. 1975. Logic and conversation. In Pe-
ter Cole and Jerry Morgan, editors, Syntax and Se-
mantics, volume 3: Speech Acts, pages 43?58. Aca-
demic Press, New York.
S Gupta, M Walker, and D Romano. 2007. How rude
are you?: Evaluating politeness and affect in inter-
action. Affective Computing and Intelligent Interac-
tion, pages 203?217.
Samuel Gyasi Obeng. 1997. Language and politics:
Indirectness in political discourse. Discourse and
Society, 8(1):49?83.
S. I. Harada. 1976. Honorifics. In Masayoshi
Shibatani, editor, Syntax and Semantics, volume
5: Japanese Generative Grammar, pages 499?561.
Academic Press, New York.
Susan Herring. 1994. Politeness in computer cul-
ture: Why women thank and men flame. In Cul-
tural performances: Proceedings of the third Berke-
ley women and language conference, volume 278,
page 94.
Janet Holmes and Maria Stubbe. 2005. Power and Po-
liteness in the Workplace: A Sociolinguistic Analysis
of Talk at Work. Longman, London.
Ken Hyland. 2005. Metadiscourse: Exploring Interac-
tion in Writing. Continuum, London and New York.
Sachiko Ide. 1989. Formal forms and discernment:
Two neglected aspects of universals of linguistic po-
liteness. Multilingua, 8(2?3):223?248.
David Kaplan. 1999. What is meaning? Explorations
in the theory of Meaning as Use. Brief version ?
draft 1. Ms., UCLA.
Robin Lakoff. 1973. The logic of politeness; or, mid-
ing your P?s and Q?s. In Proceedings of the 9th
Meeting of the Chicago Linguistic Society, pages
292?305.
Robin Lakoff. 1977. What you can do with words:
Politeness, pragmatics and performatives. In Pro-
ceedings of the Texas Conference on Performatives,
Presuppositions and Implicatures, pages 79?106.
Geoffrey N. Leech. 1983. Principles of Pragmatics.
Longman, London and New York.
Jure Leskovec, Daniel Huttenlocher, and Jon Klein-
berg. 2010. Governance in Social Media: A case
study of the Wikipedia promotion process. In Pro-
ceedings of ICWSM, pages 98?105.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion Observer: analyzing and comparing opin-
ions on the Web. In Proceedings of WWW, pages
342?351.
Yoshiko Matsumoto. 1988. Reexamination of the uni-
versality of face: Politeness phenomena in Japanese.
Journal of Pragmatics, 12(4):403?426.
Andrew McCallum, Xuerui Wang, and Andr?es
Corrada-Emmanuel. 2007. Topic and role discovery
in social networks with experiments on Enron and
academic email. Journal of Artificial Intelligence
Research, 30(1):249?272.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher
Potts, Tyler Schnoebelen, and Harry Tily. 2010.
Crowdsourcing and language studies: the new gen-
eration of linguistic data. In Proceedings of the
NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical
Turk, pages 122?130.
Kelly Peterson, Matt Hohensee, and Fei Xia. 2011.
Email formality in the workplace: A case study on
the enron corpus. In Proceedings of the ACL Work-
shop on Language in Social Media, pages 86?95.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting Overt Display of Power in
Written Dialogs. In Proceedings of NAACL-HLT,
pages 518?522.
Priscilla S. Rogers and Song Mei Lee-Wong. 2003.
Reconceptualizing politeness to accommodate dy-
namic tensions in subordinate-to-superior reporting.
Journal of Business and Technical Communication,
17(4):379?412.
Andrew J. Scholand, Yla R. Tausczik, and James W.
Pennebaker. 2010. Social language network analy-
sis. In Proceedings of CSCW, pages 23?26.
Robert Van Rooy. 2003. Being polite is a handicap:
Towards a game theoretical analysis of polite lin-
guistic behavior. In Proceedings of TARK, pages
45?58.
Marilyn A Walker, Janet E Cahn, and Stephen J Whit-
taker. 1997. Improvising linguistic style: social and
affective bases for agent personality. In Proceedings
of AGENTS, pages 96?105.
William Yang Wang, Samantha Finkelstein, Amy
Ogan, Alan W. Black, and Justine Cassell. 2012.
?love ya, jerkface?: Using sparse log-linear mod-
els to build positive and impolite relationships with
teens. In Proceedings of SIGDIAL, pages 20?29.
Richard J. Watts. 2003. Politeness. Cambridge Uni-
versity Press, Cambridge.
Ian H Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques.
Morgan Kaufmann.
259
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1650?1659,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Linguistic Models for Analyzing and Detecting Biased Language
Marta Recasens
Stanford University
recasens@google.com
Cristian Danescu-Niculescu-Mizil
Stanford University
Max Planck Institute SWS
cristiand@cs.stanford.edu
Dan Jurafsky
Stanford University
jurafsky@stanford.edu
Abstract
Unbiased language is a requirement for
reference sources like encyclopedias and
scientific texts. Bias is, nonetheless, ubiq-
uitous, making it crucial to understand its
nature and linguistic realization and hence
detect bias automatically. To this end we
analyze real instances of human edits de-
signed to remove bias from Wikipedia ar-
ticles. The analysis uncovers two classes
of bias: framing bias, such as praising or
perspective-specific words, which we link
to the literature on subjectivity; and episte-
mological bias, related to whether propo-
sitions that are presupposed or entailed in
the text are uncontroversially accepted as
true. We identify common linguistic cues
for these classes, including factive verbs,
implicatives, hedges, and subjective inten-
sifiers. These insights help us develop fea-
tures for a model to solve a new prediction
task of practical importance: given a bi-
ased sentence, identify the bias-inducing
word. Our linguistically-informed model
performs almost as well as humans tested
on the same task.
1 Introduction
Writers and editors of reference works such as
encyclopedias, textbooks, and scientific articles
strive to keep their language unbiased. For ex-
ample, Wikipedia advocates a policy called neu-
tral point of view (NPOV), according to which
articles should represent ?fairly, proportionately,
and as far as possible without bias, all signifi-
cant views that have been published by reliable
sources? (Wikipedia, 2013b). Wikipedia?s style
guide asks editors to use nonjudgmental language,
to indicate the relative prominence of opposing
points of view, to avoid presenting uncontroversial
facts as mere opinion, and, conversely, to avoid
stating opinions or contested assertions as facts.
Understanding the linguistic realization of bias
is important for linguistic theory; automatically
detecting these biases is equally significant for
computational linguistics. We propose to ad-
dress both by using a powerful resource: edits in
Wikipedia that are specifically designed to remove
bias. Since Wikipedia maintains a complete revi-
sion history, the edits associated with NPOV tags
allow us to compare the text in its biased (before)
and unbiased (after) form, helping us better under-
stand the linguistic realization of bias. Our work
thus shares the intuition of prior NLP work apply-
ing Wikipedia?s revision history (Nelken and Ya-
mangil, 2008; Yatskar et al, 2010; Max and Wis-
niewski, 2010; Zanzotto and Pennacchiotti, 2010).
The analysis of Wikipedia?s edits provides valu-
able linguistic insights into the nature of biased
language. We find two major classes of bias-driven
edits. The first, framing bias, is realized by sub-
jective words or phrases linked with a particular
point of view. In (1), the term McMansion, unlike
homes, appeals to a negative attitude toward large
and pretentious houses. The second class, episte-
mological bias, is related to linguistic features that
subtly (often via presupposition) focus on the be-
lievability of a proposition. In (2), the assertive
stated removes the bias introduced by claimed,
which casts doubt on Kuypers? statement.
(1) a. Usually, smaller cottage-style houses have been de-
molished to make way for these McMansions.
b. Usually, smaller cottage-style houses have been de-
molished to make way for these homes.
(2) a. Kuypers claimed that the mainstream press in Amer-
ica tends to favor liberal viewpoints.
b. Kuypers stated that the mainstream press in America
tends to favor liberal viewpoints.
Bias is linked to the lexical and grammatical cues
identified by the literature on subjectivity (Wiebe
et al, 2004; Lin et al, 2011), sentiment (Liu et
al., 2005; Turney, 2002), and especially stance
1650
or ?arguing subjectivity? (Lin et al, 2006; So-
masundaran and Wiebe, 2010; Yano et al, 2010;
Park et al, 2011; Conrad et al, 2012). For ex-
ample, like stance, framing bias is realized when
the writer of a text takes a particular position on
a controversial topic and uses its metaphors and
vocabulary. But unlike the product reviews or de-
bate articles that overtly use subjective language,
editors in Wikipedia are actively trying to avoid
bias, and hence biases may appear more subtly,
in the form of covert framing language, or pre-
suppositions and entailments that may not play as
important a role in other genres. Our linguistic
analysis identifies common classes of these subtle
bias cues, including factive verbs, implicatives and
other entailments, hedges, and subjective intensi-
fiers.
Using these cues could help automatically de-
tect and correct instances of bias, by first finding
biased phrases, then identifying the word that in-
troduces the bias, and finally rewording to elim-
inate the bias. In this paper we propose a so-
lution for the second of these tasks, identifying
the bias-inducing word in a biased phrase. Since,
as we show below, this task is quite challenging
for humans, our system has the potential to be
very useful in improving the neutrality of refer-
ence works like Wikipedia. Tested on a subset of
non-neutral sentences from Wikipedia, our model
achieves 34% accuracy?and up to 59% if the
top three guesses are considered?on this difficult
task, outperforming four baselines and nearing hu-
mans tested on the same data.
2 Analyzing a Dataset of Biased
Language
We begin with an empirical analysis based on
Wikipedia?s bias-driven edits. This section de-
scribes the data, and summarizes our linguistic
analysis.1
2.1 The NPOV Corpus from Wikipedia
Given Wikipedia?s strict enforcement of an NPOV
policy, we decided to build the NPOV corpus,
containing Wikipedia edits that are specifically de-
signed to remove bias. Editors are encouraged to
identify and rewrite biased passages to achieve a
more neutral tone, and they can use several NPOV
1The data and bias lexicon we developed are available at
http://www.mpi-sws.org/?cristian/Biased_
language.html
Data Articles Revisions Words Edits Sents
Train 5997 2238K 11G 13807 1843
Dev 653 210K 0.9G 1261 163
Test 814 260K 1G 1751 230
Total 7464 2708K 13G 16819 2235
Table 1: Statistics of the NPOV corpus, extracted
from Wikipedia. (Edits refers to bias-driven ed-
its, i.e., with an NPOV comment. Sents refers to
sentences with a one-word bias-driven edit.)
tags to mark biased content.2 Articles tagged this
way fall into Wikipedia?s category of NPOV dis-
putes.
We constructed the NPOV corpus by retrieving
all articles that were or had been in the NPOV-
dispute category3 together with their full revision
history. We used Stanford?s CoreNLP tools4 to to-
kenize and split the text into sentences. Table 1
shows the statistics of this corpus, which we split
into training (train), development (dev), and test.
Following Wikipedia?s terminology, we call each
version of a Wikipedia article a revision, and so an
article can be viewed as a set of (chronologically
ordered) revisions.
2.2 Extracting Edits Meant to Remove Bias
Given all the revisions of a page, we extracted the
changes between pairs of revisions with the word-
mode diff function from the Diff Match and Patch
library.5 We refer to these changes between revi-
sions as edits, e.g., McMansion > large home. An
edit consists of two strings: the old string that is
being replaced (i.e., the before form), and the new
modified string (i.e., the after form).
Our assumption was that among the edits hap-
pening in NPOV disputes, we would have a high
density of edits intended to remove bias, which we
call bias-driven edits, like (1) and (2) from Sec-
tion 1. But many other edits occur even in NPOV
disputes, including edits to fix spelling or gram-
matical errors, simplify the language, make the
meaning more precise, or even vandalism (Max
2{{POV}}, {{POV-check}}, {{POV-section}}, etc.
Adding these tags displays a template such as ?The neutrality
of this article is disputed. Relevant discussion may be found
on the talk page. Please do not remove this message until the
dispute is resolved.?
3http://en.wikipedia.org/wiki/
Category:All_NPOV_disputes
4http://nlp.stanford.edu/software/
corenlp.shtml
5http://code.google.com/p/google-diff-
match-patch
1651
and Wisniewski, 2010). Therefore, in order to ex-
tract a high-precision set of bias-driven edits, we
took advantage of the comments that editors can
associate with a revision?typically short and brief
sentences describing the reason behind the revi-
sion. We considered as bias-driven edits those that
appeared in a revision whose comment mentioned
(N)POV, e.g., Attempts at presenting some claims
in more NPOV way; or merging in a passage
from the researchers article after basic NPOV-
ing. We only kept edits whose before and af-
ter forms contained five or fewer words, and dis-
carded those that only added a hyperlink or that
involved a minimal change (character-based Lev-
enshtein distance < 4). The final number of bias-
driven edits for each of the data sets is shown in
the ?Edits? column of Table 1.
2.3 Linguistic Analysis
Style guides talk about biased language in a pre-
scriptive manner, listing a few words that should
be avoided because they are flattering, vague, or
endorse a particular point of view (Wikipedia,
2013a). Our focus is on analyzing actual bi-
ased text and bias-driven edits extracted from
Wikipedia.
As we suggested above, this analysis uncovered
two major classes of bias: epistemological bias
and framing bias. Table 2 shows the distribution
(from a sample of 100 edits) of the different types
and subtypes of bias presented in this section.
(A) Epistemological bias involves propositions
that are either commonly agreed to be true or com-
monly agreed to be false and that are subtly pre-
supposed, entailed, asserted or hedged in the text.
1. Factive verbs (Kiparsky and Kiparsky, 1970)
presuppose the truth of their complement
clause. In (3-a) and (4-a), realize and re-
veal presuppose the truth of ?the oppression
of black people...? and ?the Meditation tech-
nique produces...?, whereas (3-b) and (4-b)
present the two propositions as somebody?s
stand or an experimental result.
(3) a. He realized that the oppression of black peo-
ple was more of a result of economic exploita-
tion than anything innately racist.
b. His stand was that the oppression of black
people was more of a result of economic ex-
ploitation than anything innately racist.
(4) a. The first research revealed that the Meditation
technique produces a unique state fact.
b. The first research indicated that the Medita-
tion technique produces a unique state fact.
Bias Subtype %
A. Epistemological bias 43
- Factive verbs 3
- Entailments 25
- Assertives 11
- Hedges 4
B. Framing bias 57
- Intensifiers 19
- One-sided terms 38
Table 2: Proportion of the different bias types.
2. Entailments are directional relations that
hold whenever the truth of one word or
phrase follows from another, e.g., murder en-
tails kill because there cannot be murdering
without killing (5). However, murder en-
tails killing in an unlawful, premeditated way.
This class includes implicative verbs (Kart-
tunen, 1971), which imply the truth or un-
truth of their complement, depending on the
polarity of the main predicate. In (6-a), co-
erced into accepting entails accepting in an
unwilling way.
(5) a. After he murdered three policemen, the
colony proclaimed Kelly a wanted outlaw.
b. After he killed three policemen, the colony
proclaimed Kelly a wanted outlaw.
(6) a. A computer engineer who was coerced into
accepting a plea bargain.
b. A computer engineer who accepted a plea bar-
gain.
3. Assertive verbs (Hooper, 1975) are those
whose complement clauses assert a proposi-
tion. The truth of the proposition is not pre-
supposed, but its level of certainty depends
on the asserting verb. Whereas verbs of say-
ing like say and state are usually neutral,
point out and claim cast doubt on the cer-
tainty of the proposition.
(7) a. The ?no Boeing? theory is a controversial is-
sue, even among conspiracists, many of whom
have pointed out that it is disproved by ...
b. The ?no Boeing? theory is a controversial is-
sue, even among conspiracists, many of whom
have said that it is disproved by...
(8) a. Cooper says that slavery was worse in South
America and the US than Canada, but clearly
states that it was a horrible and cruel practice.
b. Cooper says that slavery was worse in South
America and the US than Canada, but points
out that it was a horrible and cruel practice.
1652
4. Hedges are used to reduce one?s commit-
ment to the truth of a proposition, thus
avoiding any bold predictions (9-b) or state-
ments (10-a).6
(9) a. Eliminating the profit motive will decrease the
rate of medical innovation.
b. Eliminating the profit motive may have a
lower rate of medical innovation.
(10) a. The lower cost of living in more rural areas
means a possibly higher standard of living.
b. The lower cost of living in more rural areas
means a higher standard of living.
Epistemological bias is bidirectional, that is,
bias can occur because doubt is cast on a propo-
sition commonly assumed to be true, or because
a presupposition or implication is made about a
proposition commonly assumed to be false. For
example, in (7) and (8) above, point out is replaced
in the former case, but inserted in the second case.
If the truth of the proposition is uncontroversially
accepted by the community (i.e., reliable sources,
etc.), then the use of a factive is unbiased. In con-
trast, if only a specific viewpoint agrees with its
truth, then using a factive is biased.
(B) Framing bias is usually more explicit than
epistemological bias because it occurs when sub-
jective or one-sided words are used, revealing the
author?s stance in a particular debate (Entman,
2007).
1. Subjective intensifiers are adjectives or ad-
verbs that add (subjective) force to the mean-
ing of a phrase or proposition.
(11) a. Schnabel himself did the fantastic reproduc-
tions of Basquiat?s work.
b. Schnabel himself did the accurate reproduc-
tions of Basquiat?s work.
(12) a. Shwekey?s albums are arranged by many tal-
ented arrangers.
b. Shwekey?s albums are arranged by many dif-
ferent arrangers.
2. One-sided terms reflect only one of the sides
of a contentious issue. They often belong
to controversial subjects (e.g., religion, ter-
rorism, etc.) where the same event can be
seen from two or more opposing perspec-
tives, like the Israeli-Palestinian conflict (Lin
et al, 2006).
6See Choi et al (2012) for an exploration of the interface
between hedging and framing.
(13) a. Israeli forces liberated the eastern half of
Jerusalem.
b. Israeli forces captured the eastern half of
Jerusalem.
(14) a. Concerned Women for America?s major ar-
eas of political activity have consisted of op-
position to gay causes, pro-life law...
b. Concerned Women for America?s major ar-
eas of political activity have consisted of op-
position to gay causes, anti-abortion law...
(15) a. Colombian terrorist groups.
b. Colombian paramilitary groups.
Framing bias has been studied within the liter-
ature on stance recognition and arguing subjectiv-
ity. Because this literature has focused on iden-
tifying which side an article takes on a two-sided
debate such as the Israeli-Palestinian conflict (Lin
et al, 2006), most studies cast the problem as a
two-way classification of documents or sentences
into for/positive vs. against/negative (Anand et
al., 2011; Conrad et al, 2012; Somasundaran and
Wiebe, 2010), or into one of two opposing views
(Yano et al, 2010; Park et al, 2011). The fea-
tures used by these models include subjectivity
and sentiment lexicons, counts of unigrams and
bigrams, distributional similarity, discourse rela-
tionships, and so on.
The datasets used by these studies come from
genres that overtly take a specific stance (e.g.,
debates, editorials, blog posts). In contrast,
Wikipedia editors are asked not to advocate a par-
ticular point of view, but to provide a balanced ac-
count of the different available perspectives. For
this reason, overtly biased opinion statements such
as ?I believe that...? are not common in Wikipedia.
The features used by the subjectivity literature
help us detect framing bias, but we also need fea-
tures that capture epistemological bias expressed
through presuppositions and entailments.
3 Automatically Identifying Biased
Language
We now show how the bias cues identified in Sec-
tion 2.3 can help solve a new task. Given a biased
sentence (e.g., a sentence that a Wikipedia editor
has tagged as violating the NPOV policy), our goal
in this new task is to identify the word that intro-
duces bias. This is part of a potential three-step
process for detecting and correcting biased lan-
guage: (1) finding biased phrases, (2) identifying
the word that introduces the bias, (3) rewording to
eliminate the bias. As we will see below, it can be
1653
hard even for humans to track down the sources of
bias, because biases in reference works are often
subtle and implicit. An automatic bias detector
that can highlight the bias-inducing word(s) and
draw the editors? attention to words that need to
be modified could thus be important for improving
reference works like Wikipedia or even in news re-
porting.
We selected the subset of sentences that had a
single NPOV edit involving one (original) word.
(Although the before form consists of only one
word, the after form can be either one or more
words or the null string (i.e., deletion edits); we do
not use the after string in this identification task).
The number of sentences in the train, dev and test
sets is shown in the last column of Table 1.
We trained a logistic regression model on a
feature vector for every word that appears in the
NPOV sentences from the training set, with the
bias-inducing words as the positive class, and all
the other words as the negative class. The features
are described in the next section.
At test time, the model is given a set of sen-
tences and, for each of them, it ranks the words ac-
cording to their probability to be biased, and out-
puts the highest ranked word (TOP1 model), the
two highest ranked words (TOP2 model), or the
three highest ranked words (TOP3 model).
3.1 Features
The types of features used in the logistic regres-
sion model are listed in Table 3, together with
their value space. The total number of features is
36,787. The ones targeting framing bias draw on
previous work on sentiment and subjectivity de-
tection (Wiebe et al, 2004; Liu et al, 2005). Fea-
tures to capture epistemological bias are based on
the bias cues identified in Section 2.3.
A major split separates the features that de-
scribe the word under analysis (e.g., lemma, POS,
whether it is a hedge, etc.) from those that de-
scribe its surrounding context (e.g., the POS of the
word to the left, whether there is a hedge in the
context, etc.). We define context as a 5-gram win-
dow, i.e., two words to the left of the word un-
der analysis, and two to the right. Taking con-
text into account is important given that biases can
be context-dependent, especially epistemological
bias since it depends on the truth of a proposition.
To define some of the features like POS and gram-
matical relation, we used the Stanford?s CoreNLP
tagger and dependency parser (de Marneffe et al,
2006).
Features 9?10 use the list of hedges from Hy-
land (2005), features 11?14 use the factives and
assertives from Hooper (1975), features 15?16
use the implicatives from Karttunen (1971), fea-
tures 19?20 use the entailments from Berant et
al. (2012), features 21?25 employ the subjectiv-
ity lexicon from Riloff and Wiebe (2003), and fea-
tures 26?29 use the sentiment lexicon?positive
and negative words?from Liu et al (2005). If the
word (or a word in the context) is in the lexicon,
then the feature is true, otherwise it is false.
We also included a ?bias lexicon? (feature 31)
that we built based on our NPOV corpus from
Wikipedia. We used the training set to extract the
lemmas of words that were the before form of at
least two NPOV edits, and that occurred in at least
two different articles. Of the 654 words included
in this lexicon, 433 were unique to this lexicon
(i.e., recorded in neither Riloff and Wiebe?s (2003)
subjectivity lexicon nor Liu et al?s (2005) senti-
ment lexicon) and represented many one-sided or
controversial terms, e.g., abortion, same-sex, exe-
cute.
Finally, we also included a ?collaborative fea-
ture? that, based on the previous revisions of the
edit?s article, computes the ratio between the num-
ber of times that the word was NPOV-edited and
its frequency of occurrence. This feature was de-
signed to capture framing bias specific to an article
or topic.
3.2 Baselines
Previous work on subjectivity and stance recog-
nition has been evaluated on the task of classify-
ing documents as opinionated vs. factual, for vs.
against, positive vs. negative. Given that the task
of identifying the bias-inducing word of a sentence
is novel, there were no previous results to compare
directly against. We ran the following five base-
lines.
1. Random guessing. Naively returns a random
word from every sentence.
2. Role baseline. Selects the word with the
syntactic role that has the highest probabil-
ity to be biased, as computed on the train-
ing set. This is the parse tree root (proba-
bility p = .126 to be biased), followed by
verbal arguments (p = .085), and the subject
(p = .084).
1654
ID Feature Value Description
1* Word <string> Word w under analysis.
2 Lemma <string> Lemma of w.
3* POS {NNP, JJ, ...} POS of w.
4* POS ? 1 {NNP, JJ, ...} POS of one word before w.
5 POS ? 2 {NNP, JJ, ...} POS of two words before w.
6* POS + 1 {NNP, JJ, ...} POS of one word after w.
7 POS + 2 {NNP, JJ, ...} POS of two words after w.
8 Position in sentence {start, mid, end} Position of w in the sentence (split into three parts).
9 Hedge {true, false} w is in Hyland?s (2005) list of hedges (e.g., apparently).
10* Hedge in context {true, false} One/two words) around w is a hedge (Hyland, 2005).
11* Factive verb {true, false} w is in Hooper?s (1975) list of factives (e.g., realize).
12* Factive verb in context {true, false} One/two word(s) around w is a factive (Hooper, 1975).
13* Assertive verb {true, false} w is in Hooper?s (1975) list of assertives (e.g., claim).
14* Assertive verb in context {true, false} One/two word(s) around w is an assertive (Hooper, 1975).
15 Implicative verb {true, false} w is in Karttunen?s (1971) list of implicatives (e.g., manage).
16* Implicative verb in context {true, false} One/two word(s) around w is an implicative (Karttunen, 1971).
17* Report verb {true, false} w is a report verb (e.g., add).
18 Report verb in context {true, false} One/two word(s) around w is a report verb.
19* Entailment {true, false} w is in Berant et al?s (2012) list of entailments (e.g., kill).
20* Entailment in context {true, false} One/two word(s) around w is an entailment (Berant et al, 2012).
21* Strong subjective {true, false} w is in Riloff and Wiebe?s (2003) list of strong subjectives (e.g.,
absolute).
22 Strong subjective in context {true, false} One/two word(s) around w is a strong subjective (Riloff and
Wiebe, 2003).
23* Weak subjective {true, false} w is in Riloff and Wiebe?s (2003) list of weak subjectives (e.g.,
noisy).
24* Weak subjective in context {true, false} One/two word(s) around w is a weak subjective (Riloff and
Wiebe, 2003).
25 Polarity {+, ?, both, ...} The polarity of w according to Riloff and Wiebe (2003), e.g.,
praising is positive.
26* Positive word {true, false} w is in Liu et al?s (2005) list of positive words (e.g., excel).
27* Positive word in context {true, false} One/two word(s) around w is positive (Liu et al, 2005).
28* Negative word {true, false} w is in Liu et al?s (2005) list of negative words (e.g., terrible).
29* Negative word in context {true, false} One/two word(s) around w is negative (Liu et al, 2005).
30* Grammatical relation {root, subj, ...} Whether w is the subject, object, root, etc. of its sentence.
31 Bias lexicon {true, false} w has been observed in NPOV edits (e.g., nationalist).
32* Collaborative feature <numeric> Number of times that w was NPOV-edited in the article?s prior
history / frequency of w.
Table 3: Features used by the bias detector. The star (*) shows the most contributing features.
3. Sentiment baseline. Logistic regression
model that only uses the features based on
Liu et al?s (2005) lexicons of positive and
negative words (i.e., features 26?29).
4. Subjectivity baseline. Logistic regression
model that only uses the features based on
Riloff and Wiebe?s (2003) lexicon of subjec-
tive words (i.e., features 21?25).
5. Wikipedia baseline. Selects as biased the
words that appear in Wikipedia?s list of words
to avoid (Wikipedia, 2013a).
These baselines assessed the difficulty of the
task, as well as the extent to which traditional
sentiment-analysis and subjectivity features would
suffice to detect biased language.
3.3 Results and Discussion
To measure performance, we used accuracy de-
fined as:
#sentences with the correctly predicted biased word
#sentences
The results are shown in Table 4. As explained
earlier, we evaluated all the models by outputting
as biased either the highest ranked word or the
two or three highest ranked words. These corre-
spond to the TOP1, TOP2 and TOP3 columns, re-
spectively. The TOP3 score increases to 59%. A
tool that highlights up to three words to be revised
would simplify the editors? job and decrease sig-
nificantly the time required to revise.
Our model outperforms all five baselines by a
large margin, showing the importance of consid-
ering a wide range of features. Wikipedia?s list
of words to avoid falls very short on recall. Fea-
1655
System TOP1 TOP2 TOP3
Baseline 1: Random 2.18 7.83 9.13
Baseline 2: Role 15.65 20.43 25.65
Baseline 3: Sentiment 14.78 22.61 27.83
Baseline 4: Subjectivity 16.52 25.22 33.91
Baseline 5: Wikipedia 10.00 10.00 10.00
Our system 34.35 46.52 58.70
Humans (AMT) 37.39 50.00 59.13
Table 4: Accuracy (%) of the bias detector on the
test set.
tures that contribute the most to the model?s per-
formance (in a feature ablation study on the dev
set) are highlighted with a star (*) in Table 3. In
addition to showing the importance of linguistic
cues for different classes of bias, the ablation study
highlights the role of contextual features. The bias
lexicon does not seem to help much, suggesting
that it is overfit to the training data.
An error analysis shows that our system makes
acceptable errors in that words wrongly predicted
as bias-inducing may well introduce bias in a dif-
ferent context. In (16), the system picked eschew,
whereas orthodox would have been the correct
choice according to the gold edit. Note that both
the sentiment and the subjectivity lexicons list es-
chew as a negative word. The bias type that poses
the greatest challenge to the system are terms that
are one-sided or loaded in a particular topic, such
as orthodox in this example.
(16) a. Some Christians eschew orthodox theology; such
as the Unitarians, Socinian, [...]
b. Some Christians eschew mainstream trinitarian
theology; such as the Unitarians, Socinian, [...]
The last row in Table 4 lists the performance
of humans on the same task, presented in the next
section.
4 Human Perception of Biased Language
Is it difficult for humans to find the word in a
sentence that induces bias, given the subtle, of-
ten implicit biases in Wikipedia. We used Ama-
zon Mechanical Turk7 (AMT) to elicit annotations
from humans for the same 230 sentences from the
test set that we used to evaluate the bias detector
in Section 3.3. The goal of this annotation was
twofold: to compare the performance of our bias
detector against a human baseline, and to assess
the difficulty of this task for humans. While AMT
labelers are not trained Wikipedia editors, under-
7http://www.mturk.com
standing how difficult these cases are for untrained
labelers is an important baseline.
4.1 Task
Our HIT (Human Intelligence Task) was called
?Find the biased word!?. We kept the task descrip-
tion succinct. Turkers were shown Wikipedia?s
definition of a ?biased statement? and two exam-
ple sentences that illustrated the two types of bias,
framing and epistemological. In each HIT, annota-
tors saw 10 sentences, one after another, and each
one followed by a text box entitled ?Word intro-
ducing bias.? For each sentence, they were asked
to type in the text box the word that caused the
statement to be biased. They were only allowed to
enter a single word.
Before the 10 sentences, turkers were asked to
list the languages they spoke as well as their pri-
mary language in primary school. This was En-
glish in all the cases. In addition, we included a
probe question in the form of a paraphrasing task:
annotators were given a sentence and two para-
phrases (a correct and a bad one) to choose from.
The goal of this probe question was to discard
annotators who were not paying attention or did
not have a sufficient command of English. This
simple test was shown to be effective in verifying
and eliciting linguistic attentiveness (Munro et al,
2010). This was especially important in our case
as we were interested in using the human annota-
tions as an oracle. At the end of the task, partici-
pants were given the option to provide additional
feedback.
We split the 230 sentences into 23 sets of 10
sentences, and asked for 10 annotations of each
set. Each approved HIT was rewarded with $0.30.
4.2 Results and Discussion
On average, it took turkers about four minutes to
complete each HIT. The feedback that we got from
some of them confirmed our hypothesis that find-
ing the bias source is difficult: ?Some of the ?bi-
ases? seemed very slight if existent at all,? ?This
was a lot harder than I thought it would be... Inter-
esting though!?.
We postprocessed the answers ignoring case,
punctuation signs, and spelling errors. To ensure
an answer quality as high as possible, we only
kept those turkers who answered attentively by ap-
plying two filters: we only accepted answers that
matched a valid word from the sentence, and we
discarded answers from participants who did not
1656
2 3 4 5 6 7 8 9 10
Number of times the top word was selected
Nu
mb
er o
f se
nte
nce
s
0
10
20
30
40
50
Figure 1: Distribution of the number of turkers
who selected the top word (i.e., the word selected
by the majority of turkers).
pass the paraphrasing task?there were six such
cases. These filters provided us with confidence in
the turkers? answers as a fair standard of compari-
son.
Overall, humans correctly identified the biased
word 30% of the time. For each sentence, we
ranked the words according to the number of turk-
ers (out of 10) who selected them and, like we
did for the automated system, we assessed per-
formance when considering only the top word
(TOP1), the top 2 words (TOP2), and the top 3
words (TOP3). The last row of Table 4 reports the
results. Only 37.39% of the majority answers co-
incided with the gold label, slightly higher than
our system?s accuracy. The fact that the human
answers are very close to the results of our system
reflects the difficulty of the task. Biases in refer-
ence works can be very subtle and go unnoticed
by humans; automated systems could thus be ex-
tremely helpful.
As a measure of inter-rater reliability, we com-
puted pairwise agreement. The turkers agreed
40.73% of the time, compared to the 5.1% chance
agreement that would be achieved if raters had
randomly selected a word for each sentence. Fig-
ure 1 plots the number of times the top word of
each sentence was selected. The bulk of the sen-
tences only obtained between four and six answers
for the same word.
There is a good amount of overlap (?34%) be-
tween the correct answers predicted by our system
and those from humans. Much like the automated
system, humans also have the hardest time identi-
fying words that are one-sided or controversial to
a specific topic. They also picked eschew for (16)
instead of orthodox. Compared to the system, they
do better in detecting bias-inducing intensifiers,
and about the same with epistemological bias.
5 Related Work
The work in this paper builds upon prior work on
subjectivity detection (Wiebe et al, 2004; Lin et
al., 2011; Conrad et al, 2012) and stance recogni-
tion (Yano et al, 2010; Somasundaran and Wiebe,
2010; Park et al, 2011), but applied to the genre
of reference works such as Wikipedia. Unlike the
blogs, online debates and opinion pieces which
have been the major focus of previous work, bias
in reference works is undesirable. As a result,
the expression of bias is more implicit, making it
harder to detect by both computers and humans.
Of the two classes of bias that we uncover, fram-
ing bias is indeed strongly linked to subjectiv-
ity, but epistemological bias is not. In this re-
spect, our research is comparable to Greene and
Resnik?s (2009) work on identifying implicit sen-
timent or perspective in journalistic texts, based on
semantico-syntactic choices.
Given that the data that we use is not supposed
to be opinionated, our task consists in detecting
(implicit) bias instead of classifying into side A
or B documents about a controversial topic like
ObamaCare (Conrad et al, 2012) or the Israeli-
Palestinian conflict (Lin et al, 2006; Greene and
Resnik, 2009). Our model detects whether all
the relevant perspectives are fairly represented by
identifying statements that are one-sided. To this
end, the features based on subjectivity and senti-
ment lexicons turn out to be helpful, and incor-
porating more features for stance detection is an
important direction for future work.
Other aspects of Wikipedia structure have been
used for other NLP applications. The Wikipedia
revision history has been used for spelling correc-
tion, text summarization (Nelken and Yamangil,
2008), lexical simplification (Yatskar et al, 2010),
paraphrasing (Max and Wisniewski, 2010), and
textual entailment (Zanzotto and Pennacchiotti,
2010). Ganter and Strube (2009) have used
Wikipedia?s weasel-word tags to train a hedge de-
tector. Callahan and Herring (2011) have exam-
ined cultural bias based on Wikipedia?s NPOV
policy.
1657
6 Conclusions
Our study of bias in Wikipedia has implications
for linguistic theory and computational linguis-
tics. We show that bias in reference works falls
broadly into two classes, framing and epistemo-
logical. The cues to framing bias are more ex-
plicit and are linked to the literature on subjec-
tivity; cues to epistemological bias are subtle and
implicit, linked to presuppositions and entailments
in the text. Epistemological bias has not received
much attention since it does not play a major role
in overtly opinionated texts, the focus of much re-
search on stance recognition. However, our logis-
tic regression model reveals that epistemological
and other features can usefully augment the tradi-
tional sentiment and subjectivity features for ad-
dressing the difficult task of identifying the bias-
inducing word in a biased sentence.
Identifying the bias-inducing word is a chal-
lenging task even for humans. Our linguistically-
informed model performs nearly as well as hu-
mans tested on the same task. Given the sub-
tlety of some of these biases, an automated sys-
tem that highlights one or more potentially biased
words would provide a helpful tool for editors of
reference works and news reports, not only mak-
ing them aware of unnoticed biases but also sav-
ing them hours of time. Future work could in-
vestigate the incorporation of syntactic features or
further features from the stance detection litera-
ture. Features from the literature on veridicality
(de Marneffe et al, 2012) could be informative of
the writer?s commitment to the truth of the events
described, and document-level features could help
assess the extent to which the article provides a
balanced account of all the facts and points of
view.
Finally, the NPOV data and the bias lexicon that
we release as part of this research could prove use-
ful in other bias related tasks.
Acknowledgments
We greatly appreciate the support of Jean Wu and
Christopher Potts in running our task on Ama-
zon Mechanical Turk, and all the Amazon Turkers
who participated. We benefited from comments
by Valentin Spitkovsky on a previous draft and
from the helpful suggestions of the anonymous re-
viewers. The first author was supported by a Beat-
riu de Pino?s postdoctoral scholarship (2010 BP-A
00149) from Generalitat de Catalunya. The sec-
ond author was supported by NSF IIS-1016909.
The last author was supported by the Center for
Advanced Study in the Behavioral Sciences at
Stanford.
References
Pranav Anand, Marilyn Walker, Rob Abbott, Jean
E. Fox Tree, Robeson Bowmani, and Michael Mi-
nor. 2011. Cats rule and dogs drool!: Classifying
stance in online debate. In Proceedings of ACL-
HLT 2011 Workshop on Computational Approaches
to Subjectivity and Sentiment Analysis, pages 1?9.
Jonathan Berant, Ido Dagan, Meni Adler, and Jacob
Goldberger. 2012. Efficient tree-based approxima-
tion for entailment graph learning. In Proceedings
of ACL 2012, pages 117?125.
Ewa Callahan and Susan C. Herring. 2011. Cul-
tural bias in Wikipedia articles about famous per-
sons. Journal of the American Society for Informa-
tion Science and Technology, 62(10):1899?1915.
Eunsol Choi, Chenhao Tan, Lillian Lee, Cristian
Danescu-Niculescu-Mizil, and Jennifer Spindel.
2012. Hedge detection as a lens on framing in the
GMO debates: a position paper. In Proceedings
of the ACL-2012 Workshop on Extra-Propositional
Aspects of Meaning in Computational Linguistics,
pages 70?79.
Alexander Conrad, Janyce Wiebe, and Rebecca Hwa.
2012. Recognizing arguing subjectivity and argu-
ment tags. In Proceedings of ACL-2012 Workshop
on Extra-Propositional Aspects of Meaning in Com-
putational Linguistics, pages 80?88.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC 2006.
Marie-Catherine de Marneffe, Christopher D. Man-
ning, and Christopher Potts. 2012. Did it hap-
pen? The pragmatic complexity of veridicality as-
sessment. Computational Linguistics, 38(2):301?
333.
Robert M. Entman. 2007. Framing bias: Media in the
distribution of power. Journal of Communication,
57(1):163?173.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of ACL-IJCNLP 2009, pages 173?176.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of NAACL-HLT 2009, pages 503?
511.
1658
Joan B. Hooper. 1975. On assertive predicates. In
J. Kimball, editor, Syntax and Semantics, volume 4,
pages 91?124. Academic Press, New York.
Ken Hyland. 2005. Metadiscourse: Exploring Interac-
tion in Writing. Continuum, London and New York.
Lauri Karttunen. 1971. Implicative verbs. Language,
47(2):340?358.
Paul Kiparsky and Carol Kiparsky. 1970. Fact. In
M. Bierwisch and K. E. Heidolph, editors, Progress
in Linguistics, pages 143?173. Mouton, The Hague.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? Identifying perspectives at the document and
sentence levels. In Proceedings of CoNLL 2006,
pages 109?116.
Chenghua Lin, Yulan He, and Richard Everson.
2011. Sentence subjectivity detection with
weakly-supervised learning. In Proceedings of
AFNLP 2011, pages 1153?1161.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion Observer: analyzing and comparing opin-
ions on the Web. In Proceedings of WWW 2005,
pages 342?351.
Aure?lien Max and Guillaume Wisniewski. 2010. Min-
ing naturally-occurring corrections and paraphrases
from Wikipedia?s revision history. In Proceedings
of LREC 2010, pages 3143?3148.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher
Potts, Tyler Schnoebelen, and Harry Tily. 2010.
Crowdsourcing and language studies: the new gen-
eration of linguistic data. In Proceedings of the
NAACL-HLT 2010 Workshop on Creating Speech
and Language Data With Amazons Mechanical
Turk, pages 122?130.
Rani Nelken and Elif Yamangil. 2008. Mining
Wikipedias article revision history for training Com-
putational Linguistics algorithms. In Proceedings of
the 1st AAAI Workshop on Wikipedia and Artificial
Intelligence.
Souneil Park, KyungSoon Lee, and Junehwa Song.
2011. Contrasting opposing views of news articles
on contentious issues. In Proceedings of ACL 2011,
pages 340?349.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP 2003, pages 105?112.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 116?124.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings of ACL 2002,
pages 417?424.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277?308.
Wikipedia. 2013a. Wikipedia: Manual of style / Words
to watch. http://en.wikipedia.org/
wiki/Wikipedia:Words_to_avoid. [Re-
trieved February 5, 2013].
Wikipedia. 2013b. Wikipedia: Neutral point of view.
http:http://en.wikipedia.org/wiki/
Wikipedia:Neutral_point_of_view.
[Retrieved February 5, 2013].
Tae Yano, Philip Resnik, and Noah A. Smith. 2010.
Shedding (a thousand points of) light on biased lan-
guage. In Proceedings of the NAACL-HLT 2010
Workshop on Creating Speech and Language Data
With Amazons Mechanical Turk, pages 152?158.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from Wikipedia. In Proceedings of NAACL-
HLT 2010, pages 365?368.
Fabio M. Zanzotto and Marco Pennacchiotti. 2010.
Expanding textual entailment corpora from
Wikipedia using co-training. In Proceedings of
the 2nd Coling Workshop on The People?s Web
Meets NLP: Collaboratively Constructed Semantic
Resources, pages 28?36.
1659
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 74?80,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Implicatures and Nested Beliefs in Approximate Decentralized-POMDPs
Adam Vogel, Christopher Potts, and Dan Jurafsky
Stanford University
Stanford, CA, USA
{acvogel,cgpotts,jurafsky}@stanford.edu
Abstract
Conversational implicatures involve rea-
soning about multiply nested belief struc-
tures. This complexity poses significant
challenges for computational models of
conversation and cognition. We show that
agents in the multi-agent Decentralized-
POMDP reach implicature-rich interpreta-
tions simply as a by-product of the way
they reason about each other to maxi-
mize joint utility. Our simulations involve
a reference game of the sort studied in
psychology and linguistics as well as a
dynamic, interactional scenario involving
implemented artificial agents.
1 Introduction
Gricean conversational implicatures (Grice, 1975)
are inferences that listeners make in order to
reconcile the speaker?s linguistic behavior with
the assumption that the speaker is cooperative.
As Grice conceived of them, implicatures cru-
cially involve reasoning about multiply-nested be-
lief structures: roughly, for p to count as an impli-
cature, the speaker must believe that the listener
will infer that the speaker believes p. This com-
plexity makes implicatures an important testing
ground for models of conversation and cognition.
Implicatures have received considerable atten-
tion in the context of simple reference games in
which the listener uses the speaker?s utterance
to try to identify the speaker?s intended referent
(Rosenberg and Cohen, 1964; Clark and Wilkes-
Gibbs, 1986; Dale and Reiter, 1995; DeVault and
Stone, 2007; Krahmer and van Deemter, 2012).
Many implicature patterns can be embedded in
these games using specific combinations of poten-
tial referents and message sets. The paradigm has
proven fruitful not only for evaluating computa-
tional models (Golland et al, 2010; Degen and
Franke, 2012; Frank and Goodman, 2012; Rohde
et al, 2012; Bergen et al, 2012) but also for study-
ing children?s pragmatic abilities without implic-
itly assuming they have mastered challenging lin-
guistic structures (Stiller et al, 2011).
In this paper, we extend these results beyond
simple reference games to full decision-problems
in which the agents reason about language and ac-
tion together over time. To do this, we use the De-
centralized Partially Observable Markov Decision
Process (Dec-POMDP) to implement agents that
are capable of manipulating the multiply-nested
belief structures required for implicature calcula-
tion. Optimal decision making in Dec-POMDPs
is NEXP complete, so we employ the single-agent
POMDP approximation of Vogel et al (2013).
We show that agents in the Dec-POMDP reach
implicature-rich interpretations simply as a by-
product of the way they reason about each other
to maximize joint utility. Our simulations involve
a reference game and a dynamic, interactional sce-
nario involving implemented artificial agents.
2 Decision-Theoretic Communication
The Decentralized Partially Observable Markov
Decision Process (Dec-POMDP) (Bernstein et
al., 2002) is a multi-agent generalization of the
POMDP, where agents act to maximize a shared
utility function. Formally, a Dec-POMDP con-
sists of a tuple (S,A,O,R, T,?, b0, ?). S is a
finite set of states, A is the set of actions, O is
the set of observations, and T (s?|a1, a2, s) is the
transition distribution which determines what ef-
fect the joint action (a1, a2) has on the state of the
world. The true state s ? S is not observable to
the agents, who must utilize observations o ? O,
which are emitted after each action according to
the observation distribution ?(o1, o2|s?, a). The
reward functionR(s, a1, a2) represents the goal of
the agents, who act to maximize expected reward.
Lastly, b0 ? ?(S) is the initial belief state and
74
? ? [0, 1) is the discount factor.
The true state of the world s ? S is not ob-
servable to either agent. In single-agent POMDPs,
agents maintain a belief state b(s) ? ?(S), which
is a distribution over states. Agents acting in Dec-
POMDPs must take into account not only their
beliefs about the state of the world, but also the
beliefs of their partners, leading to nested belief
states. In the model presented here, our agent
models the other agent?s beliefs about the state of
the world, and assumes that the other agent does
not take into account our own beliefs, a common
approach (Gmytrasiewicz and Doshi, 2005).
Agents make decisions according to
a policy pii : ?(S) ? A which max-
imizes the discounted expected reward??
t=0 ?tE[R(st, at1, at2)|b0, pi1, pi2]. Using
the assumption that the other agent tracks one less
level of belief, we can solve for the other agent?s
policy p?i, which allows us to estimate his actions
and beliefs over time. To construct policies,
we use Perseus (Spaan and Vlassis, 2005), a
point-based value iteration algorithm.
Even tracking just one level of nested beliefs
quickly leads to a combinatorial explosion in the
number of belief states the other agent might have.
This causes decision making in Dec-POMDPs to
be NEXP complete, limiting their application to
problems with only a handful of states (Bernstein
et al, 2002). To ameliorate this difficulty, we
use the method of Vogel et al (2013), which cre-
ates a single-agent approximation to the full Dec-
POMDP. To form this single-agent POMDP, we
augment the state space to be S ? S, where the
second set of state variables allows us to model
the other agent?s beliefs. We maintain a point
estimate b? of the other agent?s beliefs, which
is formed by summing out observations O that
the other player might have received. To ac-
complish this, we factor the transition distribu-
tion into two terms: T ((s?, s??)|a, p?i(s?), (s, s?)) =
T? (s??|s?, a, p?i(s?), (s, s?))T (s?|a, p?i(s?), (s, s?)). This
observation marginalization can be folded into the
transition distribution T? (s??|s?, a, p?i(s?), (s, s?)):
T? (s??| s?, a, p?i(s?), (s, s?)) = Pr(s??|s?, a, p?i(s?), (s, s?))
=
?
o??O
( ?(o?|s??, a, p?i(s?))T (s??|a, p?i(s?), s?)?
s??? ?(o?|s???, a, p?i(s?))T (s???|a, p?i(s?), s?)
? ?(o?|s?, a, p?i(s?))
)
(1)
Communication is treated as another type of ob-
servation, with messages coming from a finite set
M . Each message m ? M has the semantics
Pr(s|m), which represents the probability that the
world is in state s ? S given that m is true. Mes-
sages m received from a partner are combined
with perceptual observations o ? O, to form a
joint observation (m, o).
A literal listener, denoted L, interprets mes-
sages according to this semantics, without taking
into account the beliefs of the speaker. L assumes
that the perceptual observations and messages are
conditionally independent given the state of the
world. Using Bayes? rule, the literal listener?s joint
observation/message distribution is
Pr((o,m)|s, s?, a) = ?(o|s?, a) Pr(m|s)
= ?(o|s?, a) Pr(s|m) Pr(m)?
m??M Pr(s|m?) Pr(m?)
(2)
The Pr(m) prior over messages can be estimated
from corpus data, but we use a uniform prior for
simplicity.
A literal speaker, denoted S, produces mes-
sages according to the most descriptive term:
piS(s) = arg max
m?M
p(s|m). (3)
The literal speaker does not model the beliefs of
the listener.
To interpret implicatures, a level-one lis-
tener, denoted L(S), models the beliefs a literal
speaker must have had to produce an utterance:
Pr(m|s) = 1[p?iS(s) = m], where p?iS is the level-
one listener?s estimate of the speaker?s policy. In
this setting, we denote the level-one listener?s es-
timate of the speaker?s belief as s?, yielding the be-
lief update equation
Pr((o,m)|(s, s?), (s?, s??), a, p?iS(s?)) =
?(o|s?, a)1[p?iS(s?) = m] (4)
The literal semantics of messages is not explicitly
included in the level-one listener?s belief update.
Instead, when he solves for the literal speaker?s
policy p?iS , the meaning of a message is the set of
beliefs that would lead the literal speaker to pro-
duce the utterance.
A level-one speaker, S(L), produces utterances
to influence a literal listener, and a level-two lis-
tener, L(S(L)), uses two levels of belief nesting to
interpret utterances as the beliefs that a level-one
speaker might have to produce that utterance. At
each level of nesting, we apply the marginalized
75
r1 0 0 1
r2 0 1 1
r3 1 1 0
hat
glasses
mustache
r1 r2 r3
(a) Scenario.
Message r1 r2 r3
moustache 12 12 0
glasses 0 12 12
hat 0 0 1
(b) Literal interpretations.
Message r1 r2 r3
moustache 1 0 0
glasses 0 1 0
hat 0 0 1
(c) Implicature-rich interpretations.
Figure 1: A simple reference game. The matrices
give distributions Pr(t = ri|utterance)
belief-state approach of (Vogel et al, 2013), aug-
menting the state space with another copy of the
underlying world state space, where the new copy
represents the next level of belief. For instance, the
L(S(L)) agent will make decisions in the S?S?S
space. For an L(S(L)) state (s, s?, s?), s is the true
state of the world, s? is the speaker?s belief of the
state of the world, and s? is the speaker?s belief of
the listener?s beliefs. In the next two sections we
show how a level-one and level-two listener infer
implicatures.
3 Reference Game Implicatures
Fig. 1a is the scenario for a reference game of the
sort pioneered by Rosenberg and Cohen (1964)
and Dale and Reiter (1995). The potential refer-
ents are r1, r2, and r3. Speakers use a restricted
vocabulary consisting of three messages: ?mous-
tache?, ?glasses?, and ?hat?. The speaker is as-
signed a referent ri (hidden from the listener) and
produces a message on that basis. The speaker and
listener share the goal of having the listener iden-
tify the speaker?s intended referent ri.
Fig. 1b depicts the literal interpretations for
this game. It looks like the listener?s chances
of success are low. Only ?hat? refers unambigu-
ously. However, the language and scenario fa-
cilitate scalar implicature (Horn, 1972; Harnish,
1979; Gazdar, 1979). Briefly, the scalar implica-
ture pattern is that a speaker who is knowledgeable
about the relevant domain will choose a commu-
nicatively weak utterance U over a communica-
tively stronger utterance U ? iff U ? is false (assum-
ing U and U ? are relevant). The required sense of
communicative strength encompasses logical en-
tailments as well as more particularized pragmatic
partial orders (Hirschberg, 1985).
In our scenario, ?hat? is stronger than ?glasses?:
the referents wearing a hat are a proper subset
of those wearing glasses. Thus, given the play-
ers? goal, if the speaker says ?glasses?, the lis-
tener should draw the scalar implicature that ?hat?
is false. Thus, ?glasses? comes to unambiguously
refer to r2 (Fig. 1c, line 2). Similarly, though
?moustache? and ?glasses? do not literally stand in
the specific?general relationship needed for scalar
implicature, they do with ?glasses? pragmatically
associated with r2 (Fig. 1c, line 1).
Our implementation of these games as Dec-
POMDPs mirrors their intuitive description and
their treatment in iterated best response models
(Ja?ger, 2007; Ja?ger, 2012; Franke, 2009; Frank
and Goodman, 2012). The state space S encodes
the attributes of the referents (e.g., hat(r2) = T,
glasses(r1) = F) and includes a target variable t
identifying the speaker?s referent (hidden from the
listener). The speaker has three speech actions,
identified with the three messages. The listener
has four actions: ?listen? plus a ?choose? action ci
for each referent ri. The set of observations O is
just the set of messages (construed as utterances).
The agents receive a positive reward iff the listener
action ci corresponds to the speaker?s target t. Be-
cause this is a one-step reference game, the transi-
tion distribution T is the identity distribution.
The literal listener L interprets utterances as
a truth-conditional speaker would produce them
(Fig. 1b). The level-one speaker S(L) augments
the state space with a variable ?listener target? and
models L?s beliefs b? using the approximate meth-
ods of Sec. 2. Crucially, the optimal speaker pol-
icy piS(L) is such that piS(L)(t=r3) = ?hat? and
piS(L)(t=r1) = ?moustache?. The level-two lis-
tener L(S(L)) models S(L) via an estimate of the
?listener target? variable. For each speech action
m, L(S(L)) considers all values of t and the likeli-
76
hood that S(L) would have produced m:
Pr(t=ri|m) ? 1[p?iS(L)(t=ri) = m]
Since S(L) uses ?hat? to describe r3 and
?moustache? to describe r1, L(S(L)) correctly in-
fers that ?glasses? refers to r2, completing Fig. 1c?s
full implicature-rich pattern of mutual exclusivity
(Clark, 1987; Frank et al, 2009).
This basic pattern is robustly attested empiri-
cally in human data. The experimental data are,
of course, invariably less crisp than our idealized
model predicts, but many important sources of
variation could be brought into our model, with
the addition of strong salience priors (Frank and
Goodman, 2012; Stiller et al, 2011), assumptions
about bounded rationality (Camerer et al, 2004;
Franke, 2009), and a ?soft-max? view of the lis-
tener (Frank et al, 2009).
4 Cards World Implicatures
The Cards corpus1 contains 1266 metadata-rich
transcripts from a two-player chat-based game.
The world is a simple maze in which a deck of
cards has been distributed. The players? goal is to
find specific subsets of the cards, subject to a vari-
ety of constraints on what they can see and do. The
Dec-POMDP-based agents of Vogel et al (2013)
play a simplified version in which the goal is to be
co-located with a single card. Vogel et al show
that their agents? linguistic behavior is broadly
Gricean. However, their agents? language is too
simple to reveal implicatures. The present section
remedies this shortcoming. Implicature-rich inter-
pretations are an immediate consequence.
We implement the simplified Cards tasks as fol-
lows. The state space S is composed of the loca-
tion of each player and the location of the card.
The transition distribution T (s?|s, a1, a2) encodes
the outcome of movement actions. Agents receive
one of two sensor observations, indicating whether
the card is at their current location. The players are
rewarded when they are both located on the card.
Each player begins knowing his own location, but
not the location of the other player nor of the card.
The players have four movement actions (?up?,
?down?, ?left?, ?right?) and nine speech actions in-
terpreted as identifying card locations. Fig. 2 de-
picts these utterances as a partial order determined
by entailment. These general-to-specific relation-
1http://cardscorpus.christopherpotts.net
top right top left bottom right bottom left
top right left bottom middle
,,
,,
\\
\\
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 499?504,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Generating Recommendation Dialogs by Extracting Information from
User Reviews
Kevin Reschke, Adam Vogel, and Dan Jurafsky
Stanford University
Stanford, CA, USA
{kreschke,acvogel,jurafsky}@stanford.edu
Abstract
Recommendation dialog systems help
users navigate e-commerce listings by ask-
ing questions about users? preferences to-
ward relevant domain attributes. We
present a framework for generating and
ranking fine-grained, highly relevant ques-
tions from user-generated reviews. We
demonstrate our approach on a new dataset
just released by Yelp, and release a new
sentiment lexicon with 1329 adjectives for
the restaurant domain.
1 Introduction
Recommendation dialog systems have been devel-
oped for a number of tasks ranging from product
search to restaurant recommendation (Chai et al,
2002; Thompson et al, 2004; Bridge et al, 2005;
Young et al, 2010). These systems learn user re-
quirements through spoken or text-based dialog,
asking questions about particular attributes to fil-
ter the space of relevant documents.
Traditionally, these systems draw questions
from a small, fixed set of attributes, such as cuisine
or price in the restaurant domain. However, these
systems overlook an important element in users?
interactions with online product listings: user-
generated reviews. Huang et al (2012) show that
information extracted from user reviews greatly
improves user experience in visual search inter-
faces. In this paper, we present a dialog-based in-
terface that takes advantage of review texts. We
demonstrate our system on a new challenge cor-
pus of 11,537 businesses and 229,907 user reviews
released by the popular review website Yelp1, fo-
cusing on the dataset?s 4724 restaurants and bars
(164,106 reviews).
This paper makes two main contributions. First,
we describe and qualitatively evaluate a frame-
1https://www.yelp.com/dataset_challenge/
work for generating new, highly-relevant ques-
tions from user review texts. The framework
makes use of techniques from topic modeling and
sentiment-based aspect extraction to identify fine-
grained attributes for each business. These at-
tributes form the basis of a new set of questions
that the system can ask the user.
Second, we use a method based on information-
gain for dynamically ranking candidate questions
during dialog production. This allows our system
to select the most informative question at each di-
alog step. An evaluation based on simulated di-
alogs shows that both the ranking method and the
automatically generated questions improve recall.
2 Generating Questions from Reviews
2.1 Subcategory Questions
Yelp provides each business with category labels
for top-level cuisine types like Japanese, Coffee
& Tea, and Vegetarian. Many of these top-level
categories have natural subcategories (e.g., ramen
vs. sushi). By identifying these subcategories, we
enable questions which probe one step deeper than
the top-level category label.
To identify these subcategories, we run Latent
Dirichlet Analysis (LDA) (Blei et al, 2003) on
the reviews of each set of businesses in the twenty
most common top-level categories, using 10 top-
ics and concatenating all of a business?s reviews
into one document.2 Several researchers have used
sentence-level documents to model topics in re-
views, but these tend to generate topics about fine-
grained aspects of the sort we discuss in Section
2.2 (Jo and Oh, 2011; Brody and Elhadad, 2010).
We then manually labeled the topics, discarding
junk topics and merging similar topics. Table 1
displays sample extracted subcategories.
Using these topic models, we assign a business
2We use the Topic Modeling Toolkit implementation:
http://nlp.stanford.edu/software/tmt
499
Category Topic Label Top Words
pizza crust sauce pizza garlic sausage slice salad
Italian traditional pasta sauce delicious ravioli veal dishes gnocchi
bistro bruschetta patio salad valet delicious brie panini
deli sandwich deli salad pasta delicious grocery meatball
brew pub beers peaks ale brewery patio ipa brew
grill steak salad delicious sliders ribs tots drinks
bar drinks vig bartender patio uptown dive karaoke
American (New) bistro drinks pretzel salad fondue patio sanwich windsor
brunch sandwich brunch salad delicious pancakes patio
burger burger fries sauce beef potato sandwich delicious
mediterranean pita hummus jungle salad delicious mediterranean wrap
italian deli sandwich meats cannoli cheeses authentic sausage
new york deli beef sandwich pastrami corned fries waitress
Delis bagels bagel sandwiches toasted lox delicious donuts yummy
mediterranean pita lemonade falafel hummus delicious salad bakery
sandwiches sandwich subs sauce beef tasty meats delicious
sushi sushi kyoto zen rolls tuna sashimi spicy
Japanese teppanyaki sapporo chef teppanyaki sushi drinks shrimp fried
teriyaki teriyaki sauce beef bowls veggies spicy grill
ramen noodles udon dishes blossom delicious soup ramen
Table 1: A sample of subcategory topics with hand-labels and top words.
to a subcategory based on the topic with high-
est probability in that business?s topic distribution.
Finally, we use these subcategory topics to gen-
erate questions for our recommender dialog sys-
tem. Each top-level category corresponds to a sin-
gle question whose potential answers are the set of
subcategories: e.g., ?What type of Japanese cui-
sine do you want??
2.2 Questions from Fine-Grained Aspects
Our second source for questions is based on as-
pect extraction in sentiment summarization (Blair-
Goldensohn et al, 2008; Brody and Elhadad,
2010). We define an aspect as any noun-phrase
which is targeted by a sentiment predicate. For
example, from the sentence ?The place had great
atmosphere, but the service was slow.? we ex-
tract two aspects: +atmosphere and ?service.
Our aspect extraction system has two steps.
First we develop a domain specific sentiment lex-
icon. Second, we apply syntactic patterns to iden-
tify NPs targeted by these sentiment predicates.
2.2.1 Sentiment Lexicon
Coordination Graph We generate a list of
domain-specific sentiment adjectives using graph
propagation. We begin with a seed set combin-
ing PARADIGM+ (Jo and Oh, 2011) with ?strongly
subjective? adjectives from the OpinionFinder lex-
icon (Wilson et al, 2005), yielding 1342 seeds.
Like Brody and Elhadad (2010), we then construct
a coordination graph that links adjectives modify-
ing the same noun, but to increase precision we
require that the adjectives also be conjoined by
and (Hatzivassiloglou and McKeown, 1997). This
reduces problems like propagating positive sen-
timent to orange in good orange chicken. We
marked adjectives that follow too or lie in the
scope of negation with special prefixes and treated
them as distinct lexical entries.
Sentiment Propagation Negative and positive
seeds are assigned values of 0 and 1 respectively.
All other adjectives begin at 0.5. Then a stan-
dard propagation update is computed iteratively
(see Eq. 3 of Brody and Elhadad (2010)).
In Brody and Elhadad?s implementation of this
propagation method, seed sentiment values are
fixed, and the update step is repeated until the non-
seed values converge. We found that three modifi-
cations significantly improved precision. First, we
omit candidate nodes that don?t link to at least two
positive or two negative seeds. This eliminated
spurious propagation caused by one-off parsing er-
rors. Second, we run the propagation algorithm for
fewer iterations (two iterations for negative terms
and one for positive terms). We found that addi-
tional iterations led to significant error propaga-
tion when neutral (italian) or ambiguous (thick)
terms were assigned sentiment.3 Third, we update
both non-seed and seed adjectives. This allows us
to learn, for example, that the negative seed deca-
dent is positive in the restaurant domain.
Table 2 shows a sample of sentiment adjectives
3Our results are consistent with the recent finding of Whit-
ney and Sarkar (2012) that cautious systems are better when
bootstrapping from seeds.
500
Negative Sentiment
institutional, underwhelming, not nice, burn-
tish, unidentifiable, inefficient, not attentive,
grotesque, confused, trashy, insufferable,
grandiose, not pleasant, timid, degrading,
laughable, under-seasoned, dismayed, torn
Positive Sentiment
decadent, satisfied, lovely, stupendous,
sizable, nutritious, intense, peaceful,
not expensive, elegant, rustic, fast, affordable,
efficient, congenial, rich, not too heavy,
wholesome, bustling, lush
Table 2: Sample of Learned Sentiment Adjectives
derived by this graph propagation method. The
final lexicon has 1329 adjectives4, including 853
terms not in the original seed set. The lexicon is
available for download.5
Evaluative Verbs In addition to this adjective
lexicon, we take 56 evaluative verbs such as love
and hate from admire-class VerbNet predicates
(Kipper-Schuler, 2005).
2.2.2 Extraction Patterns
To identify noun-phrases which are targeted by
predicates in our sentiment lexicon, we develop
hand-crafted extraction patterns defined over syn-
tactic dependency parses (Blair-Goldensohn et al,
2008; Somasundaran and Wiebe, 2009) generated
by the Stanford parser (Klein and Manning, 2003).
Table 3 shows a sample of the aspects generated by
these methods.
Adj + NP It is common practice to extract any
NP modified by a sentiment adjective. However,
this simple extraction rule suffers from precision
problems. First, reviews often contain sentiment
toward irrelevant, non-business targets (Wayne is
the target of excellent job in (1)). Second, hypo-
thetical contexts lead to spurious extractions. In
(2), the extraction +service is clearly wrong?in
fact, the opposite sentiment is being expressed.
(1) Wayne did an excellent job addressing our
needs and giving us our options.
(2) Nice and airy atmosphere, but service could be
more attentive at times.
4We manually removed 26 spurious terms which were
caused by parsing errors or propagation to a neutral term.
5http://nlp.stanford.edu/projects/
yelp.shtml
We address these problems by filtering out sen-
tences in hypothetical contexts cued by if, should,
could, or a question mark, and by adopting the fol-
lowing, more conservative extractions rules:
i) [BIZ + have + adj. + NP] Sentiment adjec-
tive modifies NP, main verb is have, subject
is business name, it, they, place, or absent.
(E.g., This place has some really great yogurt
and toppings).
ii) [NP + be + adj.] Sentiment adjective linked
to NP by be?e.g., Our pizza was much too
jalapeno-y.
?Good For? + NP Next, we extract aspects us-
ing the pattern BIZ + positive adj. + for + NP, as in
It?s perfect for a date night. Examples of extracted
aspects include +lunch, +large groups, +drinks,
and +quick lunch.
Verb + NP Finally, we extract NPs that appear
as direct object to one of our evaluative verbs (e.g.,
We loved the fried chicken).
2.2.3 Aspects as Questions
We generate questions from these extracted as-
pects using simple templates. For example, the as-
pect +burritos yields the question: Do you want a
place with good burritos?
3 Question Selection for Dialog
To utilize the questions generated from reviews in
recommendation dialogs, we first formalize the di-
alog optimization task and then offer a solution.
3.1 Problem Statement
We consider a version of the Information Retrieval
Dialog task introduced by Kopec?ek (1999). Busi-
nesses b ? B have associated attributes, coming
from a set Att. These attributes are a combination
of Yelp categories and our automatically extracted
aspects described in Section 2. Attributes att ? Att
take values in a finite domain dom(att). We denote
the subset of businesses with an attribute att tak-
ing value val ? dom(att), as B|att=val. Attributes
are functions from businesses to subsets of values:
att : B ? P(dom(att)). We model a user in-
formation need I as a set of attribute/value pairs:
I = {(att1, val1), . . . , (att|I|, val|I|)}.
Given a set of businesses and attributes, a rec-
ommendation agent pi selects an attribute to ask
501
Chinese: Mexican:
+beef +egg roll +sour soup +orange chicken +salsa bar +burritos +fish tacos +guacamole
+noodles +crab puff +egg drop soup +enchiladas +hot sauce +carne asade +breakfast burritos
+dim sum +fried rice +honey chicken +horchata +green salsa +tortillas +quesadillas
Japanese: American (New)
+rolls +sushi rolls +wasabi +sushi bar +salmon +environment +drink menu +bar area +cocktails +brunch
+chicken katsu +crunch +green tea +sake selection +hummus +mac and cheese +outdoor patio +seating area
+oysters +drink menu +sushi selection +quality +lighting +brews +sangria +cheese plates
Table 3: Sample of the most frequent positive aspects extracted from review texts.
Input: Information need I
Set of businesses B
Set of attributes Att
Recommendation agent pi
Dialog length K
Output: Dialog history H
Recommended businesses B
Initialize dialog history H = ?
for step = 0; step < K; step++ do
Select an attribute: att = pi(B,H)
Query user for the answer: val = I(att)
Restrict set of businesses: B = B|att=val
Append answer: H = H ? {(att, val)}
end
Return (H,B)
Algorithm 1: Procedure for evaluating a recom-
mendation agent
the user about, then uses the answer value to nar-
row the set of businesses to those with the de-
sired attribute value, and selects another query.
Algorithm 1 presents this process more formally.
The recommendation agent can use both the set of
businesses B and the history of question and an-
swers H from the user to select the next query.
Thus, formally a recommendation agent is a func-
tion pi : B ? H ? Att. The dialog ends after a
fixed number of queries K.
3.2 Information Gain Agent
The information gain recommendation agent
chooses questions to ask the user by selecting
question attributes that maximize the entropy of
the resulting document set, in a manner similar to
decision tree learning (Mitchell, 1997). Formally,
we define a function infogain : Att? P(B)? R:
infogain(att, B) =
?
?
vals?P(dom(att))
|Batt=vals|
|B| log
|Batt=vals|
|B|
The agent then selects questions att ? Att that
maximize the information gain with respect to the
set of businesses satisfying the dialog history H:
pi(B,H) = argmax
att?Att
infogain(att, B|H)
4 Evaluation
4.1 Experimental Setup
We follow the standard approach of using the at-
tributes of an individual business as a simulation
of a user?s preferences (Chung, 2004; Young et al,
2010). For every business b ? B we form an in-
formation need composed of all of b?s attributes:
Ib =
?
{att?Att|att(b)6=?}
(att, att(b))
To evaluate a recommendation agent, we use
the recall metric, which measures how well an in-
formation need is satisfied. For each information
need I , let BI be the set of businesses that satisfy
the questions of an agent. We define the recall of
the set of businesses with respect to the informa-
tion need as
recall(BI , I) =
?
b?BI
?
(att,val)?I 1[val ? att(b)]
|BI ||I|
We average recall across all information needs,
yielding average recall.
We compare against a random agent baseline
that selects attributes att ? Att uniformly at ran-
dom at each time step. Other recommendation di-
alog systems such as Young et al (2010) select
questions from a small fixed hierarchy, which is
not applicable to our large set of attributes.
4.2 Results
Figure 1 shows the average recall for the ran-
dom agent versus the information gain agent with
varying sets of attributes. ?Top-level? repeatedly
queries the user?s top-level category preferences,
?Subtopic? additionally uses our topic modeling
subcategories, and ?All? uses these plus the as-
pects extracted from reviews. We see that for suf-
ficiently long dialogs, ?All? outperforms the other
systems. The ?Subtopic? and ?Top-level? systems
plateau after a few dialog steps once they?ve asked
502
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3  4  5  6  7  8  9  10
Ave
rag
e R
eca
ll
Dialog Length
Average Recall by Agent
RandomTop-levelSubtopicAll
Figure 1: Average recall for each agent.
all useful questions. For instance, most businesses
only have one or two top-level categories, so af-
ter the system has identified the top-level cate-
gory that the user is interested in, it has no more
good questions to ask. Note that the information
gain agent starts dialogs with the top-level and ap-
propriate subcategory questions, so it is only for
longer dialogs that the fine-grained aspects boost
performance.
Below we show a few sample output dialogs
from our ?All? information gain agent.
Q: What kind of place do you want?
A: American (New)
Q: What kind of American (New) do you want:
bar, bistro, standard, burgers, brew pub, or
brunch?
A: bistro
Q: Do you want a place with a good patio?
A: Yes
Q: What kind of place do you want?
A: Chinese
Q: What kind of Chinese place do you want:
buffet, dim sum, noodles, pan Asian, Panda
Express, sit down, or veggie?
A: sit down
Q: Do you want a place with a good lunch
special?
A: Yes
Q: What kind of place do you want?
A: Mexican
Q: What kind of Mexican place do you want:
dinner, taqueria, margarita bar, or tortas?
A: Margarita bar
Q: Do you want a place with a good patio?
A: Yes
5 Conclusion
We presented a system for extracting large sets
of attributes from user reviews and selecting rel-
evant attributes to ask questions about. Using
topic models to discover subtypes of businesses, a
domain-specific sentiment lexicon, and a number
of new techniques for increasing precision in sen-
timent aspect extraction yields attributes that give
a rich representation of the restaurant domain. We
have made this 1329-term sentiment lexicon for
the restaurant domain available as useful resource
to the community. Our information gain recom-
mendation agent gives a principled way to dynam-
ically combine these diverse attributes to ask rele-
vant questions in a coherent dialog. Our approach
thus offers a new way to integrate the advantages
of the curated hand-build attributes used in statisti-
cal slot and filler dialog systems, and the distribu-
tionally induced, highly relevant categories built
by sentiment aspect extraction systems.
6 Acknowledgments
Thanks to the anonymous reviewers and the Stan-
ford NLP group for helpful suggestions. The au-
thors also gratefully acknowledge the support of
the Nuance Foundation, the Defense Advanced
Research Projects Agency (DARPA) Deep Explo-
ration and Filtering of Text (DEFT) Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-13-2-0040, ONR grants
N00014-10-1-0109 and N00014-13-1-0287 and
ARO grant W911NF-07-1-0216, and the Center
for Advanced Study in the Behavioral Sciences.
References
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George A Reis, and Jeff Reynar.
2008. Building a sentiment summarizer for local
service reviews. In WWW Workshop on NLP in the
Information Explosion Era.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. The Journal of
Machine Learning Research, 3:993?1022.
Derek Bridge, Mehmet H. Go?ker, Lorraine McGinty,
and Barry Smyth. 2005. Case-based recom-
mender systems. Knowledge Engineering Review,
20(3):315?320.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
503
In Proceedings of HLT NAACL 2010, pages 804?
812.
Joyce Chai, Veronika Horvath, Nicolas Nicolov, Margo
Stys, A Kambhatla, Wlodek Zadrozny, and Prem
Melville. 2002. Natural language assistant - a di-
alog system for online product recommendation. AI
Magazine, 23:63?75.
Grace Chung. 2004. Developing a flexible spoken dia-
log system using simulation. In Proceedings of ACL
2004, pages 63?70.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of EACL 1997, pages 174?
181.
Jeff Huang, Oren Etzioni, Luke Zettlemoyer, Kevin
Clark, and Christian Lee. 2012. Revminer: An ex-
tractive interface for navigating reviews on a smart-
phone. In Proceedings of UIST 2012.
Yohan Jo and Alice H Oh. 2011. Aspect and sentiment
unification model for online review analysis. In Pro-
ceedings of the Fourth ACM International Confer-
ence on Web Search and Data Mining, pages 815?
824.
Karin Kipper-Schuler. 2005. Verbnet: A broad-
coverage, comprehensive verb lexicon.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings ACL
2003, pages 423?430.
I. Kopec?ek. 1999. Modeling of the information re-
trieval dialogue systems. In Proceedings of the
Workshop on Text, Speech and Dialogue-TSD 99,
Lectures Notes in Artificial Intelligence 1692, pages
302?307. Springer-Verlag.
Tom M. Mitchell. 1997. Machine Learning. McGraw-
Hill, New York.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of ACL 2009, pages 226?234.
Cynthia A. Thompson, Mehmet H. Goeker, and Pat
Langley. 2004. A personalized system for conver-
sational recommendations. Journal of Artificial In-
telligence Research (JAIR), 21:393?428.
Max Whitney and Anoop Sarkar. 2012. Bootstrapping
via graph propagation. In Proceedings of the ACL
2012, pages 620?628, Jeju Island, Korea.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005. Opinionfinder: A system for subjectivity
analysis. In Proceedings of HLT/EMNLP 2005 on
Interactive Demonstrations, pages 34?35.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174, April.
504
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 9?17,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Viterbi Training Improves Unsupervised Dependency Parsing
Valentin I. Spitkovsky
Computer Science Department
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc.
Mountain View, CA, 94043, USA
hiyan@google.com
Daniel Jurafsky and Christopher D. Manning
Departments of Linguistics and Computer Science
Stanford University, Stanford, CA, 94305, USA
jurafsky@stanford.edu and manning@cs.stanford.edu
Abstract
We show that Viterbi (or ?hard?) EM is
well-suited to unsupervised grammar in-
duction. It is more accurate than standard
inside-outside re-estimation (classic EM),
significantly faster, and simpler. Our ex-
periments with Klein and Manning?s De-
pendency Model with Valence (DMV) at-
tain state-of-the-art performance ? 44.8%
accuracy on Section 23 (all sentences) of
the Wall Street Journal corpus ? without
clever initialization; with a good initial-
izer, Viterbi training improves to 47.9%.
This generalizes to the Brown corpus,
our held-out set, where accuracy reaches
50.8% ? a 7.5% gain over previous best
results. We find that classic EM learns bet-
ter from short sentences but cannot cope
with longer ones, where Viterbi thrives.
However, we explain that both algorithms
optimize the wrong objectives and prove
that there are fundamental disconnects be-
tween the likelihoods of sentences, best
parses, and true parses, beyond the well-
established discrepancies between likeli-
hood, accuracy and extrinsic performance.
1 Introduction
Unsupervised learning is hard, often involving dif-
ficult objective functions. A typical approach is
to attempt maximizing the likelihood of unlabeled
data, in accordance with a probabilistic model.
Sadly, such functions are riddled with local op-
tima (Charniak, 1993, Ch. 7, inter alia), since their
number of peaks grows exponentially with in-
stances of hidden variables. Furthermore, a higher
likelihood does not always translate into superior
task-specific accuracy (Elworthy, 1994; Merialdo,
1994). Both complications are real, but we will
discuss perhaps more significant shortcomings.
We prove that learning can be error-prone even
in cases when likelihood is an appropriate mea-
sure of extrinsic performance and where global
optimization is feasible. This is because a key
challenge in unsupervised learning is that the de-
sired likelihood is unknown. Its absence renders
tasks like structure discovery inherently under-
constrained. Search-based algorithms adopt sur-
rogate metrics, gambling on convergence to the
?right? regularities in data. Their wrong objec-
tives create cases in which both efficiency and per-
formance improve when expensive exact learning
techniques are replaced by cheap approximations.
We propose using Viterbi training (Brown
et al, 1993), instead of inside-outside re-
estimation (Baker, 1979), to induce hierarchical
syntactic structure from natural language text. Our
experiments with Klein and Manning?s (2004) De-
pendency Model with Valence (DMV), a popular
state-of-the-art model (Headden et al, 2009; Co-
hen and Smith, 2009; Spitkovsky et al, 2009),
beat previous benchmark accuracies by 3.8% (on
Section 23 of WSJ) and 7.5% (on parsed Brown).
Since objective functions used in unsupervised
grammar induction are provably wrong, advan-
tages of exact inference may not apply. It makes
sense to try the Viterbi approximation ? it is also
wrong, only simpler and cheaper than classic EM.
As it turns out, Viterbi EM is not only faster but
also more accurate, consistent with hypotheses of
de Marcken (1995) and Spitkovsky et al (2009).
We begin by reviewing the model, standard data
sets and metrics, and our experimental results. Af-
ter relating our contributions to prior work, we
delve into proofs by construction, using the DMV.
9
Corpus Sentences POS Tokens Corpus Sentences POS Tokens
WSJ1 159 159 WSJ13 12,270 110,760
WSJ2 499 839 WSJ14 14,095 136,310
WSJ3 876 1,970 WSJ15 15,922 163,715
WSJ4 1,394 4,042 WSJ20 25,523 336,555
WSJ5 2,008 7,112 WSJ25 34,431 540,895
WSJ6 2,745 11,534 WSJ30 41,227 730,099
WSJ7 3,623 17,680 WSJ35 45,191 860,053
WSJ8 4,730 26,536 WSJ40 47,385 942,801
WSJ9 5,938 37,408 WSJ45 48,418 986,830
WSJ10 7,422 52,248 WSJ100 49,206 1,028,054
WSJ11 8,856 68,022 Section 23 2,353 48,201
WSJ12 10,500 87,750 Brown100 24,208 391,796 5 10 15 20 25 30 35 40 45
5
10
15
20
25
30
35
40
45
Thousands
of Sentences
Thousands
of Tokens 100
200
300
400
500
600
700
800
900
WSJk
Figure 1: Sizes of WSJ{1, . . . , 45, 100}, Section 23 of WSJ? and Brown100 (Spitkovsky et al, 2009).
NNS VBD IN NN ?
Payrolls fell in September .
P = (1?
0
z }| {
PSTOP(?, L, T)) ? PATTACH(?, L, VBD)
? (1? PSTOP(VBD, L, T)) ? PATTACH(VBD, L, NNS)
? (1? PSTOP(VBD, R, T)) ? PATTACH(VBD, R, IN)
? (1? PSTOP(IN, R, T)) ? PATTACH(IN, R, NN)
? PSTOP(VBD, L, F) ? PSTOP(VBD, R, F)
? PSTOP(NNS, L, T) ? PSTOP(NNS, R, T)
? PSTOP(IN, L, T) ? PSTOP(IN, R, F)
? PSTOP(NN, L, T) ? PSTOP(NN, R, T)
? PSTOP(?, L, F)
| {z }
1
? PSTOP(?, R, T)
| {z }
1
.
Figure 2: A dependency structure for a short sen-
tence and its probability, as factored by the DMV,
after summing out PORDER (Spitkovsky et al, 2009).
2 Dependency Model with Valence
The DMV (Klein and Manning, 2004) is a single-
state head automata model (Alshawi, 1996) over
lexical word classes {cw} ? POS tags. Its gener-
ative story for a sub-tree rooted at a head (of class
ch) rests on three types of independent decisions:
(i) initial direction dir ? {L, R} in which to attach
children, via probability PORDER(ch); (ii) whether to
seal dir, stopping with probability PSTOP(ch, dir, adj),
conditioned on adj ? {T, F} (true iff considering
dir?s first, i.e., adjacent, child); and (iii) attach-
ments (of class ca), according to PATTACH(ch, dir, ca).
This produces only projective trees. A root token
? generates the head of a sentence as its left (and
only) child. Figure 2 displays a simple example.
The DMV lends itself to unsupervised learn-
ing via inside-outside re-estimation (Baker, 1979).
Viterbi training (Brown et al, 1993) re-estimates
each next model as if supervised by the previous
best parse trees. And supervised learning from
reference parse trees is straight-forward, since
maximum-likelihood estimation reduces to count-
ing: P?ATTACH(ch, dir, ca) is the fraction of children ?
those of class ca ? attached on the dir side of a
head of class ch; P?STOP(ch, dir, adj = T), the frac-
tion of words of class ch with no children on the
dir side; and P?STOP(ch, dir, adj = F), the ratio1 of the
number of words of class ch having a child on the
dir side to their total number of such children.
3 Standard Data Sets and Evaluation
The DMV is traditionally trained and tested on
customized subsets of Penn English Treebank?s
Wall Street Journal portion (Marcus et al, 1993).
Following Klein and Manning (2004), we be-
gin with reference constituent parses and com-
pare against deterministically derived dependen-
cies: after pruning out all empty sub-trees, punc-
tuation and terminals (tagged # and $) not pro-
nounced where they appear, we drop all sentences
with more than a prescribed number of tokens
remaining and use automatic ?head-percolation?
rules (Collins, 1999) to convert the rest, as is stan-
dard practice. We experiment with WSJk (sen-
tences with at most k tokens), for 1 ? k ? 45, and
Section 23 of WSJ? (all sentence lengths). We
also evaluate on Brown100, similarly derived from
the parsed portion of the Brown corpus (Francis
and Kucera, 1979), as our held-out set. Figure 1
shows these corpora?s sentence and token counts.
Proposed parse trees are judged on accuracy: a
directed score is simply the overall fraction of cor-
rectly guessed dependencies. Let S be a set of
sentences, with |s| the number of terminals (to-
1The expected number of trials needed to get one
Bernoulli(p) success is n ? Geometric(p), with n ? Z+,
P(n) = (1 ? p)n?1p and E(n) = p?1; MoM and MLE
agree, p? = (# of successes)/(# of trials).
10
5 10 15 20 25 30 35 40
10
20
30
40
50
60
70
OracleAd-Hoc?
Uninformed
WSJk
D
irected
D
ep
end
en
cy
A
ccu
racy
o
n
W
SJ40
(a) %-Accuracy for Inside-Outside (Soft EM)
5 10 15 20 25 30 35 40
10
20
30
40
50
60
70
Oracle
Ad-Hoc? Uninformed
WSJk
(training on all WSJ sentences up to k tokens in length)
D
irected
D
ep
end
en
cy
A
ccu
racy
o
n
W
SJ40
(b) %-Accuracy for Viterbi (Hard EM)
5 10 15 20 25 30 35 40
50
100
150
200
350
400
Oracle
Ad-Hoc?
Uninformed
WSJk
Iteratio
n
s
to
C
o
nv
erg
en
ce
(c) Iterations for Inside-Outside (Soft EM)
5 10 15 20 25 30 35 40
50
100
150
200
Oracle
Ad-Hoc?
Uninformed
WSJk
Iteratio
n
s
to
C
o
nv
erg
en
ce
(d) Iterations for Viterbi (Hard EM)
Figure 3: Directed dependency accuracies attained by the DMV, when trained on WSJk, smoothed, then
tested against a fixed evaluation set, WSJ40, for three different initialization strategies (Spitkovsky et al,
2009). Red, green and blue graphs represent the supervised (maximum-likelihood oracle) initialization,
a linguistically-biased initializer (Ad-Hoc?) and the uninformed (uniform) prior. Panel (b) shows results
obtained with Viterbi training instead of classic EM ? Panel (a), but is otherwise identical (in both, each
of the 45 vertical slices captures five new experimental results and arrows connect starting performance
with final accuracy, emphasizing the impact of learning). Panels (c) and (d) show the corresponding
numbers of iterations until EM?s convergence.
kens) for each s ? S. Denote by T (s) the set
of all dependency parse trees of s, and let ti(s)
stand for the parent of token i, 1 ? i ? |s|, in
t(s) ? T (s). Call the gold reference t?(s) ? T (s).
For a given model of grammar, parameterized by
?, let t??(s) ? T (s) be a (not necessarily unique)
likeliest (also known as Viterbi) parse of s:
t??(s) ?
{
arg max
t?T (s)
P?(t)
}
;
then ??s directed accuracy on a reference set R is
100% ?
?
s?R
?|s|
i=1 1{t??i (s)=t?i (s)}?
s?R |s|
.
4 Experimental Setup and Results
Following Spitkovsky et al (2009), we trained the
DMV on data sets WSJ{1, . . . , 45} using three ini-
tialization strategies: (i) the uninformed uniform
prior; (ii) a linguistically-biased initializer, Ad-
Hoc?;2 and (iii) an oracle ? the supervised MLE
solution. Standard training is without smoothing,
iterating each run until successive changes in over-
all per-token cross-entropy drop below 2?20 bits.
We re-trained all models using Viterbi EM
instead of inside-outside re-estimation, explored
Laplace (add-one) smoothing during training, and
experimented with hybrid initialization strategies.
2Ad-Hoc? is Spitkovsky et al?s (2009) variation on Klein
and Manning?s (2004) ?ad-hoc harmonic? completion.
11
5 10 15 20 25 30 35 40
10
20
30
40
50
60
70
OracleAd-Hoc?
Uninformed
Baby Steps
WSJk
D
irected
D
ep
end
en
cy
A
ccu
racy
o
n
W
SJ40
(a) %-Accuracy for Inside-Outside (Soft EM)
5 10 15 20 25 30 35 40
10
20
30
40
50
60
70
Oracle
Ad-Hoc? Uninformed
Baby Steps
WSJk
D
irected
D
ep
end
en
cy
A
ccu
racy
o
n
W
SJ40
(b) %-Accuracy for Viterbi (Hard EM)
Figure 4: Superimposes directed accuracies attained by DMV models trained with Laplace smoothing
(brightly-colored curves) over Figure 3(a,b); violet curves represent Baby Steps (Spitkovsky et al, 2009).
4.1 Result #1: Viterbi-Trained Models
The results of Spitkovsky et al (2009), tested
against WSJ40, are re-printed in Figure 3(a); our
corresponding Viterbi runs appear in Figure 3(b).
We observe crucial differences between the two
training modes for each of the three initialization
strategies. Both algorithms walk away from the
supervised maximum-likelihood solution; how-
ever, Viterbi EM loses at most a few points of
accuracy (3.7% at WSJ40), whereas classic EM
drops nearly twenty points (19.1% at WSJ45). In
both cases, the single best unsupervised result is
with good initialization, although Viterbi peaks
earlier (45.9% at WSJ8) and in a narrower range
(WSJ8-9) than classic EM (44.3% at WSJ15;
WSJ13-20). The uniform prior never quite gets off
the ground with classic EM but manages quite well
under Viterbi training,3 given sufficient data ? it
even beats the ?clever? initializer everywhere past
WSJ10. The ?sweet spot? at WSJ15 ? a neigh-
borhood where both Ad-Hoc? and the oracle ex-
cel under classic EM ? disappears with Viterbi.
Furthermore, Viterbi does not degrade with more
(complex) data, except with a biased initializer.
More than a simple efficiency hack, Viterbi EM
actually improves performance. And its benefits to
running times are also non-trivial: it not only skips
computing the outside charts in every iteration but
also converges (sometimes an order of magnitude)
3In a concurrently published related work, Cohen and
Smith (2010) prove that the uniform-at-random initializer is a
competitive starting M-step for Viterbi EM; our uninformed
prior consists of uniform multinomials, seeding the E-step.
faster than classic EM (see Figure 3(c,d)).4
4.2 Result #2: Smoothed Models
Smoothing rarely helps classic EM and hurts in
the case of oracle training (see Figure 4(a)). With
Viterbi, supervised initialization suffers much less,
the biased initializer is a wash, and the uninformed
uniform prior generally gains a few points of ac-
curacy, e.g., up 2.9% (from 42.4% to 45.2%, eval-
uated against WSJ40) at WSJ15 (see Figure 4(b)).
Baby Steps (Spitkovsky et al, 2009) ? iterative
re-training with increasingly more complex data
sets, WSJ1, . . . ,WSJ45 ? using smoothed Viterbi
training fails miserably (see Figure 4(b)), due to
Viterbi?s poor initial performance at short sen-
tences (possibly because of data sparsity and sen-
sitivity to non-sentences ? see examples in ?7.3).
4.3 Result #3: State-of-the-Art Models
Simply training up smoothed Viterbi at WSJ15,
using the uninformed uniform prior, yields 44.8%
accuracy on Section 23 of WSJ?, already beating
previous state-of-the-art by 0.7% (see Table 1(A)).
Since both classic EM and Ad-Hoc? initializers
work well with short sentences (see Figure 3(a)),
it makes sense to use their pre-trained models to
initialize Viterbi training, mixing the two strate-
gies. We judged all Ad-Hoc? initializers against
WSJ15 and found that the one for WSJ8 mini-
mizes sentence-level cross-entropy (see Figure 5).
This approach does not involve reference parse
4For classic EM, the number of iterations to convergence
appears sometimes inversely related to performance, giving
credence to the notion of early termination as a regularizer.
12
Model Incarnation WSJ10 WSJ20 WSJ?
DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100
Less is More (Ad-Hoc? @15) (Spitkovsky et al, 2009) 56.2 48.2 44.1 43.3
A. Smoothed Viterbi Training (@15), Initialized with the Uniform Prior 59.9 50.0 44.8 48.1
B. A Good Initializer (Ad-Hoc??s @8), Classically Pre-Trained (@15) 63.8 52.3 46.2 49.3
C. Smoothed Viterbi Training (@15), Initialized with B 64.4 53.5 47.8 50.5
D. Smoothed Viterbi Training (@45), Initialized with C 65.3 53.8 47.9 50.8
EVG Smoothed (skip-head), Lexicalized (Headden et al, 2009) 68.8
Table 1: Accuracies on Section 23 of WSJ{10, 20,? } and Brown100 for three recent state-of-the-art
systems, our initializer, and smoothed Viterbi-trained runs that employ different initialization strategies.
5 10 15 20 25 30 35 40 45
4.5
5.0
5.5
WSJk
bpt
lowest cross-entropy (4.32bpt) attained at WSJ8
x-Entropy h (in bits per token) on WSJ15
Figure 5: Sentence-level cross-entropy on WSJ15
for Ad-Hoc? initializers of WSJ{1, . . . , 45}.
trees and is therefore still unsupervised. Using the
Ad-Hoc? initializer based on WSJ8 to seed classic
training at WSJ15 yields a further 1.4% gain in ac-
curacy, scoring 46.2% on WSJ? (see Table 1(B)).
This good initializer boosts accuracy attained
by smoothed Viterbi at WSJ15 to 47.8% (see Ta-
ble 1(C)). Using its solution to re-initialize train-
ing at WSJ45 gives a tiny further improvement
(0.1%) on Section 23 of WSJ? but bigger gains
on WSJ10 (0.9%) and WSJ20 (see Table 1(D)).
Our results generalize. Gains due to smoothed
Viterbi training and favorable initialization carry
over to Brown100 ? accuracy improves by 7.5%
over previous published numbers (see Table 1).5
5 Discussion of Experimental Results
The DMV has no parameters to capture syntactic
relationships beyond local trees, e.g., agreement.
Spitkovsky et al (2009) suggest that classic EM
breaks down as sentences get longer precisely be-
cause the model makes unwarranted independence
assumptions. They hypothesize that the DMV re-
serves too much probability mass for what should
be unlikely productions. Since EM faithfully al-
locates such re-distributions across the possible
parse trees, once sentences grow sufficiently long,
this process begins to deplete what began as like-
lier structures. But medium lengths avoid a flood
of exponentially-confusing longer sentences (and
5In a sister paper, Spitkovsky et al (2010) improve perfor-
mance by incorporating parsing constraints harvested from
the web into Viterbi training; nevertheless, results presented
in this paper remain the best of models trained purely on WSJ.
the sparseness of unrepresentative shorter ones).6
Our experiments corroborate this hypothesis.
First of all, Viterbi manages to hang on to su-
pervised solutions much better than classic EM.
Second, Viterbi does not universally degrade with
more (complex) training sets, except with a biased
initializer. And third, Viterbi learns poorly from
small data sets of short sentences (WSJk, k < 5).
Viterbi may be better suited to unsupervised
grammar induction compared with classic EM, but
neither is sufficient, by itself. Both algorithms
abandon good solutions and make no guarantees
with respect to extrinsic performance. Unfortu-
nately, these two approaches share a deep flaw.
6 Related Work on Improper Objectives
It is well-known that maximizing likelihood may,
in fact, degrade accuracy (Pereira and Schabes,
1992; Elworthy, 1994; Merialdo, 1994). de Mar-
cken (1995) showed that classic EM suffers from
a fatal attraction towards deterministic grammars
and suggested a Viterbi training scheme as a rem-
edy. Liang and Klein?s (2008) analysis of errors
in unsupervised learning began with the inappro-
priateness of the likelihood objective (approxima-
tion), explored problems of data sparsity (estima-
tion) and focused on EM-specific issues related to
non-convexity (identifiability and optimization).
Previous literature primarily relied on experi-
mental evidence. de Marcken?s analytical result is
an exception but pertains only to EM-specific lo-
cal attractors. Our analysis confirms his intuitions
and moreover shows that there can be global pref-
erences for deterministic grammars ? problems
that would persist with tractable optimization. We
prove that there is a fundamental disconnect be-
tween objective functions even when likelihood is
a reasonable metric and training data are infinite.
6Klein and Manning (2004) originally trained the DMV
on WSJ10 and Gillenwater et al (2009) found it useful to dis-
card data from WSJ3, which is mostly incomplete sentences.
13
7 Proofs (by Construction)
There is a subtle distinction between three differ-
ent probability distributions that arise in parsing,
each of which can be legitimately termed ?likeli-
hood? ? the mass that a particular model assigns
to (i) highest-scoring (Viterbi) parse trees; (ii) the
correct (gold) reference trees; and (iii) the sen-
tence strings (sums over all derivations). A classic
unsupervised parser trains to optimize the third,
makes actual parsing decisions according to the
first, and is evaluated against the second. There
are several potential disconnects here. First of all,
the true generative model ?? may not yield the
largest margin separations for discriminating be-
tween gold parse trees and next best alternatives;
and second, ?? may assign sub-optimal mass to
string probabilities. There is no reason why an op-
timal estimate ?? should make the best parser or
coincide with a peak of an unsupervised objective.
7.1 The Three Likelihood Objectives
A supervised parser finds the ?best? parameters
?? by maximizing the likelihood of all reference
structures t?(s) ? the product, over all sentences,
of the probabilities that it assigns to each such tree:
??SUP = arg max
?
L(?) = arg max
?
?
s
P?(t?(s)).
For the DMV, this objective function is convex ?
its unique peak is easy to find and should match
the true distribution ?? given enough data, barring
practical problems caused by numerical instability
and inappropriate independence assumptions. It is
often easier to work in log-probability space:
??SUP = arg max? logL(?)
= arg max?
?
s log P?(t?(s)).
Cross-entropy, measured in bits per token (bpt),
offers an interpretable proxy for a model?s quality:
h(?) = ?
?
s lg P?(t?(s))?
s |s|
.
Clearly, arg max? L(?) = ??SUP = arg min? h(?).
Unsupervised parsers cannot rely on references
and attempt to jointly maximize the probability of
each sentence instead, summing over the probabil-
ities of all possible trees, according to a model ?:
??UNS = arg max
?
?
s
log
?
t?T (s)
P?(t)
? ?? ?
P?(s)
.
This objective function is not convex and in gen-
eral does not have a unique peak, so in practice one
usually settles for ??UNS ? a fixed point. There is no
reason why ??SUP should agree with ??UNS, which is
in turn (often badly) approximated by ??UNS, in our
case using EM. A logical alternative to maximiz-
ing the probability of sentences is to maximize the
probability of the most likely parse trees instead:7
??VIT = arg max
?
?
s
log P?(t??(s)).
This 1-best approximation similarly arrives at ??VIT,
with no claims of optimality. Each next model is
re-estimated as if supervised by reference parses.
7.2 A Warm-Up Case: Accuracy vs. ??SUP 6= ??
A simple way to derail accuracy is to maximize
the likelihood of an incorrect model, e.g., one that
makes false independence assumptions. Consider
fitting the DMV to a contrived distribution ? two
equiprobable structures over identical three-token
sentences from a unary vocabulary { a?}:
(i) x xa? a? a?; (ii) y ya? a? a?.
There are six tokens and only two have children
on any given side, so adjacent stopping MLEs are:
P?STOP( a?, L, T) = P?STOP( a?, R, T) = 1 ?
2
6 =
2
3 .
The rest of the estimated model is deterministic:
P?ATTACH(?, L, a?) = P?ATTACH( a?, ?, a?) = 1
and P?STOP( a?, ?, F) = 1,
since all dependents are a? and every one is an
only child. But the DMV generates left- and right-
attachments independently, allowing a third parse:
(iii) x ya? a? a?.
It also cannot capture the fact that all structures are
local (or that all dependency arcs point in the same
direction), admitting two additional parse trees:
(iv) a? xa? a?; (v) ya? a? a?.
Each possible structure must make four (out of six)
adjacent stops, incurring identical probabilities:
P?STOP( a?, ?, T)4 ? (1 ? P?STOP( a?, ?, T))2 =
24
36 .
7It is also possible to use k-best Viterbi, with k > 1.
14
Thus, the MLE model does not break symmetry
and rates each of the five parse trees as equally
likely. Therefore, its expected per-token accuracy
is 40%. Average overlaps between structures (i-v)
and answers (i,ii) are (i) 100% or 0; (ii) 0 or 100%;
and (iii,iv,v) 33.3%: (3+3)/(5?3) = 2/5 = 0.4.
A decoy model without left- or right-branching,
i.e., P?STOP( a?, L, T) = 1 or P?STOP( a?, R, T) = 1,
would assign zero probability to some of the train-
ing data. It would be forced to parse every instance
of a? a? a? either as (i) or as (ii), deterministically.
Nevertheless, it would attain a higher per-token ac-
curacy of 50%. (Judged on exact matches, at the
granularity of whole trees, the decoy?s guaranteed
50% accuracy clobbers the MLE?s expected 20%.)
Our toy data set could be replicated n-fold with-
out changing the analysis. This confirms that, even
in the absence of estimation errors or data sparsity,
there can be a fundamental disconnect between
likelihood and accuracy, if the model is wrong.8
7.3 A Subtler Case: ?? = ??SUP vs. ??UNS vs. ??VIT
We now prove that, even with the right model,
mismatches between the different objective like-
lihoods can also handicap the truth. Our calcula-
tions are again exact, so there are no issues with
numerical stability. We work with a set of param-
eters ?? already factored by the DMV, so that its
problems could not be blamed on invalid indepen-
dence assumptions. Yet we are able to find another
impostor distribution ?? that outshines ??SUP = ?? on
both unsupervised metrics, which proves that the
true models ??SUP and ?? are not globally optimal,
as judged by the two surrogate objective functions.
This next example is organic. We began with
WSJ10 and confirmed that classic EM abandons
the supervised solution. We then iteratively dis-
carded large portions of the data set, so long as
the remainder maintained the (un)desired effect ?
EM walking away from its ??SUP. This procedure
isolated such behavior, arriving at a minimal set:
NP : NNP NNP ?
? Marvin Alisky.
S : NNP VBD ?
(Braniff declined).
NP-LOC : NNP NNP ?
Victoria, Texas
8And as George Box quipped, ?Essentially, all models are
wrong, but some are useful? (Box and Draper, 1987, p. 424).
This kernel is tiny, but, as before, our analysis is
invariant to n-fold replication: the problem cannot
be explained away by a small training size ? it
persists even in infinitely large data sets. And so,
we consider three reference parse trees for two-
token sentences over a binary vocabulary { a?, z?}:
(i) xa? a?; (ii) xa? z?; (iii) ya? a?.
One third of the time, z? is the head; only a? can
be a child; and only a? has right-dependents. Trees
(i)-(iii) are the only two-terminal parses generated
by the model and are equiprobable. Thus, these
sentences are representative of a length-two re-
striction of everything generated by the true ??:
PATTACH(?, L, a?) =
2
3 and PSTOP( a?, ?, T) =
4
5 ,
since a? is the head two out of three times, and
since only one out of five a??s attaches a child on
either side. Elsewhere, the model is deterministic:
PSTOP( z?, L, T) = 0;
PSTOP(?, ?, F) = PSTOP( z?, R, T) = 1;
PATTACH( a?, ?, a?) = PATTACH( z?, L, a?) = 1.
Contrast the optimal estimate ??SUP = ?? with the
decoy fixed point9 ?? that is identical to ??, except
P?STOP( a?, L, T) =
3
5 and P?STOP( a?, R, T) = 1.
The probability of stopping is now 3/5 on the left
and 1 on the right, instead of 4/5 on both sides ?
?? disallows a??s right-dependents but preserves its
overall fertility. The probabilities of leaves a? (no
children), under the models ??SUP and ??, are:
P?( a?) = P?STOP( a?, L, T)?P?STOP( a?, R, T) =
(4
5
)2
and P?( a?) = P?STOP( a?, L, T)?P?STOP( a?, R, T) =
3
5 .
And the probabilities of, e.g., structure
x
a? z?, are:
P?ATTACH(?, L, z?) ? P?STOP( z?, R, T)
? (1 ? P?STOP( z?, L, T)) ? P?STOP( z?, L, F)
? P?ATTACH( z?, L, a?) ? P?( a?)
9The model estimated from the parse trees induced by ??
over the three sentences is again ??, for both soft and hard EM.
15
= P?ATTACH(?, L, z?) ? P?( a?) =
1
3 ?
16
25
and P?ATTACH(?, L, z?) ? P?( a?) =
1
3 ?
3
5 .
Similarly, the probabilities of all four possible
parse trees for the two distinct sentences, a? a? and
a? z?, under the two models, ??SUP = ?? and ??, are:
??SUP = ?? ??
x
a? z? 13
` 16
25
?
= 13
` 3
5
?
=
16
75 = 0.213 15 = 0.2y
a? z? 0 0
x
a? a? 23
` 4
5
? `
1? 45
? ` 16
25
?
= 23
`
1 ? 35
? ` 3
5
?
=
128
1875 = 0.06826 425 = 0.16y
a? a? 0.06826 0
To the three true parses, ??SUP assigns probability
(16
75
) ( 128
1875
)2 ? 0.0009942 ? about 1.66bpt; ??
leaves zero mass for (iii), corresponding to a larger
(infinite) cross-entropy, consistent with theory.
So far so good, but if asked for best (Viterbi)
parses, ??SUP could still produce the actual trees,
whereas ?? would happily parse sentences of (iii)
and (i) the same, perceiving a joint probability of
(0.2)(0.16)2 = 0.00512 ? just 1.27bpt, appear-
ing to outperform ??SUP = ??! Asked for sentence
probabilities, ?? would remain unchanged (it parses
each sentence unambiguously), but ??SUP would ag-
gregate to
(16
75
) (
2 ? 1281875
)2 ? 0.003977, improv-
ing to 1.33bpt, but still noticeably ?worse? than ??.
Despite leaving zero probability to the truth, ??
beats ?? on both surrogate metrics, globally. This
seems like an egregious error. Judged by (extrin-
sic) accuracy, ?? still holds its own: it gets four
directed edges from predicting parse trees (i) and
(ii) completely right, but none of (iii) ? a solid
66.7%. Subject to tie-breaking, ?? is equally likely
to get (i) and/or (iii) entirely right or totally wrong
(they are indistinguishable): it could earn a perfect
100%, tie ??, or score a low 33.3%, at 1:2:1 odds,
respectively ? same as ???s deterministic 66.7%
accuracy, in expectation, but with higher variance.
8 Discussion of Theoretical Results
Daume? et al (2009) questioned the benefits of us-
ing exact models in approximate inference. In our
case, the model already makes strong simplifying
assumptions and the objective is also incorrect. It
makes sense that Viterbi EM sometimes works,
since an approximate wrong ?solution? could, by
chance, be better than one that is exactly wrong.
One reason why Viterbi EM may work well is
that its score is used in selecting actual output
parse trees. Wainwright (2006) provided strong
theoretical and empirical arguments for using the
same approximate inference method in training
as in performing predictions for a learned model.
He showed that if inference involves an approxi-
mation, then using the same approximate method
to train the model gives even better performance
guarantees than exact training methods. If our task
were not parsing but language modeling, where
the relevant score is the sum of the probabilities
over individual derivations, perhaps classic EM
would not be doing as badly, compared to Viterbi.
Viterbi training is not only faster and more accu-
rate but also free of inside-outside?s recursion con-
straints. It therefore invites more flexible model-
ing techniques, including discriminative, feature-
rich approaches that target conditional likelihoods,
essentially via (unsupervised) self-training (Clark
et al, 2003; Ng and Cardie, 2003; McClosky et
al., 2006a; McClosky et al, 2006b, inter alia).
Such ?learning by doing? approaches may be
relevant to understanding human language ac-
quisition, as children frequently find themselves
forced to interpret a sentence in order to inter-
act with the world. Since most models of human
probabilistic parsing are massively pruned (Juraf-
sky, 1996; Chater et al, 1998; Lewis and Vasishth,
2005, inter alia), the serial nature of Viterbi EM
? or the very limited parallelism of k-best Viterbi
? may be more appropriate in modeling this task
than the fully-integrated inside-outside solution.
9 Conclusion
Without a known objective, as in unsupervised
learning, correct exact optimization becomes im-
possible. In such cases, approximations, although
liable to pass over a true optimum, may achieve
faster convergence and still improve performance.
We showed that this is the case with Viterbi
training, a cheap alternative to inside-outside re-
estimation, for unsupervised dependency parsing.
We explained why Viterbi EM may be partic-
ularly well-suited to learning from longer sen-
tences, in addition to any general benefits to syn-
chronizing approximation methods across learn-
ing and inference. Our best algorithm is sim-
pler and an order of magnitude faster than clas-
sic EM. It achieves state-of-the-art performance:
3.8% higher accuracy than previous published best
16
results on Section 23 (all sentences) of the Wall
Street Journal corpus. This improvement general-
izes to the Brown corpus, our held-out evaluation
set, where the same model registers a 7.5% gain.
Unfortunately, approximations alone do not
bridge the real gap between objective functions.
This deeper issue could be addressed by drawing
parsing constraints (Pereira and Schabes, 1992)
from specific applications. One example of such
an approach, tied to machine translation, is syn-
chronous grammars (Alshawi and Douglas, 2000).
An alternative ? observing constraints induced by
hyper-text mark-up, harvested from the web ? is
explored in a sister paper (Spitkovsky et al, 2010),
published concurrently.
Acknowledgments
Partially funded by NSF award IIS-0811974 and by the Air
Force Research Laboratory (AFRL), under prime contract
no. FA8750-09-C-0181; first author supported by the Fan-
nie & John Hertz Foundation Fellowship. We thank An-
gel X. Chang, Mengqiu Wang and the anonymous reviewers
for many helpful comments on draft versions of this paper.
References
H. Alshawi and S. Douglas. 2000. Learning dependency
transduction models from unannotated examples. In
Royal Society of London Philosophical Transactions Se-
ries A, volume 358.
H. Alshawi. 1996. Head automata for speech translation. In
Proc. of ICSLP.
J. K. Baker. 1979. Trainable grammars for speech recogni-
tion. In Speech Communication Papers for the 97th Meet-
ing of the Acoustical Society of America.
G. E. P. Box and N. R. Draper. 1987. Empirical Model-
Building and Response Surfaces. John Wiley.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19.
E. Charniak. 1993. Statistical Language Learning. MIT
Press.
N. Chater, M. J. Crocker, and M. J. Pickering. 1998. The
rational analysis of inquiry: The case of parsing. In
M. Oaksford and N. Chater, editors, Rational Models of
Cognition. Oxford University Press.
S. Clark, J. Curran, and M. Osborne. 2003. Bootstrapping
POS-taggers using unlabelled data. In Proc. of CoNLL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsupervised
grammar induction. In Proc. of NAACL-HLT.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for
PCFGs: Hardness results and competitiveness of uniform
initialization. In Proc. of ACL.
M. Collins. 1999. Head-Driven Statistical Models for Nat-
ural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
H. Daume?, III, J. Langford, and D. Marcu. 2009. Search-
based structured prediction. Machine Learning, 75(3).
C. de Marcken. 1995. Lexical heads, phrase structure and
the induction of grammar. In WVLC.
D. Elworthy. 1994. Does Baum-Welch re-estimation help
taggers? In Proc. of ANLP.
W. N. Francis and H. Kucera, 1979. Manual of Information
to Accompany a Standard Corpus of Present-Day Edited
American English, for use with Digital Computers. De-
partment of Linguistic, Brown University.
J. Gillenwater, K. Ganchev, J. Grac?a, B. Taskar, and
F. Pereira. 2009. Sparsity in grammar induction. In
NIPS: Grammar Induction, Representation of Language
and Language Learning.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with richer
contexts and smoothing. In Proc. of NAACL-HLT.
D. Jurafsky. 1996. A probabilistic model of lexical and syn-
tactic access and disambiguation. Cognitive Science, 20.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In Proc. of ACL.
R. L. Lewis and S. Vasishth. 2005. An activation-based
model of sentence processing as skilled memory retrieval.
Cognitive Science, 29.
P. Liang and D. Klein. 2008. Analyzing the errors of unsu-
pervised learning. In Proc. of HLT-ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2).
D. McClosky, E. Charniak, and M. Johnson. 2006a. Effec-
tive self-training for parsing. In Proc. of NAACL-HLT.
D. McClosky, E. Charniak, and M. Johnson. 2006b. Rerank-
ing and self-training for parser adaptation. In Proc. of
COLING-ACL.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2).
V. Ng and C. Cardie. 2003. Weakly supervised natural lan-
guage learning without redundant views. In Proc. of HLT-
NAACL.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In Proc. of ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby
Steps: How ?Less is More? in unsupervised dependency
parsing. In NIPS: Grammar Induction, Representation of
Language and Language Learning.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010. Profit-
ing from mark-up: Hyper-text annotations for guided pars-
ing. In Proc. of ACL.
M. J. Wainwright. 2006. Estimating the ?wrong? graphical
model: Benefits in the computation-limited setting. Jour-
nal of Machine Learning Research, 7.
17
Using Query Patterns to Learn the Duration of Events
Andrey Gusev Nathanael Chambers Pranav Khaitan Divye Khilnani
Steven Bethard Dan Jurafsky
Department of Computer Science, Stanford University
{agusev,nc,pranavkh,divyeraj,bethard,jurafsky}@cs.stanford.edu
Abstract
We present the first approach to learning the durations of events without annotated training data,
employing web query patterns to infer duration distributions. For example, we learn that ?war?
lasts years or decades, while ?look? lasts seconds or minutes. Learning aspectual information is an
important goal for computational semantics and duration information may help enable rich document
understanding. We first describe and improve a supervised baseline that relies on event duration
annotations. We then show how web queries for linguistic patterns can help learn the duration of
events without labeled data, producing fine-grained duration judgments that surpass the supervised
system. We evaluate on the TimeBank duration corpus, and also investigate how an event?s participants
(arguments) effect its duration using a corpus collected through Amazon?s Mechanical Turk. We make
available a new database of events and their duration distributions for use in research involving the
temporal and aspectual properties of events.
1 Introduction
Bridging the gap between lexical knowledge and world knowledge is crucial for achieving natural language
understanding. For example, knowing whether a nominal is a person or organization and whether a person
is male or female substantially improves coreference resolution, even when such knowledge is gathered
through noisy unsupervised approaches (Bergsma, 2005; Haghighi and Klein, 2009). However, existing
algorithms and resources for such semantic knowledge have focused primarily on static properties of
nominals (e.g. gender or entity type), not dynamic properties of verbs and events.
This paper shows how to learn one such property: the typical duration of events. Since an event?s
duration is highly dependent on context, our algorithm models this aspectual property as a distribution
over durations rather than a single mean duration. For example, a ?war? typically lasts years, sometimes
months, but almost never seconds, while ?look? typically lasts seconds or minutes, but rarely years or
decades. Our approach uses web queries to model an event?s typical distribution in the real world.
Learning such rich aspectual properties of events is an important area for computational semantics,
and should enrich applications like event coreference (e.g., Chen and Ji, 2009) in much the same way
that gender has benefited nominal coreference systems. Event durations are also key to building event
timelines and other deeper temporal understandings of a text (Verhagen et al, 2007; Pustejovsky and
Verhagen, 2009).
The contributions of this work are:
? Demonstrating how to acquire event duration distributions by querying the web with patterns.
? Showing that a system that predicts event durations based only on our web count distributions can
outperform a supervised system that requires manually annotated training data.
? Making available an event duration lexicon with duration distributions for common English events.
We first review previous work and describe our re-implementation and augmentation of the latest
supervised system for predicting event durations. Next, we present our approach to learning event
distributions based on web counts. We then evaluate both of these models on an existing annotated corpus
of event durations and make comparisons to durations we collected using Amazon?s Mechanical Turk.
Finally, we present a generated database of event durations.
145
2 Previous Work
Early work on extracting event properties focused on linguistic aspect, for example, automatically
distinguishing culminated events that have an end point from non-culminated events that do not (Siegel
and McKeown, 2000). The more fine-grained task of predicting the duration of events was first proposed
by Pan et al (2006), who annotated each event in a small section of the TimeBank (Pustejovsky et al,
2003) with duration lower and upper bounds. They then trained support vector machines on their annotated
corpus for two prediction tasks: less-than-a-day vs. more-than-a-day, and bins like seconds, minutes,
hours, etc. Their models used features like bags of words, heads of syntactic subjects and objects, and
WordNet hypernyms of the events. This work provides a valuable resource in its annotated corpus and is
also a good baseline. We replicate their work and also add new features as described below.
Our approach to the duration problem is inspired by the standard use of web patterns for the acquisition
of relational lexical knowledge. Hearst (1998) first observed that a phrase like ?. . . algae, such as
Gelidium. . . ? indicates that ?Gelidium? is a type of ?algae?, and so hypernym-hyponym relations can
be identified by querying a text collection with patterns like ?such <noun> as <noun>? and ?<noun> ,
including <noun>?. A wide variety of pattern-based work followed, including the application of the idea
in VerbOcean to acquire aspects and temporal structure such as happens-before, using patterns like ?to
<verb> and then <verb>? (Chklovski and Pantel, 2004).
More recent work has learned nominal gender and animacy by matching patterns like ?<noun> *
himself? and ?<noun> and her? to a corpus of Web n-grams (Bergsma, 2005; Ji and Lin, 2009). Phrases like
?John Joseph?, which were observed often with masculine pronouns and never with feminine or neuter
pronouns, can thus have their gender identified as masculine. Ji and Lin found that such web-counts can
predict person names as well as a fully supervised named entity recognition system.
Our goal, then, is to integrate these two strands in the literature, applying pattern/web approaches to
the task of estimating event durations. One difference from previous work is the distributional nature of
the extracted knowledge. In the time domain, unlike in most previous relation-extraction domains, there is
rarely a single correct answer: ?war? may last months, years or decades, though years is the most likely.
Our goal is thus to produce a distribution over durations rather than a single mean duration.
3 Duration Prediction Tasks
In both our supervised and unsupervised models, we consider two types of event duration predictions: a
coarse-grained task in which we only want to know whether the event lasts more or less than a day, and a
fine-grained task in which we want to know whether the event lasts seconds, minutes, hours, days, weeks,
months or years. These two duration prediction tasks were originally suggested by Pan et al (2006), based
on their annotation of a subset of newspaper articles in the Timebank corpus (Pustejovsky et al, 2003).
Events were annotated with a minimum and maximum duration like the following:
? 5 minutes ? 1 hour: A Brooklyn woman who was watching her clothes dry in a laundromat.
? 1 week ? 3 months: Eileen Collins will be named commander of the Space Shuttle mission.
? 3 days ? 2 months: President Clinton says he is committed to a possible strike against Iraq. . .
Pan et al suggested the coarse-grained binary classification task because they found that the mean event
durations from their annotations were distributed bimodally across the corpus, roughly split into short
events (less than a day) and long events (more than a day). The fine-grained classification task provides
additional information beyond this simple two way distinction.
For both tasks, we must convert the minimum/maximum duration annotations into single labels. We
follow Pan et al (2006) and take the arithmetic mean of the minimum and maximum durations in seconds.
For example, in the first event above, 5 minutes would be converted into 300 seconds, 1 hour would be
converted into 3600 seconds, the resulting mean would be 1950 seconds, and therefore this event would
be labeled less-than-a-day for the coarse-grained task, and minutes for the fine-grained task. These labels
can then be used directly to train and evaluate our models.
146
4 Supervised Approach
Before describing our query-based approach, we describe our baseline, a replication and extension of the
supervised system from Pan et al (2006). We first briefly describe their features, which are shared across
the coarse and fine-grained tasks, and then suggest new features.
4.1 Pan et. al. Features
The Pan et al (2006) system included the following features which we also replicate:
Event Properties: The event token, lemma and part of speech (POS) tag.
Bag of Words: The n tokens to the left and right of the event word. However, because Pan et al
found that n = 0 performed best, we omit this feature.
Subject and Object: The head word of the syntactic subject and object of the event, along with their
lemmas and POS tags. Subjects and objects provide important context. For example, ?saw Europe? lasts
for weeks or months while ?saw the goal? lasts only seconds.
Hypernyms: WordNet hypernyms for the event, its subject and its object. Starting from the first
synset of each lemma, three hypernyms were extracted from the WordNet hierarchy. Hypernyms can help
cluster similar events together. For example, the event plan had three hypernym ancestors as features:
idea, content and cognition.
4.2 New Features
We present results for our implementation of the Pan et al (2006) system in Section 8. However, we also
implemented additional features.
Event Attributes: Timebank annotates individual events with four attributes: the event word?s tense
(past, present, future, none), aspect (e.g., progressive), modality (e.g., could, would, can, etc.), and event
class (occurrence, aspectual, state, etc.). We use each of these as a feature in our classifier. The aspect and
tense of the event, in particular, are well known indicators of the temporal shape of events (Vendler, 1976).
Named Entity Classes: Pan et al found the subject and object of the events to be useful features,
helping to identify the particular sense of the event. We used a named entity recognizer to add more
information about the subjects and objects, labeling them as persons, organizations, locations, or other.
Typed Dependencies: We coded aspects of the subcategorization frame of a predicate, such as
transitivity, or the presence of prepositional objects or adverbial modifiers, by adding a binary feature
for each typed dependency1 seen with a verb or noun. We experimented with including the head of the
argument itself, but results were best when only the dependency type was included.
Reporting Verbs: Many of the events in Timebank are reporting verbs (say, report, reply, etc.). We
used a list of reporting verbs to identify these events with a binary feature.
4.3 Classifier
Both the Pan et al feature set and our extended feature set were used to train supervised classifiers for the
two event duration prediction tasks. We experimented with naive bayes, logistic regression, maximum
entropy and support vector machine classifiers, but as discussed in Section 8, the maximum entropy model
performed best in cross-validations on the training data.
5 Unsupervised Approach
While supervised learning is effective for many NLP tasks, it is sensitive to the amount of available
training data. Unfortunately, the training data for event durations is very small, consisting of only 58 news
articles (Pan et al, 2006), and labeling further data is quite expensive. This motivates our desire to find an
1We parsed the documents into typed dependencies with the Stanford Parser (Klein and Manning, 2003).
147
approach that does not rely on labeled data, but instead utilizes the large amounts of text available on the
Web to search for duration-specific patterns. This section describes our web-based approach to learning
event durations.
5.1 Web Query Patterns
Temporal properties of events are often described explicitly in language-specific constructions which can
help us infer an event?s duration. Consider the following two sentences from our corpus:
? Many spend hours surfing the Internet.
? The answer is coming up in a few minutes.
These sentences explicitly describe the duration of the events. In the first, the dominating clause spend
hours tells us how long surfing the Internet lasts (hours, not seconds), and in the second, the preposition
attachment serves a similar role. These examples are very rare in the corpus, but as can be seen, are
extremely informative when present. We developed several such informative patterns, and searched the
Web to find instances of them being used with our target events.
For each pattern described below, we use Yahoo! to search for the patterns occurring with our events.
We collect the total hit counts and use them as indicators of duration. The Yahoo! search API returns two
numbers for a query: totalhits and deephits. The former excludes duplicate pages and limits the number
of documents per domain while the latter includes all duplicates. We take the sum of these two numbers
as our count (this worked better than either of the two individually on the training data and provides
a balance between the benefits of each estimate) and normalize the results as described in Section 5.2.
Queries are submitted as complete phrases with quotation marks, so the results only include exact phrase
matches. This greatly reduces the number of hits, but results in more precise distributions.
5.1.1 Coarse-Grained Patterns
The coarse grained task is a binary decision: less than a day or more than a day. We can model this
task directly by looking for constructions that can only be used with events that take less than a day.
The adverb yesterday fills this role nicely; an event modified by yesterday strongly implies that it took
place within a single day?s time. For example, ?shares closed at $18 yesterday? implies that the closing
happened in less than a day. We thus consider the following two query patterns:
? <eventpast> yesterday
? <eventpastp> yesterday
where <eventpast> is the past tense (preterite) form of the event (e.g., ran), and <eventpastp> is the past
progressive form of the event (e.g., was running).
5.1.2 Fine-Grained Patterns
For the fine-grained task, we need patterns that can identify when an event falls into any of the various
buckets: seconds, minutes, hours, etc. Thus, our fine-grained patterns are parameterized both by the event
and by the bucket of interest. We use the following patterns inspired in part by Dowty (1979):
1. <eventpast> for * <bucket>
2. <eventpastp> for * <bucket>
3. spent * <bucket> <eventger>
where <eventpast> and <eventpastp> are defined as above, <eventger> is the gerund form of the event (e.g.,
running), and the wildcard ?*? can match any single token2.
The following three patterns ultimately did not improve the system?s performance on the training data:
4. <eventpast> in * <bucket>
5. takes * <bucket> to <event>
6. <eventpast> last <bucket>
Pattern 4 returned a lot of hits, but had low precision as it picked up many non-durative expressions.
Pattern 5 was very precise but typically returned few hits, and pattern 6 worked for, e.g., last week, but did
not work for shorter durations. All reported systems use patterns 1-3 and do not include 4-6.
2We experimented with varying numbers of wildcards but found little difference in performance on the training data.
148
!"
!#$"
!#%"
!#&"
!#'"
!#("
!#)"
*+
,-
./
*"
0
1.
23
+*
"
4-
25
*"
/6
7*
"
8
++
9*
"
0
-.
34
*"
7+
65
*"
/+
,6
/+
*"
(a) ?was saying for <bucket>?
!"
!#$"
!#%"
!#&"
!#'"
!#("
!#)"
*+
,-
./
*"
0
1.
23
+*
"
4-
25
*"
/6
7*
"
8
++
9*
"
0
-.
34
*"
7+
65
*"
/+
,6
/+
*"
(b) ?for <bucket>?
!"
!#$"
!#%"
!#&"
!#'"
!#("
!#)"
*+
,-
./
*"
0
1.
23
+*
"
4-
25
*"
/6
7*
"
8
++
9*
"
0
-.
34
*"
7+
65
*"
/+
,6
/+
*"
(c) (a) counts divided by (b) counts
Figure 1: Normalizing the distribution for the pattern ?was saying for <bucket>?.
We also tried adding subjects and/or objects to the patterns when they were present for an event.
However, we found that the benefit of the extra context was outweighed by the significantly fewer hits that
resulted. We implemented several backoff approaches that removed the subject and object from the query,
however, the counts from these backoff approaches were less reliable than just using the base event.
5.2 Predicting Durations from Patterns
To predict the duration of an event from the above patterns, we first insert the event into each pattern
template and query the web to see how often the filled template occurs. These counts form a distribution
over each of the bins of interest, e.g., in the fine-grained task we have counts for seconds, minutes, hours,
etc. We discard pattern distributions with very low total counts, and normalize the remaining pattern
distributions based on the frequency with which the pattern occurs in general. Finally, we uniformly
merge the distributions from all patterns, and use the resulting distribution to select a duration label for
the event. The following sections detail this process.
5.2.1 Coarse-Grained Prediction
For the coarse-grained task of less than a day vs. more than a day, we collect counts using the two
yesterday patterns described above. We then normalize these counts by the count of the event?s occurrence
in general. For example, given the event run, we query for ?ran yesterday? and divide by the count of
?ran?. This gives us the probability of seeing yesterday given that we saw ran. We average the probabilities
from the two yesterday patterns, and classify an event as lasting less than a day if its average probability
exceeds a threshold t. We optimized t to our training set (t = .002). This basically says that if an event
occurs with yesterday more than 0.2% of the time, we will assume that the event lasts less than a day.
5.2.2 Fine-Grained Prediction
As with the coarse-grained task, our fine-grained approach begins by collecting counts using the three
fine-grained patterns discussed above. Since each fine-grained pattern has both an <event> and a <bucket>
slot to be filled, for a single event and a single pattern, we end up making 8 queries to cover each of the 8
buckets: seconds, minutes, hours, days, weeks, months, years and decades. After these queries, we have a
pattern-specific distribution of counts over the various buckets, a coarse measure of the types of durations
that might be appropriate to this event. Figure 1(a) shows an example of such a distribution.
As can be seen in Figure 1(a), this initial distribution can be skewed in various ways ? in this case,
years is given far too much mass. This is because in addition to the single event interpretation of words
like ?saying?, there are iterative or habitual interpretations (Moens and Steedman, 1988; Frawley, 1992).
Iterative events occur repeatedly over a period of time, e.g., ?he?s been saying for years that. . . ? The two
interpretations are apparent in the raw distributions of smile and run in Figure 2. The large peak at years
for run shows that it is common to say someone ?was running for years.? Conversely, it is less common to
say someone ?was smiling for years,? so the distribution for smile is less biased towards years.
149
        

	
A
B
C
D 
	AB
CDBEA	FB
Figure 2: Two double peaked distributions.
Coverage of Fine-Grained Query Patterns
Number of Patterns Total Events Precision
At least one 1359 (81.7%) 57.3
At least two 1142 (68.6%) 58.6
All three 428 (25.7%) 65.7
Figure 3: The number of events that match n fine-
grained patterns and the pattern precision on these
events. The training set consists of 1664 events.
While the problem of distinguishing single events from iterative events is out of the scope of this paper
(though an interesting avenue for future research), we can partially address the problem by recognizing
that some buckets are simply more frequent in text than others. For example, Figure 1(b) shows that it is
by far more common to see ?for <bucket>? filled with years than with any other duration unit. Thus, for
each bucket, we divide the counts collected with the event patterns by the counts we get for the pattern
without the event3. Essentially, this gives us for each bucket the probability of the event given that bucket.
Figure 1(c) shows that the resulting normalized distribution fits our intution of how long ?saying? should
last much better than the raw counts: seconds and minutes have much more of the mass now.
After normalizing an event?s counts for each pattern, we combine the distributions from the three
different patterns if their hit counts pass certain confidence thresholds. The total hit count for each pattern
must exceed a minimum threshold tmin = 100 and not exceed a maximum threshold tmax = 100, 000
(both thresholds were optimized on the training data). The former avoids building distributions from a
sparse number of hits, and the latter avoids classifying generic and polysemous events like ?to make? that
return a large number of hits. We found such events to produce generic distributions that do not help in
classification. If all three patterns pass our confidence thresholds, we merge the pattern distributions by
summing them bucket-wise together and renormalizing the resulting distribution to sum to 1. Merging the
patterns mitigates the noise from any single pattern.
To predict the event?s duration, we then select the bucket with the highest smoothed score:
score(bi) = bi?1 + bi + bi+1
where bi is a duration bucket and 0 < i < 9. We define b0 = b9 = 0. In other words, the score of the
minute bucket is the sum of three buckets: second, minute and hour. This parallels the smoothing of the
evaluation metric introduced by (Pan et al, 2006) which we also adopt for evaluation in Section 7.
In the case that fewer than three of our patterns matched, we backoff to the majority class (months for
fine-grained, and more-than-a-day for coarse-grained). We experimented with only requiring one or two
patterns to match, but found the best results on training when requiring all three. Figure 3 shows the large
jump in precision when all three are required. The evaluation is discussed in Section 7.
5.2.3 Coarse-Grained Prediction via Fine-Grained Prediction
We can also use the distributions collected from the fine-grained task to predict coarse-grained labels. We
use the above approach and return less than a day if the selected fine-grained bucket was seconds, minutes
or hours, and more than a day otherwise. We also tried summing over the duration buckets: p(seconds) +
p(minutes) + p(hours) for less than day and p(days) + p(weeks) + p(months) + p(years) + p(decades) for
more than a day, but the simpler approach outperformed these summations in training.
3We also explored normalizing not by the global distribution on the Web, but by the average of the distributions of all the
events in our dataset. However, on the training data, using the global distribution performed better.
150
6 Datasets
6.1 Timebank Duration
As described in Section 3, Pan et al (2006) labeled 58 documents with event durations. We follow their
method of isolating the 10 WSJ articles as a separate test set which we call TestWSJ (147 events). For
the remaining 48 documents, they split the 2132 event instances into a Train and Test set with 1705 and
427 events respectively. Their split was conducted over the bag of events, so their train and test sets may
include events that came from the same document. Their particular split was unavailable.
We instead use a document-split that divides the two sets into bins of documents. Each document?s
entire set of events is assigned to either the training set or the test set, so we do not mix events across
sets. Since documents often repeat mentions of events, this split is more conservative by not mixing test
mentions with the training set. Train, Test, and TestWSJ contain 1664 events (714 unique verbs), 471 events
(274 unique), and 147 events (84 unique) respectively. For each base verb, we created queries as described
in Section 5.1.2. The train/test split is available at http://cs.stanford.edu/people/agusev/durations/.
6.2 Mechanical Turk Dataset
We also collected event durations from Amazon?s Mechanical Turk (MTurk), an online marketplace from
Amazon where requesters can find workers to solve Human Intelligence Tasks (HITs) for small amounts
of money. Prior work has shown that human judgments from MTurk can often be as reliable as trained
annotators (Snow et al, 2008) or subjects in controlled lab studies (Munro et al, 2010), particularly when
judgments are aggregated over many MTurk workers (?Turkers?). Our motivation for using Turkers is to
better analyze system errors. For example, if we give humans an event in isolation (no sentence context),
how well can they guess the durations assigned by the Pan et. al. annotators? This measures how big the
gap is between a system that looks only at the event, and a system that integrates all available context.
To collect event durations from MTurk, we presented Turkers with an event from the TimeBank (a
superset of the events annotated by Pan et al (2006)) and asked them to decide whether the event was most
likely to take seconds, minutes, hours, days, weeks, months, years or decades. We had events annotated
in two different contexts: in isolation, where only the event itself was given (e.g., ?allocated?), and in
subject-object context, where a minimal phrase including the event and its subject and object was given
(e.g., ?the mayor allocated funds?). In both types of tasks, we asked 10 Turkers to label each event,
and they were paid $0.0025 for each annotation ($0.05 for a block of 20 events). To filter out obvious
spammers, we added a test item randomly to each block, e.g., adding the event ?minutes? and rejecting
work from Turkers who labeled this anything other than the duration minutes.
The resulting annotations give duration distributions for each of our events. For example, when
presented the event ?remodeling?, 1 Turker responded with days, 6 with weeks, 2 with months and 1
with years. These annotations suggest that we generally expect ?remodeling? to take weeks, but it may
sometimes take more or less. To produce a single fine-grained label from these distributions, we take the
duration bin with the largest number of Turker annotations, e.g. for ?remodeling?, we would produce the
label weeks. To produce a single coarse-grained label, we use the label less-than-a-day if the fine-grained
label was seconds, minutes or hours and more-than-a-day otherwise.
7 Experiment Setup
As discussed in Section 3, we convert the minimum and maximum duration annotations into labels by
converting each to seconds using ISO standards and calculating the arithmetic mean. If the mean is
? 86400 seconds, it is considered less-than-a-day for the coarse-grained task. The fine-grained buckets
are similarly calculated, e.g., X is labeled days if 86400 < X ? 604800. The Pan et al (2006) evaluation
does not include a decades bucket, but our system still uses ?decades? in its queries.
We optimized all parameters of both the supervised and unsupervised systems on the training set, only
running on test after selecting our best performing model. We compare to the majority class as a baseline,
151
Coarse-Grained
Test TestWSJ
Supervised, Pan 73.3 73.5
Supervised, all 73.0 74.8
Fine-Grained
Test TestWSJ
Supervised, Pan 62.2 61.9
Supervised, all 62.4 66.0
Figure 4: Accuracies of the supervised maximum entropy classifiers with two different feature sets.
Coarse-Grained
Test TestWSJ
Majority class 62.4 57.1
Supervised, all 73.0* 74.8*
Web counts, yesterday 70.7* 74.8*
Web counts, buckets 72.4* 73.5*
Fine-Grained
Test TestWSJ
Majority class 59.2 52.4
Supervised, all 62.4 66.0?
Web counts, buckets 66.5* 68.7*
Figure 5: System accuracy compared against supervised and majority class. * indicates statistical
significance (McNemar?s Test, two-tailed) against majority class at the p < 0.01 level, ? at p < 0.05
tagging all events as more-than-a-day in the coarse-grained task and months in the fine-grained task.
To evaluate our models, we use simple accuracy on the coarse-grained task, and approximate agreement
matching as in Pan et al (2006) on the fine-grained task. In this approximate agreement, a guess is
considered correct if it chooses either the gold label or its immediate neighbor (e.g., hours is correct if
minutes, hours or days is the gold class). Pan et al use this approach since human labeling agreement is
low (44.4%) on the exact agreement fine-grained task.
8 Results
Figure 4 compares the performance of our two supervised models; the reimplementation of Pan et al
(2006) (Supervised, Pan), and our improved model with new features (Supervised, all). The new model
performs similarily to the Pan model on the in-domain Test set, but better on the out-of-domain financial
news articles in the TestWSJ test. On the latter, the new model improves over Pan et al by 1.3% absolute
on the coarse-grained task, and by 4.1% absolute on the fine-grained task. We report results from the
maximum entropy model as it slightly outperformed the naive bayes and support vector machine models4.
We compare these supervised results against our web-based unsupervised systems in Figure 5. For the
coarse-grained task, we have two web count systems described in Section 5: one based on the yesterday
patterns (Web counts, yesterday), and one based on first gathering the fine-grained bucket counts and
then converting those to coarse-grained labels (Web counts, buckets). Generally, these models perform
within 1-2% of the supervised model on the coarse-grained task, though the yesterday-based classifier
exactly matches the supervised system?s performance on the TestWSJ data. The supervised system?s
higher results are not statistically significant against our web-based systems.
For the fine-grained task, Figure 5 compares our web counts algorithm based on duration distributions
(Section 5) to the baseline and supervised systems. Our web counts approach outperforms the best
supervised system by 4.1% absolute on the Test set and by 2.7% absolute on the out-of-domain TestWSJ.
To get an idea of how much the subject/object context could help predict event duration if integrated
perfectly, we evaluated the Mechanical Turk annotations against the Pan et. al. annotated dataset using
approximate agreement as described in Section 7. Figure 6 gives the performance of the Turkers given
two types of context: just the event itself (Event only), and the event plus its subject and/or object (Event
and args). Turkers performed below the majority class baseline when given only the event, but generally
above the baseline when given the subject and object, improving up to 20% over the event-only condition.
Figure 7 shows examples of events with different learned durations.
4This differs from Pan et al who found support vector machines to be the best classifier.
152
Mechanical Turk Accuracy
Coarse Fine
Test WSJ Test WSJ
Majority class 62.4 57.1 59.2 52.4
Event only 52.0 49.4 42.1 43.8
Event and args 65.0 70.1 56.7 59.9
Figure 6: Accuracy of Mechanical Turkers
against Pan et. al. annotations.
Learned Examples
talk to tourism leaders minutes
driving hours
shut down the supply route days
travel weeks
the downturn across Asia months
build a museum years
Figure 7: Examples of web query durations.
9 Discussion
Our novel approach to learning event durations showed 4.1% and 2.7% absolute gains over a state-of-the-
art supervised classifier. Although the gain is not statistically significant, these results nonetheless suggest
that we are learning as much about event durations from the web counts as we are currently able to learn
with our improvements to Pan et al?s (2006) supervised system. This is encouraging because it indicates
that we may not need extensive manual annotations to acquire event durations. Further, our final query
system achieves these results with only the event word, and without considering the subject, object or
other types of context.
Despite the fact that we saw little gains in performance when including subjects and objects in our
query patterns, the Mechanical Turk evaluation suggests that more information may still be gleaned from
the additional context. Giving Turkers the subject and object improved their label accuracy by 10-20%
absolute. This suggests that finding a way to include subjects and objects in the web queries, for example
by using thesauri to generate related queries, is a valuable line of research for future work.
Finally, these MTurk experiments suggest that classifying events for duration out of context is a
difficult task. Pan et al (2006) reported 0.88 annotator agreement on the coarse-grained task when given
the entire document context. Out of context, given just the event word, our Turkers only achieved 52%
and 49% accuracy. Not surprisingly, the task is more difficult without the document. Our system, however,
was also only given the event word, but it was able achieve over 70% in accuracy. This suggests that rich
language understanding is often needed to correctly label an event for duration, but in the absence of such
understanding, modeling the duration by web counts appears to be a practical and useful alternative.
10 A Database of Event Durations
Given the strong performance of our model on duration classification, we are releasing a database of
events and their normalized duration distributions, as predicted by our bucket-based fine-grained model.
We extracted the 1000 most frequent verbs from a newspaper corpus (the NYT portion of Gigaword
Graff (2002)) with the 10 most frequent grammatical objects of each verb. These 10, 000 events and their
duration distributions are available at http://cs.stanford.edu/people/agusev/durations/.
Acknowledgements
Thanks to Chris Manning and the anonymous reviewers for insightful comments and feedback. This
research draws on data provided by Yahoo!, Inc., through its Yahoo! Search Services offering. We
gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-
C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those
of the author(s) and do not necessarily reflect the view of DARPA, AFRL, or the US government.
153
References
Bergsma, S. (2005). Automatic acquisition of gender information for anaphora resolution. In Advances
in Artificial Intelligence, Volume 3501 of Lecture Notes in Computer Science, pp. 342?353. Springer
Berlin / Heidelberg.
Chen, Z. and H. Ji (2009). Graph-based event coreference resolution. In Proceedings of the Workshop on
Graph-based Methods for Natural Language Processing (TextGraphs-4), Singapore, pp. 54?57. ACL.
Chklovski, T. and P. Pantel (2004). Verbocean: Mining the web for fine-grained semantic verb relations.
In D. Lin and D. Wu (Eds.), Proceedings of EMNLP 2004, Barcelona, Spain, pp. 33?40.
Dowty, D. R. (1979). Word Meaning and Montague Grammar. Kluwer Academic Publishers.
Frawley, W. (1992). Linguistic Semantics. Routledge.
Graff, D. (2002). English Gigaword. Linguistic Data Consortium.
Haghighi, A. and D. Klein (2009). Simple coreference resolution with rich syntactic and semantic features.
In Proceedings of EMNLP-2009, Singapore, pp. 1152?1161.
Hearst, M. A. (1998). Automated discovery of wordnet relations. In WordNet: An Electronic Lexical
Database. MIT Press.
Ji, H. and D. Lin (2009). Gender and animacy knowledge discovery from web-scale n-grams for
unsupervised person mention detection. In Proceedings of the Pacific Asia Conference on Language,
Information and Computation.
Klein, D. and C. D. Manning (2003). Accurate unlexicalized parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics, Sapporo, Japan, pp. 423?430.
Moens, M. and M. Steedman (1988). Temporal ontology in natural language. Computational Linguis-
tics 2(14), 15?21.
Munro, R., S. Bethard, V. Kuperman, V. T. Lai, R. Melnick, C. Potts, T. Schnoebelen, and H. Tily (2010).
Crowdsourcing and language studies: the new generation of linguistic data. In Proceedings of the
NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk,
Los Angeles, pp. 122?130.
Pan, F., R. Mulkar, and J. Hobbs (2006). Learning event durations from event descriptions. In Proceedings
of COLING-ACL.
Pustejovsky, J., P. Hanks, R. Sauri, A. See, D. Day, L. Ferro, R. Gaizauskas, M. Lazo, A. Setzer, and
B. Sundheim (2003). The timebank corpus. Corpus Linguistics, 647?656.
Pustejovsky, J. and M. Verhagen (2009). Semeval-2010 task 13: Evaluating events, time expressions, and
temporal relations (tempeval-2). In Proceedings of the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-2009), Boulder, Colorado, pp. 112?116.
Siegel, E. V. and K. R. McKeown (2000). Learning methods to combine linguistic indicators: improving
aspectual classification and revealing linguistic insights. Computational Linguistics 26(4), 595?628.
Snow, R., B. O?Connor, D. Jurafsky, and A. Ng (2008). Cheap and fast ? but is it good? evaluating
non-expert annotations for natural language tasks. In Proceedings of EMNLP-2008, Hawaii.
Vendler, Z. (1976). Verbs and times. Linguistics in Philosophy, 97?121.
Verhagen, M., R. Gaizauskas, F. Schilder, M. Hepple, G. Katz, and J. Pustejovsky (2007). Semeval-2007
task 15: Tempeval temporal relation identification. In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, pp. 75?80.
154
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 19?28,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Punctuation: Making a Point in Unsupervised Dependency Parsing
Valentin I. Spitkovsky
Computer Science Department
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc.
Mountain View, CA, 94043, USA
hiyan@google.com
Daniel Jurafsky
Departments of Linguistics and Computer Science
Stanford University, Stanford, CA, 94305, USA
jurafsky@stanford.edu
Abstract
We show how punctuation can be used to im-
prove unsupervised dependency parsing. Our
linguistic analysis confirms the strong connec-
tion between English punctuation and phrase
boundaries in the Penn Treebank. However,
approaches that naively include punctuation
marks in the grammar (as if they were words)
do not perform well with Klein and Manning?s
Dependency Model with Valence (DMV). In-
stead, we split a sentence at punctuation and
impose parsing restrictions over its fragments.
Our grammar inducer is trained on the Wall
Street Journal (WSJ) and achieves 59.5% ac-
curacy out-of-domain (Brown sentences with
100 or fewer words), more than 6% higher
than the previous best results. Further evalu-
ation, using the 2006/7 CoNLL sets, reveals
that punctuation aids grammar induction in
17 of 18 languages, for an overall average
net gain of 1.3%. Some of this improvement
is from training, but more than half is from
parsing with induced constraints, in inference.
Punctuation-aware decoding works with exist-
ing (even already-trained) parsing models and
always increased accuracy in our experiments.
1 Introduction
Unsupervised dependency parsing is a type of gram-
mar induction ? a central problem in computational
linguistics. It aims to uncover hidden relations be-
tween head words and their dependents in free-form
text. Despite decades of significant research efforts,
the task still poses a challenge, as sentence structure
is underdetermined by only raw, unannotated words.
Structure can be clearer in formatted text, which
typically includes proper capitalization and punctua-
tion (Gravano et al, 2009). Raw word streams, such
as utterances transcribed by speech recognizers, are
often difficult even for humans (Kim and Woodland,
2002). Therefore, one would expect grammar induc-
ers to exploit any available linguistic meta-data. And
yet in unsupervised dependency parsing, sentence-
internal punctuation has long been ignored (Carroll
and Charniak, 1992; Paskin, 2001; Klein and Man-
ning, 2004; Blunsom and Cohn, 2010, inter alia).
HTML is another kind of meta-data that is ordi-
narily stripped out in pre-processing. However, re-
cently Spitkovsky et al (2010b) demonstrated that
web markup can successfully guide hierarchical
syntactic structure discovery, observing, for exam-
ple, that anchors often match linguistic constituents:
..., whereas McCain is secure on the topic, Obama
<a>[VP worries about winning the pro-Israel vote]</a>.
We propose exploring punctuation?s potential to
aid grammar induction. Consider a motivating ex-
ample (all of our examples are from WSJ), in which
all (six) marks align with constituent boundaries:
[SBAR Although it probably has reduced the level of
expenditures for some purchasers], [NP utilization man-
agement] ? [PP like most other cost containment strate-
gies] ? [VP doesn?t appear to have altered the long-term
rate of increase in health-care costs], [NP the Institute of
Medicine], [NP an affiliate of the National Academy of
Sciences], [VP concluded after a two-year study].
This link between punctuation and constituent
boundaries suggests that we could approximate
parsing by treating inter-punctuation fragments in-
dependently. In training, our algorithm first parses
each fragment separately, then parses the sequence
of the resulting head words. In inference, we use a
better approximation that allows heads of fragments
to be attached by arbitrary external words, e.g.:
The Soviets complicated the issue by offering to
[VP include light tanks], [SBAR which are as light as ... ].
19
Count POS Sequence Frac Cum
1 3,492 NNP 2.8%
2 2,716 CD CD 2.2 5.0
3 2,519 NNP NNP 2.0 7.1
4 2,512 RB 2.0 9.1
5 1,495 CD 1.2 10.3
6 1,025 NN 0.8 11.1
7 1,023 NNP NNP NNP 0.8 11.9
8 916 IN NN 0.7 12.7
9 795 VBZ NNP NNP 0.6 13.3
10 748 CC 0.6 13.9
11 730 CD DT NN 0.6 14.5
12 705 PRP VBD 0.6 15.1
13 652 JJ NN 0.5 15.6
14 648 DT NN 0.5 16.1
15 627 IN DT NN 0.5 16.6
WSJ +103,148 more with Count ? 621 83.4%
Table 1: Top 15 fragments of POS tag sequences in WSJ.
Count Non-Terminal Frac Cum
1 40,223 S 32.5%
2 33,607 NP 27.2 59.7
3 16,413 VP 13.3 72.9
4 12,441 PP 10.1 83.0
5 8,350 SBAR 6.7 89.7
6 4,085 ADVP 3.3 93.0
7 3,080 QP 2.5 95.5
8 2,480 SINV 2.0 97.5
9 1,257 ADJP 1.0 98.5
10 369 PRN 0.3 98.8
WSJ +1,446 more with Count ? 356 1.2%
Table 2: Top 99% of the lowest dominating non-terminals
deriving complete inter-punctuation fragments in WSJ.
2 Definitions, Analyses and Constraints
Punctuation and syntax are related (Nunberg, 1990;
Briscoe, 1994; Jones, 1994; Doran, 1998, inter alia).
But are there simple enough connections between
the two to aid in grammar induction? This section
explores the regularities. Our study of punctuation
in WSJ (Marcus et al, 1993) parallels Spitkovsky
et al?s (2010b, ?5) analysis of markup from a web-
log, since their proposed constraints turn out to be
useful. Throughout, we define an inter-punctuation
fragment as a maximal (non-empty) consecutive se-
quence of words that does not cross punctuation
boundaries and is shorter than its source sentence.
2.1 A Linguistic Analysis
Out of 51,558 sentences, most ? 37,076 (71.9%) ?
contain sentence-internal punctuation. These punc-
tuated sentences contain 123,751 fragments, nearly
all ? 111,774 (90.3%) ? of them multi-token.
Common part-of-speech (POS) sequences compris-
ing fragments are diverse (note also their flat distri-
bution ? see Table 1). The plurality of fragments
are dominated by a clause, but most are dominated
by one of several kinds of phrases (see Table 2).
As expected, punctuation does not occur at all con-
stituent boundaries: Of the top 15 productions that
yield fragments, five do not match the exact brack-
eting of their lowest dominating non-terminal (see
ranks 6, 11, 12, 14 and 15 in Table 3, left). Four of
them miss a left-adjacent clause, e.g., S? S NP VP:
[S [S It?s an overwhelming job], [NP she] [VP says.]]
This production is flagged because the fragment
NP VP is not a constituent ? it is two; still, 49.4%
of all fragments do align with whole constituents.
Inter-punctuation fragments correspond more
strongly to dependencies (see Table 3, right). Only
one production (rank 14) shows a daughter outside
her mother?s fragment. Some number of such pro-
ductions is inevitable and expected, since fragments
must coalesce (i.e., the root of at least one fragment
? in every sentence with sentence-internal punc-
tuation ? must be attached by some word from a
different, external fragment). We find it noteworthy
that in 14 of the 15 most common cases, a word in
an inter-punctuation fragment derives precisely the
rest of that fragment, attaching none of the other,
external words. This is true for 39.2% of all frag-
ments, and if we include fragments whose heads at-
tach other fragments? heads, agreement increases to
74.0% (see strict and loose constraints in ?2.2, next).
2.2 Five Parsing Constraints
Spitkovsky et al (2010b, ?5.3) showed how to ex-
press similar correspondences with markup as pars-
ing constraints. They proposed four constraints but
employed only the strictest three, omitting imple-
mentation details. We revisit their constraints, speci-
fying precise logical formulations that we use in our
code, and introduce a fifth (most relaxed) constraint.
Let [x, y] be a fragment (or markup) spanning po-
sitions x through y (inclusive, with 1 ? x < y ? l), in
a sentence of length l. And let [i, j]h be a sealed span
headed by h (1 ? i ? h ? j ? l), i.e., the word at po-
sition h dominates precisely i . . . j (but none other):
i h j
20
Count Constituent Production Frac Cum
1 7,115 PP? IN NP 5.7%
2 5,950 S? NP VP 4.8 10.6
3 3,450 NP? NP PP 2.8 13.3
4 2,799 SBAR? WHNP S 2.3 15.6
5 2,695 NP? NNP 2.2 17.8
6 2,615 S? S NP VP 2.1 19.9
7 2,480 SBAR? IN S 2.0 21.9
8 2,392 NP? NNP NNP 1.9 23.8
9 2,354 ADVP? RB 1.9 25.7
10 2,334 QP? CD CD 1.9 27.6
11 2,213 S? PP NP VP 1.8 29.4
12 1,441 S? S CC S 1.2 30.6
13 1,317 NP? NP NP 1.1 31.6
14 1,314 S? SBAR NP VP 1.1 32.7
15 1,172 SINV? S VP NP NP 0.9 33.6
WSJ +82,110 more with Count ? 976 66.4%
Count Head-Outward Spawn Frac Cum
1 11,928 IN 9.6%
2 8,852 NN 7.2 16.8
3 7,802 NNP 6.3 23.1
4 4,750 CD 3.8 26.9
5 3,914 VBD 3.2 30.1
6 3,672 VBZ 3.0 33.1
7 3,436 RB 2.8 35.8
8 2,691 VBG 2.2 38.0
9 2,304 VBP 1.9 39.9
10 2,251 NNS 1.8 41.7
11 1,955 WDT 1.6 43.3
12 1,409 MD 1.1 44.4
13 1,377 VBN 1.1 45.5
14 1,204 IN VBD 1.0 46.5
15 927 JJ 0.7 47.3
WSJ +65,279 more with Count ? 846 52.8%
Table 3: Top 15 productions yielding punctuation-induced fragments in WSJ, viewed as constituents (left) and as de-
pendencies (right). For constituents, we recursively expanded any internal nodes that did not align with the associated
fragmentation (underlined). For dependencies we dropped all daughters that fell entirely in the same region as their
mother (i.e., both inside a fragment, both to its left or both to its right), keeping only crossing attachments (just one).
Define inside(h, x, y) as true iff x ? h ? y; and let
cross(i, j, x, y) be true iff (i < x ? j ? x ? j < y) ?
(i > x ? i ? y ? j > y). The three tightest constraints
impose conditions which, when satisfied, disallow
sealing [i, j]h in the presence of an annotation [x, y]:
strict ? requires [x, y] itself to be sealed in the
parse tree, voiding all seals that straddle exactly one
of {x, y} or protrude beyond [x, y] if their head is in-
side. This constraint holds for 39.2% of fragments.
By contrast, only 35.6% of HTML annotations, such
as anchor texts and italics, agree with it (Spitkovsky
et al, 2010b). This necessarily fails in every sen-
tence with internal punctuation (since there, some
fragment must take charge and attach another), when
cross(i, j, x, y) ? (inside(h, x, y) ? (i < x ? j > y)).
... the British daily newspaper, The Financial Times .
x = i h = j = y
loose ? if h ? [x, y], requires that everything in
x . . . y fall under h, with only h allowed external at-
tachments. This holds for 74.0% of fragments ?
87.5% of markup, failing when cross(i, j, x, y).
... arrests followed a ? Snake Day ? at Utrecht ...
i x h = j = y
sprawl ? still requires that h derive x . . . y but
lifts restrictions on external attachments. Holding
for 92.9% of fragments (95.1% of markup), it fails
when cross(i, j, x, y) ? ?inside(h, x, y).
Maryland Club also distributes tea , which ...
x = i h y j
These three strictest constraints lend themselves to a
straight-forward implementation as an O(l5) chart-
based decoder. Ordinarily, the probability of [i, j]h
is computed by multiplying the probability of the as-
sociated unsealed span by two stopping probabilities
? that of the word at h on the left (adjacent if i = h;
non-adjacent if i < h) and on the right (adjacent if
h = j; non-adjacent if h < j). To impose a con-
straint, we ran through all of the annotations [x, y]
associated with a sentence and zeroed out this prob-
ability if any of them satisfied disallowed conditions.
There are faster ? e.g., O(l4), and even O(l3) ?
recognizers for split head automaton grammars (Eis-
ner and Satta, 1999). Perhaps a more practical, but
still clear, approach would be to generate n-best lists
using a more efficient unconstrained algorithm, then
apply the constraints as a post-filtering step.
Relaxed constraints disallow joining adjacent
subtrees, e.g., preventing the seal [i, j]h from merg-
ing below the unsealed span [j +1, J ]H , on the left:
i h j j + 1 H J
21
tear ? prevents x . . . y from being torn apart by
external heads from opposite sides. It holds for
94.7% of fragments (97.9% of markup), and is vi-
olated when (x ? j ? y > j ? h < x), in this case.
... they ?were not consulted about the [Ridley decision]
in advance and were surprised at the action taken .
thread ? requires only that no path from the root
to a leaf enter [x, y] twice. This holds for 95.0% of
all fragments (98.5% of markup); it is violated when
(x ? j ? y > j ? h < x) ? (H ? y), again, in this
case. Example that satisfies thread but violates tear:
The ... changes ?all make a lot of sense to me,? he added.
The case when [i, j]h is to the right is entirely sym-
metric, and these constraints could be incorporated
in a more sophisticated decoder (since i and J do
not appear in the formulae, above). We implemented
them by zeroing out the probability of the word at H
attaching that at h (to its left), in case of a violation.
Note that all five constraints are nested. In partic-
ular, this means that it does not make sense to com-
bine them, for a given annotation [x, y], since the re-
sult would just match the strictest one. Our markup
number for tear is lower (97.9 versus 98.9%) than
Spitkovsky et al?s (2010b), because theirs allowed
cases where markup was neither torn nor threaded.
Common structures that violate thread (and, con-
sequently, all five of the constraints) include, e.g.,
?seamless? quotations and even ordinary lists:
Her recent report classifies the stock as a ?hold.?
The company said its directors, management and
subsidiaries will remain long-term investors and ...
2.3 Comparison with Markup
Most punctuation-induced constraints are less ac-
curate than the corresponding markup-induced con-
straints (e.g., sprawl: 92.9 vs. 95.1%; loose: 74.0
vs. 87.5%; but not strict: 39.2 vs. 35.6%). However,
markup is rare: Spitkovsky et al (2010b, ?5.1) ob-
served that only 10% of the sentences in their blog
were annotated; in contrast, over 70% of the sen-
tences in WSJ are fragmented by punctuation.
Fragments are more than 40% likely to be dom-
inated by a clause; for markup, this number is be-
low 10% ? nearly 75% of it covered by noun
phrases. Further, inter-punctuation fragments are
spread more evenly under noun, verb, prepositional,
adverbial and adjectival phrases (approximately
27:13:10:3:1 versus 75:13:2:1:1) than markup.1
3 The Model, Methods and Metrics
We model grammar via Klein and Manning?s (2004)
Dependency Model with Valence (DMV), which
ordinarily strips out punctuation. Since this step
already requires identification of marks, our tech-
niques are just as ?unsupervised.? We would have
preferred to test punctuation in their original set-up,
but this approach wasn?t optimal, for several rea-
sons. First, Klein and Manning (2004) trained with
short sentences (up to only ten words, on WSJ10),
whereas most punctuation appears in longer sen-
tences. And second, although we could augment
the training data (say, to WSJ45), Spitkovsky et
al. (2010a) showed that classic EM struggles with
longer sentences. For this reason, we use Viterbi
EM and the scaffolding suggested by Spitkovsky et
al. (2010a) ? also the setting in which Spitkovsky et
al. (2010b) tested their markup-induced constraints.
3.1 A Basic System
Our system is based on Laplace-smoothed Viterbi
EM, following Spitkovsky et al?s (2010a) two-stage
scaffolding: the first stage trains with just the sen-
tences up to length 15; the second stage then retrains
on nearly all sentences ? those with up to 45 words.
Initialization
Klein and Manning?s (2004) ?ad-hoc harmonic? ini-
tializer does not work very well for longer sentences,
particularly with Viterbi training (Spitkovsky et al,
2010a, Figure 3). Instead, we use an improved ini-
tializer that approximates the attachment probability
between two words as an average, over all sentences,
of their normalized aggregate weighted distances.
Our weighting function is w(d) = 1+1/ lg(1+d).2
Termination
Spitkovsky et al (2010a) iterated until successive
changes in overall (best parse) per-token cross-
entropy dropped below 2?20 bits. Since smoothing
can (and does, at times) increase the objective, we
found it more efficient to terminate early, after ten
1Markup and fragments are as likely to be in verb phrases.
2Integer d ? 1 is a distance between two tokens; lg is log2.
22
steps of suboptimal models. We used the lowest-
perplexity (not necessarily the last) model found, as
measured by the cross-entropy of the training data.
Constrained Training
Training with punctuation replaces ordinary Viterbi
parse trees, at every iteration of EM, with the out-
put of a constrained decoder. In all experiments
other than #2 (?5) we train with the loose constraint.
Spitkovsky et al (2010b) found this setting to be
best for markup-induced constraints. We apply it to
constraints induced by inter-punctuation fragments.
Constrained Inference
Spitkovsky et al (2010b) recommended using the
sprawl constraint in inference. Once again, we fol-
low their advice in all experiments except #2 (?5).
3.2 Data Sets and Scoring
We trained on the Penn English Treebank?s Wall
Street Journal portion (Marcus et al, 1993). To eval-
uate, we automatically converted its labeled con-
stituents into unlabeled dependencies, using deter-
ministic ?head-percolation? rules (Collins, 1999),
discarding punctuation, any empty nodes, etc., as is
standard practice (Paskin, 2001; Klein and Manning,
2004). We also evaluated against the parsed portion
of the Brown corpus (Francis and Kuc?era, 1979),
used as a blind, out-of-domain evaluation set,3 sim-
ilarly derived from labeled constituent parse trees.
We report directed accuracies ? fractions of cor-
rectly guessed arcs, including the root, in unlabeled
reference dependency parse trees, as is also standard
practice (Paskin, 2001; Klein and Manning, 2004).
One of our baseline systems (?3.3) produces depen-
dency trees containing punctuation. In this case we
do not score the heads assigned to punctuation and
use forgiving scoring for regular words: crediting
correct heads separated from their children by punc-
tuation alone (from the point of view of the child,
looking up to the nearest non-punctuation ancestor).
3.3 Baseline Systems
Our primary baseline is the basic system without
constraints (standard training). It ignores punctu-
ation, as is standard, scoring 52.0% against WSJ45.
A secondary (punctuation as words) baseline in-
3Note that WSJ{15, 45} overlap with Section 23 ? training
on the test set is standard practice in unsupervised learning.
corporates punctuation into the grammar as if it were
words, as in supervised dependency parsing (Nivre
et al, 2007b; Lin, 1998; Sleator and Temperley,
1993, inter alia). It is worse, scoring only 41.0%.4,5
4 Experiment #1: Default Constraints
Our first experiment compares ?punctuation as con-
straints? to the baseline systems. We use default set-
tings, as recommended by Spitkovsky et al (2010b):
loose in training; and sprawl in inference. Evalua-
tion is on Section 23 of WSJ (all sentence lengths).
To facilitate comparison with prior work, we also re-
port accuracies against shorter sentences, with up to
ten non-punctuation tokens (WSJ10 ? see Table 4).
We find that both constrained regimes improve
performance. Constrained decoding alone increases
the accuracy of a standardly-trained system from
52.0% to 54.0%. And constrained training yields
55.6% ? 57.4% in combination with inference.
4We were careful to use exactly the same data sets in both
cases, not counting punctuation towards sentence lengths. And
we used forgiving scoring (?3.2) when evaluating these trees.
5To get this particular number we forced punctuation to be
tacked on, as a layer below the tree of words, to fairly compare
systems (using the same initializer). Since improved initializa-
tion strategies ? both ours and Klein and Manning?s (2004)
?ad-hoc harmonic? initializer ? rely on distances between to-
kens, they could be unfairly biased towards one approach or the
other, if punctuation counted towards length. We also trained
similar baselines without restrictions, allowing punctuation to
appear anywhere in the tree (still with forgiving scoring ? see
?3.2), using the uninformed uniform initializer (Spitkovsky et
al., 2010a). Disallowing punctuation as a parent of a real word
made things worse, suggesting that not all marks belong near
the leaves (sentence stops, semicolons, colons, etc. make more
sense as roots and heads). We tried the weighted initializer also
without restrictions and repeated all experiments without scaf-
folding, on WSJ15 and WSJ45 alone, but treating punctuation
as words never came within even 5% of (comparable) standard
training. Punctuation, as words, reliably disrupted learning.
WSJ? WSJ10
Supervised DMV 69.8 83.6
w/Constrained Inference 73.0 84.3
Punctuation as Words 41.7 54.8
Standard Training 52.0 63.2
w/Constrained Inference 54.0 63.6
Constrained Training 55.6 67.0
w/Constrained Inference 57.4 67.5
Table 4: Directed accuracies on Section 23 of WSJ? and
WSJ10 for the supervised DMV, our baseline systems and
the punctuation runs (all using the weighted initializer).
23
These are multi-point increases, but they could dis-
appear in a more accurate state-of-the-art system.
To test this hypothesis, we applied constrained de-
coding to a supervised system. We found that this
(ideal) instantiation of the DMV benefits as much or
more than the unsupervised systems: accuracy in-
creases from 69.8% to 73.0%. Punctuation seems
to capture the kinds of, perhaps long-distance, regu-
larities that are not accessible to the model, possibly
because of its unrealistic independence assumptions.
5 Experiment #2: Optimal Settings
Spitkovsky et al (2010b) recommended training
with loose and decoding with sprawl based on their
experiments with markup. But are these the right
settings for punctuation? Inter-punctuation frag-
ments are quite different from markup ? they are
more prevalent but less accurate. Furthermore, we
introduced a new constraint, thread, that Spitkovsky
et al (2010b) had not considered (along with tear).
We next re-examined the choices of constraints.
Our full factorial analysis was similar, but signifi-
cantly smaller, than Spitkovsky et al?s (2010b): we
excluded their larger-scale news and web data sets
that are not publicly available. Nevertheless, we
still tried every meaningful combination of settings,
testing both thread and tear (instead of strict, since
it can?t work with sentences containing sentence-
internal punctuation), in both training and inference.
We did not find better settings than loose for train-
ing, and sprawl for decoding, among our options.
A full analysis is omitted due to space constraints.
Our first observation is that constrained inference,
using punctuation, is helpful and robust. It boosted
accuracy (on WSJ45) by approximately 1.5%, on
average, with all settings. Indeed, sprawl was con-
sistently (but only slightly, at 1.6%, on average) bet-
ter than the rest. Second, constrained training hurt
more often than it helped. It degraded accuracy in all
but one case, loose, where it gained approximately
0.4%, on average. Both improvements are statisti-
cally significant: p ? 0.036 for training with loose;
and p ? 5.6? 10?12 for decoding with sprawl.
6 More Advanced Methods
So far, punctuation has improved grammar induction
in a toy setting. But would it help a modern system?
Our next two experiments employ a slightly more
complicated set-up, compared with the one used up
until now (?3.1). The key difference is that this sys-
tem is lexicalized, as is standard among the more ac-
curate grammar inducers (Blunsom and Cohn, 2010;
Gillenwater et al, 2010; Headden et al, 2009).
Lexicalization
We lexicalize only in the second (full data) stage, us-
ing the method of Headden et al (2009). For words
seen at least 100 times in the training corpus, we
augment their gold POS tag with the lexical item.
The first (data poor) stage remains entirely unlexi-
calized, with gold POS tags for word classes, as in
the earlier systems (Klein and Manning, 2004).
Smoothing
We do not use smoothing in the second stage except
at the end, for the final lexicalized model. Stage one
still applies ?add-one? smoothing at every iteration.
7 Experiment #3: State-of-the-Art
The purpose of these experiments is to compare the
punctuation-enhanced DMV with other, recent state-
of-the-art systems. We find that, lexicalized (?6), our
approach performs better, by a wide margin; without
lexicalization (?3.1), it was already better for longer,
but not for shorter, sentences (see Tables 5 and 4).
We trained a variant of our system without gold
part-of-speech tags, using the unsupervised word
clusters (Clark, 2000) computed by Finkel and Man-
ning (2009).6 Accuracy decreased slightly, to 58.2%
on Section 23 of WSJ (down only 0.2%). This result
improves over substantial performance degradations
previously observed for unsupervised dependency
parsing with induced word categories (Klein and
Manning, 2004; Headden et al, 2008, inter alia).
6Available from http://nlp.stanford.edu/software/
stanford-postagger-2008-09-28.tar.gz:
models/egw.bnc.200
Brown WSJ? WSJ10
(Headden et al, 2009) ? ? 68.8
(Spitkovsky et al, 2010b) 53.3 50.4 69.3
(Gillenwater et al, 2010) ? 53.3 64.3
(Blunsom and Cohn, 2010) ? 55.7 67.7
Constrained Training 58.4 58.0 69.3
w/Constrained Inference 59.5 58.4 69.5
Table 5: Accuracies on the out-of-domain Brown100 set
and Section 23 of WSJ? and WSJ10, for the lexicalized
punctuation run and other recent state-of-the-art systems.
24
Unlexicalized, Unpunctuated Lexicalized ... and Punctuated
CoNLL Year Initialization @15 Training @15 Retraining @45 Retraining @45 Net
& Language 1. w/Inference 2. w/Inference 3. w/Inference 3?. w/Inference Gain
Arabic 2006 23.3 23.6 (+0.3) 32.8 33.1 (+0.4) 31.5 31.6 (+0.1) 32.1 32.6 (+0.5) +1.1
?7 25.6 26.4 (+0.8) 33.7 34.2 (+0.5) 32.7 33.6 (+0.9) 34.9 35.3 (+0.4) +2.6
Basque ?7 19.3 20.8 (+1.5) 29.9 30.9 (+1.0) 29.3 30.1 (+0.8) 29.3 29.9 (+0.6) +0.6
Bulgarian ?6 23.7 24.7 (+1.0) 39.3 40.7 (+1.4) 38.8 39.9 (+1.1) 39.9 40.5 (+0.6) +1.6
Catalan ?7 33.2 34.1 (+0.8) 54.8 55.5 (+0.7) 54.3 55.1 (+0.8) 54.3 55.2 (+0.9) +0.9
Czech ?6 18.6 19.6 (+1.0) 34.6 35.8 (+1.2) 34.8 35.7 (+0.9) 37.0 37.8 (+0.8) +3.0
?7 17.6 18.4 (+0.8) 33.5 35.4 (+1.9) 33.4 34.4 (+1.0) 35.2 36.2 (+1.0) +2.7
Danish ?6 22.9 24.0 (+1.1) 35.6 36.7 (+1.2) 36.9 37.8 (+0.9) 36.5 37.1 (+0.6) +0.2
Dutch ?6 15.8 16.5 (+0.7) 11.2 12.5 (+1.3) 11.0 11.9 (+1.0) 13.7 14.0 (+0.3) +3.0
English ?7 25.0 25.4 (+0.5) 47.2 49.5 (+2.3) 47.5 48.8 (+1.3) 49.3 50.3 (+0.9) +2.8
German ?6 19.2 19.6 (+0.4) 27.4 28.0 (+0.7) 27.0 27.8 (+0.8) 28.2 28.6 (+0.4) +1.6
Greek ?7 18.5 18.8 (+0.3) 20.7 21.4 (+0.7) 20.5 21.0 (+0.5) 20.9 21.2 (+0.3) +0.7
Hungarian ?7 17.4 17.7 (+0.3) 6.7 7.2 (+0.5) 6.6 7.0 (+0.4) 7.8 8.0 (+0.2) +1.4
Italian ?7 25.0 26.3 (+1.2) 29.6 29.9 (+0.3) 29.7 29.7 (+0.1) 28.3 28.8 (+0.5) -0.8
Japanese ?6 30.0 30.0 (+0.0) 27.3 27.3 (+0.0) 27.4 27.4 (+0.0) 27.5 27.5 (+0.0) +0.1
Portuguese ?6 27.3 27.5 (+0.2) 32.8 33.7 (+0.9) 32.7 33.4 (+0.7) 33.3 33.5 (+0.3) +0.8
Slovenian ?6 21.8 21.9 (+0.2) 28.3 30.4 (+2.1) 28.4 30.4 (+2.0) 29.8 31.2 (+1.4) +2.8
Spanish ?6 25.3 26.2 (+0.9) 31.7 32.4 (+0.7) 31.6 32.3 (+0.8) 31.9 32.3 (+0.5) +0.8
Swedish ?6 31.0 31.5 (+0.6) 44.1 45.2 (+1.1) 45.6 46.1 (+0.5) 46.1 46.4 (+0.3) +0.8
Turkish ?6 22.3 22.9 (+0.6) 39.1 39.5 (+0.4) 39.9 39.9 (+0.1) 40.6 40.9 (+0.3) +1.0
?7 22.7 23.3 (+0.6) 41.7 42.3 (+0.6) 41.9 42.1 (+0.2) 41.6 42.0 (+0.4) +0.1
Average: 23.4 24.0 (+0.7) 31.9 32.9 (+1.0) 31.9 32.6 (+0.7) 32.6 33.2 (+0.5) +1.3
Table 6: Multi-lingual evaluation for CoNLL sets, measured at all three stages of training, with and without constraints.
8 Experiment #4: Multi-Lingual Testing
This final batch of experiments probes the general-
ization of our approach (?6) across languages. The
data are from 2006/7 CoNLL shared tasks (Buch-
holz and Marsi, 2006; Nivre et al, 2007a), where
punctuation was identified by the organizers, who
also furnished disjoint train/test splits. We tested
against all sentences in their evaluation sets.7,8
The gains are not English-specific (see Table 6).
Every language improves with constrained decod-
ing (more so without constrained training); and all
but Italian benefit in combination. Averaged across
all eighteen languages, the net change in accuracy is
1.3%. After standard training, constrained decoding
alone delivers a 0.7% gain, on average, never caus-
ing harm in any of our experiments. These gains are
statistically significant: p ? 1.59 ? 10?5 for con-
strained training; and p ? 4.27?10?7 for inference.
7With the exception of Arabic ?07, from which we discarded
one sentence with 145 tokens. We down-weighed languages
appearing in both years by 50% in our analyses, and excluded
Chinese entirely, since it had already been cut up at punctuation.
8Note that punctuation was treated differently in the two
years: in ?06, it was always at the leaves of the dependency
trees; in ?07, it matched original annotations of the source tree-
banks. For both, we used punctuation-insensitive scoring (?3.2).
We did not detect synergy between the two im-
provements. However, note that without constrained
training, ?full? data sets do not help, on average, de-
spite having more data and lexicalization. Further-
more, after constrained training, we detected no ev-
idence of benefits to additional retraining: not with
the relaxed sprawl constraint, nor unconstrained.
9 Related Work
Punctuation has been used to improve parsing since
rule-based systems (Jones, 1994). Statistical parsers
reap dramatic gains from punctuation (Engel et al,
2002; Roark, 2001; Charniak, 2000; Johnson, 1998;
Collins, 1997, inter alia). And it is even known to
help in unsupervised constituent parsing (Seginer,
2007). But for dependency grammar induction, until
now, punctuation remained unexploited.
Parsing Techniques Most-Similar to Constraints
A ?divide-and-rule? strategy that relies on punctua-
tion has been used in supervised constituent parsing
of long Chinese sentences (Li et al, 2005). For En-
glish, there has been interest in balanced punctua-
tion (Briscoe, 1994), more recently using rule-based
filters (White and Rajkumar, 2008) in a combinatory
categorial grammar (CCG). Our focus is specifically
25
on unsupervised learning of dependency grammars
and is similar, in spirit, to Eisner and Smith?s (2005)
?vine grammar? formalism. An important difference
is that instead of imposing static limits on allowed
dependency lengths, our restrictions are dynamic ?
they disallow some long (and some short) arcs that
would have otherwise crossed nearby punctuation.
Incorporating partial bracketings into grammar
induction is an idea tracing back to Pereira and Sch-
abes (1992). It inspired Spitkovsky et al (2010b) to
mine parsing constraints from the web. In that same
vein, we prospected a more abundant and natural
language-resource ? punctuation, using constraint-
based techniques they developed for web markup.
Modern Unsupervised Dependency Parsing
State-of-the-art in unsupervised dependency pars-
ing (Blunsom and Cohn, 2010) uses tree substitu-
tion grammars. These are powerful models, capa-
ble of learning large dependency fragments. To help
prevent overfitting, a non-parametric Bayesian prior,
defined by a hierarchical Pitman-Yor process (Pit-
man and Yor, 1997), is trusted to nudge training to-
wards fewer and smaller grammatical productions.
We pursued a complementary strategy: using
Klein and Manning?s (2004) much simpler Depen-
dency Model with Valence (DMV), but persistently
steering training away from certain constructions, as
guided by punctuation, to help prevent underfitting.
Various Other Uses of Punctuation in NLP
Punctuation is hard to predict,9 partly because it
can signal long-range dependences (Lu and Ng,
2010). It often provides valuable cues to NLP tasks
such as part-of-speech tagging and named-entity
recognition (Hillard et al, 2006), information ex-
traction (Favre et al, 2008) and machine transla-
tion (Lee et al, 2006; Matusov et al, 2006). Other
applications have included Japanese sentence anal-
ysis (Ohyama et al, 1986), genre detection (Sta-
matatos et al, 2000), bilingual sentence align-
ment (Yeh, 2003), semantic role labeling (Pradhan et
al., 2005), Chinese creation-title recognition (Chen
and Chen, 2005) and word segmentation (Li and
Sun, 2009), plus, recently, automatic vandalism de-
9Punctuation has high semantic entropy (Melamed, 1997);
for an analysis of the many roles played in the WSJ by the
comma ? the most frequent and unpredictable punctuation
mark in that data set ? see Beeferman et al (1998, Table 2).
tection in Wikipedia (Wang and McKeown, 2010).
10 Conclusions and Future Work
Punctuation improves dependency grammar induc-
tion. Many unsupervised (and supervised) parsers
could be easily modified to use sprawl-constrained
decoding in inference. It applies to pre-trained mod-
els and, so far, helped every data set and language.
Tightly interwoven into the fabric of writing sys-
tems, punctuation frames most unannotated plain-
text. We showed that rules for converting markup
into accurate parsing constraints are still optimal for
inter-punctuation fragments. Punctuation marks are
more ubiquitous and natural than web markup: what
little punctuation-induced constraints lack in preci-
sion, they more than make up in recall ? perhaps
both types of constraints would work better yet in
tandem. For language acquisition, a natural ques-
tion is whether prosody could similarly aid grammar
induction from speech (Kahn et al, 2005).
Our results underscore the power of simple mod-
els and algorithms, combined with common-sense
constraints. They reinforce insights from joint mod-
eling in supervised learning, where simplified, in-
dependent models, Viterbi decoding and expressive
constraints excel at sequence labeling tasks (Roth
and Yih, 2005). Such evidence is particularly wel-
come in unsupervised settings (Punyakanok et al,
2005), where it is crucial that systems scale grace-
fully to volumes of data, on top of the usual desider-
ata ? ease of implementation, extension, under-
standing and debugging. Future work could explore
softening constraints (Hayes and Mouradian, 1980;
Chang et al, 2007), perhaps using features (Eisner
and Smith, 2005; Berg-Kirkpatrick et al, 2010) or
by learning to associate different settings with var-
ious marks: Simply adding a hidden tag for ?ordi-
nary? versus ?divide? types of punctuation (Li et al,
2005) may already usefully extend our model.
Acknowledgments
Partially funded by the Air Force Research Laboratory (AFRL),
under prime contract no. FA8750-09-C-0181, and by NSF, via
award #IIS-0811974. We thank Omri Abend, Slav Petrov and
anonymous reviewers for many helpful suggestions, and we are
especially grateful to Jenny R. Finkel for shaming us into using
punctuation, to Christopher D. Manning for reminding us to ex-
plore ?punctuation as words? baselines, and to Noah A. Smith
for encouraging us to test against languages other than English.
26
References
D. Beeferman, A. Berger, and J. Lafferty. 1998.
CYBERPUNC: A lightweight punctuation annotation
system for speech. In ICASSP.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In NAACL-HLT.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In EMNLP.
E. J. Briscoe. 1994. Parsing (with) punctuation, etc.
Technical report, Xerox European Research Labora-
tory.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. Technical report, Brown University.
M.-W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
ACL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL.
C. Chen and H.-H. Chen. 2005. Integrating punctuation
rules and na??ve Bayesian model for Chinese creation
title recognition. In IJCNLP.
A. Clark. 2000. Inducing syntactic categories by context
distribution clustering. In CoNLL-LLL.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
C. D. Doran. 1998. Incorporating Punctuation into
the Sentence Grammar: A Lexicalized Tree Adjoin-
ing Grammar Perspective. Ph.D. thesis, University of
Pennsylvania.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexi-
cal context-free grammars and head-automaton gram-
mars. In ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft and
hard constraints on dependency length. In IWPT.
D. Engel, E. Charniak, and M. Johnson. 2002. Parsing
and disfluency placement. In EMNLP.
B. Favre, R. Grishman, D. Hillard, H. Ji, D. Hakkani-
Tu?r, and M. Ostendorf. 2008. Punctuating speech for
information extraction. In ICASSP.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL-HLT.
W. N. Francis and H. Kuc?era, 1979. Manual of Informa-
tion to Accompany a Standard Corpus of Present-Day
Edited American English, for use with Digital Com-
puters. Department of Linguistics, Brown University.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Technical report, University of
Pennsylvania.
A. Gravano, M. Jansche, and M. Bacchiani. 2009.
Restoring punctuation and capitalization in transcribed
speech. In ICASSP.
P. J. Hayes and G. V. Mouradian. 1980. Flexible parsing.
In ACL.
W. P. Headden, III, D. McClosky, and E. Charniak.
2008. Evaluating unsupervised part-of-speech tagging
for grammar induction. In COLING.
W. P. Headden, III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In NAACL-HLT.
D. Hillard, Z. Huang, H. Ji, R. Grishman, D. Hakkani-
Tu?r, M. Harper, M. Ostendorf, and W. Wang. 2006.
Impact of automatic comma prediction on POS/name
tagging of speech. In IEEE/ACL: SLT.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24.
B. E. M. Jones. 1994. Exploring the role of punctuation
in parsing natural text. COLING.
J. G. Kahn, M. Lease, E. Charniak, M. Johnson, and
M. Ostendorf. 2005. Effective use of prosody in pars-
ing conversational speech. In HLT-EMNLP.
J.-H. Kim and P. C. Woodland. 2002. Implementation of
automatic capitalisation generation systems for speech
input. In ICASSP.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
Y.-S. Lee, S. Roukos, Y. Al-Onaizan, and K. Papineni.
2006. IBM spoken language translation system. In
TC-STAR: Speech-to-Speech Translation.
Z. Li and M. Sun. 2009. Punctuation as implicit annota-
tions for Chinese word segmentation. Computational
Linguistics, 35.
X. Li, C. Zong, and R. Hu. 2005. A hierarchical parsing
approach with punctuation processing for long Chi-
nese sentences. In IJCNLP.
D. Lin. 1998. Dependency-based evaluation of MINI-
PAR. In Evaluation of Parsing Systems.
W. Lu and H. T. Ng. 2010. Better punctuation prediction
with dynamic conditional random fields. In EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
E. Matusov, A. Mauser, and H. Ney. 2006. Automatic
sentence segmentation and punctuation prediction for
spoken language translation. In IWSLT.
I. D. Melamed. 1997. Measuring semantic entropy. In
ACL-SIGLEX: Tagging Text with Lexical Semantics.
27
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007a. The CoNLL
2007 shared task on dependency parsing. In EMNLP-
CoNLL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryig?it,
S. Ku?bler, S. Marinov, and E. Marsi. 2007b. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13.
G. Nunberg. 1990. The Linguistics of Punctuation.
CSLI Publications.
Y. Ohyama, T. Fukushima, T. Shutoh, and M. Shutoh.
1986. A sentence analysis method for a Japanese book
reading machine for the blind. In ACL.
M. A. Paskin. 2001. Grammatical bigrams. In NIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In ACL.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25.
S. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and
D. Jurafsky. 2005. Semantic role chunking combin-
ing complementary syntactic views. In CoNLL.
V. Punyakanok, D. Roth, W.-t. Yih, and D. Zimak. 2005.
Learning and inference over constrained output. In IJ-
CAI.
B. E. Roark. 2001. Robust Probabilistic Predictive Syn-
tactic Processing: Motivations, Models, and Applica-
tions. Ph.D. thesis, Brown University.
D. Roth and W.-t. Yih. 2005. Integer linear programming
inference for conditional random fields. In ICML.
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In ACL.
D. D. Sleator and D. Temperley. 1993. Parsing English
with a link grammar. In IWPT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010a. Viterbi training improves unsupervised
dependency parsing. In CoNLL.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010b.
Profiting from mark-up: Hyper-text annotations for
guided parsing. In ACL.
E. Stamatatos, N. Fakotakis, and G. Kokkinakis. 2000.
Text genre detection using common word frequencies.
In COLING.
W. Y. Wang and K. R. McKeown. 2010. ?Got you!?: Au-
tomatic vandalism detection in Wikipedia with web-
based shallow syntactic-semantic modeling. In COL-
ING.
M. White and R. Rajkumar. 2008. A more precise analy-
sis of punctuation for broad-coverage surface realiza-
tion with CCG. In GEAF.
K. C. Yeh. 2003. Bilingual sentence alignment based on
punctuation marks. In ROCLING: Student.
28
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 124?132,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
A Study of Academic Collaboration in Computational Linguistics with
Latent Mixtures of Authors
Nikhil Johri, Daniel Ramage
Department of Computer Science
Stanford University
Stanford, CA, USA
Daniel A. McFarland
School of Education
Stanford University
Stanford, CA, USA
{njohri2,dramage,dmcfarla,jurafsky}@stanford.edu
Daniel Jurafsky
Department of Linguistics
Stanford University
Stanford, CA, USA
Abstract
Academic collaboration has often been at
the forefront of scientific progress, whether
amongst prominent established researchers, or
between students and advisors. We suggest a
theory of the different types of academic col-
laboration, and use topic models to computa-
tionally identify these in Computational Lin-
guistics literature. A set of author-specific
topics are learnt over the ACL corpus, which
ranges from 1965 to 2009. The models are
trained on a per year basis, whereby only pa-
pers published up until a given year are used
to learn that year?s author topics. To determine
the collaborative properties of papers, we use,
as a metric, a function of the cosine similarity
score between a paper?s term vector and each
author?s topic signature in the year preceding
the paper?s publication. We apply this metric
to examine questions on the nature of collabo-
rations in Computational Linguistics research,
finding that significant variations exist in the
way people collaborate within different sub-
fields.
1 Introduction
Academic collaboration is on the rise as single au-
thored work becomes less common across the sci-
ences (Rawlings and McFarland, 2011; Jones et al,
2008; Newman, 2001). In part, this rise can be at-
tributed to the increasing specialization of individual
academics and the broadening in scope of the prob-
lems they tackle. But there are other advantages to
collaboration, as well: they can speed up produc-
tion, diffuse knowledge across authors, help train
new scientists, and are thought to encourage greater
innovation. Moreover, they can integrate scholarly
communities and foster knowledge transfer between
related fields. But all collaborations aren?t the same:
different collaborators contribute different material,
assume different roles, and experience the collabo-
ration in different ways. In this paper, we present
a new frame for thinking about the variation in col-
laboration types and develop a computational metric
to characterize the distinct contributions and roles of
each collaborator within the scholarly material they
produce.
The topic of understanding collaborations has at-
tracted much interest in the social sciences over the
years. Recently, it has gained traction in computer
science, too, in the form of social network analysis.
Much work focuses on studying networks formed
via citations (Radev et al, 2009; White and Mccain,
1998), as well as co-authorship links (Nascimento
et al, 2003; Liu et al, 2005). However, these works
focus largely on the graphical structure derived from
paper citations and author co-occurrences, and less
on the textual content of the papers themselves. In
this work, we examine the nature of academic col-
laboration using text as a primary component.
We propose a theoretical framework for determin-
ing the types of collaboration present in a docu-
ment, based on factors such as the number of es-
tablished authors, the presence of unestablished au-
thors and the similarity of the established authors?
past work to the document?s term vector. These col-
laboration types attempt to describe the nature of co-
authorships between students and advisors (e.g. ?ap-
prentice? versus ?new blood?) as well as those solely
between established authors in the field. We present
a decision diagram for classifying papers into these
types, as well as a description of the intuition behind
each collaboration class.
124
We explore our theory with a computational
method to categorize collaborative works into their
collaboration types using an approach based on topic
modeling, where we model every paper as a la-
tent mixture of its authors. For our system, we use
Labeled-LDA (LLDA (Ramage et al, 2009)) to train
models over the ACL corpus for every year of the
words best attributed to each author in all the papers
they write. We use the resulting author signatures
as a basis for several metrics that can classify each
document by its collaboration type.
We qualitatively analyze our results by examin-
ing the categorization of several high impact papers.
With consultation from prominent researchers and
textbook writers in the field, we demonstrate that our
system is able to differentiate between the various
types of collaborations in our suggested taxonomy,
based only on words used, at low but statistically
significant accuracy. We use this same similarity
score to analyze the ACL community by sub-field,
finding significant deviations.
2 Related Work
In recent years, popular topic models such as La-
tent Dirichlet Allocation (Blei et al, 2003) have
been increasingly used to study the history of sci-
ence by observing the changing trends in term based
topics (Hall et al, 2008), (Gerrish and Blei, 2010).
In the case of Hall et al, regular LDA topic mod-
els were trained over the ACL anthology on a per
year basis, and the changing trends in topics were
studied from year to year. Gerrish and Blei?s work
computed a measure of influence by using Dynamic
Topic Models (Blei and Lafferty, 2006) and study-
ing the change of statistics of the language used in a
corpus.
These models propose interesting ideas for utiliz-
ing topic modeling to understand aspects of scien-
tific history. However, our primary interest, in this
paper, is the study of academic collaboration be-
tween different authors; we therefore look to learn
models for authors instead of only documents. Pop-
ular topic models for authors include the Author-
Topic Model (Rosen-Zvi et al, 2004), a simple
extension of regular LDA that adds an additional
author variable over the topics. The Author-Topic
Model learns a distribution over words for each
topic, as in regular LDA, as well as a distribution
over topics for each author. Alternatively, Labeled
LDA (Ramage et al, 2009), another LDA variation,
offers us the ability to directly model authors as top-
ics by considering them to be the topic labels for the
documents they author.
In this work, we use Labeled LDA to directly
model probabilistic term ?signatures? for authors. As
in (Hall et al, 2008) and (Gerrish and Blei, 2010),
we learn a new topic model for each year in the cor-
pus, allowing us to account for changing author in-
terests over time.
3 Computational Methodology
The experiments and results discussed in this paper
are based on a variation of the LDA topic model run
over data from the ACL corpus.
3.1 Dataset
We use the ACL anthology from years 1965 to 2009,
training over 12,908 papers authored by over 11,355
unique authors. We train our per year topic mod-
els over the entire dataset; however, when evaluating
our results, we are only concerned with papers that
were authored by multiple individuals as the other
papers are not collaborations.
3.2 Latent Mixture of Authors
Every abstract in our dataset reflects the work, to
some greater or lesser degree, of all the authors of
that work. We model these degrees explicitly us-
ing a latent mixture of authors model, which takes
its inspiration from the learning machinery of LDA
(Blei et al, 2003) and its supervised variant La-
beled LDA (Ramage et al, 2009). These models
assume that documents are as a mixture of ?topics,?
which themselves are probability distributions over
the words in the vocabulary of the corpus. LDA
is completely unsupervised, assuming that a latent
topic layer exists and that each word is generated
from one underlying topic from this set of latent top-
ics. For our purposes, we use a variation of LDA in
which we assume each document to be a latent mix-
ture of its authors. Unlike LDA, where each docu-
ment draws a multinomial over all topics, the latent
mixture of authors model we use restricts a docu-
ment to only sample from topics corresponding to
125
its authors. Also, unlike models such as the Author-
Topic Model (Rosen-Zvi et al, 2004), where au-
thors are modeled as distributions over latent top-
ics, our model associates each author to exactly one
topic, modeling authors directly as distributions over
words.
Like other topic models, we will assume a genera-
tive process for our collection of D documents from
a vocabulary of size V . We assume that each docu-
ment d has Nd terms and Md authors from a set of
authors A. Each author is described by a multino-
mial distribution ?a over words V , which is initially
unobserved. We will recover for each document a
hidden multinomial ?(d) of length Md that describes
which mixture of authors? best describes the doc-
ument. This multinomial is in turn drawn from a
symmetric Dirichlet distribution with parameter ?
restrict to the set of authors ?(d) for that paper. Each
document?s words are generated by first picking an
author zi from ?(d) and then drawing a word from
the corresponding author?s word distribution. For-
mally, the generative process is as follows:
? For each author a, generate a distribution ?a over
the vocabulary from a Dirichlet prior ?
? For each document d, generate a multinomial mix-
ture distribution ?(d) ? Dir(?.1?(d))
? For each document d,
? For each i ? {1, ..., Nd}
? Generate zi ? {?
(d)
1 , ..., ?
(d)
Md
} ?
Mult(?(d))
? Generate wi ? {1, ..., V } ?Mult(?zi)
We use Gibbs sampling to perform inference in
this model. If we consider our authors as a label
space, this model is equivalent to that of Labeled
LDA (Ramage et al, 2009), which we use for in-
ference in our model, using the variational objec-
tive in the open source implementation1. After in-
ference, our model discovers the distribution over
terms that best describes that author?s work in the
presence of other authors. This distribution serves
as a ?signature? for an author and is dominated by
the terms that author uses frequently across collabo-
rations. It is worth noting that this model constrains
the learned ?topics? to authors, ensuring directly in-
terpretable results that do not require the interpreta-
1http://nlp.stanford.edu/software/tmt/
tion of a latent topic space, such as in (Rosen-Zvi et
al., 2004).
To imbue our model with a notion of time, we
train a separate LLDA model for each year in the
corpus, training on only those papers written before
and during the given year. Thus, we have separate
?signatures? for each author for each year, and each
signature only contains information for the specific
author?s work up to and including the given year.
Table 1 contains examples of such term signatures
computed for two authors in different years. The top
terms and their fractional counts are displayed.
4 Studying Collaborations
There are several ways one can envision to differen-
tiate between types of academic collaborations. We
focus on three factors when creating collaboration
labels, namely:
? Presence of unestablished authors
? Similarity to established authors
? Number of established authors
If an author whom we know little about is present
on a collaborative paper, we consider him or her to
be a new author. We threshold new authors by the
number of papers they have written up to the pub-
lication year of the paper we are observing. De-
pending on whether this number is below or above a
threshold value, we consider an author to be estab-
lished or unestablished in the given year.
Similarity scores are measured using the trained
LLDA models described in Section 3.2. For any
given paper, we measure the similarity of the pa-
per to one of its (established) authors by calculating
the cosine similarity of the author?s signature in the
year preceding the paper?s publication to the paper?s
term-vector.
Using the aforementioned three factors, we define
the following types of collaborations:
? Apprenticeship Papers are authored by one or
more established authors and one or more un-
established authors, such that the similarity of
the paper to more than half of the established
authors is high. In this case, we say that the
new author (or authors) was an apprentice of
126
Philipp Koehn, 2002 Philipp Koehn, 2009 Fernando Pereira, 1985 Fernando Pereira, 2009
Terms Counts Terms Counts Terms Counts Terms Counts
word 3.00 translation 69.78 grammar 14.99 type 40.00
lexicon 2.00 machine 34.67 phrase 10.00 phrase 30.89
noun 2.00 phrase 26.85 structure 7.00 free 23.14
similar 2.00 english 23.86 types 6.00 grammar 23.10
translation 1.29 statistical 19.51 formalisms 5.97 constraint 23.00
purely 0.90 systems 18.32 sharing 5.00 logical 22.41
accuracy 0.90 word 16.38 unification 4.97 rules 21.72
Table 1: Example term ?signatures? computed by running a Labeled LDA model over authors in the ACL corpus on a
per year basis: top terms for two authors in different years are shown alongside their fractional counts.
the established authors, continuing in their line
of work.
? New Blood Papers are authored by one estab-
lished author and one or more unestablished au-
thors, such that the similarity of the paper to the
established author is low. In this case, we say
that the new author (or authors) provided new
ideas or worked in an area that was dissimilar to
that which the established author was working
in.
? Synergistic Papers are authored only by es-
tablished authors such that it does not heavily
resemble any authors? previous work. In this
case, we consider the paper to be a product of
synergy of its authors.
? Catalyst Papers are similar to synergistic
ones, with the exception that unestablished au-
thors are also present on a Catalyst Paper. In
this case, we hypothesize that the unestablished
authors were the catalysts responsible for get-
ting the established authors to work on a topic
dissimilar to their previous work.
The decision diagram in Figure 1 presents an easy
way to determine the collaboration type assigned to
a paper.
5 Quantifying Collaborations
Following the decision diagram presented in Figure
1 and using similarity scores based on the values
returned by our latent author mixture models (Sec-
tion 3.2), we can deduce the collaboration type to
assign to any given paper. However, absolute cate-
gorization requires an additional thresholding of au-
thor similarity scores. To avoid the addition of an
arbitrary threshold, instead of directly categorizing
papers, we rank them based on the calculated sim-
ilarity scores on three different spectra. To facili-
tate ease of interpretation, the qualitative examples
we present are drawn from high PageRank papers as
calculated in (Radev et al, 2009).
5.1 The MaxSim Score
To measure the similarity of authors? previous work
to a paper, we look at the cosine similarity between
the term vector of the paper and each author?s term
signature. We are only interested in the highest co-
sine similarity score produced by an author, as our
categories do not differentiate between papers that
are similar to one author and papers that are sim-
ilar to multiple authors, as long as high similarity
to any single author is present. Thus, we choose
our measure, the MaxSim score, to be defined as:
max
a?est
cos(asig, paper)
We choose to observe the similarity scores only
for established authors as newer authors will not
have enough previous work to produce a stable term
signature, and we vary the experience threshold by
year to account for the fact that there has been a large
increase in the absolute number of papers published
in recent years.
Depending on the presence of new authors and
the number of established authors present, each pa-
per can be placed into one of the three spectra: the
Apprenticeship-New Blood spectrum, the Synergy
spectrum and the Apprenticeship-Catalyst spectrum.
Apprenticeship and Low Synergy papers are those
with high MaxSim scores, while low scores indicate
New Blood, Catalyst or High Synergy papers.
5.2 Examples
The following are examples of high impact papers
as they were categorized by our system:
127
Figure 1: Decision diagram for determining the collaboration type of a paper. A minimum of 1 established author is
assumed.
5.2.1 Example: Apprenticeship Paper
Improvements in Phrase-Based Statistical Ma-
chine Translation (2004)
by Richard Zens and Hermann Ney
This paper had a high MaxSim score, indicating high
similarity to established author Hermann Ney. This
categorizes the paper as an Apprenticeship Paper.
5.2.2 Example: New Blood Paper
Thumbs up? Sentiment Classification using
Machine Learning Techniques (2002)
by Lillian Lee, Bo Pang and Shivakumar
Vaithyanathan
This paper had a low MaxSim score, indicating
low similarity to established author Lillian Lee.
This categorizes the paper as a New Blood Pa-
per, with new authors Bo Pang and Shivakumar
Vaithyanathan. It is important to note here that new
authors do not necessarily mean young authors or
grad students; in this case, the third author on the
paper was experienced, but in a field outside of
ACL.
5.2.3 Example: High Synergy Paper
Catching the Drift: Probabilistic Content
Models, with Applications to Generation and
Summarization (2003)
by Regina Barzilay and Lillian Lee
This paper had low similarity to both established
authors on it, making it a highly synergistic paper.
Synergy here indicates that the work done on this
paper was mostly unlike work previously done by
either of the authors.
5.2.4 Example: Catalyst Paper
Answer Extraction (2000)
by Steven Abney, Michael Collins, Amit Singhal
This paper had a very low MaxSim score, as well
as the presence of an unestablished author, making
it a Catalyst Paper. The established authors (from
an ACL perspective) were Abney and Collins, while
Singhal was from outside the area and did not have
many ACL publications. The work done in this pa-
per focused on information extraction, and was un-
like that previously done by either of the ACL estab-
lished authors. Thus, we say that in this case, Sing-
hal played the role of the catalyst, getting the other
two authors to work on an area that was outside of
their usual range.
5.3 Evaluation
5.3.1 Expert Annotation
To quantitatively evaluate the performance of
our system, we prepared a subset of 120 papers
from among the highest scoring collaborative papers
based on the PageRank metric (Radev et al, 2009).
Only those papers were selected which had at least a
128
single established author. One expert in the field was
asked to annotate each of these papers as being ei-
ther similar or dissimilar to the established authors?
prior work given the year of publication, the title of
the publication and its abstract.
We found that the MaxSim scores of papers la-
beled as being similar to the established authors
were, on average, higher than those labeled as dis-
similar. The average MaxSim score of papers anno-
tated as low MaxSim collaboration types (High Syn-
ergy, New Blood or Catalyst papers) was 0.15488,
while that of papers labeled as high MaxSim types
(Apprentice or Low Synergy papers) had a mean
MaxSim score of 0.21312. The MaxSim scores of
the different sets were compared using a t-test, and
the difference was found to be statistically signifi-
cant with a two-tailed p-value of 0.0041.
Framing the task as a binary classification prob-
lem, however, did not produce very strong results.
The breakdown of the papers and success rates (as
determined by a tuned threshold) can be seen in Ta-
ble 3. The system had a relatively low success rate of
62.5% in its binary categorization of collaborations.
5.3.2 First Author Prediction
Studies have suggested that authorship order,
when not alphabetical, can often be quantified and
predicted by those who do the work (Sekercioglu,
2008). Through a survey of all authors on a sam-
ple of papers, Slone (1996) found that in almost all
major papers, ?the first two authors are said to ac-
count for the preponderance of work?. We attempt
to evaluate our similarity scores by checking if they
are predictive of first author.
Though similarity to previous work is only a small
contributor to determining author order, we find that
using the metric of cosine similarity between author
signatures and papers performs significantly better
at determining the first author of a paper than ran-
dom chance. Of course, this feature alone isn?t ex-
tremely predictive, given that it?s guaranteed to give
an incorrect solution in cases where the first author
of a paper has never been seen before. To solve the
problem of first author prediction, we would have
to combine this with other features. We chose two
other features - an alphabetical predictor, and a pre-
dictor based on the frequency of an author appearing
as first author. Although we don?t show the regres-
Predictor Feature Accuracy
Random Chance 37.35%
Author Signature Similarity 45.23%
Frequency Estimator 56.09%
Alphabetical Ordering 43.64%
Table 2: Accuracy of individual features at predicting the
first author of 8843 papers
sion, we do explore these two other features and find
that they are also predictive of author order.
Table 2 shows the performance of our prediction
feature alongside the others. The fact that it beats
random chance shows us that there is some infor-
mation about authorial efforts in the scores we have
computed.
6 Applications
A number of questions about the nature of collabo-
rations may be answered using our system. We de-
scribe approaches to some of these in this section.
6.1 The Hedgehog-Fox Problem
From the days of the ancient Greek poet
Archilochus, the Hedgehog-Fox analogy has
been frequently used (Berlin, 1953) to describe two
different types of people. Archilochus stated that
?The fox knows many things; the hedgehog one big
thing.? A person is thus considered a ?hedgehog?
if he has expertise in one specific area and focuses
all his time and resources on it. On the other hand,
a ?fox? is a one who has knowledge of several
different fields, and dabbles in all of them instead of
focusing heavily on one.
We show how, using our computed similarity
scores, one can discover the hedgehogs and foxes
of Computational Linguistics. We look at the top
100 published authors in our corpus, and for each
author, we compute the average similarity score the
author?s signature has to each of his or her papers.
Note that we start taking similarity scores into ac-
count only after an author has published 5 papers,
thereby allowing the author to stablize a signature
in the corpus and preventing the signature from be-
ing boosted by early papers (where author similarity
would be artificially high, since the author was new).
We present the authors with the highest average
similarity scores in Table 4. These authors can be
129
Collaboration Type True Positives False Positives Accuracy
New Blood, Catalyst or High Synergy Papers 43 23 65.15%
Apprentice or Low Synergy Papers 32 22 59.25%
Overall 75 45 62.50%
Table 3: Evaluation based on annotation by one expert
considered the hedgehogs, as they have highly sta-
ble signatures that their new papers resemble. On
the other hand, Table 5 shows the list of foxes, who
have less stable signatures, presumably because they
move about in different areas.
Author Avg. Sim. Score
Koehn, Philipp 0.43456
Pedersen, Ted 0.41146
Och, Franz Josef 0.39671
Ney, Hermann 0.37304
Sumita, Eiichiro 0.36706
Table 4: Hedgehogs - authors with the highest average
similarity scores
Author Avg. Sim. Score
Marcus, Mitchell P. 0.09996
Pustejovsky, James D. 0.10473
Pereira, Fernando C. N. 0.14338
Allen, James F. 0.14461
Hahn, Udo 0.15009
Table 5: Foxes - authors with the lowest average similar-
ity scores
6.2 Similarity to previous work by sub-fields
Based on the different types of collaborations dis-
cussed in, a potential question one might ask is
which sub-fields are more likely to produce appren-
tice papers, and which will produce new blood pa-
pers. To answer this question, we first need to deter-
mine which papers correspond to which sub-fields.
Once again, we use topic models to solve this prob-
lem. We first filter out a subset of the 1,200 highest
page-rank collaborative papers from the years 1980
to 2007. We use a set of topics built by running a
standard LDA topic model over the ACL corpus, in
which each topic is hand labeled by experts based on
the top terms associated with it. Given these topic-
term distributions, we can once again use the cosine
similarity metric to discover the highly associated
Topic Score
Statistical Machine Translation 0.2695
Prosody 0.2631
Speech Recognition 0.2511
Non-Statistical Machine Translation 0.2471
Word Sense Disambiguation 0.2380
Table 6: Topics with highest MaxSim scores (papers are
more similar to the established authors? previous work)
Topic Score
Question Answering 0.1335
Sentiment Analysis 0.1399
Dialog Systems 0.1417
Spelling Correction 0.1462
Summarization 0.1511
Table 7: Topics with lowest MaxSim scores (papers are
less similar to the established authors? previous work)
topics for each given paper from our smaller sub-
set, by choosing topics with cosine similarity above
a certain threshold ? (in this case 0.1).
Once we have created a paper set for each topic,
we can measure the ?novelty? for each paper by look-
ing at their MaxSim score. We can now find the av-
erage MaxSim score for each topic. This average
similarity score gives us a notion of how similar to
the established author (or authors) a paper in the sub
field usually is. Low scores indicate that new blood
and synergy style papers are more common, while
higher scores imply more non-synergistic or appren-
ticeship style papers. This could indicate that topics
with lower scores are more open ended, while those
with higher scores require more formality or train-
ing. The top five topics in each category are shown
in Tables 6 and 7. The scores of the papers from
the two tables were compared using a t-test, and the
difference in the scores of the two tables was found
to be very statistically significant with a two-tailed p
value << 0.01.
130
7 Discussion and Future Work
Once we have a robust way to score different kinds
of collaborations in ACL, we can begin to use these
scores as a quantitative tool to study phonemena in
the computational linguistics community. With our
current technique, we discovered a number of nega-
tive results; however, given that our accuracy in bi-
nary classification of categories is relatively low, we
cannot state for sure whether these are true negative
results or a limitation of our model.
7.1 Tentative Negative Results
Among the questions we looked into, we found the
following results:
? There was no signal indicating that authors
who started out as new blood authors were any
more or less likely to survive than authors who
started out as apprentices. Survival was mea-
sured both by the number of papers eventually
published by the author as well as the year of
the author?s final publication; however, calcu-
lations by neither measure correlated with the
MaxSim scores of the authors? early papers.
? Each author in the corpus was labeled for gen-
der. Gender didn?t appear to differentiate how
people collaborated. In particular, there was no
difference between men and women based on
how they started their careers. Women and men
are equally likely to begin as new blood authors
as they are to begin as apprentices.
? On a similar note, established male authors are
equally likely to partake in new blood or ap-
prentice collaborations as their female counter-
parts.
? No noticeable difference existed between aver-
age page rank scores of a certain categorization
of collaborative papers (e.g. high synergy pa-
pers vs. low synergy papers).
It is difficult to conclusively demonstrate negative
results, particularly given that our MaxSim scores
are by themselves not particularly strong discrimi-
nators in the binary classification tasks. We consider
these findings to be tentative and an opportunity to
explore in the future.
8 Conclusion
Not everything we need to know about academic
collaborations can be found in the co-authorship
graph. Indeed, as we have argued, not all types
of collaborations are equal, as embodied by differ-
ing levels of seniority and contribution from each
co-author. In this work, we have taken a first step
toward computationally modeling these differences
using a latent mixture of authors model and ap-
plied it to our own field, Computational Linguistics.
We used the model to examine how collaborative
works differ by authors and subfields in the ACL an-
thology. Our model quantifies the extent to which
some authors are more prone to being ?hedgehogs,?
whereby they heavily focus on certain specific ar-
eas, whilst others are more diverse with their fields
of study and may be analogized with ?foxes.?
We also saw that established authors in certain
subfields have more deviation from their previous
work than established authors in different subfields.
This could imply that the former fields, such as
?Sentiment Analysis? or ?Summarization,? are more
open to new blood and synergistic ideas, while other
latter fields, like ?Statistical Machine Translation?
or ?Speech Recognition? are more formal or re-
quire more training. Alternatively, ?Summarization?
or ?Sentiment Analysis? could just still be younger
fields whose language is still evolving and being in-
fluenced by other subareas.
This work takes a first step toward a new way of
thinking about the contributions of individual au-
thors based on their network of areas. There are
many design parameters that still exist in this space,
including alternative text models that take into ac-
count richer structure and, hopefully, perform bet-
ter at discriminating between the types of collabo-
rations we identified. We intend to use the ACL an-
thology as our test bed for continuing to work on tex-
tual models of collaboration types. Ultimately, we
hope to apply the lessons we learn on modeling this
familiar corpus to the challenge of answering large-
scale questions about the nature of collaboration as
embodied by large scale publication databases such
as ISI and Pubmed.
131
Acknowledgments
This research was supported by NSF grant NSF-
0835614 CDI-Type II: What drives the dynamic cre-
ation of science? We thank our anonymous review-
ers for their valuable feedback and the members of
the Stanford Mimir Project team for their insights
and engagement.
References
Isaiah Berlin. 1953. The hedgehog and the fox: An essay
on Tolstoy?s view of history. Simon & Schuster.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd international
conference on Machine learning, ICML ?06, pages
113?120, New York, NY, USA. ACM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
Sean M. Gerrish and David M. Blei. 2010. A language-
based approach to measuring scholarly impact. In Pro-
ceedings of the 26th International Conference on Ma-
chine Learning.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 363?371, Stroudsburg, PA, USA.
Association for Computational Linguistics.
B. F. Jones, S. Wuchty, and B. Uzzi. 2008. Multi-
university research teams: Shifting impact, geography,
and stratification in science. Science, 322:1259?1262,
November.
Xiaoming Liu, Johan Bollen, Michael L. Nelson, and
Herbert Van de Sompel. 2005. Co-authorship net-
works in the digital library research community. In-
formation Processing & Management, 41(6):1462 ?
1480. Special Issue on Infometrics.
Mario A. Nascimento, Jo?rg Sander, and Jeffrey Pound.
2003. Analysis of sigmod?s co-authorship graph. SIG-
MOD Rec., 32:8?10, September.
M. E. J. Newman. 2001. From the cover: The struc-
ture of scientific collaboration networks. Proceedings
of the National Academy of Science, 98:404?409, Jan-
uary.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network cor-
pus. In Proceedings of the 2009 Workshop on Text
and Citation Analysis for Scholarly Digital Libraries,
NLPIR4DL ?09, pages 54?61, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled lda: a super-
vised topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 1 - Volume 1, EMNLP ?09, pages 248?256.
Craig M. Rawlings and Daniel A. McFarland. 2011. In-
fluence flows in the academy: Using affiliation net-
works to assess peer effects among researchers. Social
Science Research, 40(3):1001 ? 1017.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for au-
thors and documents. In Proceedings of the 20th con-
ference on Uncertainty in artificial intelligence, UAI
?04, pages 487?494.
Cagan H. Sekercioglu. 2008. Quantifying coauthor con-
tributions. Science, 322(5900):371.
RM Slone. 1996. Coauthors? contributions to major
papers published in the ajr: frequency of undeserved
coauthorship. Am. J. Roentgenol., 167(3):571?579.
Howard D. White and Katherine W. Mccain. 1998. Visu-
alizing a discipline: An author co-citation analysis of
information science. Journal of the American Society
for Information Science, 49:1972?1995.
132
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 28?34,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Stanford?s Multi-Pass Sieve Coreference Resolution System at the
CoNLL-2011 Shared Task
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers,
Mihai Surdeanu, Dan Jurafsky
Stanford NLP Group
Stanford University, Stanford, CA 94305
{heeyoung,peirsman,angelx,natec,mihais,jurafsky}@stanford.edu
Abstract
This paper details the coreference resolution
system submitted by Stanford at the CoNLL-
2011 shared task. Our system is a collection
of deterministic coreference resolution mod-
els that incorporate lexical, syntactic, seman-
tic, and discourse information. All these mod-
els use global document-level information by
sharing mention attributes, such as gender and
number, across mentions in the same cluster.
We participated in both the open and closed
tracks and submitted results using both pre-
dicted and gold mentions. Our system was
ranked first in both tracks, with a score of 57.8
in the closed track and 58.3 in the open track.
1 Introduction
This paper describes the coreference resolution sys-
tem used by Stanford at the CoNLL-2011 shared
task (Pradhan et al, 2011). Our system extends
the multi-pass sieve system of Raghunathan et
al. (2010), which applies tiers of deterministic coref-
erence models one at a time from highest to lowest
precision. Each tier builds on the entity clusters con-
structed by previous models in the sieve, guarantee-
ing that stronger features are given precedence over
weaker ones. Furthermore, this model propagates
global information by sharing attributes (e.g., gender
and number) across mentions in the same cluster.
We made three considerable extensions to the
Raghunathan et al (2010) model. First, we added
five additional sieves, the majority of which address
the semantic similarity between mentions, e.g., us-
ing WordNet distance, and shallow discourse under-
standing, e.g., linking speakers to compatible pro-
nouns. Second, we incorporated a mention detection
sieve at the beginning of the processing flow. This
sieve filters our syntactic constituents unlikely to be
mentions using a simple set of rules on top of the
syntactic analysis of text. And lastly, we added a
post-processing step, which guarantees that the out-
put of our system is compatible with the shared task
and OntoNotes specifications (Hovy et al, 2006;
Pradhan et al, 2007).
Using this system, we participated in both the
closed1 and open2 tracks, using both predicted and
gold mentions. Using predicted mentions, our sys-
tem had an overall score of 57.8 in the closed track
and 58.3 in the open track. These were the top scores
in both tracks. Using gold mentions, our system
scored 60.7 in the closed track in 61.4 in the open
track.
We describe the architecture of our entire system
in Section 2. In Section 3 we show the results of sev-
eral experiments, which compare the impact of the
various features in our system, and analyze the per-
formance drop as we switch from gold mentions and
annotations (named entity mentions and parse trees)
to predicted information. We also report in this sec-
tion our official results in the testing partition.
1Only the provided data can be used, i.e., WordNet and gen-
der gazetteer.
2Any external knowledge source can be used. We used
additional animacy, gender, demonym, and country and states
gazetteers.
28
2 System Architecture
Our system consists of three main stages: mention
detection, followed by coreference resolution, and
finally, post-processing. In the first stage, mentions
are extracted and relevant information about men-
tions, e.g., gender and number, is prepared for the
next step. The second stage implements the ac-
tual coreference resolution of the identified men-
tions. Sieves in this stage are sorted from highest
to lowest precision. For example, the first sieve (i.e.,
highest precision) requires an exact string match be-
tween a mention and its antecedent, whereas the
last one (i.e., lowest precision) implements pronom-
inal coreference resolution. Post-processing is per-
formed to adjust our output to the task specific con-
straints, e.g., removing singletons.
It is important to note that the first system stage,
i.e., the mention detection sieve, favors recall heav-
ily, whereas the second stage, which includes the ac-
tual coreference resolution sieves, is precision ori-
ented. Our results show that this design lead to
state-of-the-art performance despite the simplicity
of the individual components. This strategy has
been successfully used before for information ex-
traction, e.g., in the BioNLP 2009 event extraction
shared task (Kim et al, 2009), several of the top sys-
tems had a first high-recall component to identify
event anchors, followed by high-precision classi-
fiers, which identified event arguments and removed
unlikely event candidates (Bjo?rne et al, 2009). In
the coreference resolution space, several works have
shown that applying a list of rules from highest to
lowest precision is beneficial for coreference reso-
lution (Baldwin, 1997; Raghunathan el al., 2010).
However, we believe we are the first to show that this
high-recall/high-precision strategy yields competi-
tive results for the complete task of coreference res-
olution, i.e., including mention detection and both
nominal and pronominal coreference.
2.1 Mention Detection Sieve
In our particular setup, the recall of the mention de-
tection component is more important than its preci-
sion, because any missed mentions are guaranteed
to affect the final score, but spurious mentions may
not impact the overall score if they are left as sin-
gletons, which are discarded by our post-processing
step. Therefore, our mention detection algorithm fo-
cuses on attaining high recall rather than high preci-
sion. We achieve our goal based on the list of sieves
sorted by recall (from highest to lowest). Each sieve
uses syntactic parse trees, identified named entity
mentions, and a few manually written patterns based
on heuristics and OntoNotes specifications (Hovy et
al., 2006; Pradhan et al, 2007). In the first and
highest recall sieve, we mark all noun phrase (NP),
possessive pronoun, and named entity mentions in
each sentence as candidate mentions. In the follow-
ing sieves, we remove from this set al mentions that
match any of the exclusion rules below:
1. We remove a mention if a larger mention with
the same head word exists, e.g., we remove The
five insurance companies in The five insurance
companies approved to be established this time.
2. We discard numeric entities such as percents,
money, cardinals, and quantities, e.g., 9%,
$10, 000, Tens of thousands, 100 miles.
3. We remove mentions with partitive or quanti-
fier expressions, e.g., a total of 177 projects.
4. We remove pleonastic it pronouns, detected us-
ing a set of known expressions, e.g., It is possi-
ble that.
5. We discard adjectival forms of nations, e.g.,
American.
6. We remove stop words in a predetermined list
of 8 words, e.g., there, ltd., hmm.
Note that the above rules extract both mentions in
appositive and copulative relations, e.g., [[Yongkang
Zhou] , the general manager] or [Mr. Savoca] had
been [a consultant. . . ]. These relations are not an-
notated in the OntoNotes corpus, e.g., in the text
[[Yongkang Zhou] , the general manager], only the
larger mention is annotated. However, appositive
and copulative relations provide useful (and highly
precise) information to our coreference sieves. For
this reason, we keep these mentions as candidates,
and remove them later during post-processing.
2.2 Mention Processing
Once mentions are extracted, we sort them by sen-
tence number, and left-to-right breadth-first traversal
29
order in syntactic trees in the same sentence (Hobbs,
1977). We select for resolution only the first men-
tions in each cluster,3 for two reasons: (a) the first
mention tends to be better defined (Fox, 1993),
which provides a richer environment for feature ex-
traction; and (b) it has fewer antecedent candidates,
which means fewer opportunities to make a mis-
take. For example, given the following ordered list
of mentions, {m11, m
2
2, m
2
3, m
3
4, m
1
5, m
2
6}, where
the subscript indicates textual order and the super-
script indicates cluster id, our model will attempt
to resolve only m22 and m
3
4. Furthermore, we dis-
card first mentions that start with indefinite pronouns
(e.g., some, other) or indefinite articles (e.g., a, an)
if they have no antecedents that have the exact same
string extents.
For each selected mention mi, all previous men-
tions mi?1, . . . , m1 become antecedent candidates.
All sieves traverse the candidate list until they find
a coreferent antecedent according to their criteria
or reach the end of the list. Crucially, when com-
paring two mentions, our approach uses informa-
tion from the entire clusters that contain these men-
tions instead of using just information local to the
corresponding mentions. Specifically, mentions in
a cluster share their attributes (e.g., number, gen-
der, animacy) between them so coreference decision
are better informed. For example, if a cluster con-
tains two mentions: a group of students, which is
singular, and five students, which is plural,
the number attribute of the entire cluster becomes
singular or plural, which allows it to match
other mentions that are both singular and plural.
Please see (Raghunathan et al, 2010) for more de-
tails.
2.3 Coreference Resolution Sieves
2.3.1 Core System
The core of our coreference resolution system is
an incremental extension of the system described in
Raghunathan et al (2010). Our core model includes
two new sieves that address nominal mentions and
are inserted based on their precision in a held-out
corpus (see Table 1 for the complete list of sieves
deployed in our system). Since these two sieves use
3We initialize the clusters as singletons and grow them pro-
gressively in each sieve.
Ordered sieves
1. Mention Detection Sieve
2. Discourse Processing Sieve
3. Exact String Match Sieve
4. Relaxed String Match Sieve
5. Precise Constructs Sieve (e.g., appositives)
6-8. Strict Head Matching Sieves A-C
9. Proper Head Word Match Sieve
10. Alias Sieve
11. Relaxed Head Matching Sieve
12. Lexical Chain Sieve
13. Pronouns Sieve
Table 1: The sieves in our system; sieves new to this pa-
per are in bold.
simple lexical constraints without semantic informa-
tion, we consider them part of the baseline model.
Relaxed String Match: This sieve considers two
nominal mentions as coreferent if the strings ob-
tained by dropping the text following their head
words are identical, e.g., [Clinton] and [Clinton,
whose term ends in January].
Proper Head Word Match: This sieve marks two
mentions headed by proper nouns as coreferent if
they have the same head word and satisfy the fol-
lowing constraints:
Not i-within-i - same as Raghunathan et al (2010).
No location mismatches - the modifiers of two men-
tions cannot contain different location named entities,
other proper nouns, or spatial modifiers. For example,
[Lebanon] and [southern Lebanon] are not coreferent.
No numeric mismatches - the second mention cannot
have a number that does not appear in the antecedent, e.g.,
[people] and [around 200 people] are not coreferent.
In addition to the above, a few more rules are
added to get better performance for predicted men-
tions.
Pronoun distance - sentence distance between a pronoun
and its antecedent cannot be larger than 3.
Bare plurals - bare plurals are generic and cannot have a
coreferent antecedent.
2.3.2 Semantic-Similarity Sieves
We first extend the above system with two
new sieves that exploit semantics from WordNet,
Wikipedia infoboxes, and Freebase records, drawing
on previous coreference work using these databases
(Ng & Cardie, 2002; Daume? & Marcu, 2005;
Ponzetto & Strube, 2006; Ng, 2007; Yang & Su,
30
2007; Bengston & Roth, 2008; Huang et al, 2009;
inter alia). Since the input to a sieve is a collection of
mention clusters built by the previous (more precise)
sieves, we need to link mention clusters (rather than
individual mentions) to records in these three knowl-
edge bases. The following steps generate a query for
these resources from a mention cluster.
First, we select the most representative mention
in a cluster by preferring mentions headed by proper
nouns to mentions headed by common nouns, and
nominal mentions to pronominal ones. In case of
ties, we select the longer string. For example, the
mention selected from the cluster {President George
W. Bush, president, he} is President George W.
Bush. Second, if this mention returns nothing from
the knowledge bases, we implement the following
query relaxation algorithm: (a) remove the text fol-
lowing the mention head word; (b) select the lowest
noun phrase (NP) in the parse tree that includes the
mention head word; (c) use the longest proper noun
(NNP*) sequence that ends with the head word; (d)
select the head word. For example, the query pres-
ident Bill Clinton, whose term ends in January is
successively changed to president Bill Clinton, then
Bill Clinton, and finally Clinton. If multiple records
are returned, we keep the top two for Wikipedia and
Freebase, and all synsets for WordNet.
Alias Sieve
This sieve addresses name aliases, which are de-
tected as follows. Two mentions headed by proper
nouns are marked as aliases (and stored in the same
entity cluster) if they appear in the same Wikipedia
infobox or Freebase record in either the ?name? or
?alias? field, or they appear in the same synset in
WordNet. As an example, this sieve correctly de-
tects America Online and AOL as aliases. We also
tested the utility of Wikipedia categories, but found
little gain over morpho-syntactic features.
Lexical Chain Sieve
This sieve marks two nominal mentions as coref-
erent if they are linked by a WordNet lexical chain
that traverses hypernymy or synonymy relations. We
use all synsets for each mention, but restrict it to
mentions that are at most three sentences apart, and
lexical chains of length at most four. This sieve cor-
rectly links Britain with country, and plane with air-
craft.
To increase the precision of the above two sieves,
we use additional constraints before two mentions
can match: attribute agreement (number, gender, an-
imacy, named entity labels), no i-within-i, no loca-
tion or numeric mismatches (as in Section 2.3.1),
and we do not use the abstract entity synset in Word-
Net, except in chains that include ?organization?.
2.3.3 Discourse Processing Sieve
This sieve matches speakers to compatible pro-
nouns, using shallow discourse understanding to
handle quotations and conversation transcripts. Al-
though more complex discourse constraints have
been proposed, it has been difficult to show improve-
ments (Tetreault & Allen, 2003; 2004).
We begin by identifying speakers within text. In
non-conversational text, we use a simple heuristic
that searches for the subjects of reporting verbs (e.g.,
say) in the same sentence or neighboring sentences
to a quotation. In conversational text, speaker infor-
mation is provided in the dataset.
The extracted speakers then allow us to imple-
ment the following sieve heuristics:
? ?I?s4 assigned to the same speaker are coreferent.
? ?you?s with the same speaker are coreferent.
? The speaker and ?I?s in her text are coreferent.
For example, I, my, and she in the following sen-
tence are coreferent: ?[I] voted for [Nader] because
[he] was most aligned with [my] values,? [she] said.
In addition to the above sieve, we impose speaker
constraints on decisions made by subsequent sieves:
? The speaker and a mention which is not ?I? in the
speaker?s utterance cannot be coreferent.
? Two ?I?s (or two ?you?s, or two ?we?s) assigned to
different speakers cannot be coreferent.
? Two different person pronouns by the same speaker
cannot be coreferent.
? Nominal mentions cannot be coreferent with ?I?,
?you?, or ?we? in the same turn or quotation.
? In conversations, ?you? can corefer only with the
previous speaker.
For example, [my] and [he] are not coreferent in the
above example (third constraint).
4We define ?I? as ?I?, ?my?, ?me?, or ?mine?, ?we? as first
person plural pronouns, and ?you? as second person pronouns.
31
Annotations Coref R P F1
Gold Before 92.8 37.7 53.6
Gold After 75.1 70.1 72.6
Not gold Before 87.9 35.6 50.7
Not gold After 71.7 68.4 70.0
Table 2: Performance of the mention detection compo-
nent, before and after coreference resolution, with both
gold and actual linguistic annotations.
2.4 Post Processing
To guarantee that the output of our system matches
the shared task requirements and the OntoNotes
annotation specification, we implement two post-
processing steps:
? We discard singleton clusters.
? We discard the mention that appears later in
text in appositive and copulative relations. For
example, in the text [[Yongkang Zhou] , the
general manager] or [Mr. Savoca] had been
[a consultant. . . ], the mentions Yongkang Zhou
and a consultant. . . are removed in this stage.
3 Results and Discussion
Table 2 shows the performance of our mention de-
tection algorithm. We show results before and after
coreference resolution and post-processing (when
singleton mentions are removed). We also list re-
sults with gold and predicted linguistic annotations
(i.e., syntactic parses and named entity recognition).
The table shows that the recall of our approach is
92.8% (if gold annotations are used) or 87.9% (with
predicted annotations). In both cases, precision is
low because our algorithm generates many spurious
mentions due to its local nature. However, as the ta-
ble indicates, many of these mentions are removed
during post-processing, because they are assigned
to singleton clusters during coreference resolution.
The two main causes for our recall errors are lack
of recognition of event mentions (e.g., verbal men-
tions such as growing) and parsing errors. Parsing
errors often introduce incorrect mention boundaries,
which yield both recall and precision errors. For
example, our system generates the predicted men-
tion, the working meeting of the ?863 Program? to-
day, for the gold mention the working meeting of the
?863 Program?. Due to this boundary mismatch,
all mentions found to be coreferent with this pre-
dicted mention are counted as precision errors, and
all mentions in the same coreference cluster with the
gold mention are counted as recall errors.
Table 3 lists the results of our end-to-end system
on the development partition. ?External Resources?,
which were used only in the open track, includes: (a)
a hand-built list of genders of first names that we cre-
ated, incorporating frequent names from census lists
and other sources, (b) an animacy list (Ji and Lin,
2009), (c) a country and state gazetteer, and (d) a de-
monym list. ?Discourse? stands for the sieve intro-
duced in Section 2.3.3. ?Semantics? stands for the
sieves presented in Section 2.3.2. The table shows
that the discourse sieve yields an improvement of
almost 2 points to the overall score (row 1 versus
3), and external resources contribute 0.5 points. On
the other hand, the semantic sieves do not help (row
3 versus 4). The latter result contradicts our initial
experiments, where we measured a minor improve-
ment when these sieves were enabled and gold men-
tions were used. Our hypothesis is that, when pre-
dicted mentions are used, the semantic sieves are
more likely to link spurious mentions to existing
clusters, thus introducing precision errors. This sug-
gests that a different tuning of the sieve parameters
is required for the predicted mention scenario. For
this reason, we did not use the semantic sieves for
our submission. Hence, rows 2 and 3 in the table
show the performance of our official submission in
the development set, in the closed and open tracks
respectively.
The last three rows in Table 3 give insight on the
impact of gold information. This analysis indicates
that using gold linguistic annotation yields an im-
provement of only 2 points. This implies that the
quality of current linguistic processors is sufficient
for the task of coreference resolution. On the other
hand, using gold mentions raises the overall score by
15 points. This clearly indicates that pipeline archi-
tectures where mentions are identified first are inad-
equate for this task, and that coreference resolution
might benefit from the joint modeling of mentions
and coreference chains.
Finally, Table 4 lists our results on the held-out
testing partition. Note that in this dataset, the gold
mentions included singletons and generic mentions
32
Components MUC B3 CEAFE BLANC
ER D S GA GM R P F1 R P F1 R P F1 R P F1 avg F1?
58.8 56.5 57.6 68.0 68.7 68.4 44.8 47.1 45.9 68.8 73.5 70.9 57.3?
59.1 57.5 58.3 69.2 71.0 70.1 46.5 48.1 47.3 72.2 78.1 74.8 58.6? ?
60.1 59.5 59.8 69.5 71.9 70.7 46.5 47.1 46.8 73.8 78.6 76.0 59.1? ? ?
60.3 58.5 59.4 69.9 71.1 70.5 45.6 47.3 46.4 73.9 78.2 75.8 58.8? ? ?
63.8 61.5 62.7 71.4 72.3 71.9 47.1 49.5 48.3 75.6 79.6 77.5 61.0? ? ?
73.6 90.0 81.0 69.8 89.2 78.3 79.4 52.5 63.2 79.1 89.2 83.2 74.2? ? ? ?
74.0 90.1 81.3 70.2 89.3 78.6 79.7 53.1 63.7 79.5 89.6 83.6 74.5
Table 3: Comparison between various configurations of our system. ER, D, S stand for External Resources, Discourse,
and Semantics sieves. GA and GM stand for Gold Annotations, and Gold Mentions. The top part of the table shows
results using only predicted annotations and mentions, whereas the bottom part shows results of experiments with gold
information. Avg F1 is the arithmetic mean of MUC, B3, and CEAFE. We used the development partition for these
experiments.
MUC B3 CEAFE BLANC
Track Gold Mention Boundaries R P F1 R P F1 R P F1 R P F1 avg F1
Close Not Gold 61.8 57.5 59.6 68.4 68.2 68.3 43.4 47.8 45.5 70.6 76.2 73.0 57.8
Open Not Gold 62.8 59.3 61.0 68.9 69.0 68.9 43.3 46.8 45.0 71.9 76.6 74.0 58.3
Close Gold 65.9 62.1 63.9 69.5 70.6 70.0 46.3 50.5 48.3 72.0 78.6 74.8 60.7
Open Gold 66.9 63.9 65.4 70.1 71.5 70.8 46.3 49.6 47.9 73.4 79.0 75.8 61.4
Table 4: Results on the official test set.
as well, whereas in development (lines 6 and 7 in Ta-
ble 3), gold mentions included only mentions part of
an actual coreference chain. This explains the large
difference between, say, line 6 in Table 3 and line 4
in Table 4.
Our scores are comparable to previously reported
state-of-the-art results for coreference resolution
with predicted mentions. For example, Haghighi
and Klein (2010) compare four state-of-the-art sys-
tems on three different corpora and report B3 scores
between 63 and 77 points. While the corpora used
in (Haghighi and Klein, 2010) are different from the
one in this shared task, our result of 68 B3 suggests
that our system?s performance is competitive. In this
task, our submissions in both the open and the closed
track obtained the highest scores.
4 Conclusion
In this work we showed how a competitive end-to-
end coreference resolution system can be built using
only deterministic models (or sieves). Our approach
starts with a high-recall mention detection compo-
nent, which identifies mentions using only syntactic
information and named entity boundaries, followed
by a battery of high-precision deterministic corefer-
ence sieves, applied one at a time from highest to
lowest precision. These models incorporate lexical,
syntactic, semantic, and discourse information, and
have access to document-level information (i.e., we
share mention attributes across clusters as they are
built). For this shared task, we extended our ex-
isting system with new sieves that model shallow
discourse (i.e., speaker identification) and seman-
tics (lexical chains and alias detection). Our results
demonstrate that, despite their simplicity, determin-
istic models for coreference resolution obtain com-
petitive results, e.g., we obtained the highest scores
in both the closed and open tracks (57.8 and 58.3
respectively). The code used for this shared task is
publicly released.5
Acknowledgments
We thank the shared task organizers for their effort.
This material is based upon work supported by
the Air Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusion or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the view of the Air
Force Research Laboratory (AFRL).
5See http://nlp.stanford.edu/software/
dcoref.shtml for the standalone coreference resolution
system and http://nlp.stanford.edu/software/
corenlp.shtml for Stanford?s suite of natural language
processing tools, which includes this coreference resolution
system.
33
References
B. Baldwin. 1997. CogNIAC: high precision corefer-
ence with limited knowledge and linguistic resources.
In Proceedings of a Workshop on Operational Factors
in Practical, Robust Anaphora Resolution for Unre-
stricted Texts.
E. Bengston & D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting Complex Biological Events with Rich Graph-
Based Feature Sets. Proceedings of the Workshop on
BioNLP: Shared Task.
H. Daume? III and D. Marcu. 2005. A large-scale ex-
ploration of effective global features for a joint entity
detection and tracking model. In EMNLP-HLT.
B. A. Fox 1993. Discourse structure and anaphora:
written and conversational English. Cambridge Uni-
versity Press.
A. Haghighi and D. Klein. 2010. Coreference resolution
in a modular, entity-centered model. In Proc. of HLT-
NAACL.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R.
Weischedel 2006. OntoNotes: The 90% Solution. In
HLT/NAACL.
Z. Huang, G. Zeng, W. Xu, and A. Celikyilmaz 2009.
Accurate semantic class classifier for coreference res-
olution. In EMNLP.
J.R. Hobbs. 1977. Resolving pronoun references. Lin-
gua.
H. Ji and D. Lin. 2009. Gender and animacy knowl-
edge discovery from web-scale n-grams for unsuper-
vised person mention detection. In PACLIC.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of the BioNLP?09 Shared Task on Event Extrac-
tion. Proceedings of the NAACL-HLT 2009 Work-
shop on Natural Language Processing in Biomedicine
(BioNLP?09).
V. Ng 2007. Semantic Class Induction and Coreference
Resolution. In ACL.
V. Ng and C. Cardie. 2002. Improving Machine Learn-
ing Approaches to Coreference Resolution. in ACL
2002
S. Ponzetto and M. Strube. 2006. Exploiting semantic
role labeling, Wordnet and Wikipedia for coreference
resolution. Proceedings of NAACL.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Indentifying Entities and Events
in OntoNotes. In Proceedings of the IEEE Interna-
tional Conference on Semantic Computing (ICSC).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning (CoNLL 2011).
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning 2010.
A Multi-Pass Sieve for Coreference Resolution. In
EMNLP.
J. Tetreault and J. Allen. 2003. An Empirical Evalua-
tion of Pronoun Resolution and Clausal Structure. In
Proceedings of the 2003 International Symposium on
Reference Resolution.
J. Tetreault and J. Allen. 2004. Dialogue Structure and
Pronoun Resolution. In DAARC.
X. Yang and J. Su. 2007. Coreference Resolution Us-
ing Semantic Relatedness Information from Automat-
ically Discovered Patterns. In ACL.
34
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 16?22,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Capitalization Cues Improve Dependency Grammar Induction
Valentin I. Spitkovsky
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc., Mountain View, CA, 94043
hiyan@google.com
Daniel Jurafsky
Stanford University, Stanford, CA, 94305
jurafsky@stanford.edu
Abstract
We show that orthographic cues can be helpful
for unsupervised parsing. In the Penn Tree-
bank, transitions between upper- and lower-
case tokens tend to align with the boundaries
of base (English) noun phrases. Such signals
can be used as partial bracketing constraints to
train a grammar inducer: in our experiments,
directed dependency accuracy increased by
2.2% (average over 14 languages having case
information). Combining capitalization with
punctuation-induced constraints in inference
further improved parsing performance, attain-
ing state-of-the-art levels for many languages.
1 Introduction
Dependency grammar induction and related prob-
lems of unsupervised syntactic structure discovery
are attracting increasing attention (Rasooli and Faili,
2012; Marec?ek and Zabokrtsky?, 2011, inter alia).
Since sentence structure is underdetermined by raw
text, there have been efforts to simplify the task, via
(i) pooling features of syntax across languages (Co-
hen et al, 2011; McDonald et al, 2011; Cohen
and Smith, 2009); as well as (ii) identifying uni-
versal rules (Naseem et al, 2010) ? such as verbo-
centricity (Gimpel and Smith, 2011) ? that need not
be learned at all. Unfortunately most of these tech-
niques do not apply to plain text, because they re-
quire knowing, for example, which words are verbs.
As standard practice shifts away from relying on
gold part-of-speech (POS) tags (Seginer, 2007; Pon-
vert et al, 2010; S?gaard, 2011b; Spitkovsky et al,
2011c, inter alia), lighter cues to inducing linguistic
structure become more important. Examples of use-
ful POS-agnostic clues include punctuation bound-
aries (Ponvert et al, 2011; Spitkovsky et al, 2011b;
Briscoe, 1994) and various kinds of bracketing con-
straints (Naseem and Barzilay, 2011; Spitkovsky et
al., 2010b; Pereira and Schabes, 1992). We propose
adding capitalization to this growing list of sources
of partial bracketings. Our intuition stems from En-
glish, where (maximal) spans of capitalized words
? such as Apple II, World War I, Mayor William H.
Hudnut III, International Business Machines Corp. and
Alexandria, Va ? tend to demarcate proper nouns.
Consider a motivating example (all of our exam-
ples are from WSJ) without punctuation, in which all
(eight) capitalized word clumps and uncased numer-
als match base noun phrase constituent boundaries:
[NP Jay Stevens] of [NP Dean Witter] actually cut his
per-share earnings estimate to [NP $9] from [NP $9.50]
for [NP 1989] and to [NP $9.50] from [NP $10.35]
in [NP 1990] because he decided sales would be even
weaker than he had expected.
and another (whose first word happens to be a leaf),
where capitalization complements punctuation cues:
[NP Jurors] in [NP U.S. District Court] in [NP Miami]
cleared [NP Harold Hershhenson], a former executive
vice president; [NP John Pagones], a former vice presi-
dent; and [NP Stephen Vadas] and [NP Dean Ciporkin],
who had been engineers with [NP Cordis].
Could such chunks help bootstrap grammar induc-
tion and/or improve the accuracy of already-trained
unsupervised parsers? In answering these questions,
we will focus predominantly on sentence-internal
capitalization. But we will also show that first words
? those capitalized by convention ? and uncased
segments ? whose characters are not even drawn
from an alphabet ? could play a useful role as well.
2 English Capitalization from a Treebank
We began our study by consulting the 51,558 parsed
sentences of the WSJ corpus (Marcus et al, 1993):
30,691 (59.5%) of them contain non-trivially capi-
talized fragments ? maximal (non-empty and not
16
Count POS Sequence Frac Cum
1 27,524 NNP 44.6%
2 17,222 NNP NNP 27.9 72.5
3 4,598 NNP NNP NNP 7.5 79.9
4 2,973 JJ 4.8 84.8
5 1,716 NNP NNP NNP NNP 2.8 87.5
6 1,037 NN 1.7 89.2
7 932 PRP 1.5 90.7
8 846 NNPS 1.4 92.1
9 604 NNP NNPS 1.0 93.1
10 526 NNP NNP NNP NNP NNP 0.9 93.9
WSJ +3,753 more with Count ? 498 6.1%
Table 1: Top 10 fragments of POS tag sequences in WSJ.
sentence-initial) consecutive sequences of words
that each differs from its own lower-cased form.
Nearly all ? 59,388 (96.2%) ? of the 61,731 frag-
ments are dominated by noun phrases; slightly less
than half ? 27,005 (43.8%) ? perfectly align with
constituent boundaries in the treebank; and about as
many ? 27,230 (44.1%) are multi-token. Table 1
shows the top POS sequences comprising fragments.
3 Analytical Experiments with Gold Trees
We gauged the suitability of capitalization-induced
fragments for guiding dependency grammar induc-
tion by assessing accuracy, in WSJ,1 of parsing con-
straints derived from their end-points. Following the
suite of increasingly-restrictive constraints on how
dependencies may interact with fragments, intro-
duced by Spitkovsky et al (2011b, ?2.2), we tested
several such heuristics. The most lenient constraint,
thread, only asks that no dependency path from the
root to a leaf enter the fragment twice; tear requires
any incoming arcs to come from the same side of
the fragment; sprawl demands that there be exactly
one incoming arc; loose further constrains any out-
going arcs to be from the fragment?s head; and strict
? the most stringent constraint ? bans external
dependents. Since only strict is binding for single
words, we experimented also with strict?: applying
strict solely to multi-token fragments (ignoring sin-
gletons). In sum, we explored six ways in which
dependency parse trees can be constrained by frag-
ments whose end-points could be defined by capital-
ization (or in other various ways, e.g., semantic an-
1We converted labeled constituents into unlabeled depen-
dencies using deterministic ?head-percolation? rules (Collins,
1999), discarding any empty nodes, etc., as is standard practice.
markup punct. capital initial uncased
thread 98.5 95.0 99.5 98.4 99.2
tear 97.9 94.7 98.6 98.4 98.5
sprawl 95.1 92.9 98.2 97.9 96.4
loose 87.5 74.0 97.9 96.9 96.4
strict? 32.7 35.6 38.7 40.3 55.6
strict 35.6 39.2 59.3 66.9 61.1
Table 2: Several sources of fragments? end-points and
%-correctness of their derived constraints (for English).
notations, punctuation or HTML tags in web pages).
For example, in the sentence about Cordis, the
strict hypothesis would be wrong about five of the
eight fragments: Jurors attaches in; Court takes the
second in; Hershhenson and Pagones derive their ti-
tles, president; and (at least in our reference) Vadas
attaches and, Ciporkin and who. Based on this, we
would consider strict to be 37.5%-accurate. But
loose ? and the rest of the more relaxed constraints
? would get perfect scores. (And strict? would re-
tract the mistake about Jurors but also the correct
guesses about Miami and Cordis, scoring only 20%.)
Table 2 (capital) shows scores averaged over the
entire treebank. Columns markup (Spitkovsky et al,
2010b) and punct (Spitkovsky et al, 2011b) indicate
that capitalization yields across-the-board more ac-
curate constraints (for English) compared with frag-
ments derived from punctuation or markup (i.e., an-
chor text, bold, italics and underline tags in HTML),
for which such constraints were originally intended.
4 Pilot Experiments on Supervised Parsing
To further test the potential of capitalization-induced
constraints, we applied them in the Viterbi-decoding
phase of a simple (unlexicalized) supervised depen-
dency parser ? an instance of DBM-1 (Spitkovsky
et al, 2012, ?2.1), trained on WSJ sentences with up
punct.: thread tear sprawl loose
none: 71.8 74.3 74.4 74.5 73.3
capital:thread 72.3 74.6 74.7 74.9 73.6
tear 72.4 74.7 74.7 74.9 73.6
sprawl 72.4 74.7 74.7 74.9 73.4
loose 72.4 74.8 74.7 74.9 73.3
strict? 71.4 73.7 73.7 73.9 72.7
strict 71.0 73.1 73.1 73.2 72.1
Table 3: Supervised (directed) accuracy on Section 23
of WSJ using capitalization-induced constraints (vertical)
jointly with punctuation (horizontal) in Viterbi-decoding.
17
CoNLL Year Filtered Training Directed Accuracies with Initial Constraints Fragments
& Language Tokens / Sentences none thread tear sprawl loose strict? strict Multi Single
German 2006 139,333 12,296 36.3 36.3 36.3 39.1 36.2 36.3 30.1 3,287 30,435
Czech ?6 187,505 20,378 51.3 51.3 51.3 51.3 52.5 52.5 51.4 1,831 6,722
English ?7 74,023 5,087 29.2 28.5 28.3 29.0 29.3 28.3 27.7 1,135 2,218
Bulgarian ?6 46,599 5,241 59.4 59.3 59.3 59.4 59.1 59.3 59.5 184 1,506
Danish ?6 14,150 1,599 21.3 17.7 22.7 21.5 21.4 31.4 27.9 113 317
Greek ?7 11,943 842 28.1 46.1 46.3 46.3 46.4 31.1 31.0 113 456
Dutch ?6 72,043 7,107 45.9 45.8 45.9 45.8 45.8 45.7 29.6 89 4,335
Italian ?7 9,142 921 41.7 52.6 52.7 52.6 44.2 52.6 45.8 41 296
Catalan ?7 62,811 4,082 61.3 61.3 61.3 61.3 61.3 61.3 36.5 28 2,828
Turkish ?6 17,610 2,835 32.9 32.9 32.2 33.0 33.0 33.6 33.9 27 590
Portuguese ?6 24,494 2,042 68.9 67.1 69.1 69.2 68.9 68.9 38.5 9 953
Hungarian ?7 10,343 1,258 43.2 43.2 43.1 43.2 43.2 43.7 25.5 7 277
Swedish ?6 41,918 4,105 48.6 48.6 48.6 48.5 48.5 48.5 48.8 3 296
Slovenian ?6 3,627 477 30.4 30.5 30.5 30.4 30.5 30.5 30.8 1 63
Median: 42.5 46.0 46.1 46.0 45.0 44.7 32.5
Mean: 42.8 44.4 44.8 45.0 44.3 44.6 36.9
Table 4: Parsing performance for grammar inducers trained with capitalization-based initial constraints, tested against
14 held-out sets from 2006/7 CoNLL shared tasks, and ordered by number of multi-token fragments in training data.
to 45 words (excluding Section 23). Table 3 shows
evaluation results on held-out data (all sentences),
using ?add-one? smoothing. All constraints other
than strict improve accuracy by about a half-a-point,
from 71.8 to 72.4%, suggesting that capitalization
is informative of certain regularities not captured by
DBM grammars; moreover, it still continues to be
useful when punctuation-based constraints are also
enforced, boosting accuracy from 74.5 to 74.9%.
5 Multi-Lingual Grammar Induction
So far, we showed only that capitalization informa-
tion can be helpful in parsing a very specific genre
of English. Next, we tested its ability to generally
aid dependency grammar induction, focusing on sit-
uations when other bracketing cues are unavailable.
We experimented with 14 languages from 2006/7
CoNLL shared tasks (Buchholz and Marsi, 2006;
Nivre et al, 2007), excluding Arabic, Chinese and
Japanese (which lack case), as well as Basque and
Spanish (which are pre-processed in a way that loses
relevant capitalization information). For all remain-
ing languages we trained only on simple sentences
? those lacking sentence-internal punctuation ?
from the relevant training sets (for blind evaluation).
Restricting our attention to a subset of the avail-
able training data serves a dual purpose. First, it al-
lows us to estimate capitalization?s impact where no
other (known or obvious) cues could also be used.
Otherwise, unconstrained baselines would not yield
the strongest possible alternative, and hence not the
most interesting comparison. Second, to the extent
that presence of punctuation may correlate with sen-
tence complexity (Frank, 2000), there are benefits to
?starting small? (Elman, 1993): e.g., relegating full
data to later stages helps training (Spitkovsky et al,
2010a; Cohn et al, 2011; Tu and Honavar, 2011).
Our base systems induced DBM-1, starting from
uniformly-at-random chosen parse trees (Cohen and
Smith, 2010) of each sentence, followed by inside-
outside re-estimation (Baker, 1979) with ?add-one?
smoothing.2 Capitalization-constrained systems dif-
fered from controls in exactly one way: each learner
got a slight nudge towards more promising struc-
tures by choosing initial seed trees satisfying an ap-
propriate constraint (but otherwise still uniformly).
Table 4 contains the stats for all 14 training sets,
ordered by number of multi-token fragments. Fi-
nal accuracies on respective (disjoint, full) evalua-
tion sets are improved by all constraints other than
strict, with the highest average performance result-
ing from sprawl: 45.0% directed dependency accu-
racy,3 on average. This increase of about two points
over the base system?s 42.8% is driven primarily by
improvements in two languages (Greek and Italian).
2We used ?early-stopping lateen EM? (Spitkovsky et al,
2011a, ?2.3) instead of thresholding or waiting for convergence.
3Starting from five parse trees for each sentence (using con-
straints thread through strict?) was no better, at 44.8% accuracy.
18
6 Capitalizing on Punctuation in Inference
Until now we avoided using punctuation in grammar
induction, except to filter data. Yet our pilot exper-
iments indicated that both kinds of information are
helpful in the decoding stage of a supervised system.
We took trained models obtained using the sprawl
nudge (from ?5) and proceeded to again apply con-
straints in inference (as in ?4). Capitalization alone
increased parsing accuracy only slightly, from 45.0
to 45.1%, on average. Using punctuation constraints
instead led to more improved performance: 46.5%.
Combining both types of constraints again resulted
in slightly higher accuracies: 46.7%. Table 5 breaks
down our last average performance number by lan-
guage and shows the combined approach to be com-
petitive with state-of-the-art. We suspect that further
improvements could be attained by also incorporat-
ing both constraints in training and with full data.
7 Discussion and A Few Post-Hoc Analyses
Our discussion, thus far, has been English-centric.
Nevertheless, languages differ in how they use capi-
talization (and even the rules governing a given lan-
guage tend to change over time ? generally towards
having fewer capitalized terms). For instance, adjec-
tives derived from proper nouns are not capitalized
in French, German, Polish, Spanish or Swedish, un-
like in English (see Table 1: JJ). And while English
forces capitalization of the first-person pronoun in
the nominative case, I (see Table 1: PRP), in Danish
it is the plural second-person pronoun (also I) that
is capitalized; further, formal pronouns (and their
case-forms) are capitalized in German (Sie and Ihre,
Ihres...), Italian, Slovenian, Russian and Bulgarian.
In contrast to pronouns, single-word proper nouns
? including personal names ? are capitalized in
nearly all European languages. Such shortest brack-
etings are not particularly useful for constraining
sets of possible parse trees in grammar induction,
however, compared to multi-word expressions; from
this perspective, German appears less helpful than
most cased languages, because of noun compound-
ing, despite prescribing capitalization of all nouns.
Another problem with longer word-strings in many
languages is that, e.g., in French (as in English)
lower-case prepositions may be mixed in with con-
tiguous groups of proper nouns: even in surnames,
CoNLL Year this State-of-the-Art Systems: POS-
& Language Work (i) Agnostic (ii) Identified
Bulgarian 2006 64.5 44.3 SCAJ5 70.3 Spt
Catalan ?7 61.5 63.8 SCAJ5 56.3 MZNR
Czech ?6 53.5 50.5 SCAJ5 33.3? MZNR
Danish ?6 20.6 46.0 RF 56.5 Sar
Dutch ?6 46.7 32.5 SCAJ5 62.1 MPHel
English ?7 29.2 50.3 SAJ 45.7 MPHel
German ?6 42.6 33.5 SCAJ5 55.8 MPHnl
Greek ?7 49.3 39.0 MZ 63.9 MPHen
Hungarian ?7 53.7 48.0 MZ 48.1 MZNR
Italian ?7 50.5 57.5 MZ 69.1 MPHpt
Portuguese ?6 72.4 43.2 MZ 76.9 Sbg
Slovenian ?6 34.8 33.6 SCAJ5 34.6 MZNR
Swedish ?6 50.5 50.0 SCAJ6 66.8 MPHpt
Turkish ?6 34.4 40.9 SAJ 61.3 RFH1
Median: 48.5 45.2 58.9
Mean: 46.7 45.2 57.2?
Table 5: Unsupervised parsing with both capitalization-
and punctuation-induced constraints in inference, tested
against the 14 held-out sets from 2006/7 CoNLL shared
tasks, and state-of-the-art results (all sentence lengths) for
systems that: (i) are also POS-agnostic and monolingual,
including SCAJ (Spitkovsky et al, 2011a, Tables 5?6)
and SAJ (Spitkovsky et al, 2011b); and (ii) rely on gold
POS-tag identities to (a) discourage noun roots (Marec?ek
and Zabokrtsky?, 2011, MZ), (b) encourage verbs (Ra-
sooli and Faili, 2012, RF), or (c) transfer delexicalized
parsers (S?gaard, 2011a, S) from resource-rich languages
with parallel translations (McDonald et al, 2011, MPH).
the German particle von is not capitalized, although
the Dutch van is, unless preceded by a given name or
initial ? hence Van Gogh, yet Vincent van Gogh.
7.1 Constraint Accuracies Across Languages
Since even related languages (e.g., Flemish, Dutch,
German and English) can have quite different con-
ventions regarding capitalization, one would not ex-
pect the same simple strategy to be uniformly useful
? or useful in the same way ? across disparate lan-
guages. To get a better sense of how universal our
constraints may be, we tabulated their accuracies for
the full training sets of the CoNLL data, after all
grammar induction experiments had been executed.
Table 6 shows that the less-strict capitalization-
induced constraints all fall within narrow (yet high)
bands of accuracies of just a few percentage points:
99?100% in the case of thread, 98?100% for tear,
95?99% for sprawl and 94?99% for loose. By con-
trast, the ranges for punctuation-induced constraints
are all at least 10%. We do not see anything partic-
19
CoNLL Year Total Training Capitalization-Induced Constraints Punctuation-Induced Constraints
& Language Tokens / Sentences thr-d tear spr-l loose str.? strict thr-d tear spr-l loose str.? strict
Arabic 2006 52,752 1,460 ? ? ? ? ? ? 89.6 89.5 81.9 61.2 29.7 33.4
?7 102,375 2,912 ? ? ? ? ? ? 90.9 90.6 83.1 61.2 29.5 35.2
Basque ?7 41,013 3,190 ? ? ? ? ? ? 96.2 95.7 92.3 81.9 42.8 50.6
Bulgarian ?6 162,985 12,823 99.8 99.5 96.6 96.4 51.8 81.0 97.6 97.2 96.1 74.7 36.7 41.2
Catalan ?7 380,525 14,958 100 99.5 95.0 94.6 15.8 57.9 96.1 95.5 94.6 73.7 36.0 42.6
Chinese ?6 337,162 56,957 ? ? ? ? ? ? ? ? ? ? ? ?
?7 337,175 56,957 ? ? ? ? ? ? ? ? ? ? ? ?
Czech ?6 1,063,413 72,703 99.7 98.3 96.2 95.4 42.4 68.0 89.4 89.2 87.7 68.9 37.2 41.7
?7 368,624 25,364 99.7 98.3 96.1 95.4 42.6 67.6 89.5 89.3 87.8 69.3 37.4 41.9
Danish ?6 80,743 5,190 99.9 99.4 98.3 97.0 59.0 69.7 96.9 96.9 95.2 68.3 39.6 40.9
Dutch ?6 172,958 13,349 99.9 99.1 98.4 96.6 16.6 46.3 89.6 89.5 86.4 69.6 42.5 46.2
English ?7 395,139 18,577 99.3 98.7 98.0 96.0 17.5 24.8 91.5 91.4 90.6 76.5 39.6 42.3
German ?6 605,337 39,216 99.6 98.0 96.7 96.4 41.7 57.1 94.5 93.9 90.7 71.1 37.2 40.7
Greek ?7 58,766 2,705 99.9 99.3 98.5 96.6 13.6 50.1 91.3 91.0 89.8 75.7 43.7 47.0
Hungarian ?7 111,464 6,034 99.9 98.1 95.7 94.4 46.6 62.0 96.1 94.0 89.0 77.1 28.9 32.6
Italian ?7 60,653 3,110 99.9 99.6 99.0 98.8 12.8 68.2 97.1 96.8 96.0 77.8 44.7 47.9
Japanese ?6 133,927 17,044 ? ? ? ? ? ? 100 100 95.4 89.0 48.9 63.5
Portuguese ?6 177,581 9,071 100 99.0 97.6 97.0 14.4 37.7 96.0 95.8 94.9 74.5 40.3 45.0
Slovenian ?6 23,779 1,534 100 99.8 98.9 98.9 52.0 84.7 93.3 93.3 92.6 72.7 42.7 45.8
Spanish ?6 78,068 3,306 ? ? ? ? ? ? 96.5 96.0 95.2 75.4 33.4 40.9
Swedish ?6 163,301 11,042 99.8 99.6 99.0 97.0 24.7 58.4 90.8 90.4 87.4 66.8 31.1 33.9
Turkish ?6 48,373 4,997 100 99.8 96.2 94.0 22.8 42.8 99.8 99.7 95.1 76.9 37.7 42.0
?7 54,761 5,635 100 99.9 96.1 94.2 21.6 42.9 99.8 99.7 94.6 76.7 38.2 42.8
Max: 100 99.9 99.0 98.9 59.0 84.7 100 100 96.1 89.0 48.9 63.5
Mean: 99.8 99.1 97.4 96.4 30.8 57.7 94.6 94.2 91.7 74.0 38.5 43.3
Min: 99.3 98.0 95.0 94.0 12.8 24.8 89.4 89.2 81.9 61.2 28.9 32.6
Table 6: Accuracies for capitalization- and punctuation-induced constraints on all (full) 2006/7 CoNLL training sets.
ularly special about Greek or Italian in these sum-
maries that could explain their substantial improve-
ments (18 and 11%, respectively ? see Table 4),
though Italian does appear to mesh best with the
sprawl constraint (not by much, closely followed by
Swedish). And English ? the language from which
we drew our inspiration ? barely improved with
capitalization-induced constraints (see Table 4) and
caused the lowest accuracies of thread and strict.
These outcomes are not entirely surprising: some
best- and worst-performing results are due to noise,
since learning via non-convex optimization can be
chaotic: e.g., in the case of Greek, applying 113 con-
straints to initial parse trees could have a significant
impact on the first grammar estimated in training ?
and consequently also on a learner?s final, converged
model instance. We expect the averages (i.e., means
and medians) ? computed over many data sets ?
to be more stable and meaningful than the outliers.
7.2 Immediate Impact from Capitalization
Next, we considered two settings that are less af-
fected by training noise: grammar inducers immedi-
ately after an initial step of constrained Viterbi EM
and supervised DBM parsers (trained on sentences
with up to 45 words), for various languages in the
CoNLL sets. Table 7 shows effects of capitalization
to be exceedingly mild, both if applied alone and in
tandem with punctuation. Exploring better ways of
incorporating this informative resource ? perhaps
as soft features, rather than as hard constraints ?
and in combination with punctuation- and markup-
induced bracketings could be a fruitful direction.
7.3 Odds and Ends
Our earlier analysis excluded sentence-initial words
because their capitalization is, in a way, trivial. But
for completeness, we also tested constraints derived
from this source, separately (see Table 2: initials).
As expected, the new constraints scored worse (de-
spite many automatically-correct single-word frag-
ments) except for strict, whose binding constraints
over singletons drove up accuracy. It turns out, most
first words in WSJ are leaves ? possibly due to a
dearth of imperatives (or just English?s determiners).
We broadened our investigation of the ?first leaf?
20
CoNLL Year Evaluation Bracketings Unsupervised Training Supervised Parsing
& Language Tokens / Sents capital. punct. init. 1-step constrained none capital. punct. both
Arabic 2006 5,215 146 ? 101 18.4 20.6 ? ? 59.8 ? ? ?
?7 4,537 130 ? 311 19.0 23.5 ? ? 63.5 ? ? ?
Basque ?7 4,511 334 ? 547 17.4 22.4 ? ? 58.4 ? ? ?
Bulgarian ?6 5,032 398 44 552 19.4 28.9 28.4 -0.5 76.7 76.8 78.1 78.2
Catalan ?7 4,478 167 24 398 18.0 25.1 25.4 +0.3 78.1 78.3 78.6 78.9
Chinese ?6 5,012 867 ? ? 23.5 27.2 ? ? 83.7 ? ? ?
?7 5,161 690 ? ? 19.4 25.0 ? ? 81.0 ? ? ?
Czech ?6 5,000 365 48 549 18.6 19.7 19.8 +0.1 64.9 64.8 67.0 66.9
?7 4,029 286 57 466 18.0 21.7 ? ? 62.8 ? ? ?
Danish ?6 4,978 322 85 590 19.5 27.4 26.0 -1.3 71.9 72.0 74.2 74.3
Dutch ?6 4,989 386 28 318 18.7 17.9 17.7 -0.1 60.9 60.9 62.7 62.8
English ?7 4,386 214 151 423 17.6 24.0 21.9 -2.1 65.2 65.6 68.5 68.4
German ?6 4,886 357 135 523 16.4 23.0 23.7 +0.7 70.7 70.7 71.5 71.4
Greek ?7 4,307 197 47 372 17.1 17.1 16.6 -0.5 71.3 71.6 73.5 73.7
Hungarian ?7 6,090 390 28 893 17.1 18.5 18.6 +0.1 67.3 67.2 69.8 69.6
Italian ?7 4,360 249 71 505 18.6 32.5 34.2 +1.7 66.0 65.9 67.0 66.8
Japanese ?6 5,005 709 ? 0 26.5 36.8 ? ? 85.1 ? ? ?
Portuguese ?6 5,009 288 29 559 19.3 24.2 24.0 -0.1 80.5 80.5 81.6 81.6
Slovenian ?6 5,004 402 7 785 18.3 22.5 22.4 -0.1 67.5 67.4 70.9 70.9
Spanish ?6 4,991 206 ? 453 18.0 19.3 ? ? 69.5 ? ? ?
Swedish ?6 4,873 389 14 417 20.2 31.4 31.4 +0.0 74.9 74.9 74.7 74.6
Turkish ?6 6,288 623 18 683 20.4 26.4 26.7 +0.3 66.1 66.0 66.9 66.7
?7 3,983 300 4 305 20.3 24.8 ? ? 67.3 ? ? ?
Max: 20.4 32.5 34.2 +1.7 80.5 80.5 81.6 81.6
(aggregated as in Tables 4 and 5) Mean: 18.5 24.2 24.1 -0.1 70.1 70.2 71.8 71.8
Min: 16.4 17.1 16.6 -2.1 60.9 60.9 62.7 62.8
Table 7: Unsupervised accuracies for uniform-at-random projective parse trees (init), also after a step of Viterbi EM,
and supervised performance with induced constraints, on 2006/7 CoNLL evaluation sets (sentences under 145 tokens).
phenomenon and found that in 16 of the 19 CoNLL
languages first words are more likely to be leaves
than other words without dependents on the left;4
last words, by contrast, are more likely to take de-
pendents than expected. These propensities may be
related to the functional tendency of languages to
place old information before new (Ward and Birner,
2001) and could also help bias grammar induction.
Lastly, capitalization points to yet another class of
words: those with identical upper- and lower-case
forms. Their constraints too tend to be accurate (see
Table 2: uncased), but the underlying text is not par-
ticularly interesting. In WSJ, caseless multi-token
fragments are almost exclusively percentages (e.g.,
the two tokens of 10%), fractions (e.g., 1 1/4) or both.
Such boundaries could be useful in dealing with fi-
nancial data, as well as for breaking up text in lan-
guages without capitalization (e.g., Arabic, Chinese
4Arabic, Basque, Bulgarian, Catalan, Chinese, Danish,
Dutch, English, German, Greek, Hungarian, Italian, Japanese,
Portuguese, Spanish, Swedish vs. Czech, Slovenian, Turkish.
and Japanese). More generally, transitions between
different fonts and scripts should be informative too.
8 Conclusion
Orthography provides valuable syntactic cues. We
showed that bounding boxes signaled by capitaliza-
tion changes can help guide grammar induction and
boost unsupervised parsing performance. As with
punctuation-delimited segments and tags from web
markup, it is profitable to assume only that a single
word derives the rest, in such text fragments, without
further restricting relations to external words ? pos-
sibly a useful feature for supervised parsing models.
Our results should be regarded with some cau-
tion, however, since improvements due to capitaliza-
tion in grammar induction experiments came mainly
from two languages, Greek and Italian. Further re-
search is clearly needed to understand the ways that
capitalization can continue to improve parsing.
21
Acknowledgments
Funded, in part, by Defense Advanced Research Projects Age-
ncy (DARPA) Machine Reading Program under Air Force Re-
search Laboratory (AFRL) prime contract FA8750-09-C-0181.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reflect the views of DARPA, AFRL, or the US gov-
ernment. We also thank Ryan McDonald and the anonymous
reviewers for helpful comments on draft versions of this paper.
References
J. K. Baker. 1979. Trainable grammars for speech recognition.
In Speech Communication Papers for the 97th Meeting of the
Acoustical Society of America.
E. J. Briscoe. 1994. Parsing (with) punctuation, etc. Technical
report, Xerox European Research Laboratory.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In CoNLL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic normal dis-
tributions for soft parameter tying in unsupervised grammar
induction. In NAACL-HLT.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs:
Hardness results and competitiveness of uniform initializa-
tion. In ACL.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsupervised
structure prediction with non-parallel multilingual guidance.
In EMNLP.
T. Cohn, P. Blunsom, and S. Goldwater. 2011. Inducing tree-
substitution grammars. Journal of Machine Learning Re-
search.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
R. Frank. 2000. From regular to context-free to mildly context-
sensitive tree rewriting systems: The path of child language
acquisition. In A. Abeille? and O. Rambow, editors, Tree
Adjoining Grammars: Formalisms, Linguistic Analysis and
Processing. CSLI Publications.
K. Gimpel and N. A. Smith. 2011. Concavity and initialization
for unsupervised dependency grammar induction. Technical
report, CMU.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. Marec?ek and Z. Zabokrtsky?. 2011. Gibbs sampling with
treeness constraint in unsupervised dependency parsing. In
ROBUS.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-source trans-
fer of delexicalized dependency parsers. In EMNLP.
T. Naseem and R. Barzilay. 2011. Using semantic cues to learn
syntax. In AAAI.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010. Using
universal linguistic knowledge to guide grammar induction.
In EMNLP.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In EMNLP-CoNLL.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
E. Ponvert, J. Baldridge, and K. Erk. 2010. Simple unsuper-
vised identification of low-level constituents. In ICSC.
E. Ponvert, J. Baldridge, and K. Erk. 2011. Simple unsuper-
vised grammar induction from raw text with cascaded finite
state models. In ACL-HLT.
M. S. Rasooli and H. Faili. 2012. Fast unsupervised depen-
dency parsing with arc-standard transitions. In ROBUS-
UNSUP.
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
ACL.
A. S?gaard. 2011a. Data point selection for cross-language
adaptation of dependency parsers. In ACL-HLT.
A. S?gaard. 2011b. From ranked words to dependency trees:
two-stage unsupervised non-projective dependency parsing.
In TextGraphs.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a. From
Baby Steps to Leapfrog: How ?Less is More? in unsuper-
vised dependency parsing. In NAACL-HLT.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010b. Profiting
from mark-up: Hyper-text annotations for guided parsing. In
ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen
EM: Unsupervised training with multiple objectives, applied
to dependency grammar induction. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctu-
ation: Making a point in unsupervised dependency parsing.
In CoNLL.
V. I. Spitkovsky, A. X. Chang, H. Alshawi, and D. Jurafsky.
2011c. Unsupervised dependency parsing without gold part-
of-speech tags. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012. Three
dependency-and-boundary models for grammar induction.
In submission to EMNLP.
K. Tu and V. Honavar. 2011. On the utility of curricula in
unsupervised learning of probabilistic grammars. In IJCAI.
G. Ward and B. J. Birner. 2001. Discourse and information
structure. In D. Schiffrin, D. Tannen, and H. Hamilton, edi-
tors, Handbook of Discourse Analysis. Oxford: Basil Black-
well.
22
Workshop on Computational Linguistics for Literature, pages 8?17,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
A Computational Analysis of Style, Affect, and Imagery in Contemporary
Poetry
Justine Kao
Psychology Department
Stanford University
Stanford, CA 94305, USA
justinek@stanford.edu
Dan Jurafsky
Linguistics Department
Stanford University
Stanford, CA 94305, USA
jurafsky@stanford.edu
Abstract
What makes a poem beautiful? We use
computational methods to compare the stylis-
tic and content features employed by award-
winning poets and amateur poets. Building
upon existing techniques designed to quanti-
tatively analyze style and affect in texts, we
examined elements of poetic craft such as dic-
tion, sound devices, emotive language, and
imagery. Results showed that the most impor-
tant indicator of high-quality poetry we could
detect was the frequency of references to con-
crete objects. This result highlights the influ-
ence of Imagism in contemporary professional
poetry, and suggests that concreteness may be
one of the most appealing features of poetry to
the modern aesthetic. We also report on other
features that characterize high-quality poetry
and argue that methods from computational
linguistics may provide important insights into
the analysis of beauty in verbal art.
1 Introduction
Poetry is nerved with ideas, blooded with emotions,
held together by the delicate, tough skin of words.
?Paul Engle (1908 -1991)
Many people have experienced the astounding
and transformational power of a beautiful poem.
However, little empirical research has been done to
examine the textual features or mental processes that
engender such a sensation. In this paper, we pro-
pose a computational framework for analyzing tex-
tual features that may be responsible for generating
sensations of poetic beauty. We built a poetry cor-
pus consisting of poems by award-winning profes-
sional poets and amateur poets, and compared po-
ems in the two categories using various quantita-
tive features. Although there are many reasons why
some poems are included in prestigious anthologies
and others are never read, such as a poet?s fame,
we assume that the main distinction between poems
in well-known anthologies and poems submitted by
amateurs to online forums is that expert editors per-
ceive poems in the former category as more aesthet-
ically pleasing. Given this assumption, we believe
that the kind of comparison we propose should be
the first step towards understanding how certain tex-
tual features might evoke aesthetic sensations more
than others.
The next sections review previous computational
work on poetry and motivate the features we use; we
then introduce our corpus, our analyses, and results.
2 Computational aesthetics
Previous research on the computational analysis of
poetry focused on quantifying poetic devices such
as rhyme and meter (Hayward, 1996; Greene et al,
2010; Genzel et al, 2010), tracking stylistic influ-
ence between authors (Forstall et al, 2011), or clas-
sifying poems based on the poet and style (Kaplan
& Blei, 2007; He et al, 2007; Fang et al, 2009).
These studies showed that computational methods
can reveal interesting statistical properties in poetic
language that allow us to better understand and cate-
gorize great works of literature (Fabb, 2006). How-
ever, there has been very little work using computa-
tional techniques to answer an important question in
8
both poetics and linguistics (Jakobson, 1960): what
makes one poem more aesthetically appealing than
another?
One such attempt is the ?aesthetic measure? pro-
posed by mathematician G.D. Birkhoff, who for-
malized beauty as a ratio between order and com-
plexity (Birkhoff, 1933). Birkhoff found interest-
ing correlations between the measure and people?s
aesthetic judgments of shapes, sounds, and poems.
While the aesthetic measure enjoyed some success
in the domain of visual arts (Rigau et al, 2008),
it ran into problems of semantics when applied to
language. Birkhoff?s aesthetic measure judges a
poem?s beauty based solely on phonemic features,
such as alliterations and assonance, rhymes, and mu-
sical vowels. The formula does not capture the sub-
tlety of word choice or richness of meaning in po-
etry. Since Birkhoff?s measure only considers pho-
netic features, it fails to fully quantify the aesthetic
value of meaningful poetic texts.
In this paper, we aim to combine computational
linguistics with computational aesthetics. We in-
troduce a variety of theoretically-motivated features
that target both poetic style and content, and exam-
ine whether each feature is a distinguishing char-
acteristic of poems that are considered beautiful by
modern experts and critics.
3 Elements of Craft
One demands two things of a poem. Firstly, it must
be a well-made verbal object that does honor to the
language in which it is written. Secondly, it must say
something significant about a reality common to us
all, but perceived from a unique perspective
?W. H. Auden (1907 - 1973)
We review several elements of craft that creative
writers and critics reference in their analysis and ap-
preciation of poetry. For each feature that we con-
sider in our model, we provide theoretical motiva-
tion from creative writing and literary criticism. We
then describe how we computed the values of each
feature using tools from computational linguistics.
3.1 Diction
Aristotle argued that good writing consists of a bal-
ance of ordinary words that make the writing com-
prehensible and strange words that make the writ-
ing distinguished (Aristotle, 1998). Several hun-
dred years later, Longinus argued that ?noble dic-
tion and elevated word arrangement? is one of the
primary sources of aesthetic language (Earnshaw,
2007; Longinus, 2001). These early scholars of po-
etic craft passed down the belief that poetic beauty
stems from the level of individual words. In her
influential creative writing textbook titled, ?Imagi-
native Writing: The Elements of Craft,? Burroway
(2007) describes poetry as a high-density form of
language. Poetic language is usually intentionally
ambiguous and often packs several meanings into
a compact passage (Addonizio & Laux, 1997). As
a result, each word in a poem carries especially
heavy weight and must be carefully selected and di-
gested. Based on these ideas, we decided to examine
whether or not good poetry is defined by the use of
sophisticated vocabulary.
Diction can be evaluated from two different per-
spectives: word frequency, a measure of difficulty,
and type-token ratio, a measure of diversity.
Word frequency: Psychologists, linguists, and
testing agencies often use word frequency to esti-
mate the difficulty and readability of words and sen-
tences (Marks, Carolyn B. et al, 1974; Breland,
1996). Based on these studies, it is reasonable to
predict that poems written by professional poets may
contain more difficult words and lower average word
frequencies than poems written by amateur poets.
We measured average word frequency using a list
of top 500,000 most frequent words from the Cor-
pus of Contemporary American English (COCA)
(Davies, 2011). An average log word frequency was
obtained for each poem by looking up each word in
the poem in the word list and summing up the log
word frequencies. The total log frequency was then
divided by the number of words in the poem to ob-
tain the average.
Type-token ratio: Readability measures and au-
tomatic essay grading systems often use the ra-
tio of total word types to total number of words
in order to evaluate vocabulary sophistication, with
higher type-token ratios indicating more diverse and
sophisticated vocabulary (Ben-Simon & Bennett,
2007; Pitler & Nenkova, 2008). We predict that
professional poets utilize a larger and more varied
vocabulary and avoid using the same word several
times throughout a poem. A type-token ratio score
9
was calculated for each poem by counting all the
separate instances of words and dividing that num-
ber by the total number of words in the poem.
3.2 Sound Device
Poetry has a rich oral tradition that predates liter-
acy, and traces of this aspect of poetic history can
be found in sound devices such as rhyme, repeti-
tion, and meter. How a poem sounds is critical to
how it is perceived, understood, and remembered.
Indeed, most contemporary creative writing hand-
books devote sections to defining various sound de-
vices and analyzing notable poetry according to in-
teresting patterns of sound (Burroway, 2007; Ad-
donizio & Laux, 1997).
The sound device features described below were
computed using Kaplan?s 2006 PoetryAnalyzer. Po-
etryAnalyzer utilizes the Carnegie Mellon Pro-
nouncing Dictionary to obtain pronunciations of
words in each poem and identify patterns indicative
of poetic sound devices.
Perfect and slant end rhyme: Rhyme is one of
the most well-known and popular sound devices in
poetry. The earliest poets used strict rhyme schemes
as a mnemonic device to help them memorize and
recite long poems. Research in psychology has con-
firmed poets? intuitions about the powerful effects
of rhyme on perception and learning. For example,
an aphorism that contains a rhyme is more likely to
be perceived as true than a non-rhyming aphorism
with the same meaning (McGlone & Tofighbakhsh,
2000). Exposure to rhymes also enhances phono-
logical awareness in young children and can lead to
better reading performances (Bryant et al, 1990).
The PoetryAnalyzer identifies end rhymes in po-
ems by examining the phoneme sequences at the end
of lines. A window of four line endings is analyzed
at a time. If two words in the window have different
initial consonants but identical phoneme sequences
from the stressed vowel phoneme onward, then an
instance of a perfect end rhyme instance is recorded.
The final count of perfect end rhymes in a poem is
normalized by the total number of words. If two
words in the window of four line endings have the
same stressed vowel but different phonemes follow-
ing the stressed vowel, then an instance of a slant
end rhyme is recorded. The final count of slant end
rhymes in a poem is normalized by the total number
of words.
Alliteration and consonance: Alliteration is the
repetition of consonant sounds at the beginning of
words, and consonance is the repetition of conso-
nant sounds elsewhere. In addition to rhyme, allit-
eration was used as a powerful mnemonic device in
ancient epic poetry (Rubin, 1995). Researchers in
psychology and discourse analysis have shown that
alliteration reactivates readers? memories for previ-
ous information that was phonologically similar to
the cue (Lea et al, 2008).
The PoetryAnalyzer identifies alliteration and
consonance as follows. If the initial phoneme of two
consecutive words are identical consonants, the allit-
eration count is incremented. The total count is then
divided by the total number of words to obtain a al-
literation score for each poem. If there are at least
two identical consonant phonemes in a window of
nine syllables, the consonance count is incremented.
The count is divided by the total number of words in
a poem to obtain a consonance score.
Assonance: Assonance is the repetition of vowel
sounds. Similar to consonants, different vowel
sounds also have their own characteristics and ef-
fects. Long vowels take longer to utter and draw out
the rhythm and pacing of the line, while short vow-
els feel brief and urgent (Burroway, 2007).
We calculated an assonance score for each poem
in the same fashion as we did for the consonance
score, except the target phonemes are vowels instead
of consonants.
3.3 Affect
Studies have shown that poetry allows mental health
patients to explore and reinterpret their emotions
in useful ways. Through reading and writing po-
etry, patients are able to freely express their thoughts
without the constraints of form and logic (Harrower,
1972). On the other hand, critics of poetry therapy
have suggested that writing poetry may be harmful
to psychological health, because it allows the poet
to immerse herself in an inexplicable emotion with-
out having to make sense or order out of it (Stirman
& Pennebaker, 2001). For example, Silverman &
Will (1986) claimed that Sylvia Plath?s poetry may
have undermined her control mechanisms and con-
tributed to her death. If reading good poetry is found
to be cathartic and therapeutic, do skilled poets make
10
more references to psychological states and explore
the emotional world with more depth and intensity?
We examine this question using several existing
sentiment lexicons available for sentiment analy-
sis research. One is the Harvard General Inquirer,
which consists of 182 word categories, including
basic sentiment categories, categories for concrete
objects, and categories for abstract concepts (Stone
et al, 1966). Another sentiment lexicon is the
Linguistic Inquiry and Word Count (LIWC) (Pen-
nebaker et al, 2001). While the General Inquirer
was designed for content analysis, LIWC was de-
signed to facilitate the understanding of individuals?
cognitive and emotional states through text analy-
sis. As a result, most of the categories in LIWC in-
volve mental activity, with over 4, 500 words related
to affective, social, and cognitive processes. Six cat-
egories from the Harvard General Inquirer and two
categories from LIWC were selected because they
are most suitable for our purpose of analyzing el-
ements of poetic craft. These features are summa-
rized in Table 1.
3.4 Imagery
One of the most important and oft-repeated piece of
advice for writers is the following: ?Show, don?t
tell.? Burroway (2007) interprets this as meaning:
?Use concrete, significant details that address the
senses.? Effective imagery allows readers to bring
in their own associations to understand and truly ex-
perience a new emotion, and skilled poets and writ-
ers are able to pick out specific sensory details that
evoke deeper abstractions and generalizations.
The appeal of concrete imagery may have roots in
processes that facilitate learning and memory. Previ-
ous research has shown that concrete noun pairs are
easier to memorize than abstract noun pairs, which
suggests that imagery can enhance the learning of
word pairings (Paivio et al, 1966). Other studies
have shown that mental imagery facilitates relational
association between concepts (Bower, 1970). Fur-
thermore, Jessen et al (2000) found neural corre-
lates that suggest that concrete nouns are processed
differently in the brain than abstract nouns. One of
the reasons why we find poetic imagery striking may
be due to the psychological power of imagery to
evoke rich associations formed by culture and per-
sonal experience.
Feature Examples
Word frequency ?
Type-token ratio ?
Perfect end rhyme floor / store
Slant end rhyme bred / end
Alliteration frozen field
Consonance brown skin hung
Assonance shallower and yellowed
Positive outlook able; friend
Negative outlook abandon; enemy
Positive emotion happiness; love
Negative emotion fury; sorrow
Phys. wellbeing alive; eat
Psych. wellbeing calm; adjust
Object boat; leaf
Abstract day; love
Generalization none; all
Table 1: Summary of features
Another reason why imagery is an essential ele-
ment of poetic craft is that it allows writers to avoid
falling into cliche, which is the bane of the creative
writer?s existence. Burroway (2007) writes, ?flat
writing is. . . full of abstractions, generalizations, and
judgments. When these are replaced with nouns that
call up a sense image and with verbs that represent
actions we can visualize, the writing comes alive.?
Many abstract and common concepts can be embod-
ied or evoked by surprising imagery. In our analy-
sis, we predict that skilled poets are more likely to
describe concrete objects and less likely to reference
abstract concepts. We measure the degree to which
a poem contains concrete details rather than abstrac-
tions and generalizations using categories from the
Harvard General Inquirer (see Table 1).
4 Methods
4.1 Materials
In order to test the defining features of beautiful po-
etry described in the section above, we constructed
a corpus containing poems that vary in quality and
?beauty? by some established standard. One way
to do this would be to randomly sample poems from
various sources and ask experts to rate them for qual-
ity and beauty. However, such a method can be ex-
pensive and time-consuming. A more efficient way
11
of achieving a similar effect is to sample poems from
pre-existing categories, such as poems written by
skilled professional poets versus poems written by
amateur poets. We assume that award-winning poets
produce poems that experts would consider ?better?
and more beautiful than poetry written by amateurs.
Although there might be exceptions, since for ex-
ample experts may consider some poems written by
amateur poets to be very beautiful and sophisticated,
these pre-existing categories for the most part should
be a good approximation of expert opinions.
One hundred poems were selected from sixty-
seven professional poets whose work was published
in a collection of Contemporary American Poetry
(Poulin & Waters, 2006). The poets produced most
of their work towards the middle and end of the 20th
century and are considered some of the best contem-
porary poets in America (e.g., Louise Gluck, Mary
Oliver, Mark Strand, etc.). All of the poets are listed
in the website of the Academy of American Poets
and many have won prestigious awards. This serves
as confirmation that the poets in this collection are
widely acclaimed and that their craft is acknowl-
edged and celebrated by poetry experts and literary
critics.
We randomly selected one to three poems from
each poet, proportionate to the number of poems
each poet had in the collection. When an excessively
long poem (over 500 words) was selected, we re-
moved it and replaced it with a different poem from
the same poet. This served as a rough control for the
length of the poems in the corpus. The final selection
of one hundred professional poems ranged from 33
to 371 words in length with an average length of 175
words. We believe that these poems are a good rep-
resentation of work produced by the best and most
celebrated poets of our time.
In addition, one hundred poems were selected
from amateur poets who submitted their work
anonymously to a free and open-to-all website,
aptly called ?Amateur Writing? (www.amateur-
writing.com). At the time of selection, the website
had over 2500 amateur poem submissions by regis-
tered users. The website contains a diverse set of
poems submitted by amateur writers with a wide
range of experience and skill levels. We randomly
selected one hundred poems from the website and
corrected for misspellings and obvious grammatical
errors in the poems to control for the effect of basic
language skills. The final selection of amateur po-
ems ranged from 21 to 348 words in length with an
average length of 136 words.
4.2 Procedures
We implemented the 16 features described in sec-
tion 3, each of which target one of three separate
domains: style, sentiment, and imagery. The sound
device scores were computed using PoetryAnalyzer
(Kaplan & Blei, 2007). For each category taken
from the General Inquirer, scores were calculated
using the General Inquirer system available on a
server (Inquirer, 2011). A score for a certain cat-
egory is the number of words in a poem that ap-
pear in the category normalized by the length of the
poem. For the two categories taken from LIWC,
scores were calculated by counting the number of
words in each poem that match a word stem in the
LIWC dictionary and dividing it by the total number
of words. A score for each of the features was de-
rived for every poem in the poetry corpus. All scores
were then standardized to have zero mean and unit
variance across poems.
5 Results and Analysis
To measure the effect of each variable on the like-
lihood of a poem being written by a professional
or an amateur poet, we constructed a logistic re-
gression model in R (R: A Language and Environ-
ment for Statistical Computing). For model selec-
tion, we used the step-wise backward elimination
method. This method begins by building a model us-
ing all 16 feature variables. It then recursively elim-
inates variables that do not significantly contribute
to explaining the variance in the data according to
the Akaike information criterion (AIC), which mea-
sures the amount of information lost when using a
certain model. The selection method stops when fur-
ther eliminating a variable would result in significant
loss of information and model fit. The final logistic
regression model for the predictors of professional
versus amateur poetry is summarized in the formula
above (Table 2). Note that the variables included in
the final model might not all be statistically signifi-
cant.
Results show that poem type (professional or am-
12
Probability(poem type = professional |X), where
X? ?0.6071 =
?0.5039 * average log word frequency +
0.6646 * type token ratio +
0.4602 * slant end rhyme frequency +
?2.1 * perfect end rhyme frequency +
?0.6326 * alliteration frequency +
?1.0701 * positive outlook words +
?0.7861 * negative emotional words +
?0.5227 * psychological words +
1.3124 * concrete object words +
?1.2633 * abstract concept words +
?0.836 * generalization words
Table 2: Model formula
ateur) is significantly predicted by eight different
variables (p < 0.05): type token ratio, perfect
end rhyme frequency, alliteration frequency, positive
outlook words, negative emotional words, concrete
object words, abstract concept words, and general-
ization words. The other nine variables: average log
word frequency, slant end rhyme frequency, asso-
nance, consonance, negative outlook words, positive
emotional words, physical well-being words, and
psychological words did not have significant predic-
tive value. While positive outlook and positive emo-
tion were highly correlated (r = 0.54), as were neg-
ative outlook and negative emotion (r = 0.53), there
was no collinearity among the variables in the final
logistic regression model selected by the backward
elimination method.
The model predicts the likelihood of the poem
type (professional or amateur) using the formula de-
scribed in Table 2. The influence of each feature is
represented by the coefficient ? for each variable.
A positive value for a coefficient increases the like-
lihood of a poem being written by a professional.
For example, type token ratio and concrete object
words have positive coefficient values; thus higher
type token ratios and more concrete object words in-
crease the likelihood of a poem being a professional
poem. A negative value for a coefficient decreases
the likelihood of a poem being written by a profes-
sional. For example, perfect end rhyme frequency
has a negative coefficient value, and thus higher per-
fect end rhyme frequencies decrease the likelihood
of a poem being written by a professional poet. The
Feature variable Odds p-value
type token ratio 1.94 0.0308
perfect end rhyme frequency 0.12 5.06e?7
alliteration frequency 0.53 0.0188
positive outlook words 0.34 0.0130
negative emotional words 0.46 0.0244
concrete object words 3.72 0.0002
abstract concept words 0.28 0.0027
generalization words 0.43 0.0035
Table 3: Odds ratios and p values of significant predictors
of professional poetry
Professional Amateur
Word Count Word Count
tree 29 thing 40
room 20 wall 12
thing 18 bed 11
grass 17 clock 7
wall 14 room 7
flower 13 tree 6
glass 13 leave 6
floor 13 gift 5
car 12 mirror 4
dirt 11 flower 4
[. . .] 538 [. . .] 103
Proportion 4.1% Proportion 1.5%
Type count 250 Type count 85
Table 4: Concrete words
relative odds and p-values of each significant predic-
tor variable are presented in Table 3.
In summary, professional poems have signifi-
cantly higher type-token ratios, contain fewer per-
fect end rhymes, fewer instances of alliteration,
fewer positive outlook words, fewer negative emo-
tional words, more references to concrete objects,
less references to abstract concepts, and fewer gen-
eralizations. From the odds ratios, we can see that
the most significant predictors of professional poetry
are fewer perfect end rhymes and more references to
concrete objects.
6 Discussion
What are skilled poets doing differently from ama-
teurs when they write beautiful poetry? Based on re-
sults from our regression model, it appears that Aris-
13
Professional Amateur
Word Count Word Count
day 40 day 54
night 31 time 33
year 25 beauty 25
time 20 soul 16
death 11 night 15
new 9 new 14
morning 8 moment 13
childhood 7 christmas 12
hour 7 think 11
afternoon 7 future 9
[. . .] 139 [. . .] 143
Proportion 1.8% Proportion 2.6%
Type count 82 Type count 75
Table 5: Abstract words
Professional Amateur
Word Count Word Count
all 63 all 82
nothing 26 never 46
never 19 always 43
always 14 nothing 21
every 11 every 15
any 10 forever 14
anything 5 anything 7
nobody 5 any 6
everything 5 everything 5
forever 3 everyone 4
Proportion < 1% Proportion 1.8%
Table 6: Generalization words
totle may have been wrong about diction, at least
for modern poetry. The words in professional po-
etry are not significantly more unusual or difficult
than words used by amateur writers. This suggests
that contemporary poets are not interested in flowery
diction or obscure words, but are focused on using
ordinary words to create extraordinary effects.
However, professional poets do use more distinct
word types. The 100 poems written by professional
poets contain a total of 18, 304 words and 4, 315 dis-
tinct word types (23.57%). The 100 poems written
by amateur poets contain a total of 14, 046 words
and 2, 367 distinct word types (16.85%), a much
smaller portion. In aggregate, professional poets
have a larger and more varied vocabulary than am-
ateur poets. Moreover, professional poets use a sig-
nificantly larger number of word types within each
poem. Although professional poets do not use more
difficult and unusual words, higher type-token ra-
tio is a significant predictor of professional poetry,
suggesting that professional poems may be distin-
guished by a richer set of words.
The results on sound devices provide interesting
insight into the current stylistic trends of contempo-
rary professional poetry. While sound devices have a
long history in poetry and are considered a feature of
poetic beauty, contemporary professional poets now
use these devices much less often than amateur po-
ets. Sound devices that were traditionally important
in poetry for mnemonic purposes, such as rhyme
and alliteration, are more prevalent in amateur po-
ems. Even subtle and sophisticated sound devices
like slant rhyme, consonance, and assonance are not
significant indicators of professional poetry. These
results suggest that repetition of sound is becoming
a less aesthetically significant poetic device among
contemporary masters of poetry.
In terms of affect, our results suggest that po-
ems by professional poets are not more negatively
emotional?at least not explicitly. On the contrary,
amateur poets are significantly more likely to ref-
erence negative emotions than professional poets.
Our results reveal an interesting distinction between
words with positive and negative outlooks and con-
notations versus words that reference positive and
negative emotions. While the two pairs of cate-
gories are strongly correlated, they capture different
aspects of a text?s emotional content. The positive
14
and negative outlook categories contain many words
that are not emotions but may evoke certain emo-
tional attitudes, such as clean and death. The fact
that professional poets are significantly less likely to
use explicitly negative emotion words than amateur
poets, but not significantly less likely to use nega-
tively connotative words, suggests that professional
poets may evoke more negative sentiment through
connotation rather than explicit descriptions.
As predicted, poems written by professional poets
contain significantly more words that reference ob-
jects and significantly less words about abstract con-
cepts and generalizations. This result suggests that
professional poets follow the sacred rule of ?show,
don?t tell? and let images instead of words con-
vey emotions, concepts, and experiences that stick
to readers? minds. Professional poets not only use
more object words than amateur poets (698 counts
versus 205), but they also use a larger and more di-
verse set of object words (250 types versus 85), as
shown in Table 4. Professional poets reference natu-
ral objects very often, such as tree, grass, and flower.
On the other hand, the most frequent concrete object
word in amateur poems is the extremely vague word
thing. This suggests that even when amateur poets
reference concrete objects, they do not use words
that provide specific sensory details.
Our analysis supports the idea that Imagism has
strongly influenced the ways in which modern poets
and literary critics think about literary writing. Lit-
erary critic I.A. Richards argued that image clusters
and patterns of imagery are keys to deeper meaning
in literary works, and that critics should pay close at-
tention to these patterns in order to understand ?the
language of art? beneath the surface ordinary lan-
guage (Richards, 1893). Not only are concrete im-
ages able to render the world in spectacular detail,
they also provide windows into particular experi-
ences on which readers can project their own per-
ceptions and interpretations.
Consistent with our predictions and with the aes-
thetic ideals of Imagism, professional poets also
make significantly fewer direct references to abstract
and intangible concepts (Table 5). If the deeper
meaning of a poem is conveyed through imagery, ab-
stract words are no longer needed to reference con-
cepts and experiences explicitly. Moreover, amateur
poets use significantly more words concerned with
generalizations, as shown in Table 6. While amateur
poets embrace the human impulse to generalize, the
skilled poet must learn to extract and report unique
details that single out each experience from the rest.
Overall, our results suggest that professional po-
ets are more likely to show, while amateur poets
have a tendency to tell. This difference marks the
most significant distinction between contemporary
professional and amateur poetry in our analysis and
may be an essential aspect of craft and poetic beauty.
7 Future directions
Categorizing poetry as professional or amateur is a
rather coarse measure of quality. In order to iden-
tify defining features of more fine-grained levels
of poetic skill, future work could compare award-
winning poetry with poems written by less presti-
gious but also professionally trained poets. Exper-
imenting with different databases and lexicons for
affect and imagery could also be helpful, such as
word-emotion associations (Mohammad & Turney,
2011) and imageability ratings (Coltheart, 1981). In
addition, more sophisticated methods that consider
sense ambiguities and meaning compositionality in
affective words (Socher et al, 2011) should be ap-
plied to help enhance and improve upon our current
analyses.
While our approach reveals interesting patterns
that shed light on elements of poetic sophistication,
conclusions from the analysis need to be tested us-
ing controlled experiments. For example, does mod-
ifying a professional poem to include less concrete
words make people perceive it as less beautiful? In-
vestigating these questions using psychology exper-
iments could help identify causal relationships be-
tween linguistic elements and sensations of poetic
beauty.
In summary, our framework provides a novel way
to discover potential features of poetic beauty that
can then be experimentally tested and confirmed. By
applying both stylistic and content analyses to the
quantitative assessment of contemporary poetry, we
were able to examine poetic craft on a representative
set of poems and reveal potential elements of skill
and sophistication in modern poetry.
15
Acknowledgments
We are deeply grateful for David Kaplan?s generos-
ity in sharing the code for the PoetryAnalyzer pro-
gram, on which a substantial part of our analysis is
based. We would also like to thank Lera Boroditsky,
Todd Davies, and the anonymous reviewers for their
extremely helpful feedback.
References
Addonizio, K., & Laux, D. (1997). The Poet?s Com-
panion: A guide to the pleasures of writing po-
etry. W. W. Norton and Company.
Aristotle (1998). Poetics. The Critical Tradition:
Classical Texts and Contemporary Trends.
Ben-Simon, A., & Bennett, R. E. (2007). Toward
more substantively meaningful automated essay
scoring. The journal of Technology, Learning, and
Assessment.
Birkhoff, G. (1933). Aesthetic Measure. Kessinger
Publishing.
Bower, G. (1970). Imagery as a relational organizer
in associative learning. Journal of Verbal Learn-
ing and Verbal Behavior, 9(5), 529?533.
Breland, H. M. (1996). Word frequency and word
difficulty: A comparison of counts in four cor-
pora. Psychological Science, 7(2), pp. 96?99.
Bryant, P., Maclean, M., Bradley, L., & Crossland,
J. (1990). Rhyme and alliteration, phoneme de-
tection, and learning to read. Developmental Psy-
chology, 26(3).
Burroway, J. (2007). Imaginative Writing: The Ele-
ments of Craft. Pearson, 2 ed.
Coltheart, M. (1981). The mrc psycholinguistic
database. The Quarterly Journal of Experimen-
tal Psychology, 33(4), 497?505.
Davies, M. (2011). Word frequency data
from the Corpus of Contemporary Ameri-
can English (COCA). Downloaded from
http://www.wordfrequency.info on May 10, 2011.
Earnshaw, S. (Ed.) (2007). The Handbook of Cre-
ative Writing. Edinburgh University Press.
Fabb, N. (2006). Generated metrical form and im-
plied metrical form. Formal approaches to poetry,
(pp. 77?91).
Fang, A. C., Lo, F., & Chinn, C. K. (2009). Adapt-
ing nlp and corpus analysis techniques to struc-
tured imagery analysis in classical chinese poetry.
In Proceedings of the Workshop on Adaptation of
Language Resources and Technology to New Do-
mains, AdaptLRTtoND ?09, (pp. 27?34).
Forstall, C., Jacobson, S., & Scheirer, W. (2011).
Evidence of intertextuality: investigating paul the
deacon?s angustae vitae. Literary and Linguistic
Computing, 26(3), 285?296.
Genzel, D., Uszkoreit, J., & Och, F. (2010). Po-
etic statistical machine translation: rhyme and
meter. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, (pp. 158?166). Association for Compu-
tational Linguistics.
Greene, E., Bodrumlu, T., & Knight, K. (2010). Au-
tomatic analysis of rhythmic poetry with applica-
tions to generation and translation. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP
?10, (pp. 524?533).
Harrower, M. (1972). The therapy of poetry. Oryx,
London.
Hayward, M. (1996). Analysis of a corpus of poetry
by a connectionist model of poetic meter. Poetics,
24(1), 1?11.
He, Z., Liang, W., Li, L., & Tian, Y. (2007). Svm-
based classification method for poetry style. In
Machine Learning and Cybernetics, 2007 Inter-
national Conference on, vol. 5, (pp. 2936?2940).
IEEE.
Inquirer, H. G. (2011). How the general inquirer is
used and a comparison of general inquirer with
other text-analysis procedures.
Jakobson, R. (1960). Closing statement: Linguistics
and poetics. Style in language, 350, 377.
Jessen, F., Heun, R., Erb, M., Granath, D. O., Klose,
U., Papassotiropoulos, A., & Grodd, W. (2000).
The concreteness effect: Evidence for dual cod-
ing and context availability. Brain and Language,
74(1), 103 ? 112.
Kaplan, D. (2006). Computational analysis and vi-
sualized comparison of style in american poetry.
Unpublished undergraduate thesis.
16
Kaplan, D., & Blei, D. (2007). A computational ap-
proach to style in american poetry. In IEEE Con-
ference on Data Mining.
Lea, R., Rapp, D., Elfenbein, A., Mitchel, A., &
Romine, R. (2008). Sweet silent thought: Allit-
eration and resonance in poetry comprehension.
Psychological Science, 19(709).
Longinus (2001). On sublimity. The Norton Anthol-
ogy of Theory and Criticism.
Marks, Carolyn B., Doctorow, Marleen J., & Wit-
trock, M. C. (1974). Word frequency and reading
comprehension. The Journal of Educational Re-
search, 67(6), 259?262.
McGlone, M., & Tofighbakhsh, J. (2000). Birds of
a feather flock conjointly (?): Rhyme as reason in
aphorisms. Psychological Science, 11, 424?428.
Mohammad, S., & Turney, P. (2011). Crowdsourc-
ing a word?emotion association lexicon. Compu-
tational Intelligence, 59(000), 1?24.
Paivio, A., Yuille, J., & Smythe, P. (1966). Stimu-
lus and response abstractness, imagery, and mean-
ingfulness, and reported mediators in paired-
asscoiate learning. Canadian Journal of Psychol-
ogy, 20(4).
Pennebaker, J., Francis, M., & Booth, R. J. (2001).
Linguistic Inquiry and Word Count (LIWC):
LIWC2001. Mahwah, NJ: Erlbaum.
Pitler, E., & Nenkova, A. (2008). Revisiting read-
ability: A unified framework for predicting text
quality. In Empirical Methods in Natural Lan-
guage Processing, (pp. 186?195).
Poulin, A., & Waters, M. (2006). Contemporary
American Poetry. Houghton Mifflin Company,
eighth ed.
Richards, I. (1893). Practical criticism: a study of
literary judgment. Transaction Publishers.
Rigau, J., Feixas, M., & Sbert, M. (2008). Infor-
mational aesthetics measures. In IEEE Computer
Graphics and Applications.
Rubin, D. (1995). Memory in oral traditions:
The cognitive psychology of epic, ballads, and
counting-out rhymes. New York: Oxford Univer-
sity Press.
Silverman, M., & Will, N. (1986). Sylvia plath and
the failure of emotional self-repair through poetry.
Psychoanal Q, 55, 99?129.
Socher, R., Pennington, J., Huang, E., Ng, A., &
Manning, C. (2011). Semi-supervised recursive
autoencoders for predicting sentiment distribu-
tions. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Process-
ing, (pp. 151?161). Association for Computa-
tional Linguistics.
Stirman, S. W., & Pennebaker, J. (2001). Word use
in the poetry of suicidal and nonsuicidal poets.
Psychosomatic Medicine, 63(4), 517?22.
Stone, P., Dunphry, D., Smith, M., & Ogilvie, D.
(1966). The General Inquirer: A Computer Ap-
proach to Content Analysis. Cambridge, MA:
MIT Press.
17
Workshop on Computational Linguistics for Literature, pages 18?25,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Towards a Literary Machine Translation:
The Role of Referential Cohesion
Rob Voigt Dan Jurafsky
Center for East Asian Studies Department of Linguistics
Stanford University Stanford University
robvoigt@stanford.edu jurafsky@stanford.edu
Abstract
What is the role of textual features above the 
sentence  level  in  advancing  the  machine 
translation of literature? This paper examines 
how  referential  cohesion  is  expressed  in 
literary  and  non-literary  texts  and  how  this 
cohesion affects translation. We first show in a 
corpus study on English that literary texts use 
more dense reference chains to express greater 
referential  cohesion  than  news.  We  then 
compare the referential  cohesion of machine 
versus  human  translations  of  Chinese 
literature and news. While human translators 
capture  the  greater  referential  cohesion  of 
literature,  Google  translations  perform  less 
well at capturing literary cohesion. Our results 
suggest  that  incorporating discourse  features 
above  the  sentence  level  is  an  important 
direction for MT research if it is to be applied 
to literature.
Introduction
The concept of literary machine translation 
might  seem at  first  to  be a  near-contradiction in 
terms.  The  field  of  machine  translation  has 
traditionally aimed its sights at  the translation of 
technical or otherwise informative texts,  with the 
strongest focus on newswire and other informative 
texts relevant to the goals of government funders.
Nevertheless, the prospect of literary MT is 
appealing. Human translation of literary texts is an 
extremely time- and money-intensive task, but one 
that  is  a crucial  element of the global  system of 
transcultural  literary exchange.  From a  technical 
standpoint, since ?by definition, literature is the art 
that  uses  language?  (Chapman  1973),  literary 
translation  represents  perhaps  the  strongest 
formulation  of  the  machine  translation  problem. 
Jonathan  Slocum,  writing  in  1985,  essentially 
rejects  the  idea of  literary MT altogether,  noting 
that  it  is  serendipitous  for  technical  MT  that 
emphasis  is  placed  on  semantic  fidelity  to  the 
source text, whereas literary translation must take 
into  account  larger  considerations  such  as  style 
with which ?computers do not  fare well.?  Given 
the explosion of statistical  methodologies in MT, 
are we now at a point where we can hope to begin 
tackling some of the  questions associated with a 
potential literary machine translation?
This  problem  is  severely  understudied. 
Regardless of the plausibility (or even desirability) 
of  eventually  using  MT to  produce  full-fledged 
translations  of  literary  texts,  a  serious 
consideration  of  the  unique  difficulties  posed  by 
literary translation may well serve to push forward 
our computational understanding of literature and 
the language of translation.
In particular,  literary translation seems to 
demand  that  we  address  larger-scale  textual 
features  beyond  the  sentence-level  approach 
commonly  employed  by  contemporary  MT 
systems.  There  is  a  substantial  body of  work by 
scholars  in  the  field  of  translation  studies 
addressing  greater-than-sentence-level  textual 
features  from a  linguistic  and  literary-theoretical 
perspective,  and  this  existing  work  can  offer 
conceptual understanding and a parallel vocabulary 
with  which  to  discuss  progress  in  this  regard  in 
machine translation. 
Eugene Nida (1964), for example, used the 
terms  ?formal  equivalence?  and  ?dynamic 
equivalence? to  differentiate  between translations 
aiming to  replicate  the  form of  their  source  and 
those aiming to replicate the source text's effects 
on its  readers.  Hatim and Mason (1995)  brought 
the  ?seven  standards  of  textuality?  set  forth  by 
Beaugrande  and  Dressler  (1981)  into  the 
translation studies context as metrics for evaluating 
the  ?expectation-fulfilling?  or  ?expectation-
defying? outcome of a translated text. 
18
Cohesion  is  defined  by  Beaugrande  and 
Dressler  as  ?concern[ing]  the  ways  in  which the 
components  of  the  textual  world,  i.e.,  the 
configuration  of  concepts  and  relations  which 
underlie the surface text,  are mutually accessible 
and  relevant."  Cohesion  considers  the  limited 
human capacity for storing the ?surface materials? 
of a text long enough to relate them semantically 
during the act of reading.
We therefore  propose to  study referential 
cohesion (Halliday and Hasan 1976), the relation 
between co-referring entities in a narrative, as an 
important  component  of  cohesion.   Referential 
cohesion  has  a  significant  literature  in  natural 
language processing (Grosz et al 1995, Mani et al 
1998, Marcu 2000, Karamanis et al 2004, Kibble 
and  Power  2004,  Elsner  and  Charniak  2008, 
Barzilay  and  Lapata  2008,  inter  alia)  as  does 
automatic  coreference  resolution,  which  has 
significantly increased in accuracy in recent years 
(Bengston  and  Roth  2008,  Haghighi  and  Klein 
2009, Haghighi and Klein 2010, Rahman and Ng 
2011, Pradhan et al 2011, Lee et al 2011).
We formulate  and test  two hypotheses in 
this position paper: First, we anticipate that given 
stylistic  considerations  and  their  fundamental 
narrative  function,  prose  literary  texts  are 
inherently ?more cohesive? than news. Second, in 
light of the aforementioned necessity for ?dynamic 
equivalence?  in  the  literary  translation,  we 
anticipate that current machine translation systems, 
built  with  newswire  texts  in  mind,  will  be  less 
successful at conveying cohesion for literary texts 
than for news.
2. Investigating Literary Cohesion
Our first preliminary experiment examines 
how  referential  cohesion  in  literary  texts  differs 
from  news  text  by  examining  coreference  in  a 
monolingual  English-language  corpus,  without 
considering machine-translated texts.
We created a small corpus of twelve short 
stories  for  comparison  with  twelve  recent  long-
form news stories from the New York Times, Wall 
Street Journal, The Atlantic, and the news blog The 
Daily Beast. The stories chosen were written by a 
variety  of  authors:  Isaac  Asimov,  J.D.  Salinger, 
Edgar Allen Poe, Tobias Wolff, Vladimir Nabokov, 
Sir  Arthur  Conan  Doyle,  Shirley  Jackson,  Jack 
London,  Mark  Twain,  Willa  Cather,  Ambrose 
Bierce,  and  Stephen  Crane  ?  in  the  interest  of 
avoiding over-specificity to any particular genre or 
style.  The  corpus  thus  included  12  short  stories 
with  76,260  words  and  12  news  articles  with 
23,490  words,  for  a  total  corpus  size  of  24 
documents and 99,750 words.
We used standard publicly-available  NLP 
tools to process the corpus. We used the Stanford 
CoreNLP suite1 to tokenize and sentence-split both 
the human and MT versions of each text and then 
to run the multi-pass sieve coreference resolution 
system described in Lee et al (2011). 
This  system  works  by  making  multiple 
passes  over  the  text,  first  doing  recall-oriented 
mention  extraction,  then  resolving  coreference 
through a series of sieves moving from highest to 
lowest  precision.  This  system  is  state-of-the-art, 
with a B3 F1 score of 68.9 with no gold mention 
boundaries  on  the  CoNLL 2011  shared  task  test 
set.  Nevertheless,  it  is  likely  to  introduce  some 
measure of noise into our results.
For the rest of the paper we use the term 
?cluster? to refer to clusters agglomerated by the 
system  that  co-refer  to  the  same  entity,  and 
?mention? to refer to individual instances of each 
entity in the text.
Clusters per 
100 Tokens
Mentions per 
100 Tokens
Density:
Mentions 
per Cluster
Short 
Stories
3.6 19.3 5.4
News 
Text
3.9 15.0 3.9
Table  1.  Cohesion  as  measured  by  coreference  in 
literary  vs.  non-literary  texts.  Figures  given  are  the 
overall average across all documents.
Table 1 reports the numbers of clusters and 
mentions (normalized per 100 tokens). The literary 
texts had the same number of clusters (entities) as 
the news texts (one-tailed t-test,  p = 0.080), albeit 
with a  trend towards fewer  clusters  in  literature. 
But  literary text had more mentions (p < 0.001), 
and a higher number of mentions per cluster (p < 
0.001) than the news texts. 
The  results  of  this  preliminary  study 
suggest that the literary text tended to discuss the 
same number of entities as the non-fiction, but to 
1 Available online at 
nlp.stanford.edu/software/corenlp.shtml
19
Suddenly,  the nurse resorted to direct measures.  She 
seized  the boy?s upper arm in one  hand and dipped 
the other in the milk. She dashed the milk across his 
lips,  so  that  it dripped  down  cheeks  and  receding 
chin.
...
Always,  his frightened eyes were on  her, watching, 
watching for the one false move.  She found herself 
soothing  him, trying to move  her hand very slowly 
toward  his hair, letting  him see  it every inch of the 
way, see there was no harm in it. And she succeeded 
in stroking his hair for an instant.
?
Instead, she turned on the night light and moved the 
bed. The poor thing was huddled in the corner, knees 
up against  his chin,  looking up at  her with  blurred 
and apprehensive eyes.
?
She looked down at those eager brown eyes turned up 
to hers and passed her hands softly through his thick, 
curly hair. 
Figure  1.  Human  markup  of  cohesion  throughout 
Asimov's ?The Ugly Little Boy.? Recurring entities are 
color-coded: red is the character Edith Fellowes, grey is 
her hands, blue is the character Timmie, light green is 
his eyes, dark green is his chin, yellow is his hair, and 
magenta is  the milk.  This sample contains  149 words 
and 7 recurring entities with a total of 29 mentions.
mention each entity more often.  In other words, 
literary text uses more dense reference chains as a 
way of creating a higher level of cohesion. 
Figures  1  and  2  provide  representative 
examples, hand-labeled for coreference, to offer a 
qualitative intuition for this difference in cohesion. 
In the literary example in Figure 1 we find seven 
recurring entities with an average of 4.1 mentions 
each.  In  the  news  example  in  Figure  2  we  find 
seven  recurring  entities  but  only  3.0  average 
mentions,  resulting  in  qualitatively  less  dense 
reference chains in the news sample.
Our  results  are  consistent  with  Biber 
(1988),  whose  factor  analysis  study  found  that 
fiction tended to have a high frequency of third-
person  personal  pronouns.  This  is  true  in  our 
corpus;  third-person pronouns occur  57.7% more 
in the fiction as opposed to the non-fiction texts 
(16.9  vs  10.7  occurrences  per  100  words).  But 
even  when  we  count  ignoring  third-person 
pronouns, we found a greater density of mentions 
per cluster for literature than for news (4.0 vs 3.3, 
p = 0.015). The result that literature seems to have 
more to say about each entity thus extends and
Two studies have found that  weight-loss  operations 
worked much better than  the standard therapies for 
Type  2  diabetes in  obese  and  overweight  people 
whose blood sugar was out of control. Those who had 
surgery, which stapled the  stomach and rerouted the 
small  intestine,  were  much  more  likely  to  have  a 
complete  remission  of  diabetes,  or  to  need  less 
medicine,  than  people who  were  given  the  typical 
regimen of drugs, diet and exercise.
...
The new studies, published on Monday by The New 
England  Journal  of  Medicine,  are  the  first  to 
rigorously  compare  medical  treatment with  these 
particular  stomach and intestinal  operations as ways 
to  control  diabetes.  Doctors had  been  noticing  for 
years that weight-loss operations, also called bariatric 
surgery, could sometimes get rid of Type 2  diabetes. 
But they had no hard data.
...
One  of  the  studies,  conducted  at  the  Catholic 
University in Rome, compared two types of  surgery 
with usual medical treatment.
Figure 2. Human markup of cohesion throughout a NYT 
news article. Recurring entities are color-coded, similar 
to  the  above.  This  sample  contains  152 words  and  7 
recurring entities with a total of 21 mentions.
explains  Biber's  finding  that  literature  has  more 
third-person pronouns.
While  our  results  are  suggestive,  they 
remain  preliminary.   A more  detailed  follow-up 
will need to look at the specific realization of the 
mentions and the kind of local coherence relations 
that  link them (Althaus et al  2004,  Poesio et  al. 
2004,  Barzilay  and  Lapata  2008,  Elsner  and 
Charniak  2008),  and  to  investigate  the  different 
aspects  of  referential  chains  with  larger  corpora 
and more varying genres.
3. MT Success at Conveying Cohesion
To evaluate the impact of this difference in 
expressed  cohesion  on  machine  translation 
systems, we compared coreference output between 
human  and  machine  translations  of  literary  and 
informative texts from Chinese.  For this task we 
chose  a  small  dataset  of  sixteen  short  stories  in 
Chinese by the early 20th-century author Lu Xun 
(??) and their corresponding English translations 
by  Gladys  Yang.  We  chose  Lu  Xun  for  his 
prominence  as  the  ?father  of  modern  Chinese 
literature? and vernacular style, and because Yang's 
English translations are widely accepted as being 
20
of  high  quality  by  the  literary  community.  For 
comparison to news text, we chose a series of six 
long-form  articles  from  the  magazine  Sinorama 
and  their  corresponding  English  reference 
translations in the  LDC's ?Chinese English News 
Magazine  Parallel  Text?  corpus  (LDC2005T10). 
These  magazine  texts  were  chosen  because  the 
brief newswire texts often used in MT evaluation 
are too short to allow for meaningful textual-level 
comparisons  of  this  sort.  Thus  our  corpus 
contained  16  human-translated  short  stories  with 
90,712 words, 16 machine-translated short stories 
with 82,475 words, 6 human-translated magazine 
articles  with  45,310  words,  and  6  machine-
translated magazine articles with 39,743 words, for 
a total size of 44 documents and 258,240 words.
We  used  Google  Translate  as  our  MT 
translation  engine,  first  because  the  large  web-
based resources behind that system might help to 
mitigate  the  inevitable  complication  of  domain 
specificity in the training data, and second because 
of  its  social  position  internationally  as  the  most 
likely  way  average  readers  might  encounter 
machine translation. 
We first used Google Translate to produce 
machine  translations  of  both  the  literary  and 
magazine texts, and then used the Lee et al (2011) 
coreference  system  in  Stanford  CoreNLP  as 
described above to evaluate cohesion on both the 
human  and  machine  English  translations.  As 
acknowledged  in  the  prior  section,  automatic 
coreference is likely to introduce some amount of 
noise, but there is no reason to think that this noise 
would be biased in any particular direction for MT.
Results  from the  coreference  analysis  of 
the literary and magazine texts are shown in Table 
2.  The results  in  the  two rows labeled ?Human? 
substantiate our findings from the previous section. 
The human translations of the short stories have a 
significantly (p  =  0.003)  higher  referential  chain 
density  (5.2)  than  the  human  translations  of  the 
magazine  pieces  (4.2).  Translators,  or  at  least 
Gladys  Yang  in  these  translations,  seem  to  act 
similarly to  source-text  writers  in  creating  more 
dense referential  chains in literature than in non-
fiction genres.
In order to study the success of machine 
translation in dealing with cohesion, we took the 
human translations as a gold standard in each case, 
using this translation to normalize the number of 
clusters and mentions to the length of the reference
Clusters per
100 Tokens
Mentions per 
100 Tokens
Density:
Mentions 
per Cluster
Short Story
   Human 3.7 19.0 5.2
   Machine 4.1 16.4 3.8
Magazine
   Human 3.9 16.0 4.2
   Machine 3.9 14.0 3.7
Table 2. Cohesion as measured by coreference in human 
and machine translations of  Lu Xun short  stories  and 
Sinorama magazine articles. The first two columns are 
normalized  to  the  length  of  the  human  ?gold? 
translations,  and figures given are the overall  average 
across all documents.
documents to address the length variance caused 
by the MT system.
The  results  in  Table  2  show  little 
underclustering for the MT output.  The number of 
clusters (entities) in the machine translations (4.1 
and 3.9) do not differ from the human translations 
(3.7 and 3.9), (p = 0.074), although there is a trend 
toward underclustering for literature.
The main difference we see is in referential 
chain  density  (mentions  per  cluster).  Whereas 
these  experiments  reconfirm  the  trend  towards 
more  mentions  per  cluster  in  literature  than 
informative  text,  referential  chains  in  the  MT 
output do not differ between the two genres. The 
machine translation only captures 79.4% (13,846 
vs.  17,438)  of  the  human-translated  mentions  in 
the literary texts.
In  the  literary  genre  the  automatic 
coreference system finds more than one additional 
mention per  cluster  in  the  human translations  as 
compared  to  MT  (p  <  0.001),  while  in  the 
magazine case the human and MT translations are 
the same, though there is a similar trend towards 
less  dense  referential  chains  in  MT output  (p  = 
0.055).
4. Examples and Discussion
It  is  worth  first  acknowledging  the 
somewhat  surprising  ability  of  MT  to  maintain 
cohesion in both domains. The fact that a system 
operating  almost  exclusively  on  a  sentence-by-
sentence basis is able to maintain upwards of three-
quarters  of  the  mentions  in  the  difficult  and 
linguistically distant context of Chinese-to-English 
21
MT is remarkable in and of itself, and speaks to the 
relative success of modern MT. There is, of course, 
no  guarantee  that  these  mentions  found  by  the 
coreference system are in fact all the correct ones, 
so the true figure is likely somewhat lower, but a 
qualitative  examination  of  the  system's  output 
shows that they are largely accurate.
What is actually causing the discrepancies 
in  cohesion  noted  above  as  regards  our  two 
domains? Below we look at some specific cases of 
reduced cohesion in our results from the Lu Xun 
story ?Flight to the Moon.? In these examples the 
human  translator  was  forced  to  rely  on  greater-
than-sentence-level features of the text to effect an 
appropriately  cohesive  translation  that  the  MT 
system was unable to convey.
Zero Anaphora
Zero  anaphora  is  a  well-documented  and 
common linguistic phenomena in Chinese (Li and 
Thompson  1979,  Huang  1989).  Kim  (2000) 
investigated subject drop in Chinese and English, 
finding  that  English  overtly specifies  subjects  in 
96% of cases, while the figure for Chinese is only 
64%, and a significant amount of prior work has 
focused  on  the  computational  identification  and 
resolution  of  zero  anaphora in  Chinese (see  Yeh 
and  Chen  2001,  Converse  2006,  Zhao  and  Ng 
2007,  Kong  and  Zhou  2010).  The  following 
example sentences demonstrate this difficulty. 
Human Translation
    When the big game was finished  they ate 
wild boars, rabbits and pheasants. He was such a fine 
archer, he could shoot as much as he pleased.
Machine Translation
        Later large animal shot down, ate wild boar, 
rabbit pheasant; shooting method and high strength, 
many as you want.
Original Chinese
        ?????????????????
????????????
Figure 3. Reduced cohesion via zero anaphora in MT 
output. Relevant mentions are hand-annotated in bold.
In  a  qualitative  analysis  of  our  results, 
problems  such  as  these  were  by  far  the  most 
common  cause  of  cohesion  errors,  and  as  the 
reader will notice, they often lead to an output that 
loses crucial elements for maintaining the cohesion 
of the narrative, such as in this case the distinction 
between the husband/wife couple, ?they,? and the 
husband individually, ?he.? 
Inconsistent Reference
Having  no  process  for  maintaining 
consistency of reference to entities in the narrative, 
the  following  non-consecutive  coreferencing 
sentences illustrate how in the MT version of the 
text  the  cohesiveness  of  the  ?hen? cluster  in  the 
original is lost.
Human Translation
-"Who are you? Why have you shot my best black 
laying hen?"
-"What! A hen?" he echoed nervously. "I thought  it 
was a wood pigeon."
-"Imagine mistaking a hen for a wood pigeon!"
-"I am Yi." While saying this he saw that his arrow 
had pierced the hen's heart, killing it outright.
-"What about this hen?"
-"She was my best: she laid me an egg every day."
-"I'll give you these for your hen"
Machine Translation
-"Who are you what? How good black  hen shot to 
the top of my house?"
-"Ah!  Chicken?  I  only  said  a  wood  pigeon 
partridge," he said in dismay.
-"hens do  not  know,  will  be  treated  as  the  wood 
pigeon partridge"
-"I Yi Yi." He said, to see his shot arrows, is being 
consistently the heart of the hen, of course, died
-"Chicken how to do it?"
-"Lost my best hen every day to lay eggs."
-"they brought lost your chicken."
Original Chinese
-????????????????????
??"
-??????????????? ?? ??????
 
-"??????????????"
-? ??????? ??????????????
????????????
-??? ? ?????
-????????????????"
-"????????"
Figure 4. Reduced cohesion via inconsistent reference in 
MT output.  Relevant  mentions  are  hand-annotated  in 
bold.
The reader will notice that in the original 
Chinese,  ji (? ,  lit.  ?chicken?) is used here as a 
22
shortened  version  of  muji (?? ,  lit.  ?hen?)  in 
colloquial  speech,  which  the  human  translator 
clearly  notes  and  translates  each  mention 
consistently to maintain cohesion. Similarly, being 
that  number is  not  explicitly marked in  Chinese, 
the MT system translates  lian muji (??? ,  lit. 
?even hen?) as ?hens? instead of catching that here 
 ?? refers back to the entity being discussed.
De (?) Drops
It is common in Chinese for the noun head 
of a nominalization formed by the particle de (?) 
to  be  implicit,  yet  in  many  cases  the  human 
translator will add it for clarity and, presumably, to 
maintain cohesion.
Human Translation
"There are those who know my name."
      
Machine Translation
?Some people is one to know."
Original Chinese
?                                       ? ? ? ? ? ? ? ?? ??" 
Exist  some  people be  one  hear  then    know  NOM
Figure  5.  Reduced  cohesion  via  de dropping  in  MT 
output. Relevant mentions are hand-annotated in bold.
This  phenomenon  reminds  of  translation 
theorist  Mona  Baker's  (1996)  concept  of 
?explicitation?: ?an overall tendency to spell things 
out rather than leave them implicit in translation.? 
Indeed, Olohan and Baker (2000) demonstrate this 
empirically using the Translational English Corpus, 
finding  a  strong  tendency  in  translated  texts  to 
explicitly  mark  the  ?that?-connective  following 
words such as ?say,? ?tell,? ?promise,? and so on 
where it could have been omitted. 
5. Implications and Future Research
We  found  in  two  separate  analyses that 
literary texts had more dense reference chains than 
informative  texts.  This  result  supports  our 
hypothesis  that  literary  texts  are  indeed  more 
cohesive in general than informative texts; that is 
to  say,  the  stylistic  and  narrative  demands  of 
literature  lead  to  prose  being  more  cohesively 
?about?  its  subjects  than  news.  It  remains  to 
replicate  this  experiment  on  a  large,  carefully 
sampled  cross-genre  corpus  to  confirm  these 
preliminary findings,  perhaps  integrating  a  more 
complex measure of cohesion as in Barzilay and 
Lapata (2008).
We  also  found  that  MT  systems  had 
difficulty  in  conveying  the  cohesion  in  literary 
texts. Of course these results are preliminary and 
may be confounded by the nature of the training 
data  used  by  modern  MT systems.  The  uses  of 
Google  Translate  as  an  MT system and  longer-
form  magazine  articles  as  our  informative  texts 
were aimed at mitigating these concerns to some 
extent, but for now these results primarily serve as 
indicative of the need for further research in this 
area.
Cohesion, as well, is only one of the seven 
?standards of textuality? put forth by Beaugrande 
and Dressler  (1981)  and taken up by Hatim and 
Mason (1997) in the translation context. Some of 
these  have  an  existing  literature  addressing  their 
computational  identification  and  analysis  (eg. 
Morris and Hirst 1991), in which cases we might 
apply existing methods to identify genre effects in 
literary text.  For  others,  such  as  situationality,  it 
remains  to  investigate  appropriate  computational 
analogues  for  large-scale  automatic  analysis  and 
application  to  literary  text.  Studies  addressing 
relevant  textual-level  concerns  in  literature  show 
increasing promise,  such as  Elson et  al.  (2010)'s 
work  in  automatically extracting  social  networks 
from fiction.
Once  these  sorts  of  genre  effects  in 
literature are more clearly understood, they can be 
addressed  on  a  large  scale  for  comparisons 
between  machine-  and  human-translated  literary 
texts  in  the  manner  carried  out  in  this  paper,  in 
order to identify further potential stumbling blocks 
for  machine  translation  on  the  textual  level  as 
regards  literary  texts.  Our  preliminary  work  as 
presented  here  suggests,  at  the  very  least,  the 
potential value and necessity of such analyses if we 
are  to  make  progress  towards  a  true  literary 
machine translation.
    Acknowledgements
Thanks  to  Heeyoung  Lee  for  help  with  the 
coreference  system,  three  anonymous  reviewers  for  their 
careful  reading  and  helpful  comments,  and  the  U.S. 
Department of Education for the Foreign Language and Area 
Studies grant that helped fund this research.
23
    References
Althaus,  Ernst,  Nikiforos  Karamanis,  and 
Alexander  Koller.  2004.  Computing 
locallycoherent discourses. In ACL.
Baker,  Mona.  1996.  Corpus-based  translation 
studies:  The  challenges  that  lie  ahead.  In 
Terminology, LSP and Translation: Studies in 
language  engineering. John  Benjamins, 
Amsterdam.
Barzilay,  Regina  and  Mirella  Lapata.  2008.
Modeling  Local  Coherence:  An  Entity-based 
Approach. Computational Linguistics, 34(1).
Beaugrande, Robert and Wolfgang Dressler. 1981. 
Introduction  to  Text  Linguistics.  Longman, 
London.
Bengston, E. and Dan Roth. 2008. Understanding 
the value of features for coreference resolution. 
In EMNLP.
Biber, Douglas. 1988. Variation across speech and 
writing. Cambridge  University  Press, 
Cambridge.
Chapman,  Raymond.  1973.  Linguistics  and 
Literature. Edward Arnold, London.
Converse,  Susan.  2006.  Pronominal  anaphora 
resolution for Chinese. Ph.D. thesis.
Elsner,  Micha  and  Eugene  Charniak.  2008. 
Coreference-inspired  Coherence  Modeling.  In 
Proceedings of ACL 2008. 
Elson,  David,  Nicholas  Dames,  and  Kathleen 
McKeown.  2010.  Extracting  social  networks 
from literary fiction. In ACL.
Grosz,  Barbara,  Aravind  K.  Joshi,  and  Scott 
Weinstein.  1995.  Centering:  A framework  for 
modeling  the  local  coherence  of  discourse. 
Computational Linguistics, 21(2).
Haghighi,  Aria  and  Dan  Klein.  2009.  Simple 
coreference  resolution  with  rich  syntactic  and 
semantic features. In EMNLP.
Haghighi, Aria and Dan Klein. 2010. Coreference 
resolution in a modular, entity-centered model. 
In HLT-NAACL.
Halliday,  M.  A.  K.  and  Ruqaiya  Hasan.  1976. 
Cohesion in English. Longman, London.
Hatim, Basil and Ian Mason. 1997. The Translator 
as Communicator. Routledge, London.
Huang, James C.-T. 1989. Pro drop in Chinese, a 
generalized control approach. In O, Jaeggli and 
K. Safir,  editors,  The Null  Subject  Parameter. 
D. Reidel Dordrecht.
Karamanis,  Nikiforos,  Massimo  Poesio,  Chris 
Mellish, and Jon Oberlander. 2004. Evaluating 
centering-based  metrics  of  coherence  for  text 
structuring using a reliably annotated corpus. In 
ACL.
Kibble,  Rodger  and  Richard  Power.  2004. 
Optimizing  Referential  Coherence  in  Text 
Generation. Computational Linguistics 30(4).
Kim, Young-Joo. 2000. Subject/object drop in the 
acquisition  of  Korean:  A  cross-linguistic 
comparison.  Journal of East Asian Linguistics, 
9(4).
Kong,  Fang  and  Guodong  Zhou,  2010.  A Tree 
Kernel-based  Unified  Framework  for  Chinese 
Zero Anaphora Resolution. In EMNLP.
Lee,  Heeyoung,  Yves  Peirsman,  Angel  Chang, 
Nathanael  Chambers,  Mihai  Surdeanu,  Dan 
Jurafsky.  Stanford's  Multi-Pass  Sieve 
Coreference Resolution System at the CoNLL-
2011 Shared Task. 2011.  In Proceedings of the  
CoNLL-2011 Shared Task.
Li,  Charles  and Sandra Thompson.  1979.  Third-
person pronouns and zero-anaphora in Chinese 
discourse. Syntax and Semantics, 12:311-335.
Ma,  Xiaoyi.  2005.  Chinese  English  News 
Magazine Parallel Text. LDC2005T10.
Mani, Inderjeet, Barbara Gates, and Eric Bloedorn. 
1998.  Using Cohesion and Coherence Models 
for Text Summarization. In AAAI.
Marcu, Daniel. 2000. The Theory and Practice of  
Discourse  Parsing  and  Summarization.  MIT 
Press, Cambridge, MA.
Morris,  Jane  and  Graeme  Hirst.  1991.  Lexical 
Cohesion Computed by Thesaural Relations as 
an  Indicator  of  the  Structure  of  Text. 
Computational Linguistics, 17(1).
Nida,  Eugene.  1964.  Towards  a  Science  of  
Translating. Brill, Leiden.
Olohan, Maeve and Mona Baker. 2000. Reporting 
that in  translated  English:  Evidence  for 
subconscious processes of explicitation? Across  
Languages and Cultures 1.
Poesio, Massimo, Rosemary Stevenson, Barbara di 
Eugenio, and Janet Hitzeman, 2004. Centering: 
A  Parametric  theory  and  its  instantiations. 
Computational Linguistics, 30(3).
Pradhan,  Sameer,  Lance  Ramshaw,  Mitchell 
Marcus, Martha Palmer, Ralph Weischedel, and 
Nianwen Xue. 2011. CoNLL-2011 Shared Task: 
Modeling  Unrestricted  Coreference  in 
OntoNotes. In CoNLL.
24
Rahman, Altaf and Vincent Ng. 2011. Coreference 
resolution with world knowledge. In ACL.
Slocum,  Jonathan.  1985.  A Survey  of  Machine 
Translation:  its  History,  Current  Status,  and 
Future  Prospects.  Computational  Linguistics, 
11(1).
Zhao,  Shanheng  and  Hwee  Tou  Ng.  2007. 
Identification and Resolution of Chinese Zero 
Pronouns:  A Machine  Learning  Approach.  In 
Proceedings  of  EMNLP  CoNLL  Joint  
Conference.
25
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 13?21,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Towards a Computational History of the ACL: 1980?2008
Ashton Anderson
Stanford University
ashtona@stanford.edu
Dan McFarland
Stanford University
dmcfarla@stanford.edu
Dan Jurafsky
Stanford University
jurafsky@stanford.edu
Abstract
We develop a people-centered computational
history of science that tracks authors over top-
ics and apply it to the history of computa-
tional linguistics. We present four findings
in this paper. First, we identify the topical
subfields authors work on by assigning auto-
matically generated topics to each paper in the
ACL Anthology from 1980 to 2008. Next, we
identify four distinct research epochs where
the pattern of topical overlaps are stable and
different from other eras: an early NLP pe-
riod from 1980 to 1988, the period of US
government-sponsored MUC and ATIS eval-
uations from 1989 to 1994, a transitory period
until 2001, and a modern integration period
from 2002 onwards. Third, we analyze the
flow of authors across topics to discern how
some subfields flow into the next, forming dif-
ferent stages of ACL research. We find that the
government-sponsored bakeoffs brought new
researchers to the field, and bridged early top-
ics to modern probabilistic approaches. Last,
we identify steep increases in author retention
during the bakeoff era and the modern era,
suggesting two points at which the field be-
came more integrated.
1 Introduction
The rise of vast on-line collections of scholarly pa-
pers has made it possible to develop a computational
history of science. Methods from natural language
processing and other areas of computer science can
be naturally applied to study the ways a field and
its ideas develop and expand (Au Yeung and Jatowt,
2011; Gerrish and Blei, 2010; Tu et al, 2010; Aris et
al., 2009). One particular direction in computational
history has been the use of topic models (Blei et al,
2003) to analyze the rise and fall of research top-
ics to study the progress of science, both in general
(Griffiths and Steyvers, 2004) and more specifically
in the ACL Anthology (Hall et al, 2008).
We extend this work with a more people-centered
view of computational history. In this framework,
we examine the trajectories of individual authors
across research topics in the field of computational
linguistics. By examining a single author?s paper
topics over time, we can trace the evolution of her
academic efforts; by superimposing these individual
traces over each other, we can learn how the entire
field progressed over time. One goal is to investi-
gate the use of these techniques for computational
history in general. A second goal is to use the ACL
Anthology Network Corpus (Radev et al, 2009) and
the incorporated ACL Anthology Reference Corpus
(Bird et al, 2008) to answer specific questions about
the history of computational linguistics. What is the
path that the ACL has taken throughout its 50-year
history? What roles did various research topics play
in the ACL?s development? What have been the piv-
otal turning points?
Our method consists of four steps. We first run
topic models over the corpus to classify papers into
topics and identify the topics that people author in.
We then use these topics to identify epochs by cor-
relating over time the number of persons that topics
share in common. From this, we identify epochs as
sustained patterns of topical overlap.
Our third step is to look at the flow of authors be-
tween topics over time to detect patterns in how au-
thors move between areas in the different epochs.
We group topics into clusters based on when au-
thors move in and out of them, and visualize the flow
13
of people across these clusters to identify how one
topic leads to another.
Finally, in order to understand how the field grows
and declines, we examine patterns of entry and exit
within each epoch, studying how author retention
(the extent to which authors keep publishing in the
ACL) varies across epochs.
2 Identifying Topics
Our first task is to identify research topics within
computational linguistics. We use the ACL Anthol-
ogy Network Corpus and the incorporated ACL An-
thology Reference Corpus, with around 13,000 pa-
pers by approximately 11,000 distinct authors from
1965 to 2008. Due to data sparsity in early years, we
drop all papers published prior to 1980.
We ran LDA on the corpus to produce 100 genera-
tive topics (Blei et al, 2003). Two senior researchers
in the field (the third author and Chris Manning) then
collaboratively assigned a label to each of the 100
topics, which included marking those topics which
were non-substantive (lists of function words or af-
fixes) to be eliminated. They produced a consensus
labeling with 73 final topics, shown in Table 1 (27
non-substantive topics were eliminated, e.g. a pro-
noun topic, a suffix topic, etc.).
Each paper is associated with a probability distri-
bution over the 100 original topics describing how
much of the paper is generated from each topic. All
of this information is represented by a matrix P ,
where the entry Pij is simply the loading of topic
j on paper i (since each row is a probability distri-
bution,
?
j Pij = 1). For ease of interpretation, we
sparsify the matrix P by assigning papers to topics
and thus set al entries to either 0 or 1. We do this
by choosing a threshold T and setting entries to 1 if
they exceed this threshold. If we call the new ma-
trix Q, Qij = 1 ?? Pij ? T . Throughout all
our analyses we use T = 0.1. This value is approx-
imately two standard deviations above P , the mean
of the entries in P . Most papers are assigned to 1
or 2 topics; some are assigned to none and some are
assigned to more.
This assignment of papers to topics also induces
an assignment of authors to topics: an author is as-
signed to a topic if she authored a paper assigned
to that topic. Furthermore, this assignment is natu-
rally dynamic: since every paper is published in a
particular year, authors? topic memberships change
over time. This fact is at the heart of our methodol-
ogy ? by assigning authors to topics in this princi-
pled way, we can track the topics that authors move
through. Analyzing the flow of authors through top-
ics enables us to learn which topics beget other top-
ics, and which topics are related to others by the peo-
ple that author across them.
3 Identifying Epochs
What are the major epochs of the ACL?s history? In
this section, we seek to partition the years spanned
by the ACL?s history into clear, distinct periods of
topical cohesion, which we refer to as epochs. If
the dominant research topics people are working on
suddenly change from one set of topics to another,
we view this as a transition between epochs.
To identify epochs that satisfy this definition, we
generate a set of matrices (one for each year) de-
scribing the number of people that author in every
pair of topics during that year. For year y, let Ny
be a matrix such that Nyij is the number of people
that author in both topics i and j in year y (where
authoring in topic j means being an author on a pa-
per p such that Qpj = 1). We don?t normalize by
the total number of people in each topic, thus pro-
portionally representing bigger topics since they ac-
count for more research effort than smaller topics.
Each matrix is a signature of which topic pairs have
overlapping author sets in that year.
From these matrices, we compute a final matrix
C of year-year correlations. Cij is the Pearson cor-
relation coefficient between N i and N j . C captures
the degree to which years have similar patterns of
topic authorship overlap, or the extent to which a
consistent pattern of topical research is formed. We
visualize C as a thermal in Figure 1.
To identify epochs in ACL?s history, we ran hier-
archical complete link clustering on C. This resulted
in a set of four distinct epochs: 1980?1988, 1989?
1994, 1995?2001, and 2002?2008. For three of
these periods (all except 1995?2001), years within
each of these ranges are much more similar to each
other than they are to other years. During the third
period (1995?2001), none of the years are highly
similar to any other years. This is indicative of a
14
Number Name Topics
1 Big Data
NLP
Statistical Machine Translation (Phrase-Based): bleu, statistical, source, target, phrases, smt, reordering
Dependency Parsing: dependency/ies, head, czech, depen, dependent, treebank
MultiLingual Resources: languages, spanish, russian, multilingual, lan, hindi, swedish
Relation Extraction: pattern/s, relation, extraction, instances, pairs, seed
Collocations/Compounds: compound/s, collocation/s, adjectives, nouns, entailment, expressions, MWEs
Graph Theory + BioNLP: graph/s, medical, edge/s, patient, clinical, vertex, text, report, disease
Sentiment Analysis: question/s, answer/s, answering, opinion, sentiment, negative, positive, polarity
2 Probabilistic
Methods
Discriminative Sequence Models: label/s, conditional, sequence, random, discriminative, inference
Metrics + Human Evaluation: human, measure/s, metric/s, score/s, quality, reference, automatic, correlation, judges
Statistical Parsing: parse/s, treebank, trees, Penn, Collins, parsers, Charniak, accuracy, WSJ
ngram Language Models: n-gram/s, bigram/s, prediction, trigram/s, unigram/s, trigger, show, baseline
Algorithmic Efficiency: search, length, size, space, cost, algorithms, large, complexity, pruning
Bilingual Word Alignment: alignment/s, align/ed, pair/s, statistical, source, target, links, Brown
ReRanking: score/s, candidate/s, list, best, correct, hypothesis, selection, rank/ranking, scoring, top, confidence
Evaluation Metrics: precision, recall, extraction, threshold, methods, filtering, extract, high, phrases, filter, f-measure
Methods (Experimental/Evaluation): experiments, accuracy, experiment, average, size, 100, baseline, better, per, sets
Machine Learning Optimization: function, value/s, parameter/s, local, weight, optimal, solution, criterion, variables
3 Linguistic
Supervision
Biomedical Named Entity Recognition: biomedical, gene, term, protein, abstracts, extraction, biological
Word Segmentation: segment/ation, character/s, segment/s, boundary/ies, token/ization
Document Retrieval: document/s, retrieval, query/ies, term, relevant/ance, collection, indexing, search
SRL/Framenet: argument/s, role/s, predicate, frame, FrameNet, predicates, labeling, PropBank
Wordnet/Multilingual Ontologies: ontology/ies, italian, domain/s, resource/s, i.e, ontological, concepts
WebSearch + Wikipedia: web, search, page, xml, http, engine, document, wikipedia, content, html, query, Google
Clustering + Distributional Similarity: similar/ity, cluster/s/ing, vector/s, distance, matrix, measure, pair, cosine, LSA
Word Sense Disambiguation: WordNet, senses, disambiguation, WSD, nouns, target, synsets, Yarowsky
Machine Learning Classification: classification, classifier/s, examples, kernel, class, SVM, accuracy, decision
Linguistic Annotation: annotation/s/ed, agreement, scheme/s, annotators, corpora, tools, guidelines
Tutoring Systems: student/s, reading, course, computer, tutoring, teaching, writing, essay
Chunking/Memory Based Models: chunk/s/ing, pos, accuracy, best, memory-based, Daelemans
Named Entity Recognition: entity/ies, name/s/d, person, proper, recognition, location, organization, mention
Dialog: dialogue, utterance/s, spoken, dialog/ues, act, interaction, conversation, initiative, meeting, state, agent
Summarization: topic/s, summarization, summary/ies, document/s, news, articles, content, automatic, stories
4 Discourse Multimodal (Mainly Generation): object/s, multimodal, image, referring, visual, spatial, gesture, reference, description
Text Categorization: category/ies, group/s, classification, texts, categorization, style, genre, author
Morphology: morphological, arabic, morphology, forms, stem, morpheme/s, root, suffix, lexicon
Coherence Relations: relation, rhetorical, unit/s, coherence, texts, chains
Spell Correction: error/s, correct/ion, spelling, detection, rate
Anaphora Resolution: resolution, pronoun, anaphora, antecedent, pronouns, coreference, anaphoric
Question Answering Dialog System: response/s, you, expert, request, yes, users, query, question, call, database
UI/Natural Language Interface: users, database, interface, a71, message/s, interactive, access, display
Computational Phonology: phonological, vowel, syllable, stress, phonetic, phoneme, pronunciation
Neural Networks/Human Cognition: network/s, memory, acquisition, neural, cognitive, units, activation, layer
Temporal IE/Aspect: event/s, temporal, tense, aspect, past, reference, before, state
Prosody: prosody/ic, pitch, boundary/ies, accent, cues, repairs, phrases, spoken, intonation, tone, duration
5 Early
Probability
Lexical Acquisition Of Verb Subcategorization: class/es, verb/s, paraphrase/s, subcategorization, frames
Probability Theory: probability/ies, distribution, probabilistic, estimate/tion, entropy, statistical, likelihood, parameters
Collocations Measures: frequency/ies, corpora, statistical, distribution, association, statistics, mutual, co-occurrences
POS Tagging: tag/ging, POS, tags, tagger/s, part-of-speech, tagged, accuracy, Brill, corpora, tagset
Machine Translation (Non Statistical + Bitexts): target, source, bilingual, translations, transfer, parallel, corpora
6 Automata Automata Theory: string/s, sequence/s, left, right, transformation, match
Tree Adjoining Grammars : trees, derivation, grammars, TAG, elementary, auxiliary, adjoining
Finite State Models (Automata): state/s, finite, finite-state, regular, transition, transducer
Classic Parsing: grammars, parse, chart, context-free, edge/s, production, CFG, symbol, terminal
Syntactic Trees: node/s, constraints, trees, path/s, root, constraint, label, arcs, graph, leaf, parent
7 Classic
Linguistics
Planning/BDI: plan/s/ning, action/s, goal/s, agent/s, explanation, reasoning
Dictionary Lexicons: dictionary/ies, lexicon, entry/ies, definition/s, LDOCE,
Linguistic Example Sentences: John, Mary, man, book, examples, Bill, who, dog, boy, coordination, clause
Syntactic Theory: grammatical, theory, functional, constituent/s, constraints, LFG
Formal Computational Semantics: semantics, logic/al, scope, interpretation, meaning, representation, predicate
Speech Acts + BDI: speaker, utterance, act/s, hearer, belief, proposition, focus, utterance
PP Attachment: ambiguity/ies/ous, disambiguation, attachment, preference, preposition
Natural Language Generation: generation/ing, generator, choice, generated, realization, content
Lexical Semantics: meaning/s, semantics, metaphor, interpretation, object, role
Categorial Grammar/Logic: proof, logic, definition, let, formula, theorem, every, iff, calculus
Syntax: clause/s, head, subject, phrases, object, verbs, relative, nouns, modifier
Unification Based Grammars: unification, constraints, structures, value, HPSG, default, head
Concept Ontologies / Knowledge Rep: concept/s, conceptual, attribute/s, relation, base
8 Government MUC-Era Information Extraction: template/s, message, slot/s, extraction, key, event, MUC, fill/s
Speech Recognition: recognition, acoustic, error, speaker, rate, adaptation, recognizer, phone, ASR
ATIS dialog: spoken, atis, flight, darpa, understanding, class, database, workshop, utterances
9 Early NLU 1970s-80s NLU Work: 1975-9, 1980-6, computer, understanding, syntax, semantics, ATN, Winograd, Schank, Wilks, lisp
Code Examples: list/s, program/s, item/s, file/s, code/s, computer, line, output, index, field, data, format
Speech Parsing And Understanding: frame/s, slot/s, fragment/s, parse, representation, meaning
Table 1: Results of topic clustering, showing some high-probability representative words for each cluster.
15
Figure 1: Year-year correlation in topic authoring
patterns. Hotter colors indicate high correlation,
colder colors denote low correlation.
state of flux in which authors are constantly chang-
ing the topics they are in. As such, we refer to
this period as a transitory epoch. Thus our analysis
has identified four main epochs in the ACL corpus
between 1980 and 2008: three focused periods of
work, and one transitory phase.
These epochs correspond to natural eras in the
ACL?s history. During the 1980?s, there were co-
herent communities of research on natural language
understanding and parsing, generation, dialog, uni-
fication and other grammar formalizations, and lex-
icons and ontologies.
The 1989?1994 era corresponds to a number of
important US government initiatives: MUC, ATIS,
and the DARPA workshops. The Message Under-
standing Conferences (MUC) were an early initia-
tive in information extraction, set up by the United
States Naval Oceans Systems Center with the sup-
port of DARPA, the Defense Advanced Research
Projects Agency. A condition of attending the MUC
workshops was participation in a required evalua-
tion (bakeoff) task of filling slots in templates about
events, and began (after an exploratory MUC-1 in
1987) with MUC-2 in 1989, followed by MUC-3
(1991), MUC-4 (1992), MUC-5 (1993) and MUC-
6 (1995) (Grishman and Sundheim, 1996). The
Air Travel Information System (ATIS) was a task
for measuring progress in spoken language under-
standing, sponsored by DARPA (Hemphill et al,
1990; Price, 1990). Subjects talked with a system
to answer questions about flight schedules and air-
line fares from a database; there were evaluations
in 1990, 1991, 1992, 1993, and 1994 (Dahl et al,
1994). The ATIS systems were described in pa-
pers at the DARPA Speech and Natural Language
Workshops, a series of DARPA-sponsored worksh-
sop held from 1989?1994 to which DARPA grantees
were strongly encouraged to participate, with the
goal of bringing together the speech and natural lan-
guage processing communities.
After the MUC and ATIS bakeoffs and the
DARPA workshops ended, the field largely stopped
publishing in the bakeoff topics and transitioned to
other topics; participation by researchers in speech
recognition also dropped off significantly. From
2002 onward, the field settled into the modern era
characterized by broad multilingual work and spe-
cific areas like dependency parsing, statistical ma-
chine translation, information extraction, and senti-
ment analysis.
In summary, our methods identify four major
epochs in the ACL?s history: an early NLP period,
the ?government? period, a transitory period, and a
modern integration period. The first, second, and
fourth epochs are periods of sustained topical co-
herence, whereas the third is a transitory phase dur-
ing which the field moved from the bakeoff work to
modern-day topics.
4 Identifying Participant Flows
In the previous section, we used topic co-
membership to identify four coherent epochs in the
ACL?s history. Now we turn our attention to a finer-
grained question: How do scientific areas or move-
ments arise? How does one research area develop
out of another as authors transition from a previous
research topic to a new one? We address this ques-
tion by tracing the paths of authors through topics
over time, in aggregate.
4.1 Topic Clustering
We first group topics into clusters based on how au-
thors move through them. To do this, we group years
into 3-year time windows and consider adjacent time
periods. We aggregate into 3-year windows because
16
the flow across adjacent single years is noisy and of-
ten does not accurately reflect shifts in topical fo-
cus. For each adjacent pair of time periods (for ex-
ample, 1980?1982 and 1983?1985), we construct a
matrix S capturing author flow between each topic
pair, where the Sij entry is the number of authors
who authored in topic i during the first time period
and authored in topic j during the second time pe-
riod. These matrices capture people flow between
topics over time.
Next we compute similarity between topics. We
represent each topic by its flow profile, which is sim-
ply the concatenation of all its in- and out-flows in
all of the S matrices. More formally, let Fi be the re-
sulting vector after concatenating the i-th row (trans-
posed into a column) and i-th column of every S
matrix. We compute a topic-topic similarity matrix
T where Tij is the Pearson correlation coefficient
between Fi and Fj . Two topics are then similar
if they have similar flow profiles. Note that topics
don?t need to share authors to be similar ? authors
just need to move in and out of them at roughly the
same times. Through this approach, we identify top-
ics that play similar roles in the ACL?s history.
To find a grouping of topics that play similar roles,
we perform hierarchical complete link clustering on
the T matrix. The goal is to identify clusters of
topics that are highly similar to each other but are
dissimilar from those in other clusters. Hierarchi-
cal clustering begins with every topic forming a sin-
gleton cluster, then iteratively merges the two most
similar clusters at every step until there is only one
cluster of all topics remaining. Every step gives
a different clustering solution, so we assess clus-
ter fitness using Krackhard and Stern?s E-I index,
which measures the sum of external ties minus the
sum of internal ties divided by the sum of all ties.
Given T as an input, the E-I index optimizes iden-
tical profiles as clusters (i.e., topic stages), not dis-
crete groups. The optimal solution we picked using
the E-I index entails 9 clusters (shown in Table 1),
numbered roughly backwards from the present to the
past. We?ll discuss the names of the clusters in the
next section.
4.2 Flows Between Topic Clusters
Now that we have grouped topics into clusters by
how authors flow in and out of them, we can com-
pute the flow between topics or between topic clus-
ters over time. First we define what a flow between
topics is. We use the same flow matrix used in the
above topic clustering: the flow between topic i in
one time period and topic j in the following time pe-
riod is simply the number of authors present in both
at the respective times. Again we avoid normaliz-
ing because the volume of people moving between
topics is relevant.
Now we can define flow between clusters. Let A
be the set of topics in cluster C1 and let B be the set
of topics in cluster C2. We define the flow between
C1 and C2 to be the average flow between topics in
A and B:
f(C1, C2) =
?
A?A,B?B f(A,B)
|A| ? |B|
(where f(A,B) represents the topic-topic flow
defined above). We also tried defining cluster-
cluster flow as the maximum over all topic-topic
flows between the clusters, and the results were
qualitatively the same.
Figure 2 shows the resulting flows between clus-
ters. Figure 2a shows the earliest period in our
(post-1980) dataset, where we see reflections of ear-
lier natural language understanding work by Schank,
Woods, Winograd, and others, quickly leading into
a predominance of what we?ve called ?Classic Lin-
guistic Topics?. Research in this period is charac-
terized by a more linguistically-oriented focus, in-
cluding syntactic topics like unification and catego-
rial grammars, formal syntactic theory, and preposi-
tional phrase attachments, linguistic semantics (both
lexical semantics and formal semantics), and BDI
dialog models. Separately we see the beginnings of
a movement of people into phonology and discourse
and also into the cluster we?ve called ?Automata?,
which at this stage includes (pre-statistical) Parsing
and Tree Adjoining Grammars.
In Figure 2b we see the movement of people
into the cluster of government-sponsored topics: the
ATIS and MUC bakeoffs, and speech.
In Figure 2c bakeoff research is the dominant
theme, but people are also beginning to move in and
out of two new clusters. One is Early Probabilistic
Models, in which people focused on tasks like Part
of Speech tagging, Collocations, and Lexical Acqui-
17
87 Classic Ling.
9 Early NLU
6
Automata
4
Discourse
5
2
3
1
6.9
2.6
0.8
1.0
1.7
1.4
0.6
0.8
0.6
0.7
(a) 1980?1983 ? 1984?1988
8
Gov?t
7
9 Early NLU
6
Automata
4
Discourse
5
2
3
1
3.1
1.8
1.6
0.7
0.7
1.2
1.0
0.9
1.8
(b) 1986?1988 ? 1989?1991
8
7
Classic Ling.
9
Early NLU
6
Automata
4
5
Early Prob.
2
Prob. Methods
3
1
19.6
3.1
2.6
2.8
2.7
2.1
2.7
2.4
2.3
2.1
(c) 1989?1991?1992?1994
8
7
Classic Ling.
9
6Automata
4
5Early Prob.
2
3
Ling. Supervision
1
3.7
1.2
2.7
1.5
1.8
1.1
1.0
3.4
0.9
0.9
(d) 1992?1994?1995?1998
8
7
9
6
4
5Early Prob.
2
Prob. Methods
3 Ling. Supervision
1 Big Data NLP6.7
3.3
3.2
3.7
3.6
2.6
5.7
4.2
2.9
2.7
(e) 2002?2004?2005?2007
Figure 2: Author flow between topic clusters in five key time periods. Clusters are sized according to how
many authors are in those topics in the first time period of each diagram. Edge thickness is proportional to
volume of author flow between nodes, relative to biggest flow in that diagram (i.e. edge thicknesses in are
not comparable across diagrams).
18
sition of Verb Subcategorization. People also begin
to move specifically from the MUC Bakeoffs into a
second cluster we call Probabilistic Methods, which
in this very early stage focused on Evaluations Met-
rics and Experimental/Evaluation Methods. People
working in the ?Automata? cluster (Tree Adjoining
Grammar, Parsing, and by this point Finite State
Methods) continue working in these topics.
By Figure 2d, the Early Probability topics are
very central, and probabilistic terminology and early
tasks (tagging, collocations, and verb subcategoriza-
tion) are quite popular. People are now moving
into a new cluster we call ?Linguistic Supervised?, a
set of tasks that apply supervised machine learning
(usually classification) to tasks for which the gold la-
bels are created by linguists. The first task to appear
in this area was Named Entity Recognition, popu-
lated by authors who had worked on MUC, and the
core methods topics of Machine Learning Classifi-
cation and Linguistic Annotation. Other tasks like
Word Sense Disambiguation soon followed.
By Figure 2e, people are leaving Early Probabil-
ity topics like part of speech tagging, collocations,
and non-statistical MT and moving into the Linguis-
tic Supervised (e.g., Semantic Role Labeling) and
Probabilistic Methods topics, which are now very
central. In Probabilistic Methods, there are large
groups of people in Statistical Parsing and N-grams.
By the end of this period, Prob Methods is sending
authors to new topics in Big Data NLP, the biggest of
which are Statistical Machine Translation and Sen-
timent Analysis.
In sum, the patterns of participant flows reveal
how sets of topics assume similar roles in the his-
tory of the ACL. In the initial period, authors move
mostly between early NLP and classic linguistics
topics. This period of exchange is then transformed
by the arrival of government bakeoffs that draw au-
thors into supervised linguistics and probabilistic
topics. Only in the 2000?s did the field mature and
begin a new period of cohesive exchange across a
variety of topics with shared statistical methods.
5 Member Retention and Field Integration
How does the ACL grow or decline? Do authors
come and go, or do they stay for long periods? How
much churn is there in the author set? How do these
1980 1985 1990 1995 2000 2005
First year of time frame
0.2
0.3
0.4
0.5
0.6
0.7
A
u
t
h
o
r
 
r
e
t
e
n
t
i
o
n
Figure 3: Overlap of authors in successive 3-year
time periods over time. The x-axis indicates the
first year of the 6-year time window being consid-
ered. Vertical dotted lines indicate epoch bound-
aries, where a year is a boundary if the first time
period is entirely in one epoch and the second is en-
tirely in the next.
trends align with the epochs we identified? To ad-
dress these questions, we examine author retention
over time ? how many authors stay in the field ver-
sus how many enter or exit.
In order to calculate membership churn, we cal-
culate the Jaccard overlap in the sets of people that
author in adjacent 3-year time periods. This met-
ric reflects the author retention from the first period
to the second, and is inherently normalized by the
number of authors (so the growing number of au-
thors over time doesn?t bias the trend). We use 3-
year time windows since it?s not unusual for authors
to not publish in some years while still remaining ac-
tive. We also remove the bulk of one-time authors by
restricting the authors under consideration to those
who have published at least 10 papers, but the ob-
served trend is similar for any threshold (including
no threshold). The first computation is the Jaccard
overlap between those who authored in 1980?1982
and those who authored in 1983?1985; the last is
between the author sets of the 2003?2005 and 2006?
2008 time windows. The trend is shown in Figure 3.
The author retention curve shows a clear align-
ment with the epochs we identified. In the first
19
epoch, the field is in its infancy: authors are work-
ing in a stable set of topics, but author retention is
relatively low. Once the bakeoff epoch starts, au-
thor retention jumps significantly ? people stay in
the field as they continue to work on bakeoff pa-
pers. As soon as the bakeoffs end, the overlap in
authors drops again. The fact that author retention
rocketed upwards during the bakeoff epoch is pre-
sumably caused by the strong external funding in-
centive attracting external authors to enter and re-
peatedly publish in these conferences.
To understand whether this drop in overlap of au-
thors was indeed indicative of authors who entered
the field mainly for the bakeoffs, we examined au-
thors who first published in the database in 1989. Of
the 50 most prolific such authors (those with more
than 8 publications in the database), 25 (exactly
half) were speech recognition researchers. Of those
25 speech researchers, 16 exited (never published
again in the ACL conferences) after the bakeoffs.
But 9 (36%) of them remained, mainly by adapting
their (formerly speech-focused) research areas to-
ward natural language processing topics. Together,
these facts suggest that the government-sponsored
period led to a large influx of speech recognition
researchers coming to ACL conferences, and that
some fraction of them remained, continuing with
natural language processing topics.
Despite the loss of the majority of the speech
recognition researchers at the end of the bakeoff
period, the author retention curve doesn?t descend
to pre-bakeoff levels: it stabilizes at a consistently
higher value during the transitory epoch. This may
partly be due to these new researchers colonizing
and remaining in the field. Or it may be due to the
increased number of topics and methods that were
developed during the government-sponsored period.
Whichever it is, the fact that retention didn?t return
to its previous levels suggests that the government
sponsorship that dominated the second epoch had a
lasting positive effect on the field.
In the final epoch, author retention monotonically
increases to its highest-ever levels; every year the
rate of authors publishing continuously rises, as does
the total number of members, suggesting that the
ACL community is coalescing as a field. It is plau-
sible that this final uptick is due to funding ? gov-
ernmental, industrial, or otherwise ? and it is an in-
teresting direction for further research to investigate
this possibility.
In sum, we observe two epochs where member
retention increases: the era of government bakeoffs
(1989?1994) and the more recent era where NLP
has received significantly increased industry interest
as well as government funding (2002?2008). These
eras may thus both be ones where greater external
demand increased retention and cohesion.
6 Conclusion
We offer a new people-centric methodology for
computational history and apply it to the AAN to
produce a number of insights about the field of com-
putational linguistics.
Our major result is to elucidate the many ways
in which the government-sponsored bakeoffs and
workshops had a transformative effect on the field
in the early 1990?s. It has long been understood that
the government played an important role in the field,
from the early support of machine translation to the
ALPAC report. Our work extends this understand-
ing, showing that the government-supported bake-
offs and workshops from 1989 to 1994 caused an in-
flux of speech scientists, a large percentage of whom
remained after the bakeoffs ended. The bakeoffs
and workshops acted as a major bridge from early
linguistic topics to modern probabilistic topics, and
catalyzed a sharp increase in author retention.
The significant recent increase in author overlap
also suggests that computational linguistics is in-
tegrating into a mature field. This integration has
drawn on modern shared methodologies of statistical
methods and their application to large scale corpora,
and may have been supported by industry demands
as well as by government funding. Future work will
be needed to see whether the current era is one much
like the bakeoff era with an outflux of persons once
funding dries up, or if it has reached a level of matu-
rity reflective of a well-established discipline.
Acknowledgments
This research was generously supported by the Of-
fice of the President at Stanford University and the
National Science Foundation under award 0835614.
Thanks to the anonymous reviewers, and to Steven
Bethard for creating the topic models.
20
References
A. Aris, B. Shneiderman, V. Qazvinian, and D. Radev.
2009. Visual overviews for discovering key papers and
influences across research fronts. Journal of the Amer-
ican Society for Information Science and Technology,
60(11):2219?2228.
C. Au Yeung and A. Jatowt. 2011. Studying how the
past is remembered: towards computational history
through large scale text mining. In Proceedings of
the 20th ACM international conference on Information
and knowledge management, pages 1231?1240. ACM.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M. Joseph, M.Y.
Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan.
2008. The acl anthology reference corpus: A refer-
ence dataset for bibliographic research in computa-
tional linguistics. In Proc. of the 6th International
Conference on Language Resources and Evaluation
Conference (LREC?08), pages 1755?1759.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning Re-
search, 3(5):993?1022.
D.A. Dahl, M. Bates, M. Brown, W. Fisher, K. Hunicke-
Smith, D. Pallett, C. Pao, A. Rudnicky, and
E. Shriberg. 1994. Expanding the scope of the atis
task: The atis-3 corpus. In Proceedings of the work-
shop on Human Language Technology, pages 43?48.
Association for Computational Linguistics.
S. Gerrish and D.M. Blei. 2010. A language-based ap-
proach to measuring scholarly impact. In Proceed-
ings of the 26th International Conference on Machine
Learning.
T.L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences of the United States of America, 101(Suppl
1):5228.
R. Grishman and B. Sundheim. 1996. Message under-
standing conference-6: A brief history. In Proceedings
of COLING, volume 96, pages 466?471.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using topic
models. In Proceedings of EMNLP 2008.
C.T. Hemphill, J.J. Godfrey, and G.R. Doddington. 1990.
The atis spoken language systems pilot corpus. In Pro-
ceedings of the DARPA speech and natural language
workshop, pages 96?101.
P. Price. 1990. Evaluation of spoken language systems:
The atis domain. In Proceedings of the Third DARPA
Speech and Natural Language Workshop, pages 91?
95. Morgan Kaufmann.
D.R. Radev, P. Muthukrishnan, and V. Qazvinian. 2009.
The acl anthology network corpus. In Proceedings of
the 2009 Workshop on Text and Citation Analysis for
Scholarly Digital Libraries, pages 54?61. Association
for Computational Linguistics.
Y. Tu, N. Johri, D. Roth, and J. Hockenmaier. 2010. Ci-
tation author topic model in expert search. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics: Posters, pages 1265?1273. Asso-
ciation for Computational Linguistics.
21
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 33?41,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
He Said, She Said: Gender in the ACL Anthology
Adam Vogel
Stanford University
av@cs.stanford.edu
Dan Jurafsky
Stanford University
jurafsky@stanford.edu
Abstract
Studies of gender balance in academic com-
puter science are typically based on statistics
on enrollment and graduation. Going beyond
these coarse measures of gender participation,
we conduct a fine-grained study of gender
in the field of Natural Language Processing.
We use topic models (Latent Dirichlet Allo-
cation) to explore the research topics of men
and women in the ACL Anthology Network.
We find that women publish more on dialog,
discourse, and sentiment, while men publish
more than women in parsing, formal seman-
tics, and finite state models. To conduct our
study we labeled the gender of authors in the
ACL Anthology mostly manually, creating a
useful resource for other gender studies. Fi-
nally, our study of historical patterns in fe-
male participation shows that the proportion
of women authors in computational linguis-
tics has been continuously increasing, with
approximately a 50% increase in the three
decades since 1980.
1 Introduction
The gender imbalance in science and engineering is
particularly striking in computer science, where the
percentage of graduate students in computer science
that are women seems to have been declining rather
than increasing recently (Palma, 2001; Beaubouef
and Zhang, 2011; Spertus, 1991; Hill et al, 2010;
Singh et al, 2007).
While many studies have examined enrollment
and career advancement, less attention has been
paid to gender differences in scientific publications.
This paper studies author gender in the Associa-
tion for Computational Linguistics Anthology Net-
work (AAN) corpus (Radev et al, 2009), (based on
the ACL Anthology Reference Corpus (Bird et al,
2008)) from which we used 13,000 papers by ap-
proximately 12,000 distinct authors from 1965 to
2008.
The AAN corpus disambiguates author names,
but does not annotate these names for gender. We
first performed a mostly-manual annotation of the
gender of each author (details in Section 2). We
make these annotation available as a useful resource
for other researchers.1
We then study a number of properties of the ACL
authors. We first address surface level questions re-
garding the balance of genders in publications. In
2008, women were granted 20.5% of computer sci-
ence PhDs (CRA, 2008). Does this ratio hold also
for the percentages of papers written by women in
computational linguistics as well? We explore dif-
ferences in publication count between genders, look-
ing at total publications and normalized values like
publications per year and trends over time.
Going beyond surface level analysis, we then turn
to document content. We utilize Latent Dirichlet Al-
location (LDA) topic models (Blei et al, 2003) to
study the difference in topics that men and women
write about.
2 Determining Gender
The gender of an author is in general difficult to
determine automatically with extremely high pre-
cision. In many languages, there are gender-
differentiated names for men and women that can
make gender-assignment possible based on gen-
1http://nlp.stanford.edu/projects/
gender.shtml
33
dered name dictionaries. But the fact that ACL
authors come from many different language back-
ground makes this method prone to error. For exam-
ple, while U.S. Census lists of frequently occurring
names by gender (Census, 2012) can resolve a large
proportion of commonly occurring names from au-
thors in the United States and Canada, they incor-
rectly list the name ?Jan? as female. It turns out
that authors in the ACL Anthology who are named
?Jan? are in fact male, since the name is a very com-
mon male name in many parts of Europe, and since
US female researchers named ?Jan? often use the
full form of their name rather than the shortening
?Jan? when publishing. Furthermore, a significant
percentage of ACL authors have Chinese language
names, which are much less clearly linked with per-
sonal names (e.g., Weiwei Sun is female whereas
Weiwei Ding is male).
We found that Chinese names as well as ambigu-
ous names like ?Jan? were poorly predicted by on-
line name gender website algorithms we looked at,
leading to a high error rate. To insure high precision,
we therefore instead chose to annotate the authors
in the corpus with a high-precision method; mainly
hand labeling the names but also using some auto-
matic help.
We used unambiguous name lists for various lan-
guages to label a large proportion of the name; for
example we used the subset of given names (out
of the 4221 first names reported in the 1990 U.S.
Census) that were unambiguous (occurring consis-
tently with only one gender in all of our name lists)
used morphological gender for languages like Czech
or Bulgarian which mark morphological gender on
names, and relied on lists of Indian and Basque
names (from which we had removed any ambigu-
ous names). For all ambiguous names, we next used
our personal cognizance of many of the ACL au-
thors, also asking for help from ACL researchers
in China, Taiwan, and Singapore (to help label Chi-
nese names of researchers they were familiar with)
and other researchers for help on the Japanese and
Korean names. Around 1100 names were hand-
labeled from personal cognizance or photos of the
ACL researchers on their web pages. The combina-
tion of name lists and personal cognizance left only
2048 names (15% of the original 12,692) still unla-
beled. We then used a baby name website, www.
Total First Author
Gender Papers % Papers %
Female 6772 33% 4034 27%
Male 13454 64% 10813 71%
Unknown 702 3% 313 2%
Table 1: Number of publications by gender. The to-
tal publications column shows the number of papers for
which at least one author was a given gender, in any au-
thorship position. The first authored publications column
shows the number of papers for which a given gender is
the first author.
gpeters.com/names/, originally designed for
reporting the popularity and gender balance of first
names, to find the gender of 1287 of these 2048
names.2 The remaining 761 names remained unla-
beled.
 0 1000 2000
 3000 4000 5000
 6000 7000 8000
 9000
Female Male UnknownN
u
m
b
e
r
 
o
f
 
A
u
t
h
o
r
s
Gender
Authorship by Gender
Figure 1: The total number of authors of a given gender.
3 Overall Statistics
We first discuss some overall gender statistics for the
ACL Anthology. Figure 1 shows the number of au-
thors of each gender. Men comprised 8573 of the
12692 authors (67.5%) and there were 3359 female
authors (26.5%). We could not confidently deter-
mine the gender of 761 out of 12692 (6.0%) of the
authors. Some of these are due to single letter first
names or problems with ill-formatted data.
Table 1 lists the number of papers for each gen-
der. About twice as many papers had at least one
2The gender balance of these 1287 automatically-
determined names was 34% female, 66% male, slightly
higher than the average for the whole corpus.
34
male author (64%) as had at least one female au-
thor (33%). The statistics for first authorship were
slightly more skewed; women were the first author
of 27% of papers, whereas men first authored 71%.
In papers with at least one female author, the first au-
thor was a woman 60% of the time, whereas papers
with at least one male author had a male first author
80% of the time. Thus men not only write more pa-
pers, but are also more frequently first authors.
 0 500
 1000 1500
 2000 2500
 1960  1970  1980  1990  2000  2010
P
u
b
l
i
c
a
t
i
o
n
s
Year
Authorship by YearFemaleMaleUNK
Figure 2: The number of authors of a given gender for a
given year.
Figure 2 shows gender statistics over time, giving
the number of authors of a given gender for a given
year. An author is considered active for a year if he
or she was an author of at least one paper. The num-
ber of both men and women authors increases over
the years, reflecting the growth of computational lin-
guistics.
Figure 3 shows the percentage of authors of a
given gender over time. We overlay a linear re-
gression of authorship percentage for each gender
showing that the proportion of women is growing
over time. The male best fit line has equation y =
?0.3025x + 675.49(R2 = 0.41, p = 1.95 ? 10?5)
and the female best fit line is y = 0.3429x ?
659.48(R2 = 0.51, p = 1.48?10?5). Female author-
ship percentage grew from 13% in 1980 to 27% in
2007, while male authorship percentage decreased
from 79% in 1980 to 71% in 2007. Using the best
fit lines as a more robust estimate, female authorship
grew from 19.4% to 29.1%, a 50% relative increase.
This increase of the percentage of women author-
ship is substantial. Comparable numbers do not
seem to exist for computer science in general, but
according to the CRA Taulbee Surveys of computer
science (CRA, 2008), women were awarded 18% of
the PhDs in 2002 and 20.5% in 2007. In computa-
tional linguistics in the AAN, women first-authored
26% of papers in 2002 and 27% of papers in 2007.
Although of course these numbers are not directly
comparable, they at least suggest that women partic-
ipate in computational linguistics research at least as
much as in the general computer science population
and quite possibly significantly more.
We next turn attention to how the most prolific
authors of each gender compare. Figure 4 shows the
number of papers published by the top 400 authors
of each gender, sorted in decreasing order. We see
that the most prolific authors are men.
There is an important confound in interpreting the
number of total papers by men and the statistics on
prolific authors. Since, as Figure 3 shows, there was
a smaller proportion of women in the field in the
early days of computational linguistics, and since
authors publish more papers the longer they are in
the field, it?s important to control for length of ser-
vice.
Figure 5 shows the average number of active years
for each gender. An author is considered active in
the years between his or her first and last publication
in the anthology. Comparing the number of years
of service for each gender, we find that on average
men indeed have been in the field longer (t-test, p =
10?6).
Accounting for this fact, Figure 6 shows the aver-
age number of publications per active year. Women
published an average of 1.07 papers per year active,
while men published 1.03 papers per active year.
This difference is significant (t-test, p = 10?3), sug-
gesting that women are in fact slightly more prolific
than men per active year.
In the field of Ecology, Sih and Nishikawa (1988)
found that men and women published roughly the
same number of papers per year of service. They
used a random sample of 100 researchers in the field.
In contrast, Symonds et al (2006) found that men
published more papers per year than women in ecol-
ogy and evolutionary biology. This study also used
random sampling, so it is unclear if the differing re-
sults are caused by a sampling error or by some other
source.
35
 0
 20
 40
 60
 80
 100
 1980  1990  2000  2010P
e
r
c
e
n
t
a
g
e
 
o
f
 
A
u
t
h
o
r
s
Year
Percentage Authorship by Year
FemaleMaleUNK
Figure 3: The percentage of authors of a given gender per year. Author statistics before 1980 are sparse and noisy, so
we only display percentages from 1980 to 2008.
 0 20
 40 60
 80 100
 120 140
 0  50  100 150 200 250 300 350 400
P
u
b
l
i
c
a
t
i
o
n
s
Rank
Number of Publications by GenderFemaleMale
Figure 4: The number of publications per author sorted
in decreasing order.
4 Topic Models
In this section we discuss the relationship between
gender and document content. Our main tool is La-
tent Dirichlet Allocation (LDA), a model of the top-
ics in a document. We briefly describe LDA; see
(Blei et al, 2003) for more details. LDA is a genera-
tive model of documents, which models documents
as a multinomial mixture of topics, which in turn are
 0 0.5
 1 1.5
 2 2.5
 3 3.5
Female MaleNum
b
e
r
 
o
f
 
A
c
t
i
v
e
 
Y
e
a
r
s
Gender
Average Number of Active Years
Figure 5: The average number of active years by gender
multinomial distributions over words. The genera-
tive story proceeds as follows: a document first picks
the number of words N it will contain and samples a
multinomial topic distribution p(z|d) from a Dirich-
let prior. Then for each word to be generated, it picks
a topic z for that word, and then a word from the
multinomial distribution p(w|z).
Following earlier work like Hall et al (2008), we
ran LDA (Blei et al, 2003) on the ACL Anthology,
36
 0
 0.5
 1
 1.5
 2
Female MaleN
u
m
b
e
r
 
o
f
 
P
a
p
e
r
s
Gender
Average Papers Per Year Active
Figure 6: The average number of papers per active year,
where an author is considered active in years between his
or her first and last publication.
producing 100 generative topics. The second author
and another senior expert in the field (Christopher D.
Manning) collaboratively assigned labels to each of
the 100 topics including marking those topics which
were non-substantive (lists of function words or af-
fixes) to be eliminated. Their consensus labeling
eliminated 27 topics, leaving 73 substantive topics.
In this study we are interested in how documents
written by men and women differ. We are mainly in-
terested in Pr(Z|G), the probability of a topic being
written about by a given gender, and Pr(Z|Y,G),
the probability of a topic being written about by a
particular gender in a given year. Random variable
Z ranges over topics, Y over years, and G over gen-
der. Our topic model gives us Pr(z|d), where d is a
particular document. For a document d ? D, let dG
be the gender of the first author, and dY the year it
was written.
To compute Pr(z|g), we sum over documents
whose first author is gender g:
Pr(z|g) =
?
{d?D|dG=g}
Pr(z|d) Pr(d|g)
=
?
{d?D|dG=g}
Pr(z|d)
|{d ? D|dG = g}|
To compute Pr(z|y, g), we additionally condition
on the year a document was written:
Pr(z|y, g) =
?
{d?D|dY =y}
Pr(z|d) Pr(d|y, g)
=
?
{d?D|dY =y,dG=g}
Pr(z|d)
|{d ? D|dY = y, dG = g}|
To determine fields in which one gender publishes
more than another, we compute the odds-ratio
Pr(z|g = female)(1? Pr(z|g = female))
Pr(z|g = male)(1? Pr(z|g = male))
for each of the 73 topics in our corpus.
5 Topic Modeling Results
Using the odds-ratio defined above, we computed
the top eight male and female topics. The top
female-published topics are speech acts + BDI,
prosody, sentiment, dialog, verb subcategorization,
summarization, anaphora resolution, and tutoring
systems. Figure 9 shows the top words for each of
those topics. Figure 7 shows how they have evolved
over time.
The top male-published topics are categorial
grammar + logic, dependency parsing, algorithmic
efficiency, parsing, discriminative sequence models,
unification based grammars, probability theory, and
formal semantics. Figure 8 and 10 display these top-
ics over time and their associated words.
There are interesting possible generalizations in
these topic differences. At least in the ACL cor-
pus, women tend to publish more in speech, in social
and conversational topics, and in lexical semantics.
Men tend to publish more in formal mathematical
approaches and in formal syntax and semantics.
Of course the fact that a certain topic is more
linked with one gender doesn?t mean the other gen-
der does not publish in this topic. In particular, due
to the larger number of men in the field, there can be
numerically more male-authored papers in a female-
published topic. Instead, what our analysis yields
are topics that each gender writes more about, when
adjusted by the number of papers published by that
gender in total.
Nonetheless, these differences do suggest that
women and men in the ACL corpus may, at least
to some extent, exhibit some gender-specific tenden-
cies to favor different areas of research.
37
 0 0.01 0.02
 0.03 0.04 0.05
 0.06 0.07 0.08
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Speech Acts + BDIFemaleMale
 0 0.005 0.01
 0.015 0.02 0.025
 0.03 0.035 0.04
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
ProsodyFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Sentiment AnalysisFemaleMale
 0 0.005 0.01
 0.015 0.02 0.025
 0.03 0.035 0.04
 0.045 0.05
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
DialogFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 0.03
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Lexical Acquisition Of Verb SubcategorizationFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 0.03
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
SummarizationFemaleMale
 0 0.005 0.01
 0.015 0.02 0.025
 0.03 0.035 0.04
 0.045
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Anaphora ResolutionFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Tutoring SystemsFemaleMale
Figure 7: Plots of some topics for which P (topic|female) > P (topic|male). Note that the scale of the y-axis differs
between plots.
38
 0 0.005 0.01
 0.015 0.02 0.025
 0.03 0.035 0.04
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Categorial GrammarFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Dependency ParsingFemaleMale
 0 0.002 0.004
 0.006 0.008 0.01
 0.012 0.014 0.016
 0.018
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Algorithmic EfficiencyFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 0.03
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
ParsingFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 0.03
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Discriminative Sequence ModelsFemaleMale
 0 0.005 0.01
 0.015 0.02 0.025
 0.03 0.035 0.04
 0.045
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Unification Based GrammarsFemaleMale
 0 0.005
 0.01 0.015
 0.02 0.025
 0.03
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Probability TheoryFemaleMale
 0 0.005 0.01
 0.015 0.02 0.025
 0.03 0.035 0.04
 0.045
 1980  1990  2000  2010P(t
o
p
i
c
|
g
e
n
d
e
r
,
y
e
a
r
)
Year
Formal Computational SemanticsFemaleMale
Figure 8: Plots of some topics for which P (topic|male) > P (topic|female). Note that the scale of the y-axis differs
between plots.
39
Speech Acts + BDI speaker utterance act hearer belief proposition acts beliefs focus evidence
Prosody prosodic pitch boundary accent prosody boundaries cues repairs speaker phrases
Sentiment question answer questions answers answering opinion sentiment negative trec positive
Dialog dialogue utterance utterances spoken dialog dialogues act turn interaction conversation
Verb Subcategorization class classes verbs paraphrases classification subcategorization paraphrase frames
Summarization topic summarization summary document news summaries documents topics articles
Anaphora Resolution resolution pronoun anaphora antecedent pronouns coreference anaphoric definite
Tutoring Systems students student reading course computer tutoring teaching writing essay native
Figure 9: Top words for each topic that women publish in more than men
Categorial Grammar + Logic proof logic definition let formula theorem every defined categorial axioms
Dependency Parsing dependency dependencies head czech depen dependent treebank structures
Algorithmic Efficiency search length size space cost algorithms large complexity pruning efficient
Parsing grammars parse chart context-free edge edges production symbols symbol cfg
Discriminative Sequence Models label conditional sequence random labels discriminative inference crf fields
Unification Based Grammars unification constraints structures value hpsg default head grammars values
Probability Theory probability probabilities distribution probabilistic estimation estimate entropy
Formal Semantics semantics logical scope interpretation logic meaning representation predicate
Figure 10: Top words for each topic that men publish in more than women
6 Conclusion
Our study of gender in the ACL Anthology shows
important gains in the percentage of women in the
field over the history of the ACL (or at least the last
30 years of it). More concretely, we find approxi-
mately a 50% increase in the proportion of female
authors since 1980. While women?s smaller num-
bers means that they have produced less total pa-
pers in the anthology, they have equal (or even very
slightly higher) productivity of papers per year.
In topics, we do notice some differing tenden-
cies toward particular research topics. In current
work, we are examining whether these differences
are shrinking over time, as a visual overview of Fig-
ure 7 seems to suggest, which might indicate that
gender balance in topics is a possible outcome, or
possibly that topics first addressed by women are
likely to to be taken up by male researchers. Ad-
ditionally, other applications of topic models to the
ACL Anthology allow us to study the topics a sin-
gle author publishes in over time (Anderson et al,
2012). These techniques would allow us to study
how gender relates to an author?s topics throughout
his or her career.
Our gender labels for ACL authors (available
at http://nlp.stanford.edu/projects/
gender.shtml) provide an important resource
for other researchers to expand on the social study
of computational linguistics research.
7 Acknowledgments
This research was generously supported by the Of-
fice of the President at Stanford University and the
National Science Foundation under award 0835614.
Thanks to Steven Bethard and David Hall for cre-
ating the topic models, Christopher D. Manning for
helping label the topics, and Chu-Ren Huang, Olivia
Kwong, Heeyoung Lee, Hwee Tou Ng, and Nigel
Ward for helping with labeling names for gender.
Additional thanks to Martin Kay for the initial pa-
per idea.
References
Ashton Anderson, Dan McFarland, and Dan Jurafsky.
2012. Towards a computational history of the acl:
1980 - 2008. In ACL 2012 Workshop: Rediscovering
50 Years of Discoveries.
Theresa Beaubouef and Wendy Zhang. 2011. Where are
the women computer science students? J. Comput.
Sci. Coll., 26(4):14?20, April.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M. Joseph, M.Y.
Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan.
40
2008. The ACL Anthology Reference Corpus: A ref-
erence dataset for bibliographic research in computa-
tional linguistics. In LREC-08, pages 1755?1759.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022, March.
US Census. 2012. First name frequency by gender.
http://www.census.gov/genealogy/names/names files.html.
CRA. 2008. CRA Taulbee Survey (web site).
http://www.cra.org/resources/taulbee/.
David L.W. Hall, Daniel Jurafsky, and Christopher D.
Manning. 2008. Studying the history of ideas using
topic models. In Proceedings of Conference on Em-
pirical Methods on Natural Language Processing.
Catherine Hill, Christianne Corbett, and Andresse
St Rose. 2010. Why So Few? Women in Science,
Technology, Engineering, and Mathematics. Ameri-
can Association of University Women.
Paul De Palma. 2001. Viewpoint: Why women avoid
computer science. Commun. ACM, 44:27?30, June.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The ACL Anthology Network cor-
pus. In Proceedings of the 2009 Workshop on Text
and Citation Analysis for Scholarly Digital Libraries,
NLPIR4DL ?09, pages 54?61, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Andrew Sih and Kiisa Nishikawa. 1988. Do men
and women really differ in publication rates and con-
tentiousness? an empirical survey. Bulletin of the Eco-
logical Society of America, 69(1):pp. 15?18.
Kusum Singh, Katherine R Allen, Rebecca Scheckler,
and Lisa Darlington. 2007. Women in computer-
related majors: A critical synthesis of research and
theory from 1994 to 2005. Review of Educational Re-
search, 77(4):500?533.
Ellen Spertus. 1991. Why are there so few female com-
puter scientists? Technical report, Massachusetts In-
stitute of Technology, Cambridge, MA, USA.
Matthew R.E. Symonds, Neil J. Gemmell, Tamsin L.
Braisher, Kylie L. Gorringe, and Mark A. Elgar. 2006.
Gender differences in publication output: Towards an
unbiased metric of research performance. PLoS ONE,
1(1):e127, 12.
41
Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 17?22,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
Tradition and Modernity in 20th Century Chinese Poetry
Rob Voigt
Center for East Asian Studies
Stanford University
robvoigt@stanford.edu
Dan Jurafsky
Linguistics Department
Stanford University
jurafsky@stanford.edu
Abstract
Scholars of Chinese literature note that
China?s tumultuous literary history in the
20th century centered around the uncomfort-
able tensions between tradition and modernity.
In this corpus study, we develop and auto-
matically extract three features to show that
the classical character of Chinese poetry de-
creased across the century. We also find that
Taiwan poets constitute a surprising excep-
tion to the trend, demonstrating an unusually
strong connection to classical diction in their
work as late as the ?50s and ?60s.
1 Introduction
For virtually all of Chinese history through the fall
of the Qing Dynasty, poetry was largely written in
Classical Chinese and accessible to a small, edu-
cated fraction of the population. With the rise of the
May Fourth Movement in 1919, prominent intellec-
tuals such as Hu Shi and Lu Xun began to advocate
for and produce a fresh vernacular literature.
This upheaval of tradition has been much dis-
cussed in literary studies; Michelle Yeh calls ver-
nacular poetry ?a self-proclaimed iconoclast strug-
gling against a formidable predecessor: the heritage
of three millennia of classical poetry? (Yeh, 1991).
While some propose that the May Fourth intel-
lectuals ?abolished the classical language and all of
its literary genres? (Hockx and Smits, 2003), others
make more measured claims: Mao Chen, for exam-
ple, maintains that ?a special relationship to tradi-
tion informs all phases of cultural activity during the
May Fourth period? (Chen, 1997).
Julia Lin notes that the period following the May
Fourth Movement through 1937 saw ?the most ex-
citing and diverse experimentation in the history of
modern Chinese poetry? (Lin, 1973). Much of this
experimentation was concerned with the question
of modernity versus tradition, wherein some poets
?adapt[ed] the reality of the modern spoken lan-
guage to what they felt was the essence of the old
classical Chinese forms? (Haft, 1989).
The founding of the People?s Republic of China
in 1949 was a second major turning point in the
century, when ?the Communists in one cataclysmic
sweep [...] ruthlessly altered the course of the arts?
and poetry ?became totally subservient to the dic-
tates of the party? (Lin, 1973). With the ?physi-
cal removal of the old cultural leadership,? many of
whom fled to Taiwan, this period saw a substantial
?vacuum in literature and the arts? (McDougall and
Louie, 1997).
Post-Mao, publication restrictions gradually loos-
ened and earlier cultural journals re-entered circu-
lation. Poetry began to reclaim its audience, and a
Chinese avant-garde associated with the ?Misty Po-
ets? developed (McDougall and Louie, 1997).
However, we lack broad-scale empirical evidence
of the linguistic features that constituted the shift
from tradition to modernity. Therefore, we propose
a study that asks: To what extent were classical po-
etic forms and classical language immediately dis-
carded with the advent of vernacular poetry? What
is the status of classical language after 1949 and
amidst the Maoist era, when we might expect its to-
tal absence? Does more contemporary poetry still
draw connections to classical language?
17
2 Prior Work on Chinese Poetry in NLP
The majority of existing studies in NLP on Chinese
poetry deal exclusively with the classical language.
Jiang and Zhou (2008) explore the problem of
classical Chinese poetic couplets, and to develop a
system to generate them automatically using tech-
niques from machine translation.
Fang et al (2009) use an ontology of imagery de-
veloped by Lo (2008) to identify imagery in classical
Chinese poems, and develop a parser that is able to
extract tree structures that identify complex imagis-
tic language in the same.
More recent work develops useful resources for
understanding classical poetry. Lee (2012) develops
a corpus of classical Chinese poems that are word-
segmented and annotated with nested part-of-speech
tags that allow for different interpretations of ?word-
hood? - a non-trivial concept in considering Chinese
texts classical and modern. Lee and Kong (2012)
introduce a large-scale dependency treebank anno-
tated on a corpus of 8th-century poems.
To our knowledge, there is no existing computa-
tional work that attempts to understand the develop-
ment of modern Chinese poetry over time.
3 Data Collection
For this project, we use a corpus of modern po-
ems collected on the site ?Chinese Poetry Treasury?
(?????, www.shigeku.com) entitled the ?Se-
lected Database of Chinese Modern Poetry? (??
?????????). It is important to note that
the poems in this collection were hand-selected by
the group running the site for their canonicity, so our
data are biased towards those poems that have, in a
sense, ?stood the test of time? in the eyes of a main-
land Chinese readership.
This corpus is distributed through their site as a
collection of html documents, one page per poet,
which include brief biographical information for the
poet and a collection of their works. We use unix
command-line tools (sed, tr, iconv, grep) and basic
python scripting to process these documents into a
usable corpus with each poem as a separate, clean
file, segmented character-by-character. 1
1Scripts and further information are available here:
http://nlp.stanford.edu/robvoigt/chpoetry/
The site categorizes poets by their ?most active?
decade, from the 1920s through the 1990s, and we
extract this metadata to allow for comparisons over
time. In our analysis, however, a methodological im-
pediment arose: namely, the Cultural Revolution.
As discussed in the introduction, this tumultuous
period severely disrupted the developmental path of
modern Chinese literature. Indeed, we find in our
corpus that almost none of the poets tagged as active
in the ?50s and ?60s were mainland Chinese, but in-
stead Taiwanese poets who fled to the island at the
climax of the Chinese Civil War.
For this reason, combined with the potential nois-
iness induced by the fact that decade tags are per-
poet instead of per-poem, we manually identify Tai-
wan poets and divide our corpus into three subsets
for analysis: ?early modern? poetry in the 1920s and
?30s; ?late modern? poetry in the ?40s interrupted by
the Maoist era but resuming in the late ?70s, ?80s,
and ?90s; and ?Taiwan? poetry by Taiwan natives
and transplanted mainlanders in Taiwan post-1949.
After pre-processing, our full corpus for analysis
(denoted Eval in Table 1) contains 3,611 poems by
305 poets, with a total of 1,128,428 Chinese charac-
ters. This size is large enough for meaningful com-
putational results, but small enough to allow for sig-
nificant qualitative analysis.
We will later define metrics for evaluating the
?classicality? of individual characters and radicals,
so we process auxiliary corpora (denoted Aux in Ta-
ble 1) of classical poetry and contemporary prose.
For classical Chinese, we use a large corpus, from
the same source (www.shigeku.com), of poems from
the Tang Dynasty (618-907 AD), often considered
the greatest classical era for Chinese poetry. For
modern Chinese, we use a subset of a machine trans-
lation bi-text, comprised primarily of contemporary
newswire, legal, and other prose texts.2
Since we aim to discover the overall ?classicality?
of association for individual characters, our auxil-
iary corpora are cross-genre to exaggerate the ef-
fects ? a high ?classicality? score will indicate both
a period-specific classicality and a classical poetic
genre association.
2From the BOLT Phase 1 Evaluation training data; see
http://www.nist.gov/itl/iad/mig/bolt p1.cfm
18
Table 1: Corpus inventory.
Poems Chars Vocab
Eval Early 351 89,226 3,299
Taiwan 513 126,369 3,878
Late 2,747 912,833 4,852
Aux Classical 2,712,685 6,263
Modern 9,405,549 5,517
4 Methodology
Speak in the language of the time in which you live.
? Hu Shi, 1917
As suggested in the introduction, modern poetry
is distinguished linguistically from classical poetry
in its explicit shift to the use of vernacular language.
Classical poetry is formalized, concise, and imagis-
tic. We propose three features to operationalize this
classicality and computationally observe the shift to
a poetic vernacular across the 20th century.
Final Rhyme Classical Chinese poetry in general
has a highly regular structure, following strict metri-
cal and rhyming conventions, and most prominently
employs a highly consistent end-rhyme. We use the
CJKLIB python library3 to obtain the pronunciation
for the last character in each line of each poem. The
pronunciation of a given Chinese character may be
divided into precisely one consonant (known as an
?initial?) and one vowel (known as a ?final?).
We therefore qualify a given line as ?rhyming? if
the last character of any line within a 3-line window
shares its vowel final pronunciation, and for each
poem calculate the proportion of rhyming lines.
Character-based Probability Ratio Inspired by
the work of Underwood and Sellers (2012) in track-
ing shifts in literary diction in English poetry, we use
our auxiliary corpora of Tang Dynasty poems and
modern Chinese language text to create two simple
metrics for understanding the ?classicality? of poetic
diction.
The extreme concision of classical poetry ?fo-
cuses attention on the characters themselves? (Hin-
ton, 2010), with common classical forms containing
as few as ten or twenty characters. To analyze clas-
sical diction, for each character we aim to get a ratio
describing how classical it sounds.
3http://code.google.com/p/cjklib/
For this metric, we calculate the probability of
each character occurring in its respective corpus us-
ing add-one smoothing. We then define the score
for a given character as the difference of the char-
acter?s log likelihood of occurring in the classical
auxiliary corpus with its log likelihood of occur-
ring in the modern auxiliary corpus. Scores range
from -8 to +8, where a higher score indicates a more
?classically?-tinged character.
We find these scores match up well with intu-
ition. In the highly negative range, we find recently-
invented, conversational, and grammatical charac-
ters unique to the modern vernacular. In the highly
positive range, we find rareified literary, poetic
characters. In the range surrounding 0.0, we find
many common, persistent characters whose mean-
ings have changed little over time. Selected exam-
ples of these scores can be seen in Table 2.
Table 2: Example classicality scores for selected charac-
ters on the Character-based Probability Ratio metric.
Character Meaning Score
HIGHLY CLASSICAL
? yu To meet; to encounter 7.94
? qin A thin quilt used to cover 6.42
a corpse in a coffin
? xiao A type of bamboo flute 5.99
? liu Willow 4.68
SIMILAR ACROSS PERIODS
? ting Listen; hear 0.64
? qu? To go; towards 0.61
? zhi Directly -0.11
? shou To receive; to harvest -0.53
HIGHLY MODERN
? ni Second-person pronoun -4.49
? gou Sufficient; enough -6.02
? ne Sentence-final particle -6.67
? ta Third-person female pronoun -7.82
We calculate a score for a given poem on this met-
ric by simply taking the average of the character-
based probability ratio for each character in the
poem. These results are denoted Char in Table 4.
Radical-based Probability Ratio This metric is
fundamentally similar to the above character-based
method, but offers the potential to provide a different
kind of insight. The majority of Chinese characters
are compositional, with a semantic component and a
phonetic component.
19
We start from the intuition that contemporary
texts will be more likely to use characters that con-
tain the ? (kou, ?mouth?) radical as their seman-
tic component, because this radical is commonly
found in modern conversational particles that were
not used in ancient texts. We generalize this hypoth-
esis and consider that the use of characters with cer-
tain semantic radicals is correlated with the classi-
cality of a text.
We again use the CJKLIB python library to pro-
cess our auxiliary corpora, extracting the seman-
tic component radical from each character and cal-
culating the ratio of its probability of occurrence,
with add-one smoothing, in the auxiliary classical
and modern corpora. As above, we obtain the ratio
scores for each radical, and score each poem in our
corpus by averaging these scores for each character
in the poem.
While these scores are less immediately acces-
sible to intuition than those of the character-based
metric, the radical-based scores, with examples seen
in Table 3, demonstrate a consistency that parallels
the character-based scores.
The semantic radicals most prevalent in classical
poetry include those signifying bird, horse, valley,
mountain, ghost, dragon, and so on; classical po-
etry has a pastoral and mythological aesthetic that
is directly reflected in the distribution of its radi-
cals. Conversely, modern prose is more likely to use
semantic radicals related to work, family, money,
speech, and movement; they convey the practical re-
alism of contemporary conversational speech.
Table 3: Example classicality scores for selected seman-
tic radicals on the Radical-based Probability Ratio metric.
Radical Meaning Score
HIGHLY CLASSICAL
? gui Ghost 2.18
? shan Mountain 2.09
? chong Insect 1.43
SIMILAR ACROSS PERIODS
? nu? Female 0.01
? wen Culture; language -0.02
? sheng Life; birth -0.01
HIGHLY MODERN
? shou Hand -0.48
? yan Words; speech -0.61
? li Force; work -0.94
4.1 Diachronic Statistical Analysis
We began from the hypothesis that each of the met-
rics described above will demonstrate, broadly, that
the classical nature of Chinese poetry decreased over
the course of the 20th century. The raw statistical
counts for our features can been seen in Table 4.
Table 4: Raw feature statistics across sub-corpora.
Higher values in the AVG rows indicate a greater ?classi-
cality.? For all three features, classicality decreased over
the century, with the exception of Taiwan.
Early Taiwan Late
Rhyme AVG 0.281 0.244 0.226
STDDEV 0.193 0.169 0.152
Char AVG -0.695 -0.620 -0.882
STDDEV 0.494 0.446 0.404
Radical AVG -0.072 -0.081 -0.116
STDDEV 0.121 0.105 0.097
We calculate the presence of the ?classical? fea-
tures defined above for each subset, and compute
a binary logistic regression with the scikit-learn
python library (Pedregosa et al, 2011)4 to find cor-
relation coefficients for those features between the
?early modern? and ?late modern? subsets.
5 Results and Discussion
Several claims from the literary community are well-
supported by our results.
Logistic regression reveals a significant down-
ward trend for our features as we shift from ?early
modern? to ?late modern? poetry (R2 = 0.89), in-
dicating decreased use of end-rhyme, increased use
of modern characters, and increased prevalence of
modern semantic radicals over the course of the cen-
tury.
Though the early works use more classical char-
acters on the whole, we also observe a higher statisti-
cal variance for all metrics in the ?20s and ?30s, sup-
porting the literary hypothesis that the May Fourth
period was one of increased experimentation that
later settled into a somewhat more consistent moder-
nity.
We find, however, less support for the idea that
Chinese modern poets ?abolished the classical lan-
guage? in their work (Hockx and Smits, 2003).
4http://scikit-learn.org
20
Throughout the century we find repeated instances
of highly classical language, with individual poems
reaching a maximum character-based probability ra-
tio of 0.70 in the ?early? works, 0.76 in the ?late?
works, and 0.87 in the ?Taiwan? works; compare
these with an average score of 1.20 for the auxiliary
classical dataset overall. Considering that a score of
0.0 would indicate an equal distribution of weight
between ?classical? and ?modern? characters, it?s
clear that these 20th-century poems still contain a
substantial proportion of characters drawn from the
classical language.
Poems from Taiwan in the ?50s and ?60s offer
perhaps the most interesting results in this study.
It?s notable in the first place that poets in our cor-
pus selected as worth remembering by contempo-
rary mainland Chinese from the most authoritarian
period of Communist control are almost exclusively
from Taiwanese authors. Furthermore, the dip to-
wards modernity we see in ?40s mainland poetry was
rejected in the next decade by those mainland poets
who found themselves in Taiwan after 1949; the Tai-
wan poems bear far greater resemblance to the early
subset of our data than to the late.
This finding parallels work on this period from lit-
erary scholars. Yvonne Chang writes that in ?50s
and ?60s Taiwan, valorization of traditional Chi-
nese culture and romanticization of the early 20th-
century Nationalist period in mainland China was
heavily encouraged. In particular, the concept of ??
??? (chun wenxue, ?pure literature?) gained pop-
ularity in Taiwan?s literary circles, and with it came
a resurgence of more traditional diction and forms
(Chang, 1993).
Fangming Chen further describes poetry in post-
war Taiwan as a political outlet for the Kuomintang,
the sole ruling party of Taiwan at the time, as they
?forcefully brought Chinese nationalism? to the is-
land. Poets who demonstrated a deep ?nostalgia? for
the ?motherland? of mainland China were far more
likely to be rewarded with cultural resources such as
grants and publication money, being that the govern-
ment had a vested interest in keeping the public on
board with plans to ?reclaim the homeland? (Chen,
2007). It is fascinating, then, that we observe this
tendency computationally with a return to the levels
of classicality seen in ?20s and ?30s mainland China.
In spite of these encouraging results, this work has
several limitations. Our reliance on decade-based la-
bels applied to poets, rather than poems, introduces
significant noise. The outlier behavior observed in
Taiwan poets is indicative of the need for a better
understanding of regional differences, and a com-
parison with a similarly isolated Sinophone region
such as Hong Kong would be productive in this re-
gard. In both cases, information extraction tech-
niques might allow us to tag poems with their date of
publication and poets with their hometown, facilitat-
ing fine-grained analysis, as would a broader dataset
that goes beyond the modern canon.
6 Conclusion
In this paper, we computationally operationalized
three features that successfully track the declining
influence of classical poetic style and language in
20th-century Chinese poetry. We identified Taiwan
poets as an outlier in the dataset, and found empiri-
cal evidence for the political externalities of the ?50s
and ?60s that called for a return to a nostalgic clas-
sicism. In this way, this work presents a promising
first step to a thorough empirical understanding of
the development of modern Chinese poetry.
Acknowledgments
Thanks to three anonymous reviewers for detailed
and insightful comments. This research was sup-
ported in part by the Foreign Language and Area
Studies Fellowships, United States Department of
Education.
References
Sung-sheng Yvonne Chang. 1993. Modernism and the
Nativist Resistance. Duke University Press: Durham
and London.
Fangming Chen. 2007. Postmodern or Postcolonial? An
Inquiry into Postwar Taiwanese Literary History. In
Writing Taiwan, David Der-wei Wang and Carlos Ro-
jas, eds. Duke University Press, Durham and London.
Mao Chen. 1997. Between Tradition and Change. Uni-
versity Press of America, Lanham, MA.
Alex Chengyu Fang, Fengju Lo, and Cheuk Kit Chinn.
2009. Adapting NLP and Corpus Analysis Techniques
to Structured Imagery Analysis in Classical Chinese
Poetry. In Workshop Adaptation of Language Re-
sources and Technology to New Domains, Borovets,
Bulgaria.
21
Lloyd Haft. 1989. A Selective Guide to Chinese Litera-
ture: 1900-1949. E.J. Brill, New York.
David Hinton, ed. 2010. Classical Chinese Poetry: An
Anthology. Farrar, Straus, and Giroux.
Michel Hockx and Ivo Smits, eds. 2003. Reading East
Asian Writing: The Limits of Literary Theory. Rout-
ledgeCurzon, London and New York.
Long Jiang and Ming Zhou. 2008. Generating Chinese
Couplets using a Statistical MT Approach. In COL-
ING.
John Lee. 2012. A Classical Chinese Corpus with
Nested Part-of-Speech Tags. In Proceedings of the 6th
EACLWorkshop on Language Technology for Cultural
Heritage, Social Sciences, and Humanities, Avignon,
France.
John Lee and Yin Hei Kong. 2012. A Dependency Tree-
bank of Classical Chinese Poems. In NAACL-HLT,
Montreal, Canada.
Julia Lin. 1973. Modern Chinese Poetry: An Introduc-
tion. University of Washington Press, Seattle, WA.
Fengju Lo. 2008. The Research of Building a Semantic
Cetegory System Based on the Language Characteris-
tic of Chinese Poetry. In Proceedings of the 9th Cross-
Strait Symposium on Library Information Science.
Lu Zhiwei. 1984. Five Lectures on Chinese Poetry. Joint
Publishing Co., Hong Kong.
Bonnie McDougall and Kam Louie, eds. 1997. The Lit-
erature of China in the Twentieth Century. Hurst and
Company, London.
Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, Jake Vanderplas, Alexandre Passos,
David Cournapeau, Matthieu Brucher, Matthieu Per-
rot, and E?douard Duchesnay. 2011. Scikit-learn: Ma-
chine Learning in Python. Journal of Machine Learn-
ing Research. 12:2825-2830
Ted Underwood and Jordan Sellers. 2012.
The Emergence of Literary Diction. The
Journal of Digital Humanities, 1(2).
http://journalofdigitalhumanities.org/1-2/the-
emergence-of-literary-diction-by-ted-underwood-
and-jordan-sellers/
Michelle Yeh. 1991. Modern Chinese Poetry: Theory
and Practice since 1917. Yale University Press, New
Haven, CT.
22
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 320?328,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Positive Diversity Tuning
for Machine Translation System Combination
Daniel Cer, Christopher D. Manning and Daniel Jurafsky
Stanford University
Stanford, CA 94305, USA
{danielcer,manning,jurafsky}@stanford.edu
Abstract
We present Positive Diversity Tuning, a
newmethod for tuningmachine translation
models specifically for improved perfor-
mance during system combination. Sys-
tem combination gains are often limited
by the fact that the translations produced
by the different component systems are
too similar to each other. We propose a
method for reducing excess cross-system
similarity by optimizing a joint objective
that simultaneously rewards models for
producing translations that are similar to
reference translations, while also punish-
ing them for translations that are too sim-
ilar to those produced by other systems.
The formulation of the Positive Diversity
objective is easy to implement and allows
for its quick integration with most machine
translation tuning pipelines. We find that
individual systems tuned on the same data
to Positive Diversity can be even more
diverse than systems built using different
data sets, while still obtaining good BLEU
scores. When these individual systems are
used together for system combination, our
approach allows for significant gains of 0.8
BLEU even when the combination is per-
formed using a small number of otherwise
identical individual systems.
1 Introduction
The best performing machine translation sys-
tems are typically not individual decoders but
rather are ensembles of two ormore systemswhose
output is then merged using system combination
algorithms. Since combining multiple distinct
equally good translation systems reliably produces
gains over any one of the systems in isolation, it is
widely used in situations where high quality is es-
sential.
Exploiting system combination brings signifi-
cant cost: Macherey and Och (2007) showed that
successful system combination requires the con-
struction of multiple systems that are simultane-
ously diverse and well-performing. If the systems
are not distinct enough, they will bring very lit-
tle value during system combination. However,
if some of the systems produce diverse transla-
tions but achieve lower overall translation quality,
their contributions risk being ignored during sys-
tem combination.
Prior work has approached the need for diverse
systems by using different system architectures,
model components, system build parameters, de-
coder hyperparameters, as well as data selection
and weighting (Macherey and Och, 2007; DeNero
et al, 2010; Xiao et al, 2013). However, during
tuning, each individual system is still just trained to
maximize its own isolated performance on a tune
set, or at best an error-driven reweighting of the
tune set, without explicitly taking into account the
diversity of the resulting translations. Such tuning
does not encourage systems to rigorously explore
model variations that achieve both good translation
quality and diversity with respect to the other sys-
tems. It is reasonable to suspect that this results in
individual systems that under exploit the amount
of diversity possible, given the characteristics of
the individual systems.
For better system combination, we propose
building individual systems to attempt to simulta-
neously maximize the overall quality of the indi-
vidual systems and the amount of diversity across
systems. We operationalize this problem formu-
lation by devising a new heuristic measure called
Positive Diversity that estimates the potential use-
fulness of individual systems during system com-
bination. We find that optimizing systems toward
Positive Diversity leads to significant performance
gains during system combination even when the
combination is performed using a small number of
320
otherwise identical individual translation systems.
The remainder of this paper is organized as fol-
lows. Section 2 and 3 briefly review the tuning
of individual machine translation systems and how
system combination merges the output of multiple
systems into an improved combined translation.
Section 4 introduces our Positive Diversity mea-
sure. Section 5 introduces an algorithm for training
a collection of translation systems toward Positive
Diversity. Experiments are presented in sections 6
and 7. Sections 8 and 9 conclude with discussions
of prior work and directions for future research.
2 Tuning Individual Translation Systems
Machine translation systems are tuned toward
somemeasure of the correctness of the translations
produced by the system according to one or more
manually translated references. As shown in equa-
tion (1), this can be written as finding parameter
values? that produce translations sys? that in turn
achieve a high score on some correctness measure:
argmax
?
Correctness(ref[],sys?) (1)
The correctness measure that systems are typi-
cally tuned toward is BLEU (Papineni et al, 2002),
which measures the fraction of the n-grams that
are both present in the reference translations and
the translations produced by a system. The BLEU
score is computed as the geometric mean of the
resulting n-gram precisions scaled by a brevity
penalty.
The most widely used machine translation
tuning algorithm, minimum error rate training
(MERT) (Och, 2003), attempts to maximize the
correctness objective directly. Popular alternatives
such as pairwise ranking objective (PRO) (Hop-
kins and May, 2011), MIRA (Chiang et al, 2008),
and RAMPION (Gimpel and Smith, 2012) use sur-
rogate optimization objectives that indirectly at-
tempt to maximize the correctness function by us-
ing it to select targets for training discriminative
classification models. In practice, either optimiz-
ing correctness directly or optimizing a surrogate
objective that uses correctness to choose optimiza-
tion targets results in roughly equivalent transla-
tion performance (Cherry and Foster, 2012).
Even when individual systems are being built
to be used in a larger combined system, they are
still usually tuned to maximize their isolated in-
dividual system performance rather than to maxi-
mize the potential usefulness of their contribution
during system combination.1 To our knowledge,
no effort has been made to explicitly tune toward
criteria that attempts to simultaneously maximize
the translation quality of individual systems and
their mutual diversity. This is unfortunate since the
most valuable component systems for system com-
bination should not only obtain good translation
performance, but also produce translations that are
different from those produced by other systems.
3 System Combination
Similar to speech recognition?s Recognizer Out-
put Voting Error Reduction (ROVER) algorithm
(Fiscus, 1997), machine translation system com-
bination typically operates by aligning the transla-
tions produced by two or more individual transla-
tion systems and then using the alignments to con-
struct a search space that allows new translations to
be pieced together by picking and choosing parts
of the material from the original translations (Ban-
galore et al, 2001; Matusov et al, 2006; Rosti et
al., 2007a; Rosti et al, 2007b; Karakos et al, 2008;
Heafield and Lavie, 2010a).2 The alignment of the
individual system translations can be performed
using alignment driven evaluation metrics such as
invWER, TERp, METEOR (Leusch et al, 2003;
Snover et al, 2009; Denkowski and Lavie, 2011).
The piecewise selection of material from the orig-
inal translations is performed using the combina-
tion model?s scoring features such as n-gram lan-
guage models, confidence models over the indi-
vidual systems, and consensus features that score a
combined translation using n-gramsmatches to the
individual system translations (Rosti et al, 2007b;
Zhao and He, 2009; Heafield and Lavie, 2010b).
Both system confidence model features and n-
gram consensus features score contributions based
in part on how confident the system combination
model is in each individual machine translation
system. This means that little or no gains will typ-
ically be seen when combining a good system with
poor performing systems even if the systems col-
1The exception being Xiao et al (2013)?s work using
boosting for error-driven reweighting of the tuning set
2Other system combination techniques exist such as can-
didate selection systems, whereby the combination model at-
tempts to find the best single candidate produced by one of the
translation engines (Paul et al, 2005; Nomoto, 2004; Zwarts
and Dras, 2008), decoder chaining (Aikawa and Ruopp,
2009), re-decoding informed by the decoding paths taken
by other systems (Huang and Papineni, 2007), and decoding
model combination (DeNero et al, 2010).
321
Input : systems [], tune(), source, refs [], ?, EvalMetric (), SimMetric ()
Output: models []
// start with an empty set of translations from prior iterations
other_sys []? []
for i? 1 to len(systems []) do
// new Positive Diversity measure using prior translations
PD?,i()? new PD(?, EvalMetric(), SimMetric(), refs [], other_sys [])
// tune a new model to fit PD?,i
// e.g., using MERT, PRO, MIRA, RAMPION, etc.
models [i]? tune(systems [i], source, PD?,i())
// Save translations from tuned modeli for use during
// the diversity computation for subsequent systems
push(other_sys [], translate(systems [i],models [i], source))
end
returnmodels []
Algorithm 1: Positive Diversity Tuning (PDT)
lectively produce very diverse translations.3
The requirement that the systems used for sys-
tem combination be both of high quality and di-
verse can be and often is met by building several
different systems using different system architec-
tures, model components or tuning data. However,
as will be shown in the next few sections, by ex-
plicitly optimizing an objective that targets both
translation quality and diversity, it is possible to
obtain meaningful system combination gains even
using a single system architecture with identical
model components and the same tuning set.
4 Positive Diversity
We propose Positive Diversity as a heuristic
measurement of the value of potential contribu-
tions from an individual system to system combi-
nation. As given in equation (2), PositiveDiversity
is defined as the correctness of the translations pro-
duced by a systemminus a penalty term that scores
how similar the systems translations are with those
produced by other systems:
PD? = ? Correctness(ref[],sys?)?
(1? ?) Similarity(other_sys[],sys?)
(2)
The hyperparameter ? explicitly trades-off the
preference for a well performing individual sys-
3The machine learning theory behind boosting suggests
that it should be possible to combine a very large number of
poor performing systems into a single good system. However,
for machine translation, using a very large number of individ-
ual systems brings with it difficult computational challenges.
tem with system combination diversity. Higher
? values result in a Positive Diversity metric that
mostly favors good quality translations. However,
even for large ? values, if two translations are of
approximately the same quality, the Positive Di-
versity metric will prefer the one that is the most
diverse given the translations being produced by
other systems.
The Correctness() and Similarity()
measures are any function that can score transla-
tions from a single system against other transla-
tions. This includes traditionalmachine translation
evaluation metrics (e.g, BLEU, TER, METEOR)
as well as any other measure of textual similarity.
For the remainder of this paper, we use BLEU to
measure both correctness and the similarity of the
translations produced by the individual systems.
When tuning individual translation systems toward
Positive Diversity, our task is then to maximize
equation (3) rather than equation (1):
argmax? ? BLEU(ref[],sys)?
(1? ?) BLEU(other_sys[],sys) (3)
Since this learning objective is simply the differ-
ence between two BLEU scores, it should be easy
to integrate into most existing machine translation
tuning pipelines that are already designed to op-
timize performance on translation evaluation met-
rics.
322
PDT Individual System Diversity
System \ Iteration 1 2 3 4 5 6 7 8 9 10
? = 0.95 36.6 32.0 19.0 13.6 11.9 8.2 15.9 8.7 7.3 2.3
? = 0.97 32.9 21.7 17.7 10.4 2.7 7.4 2.3 7.3 2.1 2.9
? = 0.99 23.9 13.1 7.9 2.3 3.2 2.6 2.2 1.5 3.4 0.7
Table 1: Diversity scores for PDT individual systems onBOLT dev12 dev. Individual systems are tuned to
Positive Diversity on GALE dev10 web tune. A system?s diversity score is measured as its 1.0?BLEU
score on the translations produced by PDT systems from earlier iterations. Higher scores mean more
diversity.
Diversity of Baseline System vs. Individual PDT Systems Available at Iteration i
PDT Systems \ Iteration 0 1 2 3 4 5 6 7 8 9 10
? = 0.95 27.3 20.4 16.8 14.9 12.8 11.4 9.4 8.6 8.3 8.1 7.9
? = 0.97 28.4 21.3 15.8 14.7 13.3 13.0 12.5 12.2 10.3 10.0 9.7
? = 0.99 27.5 22.6 18.5 17.1 16.8 15.9 15.4 14.6 14.3 13.5 13.4
Table 2: Diversity scores of a baseline system tuned to BOLT dev12 tune, a different tuning set than what
was used for the PDT individual systems. The baseline system diversity is scored against all of the PDT
individual systems available at iteration i for a given ? value and over translations of BOLT dev12 dev.
5 Tuning to Positive Diversity
To tune a collection of machine transla-
tion systems using Positive Diversity, we pro-
pose a staged process, whereby systems are
tuned one-by-one to maximize equation (2)
using the translations produced by previously
trained systems to compute the diversity term,
Similarity(other_sys[], sys?).
As shown in Algorithm 1, Positive Diversity
Tuning (PDT) takes as input: a list of machine
translation systems, systems[]; a tuning proce-
dure for training individual systems, tune(); a
tuning data set with source and reference trans-
lations, source and refs; a hyperparameter ?
to adjust the trade-off between fitting the refer-
ence translations and diversity between the sys-
tems; and metrics to measure correctness and
cross-system similarity, Correctness() and
Similarity().
The list of systems can contain any translation
system that can be parameterized using tune().
This can be a heterogeneous collection of substan-
tially different systems (e.g., phrase-based, hier-
archical, syntactic, or tunable hybrid systems) or
even multiple copies of a single machine transla-
tion system. In all cases, systems later in the list
will be trained to produce translations that both fit
the references and are encouraged to be distinct
from the systems earlier in the list.
During each iteration, the system constructs a
new Positive Diversity measure PD?,i using the
translations produced during prior iterations of
training. This PD?,i measure is then given to
tune() as the the training criteria for modeli
of systemi. The function tune() is any al-
gorithm that allows a translation system?s perfor-
mance to be fit to an evaluation metric. This
includes both minimum error rate training algo-
rithms (MERT) that attempt to directly optimize a
system?s performance on a metric, as well as other
techniques such as Pairwaise Ranking Objective
(PRO),MIRA, and RAMPION that optimize a sur-
rogate loss based on the preferences of an evalua-
tion metric.
After training a model for each system, the re-
sulting model-system pairs can be combined using
any arbitrary system combination strategy.
6 Experiments
Experiments are performed using a single
phrase-based Chinese-to-English translation sys-
tem, built with the Stanford Phrasal machine trans-
lation toolkit (Cer et al, 2010). The system was
built using all of the parallel data available for
Phase 2 of the DARPA BOLT program. The Chi-
nese data was segmented to the Chinese Tree-
Bank (CTB) standard using a maximum match
word segmenter, trained on the output of a CRF
segmenter (Xiang et al, 2013). The bitext was
word aligned using the Berkeley aligner (Liang et
al., 2006). Standard phrase-pair extraction heuris-
323
BLEU scores from individual systems
tuned during iteration i of PDT
PDT System 0 1 2 3 4 5 6 7 8 9 10
? = 0.95 16.2 16.0 15.7 15.9 16.1 16.1 15.9 15.4 16.1 15.9 16.2
? = 0.97 16.4 15.8 15.8 15.9 16.0 16.2 16.1 16.2 16.2 16.4 16.1
? = 0.99 16.3 16.1 16.2 15.9 16.3 16.4 16.4 16.3 16.4 16.5 16.3
Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE
dev10 web tune. Scores report individual system performance before system combination.
tics were used to extract a phrase-table over word
alignments symmetrized using grow-diag (Koehn
et al, 2003). We made use of a hierarchical re-
ordering model (Galley and Manning, 2008) as
well as a 5-gram languagemodel trained on the tar-
get side of the bi-text and smoothed usingmodified
Kneeser-Ney (Chen and Goodman, 1996).
Individual PDT systems were tuned on the
GALE dev10 web tune set using online-PRO
(Green et al, 2013; Hopkins and May, 2011)
to the Positive Diversity Tuning criterion.4 The
Multi-EngineMachine Translation (MEMT) pack-
age was used for system combination (Heafield
and Lavie, 2010a). We used BOLT dev12 dev as
a development test set to explore different ? pa-
rameterizations of the Positive Diversity criteria.
7 Results
Table 1 illustrates the amount of diversity
achieved by individual PDT systems on the BOLT
dev12 dev evaluation set for ? values 0.95, 0.97,
and 0.99.5 Using different tuning sets is one of the
common strategies for producing diverse compo-
nent systems for system combination. Thus, as a
baseline, Table 2 gives the diversity of a system
tuned to BLEU using a different tuning set, BOLT
dev12 tune, with respect to the PDT systems avail-
able at each iteration. As in Table 1, the diver-
sity computation is performed using translations of
BOLT dev12 dev.
Like the cross-system diversity term in the for-
mulation of Positive Diversity using BLEU in
4Preliminary experiments performed using MERT to train
the individual systems produced similar results to those seen
here. However, we switched to online-PRO since it dramat-
ically reduced the amount time required to train each indi-
vidual system. We expect similar results when using other
tuning algorithms for the individual systems, such as MIRA
or RAMPION.
5Due to time constraints, wewere not able to try additional
? values. Given that our results suggest the lowest ? value
from the ones we tried works best (i.e., ? = 0.95), it would
be worth trying additional smaller ? values such as 0.90
equation (3), we measure the diversity of trans-
lations produced by an individual system as the
negative BLEU score of the translations with re-
spect to the translations from systems built during
prior iterations. For clarity of presentation, these
diversity scores are reported as 1.0?BLEU. Using
1.0?BLEU to score cross-system diversity, means
that the reported numbers can be roughly inter-
preted as the fraction of n-grams from the individ-
ual systems built during iteration i that have not
been previously produced by other systems built
during any iteration < i.6
In our experiments, we find that for ? ? 0.97,
during the first three iterations of PDT, there is
more diversity among the PDT systems tuned on a
single data set (GALE dev10 web tune) than there
is between systems tuned on different datasets
(BOLT dev12 tune vs. GALE dev10 wb tune). This
is significant since using different tuning sets is a
common strategy for increasing diversity during
system combination. These results suggest PDT
is better at producing additional diversity than us-
ing different tuning sets. The PDT systems also
achieve good coverage of the n-grams present in
the baseline system that was tuned using different
data. At iteration 10 and using ? = 0.95, the base-
line systems receive a diversity score of only 7.9%
when measured against the PDT systems.7
As PDT progresses, it becomes more difficult to
tune systems to produce high quality translations
that are substantially different from those already
being produced by other systems. This is seen in
the per iteration diversity scores, whereby during
iteration 5, the individual PDT translation systems
have a 1.0?BLEU diversity score with prior sys-
tems ranging from 11.9%, when using an ? value
6This intuitive interpretation assumes a brevity penalty
that is approximately 1.0.
7For this diversity score, the brevity penalty is 1.0, mean-
ing the diversity score is based purely on the n-grams present
in the baseline system that are not present in translations pro-
duced by one or more of the PDT systems
324
Figure 1: System combination BLEU score achieved using Positive Diversity Tuning with the ? values
0.95, 0.97, and 0.99. Four iterations of PDT with ? = 0.95 results in a 0.8 BLEU gain over the initial
BLEU tuned system. We only examine combinations of up to 6 systems (i.e., iterations 0-5), as the time
required to tune MEMT increases dramatically as additional systems are added.
of 0.95, to 3.2% when using an ? value of 0.99.
A diversity score of 3.2% when using ? = 0.99
suggests that by iteration 5, very high ? values
put insufficient pressure on learning to find mod-
els that produce diverse translations. When using
an ? of 0.95, a sizable amount of diversity still ex-
ists across the systems translations all the way to
iteration 7. By iteration 10, only a small amount
of additional diversity is contributed by each addi-
tional system for all of the alpha values (< 3%).8
Table 3 shows the BLEU scores obtained on the
BOLT dev12 dev evaluation set by the individual
systems tuned during each iteration of PDT. The
0th iteration for each ? value has an empty set of
translations for the diversity term. This means the
resulting systems are effectively tuned to just max-
imize BLEU. Differences in system performance
during this iteration are only due to differences in
the random seeds used during training. Starting at
iteration 1, the individual systems are optimized to
produce translations that both score well on BLEU
8We speculate that if heterogeneous translation systems
were used with PDT, it could be possible to run with higher ?
values and still obtain diverse translations after a large number
of PDT iterations
and are diverse from the systems produced dur-
ing prior iterations. It is interesting to note that
the systems trained during these subsequent itera-
tions obtain BLEU scores that are usually competi-
tive with those obtained by the iteration 0 systems.
Taken together with the diversity scores in Table
1, this strongly suggests that PDT is succeeding
at increasing diversity while still producing high
quality individual translation systems.
Figure 1 graphs the system combination BLEU
score achieved by using varying numbers of Pos-
itive Diversity Tuned translation systems and dif-
ferent ? values to trade-off translation quality with
translation diversity. After running 4 iterations of
PDT, the best configuration, ? = 0.95, achieves a
BLEU score that is 0.8 BLEU higher than the cor-
responding BLEU trained iteration 0 system.9
From the graph, it appears that PDT perfor-
mance initially increases as additional systems are
added to the system combination and then later
plateaus or even drops after too many systems are
included. The combinations using PDT systems
9Recall that the iteration 0 system is effectively just tuned
to maximize BLEU since we have an empty set of translations
from other systems that are used to compute diversity
325
built with higher ? values reach the point of di-
minishing returns faster than combinations using
systems built with lower alpha values. For in-
stance, ? = 0.99 plateaus on iteration 2, while
? = 0.95 peaks on iteration 4. It might be pos-
sible to identify the point at which additional sys-
tems will likely not be useful by using the diversity
scores in Table 1. Scoring about 10% or less on
the 1?BLEUdiversitymeasure, with respect to the
other systems being used within the system combi-
nation, seems to suggest the individual system will
not be very helpful to add into the combination.
8 Related Work
While the idea of encouraging diversity in indi-
vidual systems that will be used for system combi-
nation has been proven effective in speech recogni-
tion and document summarization (Hinton, 2002;
Breslin and Gales, 2007; Carbonell and Goldstein,
1998; Goldstein et al, 2000), there has only been
a modest amount of prior work exploring such
approaches for machine translation. Prior work
within machine translation has investigated adapt-
ing machine learning techniques for building en-
sembles of classifiers to translation system tuning,
encouraging diversity by varying both the hyper-
parameters and the data used to build the individual
systems, and chaining together individual transla-
tion systems.
Xiao et al (2013) explores using boosting to
train an ensemble of machine translation systems.
Following the standard Adaboost algorithm, each
system was trained in sequence on an error-driven
reweighting of the tuning set that focuses learning
on the material that is the most problematic for the
current ensemble. They found that using a single
system to tune a large number of decoding mod-
els to different Adaboost guided weightings of the
tuning data results in significant gains during sys-
tem combination.
Macherey and Och (2007) investigated system
combination using automatic generation of diverse
individual systems. They programmatically gener-
ated variations of systems using different build and
decoder hyperparameters such as choice of word-
alignment algorithm, distortion limit, variations of
model feature function weights, and the set of lan-
guage models used. Then, in a process similar to
forward feature selection, they constructed a com-
bined system by iteratively adding the individual
automatically generated system that produced the
largest increase in quality when used in conjunc-
tion with the systems already selected for the com-
bined system. They also explored producing varia-
tion by using different samplings of the the training
data. The individual and combined systems pro-
duced by sampling the training data were inferior
to systems that used all of the available data. How-
ever, the experiments facilitated insightful analysis
on what properties an individual system must have
in order to be useful during system combination.
They found that in order to be useful within a com-
bination, individual systems need to produce trans-
lations of similar quality to other individual sys-
tems within the system combination while also be-
ing as uncorrelated as possible from the other sys-
tems. The Positive Diversity Tuning method in-
troduced in our work is an explicit attempt to build
individual translation systems that meet this crite-
ria, while being less computationally demanding
than the diversity generating techniques explored
by Macherey and Och (2007).
Aikawa and Ruopp (2009) investigated build-
ing machine translations systems specifically for
use in sequential combination with other systems.
They constructed chains of systems whereby the
output of one decoder is feed as input to the next
decoder in the pipeline. The downstream systems
are built and tuned to correct errors produced by
the preceding system. In this approach, the down-
stream decoder acts as a machine learning based
post editing system.
9 Conclusion
We have presented Positive Diversity as a new
way of jointly measuring the quality and diversity
of the contribution of individual machine transla-
tion systems to system combination. This method
heuristically assesses the value of individual trans-
lation systems by measuring their similarity to the
reference translations as well as their dissimilarity
from the other systems being combined. We op-
erationalize this metric by reusing existing tech-
niques from machine translation evaluation to as-
sess translation quality and the degree of similar-
ity between systems. We also give a straightfor-
ward algorithm for training a collection of individ-
ual systems to optimize Positive Diversity. Our
experimental results suggest that tuning to Positive
Diversity leads to improved cross-system diversity
and system combination performance even when
combining otherwise identical machine translation
326
systems.
The Positive Diversity Tuning method explored
in this work can be used to tune individual systems
for any ensemble in which individual models can
be fit to multiple extrinsic loss functions. Since
Hall et al (2011) demonstrated the general purpose
application of multiple extrinsic loss functions to
training structured prediction models, Positive Di-
versity Tuning could be broadly useful within nat-
ural language processing and for other machine
learning tasks.
In future work within machine translation, it
may prove fruitful to examine more sophisticated
measures of dissimilarity. For example, one could
imagine a metric that punishes instances of simi-
lar material in proportion to some measure of the
expected diversity of the material. It might also be
useful to explore joint rather than sequential train-
ing of the individual translation systems.
Acknowledgments
We thank the reviewers and the members of the Stan-
ford NLP group for their helpful comments and sugges-
tions. This work was supported by the Defense Advanced
Research Projects Agency (DARPA) Broad Operational Lan-
guage Translation (BOLT) program through IBM and a fel-
lowship to one of the authors from the Center for Advanced
Study in the Behavioral Sciences. Any opinions, findings,
and conclusions or recommendations expressed in this mate-
rial are those of the author(s) and do not necessarily reflect
the view of DARPA or the US government.
References
TakakoAikawa andAchimRuopp. 2009. Chained sys-
tem: A linear combination of different types of sta-
tistical machine translation systems. In Proceedings
of MT Summit XII.
S. Bangalore, G. Bordel, and Giuseppe Riccardi. 2001.
Computing consensus translation from multiple ma-
chine translation systems. In ASRU.
C. Breslin and M. J F Gales. 2007. Complementary
system generation using directed decision trees. In
ICASSP.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In SIGIR.
Daniel Cer, Michel Galley, Daniel Jurafsky, and
Christopher D. Manning. 2010. Phrasal: A statis-
tical machine translation toolkit for Exploring new
model features. In NAACL/HLT.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In ACL.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL/HLT.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
John DeNero, Shankar Kumar, Ciprian Chelba, and
Franz Och. 2010. Model combination for machine
translation. In NAACL/HLT.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: AutomaticMetric for Reliable Optimization and
Evaluation of Machine Translation Systems. In Pro-
ceedings of the EMNLP 2011 Workshop on Statisti-
cal Machine Translation.
J.G. Fiscus. 1997. A post-processing system to yield
reduced word error rates: Recognizer output voting
error reduction (ROVER). In ASRU.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
NAACL/HLT.
J. Goldstein, V. Mittal, J. Carbonell, and
M. Kantrowitz. 2000. Multi-document summa-
rization by sentence extraction. In ANLP/NAACL
Workshop on Automatic Summarization.
Spence Green, Sida Wang, Daniel Cer, and Christo-
pher D. Manning. 2013. Fast and adaptive online
training of feature-rich translation models. In (to ap-
pear) ACL.
Keith Hall, Ryan McDonald, and Slav Petrov. 2011.
Training structured prediction models with extrinsic
loss functions. In Domain Adaptation Workshop at
NIPS.
Kenneth Heafield and Alon Lavie. 2010a. CMUmulti-
enginemachine translation forWMT2010. InWMT.
Kenneth Heafield and Alon Lavie. 2010b. Voting on n-
grams for machine translation system combination.
In AMTA.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Comput., 14(8):1771?1800, August.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP.
Fei Huang and Kishore Papineni. 2007. Hierarchi-
cal system combination for machine translation. In
EMNLP-CoNLL.
327
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
ACL/HLT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2003. A novel string-to-string distancemeasure with
applications to machine translation evaluation. In
MT Summit.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In NAACL/HLT.
Wolfgang Macherey and Franz J. Och. 2007. An
empirical study on computing consensus transla-
tions from multiple machine translation systems. In
EMNLP/CoNLL.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In EMNLP.
Tadashi Nomoto. 2004. Multi-engine machine transla-
tion with voted language model. In ACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL.
Michael Paul, Takao Doi, Youngsook Hwang, Kenji
Imamura, Hideo Okuma, and Eiichiro Sumita. 2005.
Nobody is perfect: ATR?s hybrid approach to spoken
language translation. In IWSLT.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007a. Combining outputs from multiple ma-
chine translation systems. In NAACL/HLT.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system
combination for machine translation. In ACL.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER?: exploring different human judgments with
a tunable MT metric. InWMT.
Bing Xiang, Xiaoqiang Luo, and Bowen Zhou. 2013.
Enlisting the ghost: Modeling empty categories for
machine translation. In ACL.
Tong Xiao, Jingbo Zhu, and Tongran Liu. 2013. Bag-
ging and boosting statistical machine translation sys-
tems. Artif. Intell., 195:496?527, February.
Yong Zhao and Xiaodong He. 2009. Using n-gram
based features for machine translation system com-
bination. In NAACL/HLT.
Simon Zwarts and Mark Dras. 2008. Choosing the
right translation: A syntactically informed classifi-
cation approach. In CoLING.
328
