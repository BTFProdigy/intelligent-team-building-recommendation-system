Coling 2010: Poster Volume, pages 1507?1514,
Beijing, August 2010
Predicting Discourse Connectives for Implicit Discourse Relation
Recognition
Zhi-Min Zhou and Yu Xu
East China Normal University
51091201052@ecnu.cn
Zheng-Yu Niu
Toshiba China R&D Center
zhengyu.niu@gmail.com
Man Lan and Jian Su
Institute for Infocomm Research
sujian@i2r.a-star.edu.sg
Chew Lim Tan
National University of Singapore
tancl@comp.nus.edu.sg
Abstract
Existing works indicate that the absence
of explicit discourse connectives makes
it difficult to recognize implicit discourse
relations. In this paper we attempt to
overcome this difficulty for implicit rela-
tion recognition by automatically insert-
ing discourse connectives between argu-
ments with the use of a language model.
Then we propose two algorithms to lever-
age the information of these predicted
connectives. One is to use these pre-
dicted implicit connectives as additional
features in a supervised model. The other
is to perform implicit relation recognition
based only on these predicted connectives.
Results on Penn Discourse Treebank 2.0
show that predicted discourse connectives
help implicit relation recognition and the
first algorithm can achieve an absolute av-
erage f-score improvement of 3% over a
state of the art baseline system.
1 Introduction
Discourse relation analysis is to automatically
identify discourse relations (e.g., explanation re-
lation) that hold between arbitrary spans of text.
This analysis may be a part of many natural lan-
guage processing systems, e.g., text summariza-
tion system, question answering system. If there
are discourse connectives between textual units
to explicitly mark their relations, the recognition
task on these texts is defined as explicit discourse
relation recognition. Otherwise it is defined as im-
plicit discourse relation recognition.
Previous study indicates that the presence of
discourse connectives between textual units can
greatly help relation recognition. In Penn Dis-
course Treebank (PDTB) corpus (Prasad et al,
2008), the most general senses, i.e., Comparison
(Comp.), Contingency (Cont.), Temporal (Temp.)
and Expansion (Exp.), can be disambiguated in
explicit relations with more than 90% f-scores
based only on the discourse connectives explicitly
used to signal the relation (Pitler and Nenkova.,
2009b).
However, for implicit relations, there are no
connectives to explicitly mark the relations, which
makes the recognition task quite difficult. Some of
existing works attempt to perform relation recog-
nition without hand-annotated corpora (Marcu
and Echihabi, 2002), (Sporleder and Lascarides,
2008) and (Blair-Goldensohn, 2007). They use
unambiguous patterns such as [Arg1, but Arg2]
to create synthetic examples of implicit relations
and then use [Arg1, Arg2] as an training example
of an implicit relation. Another research line is
to exploit various linguistically informed features
under the framework of supervised models, (Pitler
et al, 2009a) and (Lin et al, 2009), e.g., polarity
features, semantic classes, tense, production rules
of parse trees of arguments, etc.
Our study on PDTB test data shows that the av-
erage f-score for the most general 4 senses can
reach 91.8% when we simply mapped the ground
truth implicit connective of each test instance to
its most frequent sense. It indicates the impor-
tance of connective information for implicit rela-
tion recognition. However, so far there is no previ-
ous study attempting to use such kind of connec-
tive information for implicit relation. One possi-
1507
ble reason is that implicit connectives do not ex-
ist in unannotated real texts. Another evidence
of the importance of connectives for implicit re-
lations is shown in PDTB annotation. The PDTB
annotation consists of inserting a connective ex-
pression that best conveys the inferred relation by
the readers. Connectives inserted in this way to
express inferred relations are called implicit con-
nectives, which do not exist in real texts. These
evidences inspire us to consider two interesting re-
search questions:
(1) Can we automatically predict implicit connec-
tives between arguments?
(2) How to use the predicted implicit connectives
to build an automatic discourse relation analysis
system?
In this paper we address these two questions as
follows: (1) We insert appropriate discourse con-
nectives between two textual units with the use of
a language model. Here we train the language
model on large amount of raw corpora without
the use of any hand-annotated data. (2) Then we
present two algorithms to use these predicted con-
nectives for implicit relation recognition. One is
to use these connectives as additional features in a
supervised model. The other is to perform relation
recognition based only on these connectives.
We performed evaluation of the two algorithms
and a baseline system on PDTB 2.0 corpus. Ex-
perimental results showed that using predicted
discourse connectives as additional features can
significantly improve the performance of implicit
discourse relation recognition. Specifically, the
first algorithm achieved an absolute average f-
score improvement of 3% over a state of the art
baseline system.
The rest of this paper is organized as follows.
Section 2 describes the two algorithms for implicit
discourse relation recognition. Section 3 presents
experiments and results on PDTB data. Section
4 reviews related work. Section 5 concludes this
work.
2 Our Algorithms for Implicit Discourse
Relation Recognition
2.1 Prediction of implicit connectives
Explicit discourse relations are easily identifiable
due to the presence of discourse connectives be-
tween arguments. (Pitler and Nenkova., 2009b)
showed that in PDTB corpus, the most general
senses, i.e., Comparison (Comp.), Contingency
(Cont.), Temporal (Temp.) and Expansion (Exp.),
can be disambiguated in explicit relations with
more than 90% f-scores based only on discourse
connectives.
But for implicit relations, there are no connec-
tives to explicitly mark the relations, which makes
the recognition task quite difficult. PDTB data
provides implicit connectives that are inserted be-
tween paragraph-internal adjacent sentence pairs
not marked by any of explicit connectives. The
availability of ground-truth implicit connectives
makes it possible to evaluate the contribution of
these connectives for implicit relation recognition.
Our initial study on PDTB data show that the av-
erage f-score for the most general 4 senses can
reach 91.8% when we obtained the sense of each
test example by mapping each ground truth im-
plicit connective to its most frequent sense. We
see that connective information is an important
knowledge source for implicit relation recogni-
tion. However these implicit connectives do not
exist in real texts. In this paper we overcome this
difficulty by inserting a connective between two
arguments with the use of a language model.
Following the annotation scheme of PDTB, we
assume that each implicit connective takes two ar-
guments, denoted as Arg1 and Arg2. Typically,
there are two possible positions for most of im-
plicit connectives1, i.e., the position before Arg1
and the position between Arg1 and Arg2. Given a
set of possible implicit connectives {ci}, we gen-
erate two synthetic sentences, ci+Arg1+Arg2 and
Arg1+ci+Arg2 for each ci, denoted as Sci,1 and
Sci,2. Then we calculate the perplexity (an intrin-
sic score) of these sentences with the use of a lan-
guage model, denoted as PPL(Sci,j). According
1For parallel connectives, e.g., if . . . then. . . , the two con-
nectives will take the two arguments together, so there is only
one possible combination for connectives and arguments.
1508
to the value of PPL(Sci,j) (the lower the better),
we can rank these sentences and select the con-
nectives in top N sentences as implicit connec-
tives for this argument pair. The language model
may be trained on large amount of unannotated
corpora that can be cheaply acquired, e.g., North
American News corpus.
2.2 Using predicted implicit connectives as
additional features
We predict implicit connectives on both training
set and test set. Then we can use the predicted
implicit connectives as additional features for su-
pervised implicit relation recognition. Previous
works exploited various linguistically informed
features under the framework of supervised mod-
els. In this paper, we include 9 types of features
in our system due to their superior performance
in previous studies, e.g., polarity features, seman-
tic classes of verbs, contextual sense, modality,
inquirer tags of words, first-last words of argu-
ments, cross-argument word pairs, ever used in
(Pitler et al, 2009a), production rules of parse
trees of arguments used in (Lin et al, 2009), and
intra-argument word pairs inspired by the work of
(Saito et al, 2006).
Here we provide the details of the 9 features,
shown as follows:
Verbs: Similar to the work in (Pitler et al,
2009a), the verb features consist of the number of
pairs of verbs in Arg1 and Arg2 if they are from
the same class based on their highest Levin verb
class level (Dorr, 2001). In addition, the average
length of verb phrase and the part of speech tags
of main verb are also included as verb features.
Context: If the immediately preceding (or fol-
lowing) relation is an explicit, its relation and
sense are used as features. Moreover, we use an-
other feature to indicate if Arg1 leads a paragraph.
Polarity: We use the number of positive,
negated positive, negative and neutral words in ar-
guments and their cross product as features. For
negated positives, we locate the negated words in
text span and then define the closely behind posi-
tive word as negated positive.
Modality: We look for modal words including
their various tenses or abbreviation forms in both
arguments. Then we generate a feature to indicate
the presence or absence of modal words in both
arguments and their cross product.
Inquirer Tags: Inquirer Tags extracted from
General Inquirer lexicon (Stone et al, 1966) con-
tains positive or negative classification of words.
In fact, its fine-grained categories, such as Fall
versus Rise, or Pleasure versus Pain, can indi-
cate the relation between two words, especially
for verbs. So we choose the presence or absence
of 21 pair categories with complementary relation
in Inquirer Tags as features. We also include their
cross production as features.
FirstLastFirst3: We choose the first and last
words of each argument as features, as well as the
pair of first words, the pair of last words, and the
first 3 words in each argument. In addition, we ap-
ply Porter?s Stemmer (Porter, 1980) to each word
before preparation of these features.
Production Rule: According to (Lin et al,
2009), we extract all the possible production rules
from arguments, and check whether the rules ap-
pear in Arg1, Arg2 and both arguments. We re-
move the rules occurring less than 5 times in train-
ing data.
Cross-argument Word Pairs: We perform the
Porter?s stemming (Porter, 1980), and then group
all words from Arg1 and Arg2 into two sets W1
and W2 respectively. Then we generate any possi-
ble word pair (wi, wj) (wi ? W1, wj ? W2). We
remove the word pairs with less than 5 times.
Intra-argument Word Pairs: Let
Q1 = (q1, q2, . . . , qn) be the word se-
quence of Arg1. The intra-argument word
pairs for Arg1 is defined as WP1 =
((q1, q2), (q1, q3), . . . , (q1, qn), (q2, q3), . . . ,
(qn?1, qn)). We extract all the intra-argument
word pairs from Arg1 and Arg2 and remove word
pairs appearing less than 5 times in training data.
2.3 Relation recognition based only on
predicted implicit connectives
After the prediction of implicit connectives, we
can address the implicit relation recognition task
with the methods for explicit relation recogni-
tion due to the presence of implicit connectives,
e.g., sense classification based only on connec-
tives (Pitler and Nenkova., 2009b). The work of
(Pitler and Nenkova., 2009b) showed that most
1509
of connectives are unambiguous and it is possible
to obtain high performance in prediction of dis-
course sense due to the simple mapping relation
between connectives and senses. Given two ex-
amples:
(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining is
getting heavier and heavier.
The two connectives, i.e., but in E1 and because
in E2, convey Comparison and Contingency sense
respectively. In most cases, we can easily recog-
nize the relation sense by the appearance of dis-
course connective since it can be interpreted in
only one way. That means, the ambiguity of the
mapping between sense and connective is quite
few.
We count the frequency of sense tags for each
possible connective on PDTB training data for im-
plicit relation. Then we build a sense recognition
model by simply mapping each connective to its
most frequent sense. Here we do not perform con-
nective prediction on training data. During test-
ing, we use the language model to insert implicit
connectives into each test argument pair. Then we
perform relation recognition by mapping each im-
plicit connective to its most frequent sense.
3 Experiments and Results
3.1 Experiments
3.1.1 Data sets
In this work we used the PDTB 2.0 corpus for
evaluation of our algorithms. Following the work
of (Pitler et al, 2009a), we used sections 2-20 as
training set, sections 21-22 as test set, and sec-
tions 0-1 as development set for parameter opti-
mization. For comparison with the work of (Pitler
et al, 2009a), we ran four binary classification
tasks to identify each of the main relations (Cont.,
Comp., Exp., and Temp.) from the rest. For each
relation, we used equal numbers of positive and
negative examples as training data2. The negative
examples were chosen at random from sections 2-
20. We used all the instances in sections 21 and
22 as test set, so the test set is representative of
2Here the numbers of training and test instances for Ex-
pansion relation are different from those in (Pitler et al,
2009a). The reason is that we do not include instances of
EntRel as positive examples.
the natural distribution. The numbers of positive
and negative instances for each sense in different
data sets are listed in Table 1.
Table 1: Statistics of positive and negative sam-
ples in training, development and test sets for each
relation.
Relation Train Dev Test
Pos/Neg Pos/Neg Pos/Neg
Comp. 1927/1927 191/997 146/912
Cont. 3375/3375 292/896 276/782
Exp. 6052/6052 651/537 556/502
Temp. 730/730 54/1134 67/991
In this work we used LibSVM toolkit to con-
struct four linear SVM models for a baseline sys-
tem and the system in Section 2.2.
3.1.2 A baseline system
We first built a baseline system, which used 9
types of features listed in Section 2.2.
We tuned the numbers of firstLastFirst3, cross-
argument word pair, intra-argument word pair on
development set. Finally we set the frequency
threshold at 3, 5 and 5 respectively.
3.1.3 Prediction of implicit connectives
To predict implicit connectives, we adopt the
following two steps:(1) train a language model;
(2) select top N implicit connectives.
Step 1: We used SRILM toolkit to train the lan-
guage models on three benchmark news corpora,
i.e., New York part in the BLLIP North Ameri-
can News, Xin and Ltw parts of English Gigaword
(4th Edition). We also tried different values for
n in n-gram model. The parameters were tuned
on the development set to optimize the accuracy
of prediction. In this work we chose 3-gram lan-
guage model trained on NY corpus.
Step 2: We combined each instance?s Arg1 and
Arg2 with connectives extract from PDTB2 (100
in all). There are two types of connectives, sin-
gle connective (e.g. because and but) and paral-
lel connective (such as ?not only . . . , but also?).
Since discourse connectives may appear not only
ahead of the Arg1, but also between Arg1 and
Arg2, we considered this case. Given a set of pos-
sible implicit connectives {ci}, for single connec-
tive {ci}, we constructed two synthetic sentences,
ci+Arg1+Arg2 and Arg1+ci+Arg2. In case of
1510
parallel connective, we constructed one synthetic
sentence like ci1+Arg1+ci2+Arg2.
As a result, we can get 198 synthetic sentences
for each argument pair. Then we converted all
words to lower cases and used the language model
trained in the above step to calculate perplexity
on sentence level. The perplexity scores were
ranked from low to high. For example, we got the
perplexity (ppl) for two sentences as follows:
(1) but this is an old story, we?re talking about
years ago before anyone heard of asbestos having
any questionable properties.
ppl= 652.837
(2) this is an old story, but we?re talking about
years ago before anyone heard of asbestos having
any questionable properties.
ppl= 583.514
We considered the combination of connectives
and their position as final features like mid but,
first but, where the features are binary, that is, the
presence and absence of the specific connective.
According to the value of PPL(Sci,j) (the
lower the better), we selected the connectives in
top N sentences as implicit connectives for this
argument pair. In order to get the optimal N value,
we tried various values of N on development set
and selected the minimum value of N so that the
ground-truth connectives appeared in top N con-
nectives. The final N value is set to 60 based on
the trade-off between performance and efficiency.
3.1.4 Using predicted connectives as
additional features
This system combines the predicted implicit
connectives as additional features and the 9 types
of features in an supervised framework. The 9
types of features are listed as shown in Section 2.2
and tuned on development set.
We combined predicted connectives with the
best subset features from the development data set
with respect to f-score. In our experiment of se-
lecting best subset features, single features rather
than the combination of several features achieved
much higher scores. So we combine single fea-
tures with predicted connectives as final features.
3.1.5 Using only predicted connectives for
implicit relation recognition
We built two variants for the algorithm in Sec-
tion 2.3. One is to use the data for explicit re-
lations in PDTB sections 2-20 as training data.
The other is to use the data for implicit relations
in PDTB sections 2-20 as training data. Given
training data, we obtained the most frequent sense
for each connective appearing in the training data.
Then given test data, we recognized the sense of
each argument pair by mapping each predicted
connective to its most frequent sense. In this
work we conducted another experiment to see the
upper-bound performance of this algorithm. Here
we performed recognition based on ground-truth
implicit connectives and used the data for implicit
relations as training data.
3.2 Results
3.2.1 Result of baseline system
Table 2 summarizes the best performance
achieved by the baseline system in compari-
son with previous state-of-the-art performance
achieved in (Pitler et al, 2009a). The first two
lines in the table show their best results using sin-
gle feature and using combined feature subset. It
indicates that the performance of using combined
feature subset is higher than that using single fea-
ture alone.
From this table, we can find that our base-
line system has a comparable result on Contin-
gency and Temporal. On Comparison, our system
achieved a better performance around 9% f-score
higher than their best result. However, for Expan-
sion, they expanded both training and testing sets
by including EntRel relation as positive examples,
which makes it impossible to perform direct com-
parison. Generally, our baseline system is reason-
able and thus the consequent experiments on it are
reliable.
3.2.2 Result of algorithm 1: using predicted
connectives as additional features
Table 3 summarizes the best performance
achieved by the baseline system and the first al-
gorithm (i.e., baseline + Language Model) on test
set. The second and third column show the best
performance achieved by the baseline system and
1511
Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test
set.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Using the best single feature (Pitler et al, 2009a) 21.01(52.59) 36.75(62.44) 71.29(59.23) 15.93(61.20)
Using the best feature subset (Pitler et al, 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49)
The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)
the first algorithm using predicted connectives as
additional features.
Table 3: Performance comparison of the algo-
rithm in Section 2.2 with the baseline system on
test set.
Rela- Features Baseline Baseline+LM
tion F1 (Acc) F1 (Acc)
Comp. Production Rule 30.72(78.26) 31.08(68.15)
Context 24.66(42.25) 27.64(53.97)
InquirerTags 23.31(73.25) 27.87(55.48)
Polarity 21.11(40.64) 23.64(52.36)
Modality 17.25(80.06) 26.17(55.20)
Verbs 25.00(53.50) 31.79(58.22)
Cont. Prodcution Rule 45.38(40.17) 47.16(48.96)
Context 37.61(44.70) 34.74(48.87)
Polarity 35.57(50.00) 43.33(33.74)
InquirerTags 38.04(41.49) 42.22(36.11)
Modality 32.18(66.54) 35.26(55.58)
Verbs 40.44(54.06) 42.04(32.23)
Exp. Context 48.34(54.54) 68.32(53.02)
FirstLastFirst3 65.95(57.94) 68.94(53.59)
InquirerTags 61.29(52.84) 68.49(53.21)
Modality 64.36(56.14) 68.9(52.55)
Polarity 49.95(50.38) 68.62(53.40)
Verbs 52.95(53.31) 70.11(54.54)
Temp. Context 13.52(64.93) 16.99(79.68)
FirstLastFirst3 15.75(66.64) 19.70(64.56)
InquirerTags 8.51(83.74) 19.20(56.24)
Modality 16.46(29.96) 19.97(54.54)
Polarity 16.29(51.42) 20.30(55.48)
Verbs 13.88(54.25) 13.53(61.34)
From this table, we found that this additional
feature obtained from language model showed
significant improvements in almost four relations.
Specifically, the top two improvements are on Ex-
pansion and Temporal relations, which improved
4.16% and 3.84% in f-score respectively. Al-
though on Comparison relation there is only a
slight improvement (+1.07%), our two best sys-
tems both got around 10% improvements of f-
score over a state-of-the-art system in (Pitler et al,
2009a). As a whole, the first algorithm achieved
3% improvement of f-score over a state of the art
baseline system. All these results indicate that
predicted implicit connectives can help improve
the performance.
3.2.3 Result of algorithm 2: using only
predicted connectives for implicit
relation recognition
Table 4 summarizes the best performance
achieved by the second algorithm in comparison
with the baseline system on test set.
The experiment showed that the baseline sys-
tem using just gold-truth implicit connectives can
achieve an f-score of 91.8% for implicit relation
recognition. It once again proved that implicit
connectives make significant contributions for im-
plicit relation recognition. This also encourages
our future work on finding the most suitable con-
nectives for implicit relation recognition.
From this table, we found that, using only pre-
dicted implicit connectives achieved an compara-
ble performance to (Pitler et al, 2009a), although
it was still a bit lower than our best baseline. But
we should bear in mind that this algorithm only
uses 4 features for implicit relation recognition
and these 4 features are easy computable and fast
run, which makes the system more practical in ap-
plication. Furthermore, compared with other al-
gorithms which require hand-annotated data for
training, the performance of this second algorithm
is acceptable if we take into account that no la-
beled data is used for model training.
3.3 Analysis
Experimental results on PDTB showed that using
the predicted implicit connectives significantly
improves the performance of implicit discourse
relation recognition. Our first algorithm achieves
an average f-score improvement of 3% over a
state of the art baseline system. Specifically, for
the relations: Comp., Cont., Exp., Temp., our
first algorithm can achieve 1.07%, 1.78%, 4.16%,
3.84% f-score improvements over a state of the
art baseline system. Since (Pitler et al, 2009a)
1512
Table 4: Performance comparison of the algorithm in Section 2.3 with the baseline system on test set.
System Comp. vs. Other Cont. vs. Other Exp. vs. Other Temp. vs. Other
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)
Our algorithm with training data for explicit relation 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97)
Our algorithm with training data for implicit relation 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51)
Sense recognition using gold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07)
used different selection of instances for Expan-
sion sense3, we cannot make a direct compari-
son. However, we achieve the best f-score around
70%, which provide 5% improvements over our
baseline system. On the other hand, the second
proposed algorithm using only predicted connec-
tives still achieves promising results for each rela-
tion. Specifically, the model for the Comparison
relation achieves an f-score of 26.02% (5% over
the previous work in (Pitler et al, 2009a)). Fur-
thermore, the models for Contingency and Tem-
poral relation achieve 35.72% and 13.76% f-score
respectively, which are comparable to the previ-
ous work in (Pitler et al, 2009a). The model for
Expansion relation obtains an f-score of 64.95%,
which is only 1% less than our baseline system
which consists of ten thousands of features.
4 Related Work
Existing works on automatic recognition of dis-
course relations can be grouped into two cat-
egories according to whether they used hand-
annotated corpora.
One research line is to perform relation recog-
nition without hand-annotated corpora.
(Marcu and Echihabi, 2002) used a pattern-
based approach to extract instances of discourse
relations such as Contrast and Elaboration from
unlabeled corpora. Then they used word-pairs be-
tween two arguments as features for building clas-
sification models and tested their model on artifi-
cial data for implicit relations.
There are other efforts that attempt to extend the
work of (Marcu and Echihabi, 2002). (Saito et al,
2006) followed the method of (Marcu and Echi-
habi, 2002) and conducted experiments with com-
bination of cross-argument word pairs and phrasal
3They expanded the Expansion data set by adding ran-
domly selected EntRel instances by 50%, which is consid-
ered to significantly change data distribution.
patterns as features to recognize implicit relations
between adjacent sentences in a Japanese corpus.
They showed that phrasal patterns extracted from
a text span pair provide useful evidence in the re-
lation classification. (Sporleder and Lascarides,
2008) discovered that Marcu and Echihabi?s mod-
els do not perform as well on implicit relations as
one might expect from the test accuracies on syn-
thetic data. (Blair-Goldensohn, 2007) extended
the work of (Marcu and Echihabi, 2002) by re-
fining the training and classification process using
parameter optimization, topic segmentation and
syntactic parsing.
(Lapata and Lascarides, 2004) dealt with tem-
poral links between main and subordinate clauses
by inferring the temporal markers linking them.
They extracted clause pairs with explicit temporal
markers from BLLIP corpus as training data.
Another research line is to use human-
annotated corpora as training data, e.g., the RST
Bank (Carlson et al, 2001) used by (Soricut and
Marcu, 2003), adhoc annotations used by (?),
(Baldridge and Lascarides, 2005), and the Graph-
Bank (Wolf et al, 2005) used by (Wellner et al,
2006).
Recently the release of the Penn Discourse
TreeBank (PDTB) (Prasad et al, 2008) bene-
fits the researchers with a large discourse anno-
tated corpora, using a comprehensive scheme for
both implicit and explicit relations. (Pitler et al,
2009a) performed implicit relation classification
on the second version of the PDTB. They used
several linguistically informed features, such as
word polarity, verb classes, and word pairs, show-
ing performance increases over a random classi-
fication baseline. (Lin et al, 2009) presented an
implicit discourse relation classifier in PDTB with
the use of contextual relations, constituent Parse
Features, dependency parse features and cross-
argument word pairs.
1513
In comparison with existing works, we investi-
gated a new knowledge source, implicit connec-
tives, for implicit relation recognition. Moreover,
our two models can exploit both labeled and un-
labeled data by training a language model on un-
labeled data and then using this language model
to generate implicit connectives for recognition
models trained on labeled data.
5 Conclusions
In this paper we use a language model to auto-
matically generate implicit connectives and then
present two methods to use these connectives for
recognition of implicit relations. One method is to
use these predicted implicit connectives as addi-
tional features in a supervised model and the other
is to perform implicit relation recognition based
only on these predicted connectives. Results on
Penn Discourse Treebank 2.0 show that predicted
discourse connectives help implicit relation recog-
nition and the first algorithm achieves an absolute
average f-score improvement of 3% over a state of
the art baseline system.
Acknowledgments
This work is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse structure. Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. Proceedings of
the Second SIG dial Workshop on Discourse and Di-
alogue.
B. Dorr. LCS Verb Database. Technical Report Online
Software Database, University of Maryland, Col-
lege Park, MD,2001.
R. Girju. 2003. Automatic detection of causal rela-
tions for question answering. In ACL 2003 Work-
shops.
S. Blair-Goldensohn. 2007. Long-Answer Ques-
tion Answering and Rhetorical-Semantic Relations.
Ph.D. thesis, Columbia Unviersity.
M. Lapata and A. Lascarides. 2004. Inferring
Sentence-internal Temporal Relations. Proceedings
of the North American Chapter of the Assocation of
Computational Linguistics.
Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing
Implicit Discourse Relations in the Penn Discourse
Treebank. Proceedings of the 2009 Conference on
EMNLP.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. Pro-
ceedings of the 40th ACL.
E. Pitler, A. Louis, A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. Proceedings of the 47th ACL.
E. Pitler and A. Nenkova. 2009. Using Syntax to Dis-
ambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.
M. Porter. 1980. An algorithm for suffix stripping. In
Program, vol. 14, no. 3, pp.130-137.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.
Robaldo, A. Joshi, B. Webber. 2008. The Penn Dis-
course TreeBank 2.0. Proceedings of LREC?08.
M. Saito, K.Yamamoto, S.Sekine. 2006. Using
Phrasal Patterns to Identify Discourse Relations.
Proceeding of the HLTCNA Chapter of the ACL.
R. Soricut and D. Marcu. Sentence Level Discourse
Parsing using Syntactic and Lexical Information.
Proceedings of HLT/NAACL 2003.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: an assessment. Natural Language Engineer-
ing, Volume 14, Issue 03.
P.J. Stone, J. Kirsh, and Cambridge Computer Asso-
ciates. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.
2006. Classification of discourse coherence rela-
tions: An exploratory study using multiple knowl-
edge sources. Proceedings of the 7th SIGDIAL
Workshop on Discourse and Dialogue.
F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005.
The Discourse GraphBank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
1514
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 476?485,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit
Discourse Relation Recognition
Man Lan and Yu Xu
Department of Computer Science and Technology
East China Normal University
Shanghai, P.R.China
mlan@cs.ecnu.edu.cn
51101201049@ecnu.cn
Zheng-Yu Niu
Baidu Inc.
Beijing, P.R.China
niuzhengyu@baidu.com
Abstract
To overcome the shortage of labeled data
for implicit discourse relation recogni-
tion, previous works attempted to auto-
matically generate training data by remov-
ing explicit discourse connectives from
sentences and then built models on these
synthetic implicit examples. However, a
previous study (Sporleder and Lascarides,
2008) showed that models trained on these
synthetic data do not generalize very well
to natural (i.e. genuine) implicit discourse
data. In this work we revisit this issue and
present a multi-task learning based system
which can effectively use synthetic data
for implicit discourse relation recognition.
Results on PDTB data show that under the
multi-task learning framework our models
with the use of the prediction of explicit
discourse connectives as auxiliary learn-
ing tasks, can achieve an averaged F1 im-
provement of 5.86% over baseline models.
1 Introduction
The task of implicit discourse relation recognition
is to identify the type of discourse relation (a.k.a.
rhetorical relation) hold between two spans of
text, where there is no discourse connective (a.k.a.
discourse marker, e.g., but, and) in context to ex-
plicitly mark their discourse relation (e.g., Con-
trast or Explanation). It can be of great benefit
to many downstream NLP applications, such as
question answering (QA) (Verberne et al, 2007),
information extraction (IE) (Cimiano et al, 2005),
and machine translation (MT), etc. This task is
quite challenging due to two reasons. First, with-
out discourse connective in text, the task is quite
difficult in itself. Second, implicit discourse rela-
tion is quite frequent in text. For example, almost
half the sentences in the British National Corpus
held implicit discourse relations (Sporleder and
Lascarides, 2008). Therefore, the task of implicit
discourse relation recognition is the key to im-
proving end-to-end discourse parser performance.
To overcome the shortage of manually anno-
tated training data, (Marcu and Echihabi, 2002)
proposed a pattern-based approach to automat-
ically generate training data from raw corpora.
This line of research was followed by (Sporleder
and Lascarides, 2008) and (Blair-Goldensohn,
2007). In these works, sentences containing cer-
tain words or phrases (e.g. but, although) were
selected out from raw corpora using a pattern-
based approach and then these words or phrases
were removed from these sentences. Thus the
resulting sentences were used as synthetic train-
ing examples for implicit discourse relation recog-
nition. Since there is ambiguity of a word or
phrase serving for discourse connective (i.e., the
ambiguity between discourse and non-discourse
usage or the ambiguity between two or more dis-
course relations if the word or phrase is used as a
discourse connective), the synthetic implicit data
would contain a lot of noises. Later, with the re-
lease of manually annotated corpus, such as Penn
Discourse Treebank 2.0 (PDTB) (Prasad et al,
2008), recent studies performed implicit discourse
relation recognition on natural (i.e., genuine) im-
plicit discourse data (Pitler et al, 2009) (Lin et al,
2009) (Wang et al, 2010) with the use of linguis-
tically informed features and machine learning al-
gorithms.
(Sporleder and Lascarides, 2008) conducted a
study of the pattern-based approach presented by
(Marcu and Echihabi, 2002) and showed that the
model built on synthetical implicit data has not
generalize well on natural implicit data. They
found some evidence that this behavior is largely
independent of the classifiers used and seems to
lie in the data itself (e.g., marked and unmarked
examples may be too dissimilar linguistically and
476
removing unambiguous markers in the automatic
labelling process may lead to a meaning shift in
the examples). We state that in some cases it is
true while in other cases it may not always be so.
A simple example is given here:
(E1) a. We can?t win.
b. [but] We must keep trying.
We may find that in this example whether the in-
sertion or the removal of connective but would
not lead to a redundant or missing information be-
tween the above two sentences. That is, discourse
connectives can be inserted between or removed
from two sentences without changing the seman-
tic relations between them in some cases. An-
other similar observation is in the annotation pro-
cedure of PDTB. To label implicit discourse re-
lation, annotators inserted connective which can
best express the relation between sentences with-
out any redundancy1. We see that there should
be some linguistical similarities between explicit
and implicit discourse examples. Therefore, the
first question arises: can we exploit this kind of
linguistic similarity between explicit and implicit
discourse examples to improve implicit discourse
relation recognition?
In this paper, we propose a multi-task learning
based method to improve the performance of im-
plicit discourse relation recognition (as main task)
with the help of relevant auxiliary tasks. Specif-
ically, the main task is to recognize the implicit
discourse relations based on genuine implicit dis-
course data and the auxiliary task is to recognize
the implicit discourse relations based on synthetic
implicit discourse data. According to the princi-
ple of multi-task learning, the learning model can
be optimized by the shared part of the main task
and the auxiliary tasks without bring unnecessary
noise. That means, the model can learn from syn-
thetic implicit data while it would not bring unnec-
essary noise from synthetic implicit data.
Although (Sporleder and Lascarides, 2008) did
not mention, we speculate that another possible
reason for the reported worse performance may
result from noises in synthetic implicit discourse
data. These synthetic data can be generated from
two sources: (1) raw corpora with the use of
pattern-based approach in (Marcu and Echihabi,
1According to the PDTB Annotation Manual (PDTB-
Group, 2008), if the insertion of connective leads to ?redun-
dancy?, the relation is annotated as Alternative lexicalizations
(AltLex), not implicit.
2002) and (Sporleder and Lascarides, 2008), and
(2) manually annotated explicit data with the re-
moval of explicit discourse connectives. Obvi-
ously, the data generated from the second source
is cleaner and more reliable than that from the
first source. Therefore, the second question to ad-
dress in this work is: whether synthetic implicit
discourse data generated from explicit discourse
data source (i.e., the second source) can lead to
a better performance than that from raw corpora
(i.e., the first source)? To answer this question,
we will make a comparison of synthetic discourse
data generated from two corpora, i.e., the BILLIP
corpus and the explicit discourse data annotated in
PDTB.
The rest of this paper is organized as follows.
Section 2 reviews related work on implicit dis-
course relation classification and multi-task learn-
ing. Section 3 presents our proposed multi-task
learning method for implicit discourse relation
classification. Section 4 provides the implemen-
tation technique details of the proposed multi-task
method. Section 5 presents experiments and dis-
cusses results. Section 6 concludes this work.
2 Related Work
2.1 Implicit discourse relation classification
2.1.1 Unsupervised approaches
Due to the lack of benchmark data for implicit
discourse relation analysis, earlier work used un-
labeled data to generate synthetic implicit dis-
course data. For example, (Marcu and Echi-
habi, 2002) proposed an unsupervised method
to recognize four discourse relations, i.e., Con-
trast, Explanation-evidence, Condition and Elab-
oration. They first used unambiguous pattern to
extract explicit discourse examples from raw cor-
pus. Then they generated synthetic implicit dis-
course data by removing explicit discourse con-
nectives from sentences extracted. In their work,
they collected word pairs from synthetic data set
as features and used machine learning method to
classify implicit discourse relation. Based on this
work, several researchers have extended the work
to improve the performance of relation classifica-
tion. For example, (Saito et al, 2006) showed that
the use of phrasal patterns as additional features
can help a word-pair based system for discourse
relation prediction on a Japanese corpus. Further-
more, (Blair-Goldensohn, 2007) improved previ-
ous work with the use of parameter optimization,
477
topic segmentation and syntactic parsing. How-
ever, (Sporleder and Lascarides, 2008) showed
that the training model built on a synthetic data
set, like the work of (Marcu and Echihabi, 2002),
may not be a good strategy since the linguistic dis-
similarity between explicit and implicit data may
hurt the performance of a model on natural data
when being trained on synthetic data.
2.1.2 Supervised approaches
This line of research work approaches this relation
prediction problem by recasting it as a classifica-
tion problem. (Soricut and Marcu, 2003) parsed
the discourse structures of sentences on RST Bank
data set (Carlson et al, 2001) which is annotated
based on Rhetorical Structure Theory (Mann and
Thompson, 1988). (Wellner et al, 2006) pre-
sented a study of discourse relation disambigua-
tion on GraphBank (Wolf et al, 2005). Recently,
(Pitler et al, 2009) (Lin et al, 2009) and (Wang
et al, 2010) conducted discourse relation study on
PDTB (Prasad et al, 2008) which has been widely
used in this field.
2.1.3 Semi-supervised approaches
Research work in this category exploited both la-
beled and unlabeled data for discourse relation
prediction. (Hernault et al, 2010) presented a
semi-supervised method based on the analysis of
co-occurring features in labeled and unlabeled
data. Very recently, (Hernault et al, 2011) in-
troduced a semi-supervised work using structure
learning method for discourse relation classifica-
tion, which is quite relevant to our work. However,
they performed discourse relation classification on
both explicit and implicit data. And their work is
different from our work in many aspects, such as,
feature sets, auxiliary task, auxiliary data, class la-
bels, learning framework, and so on. Furthermore,
there is no explicit conclusion or evidence in their
work to address the two questions raised in Sec-
tion 1.
Unlike their previous work, our previous work
(Zhou et al, 2010) presented a method to predict
the missing connective based on a language model
trained on an unannotated corpus. The predicted
connective was then used as a feature to classify
the implicit relation.
2.2 Multi-task learning
Multi-task learning is a kind of machine learning
method, which learns a main task together with
other related auxiliary tasks at the same time, us-
ing a shared representation. This often leads to
a better model for the main task, because it al-
lows the learner to use the commonality among
the tasks. Many multi-task learning methods have
been proposed in recent years, (Ando and Zhang,
2005a), (Argyriou et al, 2008), (Jebara, 2004),
(Bonilla et al, 2008), (Evgeniou and Pontil, 2004),
(Baxter, 2000), (Caruana, 1997), (Thrun, 1996).
One group uses task relations as regularization
terms in the objective function to be optimized.
For example, in (Evgeniou and Pontil, 2004) the
regularization terms make the parameters of mod-
els closer for similar tasks. Another group is pro-
posed to find the common structure from data and
then utilize the learned structure for multi-task
learning (Argyriou et al, 2008) (Ando and Zhang,
2005b).
3 Multi-task Learning for Discourse
Relation Prediction
3.1 Motivation
The idea of using multi-task learning for implicit
discourse relation classification is motivated by
the observations that we have made on implicit
discourse relation.
On one hand, since building a hand-annotated
implicit discourse relation corpus is costly and
time consuming, most previous work attempted to
use synthetic implicit discourse examples as train-
ing data. However, (Sporleder and Lascarides,
2008) found that the model trained on synthetic
implicit data has not performed as well as expected
in natural implicit data. They stated that the reason
is linguistic dissimilarity between explicit and im-
plicit discourse data. This indicates that straightly
using synthetic implicit data as training data may
not be helpful.
On the other hand, as shown in Section 1, we
observe that in some cases explicit discourse rela-
tion and implicit discourse relation can express the
same meaning with or without a discourse connec-
tive. This indicates that in certain degree they must
be similar to each other. If it is true, the synthetic
implicit relations are expected to be helpful for im-
plicit discourse relation classification. Therefore,
what we have to do is to find a way to train a model
which has the capabilities to learn from their sim-
ilarity and to ignore their dissimilarity as well.
To solve it, we propose a multi-task learn-
ing method for implicit discourse relation classi-
478
fication, where the classification model seeks the
shared part through jointly learning main task and
multiple auxiliary tasks. As a result, the model can
be optimized by the similar shared part without
bringing noise in the dissimilar part. Specifically,
in this work, we use alternating structure optimiza-
tion (ASO) (Ando and Zhang, 2005a) to construct
the multi-task learning framework. ASO has been
shown to be useful in a semi-supervised learning
configuration for several NLP applications, such
as, text chunking (Ando and Zhang, 2005b) and
text classification (Ando and Zhang, 2005a).
3.2 Multi-task learning and ASO
Generally, multi-task learning(MTL) considers m
prediction problems indexed by ? ? {1, ...,m},
each with n? samples (X?i , Y ?i ) for i ? {1, ...n?}
(Xi are input feature vectors and Yi are corre-
sponding classification labels) and assumes that
there exists a common predictive structure shared
by these m problems. Generally, the joint linear
model for MTL is to predict problem ? in the fol-
lowing form:
f?(?, X) = wT? X + vT? ?X,??T = I, (1)
where I is the identity matrix,w? and v? are weight
vectors specific to each problem ?, and ? is the
structure matrix shared by all the m predictors.
The main goal of MTL is to learn a common good
feature map ?X for all the m problems. Several
MTL methods have been presented to learn ?X
for all the m problems. In this work, we adopt the
ASO method.
Specifically, the ASO method adopted singu-
lar value decomposition (SVD) to obtain ? and
m predictors that minimize the empirical risk
summed over all the m problems. Thus, the prob-
lem of optimization becomes the minimization of
the joint empirical risk written as:
m?
?=1
( n??
i=1
L(f?(?, X?i ), Yi)
n?
+ ?||W?||2
)
(2)
where loss function L(.) quantifies the difference
between the prediction f(Xi) and the true out-
put Yi for each predictor, and ? is a regulariza-
tion parameter for square regularization to control
the model complexity. To minimize the empirical
risk, ASO repeats the following alternating opti-
mization procedure until a convergence criterion
is met:
1) Fix (?, V?), and find m predictors f? that
minimize the above joint empirical risk.
2) Fix m predictors f?, and find (?, V?) that
minimizes the above joint empirical risk.
3.3 Auxiliary tasks
There are two main principles to create auxiliary
tasks. First, the auxiliary tasks should be auto-
matically labeled in order to reduce the cost of
manual labeling. Second, since the MTL model
learns from the shared part of main task and aux-
iliary tasks, the auxiliary tasks should be quite rel-
evant/similar to the main task. It is generally be-
lieved that the more the auxiliary tasks are relevant
to the main task, the more the main task can ben-
efit from the auxiliary tasks. Following these two
principles, we create the auxiliary tasks by gener-
ating automatically labeled data as follows.
Previous work (Marcu and Echihabi, 2002) and
(Sporleder and Lascarides, 2008) adopted prede-
fined pattern-based approach to generate synthetic
labeled data, where each predefined pattern has
one discourse relation label. In contrast, we adopt
an automatic approach to generate synthetic la-
beled data, where each discourse connective be-
tween two texts serves as their relation label. The
reason lies in the very strong connection between
discourse connectives and discourse relations. For
example, the connective but always indicates a
contrast relation between two texts. And (Pitler et
al., 2008) proved that using only connective itself,
the accuracy of explicit discourse relation classifi-
cation is over 93%.
To build the mapping between discourse con-
nective and discourse relation, for each connec-
tive, we count the times it appears in each relation
and regard the relation in which it appears most
frequently as its most relevant relation. Based on
this mapping between connective and relation, we
extract the synthetic labeled data containing the
connective as training data for auxiliary tasks.
For example, and appears 3, 000 times in PDTB
as a discourse connective. Among them, it is man-
ually annotated as an Expansion relation for 2, 938
times. So we regard the Expansion relation as its
most relevant relation and generate a mapping pat-
tern like: ?and ? Expansion?. Then we extract
all sentences which contain discourse ?and? and
remove this connective ?and? from sentences to
generate synthetic implicit data. The resulting sen-
tences are used in auxiliary task and automatically
479
marked as Expansion relation.
4 Implementation Details of Multi-task
Learning Method
4.1 Data sets for main and auxiliary tasks
To examine whether there is a difference in syn-
thetic implicit data generated from unannotated
and annotated corpus, we use two corpora. One
is a hand-annotated explicit discourse corpus, i.e.,
the explicit discourse relations in PDTB, denoted
as exp. Another is an unannotated corpus, i.e.,
BLLIP (David McClosky and Johnson., 2008).
4.1.1 Penn Discourse Treebank
PDTB (Prasad et al, 2008) is the largest hand-
annotated corpus of discourse relation so far. It
contains 2, 312 Wall Street Journal (WSJ) articles.
The sense label of discourse relations is hierarchi-
cally with three levels, i.e., class, type and sub-
type. The top level contains four major seman-
tic classes: Comparison (denoted as Comp.), Con-
tingency (Cont.), Expansion (Exp.) and Temporal
(Temp.). For each class, a set of types is used to
refine relation sense. The set of subtypes is to fur-
ther specify the semantic contribution of each ar-
gument. In this paper, we focus on the top level
(class) and the second level (type) relations be-
cause the subtype relations are too fine-grained
and only appear in some relations.
Both explicit and implicit discourse relations
are labeled in PDTB. In our experiment, the im-
plicit discourse relations are used in the main task
and for evaluation. While the explicit discourse
relations are used in the auxiliary task. A detailed
description of the data sources for different tasks
is given below.
Data set for main task Following previous
work in (Pitler et al, 2009) and (Zhou et al, 2010),
the implicit relations in sections 2-20 are used as
training data for the main task (denoted as imp)
and the implicit relations in sections 21-22 are
for evaluation. Table 1 shows the distribution of
implicit relations. There are too few training in-
stances for six second level relations (indicated by
* in Table 1), so we removed these six relations in
our experiments.
Data set for auxiliary task All explicit in-
stances in sections 00-24 in PDTB, i.e., 18, 459
instances, are used for auxiliary task (denoted as
exp). Following the method described in Section
3.3, we build the mapping patterns between con-
Top level Second level train test
Temp 736 83
Synchrony 203 28
Asynchronous 532 55
Cont 3333 279
Cause 3270 272
Pragmatic Cause* 64 7
Condition* 1 0
Pragmatic condition* 1 0
Comp 1939 152
Contrast 1607 134
Pragmatic contrast* 4 0
Concession 183 17
Pragmatic concession* 1 0
Exp 6316 567
Conjunction 2872 208
Instantiation 1063 119
Restatement 2405 213
Alternative 147 9
Exception* 0 0
List 338 12
Table 1: Distribution of implicit discourse rela-
tions in the top and second level of PDTB
nectives and relations in PDTB and generate syn-
thetic labeled data by removing the connectives.
According to the most relevant relation sense of
connective removed, the resulting instances are
grouped into different data sets.
4.1.2 BLLIP
BLLIP North American News Text (Complete) is
used as unlabeled data source to generate syn-
thetic labeled data. In comparison with the syn-
thetic labeled data generated from the explicit re-
lations in PDTB, the synthetic labeled data from
BLLIP contains more noise. This is because the
former data is manually annotated whether a word
serves as discourse connective or not, while the
latter does not manually disambiguate two types
of ambiguity, i.e., whether a word serves as dis-
course connective or not, and the type of discourse
relation if it is a discourse connective. Finally, we
extract 26, 412 instances from BLLIP (denoted as
BLLIP) and use them for auxiliary task.
4.2 Feature representation
For both main task and auxiliary tasks, we adopt
the following three feature types. These features
are chosen due to their superior performance in
previous work (Pitler et al, 2009) and our previ-
ous work (Zhou et al, 2010).
Verbs: Following (Pitler et al, 2009), we ex-
tract the pairs of verbs from both text spans. The
number of verb pairs which have the same highest
480
Levin verb class levels (Levin, 1993) is counted
as a feature. Besides, the average length of verb
phrases in each argument is included as a feature.
In addition, the part of speech tags of the main
verbs (e.g., base form, past tense, 3rd person sin-
gular present, etc.) in each argument, i.e., MD,
VB, VBD, VBG, VBN, VBP, VBZ, are recorded
as features, where we simply use the first verb in
each argument as the main verb.
Polarity: This feature records the number of
positive, negated positive, negative and neutral
words in both arguments and their cross product
as well. For negated positives, we first locate the
negated words in text span and then define the
closely behind positive word as negated positive.
The polarity of each word in arguments is de-
rived from Multi-perspective Question Answering
Opinion Corpus (MPQA) (Wilson et al, 2009).
Modality: We examine six modal words (i.e.,
can, may, must, need, shall, will) including their
various tenses or abbreviation forms in both argu-
ments. This feature records the presence or ab-
sence of modal words in both arguments and their
cross product.
4.3 Classifiers used multi-task learning
We extract the above linguistically informed fea-
tures from two synthetic implicit data sets (i.e.,
BLLIP and exp) to learn the auxiliary classifier and
from the natural implicit data set (i.e., imp) to learn
the main classifier. Under the ASO-based multi-
task learning framework, the model of main task
learns from the shared part of main task and aux-
iliary tasks. Specifically, we adopt multiple binary
classification to build model for main task. That
is, for each discourse relation, we build a binary
classifier.
5 Experiments and Results
5.1 Experiments
Although previous work has been done on PDTB
(Pitler et al, 2009) and (Lin et al, 2009), we can-
not make a direct comparison with them because
various experimental conditions, such as, differ-
ent classification strategies (multi-class classifica-
tion, multiple binary classification), different data
preparation (feature extraction and selection), dif-
ferent benchmark data collections (different sec-
tions for training and test, different levels of dis-
course relations), different classifiers with various
parameters (MaxEnt, Na??ve Bayes, SVM, etc) and
even different evaluation methods (F1, accuracy)
have been adopted by different researchers.
Therefore, to address the two questions raised in
Section 1 and to make the comparison reliable and
reasonable, we performed experiments on the top
and second level of PDTB using single task learn-
ing and multi-task learning, respectively. The sys-
tems using single task learning serve as baseline
systems. Under the single task learning, various
combinations of exp and BLLIP data are incorpo-
rated with imp data for the implicit discourse rela-
tion classification task.
We hypothesize that synthetical implicit data
would contribute to the main task, i.e., the implicit
discourse relation classification. Specifically, the
natural implicit data (i.e., imp) are used to create
main task and the synthetical implicit data (exp or
BLLIP) are used to create auxiliary tasks for the
purpose of optimizing the objective functions of
main task. If the hypothesis is correct, the perfor-
mance of main task would be improved by auxil-
iary tasks created from synthetical implicit data.
Thus in the experiments of multi-task learning,
only natural implicit examples (i.e., imp) data are
used for main task training while different combi-
nations of synthetical implicit examples (exp and
BLLIP) are used for auxiliary task training.
We adopt precision, recall and their combina-
tion F1 for performance evaluation. We also per-
form one-tailed t-test to validate if there is signif-
icant difference between two methods in terms of
F1 performance analysis.
5.2 Results
Table 2 summarizes the experimental results under
single and multi-task learning on the top level of
four PDTB relations with respect to different com-
binations of synthetic implicit data. For each rela-
tion, the first three rows indicate the results of us-
ing different single training data under single task
learning and the last three rows indicate the results
using different combinations of training data un-
der single task and multi-task learning. The best
F1 for every relation is shown in bold font. From
this table, we can find that on four relations, our
multi-task learning systems achieved the best per-
formance using the combination of exp and BLLIP
synthetic data.
Table 3 summarizes the best single task and the
best multi-task learning results on the second level
of PDTB. For four relations, i.e., Synchrony, Con-
481
Single-task Multi-task
Level 1 class Data P R F1 Data Data P R F1
(main) (aux)
Comp. imp 21.43 37.50 27.27 - - - - -
BLLIP 12.68 53.29 20.48 - - - - -
exp 15.25 50.66 23.44 - - - - -
imp + exp 16.94 40.13 23.83 imp exp 22.94 49.34 30.90
imp + BLLIP 13.56 44.08 20.74 imp BLLIP 20.47 63.16 30.92
imp + exp + BLLIP 14.54 38.16 21.05 imp exp + BLLIP 23.47 48.03 31.53
Cont. imp 37.65 43.73 40.46 - - - - -
BLLIP 33.72 31.18 32.40 - - - - -
exp 35.24 26.52 30.27 - - - - -
imp + exp 39.00 13.98 20.58 imp exp 39.94 45.52 42.55
imp + BLLIP 37.30 24.73 29.74 imp BLLIP 37.80 63.80 47.47
imp + exp + BLLIP 39.37 31.18 34.80 imp exp + BLLIP 35.90 70.25 47.52
Exp. imp 56.59 66.67 61.21 - - - - -
BLLIP 53.29 40.04 45.72 - - - - -
exp 57.97 58.38 58.17 - - - - -
imp + exp 57.32 65.61 61.18 imp exp 59.14 67.90 63.22
imp + BLLIP 56.28 65.61 60.59 imp BLLIP 53.80 99.82 69.92
imp + exp + BLLIP 55.81 65.26 60.16 imp exp + BLLIP 53.90 99.82 70.01
Temp. imp 16.46 63.86 26.17 - - - - -
BLLIP 17.31 43.37 24.74 - - - - -
exp 15.46 36.14 21.66 - - - - -
imp + exp 15.35 39.76 22.15 imp exp 18.60 63.86 28.80
imp + BLLIP 14.74 33.73 20.51 imp BLLIP 18.12 67.47 28.57
imp + exp + BLLIP 15.94 39.76 22.76 imp exp + BLLIP 19.08 65.06 29.51
Table 2: Performance of precision, recall and F1 for 4 Level 1 relation classes. ?-? indicates N.A.
Single-task Multi-task
Level 2 type Data P R F1 Data Data P R F1
(main) (aux)
Asynchronous imp 11.36 74.55 19.71 imp exp + BLLIP 23.08 21.82 22.43
Synchrony imp - - - imp exp + BLLIP - - -
Cause imp 36.38 64.34 46.48 imp exp + BLLIP 36.01 67.65 47.00
Contrast imp 20.07 42.54 27.27 imp exp + BLLIP 20.70 52.99 29.77
Concession imp - - - imp exp + BLLIP - - -
Conjunction imp 26.35 63.46 37.24 imp exp + BLLIP 26.29 73.56 38.73
Instantiation imp 22.78 53.78 32.00 imp exp + BLLIP 22.55 57.98 32.47
Restatement imp 23.11 67.61 34.45 imp exp + BLLIP 26.93 53.99 35.94
Alternative imp - - - imp exp + BLLIP - - -
List imp - - - imp exp + BLLIP - - -
Table 3: Performance of precision, recall and F1 for 10 Level 2 relation types. ?-? indicates 0.00.
cession, Alternative and List, the classifier labels
no instances due to the small percentages for these
four types.
Table 4 summarizes the one-tailed t-test results
on the top level of PDTB between the best single
task learning system (i.e., imp) and three multi-
task learning systems (imp:exp+BLLIP indicates
that imp is used for main task and the combi-
nation of exp and BLLIP are for auxiliary task).
The systems with insignificant performance differ-
ences are grouped into one set and ?>? and ?>>?
denote better than at significance level 0.01 and
0.001 respectively.
5.3 Discussion
From Table 2 to Table 4, several findings can be
found as follows.
We can see that the multi-task learning sys-
tems perform consistently better than the single
task learning systems for the prediction of implicit
discourse relations. Our best multi-task learning
system achieves an averaged F1 improvement of
5.86% over the best single task learning system on
the top level of PDTB relations. Specifically, for
482
Class One-tailed t-test results
Comp. (imp:exp+BLLIP, imp:exp, imp:BLLIP) >> (imp)
Cont. (imp:exp+BLLIP, imp:BLLIP) >> (imp:exp) > (imp)
Exp. (imp:exp+BLLIP, imp:BLLIP) >> (imp:exp) > (imp)
Temp. (imp:exp+BLLIP, imp:exp, imp:BLLIP) >> (imp)
Table 4: Statistical significance tests results.
the relations Comp., Cont., Exp., Temp., our best
multi-task learning system achieve 4.26%, 7.06%,
8.8% and 3.34% F1 improvements over the best
single task learning system. It indicates that using
synthetic implicit data as auxiliary task greatly im-
proves the performance of the main task. This is
confirmed by the following t-tests in Table 4.
In contrast to the performance of multi-task
learning, the performance of the best single task
learning system has been achieved on natural im-
plicit discourse data alone. This finding is con-
sistent with (Sporleder and Lascarides, 2008). It
indicates that under single task learning, directly
adding synthetic implicit data to increase the num-
ber of training data cannot be helpful to implicit
discourse relation classification. The possible rea-
sons result from (1) the different nature of implicit
and explicit discourse data in linguistics and (2)
the noise brought from synthetic implicit data.
Based on the above analysis, we state that it is
the way of utilizing synthetic implicit data that is
important for implicit discourse relation classifica-
tion.
Although all three multi-task learning systems
outperformed single task learning systems, we
find that the two synthetic implicit data sets have
not been shown a universally consistent perfor-
mance on four top level PDTB relations. On one
hand, for the relations Comp. and Temp., the per-
formance of the two synthetic implicit data sets
alone and their combination are comparable to
each other and there is no significant difference
between them. On the other hand, for the rela-
tions Cont. and Exp., the performance of exp data
is inferior to that of BLLIP and their combination.
This is contrary to our original expectation that exp
data which has been manually annotated for dis-
course connective disambiguation should outper-
form BLLIP which contains a lot of noise. This
finding indicates that under the multi-task learn-
ing, it may not be worthy of using manually anno-
tated corpus to generate auxiliary data. It is quite
promising since it can provide benefits to reducing
the cost of human efforts on corpus annotation.
5.4 Ambiguity Analysis
Although our experiments show that synthetic im-
plicit data can help implicit discourse relation clas-
sification under multi-task learning framework,
the overall performance is still quite low (44.64%
in F1). Therefore, we analyze the types of ambi-
guity in relations and connectives in order to mo-
tivate possible future work.
5.4.1 Ambiguity of implicit relation
Without explicit discourse connective, the implicit
discourse relation instance can be understood in
two or more different ways. Given the example
E2 in PDTB, the PDTB annotators explain it as
Contingency or Expansion relation and manually
insert corresponding implicit connective for one
thing or because to express its relation.
(E2) Arg1:Now the stage is set for the battle to
play out
Arg2:The anti-programmers are getting
some helpful thunder from Congress
Connective1:because
Sense1:Contingency.Cause.Reason
Connective2:for one thing
Sense2:Expansion.Instantiation
(wsj 0118)
Thus the ambiguity of implicit discourse rela-
tions makes this task difficult in itself.
5.4.2 Ambiguity of discourse connectives
As we mentioned before, even given an explicit
discourse connective in text, its discourse rela-
tion still can be explained in two or more differ-
ent ways. And for different connectives, the am-
biguity of relation senses is quite different. That
is, the most frequent sense is not always the only
sense that a connective expresses. In example E3,
?since? is explained by annotators to express Tem-
poral or Contingency relation.
(E3) Arg1:MiniScribe has been on the rocks
Arg2:since it disclosed early this year that
its earnings reports for 1988 weren?t accu-
rate.
483
Sense1:Temporal.Asynchronous.Succession
Sense2:Contingency.Cause.Reason
(wsj 0003)
In PDTB, ?since? appears 184 times in explicit
discourse relations. It expresses Temporal relation
for 80 times, Contingency relation for 94 times
and both Temporal and Contingency for 10 time
(like example E3). Therefore, although we use its
most frequent sense, i.e., Contingency, to automat-
ically extract sentences and label them, almost less
than half of them actually express Temporal rela-
tion. Thus the ambiguity of discourse connectives
is another source which has brought noise to data
when we generate synthetical implicit discourse
relation.
6 Conclusions
In this paper, we present a multi-task learning
method to improve implicit discourse relation
classification by leveraging synthetic implicit dis-
course data. Results on PDTB show that under
the framework of multi-task learning, using syn-
thetic discourse data as auxiliary task significantly
improves the performance of main task. Our best
multi-task learning system achieves an averaged
F1 improvement of 5.86% over the best single task
learning system on the top level of PDTB rela-
tions. Specifically, for the relations Comp., Cont.,
Exp., Temp., our best multi-task learning system
achieves 4.26%, 7.06%, 8.8%, and 3.34% F1 im-
provements over a state of the art baseline system.
This indicates that it is the way of utilizing syn-
thetic discourse examples that is important for im-
plicit discourse relation classification.
Acknowledgements
This research is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500), Doctoral Fund of Ministry of
Education of China (No. 20090076120029) and
Shanghai Knowledge Service Platform Project
(No. ZF1213).
References
R.K. Ando and T. Zhang. 2005a. A framework for
learning predictive structures from multiple tasks
and unlabeled data. The Journal of Machine Learn-
ing Research, 6:1817?1853.
R.K. Ando and T. Zhang. 2005b. A high-performance
semi-supervised learning method for text chunking.
pages 1?9. Association for Computational Linguis-
tics. Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
A. Argyriou, C.A. Micchelli, M. Pontil, and Y. Ying.
2008. A spectral regularization framework for
multi-task structure learning. Advances in Neural
Information Processing Systems, 20:2532.
J. Baxter. 2000. A model of inductive bias learning. J.
Artif. Intell. Res. (JAIR), 12:149?198.
S.J. Blair-Goldensohn. 2007. Long-answer question
answering and rhetorical-semantic relations. Ph.D.
thesis.
E. Bonilla, K.M. Chai, and C. Williams. 2008. Multi-
task gaussian process prediction. Advances in Neu-
ral Information Processing Systems, 20(October).
L. Carlson, D. Marcu, and M.E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. pages 1?10. As-
sociation for Computational Linguistics. Proceed-
ings of the Second SIGdial Workshop on Discourse
and Dialogue-Volume 16.
R. Caruana. 1997. Multitask learning. Machine
Learning, 28(1):41?75.
P. Cimiano, U. Reyle, and J. Saric. 2005. Ontology-
driven discourse analysis for information extraction.
Data and Knowledge Engineering, 55(1):59?83.
Eugene Charniak David McClosky and Mark Johnson.
2008. Bllip north american news text, complete.
T. Evgeniou and M. Pontil. 2004. Regularized multi?
task learning. pages 109?117. ACM. Proceedings
of the tenth ACM SIGKDD international conference
on Knowledge discovery and data mining.
H. Hernault, D. Bollegala, and M. Ishizuka. 2010. A
semi-supervised approach to improve classification
of infrequent discourse relations using feature vector
extension. pages 399?409. Association for Compu-
tational Linguistics. Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing.
H. Hernault, D. Bollegala, and M. Ishizuka. 2011.
Semi-supervised discourse relation classification
with structural learning. In Proceedings of the 12th
international conference on Computational linguis-
tics and intelligent text processing - Volume Part
I, CICLing?11, pages 340?352, Berlin, Heidelberg.
Springer-Verlag.
T. Jebara. 2004. Multi-task feature and kernel se-
lection for svms. page 55. ACM. Proceedings of
the twenty-first international conference on Machine
learning.
B. Levin. 1993. English verb classes and alternations:
A preliminary investigation, volume 348. University
of Chicago press Chicago, IL:.
484
Z. Lin, M.Y. Kan, and H.T. Ng. 2009. Recogniz-
ing implicit discourse relations in the penn discourse
treebank. pages 343?351. Association for Compu-
tational Linguistics. Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1.
W.C. Mann and S.A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text-Interdisciplinary Journal for the
Study of Discourse, 8(3):243?281.
D. Marcu and A. Echihabi. 2002. An unsupervised
approach to recognizing discourse relations. pages
368?375. Association for Computational Linguis-
tics. Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics.
PDTB-Group. 2008. The penn discourse treebank 2.0
annotation manual. Technical report, Institute for
Research in Cognitive Science, University of Penn-
sylvania.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova,
A. Lee, and A. Joshi. 2008. Easily identifiable dis-
course relations. Citeseer. Proceedings of the 22nd
International Conference on Computational Linguis-
tics (COLING 2008), Manchester, UK, August.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. pages 683?691. Association for Computational
Linguistics. Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2-Volume 2.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
In Proceedings of LREC.
M. Saito, K. Yamamoto, and S. Sekine. 2006. Us-
ing phrasal patterns to identify discourse relations.
pages 133?136. Association for Computational Lin-
guistics. Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers on XX.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. pages 149?156. Association for Computational
Linguistics. Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14(03):369?416.
S. Thrun. 1996. Is learning the n-th thing any easier
than learning the first? Advances in Neural Infor-
mation Processing Systems, pages 640?646.
S. Verberne, L. Boves, N. Oostdijk, and P.A. Coppen.
2007. Evaluating discourse-based answer extraction
for why-question answering. pages 735?736. ACM.
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieval.
W.T. Wang, J. Su, and C.L. Tan. 2010. Kernel based
discourse relation recognition with temporal order-
ing information. pages 710?719. Association for
Computational Linguistics. Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics.
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,
and R. Sauri. 2006. Classification of discourse co-
herence relations: An exploratory study using multi-
ple knowledge sources. pages 117?125. Association
for Computational Linguistics. Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Rec-
ognizing contextual polarity: An exploration of fea-
tures for phrase-level sentiment analysis. Computa-
tional Linguistics, 35(3):399?433.
F. Wolf, E. Gibson, A. Fisher, and M. Knight. 2005.
The discourse graphbank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
Z.M. Zhou, Y. Xu, Z.Y. Niu, M. Lan, J. Su, and C.L.
Tan. 2010. Predicting discourse connectives for im-
plicit discourse relation recognition. pages 1507?
1514. Association for Computational Linguistics.
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters.
485
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 226?229,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
ECNU: Effective Semantic Relations Classification without Complicated
Features or Multiple External Corpora
Yuan Chen
?
, Man Lan
?,?
, Jian Su
?
, Zhi Min Zhou
?
, Yu Xu
?
?
East China Normal University, Shanghai, PRC.
?
Institute for Infocomm Research, Singapore.
lanman.sg@gmail.com
Abstract
This paper describes our approach to the
automatic identification of semantic rela-
tions between nominals in English sen-
tences. The basic idea of our strategy
is to develop machine-learning classifiers
which: (1) make use of class-independent
features and classifier; (2) make use of
a simple and effective feature set without
high computational cost; (3) make no use
of external annotated or unannotated cor-
pus at all. At SemEval 2010 Task 8 our
system achieved an F-measure of 75.43%
and a accuracy of 70.22%.
1 Introduction
Knowledge extraction of semantic relations be-
tween pairs of nominals from English text is one
important application both as an end in itself and
as an intermediate step in various downstream
NLP applications, such as information extraction,
summarization, machine translation, QA etc. It is
also useful for many auxiliary tasks such as word
sense disambiguation, language modeling, para-
phrasing and discourse relation processing.
In the past decade, semantic relation classifica-
tion has attracted a lot of interest from researchers
and a wide variety of relation classification
schemes exist in the literature. However, most
research work is quite different in definition of
relations and granularities of various applications.
That is, there is little agreement on relation
inventories. SemEval 2010 Task 8 (Hendrickx
et al, 2008) provides a new standard benchmark
for semantic relation classification to a wider
community, where it defines 9 relations includ-
ing CAUSE-EFFECT, COMPONENT-WHOLE,
CONTENT-CONTAINER, ENTITY-DESTINATION,
ENTITY-ORIGIN, INSTRUMENT-AGENCY,
MEMBER-COLLECTION, MESSAGE-TOPIC,
PRODUCT-PRODUCER, and a tenth pseudo-
relation OTHER (where relation is not one of the
9 annotated relations).
Unlike the previous semantic relation task in
SemEval 2007 Task 4, the current evaluation pro-
vides neither query pattern for each sentence nor
manually annotated word sense (in WordNet se-
mantic) for each nominals. Since its initiative is
to provide a more realistic real-world application
design that is practical, any classification system
must be usable without too much effort. It needs
to be easily computable. So we need to take into
account the following special considerations.
1. The extracted features for relation are ex-
pected to be easily computable. That is, the
steps in the feature extraction process are to
be simple and direct for the purpose of reduc-
ing errors possibly introduced by many NLP
tools. Furthermore, a unified (global) feature
set is set up for all relations rather than for
each relation.
2. Most previous work at SemEval 2007 Task
4 leveraged on external theauri or corpora
(whether unannotated or annotated) (Davi-
dov and Rappoport, 2008), (Costello, 2007),
(Beamer et al, 2007) and (Nakov and Hearst,
2008) that make the task adaption to different
domains and languages more difficult, since
they would not have such manually classified
or annotated corpus available. From a practi-
cal point of view, our system would make use
of less resources.
3. Most previous work at Semeval 2007 Task
4 constructed several local classifiers on dif-
ferent algorithms or different feature subsets,
one for each relation (Hendrickx et al, 2007)
and (Davidov and Rappoport, 2008). Our ap-
proach is to build a global classifier for all
relations in practical NLP settings.
226
Based on the above considerations, the idea of
our system is to make use of external resources as
less as possible. The purpose of this work is two-
fold. First, it provides an overview of our simple
and effective process for this task. Second, it com-
pares different features and classification strate-
gies for semantic relation.
Section 2 presents the system description. Sec-
tion 3 describes the results and discussions. Sec-
tion 4 concludes this work.
2 System Description
2.1 Features Extraction
For each training and test sentence, we reduce the
annotated target entities e1 and e2 to single nouns
noun1 and noun2, by keeping their last nouns only,
which we assume to be heads.
We create a global feature set for all relations.
The features extracted are of three types, i.e., lex-
ical, morpho-syntactic and semantic. The feature
set consists of the following 6 types of features.
Feature set 1: Lemma of target entities e1
and e2. The lemma of the entities annotated in
the given sentence.
Feature set 2: Stem and POS of words be-
tween e1 and e2. The stem and POS tag of the
words between two nominals. First all the words
between two nominals were extracted and then the
Porter?s stemming was performed to reduce words
to their base forms (Porter, 1980). Meanwhile,
OpenNLP postag tool was used to return part-of-
speech tagging for each word.
Feature set 3: syntactic pattern derived from
syntactic parser between e1 and e2. Typically,
the verb phrase or preposition phrase which con-
tain the nominals are important for relation clas-
sification. Therefore, OpenNLP Parser was per-
formed to do full syntactic parsing for each sen-
tence. Then for each nominal, we look for its par-
ent node in the syntactic tree until the parent node
is a verb phrase or preposition phrase. Then the
label of this phrase and the verb or preposition of
this phrase were extracted as the syntactic features.
Besides, we also extracted other 3 feature types
with the aid of WordNet.
Feature set 4: WordNet semantic class of e1
and e2. The WordNet semantic class of each an-
notated entity in the relation. If the nominal has
two and more words, then we examine the seman-
tic class of ?w1 w2? in WordNet. If no result re-
turned from WordNet, we examine the semantic
class of head in the nominal. Since the cost of
manually WSD is expensive, the system simply
used the first (most frequent) noun senses for those
words.
Feature set 5: meronym-holonym relation
between e1 and e2. The meronym-holonym
relation between nominals. These information
are quite important for COMPONENT-WHOLE and
MEMBER-COLLECTION relations. WordNet3.0
provides meronym and holonym information for
some nouns. The features are extracted in the fol-
lowing steps. First, for nominal e1, we extract its
holonym from WN and for nominal e2, we extract
its Synonyms/Hypernyms. Then, the system will
check if there is same word between e1?s holonym
and e2?s synonym & hypernym. The yes or no
result will be a binary feature. If yes, we also ex-
amine the type of this match is ?part of ? or ?mem-
ber of ? in holonym result. Then this type is also
a binary feature. After that, we exchange the posi-
tion of e1 and e2 and perform the same process-
ing. By creating these features, the system can
also take the direction of relations into account.
Feature set 6: hyponym-hypernym rela-
tion between nominal and the word of ?con-
tainer?. This feature is designed for CONTENT-
CONTAINER relation. For each nominal, WordNet
returns its hypernym set. Then the system examine
if the hypernym set contains the word ?container?.
The result leads to a binary feature.
2.2 Classifier Construction
Our system is to build up a global classifier based
on global feature set for all 9 non-Other relations.
Generally, for this multi-class task, there are two
strategies for building classifier, which both con-
struct classifier on a global feature set. The first
scheme is to treat this multi-class task as an multi-
way classification. Since each pair of nominals
corresponds to one relation, i.e., single label clas-
sification, we build up a 10-way SVM classifier for
all 10 relations. Here, we call it multi-way clas-
sification. That is, the system will construct one
single global classifier which can classify 10 rela-
tions simultaneously in a run. The second scheme
is to split this multi-class task into multiple binary
classification tasks. Thus, we build 9 binary SVM
classifiers, one for each non-Other relation. Noted
that in both strategies the classifiers are built on
global feature set for all relations. For the sec-
ond multiple binary classification, we also exper-
227
imented on different prob. thresholds, i.e., 0.25
and 0.5. Furthermore, in order to reduce errors
and boost performance, we also adopt the major-
ity voting strategy to combine different classifiers.
3 Results and Discussion
3.1 System Configurations and Results
The classifiers for all relations were optimized
independently in a number of 10-fold cross-
validation (CV) experiments on the provided train-
ing sets. The feature sets and learning algorithms
which were found to obtain the highest accuracies
for each relation were then used when applying the
classifiers to the unseen test data.
Table 1 summaries the 7 system configurations
we submitted and their performance on the test
data.
Among the above 7 system, SR5 system shows
the best macro-averaged F1 measure. Table 2 de-
scribes the statistics and performance obtained per
relation on the SR5 system.
Table 3 shows the performance of these 7 sys-
tems on the test data as a function of training set
size.
3.2 Discussion
The first three systems are based on three feature
sets, i.e.,F1-F3, with different classification strat-
egy. The next three systems are based on all six
feature sets with different classification strategy.
The last system adopts majority voting scheme on
the results of four systems, i.e., SR1, SR2, SR4
and SR5. Based on the above series of exper-
iments and results shown in the above 3 tables,
some interesting observations can be found as fol-
lows.
Obviously, although we did not perform WSD
on each nominal and only took the first noun sense
as semantic class, WordNet significantly improved
the performance. This result is consistent with
many previous work on Semeval 2007 Task 4 and
once again it shows that WordNet is important
for semantic relation classification. Specifically,
whether for multi-way classification or multiple
binary classification, the systems involved features
extracted from WordNet performed better than the
others not involved WN, for example, SR4 better
than SR1 (74.82% vs 60.08%), SR5 better than
SR2 (75.43% vs 72.59%), SR6 better than SR3
(72.19% vs 68.50%).
Generally, the performance of multiple binary
classifier is better than multi-way classifier. That
means, given a global feature set for 9 relations,
the performance of 9 binary classifiers is better
than a 10-way classifier. Specifically, when F1-F3
are involved, SR2 (72.59%) and SR3 (68.50%) are
both better than SR1 (60.08%). However, when
F1-F6 feature sets are involved, the performance
of SR4 is between that of SR5 and SR6 in terms of
macro-averaged F
1
measure. With respect to ac-
curacy measure (Acc), SR4 system performs the
best.
Moreover, for multiple binary classification, the
threshold of probability has impact on the perfor-
mance. Generally, the system with prob. threshold
0.25 is better than that with 0.5, for example, SR2
better than SR3 (72.59% vs 68.50%), SR5 better
than SR6 (75.43% vs 72.19%).
As an ensemble system, SR7 combines the re-
sults of SR1, SR2, SR4 and SR5. However, this
majority voting strategy has not shown significant
improvements. The possible reason may be that
these classifiers come from a family of SVM clas-
sifiers and thus the random errors are not signifi-
cantly different.
Besides, one interesting observation is that SR4
system achieved the top 2 performance on TD1
data amongst all participating systems. This
shows that, even with less training data, SR4 sys-
tem achieves good performance.
Acknowledgments
This work is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
I. Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov, D.
?
O S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Ro-
mano and S. Szpakowicz. SemEval-2010 Task 8:
Multi-Way Classification of Semantic Relations Be-
tween Pairs of Nominals. In Proceedings of the 5th
SIGLEX Workshop on Semantic Evaluation, pp.94-
99, 2010, Uppsala, Sweden.
D. Davidov and A. Rappoport. Classification of
Semantic Relationships between Nominals Using
Pattern Clusters. Proceedings of ACL-08: HLT,
pp.227-235, 2008.
F. J. Costello. UCD-FC: Deducing semantic rela-
tions using WordNet senses that occur frequently
228
Run Feature Set Classifier P (%) R (%) F
1
(%) Acc (%)
SR1 F1-F3 multi-way classification 70.69 58.05 60.08 57.05
SR2 F1-F3 multiple binary (prob. threshold =0.25) 74.02 71.61 72.59 67.10
SR3 F1-F3 multiple binary (prob. threshold =0.5) 80.25 60.92 68.50 62.02
SR4 F1-F6 multi-way classification 75.72 74.16 74.82 70.52
SR5 F1-F6 multiple binary (prob. threshold =0.25) 75.88 75.29 75.43 70.22
SR6 F1-F6 multiple binary (prob. threshold =0.5) 83.08 64.72 72.19 65.81
SR7 F1-F6 majority voting based on SR1, SR2, SR4 and SR5 74.83 75.97 75.21 70.15
Table 1: Summary of 7 system configurations and performance on the test data. Precision, Recall, F1
are macro-averaged for system?s performance on 9 non-Other relations and evaluated with directionality
taken into account.
Run Total # P (%) R (%) F
1
(%) Acc (%)
Cause-Effect 328 83.33 86.89 85.07 86.89
Component-Whole 312 74.82 65.71 69.97 65.71
Content-Container 192 79.19 81.25 80.21 81.25
Entity-Destination 292 79.38 86.99 83.01 86.99
Entity-Origin 258 81.01 81.01 81.01 81.01
Instrument-Agency 156 63.19 58.33 60.67 58.33
Member-Collection 233 73.76 83.26 78.23 83.26
Message-Topic 261 75.2 73.18 74.17 73.18
Product-Producer 231 73.06 61.04 66.51 61.04
Other 454 38.56 40.09 39.31 40.09
Micro-Average 76.88 76.27 76.57 70.22
Macro-Average 75.88 75.29 75.43 70.22
Table 2: Performance obtained per relation on SR5 system. Precision, Recall, F1 are macro-averaged for
system?s performance on 9 non-Other relations and evaluated with directionality taken into account.
Run TD1 TD2 TD3 TD4
F
1
(%) Acc (%) F
1
(%) Acc (%) F
1
(%) Acc (%) F
1
(%) Acc (%)
SR1 52.13 49.50 56.58 54.84 58.16 56.16 60.08 57.05
SR2 46.24 38.90 47.99 40.45 69.83 64.67 72.59 67.10
SR3 39.89 34.56 42.29 36.66 65.47 59.59 68.50 62.02
SR4 67.95 63.45 70.58 66.14 72.99 68.94 74.82 70.52
SR5 49.32 41.59 50.70 42.77 72.63 67.72 75.43 70.22
SR6 42.88 36.99 45.54 39.57 69.87 64.00 72.19 65.81
SR7 58.67 52.71 58.87 53.18 72.79 68.09 75.21 70.15
Table 3: Performance of these 7 systems on the test data as a function of training set size. The four
training subsets, TD1, TD2, TD3 and TD4, have 1000, 2000, 4000 and 8000 (complete) training samples
respectively. F1 is macro-averaged for system?s performance on 9 non-Other relations and evaluated
with directionality taken into account.
in a database of noun-noun compounds. ACL Se-
mEval?07 Workshop, pp.370C373, 2007.
B. Beamer, S. Bhat, B. Chee, A. Fister, A. Rozovskaya
and R.Girju. UIUC: A knowledge-rich approach
to identifying semantic relations between nominals.
ACL SemEval?07 Workshop, pp.386-389, 2007.
I. Hendrickx, R. Morante, C. Sporleder and A. Bosch.
ILK: machine learning of semantic relations with
shallow features and almost no data. ACL Se-
mEval?07 Workshop, pp.187C190, 2007.
P. Nakov and M. A. Hearst. Solving Relational Simi-
larity Problems Using the Web as a Corpus. In Pro-
ceedings of ACL, pp.452-460, 2008.
M. Porter. An algorithm for suffix stripping. In Pro-
gram, vol. 14, no. 3, pp.130-137, 1980.
229
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 139?146,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
The Effects of Discourse Connectives Prediction on Implicit Discourse
Relation Recognition
Zhi Min Zhou?, Man Lan?,?, Zheng Yu Niu?, Yu Xu?, Jian Su?
?East China Normal University, Shanghai, PRC.
?Baidu.com Inc., Beijing, PRC.
?Institute for Infocomm Research, Singapore.
51091201052@ecnu.cn, lanman.sg@gmail.com
Abstract
Implicit discourse relation recognition is
difficult due to the absence of explicit
discourse connectives between arbitrary
spans of text. In this paper, we use lan-
guage models to predict the discourse con-
nectives between the arguments pair. We
present two methods to apply the pre-
dicted connectives to implicit discourse
relation recognition. One is to use the
sense frequency of the specific connec-
tives in a supervised framework. The
other is to directly use the presence of the
predicted connectives in an unsupervised
way. Results on PDTB2 show that using
language model to predict the connectives
can achieve comparable F-scores to the
previous state-of-art method. Our method
is quite promising in that not only it has
a very small number of features but also
once a language model based on other re-
sources is trained it can be more adaptive
to other languages and domains.
1 Introduction
Discourse relation analysis involves identifying
the discourse relations (e.g., the comparison re-
lation) between arbitrary spans of text, where
the discourse connectives (e.g., ?however?, ?be-
cause?) may or may not explicitly exist in the text.
This analysis is one important application both as
an end in itself and as an intermediate step in var-
ious downstream NLP applications, such as text
summarization, question answering etc.
As discussed in (Pitler and Nenkova., 2009b),
although explicit discourse connectives may have
two types of ambiguity, i.e., one is discourse or
non-discourse usage (?once? can be either a tem-
poral connective or a word meaning ?formerly?),
the other is discourse relation sense ambiguity
(?since? can serve as either a temporal or causal
connective), their study shows that for explicit
discourse relations in Penn Discourse Treebank
(PDTB) corpus, the most general 4 senses, i.e.,
Comparison (Comp.), Contingency (Cont.), Tem-
poral (Temp.) and Expansion (Exp.), can be eas-
ily addressed by the presence of discourse con-
nectives and a simple method only considering the
sense frequency of connectives can achieve more
than 93% accuracy. This indicates the importance
of connectives for discourse relation recognition.
However, with implicit discourse relation
recognition, there is no connective between the
textual arguments, which results in a very difficult
task. In recent years, a multitude of efforts have
been employed to solve this task. One approach
is to exploit various linguistically informed fea-
tures extracted from human-annotated corpora in
a supervised framework (Pitler et al, 2009a) and
(Lin et al, 2009). Another approach is to perform
recognition without human-annotated corpora by
creating synthetic examples of implicit relations in
an unsupervised way (Marcu and Echihabi, 2002).
Moreover, our initial study on PDTB implicit
relation data shows that the averaged F-score for
the most general 4 senses can reach 91.8% when
we obtain the sense of test examples by map-
ping each implicit connective to its most frequent
sense (i.e., sense recognition using gold-truth im-
plicit connectives). This high F-score performance
again proves that the connectives are very crucial
source for implicit relation recognition.
In this paper, we present a new method to ad-
dress the problem of recognizing implicit dis-
course relation. This method is inspired by the
above observations, especially the two gold-truth
results, which reveals that discourse connectives
are very important signals for discourse relation
recognition. Our basic idea is to recover the im-
plicit connectives (not present in real text) be-
tween two spans of text with the use of a language
139
model trained on large amount of raw data without
any human-annotation. Then we use these pre-
dicted connectives to generate feature vectors in
two ways for implicit discourse relation recogni-
tion. One is to use the sense frequency of the spe-
cific connectives in a supervised framework. The
other is to directly use the presence of the pre-
dicted connectives in an unsupervised way.
We performed evaluation on explicit and im-
plicit relation data sets in the PDTB 2 corpus. Ex-
perimental results showed that the two methods
achieved comparable F-scores to the state-of-art
methods. It indicates that the method using lan-
guage model to predict connectives is very useful
in solving this task.
The rest of this paper is organized as follows.
Section 2 reviews related work. Section 3 de-
scribes our methods for implicit discourse relation
recognition. Section 4 presents experiments and
results. Section 5 offers some conclusions.
2 Related Work
Existing works on automatic recognition of im-
plicit discourse relations fall into two categories
according to whether the method is supervised or
unsupervised.
Some works perform relation recognition with
supervised methods on human-annotated corpora,
for example, the RST Bank (Carlson et al, 2001)
used by (Soricut and Marcu, 2003), adhoc anno-
tations used by (Girju, 2003) and (Baldridge and
Lascarides, 2005), and the GraphBank (Wolf et al,
2005) used by (Wellner et al, 2006).
Recently the release of the Penn Discourse
TreeBank (PDTB) (Prasad et al, 2006) has sig-
nificantly expanded the discourse-annotated cor-
pora available to researchers, using a comprehen-
sive scheme for both implicit and explicit rela-
tions. (Pitler et al, 2009a) performed implicit re-
lation classification on the second version of the
PDTB. They used several linguistically informed
features, such as word polarity, verb classes, and
word pairs, showing performance increases over a
random classification baseline. (Lin et al, 2009)
presented an implicit discourse relation classifier
in PDTB with the use of contextual relations, con-
stituent Parse Features, dependency parse features
and cross-argument word pairs. Although both of
two methods achieved the state of the art perfor-
mance for automatical recognition of implicit dis-
course relations, due to lack of human-annotated
corpora, their approaches are not very useful in the
real word.
Another line of research is to use the unsuper-
vised methods on unhuman-annotated corpus.
(Marcu and Echihabi, 2002) used several pat-
terns to extract instances of discourse relations
such as contrast and elaboration from unlabeled
corpora. Then they used word-pairs between argu-
ments as features for building classification mod-
els and tested their model on artificial data for im-
plicit relations.
Subsequently other studies attempt to ex-
tend the work of (Marcu and Echihabi, 2002).
(Sporleder and Lascarides, 2008) discovered that
Marcu and Echihabi?s models do not perform as
well on implicit relations as one might expect
from the test accuracy on synthetic data. (Gold-
ensohn, 2007) extended the work of (Marcu and
Echihabi, 2002) by refining the training and clas-
sification process using parameter optimization,
topic segmentation and syntactic parsing. (Saito
et al, 2006) followed the method of (Marcu and
Echihabi, 2002) and conducted experiments with
a combination of cross-argument word pairs and
phrasal patterns as features to recognize implicit
relations between adjacent sentences in a Japanese
corpus.
Previous work showed that with the use of some
patterns, structures, or the pairs of words, rela-
tion classification can be performed using unsu-
pervised methods.
In contrast to existing work, we investigated a
new knowledge source, i.e., implicit connectives
predicted using a language model, for implicit re-
lation recognition. Moreover, this method can
be applied in both supervised and unsupervised
ways by generating features on labeled and unla-
beled training data and then performing implicit
discourse connectives recognition.
3 Methodology
3.1 Predicting implicit connectives via a
language model
Previous work (Pitler and Nenkova., 2009b)
showed that with the presence of discourse con-
nectives, explicit discourse relations in PDTB can
be easily identified with more than 90% F-score.
Our initial study on PDTB human-annotated im-
plicit relation data shows that the averaged F-score
for the most general 4 senses can reach 91.8%
when we simply map each implicit connective to
140
its most frequent sense. These high F-scores indi-
cate that the connectives are very crucial source of
information for both explicit and implicit relation
recognition. However, for implicit relations, there
are no explicitly discourse connectives in real text.
This built-in absence makes the implicit relation
recognition task quite difficult. In this work we
overcome this difficulty by inserting connectives
into the two arguments with the use of a language
model.
Following the annotation scheme of PDTB, we
assume that each implicit connective takes two
arguments, denoted as Arg1 and Arg2. Typi-
cally, there are two possible positions for most
of implicit connectives, i.e., the position before
Arg1 and the position between Arg1 and Arg2.
Given a set of implicit connectives {ci}, we gen-
erate two synthetic sentences, ci+Arg1+Arg2 and
Arg1+ci+Arg2 for each ci, denoted as Sci,1 and
Sci,2. Then we calculate the perplexity (an intrin-
sic score) of these sentences with the use of a lan-
guage model, denoted as Ppl(Sci,j). According to
the value of Ppl(Sci,j) (the lower the better), we
can rank these sentences and select the connec-
tives in top N sentences as implicit connectives
for this argument pair. Here the language model
may be trained on any large amount of unanno-
tated corpora that can be cheaply acquired. Typi-
cally, a large corpora with the same domain as the
test data will be used for training language model.
Therefore, we chose news corpora, such as North
American News Corpora.
After that, we use the top N predicted connec-
tives to generate different feature vectors and per-
form the classification in two ways. One is to use
the sense frequency of predicted connectives in a
supervised framework. The other is to directly use
the presence of the predicted connectives in an un-
supervised way. The two approaches are described
as follows.
3.2 Using sense frequency of predicted
discourse connectives as features
After the above procedure, we get a sorted set of
predicted discourse connectives. Due to the pres-
ence of an implicit connective, the implicit dis-
course relation recognition task can be addressed
with the methods for explicit relation recognition,
e.g., sense classification based only on connectives
(Pitler et al, 2009a). Inspired by their work, the
first approach is to use sense frequency of pre-
dicted discourse connectives as features. We take
the connective with the lowest perplexity value
(i.e., top 1 connective) as the real connective for
the arguments pair. Then we count the sense
frequency of this connective on the training set.
Figure 1 illustrates the procedure of generating
predicted discourse connective from a language
model and calculating its sense frequency from
training data. Here the calculation of sense fre-
quency of connective is based on the annotated
training data which has labeled discourse rela-
tions, thus this method is a supervised one.
Figure 1: Procedure of generating a predicted dis-
course connective and its sense frequency from the
training set and a language model.
Then we can directly use the sense frequency
to generate a 4-feature vector to perform the clas-
sification. For example, the sense frequency of
the connective but in the most general 4 senses
can be counted from training set as 691, 6, 49,
2, respectively. For a given pair of arguments,
if but is predicted as the top 1 connective based
on a language model, a 4-dimension feature vec-
tor (691, 6, 49, 2) is generated for this pair and
used for training and test procedure. Figure 2
and 3 show the training and test procedure for this
method.
Figure 2: Training procedure for the first ap-
proach.
141
Figure 3: Test procedure for the first approach.
3.3 Using presence or absence of predicted
discourse connective as features
(Pitler et al, 2008) showed that most connectives
are unambiguous and it is possible to obtain high-
accuracy in prediction of discourse senses due to
the simple mapping relation between connectives
and senses. Given two examples:
(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining is
getting heavier and heavier.
The two connectives, i.e., but in E1 and because
in E2, convey the Comparison and Contingency
senses respectively. In most cases, we can easily
recognize the relation sense by the appearance of
a discourse connective since it can be interpreted
in only one way. That means the ambiguity of
the mapping between sense and connective is quite
low. Therefore, the second approach is to use only
the presence of the top N predicted discourse con-
nectives to generate a feature vector for a given
pair of arguments.
4 Experiment
4.1 Data sets
We used PDTB as our data set to perform the eval-
uation of our methods. The corpus contains anno-
tations of explicit and implicit discourse relations.
The first evaluation is performed on the annotated
implicit data set. Following the work of (Pitler et
al., 2009a), we used sections 2-20 as the training
set, sections 21-22 as the test set and sections 0-
1 as the development set for parameter optimiza-
tion (e.g., N value). The second evaluation is per-
formed on the annotated explicit data set. We fol-
low the method used in (Sporleder and Lascarides,
2008) to remove the discourse connective from the
explicit instances and consider these processed in-
stances as implicit ones.
We constructed four binary classifiers to recog-
nize each main senses (i.e., Cont., Cont., Exp.,
Temp.) from the rest. For each sense we used
equal numbers of positive and negative instances
in training set. The negative instances were cho-
sen at random from the rest of training set. For
both evaluations all instances in sections 21-22
were used as test set. Table 1 lists the numbers
of positive and negative instances for each sense
in training, development and test sets of implicit
and explicit relation data sets.
4.2 Evaluation and classifier
To evaluate the performance of above systems, we
used two widely-used measures, F-score ( i.e., F1)
and accuracy. In addition, in this work we used
the LIBSVM toolkit to construct four linear SVM
classifiers for each sense.
4.3 Preprocessing
We used the SRILM toolkit to build a language
model and calculated the perplexity value for each
training and test sample. The steps are described
as follows. First, since perplexity is an intrin-
sic score to measure the similarity between train-
ing and test samples, in order to fit the restric-
tion of perplexity we chose 3 widely-used cor-
pora in the Newswire domain to train the language
model, i.e., (1) the New York part of BLLIP North
American News Text (Complete), (2) the Xin and
(3) the Ltw parts of the English Gigaword Fourth
Edition. For the BLLIP corpus with 1,796,386
automatically parsed English sentences, we con-
verted the parsed sentences into original textual
data. Some punctuation marks such as commas,
periods, minuses, right/left parentheses are con-
verted into their original form. For the Xin and
Ltw parts, we only used the Sentence Detector
toolkit in OpenNLP to split each sentence. Finally
we constructed 3-, 4- and 5-grams language mod-
els from these three corpora. Table 2 lists statis-
tics of different n-grams in the different language
models and different corpora.
Next, for each instance we combined its Arg1
and Arg2 with connectives obtained from PDTB.
There are two types of connectives, single con-
nectives (e.g. ?because? and ?but?) and paral-
142
Table 1: Statistics of positive and negative instances for each sense in training, development and test sets
of implicit and explicit relation data sets.
Implicit Explicit
Comp. Cont. Exp. Temp. Comp. Cont. Exp. Temp.
Train(Pos/Neg) 1927/1927 3375/3375 6052/6052 730/730 4080/4080 2732/2732 4609/4609 2663/2663
Dev(Pos/Neg) 191/997 292/896 651/537 54/1134 438/1071 295/1214 514/995 262/1247
Test(Pos/Neg) 146/912 276/782 556/502 67/991 388/1025 235/1178 501/912 289/1124
Table 2: Statistics of different n-grams in the dif-
ferent language models and different corpora.
n-gram BLLIP - Gigaword- Gigaword-
New York Xin Ltw
1-gram 1638156 2068538 2276491
2-grams 26156851 23961796 33504873
3-grams 80876435 77799100 101855639
4-grams 127142452 134410879 159791916
5-grams 146454530 168166195 183794771
lel connectives (such as ?not only . . . , but also?).
Since discourse connectives may appear not only
ahead of the Arg1, but also between Arg1 and
Arg2, we considered this case. Given a set of
possible implicit connectives {ci}, for a single
connective ci, we constructed two synthetic sen-
tences, ci+Arg1+Arg2 and Arg1+ci+Arg2. In case
of parallel connectives, we constructed one syn-
thetic sentence like ci,1+Arg1+ci,2+Arg2.
As a result, we obtain 198 synthetic sentences
(|ci| ? 2 for single connective or |ci| for parallel
connective) for each pair of arguments. Then we
converted all words to lower cases and used the
language model trained in the above step to calcu-
late its perplexity (the lower the better) value on
sentence level. The sentences were ranked from
low to high according to their perplexity scores.
For example, given a sentence with arguments pair
as follows:
Arg1: it increased its loan-loss reserves by $93
million after reviewing its loan portfolio,
Arg2: before the loan-loss addition it had operat-
ing profit of $10 million for the quarter.
we got the perplexity (Ppl) values for this argu-
ments pair in combination with two connectives
(but and by comparison) in two positions as fol-
lows:
1. but + Arg1 + Arg2: Ppl= 349.622
2. Arg1 + but + Arg2: Ppl= 399.339
3. by comparison + Arg1 + Arg2: Ppl= 472.206
4. Arg1 + by comparison + Arg2: Ppl= 543.051
In our second approach described in Section
3.3, we considered the combination of connectives
and their position as final features like mid but,
first but, where the features are binary, that is,
the presence or absence of the specific connective.
According to the value of Ppl(Sci,j), we tried var-
ious N values on development set to get the opti-
mal N value.
4.4 Results
Table 3 summarizes the best performance
achieved using gold-truth implicit connectives,
the previous state-of-art performance achieved
by (Pitler et al, 2009a) and our approaches.
The first line shows the result by mapping the
gold-truth implicit connectives directly to the
relation?s sense. The second line presents the best
result of (Pitler et al, 2009a). One thing worth
mentioning here is that for the Expansion relation,
(Pitler et al, 2009a) expanded both training and
test sets by including EntRel relation as positive
examples, which makes it impossible to perform
direct comparison. The third and fourth lines
show the best results using our first approach,
where the sense frequency is counted on explicit
and implicit training set respectively. The last line
shows the best result of our second approach only
considering the presence of top N connectives.
Table 4 summarizes the best performance using
gold-truth explicit connectives reported in (Pitler
and Nenkova., 2009b) and our two approaches.
Figure 4 shows the curves of averaged F-scores
on implicit connective classification with differ-
ent n-gram language models. From this figure we
can see that all 4-grams language models achieved
around 0.5% better averaged F-score than 3-grams
models. And except for Ltw corpus, other 5-grams
models achieved lower averaged F-score than 4-
grams models. Specially the 5-grams result of
New York corpus is much lower than its 3-grams
result.
Figure 5 shows the averaged F-scores of dif-
ferent top N on the New York corpus with 3-,
4- and 5-grams language models. The essential
143
Table 3: Best result of implicit relations compared with state-of-art methods.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Averaged
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Sense recognition using
gold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07) 91.78(98.02)
Best result in (Pitler et al, 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49) 40.57(62.75)
Use sense frequency in explicit training set 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97) 35.10(49.95)
Use sense frequency in implicit training set 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51) 29.07(64.70)
Use presence of top N connectives only 21.91(52.84) 39.53(50.85) 68.84(52.93) 11.91(6.33) 35.55(40.74)
Table 4: Best result of explicit relation conversion to implicit relation compared with results using the
same method.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Average
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Sense recognition using gold-truth
explicit connectives in (Pitler et al, 2009a) N/A N/A N/A N/A N/A(93.67)
Use sense frequency in explicit training set 41.62(50.96) 27.46(59.24) 48.44(50.88) 35.14(54.28) 38.17(53.84)
Use presence of top N connectives only 42.92(55.77) 31.83(56.05) 47.26(55.77) 37.89(58.24) 39.98(56.46)
0 10 20 30 40 50 60 70 80 90 100110120130140150160170180190200
30.0
30.5
31.0
31.5
32.0
32.5
33.0
33.5
34.0
34.5
 
 NY 3-gram
 NY 4-gram
 NY 5-gram
Top N value
A
v
e
r
a
g
e
d
 
F
-
S
c
o
r
e
Figure 5: Curves of averages F-score on New York 3-, 4- and 5-grams language models with different
top N values.
trend of these curves cannot be summarized in
one sentence. But we can see that the best aver-
aged F-scores mostly appeared in the range from
100 ? 160. For 4-grams and 5-grams models, the
system achieved the top averaged F-scores when
N = 20 as well.
4.5 Discussion
Experimental results on PDTB showed that using
predicted connectives achieved the comparable F-
scores of the state-of-art method.
From Table 3 we can find that our results are
closely to the best performance of previous state-
of-art methods in terms of averaged F-score. On
the Comparison sense, our first approach has an
improvement of more than 4% F-score on the pre-
vious state-of-art method (Pitler et al, 2009a). As
we mentioned before, for the Expansion sense,
they included EntRel relation to expand the train-
ing set and test set, which makes it impossible to
perform a direct comparison. Since the positive in-
stances size has been increased by 50%, they may
achieve a higher F-score than our approach. For
other relations, our best performance is slightly
lower than theirs. While bearing in mind that our
approach only uses a very small amount of fea-
tures for implicit relation recognition. Compared
144
3-gram 4-gram 5-gram
31.0
31.2
31.4
31.6
31.8
32.0
32.2
32.4
32.6
 
 New York
 Xin
 Ltw
n-gram
A
v
e
r
a
g
e
d
 
F
-
s
c
o
r
e
Figure 4: Curves of averaged F-score on implicit
connective classification with n-Gram language
model.
with other approaches involving thousands of fea-
tures, our method is quite promising.
From Table 4 we observe comparable averaged
F-score (39.98% F-score) on explicit relation data
set to that on implicit relation data set. Previ-
ously, (Sporleder and Lascarides, 2008) also used
the same conversion method to perform implicit
relation recognition on different corpora and their
best result is around 33.69% F-score. Although
the two results cannot be compared directly due to
different data sets, the magnitude of performance
quantities is comparable and reliable.
By comparing with the above different systems,
we find several useful observations. First, our
method using predicted implicit connectives via a
language model can help the task of implicit dis-
course relation recognition. The results are com-
parable to the previous state-of-art studies. Sec-
ond, our method has a lot of advantages, i.e., a
very small amount of features (several or no more
than 200 vs. ten thousand), easy computation
(only based on the trained language model vs. us-
ing a lot of NLP tools to extract a large amount of
linguistically informed features) and fast running,
which makes it more practical in real world appli-
cation. Furthermore, since the language model can
be trained on many corpora whether annotated or
unannotated, this method is more adaptive to other
languages and domains.
5 Conclusions
In this paper we have presented an approach to
implicit discourse relation recognition using pre-
dicted implicit connectives via a language model.
The predicted connectives have been used for im-
plicit relation recognition in two ways, i.e., super-
vised and unsupervised framework. Results on the
Penn Discourse Treebank 2.0 show that the pre-
dicted discourse connectives can help implicit re-
lation recognition and the two algorithms achieve
comparable F-scores with the state-of-art method.
In addition, this method is quite promising due to
its simple, easy to retrieve, fast run and increased
adaptivity to other languages and domains.
Acknowledgments
We thank the reviewers for their helpful com-
ments and Jonathan Ginzburg for his mentor-
ing. This work is supported by grants from
National Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse structure. Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. Proceedings of
the Second SIG dial Workshop on Discourse and Di-
alogue.
B. Dorr. LCS Verb Database. Technical Report Online
Software Database, University of Maryland, College
Park, MD,2001.
R. Girju. 2003. Automatic detection of causal relations
for question answering. In ACL 2003 Workshops.
S. Blair-Goldensohn. 2007. Long-Answer Ques-
tion Answering and Rhetorical-Semantic Relations.
Ph.D. thesis, Columbia Unviersity.
M. Lapata and A. Lascarides. 2004. Inferring
Sentence-internal Temporal Relations. Proceedings
of the North American Chapter of the Assocation of
Computational Linguistics.
Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing
Implicit Discourse Relations in the Penn Discourse
Treebank. Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics.
145
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A.
Lee, A. Joshi. 2008. Easily Identifiable Dis-
course Relations. Coling 2008: Companion vol-
ume: Posters.
E. Pitler, A. Louis, A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics.
E. Pitler and A. Nenkova. 2009. Using Syntax to Dis-
ambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.
M. Porter. An algorithm for suffix stripping. In Pro-
gram, vol. 14, no. 3, pp.130-137, 1980.
R. Prasad, N. Dinesh, A. Lee, A. Joshi, B. Webber.
2006. Annotating attribution in the Penn Discourse
TreeBank. Proceedings of the COLING/ACL Work-
shop on Sentiment and Subjectivity in Text.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.
Robaldo, A. Joshi, B. Webber. 2008. The Penn Dis-
course TreeBank 2.0. Proceedings of LREC?08.
M. Saito, K.Yamamoto, S.Sekine. 2006. Using
Phrasal Patterns to Identify Discourse Relations.
Proceeding of the HLTCNA Chapter of the ACL.
R. Soricut and D. Marcu. 2003. Sentence Level Dis-
course Parsing using Syntactic and Lexical Informa-
tion. Proceedings of the Human Language Technol-
ogy and North American Association for Computa-
tional Linguistics Conference.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: an assessment. Natural Language Engineer-
ing, Volume 14, Issue 03.
B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.
2006. Classification of discourse coherence rela-
tions: An exploratory study using multiple knowl-
edge sources. Proceedings of the 7th SIGDIAL
Workshop on Discourse and Dialogue.
F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005.
The Discourse GraphBank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
146
