Tagging Gene and Protein Names in Full Text Articles 
Lorraine Tanabe and W. John Wilbur 
National Center for Biotechnology Information 
NLM, NIH 
Bethesda, Maryland 20894 
 
 
Abstract 
Current information extraction efforts 
in the biomedical domain tend to 
focus on finding entities and facts in 
structured databases or MEDLINE? 
abstracts.  We apply a gene and 
protein name tagger trained on 
Medline abstracts (ABGene) to a 
randomly selected set of full text 
journal articles in the biomedical 
domain.  We show the effect of 
adaptations made in response to the 
greater heterogeneity of full text. 
1 Introduction 
The application of large-scale genomics and 
proteomics technologies towards a wide variety 
of biological questions has resulted in a 
continuous stream of information regarding 
thousands of genes and gene products into the 
Medline database of biomedical abstracts.  This 
repository has been recognized as a rich 
knowledge source for biological information 
retrieval, information extraction and text mining.  
However, abbreviated scientific abstracts cannot 
contain the same volume of information as the 
full text articles that they represent.  It was 
recently shown that only 30% of protein 
interactions contained in the Dictionary of 
Interacting Proteins (DIP) (Xenarios et al, 
2000) could be found in Medline sentences 
containing DIP protein pairs (Blaschke et al, 
2000).  This finding suggests that current 
information extraction efforts being applied to 
biomedical abstracts should be extended to full 
text databases.   
The basic task of identifying gene and 
protein names is a necessary first step towards 
making full use of the information encoded in 
biomedical text.  This remains a challenging 
task due to the irregularities and ambiguities in 
gene and protein nomenclature. The 
irregularities are mainly the result of a lack of 
naming conventions, as well as the widespread 
practice of using many synonyms for one gene 
or protein.  A glance at the Nomenclature 
section of the Nature Genetics website 
(http://www.nature.com/ng/web_specials/nomen
/) shows the scope of the problem, as well as 
ideas for addressing it.  The nomenclature 
guidelines implore authors to consult relevant 
nomenclature committees before announcing 
new genes, and to provide synonyms for genes 
in abstracts.  Additional rules specify that: 
4. Gene symbols are always italicised 
and never contain hyphens, greek 
letters, roman numerals, subscripts or 
superscripts. 
5. All letters in human genes are upper-
case?all letters in mouse genes are 
lower-case? 
 Unfortunately, we are currently at a stage 
where these types of rules are not consistently 
applied to most biomedical abstracts, let alne to 
full text documents.  Until the biomedical 
community adheres uniformly to nomenclature 
guidelines, ambiguities regarding gene/protein 
names will continue to be an obstacle for natural 
language processing of biomedical text.  These 
ambiguities become apparent at the 
morphological, syntactic and semantic levels.  
For example,  caco-2 refers to a cell line, but 
pai-1 is a gene name.  Gene and protein names 
can contain verbs and other parts of speech that 
are hard to distinguish from the surrounding 
text, as in deleted in azoospermia-like, son of 
sevenless, ran, man, young arrest and never in 
mitosis.  Genes can be transfected into cells, or 
combined with chemicals, resulting in 
ambiguous terms like CHO-A(3) and 
ca2+/calmodulin.  The semantic notion of a gene 
or protein is quite arbitrary ? is 
ACTTGGAATGACC a gene name?  In addition 
to sequences, there are mutations, motifs, 
receptors, antibodies, hormones, channels, 
                                            Association for Computational Linguistics.
                             the Biomedical Domain, Philadelphia, July 2002, pp. 9-13.
                         Proceedings of the Workshop on Natural Language Processing in
chromosomal locations and disease loci to 
consider.  The domain-specific irregularities and 
ambiguities just described are superimposed 
upon the ambiguities in the natural language 
itself, so it is not surprising that the 
identification of gene and protein names in 
biomedical text remains a difficult and 
challenging task.  The methodologies applied to 
this fundamental problem include rule-based 
and/or pattern matching methods (Fukuda et al, 
1998) (Thomas et al, 2000) (Yoshida et al, 
2000) (Jenssen et al, 2001) (Ono et al, 2001) 
(Yu at al, 2002) (Bunescu et al, 2002), a 
modified BLAST algorithm (Krauthammer et 
al., 2000), Hidden Markov Models (HMMs) 
(Collier et al, 2000) (Proux et al, 1998), Naive 
Bayes and decision trees (Nobata et al, 1999), 
under specified parsing with knowledge sources 
(Rindflesch et al 2000), and context-free 
grammars (Gaizauskas, 2000). 
In this paper, we evaluate the application of a 
gene and protein name tagger trained on 
Medline abstracts (ABGene) (Tanabe and 
Wilbur, 2002) to a randomly selected set of 
1,000 PUBMEDCENTRAL? (PMC) articles.  
PMC is a digital archive of full text peer-
reviewed biomedical articles launched in 
February 2000 by the National Center for 
Biotechnology Information (NCBI) and the U.S. 
National Library of Medicine (NLM?) (Roberts 
et al, 2001). We present two adaptations made 
in response to the greater heterogeneity of full 
text, and evaluate how they affect the 
performance of ABGene on a test set of 2600 
full text sentences.  
2 Methods 
We first give an overview of ABGene?s 
method for extracting gene and protein names 
from biomedical citations, and then present 
some modifications to ABGene designed to 
improve its performance on full text articles.  
 
2.1 ABGene Overview 
We previously trained the Brill POS tagger 
(Brill, 1994) to recognize protein and gene 
names in biomedical text using a training set of 
7,000 Medline sentences. We updated the 
lexicon included in the Brill package (Brown 
Corpus plus Wall Street Journal corpus) with 
entries from the UMLS? SPECIALIST lexicon 
(McCray et al 1994, Humphreys et al 1998), 
and generated a list of bigrams and a word list 
from all of MEDLINE to customize the training 
for our purposes.  ABGene processing begins by 
using these automatically generated rules from 
the Brill tagger to extract single word gene and 
protein names from biomedical abstracts (see 
Table 1). 
 
    
 
 
 
 
   This is followed by extensive filtering for false 
positives and false negatives. A key step during 
the filtering stage is the extraction of multi-word 
gene and protein names that are prevalent in the 
literature but inaccessible to the Brill tagger. 
 During the false positive filtering step, the 
GENE tag is removed from a word if it matches 
a term from a list of 1,505 precompiled general 
biological terms (acids, antagonist, assembly, 
antigen, etc.), 39 amino acid names, 233 
restriction enzymes, 593 cell lines, 63,698 
organism names from the NCBI Taxonomy 
Lexical Rule Description 
NNP gene fgoodleft 
GENE 
Change the tag of a word 
from NNP to GENE if the 
word gene can appear to the 
right 
-A hassuf 2 GENE 
Change the tag of a word 
from anything to GENE if it 
contains the suffix -A 
c- haspref 2 GENE 
Change the tag of a word 
from anything to GENE if it 
contains the prefix c- 
GENE cell 
fgoodright NNP 
Change the tag of a word 
from GENE to NNP if the 
word cell can appear to the 
left 
Contextual Rule Description 
NNP GENE 
PREV1OR2WD 
genes 
Change the tag of a word 
from NNP to GENE if one 
of the two preceding words 
is genes 
NNP GENE 
NEXTBIGRAM ( 
GENE 
Change the tag of a word 
from NNP to GENE if the 
two following words are 
tagged ( and GENE 
CD GENE 
SURROUNDTAG 
CC ) 
Change the tag of a word 
from CD to GENE if the 
preceding word is tagged 
CC and the following word 
is tagged ) 
VBG JJ NEXTTAG 
GENE 
Change the tag of a word 
from VBG to JJ if the next 
word is tagged GENE 
Table 1.  Examples of lexical and contextual rules learned by 
the Brill tagger.  NNP = proper noun, CD = cardinal number, 
CC = coordinating conjunction, JJ = adjective, VBG = verb, 
gerund/present participle 
Database (Wheeler et al 2000) or 4,357 non-
biological terms. Non-biological terms were 
obtained by comparing word frequencies in 
MEDLINE versus the Wall Street Journal (WSJ) 
using the following expression, where p is the 
probability of occurrence: 
log(p(word occurs in MEDLINE)/ p(word occurs in WSJ) )< 1 
 Additional false positives are found by regular 
expressions including numbers followed by 
measurements (25 mg/ml) and common drug 
suffixes (-ole, -ane, -ate, -ide, -ine, -ite, -ol, -ose, 
cooh). 
 The false negative filter recovers a single 
word name if it: 1) matches a list of 34,555 
single word names and 7611 compound word 
names compiled from LocusLink (Pruitt & 
Maglott 2001) and the Gene Ontology 
Consortium (2000) (Wain et al, 2002) and 
contains a good context word before or after the 
name, or 2) contains a low frequency trigram 
and a good context word before or after the 
name. The context words were automatically 
generated by a probabilistic algorithm, using the 
LocusLink/Gene Ontology set and a large 
collection of texts in which these gene names 
occur. We computed a log odds score or 
Bayesian weight for all non-gene name words 
indicating their propensity to predict an adjacent 
gene name in the texts.  
Compound word names are recovered using 
terms that occur frequently in known gene 
names.  Recombination of these terms produce 
compound words that also tend to be 
gene/protein names.  These terms include the 
digits 1-9, the letters a-z, the roman numerals, 
the Greek letters, functional descriptors 
(adhesion), organism identifiers (hamster), 
activity descriptors (promoting), placement 
indicators (early), and generic descriptors 
(light). In addition to the 415 exact terms, we 
added regular expressions that allow for partial 
matches or special patterns such as words 
without vowels, words with numbers and letters, 
words in capital letters, and common prefixes 
and suffixes (-gene, -like, -ase).   
Finally, Bayesian learning (Langley 1996, 
Mitchell 1997, Wilbur 2000) is applied to rank 
documents by similarity to documents with 
known gene/protein names. Documents below a 
certain threshold are considered to have no 
gene/protein names in them. 
 
2.2 Modifications for Full Text Articles 
The full text PMC articles are longer than 
abstracts, and contain extraneous information 
like grant numbers and laboratory reagents, 
along with figures and tables.  An attempt to 
take windows of varying sizes of the full text in 
order to rank the windows by similarity to 
abstracts with known gene names was 
unsuccessful.  High scoring windows often hid 
false positives, and low scoring windows could 
contain gene and protein name contexts 
infrequently encountered in Medline abstracts.  
However, we determined that the classifier 
could be used on the sentence level for full text 
articles, and show the effect of an assumption 
that sentences below a zero threshold do not 
contain gene/protein names. 
We tried to increase the performance of 
ABGene on the PMC articles by adding a final  
processing step.  We ran ABGene on 2.16 
million Medline abstracts similar to documents 
with known gene names, and extracted 2.42 
million unique gene/protein names.  We counted 
the number of times each unique name was 
given the GENE tag by ABGene in the 2.16 
million abstracts, and  then extracted three 
groups of putative gene/protein names from this 
large set, with count thresholds at 10 (134,809 
names), 100 (13,865 names) and 1000 (1136 
names).   
During the final stage of processing, terms in 
sentences with scores greater than 2 are checked 
against these lists of supposed gene/protein 
names.  We show the effect of tagging terms 
with counts of at least 10, 100 and 1000 in the 
putative gene/protein list.   
3 Experiment and Results 
We evaluated the performance of ABGene 
on 2600 PMC sentences from 13 score levels 
ranging from ?8 to 60+.  No attempt was made 
to narrow the set using query terms.  The 
sentences were selected as follows:  half of the 
test set consists of the first 100 sentences from 
each score level, and the other half consists of 
100 sentences selected at random from each 
score level.  Precision and recall results are 
shown for each individual score range in Table 
2, and cumulative results are shown in Table 3.  
The number of words tested varies for each 
score level because longer sentences tend to 
have higher scores.  Also, sentences with scores 
near zero tend to be table or figure entries, with 
only a few words each.   
 
 
Table 2.  Precision and recall for each score range.  TP+FN = number of gene names; P = precision without final step, R = recall without 
final step, P 1000 = precision with 1000 count threshold at final step, R 1000 = recall with 1000 count threshold at final step, P 100 = 
precision with 100 count threshold at final step, R 100 = recall with 100 count threshold at final step, P 10 = precision with 10 count 
threshold at final step, R 10 = recall with 10 count threshold at final step. 
 
Table 3.  Cumulative precision and recall using the score as a lower threshold.
 
3.1 Problematic Areas in Full Text 
The false positive gene/protein names found in 
the PMC articles reveal new difficulties for the 
basic task of identifying gene and protein names 
in biomedical text.  For example, in abstracts, 
entities like restriction enzyme sites, laboratory 
protocol kits, primers, vectors, molecular 
biology supply companies and chemical 
reagents are usually scarce.  However, in the 
methods section of a full document, they appear 
regularly, adding to the morphological, syntactic 
and semantic ambiguities previously mentioned.  
Illustrative examples include bio-rad, centricon-
30 spin, xbai sites, mg2, geneamp and pgem3z.  
A significant source of false negatives consists 
of tables and figures from full text, which 
completely lack contextual cues and/or indicator 
words.  These problems can be addressed by 
eliminating processing of materials and methods 
sections, tables and figures.  Another significant 
source of false negatives is an artifact of the 
PMC format, for example, beta is translated to 
[beta], thus a name like beta1 integrin becomes 
[beta]1 integrin in PMC. This is easily 
addressed by removing the PMC formatting 
prior to processing, and has already been 
completed for future work on PMC articles. 
4 Conclusion 
We conclude that an information extraction 
system to tag gene and protein names in 
Medline abstracts (ABGene) can be applied to 
full text articles in the biomedical domain. We 
Score 
Range  
#words 
tested TP + FN P R P 1000 R 1000
P 
100 
R 
100 
P 
10 
R 
10 
60+ 13,442 1347 0.742 0.640 0.726 0.667 0.686 0.692 0.603 0.716 
30 to 60 7,953 530 0.672 0.638 0.673 0.667 0.649 0.699 0.590 0.765 
20 to 30 6,392 401 0.757 0.646 0.751 0.671 0.708 0.748 0.624 0.801 
15 to 20 5,508 302 0.722 0.593 0.719 0.619 0.672 0.659 0.561 0.735 
10 to 15 5,100 269 0.755 0.688 0.743 0.710 0.681 0.747 0.579 0.792 
8 to 10 4,618 226 0.707 0.588 0.689 0.637 0.615 0.686 0.512 0.770 
6 to 8 4,327 170 0.703 0.571 0.692 0.594 0.641 0.641 0.479 0.724 
4 to 6 4,054 122 0.571 0.590 0.562 0.631 0.500 0.648 0.392 0.713 
2 to 4 3,667 59 0.541 0.559 0.508 0.559 0.404 0.610 0.270 0.644 
0 to 2 1,551 9 0.200 0.444 0.200 0.444 0.200 0.444 0.200 0.444 
-2 to 0 4,595 0 no tp no tp no tp no tp no tp no tp no tp no tp 
-4 to -2 5,299 1 0.040 1.000 0.040 1.000 0.040 1.000 0.040 1.000 
-8 to -4 5,495 0 no tp no tp no tp no tp no tp no tp no tp no tp 
SCORE P R P  1000 
R  
1000 
P  
100 
R  
100 
P  
10 
R  
10 
60 0.742 0.251 0.726 0.261 0.686 0.273 0.603 0.283 
30 0.721 0.349 0.710 0.364 0.675 0.381 0.599 0.402 
20 0.727 0.424 0.717 0.443 0.681 0.468 0.604 0.496 
15 0.727 0.476 0.717 0.497 0.680 0.526 0.598 0.560 
10 0.729 0.530 0.720 0.553 0.680 0.584 0.596 0.622 
8 0.728 0.569 0.718 0.595 0.675 0.629 0.589 0.673 
6 0.727 0.597 0.716 0.624 0.673 0.661 0.582 0.709 
4 0.720 0.618 0.710 0.646 0.665 0.684 0.573 0.734 
2 0.716 0.628 0.706 0.656 0.659 0.695 0.563 0.745 
0 0.713 0.629 0.702 0.657 0.656 0.696 0.562 0.746 
have shown how modifications to the processing 
(applying a sentence score threshold, and using 
a large pool of putative gene/protein names) can 
affect the system?s performance.  We are 
currently exploring methods to filter the 2.16 
million putative gene/protein names extracted 
from Medline using our system.  The resulting 
set of gene/protein names, a significant addition 
to the 42K names available from the Gene 
Ontology Consortium and LocusLink, will be 
used to improve the performance of text 
processing on full text articles in the biomedical 
domain. 
 
References 
Blaschke, C. and Valencia, A.  (2001)  Can bibliographic pointers 
for known biological data be found automatically?  Protein 
interactions as a case study.  Comparative and Functional 
Genomics, 2, 196-206. 
 
Brill, Eric. (1994) Some advances in transformation-based   part of 
speech tagging. In Proceedings of the National Conference on 
Artificial Intelligence.  AAAI Press, pp.  722-727. 
 
Bunescu, R., Ge, R., Mooney, R.J., Marcotte, E., and Ramani, 
A.K.  (2002) Extracting gene and protein names from biomedical 
abstracts.  http://www.cs.utexas.edu/users/ml/publication/ie.html. 
 
Collier, N., Nobata, C., and Tsujii, J. (2000) Extracting the names 
of genes and gene products with a hidden markov model. In 
Proceedings of the 18th International Conference on 
Computational Linguistics (COLING?2000), pp. 201-207. 
 
Fukuda, K., Tsunoda. T., Tamura, A. and Takagi. T. (1998) 
Toward information extraction: identifying protein names from 
biological papers. In Proceedings of the Pacific Symposium on 
Biocomputing (PSB98), pp. 705-716. 
 
The Gene Ontology Consortium. (2000) Gene ontology:  tool for 
the unification of biology. Nat. Genet., 25, 25-29. 
 
Humphreys K., Demetriou G., and Gaizauskas, R. (2000) Two 
applications of information extraction to biological science journal 
articles: enzyme interactions and protein structures. In 
Proceedings of the Pacific Symposium on Biocomputing 
(PSB2000) ,  pp. 502-513. 
  
Jenssen, T., Laegreid, A., Kormorowski, J., and Hovig, E.  (2001) 
A literature network of human genes for high-throughput analysis 
of gene expression.  Nat Genet., 28, 21-28. 
 
Krauthammer, M., Rzhetsky, A., Morozov, P., and Friedman, C.  
(2000) Using BLAST for identifying gene and protein names in 
journal articles.  Gene, 259, 245-252. 
 
Langley, P. (1996) Elements of Machine Learning. Morgan 
Kaufmann Publishers, Inc., San Francisco. 
 
McCray, A.T., Srinivasan, S. and Browne, A. C. Lexical methods 
for managing variation in biomedical terminologies.  In SCAMC 
?94, pp. 235-239. 
 
Mitchell, T. M. (1997) Machine Learning. WCB/McGraw-Hill, 
Boston. 
 
Nobata, C., Collier, N., and Tsujii, J. (1999) Automatic term 
identification and classification in biology texts.  In  Proceedings 
of the Natural Language Pacific Rim Symposium, pp. 369-374. 
 
Ono, T., Hishigaki, H., Tanigami, A., and Takagi, T.  (2001) 
Automated extraction of information on protein-protein 
interactions from the biological literature.  Bioinformatics, 17, 
155-161. 
 
Proux, D., Rechenmann, F., Julliard, L., Pillet, V., and Jacq, B. 
(1998) Detecting gene symbols and names in biological texts: a 
first step toward pertinent information extraction. In Proceedings 
of the Ninth Workshop on Genome Informatics, pp. 72-80. 
 
Pruitt, K.D. and Maglott, D.R.  (2001)  RefSeq and LocusLink:  
NCBI gene-centered resources.  Nucleic Acids Res., 29, 137-140. 
 
Rindflesch, T. C., Tanabe, L., Weinstein, J. W., and Hunter, L. 
(2000)  EDGAR:  extraction of drugs, genes and relations from the 
biomedical literature.  In Proceedings of the Pacific Symposium on 
Biocomputing (PSB2000), pp. 514-525. 
 
Roberts, R.J., Varmus, H.E., and Ashburner, M.  (2001)  
Information access:  building a Genbank of the published 
literature.  Science, 291, 2318-2319. 
 
Tanabe, L., and Wilbur, W.J.  (2002)  Tagging gene and protein 
names in biomedical text.  Bioinformatics, in press. 
 
Thomas, J., Milward, D., Ouzounis, C., Pulman, S., and Carroh, 
M. (2000) Automatic extraction of protein interactions from 
scientific abstracts.  In Proceedings of the Pacific Symposium on 
Biocomputing (PSB2000), pp. 541-552. 
 
Wain, H. M., Lush, M., Ducluzeau, F. , and Povey, S.  (2002) 
Genew:  the human gene nomenclature database.  Nucleic Acids 
Res., 30, 169-171. 
 
Wheeler, D.L., Chappey, C., Lash, A.E., Leipe, D.D., Madden, 
T.L., Schuler, G.D., Tatusova, T.A., and Rapp, B.A.  (2000)  
Database resources of the National Center for Biotechnology 
Information.  Nucleic Acids Res., 28, 10-14. 
 
Wilbur, W. J. (2000) Boosting naive bayesian learning on a large 
subset of MEDLINE. In American Medical Informatics 2000 
Annual Symposium, Los Angeles, CA, pp. 918-922. 
 
Xenarios, I., Rice, D.W., Salwinski, L., Baron, M.K., Marcotte, 
E.M., and Eisenberg, D.  (2000)  DIP:  the database of interacting 
proteins.  Nucleic Acids Res., 28, 289-291. 
 
Yoshida, M., Fukuda, K., and Takagi, T.  (2000)  PNAD-CSS:  a 
workbench for constructing a protein name abbreviation 
dictionary.  Bioinformatics, 16, 169-175. 
 
Yu, H., Hripcsak, G., and Friedman, C.  (2002)  Mapping 
abbreviations to full forms in biomedical articles.  J Am Med 
Inform Assoc., 9, 262-272. 
Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining
Biological Semantics, pages 32?37, Detroit, June 2005. c?2005 Association for Computational Linguistics
MedTag: A Collection of Biomedical Annotations
L.H. Smith
 
, L. Tanabe
 
, T. Rindflesch

, W.J. Wilbur
 
 
National Center for Biotechnology Information

Lister Hill National Center for Biomedical Communications
NLM, NIH, 8600 Rockville Pike, Bethesda, MD 20894

lsmith,tanabe,wilbur  @ncbi.nlm.nih.gov
rindesch@nlm.nih.gov
Abstract
We present a database of annotated
biomedical text corpora merged into a
portable data structure with uniform con-
ventions. MedTag combines three cor-
pora, MedPost, ABGene and GENETAG,
within a common relational database data
model. The GENETAG corpus has been
modified to reflect new definitions of
genes and proteins. The MedPost cor-
pus has been updated to include 1,000
additional sentences from the clinical
medicine domain. All data have been up-
dated with original MEDLINE text ex-
cerpts, PubMed identifiers, and tokeniza-
tion independence to facilitate data accu-
racy, consistency and usability.
The data are available in flat files along
with software to facilitate loading the
data into a relational SQL database
from ftp://ftp.ncbi.nlm.nih.gov/pub/lsmith
/MedTag/medtag.tar.gz.
1 Introduction
Annotated text corpora are used in modern computa-
tional linguistics research and development to fine-
tune computer algorithms for analyzing and classi-
fying texts and textual components. Two important
factors for useful text corpora are 1) accuracy and
consistency of the annotations, and 2) usability of
the data. We have recently updated the text corpora
we use in our research with respect to these criteria.
Three different corpora were combined. The AB-
Gene corpus consists of over 4 000 sentences anno-
tated with gene and protein named entities. It was
originally used to train the ABGene tagger to recog-
nize gene/protein names in MEDLINE records, and
recall and precision rates in the lower 70 percentile
range were achieved (Tanabe and Wilbur, 2002).
The MedPost corpus consists of 6 700 sentences,
and is annotated with parts of speech, and gerund
arguments. The MedPost tagger was trained on
3 700 of these sentences and achieved an accuracy
of 97.4% on the remaining sentences (Smith et. al.,
2004). The GENETAG corpus for gene/protein
named entity identification, consists of 20 000 sen-
tences and was used in the BioCreative 2004 Work-
shop (Yeh et. al., 2005; Tanabe et. al., 2005) (only
15 000 sentences are currently released, the remain-
ing 5 000 are being retained for possible use in a fu-
ture workshop). Training on a portion of the data,
the top performing systems achieved recall and pre-
cision rates in the lower 80 percentile range. Be-
cause of the scarcity of good annotated data in the
realm of biomedicine, and because good perfor-
mance has been obtained using this data, we feel
there is utility in presenting it to a wider audience.
All of the MedTag corpora are based on MED-
LINE abstracts. However, they were queried at dif-
ferent times, and used different (but similar) algo-
rithms to perform tokenization and sentence seg-
mentation. The original annotations were assigned
to tokens, or sequences of tokens, and extensively
reviewed by the authors at different times for the dif-
ferent research projects.
The main goals in combining and updating these
32
MedTag? Collection?
MedPost? ABGene? GENETAG?
6,700 sentences?
5,700 molecular biology?
1,000 clinical medicine?
60 part-of-speech tags?
97.4% accuracy?
1 annotator?
4,265 sentences?
Molecular Biology?
Single Genes/Proteins tagged?
1 annotator?
15,000 sentences?
50% Molecular Biology?
50% Other Biomedical?
All Genes/Proteins tagged?
3 annotators?
EXCERPT?
ID?
PubMed?  ID?
Corpus code?
Original? text?
Citation?
ANNOTATION?
ID?
Corpus code?
Character offsets?
Annotated text?
Data Model?
SQL Relational?
Database with?
Web Interface?
MEDLINE?
MEDLINE?
Figure 1: Component corpora, common data model
and main record types of the MedTag collection.
corpora into a single corpus were to
1. update the text for all corpora to that currently
found in MEDLINE, storing a correct citation
and the original, untokenized text for each ex-
cerpt
2. eliminate tokenization dependence
3. put all text and annotations into a common
database format
4. provide programs to convert from the new cor-
pus format to the data formats used in previous
research
2 Merging the Corpora
We describe what was done to merge the original
corpora, locating original sources and modifying the
text where needed. An overview is given in Figure
1. Some basic statistics are given in Table 1.
2.1 Identifying Source Data
The original data of the three corpora were assem-
bled and the text was used to search MEDLINE to
Corpus sentences tokens most frequent tag
GENETAG-05 15,000 418,246 insulin GENE(112)
MedPost 6,700 181,626 the DD(8,507)
AbGene 4,265 123,208 cyclin GENE(165)
MedPost
Adj Adv Aux Noun Punct Verb
14,648 4,553 56,262 60,732 21,806 23,625
GENETAG-05
GENE ALTGENE
24,562 19,216
ABGene
GENE ALTGENE
8,185 0
Table 1: MedTag Corpora. GENE = gene and pro-
tein names, ALTGENE = acceptable alternatives for
gene and protein names. MedPost tagset contains
60 parts of speech which have been binned here for
brevity.
find the closest match. An exact or near exact match
was found for all but a few excerpts. For only a
few excerpts, the MEDLINE record from which the
excerpt was originally taken had been removed or
modified and an alternative sentence was selected.
Thus, each excerpt in the database is taken from a
MEDLINE record as it existed at one time in 2004.
In order to preserve the reference for future work,
the PubMed ID and citation data were also retrieved
and stored with each excerpt. Each excerpt in the
current database roughly corresponds to a sentence,
although the procedure that extracted the sentence is
not specified.
2.2 Eliminating Tokenization Dependence
In the original ABGene and GENETAG corpora, the
gene and protein phrases were specified by the to-
kens contained in the phrase, and this introduced
a dependence on the tokenization algorithm. This
created problems for researchers who wished to use
a different tokenization. To overcome this depen-
dence, we developed an alternative way of specify-
33
ing phrases. Given the original text of an excerpt,
the number of non-whitespace characters to the start
of the phrase does not depend on the tokenization.
Therefore, all annotations now refer to the first and
last character of the phrase that is annotated. For
example the protein serum LH in the excerpt
There was no correlation between serum
LH and chronological or bone age in this
age group, which suggests that the corre-
lation found is not due to age-related par-
allel phenomena.
is specified as characters 28 to 34 (the first character
is 0).
2.3 Data Model
There are two main record types in the database,
EXCERPT and ANNOTATION. Each EXCERPT
record stores an identifier and the original corpus
code (abgene, medpost, and genetag) as well as sub-
corpus codes that were defined in the original cor-
pora. The original text, as it was obtained from
MEDLINE, is also stored, and a human readable ci-
tation to the article containing the reference.
Each ANNOTATION record contains a reference
to the excerpt (by identifier and corpus), the char-
acter offset of the first and last characters of the
phrase being annotated (only non-whitespace char-
acters are counted, starting with 0), and the corre-
sponding annotation. The annotated text is stored
for convenience, though it can be obtained from
the corresponding excerpt record by counting non-
whitespace characters.
The data is provided as an ASCII file in a standard
format that can be read and loaded into a relational
database. Each record in the file begins with a line
of the form  table name where table name is the
name of the table for that record. Following the table
name is a series of lines with the form eld: value
where eld is the name of the field and value is the
value stored in that field.
Scripts are provided for loading the data into a re-
lational database, such as mysql or ORACLE. SQL
queries can then be applied to retrieve excerpts and
annotations satisfying any desired condition. For
example, here is an SQL query to retrieve excerpts
from the MedPost corpus containing the token p53
and signaling or signalling
Figure 2: A screen capture of the annotator?s inter-
face and the GENETAG-05 annotations for a sen-
tence.
select text from excerpt
where text like ?%p53%?
and text rlike ?signa[l]*ing?;
2.4 Web Interface
A web-based corpus editor was used to enter and
review annotations. The code is being made avail-
able, as is, and requires that the data are loaded into a
mysql database that can be accessed by a web server.
The interface supports two annotation types: Med-
Post tags and arbitrary phrase annotations. MedPost
tags are selectable from a pull-down menu of pre-
programmed likely tags. For entering phrase anno-
tations, the user highlights the desired phrase, and
pressing the enter key computes and saves the first
and last character offsets. The user can then enter
the annotation code and an optional comment be-
fore saving it in the database. A screen dump of the
phrase annotations for a sentence in the genetag cor-
pus is shown in figure 2.
The data from the database was dumped to the flat
file format for this release. We have also included
some files to accommodate previous users of the
corpora. A perl program, alt eval.perl is in-
34
cluded that replaces the GENETAG evaluation pro-
gram using non-whitespace character numbers in-
stead of token numbers. Copies of the ABGene and
MedPost corpora, in the original formats, are also
included.
3 Updates of Component Corpora
3.1 MedPost Update
The MedPost corpus (Smith et. al., 2004) originally
contained 5 700 tokenized sentences. An additional
1 000 annotated sentences have been added for this
release. Each sentence in the MedPost corpus is
fully tokenized, that is, divided into non-overlapping
annotated portions, and each token is annotated with
one of 60 part of speech tags (see Table 1). Minor
corrections to the annotations have been made since
the original release.
Since most of the original corpus, and all of the
sentences used for training the MedPost tagger, were
in the area of molecular biology, we added an addi-
tional 1 000 sentences selected from random MED-
LINE abstracts on the subject of clinical medicine.
As a preliminary result, the trained MedPost tag-
ger achieves approximately 96.9% accuracy, which
is comparable to the 97.4% accuracy achieved on the
subset of 1 000 sentences selected randomly from all
of MEDLINE. An example of a sentence from the
clinical medicine collection is
EvidenceNN isVBZ nowRR availableJJ
toTO showVVI aDD beneficialJJ effectNN
ofII bezafibrateNN onII retardingVVGN
atheroscleroticJJ processesNNS andCC inII
reducingVVGN riskNN ofII coronaryJJ heartNN
diseaseNN .
In addition to the token-level annotations, all
of the gerunds in the MedPost corpus (these are
tagged VVGN) were also examined and it was noted
whether the gerund had an explicit subject, direct
object, or adjective complement. This annotation
is stored with an annotation of type gerund. To il-
lustrate, the two gerunds in the previous example,
retarding and reducing both have direct objects (re-
tarding processes and reducing risk), and the gerund
tag is entered as ?o?. The gerund annotations have
been used to improve a noun phrase bracketer able
to recognize gerundive phrases.
3.2 GENETAG Update
GENETAG is a corpus of MEDLINE sentences that
have been annotated with gene and protein names.
The closest related work is the GENIA corpus (Kim
et. al., 2003). GENIA provides detailed coverage of
a large number of semantic entities related to a spe-
cific subset of human molecular biology, whereas
GENETAG provides gene and protein name anno-
tations only, for a wide range of organisms and
biomedical contexts (molecular biology, genetics,
biochemistry, clinical medicine, etc.)
We are including a new version of GENE-
TAG, GENETAG-05, as part of the MedTag sys-
tem. GENETAG-05 differs from GENETAG in
four ways: 1) the definition of a gene/protein en-
tity has been modified, 2) significant annotation er-
rors in GENETAG have been corrected, 3) the con-
cept of a non-specific entity has been refined, and 4)
character-based indices have been introduced to re-
duce tokenization problems. We believe that these
changes result in a more accurate and robust corpus.
GENETAG-05 maintains a wide definition of a
gene/protein entity including genes, proteins, do-
mains, sites, sequences, and elements, but exclud-
ing plasmids and vectors. The specificity con-
straint requires that a gene/protein name must be
included in the tagged entity. This constraint has
been applied more consistently in GENETAG-05.
Additionally, plain sequences like ATTGGCCTT-
TAAC are no longer tagged, embedded names are
tagged (ras-mediated), and significantly more terms
have been judged to violate the specificity constraint
(growth factor, proteases, protein kinase, ribonu-
clease, snoRNA, rRNA, tissue factor, tumor anti-
gen, complement, hormone receptors, nuclear fac-
tors, etc.).
The original GENETAG corpus contains some en-
tities that were erroneously tagged as gene/proteins.
Many of these errors have been corrected in the up-
dated corpus. Examples include camp-responsive
elements, mu element, VDRE, melanin, dentin,
myelin, auxin, BARBIE box, carotenoids, and cel-
lulose. Error analysis resulted in the updated anno-
tation conventions given in Table 1.
Enzymes are a special class of proteins that cat-
alyze biochemical reactions. Enzyme names have
varying degrees of specificity, so the line drawn for
35
tagging purposes is based on online resources1 as
well as background knowledge. In general, tagged
enzymes refer to more specific entities than un-
tagged enzymes (tyrosine kinase vs. protein kinase,
ATPase vs. protease). Enzymes that can refer to
either DNA or RNA are tagged if the reference is
specified (DNA endonuclease vs. endonuclease).
Enzymes that do not require DNA/RNA distinction
are tagged (lipase vs. ligase, cyclooxygenase vs.
methylase). Non-specific enzymes are tagged if they
clearly refer to a gene or protein, as in (1).
1) The structural gene for hydrogenase en-
codes a protein product of molecular mass
45820 Da.
Semantic constraints in GENETAG-05 are the
same as those for GENETAG. To illustrate, the name
in (2) requires rabies because RIG implies that the
gene mentioned in this sentence refers to the rabies
immunoglobulin, and not just any immunoglobulin.
In (3), the word receptor is necessary to differen-
tiate IGG receptor from IGG, a crucial biological
distinction. In (4), the number 1 is needed to ac-
curately describe a specific type of tumor necrosis
factor, although tumor necrosis factor alone might
be adequate in a different context.
2) rabies immunoglobulin (RIG)
3) IGG receptor
4) Tumor necrosis factor 1
Application of the semantic constraint can result in
apparent inconsistencies in the corpus (immunoglob-
ulin is sufficient on its own in some sentences in the
corpus, but is insufficient in (2)). However, we be-
lieve it is important that the tagged entity retain its
true meaning in the sentence context.
4 Recommended Uses
We have found the component corpora of MedTag
to be useful for the following functions:
1) Training and evaluating part-of-speech
taggers
2) Training and evaluating gene/protein
named entity taggers
1http://cancerweb.ncl.ac.uk/omd/copyleft.html
http://www.onelook.com/
3) Developing and evaluating a noun
phrase bracketer for PubMed phrase
indexing
4) Statistical analysis of grammatical
usage in medical text
5) Feature generation for machine learn-
ing
The MedPost tagger was recently ported to Java
and is currently being employed in MetaMap, a pro-
gram that maps natural language text into the UMLS
(Aronson,A.R., 2001).
5 Conclusion
We have merged three biomedical corpora into a col-
lection of annotations called MedTag. MedTag uses
a common relational database format along with a
web interface to facilitate annotation consistency.
We have identified the MEDLINE excerpts for each
sentence and eliminated tokenization dependence,
increasing the usability of the data. In GENETAG-
05, we have clarified many grey areas for annotation,
providing better guidelines for tagging these cases.
For users of previous versions of the component cor-
pora, we have included programs to convert from the
new standardized format to the formats used in the
older versions.
References
Aronson, A. R. 2001. Effective mapping of biomedical
text to the UMLS Metathesaurus: the MetaMap pro-
gram. Proc. AMIA Symp., 1721.
Kim, J.-D., Ohta, T., Tateisi, Y. and Tsujii, J. 2003. GE-
NIA corpus: a semantically annotated corpus for bio-
textmining. Bioinformatics, 19: 180 - 182.
Tanabe, L and Wilbur, WJ. 2002. Tagging gene and
protein names in biomedical text. Bioinformatics, 18,
1124-1132.
Tanabe L, Xie N, Thom, LH, Matten W, Wilbur, WJ:
GENETAG: a tagged gene corpus for gene/protein
named entity recognition. BMC Bioinformatics 2005.
Smith, L, Rindflesch, T, and Wilbur, WJ. 2004. MedPost:
a part of speech tagger for biomedical text. Bioinfor-
matics, 20(13) 2320-2321.
Yeh A, Hirschman L, Morgan A, Colosimo M: BioCre-
AtIvE task 1A: gene mention finding evaluation. BMC
Bioinformatics 2005.
36
Entity Type Problem GENETAG-05
Convention
Positive Examples Negative
Examples
Protein
Families
Some are named after
structural motifs.
Do not tag
structures alone,
but tag structurally
related gene and
protein families.
Zinc finger protein,
bZIP transcription
factor, homeobox
gene, TATA binding
protein
Zinc finger,
helix-turn-helix
motif, leucine
zipper, homeobox,
TATA box
Domains Name can refer to 1) the
amino acid content of a
sequence (PEST), 2) the
protein that binds the
sequence (TFIIIA DNA
binding domain), 3) a
homologous gene (SH2 - Src
homology domain 2), 4) the
first proteins in which the
domain was discovered (LIM,
PDZ), or 5) structural entities
(POZ, zinc finger domain).
Tag only if the
domain refers to a
gene or protein.
Immuno-globulin
regions are tagged.
(VH refers to the
Immuno-globulin
heavy chain V
region).
BTB domain, LIM
domain, HECT
domain, VH
domain, SH2
domain, TFIIIA
DNA binding
domain,
Kru?ppel-associated
box (KRAB)
domains, NF-IL6
beta leucine zipper
domain
PEST domain, SR
domain, zinc finger
domain, b-Zip
domain, POZ
domain, GATA
domain, RS
domain, GAR
domain
Boxes,
Response
Elements and
Sites
Name can refer to 1) the
sequence or site itself
(TAAG), 2) a non-protein that
binds to it (Glucocorticoid
Response Element), 3) a
protein that binds to it (Sp1),
or 4) to homologous genes
(VL30).
Tag only if the
sequence or site
refers to a gene or
protein.
VL30 element, Zta
response elements,
activating protein 1
(AP-1) site, Ets
binding site, SP1
site, AP-2 box
GRE, TRE, cyclic
AMP response
element ( CRE),
TAAG sites, TGn
motif, TAR element,
UP element
Hormones Some are peptide hormones. Tag only peptide
hormones.
Insulin, Glucagon,
growth hormone
Estrogen,
Progesterone,
thyroid hormone
?and?
constructs
Some conjuncts require the
entire construct.
Unless both
conjuncts can stand
alone, tag them
together.
TCR alpha and
beta, D-lactate and
D-glycerate
dehydrogenase
TCR alpha, beta,
D-lactate,
D-glycerate
dehydrogenase
Viral
Sequences
Promoters, enhancers, repeats
are distinguished by
organism.
Tag only if the
organism is present.
Viral LTR, HIV
long terminal
repeat, SV40
promoter
LTR, long terminal
repeat
Sequences Some sequences lack gene or
protein names.
Tag only if a gene
name is included.
NF kappa B
enhancer
(TGGAAATTCC)
TCTTAT, TTGGGG
repeats
Embedded
Names
Some names are embedded in
non-gene text.
Tag only the gene
part.
P-47-deficient,
ras-transformed
P-47-deficient,
ras-transformed
Transposons,
Satellites
Often repetitive sequences. Tag if specific. L1 element, TN44,
copia
retrotransposon
non-LTR
retrotransposon
Antibodies Often use organism or disease
name.
Tag if specific. anti-SF group
rickettsiae (SFGR)
antinuclear
antibody
Alternative
Transcripts
Names differ from primary
transcript.
Tag if primary
transcript named.
I kappa B
gamma,VEGF20
Exon 2, IIA
Table 2: Some problematic gene/protein annotations and conventions followed in GENETAG-05.
37
Proceedings of the Workshop on BioNLP, pages 144?152,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring Two Biomedical Text Genres for Disease Recognition 
 
 
Aur?lie N?v?ol, Won Kim, W. John Wilbur, Zhiyong Lu* 
National Center for Biotechnology Information 
U.S. National Library of Medicine 
Bethesda, MD 20894, USA 
{neveola,wonkim,wilbur,luzh}@ncbi.nlm.nih.gov 
 
  
 
 
Abstract 
In the framework of contextual information 
retrieval in the biomedical domain, this paper 
reports on the automatic detection of disease 
concepts in two genres of biomedical text: 
sentences from the literature and PubMed user 
queries. A statistical model and a Natural 
Language Processing algorithm for disease 
recognition were applied on both corpora. 
While both methods show good performance 
(F=77% vs. F=76%) on the sentence corpus, 
results on the query corpus indicate that the 
statistical model is more robust (F=74% vs. 
F=70%).  
1 Introduction 
Contextual Information Retrieval (IR) is making 
use of additional information or assumptions about 
the users? needs beyond the obvious intent of the 
query. IR systems need to go beyond the task of 
providing generally relevant information by assist-
ing users in finding information that is relevant to 
them and their specific needs at the time of the 
search. A practical example of a Google contextual 
IR feature is when the search engine returns a map 
showing restaurant locations to a user entering a 
query such as ?Paris restaurants.? 
The contextual aspects of a user?s search were 
defined for example by Saracevic (1997) who dis-
cussed integrating the cognitive, affective, and sit-
uational levels of human computer interaction in 
IR systems. Other research efforts studied users? 
search behavior based on their level of domain 
knowledge (Zhang et al, 2005) or aimed at  mod-
eling users? interests and search habits (Rose and 
Levinson, 2004; Teevan et al, 2005).  
Information about the search context may be 
sought explicitly from the user through profiling or 
relevance feedback (Shen et al, 2005). Recent 
work also exploited query log analysis and basic 
computer environment information (Wen et al 
2004), which involve no explicit interaction with 
the user. In adaptive information retrieval, context 
information is inferred based on query analysis and 
collection characteristics (Bai and Nie 2008).  
In the biomedical domain, a need for contextual 
information retrieval was identified in particular 
for clinical queries submitted to PubMed (Pratt and 
Wasserman, 2000). Building on the idea that a spe-
cific type of document is required for searches with 
a ?clinical? context, the PubMed Clinical Queries 
portal was developed (Haynes and Wilczynski, 
2004). A perhaps more prominent contextual fea-
ture of PubMed is the ?citation sensor?, which 
identifies queries classified by Rose and Levinson 
as reflecting a ?Navigational? or ?Obtain resource? 
goal. For example, the citation sensor will identify 
and retrieve a specific citation if the user enters the 
article title as the query. The analysis of Entrez 
logs shows that MEDLINE is the most popular 
database among the 30 or so databases maintained 
by the National Center for Biotechnology Informa-
tion (NCBI) as it receives most of Entrez traffic. 
This suggests that there is a need to complement 
the information retrieved from MEDLINE by giv-
ing contextual access to other NCBI resources re-
144
levant to users? queries, such as Entrez Gene, Clin-
ical Q&A or BookShelf. In addition, the NLM es-
timated that about 1/3 of PubMed users are not 
biomedical professionals. In this light, providing 
an access point to consumer information such as 
the Genetics Home Reference might also be useful. 
To achieve this, the sensor project was recently 
launched with the goal of recognizing a variety of 
biomedical concepts (e.g. gene, protein and drug 
names) in PubMed queries. These high-level con-
cepts will help characterize users? search context in 
order to provide them with information related to 
their need beyond PubMed. For instance, if a user 
query contains the drug name ?Lipitor?, it will be 
recognized by the drug sensor and additional in-
formation on this drug from Clinical Q&A will be 
shown in the side bar in addition to default 
PubMed results. Since disease names are common 
in PubMed queries, the goal of this work is to in-
vestigate and benchmark computational techniques 
for automatic disease name recognition as an aid to 
implementing PubMed search contexts. 
2 Related Work 
Despite a significant body of literature in biomedi-
cal named entity recognition, most work has been 
focused on gene, protein, drug and chemical names 
through challenges such as BioCreAtIvE1 or the 
TREC Genomics/Chemical tracks (Park and Kim, 
2006). Other work addressed the identification of 
?medical problems? in clinical text (Aronson et al 
2007; Meystre and Haug, 2005). This task was the 
topic of a Medical NLP challenge2, which released 
a corpus of anonymized radiography reports anno-
tated with ICD9 codes. Although there is some 
interest in the biomedical community in the identi-
fication of disease names and more specifically the 
identification of relationships between diseases and 
genes or proteins (Rindflesh and Fizman, 2003), 
there are very few resources available to train or 
evaluate automatic disease recognition systems. To 
the best of our knowledge, the only publicly avail-
able corpus for disease identification in the litera-
ture was developed by Jimeno et al (2008). The 
authors annotated 551 MEDLINE sentences with 
UMLS concepts and used this dataset to bench-
mark three different automatic methods for disease 
name recognition. A MEDLINE corpus annotated 
                                                        
1 http://biocreative.sourceforge.net/ 
2 http://www.computationalmedicine.org/challenge/index.php 
with ?malignancy? mentions and part-of-speech 
tags is also available (Jin et al 2006). This corpus 
is targeted to a very restricted type of diseases. The 
annotations are also domain specific, so that ?can-
cer of the lung? is not considered a malignancy 
mention but a mention of malignancy and a men-
tion of malignancy location. 
As in previous studies, we aim to investigate the 
complexity of automatic disease recognition using 
state-of-the-art computational techniques. This 
work is novel in at least three aspects: first, in ad-
dition to using the MEDLINE sentence corpus 
(Jimeno et al2008), we developed a new corpus 
comprising disease annotations on 500 randomly 
selected PubMed queries. This allowed us to inves-
tigate the influence of local context3 through the 
comparison of system performance between two 
different genres of biomedical text. Second, by 
using a knowledge based tool previously ben-
chmarked on the same MEDLINE corpus (Jimeno 
et al 2008), we show that significant performance 
differences can be observed when parameters are 
adjusted. Finally, a state-of-the-art statistical ap-
proach was adapted for disease name recognition 
and evaluated on both corpora.  
3 Two Biomedical Corpora with disease 
annotations 
The first issue in the development of such a corpus 
is to define the very concept of disease. Among the 
numerous terminological resources available, such 
as Medical Subject Headings (MeSH?, 4,354 dis-
ease concepts) or the International Classification of 
Diseases (ICD9, ~18,000 disease concepts), the 
UMLS Metathesaurus? is the most comprehensive: 
the 2008AB release includes 252,284 concepts in 
the disorder Semantic Group defined by McCray 
et al (2001). The UMLS Metathesaurus is part of 
the Semantic Network, which also includes a set of 
broad subject categories, or Semantic Types, that 
provide a consistent categorization of all concepts 
represented in the Metathesaurus. The Semantic 
Groups aim at providing an even broader categori-
zation for UMLS concepts. For example, the dis-
order Semantic Group comprises 12 Semantic 
Types including Disease or Syndrome, Cell or Mo-
lecular Dysfunction and Congenital Abnormalities.  
                                                        
3 Here, by context, we mean the information surrounding a 
disease mention available in the corpora. This is different from 
the ?search context? previously discussed.   
145
Furthermore, like the gene mention (Morgan et 
al. 2008) and gene normalization (Smith et al 
2008) tasks in BioCreative II, the task of disease 
name recognition can also be performed at two 
different levels: 
 
1. disease mention: the detection of a snippet 
of text that refers to a disease concept (e.g. 
?alzheimer? in the sample query shown in 
Table 2)  
2. disease concept: the recognition of a con-
trolled vocabulary disease concept (e.g. 
?C0002395-alzheimer?s disease? in our Ta-
ble 2 example) in text.  
 
In this work, we evaluate and report system per-
formance at the concept level. 
3.1 Biomedical literature corpus 
Sentence Kniest dysplasia is a moderately 
severe chondrodysplasia pheno-
type that results from mutations 
in the gene for type ii collagen 
col2a1.  
Annotations C0265279-Kniest dysplasia 
C0343284-Chondrodysplasia, 
unspecified 
Table 1: Excerpt of literature corpus (PMID: 7874117) 
 
The corpus made available by Jimeno et al con-
sists of 551 MEDLINE sentences annotated with 
UMLS concepts or concept clusters: concepts that 
were found to be linked to the same term. For ex-
ample, the concepts ?Pancreatic carcinoma? 
(C0235974) and ?Malignant neoplasm of pan-
creas? (C0346647) share the same synonym ?Pan-
creas Cancer?, thus they were clustered. The 
sentences were selected from a set of articles cu-
rated for Online Mendelian Inheritance in Man 
(OMIM) and contain an average of 27(+/- 11) to-
kens, where tokens are defined as sequences of 
characters separated by white space. A set of 
UMLS concepts (or clusters) is associated with 
each sentence in the corpus. However, no boun-
dary information linking a phrase in a sentence to 
an annotation was available. Table 1 shows a sam-
ple sentence and its annotations. 
 
 
 
3.2 Biomedical query corpus 
A total of 500 PubMed queries were randomly se-
lected and divided into two batches of 300 and 200 
queries, respectively. Queries were on average 
3.45(+/- 2.64) tokens long in the 300 query batch 
and 3.58(+/- 4.63) for the 200 query batch, which 
is consistent with the average length of PubMed 
queries (3 tokens) reported by Herskovic et al 
(2007).  
The queries in the first set were annotated using 
Knowtator (Ogren, 2006) by three annotators with 
different backgrounds (one biologist, one informa-
tion scientist, one computational linguist). Two 
annotators annotated the queries using UMLS con-
cepts from the disorder group, while the other an-
notator simply annotated diseases without 
reference to UMLS concepts. Table 2 shows a 
sample query and its annotations. A consensus set 
was obtained after a meeting between the annota-
tors where diverging annotations were discussed 
and annotators agreed on a final, unique, version of 
all annotations.  The consensus set contains 89 dis-
ease concepts (76 unique). 
 
Query alzheimer csf amyloid 
Annotations  Ann. 1: ?alzheimer?; 0-8;  
Ann. 2, 3: ?alzheimer?; 0-8; 
C0002395-alzheimer?s disease 
Table 2: Excerpt of annotated 300-query corpus. Boun-
dary information is given as the character interval of the 
annotated string in the query (here, 0-8). 
 
The queries in the second set were annotated 
with UMLS concepts from the disorder group by 
one of the annotators who also worked on the pre-
vious set. In this set, 53 disease concepts were an-
notated (51 unique). 
4 Automatic disease recognition 
With the perspective of a contextual IR applica-
tion where the disease concepts found in queries 
will be used to refer users to disease-specific in-
formation in databases other than MEDLINE, we 
are concerned with high precision performance. 
For this reason, we decided to experiment with 
methods that showed the highest precision when 
compared to others. In addition, given the size of 
the corpora available and the type of the annota-
146
tions, machine learning methods such as CRFs or 
SVM did not seem applicable.  
Table 3 shows a description of the training and 
test sets for each corpus. 
 
 Table 3: Description of the training and test sets 
4.1 Natural Language Processing 
Disease recognition was performed using the Natu-
ral Language Processing algorithm implemented in 
MetaMap (Aronson, 2001)4. The tool was re-
stricted to retrieve concepts from the disorder 
group, using the UMLS 2008AB release and 
?longest match? feature. 
In practice, MetaMap parses the input text into 
noun phrases, generates variants of these phrases 
using knowledge sources such as the SPECIALIST 
lexicon, and maps the phrases to UMLS concepts.  
4.2 Priority Model 
The priority model was first introduced in (Tanabe 
and Wilbur, 2006) and is adapted here to detect 
disease mentions in free text. Because our evalua-
tion is performed at the concept level, the mentions 
extracted by the model are then mapped to UMLS 
using MetaMap.  
The priority model approach is based on two sets 
of phrases: one names of diseases, D, and one 
names of non-diseases, N. One trains the model to 
assign two numbers, p and q, to each token t that 
appears in a phrase in either D or N. Roughly, p is 
the probability that a phrase from D or N that has 
the token t in it is actually from D and q is the rela-
tive weight that should be assigned to t for this 
purpose and represents a quality estimate. Given a 
phrase 
                                                        
4 Additional information is also available at 
http://metamap.nlm.nih.gov/ 
 
1 2 kph t t t?
    (1) 
and for each it  the corresponding numbers ip  and 
iq  we estimate the probability that ph D  by 
 
1 22 11 1k kkj i i jij j iprob p q q p q  
(2) 
 
The training procedure for the model actually 
chooses the values of all the p and q quantities to 
optimize the 
prob
 values over all of D and N.  
For this work we have extended the approach to 
include a quantity  
21 1 22 11 1k kkj i i jij j iqual q p q q p q prob
(3) 
 
which represents a weighted average of all the 
quality numbers iq . We apply this formula to ob-
tain 
qual
as long as 
0.5.prob
 If 
0.5prob
we 
replace all numbers 
ip  by 1 ip  in (2) and (3) to 
obtain 
qual
.  
For this application we obtained the sets D and 
N from the SEMCAT data (Tanabe, Thom et al 
2006) supplemented with the latest UMLS data. 
We removed any term from D and N that contained 
less than five characters in order to decrease the 
occurrence of ambiguous terms.  Also the 1,000 
most frequent terms from D were examined ma-
nually and the ambiguous ones were removed. The 
end result is a set of 332,984 phrases in D and 
4,253,758 phrases in N. We trained the priority 
model on D and N and applied the resulting train-
ing to compute for each phrase in D and N a vector 
of values 
,prob qual
. In this way D and N are 
converted to 
DV  and NV . We then constructed a 
Mahalanobis classifier (Duda, Hart and Stork, 
2001) for two dimensional vectors as the differ-
ence in the Mahalanobis distance of any such vec-
tor to Gaussian approximations to 
DV  and NV .  We 
refer to the number produced by this classifier as 
the Mahalanobis score.  By randomly dividing both 
D and N into three equal size pieces and training 
on two from each and testing on the third, in a 
three-fold cross validation we found the Mahala-
nobis classifier to perform at 98.4% average preci-
sion and 93.9% precision-recall breakeven point. 
In a final step we applied a simple regression me-
thod to estimate the probability that a given Maha-
Data Lit. Corpus Query Corpus 
Training 276 sentences 
(487 disease con-
cepts, 185 unique) 
300 queries (89 
disease concepts, 
76 unique) 
Testing 275 sentences 
(437 disease con-
cepts, 185 unique) 
200 queries (53 
disease concepts, 
51 unique) 
All 551 sentences 
(924 disease con-
cepts, 280 unique) 
500 queries (142 
disease concepts, 
120 unique) 
147
lanobis score was produced by a phrase belonging 
to D and not N. Given a phrase phr we will denote 
this final probability produced as PMA(phr).  
The second important ingredient of our statistic-
al process is how we produce phrases from a piece 
of text. Given a string of text TX we apply tokeni-
zation to TX to produce an ordered set of tokens 
1 2, , , nt t t?
. Among the tokens produced will be 
punctuation marks and stop words and we denote 
the set of all such tokens by Z . We call a token 
segment 
, ,j kt t?
 maximal if it contains no ele-
ment of Z  and if either 1j  or 
1jt Z
 and 
likewise if k n  or 
1kt Z
. Given text TX we 
will denote the set of all maximal token segments 
produced in this way by 
max ( ).S TX
 Now given a 
maximal token segment mts=
, ,j kt t?
 we define 
two different methods of finding phrases in mts. 
The first assumes we are given an arbitrary set of 
phrases PH.  We recursively define a set of phrases 
,I mts PH
 beginning with this set empty and 
with the parameter 
u j
.  Each iteration consists 
of asking for the largest v k  for which 
, ,u vt t PH?
. If there is such a v  we add 
, ,u vt t?
 to 
,I mts PH
 and set 
1u v
. 
Otherwise we set 
1u u
. We repeat this process 
as long as u k .  The second approach assumes 
we are given an arbitrary set of two token phrases 
P2.  Again we recursively define a set of phrases 
, 2J mts P
 beginning with this set empty and 
with the parameter 
u j
. Each iteration consists 
of asking for the largest v k  for which given any 
,  i u i v
,  
1, 2i it t P
. If there is such a v  
we add 
, ,u vt t?
 to 
, 2J mts P
 and set 
1u v
. Otherwise we set 
1u u
. We repeat 
this process as long as u k .   
In order to apply our phrase extraction proce-
dures we need good sets of phrases. In addition to 
D and N already defined above, we use another set 
of phrases defined as follows. Let R denote the set 
of all token strings with two or more tokens which 
do not contain tokens from Z and for which there 
are at least three MEDLINE records (title and ab-
stract text only) in which the token string is re-
peated at least twice. 
We then define
R R D N
. We make 
use of R  in addition to D and N. For the set 2P  
we take the set of all two token phrases in 
MEDLINE documents for which the two tokens 
co-occur as this phrase much more than expected, 
i.e., with a 
2 10,000
(based on the two-by-two 
contingency table).  
 
 
#Initialization: Given a text TX, set 
maxS S TX
 and .X  
#Processing: While(S ){ 
  I. select mts S  
  II. If( ,I mts D ) ,K I mts D 
       else if( ,I mts R ) ,K I mts R  
        else if( ,I mts N ) K  
        else 
if( , 2J mts P ) , 2K J mts P 
        else K  
  III. X X K  
  IV. S S mts  
     } 
#Return: All pairs , ,  phr PMA phr phr X 
 
Figure 1: Phrase finding algorithm 
 
With these preliminaries, our phrase finding al-
gorithm in pseudo-code is shown in Figure 1. 
The output of this algorithm may then be filtered 
by setting a threshold on the PMA values to accept. 
5 Results  
5.1 Assessing the difficulty of the task 
To assess the difficulty of disease recognition, we 
computed the inter-annotator agreement (IAA) on 
the 300-query corpus. Agreement was computed at 
the disease mention level for all three annotators 
and at the disease concept level for the two annota-
tors who produced UMLS annotations.  
Inter-annotator agreement measures for NLP 
applications have been recently discussed by 
Artstein and Poesio (2008) who advocate for the 
use of chance corrected measures. However, in our 
case, agreement was partly computed on a very 
large set of categories (UMLS concepts) so we 
decided to use Knowtator?s built-in feature, which 
computes IAA as the percentage of agreement and 
148
allows partial string matches. For example, in the 
query ?dog model transient ischemic attacks?, an-
notator 1 selected ?ischemic attacks? as a disorder 
while annotator 2 and 3 selected ?transient ischem-
ic attacks? as UMLS concept C0007787: Attacks, 
Transient Ischemic. In this case, at the subclass 
level (?disorder?) we have a match for this annota-
tion. But at the exact span or exact category level, 
there is no match. Table 4 shows details of IAA at 
the disease mention level when partial matches are 
taken into account. For exact span matches, the 
IAA is lower, at 64.87% on average. 
 
Disorder IAA Ann. 1 Ann. 2 Ann. 3 
Ann. 1 100% 71.77% 75.86% 
Ann. 2  100% 71.68% 
Ann. 3   100% 
Table 4: Agreement on disease mention annotations 
(partial match allowed) ? average is 73.10% 
 
At the concept level, the agreement (when par-
tial matches were allowed) varied significantly 
depending on the semantic types. It ranged be-
tween 33% for Findings and 83% for Mental or 
Behavioral Dysfunction. However, agreement on 
the most frequent category, Disease or Syndrome, 
was 72%, which is close to the annotators? overall 
agreement at the mention level. One major cause 
of disagreement was ambiguity caused by concepts 
that were clustered by Jimeno et al For example, 
in query ?osteoporosis and ?fracture pattern?, an-
notator 2 marked ?osteoporosis? with both 
?C0029456-osteoporosis?(a Disease or Syndrome 
concept) and ?C1962963-osteoporosis adverse 
event?(a Finding concept) while annotator 3 only 
used ?C0029456-osteoporosis?.    
5.2 Results on Literature corpus 
As shown in Table 3, the corpus was randomly 
split into a training set (276 sentences) and a test 
set (275 sentences). The training set was used to 
determine the optimal probability threshold for the 
Priority Model and parameter selection for Meta-
Map, respectively. 
 
Priority Model parameter adjustments: the first 
result observed from applying the Priority Model 
was that D yielded about 90% of the output of the 
algorithm. Also results coming from R  and 2P  
were not well mapped to UMLS concepts by Me-
taMap. As a result, in this work we ignored disease 
candidates retrieved based on R  and 2P . The best 
F-measure was obtained for a threshold of 0.3, 
which was consequently used on the test set.  
Since the Priority Model algorithm does not per-
form any mapping to a controlled vocabulary 
source, the mapping was performed by applying 
MetaMap to the snippets of text returned with a 
probability value above the threshold. 
 
Threshold P R F 
0 64 73 67 
.1 67 73 70 
.2 67 73 70 
.3 68 73 71 
.4 68 73 70 
.5 68 72 69 
.6 68 72 69 
.7 68 72 69 
.8 68 68 68 
.9 65 60 62 
Table 5: Precision (P), Recall (R) and F-measure of the 
Priority Model on the training set for different values of 
the probability threshold. 
 
The results presented in Table 5 were obtained 
before any MetaMap adjustments were made.  
 
MetaMap parameter adjustments: an error anal-
ysis was performed to adjust MetaMap settings. 
Errors fell into the following categories:  
 A more specific disease should have been 
recognized (e.g. ?deficiency? vs. ?C2 defi-
ciency?) 
 The definition of a cluster was lacking 
(e.g. ?G6PD deficiency? comprised 
C0237987- Glucose-6-phosphate dehydro-
genase deficiency anemia and C0017758- 
Glucosphosphate Dehydrogenase Defi-
ciency but not C0017920- Deficiency of 
glucose-6-phosphatase)  
 MetaMap mapping was erroneous (e.g. 
?hereditary breast? was mapped to 
C0729233-Dissecting aneurysm of the 
thoracic aorta instead of C0346153-
Hereditary Breast Cancer)  
 
The results of inter-annotator agreement and fur-
ther study of MetaMap mappings indicated that 
concepts with the semantic type Findings seemed 
149
to be frequently retrieved erroneously. For this rea-
son, we also experimented not taking Findings into 
account as an additional adjustment for MetaMap. 
Table 6 shows the results of applying the MetaMap 
adjustments yielded from the error analysis on the 
training corpus. 
 
Threshold Findings P R F 
.3 Yes 80 78 79 
.3 No 85 78 81 
Table 6: performance of the Priority Model on the train-
ing set for threshold .3 depending on whether mappings 
to Findings are used in the ?adjustments?      
 
MetaMap disorder detection was also performed 
directly on the training corpus. An error analysis 
similar to what was presented above was carried 
out to determine the best parameters. Table 7 be-
low shows the results obtained when all concepts 
from the 12 Semantic Types (STs) in the disorder 
group are taken into account with no adjustments 
(?raw?). Then, results including the adjustments 
from the error analysis are shown when all 12 STs 
are taken into account, when Findings are excluded 
(11STs) and when only the most frequent 6STs in 
the training set are taken into account.    
 
Processing P R F 
Raw (12 STs) 50 77 61 
Adjusted (12 STs) 52 75 61 
Adjusted (11 STs) 57 73 64 
Adjusted (6 STs) 77 72 74 
Table 7: Performance of MetaMap on the training set      
 
Finally, Table 8 shows the performance of both 
methods on the test set, using the optimal settings 
determined on the training set:  
 
Method P R F 
Priority Model 80 74 77 
MetaMap 75 78 76 
Table 8: Precision (P), Recall (R) and F-measure of the 
Priority Model and MetaMap on the test set     
5.3 Results on Query Corpus 
The 300-query corpus was used as a training set 
and the 200-query corpus was used as a test set. 
For consistency with work on the literature corpus, 
we assessed the disease recognition on a gold stan-
dard set including ?clusters? of UMLS concepts 
were appropriate. As previously with the Literature 
corpus, we used the training set to determine the 
best settings for each method. The performance of 
the Priority Model at different values of the proba-
bility threshold, based on the use of D and N as the 
sets of sample phrases is similar to that obtained 
with the literature corpus; 0.3 stands out as one of 
the three values for which the best F-measure is 
obtained (tied with .5 and .8).  
Because of the brevity of queries vs. sentences, 
the MetaMap error analysis was very succinct and 
resulted in:  
 Removal of C0011860-Diabetes mellitus 
type 2  as mapping for ?diabetes? 
 Removal of all occurrences of C0600688-
Toxicity and C0424653-Weight symptom 
(finding)  
 Adjustment on the number of STs taken in-
to account 
 
The difference in performance obtained on the 
training set for the different MetaMap adjustments 
considered is shown in Table 9 when MetaMap 
was applied to Priority Model output and in Table 
10 when it was applied directly on the queries.    
 
Threshold Findings P R F 
.3 Yes 60 72 65 
.3 No 73 70 71 
Table 9: performance of the Priority Model on the train-
ing set for threshold .3 depending on whether mappings 
to Findings are used in the ?adjustments? 
 
Processing P R F 
Raw (12 STs) 41 82 55 
Adjusted (12 STs) 44 82 57 
Adjusted (11 STs) 58 81 68 
Adjusted (6 STs) 64 75 69 
Table 10: performance of MetaMap on the training set 
 
Finally, Table 11 shows the performance of both 
methods on the test set, using the optimal settings 
determined on the training set:  
 
Method P R F 
Priority Model 76 72 74 
MetaMap 66 74 70 
Table 11: Precision (P), Recall (R) and F-measure of 
the Priority Model and MetaMap on the test set 
150
6 Discussion 
Comparing the Two Methods. The performance 
of both methods on the query corpus is comparable 
to inter-annotator agreement (F=70-74 vs. IAA=72 
on Disease and Syndromes). On both corpora, the 
Priority Model achieves higher precision and F-
measure, while MetaMap achieves better recall.  
Comparing the results obtained with MetaMap 
with those reported by Jimeno et al, precision is 
lower, but recall is much higher. This is likely to 
be due to the different MetaMap settings, and the 
use of different UMLS versions - Jimeno et al did 
not provide any of this information, but based on 
the publication date of their paper, it is likely that 
they used one of the 2006 UMLS releases. Meystre 
and Haug (2006) also found that significant per-
formance differences could be obtained with Me-
taMap by adjusting the content of the knowledge 
sources used.   
On both text genres, 0.3 was found to be the op-
timal probability threshold for the Priority Model. 
Based on the performance at different values of the 
threshold, it seems that the model is quite efficient 
at ruling out highly unlikely diseases. However, for 
values above .3 the performance does not vary 
greatly.  
 
Comparing Text Genres. For both methods, 
disease recognition seems more efficient on sen-
tences. This is to be expected: sentences provide 
more context (e.g. more tokens surrounding the 
disease mention are available) and allow for more 
efficient disambiguation, for example on acro-
nyms. Although acronyms are frequent both in 
queries and sentences, more undefined acronyms 
are found in queries. However, the difference in 
performance between the two methods seems 
higher on the query corpus. This indicates that the 
Priority Model could be more robust to sparse con-
text.  
It should be noted that there were diseases in all 
sentences in the literature corpus vs. about 1/3 to 
1/2 of the queries. In addition, the query corpus 
included many author names, which could create 
confusion with disease names (in particular for the 
Priority Model). This difficulty was not found in 
the sentence corpus. However, sentences some-
times contain negated mention of diseases, which 
never occurred in the query corpus where little to 
no syntax is used.  
We also noticed that while Findings seemed to 
be generally problematic concepts in both corpora, 
other concepts such as Injury and Poisoning were 
much more prevalent in the query corpus. For this 
reason, for the general task of disease recognition, 
a drastic restriction to as little as 6 STs is probably 
not advisable.  
 
Limitations of the study. One limitation of our 
study is the relatively small number of disease 
concepts in the query corpus. Although the query 
and sentence corpus contain about 500 que-
ries/sentences each, there are significantly less dis-
ease concepts found in queries compared to 
sentences. As a result, there is also less repetition 
in the disease concept found. This is partly due to 
the brevity of queries compared to sentences but 
mainly to the fact that while all the sentences in the 
literature corpus had at least one disease concept, 
this was not the case for the query corpus. We are 
currently addressing this issue with the ongoing 
development of a large scale query corpus anno-
tated for diseases and other relevant biomedical 
entities.  
7 Conclusions 
We found that of the two steps of disease recogni-
tion, disease mention gets the higher inter-
annotator agreement (vs. concept mapping). We 
have applied a statistical and an NLP method for 
the automatic recognition of disease concepts in 
two genres of biomedical text. While both methods 
show good performance (F=77% vs. F=76%) on 
the sentence corpus, results indicate that the statis-
tical model is more robust on the query corpus 
where very little disease context information is 
available (F=74% vs. F=70%). As a result, the 
priority model will be used for disease detection in 
PubMed queries in order to characterize users? 
search contexts for contextual IR. 
Acknowledgments 
This research was supported by the Intramural Re-
search Program of the NIH, National Library of 
Medicine. The authors would like to thank S. 
Shooshan and T. Tao for their contribution to the 
annotation of the query corpus; colleagues in the 
NCBI engineering branch for their valuable feed-
back at every step of the project.   
151
References  
Alan R. Aronson, Olivier Bodenreider, Dina Demner-
Fushman, Kin Wah Fung, Vivan E. Lee, James G. 
Mork et al 2007. From Indexing the Biomedical Li-
terature to Coding Clinical Text: Experience with 
MTI and Machine Learning Approaches. ACL 
Workshop BioNLP.  
Alan Aronson. 2001. Effective mapping of biomedical 
text to the UMLS Metathesaurus: the MetaMap pro-
gram. Proceedings of AMIA Symp:17-21. 
Ron Artstein and Massimo Poesio. 2008. Inter-Coder 
Agreement for Computational Linguistics. Compu-
tational Linguistics 34(4): 555-596 
Jing Bai, and Jian-Yun Nie. 2008. Adapting information 
retrieval to query contexts. Information Processing & 
Management. 44(6):1902-22 
Robert O. Duda, Peter. E. Hart and David G. Stork. 
2001. Pattern Classification. New York: John Wiley 
& Sons, Inc. 
R. Brian Haynes and Nancy L. Wilczynski. 2004. Op-
timal search strategies for retrieving scientifically 
strong studies of diagnosis from Medline: analytical 
survey. BMJ. 328(7447):1040. 
Jorge R. Herskovic, Len Y. Tanaka, William Hersh and 
Elmer V. Bernstam. 2007. A day in the life of 
PubMed: analysis of a typical day's query log. Jour-
nal of the American Medical Informatics Association. 
14(2):212-20. 
Antonio Jimeno, Ernesto Jimenez-Ruiz, Vivian Lee, 
Sylvain Gaudan, Rafael Berlanga and Dietrich 
Rebholz-Schuhmann. 2008. Assessment of disease 
named entity recognition on a corpus of annotated 
sentences. BMC Bioinformatics. 11;9 Suppl 3:S3. 
Yang Jin, Ryan T McDonald, Kevin Lerman, Mark A 
Mandel, Steven Carroll, Mark Y Liberman et al 
2006. Automated recognition of malignancy men-
tions in biomedical literature. BMC Bioinformatics.  
7:492. 
Alexa T. McCray, Anita Burgun and Olivier Bodenreid-
er. 2001. Aggregating UMLS semantic types for 
reducing conceptual complexity. Proceedings of 
Medinfo 10(Pt 1):216-20. 
St?phane Meystre and Peter J. Haug. 2006. Natural lan-
guage processing to extract medical problems from 
electronic clinical documents: performance evalua-
tion. J Biomed Inform. 39(6):589-99. 
Alexander A. Morgan, Zhiyong Lu, Xinglong Wang, 
Aaron M. Cohen, Juliane Fluck, Patrick Ruch et al 
2008. Overview of BioCreative II gene normaliza-
tion. Genome Biol. 9 Suppl 2:S3. 
Phillip V. Ogren. 2006. Knowtator: A plug-in for creat-
ing training and evaluation data sets for Biomedical 
Natural Language systems. 9th Intl. Prot?g? Confe-
rence  
Jong C. Park and Jung-Jae Kim. 2006. Named Entity 
Recognition. In S. Ananiadou and J. McNaught 
(Eds), Text Mining for Biology and Biomedicine (pp. 
121-42). Boston|London:Artech House Inc.  
Wanda Pratt and Henry Wasserman. 2000. QueryCat: 
automatic categorization of MEDLINE queries. Pro-
ceedings of AMIA Symp:655-9.  
Tom C. Rindflesh and Marcelo Fiszman. 2003. The 
interaction of domain knowledge and linguistic struc-
ture in natural language processing: interpreting 
hypernymic propositions in biomedical text. J Bio-
med Inform. 36(6):462-77 
Daniel E. Rose and Danny Levinson. 2004. Understand-
ing user goals in web search. In Proceedings of the 
13th international Conference on World Wide 
Web:13-9  
Tefko Saracevic. 1997. The Stratified Model of Infor-
mation Retrieval Interaction: Extension and Applica-
tion. Proceedings of the 60th meeting of the. 
American Society for Information Science:313-27 
Xuehua Shen, Bin Tan and ChengXiang Zhai. 2005 
Context-sensitive information retrieval using impli-
cit feedback, In Proceedings of the 28th annual in-
ternational conference ACM SIGIR conference on 
Research and development in information retrieval: 
43-50.  
Larry Smith, Laurraine K. Tanabe, Rie J. Ando, Cheng-
Ju Kuo, I-Fang Chung , Chun-Nan Hsu et al 2008. 
Overview of BioCreative II gene mention recogni-
tion. Genome Biol. 9 Suppl 2:S2. 
Laurraine K. Tanabe, Lynn. H. Thom, Wayne Matten, 
Donald C. Comeau and W. John Wilbur. 2006. 
SemCat: semantically categorized entities for ge-
nomics. Proceedings of AMIA Symp: 754-8. 
Laurraine K. Tanabe and W. John Wilbur. 2006. A 
Priority Model for Named Entities. Proceedings of 
HLT-NAACL BioNLP Workshop:33-40 
Jaime Teevan, Susan T. Dumais and Eric Horvitz. 2005. 
Personalizing search via automated analysis of in-
terests and activities. In Proceeding of ACM-
SIGIR?05:449?56. 
Ji-Rong Wen, Ni Lao, Wei-Ying Ma. 2004. Probabilis-
tic model for contextual retrieval. Proceedings of 
ACM-SIGIR?04:57?63 
Xiangmin Zhang, Hermina G.B. Anghelescu and Xiao-
jun Yuan. 2005. Domain knowledge, search beha-
vior, and search effectiveness of engineering and 
science students: An exploratory study, Information 
Research 10(2): 217. 
152
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 33?40,
New York City, June 2006. c?2006 Association for Computational Linguistics
A Priority Model for Named Entities 
 
 
Lorraine Tanabe W. John Wilbur 
National Center for Biotechnology 
Information 
National Center for Biotechnology 
Information 
Bethesda, MD 20894 Bethesda, MD 20894 
tanabe@ncbi.nlm.nih.gov wilbur@ncbi.nlm.nih.gov 
  
 
 
 
Abstract 
We introduce a new approach to named 
entity classification which we term a Pri-
ority Model. We also describe the con-
struction of a semantic database called 
SemCat consisting of a large number of 
semantically categorized names relevant 
to biomedicine. We used SemCat as train-
ing data to investigate name classification 
techniques. We generated a statistical lan-
guage model and probabilistic context-
free grammars for gene and protein name 
classification, and compared the results 
with the new model.  For all three meth-
ods, we used a variable order Markov 
model to predict the nature of strings not 
represented in the training data.  The Pri-
ority Model achieves an F-measure of 
0.958-0.960, consistently higher than the 
statistical language model and probabilis-
tic context-free grammar.   
1 Introduction 
Automatic recognition of gene and protein names 
is a challenging first step towards text mining the 
biomedical literature. Advances in the area of gene 
and protein named entity recognition (NER) have 
been accelerated by freely available tagged corpora 
(Kim et al, 2003, Cohen et al, 2005, Smith et al, 
2005, Tanabe et al, 2005).  Such corpora have 
made it possible for standardized evaluations such 
as Task 1A of the first BioCreative Workshop 
(Yeh et al, 2005). 
Although state-of-the-art systems now perform 
at the level of 80-83% F-measure, this is still well 
below the range of 90-97% for non-biomedical 
NER.  The main reasons for this performance dis-
parity are 1) the complexity of the genetic nomen-
clature and 2) the confusion of gene and protein 
names with other biomedical entities, as well as 
with common English words. In an effort to allevi-
ate the confusion with other biomedical entities we 
have assembled a database consisting of named 
entities appearing in the literature of biomedicine 
together with information on their ontological 
categories. We use this information in an effort to 
better understand how to classify names as repre-
senting genes/proteins or not.  
2 Background  
A successful gene and protein NER system must 
address the complexity and ambiguity inherent in 
this domain.  Hand-crafted rules alone are unable 
to capture these phenomena in large biomedical 
text collections.  Most biomedical NER systems 
use some form of language modeling, consisting of 
an observed sequence of words and a hidden se-
quence of tags.  The goal is to find the tag se-
quence with maximal probability given the 
observed word sequence.  McDonald and Pereira 
(2005) use conditional random fields (CRF) to 
identify the beginning, inside and outside of gene 
and protein names.   GuoDong et al (2005) use an 
ensemble of one support vector machine and two 
Hidden Markov Models (HMMs).  Kinoshita et al 
(2005) use a second-order Markov model.  Dingare 
et al (2005) use a maximum entropy Markov 
model (MEMM) with large feature sets.   
33
NER is a difficult task because it requires both 
the identification of the boundaries of an entity in 
text, and the classification of that entity.  In this 
paper, we focus on the classification step.  Spasic 
et al (2005) use the MaSTerClass case-based rea-
soning system for biomedical term classification.  
MaSTerClass uses term contexts from an annotated 
corpus of 2072 MEDLINE abstracts related to nu-
clear receptors as a basis for classifying new 
terms.  Its set of classes is a subset of the UMLS 
Semantic Network (McCray, 1989), that does not 
include genes and proteins.  Liu et al (2002) clas-
sified terms that represent multiple UMLS con-
cepts by examining the conceptual relatives of the 
concepts.  Hatzivassiloglou et al (2001) classified 
terms known to belong to the classes Protein, Gene 
and/or RNA using unsupervised learning, achieving 
accuracy rates up to 85%.  The AZuRE system 
(Podowski et al, 2004) uses a separate modified 
Naive Bayes model for each of 20K genes.  A term 
is disambiguated based on its contextual similarity 
to each model. Nenadic et al (2003) recognized 
the importance of terminological knowledge for 
biomedical text mining. They used the C/NC-
methods, calculating both the intrinsic characteris-
tics of terms (such as their frequency of occurrence 
as substrings of other terms), and the context of 
terms as linear combinations.  These biomedical 
classification systems all rely on the context sur-
rounding named entities. While we recognize the 
importance of context, we believe one must strive 
for the appropriate blend of information coming 
from the context and information that is inherent in 
the name itself.  This explains our focus on names 
without context in this work.  
We believe one can improve gene and protein 
entity classification by using more training data 
and/or using a more appropriate model for names.  
Current sources of training data are deficient in 
important biomedical terminologies like cell line 
names.  To address this deficiency, we constructed 
the SemCat database, based on a subset of the 
UMLS Semantic Network enriched with categories 
from the GENIA Ontology (Kim et al 2003), and a 
few new semantic types. We have populated Sem-
Cat with over 5 million entities of interest from 
Figure 1. SemCat Physical Object Hierarchy.  White = UMLS SN, Light Grey = GENIA semantic 
types, Dark Grey = New semantic types. 
34
standard knowledge sources like the UMLS 
(Lindberg et al, 1993), the Gene Ontology (GO) 
(The Gene Ontology Consortium, 2000), Entrez 
Gene (Maglott et al, 2005), and GENIA, as well as 
from the World Wide Web.  In this paper, we use 
SemCat data to compare three probabilistic frame-
works for named entity classification.   
3 Methods 
We constructed the SemCat database of biomedical 
entities, and used these entities to train and test 
three probabilistic approaches to gene and protein 
name classification: 1) a statistical language model 
with Witten-Bell smoothing, 2) probabilistic con-
text-free grammars (PCFGs) and 3) a new ap-
proach we call a Priority Model for named entities. 
As one component in all of our classification algo-
rithms we use a variable order Markov Model for 
strings.   
3.1 SemCat Database Construction 
The UMLS Semantic Network (SN) is an ongoing 
project at the National Library of Medicine.  Many 
users have modified the SN for their own research 
domains.  For example, Yu et al (1999) found that 
the SN was missing critical components in the ge-
nomics domain, and added six new semantic types 
including Protein Structure and Chemical Com-
plex. We found that a subset of the SN would be 
sufficient for gene and protein name classification, 
and added some new semantic types for better cov-
erage.  We shifted some semantic types from 
suboptimal nodes to ones that made more sense 
from a genomics standpoint. For example, there 
were two problems with Gene or Genome. Firstly, 
genes and genomes are not synonymous, and sec-
ondly, placement under the semantic type Fully 
Formed Anatomical Structure is suboptimal from a 
genomics perspective. Since a gene in this context 
is better understood as an organic chemical, we 
deleted Gene or Genome, and added the GENIA 
semantic types for genomics entities under Or-
ganic Chemical. The SemCat Physical Object hier-
archy is shown in Figure 1.  Similar hierarchies 
exist for the SN Conceptual Entity and Event trees.  
A number of the categories have been supple-
mented with automatically extracted entities from 
MEDLINE, derived from regular expression pat-
tern matching.  Currently, SemCat has 77 semantic 
types, and 5.11M non-unique entries. Additional 
entities from MEDLINE are being manually classi-
fied via an annotation website.  Unlike the Ter-
mino database (Harkema et al (2004), which 
contains terminology annotated with morpho-
syntactic and conceptual information, SemCat cur-
rently consists of gazetteer lists only.  
For our experiments, we generated two sets of 
training data from SemCat, Gene-Protein (GP) and 
Not-Gene-Protein (NGP).  GP consists of specific 
terms from the semantic types DNA MOLECULE, 
PROTEIN MOLECULE, DNA FAMILY, 
PROTEIN FAMILY, PROTEIN COMPLEX and 
PROTEIN SUBUNIT.  NGP consists of entities 
from all other SemCat types, along with generic 
entities from the GP semantic types.  Generic enti-
ties were automatically eliminated from GP using 
pattern matching to manually tagged generic 
phrases like abnormal protein, acid domain, and 
RNA.  
Many SemCat entries contain commas and pa-
rentheses, for example, ?receptors, tgf beta.?  A 
better form for natural language processing would 
be ?tgf beta receptors.?  To address this problem, 
we automatically generated variants of phrases in 
GP with commas and parentheses, and found their 
counts in MEDLINE.  We empirically determined 
the heuristic rule of replacing the phrase with its 
second most frequent variant, based on the obser-
vation that the most frequent variant is often too 
generic.  For example, the following are the phrase 
variant counts for ?heat shock protein (dnaj)?: 
 
? heat shock protein (dnaj) 0 
? dnaj heat shock protein  84 
? heat shock protein  122954 
? heat shock protein dnaj  41 
 
Thus, the phrase kept for GP is dnaj heat shock 
protein.  
After purifying the sets and removing ambigu-
ous full phrases (ambiguous words were retained), 
GP contained 1,001,188 phrases, and NGP con-
tained 2,964,271 phrases. From these, we ran-
domly generated three train/test divisions of 90% 
train/10% test (gp1, gp2, gp3), for the evaluation.   
3.2    Variable Order Markov Model for Strings 
As one component in our classification algorithms 
we use a variable order Markov Model for strings.  
Suppose C represents a class and 1 2 3... nx x x x  repre-
35
sents a string of characters. In order to estimate the 
probability that 1 2 3... nx x x x  belongs to  we apply 
Bayes? Theorem to write 
C
 
( ) ( ) ( )( )1 2 31 2 3 1 2 3
... |
| ...
...
n
n
n
p x x x x C p C
p C x x x x
p x x x x
=      (1) 
 
Because ( )1 2 3... np x x x x does not depend on the 
class and because we are generally comparing 
probability estimates between classes, we ignore 
this factor in our calculations and concentrate our 
efforts on evaluating ( ) ( )1 2 3... |np x x x x C p C . 
First we write 
 
( ) (1 2 3 1 2 3 11... | | ... ,nn kkp x x x x C p x x x x x C?==? )k
)
     (2) 
 
which is an exact equality. The final step is to give 
our best approximation to each of the num-
bers ( 1 2 3 1| ... ,k kp x x x x x C? . To make these ap-
proximations we assume that we are given a set of 
strings and associated probabilities ( ){ } 1, Mi i is p =  
where for each i ,  and 0ip > ip  is assumed to 
represent the probability that  belongs to the 
class C .  Then for the given string 
is
1 2 3... nx x x x  and 
a given  we let  be the smallest integer for 
which 
k 1r ?
1 2...r r r kx x x x+ +  is a contiguous substring in 
at least one of the strings .  Now let is N?  be the 
set of all i  for which 1 2...r r r kx x x x+ +  is a substring 
of  and let  be the set of all  for which is N i
1 2... 1r r r kx x x x+ + ?  is a substring of . We set is
( )1 2 3 1| ... , ii Nk k
ii N
p
p x x x x x C
p
??
?
?
= ?? . (3) 
In some cases it is appropriate to assume that 
( )p C  is proportional to 1M ii p=?  or there may be 
other ways to make this estimate. This basic 
scheme works well, but we have found that we can 
obtain a modest improvement by adding a unique 
start character to the beginning of each string. This 
character is assumed to occur nowhere else but as 
the first character in all strings dealt with including 
any string whose probability we are estimating.  
This forces the estimates of probabilities near the 
beginnings of strings to come from estimates based 
on the beginnings of strings.  We use this approach 
in all of our classification algorithms. 
 
Table 1. Each fragment in the left column appears in the 
training data and the probability in the right column 
represents the probability of seeing the underlined por-
tion of the string given the occurrence of the initial un-
underlined portion of the string in a training string.  
GP 
!apoe 79.55 10??  
oe-e 32.09 10??  
e-epsilon 24.00 10??  
( )|p apoe epsilon GP?  117.98 10??  
( )|p GP apoe epsilon?  0.98448 
NGP 
!apoe 88.88 10??  
poe- 21.21 10??  
oe-e 26.10 10??  
e-epsilon 36.49 10??  
( )|p apoe epsilon NGP?  134.25 10??  
( )|p NGP apoe epsilon?  0.01552 
In Table 1, we give an illustrative example of 
the string apoe-epsilon which does not appear in 
the training data.  A PubMed search for apoe-
epsilon gene returns 269 hits showing the name is 
known. But it does not appear in this exact form in 
SemCat. 
3.3   Language Model with Witten-Bell Smooth-
ing 
A statistical n-gram model is challenged when a 
bigram in the test set is absent from the training 
set, an unavoidable situation in natural language 
due to Zipf?s law.  Therefore, some method for 
assigning nonzero probability to novel n-grams is 
required.  For our language model (LM), we used 
Witten-Bell smoothing, which reserves probability 
mass for out of vocabulary values (Witten and 
Bell, 1991, Chen and Goodman, 1998).  The dis-
counted probability is calculated as 
 
   
)...()...(#
)...(#
)...(?
1111
1
11
?+??+?
+?
?+? += iniini
ini
ini wwDww
ww
wwP    (4)   
 
36
where  is the number of distinct 
words that can appear after in the 
training data. Actual values assigned to tokens out-
side the training data are not assigned uniformly 
but are filled in using a variable order Markov 
Model based on the strings seen in the training 
data.  
)...( 11 ?+? ini wwD
11... ?+? ini ww
3.4   Probabilistic Context-Free Grammar 
The Probabilistic Context-Free Grammar 
(PCFG) or Stochastic Context-Free Grammar 
(SCFG) was originally formulated by Booth 
(1969).  For technical details we refer the reader to 
Charniak (1993). For gene and protein name classi-
fication, we tried two different approaches.  In the 
first PCFG method (PCFG-3), we used the follow-
ing simple productions: 
 
1) CATP ? CATP CATP 
2) CATP ? CATP postCATP 
3) CATP ? preCATP CATP 
 
CATP refers to the category of the phrase, GP 
or NGP.  The prefixes pre and post refer to begin-
nings and endings of the respective strings.  We 
trained two separate grammars, one for the positive 
examples, GP, and one for the negative examples, 
NGP.  Test cases were tagged based on their score 
from each of the two grammars. 
In the second PCFG method (PCFG-8), we 
combined the positive and negative training exam-
ples into one grammar.  The minimum number of 
non-terminals necessary to cover the training sets 
gp1-3 was six {CATP, preCATP, postCATP, Not-
CATP, preNotCATP, postNotCATP}. CATP 
represents a string from GP, and NotCATP repre-
sents a string from NGP.  We used the following 
production rules: 
 
1) CATP ? CATP CATP 
2) CATP ? CATP postCATP 
3) CATP ? preCATP CATP 
4) CATP ? NotCATP CATP 
5) NotCATP ? NotCATP NotCATP 
6) NotCATP ? NotCATP postNotCATP 
7) NotCATP? preNotCATP NotCATP 
8) NotCATP ? CATP NotCATP 
 
It can be seen that (4) is necessary for strings like 
?human p53,? and (8) covers strings like ?p53 
pathway.? 
     In order to deal with tokens that do not ap-
pear in the training data we use variable order 
Markov Models for strings. First the grammar is 
trained on the training set of names. Then any to-
ken appearing in the training data will have as-
signed to it the tags appearing on the right side of 
any rule of the grammar (essentially part-of-speech 
tags) with probabilities that are a product of the 
training.  We then construct a variable order 
Markov Model for each tag type based on the to-
kens in the training data and the assigned prob-
abilities for that tag type. These Models (three for 
PCFG-3 and six for PCFG-8) are then used to as-
sign the basic tags of the grammar to any token not 
seen in training.  In this way the grammars can be 
used to classify any name even if its tokens are not 
in the training data.  
3.5   Priority Model 
There are problems with the previous ap-
proaches when applied to names. For example, 
suppose one is dealing with the name ?human liver 
alkaline phosphatase? and class  represents pro-
tein names and class  anatomical names. In that 
case a language model is no more likely to favor 
 than . We have experimented with PCFGs 
and have found the biggest challenge to be how to 
choose the grammar. After a number of attempts 
we have still found problems of the ?human liver 
alkaline phosphatase? type to persist. 
1C
2C
1C 2C
The difficulties we have experienced with lan-
guage models and PCFGs have led us to try a dif-
ferent approach to model named entities. As a 
general rule in a phrase representing a named en-
tity a word to the right is more likely to be the head 
word or the word determining the nature of the 
entity than a word to the left. We follow this rule 
and construct a model which we will call a Priority 
Model. Let  be the set of training data (names) 
for class  and likewise  for . Let 
1T
1C 2T 2C { } At? ??  
denote the set of all tokens used in names con-
tained in . Then for each token 1T T? 2 ,  t A? ? ? , 
we assume there are associated two probabilities 
p?  and q?  with the interpretation that p?  is the 
37
probability that the appearance of the token t?  in a 
name indicates that name belongs to class  and 1C
q?  is the probability that t?  is a reliable indicator 
of the class of a name. Let  be 
composed of the tokens on the right in the given 
order. Then we compute the probability 
( ) ( ) ( )1 2 kn t t t? ? ?= ?
 
( ) ( ) ( )( ) ( ) ( ) ( )( )1 1 22 1 .| 1 1k kj i iijp C n p q q p q? ? ?? == == ? + ??? ?k jj i ?+
      (5)                                         
 
This formula comes from a straightforward in-
terpretation of priority in which we start on the 
right side of a name and compute the probability 
the name belongs to class  stepwise. If  is 
the rightmost token we multiple the reliability 
 times the significance 
1C ( )kt?
( )kq? ( )kp?  to obtain 
, which represents the contribution of 
. The remaining or unused probability is 
 and this is passed to the next token to the 
left, . The probability  is scaled by 
the reliability and then the significance of 
( ) ( )k kq p? ?
( )kt?
( )1 kq??
( )1kt? ? ( )1 kq??
( )1kt? ?  to 
obtain , which is the contri-
bution of  toward the probability that the 
name is of class . The remaining probability is 
now  and this is again 
passed to the next token to the left, etc. At the last 
token on the left the reliability is not used to scale 
because there are no further tokens to the left and 
only significance 
( ) ( ) ( )1(1 )k k kq q p? ? ??? 1?
)k?
( )1kt? ?
1C
( )( ) ( )(11 1kq q? ?? ?
( )1p?  is used.  
We want to choose all the parameters p?  and 
q?  to maximize the probability of the data. Thus 
we seek to maximize 
 
 ( )( ) ( )( )
1 2
1log | log 2 |n T n TF p C n p C n? ?= +? ? .
 (6) 
                 
Because probabilities are restricted to be in the 
interval [ ]0,1 , it is convenient to make a change of 
variables through the definitions 
,  
1 1
x y
x
ep q
e e
? ?
y
e
? ?? ?= =+ + . (7) 
Then it is a simple exercise to show that 
( ) (1 ,  1dp dqp p q q
dx dy
? ? )? ? ?
? ?
= ? = ? ? . (8) 
From (5), (6), and (8) it is straightforward to com-
pute the gradient of  as a function of F x?  and y?  
and because of (8) it is most naturally expressed in 
terms of p?  and q? . Before we carry out the op-
timization one further step is important. Let B  
denote the subset of A? ?  for which all the oc-
currences of t?  either occur in names in  or all 
occurrences occur in names in . For any such 
1T
2T ?  
we set 1q? =  and if all occurrences of t?  are in 
names in   we set 1T 1p? = , while if all occur-
rences are in names in  we set .  These 
choices are optimal and because of the form of (8) 
it is easily seen that 
2T 0p? =
0F F
x y? ?
? ?= =? ?  (9) 
for such an ? . Thus we may ignore all the B? ?  
in our optimization process because the values of 
p?  and q?  are already set optimally. We therefore 
carry out optimization of  using the F
,  ,  x y A? ? B? ? ? . For the optimization we have 
had good success using a Limited Memory BFGS 
method (Nash et al, 1991). 
 
When the optimization of  is complete we 
will have estimates for all the 
F
p?  and q? , A? ? . 
We still must deal with tokens t?  that are not in-
cluded among the t? .  For this purpose we train 
variable order Markov Models 1MP  based on the 
weighted set of strings ( ){ }, At p? ? ??  and 2MP  
based on ( ){ },1 At p? ? ??? . Likewise we train 
1MQ  based on ( ){ }, At q? ? ??  and 2MQ  based on 
( ){ },1 At q? ? ??? . Then if we allow ( )imp t?  to 
represent the prediction from model iMP  and ( )imq t?  that from model iMQ , we set 
38
 ( )
( ) ( )
( )
( ) ( )11 2 1 2,  
mp t mq t
p q
mp t mp t mq t mq t
?
? ?
? ? ?
= =+
1 ?
?+
 (10) 
This allows us to apply the priority model to 
any name to predict its classification based on 
equation 5.  
4 Results 
We ran all three methods on the SemCat sets gp1, 
gp2 and gp3.  Results are shown in Table 2.  For 
evaluation we applied the standard information 
retrieval measures precision, recall and F-measure.   
_
( _ _ )
rel retprecision
rel ret non rel ret
= + ?  
_
( _ _ _ )
rel retrecall
rel ret rel not ret
= +  
 
2* *
( )
precision recallF measure
precision recall
? = +  
 
For name classification, rel_ret refers to true posi-
tive entities, non-rel_ret to false positive entities 
and rel_ not_ret to false negative entities. 
 
Table 2. Three-fold cross validation results. P = Preci-
sion, R = Recall, F = F-measure. PCFG = Probabilistic 
Context-Free Grammar, LM = Bigram Model with Wit-
ten-Bell smoothing,  PM = Priority Model. 
Method Run P R F 
PCFG-3 gp1 0.883 0.934 0.908 
 gp2 0.882 0.937 0.909 
 gp3 0.877 0.936 0.906 
PCFG-8 gp1 0.939 0.966 0.952 
 gp2 0.938 0.967 0.952 
 gp3 0.939 0.966 0.952 
LM gp1 0.920 0.968 0.944 
 gp2 0.923 0.968 0.945 
 gp3 0.917 0.971 0.943 
PM gp1 0.949 0.968 0.958 
 gp2 0.950 0.968 0.960 
 gp3 0.950 0.967 0.958 
5 Discussion 
Using a variable order Markov model for strings 
improved the results for all methods (results not 
shown).  The gp1-3 results are similar within each 
method, yet it is clear that the overall performance 
of these methods is PM > PCFG-8 > LM > PCFG-
3. The very large size of the database and the very 
uniform results obtained over the three independ-
ent random splits of the data support this conclu-
sion.  
The improvement of PCFG-8 over PCFG-3 can 
be attributed to the considerable ambiguity in this 
domain. Since there are many cases of term over-
lap in the training data, a grammar incorporating 
some of this ambiguity should outperform one that 
does not. In PCFG-8, additional production rules 
allow phrases beginning as CATPs to be overall 
NotCATPs, and vice versa.   
The Priority Model outperformed all other meth-
ods using F-measure.  This supports our impres-
sion that the right-most words in a name should be 
given higher priority when classifying names.  A 
decrease in performance for the model is expected 
when applying this model to the named entity ex-
traction (NER) task, since the model is based on 
terminology alone and not on the surrounding 
natural language text.  In our classification experi-
ments, there is no context, so disambiguation is not 
an issue. However, the application of our model to 
NER will require addressing this problem. 
 SemCat has not been tested for accuracy, but 
we retain a set of manually-assigned scores that 
attest to the reliability of each contributing list of 
terms.  Table 2 indicates that good results can be 
obtained even with noisy training data.   
6 Conclusion 
In this paper, we have concentrated on the infor-
mation inherent in gene and protein names versus 
other biomedical entities.  We have demonstrated 
the utility of the SemCat database in training prob-
abilistic methods for gene and protein entity classi-
fication.  We have also introduced a new model for 
named entity prediction that prioritizes the contri-
bution of words towards the right end of terms. 
The Priority Model shows promise in the domain 
of gene and protein name classification.  We plan 
to apply the Priority Model, along with appropriate 
contextual and meta-level information, to gene and 
protein named entity recognition in future work.  
We intend to make SemCat freely available. 
 
39
Acknowledgements 
This research was supported in part by the Intra-
mural Research Program of the NIH, National Li-
brary of Medicine. 
References  
T. L. Booth.  1969.  Probabilistic representation of for-
mal languages.  In:  IEEE Conference Record of the 
1969 Tenth Annual Symposium on Switching and 
Automata Theory, 74-81. 
 
Stanley F. Chen and Joshua T. Goodman. 1998.  An 
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University. 
 
Eugene Charniak.  1993.  Statistical Language Learn-
ing.  The MIT Press,  Cambridge, Massachusetts. 
 
K. Bretonnel Cohen, Lynne Fox, Philip V. Ogren and 
Lawrence Hunter. 2005. Corpus design for biomedi-
cal natural language processing. Proceedings of the 
ACL-ISMB Workshop on Linking Biological Litera-
ture, Ontologies and Databases, 38-45. 
 
The Gene Ontology Consortium.  2000. Gene Ontology: 
tool for the unification of biology, Nat Genet. 25: 25-
29. 
Henk Harkema, Robert Gaizauskas, Mark Hepple, An-
gus Roberts, Ian Roberts, Neil Davis and Yikun Guo.  
2004.  A large scale terminology resource for bio-
medical text processing.  Proc BioLINK 2004, 53-60. 
Vasileios Hatzivassiloglou, Pablo A. Dubou? and An-
drey Rzhetsky.  2001.  Disambiguating proteins, 
genes, and RNA in text: a machine learning ap-
proach.  Bioinformatics 17 Suppl 1:S97-106. 
J.-D. Kim, Tomoko Ohta, Yuka Tateisi and Jun-ichi 
Tsujii. 2003. GENIA corpus--semantically annotated 
corpus for bio-textmining. Bioinformatics 19 Suppl 
1:i180-2.  
 
Donald A. Lindberg, Betsy L. Humphreys and Alexa T. 
McCray. 1993. The Unified Medical Language Sys-
tem. Methods Inf Med 32(4):281-91. 
 
Hongfang Liu, Stephen B. Johnson, and Carol Fried-
man.  2002.  Automatic resolution of ambiguous terms 
based on machine learning and conceptual relations in 
the UMLS.  J Am Med Inform Assoc 9(6): 621?636. 
 
Donna Maglott, Jim Ostell, Kim D. Pruitt and Tatiana 
Tatusova. 2005. Entrez Gene: gene-centered informa-
tion at NCBI. Nucleic Acids Res. 33:D54-8.   
 
Alexa T. McCray. 1989. The UMLS semantic network. 
In: Kingsland LC (ed). Proc 13rd Annu Symp Com-
put Appl Med Care. Washington, DC: IEEE Com-
puter Society Press, 503-7. 
 
Ryan McDonald and Fernando Pereira.  2005.  Identify-
ing gene and protein mentions in text using condi-
tional random fields.  BMC Bioinformatics 6 Supp 
1:S6. 
 
S. Nash and J. Nocedal. 1991. A numerical study of the 
limited memory BFGS method and the truncated-
Newton method for large scale optimization, SIAM J. 
Optimization1(3): 358-372. 
 
Goran Nenadic, Irena Spasic and Sophia Ananiadou.  
2003.  Terminology-driven mining of biomedical lit-
erature.  Bioinformatics 19:8, 938-943. 
 
Raf M. Podowski, John G. Cleary, Nicholas T. Gon-
charoff, Gregory Amoutzias and William S. Hayes.  
2004.  AZuRE, a scalable system for automated term 
disambiguation of gene and protein Names IEEE 
Computer Society Bioinformatics Conference, 415-
424. 
 
Lawrence H. Smith, Lorraine Tanabe, Thomas C. Rind-
flesch and W. John Wilbur. 2005. MedTag: A collec-
tion of biomedical annotations. Proceedings of the 
ACL-ISMB Workshop on Linking Biological Litera-
ture, Ontologies and Databases, 32-37.  
 
Lorraine Tanabe, Natalie Xie, Lynne H. Thom, Wayne 
Matten and W. John Wilbur. 2005. GENETAG: a 
tagged corpus for gene/protein named entity recogni-
tion. BMC Bioinformatics 6 Suppl 1:S3.  
 
I. Witten and T. Bell, 1991. The zero-frequency prob-
lem:  Estimating the probabilities of novel events in 
adaptive text compression. IEEE Transactions on In-
formation Theory 37(4).  
 
Alexander Yeh, Alexander Morgan, Mark Colosimo and 
Lynette Hirschman. 2005. BioCreAtIvE Task 1A: 
gene mention finding evaluation. BMC Bioinformat-
ics 6 Suppl 1:S2.  
 
Hong Yu, Carol Friedman, Andrey Rhzetsky and 
Pauline Kra. 1999. Representing genomic knowledge 
in the UMLS semantic network. Proc AMIA Symp. 
181-5. 
40
BioNLP 2007: Biological, translational, and clinical language processing, pages 201?208,
Prague, June 2007. c?2007 Association for Computational Linguistics
Unsupervised Learning of the Morpho-Semantic Relationship in 
MEDLINE? 
W. John Wilbur 
National Center for Biotechnology 
Information / National Library of 
Medicine, National Institutes of 
Health, Bethesda, MD, U.S.A. 
wilbur@ncbi.nlm.nih.gov 
 
 
Abstract 
Morphological analysis as applied to Eng-
lish has generally involved the study of 
rules for inflections and derivations. Recent 
work has attempted to derive such rules 
from automatic analysis of corpora. Here 
we study similar issues, but in the context 
of the biological literature. We introduce a 
new approach which allows us to assign 
probabilities of the semantic relatedness of 
pairs of tokens that occur in text in conse-
quence of their relatedness as character 
strings. Our analysis is based on over 84 
million sentences that compose the MED-
LINE database and over 2.3 million token 
types that occur in MEDLINE and enables 
us to identify over 36 million token type 
pairs which have assigned probabilities of 
semantic relatedness of at least 0.7 based 
on their similarity as strings. 
1 Introduction 
Morphological analysis is an important element 
in natural language processing. Jurafsky and 
Martin (2000) define morphology as the study of 
the way words are built up from smaller meaning 
bearing units, called morphemes.  Robust tools 
for morphological analysis enable one to predict 
the root of a word and its syntactic class or part 
of speech in a sentence. A good deal of work has 
been done toward the automatic acquisition of 
rules, morphemes, and analyses of words from 
large corpora (Freitag, 2005; Jacquemin, 1997; 
Monson, 2004; Schone and Jurafsky, 2000; 
Wicentowski, 2004; Xu and Croft, 1998; 
Yarowsky and Wicentowski, 2000). While this 
work is important it is mostly concerned with 
inflectional and derivational rules that can be 
derived from the study of texts in a language. 
While our interest is related to this work, we are 
concerned with the multitude of tokens that ap-
pear in English texts on the subject of biology.  
We believe it is clear to anyone who has exam-
ined the literature on biology that there are many 
tokens that appear in textual material that are 
related to each other, but not in any standard way 
or by any simple rules that have general applica-
bility even in biology. It is our goal here to 
achieve some understanding of when two tokens 
can be said to be semantically related based on 
their similarity as strings of characters.  
Thus for us morphological relationship will be a 
bit more general in that we wish to infer the re-
latedness of two strings based on the fact that 
they have a certain substring of characters on 
which they match. But we do not require to say 
exactly on what part of the matching substring 
their semantic relationship depends. In other 
words we do not insist on the identification of 
the smaller meaning bearing units or mor-
phemes. Key to our approach is the ability to 
measure the contextual similarity between two 
token types as well as their similarity as strings. 
Neither kind of measurement is unique to our 
application. Contextual similarity has been stud-
ied and applied in morphology (Jacquemin, 
1997; Schone and Jurafsky, 2000; Xu and Croft, 
1998; Yarowsky and Wicentowski, 2000) and 
more generally (Means and others, 2004).  String 
201
similarity has also received much attention 
(Adamson and Boreham, 1974; Alberga, 1967; 
Damashek, 1995; Findler and Leeuwen, 1979; 
Hall and Dowling, 1980; Wilbur and Kim, 2001; 
Willett, 1979; Zobel and Dart, 1995).  However, 
the way we use these two measurements is, to 
our knowledge, new. Our approach is based on a 
simple postulate: If two token types are similar 
as strings, but they are not semantically related 
because of their similarity, then their contextual 
similarity is no greater than would be expected 
for two randomly chosen token types. Based on 
this observation we carry out an analysis which 
allows us to assign a probability of relatedness to 
pairs of token types. This proves sufficient to 
generate a large repository of related token type 
pairs among which are the expected inflectional 
and derivationally related pairs and much more 
besides.  
2 Methodology 
We work with a set of 2,341,917 token types 
which are the unique token types that occurred 
throughout MEDLINE in the title and abstract re-
cord fields in November of 2006. These token 
types do not include a set of 313 token types that 
represent stop words and are removed from con-
sideration. Our analysis consists of several steps. 
2.1 Measuring Contextual Similarity 
In considering the context of a token in a MED-
LINE record we do not consider all the text of 
the record. In those cases when there are multi-
ple sentences in the record the text that does not 
occur in the same sentence as the token may be 
too distant to have any direct bearing on the in-
terpretation of the token and will in such cases 
add noise to our considerations. Thus we break 
the whole of MEDLINE into sentences and con-
sider the context of a token to be the additional 
tokens of the sentence in which it occurs. Like-
wise the context of a token type consists of all 
the additional token types that occur in all the 
sentences in which it occurs. We used our own 
software to identify sentence boundaries (unpub-
lished), but suspect that published and freely 
available methods could equally be used for this 
purpose. This produced 84,475,092 sentences 
over all of MEDLINE. While there is an advan-
tage in the specificity that comes from consider-
ing context at the sentence level, this approach 
also gives rise to a problem. It is not uncommon 
for two terms to be related semantically, but to 
never occur in the same sentence. This will hap-
pen, for example, if one term is a misspelling of 
the other or if the two terms are alternate names 
for the same object. Because of this we must es-
timate the context of each term without regard to 
the occurrence of the other term. Then the two 
estimates can be compared to compute a similar-
ity of context. This we accomplish using formu-
las of probability theory applied to our setting. 
Let T  denote the set of 2,341,917 token types 
we consider and let 1t  and 2t  be two token types 
we wish to compare. Then we define 
1 1
2 2
( ) ( | ) ( ) and 
( ) ( | ) ( ) 
c i T
c i T
p t p t i p i
p t p t i p i
?
?
=
=
?
? . (1) 
Here we refer to 1( )cp t  and 2( )cp t  as contextual 
probabilities for 1t  and 2t , respectively. The ex-
pressions on the right sides in (1) are given the 
standard interpretations. Thus ( )p i  is the frac-
tion of tokens in MEDLINE that are equal to i  
and 1( | )p t i  is the fraction of sentences in 
MEDLINE that contain i  that also contain 1t . 
We make a similar computation for the pair of 
token types 
1 2 1 2
1 2
( ) ( | ) ( )
( | ) ( | ) ( )
c i T
i T
p t t p t t i p i
p t i p t i p i
?
?
? = ?
=
?
? . (2) 
Here we have made use of an additional assump-
tion, that given i , 1t  and 2t  are independent in 
their probability  of occurrence. While inde-
pendence is not true, this seems to be just the 
right assumption for our purposes. It allows our 
estimate of 1 2( )cp t t?  to be nonzero even 
though 1t  and 2t  may never occur together in a 
sentence. In other words it allows our estimate to 
reflect what context would imply if there were 
no rule that says the same intended word will 
almost never occur twice in a single sentence, 
202
etc. Our contextual similarity is then the mutual 
information based on contextual probabilities 
1 2
1 2
1 2
( )
( , ) log
( ) ( )
c
c c
p t t
conSim t t
p t p t
? ??= ? ?? ?
 (3) 
There is one minor practical difficulty with this 
definition. There are many cases where 1 2( )cp t t?  
is zero. In any such case we define 1 2( , )conSim t t  
to be -1000. 
2.2 Measuring Lexical Similarity 
Here we treat the two token types, 1t  and 2t  of 
the previous section, as two ASCII strings and 
ask how similar they are as strings. String simi-
larity has been studied from a number of view-
points (Adamson and Boreham, 1974; Alberga, 
1967; Damashek, 1995; Findler and Leeuwen, 
1979; Hall and Dowling, 1980; Wilbur and Kim, 
2001; Willett, 1979; Zobel and Dart, 1995). We 
avoided approaches based on edit distance or 
other measures designed for spell checking  be-
cause our problem requires the recognition of 
relationships more distant than simple misspell-
ings. Our method is based on letter ngrams as 
features to represent any string (Adamson and 
Boreham, 1974; Damashek, 1995; Wilbur and 
Kim, 2001; Willett, 1979). If " "t abcdefgh=  
represents a token type, then we define ( )F t  to 
be the feature set associated with t  and we take 
( )F t  to be composed of i) all the contiguous 
three character substrings  ?abc?, ?bcd?, ?cde?, 
?def?, ?efg?, ?fgh?; ii) the specially marked first 
trigram " !"abc ; and iii) the specially marked 
first letter " #"a .  This is the form of ( )F t  for 
any t  at least three characters long. If t  consists 
of only two characters, say " "ab , we take i) 
" "ab ; ii) " !"ab ; and iii) is unchanged. If t  con-
sists of only a single character " "a , we likewise 
take i) ?a?; ii) ?a!?; and iii) is again unchanged. 
Here ii) and iii) are included to allow the empha-
sis of the beginning of strings as more important 
for their recognition than the remainder. We em-
phasize that ( )F t  is a set of features, not a ?bag-
of-words?, and any duplication of features is ig-
nored. While this is a simplification, it does have 
the minor drawback that different strings, e.g., 
" "aaab  and " "aaaaab , can be represented by 
the same set of features.  
Given that each string is represented by a set of 
features, it remains to define how we compute 
the similarity between two such representations. 
Our basic assumption here is that the probability 
2 1( | )p t t , that the semantic implications of 1t  are 
also represented at some level in 2t , should be 
represented by the fraction of the features repre-
senting 1t  that also appear in 2t . Of course there 
is no reason that all features should be consid-
ered of equal value. Let F  denote the set of all 
features coming from all 2.34 million strings we 
are considering. We will make the assumption 
that there exists a set of weights ( )w f  defined 
over all of f F?  and representing their seman-
tic importance. Then we have 
( ) ( )1 2 12 1 ( )( | ) ( ) / ( )f F t F t f F tp t t w f w f? ? ?=? ? . (4) 
Based on (4) we define the lexical similarity of 
two token types as 
1 2 2 1 1 2( , ) ( ( | ) ( | )) / 2lexSim t t p t t p t t= +  (5) 
In our initial application of lexSim we take as 
weights the so-called inverse document fre-
quency weights that are commonly used in in-
formation retrieval (Sparck Jones, 1972). If 
2,341,917, N =  the number of token types, and 
for any feature f , fn  represents the number of 
token types with the feature f , the inverse 
document frequency weight is 
( ) log
f
N
w f
n
? ?= ? ?? ?? ?
. (6) 
This weight is based on the observation that very 
frequent features tend not to be very important, but 
importance increases on the average as frequency 
decreases. 
2.3 Estimating Semantic Relatedness 
The first step is to compute the distribution of 
1 2( , )conSim t t  over a large random sample of 
pairs of token types 1t  and 2t .  For this purpose 
we computed 1 2( , )conSim t t  over a random 
203
sample of 302,515 pairs. This resulted in the 
value -1000, 180,845 times (60% of values).  
The remainder of the values, based on nonzero 
1 2( )cp t t?  are distributed as shown in Figure 1. 
Let ?  denote the probability density for 
1 2( , )conSim t t  over random pairs 1t  and 2t . Let 
1 2( , )Sem t t  denote the predicate that asserts that 1t  
and 2t  are semantically related. Then our main 
assumption which underlies the method is  
Postulate. For any nonnegative real number r  
{ }1 2 1 2 1 2( , ) | ( , ) ( , )Q conSim t t lexSim t t r Sem t t= > ? ? (7) 
-3 -1 1 3 5 7
conSim
0
2000
4000
6000
8000
10000
Fr
eq
ue
nc
y
Distribution of conSim Values
 
Figure 1.  Distribution of  conSim values for the 
40% of randomly selected token type pairs 
which gave values above -1000, i.e., for which 
1 2( ) 0cp t t? > .  
has probability density function equal to ? . 
 
This postulate says that if you have two token 
types that have some level of similarity as strings 
( 1 2( , )lexSim t t r> ) but which are not semantically 
related, then 1 2( , )lexSim t t r>   is just an accident 
and it provides no information about 
1 2( , )conSim t t .  
 
The next step is to consider a pair of real numbers 
1 20 r r? <  and the set 
{ }1 2 1 2 1 1 2 2( , ) ( , ) | ( , )S r r t t r lexSim t t r= ? <  (8) 
they define.  We will refer to such a set as a lexSim 
slice. According to our postulate the subset of 
1 2( , )S r r  which are pairs of tokens without a se-
mantic relationship will produce conSim values 
obeying the ?  density. We compute the conSim 
values and assume that all of those pairs that pro-
duce a conSim value of -1000 represent pairs that 
are unrelated semantically. As an example, in one 
of our computations we computed a slice 
(0.7,0.725)S  and found the lexSim value -1000 
produced 931,042 times. In comparing this with 
the random sample which produced 180,845 values 
of -1000, we see that  
931,042 180,845 5.148=  (9) 
So we need to multiply the frequency distribution 
for the random sample (shown in Figure 1) by 
5.148 to represent the part of the slice 
(0.7,0.725)S  that represents pairs not semantically 
related. This situation is illustrated in Figure 2.  
Two observations are important here. First, the two 
curves match almost perfectly along their left 
edges for conSim values below zero. This suggests 
that sematically related pairs do not produce con-
Sim scores below about -1 and adds some credibil-
ity to our assumption that semantically related 
pairs do not produce conSim values of -1000.  The 
second observation is that while the higher graph 
in Figure 2 represents all pairs in the lexSim slice 
and the lower graph all pairs that are not semanti-
cally related, we do not know which pairs are not 
semantically related. We can only estimate the 
probability of any pair at a particular conSim score 
level being semantically related. If we let ?  rep-
resent the upper curve coming from the lexSim 
slice and ?  the lower curve coming from the ran-
dom sample, then (10) represents the probability  
( ) ( )
( )
( )
x x
p x
x
? ??= ?  (10) 
that a token type pair with a conSim score of x  is a 
semantically related pair. Curve fitting or regres-
sion methods can be used to estimate p . Since it is 
reasonable to expect p  to be a nondecreasing 
function of its argument, we use isotonic regres-
sion to make our estimates. For a full analysis we 
set 
0.5 0.025ir i= + ?  (11) 
204
and consider the set of lexSim slices { }201 0( , )i i iS r r + =  
and determine the corresponding set of probability 
functions { }200i ip = .  
2.4 Learned Weights  
Our initial step was to use the IDF weights defined 
in equation (6) and compute a database of all non-
identical token type pairs among the 2,341,917 
token types occurring in MEDLINE for which 
1 2( , ) 0.5lexSim t t ? .  We focus on the value 0.5 be-
cause the similarity measure lexSim has the  
-4 -2 0 2 4 6 8 10
conSim
0
20000
40000
60000
80000
F
re
qu
en
cy
Random Sample x 5.148
lexSim Slice S(0.7,0.725)
Comparison of Histograms
Figure 2. The distribution based on the random sample 
of pairs represents those pairs in the slice that are not 
semantically related, while the portion between the two 
curves represents the number of semantically related 
pairs. 
property that if one of 1t  or 2t  is an initial seg-
ment of the other (e.g., ?glucuron? is an initial 
segment of ?glucuronidase?) then 
1 2( , ) 0.5lexSim t t ?  will be satisfied regardless of 
the set of weights used. The resulting data in-
cluded the lexSim and the conSim scores and 
consisted of 141,164,755 pairs. We performed a 
complete slice analysis of this data and based on 
the resulting probability estimates 20,681,478 
pairs among the 141,164,755 total had a prob-
ability of being semantically related which was 
greater than or equal to 0.7.  While this seems 
like a very useful result, there is reason to be-
lieve the IDF weights used to compute lexSim 
are far from optimal. In an attempt to improve 
the weighting we divided the 141,164,755 pairs 
into 1C?  consisting of 68,912,915 pairs with a 
conSim score of -1000 and 1C  consisting of the 
remaining 72,251,839 pairs. Letting wG  denote 
the vector of weights we defined a cost function 
( )
( )
1 2 1
1 2 1
1 2( , )
1 2( , )
( ) log ( , )
log 1 ( , )
t t C
t t C
w lexSim t t
lexSim t t
?
?
?
? = ?
+ ? ?
?
?
G
 (12) 
and carried out a minimization of ?  to obtain a 
set of learned weights which we will denote by 
0w
G . The minimization was done using the L-
BFGS algorithm (Nash and Nocedal, 1991). 
Since it is important to avoid negative weights 
we associate a potential ( )v f  with each ngram 
feature f  and set  
( ) exp( ( ))w f v f= . (13) 
The optimization is carried out using the poten-
tials.  
The optimization can be understood as an at-
tempt to make lexSim as close to zero as possible 
on the large set 1C?  where 1000conSim = ?  and 
we have assumed there are no semantically re-
lated pairs, while at the same time making lex-
Sim large on the remainder. While this seems 
reasonable as a first step it is not conservative as 
many pairs in 1C  will not be semantically re-
lated.  Because of this we would expect that 
there are ngrams for which we have learned 
weights that are not really appropriate outside of 
the set of 141,164,755 pairs on which we 
trained. If there are such, presumably the most 
important cases would be those where we would 
score pairs with inappropriately high lexSim 
scores. Our approach to correct for this possibil-
ity is to add to the initial database of 
141,164,755 pairs all additional pairs which pro-
duced a 1 2( , ) 0.5lexSim t t ?  based on the new 
weight set 0w
G . This augmented the data to a new 
set of 223,051,360 pairs with conSim scores. We 
then applied our learning scheme based on 
minimization of the function ?  to learn a new 
set of weights 1w
G . There was one difference. 
Here and in all subsequent rounds we chose to 
define 1C?  as all those pairs with 
205
1 2( , ) 0conSim t t ?  and 1C  those pairs with 
1 2( , ) 0conSim t t > . We take this to be a conserva-
tive approach as one would expect semantically 
related pairs to have a similar context and satisfy 
1 2( , ) 0conSim t t > and  graphs such as Figure 2 
support this. In any case we view this as a con-
servative move and calculated to produce fewer 
false positives based on lexSim score recommen-
dations of semantic relatedness.  We actually go 
through repeated rounds of training and adding 
new pairs to the set of pairs. This process is con-
vergent as we reach a point where the weights 
learned on the set of pairs does not result in the 
addition of a significant amount of new material. 
This happened with weight set 4w
G  and a total 
accumulation of 440.4 million token type pairs.  
Table 1. Number of token pairs and the level of 
their predicted probability of semantic related-
ness found with three different weight sets.  
Weight 
Set 
Prob. Se-
mantically 
Related 
0.7?  
Prob. Se-
mantically 
Related 
0.8?  
Prob. Se-
mantically 
Related 
0.9?  
4w
G
 36,173,520 22,381,318 10,805,085 
Constant 34,667,988 20,282,976 8,607,863 
IDF 31,617,441 18,769,424 8,516,329 
3 Probability Predictions 
Based on the learned weight set 4w
G  we per-
formed a slice analysis of the 440 million token 
pairs on which the weights were learned and ob-
tained a set of 36,173,520 token pairs with pre-
dicted probabilities of being semantically related 
of 0.7 or greater. We performed the same slice 
analysis on this 440 million token pair set with 
the IDF weights and the set of constant weights 
all equal to 1. The results are given in Table 1. 
Here it is interesting to note that the constant 
weights perform substantially better than the IDF 
weights and come close to the performance of 
the 4w
G  weights.  While the 4wG  predicted about 
1.5 million more relationships at the 0.7 prob-
ability level, it is also interesting to note that the 
difference between the 4w
G  and constant weights 
actually increases as one goes to higher probabil-
ity levels so that the learned weights allow us to  
Table 2. A table showing 30 out of a total of 379 
tokens predicted to be semantically related to 
?lacz? and the estimated probabilities. Ten en-
tries are from the beginning of the list, ten from 
the middle, and ten from the end. Breaks where 
data was omitted are marked with asterisks.  
Probability 
Semantic 
Relation Token 1  Token 2 
0.973028 lacz 'lacz 
0.975617 lacz 010cblacz 
0.963364 lacz 010cmvlacz 
0.935771 lacz 07lacz 
0.847727 lacz 110cmvlacz 
0.851617 lacz 1716lacz 
0.90737 lacz 1acz 
0.9774 lacz 1hsplacz 
0.762373 lacz 27lacz 
0.974001 lacz 2hsplacz 
*** *** *** 
0.95951 lacz laczalone 
0.95951 lacz laczalpha 
0.989079 lacz laczam 
0.920344 lacz laczam15 
0.903068 lacz laczamber 
0.911691 lacz laczatttn7 
0.975162 lacz laczbg 
0.953791 lacz laczbgi 
0.995333 lacz laczbla 
0.991714 lacz laczc141 
*** *** *** 
0.979416 lacz ul42lacz 
0.846753 lacz veroicp6lacz 
0.985656 lacz vglacz1 
0.987626 lacz vm5lacz 
0.856636 lacz vm5neolacz 
0.985475 lacz vtkgpedeltab8rlacz 
0.963028 lacz vttdeltab8rlacz 
0.993296 lacz wlacz 
0.990673 lacz xlacz 
0.946067 lacz zflacz 
predict over 2 million more relationships at the 
0.9 level of reliability. This is more than a 25% 
increase at this high reliability level and justifies 
the extra effort in learning the weights.  
206
Table 3. A table showing 30 out of a total of 96 
tokens predicted to be semantically related to 
?nociception? and the estimated probabilities. 
Ten entries are from the beginning of the list, 
ten from the middle, and ten from the end. 
Breaks where data was omitted are marked 
with asterisks. 
Probability 
Semantic 
Relation Token 1  Token 2 
0.727885 nociception actinociception 
0.90132 nociception actinociceptive 
0.848615 nociception anticociception 
0.89437 nociception anticociceptive 
0.880249 nociception antincociceptive 
0.82569 nociception antinoceiception 
0.923254 nociception antinociceptic 
0.953812 nociception antinociceptin 
0.920291 nociception antinociceptio 
0.824706 nociception antinociceptions 
*** *** *** 
0.802133 nociception nociceptice 
0.985352 nociception nociceptin 
0.940022 nociception nociceptin's 
0.930218 nociception nociceptine 
0.944004 nociception nociceptinerg 
0.882768 nociception nociceptinergic 
0.975783 nociception nociceptinnh2 
0.921745 nociception nociceptins 
0.927747 nociception nociceptiometric 
0.976135 nociception nociceptions 
*** *** *** 
0.88983 nociception subnociceptive 
0.814733 nociception thermoantinociception 
0.939505 nociception thermonociception 
0.862587 nociception thermonociceptive 
0.810878 nociception thermonociceptor 
0.947374 nociception thermonociceptors 
0.81756 nociception tyr14nociceptin 
0.981115 nociception visceronociception 
0.957359 nociception visceronociceptive 
0.862587 nociception withnociceptin 
A sample of the learned relationships based on 
the 4w
G  weights is contained in  
Table 2 and Table 3. The symbol ?lacz? stands 
for a well known and much studied gene in the 
E. coli bacterium. Due to its many uses it has 
given rise to myriad strings representing differ-
ent aspects of molecules, systems, or method-
ologies derived from or related to it. The results 
are not typical of the inflectional or derivational 
methods generally found useful in studying the 
morphology of English. Some might represent 
misspellings, but this is not readily apparent by 
examining them.  On the other hand ?nocicep-
tion? is an English word found in a dictionary 
and meaning ?a measurable physiological event 
of a type usually associated with pain and agony 
and suffering? (Wikepedia). The data in Table 3 
shows that ?nociception? is related to the 
expected inflectional and derivational forms, 
forms with affixes unique to biology, readily 
apparent misspellings, and foreign analogs. 
4 Discussion & Conclusions 
There are several possible uses for the type of 
data produced by our analysis. Words semanti-
cally related to a query term or terms typed by a 
search engine user can provide a useful query 
expansion in either an automatic mode or with 
the user selecting from a displayed list of options 
for query expansion. Many misspellings occur in 
the literature and are disambiguated in the token 
pairs produced by the analysis. They can be rec-
ognized as closely related low frequency-high 
frequency pairs. They may allow better curation 
of the literature on the one hand or improved 
spelling correction of user queries on the other. 
In the area of more typical language analysis, a 
large repository of semantically related pairs can 
contribute to semantic tagging of text and ulti-
mately to better performance on the semantic 
aspects of parsing. Also the material we have 
produced can serve as a rich source of morpho-
logical information. For example, inflectional 
and derivational transformations applicable to 
the technical language of biology are well repre-
sented in the data.   
There is the possibility of improving on the 
methods we have used, while still applying the 
general approach. Either a more sensitive con-
Sim or lexSim measure or both could lead to su-
perior results. While it is unclear to us how con-
Sim might be improved, it seems there is more 
potential with lexSim. lexSim treats features as 
basically independent contributors to the similar-
ity of token types and this is not ideal. For ex-
ample the feature ?hiv? usually refers to the hu-
207
man immunodeficiency virus. However, if ?ive? 
is also a feature of the token we may well be 
dealing with the word ?hive? which has no rela-
tion to a human immunodeficiency virus. Thus a 
more complicated model of the lexical similarity 
of strings could result in improved recognition of 
semantically related strings.  
In future work we hope to investigate the applica-
tion of the approach we have developed to multi-
token terms. We also hope to investigate the possi-
bility of more sensitive lexSim measures for im-
proved performance. 
Acknowledgment This research was supported by 
the Intramural Research Program of the National Center 
for Biotechnology Information, National Library of 
Medicine, NIH, Bethesda, MD, USA. 
References 
Adamson, G. W., and Boreham, J. 1974. The use of an 
association measure based on character structure to 
identify semantically related pairs of words and 
document titles. Information Storage and Retrieval, 
10: 253-260. 
Alberga, C. N. 1967. String similarity and misspellings. 
Communications of the ACM, 10: 302-313. 
Damashek, M. 1995. Gauging similarity with n-grams: 
Language-independent categorization of text. Sci-
ence, 267: 843-848. 
Findler, N. V., and Leeuwen, J. v. 1979. A family of 
similarity measures between two strings. IEEE 
Transactions on Pattern Analysis and Machine Intel-
ligence, PAMI-1: 116-119. 
Freitag, D. 2005. Morphology Induction From Term 
Clusters, 9th Conference on Computational Natural 
Language Learning (CoNLL): Ann Arbor, Michigan, 
Association for Computational Linguistics. 
Hall, P. A., and Dowling, G. R. 1980. Approximate 
string matching. Computing Surveys, 12: 381-402. 
Jacquemin, C. 1997. Guessing morphology from terms 
and corpora, in Belkin, N. J., Narasimhalu, A. D., 
and Willett, P., editors, 20th Annual International 
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval: Philadelphia, PA, 
ACM Press, p. 156-165. 
Jurafsky, D., and Martin, J. H. 2000. Speech and Lan-
guage Processing: Upper Saddle River, New Jersey, 
Prentice Hall. 
Means, R. W., Nemat-Nasser, S. C., Fan, A. T., and 
Hecht-Nielsen, R. 2004. A Powerful and General 
Approach to Context Exploitation in Natural Lan-
guage Processing, HLT-NAACL 2004: Workshop on 
Computational Lexical Semantics Boston, Massachu-
setts, USA, Association for Computational Linguis-
tics. 
Monson, C. 2004. A framework for unsupervised natu-
ral language morphology induction, Proceedings of 
the ACL 2004 on Student research workshop: Barce-
lona, Spain, Association for Computational Linguis-
tics. 
Nash, S. G., and Nocedal, J. 1991. A numerical study of 
hte limited memory BFGS method and hte truncated-
Newton method for large scale optimization. SIAM 
Journal of Optimization, 1: 358-372. 
Schone, P., and Jurafsky, D. 2000. Knowledge-free in-
duction of morphology using latent semantic analy-
sis, Proceedings of the 2nd workshop on Learning 
language in logic and the 4th conference on Compu-
tational natural language learning - Volume 7: Lis-
bon, Portugal, Association for Computational Lin-
guistics. 
Sparck Jones, K. 1972. A statistical interpretation of 
term specificity and its application in retrieval. The 
Journal of Documentation, 28: 11-21. 
Wicentowski, R. 2004. Multilingual Noise-Robust Su-
pervised Morphological Analysis using the Word-
Frame Model, SIGPHON: Barcelona, Spain, Asso-
ciation for Computational Linguistics. 
Wilbur, W. J., and Kim, W. 2001. Flexible phrase based 
query handling algorithms, in Aversa, E., and Man-
ley, C., editors, Proceedings of the ASIST 2001 An-
nual Meeting: Washington, D.C., Information Today, 
Inc., p. 438-449. 
Willett, P. 1979. Document retrieval experiments using 
indexing vocabularies of varying size. II. Hashing, 
truncation, digram and trigram encoding of index 
terms. Journal of Documentation, 35: 296-305. 
Xu, J., and Croft, W. B. 1998. Corpus-based stemming 
using cooccurrence of word variants. ACM TOIS, 
16: 61-81. 
Yarowsky, D., and Wicentowski, R. 2000. Minimally 
supervised morphological analysis by multimodal 
alignment, Proceedings of the 38th Annual Meeting 
on Association for Computational Linguistics: Hong 
Kong, Association for Computational Linguistics. 
Zobel, J., and Dart, P. 1995. Finding approximate 
matches in large lexicons. Software-Practice and Ex-
perience, 25: 331-345. 
 
208
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 103?104,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Automatic extraction of data deposition sentences:  
where do the research results go? 
Aur?lie N?v?ol, W. John Wilbur, Zhiyong Lu 
National Center for Biotechnology Information 
U.S. National Library of Medicine 
Bethesda, MD 20894, USA 
{Aurelie.Neveol,John.Wilbur,zhiyong.lu}@nih.gov 
  
Abstract 
Research in the biomedical domain can have a 
major impact through open sharing of data 
produced. In this study, we use machine learn-
ing for the automatic identification of data 
deposition sentences in research articles. Arti-
cles containing deposition sentences are cor-
rectly identified with 73% f-measure. These 
results show the potential impact of our meth-
od for literature curation.  
1 Background 
Research in the biomedical domain aims at further-
ing the knowledge of biological processes and im-
proving human health. Major contributions 
towards this goal can be achieved by sharing the 
results of research efforts with the community, in-
cluding datasets produced in the course of the re-
search work. While such sharing behavior is 
encouraged by funding agencies and scientific 
journals, recent work has shown that the ratio of 
data sharing is still modest compared to actual data 
production. For instance, Ochsner et al (2008) 
found the deposition rate of microarray data to be 
less than 50% for work published in 2007.  
Information about the declaration of data depo-
sition in research papers can be used both for data 
curation and for the analysis of emerging research 
trends. Our long-term research interest is in as-
sessing the value of deposition sentences for pre-
dicting future trends of data production. The initial 
step of automatically identifying deposition sen-
tences would then lead to an assessment of the 
need for storage space of incoming data in public 
repositories. 
2 Objective 
In this study, we aim at automatically perform-
ing a fine-grained identification of biological data 
deposition sentences in biomedical text. That is, 
we aim at identifying articles containing deposition 
sentences, extracting the specific sentences and 
characterizing the information contained in the 
sentences in terms of data type and deposition lo-
cation (e.g. database, accession numbers).  
3 Material and Methods 
Data deposition sentences . A collection of sen-
tences reporting the deposition of biological data 
(such as microarray data, protein structure, gene 
sequences) in public repositories was compiled 
based on previous work that we extended. We take 
these sentences as a primary method of identifying 
articles reporting on research that produced the 
kind of data deposited in public repositories. (1) 
and (2) show examples of such sentences. In con-
trast, (3) and (4) contain elements related to data 
deposition while focusing on other topics.   
(1) The sequences reported in this paper have been 
deposited in the GenBank database (acces sion 
numbers AF034483 for susceptible strain RC688s 
and AF034484 for resistant strain HD198r). 
(2) The microarray data were submitted to MIAMEx-
press at the EMBL-EBI. 
(3) Histone TAG Arrays are a repurposing of a micro-
array design originally created to represent the 
TAG sequences in the Yeast Knockout collection 
(Yuan et al2005 NCBI GEO Accession Number 
GPL1444). 
(4) The primary sequence of native Acinetobacter 
CMO is identical to the gene sequence for chnB 
deposited under accession number AB006902. 
103
Sentence classification. A Support Vector Ma-
chine (SVM) classifier was built using a corpus of 
583 positive data deposition sentences and 578 
other negative sentences. Several sets of features 
were tested, including the following: sentence to-
kens, associated part-of-speech tags obtained using 
MEDPOST1, relative position of the sentence in 
the article, identification of elements related to data 
deposition (data, deposition action, database, ac-
cession number) obtained using a CRF model2.   
Article classification. The automatic classification 
of articles relied on sentence analysis. The full text 
of articles was segmented into sentences, which 
were then scored by the sentence-level SVM clas-
sifier described above. An article is classified as 
positive if its top-scored sentence is scored higher 
than a threshold, which is predetermined as the 25th 
percentile score for positive sentences in the train-
ing set.  
Evaluation corpus . A corpus composed of 670 
PubMed Central articles was used to evaluate arti-
cle classification. 200 articles were considered as 
?positive? for data deposition based on MEDLINE 
gold standard annotations in the [si] field used to 
curate newly reported accession numbers.  
4 Results  
Table 1 shows the performance of selected SVM 
models for article classification on the test set. 
While differences were very small for cross-
validation on the training set, they are emphasized 
on the test set.   
 
Features P         R           F 
Tokens, position, part-of-
speech tags 
52%      56%     54% 
Token, position, CRF+, 
part-of-speech tags  
65%      58%     62% 
Tokens, position, CRF+/-, 
part-of-speech tags 
69%     78%     73% 
Table 1: Precision, Recall and F-measure of SVM 
models for article classification on test set. 
5 Discussion and Conclusion 
Portability of the method. Although trained 
mainly on microarray data deposition sentences, 
the method adapts well to the identification of oth-
                                                                 
1 http://www.ncbi.nlm.nih.gov/staff/lsmith/MedPost.html 
2 http://mallet.cs.umass.edu/ 
er data deposition sentences, e.g. gene sequences, 
protein coordinates.  
Comparison to other work. Our approach is not 
directly comparable to any of the previous studies. 
At the article level, we perform an automatic clas-
sification of articles containing data deposition 
sentences, in contrast with Oshner et al who per-
formed a one-time manual classification. Piwowar 
et alused machine learning and rule-based algo-
rithms for article classification. However, they re-
lied on identifying the names of five predetermined 
databases in the full text of articles. Our approach 
is generic and aiming at the automatic identifica-
tion of any biological data deposition in any public 
repository. Furthermore, our approach also re-
trieves specific data deposition sentences where 
data and deposition location are identified. At the 
sentence level, this is also different from the classi-
fication of databank accession number sentences 
performed by Kim et al (2010) in two ways: first, 
we focus on retrieving sentences containing acces-
sion numbers if they are deposition sentences (vs. 
data re-use, etc.) and second, we are also interested 
in retrieving data deposition sentences that do not 
contain accession numbers.  
Error analysis . Almost half of the articles clas-
sified as containing a deposition sentence by our 
method but not by the gold standard were found to 
indeed contain a deposition sentence.  
Conclusion. These results show the potential 
impact of our method for literature curation. In 
addition, it provides a robust tool for future work 
assessing the need for storage space of incoming 
data in public repositories. 
Acknowledgments 
This research was supported by the Intramural Re-
search Program of the NIH, NLM.  
References  
Jongwoo Kim, Daniel Le, Georges R. Thoma. Na?ve 
bayes and SVM classifiers for classifying databank 
accession number sentences from online biomedical 
articles. Proc. SPIE 2010 (7534): 7534OU-OU8 
Scott A. Ochsner, Davd L Steffen, Christian J Stoeckert 
Jr, Neil J. McKenna. Much room for improvement 
in deposition rates of expression microarray da-
tasets. Nat Methods. 2008 Dec;5(12):991.  
Heather A. Piwowar, Wendy W. Chapman. Identifying 
data sharing in biomedical literature.AMIA Annu 
Symp Proc. 2008 Nov 6:596-600. 
104
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 155?163,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Text Mining Techniques for Leveraging Positively Labeled Data 
Lana Yeganova*, Donald C. Comeau, Won Kim, W. John Wilbur 
National Center for Biotechnology Information, NLM, NIH, Bethesda, MD 20894 USA 
{yeganova, comeau, wonkim, wilbur}@mail.nih.gov 
* Corresponding author. Tel.:+1 301 402 0776 
Abstract 
Suppose we have a large collection of 
documents most of which are unlabeled. Suppose 
further that we have a small subset of these 
documents which represent a particular class of 
documents we are interested in, i.e. these are 
labeled as positive examples. We may have reason 
to believe that there are more of these positive 
class documents in our large unlabeled collection. 
What data mining techniques could help us find 
these unlabeled positive examples? Here we 
examine machine learning strategies designed to 
solve this problem. We find that a proper choice of 
machine learning method as well as training 
strategies can give substantial improvement in 
retrieving, from the large collection, data enriched 
with positive examples. We illustrate the principles 
with a real example consisting of multiword 
UMLS phrases among a much larger collection of 
phrases from Medline. 
 
1 Introduction 
Given a large collection of documents, a few of 
which are labeled as interesting, our task is to 
identify unlabeled documents that are also 
interesting. Since the labeled data represents the 
data we are interested in, we will refer to it as the 
positive class and to the remainder of the data as 
the negative class. We use the term negative class, 
however, documents in the negative class are not 
necessarily negative, they are simply unlabeled and 
the negative class may contain documents relevant 
to the topic of interest. Our goal is to retrieve these 
unknown relevant documents. 
A na?ve approach to this problem would simply 
take the positive examples as the positive class and 
the rest of the collection as the negative class and 
apply machine learning to learn the difference and 
rank the negative class based on the resulting 
scores. It is reasonable to expect that the top of this 
ranking would be enriched for the positive class. 
But an appropriate choice of methods can improve 
over the na?ve approach.  
One issue of importance would be choosing the 
most appropriate machine learning method. Our 
problem can be viewed from two different 
perspectives: the problem of learning from 
imbalanced data as well as the problem of 
recommender systems. In terms of learning from 
imbalanced data, our positive class is significantly 
smaller than the negative, which is the remainder 
of the collection. Therefore we are learning from 
imbalanced data. Our problem is also a 
recommender problem in that based on a few 
examples found of interest to a customer we seek 
similar positive examples amongst a large 
collection of unknown status. Our bias is to use 
some form of wide margin classifier for our 
problem as such classifiers have given good 
performance for both the imbalanced data problem 
and the recommender problem (Zhang and Iyengar 
2002; Abkani, Kwek et al 2004; Lewis, Yang et 
al. 2004).  
Imbalanced data sets arise very frequently in 
text classification problems. The issue with 
imbalanced learning is that the large prevalence of 
negative documents dominates the decision 
process and harms classification performance. 
Several approaches have been proposed to deal 
with the problem including sampling methods and 
cost-sensitive learning methods and are described 
in (Chawla, Bowyer et al 2002; Maloof 2003; 
Weiss, McCarthy et al 2007). These studies have 
shown that there is no clear advantage of one 
approach versus another. Elkan (2001) points out 
that cost-sensitive methods and sampling methods 
are related in the sense that altering the class 
distribution of training data is equivalent to 
altering misclassification cost. Based on these 
studies we examine cost-sensitive learning in 
which the cost on the positive set is increased, as a 
useful approach to consider when using an SVM.  
In order to show how cost-sensitive learning for 
an SVM is formulated, we write the standard 
equations for an SVM following (Zhang 2004). 
155
Given training data ? ?? ?,i ix y  where iy  is 1 or ?1 
depending on whether the data point 
ix  is 
classified as positive or negative, an SVM seeks 
that vector 
iw  which minimizes  
? ? 2( )     (1)2i ii h y x w w??? ? ??
 
 
where the loss function is defined by  
? ? 1 ,  1           (2) 0,         z>1.
z zh z ? ??? ??
 
The cost-sensitive version modifies (1) to become  
 
 
and now we can choose r?  and r?  to magnify the 
losses appropriately. Generally we take r?  to be 1, 
and r?  to be some factor larger than 1. We refer to 
this formulation as CS-SVM. Generally, the same 
algorithms used to minimize (1) can be used to 
minimize (3). 
Recommender systems use historical data on 
user preferences, purchases and other available 
data to predict items of interest to a user. Zhang 
and Iyengar (2002) propose a wide margin 
classifier with a quadratic loss function as very 
effective for this purpose (see appendix). It is used 
in (1) and requires no adjustment in cost between 
positive and negative examples. It is proposed as a 
better method than varying costs because it does 
not require searching for the optimal cost 
relationship between positive and negative 
examples. We will use for our wide margin 
classifier the modified Huber loss function (Zhang 
2004).  The modified Huber loss function is 
quadratic where this is important and has the form  
? ? ? ?2
4 ,     1
                    (4)1 ,  -1 1
0,     z>1.
z z
h z z z
? ? ? ??
?
? ? ? ??
?
?
 
We also use it in (1). We refer to this approach as 
the Huber method (Zhang 2004) as opposed to 
SVM. We compare it with SVM and CS-SVM. We 
used our own implementations for SVM, CS-SVM, 
and Huber that use gradient descent to optimize the 
objective function. 
The methods we develop are related to semi-
supervised learning approaches (Blum and 
Mitchell 1998; Nigam, McCallum et al 1999) and 
active learning (Roy and McCallum 2001; Tong 
and Koller 2001). Our method differs from active 
learning in that active learning seeks those 
unlabeled examples for which labels prove most 
informative in improving the classifier. Typically 
these examples are the most uncertain. Some semi-
supervised learning approaches start with labeled 
examples and iteratively seek unlabeled examples 
closest to already labeled data and impute the 
known label to the nearby unlabeled examples. Our 
goal is simply to retrieve plausible members for the 
positive class with as high a precision as possible. 
Our method has value even in cases where human 
review of retrieved examples is necessary. The 
imbalanced nature of the data and the presence of 
positives in the negative class make this a 
challenging problem. 
In Section 2 we discuss additional strategies 
proposed in this work, describe the data used and 
design of experiments, and provide the evaluation 
measure used. In Section 3 we present our results, 
in Sections 4 and 5 we discuss our approach and 
draw conclusions.  
 
2 Methods 
2.1 Cross Training 
Let D  represent our set of documents, and C?  
those documents that are known positives in D . 
Generally C?  would be a small fraction of D  and 
for the purposes of learning we assume that 
\C D C? ?? . 
 We are interested in the case when some of the 
negatively labeled documents actually belong to 
the positive class. We will apply machine learning 
to learn the difference between the documents in 
the class C?  and documents in the class C?  and 
use the weights obtained by training to score the 
documents in the negative class C? . The highest 
scoring documents in set C?  are candidate 
mislabeled documents. However, there may be a 
problem with this approach, because the classifier 
is based on partially mislabeled data. Candidate 
? ? ? ? 2( ) ( )  (3)2i i i ii C i Cr h y x w r h y x w w
?? ?
? ?
? ?? ?
? ? ? ? ? ? ? ?? ?
156
mislabeled documents are part of the C?  class. In 
the process of training, the algorithm purposely 
learns to score them low. This effect can be 
magnified by any overtraining that takes place. It 
will also be promoted by a large number of 
features, which makes it more likely that any 
positive point in the negative class is in some 
aspect different from any member of C? . 
Another way to set up the learning is by 
excluding documents from directly participating in 
the training used to score them. We first divide the 
negative set into disjoint pieces 
1 2C Z Z? ? ?  
Then train documents in C?  versus documents in 
1Z  to rank documents in 2Z  and train documents 
in C?  versus documents in 2Z  to rank documents 
in
1Z . We refer to this method as cross training 
(CT). We will apply this approach and show that it 
confers benefit in ranking the false negatives in 
C? .  
2.2 Data Sources and Preparation 
The databases we studied are MeSH25, Reuters, 
20NewsGroups, and MedPhrase. 
MeSH25.   We selected 25 MeSH? terms with 
occurrences covering a wide frequency range: from 
1,000 to 100,000 articles. A detailed explanation of 
MeSH can be found at 
http://www.nlm.nih.gov/mesh/. 
For a given MeSH term m, we treat the records 
assigned that MeSH term m as positive. The 
remaining MEDLINE? records do not have m 
assigned as a MeSH and are treated as negative. 
Any given MeSH term generally appears in a small 
minority of the approximately 20 million MEDLINE 
documents making the data highly imbalanced for 
all MeSH terms.  
Reuters. The data set consists of 21,578 Reuters 
newswire articles in 135 overlapping topic 
categories. We experimented on the 23 most 
populated classes. 
For each of these 23 classes, the articles in the 
class of interest are positive, and the rest of 21,578 
articles are negative. The most populous positive 
class contains 3,987 records, and the least 
populous class contains 112 records.  
 20NewsGroups. The dataset is a collection of 
messages from twenty different newsgroups with 
about one thousand messages in each newsgroup. 
We used each newsgroup as the positive class and 
pooled the remaining nineteen newsgroups as the 
negative class. 
Text in the MeSH25 and Reuters databases has 
been preprocessed as follows: all alphabetic 
characters were lowercased, non-alphanumeric 
characters replaced by blanks, and no stemming 
was done. Features in the MeSH25 dataset are all 
single nonstop terms and all pairs of adjacent 
nonstop terms that are not separated by 
punctuation. Features in the Reuters database are 
single nonstop terms only. Features in the 
20Newsgroups are extracted using the Rainbow 
toolbox (McCallum 1996).  
MedPhrase. We process MEDLINE to extract all 
multiword UMLS? 
(http://www.nlm.nih.gov/research/umls/) phrases 
that are present in MEDLINE. From the resulting 
set of strings, we drop the strings that contain 
punctuation marks or stop words. The remaining 
strings are normalized (lowercased, redundant 
white space is removed) and duplicates are 
removed. We denote the resulting set of 315,679 
phrases by 
phrasesU .  
For each phrase in ,phrasesU  we randomly 
sample, as available, up to 5 MEDLINE sentences 
containing it. We denote the resulting set of 
728,197 MEDLINE sentences by 
phrasesS . From
phrasesS  we extract all contiguous multiword 
expressions that are not present in 
phrasesU . We 
call them n-grams, where n>1. N-grams containing 
punctuation marks and stop words are removed 
and remaining n-grams are normalized and 
duplicates are dropped. The result is 8,765,444 n-
grams that we refer to as .ngramM  We believe that 
ngramM contains many high quality biological 
phrases. We use 
phrasesU  , a known set of high 
quality biomedical phrases, as the positive class, 
and 
ngramM  
as the negative class. 
In order to apply machine learning we need to 
define features for each n-gram. Given an n-gram 
grm that is composed of n  words,
1 2 ngrm w w w? , we extract a set of 11 numbers 
157
? ?111i if ?  associated with the n-gram grm. These are 
as follows: 
f1: number of occurrences of grm throughout 
Medline; 
f2: -(number of occurrences of w2?wn not 
following w1 in documents that contain grm)/ f1; 
f3: -(number of occurrences of w1?wn-1 not 
preceding wn in documents that contain grm)/ f1; 
f4: number of occurrences of (n+1)-grams of the 
form xw1?wn throughout Medline; 
f5: number of occurrences of (n+1)-grams  of 
the form w1?wn x throughout Medline; 
f6: ? ? ? ?? ?
? ?? ? ? ?
1 2 1 2
1 2 1 2
| 1 |log 1 | |
p w w p w w
p w w p w w
? ?? ?
? ?? ?? ?? ?
 
f7: mutual information between w1 and w2; 
f8: ? ? ? ?? ?
? ?? ? ? ?
1 1
1 1
| 1 |log 1 | |
n n n n
n n n n
p w w p w w
p w w p w w
? ?
? ?
? ?? ?
? ?? ?? ?
 
f9: mutual information between wn-1 and wn; 
f10: -(number of different multiword expressions 
beginning with w1 in Medline); 
f11: -(number of different multiword expressions 
ending with wn in Medline).   
We discretize the numeric values of the ? ?111i if ?  
into categorical values.  
In addition to these features, for every n-gram 
grm, we include the part of speech tags predicted 
by the MedPost tagger (Smith, Rindflesch et al 
2004).  To obtain the tags for a given n-gram grm 
we randomly select a sentence from 
phrasesS  
containing grm, tag the sentence, and consider the 
tags 
0 1 2 1 1n n nt t t t t t? ?  where 0t is the tag of the 
word preceding word 
1w in n-gram grm, 1t  is the 
tag of word 
1w  in n-gram grm, and so on. We 
construct the features  
  
 
These features emphasize the left and right ends of 
the n-gram and include parts-of-speech in the 
middle without marking their position. The 
resulting features are included with ? ?111i if ?  to 
represent the n-gram. 
2.3 Experimental Design  
A standard way to measure the success of a 
classifier is to evaluate its performance on a 
collection of documents that have been previously 
classified as positive or negative. This is usually 
accomplished by randomly dividing up the data 
into training and test portions which are separate. 
The classifier is then trained on the training 
portion, and is tested on test portion. This can be 
done in a cross-validation scheme or by randomly 
re-sampling train and test portions repeatedly.   
We are interested in studying the case where 
only some of the positive documents are labeled. 
We simulate that situation by taking a portion of 
the positive data and including it in the negative 
training set. We refer to that subset of positive 
documents as tracer data (Tr). The tracer data is 
then effectively mislabeled as negative. By 
introducing such an artificial supplement to the 
negative training set we are not only certain that 
the negative set contains mislabeled positive 
examples, but we know exactly which ones they 
are. Our goal is to automatically identify these 
mislabeled documents in the negative set and 
knowing their true labels will allow us to measure 
how successful we are. Our measurements will be 
carried out on the negative class and for this 
purpose it is convenient to write the negative class 
as composed of true negatives and tracer data 
(false negatives) 
'C C Tr? ?? ? . 
 
When we have trained a classifier, we evaluate 
performance by ranking 'C?  and measuring how 
well tracer data is moved to the top ranks. The 
challenge is that Tr appears in the negative class 
and will interact with the training in some way.  
2.4 Evaluation 
We evaluate performance using Mean Average 
Precision (MAP) (Baeza-Yates and Ribeiro-Neto 
1999). The mean average precision is the mean 
value of the average precisions computed for all 
topics in each of the datasets in our study. Average 
precision is the average of the precisions at each 
rank that contains a true positive document. 
 
? ? ? ?? ?? ?? ?
? ? ? ?? ?? ?? ?
0 1 1 2 1
0 1 1
if 2 :  ,1 , , 2 ,3 ,4 , ,...,  
otherwise: ,1 , , 2 ,3 ,4 .
n n
n n
n t t t t t t
t t t t
? ?
?
? ???
??
158
Table 1: MAP scores trained with three levels of tracer data introduced to the negative training set. 
 
No Cross Training No Tracer Data Tr20 in training Tr50 in training 
MeSH Terms Huber SVM Huber SVM Huber SVM 
celiac disease 0.694 0.677 0.466 0.484 0.472 0.373 
lactose intolerance 0.632 0.635 0.263 0.234 0.266 0.223 
myasthenia gravis 0.779 0.752 0.632 0.602 0.562 0.502 
carotid stenosis 0.466 0.419 0.270 0.245 0.262 0.186 
diabetes mellitus 0.181 0.181 0.160 0.129 0.155 0.102 
rats, wistar 0.241 0.201 0.217 0.168 0.217 0.081 
myocardial infarction 0.617 0.575 0.580 0.537 0.567 0.487 
blood platelets 0.509 0.498 0.453 0.427 0.425 0.342 
serotonin 0.514 0.523 0.462 0.432 0.441 0.332 
state medicine 0.158 0.164 0.146 0.134 0.150 0.092 
urinary bladder 0.366 0.379 0.312 0.285 0.285 0.219 
drosophila melanogaster 0.553 0.503 0.383 0.377 0.375 0.288 
tryptophan 0.487 0.480 0.410 0.376 0.402 0.328 
laparotomy 0.186 0.173 0.138 0.101 0.136 0.066 
crowns 0.520 0.497 0.380 0.365 0.376 0.305 
streptococcus mutans 0.795 0.738 0.306 0.362 0.218 0.306 
infectious mononucleosis 0.622 0.614 0.489 0.476 0.487 0.376 
blood banks 0.283 0.266 0.170 0.153 0.168 0.115 
humeral fractures 0.526 0.495 0.315 0.307 0.289 0.193 
tuberculosis, lymph node 0.385 0.397 0.270 0.239 0.214 0.159 
mentors 0.416 0.420 0.268 0.215 0.257 0.137 
tooth discoloration 0.499 0.499 0.248 0.215 0.199 0.151 
pentazocine 0.710 0.716 0.351 0.264 0.380 0.272 
hepatitis e 0.858 0.862 0.288 0.393 0.194 0.271 
genes, p16 0.278 0.313 0.041 0.067 0.072 0.058 
Avg 0.491 0.479 0.321 0.303 0.303 0.238 
 
3 Results 
3.1 MeSH25, Reuters, and 20NewsGroups 
We begin by presenting results for the MeSH25 
dataset. Table 1 shows the comparison between 
Huber and SVM methods. It also compares the 
performance of the classifiers with different levels 
of tracer data in the negative set. We set aside 50% 
of C?  to be used as tracer data and used the 
remaining 50% of C?  as the positive set for 
training. We describe three experiments where we 
have different levels of tracer data in the negative 
set at training time. These sets are ,C?  20 ,C Tr? ?  
and 
50  C Tr? ? representing no tracer data, 20% of 
C?  as tracer data and 50% of C?  as tracer data, 
respectively. The test set 
20C Tr? ?  is the same for 
all of these experiments. Results indicate that on 
average Huber outperforms SVM on these highly 
imbalanced datasets. We also observe that 
performance of both methods deteriorates with 
increasing levels of tracer data.   
Table 2 shows the performance of Huber and 
SVM methods on negative training sets with tracer 
data 
20C Tr? ?  and 50  C Tr? ? as in Table 1, but 
with cross training. As mentioned in the Methods 
section, we first divide each negative training set 
into two disjoint pieces
1Z  and 2Z . We then train 
documents in the positive training set versus 
documents in 
1Z  to score documents in 2Z  and 
train documents in the positive training set versus  
documents in 
2Z  to score documents in 1Z . We 
then merge 
1Z  and 2Z  as scored sets and report 
measurements on the combined ranked set of 
documents. Comparing with Table 1, we see a 
significant improvement in the MAP when using 
cross training. 
 
159
 
Table 2: MAP scores for Huber and SVM trained with two levels of tracer data introduced to the 
negative training set using cross training technique. 
 
2-fold Cross Training Tr20 in training Tr50 in training 
MeSH Terms Huber SVM Huber SVM 
celiac disease 0.550 0.552 0.534 0.521 
lactose intolerance 0.415 0.426 0.382 0.393 
myasthenia gravis 0.652 0.643 0.623 0.631 
carotid stenosis 0.262 0.269 0.241 0.241 
diabetes mellitus 0.148 0.147 0.144 0.122 
rats, wistar 0.212 0.186 0.209 0.175 
myocardial infarction 0.565 0.556 0.553 0.544 
blood platelets 0.432 0.435 0.408 0.426 
serotonin 0.435 0.447 0.417 0.437 
state medicine 0.135 0.136 0.133 0.132 
urinary bladder 0.295 0.305 0.278 0.280 
drosophila melanogaster 0.426 0.411 0.383 0.404 
tryptophan 0.405 0.399 0.390 0.391 
laparotomy 0.141 0.128 0.136 0.126 
crowns 0.375 0.376 0.355 0.353 
streptococcus mutans 0.477 0.517 0.448 0.445 
infectious mononucleosis 0.519 0.514 0.496 0.491 
blood banks 0.174 0.169 0.168 0.157 
humeral fractures 0.335 0.335 0.278 0.293 
tuberculosis, lymph node 0.270 0.259 0.262 0.244 
mentors 0.284 0.278 0.275 0.265 
tooth discoloration 0.207 0.225 0.209 0.194 
pentazocine 0.474 0.515 0.495 0.475 
hepatitis e 0.474 0.499 0.482 0.478 
genes, p16 0.102 0.101 0.083 0.093 
Avg 0.350 0.353 0.335 0.332 
 
 
We performed similar experiments with the 
Reuters and 20NewsGroups datasets, where 20%  
and 50% of the good set is used as tracer data. We 
report MAP scores for these datasets in Tables 3 
and 4. 
 
3.2 Identifying high quality biomedical 
phrases in the MEDLINE Database 
We illustrate our findings with a real example 
of detecting high quality biomedical phrases 
among ,ngramM a large collection of multiword 
expressions from Medline. We believe that 
ngramM  
contains many high quality biomedical phrases. 
These examples are the counterpart of the 
mislabeled positive examples (tracer data) in the 
previous tests. 
  
Table 3: MAP scores for Huber and SVM 
trained with 20% and 50% tracer data introduced to 
the negative training set for Reuters dataset. 
 
Reuters 
Tr20 in training Tr50 in training 
Huber SVM Huber SVM 
No CT 0.478 0.451 0.429 0.403 
2-Fold CT 0.662 0.654 0.565 0.555 
 
Table 4: MAP scores for Huber and SVM 
trained with 20% and 50% tracer data introduced to 
the negative training set for 20NewsGroups dataset. 
 
20News 
Groups 
Tr20 in training Tr50 in training 
Huber SVM Huber SVM 
No CT 0.492 0.436 0.405 0.350 
2-Fold CT 0.588 0.595 0.502 0.512 
160
To identify these examples, we learn the 
difference between the phrases in 
phrasesU  
 and 
.ngramM  Based on the training we rank the n-grams 
in .ngramM  
We expect the n-grams that cannot be 
separated from UMLS phrases are high quality 
biomedical phrases. In our experiments, we 
perform 3-fold cross validation for training and 
testing. This insures we obtain any possible benefit 
from cross training. The results shown in figure 1 
are MAP values for these 3 folds.  
 
Figure 1. Huber, CS-SVM, and na?ve Bayes 
classifiers applied to the MedPhrase dataset. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We trained na?ve Bayes, Huber, and CS-SVM 
with a range of different cost factors. The results 
are presented in Figure 1. We observe that the 
Huber classifier performs better than na?ve Bayes. 
CS-SVM with the cost factor of 1 (standard SVM) 
is quite ineffective. As we increase the cost factor, 
the performance of CS-SVM improves until it is 
comparable to Huber. We believe that the quality 
of ranking is better when the separation of 
phrasesU  
from 
ngramM  is better.  
Because we have no tracer data we have no 
direct way to evaluate the ranking of .ngramM  
However, we selected a random set of 100 n-grams 
from ,ngramM  which score as high as top-scoring 
10% of phrases in 
phrasesU . Two reviewers 
manually reviewed that list and identified that 99 
of these 100 n-grams were high quality biomedical 
phrases. Examples are: aminoshikimate pathway, 
berberis aristata, dna hybridization, subcellular 
distribution, acetylacetoin synthase, etc. One false-
positive example in that list was congestive heart.  
 
 
4 Discussion 
We observed that the Huber classifier performs 
better than SVM on imbalanced data with no cross 
training (see appendix). The improvement of 
Huber over SVM becomes more marked as the 
percentage of tracer data in the negative training 
set is increased. However, the results also show 
that cross training, using either SVM or Huber 
(which are essentially equivalent), is better than 
using Huber without cross training. This is 
demonstrated in our experiments using the tracer 
data. The results are consistent over the range of 
different data sets. We expect cross training to 
have benefit in actual applications.  
Where does cost-sensitive learning fit into this 
picture? We tested cost-sensitive learning on all of 
our corpora using the tracer data. We observed 
small and inconsistent improvements (data not 
shown). The optimal cost factor varied markedly 
between cases in the same corpus. We could not 
conclude this was a useful approach and instead 
saw better results simply using Huber. This 
conclusion is consistent with (Zhang and Iyengar 
2002) which recommend using a quadratic loss 
function. It is also consistent with results reported 
in (Lewis, Yang et al 2004) where CS-SVM is 
compared with SVM on multiple imbalanced text 
classification problems and no benefit is seen using 
CS-SVM. Others have reported a benefit with CS-
SVM (Abkani, Kwek et al 2004; Eitrich and Lang 
2005). However, their datasets involve relatively 
few features and we believe this is an important 
aspect where cost-sensitive learning has proven 
effective. We hypothesize that this is the case 
because with few features the positive data is more 
likely to be duplicated in the negative set. In our 
case, the MedPhrase dataset involves relatively 
few features (410) and indeed we see a dramatic 
improvement of CS-SVM over SVM. 
One approach to dealing with imbalanced data 
is the artificial generation of positive examples as 
seen with the SMOTE algorithm (Chawla, Bowyer 
et al 2002). We did not try this method and do not 
know if this approach would be beneficial for 
1 11 21 31 41
Cost  Factor r
+
0.20
0.22
0.24
0.26
0.28
A
v
e
r
a
g
e
 
P
r
e
c
i
s
i
o
n
Comparison of different Machine Learning Methods
Huber
CS-SVM
Bayes
161
textual data or data with many features. This is an 
area for possible future research. 
Effective methods for leveraging positively 
labeled data have several potential applications:  
? Given a set of documents discussing a 
particular gene, one may be interested in 
finding other documents that talk about the 
same gene but use an alternate form of the 
gene name.  
? Given a set of documents that are indexed with 
a particular MeSH term, one may want to find 
new documents that are candidates for being 
indexed with the same MeSH term. 
? Given a set of papers that describe a particular 
disease, one may be interested in other 
diseases that exhibit a similar set of symptoms. 
? One may identify incorrectly tagged web 
pages.  
These methods can address both removing 
incorrect labels and adding correct ones. 
 
5 Conclusions 
Given a large set of documents and a small set 
of positively labeled examples, we study how best 
to use this information in finding additional 
positive examples. We examine the SVM and 
Huber classifiers and conclude that the Huber 
classifier provides an advantage over the SVM 
classifier on such imbalanced data. We introduce a 
technique which we term cross training. When this 
technique is applied we find that the SVM and 
Huber classifiers are essentially equivalent and 
superior to applying either method without cross 
training.  We confirm this on three different 
corpora. We also analyze an example where cost-
sensitive learning is effective. We hypothesize that 
with datasets having few features, cost-sensitive 
learning can be beneficial and comparable to using 
the Huber classifier.  
 
Appendix: Why Huber Loss Function works 
better for problems with Unbalanced Class 
Distributions. 
The drawback of the standard SVM for the 
problem with an unbalanced class distribution 
results from the shape of ( )h z  in (2). Consider the 
initial condition at 0w ?  and also imagine that there is 
a lot more C?  training data than C?  training data.  In 
this case, by choosing 1? ? ? , we can achieve the 
minimum value of the loss function in (1) for the initial 
condition 0w ? . Under these conditions, all C?  points 
yield 1z ?  and ( ) 0h z ? and all C?  points yield 
1z ? ?  and ( ) 2h z ? . The change of the loss function 
( )h z?  in (2) with a change w?  is given by 
 
 
 
In order to reduce the loss at a C?  
data point ( , )i ix y , 
we must choose w?  such that 0.ix w?? ?   But we 
assume that there are significantly more C?  class 
data points than C?  and many such points x? are 
mislabeled and close to
ix  such that 0.x w? ?? ?  
Then ( )h z  is likely be increased by ( 0)x w? ?? ?  
for these mislabeled points. Clearly, if there are 
significantly more C?  class data than those of  C?
class and the C? set  contains a lot of mislabeled 
points, it may be difficult to find w?  that can 
result in a net effect of decreasing the right hand 
side of (2). The above analysis shows why the 
standard support vector machine formulation in (2) 
is vulnerable to an unbalanced and noisy training 
data set. The problem is clearly caused by the fact 
that the SVM loss function ( )h z  in (2) has a 
constant slope for 1z ? . In order to alleviate this 
problem, Zhang and Iyengar (2002) proposed the 
loss function 2 ( )h z  which is a smooth non-
increasing function with slope 0 at 1z ? . This 
allows the loss to decrease while the positive 
points move a small distance away from the bulk 
of the negative points and take mislabeled points 
with them. The same argument applies to the 
Huber loss function defined in (4). 
 
Acknowledgments 
This research was supported by the Intramural Research 
Program of the NIH, National Library of Medicine. 
  
? ? ( )      (5).w i idh zh z z w y x wdz? ? ? ?? ? ? ??
162
References  
 
Abkani, R., S. Kwek, et al (2004). Applying Support 
Vector Machines to Imballanced Datasets. ECML. 
  
Baeza-Yates, R. and B. Ribeiro-Neto (1999). Modern 
Information Retrieval. New York, ACM Press. 
  
Blum, A. and T. Mitchell (1998). "Combining Labeled 
and Unlabeled Data with Co-Training." COLT: 
Proceedings of the Workshop on Computational 
Learning Theory: 92-100. 
  
Chawla, N. V., K. W. Bowyer, et al (2002). "SMOTE: 
Synthetic Minority Over-sampling Technique." Journal 
of Artificial Intelligence Research 16: 321-357. 
  
Eitrich, T. and B. Lang (2005). "Efficient optimization 
of support vector machine learning parameters for 
unbalanced datasets." Journal of Computational and 
Applied Mathematics 196(2): 425-436. 
  
Elkan, C. (2001). The Foundations of Cost Sensitive 
Learning. Proceedings of the Seventeenth International 
Joint Conference on Artificial Intelligence. 
  
Lewis, D. D., Y. Yang, et al (2004). "RCV1: A New 
Benchmark Collection for Text Categorization 
Research." Journal of Machine Learning Research 5: 
361-397. 
  
Maloof, M. A. (2003). Learning when data sets are 
imbalanced and when costs are unequal and unknown. 
ICML 2003, Workshop on Imballanced Data Sets. 
  
McCallum, A. K. (1996). "Bow: A toolkit for statistical 
language modeling, text retrieval, classification and 
clustering. http://www.cs.cmu.edu/~mccallum/bow/." 
  
Nigam, K., A. K. McCallum, et al (1999). "Text 
Classification from Labeled and Unlabeled Documents 
using EM." Machine Learning: 1-34. 
  
Roy, N. and A. McCallum (2001). Toward Optimal 
Active Learning through Sampling Estimation of Error 
Reduction. Eighteenth International Conference on 
Machine Learning. 
  
Smith, L., T. Rindflesch, et al (2004). "MedPost: A part 
of speech tagger for biomedical text." Bioinformatics 
20: 2320-2321. 
  
Tong, S. and D. Koller (2001). "Support vector machine 
active learning with applications to text classification." 
Journal of Machine Learning Research 2: 45-66. 
  
Weiss, G., K. McCarthy, et al (2007). Cost-Sensitive 
Learning vs. Sampling: Which is Best for Handling 
Unbalanced Classes with Unequal Error Costs? 
Proceedings of the 2007 International Conference on 
Data Mining. 
  
Zhang, T. (2004). Solving large scale linear prediction 
problems using stochastic gradient descent algorithms. 
Twenty-first International Conference on Machine 
learning, Omnipress. 
  
Zhang, T. and V. S. Iyengar (2002). "Recommender 
Systems Using Linear Classifiers." Journal of Machine 
Learning Research 2: 313-334. 
  
 
 
163
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 185?192,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Classifying Gene Sentences in Biomedical Literature by 
Combining High-Precision Gene Identifiers 
 
 
Sun Kim, Won Kim, Don Comeau, and W. John Wilbur 
National Center for Biotechnology Information 
National Library of Medicine, National Institutes of Health 
Bethesda, MD 20894, USA 
{sun.kim,won.kim,donald.comeau,john.wilbur}@nih.gov 
 
 
 
 
 
 
Abstract 
Gene name identification is a fundamental 
step to solve more complicated text mining 
problems such as gene normalization and pro-
tein-protein interactions. However, state-of-
the-art name identification methods are not 
yet sufficient for use in a fully automated sys-
tem. In this regard, a relaxed task, 
gene/protein sentence identification, may 
serve more effectively for manually searching 
and browsing biomedical literature. In this pa-
per, we set up a new task, gene/protein sen-
tence classification and propose an ensemble 
approach for addressing this problem. Well-
known named entity tools use similar gold-
standard sets for training and testing, which 
results in relatively poor performance for un-
known sets. We here explore how to combine 
diverse high-precision gene identifiers for 
more robust performance. The experimental 
results show that the proposed approach out-
performs BANNER as a stand-alone classifier 
for newly annotated sets as well as previous 
gold-standard sets. 
1 Introduction 
With the rapidly increasing biomedical literature, 
text mining has become popular for finding bio-
medical information in text. Among others, named 
entity recognition (NER) for bio-entities such as 
genes and proteins is a fundamental task because 
extracting biological relationships begins with enti-
ty identification. However, NER in biomedical 
literature is challenging due to the irregularities 
and ambiguities in bio-entities nomenclature (Yang 
et al, 2008). In particular, compound entity names 
make this problem difficult because it also requires 
deciding word boundaries. 
Recent bio-text competitions such as JNLPBA 
(Kim et al, 2004) and BioCreative (Lu et al, 2011; 
Smith et al, 2008) have evaluated NER systems 
for gene mentions. Even though progress has been 
made in several areas, gene identification methods 
are not yet sufficient for real-world use without 
human interaction (Arighi et al, 2011). Thus, at 
the present, a realistic suggestion is to use these 
algorithms as an aid to human curation and infor-
mation retrieval (Altman et al, 2008). 
In this paper, we define a new task, gene/protein 
sentence classification. A gene or protein sentence 
means a sentence including at least one specific 
gene or protein name. This new task has ad-
vantages over gene mention identification. First, 
gene name boundaries are not important at the sen-
tence level and human judges will agree more in 
their judgments. Second, highlighting gene sen-
tences may be more useful in manual search and 
browsing environments since this can be done 
more accurately and with less distraction from in-
correct annotations. 
To classify gene/protein sentences, we here pro-
pose an ensemble approach to combine different 
NER identifiers. Previous NER approaches are 
mostly developed on a small number of gold-
185
standard sets including GENIA (Kim et al, 2003) 
and BioCreative (Smith et al, 2008) corpora. The-
se sets help to find regular name patterns in a lim-
ited set of articles, but also limit the NER 
performance for real-world use. In the proposed 
approach, we use a Semantic Model and a Priority 
Model along with BANNER (Leaman and 
Gonzalez, 2008). The Semantic and Priority Mod-
els are used to provide more robust performance on 
gene/protein sentence classification because they 
utilize larger resources such as SemCat and Pub-
Med?R  to detect gene names. 
For experiments, we created three new gold-
standard sets to include cases appearing in the most 
recent publications. The experimental results show 
that our approach outperforms machine learning 
classifiers using unigrams and substring features as 
well as stand-alone BANNER classification on five 
gold-standard datasets. 
The paper is organized as follows. In Section 2, 
the ensemble approach for gene/protein sentence 
classification is described. Section 3 explains the 
gold-standard sets used for our experiments. Sec-
tion 4 presents and discusses the experimental re-
sults. Conclusions are drawn in Section 5. 
2 Methods 
 Figure 1. Method Overview. 
 
Figure 1 shows the overall framework for our pro-
posed approach. We basically assume that a main 
NER module works as a strong predictor, i.e., the 
majority of outputs obtained from this module are 
correct. We here use BANNER (Leaman and 
Gonzalez, 2008) as the main NER method because 
it adopts features and methods which are generally 
known to be effective for gene name recognition. 
While BANNER shows good performance on 
well-known gold-standard sets, it suffers from rela-
tively poor performance on unknown examples. To 
overcome this problem, we combine BANNER 
with two other predictors, a Sematic Model and a 
Priority Model. First, the Semantic Model and the 
Priority Model do not use previous gold-standard 
sets for training. Second, these two models learn 
name patterns in different ways, i.e., semantic rela-
tionships for the Semantic Model and positional 
and lexical information for the Priority Model. 
This combination of a strong predictor and two 
weaker but more general predictors can respond 
better to unknown name patterns. 
As described above, the proposed method main-
ly relies on outputs from different NER methods, 
whereas word features can still provide useful evi-
dence for discriminating gene and non-gene sen-
tences. Hence, we alternatively utilize word 
features such as unigrams and substrings along 
with NER features. For NER features only, the 
output is the sum of binary decisions from three 
NER modules. For word and NER features, the 
Huber classifier (Kim and Wilbur, 2011) is trained 
to combine the features. The parameter set in the 
Huber classifier is optimized to show the best clas-
sification performance on test sets. The following 
subsections describe each feature type used for 
gene sentence classification. 
2.1 Word Features 
Unigrams are a set of words obtained from to-
kenizing sentences on white space. All letters in 
unigrams are converted to lower case.  
Substrings are all contiguous substrings of a sen-
tence, sized three to six characters. This substring 
feature may help reduce the difference between 
distributions on training and test sets (Huang et al, 
2008). Substrings encode the roots and morpholo-
gy of words without identifying syllables or stems. 
They also capture neighboring patterns between 
words. 
2.2 BANNER 
BANNER is a freely available tool for identifying 
gene mentions. Due to its open-source policy and 
Java implementation, it has become a popular tool. 
BANNER uses conditional random fields (CRF) 
as a discriminative method and utilizes a set of fea-
ture types that are known to be good for identify-
ing gene names. The feature sets used are 
186
orthographic, morphological and shallow syntax 
features (Leaman and Gonzalez, 2008): 
 
(1) The part of speech (POS) of a token in a sen-
tence. 
(2) The lemma of a word. 
(3) 2, 3 and 4-character prefixes and suffixes. 
(4) 2 and 3 character n-grams including start-of-
token and end-of-token indicators. 
(5) Word patterns by converting upper-case letters, 
lower-case letters and digits to their corresponding 
representative characters (A, a, 0). 
(6) Numeric normalization by converting digits to 
?0?s. 
(7) Roman numerals. 
(8) Names of Greek letters. 
 
Even though BANNER covers most popular 
feature types, it does not apply semantic features or 
other post-processing procedures such as abbrevia-
tion processing. However, these features may not 
have much impact for reducing performance since 
our goal is to classify gene sentences, not gene 
mentions. 
2.3 Semantic Model  
The distributional approach to semantics (Harris, 
1954) has become more useful as computational 
power has increased, and we have found this ap-
proach helpful in the attempt to categorize entities 
found in text. We use a vector space approach to 
modeling semantics (Turney and Pantel, 2010) and 
compute our vectors as described in (Pantel and 
Lin, 2002) except we ignore the actual mutual in-
formation and just include a component of 1 if the 
dependency relation occurs at all for a word, else 
the component is set to 0. We constructed our vec-
tor space from all single tokens (a token must have 
an alphabetic character) throughout the titles and 
abstracts of the records in the whole of the Pub-
Med database based on a snapshot of the database 
taken in January 2012. We included only tokens 
that occurred in the data sufficient to accumulate 
10 or more dependency relations. There were just 
over 750 thousand token types that satisfied this 
condition and are represented in the space. We de-
note this space by h. We then took all the single 
tokens and all head words from multi-token strings 
in the categories ?chemical?, ?disease?, and 
?gene/protein? from an updated version of the  
SemCat database (Tanabe et al, 2006) and placed 
all the other SemCat categories similarly processed 
into a category we called ?other?. We consider on-
ly the tokens in these categories that also occur in 
our semantic vector space h and refer to these sets 
as Chemicalh , Diseaseh , inGene/Proteh , Otherh . Table 1 shows 
the size of overlaps between sets. 
 
 Chemicalh Diseaseh  inGene/Proteh  Otherh
Chemicalh 54478 209 4605 5495 
Diseaseh  8801 1139 169 
inGene/Proteh   76440 9466 
Otherh    127337 
Table 1. Pairwise overlap between sets representing the 
different categories. 
 
Class 'Chemicalh 'Diseaseh  ' inGene/Proteh  'OtherhStrings 49800 7589 70832 113815 
Ave. Prec. 0.8680 0.7060 0.9140 0.9120 
Table 2. Row two contains the number of unique strings 
in the four different semantic classes studied. The last 
row shows the mean average precisions from a 10-fold 
cross validation to learn how to distinguish each class 
from the union of the other three. 
 
In order to remove noise or ambiguity in the 
training set, we removed the tokens that appeared 
in more than one semantic class as follows. 
 ? ?
? ?
? ?
? ?inGene/ProteDiseaseChemicalOther'Other
DiseaseChemicalinGene/Prote
'
inGene/Prote
inGene/ProteChemicalDisease
'
Disease
inGene/ProteDiseaseChemical
'
Chemical
hhhhh
hhhh
hhhh
hhhh
????
???
???
???
       
   (1)
  
We then applied Support Vector Machine learn-
ing to the four resulting disjoint semantic classes in 
a one-against-all strategy to learn how to classify 
into the different classes. We used 31064.1 ??C  
based upon the size of the training set. As a test of 
this process we applied this same learning with 10-
fold cross validation on the training data and the 
results are given in the last row of Table 2. 
This Semantic Model is an efficient and general 
way to identify words indicating gene names. Un-
like other NER approaches, this model decides a 
target class solely based on a single word. Howev-
er, evaluating all tokens from sentences may in-
crease incorrect predictions. A dependency parser 
analyzes a sentence as a set of head- and depend-
187
ent-word combinations. Since gene names likely 
appear in describing a relationship with other enti-
ties, a name indicating a gene mention will be 
mostly placed in a dependent position. Thus, we 
first apply the C&C CCG parser (Curran et al, 
2007), and evaluate words in dependent positions 
only. 
 
2.4 Priority Model 
The Semantic Model detects four different catego-
ries for a single word. However, the Priority Model 
captures gene name patterns by analyzing the order 
of words and the character strings making up 
words. Since gene names are noun phrases in gen-
eral, we parse sentences and identify noun phrases 
first. These phrases are then evaluated using the 
Priority Model. 
The Priority Model is a statistical language 
model for named entity recognition (Tanabe and 
Wilbur, 2006). For named entities, a word to the 
right is more likely to be the word determining the 
nature of the entity than a word to the left in gen-
eral.  
Let T1 be the set of training data for class C1 and 
T2 for class C2. Let ? ? At ???  denote the set of all to-
kens used in names contained in 21 TT ? . For each 
token t?, A?? , it is assumed that there are associ-
ated two probabilities p? and q?, where p? is the 
probability that the appearance of the token t?  in a 
name indicates that name belongs to class C1 and 
q? is the probability that t? is a more reliable indi-
cator of the class of a name than any token to its 
left. Let )()2()1( ktttn ??? ??  be composed of the 
tokens on the right in the given order. Then the 
probability of n belonging to class C1 can be com-
puted as follows. 
 
? ? ? ? ? ?? ??
? ???
????
k
i
k
ij
jii
k
j
j qpqqpnCp
2 1
)()()(
2
)()1(1 11| ?????  (2) 
 
A limited memory BFGS method (Nash and 
Nocedal, 1991) and a variable order Markov model 
(Tanabe and Wilbur, 2006) are used to obtain p?   
and q?. An updated version of SemCat (Tanabe and 
Wilbur, 2006) was used to learn gene names. 
2.5 Semantic and Priority Models for High-
Precision Scores 
The Semantic and Priority Models learn gene 
names and other necessary information from the 
SemCat database, where names are semantically 
categorized based on UMLS?R  (Unified Medical 
Language System) Semantic Network. Even 
though the Semantic and Priority Models show 
good performance on names in SemCat, they can-
not avoid noise obtained from incorrect pre-
processing, e.g., parsing errors. The use of a gen-
eral category for training may also limit perfor-
mance. To obtain high-precision scores for our 
ensemble approach, it is important to reduce the 
number of false positives from predictions. Hence, 
we apply the Semantic and Priority Models on 
training sets, and mark false positive cases. These 
false positives are automatically removed from 
predictions on test sets. These false positive cases 
tend to be terms for entities too general to warrant 
annotation. 
Table 3 shows the classification performance 
with and without false positive corrections on 
training data. For both Semantic and Priority Mod-
els, precision rates are increased by removing false 
positives. Even though recall drops drastically, this 
does not cause a big problem in our setup since 
these models try to detect gene names which are 
not identified by BANNER. 
 
 SEM SEMFP PM PMFP
Accuracy 0.7907 0.7773 0.7805 0.8390 
Precision 0.7755 0.8510 0.7405 1.0000 
Recall 0.8323 0.6852 0.8799 0.6856 
F1 0.8029 0.7592 0.8042 0.8135 
Table 3. Performance changes on training set for the 
Semantic Model (SEM) and the Priority Model (PM). 
FP indicates that learned false positives were removed 
from predictions. 
3 Datasets 
For experiments, we rigorously tested the proposed 
method on gene mention gold-standard sets and 
newly annotated sets. GENETAG (Smith et al, 
2008) is the dataset released for BioCreative I and 
BioCreative II workshops. Since it is well-known 
for a gene mention gold-standard set, we used 
GENETAG as training data. 
For test data, two previous gold-standard sets 
were selected and new test sets were also built for 
gene sentence classification. YAPEX (Franzen et 
al., 2002) and JNLPBA (Kim et al, 2004) are con-
sidered of moderate difficulty because they are 
188
both related to GENIA corpus, a well-known gold-
standard set. However, Disease, Cell Line and 
Reptiles are considered as more difficult tasks be-
cause they represent new areas and contain recent-
ly published articles. The annotation guideline for 
new test sets basically followed those used in 
GENETAG (Tanabe et al, 2005), however do-
mains, complexes, subunits and promoters were 
not included in new sets. 
 
(1) ?Disease? Set: This set of 60 PubMed docu-
ments was obtained from two sources. Fifty of the 
documents were obtained from the 793 PubMed 
documents used to construct the AZDC (Leaman et 
al., 2009). They are the fifty most recent among 
these records. In addition to these fifty documents, 
ten documents were selected from PubMed on the 
topic of maize to add variety to the set and because 
one of the curators who worked with the set had 
experience studying the maize genome.  These ten 
were chosen as recent documents as of early March 
2012 and which contained the text word maize and 
discussed genetics.  The whole set of 60 docu-
ments were annotated by WJW to produce a gold 
standard. 
 
(2) ?CellLine? Set: This set comprised the most 
recent 50 documents satisfying the query ?cell 
line[MeSH]? in PubMed on March 15, 2012. This 
query was used to obtain documents which discuss 
cell lines, but most of these documents also discuss 
genes and for this reason the set was expected to be 
challenging. The set was annotated by WJW and 
DC and after independently annotating the set they 
reconciled differences to produce a final gold 
standard. 
 
(3) ?Reptiles? Set: This set comprised the most 
recent 50 documents satisfying the query ?reptiles 
AND genes [text]? in PubMed on March 15, 2012. 
This set was chosen because it would have little 
about human or model organisms and for this rea-
son it was expected to be challenging.  The set was 
annotated by WJW and DC and after independent-
ly annotating the set they reconciled differences to 
produce a final gold standard. 
 
For both ?CellLine? and ?Reptiles? Sets, the 
most recent data was chosen in an effort to make 
the task more challenging. Presumably such docu-
ments will contain more recently created names 
and phrases that do not appear in the older training 
data. This will then pose a more difficult test for 
NER systems. 
Table 4 shows all datasets used for training and 
testing. The new sets, ?Disease?, ?CellLine? and 
?Reptiles? are also freely available at 
http://www.ncbi.nlm.nih.gov/CBBresearch/Wilbur/
IRET/bionlp.zip 
 
 Positives Negatives Total 
GENETAG 10245 9755 20000 
YAPEX 1298 378 1676 
JNLPBA 17761 4641 22402 
Disease 345 251 596 
CellLine 211 217 428 
Reptiles 179 328 507 
Table 4. Datasets. ?GENETAG? was used for training 
data and others were used for test data. ?YAPEX? and 
?JNLPBA? were selected from previous gold-standard 
corpora. ?Disease?, ?Cell Line? and ?Reptiles? are new-
ly created from recent publications and considered as 
difficult sets. 
4 Results and Discussion  
In this paper, our goal is to achieve higher-
prediction performance on a wide range of gene 
sentences by combining multiple gene mention 
identifiers. The basic assumption here is that there 
is a strong predictor that performs well for previ-
ously known gold-standard datasets. For this 
strong predictor, we selected BANNER since it 
includes basic features that are known to give good 
performance. 
 
 Accuracy Precision Recall F1 
GENETAG 0.9794 0.9817 0.9779 0.9799 
YAPEX 0.9051 0.9304 0.9483 0.9392 
JNLPBA 0.8693 0.9349 0.8976 0.9159 
Disease 0.8591 0.9223 0.8261 0.8716 
Cell Line 0.8925 0.9146 0.8626 0.8878 
Reptiles 0.8994 0.8478 0.8715 0.8595 
Table 5. Performance of BANNER on training and test 
datasets. 
 
Table 5 presents the gene sentence classification 
performance of BANNER on training and test sets. 
We emphasize that performance here means that if 
BANNER annotates a gene/protein name in a sen-
tence, that sentence is classified as positive, other-
wise it is classified as negative. BANNER used 
GENETAG as training data, hence it shows excel-
lent classification performance on the same set. 
189
 Unigrams Substrings BANNER Ensemble Uni+Ensemble Sub+Ensemble
YAPEX 0.9414 0.9491 0.9685 0.9704 0.9624 0.9678 
JNLPBA 0.9512 0.9504 0.9584 0.9651 0.9625 0.9619 
Disease 0.8255 0.8852 0.9238 0.9501 0.9573 0.9610 
CellLine 0.8174 0.9004 0.9281 0.9539 0.9429 0.9496 
Reptiles 0.6684 0.7360 0.8696 0.9049 0.9001 0.8937 
Table 6. Average precision results on test sets for different feature combinations. 
 
 Unigrams Substrings BANNER Ensemble Uni+Ensemble Sub+Ensemble
YAPEX 0.8735 0.8819 0.9321 0.9196 0.9298 0.9336 
JNLPBA 0.8902 0.8938 0.9111 0.9197 0.9262 0.9264 
Disease 0.7449 0.7884 0.8479 0.8894 0.8957 0.9043 
CellLine 0.7346 0.8057 0.8698 0.9017 0.9052 0.8957 
Reptiles 0.6257 0.6816 0.8499 0.8199 0.8547 0.8547 
Table 7. Breakeven results on test sets for different feature combinations. 
 
 
? Just one fiber gene was revealed in this strain. 
 
? This transcription factor family is characterized by 
a DNA-binding alpha-subunit harboring the Runt 
domain and a secondary subunit, beta, which binds 
to the Runt domain and enhances its interaction 
with DNA.  
  
Figure 2. False positive examples including misleading 
words. 
 
YAPEX and JNLPBA are gold-standard sets that 
partially overlap the GENIA corpus. Since 
BANNER utilizes features from previous research 
on GENETAG, YAPEX and JNLPBA, we expect 
good performance on these data sets. For that rea-
son, we created the three additional gold-standard 
sets to use in this study, and we believe the per-
formance on these sets is more representative of 
what could be expected when our method is ap-
plied to cases recently appearing in the literature. 
Table 6 show average precision results for the 
different methods and all the test sets. GENETAG 
is left out because BANNER is trained on 
GENETAG. We observe improved performance of 
the ensemble methods over unigrams, substrings 
and BANNER. The improvement is small on 
YAPEX and JNLPBA, but larger for Disease, 
CellLine and Reptiles. We see that unigrams and 
substrings tend to add little to the plain ensemble. 
The MAP (Mean Average Precision) values in 
Table 6 are in contrast to the breakeven results in 
Table 7, where we see that unigrams and sub-
strings included with the ensemble generally give 
improved results.  Some of the unigrams and sub-
strings are specific enough to detect gene/protein 
names with high accuracy, and improve precision 
in top ranks in a way that cannot be duplicated by 
the annotations coming from Semantic or Priority 
Models or BANNER. In addition, substrings may 
capture more information than unigrams because 
of their greater generality. 
Some of our errors are due to false positive NER 
identifications. By this we mean a token was clas-
sified as a gene/protein by BANNER or the Se-
mantic or Priority Models. This often happens 
when the name indeed represents a gene/protein 
class, which is too general to be marked positive 
(Figure 2). A general way in which this problem 
could be approached is to process a large amount 
of literature discussing genes or proteins and look 
for names that are marked as positives by one of 
the NER identifiers, and which appear frequently 
in plural form as well as in the singular. Such 
names are likely general class names, and have a 
high probability to be false positives. 
Another type of error will arise when unseen to-
kens are encountered. If such tokens have string 
similarity to gene/protein names already encoun-
tered in the SemCat data, they may be recognized 
by the Priority Model. But there will be completely 
new strings. Then one must rely on context and 
this may not be adequate. We think there is little 
that can be done to solve this short of better lan-
guage understanding by computers. 
There is a benefit in considering whole sentenc-
es as opposed to named entities. By considering 
whole sentences, name boundaries become a non-
issue. For this reason, one can expect training data 
to be more accurate, i.e., human judges will tend to 
agree more in their judgments. This may allow for 
improved training and testing performance of ma-
190
chine learning methods. We believe it beneficial 
that human users are directed to sentences that con-
tain the entities they seek without necessity of 
viewing the less accurate entity specific tagging 
which they may then have to correct. 
5 Conclusions 
We defined a new task for classifying gene/protein 
sentences as an aid to human curation and infor-
mation retrieval. An ensemble approach was used 
to combine three different NER identifiers for im-
proved gene/protein sentence recognition. Our ex-
periments show that one can indeed find improved 
performance over a single NER identifier for this 
task. An additional advantage is that performance 
at this task is significantly more accurate than 
gene/protein NER. We believe this improved accu-
racy may benefit human users of this technology. 
We also make available to the research community 
three gold-standard gene mention sets, and two of 
these are taken from the most recent literature ap-
pearing in PubMed. 
Acknowledgments 
This work was supported by the Intramural Re-
search Program of the National Institutes of 
Health, National Library of Medicine. 
References  
R. B. Altman, C. M. Bergman, J. Blake, C. Blaschke, A. 
Cohen, F. Gannon, L. Grivell, U. Hahn, W. Hersh, L. 
Hirschman, L. J. Jensen, M. Krallinger, B. Mons, S. 
I. O'donoghue, M. C. Peitsch, D. Rebholz-
Schuhmann, H. Shatkay, and A. Valencia. 2008. Text 
mining for biology - the way forward: opinions from 
leading scientists. Genome Biol, 9 Suppl 2:S7. 
C. N. Arighi, Z. Lu, M. Krallinger, K. B. Cohen, W. J. 
Wilbur, A. Valencia, L. Hirschman, and C. H. Wu. 
2011. Overview of the BioCreative III workshop. 
BMC Bioinformatics, 12 Suppl 8:S1. 
J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically 
motivated large-scale NLP with C&C and boxer. In 
Proceedings of the 45th Annual Meeting of the ACL 
on Interactive Poster and Demonstration Sessions, 
pages 33-36. 
K. Franzen, G. Eriksson, F. Olsson, L. Asker, P. Liden, 
and J. Coster. 2002. Protein names and how to find 
them. Int J Med Inform, 67:49-61. 
Z. S. Harris. 1954. Distributional structure. Word, 
10:146-162. 
M. Huang, S. Ding, H. Wang, and X. Zhu. 2008. 
Mining physical protein-protein interactions from the 
literature. Genome Biol, 9 Suppl 2:S12. 
J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. 
GENIA corpus - semantically annotated corpus for 
bio-textmining. Bioinformatics, 19 Suppl 1:i180-
i182. 
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. 
Collier. 2004. Introduction to the bio-entity 
recognition task at JNLPBA. In Proceedings of the 
International Joint Workshop on Natural Language 
Processing in Biomedicine and its Applications, 
pages 70-75. 
S. Kim and W. J. Wilbur. 2011. Classifying protein-
protein interaction articles using word and syntactic 
features. BMC Bioinformatics, 12 Suppl 8:S9. 
R. Leaman and G. Gonzalez. 2008. BANNER: an 
executable survey of advances in biomedical named 
entity recognition. In Proceedings of the Pacific 
Symposium on Biocomputing, pages 652-663. 
R. Leaman, C. Miller, and G. Gonzalez. 2009. Enabling 
recognition of diseases in biomedical text with 
machine learning: corpus and benchmark. In 2009 
Symposium on Languages in Biology and Medicine. 
Z. Lu, H. Y. Kao, C. H. Wei, M. Huang, J. Liu, C. J. 
Kuo, C. N. Hsu, R. T. Tsai, H. J. Dai, N. Okazaki, H. 
C. Cho, M. Gerner, I. Solt, S. Agarwal, F. Liu, D. 
Vishnyakova, P. Ruch, M. Romacker, F. Rinaldi, S. 
Bhattacharya, P. Srinivasan, H. Liu, M. Torii, S. 
Matos, D. Campos, K. Verspoor, K. M. Livingston, 
and W. J. Wilbur. 2011. The gene normalization task 
in BioCreative III. BMC Bioinformatics, 12 Suppl 
8:S2. 
S. G. Nash and J. Nocedal. 1991. A numerical study of 
the limited memory BFGS method and the truncated-
Newton method for large scale optimization. SIAM 
Journal on Optimization, 1:358-372. 
P. Pantel and D. Lin. 2002. Discovering word senses 
from text. In Proceedings of the Eighth ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining, pages 613-619. 
L. Smith, L. K. Tanabe, R. J. Ando, C. J. Kuo, I. F. 
Chung, C. N. Hsu, Y. S. Lin, R. Klinger, C. M. 
Friedrich, K. Ganchev, M. Torii, H. Liu, B. Haddow, 
C. A. Struble, R. J. Povinelli, A. Vlachos, W. A. 
Baumgartner, Jr., L. Hunter, B. Carpenter, R. T. Tsai, 
H. J. Dai, F. Liu, Y. Chen, C. Sun, S. Katrenko, P. 
Adriaans, C. Blaschke, R. Torres, M. Neves, P. 
Nakov, A. Divoli, M. Mana-Lopez, J. Mata, and W. 
J. Wilbur. 2008. Overview of BioCreative II gene 
mention recognition. Genome Biol, 9 Suppl 2:S2. 
L. Tanabe, L. H. Thom, W. Matten, D. C. Comeau, and 
W. J. Wilbur. 2006. SemCat: semantically 
categorized entities for genomics. In AMIA Annu 
Symp Proc, pages 754-758. 
191
L. Tanabe and W. J. Wilbur. 2006. A priority model for 
named entities. In Proceedings of the Workshop on 
Linking Natural Language Processing and Biology: 
Towards Deeper Biological Literature Analysis, 
pages 33-40. 
L. Tanabe, N. Xie, L. H. Thom, W. Matten, and W. J. 
Wilbur. 2005. GENETAG: a tagged corpus for 
gene/protein named entity recognition. BMC 
Bioinformatics, 6 Suppl 1:S3. 
P. D. Turney and P. Pantel. 2010. From frequency to 
meaning: vector space models of semantics. Journal 
of Artificial Intelligence Research, 37:141-188. 
Z. Yang, H. Lin, and Y. Li. 2008. Exploiting the 
contextual cues for bio-entity name recognition in 
biomedical literature. J Biomed Inform, 41:580-587. 
 
 
192
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 35?44,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Extracting Biomedical Events and Modifications Using Subgraph
Matching with Noisy Training Data
Andrew MacKinlay?, David Martinez?, Antonio Jimeno Yepes?,
Haibin Liu?, W. John Wilbur? and Karin Verspoor?
? NICTA Victoria Research Laboratory, University of Melbourne, Australia
{andrew.mackinlay, david.martinez}@nicta.com.au
{antonio.jimeno, karin.verspoor}@nicta.com.au
? National Center for Biotechnology Information, Bethesda, MD, USA
haibin.liu@nih.gov, wilbur@ncbi.nlm.nih.gov
Abstract
The Genia Event (GE) extraction task of
the BioNLP Shared Task addresses the ex-
traction of biomedical events from the nat-
ural language text of the published litera-
ture. In our submission, we modified an
existing system for learning of event pat-
terns via dependency parse subgraphs to
utilise a more accurate parser and signifi-
cantly more, but noisier, training data. We
explore the impact of these two aspects of
the system and conclude that the change in
parser limits recall to an extent that cannot
be offset by the large quantities of training
data. However, our extensions of the sys-
tem to extract modification events shows
promise.
1 Introduction
In this paper, we describe our submission to the
Genia Event (GE) information extraction subtask
of the BioNLP Shared Task. This task requires the
development of systems that are capable of iden-
tifying bio-molecular events as those events are
expressed in full-text publications. The task rep-
resents an important contribution to the broader
problem of converting unstructured information
captured in the biomedical literature into struc-
tured information that can be used to index and
analyse bio-molecular relationships.
This year?s task builds on previous instantia-
tions of this task (Kim et al, 2009; Kim et al,
2012), with only minor changes in the task defini-
tion introduced for 2011. The task organisers pro-
vided full text publications annotated with men-
tions of biological entities including proteins and
genes, and asked participants to provide annota-
tions of simple events including gene expression,
binding, localization, and protein modification, as
well as higher-order regulation events (e.g., pos-
itive regulation of gene expression). In our sub-
mission, we built on a system originally developed
for the BioNLP-ST 2011 (Liu et al, 2011) and ex-
tended in more recent work (Liu et al, 2013a; Liu
et al, 2013b). This system learns to recognise sub-
graphs of syntactic dependency parse graphs that
express a given bio-molecular event, and matches
those subgraphs to new text using an algorithm
called Approximate Subgraph Matching.
Due to the method?s fundamental dependency
on the syntactic dependency parse of the text, in
this work we set out to explore the impact of
substituting the previously employed dependency
parsers with a different parser which has been
demonstrated to achieve higher performance than
other commonly used parsers for full-text biomed-
ical literature (Verspoor et al, 2012).
In addition, we aimed to address the relatively
lower recall of the method through incorporation
of large quantities of external training data, ac-
quired through integration of previously automat-
ically extracted bio-molecular events available in
a web repository of such extracted events, EVEX
(Van Landeghem et al, 2011; Van Landeghem
et al, 2012), and additional bio-molecular events
generated from a large sample of full text pub-
lications using one of the state-of-the-art event
extraction systems, TEES (Bjo?rne and Salakoski,
2011). Since the performance of the subgraph
matching method, as an instance-based learning
strategy (Alpaydin, 2004), is dependent on having
good training examples that express the events in a
range of syntactic structures, the motivation under-
lying this was to increase the amount of training
data available to the system, even if that data was
derived from a less-than-perfect source. The aug-
mentation of training corpora with external unla-
belled data that is automatically processed to gen-
erate additional labels has been explored for re-
training the same system, in an approach known as
self-training. This approach has been shown to be
35
very effective for improving parsing performance
(McClosky et al, 2006; McClosky and Charniak,
2008). Self-training of the TEES system has been
previously explored (Bjorne et al, 2012), with
somewhat mixed results, but with evidence sug-
gesting it could be useful with an appropriate strat-
egy for selecting training examples. Here, rather
than training our system with its own output over
external data, we explore a semi-supervised learn-
ing approach in which we train our system with the
outputs of a different system (TEES) over external
data.
2 Methodology
2.1 Base Event Extraction System
The event extraction algorithm is essentially the
same as the one used in Liu et al (2013b). A fuller
description can be found there, but we summarise
the most important aspects of it here.
2.1.1 Event Extraction with ASM
The principal method used in event extraction is
Approximate Subgraph Matching, or ASM (Liu et
al., 2013a). Broadly, we learn subgraph patterns
from the event structures in the training data, and
then apply them by looking for matches with the
patterns of the learned rules, using ASM to allow
for non-exact matches of the patterns.
The first stage in this is learning the rules which
link subgraphs to associated patterns. The input
is a set of dependency-parsed articles (the setup
is described in ?2.1.2), and a set of gold-standard
annotations of proteins and events in the shared
task format. Using the standoff annotations in the
training data, every protein and trigger is mapped
to one or more nodes in the corresponding depen-
dency graphs. In addition, the textual content of
every protein is replaced with a generic string en-
abling abstraction over individual protein names.
Then, for each event annotation in the training
data, we retrieve the nodes from the graph corre-
sponding to the associated trigger and protein en-
tities. We determine the shortest path (or paths, in
case of a tie) connecting the graph trigger to each
of the event argument nodes. For arguments which
are themselves events (e.g., for regulatory events),
the node corresponding to the trigger of the event
argument is used instead of a protein node. Where
there are multiple arguments, we take the union of
the shortest paths to each individual argument.
This path is then used as the pattern compo-
nent of an event rule. The rule also consists of an
event type, and a mapping from event arguments
to nodes from the pattern graph, or to an event
type/node pair for nested event arguments. Af-
ter processing all training documents, we get on
the order of a few thousand rules; this can be de-
creased slightly by removing rules with subgraphs
that are isomorphic to those of other rules.
In principle, this set of rules could then be di-
rectly applied to the test documents, by searching
for any matching subgraphs. However, in practice
doing so leads to very low recall, since the pat-
terns are not general enough to get a broad range of
matches on new data. We can alleviate this by re-
laxing the strictness of the subgraph matching pro-
cess. Most basically, we relax node matching. In-
stead of requiring an exact match between both the
token and the part-of-speech of the nodes of the
sentence graph and those from the rule subgraph,
we also allow a match on the basis of the lemma
(according to BioLemmatizer (Liu et al, 2012)),
and a coarse-grained POS-tag (where there is only
one POS-tag for nouns, verbs and adjectives).
More importantly, we also relax the require-
ments on how closely the graphs must match, by
using ASM. ASM defines distances measures be-
tween subgraphs, based on structure, edge labels
and edge directions, and uses a set of specified
weights to combine them into an overall subgraph
distance. We have a pre-configured set of distance
thresholds for each event type, and for each sen-
tence/rule pairing, we extract events for any rules
with subgraphs under the given threshold.
The problem with this approximate matching is
that some rules now match too broadly, and pre-
cision is reduced. This is mitigated by adding
an iterative optimisation phase. In each iteration,
we run the event extraction using the current rule
set over some dataset ? usually the training set,
or a subset of it. We check the contribution of
each rule in terms of postulated events and actual
events which match the gold standard. If the ra-
tio of matched to postulated events is too low (for
the work reported here, the threshold is 0.25), the
rule is discarded. This process is repeated until no
more rules are discarded. This can take multiple
iterations since the rules are interdependent due to
the presence of nested event arguments.
The optimisation step is by far the most time-
consuming step of our process, especially for the
large rule sets produced in some configurations.
36
We were able to improve optimisation times some-
what by parallelising the event extraction, and
temporarily removing documents with long ex-
traction times from the optimisation process un-
til as late as possible, but it remained the primary
bottleneck in our experimentation.
2.1.2 Parsing Pipeline
In our parsing pipeline, we first split sentences
using the JULIE Sentence Boundary Detector, or
JSBD (Tomanek et al, 2007). We then parse
using a version of clearnlp1 (Choi and McCal-
lum, 2013), a successor to ClearParser (Choi and
Palmer, 2011), which was shown to have state-
of-the-art performance over the CRAFT corpus
of full-text biomedical articles (Verspoor et al,
2012). We use dependency and POS-tagging mod-
els trained on the CRAFT corpus (except where
noted); these pre-trained models are provided with
clearnlp. Our fork of clearnlp integrates to-
ken span marking into the parsing process, so the
dependency nodes can easily be matched to the
standoff annotations provided with the shared task
data. This pipeline is not dependent on any pre-
annotated data, so can thus be trivially applied to
extra data not provided as part of the shared task.
In addition the parsing is fast, requiring roughly 46
wall-clock seconds (processing serially) to parse
the 5059 sentences from the training and develop-
ment sets of the 2013 GE task ? an average of 9 ms
per sentence. The ability to apply the same pars-
ing configuration to new text was useful for adding
extra training data, as discussed in ?2.2.
The usage of clearnlp as the parser is the pri-
mary point of difference between our system and
that of Liu et al (2013b), who use the Charniak-
Johnson parser with the McClosky biomedical
model (CJM; McClosky and Charniak (2008)), al-
though there are other minor differences in tokeni-
sation and sentence splitting. We expected that the
higher accuracy of clearnlp over biomedical text
would translate into increased accuracy of event
detection in the shared task; we consider this ques-
tion in some detail below.
2.2 Adding Noisy Training Data
One of the limitations of the ASM approach is that
the high precision comes at the cost of lower re-
call. Our hypothesis is that adding extra training
instances, even if some are errors, will raise re-
call and improve overall performance. We utilised
1https://code.google.com/p/clearnlp/
two sources of automatically-annotated data: the
EVEX database, and running an automatic event
annotator over documents from PubMed Central
(PMC) and MEDLINE.
To test our hypothesis, we utilise one of the
best performing automatic event extractors in pre-
vious BioNLP tasks: TEES (Turku Event Extrac-
tion System)2 (Bjo?rne et al, 2011). We expand our
pool of training examples by adding the highest-
confidence events TEES identifies in unlabelled
text. We explored different approaches to ranking
events based on classifier confidence empirically.
TEES relies on multi-class SVMs both for trig-
ger and event classification, and produces confi-
dence scores for each prediction. We explored
ranking events according to: (i) score of the trig-
ger prediction, (ii) score of the event-type predic-
tion, and (iii) sum of trigger and event type predic-
tions. We also compared the performance when
selecting the top-k events overall, versus choos-
ing the top-k events for each event type. We also
tested adding as many instances per event-type as
there were in the manually-annotated dataset, with
different multiplying factors. Finally, we evalu-
ated the effect of using different splits of the data
for the evaluation and optimisation steps of ASM.
This is the full list of parameters that we tested
over held-out data:
? Original confidence scores: we ranked events
according to the three SVM scores mentioned
above: trigger prediction, event-type predic-
tion, and combined.
? Overall top-k: we selected the top 1,000,
5,000, 10,000, 20,000, 30,000, 40,000, and
50,000 for the different experimental runs.
? Top-k per type: for each event type, we se-
lected the top 400, 1,000, and 2,000.
? Training bias per type: we add as many in-
stances from EVEX per type as there are in
the manually annotated data. We experiment
with adding up to 6 times as many as in man-
ually annotated data.
? Training/optimisation split: we combine
manually and automatically annotated data
for training. For optimisation we tested
different options: manually annotated only,
manual + automatic, manual + top-100
events, and manual + top-1000 events.
2http://jbjorne.github.com/TEES/
37
We did not explore all these settings exhaus-
tively due to time constraints, and we report here
the most promising settings. It is worth mention-
ing that most of the configurations contributed to
improve the baseline performance. We only ob-
served drops when using automatically-annotated
data in the optimisation step.
2.2.1 Data from EVEX
Conveniently, the developers of TEES have re-
leased the output of their tool over the full 2009
collection of MEDLINE, consisting of abstracts of
biomedical articles, in a collection known as the
EVEX dataset. We used the full EVEX dataset as
provided by the University of Turku, and explored
different ways of ranking the full list of events as
described above.
2.2.2 Data from TEES
To augment the training data, we annotated two
data sets with TEES based on MEDLINE and
PubMed Central (PMC). The developers of TEES
released a trained model for the GE 2013 training
data that we utilised.
Due to the long pre-processing time of TEES,
which includes gene named entity recognition,
part-of-speech tagging and parsing, we used the
EVEX pre-processed MEDLINE, which required
some adaptation of the EVEX XML to the XML
format accepted by TEES. Once this adaptation
was finished, the files were processed by TEES.
Then, we have selected articles from PMC us-
ing a query containing specific MeSH headings
related to the GE task and limiting the result to
only the Open Access part of PMC. From the al-
most 600k articles from the PMC Open Access set,
we reduced the total number of articles to around
155k. The PMC query is the following:
(Genetic Phenomena[MH] OR Metabolic
Phenomena[MH] OR Cell Physiological
Phenomena[MH] OR Biochemical
Processes[MH]) AND open access[filter]
Furthermore, the articles were split into sections
and specific sections from the full text like Intro-
duction, Background and Methods were removed
to reduce the quantity of text to be annotated by
TEES. The PMC files produced by this filtering
were processed by TEES on the NICTA cluster.
2.3 Modification Detection
To evaluate the utility of ASM for a diverse range
of tasks, we also applied it to the task of detect-
ing modification (SPECULATION or NEGATION)
NEGATION cues
? Basic: not, no, never, nor, only, neither, fail, cease,
stop, terminate, end, lacking, missing, absent, absence,
failure, negative, unlikely, without, lack, unable
? Data-derived: any, prevention, prevent, disrupt, dis-
ruption
SPECULATION cues:
? Basic: analysis, whether, may, should, can, could, un-
certain, questionable, possible, likely, probable, prob-
ably, possibly, conceivable, conceivably, perhaps, ad-
dress, analyze, analyse, assess, ask, compare, consider,
enquire, evaluate, examine, experiment, explore, inves-
tigate, test, research, study, speculate
? Data-derived: measure, measurement, suggest, sug-
gestion, value, quantify, quantification, determine, de-
termination, detect, detection, calculate, calculation
Table 1: Modification cues
of events. In event detection, triggers are explic-
itly annotated, so the linguistic cue which indi-
cates that an event is occurring is easy to identify.
As described in Section 3.2, these triggers are im-
portant for learning event patterns.
The event extraction method is based on paths
between dependency graph nodes, so it is neces-
sary to have at least two relevant graph nodes be-
fore we can determine a path between them. For
learning modification rules, one graph node is the
trigger of the event which is subjec to modifica-
tion. However here we needed a method to deter-
mine another node in the sentence which provided
evidence that NEGATION or SPECULATION was
occurring, and could thus form an endpoint for a
semantically relevant graph pattern. To achieve
this, we specified a set cue lemmas for NEGATION
and SPECULATION. The basic set of cue lemmas
came from a variety of sources. Some were man-
ually specified and some were derived from previ-
ous work on modification detection (Cohen et al,
2011; MacKinlay et al, 2012). We manually ex-
panded this cue list to include obvious derivational
variants. This gave us a basic set of 34 SPECULA-
TION and 21 NEGATION cues.
We also used a data-driven strategy to find ad-
ditional lemmas indicative of modification. We
adapted the method of Rayson and Garside (2000)
which uses log-likelihood for finding words that
characterise differences between corpora. Here,
the ?corpora? are sentences attached to all events
in the training set, and sentences attached to events
which are subject to NEGATION or SPECULATION
(treated separately). We build a frequency distri-
bution over lemmas in each set of sentences, and
calculate the log-likelihood for all lemmas, us-
38
ing the observed frequency from the modification
events and the expected frequency over all events.
Sorting by decreasing log-likelihood, we get a
list of lemmas which are most strongly associated
with NEGATION or SPECULATION. We manually
examined the highest-ranked lemmas from these
two lists and noted lemmas which may occur,
according to human judgment, in phrases which
would denote the relevant modification type. We
found seven extra SPECULATION cues and three
extra NEGATION cues. Expanding with morpho-
logical variants as described above yielded 47
SPECULATION cues and 26 NEGATION cues to-
tal. These cues are shown, divided into basic and
data-derived, in Table 1.
For every node N with a lemma in the appro-
priate set of cue lemmas, we create a rule based
on the shortest path between the cue lemma node
N and the event trigger node. The trigger lem-
mas are replaced with generic lemmas which only
reflect the POS-tag of the trigger, to broaden the
range of possible matches. Each rule thus consists
of the POS-tag of an event trigger, and a subgraph
pattern including the abstracted event trigger node.
At modification detection time, the rules are ap-
plied in a similar way to the event rules. After
detecting events, we look for matches of each ex-
tracted event with every modification rule. A rule
R is considered to match if the event trigger node
POS tag matches the POS tag of the rule, and the
subgraph pattern of the rule matches the graph of
the sentence, including a node corresponding to
the event trigger node. If R is found to match
for a given event and sentence, any events which
have the trigger defined in the rule are marked as
SPECULATION or NEGATION as appropriate. As
in event extraction, we use ASM to allow a looser
match between graphs, but initial experimentation
showed that increasing the match thresholds be-
yond a relatively small distance was detrimental.
We have not yet added an optimisation phase for
modification, which might allow larger ASM dis-
tance threshold to have more benefit.
3 Results
We present our results over development data,
and the official test. We report the Approximate
Span/Approximate Recursive metric in all our ta-
bles, for easy comparison of scores. We describe
the data split used for development, explain our
event extraction results, and finally describe our
performance in modification detection.
3.1 Data division for development
In the data provided by the task organisers, the
split of data between training and development
sets, with 249 and 222 article sections respec-
tively, was fairly even. If we had used such a split,
we would have had an unfeasibly small amount
of data to train from during development, and
possible unexpected effects when we sharply in-
creased the amount of training data for running
over the held-out test set. We instead used our
own data set split during development, pooling
the provided training and development sets, and
randomly selecting six PMC articles (PMC IDs
2626671, 2674207, 3062687, 3148254, 3333881
and 3359311) for the development set, with the
remainder available for training. We respected ar-
ticle boundaries in the new split to avoid training
and testing on sentences taken from different sec-
tions of the same article. Results over the devel-
opment set reported in this section are over this
data split. We will refer to our training subset as
GE13tr, and to the testing subset as GE13dev.
For our runs over the official test of this chal-
lenge, we merged all the manually annotated data
from 2013 to be used as training. We also per-
formed some experiments with adding the exam-
ples from the 2011 GE task to our training data.
3.2 Event Extraction
For our first experiment, we evaluated the contri-
bution of the automatically annotated data over us-
ing GE13tr data only. We performed a set of ex-
periments to explore the parameters described in
Section 2.2 over two sources of extra examples:
EVEX and TEES.
Using EVEX data in training resulted in clear
improvements in performance when only manu-
ally annotated data was consulted for optimisa-
tion. The increase was mainly due to the better
recall, with small variations in precision over the
baseline for the majority of experiments. Our best
run over the GE13dev data followed this setting:
rank events according to trigger scores, include all
top-30000 events (without considering the types of
the events), and use only manually annotated data
for the optimisation step. Other settings also per-
formed well, as we will see below.
For TEES, we selected noisy examples from
MEDLINE and PMC to be used as additional
39
System Prec. Rec. F-sc.
GE13tr 60.40 27.02 37.34
+TEES 59.27 29.89 39.74
+TEES +EVEX (top5k) 46.93 30.78 37.18
+TEES +EVEX (top20k) 56.32 31.90 40.73
+TEES +EVEX (top30k) 55.34 32.48 40.93
+TEES +EVEX (pt1k) 58.54 30.96 40.50
+TEES +EVEX (trx4) 57.83 31.23 40.56
Table 2: Impact of adding extra training data to the
ASM method. top5k,20k,30k: using the top 5,000,
20,000, and 30,000 events. pt1k: using the top
1,000 events per event-type. trx4: following the
training bias of events, with a multiplying factor
of four. For TEES we always use the top 10,000
events. Evaluated over GE13dev.
training data. Initial results showed that when us-
ing only MEDLINE annotated data in the train-
ing step, the performance decreased compared to
not using any additional data. This might have
been due to differences between the EVEX pre-
processed data that we used and what TEES was
expecting, so the MEDLINE set was not consid-
ered for further experimentation. Using PMC ar-
ticles annotated with TEES in the training step se-
lected by the evidence score of TEES shows an in-
crease of recall while slightly decreasing the pre-
cision, which was expected. We selected the top
10000 events from the PMC set based on the evi-
dence score as additional training data.
Table 2 summarises the results of combin-
ing different settings of EVEX with TEES. We
achieve a considerable boost in recall, at the cost
of precision for most configurations. The only set-
ting where there is a slight drop in F-score is the
experiment with only 5000 events from EVEX; in
the remaining runs we are able to alleviate the drop
in precision, and improve the F-score. Consider-
ing the addition of top-events according to their
type, the increment in recall is slightly lower, but
these runs are able to reach similar F-score to the
best ones, using less training data. Results with
TEES might be slightly overoptimistic since the
PMC annotation is based on a TEES model trained
on the 2013 GE data and our configurations are
evaluated on a subset of this data.
For our next experiment, we tested the contribu-
tion of adding the dataset from the 2011 GE task
to the training dataset. We use this data both in
the training and optimisation steps. The results are
Train Prec. Rec. F-sc.
GE13tr 60.40 27.02 37.34
+GE11 53.41 32.62 40.50
Table 3: Adding GE11 data to the training and op-
timisation steps. Evaluated over GE13dev.
Parser Train Prec. Rec. F-sc.
clearnlp
GE13 60.40 27.02 37.34
+GE11 53.41 32.62 40.50
CJM
GE13 60.96 33.11 42.91
+GE11 64.11 38.93 48.44
Table 4: Performance depending on the applied
parsing pipeline (clearnlp for this work against
the CJM pipeline of Liu et al (2013b)) over
GE13dev. For each run, the available data was
used both in training and optimisation.
given in Table 3, where we can observe a boost in
recall at the cost of precision. Overall, the im-
proved F-score suggests that this dataset would
make a useful contribution to the system.
We also compared our system to that of Liu
et al (2013b), where the primary difference
(although not the only difference, as noted in
?2.1.2) is the use of clearnlp instead of the CJM
(Charniak-Johnson/McClosky) pipeline. It is thus
somewhat surprising to see in Table 4 that the
CJM pipeline outperforms our clearnlp pipeline
by 5.5?8% in F-score, depending on the train-
ing data. For the smaller GE13-only training set,
the gap is smaller, and the precision figures are
in fact comparable. However, the recall is uni-
formly lower, suggesting that the rules learned
from clearnlp parses are for some reason less gen-
erally applicable. Another interesting difference
is that our clearnlp pipeline gets a smaller benefit
from the addition of the GE11 training data. We
consider possible reasons for this in ?4.1.
Table 5 contains the evaluation of different ex-
periments on the official test data. We tested the
baseline system using the training and develop-
ment data from 2011 and 2013 GE tasks and the
addition of TEES and EVEX data. The additional
data improves the recall slightly compared to not
using it, while, as expected, it decreases the pre-
cision. Table 5 also shows the results for our of-
ficial submission (+TEES+EVEX sub), which due
to time constraints was a combination of the opti-
mised rules of different data splits and has a lower
40
Train Prec. Rec. F-sc.
GE11, GE13 65.71 32.57 43.55
+TEES+EVEX 63.67 33.50 43.91
+TEES+EVEX * 50.68 36.99 42.77
Table 5: Test set results, always optimised over
gold data only. * denotes the official submission.
performance compared to the other results.
3.3 Modification Detection
We show results for selected modification detec-
tion experiments in Table 6. In all cases we used
all of the available gold training data from the
GE11 and GE13 datasets. To assess the impact of
modification cues, we show results using the basic
set as well as with the addition of the data-derived
cues. It has often been noted (MacKinlay et al,
2012; Cohen et al, 2011) that modification detec-
tion accuracy is strongly dependent on the quality
of the upstream event annotation, so we provide an
oracle evaluation, using gold-standard event anno-
tations rather than automatic output.
The performance over the automatically-
annotated runs is respectable, given that the recall
is fundamentally limited by the recall of the input
event annotations, which is only around 30% for
the configurations shown. With the oracle event
annotations, the results improve substantially,
with considerable gains in precision, and recall
increasing by a factor of 4?6. This boost in recall
in particular is more than we would naively expect
from the roughly threefold increase in recall over
the events. It seems that many of the modification
rules we learned were even more effective over
events which our pipeline was unable to detect.
The modification rules were learned from oracle
event data, but this does not fully explain the
discrepancy. Regardless, our algorithm for mod-
ification detection showed excellent performance
over the oracle annotations. Over the 2009 version
of the BioNLP shared task data, MacKinlay et al
(2012) report F-scores of 54.6% for NEGATION
and 51.7% for SPECULATION. These are not
directly comparable with those in Table 6, but
running our newer algorithm over the same 2009
data gives F-scores of 84.2% for NEGATION and
69.1% for SPECULATION.
For the official run, which conflates event
extraction and modification detection accuracy,
our system was ranked third for NEGATION and
SPECULATION out of the three competing teams,
although the other teams had event extraction F-
scores of roughly 8% higher than our system. For
SPECULATION, our system had the highest preci-
sion of 34.15%, while the F-score of 20.22% was
close to the best result of 23.92%. Our NEGA-
TION detection was less competitive, with an F-
score of 20.94% ? roughly 6% lower than the other
teams. We cannot extrapolate directly from the or-
acle evaluation in Table 6, but it seems to indicate
that an increase in event extraction accuracy would
have flow-on benefits in modification detection.
4 Discussion
4.1 Detrimental Effects of Parser Choice
The biggest surprise here was that clearnlp, a
more accurate dependency parser for the biomed-
ical domain, as evaluated on the CRAFT tree-
bank, gave a substantially lower event extrac-
tion F-score than the CJM parser. To determine
whether preprocessing caused the differences, we
replaced the existing modules (sentence-splitting
from JSBD and tokenisation/POS-tagging from
clearnlp) with the BioC-derived versions from the
CJM pipeline, but this yielded only an insignifi-
cant decrease in accuracy.
Over the same training data, the optimised rules
from CJM have an average of 2.6 nodes per sub-
graph path, compared to 3.9 nodes per path using
clearnlp. A longer path is less likely to match
than a shorter path, so this may help to explain
the lower generalisability of the clearnlp-derived
rules. While it is possible for a longer subgraph
to match just as generally, if the test sentences
are parsed consistently, in general there are more
nodes and edges which can fail to match due to mi-
nor surface variations. One way to mitigate this is
to raise the ASM distance thresholds to compen-
sate for this; preliminary experiments suggest it
would provide a small (? 1%) boost in F-score but
this would not close the gap between the parsers.
Both parsers produce outputs with Stanford
Dependency labels (de Marneffe and Manning,
2008), so we might naively expect similar graph
topology and subgraph pattern lengths. However,
the CJM pipeline produces graphs in the ?CCpro-
cessed? SD format, which are simpler and denser.
If a node N has a link to a node O with a conjunc-
tion link to another node P (from e.g. and), an ex-
tra link with the same label is added directly from
N to P in the CCprocessed format. This means
41
NEGATION SPECULATION
Eval Events (F-sc) Cues P / R / F P / R / F
Dev
GE13+TEES+EVEX (40.93) Basic 32.69 / 13.71 / 19.32 37.04 / 14.49 / 20.83
GE13+TEES+EVEX (40.93) B + Data 32.69 / 12.88 / 18.48 39.71 / 17.20 / 24.00
Oracle (100.0) B + Data 82.48 / 71.07 / 76.35 78.79 / 67.71 / 72.83
Test
GE11+GE13 (43.55) B + Data 39.53 / 13.99 / 20.66 50.00 / 13.85 / 21.69
GE11+GE13+TEES+EVEX * (42.77) B + Data 32.76 / 15.38 / 20.94 34.15 / 14.36 / 20.22
Table 6: Results for SPECULATION and NEGATION using automatically-annotated events (showing the
F-score of the configuration), as well as using oracle event annotations from the gold standard, over our
development set and the official test set. Rules are learned from GE13+GE11 gold data (excluding any
test data). Cues for learning rules are either the basic manually-specified set (34 SPEC/21 NEG) or the
augmented set with data-driven additions (47 SPEC/26 NEG). * denotes the official submission.
there are more direct links in the graph, match-
ing the semantics more closely. The shortest path
fromN to P is now direct, instead of viaO, which
could enable the CJM pipeline to produce more
general rules.
To evaluate how much this detrimentally af-
fects the clearnlp pipeline, as a post hoc in-
vestigation, we implemented a conversion mod-
ule. Using Stanford Dependency parser code,
we replicated the CCprocessed conversion on the
clearnlp graphs, reducing the average subgraph
pattern length to 2.8, and slightly improving ac-
curacy. Over our development set, compared to
the results in Table 3 it gave a 0.7% absolute F-
score boost over using GE13 training-data only,
and 1.1% over using GE11 and GE13 training data
(in both cases improving recall). Over the test
set, the improvement was greater, with a P/R/F
of 35.66/64.99/46.05, a 2.5% increase in F-score
compared to the results in Table 5 and only 2.9%
less than the official Liu et al (2012) submission.
Clearly some of the inter-parser discrepancies
are due to surface features and post-processing,
and as noted above, we can also achieve small im-
provements by relaxing ASM thresholds, so some
problems may be caused by the default parameters
being suboptimal for the parser. However, the ac-
curacy is still lower where we would expect it to
be higher, and this remaining discrepancy is diffi-
cult to explain without performing a detailed error
analysis, which we leave for future work.
4.2 Effect of additional data
Our initial intuition that using additional noisy
training data during the training of the system
would improve the performance is supported by
the results in Table 2. Table 3 shows that us-
ing a larger set of manually annotated data based
on 2011 GE task data also improves performance.
However, these tables also indicate that adding
manually annotated data produces an increase in
performance comparable to adding the noisy data,
despite its smaller size, and when using this man-
ually annotated set together with the noisy data,
the improvement resulting from the noisy data is
smaller (Table 5). Noisy data was only used dur-
ing training, which limits its effectiveness?any
rule extracted from automatically acquired anno-
tations that are not seen during optimisation of the
rule set will have a lower weight. On the other
hand, we found that using noisy data for optimi-
sation seemed to decrease performance. Together,
these results suggest that studying strategies, pos-
sibly self-training, for selection of events from the
noisy data to be used during rule set optimisation
in the ASM method are warranted.
5 Conclusion
Using additional training data, whether manually
annotated or noisy, improves the performance of
our baseline event extraction system. The gains
that we achieved by adding training data, however,
were outweighed by a loss of performance due to
our parser substitution, with longer dependency
subgraphs limiting rule generalisability the most
likely explanation. Our experiments demonstrate
that while a given parser might be ?better? in one
evaluation context, that advantage may not trans-
late to improved performance in a downstream
task that depends strongly on the parser output.
We presented an extension of the subgraph match-
ing methodology to extract modification events
which, when based on a good core event extrac-
tion system, shows very promising results.
42
Acknowledgments
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and
the Australian Research Council through the ICT
Centre of Excellence program. This research was
supported in part by the Intramural Research Pro-
gram of the NIH, NLM.
References
Ethem Alpaydin. 2004. Introduction to Machine
Learning. MIT Press.
Jari Bjo?rne and T. Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 183?
191.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2011. Ex-
tracting contextualized complex biological events
with rich graph-based features sets. Computational
Intelligence, 27(4):541?557.
Jari Bjorne, Filip Ginter, and Tapio Salakoski. 2012.
University of turku in the bionlp?11 shared task.
BMC Bioinformatics, 13(Suppl 11):S4.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, Sofia, Bulgaria.
Jinho D. Choi and Martha Palmer. 2011. Getting the
most out of transition-based dependency parsing. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 687?692, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
K.B. Cohen, K. Verspoor, H.L. Johnson, C. Roeder,
P.V. Ogren, W.A. Baumgartner, E. White, H. Tip-
ney, and L. Hunter. 2011. High-precision biological
event extraction: Effects of system and data. Com-
putational Intelligence, 27(4):681701, November.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In CrossParser ?08: Coling 2008: Pro-
ceedings of the workshop on Cross-Framework and
Cross-Domain Parser Evaluation, pages 1?8, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
J.D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsu-
jii. 2009. Overview of bionlp?09 shared task on
event extraction. Proceedings of Natural Language
Processing in Biomedicine (BioNLP) NAACL 2009
Workshop, pages 1?9.
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsu-
jii, Toshihisa Takagi, and Akinori Yonezawa. 2012.
The genia event and protein coreference tasks of
the bionlp shared task 2011. BMC Bioinformatics,
13(Suppl 11):S1.
H. Liu, R. Komandur, and K. Verspoor. 2011. From
graphs to events: A subgraph matching approach for
information eextraction from biomedical text. ACL
HLT 2011, page 164.
Haibin Liu, Tom Christiansen, William Baumgartner,
and Karin Verspoor. 2012. Biolemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of Biomedical Semantics,
3(1):3.
Haibin Liu, Lawrence Hunter, Vlado Keselj, and Karin
Verspoor. 2013a. Approximate subgraph matching-
based literature mining for biomedical events and re-
lations. PLoS ONE, 8(4):e60954, 04.
Haibin Liu, Karin Verspoor, Don Comeau, Andrew
MacKinlay, and W. John Wilbur. 2013b. General-
izing an approximate subgraph matching-based sys-
tem to extract events in molecular biology and can-
cer genetics. In Proceedings of the 2013 BioNLP
Workshop Companion Volume for the Shared Task.
Andrew MacKinlay, David Martinez, and Timo-
thy Baldwin. 2012. Detecting modification of
biomedical events using a deep parsing approach.
BMC Medical Informatics and Decision Making,
12(Suppl 1):S4.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
the Association for Computational Linguistics (ACL
2008, short papers).
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
conference of the North American chapter of the
ACL, pages 152?159.
Paul Rayson and Roger Garside. 2000. Comparing
corpora using frequency profiling. In The Workshop
on Comparing Corpora, pages 1?6, Hong Kong,
China, October. Association for Computational Lin-
guistics.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Sentence and token splitting based on con-
ditional random fields. In Proceedings of the 10th
Conference of the Pacific Association for Compu-
tational Linguistics, pages 49?57, Melbourne, Aus-
tralia.
S. Van Landeghem, F. Ginter, Y. Van de Peer, and
T. Salakoski. 2011. EVEX: A pubmed-scale re-
source for homology-based generalization of text
mining predictions. In Proceedings of BioNLP 2011
Workshop, pages 28?37.
43
S. Van Landeghem, K. Hakala, S. Ro?nnqvist,
T. Salakoski, Y. Van de Peer, and F. Ginter. 2012.
Exploring biomolecular literature with EVEX: Con-
necting genes through events, homology and indirect
associations. Advances in Bioinformatics, Special
issue Literature-Mining Solutions for Life Science
Research:ID 582765.
Karin Verspoor, K. Bretonnel Cohen, Arrick Lan-
franchi, Colin Warner, Helen L. Johnson, Christophe
Roeder, Jinho D. Choi, Christopher Funk, Yuriy
Malenkiy, Miriam Eckert, Nianwen Xue, William
A. Baumgartner Jr., Michael Bada, Martha Palmer, ,
and Lawrence E. Hunter. 2012. A corpus of full-text
journal articles is a robust evaluation tool for reveal-
ing differences in performance of biomedical natural
language processing tools. BMC Bioinformatics.
44
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 76?85,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Generalizing an Approximate Subgraph Matching-based System to Extract
Events in Molecular Biology and Cancer Genetics
Haibin Liu
haibin.liu@nih.gov
NCBI, Bethesda, MD, USA
Karin Verspoor
karin.verspoor@nicta.com.au
NICTA, Melbourne, VIC, Australia
Donald C. Comeau
comeau@ncbi.nlm.nih.gov
NCBI, Bethesda, MD, USA
Andrew MacKinlay
andrew.mackinlay@nicta.com.au
NICTA, Melbourne, VIC, Australia
W. John Wilbur
wilbur@ncbi.nlm.nih.gov
NCBI, Bethesda, MD, USA
Abstract
We participated in the BioNLP 2013 shared
tasks, addressing the GENIA (GE) and the Can-
cer Genetics (CG) event extraction tasks. Our
event extraction is based on the system we re-
cently proposed for mining relations and events
involving genes or proteins in the biomedical
literature using a novel, approximate subgraph
matching-based approach. In addition to han-
dling the GE task involving 13 event types uni-
formly related to molecular biology, we gener-
alized our system to address the CG task tar-
geting a challenging set of 40 event types re-
lated to cancer biology with various arguments
involving 18 kinds of biological entities. More-
over, we attempted to integrate a distributional
similarity model into our system to extend the
graph matching scheme for more events. In ad-
dition, we evaluated the impact of using paths of
all possible lengths among event participants as
key contextual dependencies to extract potential
events as compared to using only the shortest
paths within the framework of our system.
We achieved a 46.38% F-score in the CG task
and a 48.93% F-score in the GE task, ranking
3rd and 4th respectively. The consistent perfor-
mance confirms that our system generalizes well
to various event extraction tasks and scales to
handle a large number of event and entity types.
1 Introduction
Understanding the sophisticated interactions between
various components of biological systems and conse-
quences of these biological processes on the function
and behavior of the systems provides profound im-
pacts on translational biomedical research, leading to
more rapid development of new therapeutics and vac-
cines for combating diseases. For the past five years,
the BioNLP shared task series has served as an in-
strumental platform to promote the development of
text mining methodologies and resources for the au-
tomatic extraction of semantic events involving genes
or proteins such as gene expression, binding, or reg-
ulatory events from the biomedical literature (Kim et
al., 2009; Kim et al, 2011). An event typically cap-
tures the association of multiple participants of vary-
ing numbers and with diverse semantic roles (Anani-
adou et al, 2010). Since events often serve as partic-
ipants in other events, the extraction of such nested
event structures provides an integrated, network view
of these biological processes.
Previous shared tasks focused exclusively on
events at the molecular and sub-cellular level. How-
ever, biological processes at higher levels of organi-
zation are equally important, such as cell prolifer-
ation, organ growth and blood vessel development.
While preserving the classic event extraction tasks
such as the GE task, the BioNLP-ST 2013 broad-
ens the scope of application domains by introducing
many new issues in biology such as cancer genetics
and pathway curation. On behalf of NCBI (National
Center for Biotechnology Information), our team par-
ticipated in the GENIA (GE) task and the Cancer Ge-
netics (CG) task. Compared to the GE task that aims
for 13 types of events concerning the protein NF-?B,
the CG task targets a challenging set of 40 types of
biological processes related to the development and
progression of cancer involving 18 entity types. This
additionally requires that event extraction systems be
able to associate entities and events at the molecular
level with anatomy level effects and organism level
outcomes of cancer biology.
Our event extraction is based on the system we re-
cently proposed for mining relations and events in-
volving genes or proteins in the biomedical litera-
ture using a novel, Approximate Subgraph Matching-
based (ASM) approach (Liu et al, 2013a). When
evaluated on the GE task of the BioNLP-ST 2011, its
performance is comparable to the top systems in ex-
tracting 9 types of biological events. In the BioNLP-
76
ST 2013, we generalized our system to investigate
both CG and GE tasks. Moreover, we attempted to in-
tegrate a distributional similarity model into the sys-
tem to extend the graph matching scheme for more
events. The graph representation that considers paths
of all possible lengths (all-paths) between any two
nodes has been encoded in graph kernels used in
conjunction with Support Vector Machines (SVM),
and led to state-of-the-art performance in extracting
protein-protein (Airola et al, 2008) and drug-drug in-
teractions (Zhang et al, 2012). Borrowing from the
idea of the all-paths representation, in addition, we
evaluated the impact of using all-paths among event
participants as key contextual dependencies to extract
potential events as compared to using only the short-
est paths within the framework of our system.
The rest of the paper is organized as follows: In
Section 2, we briefly introduce our ASM-based event
extraction system. Section 3 describes our experi-
ments aiming to extend our system. Section 4 elab-
orates some implementation details and Section 5
presents our results and discussion. Finally, Section
6 summarizes the paper and introduces future work.
2 ASM-based Event Extraction
The underlying assumption of our event extraction
approach is that the contextual dependencies of each
stated biological event represent a typical context for
such events in the biomedical literature. Our ap-
proach falls into the machine learning category of
instance-based reasoning (Alpaydin, 2004). Specif-
ically, the key contextual structures are learned from
each labeled positive instance in a set of train-
ing data and maintained as event rules in the form
of subgraphs. Extraction of events is performed
by searching for an approximate subgraph isomor-
phism between key dependencies and input sen-
tence graphs using an approximate subgraph match-
ing (ASM) algorithm designed for literature-based
relational knowledge extraction (Liu et al, 2013a).
By introducing error tolerance into the graph match-
ing process, our approach is capable of retrieving
events encoded within complex dependency contexts
while maintaining the extraction precision at a high
level. The ASM algorithm has been released as open
source software1. See (Liu et al, 2013a) for more de-
tails on the ASM algorithm, its complexity and the
comparison with existing graph distance metrics.
Figure 1 illustrates the overall architecture of our
ASM-based system with three core components high-
1http://asmalgorithm.sourceforge.net
lighted: rule induction, sentence matching and rule
set optimization. Our approach focuses on extract-
ing events expressed within the boundaries of a single
sentence. It is also assumed that entities involved in
the target event have been annotated. Next, we briefly
describe the core components of the system.
Rule Induction
Preprocessing
Sentence Matching
Postprocessing
Training data Testing data
Rule Set
Optimization
Figure 1: ASM-based Event Extraction Framework
2.1 Rule Induction
Event rules are learned automatically using the fol-
lowing method. Starting with the dependency graph
of each training sentence, for each annotated event,
the shortest dependency path connecting the event
trigger to each event argument in the undirected ver-
sion of the graph is selected. While additional in-
formation such as individual words in each sentence
(bag-of-words), sequences of words (n-grams) and
semantic concepts is typically used in the state-of-
the-art supervised learning-based systems to cover a
broader context (Airola et al, 2008; Buyko et al,
2009; Bjo?rne et al, 2012), the shortest path be-
tween two tokens in the dependency graph is par-
ticularly likely to carry the most valuable informa-
tion about their mutual relationship (Bunescu and
Mooney, 2005a; Thomas et al, 2011b; Rinaldi et
al., 2010). In case there exists more than one short-
est path, all of them are considered. For multi-token
event triggers, the shortest path connecting every trig-
ger token to each event argument is extracted, and the
union of the paths is then computed for each trigger.
For regulatory events that take a sub-event as an ar-
gument, the shortest path is extracted so as to connect
the trigger of the main event to that of the sub-event.
For complex events that involve multiple argu-
ments, we computed the dependency path union of
all shortest paths from trigger to each event argument,
resulting in a graph in which all event participants are
jointly depicted. Individual dependency paths con-
necting triggers to each argument are also considered
to determine event arguments independently. If the
77
resulting arguments share the same event trigger, they
are grouped together to form a potential event. In our
approach, the individual paths aim to retrieve more
potential events while the path unions retain the pre-
cision advantage of joint inference.
While the dependencies of such paths are used as
the graph representation of the event, a detailed de-
scription records the participants of the event, their
semantic role labels and the associated nodes in the
graph. All participating biological entities are re-
placed with a tag denoting their entity type, e.g. ?Pro-
tein? or ?Organism?, to ensure generalization of the
learned rules. As a result, each annotated event is
generalized and transformed into a generic graph-
based rule. The resulting event rules are categorized
into different target event types.
2.2 Sentence Matching
Event extraction is achieved by matching the induced
rules to each testing sentence and applying the de-
scriptions of rule tokens (e.g. role labels) to the cor-
responding sentence tokens. Since rules and sentence
parses all possess a graph representation, event recog-
nition becomes a subgraph matching problem. We
introduced a novel approximate subgraph matching
(ASM) algorithm (Liu et al, 2013a) to identify a sub-
graph isomorphic to a rule graph within the graph of
a testing sentence. The ASM problem is defined as
follows.
Definition 1. An event rule graph Gr =
(Vr, Er) is approximately isomorphic to a subgraph
Ss of a sentence graph Gs = (Vs, Es), denoted
by Gr ?=t Ss ? Gs, if there is an injective
mapping f : Vr ? Vs such that, for a given
threshold t, t ? 0, the subgraph distance be-
tween Gr and Gs satisfies 0 ? subgraphDistf (Gr,
Gs) ? t, where subgraphDistf (Gr, Gs) = ws ?
structDistf (Gr, Gs) + wl ? labelDistf (Gr, Gs) +
wd ? directionalityDistf (Gr, Gs).
The subgraph distance is proposed to be the
weighted summation of three penalty-based measures
for a candidate match between the two graphs. The
measure structDist compares the distance between
each pair of matched nodes in one graph to the
distance between corresponding nodes in the other
graph, and accumulates the structural differences.
The distance in rule graphs is defined as the length
of the shortest path between two nodes. The distance
in sentence graphs is defined as the length of the path
between corresponding nodes that leads to minimum
structural difference with the distance in rule graphs.
Because dependency graphs are edge-labeled, ori-
ented graphs, the measures labelDist and direction-
alityDist evaluate respectively the overall differences
in edge labels and directionalities on the compared
path between each pair of matched nodes in the two
graphs. The real numbers ws, wl and wd are non-
negative weights associated with the measures.
The weights ws, wl and wd are defaulted to be
equal but can be tuned to change the emphasis of the
overall distance function. The distance threshold t
controls the isomorphism quality of the retrieved sub-
graphs from sentences. A smaller t allows only lim-
ited variations and always looks for a sentence sub-
graph as closely isomorphic to the rule graph as pos-
sible. A larger t enables the extraction of events de-
scribed in complicated dependency contexts, thus in-
creasing the chance of retrieving more events. How-
ever, it can incur a bigger search cost due to the eval-
uation of more potential solutions.
An iterative, bottom-up matching process is used
to ensure the extraction of complex and nested events.
Starting with the extraction of simple events, simple
event rules are first matched with a testing sentence.
Next, as potential arguments of higher level events,
obtained simple events continue to participate in the
subsequent matching process between complex event
rules and the sentence to initiate the iterative process
for detecting complex events with nested structures.
The process terminates when no new candidate event
is generated for the testing sentence.
During the matching phase we relax the event
rules that contain sub-event arguments such that any
matched event can substitute for the sub-event. We
believe that the contextual structures linking anno-
tated sub-events of a certain type are generalizable
to other event types. This relaxation increases the
chance of extracting complex events with nested
structures but still takes advantage of the contextual
constraints encoded in the rule graphs.
2.3 Rule Set Optimization
Typical of instance-based reasoners, the accuracy of
rules with which to compare an unseen sentence is
crucial to the success of our approach. For instance, a
Transcription rule encoding a noun compound mod-
ification dependency between ?TNF? and ?mRNA?
derived from an event context ?expression of TNF
mRNA? should not produce a Transcription event
for the general phrase ?level of TNF mRNA? even
though they share a matchable dependency. Such
matches result in false positive events.
78
Therefore, we measured the accuracy of each rule
ri in terms of its prediction result via Eq.(1). For rules
that produce at least one prediction, we ranked them
byAcc(ri) and excluded the ones with aAcc(ri) ratio
lower than an empirical threshold, e.g. 1:4.
Acc(ri) =
#correct predictions by ri
#total predictions by ri
(1)
Because of nested event structures, the removal
of some rules might incur a propagating effect on
rules relying on them to produce arguments for the
extraction of higher order events. Therefore, an it-
erative rule set optimization process, in which each
iteration performs sentence matching, rule ranking
and rule removal sequentially, is conducted, lead-
ing to a converged, optimized rule set. While the
ASM algorithm aims to extract more potential events,
this performance-based evaluation component en-
sures the precision of our event extraction framework.
3 Extensions to Event Extraction System
In the BioNLP-ST 2013, we attempted two different
ways to extend the current event extraction system:
(1) integrate a distributional similarity model into the
system to extend the graph matching scheme for more
events; (2) use paths of all possible lengths (all-paths)
among event participants as key contextual depen-
dencies to extract events. We next elaborate these
system extensions in detail.
3.1 Integrating Distributional Similarity Model
The proposed subgraph distance measure of the ASM
algorithm focuses on capturing differences in the
overall graph structure, edge labels and directional-
ities. However, when determining the injective node
mapping between graphs, the matching remains at the
surface word level.
In the current setting, various node features can be
considered when comparing two graph nodes, result-
ing in different matching criteria. The features in-
clude POS tags (P), event trigger (T), token lemmas
(L) and tokens themselves (A). For instance, a match-
ing criterion, ?P*+L?, requires that the relaxed POS
tags (P*) and the lemmatized form (L) of tokens be
identical for each rule node to match with a sentence
node. The relaxed POS allows the plural form of
nouns to match with the singular form, and the con-
jugations of verbs to match with each other. How-
ever, the inability to go beyond surface level match-
ing prevents node tokens that share similar meaning
but possess distinct orthography from matching with
each other. For instance, a mismatch between rule
token ?crucial? and a sentence token ?critical? could
lead to an undiscovered Positive regulation event.
We attempted to use only POS information in the
node matching scheme and observed a nearly 14%
increase in recall (Liu et al, 2013b). However, the
precision drops sharply, resulting in an undesirable
F-score. This indicates that the lexical information
is a critical supplement to the contextual dependency
constraints in accurately capturing events within the
framework of our system. Moreover, we attempted to
extend the node matching using the synsets of Word-
Net (Fellbaum, 1998) to allow tokens to match with
their synonyms (Liu et al, 2011). However, since
WordNet is developed for the general English lan-
guage, it relates biomedical terms e.g., ?expression?
with general words such as ?aspect? and ?face?, thus
leading to incorrect events.
In this work, we integrated a distributional simi-
larity model (DSM) into our node matching scheme
to further improve the generalization of event rules.
A distributional similarity model is constructed
based on the distributional hypothesis (Harris, 1954):
words that occur in the same contexts tend to share
similar meanings. We expect that the incorporation
of DSM will enable our system to capture matching
tokens in testing sentences that do not appear in the
training data while maintaining the extraction pre-
cision at a high level. There have been many ap-
proaches to compute the similarity between words
based on their distribution in a corpus (Landauer and
Dumais, 1997; Pantel and Lin, 2002). The output is a
ranked list of similar words to each word. We reim-
plemented the model proposed by (Pantel and Lin,
2002) in which each word is represented by a fea-
ture vector and each feature corresponds to a context
where the word appears. The value of the feature
is the pointwise mutual information (Manning and
Schu?tze, 1999) between the feature and the word. Let
c be a context and Fc(w) be the frequency count of a
word w occurring in context c. The pointwise mutual
information, miw,c between c and w is defined as:
miw,c =
Fc(w)
N?
i
Fi(w)
N ?
?
j
Fc(j)
N
(2)
where N =
?
i
?
j
Fi(j) is the total frequency count
of all words and their contexts.
Since mutual information is known to be biased
towards infrequent words/features, the above mutual
79
information value is multiplied by a discounting fac-
tor as described in (Pantel and Lin, 2002). The simi-
larity between two words is then computed using the
cosine coefficient (Salton and McGill, 1986) of their
mutual information vectors.
We experimented with two different approaches to
integrate the DSM into our event extraction system.
First, the model is directly embedded into the node
matching scheme. Once a match cannot be deter-
mined by surface tokens, the DSM is invoked to allow
a match if the sentence token appears in the list of the
top M most similar words to the rule token. Sec-
ond, additional event rules are generated by replac-
ing corresponding rule tokens with their top M most
similar words, rather than allow DSM to participate
in the node matching. While the first method mea-
sures the consolidated extraction ability of an event
rule by combining its DSM-generalized performance,
the second approach provides a chance to evaluate the
impact of each DSM-introduced similar word indi-
vidually on event extraction.
3.2 Adopting All-paths for Event Rules
Airola et al proposed an all-paths graph (APG) ker-
nel for extracting protein-protein interactions (PPI),
in which the kernel function counts weighted shared
dependency paths of all possible lengths (Airola et
al., 2008). Thomas et al adopted this kernel as
one of the three models used in the ensemble learn-
ing for extracting drug-drug interactions (Thomas et
al., 2011a) and won the recent DDIExtraction 2011
challenge (Segura-Bedmar et al, 2011). The JULIE
lab adapted the APG kernel to event extraction us-
ing syntactically pruned and semantically enriched
dependency graphs (Buyko et al, 2009).
The graph representation of the kernel consists of
two sub-representations: the full dependency parse
and the surface word sequence of the sentence where
a pair of interacting entities occurs. At the expense
of computational complexity, this representation en-
ables the kernel to explore broader contexts of an
interaction, thus taking advantage of the entire de-
pendency graph of the sentence. When comparing
two interaction instances, instead of using only the
shortest path that might not always provide suffi-
cient syntactic information about relations, the ker-
nel considers paths of all possible lengths between
any two nodes. More recently, a hash subgraph pair-
wise (HSP) kernel-based approach was also proposed
for drug-drug interactions and adopts the same graph
representation as the APG kernel (Zhang et al, 2012).
In contrast, the graph representation that our ASM
algorithm searches in a sentence is inherently re-
stricted to the shortest path among target entities in
event rules, as described in Section 2.2. Borrowing
from the idea of the all-path graph representation, in
this work we attempted to explore contexts beyond
the shortest paths to enrich our rule set. We evalu-
ated within the framework of our system the impact
of using acyclic paths of all possible lengths among
event participants as key contextual dependencies to
populate the event rule set as compared to using only
the shortest paths in the current system setting.
4 Implementation
4.1 Preprocessing
We employed the preprocessed data in the
BioC (Comeau et al, 2013) compliant XML format
provided by the shared task organizers as supporting
resources. The BioC project attempts to address
the interoperability among existing natural language
processing tools by providing a unified BioC XML
format. The supporting analyses include tokeniza-
tion, sentence segmentation, POS tagging and
lemmatization. Different syntactic parsers analyze
text based on different underlying methodologies, for
instances, the Stanford parser (Klein and Manning,
2003) performs joint inference over the product of an
unlexicalized Probabilistic Context-Free Grammar
(PCFG) parser and a lexicalized dependency parser
while the McClosky-Charniak-Johnson (Charniak)
parser (McClosky and Charniak, 2008) is based on
N -best parse reranking over a lexicalized PCFG
model. In order to take advantage of multiple aspects
of structural analysis of sentences, both Stanford
parser and Charniak parser, which are among the best
performing parsers trained on the GENIA Treebank
corpus, are used to parse the training sentences and
produce dependency graphs for learning event rules.
Only the Charniak parser is used on the testing
sentences in the event extraction phase.
4.2 ASM Parameter Setting
The GE task includes 13 different event types. Since
each type possesses its own event contexts, an indi-
vidual threshold te is assigned to each type. Together
with the 3 distance function weights ws, wl and wd,
the ASM requires 16 parameters for the GE event ex-
traction task. Similarly, the ASM requires 43 param-
eters to cater to the 40 diverse event types of the CG
task. As reported in (Liu et al, 2013a), we used a
genetic algorithm (GA) (Cormen et al, 2001) to au-
80
tomatically determine values of the 12 ASM param-
eters for the 2011 GE task using the training data.
We inherited these previously determined parameters
and adapted them into the 2013 tasks according to
the event type and its argument configuration. For in-
stance, ?Pathway? events in the CG task is assigned
the same te as the ?Binding? events in the GE task as
they possess similar argument configurations.
Table 1 shows the parameter setting for the 2013
GE task with the equal weights ws = wl = wd con-
straint. The graph node matching criterion ?P*+L?
that requires the relaxed POS tags and the token lem-
mas to be identical is used in the ASM.
Parameter Value Parameter Value
tGene expression 8 tUbiquitination 3
tTranscription 7 tBinding 7
tProtein catabolism 10 tRegulation 3
tPhosphorylation 8 tPositive regulation 3
tLocalization 8 tNegative regulation 3
tAcetylation 3 ws 10
tDeacetylation 3 wl 10
tProteinmodification 3 wd 10
Table 1: ASM parameter setting for the 2013 GE task
4.3 Distributional Similarity Model
In our implementation, we made following improve-
ments to the original Pantel model (Pantel and Lin,
2002): (1) lemmas of words generated by the Bi-
oLemmatizer (Liu et al, 2012) are used to achieve
generalization. The POS information is combined
with each lemmatized word to disambiguate its cat-
egory. (2) instead of the linear context where a
word occurs, we take advantage of dependency con-
texts inferred from dependency graphs. For instance,
?toxicity?amod? is extracted as a feature of the to-
ken ?nonhematopoietic JJ?. It captures the dependent
token, the type and the directionality of the depen-
dency. (3) the resulting miw,c is scaled into the [0, 1]
range by
? ?miw,c
1 + ? ?miw,c
to avoid greater miw,c values
dominating the similarity calculation between words.
An empirical ? = 0.01 is used. (4) while only the
immediate dependency contexts of a word are used
in our model, our implementation is flexible so that
contexts of various dependency depths could be taken
into consideration.
In order to cover a wide range of words and capture
the diverse usages of them in biomedical texts, in-
stead of resorting to an existing corpus, our distribu-
tional similarity model is built based on a random se-
lection of 5 million abstracts from the entire PubMed.
When computing miw,c, we filtered out contexts of
each word where the word occurs less than 5 times.
Eventually, the model contains 2.8 million distinct to-
kens and 0.4 million features. When it is queried with
an amino acid, e.g, ?lysine?, the top 15 tokens in the
resulting ranked list are all correct amino acid names.
5 Results and Discussion
This section reports our results on the GE and the CG
tasks respectively, including the attempted extensions
to our ASM-based event extraction system.
5.1 GE task
5.1.1 Datasets
The 2013 GE task dataset is composed of full-text
articles from PubMed Central, which are divided into
smaller segments by the task organizers according to
various sections of the articles. Table 2 presents some
statistics of the GE dataset.
Attributes Counted Training Development Testing
Full article segments 222 249 305
Proteins 3,571 4,138 4,359
Annotated events 2,817 3,199 3,301
Table 2: Statistics of BioNLP-ST 2013 GE dataset
As distributed, the development set is bigger than
the training set. For better system generalization, we
randomly reshuffled the data and created a 353/118
training/development division, a roughly 3:1 ratio
consistent with the settings in previous GE tasks.
The results reported on the training/development data
thereafter are based on our new data partition.
5.1.2 GE Results on Development Set
Table 3 shows the event extraction results on the 118
development documents based on event rules derived
from different parsers. Only the numbers of unique,
optimized rules are reported and those that possess
isomorphic graph representations determined by an
Exact Subgraph Matching (ESM) algorithm (Liu et
al., 2013b) are removed. The ensemble rule set com-
bines rules derived from both parsers and achieves
a better performance than that of using individual
parsers. It makes sense that the Charniak parser is
favored and leads to a performance close to the en-
semble performance because sentences from which
events are extracted are parsed by the Charniak parser
as well. However, we retained the additional rules
from the Stanford parser in the hope that they may
contribute to the testing data.
When embedding the distributional similarity
model (DSM) directly into the graph node matching
81
Parser Type Event Rule Recall Precision F-score
Charniak 2,923 47.01% 66.01% 54.91%
Stanford 3,305 43.66% 67.67% 53.08%
Ensemble 4,617 47.45% 65.65% 55.09%
Table 3: Performance of using different parsers
scheme, we performed the DSM on all rule tokens ex-
cept biological entities, meaning that for each rule to-
ken, if a match will be granted if a rule token appears
in the top M most similar word list of a sentence to-
ken, e.g., ?DSM 3? denotes the top 3 similar words
determined by the DSM. We further performed DSM
only on trigger tokens for comparison, as presented
in Table 4.
All Tokens Recall Precision F-score
DSM 1 47.98% 52.56% 50.17%
DSM 3 48.68% 35.07% 40.77%
DSM 10 53.43% 19.38% 28.44%
Trigger Tokens Recall Precision F-score
DSM 1 48.06% 54.22% 50.95%
DSM 3 48.59% 37.00% 42.01%
DSM 10 53.35% 24.65% 33.72%
Table 4: Performance of integrated DSM
Even though the DSM helps to substantially in-
crease the recall to 53.43%, we observed a significant
precision drop which leads to an inferior F-score to
the ensemble baseline in Table 3. A close evaluation
of the generated graph matches reveals that antonyms
produced by the DSM contributes to most of the false
positive events. For instance, the most similar words
for the verb ?increase? and the adjective ?high? re-
turned by the model are ?decrease? and ?low? be-
cause they tend to occur in the same contexts. Fur-
ther investigation is needed to automatically filter out
the antonyms. When generating additional rules us-
ing the top M most similar words from the DSM,
since all the rules undergo the optimization process,
the event extraction precision is ensured. However,
the recall increase from simple events is diluted by
the counter effect of the introduced false positives in
detecting regulation-related complex events, result-
ing in a comparable performance to the baseline.
Table 5 gives the performance comparison of us-
ing all-paths and the shortest paths in our event ex-
traction system. Using all-paths does not bring in a
significant improvement in F-score but takes 27 it-
erations to optimize as compared to the 5-iteration
optimization on shortest paths. Most of the rules in-
duced from all-paths are eventually discarded by the
optimization process. The all-paths graph represen-
tation was motivated by the observation that short-
est paths between candidate entities often exclude
relation-signaling words when detecting binary re-
lationships (Airola et al, 2008). Exploring broader
contexts ensures such words to be considered. In the
event extraction task, however, since triggers have
been annotated, they are naturally incorporated into
the shortest paths connecting trigger to each event ar-
gument. This in part explains why contexts beyond
shortest paths did not bring in an appreciable benefit.
All Tokens Recall Precision F-score
All-paths 48.77% 64.64% 55.59%
Shortest paths 47.45% 65.65% 55.09%
Table 5: Performance of using all-paths
5.1.3 GE Results on Testing Set
Since integrating the DSM and all-paths do not pro-
vide significant performance improvements to our
system, we decided to retain the original settings in
the ASM when extracting events from the testing
data. While most of the 2011 shared task datasets are
composed of PubMed abstracts compared to full-text
articles in the 2013 GE task, our system focuses on
extracting events expressed within the boundaries of
a single sentence. Therefore, in order to take advan-
tage of existing annotated resources, we incorporated
the annotated data of 2011 GE task and EPI (Epi-
genetics and Post-translational Modifications) task to
enrich the training instances of corresponding event
types of the 2013 GE task. Eventually, we obtained a
total of 14,448 rules of different event types from our
training data. In practice, it takes the ASM less than a
second to match the entire rule set with one document
and return results.
Our submitted system achieves a 48.93% F-score
on the 305 testing documents of the GE task, ranking
4th among 12 participating teams. Table 6 presents
the performance of the top eight systems.
System Recall Precision F-score
EVEX 45.44% 58.03% 50.97%
TEES 2.1 46.17% 56.32% 50.74%
BioSEM 42.47% 62.83% 50.68%
NCBI 40.53% 61.72% 48.93%
DlutNLP 40.81% 57.00% 47.56%
HDS4NLP 37.11% 51.19% 43.03%
NICTANLM 36.99% 50.68% 42.77%
USheff 31.69% 63.28% 42.23%
Table 6: Performance of top 8 systems in GE task
Our performance is within a reasonable mar-
gin from the best-performing system ?EVEX?, and
shows an overall superior precision over most partic-
ipating teams; only two of the top 5 systems obtained
82
a precision in the 60% range. Particularly for the
regulation-related complex events, we are the only
team that achieved a precision over 55% among all
12 participating systems. This indicates that event
rules automatically learned and optimized over train-
ing data generalize well to the unseen text, and have
the ability to identify precisely corresponding events.
We further evaluated the impact of the additonal
training instances from 2011 tasks and the ensemble
rule set derived from different parsers as presented
in Table 7. With the help from the 2011 data, our
F-score is increased by 3% and we became the only
team that detected ?Ubiquitination? events from test-
ing data. In addition, rules derived from the Stanford
parser do not provide additional benefits on the test-
ing data compared to using the Charniak parser alone.
System Attribute Recall Precision F-score
Ensemble 2013 + 2011 data 40.53% 61.72% 48.93%
Ensemble 2013 data 35.63% 63.91% 45.75%
Charniak 2013 data 35.29% 65.71% 45.92%
Table 7: Impact of 2011 data and ensemble rule set
5.2 CG task
5.2.1 Datasets
The CG task dataset is prepared based on a previ-
ously released corpus of angiogenesis domain ab-
stracts (Wang et al, 2011). It targets a challenging
set of 40 types of biological processes related to the
development and progression of cancer involving 18
entity types (Pyysalo et al, 2012). Table 8 presents
some statistics of the CG dataset.
Attributes Counted Training Development Testing
Abstracts 300 100 200
Entities 10,935 3,634 6,955
Annotated events 8,803 2,915 5,972
Table 8: Statistics of BioNLP-ST 2013 CG dataset
5.2.2 CG Results on Testing Set
We generalized our event extraction system to the CG
task and the corresponding annotated data of the 2011
tasks is also incorporated in the training phase to ob-
tain the optimized event rule set. Due to time con-
straints, the impact of integrating the DSM and all-
paths is not evaluated on the CG task. We achieved
a 46.38% F-score on the 200 testing documents of
the CG task, ranking 3rd among the 6 participating
teams. Table 9 gives the primary evaluation results of
the 6 participating teams; only ?TEES-2.1? and we
participated in both GE and CG tasks. The detailed
results of each of the targeted 40 event types is avail-
able from the official CG task website.
Team Recall Precision F-score
TEES-2.1 48.76% 64.17% 55.41%
NaCTeM 48.83% 55.82% 52.09%
NCBI 38.28% 58.84% 46.38%
RelAgent 41.73% 49.58% 45.32%
UET-NII 19.66% 62.73% 29.94%
ISI 16.44% 47.83% 24.47%
Table 9: Performance of all systems in 2013 CG task
Inconsistent with other biological entities, the en-
tity annotation for the optional ?Site? argument in-
volved in events such as ?Binding?, ?Mutation? and
?Phosphorylation? are not provided by the task orga-
nizers. We consider that detecting ?Site? entities is
related to entity detection and we would like to focus
our system on the event extraction itself. Thus, we
decided to ignore the ?Site? argument in our system.
However, a problem will arise that even though the
other arguments are correctly identified for an event,
it might still be evaluated as false positive if a ?Site?
argument is not detected. This results in both false
positive and false negative events. In addition, since
we did not perform the secondary task which requires
us to detect modifications of the predicted events, in-
cluding negation and speculation, about 7.5% anno-
tated instances in the testing data are thus missed,
causing damage to our recall in the overall evalua-
tion. The organizers have agreed to issue an additonal
evaluation that will focus on core event extraction tar-
gets excluding optional arguments such as ?Site? and
the secondary task. We will conduct more detailed
analysis on the results once they are made available.
6 Conclusion and Future Work
In the BioNLP-ST 2013, we generalized our ASM-
based system to address both GE and CG tasks.
We attempted to integrate a distributional similarity
model into our system to extend the graph match-
ing scheme. We also evaluated the impact of using
paths of all possible lengths among event participants
as key contextual dependencies to extract potential
events as compared to using only the shortest paths
within the framework of our system.
We achieved a 46.38% F-score in the CG task and
a 48.93% F-score in the GE task, ranking 3rd and
4th respectively. While the distributional similarity
model did not improve the overall performance of our
system in the tasks, we would like to further investi-
gate the antonym problem introduced by the model in
our future work.
83
Acknowledgments
This research was supported by the Intramural Re-
search Program of the NIH, NLM.
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski1. 2008.
All-paths graph kernel for protein-protein interaction
extraction with evaluation of cross-corpus learning.
BMC Bioinformatics, 9 Suppl 11:s2.
Ethem Alpaydin. 2004. Introduction to Machine Learn-
ing. MIT Press.
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for systems
biology by text mining the literature. Trends in Biotech-
nology, 28(7):381?390.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012. Uni-
versity of turku in the BioNLP?11 shared task. BMC
Bioinformatics, 13 Suppl 11:S4.
Razvan C. Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation extrac-
tion. In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing, pages 724?731.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In Pro-
ceedings of the 19th Conference on Neural Information
Processing Systems (NIPS). Vancouver, BC, December.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed de-
pendency graphs. In BioNLP ?09: Proceedings of the
Workshop on BioNLP, pages 19?27, Morristown, NJ,
USA. Association for Computational Linguistics.
Donald C. Comeau, Rezarta Islamaj Dog?an, Paolo Ci-
ccarese, Kevin Bretonnel Cohen, Martin Krallinger,
Florian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-
naldi, Manabu Torii, Alfonso Valencia, Karin Verspoor,
Thomas C. Wiegers, Cathy H. Wu, and W. John Wilbur.
2013. BioC: A minimalist approach to interoperability
for biomedical text processing. submitted.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to Al-
gorithms. The MIT Press.
Christiane Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. Bradford Books.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of BioNLP Shared Task 2009 Workshop, pages
1?9. Association for Computational Linguistics.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of BioNLP shared task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop, pages 1?6. As-
sociation for Computational Linguistics, June.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL ?03: Proceedings of the
41st Annual Meeting on Association for Computational
Linguistics, pages 423?430. Association for Computa-
tional Linguistics.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, 104(2):211?240.
Haibin Liu, Ravikumar Komandur, and Karin Verspoor.
2011. From graphs to events: A subgraph matching ap-
proach for information extraction from biomedical text.
In Proceedings of BioNLP Shared Task 2011 Work-
shop, pages 164?172. Association for Computational
Linguistics, June.
Haibin Liu, Tom Christiansen, William A Baumgartner,
and Karin Verspoor. 2012. Biolemmatizer: a lemmati-
zation tool for morphological processing of biomedical
text. Journal of Biomedical Semantics, 3:3.
Haibin Liu, Lawrence Hunter, Vlado Keselj, and Karin
Verspoor. 2013a. Approximate subgraph matching-
based literature mining for biomedical events and re-
lations. PLOS ONE, 8:4 e60954.
Haibin Liu, Vlado Keselj, and Christian Blouin. 2013b.
Exploring a subgraph matching approach for extracting
biological events from literature. Computational Intel-
ligence.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language processing.
MIT Press, Cambridge, MA, USA.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of the
Association for Computational Linguistics, pages 101?
104, Columbus, Ohio. The Association for Computer
Linguistics.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?02, pages 613?619,
New York, NY, USA. ACM.
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-Cheol
Cho, Jun?ichi Tsujii, and Sophia Ananiadou. 2012.
Event extraction across multiple levels of biological or-
ganization. Bioinformatics, 28:i575?i581.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, Simon
Clematide, Thrse Vachon, and Martin Romacker. 2010.
Ontogene in BioCreative II.5. IEEE/ACM Trans. Com-
put. Biology Bioinform., 7(3):472?480.
84
Gerard Salton and Michael J. McGill. 1986. Introduction
to Modern Information Retrieval. McGraw-Hill, Inc.,
New York, NY, USA.
Isabel Segura-Bedmar, Paloma Martinez, and Daniel
Sanchez-Cisneros. 2011. The 1st DDIExtraction-2011
Challenge Task: Extraction of Drug-Drug Interactions
from Biomedical Texts. In Proceedings of the 1st Chal-
lenge Task on Drug-Drug Interaction Extraction 2011,
pages 1?9.
Philippe Thomas, Mariana Neves, Illes Solt, Domonkos
Tikk, and Ulf Leser. 2011a. Relation extraction for
drug-drug interactions using ensemble learning. In Pro-
ceedings of DDIExtraction-2011 challenge task, pages
11?18.
Philippe Thomas, Stefan Pietschmann, Ille?s Solt,
Domonkos Tikk, and Ulf Leser. 2011b. Not all
links are equal: Exploiting dependency types for the
extraction of protein-protein interactions from text. In
Proceedings of BioNLP 2011 Workshop, pages 1?9.
Association for Computational Linguistics, June.
Domonkos Tikk, Philippe Thomas, Peter Palaga, Jo?rg
Hakenberg, and Ulf Leser. 2010. A comprehensive
benchmark of kernel methods to extract protein?protein
interactions from literature. PLoS Computational Biol-
ogy, 6:e1000837, July.
Xinglong Wang, Iain McKendrick, Ian Barrett, Ian Dix,
Tim French, Jun?ichi Tsujii, and Sophia Ananiadou.
2011. Automatic extraction of angiogenesis bioprocess
from text. Bioinformatics, 27(19):2730?2737.
Yijia Zhang, Hongfei Lin, Zhihao Yang, Jian Wang, and
Yanpeng Li. 2012. A single kernel-based approach to
extract drug-drug interactions from biomedical litera-
ture. PLOS ONE, 7(11): e48901.
85
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 99?103,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP Shared Task 2013: Supporting Resources
Pontus Stenetorp 1 Wiktoria Golik 2 Thierry Hamon 3
Donald C. Comeau 4 Rezarta Islamaj Dog?an 4 Haibin Liu 4 W. John Wilbur 4
1 National Institute of Informatics, Tokyo, Japan
2 French National Institute for Agricultural Research (INRA), Jouy-en-Josas, France
3 University Paris 13, Paris, France
4 National Center for Biotechnology Information, National Library of Medicine,
National Institutes of Health, Bethesda, MD, USA
pontus@nii.ac.jp wiktoria.golik@jouy.inra.fr thierry.hamon@univ-paris13.fr
{comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov
Abstract
This paper describes the technical con-
tribution of the supporting resources pro-
vided for the BioNLP Shared Task 2013.
Following the tradition of the previous
two BioNLP Shared Task events, the task
organisers and several external groups
sought to make system development easier
for the task participants by providing auto-
matically generated analyses using a vari-
ety of automated tools. Providing analy-
ses created by different tools that address
the same task also enables extrinsic evalu-
ation of the tools through the evaluation of
their contributions to the event extraction
task. Such evaluation can improve under-
standing of the applicability and benefits
of specific tools and representations. The
supporting resources described in this pa-
per will continue to be publicly available
from the shared task homepage
http://2013.bionlp-st.org/
1 Introduction
The BioNLP Shared Task (ST), first organised in
2009, is an ongoing series of events focusing on
novel challenges in biomedical domain informa-
tion extraction. In the first BioNLP ST, the or-
ganisers provided the participants with automat-
ically generated syntactic analyses from a variety
of Natural Language Processing (NLP) tools (Kim
et al, 2009) and similar syntactic analyses have
since then been a key component of the best per-
forming systems participating in the shared tasks.
This initial work was followed up by a similar ef-
fort in the second event in the series (Kim et al,
2011), extended by the inclusion of software tools
and contributions from the broader BioNLP com-
munity in addition to task organisers (Stenetorp et
al., 2011).
Although no formal study was carried out to es-
timate the extent to which the participants utilised
the supporting resources in these previous events,
we note that six participating groups mention us-
ing the supporting resources in published descrip-
tions of their methods (Emadzadeh et al, 2011;
McClosky et al, 2011; McGrath et al, 2011;
Nguyen and Tsuruoka, 2011; Bjo?rne et al, 2012;
Vlachos and Craven, 2012). These resources have
been available also after the original tasks, and
several subsequent studies have also built on the
resources. Van Landeghem et al (2012) applied a
visualisation tool that was made available as a part
of the supporting resources, Vlachos (2012) em-
ployed the syntactic parses in a follow-up study
on event extraction, Van Landeghem et al (2013)
used the parsing pipeline created to produce the
syntactic analyses, and Stenetorp et al (2012) pre-
sented a study of the compatibility of two different
representations for negation and speculation anno-
tation included in the data.
These research contributions and the overall
positive reception of the supporting resources
prompted us to continue to provide supporting re-
sources for the BioNLP Shared Task 2013. This
paper presents the details of this technical contri-
bution.
2 Organisation
Following the practice established in the
BioNLP ST 2011, the organisers issued an
open call for supporting resources, welcoming
contributions relevant to the task from all authors
of NLP tools. In the call it was mentioned that
points such as availability for research purposes,
support for well-established formats and access
99
Name Annotations Availability
BioC Lemmas and syntactic constituents Source
BioYaTeA Terms, lemmas, part-of-speech and syntactic constituencies Source
Cocoa Entities Web API
Table 1: Summary of tools/analyses provided by external groups.
to technical documentation would considered
favourable (but not required) and each supporting
resource provider was asked to write a brief
description of their tools and how they could
potentially be applied to aid other systems in the
event extraction task. This call was answered
by three research groups that offered to provide
a variety of semantic and syntactic analyses.
These analyses were provided to the shared
task participants along with additional syntactic
analyses created by the organisers.
However, some of the supporting resource
providers were also participants in the main event
extraction tasks, and giving them advance access
to the annotated texts for the purpose of creating
the contributed analyses could have given those
groups an advantage over others. To address this
issue, the texts were made publicly available one
week prior to the release of the annotations for
each set of texts. During this week, the supporting
analysis providers annotated the texts using their
automated tools and then handed the analyses to
the shared task organisers, who made them avail-
able to the task participants via the shared task
homepage.
3 Analyses by External Groups
This section describes the tools that were applied
to create supporting resources by the three exter-
nal groups. These contributions are summarised in
Table 1.
BioC Don Comeau, Rezarta Islamaj, Haibin
Liu and John Wilbur of the National Center for
Biotechnology Information provided the output of
the shallow parser MedPost (Smith et al, 2004)
and the BioLemmatizer tool (Liu et al, 2012),
supplied in the BioC XML format1 for annota-
tion interchange (Comeau et al, 2013). The BioC
format address the problem of interoperability be-
tween different tools and platforms by providing a
unified format for use by various tools. Both Med-
Post and BioLemmatizer are specifically designed
1http://bioc.sourceforge.net/
for biomedical texts. The former annotates parts-
of-speech and performs sentence splitting and to-
kenisation, while the latter performs lemmatisa-
tion. In order to make it easier for participants
to get started with the BioC XML format, the
providers also supplied example code for parsing
the format in both the Java and C++ programming
languages.
BioYaTeA Wiktoria Golik of the French Na-
tional Institute for Agricultural Research (INRA)
and Thierry Hamon of University Paris 13 pro-
vided analyses created by BioYaTeA2 (Golik et
al., 2013). BioYaTeA is a modified version of the
YaTeA term extraction tool (Aubin and Hamon,
2006) adapted to the biomedical domain. Working
on a noun-phrase level, BioYaTeA provides anno-
tations such as lemmas, parts-of-speech, and con-
stituent analysis. The output formats used were a
simple tabular format as well as BioYaTeA-XML,
an XML representation specific to the tool.
Cocoa S. V. Ramanan of RelAgent Private Ltd
provided the output of the Compact cover anno-
tator (Cocoa) for biological noun phrases.3 Co-
coa provides noun phrase-level entity annotations
for over 20 different semantic categories such as
macromolecules, chemicals, proteins and organ-
isms. These annotations were made available for
the annotated texts for the shared task along with
the opportunity for the participants to use the Co-
coa web API to annotate any text they may con-
sider beneficial for their system. The data format
used by Cocoa is a subset of the standoff format
used for the shared task entity annotations, and it
should thus be easy to integrate into existing event
extraction systems.
4 Analyses by Task Organisers
This section describes the syntactic parsers ap-
plied by the task organisers and the pre-processing
2http://search.cpan.org/?bibliome/
Lingua-BioYaTeA/
3http://npjoint.com/
100
Name Model Availability
Enju Biomedical Binary
Stanford Combination Binary, Source
McCCJ Biomedical Source
Table 2: Parsers used for the syntactic analyses.
and format conversions applied to their output.
The applied parsers are listed in Table 2.
4.1 Syntactic Parsers
Enju Enju (Miyao and Tsujii, 2008) is a deep
parser based on the Head-Driven Phrase Struc-
ture Grammar (HPSG) formalism. Enju analyses
its input in terms of phrase structure trees with
predicate-argument structure links, represented in
a specialised XML-format. To make the analyses
of the parser more accessible to participants, we
converted its output into the Penn Treebank (PTB)
format using tools included with the parser. The
use of the PTB format also allow for its output to
be exchanged freely for that of the other two syn-
tactic parsers and facilitates further conversions
into dependency representations.
McCCJ The BLLIP Parser (Charniak and John-
son, 2005), also variously known as the Charniak
parser, the Charniak-Johnson parser, or the Brown
reranking parser, has been applied in numerous
biomedical domain NLP efforts, frequently using
the self-trained biomedical model of McClosky
(2010) (i.e. the McClosky-Charniak-Johnson or
McCCJ parser). The BLLIP Parser is a con-
stituency (phrase structure) parser and the applied
model produces PTB analyses as its native out-
put. These analyses were made available to par-
ticipants without modification.
Stanford The Stanford Parser (Klein and Man-
ning, 2002) is a widely used publicly available
syntactic parser. As for the Enju and BLLIP
parsers, a model trained on a dataset incorporating
biomedical domain annotations is available also
for the Stanford parser. Like the BLLIP parser,
the Stanford parser is constituency-based and pro-
duces PTB analyses, which were provided to task
participants. The Stanford tools additionally in-
corporate methods for automatic conversion from
this format to other representations, discussed fur-
ther below.
4.2 Pre-processing and Conversions
To create the syntactic analyses from the Enju,
BLLIP and Stanford Parser systems, we first ap-
plied a uniform set of pre-processing steps in order
to normalise over differences in e.g. tokenisation
and thus ensure that the task participants can eas-
ily swap the output of one system for another. This
pre-processing was identical to that applied in the
BioNLP 2011 Shared Task, and included sentence
splitting of the annotated texts using the Genia
Sentence Splitter,4 the application of a set of post-
processing heuristics to correct frequently occur-
ring sentence splitting errors, and Genia Treebank-
like tokenisation (Tateisi et al, 2004) using a to-
kenisation script created by the shared task organ-
isers. 5
Since several studies have indicated that repre-
sentations of syntax and aspects of syntactic de-
pendency formalism differ in their applicability to
support information extraction tasks (Buyko and
Hahn, 2010; Miwa et al, 2010; Quirk et al, 2011),
we further converted the output of each of the
parsers from the PTB representation into three
other representations: CoNNL-X, Stanford De-
pendencies and Stanford Collapsed Dependencies.
For the CoNLL-X format we employed the con-
version tool of Johansson and Nugues (2007), and
for the two Stanford Dependency variants we used
the converter provided with the Stanford CoreNLP
tools (de Marneffe et al, 2006). These analyses
were provided to participants in the output for-
mats created by the respective tools, i.e. the TAB-
separated column-oriented format CoNLL and the
custom text-based format of the Stanford Depen-
dencies.
5 Results and Discussion
Just like in previous years the supporting resources
were well-received by the shared task participants
and as many as five participating teams mentioned
utilising the supporting resources in their initial
submissions (at the time of writing, the camera-
ready versions were not yet available). This level
of usage of the supporting resources by the partici-
pants is thus comparable to what was observed for
the 2011 shared task.
Following in the tradition of the 2011 support-
4https://github.com/ninjin/geniass
5https://github.com/ninjin/bionlp_
st_2013_supporting/blob/master/tls/
GTB-tokenize.pl
101
ing resources, to aim for reproducibility, the pro-
cessing pipeline containing pre/post-processing
and conversion scripts for all the syntactic parses
has been made publicly available under an open
licence.6 The repository containing the pipeline
also contains detailed instructions on how to re-
produce the output and how it can potentially be
applied to other texts.
Given the experience of the organisers in
analysing medium-sized corpora with a variety of
syntactic parsers, many applied repeatedly over
several years, we are also happy to report that the
robustness of several publicly available parsers has
recently improved noticeably. Random crashes,
corrupt outputs and similar failures appear to be
transitioning from being expected to rare occur-
rences.
In this paper, we have introduced the supporting
resources provided for the BioNLP 2013 Shared
Task by the task organisers and external groups.
These resources included both syntactic and se-
mantic annotations and were provided to allow the
participants to focus on the various novel chal-
lenges of constructing event extraction systems by
minimizing the need for each group to separately
perform standard processing steps such as syntac-
tic analysis.
Acknowledgements
We would like to give special thanks to Richard
Johansson for providing and allowing us to dis-
tribute an improved and updated version of his for-
mat conversion tool.7 We would also like to ex-
press our appreciation to the broader NLP com-
munity for their continued efforts to improve the
availability of both code and data, thus enabling
other researchers to stand on the shoulders of gi-
ants.
This work was partially supported by the
Quaero programme funded by OSEO (the French
agency for innovation). The research of Donald
C. Comeau, Rezarta Islamaj Dog?an, Haibin Liu
and W. John Wilbur was supported by the Intra-
mural Research Program of the National Institutes
of Health (NIH), National Library of Medicine
(NLM).
6https://github.com/ninjin/bionlp_st_
2013_supporting
7https://github.com/ninjin/
pennconverter
References
Sophie Aubin and Thierry Hamon. 2006. Improving
term extraction with terminological resources. In
Advances in Natural Language Processing, pages
380?387. Springer.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012.
University of Turku in the BioNLP?11 Shared Task.
BMC Bioinformatics, 13(Suppl 11):S4.
Ekaterina Buyko and Udo Hahn. 2010. Evaluating
the Impact of Alternative Dependency Graph Encod-
ings on Solving Event Extraction Tasks. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 982?992,
Cambridge, MA, October.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Donald C. Comeau, Rezarta Islamaj Dog?an, Paolo Ci-
ccarese, Kevin Bretonnel Cohen, Martin Krallinger,
Florian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-
naldi, Manabu Torii, Alfonso Valencia, Karin Ver-
spoor, Thomas C. Wiegers, Cathy H. Wu, and
W. John Wilbur. 2013. BioC: A minimalist ap-
proach to interoperability for biomedical text pro-
cessing. submitted.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double layered learning for bio-
logical event extraction from text. In Proceedings
of the BioNLP Shared Task 2011 Workshop, pages
153?154. Association for Computational Linguis-
tics.
Wiktoria Golik, Robert Bossy, Zorana Ratkovic, and
Claire Ne?dellec. 2013. Improving Term Extraction
with Linguistic Analysis in the Biomedical Domain.
In Special Issue of the journal Research in Comput-
ing Science, Samos, Greece, March. 14th Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proc. of the 16th Nordic Conference
on Computational Linguistics (NODALIDA), pages
105?112.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop Com-
panion Volume for Shared Task, pages 1?9, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
102
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of Genia Event
Task in BioNLP Shared Task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop.
Dan Klein and Christopher D Manning. 2002. Fast ex-
act inference with a factored model for natural lan-
guage parsing. Advances in neural information pro-
cessing systems, 15(2003):3?10.
Haibin Liu, Tom Christiansen, William Baumgartner,
and Karin Verspoor. 2012. BioLemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of Biomedical Semantics,
3(1):3.
David McClosky, Mihai Surdeanu, and Christopher D
Manning. 2011. Event Extraction as Dependency
Parsing for BioNLP 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 41?45.
Association for Computational Linguistics.
David McClosky. 2010. Any domain parsing: Auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Brown University.
Liam R McGrath, Kelly Domico, Courtney D Cor-
ley, and Bobbie-Jo Webb-Robertson. 2011. Com-
plex biological event extraction from full text us-
ing signatures of linguistic and semantic features.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 130?137. Association for Compu-
tational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating Dependency Rep-
resentations for Event Extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 779?787,
Beijing, China, August.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Nhung TH Nguyen and Yoshimasa Tsuruoka. 2011.
Extracting bacteria biotopes with semi-supervised
named entity recognition and coreference resolution.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 94?101. Association for Compu-
tational Linguistics.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. MSR-NLP Entry in
BioNLP Shared Task 2011. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 155?
163, Portland, Oregon, USA, June.
Larry Smith, Thomas Rindflesch, and W. John Wilbur.
2004. MedPost: a part-of-speech tagger for bio
medical text. Bioinformatics, 20(14):2320?2321.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo,
Tomoko Ohta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011. BioNLP Shared Task 2011: Supporting Re-
sources. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 112?120, Portland, Oregon,
USA, June.
Pontus Stenetorp, Sampo Pyysalo, Tomoko Ohta,
Sophia Ananiadou, and Jun?ichi Tsujii. 2012.
Bridging the gap between scope-based and event-
based negation/speculation annotations: a bridge not
too far. In Proceedings of the Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, pages 47?56. Association for Computa-
tional Linguistics.
Y Tateisi, T Ohta, and J Tsujii. 2004. Annotation of
predicate-argument structure on molecular biology
text. Proceedings of the Workshop on the 1st In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-04).
Sofie Van Landeghem, Kai Hakala, Samuel Ro?nnqvist,
Tapio Salakoski, Yves Van de Peer, and Filip Gin-
ter. 2012. Exploring biomolecular literature with
EVEX: connecting genes through events, homology,
and indirect associations. Advances in Bioinformat-
ics, 2012.
Sofie Van Landeghem, Jari Bjo?rne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, et al 2013. Large-scale event extrac-
tion from literature with multi-level gene normaliza-
tion. PloS one, 8(4):e55814.
Andreas Vlachos and Mark Craven. 2012. Biomedical
event extraction from abstracts and full papers using
search-based structured prediction. BMC Bioinfor-
matics, 13(Suppl 11):S5.
Andreas Vlachos. 2012. An investigation of imita-
tion learning algorithms for structured prediction. In
Workshop on Reinforcement Learning, page 143.
103
