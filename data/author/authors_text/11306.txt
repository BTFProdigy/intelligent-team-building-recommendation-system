Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1089?1096
Manchester, August 2008
    Sentence Type Based Reordering Model for Statistical Machine 
Translation 
Jiajun Zhang, Chengqing Zong, Shoushan Li 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China 
{jjzhang, cqzong, sshanli}@nlpr.ia.ac.cn 
 
 
Abstract 
Many reordering approaches have been 
proposed for the statistical machine 
translation (SMT) system. However, the 
information about the type of source 
sentence is ignored in the previous 
works. In this paper, we propose a group 
of novel reordering models based on the 
source sentence type for Chinese-to-
English translation. In our approach, an 
SVM-based classifier is employed to 
classify the given Chinese sentences into 
three types: special interrogative sen-
tences, other interrogative sentences, and 
non-question sentences. The different 
reordering models are developed ori-
ented to the different sentence types. 
Our experiments show that the novel re-
ordering models have obtained an im-
provement of more than 2.65% in BLEU 
for a phrase-based spoken language 
translation system.  
1 Introduction 
The phrase-based translation approach has been 
the popular and widely used strategy to the sta-
tistical machine translation (SMT) since Och, et 
al. (2002) proposed the log-linear model. How-
ever, reordering is always a key issue in the de-
coding process. A number of models have been 
developed to deal with the problem of reorder-
ing. The existing reordering approaches could 
be divided into two categories: one is integrated 
into the decoder and the other is employed as a 
preprocessing module.   
                                                 
  ? 2008. Licensed under the Creative Commons Attribu-
tion-Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
Many reordering methods belong to the for-
mer category. Distortion model was first em-
ployed by Koehn et al (2003); a lexicalized re-
ordering model was proposed by Och et al 
(2004) and Koehn et al (2005); and the formal 
syntax-based reordering models were proposed 
by Chiang (2005) and Xiong et al (2006). It is 
worthy to note that little syntactic knowledge is 
used in the models mentioned above. 
Compared to the reordering models that are 
integrated into the decoder, the reordering at the 
source side can utilize more syntactic knowl-
edge, with the goal of  adjusting the source lan-
guage sentence to make its word order closer to 
that of the target language. The most notable 
models are given by Xia and McCord (2004), 
Collins et al (2005), Li et al (2007) and Wang 
et al (2007). Xia and McCord (2004) parsed the 
source and target sides of the training data and 
then automatically extracted the rewriting pat-
terns. The rewriting patterns are employed on 
the input source sentence to make the word or-
der more accordant to target language. Collins et 
al. (2005) described an approach to reorder Ger-
man in German-to-English translation. The 
method concentrates on the German clauses and 
six types of transforming rules are applied to the 
parsed source sentence. However, all the rules 
are manually built. Li et al (2007) used a parser 
to get the syntactic tree of the source language 
sentence. In this method, a maximum entropy 
model is developed to determine how probable 
the children of a node are to be reordered. Obvi-
ously, there is also disadvantage in this method 
because the parsing tree is obtained by a full 
parser and contains too many nodes that are not 
involved in desired reorderings. Wang et al 
(2007) discussed three categories which are con-
sidered to be the most prominent candidates for 
reordering in Chinese-to-English translation, 
including verb phrases (VPs), noun phrases 
(NPs), and localizer phrases (LCPs). The 
1089
method deals with some special modifiers of 
VPs and NPs because they have the property 
that some specific modifiers appear before VPs 
or NPs in Chinese but occur after VPs or NPs in 
its English translation. We observe that all the 
transformation rules in this method are hard 
crafted. Furthermore, there are some other re-
lated works, such as Costa-jussa and Fonollosa?s 
work (2006) and Zhang et al?s work (2007). 
Costa-jussa and Fonollosa (2006) considered the 
source reordering as a translation task which 
translates the source sentence into reordered 
source sentence. A chunk-level reordering 
model was first proposed by Zhang et al (2007). 
However, all the existing models make no 
distinction between the different types of the 
source sentence. Intuitively, we have different 
reordering information in different sentence type. 
Taking Chinese special interrogative sentence as 
an example, there is a fixed phrase that usually 
occurs at the end of Chinese sentence but ap-
pears at the beginning part of its English transla-
tion. See the following Chinese to English trans-
lation: 
Chinese: ? ? ? ??? ? ?? ? 
English: What kind of seats do you like ? 
Obviously, the Chinese question phrase ??
?? ? ?? (What kind of seats)? should be 
put at the beginning of its English translation. 
However, many phrase-based systems fail to do 
this. 
In this paper, we are interested in investigat-
ing the value of Chinese sentence types in reor-
dering for Chinese-to-English spoken language 
translation. Due to the syntactic difference be-
tween Chinese and English, different sentence 
type provides different reordering information. 
A phrase-ahead model is developed to exploit 
and utilize the reordering information of special 
interrogative sentences. A phrase-back model is 
employed to catch and make use of the reorder-
ing information of other sentence types. How-
ever, the sentence type should be first identified 
by an SVM-based classifier before reordering 
the source sentence. The method overall is used 
as a preprocessing module for translation. We 
will introduce our method in detail later. 
 The remainder of this paper is organized as 
follows: Section 2 introduces our motivations; 
Section 3 gives the details on the implementa-
tion of our approach; the experiments are shown 
in Section 4; and the final concluding remarks 
are given in Section 5. 
2 Our Motivations 
In this section, before we analyze the Chinese-
to-English spoken language translation corpus,  
some definitions are given first. 
2.1 Definitions 
z Special interrogative sentence / other inter-
rogative sentence / non-question sentence 
Chinese sentence can be divided into question 
sentence and non-question sentence. If a Chi-
nese question sentence is translated into the 
English sentence of wh-questions, the sentence 
is named as a Chinese special interrogative sen-
tence; otherwise, it is called the Chinese other 
interrogative sentence. Figure 1-3 show some 
examples for the three sentence types respec-
tively. 
z SQP / TP / SP 
In Chinese special interrogative sentence, the 
question phrase is always moved ahead while it 
is translated into English. Correspondingly, the 
question phrase is named as the special question 
phrase (SQP). For example, the question  phrase  
???? ? ?? (What kind of seats)? in the 
example mentioned above is an SQP.  
A few quantifier phrases (QPs) like ?? ? 
(many times)?, ??? ? (many years)? in Chi-
nese and some LCPs like ??? ?? ? (after 
the accident happened)?, ??? ?? ? (before 
the meeting ends)? together with some NPs like 
temporal phrases are named temporal phrase 
(TP) in our model. Some LCPs like ??? ? (at 
the front of the hotel)?, ??? ? (near the ta-
ble)? and a few NPs like spatial phrases are 
called spatial phrase (SP) in our model. As PPs1, 
TPs and SPs are the most prominent candidates 
for reordering in Chinese other interrogative 
sentences and non-question sentences, they will 
be handled in the phrase-back reordering model.  
 
Figure 1.  An example of Chinese special inter-
rogative sentence with its English translation. 
 
Figure 2.  An example of Chinese other inter-
rogative sentence with its English translation. 
                                                 
1 PPs here mean prepositional phrases 
?  ?  ?  ??  ?  ? 
Can you speak Japanese ? 
?  ?  ?  ???  ?  ??  ? 
What kind of seats do you like ? 
1090
My wallet was stolen in the subway . 
 
Figure 3.  An example of Chinese non-question 
sentence with its English translation. 
2.2 Analysis of Corpus and  Our Motivations 
In order to have an overview of the distribution 
of the Chinese sentence types, we have made a 
survey based on our training set for translation, 
which contains about 277k Chinese and English 
sentence pairs. We found that about 17.2% of 
the sentences are special interrogative sentences, 
about 25.5% of sentences are other interrogative 
sentences and the remainders are all non-
question sentences. 
Each sentence type has its own reordering 
strategy, as demonstrated in Figures 1-3. There 
is a settled phrase (SQP) in Chinese special in-
terrogative sentence which usually appears at 
the end but will be translated first in English, 
just as Figure 1 illustrates. For other interroga-
tive sentences, some specific Chinese words like 
???????? will just be translated into 
?Can? or ?Do?  and come first in English. At 
present, this information is not used in our ap-
proach. Figure 2 gives an example. For non-
questions, the reordering candidates usually 
need to be moved back during translation. An 
example is shown in Figure 3. 
According to the analysis above, it is mean-
ingful to develop reordering models based on 
the source sentence types. 
2.3 Framework 
As we mentioned above, our framework is illus-
trated as follows: 
 
Figure 4.  Architecture of the framework, where 
C1 means the special interrogative sentence, C2 
is other interrogative sentence and C3 is non-
question sentence. 
 
Conventional preprocessing approaches di-
vide the translation into two phases: 
                                       (1) 'S S T? ?
'
'
'cS S S T? ? ?
c
cS
'S
                                                
Reordering is first done in the source side 
which changes the source sentence S into reor-
dered one S , and then a standard phrase-based 
translation engine is used to translate the reor-
dered source sentence S  into target language 
sentence T. 
? ??  ? ?? ? ?? ? ? 
In our method, to utilize the information of 
sentence types, a new approach is proposed to 
improve the translation performance by devel-
oping a hybrid model as follows: 
                      (2) 
Before the source sentence is reordered, an 
SVM-based classifier is first employed to de-
termine its sentence type S , then, different re-
ordering model is used to reorder the source 
sentence with the specific sentence type . Af-
ter getting the reordered source sentence , we 
use our phrase-based SMT to obtain the optimal 
target language sentence.  
The contribution of this paper is embodied in 
the first two steps of our method. 
In the first step, an SVM classifier is used to 
identify the type of source sentence2.  
In the second step, two reordering models are 
built according to the different sentence types. A 
phrase-ahead reordering model is developed for 
the special interrogative sentences which uses 
shallow parsing technology to recognize the 
most prominent candidates for reordering (spe-
cial question phrase) and extracts reordering 
templates from bilingual corpus. For other sen-
tence types, we build a phrase-back reordering 
model which uses shallow parsing technology to 
identify the phrases that are almost always 
moved back during translation and applies 
maximum entropy algorithm to determine 
whether we should reorder them. 
Source text 
sentence 3 Models and Algorithms 
In this section, we first introduce the sentence 
type classifier model, and then we describe in 
detail the two reordering models, phrase-ahead 
reordering model and phrase-back reordering 
model. 
3.1 Sentence Type Identification 
Many models are used for classification such as 
Na?ve Bayes, decision tree and maximum en-
tropy. In our approach, we use an SVM-based 
classifier to classify the sentence types. SVM 
 
2 There are three sentence types: special interrogative sen-
tence, other interrogative sentence and non-question sen-
tence, which are defined in sub-section 2.1. 
 
Target 
sentence 
C1 
C3 
C2 
Phrase-ahead 
model 
Phrase-back 
model 
Phrase-
based 
decoder 
SVM 
classifier 
Phrase-back 
model 
1091
has been shown to be highly effective at tradi-
tional text categorization. For our problem, we 
regard a sentence as a text. The decision bound-
ary in SVM is a hyperplane, represented by vec-
tor , which separates the two classes, leaving 
the largest margin between the vectors of the 
two classes (Vapnik, 1998). The search of mar-
gin corresponds to a constrained optimization 
problem. Suppose 
w
G
{1, 1}jc ? ? (positive and 
negative) be the correct class of sentence js , the 
solution can be formalized as: 
: j j j
j
w c?=?G Gs 0j   ? ?              (3) 
Where the js
G
 is feature vector of our sen-
tence js .  We get j? s through solving a dual 
optimization problem. Identifying the type of a 
sentence is just to determine which side of w
G
?s 
hyperplane it will fall in. 
Feature selection is an important issue. We 
directly use all the words occurring in the sen-
tence as features. 
Some readers may argue that the features to 
distinguish the sentence types are very obvious 
in Chinese. For example, ??? can easily sepa-
rate the interrogative sentences from non-
question sentences. In this case, a simple classi-
fier like decision tree will work. It is true when 
the punctuation always appears in the sentence. 
However, sometimes there is no punctuation in 
the spoken language text. Under this situation, 
the decision tree will lose the most powerful 
features, but the performance of SVM is not af-
fected by the punctuations. The experimental 
results verifying this will be given in Section4. 
3.2 Phrase-ahead Reordering Model 
As we mentioned above, about 17.2% of the 
spoken language sentences are special interroga-
tive sentences. Furthermore, we note that each 
Chinese special interrogative sentence has one 
or more special question phrases (SQP) that we 
defined in section 2.1. Due to the difference be-
tween Chinese and English word order, the SQP 
needs to be moved ahead3 when it is translated 
into English. 
    Let S be a Chinese special interrogative sen-
tence, our first problem is to recognize the SQPs 
in S. If we have known the SQP, namely S be-
comes  (  is the left part of  the 0    S SQP S
                                                
1 0S
 
1S
0S
3 There is a specific situation that the SQP don?t have to be 
moved. In this case, we suppose it needs to be moved, but 
the distance is 0. 
sentence before SQP, and  is the right part of 
the sentence after SQP), our second problem is 
to find the correct position in where SQP will 
be moved to. 
 For the first problem, because each syntactic 
component is possible a SQP, for example, ??
?? ? ??? in Figure 1 is NP, ?? ??
(Where)? in Chinese sentence ?? ? ?? ? 
? ? ? ?(Where can I buy the ticket?)? is 
PP (also a VP modifier), ???  ?  (How to 
go)? in ?? ?? ?? ? ?(How to go to the 
beach?)? is VP, it is very difficult to find the 
SQP by syntax. In our model, we first find out 
all the key words, which we list below, in the 
special interrogative sentences through mutual 
information. Then, we define the syntactic com-
ponent containing the key word as an SQP. In-
stead of full syntactic parser, we utilize a CRF 
toolkit named FlexCrfs4 to train, test and predict 
the SQPs chunking. 
 
?? What 
? (?? / ???) Where 
? (?? / ???) How much/many/old?
? (??/??? ?) What about/How 
? (?? / ???) Who/whose/whom 
? (?? / ???) How many/old When?
??? Why 
?(?? / ???) When/where 
Table 1.  The special key words set 
 
For the second problem, we note that there 
are only three positions where the SQP will be 
moved to:  (1) the beginning of the sentence; (2) 
just after the rightmost punctuation (?,?, ?;? or 
?:?) before the SQP; (3) or after a regular phrase 
such as ???  (May I ask)? and ???  ? 
(Please tell me)?. Therefore, we can learn the 
reordering templates from bilingual corpus 5 . 
The simple algorithm is illustrated in Figure 5, 
and some reordering templates are shown in Ta-
bl
                                                
e 2.  
On the whole, When we reorder the special 
interrogative sentence, we first identify the SQP, 
then we find out whether there are punctuations 
(?,? , ?;? or ?:?) before SQP; if any, we keep the 
rightmost punctuation index, otherwise we keep 
the index 0 (beginning of sentence). In the third 
 
4 See http://flexCRF.sourceforge.net 
5 The bilingual corpus is the corpus combined by the train-
ing corpus for chunking SQPs and its corresponding Eng-
lish translation. 
1092
step, if we find that a reordering template like 
some one given in Table 2 can match the sen-
tence, we just apply the template, otherwise we 
just move the SQP after the index that we kept 
efore (0 or punctuation index). 
 
 empirical value N is 10 in our ex-
eriment. 
 
b
 
Figure 5. Reordering template extraction algo-
rithm. The
p
X1?? X2 SQP X1 ?? SQP X2 
X1 ?? ? X2 SQP X1 ?? ? SQP X2 
X1 P X1 ? ? ?? X2 SQ ? ? ?? SQP X2
X1 ? SQP X1 ? P X2? X2 ? SQ  
?? ?? 
 Table 2.  Some reordering templates 
3.3 Phrase-back Reordering Model 
In this paper, we employ the phrase-back reor-
dering model for Chinese other interrogative 
 posi-
tio
makes our model suitable for 
m e
 
??? (sign your name)? 
 identified as a NP. 
 
 
z The form of phrase-back reordering rules: 
sentences and non-question sentences. 
   Inspired by the work of Wang et al (2007), 
we only consider the most prominent candidates 
for reordering. The VP modifiers like PP, TP, 
and SP which we defined in sub-section 2.1 are 
typically in pre-verb position in Chinese but al-
most always appear after the verb in its corre-
sponding English translation. Wang et al (2007) 
concentrate on VP, NP, then determine whether 
their modifiers should be moved back. Instead, 
our interests are focused on the modifiers: PP, 
TP and SP; namely, we consider the modifiers 
PP, TP and SP as triggers, and the first VP oc-
curring after triggers will be the candidate
n where the triggers may be moved to. 
Changing the focus gives us the ability to 
handle a specific situation that there is no VP 
after the triggers for recognition error or other 
reasons. As the example in Figure 6, there is no 
VP after PP (?? ???) because the phrase ??
?? next to PP is wrongly recognized to be a NP. 
To deal with the case, we will further define a 
fake verb phrase (FVP): the phrase after PP (TP 
or SP) until the punctuation (?,?, ?;? or ?.?). The 
phrase ??? (sign your name)? in Figure 6 is 
an FVP. Here, FVP is given the same function 
with VP, thus it 
or  situations. 
 
Figure 6.  An example of FVP. In our model the 
whole sentence is recognized as a VP, ?? ?? 
(here)? is a PP, and 
is
Unlike hard reordering rules of Wang et al 
(2007), we develop a probabilistic reordering 
model to alleviate the impact of the errors 
caused by the parser when recognizing PPs, TPs, 
SPs and VPs. We believe that no reordering is 
better than bad reordering. The rule forms and 
1:  Input: special interrogative sentence pair (s, t) in which 
se which aligns to 
ndex-1] 
NONE then 
; 
_Phrase if Count(C_Phrase)<N 
SQP is labeled and their alignment M is given 
2:  R={} 
3: Find the rightmost punctuation index c_punc_index before 
SQP and English index e_punc_index aligned to 
c_punc_index 
4: Find the smallest index e_smallest_index of English which 
align to the SQP  
 C_Phra5: Get the Chinese phrase
[e_punc_index+1, e_smallest_i
6:  if C_Phrase is 
7:       Continue ; 
8:  end if 
Phrase in R then 8:  if C_
9:       Count(C_Phrase)++; 
10: else 
11:     Insert C_Phrase into R
12:     Count(C_Phrase)=1; 
13: end if 
14: remove C
? ? ??   ?? ? 
the probabilistic model will be given as follows:
A : 1 22
2 1
A XA straight
A XA1 XA A inver
?? ??
 
Where, 1 { , , }A PP TP SP
ted
?   { , }VP FVP?   2A
1 2{ }X phrases between A  and A?  
z We use the Maximum Entropy Model  
which is implemented by Zhang6.  The model is 
trained from bilingual spoken language corpus 
determine whether 1A  should be moved after 
2A . The features that we investigated include 
the leftmost, rightmost, and their POSs 
to 
of 1A  
and 2A . It leads to the following formula: 
exp( ( , ))
( | )
exp( ( , ))
i ii
i iO i
h O A
P O A
h O A
?
?=
?
? ?           (4) 
sWhere, { , }O traignt inverted? , ( , )ih O A  is a 
feature, and i? is the weight of the feature. 
When app  the rules, we first identify 
pairs like ( 1 2A XA ) in the sentence, and then 
m beginning t  
1A  behind 2A  if ( | ) ( | )P inverted A P straight A> . 
After all the pairs are pr
lying
fro o end of the sentence, we move
ocessed, we will get the 
reordered source result. 
                                                 
6http://homepages.inf.ed.ac.uk/s0450736/maxent_too
lkit.html 
15: output R 
1093
4 Experiments 
We have conducted several experiments to 
evaluate the models.  In this section, we first 
introduce the corpora, and then we discuss the 
performance of the SVM-based classifier, 
chunking and reordering models respectively. 
4.1 Corpora 
We perform our experiments on Chinese-to-
English speech translation task. The statistics of 
the corpus is given in Table 3 where CE_train 
means the Chinese-to-English training data re-
leased by IWSLT 2007; CE_sent_filtered means 
the bilingual sentence pairs filtered from the 
open resources of the bilingual sentences on the 
website; CE_dict_filtered means the bilingual 
dictionary filtered from the open resources of 
the bilingual dictionaries on the website; 
CE_dev123 denotes the bilingual sentence pairs 
obtained by the combination of the development 
data IWSLT07_CE_devset1, IWSLT07_CE_devset2 
and IWSLT07_CE_devset3 which are released 
by the IWSLT 2007; CE_dev4 and CE_dev5 are 
the remainder of development data released by 
IWSLT 2007; CE_test means the final test set 
released by IWSLT 2007. 
We combine the data from the top four rows 
as our training set. We use CE_dev4 as our de-
velopment set. CE_dev5 and CE_test are our 
two test data. The test data released by IWSLT 
2007 is based on the clean text with punctuation 
information, so we add the punctuation informa-
tion on the Chinese sentences of CE_dev4 and 
CE_dev5 by our SVM sentence type classifier to 
form the final development set. The detailed 
statistics are given in Table 4. 
4.2 Classification Result 
To evaluate the performance of SVM-based 
classifier on classifying the sentence types, we 
first use a simple decision tree to divide the 
Chinese sentences of our training data for trans-
lation into three sentence types. Then we clean 
them by hand in order to remove the errors. At 
last, 10k sentences for each sentence type are 
randomly selected as the experiment data. For 
each sentence type, 80% of the data are used as 
training data, 20% as test data. Table 5 gives the 
classification results. 
Punctuation in Table 5 means the punctuation 
which occurs at the end of the sentence such as  
??? and ???. We can see from the table that 
SVM classifier performs very well even if we 
remove the punctuations at the end of every sen-
tence. Therefore, almost no errors will be passed 
to the reordering stage. 
 
Data Chinese English 
CE_train 39,953 39,953 
CE_sent_filtered 188,282 188,282 
CE_dict_filtered 31,132 31,132 
CE_dev123 24,192 24,192 
CE_dev4 489 3,423 
CE_dev5 500 3,500 
CE_test 489 2,934 
Table 3.  Statistics of training data, development 
data and test data 
 
 Chinese English 
sentences 276,633 
Train set
words 1,665,073 1,198,984
sentences 489 489*7 Dev set  
CE_dev4 words 6241 47609 
sentences 500 500*7 Test set  
CE_dev5 words 6596 52567 
sentences 489 489*6 Test set   
CE_test words 3166 22574 
Table 4.  Detailed statistics of training data on 
development set 
 
 Accuracy (%)
With punctuation 99.80 
Without punctuation 98.00 
Table 5.  The accuracy of SVM classifier 
4.3 Chunking Results 
In our experiment, except that VPs are obtained 
by a syntactic parser (Klein and Manning, 2003),  
SQPs, PPs, TPs, SPs are all chunked by the 
FlexCrfs. 
The chunking data used for training and test 
in Table 6 are annotated by ourselves. Every 
chunk is  annotated according to the definition 
that we define in sub-section 2.1. The raw train-
ing and test data are all extracted from our train-
ing set for translation. TPs, SPs are annotated 
together; SQPs, PPs are annotated respectively. 
The statistics of the training and test data are 
shown in Table 6. Table 7 gives the chunking 
results. 
The precision, recall and F-Measure are met-
rics for the chunking results. F-Measure follows 
the criteria of CoNLL-20007.  
2*( * )precision recall
F Measure  
precision recall
? = +
                                                 
7 See  http://www.cnts.ua.ac.be/conll2000/chunking/ 
1094
Because the SQPs have the regularity that 
each one contains a key word listed in Table 1, 
the result of SQPs chunking is quite good. 
Moreover, the chunking of PPs, TPs and SPs 
also performs well. 
 
 Train Test 
sentences 10,000 500 SQP 
chunks 10030 501 
sentences 10,000 500 PP 
chunks 10106 512 
sentences 11,000 500 SP and TP 
chunks 10342 523 
Table 6.  Statistics of train and test data 
 
 Precision (%) 
Recall 
(%) 
F-Measure 
(%) 
SQP 95.52 95.52 95.52 
PP 94.65 93.31 93.98 
SP and TP 93.92 92.68 93.25 
Table 7.  Chunking results on test set 
4.4 Translation Results 
For the translation experiments, BLEU-4 and 
NIST are used as the evaluation metric. The 
baseline SMT uses the standard phrase-based 
decoder that applies the log-linear model (Och 
and Ney, 2002).  
  In the preprocessing module, all the Chinese 
words are segmented by the free software toolkit 
ICTCLAS3.08, and the POS tags are obtained 
by using the Stanford parser with its POS pars-
ing function. For the decoder, the phrase table is 
obtained as described in (Koehn et al, 2005), 
and our 4-gram language model is trained by the 
open SRILM9 toolkit. It should be noted that we 
use monotone decoding in translation. 
We have done three groups of experiments 
for translation. The first one is to test the effect 
of phrase-ahead reordering model, the result of 
which is shown in Table 8. Compared to the 
baseline system, phrase-ahead reordering model 
improves the results of the two test sets by 
0.41% and 1.87% in BLEU respectively. The 
difference in the performance gains can be at-
tributed to the fact that there are 100 Chinese 
special interrogative sentences in Test 2, while 
only 30 are found in Test 1. Accordingly, the 
reordering candidates of Test 1 are much fewer 
than that of Test 2. Thus, we can conclude that 
the more special interrogative sentences the bet-
ter performance of the translation. Furthermore, 
                                                 
8 See http://www.nlp.org.cn 
9 See http://www.speech.sri.com/projects/srilm 
the results show that the reordering on special 
interrogative sentences is a good try. 
The second experiment is conducted to test 
the effect of phrase-back reordering model. Ta-
ble 8 gives the results. For the two test sets, the 
model brings an improvement to the baseline by 
2.24% and 0.93% in BLEU respectively. How-
ever, the difference between them is still very 
big. We think there are two reasons: firstly, 
there are much more special interrogative sen-
tences in Test 2 than in Test 1, so the sentences 
of other sentence types in Test 2 are much fewer 
than that in Test 1. Thus, fewer candidates are 
found in Test 2 than in Test 1. Secondly, the 
average sentence length of Test 2 (6.5 words) is 
much shorter than that of Test 1 (13.2 words). 
We know that if the sentence is very short, the 
PP, TP, and SP will seldom occur. Naturally, 
only 89 candidates are found in Test 2 but 366 
in Test 1. Regardless of the difference, the 
phrase-back reordering model indeed improves 
the translation quality significantly. 
The last experiment merges the two reorder-
ing model together. The results in Table 8 show 
that the overall reordering model has done very 
well in both test sets: it improves the two test 
sets by 2.65% and 2.78% in BLEU score respec-
tively. It demonstrates that every reordering 
model has a positive effect on translation. 
Therefore, our reordering model based on the 
sentence type is quite successful. 
5 Conclusions and Future Work 
In this paper, we have investigated the effect of 
the Chinese sentence types on reordering prob-
lem for Chinese-to-English statistical machine 
translation. We have succeeded in applying a 
phrase-ahead reordering model to process the 
special interrogative sentences and a phrase-
back reordering model to deal with other sen-
tence types. Experiments show that our reorder-
ing model obtains a significant improvement in 
BLEU score on the IWSLT-07 task. 
With the encouraging experimental results, 
we believe that we can mine more reordering 
information from the Chinese sentence types. In 
this paper, we only apply a phrase-back model 
to reorder Chinese other interrogative sentences. 
In the next step, we will try to develop a special 
reordering model for this sentence type. Fur-
thermore, we plan to integrate the phrase-back 
model into phrase-ahead model for special inter-
rogative sentences and investigate the value of 
this integration. 
1095
 Table 8.  Statistics of translation results 
Notes: candidates here mean how many candidate reordering phrases are recognized for each model. Sentences 
mean the number of sentences belonging to the specific sentence type, i.e. for phrase-ahead reordering in Test 1, 
31 special question phrases (SQP) are recognized in 30 Chinese special interrogative sentences. 
 
Acknowledgments 
The research work described in this paper has 
been partially supported by the Natural Science 
Foundation of China under Grant No. 60575043 
and 60736014, the National High-Tech Research 
and Development Program (863 Program) of 
China under Grant No. 2006AA01Z194 and 
2006AA010108, the National Key Technologies 
R&D Program of China under Grant No. 
2006BAH03B02, and Nokia (China) Co. Ltd as 
well. 
 
References 
Cao Wang, Michael Collins and Philipp Koehn. 2007. 
Chinese syntactic reordering for statistical machine 
translation. In Proceedings of joint Conference on 
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, 2007.  
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou 
Minghui Li and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical 
machine translation. In Proceedings of 45th Meet-
ing of the Association for Computational Linguis-
tics . 
Dan Klein and Christopher D. manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of  41st  
Meeting of the Association for Computational Lin-
guistics.  
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Pro-
ceedings of 43rd Meeting of Association for Com-
putational Linguistics.  
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. maxi-
mum entropy based phrase reordering model for 
statistical machine translation. In Proceedings of 
the joint conference of the International Committee 
on Computational Linguistics and the Association 
for Computational Linguistics 2006.  
Fei Xia and Michael McCord. 2004. Improving a Sta-
tistical MT system with automatically learned re-
write patterns. In Proceedings of 20th International 
Conference on Computational Linguistics. 
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for 
statistical machine translation. In Proceedings of 
40th Meeting of Association for Computational 
Linguistics.  
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine 
translation. Computational Linguistics, 30:417-449 
Marta R. Costa-jussa and Jose A.R. Fonollosa. 2006. 
Statistical machine reordering. In proceedings of 
Conference on Empirical Methods in Natural Lan-
guage Processing 2006.  
Michael Collins, Philipp Koehn, and Ivona Kucerova. 
2005. Clause restructuring for statistical machine 
translation. In proceedings of 43rd Meeting of the 
Association for Computational Linguistics.  
Philipp Koehn, Franz J. Och. and Daniel Marcu. 2003. 
Statistical Phrase-based Translation. In proceed-
ings of HLT-NAACL 2003. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne and 
David Talbot. 2005. Edinburgn System Description 
for the 2005 IWSLT Speech Translation Evalua-
tion. In International Workshp on Spoken Lan-
guage Translation. 
Yuqi Zhang, Richard Zens and Hermann Ney. 2007. 
Chunk-level reordering of source language sen-
tence with automatically learned rules for statistical 
machine translation. In Proceedings of SSST, 
NAACL-HLT 2007/AMTA Workshop on Syntax and 
Structure in Statistical Translation. 
Vladimir Naumovich Vapnik. 1998. Statistical Learn-
ing Theory. John Wiley and Sons, Inc.  
 BLEU (%) NIST Sentences Candidates
Baseline 32.16 6.4844 500  
Phrase-ahead reordering 32.57 6.5579 30 31 
Phrase-back reordering 34.40 6.6857 470 366 
Test 1 
CE_dev5 
Phrase-ahead+phrase-back 34.81 6.7584 500 397 
Baseline 34.04 5.8340 489  
Phrase-ahead reordering 35.91 6.0693 100 97 
Phrase-back reordering 34.97 5.9172 389 89 
Test 2 
CE_test 
Phrase-ahead+phrase-back 36.82 6.1535 489 186 
1096
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 204?215,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Augmenting String-to-Tree Translation Models with Fuzzy Use of 
Source-side Syntax 
Jiajun Zhang, Feifei Zhai and Chengqing Zong 
Institute of Automation, Chinese Academy of Sciences 
Beijing, China 
{jjzhang, ffzhai, cqzong}@nlpr.ia.ac.cn 
 
 
 
 
 
Abstract 
Due to its explicit modeling of the 
grammaticality of the output via target-side 
syntax, the string-to-tree model has been 
shown to be one of the most successful 
syntax-based translation models. However, 
a major limitation of this model is that it 
does not utilize any useful syntactic 
information on the source side. In this 
paper, we analyze the difficulties of 
incorporating source syntax in a string-to-
tree model. We then propose a new way to 
use the source syntax in a fuzzy manner, 
both in source syntactic annotation and in 
rule matching. We further explore three 
algorithms in rule matching: 0-1 matching, 
likelihood matching, and deep similarity 
matching. Our method not only guarantees 
grammatical output with an explicit target 
tree, but also enables the system to choose 
the proper translation rules via fuzzy use of 
the source syntax. Our extensive 
experiments have shown significant 
improvements over the state-of-the-art 
string-to-tree system. 
1 Introduction 
In recent years, statistical translation models based 
upon linguistic syntax have shown promising 
progress in improving translation quality. It 
appears that encoding syntactic annotations on 
either side or both sides in translation rules can 
increase the expressiveness of rules and can 
produce more accurate translations with improved 
reordering.  
One of the most successful syntax-based models 
is the string-to-tree model (Galley et al, 2006; 
Marcu et al, 2006; Shen et al, 2008; Chiang et al, 
2009). Since it explicitly models the 
grammaticality of the output via target-side syntax, 
the string-to-tree model (Xiao et al, 2010) 
significantly outperforms both the state-of-the-art 
phrase-based system Moses (Koehn et al, 2007) 
and the formal syntax-based system Hiero (Chiang, 
2007). However, there is a major limitation in the 
string-to-tree model: it does not utilize any useful 
source-side syntactic information, and thus to some 
extent lacks the ability to distinguish good 
translation rules from bad ones. 
The source syntax is well-known to be helpful in 
improving translation accuracy, as shown 
especially by tree-to-string systems (Quirk et al, 
2005; Liu et al, 2006; Huang et al, 2006; Mi et al, 
2008; Zhang et al, 2009).  The tree-to-string 
systems are simple and efficient, but they also have 
a major limitation: they cannot guarantee the 
grammaticality of the translation output because 
they lack target-side syntactic constraints.  
Thus a promising solution is to combine the 
advantages of the tree-to-string and string-to-tree 
approaches. A natural idea is the tree-to-tree model 
(Ding and Palmer, 2005; Cowan et al, 2006; Liu et 
al., 2009). However, as discussed by Chiang 
(2010), while tree-to-tree translation is indeed 
promising in theory, in practice it usually ends up 
over-constrained. Alternatively, Mi and Liu (2010) 
proposed to enhance the tree-to-string model with 
target dependency structures (as a language model). 
In this paper, we explore in the other direction: 
based on the strong string-to-tree model which 
builds an explicit target syntactic tree during 
decoding rather than apply only a syntactic 
language model, we aim to find a useful way to 
incorporate the source-side syntax. 
204
First, we give a motivating example to show the 
importance of the source syntax for a string-to-tree 
model. Then we discuss the difficulties of 
integrating the source syntax into the string-to-tree 
model. Finally, we propose our solutions. 
Figure 1 depicts a standard process that 
transforms a Chinese string into an English tree 
using several string-to-tree translation rules. The 
tree with solid lines is produced by the baseline 
string-to-tree system. Although the yield is 
grammatical, the translation is not correct since the 
system mistakenly applies rule r2, thus translating 
the Chinese preposition ? (h? ) in the example 
sentence into the English conjunction and. As a 
result, the Chinese prepositional phrase ?? ?? 
??  ?? (?with terrorist networks?) is wrongly 
translated as a part of the relevant noun phrase 
(?[Hussein] and terrorists networks?). Why does 
this happen? We find that r2 occurs 103316 times 
in our training data, while r3 occurs only 1021 
times. Thus, without source syntactic clues, the 
Chinese word ? (h ? ) is converted into the 
conjunction and in most cases. In general, this 
conversion is correct when the word?(h?) is used 
as a conjunction. But?(h?) is a preposition in the 
source sentence. If we are given this source 
syntactic clue, rule r3 will be preferred. This 
example motivates us to provide a moderate 
amount of source-side syntactic information so as 
to obtain the correct English tree with dotted lines 
(as our proposed system does). 
A natural question may arise that is it easy to 
incorporate source syntax in the string-to-tree 
model? To the best of our knowledge, no one has 
studied this approach before. In fact, it is not a 
trivial question if we look into the string-to-tree 
model. We find that the difficulties lie in at least 
three problems: 1) For a string-to-tree rule such as 
r6 in figure 1, how should we syntactically annotate 
its source string? 2) Given the source-annotated 
string-to-tree rules, how should we match these 
rules according to the test source tree during 
decoding? 3) How should we binarize the source-
annotated string-to-tree rules for efficient decoding? 
For the first problem, one may require the 
source side of a string-to-tree rule to be a 
constituent. However, such excessive constraints 
will exclude many good string-to-tree rules whose 
source strings are not constituents. Inspired by 
Chiang (2010), we adopt a fuzzy way to label 
every source string with the complex syntactic 
categories of SAMT (Zollmann and Venugopal, 
2006). This method leads to a one-to-one 
correspondence between the new rules and the 
string-to-tree rules. We will detail our fuzzy 
labeling method in Section 2. 
For the second problem, it appears simple and 
intuitive to match rules by requiring a rule?s source 
syntactic category to be the same as the category of 
the test string. However, this hard constraint will 
greatly narrow the search space during decoding. 
Continuing to pursue the fuzzy methodology, we 
adopt a fuzzy matching procedure to enable 
matching of all the rules whose source strings 
match the test string, and then determine the 
degree of matching between the test source tree 
and each rule. We will discuss three fuzzy 
matching algorithms, from simple to complex, in 
Section 3. 
The third question is a technical problem, and 
we will give our solution in Section 4. 
Our method not only guarantees the 
grammaticality of the output via the target tree 
structure, but also enables the system to choose 
appropriate translation rules during decoding 
through source syntactic fuzzy labeling and fuzzy 
matching.  
The main contributions of this paper are as 
follows: 
1) We propose a fuzzy method for both source 
syntax annotation and rule matching for 
augmenting string-to-tree models. 
2) We design and investigate three fuzzy rule 
matching algorithms: 0-1 matching, 
likelihood matching, and deep similarity 
matching. 
We hope that this paper will demonstrate how to 
effectively incorporate both source and target 
syntax into a translation model with promising 
results. 
2 Rule Extraction 
Since we annotate the source side of each string-to-
tree rule with source parse tree information in a 
fuzzy way, we will henceforward denote the 
source-syntax-decorated string-to-tree rule as a 
fuzzy-tree to exact-tree rule. We first briefly 
review issues of string-to-tree rule extraction; then 
we discuss how to augment the string-to-tree rules 
to yield fuzzy-tree to exact-tree rules. 
205
 Figure 1:  Two alternative derivations for a sample string-to-tree translation. The rules used are listed on the right. 
The target yield of the tree with solid lines is hussein and terrorist networks established relations. The target yield 
of the tree with dotted lines is hussein established relations with terrorist networks. 
 
2.1 String-to-Tree Rule Extraction 
Galley et al (2004) proposed the GHKM algorithm 
for extracting (minimal) string-to-tree translation 
rules from a triple (f, et, a), where f is the source-
language sentence, et is a target-language parse tree 
whose yield e is the translation of f, and a is the set 
of word alignments between e and f. The basic idea 
of GHKM is to obtain the set of minimally-sized 
translation rules which can explain the mappings 
between source string and target parse tree. The 
minimal string-to-tree rules are extracted in three 
steps: (1) frontier set computation; (2) 
fragmentation; and (3) extraction. 
  The frontier set (FS) is the set of potential points 
at which to cut the graph G constructed by the 
triple (f, et, a) into fragments. A node satisfying the 
word alignment is a frontier. Bold italic nodes in 
the English parse tree in Figure 2 are all frontiers. 
   Given the frontier set, a well-formed 
fragmentation of G is generated by restricting each 
fragment to take only nodes in FS as the root and 
leaf nodes. 
   With fragmentation completed, the rules are 
extracted through a depth-first traversal of te : for 
each frontier being visited, a rule is extracted. 
These extracted rules are called minimal rules 
(Galley et al, 2004). For example, rules r ra i? in 
Figure 2 are part of the total of 13 minimal rules.  
To improve the rule coverage, SPMT models 
can be employed to obtain phrasal rules (Marcu et 
at., 2006). In addition, the minimal rules which 
share the adjacent tree fragments can be connected 
together to form composed rules (Galley et al, 
2006). In Figure 2, jr  is a rule composed by 
combining cr and gr . 
2.2 Fuzzy-tree to Exact-tree Rule Extraction 
Our fuzzy-tree to exact-tree rule extraction works 
on word-aligned tree-to-tree data (Figure 2 
illustrates a Chinese-English tree pair).  Basically, 
the extraction algorithm includes two parts: 
(1) String-to-tree rule extraction (without 
considering the source parse tree); 
(2) Decoration of the source side of the string-to-
tree rules with syntactic annotations. 
We use the same algorithm introduced in the 
previous section for extracting the base string-to-
tree rules. The source-side syntactic decoration is 
much more complicated. 
The simplest way to decorate, as mentioned in 
the Introduction, is to annotate the source-side of a 
string-to-tree rule with the syntactic tag that 
exactly covers the source string. This is what the 
exact tree-to-tree procedure does (Liu et al, 2009). 
However, many useful string-to-tree rules will 
become invalid if we impose such a tight 
restriction. For example, in Figure 2, the English 
phrase discuss ? them is a VP, but its Chinese 
counterpart is not a constituent. Thus we will miss 
the rule rh although it is a useful reordering rule. 
According to the analysis of our training data, the 
rules with rigid source-side syntactic constraints 
account for only about 74.5% of the base string-to-
tree rules. In this paper, we desire more general 
applicability. 
206
IP
NP VP
ADJP PP VP
AD P NP VP NP
PN VV NN
PN
?
?? ?
?? ?? ??
i
the
happy
to
discuss
matter
am
with them
NPIN
PP
NNDT
NPVB
VPTO
 VPJJ
  ADJPVBP
VP
S
NP
FW
rb: ?? JJ(happy)
String-to-Tree rules:
ra: ? FW(i)
rm: ?{P} IN(with)
rd: ?? NP(them)re: ?? VB(discuss)rf: ?? NP(DT(the) NN(matter))rg: x0 x1 PP(x0:IN x1:NP)rh: x2 x0 x1 VP(x0:VB x1:NP x2:PP)ri: x0 VP(TO(to) x0:VP)
rj: ? x0 PP(IN(with) x0:VP)
Fuzzy-tree to exact-tree rules:
rk: ?{PN} FW(i)
rl: ??{AD} JJ(happy)
rc: ? IN(with)
rn: x2 x0 x1{PP*VP} VP(x0:VB x1:NP x2:PP)
ro: x0{PP*VP} VP(TO(to) x0:VP)
...
...
 Figure 2:  A sample Chinese-English tree pair for rule extraction. The bold italic nodes in the target English tree are 
frontiers. Note that string-to-tree rules are extracted without considering source-side syntax (upper-right). The new 
fuzzy-tree to exact-tree rules are extracted with both-side parse trees (bottom-right). 
 
Inspired by (Zollmann and Venugopal, 2006; 
Chiang, 2010), we resort to SAMT-style syntactic 
categories in the style of categorial grammar (Bar-
Hillel, 1953). The annotation of the source side of 
string-to-tree rules is processed in three steps: (1) 
If the source-side string corresponds to a syntactic 
category C in the source parse tree, we label the 
source string with C. (2) Otherwise, we check if 
there exists an extended category of the forms 
C1*C2, C1/C2 or C2\C11, indicating respectively that 
the source string spans two adjacent syntactic 
categories, a partial syntactic category C1 missing a 
C2 on the right, or a partial C1 missing a C2 on the 
left. (3) If the second step fails, we check if there is 
an extended category of the forms C1*C2*C3 or 
C1..C2, showing that the source string spans three 
adjacent syntactic categories or a partial category 
with C1 and C2 on each side. In the worst case, 
C1..C2 can denote every source string, thus all of 
the decorations in our training data can be 
explained within the above three steps. Using the 
SAMT-style grammar, each source string can be 
associated with a syntactic category. Thus our 
fuzzy-tree to exact-tree extraction does not lose 
                                                          
1 The kinds of categories are checked in order. This means that 
if C1*C2, C1/C2 can both describe the same source string, we will choose C1*C2. 
any rules as compared with string-to-tree 
extraction. For example, rule ro in Figure 2 uses the 
product category *PP VP  on the source side. 
A problem may arise: How should we handle the 
situation where several rules are observed which 
only differ in their source-side syntactic categories? 
For example, besides the rule rm in Figure 2, we 
encountered rules like ? ? ? ?CC IN with??  in the 
training data. Which source tag should we retain? 
We do not make a partial choice in the rule 
extraction phase. Instead, we simply make a union 
of the relevant rules and retain the respective tag 
counts. Applying this strategy, the rule takes the 
form of ? ? ? ?: 6, : 4P CC IN with?? 2, indicating that 
the source-side preposition tag appears six times 
while the conjunction occurs four times. Note that 
the final rule format used in translation depends on 
the specific fuzzy rule matching algorithm adopted. 
3 Fuzzy Rule Matching Algorithms 
The extracted rules will ultimately be applied to 
derive translations during decoding. One way to 
apply the fuzzy-tree to exact-tree rules is to narrow 
the rule search space. Given a test source sentence 
                                                          
2 6 and 4 are not real counts. They are used for illustration 
only. 
207
with its parse tree, we can according to this 
strategy choose only the rules whose source syntax 
matches the test source tree.  However, this 
restriction will rule out many potentially correct 
rules. In this study, we keep the rule search space 
identical to that of the string-to-tree setting, and 
postpone the use of source-side syntax until the 
derivation stage. During derivation, a fuzzy 
matching algorithm will be adopted to compute a 
score to measure the compatibility between the 
rule and the test source syntax. The translation 
model will learn to distinguish good rules from bad 
ones via the compatibility scores. 
   In this section, three fuzzy matching algorithms, 
from simple to complex, are investigated in order. 
3.1 0-1 Matching 
0-1 matching is a straightforward approach that 
rewards rules whose source syntactic category 
exactly matches the syntactic category of the test 
string and punishes mismatches. It has mainly been 
employed in hierarchical phrase-based models for 
integrating source or both-side syntax (Marton and 
Resnik, 2008; Chiang et al, 2009; Chiang, 2010). 
Since it is verified to be very effective in 
hierarchical models, we borrow this idea in our 
source-syntax-augmented string-to-tree translation.  
In 0-1 matching, the rule?s source side must 
contain only one syntactic category, but a rule may 
have been decorated with more than one syntactic 
category on the source side. Thus we have to 
choose the most reliable category and discard the 
others. Here, we select the one with the highest 
frequency. For example, the tag P in the rule 
? ? ? ?: 6, : 4P CC IN with??  appears more frequently, 
so the final rule used in 0-1 matching will be 
? ? ? ?P IN with?? . Accordingly, we design two 
features: 
1. match_count calculates in a derivation the 
number of rules whose source-side syntactic 
category matches the syntactic category of the 
test string. 
2. unmatch_count counts the number of 
mismatches. 
For example, in the derivations of Figure 1, we 
know the Chinese word?(h?)  is a preposition in 
this sentence (and thus can be written as P(?)), 
therefore, match_count += 1 if the above rule 
? ? ? ?P IN with?? is employed. 
These two features are integrated into the log-
linear translation model and the corresponding 
feature weights will be tuned along with other 
model features to learn which rules are preferred. 
3.2 Likelihood Matching 
It appears intuitively that the 0-1 matching 
algorithm does not make full use of the source-side 
syntax because it keeps only the most-frequent 
syntactic label and discards some potentially useful 
information. Therefore, it runs the risk of treating 
all the discarded source syntactic categories of the 
rule as equally likely. For example, there is an 
extracted rule as follows: 
? ? ? ?:11233, :11073, : 65DEC DEG DEV IN of??  
 0-1 matching converts it into ? ? ? ?DEC IN of?? . 
The use of this rule will be penalized if the 
syntactic category of the test string ?(d?) is parsed 
as DEG or DEV. On one hand, the frequency of the 
tag DEG is just slightly less than that of DEC, but 
the 0-1 matching punishes the former while 
rewarding the latter. On the other hand, the 
frequency of DEG is much more than that of DEV, 
but they are penalized equally. It is obvious that 
the syntactic categories are not finely distinguished. 
   Considering this situation, we propose the 
likelihood matching algorithm. First, we compute 
the likelihood of the rule?s source syntactic 
categories. Since we need to deal with the potential 
problem that the rule is hit by the test string but the 
syntactic category of the test string is not in the 
category set of the rule?s source side, we apply the 
m-estimate of probability (Mitchell, 1997) to 
calculate a smoothed likelihood 
c
c
n mplikelihood n m
?? ?                     (1) 
in which nc is the count of each syntactic category 
c in a specific rule, n denotes the total count of the 
rule, m is a constant called the equivalent sample 
size, and p is the prior probability of the category c. 
In our work, we set the constant m=1 and the prior 
p to 1/12599 where 12599 is the total number of 
source-side syntactic categories in our training data.  
For example, the rule ? ? ? ?: 6, : 4P CC IN with??  
becomes ? ? ? ?: 0.545, : 0.364, 7.2 -6P CC e IN with? ?  
after likelihood computation. Then, if we apply 
likelihood matching in the derivations in Figure 1 
where the test string is? and its syntax is P(?), 
208
the matching score with the above rule will be 
0.545. When the test Chinese word ? is parsed as 
a category other than P or CC, the matching score 
with the above rule will be 7.2e-6. 
   Similar to 0-1 matching, likelihood matching will 
serve as an additional model feature representing 
the compatibility between categories and rules. 
3.3 Deep Similarity Matching 
Considering the two algorithms above, we can see 
that the purpose of fuzzy matching is in fact to 
calculate a similarity. 0-1 matching assigns 
similarity 1 for exact matches and 0 for mismatch, 
while likelihood matching directly utilizes the 
likelihood to measure the similarity. Going one 
step further, we adopt a measure of deep similarity, 
computed using latent distributions of syntactic 
categories. Huang et al (2010) proposed this 
method to compute the similarity between two 
syntactic tag sequences, used to impose soft 
syntactic constraints in hierarchical phrase-based 
models. Analogously, we borrow this idea to 
calculate the similarity between two SAMT-style 
syntactic categories, and then apply it to calculate 
the degree of matching between a translation rule 
and the syntactic category of a test source string 
for purposes of fuzzy matching. We call this 
procedure deep similarity matching. 
Instead of directly using the SAMT-style 
syntactic categories, we represent each category by 
a real-valued feature vector. Suppose there is a set 
of n latent syntactic categories ? ?1, , nV v v? ?  (n=16 
in our experiments). For each SAMT-style 
syntactic category, we compute its distribution of 
latent syntactic categories ? ? ? ? ? ?? ?1 , ,c c c nP V P v P v?? ? .  
For example, ? ? ? ?* 0.4, 0.2, 0.3, 0.1VP NPP V ??  means that 
the latent syntactic categories v1, v2, v3, v4 are 
distributed as p(v1)=0.4, p(v2)=0.2, p(v3)=0.3 and 
p(v4)=0.1 for the SAMT-style syntactic category 
VP*NP. Then we further transform the distribution 
to a normalized feature vector 
? ? ? ? ? ?c cF c P V P V?? ? ?  to represent the SAMT-style 
syntactic category c. 
With the real-valued vector representation for 
each SAMT-style syntactic category, the degree of 
similarity between two syntactic categories can be 
simply computed as a dot-product of their feature 
vectors: 
? ? ? ? ? ? ? ?
1
' 'i i
i n
F c F c f c f c
? ?
? ? ??? ??                   (2) 
This computation yields a similarity score ranging 
from 0 (totally different syntactically) to 1 (totally 
identical syntactically). 
Since we can now compute the similarity of any 
syntactic category pair, we are currently ready to 
compute the matching degree between the 
syntactic category of a test source string and a 
fuzzy-tree to exact-tree rule. To do this, we first 
convert the original fuzzy-tree to exact-tree rule to 
the rule of likelihood format without any 
smoothing. For example, the rule 
? ? ? ?: 6, : 4P CC IN with? ? becomes 
? ? ? ?: 0.6, : 0.4P CC IN with?? after conversion. We 
then denote the syntax of a rule?s source-side RS 
by weighting all the SAMT-style categories in RS 
? ? ? ? ? ?RS
c RS
F RS P c F c
?
? ?? ?                     (3) 
where ? ?RSP c  is the likelihood of the category c. 
Finally, the deep similarity between a SAMT-style 
syntactic category tc of a test source string and a 
fuzzy-tree to exact-tree rule is computed as follows: 
? ? ? ? ? ?,DeepSim tc RS F tc F RS? ?? ?                   (4) 
This deep similarity score will serve as a useful 
feature in the string-to-tree model which will 
enable the model to learn how to take account of 
the source-side syntax during translation. 
We have ignored the details of latent syntactic 
category induction in this paper. In brief, the set of 
latent syntactic categories is automatically induced 
from a source-side parsed, word-aligned parallel 
corpus. The EM algorithm is employed to induce 
the parameters. We simply follow the algorithm of 
(Huang et al, 2010), except that we replace the tag 
sequence with SAMT-style syntactic categories.  
4 Rule Binarization 
In the baseline string-to-tree model, the rules are 
not in Chomsky Normal Form. There are several 
ways to ensure cubic-time decoding. One way is to 
prune the extracted rules using a scope-3 grammar 
and do SCFG decoding without binarization 
(Hopkins and Lengmead, 2010). The other, and 
most popular way is to binarize the translation 
rules (Zhang et al, 2006). We adopt the latter 
approach for efficient decoding with integrated n-
gram language models since this binarization 
technique has been well studied in string-to-tree 
209
translation. However, when the rules? source string 
is decorated with syntax (fuzzy-tree to exact-tree 
rules), how should we binarize these rules? 
    We use the rule rn in Figure 2 for illustration: ? ? ? ?2 0 1 0 1 2: * : : :nr x x x PP VP VP x VB x NP x PP? . 
Without regarding the source-side syntax, we 
obtain the following two binarized rules: ? ?
? ?
0 1
0 1
2 0*1 0*1 * 2
0 1 * 0 1
1: : :
2 : : :
x x
x x
B x x VP x V x PP
B x x V x VB x NP
?
?
 
Since the source-side syntax PP*VP in rule rn 
only accounts for the entire source side, it is 
unclear how to annotate the source side of a partial 
rule such as the second binary rule B2.  
Analyzing the derivation process, we observe 
that a partial rule such as binary rule B2 never 
appears in the final derivation unless the rooted 
binary rule B1 also appears in the derivation. 
Based on this observation, we design a heuristic3 
strategy: we simply attach the syntax PP*VP in the 
rooted binary rule B1, and do not decorate other 
binary rules with source syntax. Thus rule rn will 
be binarized as: 
? ? ? ? ? ?
? ? ? ?
0 1
0 1
2 0*1 0*1 * 2
0 1 * 0 1
1 * : :
2 : :
x x
x x
x x PP VP VP x V x PP
x x V x VB x NP
?
?
 
5 Translation Model and Decoding 
The proposed translation system is an 
augmentation of the string-to-tree model. In the 
baseline string-to-tree model, the decoder searches 
for the optimal derivation *d  that parses a source 
string f into a target tree et from all possible 
derivations D: 
? ?? ? ? ?
? ?
*
1 2
3
arg max log
|
LMd D
d p d d
d R d f
? ? ? ?
?
?
? ?
? ?
                  (5) 
where the first element is a language model score 
in which ? ?d?  is the target yield of derivation d ; 
the second element is the translation length penalty; 
the third element is used to control the derivation 
length; and the last element is a translation score 
that includes six features: 
                                                          
3 We call it heuristic because there may be other syntactic 
annotation strategies for the binarized rules. It should be noted 
that our strategy makes the annotated binarized rules 
equivalent to the original rule. 
? ? ? ? ? ?
? ? ? ?
? ? ? ?
4 5
6 7
8 9
| log | ( ) log | ( )
log | ( ) log ( ) | ( )
log ( ) | ( ) _
r d
lex
lex
R d f p r root r p r lhs r
p r rhs r p lhs r rhs r
p rhs r lhs r is comp
? ?
? ?
? ? ?
?
? ?
? ?
? ?
?
(6) 
In equation (6), the first three elements denote the 
conditional probability of the rule given the root, 
the source-hand side, and the target-hand side. The 
next two elements are bidirectional lexical 
translation probabilities. The last element is the 
preferred binary feature for learning: either the 
composed rule or the minimal rule. 
    In our source-syntax-augmented model, the 
decoder also searches for the best derivation. With 
the help of the source syntactic information, the 
derivation rules in our new model are much more 
distinguishable than that in the string-to-tree model: 
? ?? ? ? ?
? ?
*
1 2
3
arg max log
|
LMd D
d p d d
d R d f
? ? ? ?
?
?
? ?
? ?
            (7) 
Here, all elements except the last one are the same 
as in the string-to-tree model. The last item is: 
? ? ? ?
? ? ? ?? ?
? ? ? ?? ?
? ? ? ? ? ?? ?
10
11
12 13
| |
log ,
log ,
01
r d
R d f R d f
DeepSim DeepSim tag r
likelihood likelihood tag r
match unmatch
? ?
? ?
? ? ? ? ?
?
?
?
?
? ?
?      (8) 
The 0-1 matching4 is triggered only when we set 
? ?01 1? ? . The other two fuzzy matching algorithms 
are triggered in a similar way. 
During decoding, we use a CKY-style parser 
with beam search and cube-pruning (Huang and 
Chiang, 2007) to decode the new source sentences. 
6 Experiments 
6.1 Experimental Setup 
The experiments are conducted on Chinese-to-
English translation, with training data consisting of 
about 19 million English words and 17 million 
Chinese words5. We performed bidirectional word 
alignment using GIZA++, and employed the grow-
diag-final balancing strategy to generate the final 
                                                          
4  In theory, the features unmatch_count, match_count and 
derivation_length are linearly dependent, so the 
unmatch_count is redundant. In practice, since the derivation 
may include glue rules which are not scored by fuzzy 
matching. Thus, "unmatch_count + match_count + 
glue_rule_number = derivation_length". 
5  LDC catalog number: LDC2002E18, LDC2003E14, 
LDC2003E07, LDC2004T07 and LDC2005T06. 
210
symmetric word alignment. We parsed both sides 
of the parallel text with the Berkeley parser (Petrov 
et al, 2006) and trained a 5-gram language model 
with the target part of the bilingual data and the 
Xinhua portion of the English Gigaword corpus. 
    For tuning and testing, we use NIST MT 
evaluation data for Chinese-to-English from 2003 
to 2006 (MT03 to MT06). The development data 
set comes from MT06 in which sentences with 
more than 20 words are removed to speed up 
MERT6 (Och, 2003). The test set includes MT03 
to MT05. 
   We implemented the baseline string-to-tree 
system ourselves according to (Galley et al, 2006; 
Marcu et al, 2006). We extracted minimal GHKM 
rules and the rules of SPMT Model 1 with source 
language phrases up to length L=4. We further 
extracted composed rules by composing two or 
three minimal GHKM rules. We also ran the state-
of-the-art hierarchical phrase-based system Joshua 
(Li et al, 2009) for comparison. In all systems, we 
set the beam size to 200. The final translation 
quality is evaluated in terms of case-insensitive 
BLEU-4 with shortest length penalty. The 
statistical significance test is performed using the 
re-sampling approach (Koehn, 2004). 
6.2 Results 
Table 1 shows the translation results on 
development and test sets. First, we investigate the 
performance of the strong baseline string-to-tree 
model (s2t for short). As the table shows, s2t 
outperforms the hierarchical phrase-based system 
Joshua by more than 1.0 BLEU point in all 
translation tasks. This result verifies the superiority 
of the baseline string-to-tree model. 
   With the s2t system providing a baseline, we 
further study the effectiveness of our source-
syntax-augmented string-to-tree system with 
fuzzy-tree to exact-tree rules (we use FT2ET to 
denote our proposed system). The last three lines 
in Table 1 show that, for each fuzzy matching 
algorithm, our new system TF2ET performs 
significantly better than the baseline s2t system, 
with an improvement of more than 0.5 absolute 
BLEU points in all tasks. This result demonstrates 
the success of our new method of incorporating 
source-side syntax in a string-to-tree model. 
                                                          
6 The average decoding speed is about 50 words per minute in 
the baseline string-to-tree system and our proposed systems. 
System MT06
(dev)
MT03 MT04 MT05
Joshua 29.42 28.62 31.52 31.39 
s2t 30.84 29.75 32.68 32.41 
0-1 31.61** 30.60** 33.45** 33.37**
LH 31.35* 30.34* 33.21* 33.05*
 
FT2ET
DeepSim 31.77** 30.82** 33.69** 33.50**
Table 1: Results (in BLEU scores) of different 
translation models in multiple tasks. LH=likelihood. 
*or**=significantly better than s2t system (p<0.05 or 
0.01 respectively). 
 
 Very similar 
? ? ? ?'F c F c?? ? >0.9 
Very dissimilar 
? ? ? ?'F c F c?? ? <0.1
ADJP JJ;  AD\ADJP VP;  ADVP\NP 
NP DT*NN;  LCP*P*NP CP;  BA*CP 
Table 2: Example of similar and dissimilar categories. 
 
Specifically, the FT2ET system with deep 
similarity matching obtains the best translation 
quality in all tasks and surpasses the baseline s2t 
system by 0.93 BLEU points in development data 
and by more than 1.0 BLEU point in test sets. The 
0-1 matching algorithm is simple but effective, and 
it yields quite good performance (line 3). The 
contribution of 0-1 matching as reflected in our 
experiments is consistent with the conclusions of 
(Marton and Resnik, 2008; Chiang, 2010). By 
contrast, the system with likelihood matching does 
not perform as well as the other two algorithms, 
although it also significantly improves the baseline 
s2t in all tasks. 
6.3 Analysis and Discussion 
We are a bit surprised at the large improvement 
gained by the 0-1 matching algorithm. This 
algorithm has several advantages: it is simple and 
easy to implement, and enhances the translation 
model by enabling its rules to take account of the 
source-side syntax to some degree. However, a 
major deficiency of this algorithm is that it does 
not make full use of the source side syntax, since it 
retains only the most frequent SAMT-style 
syntactic category to describe the rule?s source 
syntax. Thus this algorithm penalizes all the other 
categories equally, although some may be more 
frequent than others, as in the case of DEG and 
DEV in the rule 
? ? ? ?:11233, :11073, : 65DEC DEG DEV IN of?? .  
To some extent, the likelihood matching 
algorithm solves the main problem of 0-1 matching. 
211
Instead of rewarding or penalizing, this algorithm 
uses the likelihood of the syntactic category to 
approximate the degree of matching between the 
test source syntactic category and the rule. For a 
category not in the rule?s source syntactic category 
set, the likelihood algorithm computes a smoothed 
likelihood. However, the likelihood algorithm does 
not in fact lead to very promising improvement. 
We conjecture that this disappointing performance 
is due to the simple smoothing method we 
employed. Future work will investigate more fully. 
Compared with the above two matching 
algorithms, the deep similarity matching algorithm 
based on latent syntactic distribution is much more 
beautiful in theory. This algorithm can successfully 
measure the similarity between any two SAMT-
style syntactic categories (Table 2 gives some 
examples of similar and dissimilar category pairs).  
Then it can accurately compute the degree of 
matching between a test source syntactic category 
and a fuzzy-tree to exact-tree rule. Thus this 
algorithm obtains the best translation quality. 
However, the deep similarity matching algorithm 
has two practical shortcomings. First, it is not easy 
to determine the number of latent categories. We 
have to conduct multiple experiments to arrive at a 
number which can yield a tradeoff between 
translation quality and model complexity. In our 
work, we have tried the numbers n=4, 8, 16, 32, 
and have found n=16 to give the best tradeoff. The 
second shortcoming is that the induction of latent 
syntactic categories has been very time consuming, 
since we have applied the EM algorithm to the 
entire source-parsed parallel corpus. Even with 
n=8, it took more than a week to induce the latent 
syntactic categories on our middle-scale training 
data when using a Xeon four-core computer 
( 2.5 2 16GHz CPU GB? ? memory). When the training 
data contains tens of millions of sentence pairs, the 
computation time may no longer be tolerable. 
Table 3 shows some translation examples for 
comparison. In the first example, the Chinese 
preposition word ? is mistakenly translated into 
English conjunction word and in Joshua and 
baseline string-to-tree system s2t, however, our 
source-syntax-augmented system FT2ET-DeepSim 
correctly converts the Chinese word ?  into 
English preposition with and finally yield the right 
translation. In the second example, our proposed 
system moves the prepositional phrase at an early 
date after the sibling verb phrase. It is more 
reasonable compared with the baseline system s2t. 
In the third example, the proposed system FT2ET-
DeepSim successfully recognizes the Chinese long 
prepositional phrase ? ? ?? ?? ??? ?? ?
? ? ?? ??? ?? ? and short verb phrase ?, 
and obtains the correct phrase reordering at last. 
7 Related Work 
Several studies have tried to incorporate source or 
target syntax into translation models in a fuzzy 
manner. 
Zollmann and Venugopal (2006) augment the 
hierarchical string-to-string rules (Chiang, 2005) 
with target-side syntax. They annotate the target 
side of each string-to-string rule using SAMT-style 
syntactic categories and aim to generate the output 
more syntactically. Zhang et al (2010) base their 
approach on tree-to-string models, and generate 
grammatical output more reliably with the help of 
tree-to-tree sequence rules. Neither of them builds 
target syntactic trees using target syntax, however. 
Thus they can be viewed as integrating target 
syntax in a fuzzy manner. By contrast, we base our 
approach on a string-to-tree model which does 
construct target syntactic trees during decoding. 
(Marton and Resnik, 2008; Chiang et al, 2009 
and Huang et al, 2010) apply fuzzy techniques for 
integrating source syntax into hierarchical phrase-
based systems (Chiang, 2005, 2007). The first two 
studies employ 0-1 matching and the last tries deep 
similarity matching between two tag sequences. By 
contrast, we incorporate source syntax into a 
string-to-tree model. Furthermore, we apply fuzzy 
syntactic annotation on each rule?s source string 
and design three fuzzy rule matching algorithms. 
Chiang (2010) proposes a method for learning to 
translate with both source and target syntax in the 
framework of a hierarchical phrase-based system. 
He not only executes 0-1 matching on both sides of 
rules, but also designs numerous features such as 
. 'X Xroot  which counts the number of rules whose 
source-side root label is X  and target-side root 
label is 'X .  This fuzzy use of source and target 
syntax enables the translation system to learn 
which tree labels are similar enough to be 
compatible, which ones are harmful to combine, 
and which ones can be ignored. The differences 
between us are twofold: 1) his work applies fuzzy 
syntax in both sides, while ours bases on the string- 
212
Source sentence ?? ? [? ?? ???] ?? ? ?? 
Reference hussein also established ties with terrorist networks 
Joshua hussein also and terrorist networks established relations 
s2t hussein also and terrorist networks established relations 
 
 
1 
FT2ET- DeepSim hussein also established relations with terrorist networks 
Source sentence ? [? ?] [??] [??] [? ? ?? ?? ? ?? ??] 
Reference .. to end years of bloody conflict between israel and palestine as soon as possible 
.. to end at an early date years of bloody conflict between israel and palestine 
Joshua ? in the early period to end years of blood conflict between israel and palestine 
s2t ? at an early date to end years of blood conflict between israel and palestine 
 
 
 
2 
FT2ET- DeepSim ? to end years of blood conflict between israel and palestine at an early date 
Source sentence ?? [? ? ?? ?? ??? ?? ?? ? ?? ??? ?? ?] [?] ? 
 
Reference 
the europen union said in a joint statement issued after its summit meeting with china ?s 
premier wen jiabao ? 
in a joint statement released after the summit with chinese premier wen jiabao , the 
europen union said ? 
Joshua the europen union with chinese premier wen jiabao in a joint statement issued after the 
summit meeting said ? 
s2t the europen union in a joint statement issued after the summit meeting with chinese 
premier wen jiabao said ? 
 
 
 
 
 
3 
FT2ET- DeepSim the europen union said in a joint statement issued after the summit meeting with chinese 
premier wen jiabao ? 
 
Table 3: Some translation examples produced by Joshua, string-to-tree system s2t and source-syntax-augmented 
string-to-tree system FT2ET with deep similarity matching algorithm 
 
to-tree model and applies fuzzy syntax on source 
side; and 2) we not only adopt the 0-1 fuzzy rule 
matching algorithm, but also investigate likelihood 
matching and deep similarity matching algorithms. 
8 Conclusion and Future Work 
In this paper, we have proposed a new method for 
augmenting string-to-tree translation models with 
fuzzy use of the source syntax. We first applied a 
fuzzy annotation method which labels the source 
side of each string-to-tree rule with SAMT-style 
syntactic categories. Then we designed and 
explored three fuzzy rule matching algorithms: 0-1 
matching, likelihood matching, and deep similarity 
matching. The experiments show that our new 
system significantly outperforms the strong 
baseline string-to-tree system. This substantial 
improvement verifies that our fuzzy use of source 
syntax is effective and can enhance the ability to 
choose proper translation rules during decoding 
while guaranteeing grammatical output with 
explicit target trees. We believe that our work may 
demonstrate effective ways of incorporating both-
side syntax in a translation model to yield 
promising results. 
   Next, we plan to further study the likelihood 
fuzzy matching and deep similarity matching 
algorithms in order to fully exploit their potential. 
For example, we will combine the merits of 0-1 
matching and likelihood matching so as to avoid 
the setting of parameter m in likelihood matching. 
We also plan to explore another direction: we will 
annotate the source side of each string-to-tree rule 
with subtrees or subtree sequences. We can then 
apply tree-kernel methods to compute a degree of 
matching between a rule and a test source subtree 
or subtree sequence. 
Acknowledgments 
The research work has been funded by the Natural 
Science Foundation of China under Grant No. 
60975053, 61003160 and 60736014 and supported 
by the External Cooperation Program of the 
Chinese Academy of Sciences. We would also like 
to thank Mark Seligman and Yu Zhou for revising 
the early draft, and anonymous reviewers for their 
valuable suggestions.  
 
 
213
References  
Yehoshua Bar-Hillel, 1953. A quasi-arithmetical 
notation for syntactic description. Language, 29 (1). 
pages 47-58. 
David Chiang, 2005. A hiearchical phrase-based model 
for statistical machine translation. In Proc. of ACL 
2005, pages 263-270. 
David Chiang, 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33 (2). 
pages 201-228. 
David Chiang, 2010. Learning to translate with source 
and target syntax. In Proc. of ACL 2010, pages 
1443-1452. 
David Chiang, Kevin Knight and Wei Wang, 2009. 
11,001 new features for statistical machine 
translation. In Proc. of NAACL 2009, pages 218-
226. 
Brooke Cowan, Ivona Kucerova and Michael Collins, 
2006. A discriminative model for tree-to-tree 
translation. In Proc. of EMNLP, pages 232-241. 
Yuan Ding and Martha Palmer, 2005. Machine 
translation using probabilistic synchronous 
dependency insertion grammars. In Proc. of ACL 
2005, pages 541-548. 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu, 2004. What?s in a translation rule. In Proc. 
of HLT-NAACL 2004, pages 273?280. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer, 2006. Scalable inference and training of 
context-rich syntactic translation models. In Proc. 
of ACL-COLING 2006. 
Mark Hopkins and Greg Langmead, 2010. SCFG 
decoding without binarization. In Proc. of EMNLP 
2010, pages 646-655. 
Liang Huang and David Chiang, 2007. Forest rescoring: 
Faster decoding with integrated language models. 
In Proc. of ACL 2007, pages 144-151. 
Liang Huang, Kevin Knight and Aravind Joshi, 2006. A 
syntax-directed translator with extended domain of 
locality. In Proc. of AMTA 2006, pages 65-73. 
Zhongqiang Huang, Martin Cmejrek and Bowen Zhou, 
2010. Soft syntactic constraints for hierarchical 
phrase-based translation using latent syntactic 
distributions. In Proc. of EMNLP 2010, pages 138-
147. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, 
Alexandra Constantin and Evan Herbst, 2007. 
Moses: Open source toolkit for statistical machine 
translation. In Proc. of ACL 2007, pages 177-180. 
Philipp Koehn, 2004. Statistical significance tests for 
machine translation evaluation. In Proc. of EMNLP 
2004, pages 388?395. 
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri 
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, 
Wren N.G. Thornton, Jonathan Weese and Omar F. 
Zaidan, 2009. Joshua: An open source toolkit for 
parsing-based machine translation. In Proc. of ACL 
2009, pages 135-139. 
Yang Liu, Qun Liu and Shouxun Lin, 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. of ACL-COLING 2006, pages 
609-616. 
Yang Liu, Yajuan Lv and Qun Liu, 2009. Improving 
tree-to-tree translation with packed forests. In Proc. 
of ACL-IJCNLP 2009, pages 558-566. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight, 2006. SPMT: Statistical machine 
translation with syntactified target language 
phrases. In Proc. of EMNLP 2006, pages 44-52. 
Yuval Marton and Philip Resnik, 2008. Soft syntactic 
constraints for hierarchical phrased-based 
translation. In Proc. of ACL-08: HLT. pages 1003?
1011. 
Haitao Mi, Liang Huang and Qun Liu, 2008. Forest-
based translation. In Proc. of ACL-08: HLT. pages 
192?199. 
Haitao Mi and Qun Liu, 2010. Constituency to 
dependency translation with forests. In Proc. of 
ACL 2010, pages 1433-1442. 
Tom M. Mitchell, 1997. Machine learning. Mac Graw 
Hill. 
Franz Josef Och, 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL 
2003, pages 160-167. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein, 2006. Learning accurate, compact, and 
interpretable tree annotation. In Proc. of COLING-
ACL 2006, pages 433-440. 
Chris Quirk, Arul Menezes and Colin Cherry, 2005. 
Dependency treelet translation: Syntactically 
informed phrasal SMT. In Proc. of ACL 2005, 
pages 271-279. 
Libin Shen, Jinxi Xu and Ralph Weischedel, 2008. A 
new string-to-dependency machine translation 
algorithm with a target dependency language 
model. In Proc. of ACL-08: HLT, pages 577-585. 
Tong Xiao, Jingbo Zhu, Muhua Zhu and and Huizhen 
Wang, 2010. Boosting-based System Combination 
for Machine Translation. In Proc. of ACL 2010, 
pages 739-748. 
Hao Zhang, Liang Huang, Daniel Gildea and Kevin 
Knight, 2006. Synchronous binarization for 
machine translation. In Proc. of HLT-NAACL 2006, 
pages 256-263. 
214
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, Chew 
Lim Tan, 2009. Forest-based tree sequence to 
string translation model. In Proc. of ACL-IJCNLP 
2009, pages 172-180. 
Hui Zhang, Min Zhang, Haizhou Li and Chng Eng 
Siong, 2010. Non-isomorphic forest pair 
translation. In Proc. of EMNLP 2010, pages 440-
450. 
Andreas Zollmann and Ashish Venugopal, 2006. Syntax 
augmented machine translation via chart parsing. 
In Proc. of Workshop on Statistical Machine 
Translation 2006, pages 138-141. 
 
 
215
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1127?1136,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Handling Ambiguities of Bilingual Predicate-Argument Structures for 
Statistical Machine Translation 
 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong 
National Laboratory of Pattern Recognition, Institute of Automation, 
Chinese Academy of Sciences, Beijing, China 
{ffzhai,jjzhang,yzhou,cqzong}@nlpr.ia.ac.cn 
 
 
 
Abstract 
Predicate-argument structure (PAS) has been 
demonstrated to be very effective in improving 
SMT performance. However, since a source-
side PAS might correspond to multiple differ-
ent target-side PASs, there usually exist many 
PAS ambiguities during translation. In this pa-
per, we group PAS ambiguities into two types: 
role ambiguity and gap ambiguity. Then we 
propose two novel methods to handle the two 
PAS ambiguities for SMT accordingly: 1) in-
side context integration; 2) a novel maximum 
entropy PAS disambiguation (MEPD) model. 
In this way, we incorporate rich context in-
formation of PAS for disambiguation. Then 
we integrate the two methods into a PAS-
based translation framework. Experiments 
show that our approach helps to achieve sig-
nificant improvements on translation quality. 
1 Introduction 
Predicate-argument structure (PAS) depicts the 
relationship between a predicate and its associat-
ed arguments, which indicates the skeleton struc-
ture of a sentence on semantic level. Basically, 
PAS agrees much better between two languages 
than syntax structure (Fung et al, 2006; Wu and 
Fung, 2009b). Considering that current syntax-
based translation models are always impaired by 
cross-lingual structure divergence (Eisner, 2003; 
Zhang et al, 2010), PAS is really a better repre-
sentation of a sentence pair to model the bilin-
gual structure mapping. 
However, since a source-side PAS might 
correspond to multiple different target-side PASs, 
there usually exist many PAS ambiguities during 
translation. For example, in Figure 1, (a) and (b) 
carry the same source-side PAS <[A0]1 
[Pred(?)]2 [A1]3> for Chinese predicate ???. 
However, in Figure 1(a), the corresponding 
target-side-like PAS is <[X1] [X2] [X3]>, while in 
Figure 1(b), the counterpart target-side-like PAS1 
is <[X2] [X3] [X1]>. This is because the two 
PASs play different roles in their corresponding 
sentences. Actually, Figure 1(a) is an independ-
ent PAS, while Figure 1(b) is a modifier of the 
noun phrase ??? ? ????. We call this kind 
of PAS ambiguity role ambiguity. 
??  ?  ??? ?? ???
[           A0         ]1 [     A1    ]3[Pred]2
?
being , should  ?two major countries
[           X3            ][X2]
China and Russia
[          X1           ]
? ?
?? ?? ? ???
[ A0 ]1 [          A1         ]3[Pred]2
flood  prevention is the  primary  mission
[           X1          ] [ X2 ] [              X3              ]
??? ? ?? ? ??? ? ? ? ?
[      A0      ]1 [    A1   ]3[Pred]2
the location of the olympic village for athletesis the best
[     X3    ][X2][                    X1                     ]
(a)
(c)
(b)
 
Figure 1. An example of ambiguous PASs. 
Meanwhile, Figure 1 also depicts another kind 
of PAS ambiguity. From Figure 1, we can see 
that (a) and (c) get the same source-side PAS and 
target-side-like PAS. However, they are different 
because in Figure 1(c), there is a gap string ?? 
???? between [A0] and [Pred]. Generally, the 
gap strings are due to the low recall of automatic 
semantic role labeling (SRL) or complex sen-
tence structures. For example, in Figure 1(c), the 
gap string ?? ???? is actually an argument 
?AM-PRP? of the PAS, but the SRL system has 
                                                 
1We use target-side-like PAS to refer to a list of general 
non-terminals in target language order, where a non-
terminal aligns to a source argument. 
1127
ignored it. We call this kind of PAS ambiguity 
gap ambiguity. 
During translation, these PAS ambiguities will 
greatly affect the PAS-based translation models. 
Therefore, in order to incorporate the bilingual 
PAS into machine translation effectively, we 
need to decide which target-side-like PAS should 
be chosen for a specific source-side PAS. We 
call this task PAS disambiguation. 
In this paper, we propose two novel methods 
to incorporate rich context information to handle 
PAS ambiguities. Towards the gap ambiguity, 
we adopt a method called inside context 
integration to extend PAS to IC-PAS. In terms of 
IC-PAS, the gap strings are combined effectively 
to deal with the gap ambiguities. As to the role 
ambiguity, we design a novel maximum entropy 
PAS disambiguation (MEPD) model to combine 
various context features, such as context words 
of PAS. For each ambiguous source-side PAS, 
we build a specific MEPD model to select 
appropriate target-side-like PAS for translation. 
We will detail the two methods in Section 3 and 
4 respectively. 
Finally, we integrate the above two methods 
into a PAS-based translation framework (Zhai et 
al. 2012). Experiments show that the two PAS 
disambiguation methods significantly improve 
the baseline translation system. The main 
contribution of this work can be concluded as 
follows: 
1) We define two kinds of PAS ambiguities: 
role ambiguity and gap ambiguity. To our 
best knowledge, we are the first to handle 
these PAS ambiguities for SMT. 
2) Towards the two different ambiguities, we 
design two specific methods for PAS 
disambiguation: inside context integration 
and the novel MEPD model.  
2 PAS-based Translation Framework 
PAS-based translation framework is to perform 
translation based on PAS transformation (Zhai et 
al., 2012). In the framework, a source-side PAS 
is first converted into target-side-like PASs by 
PAS transformation rules, and then perform 
translation based on the obtained target-side-like 
PASs. 
2.1 PAS Transformation Rules 
PAS transformation rules (PASTR) are used to 
convert a source-side PAS into a target one. 
Formally, a PASTR is a triple <Pred, SP, TP>: 
? Pred means the predicate where the rule is 
extracted. 
? SP denotes the list of source elements in 
source language order. 
? TP refers to the target-side-like PAS, i.e., a 
list of general non-terminals in target 
language order. 
For example, Figure 2 shows the PASTR 
extracted from Figure 1(a). In this PASTR, Pred 
is Chinese verb ???, SP is the source element 
list <[A0]1 [Pred]2 [A1]3>, and TP is the list of 
non-terminals <X1 X2 X3>. The same subscript in 
SP and TP means a one-to-one mapping between 
a source element and a target non-terminal. Here, 
we utilize the source element to refer to the 
predicate or argument of the source-side PAS. 
[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1] 
source-side PAS(?) target-side-like PAS
 
Figure 2. An example PASTR. 
2.2 PAS Decoding 
The PAS decoding process is divided into 3 steps: 
(1) PAS acquisition: perform semantic role 
labeling (SRL) on the input sentences to achieve 
their PASs, i.e., source-side PASs; 
(2) Transformation: use the PASTR to match 
the source-side PAS i.e., the predicate Pred and 
the source element list SP. Then by the matching 
PASTRs, transform source-side PASs to target-
side-like PASs. 
(3) Translation: in this step, the decoder first 
translates each source element respectively, and 
then a CKY-style decoding algorithm is adopted 
to combine the translation of each element and 
get the final translation of the PAS.  
2.3 Sentence Decoding with the PAS-based 
translation framework 
Sometimes, the source sentence cannot be fully 
covered by the PAS, especially when there are 
several predicates. Thus to translate the whole 
sentence, Zhai et al (2012) further designed an 
algorithm to decode the entire sentence.  
In the algorithm, they organized the space of 
translation candidates into a hypergraph. For the 
span covered by PAS (PAS span), a multiple-
branch hyperedge is employed to connect it to 
the PAS?s elements. For the span not covered by 
PAS (non-PAS span), the decoder considers all 
the possible binary segmentations of it and uti-
lizes binary hyperedges to link them. 
1128
During translation, the decoder fills the spans 
with translation candidates in a bottom-up man-
ner. For the PAS span, the PAS-based translation 
framework is adopted. Otherwise, the BTG sys-
tem (Xiong et al, 2006) is used. When the span 
covers the whole sentence, we get the final trans-
lation result. 
 
Obviously, PAS ambiguities are not 
considered in this framework at all. The target-
side-like PAS is selected only according to the 
language model and translation probabilities, 
without considering any context information of 
PAS. Consequently, it would be difficult for the 
decoder to distinguish the source-side PAS from 
different context. This harms the translation 
quality. Thus to overcome this problem, we de-
sign two novel methods to cope with the PAS 
ambiguities: inside-context integration and a 
maximum entropy PAS disambiguation (MEPD) 
model. They will be detailed in the next two sec-
tions. 
3 Inside Context Integration 
In this section, we integrate the inside context of 
the PAS into PASTRs to do PAS disambiguation. 
Basically, a PAS consists of several elements (a 
predicate and several arguments), which are ac-
tually a series of continuous spans. For a specific 
PAS <E1,?, En>, such as the source-side PAS 
<[A0][Pred][A1]> in Figure 2, its controlled range 
is defined as: 
( ) { ( ), [1, ]}irange PAS s E i n= ? ?  
where s(Ei) denotes the span of element Ei. Fur-
ther, we define the closure range of a PAS. It 
refers to the shortest continuous span covered by 
the entire PAS: 
0( ) ( )
_ min , max
nj s E j s E
closure range j j
? ?
? ?= ? ?? ?
 
Here, E0 and En are the leftmost and rightmost 
element of the PAS respectively. The closure 
range is introduced here because adjacent source 
elements in a PAS are usually separated by gap 
strings in the sentence. We call these gap strings 
the inside context (IC) of the PAS, which satisfy: 
_ ( ) ( ( ) ( ) )closure range PAS IC PAS range PAS= ? ?  
The operator ?  takes a list of neighboring spans 
as input2, and returns their combined continuous 
span. As an example, towards the PAS ?<[A0] 
[Pred][A1]>? (the one for Chinese predicate ??
(shi)?) in Figure 3, its controlled range is 
{[3,5],[8,8],[9,11]} and its closure range is [3,11]. 
The IC of the PAS is thus {[6,7]}. 
To consider the PAS?s IC during PAS trans-
formation process, we incorporate its IC into the 
extracted PASTR. For each gap string in IC, we 
abstract it by the sequence of highest node cate-
gories (named as s-tag sequence). The s-tag se-
quence dominates the corresponding syntactic 
tree fragments in the parse tree. For example, in 
Figure 3, the s-tag sequence for span [6,8] is ?PP 
VC?. Thus, the sequence for the IC (span [6,7]) 
in Figure 3 is ?PP?. We combine the s-tag se-
quences with elements of the PAS in order. The 
resulting PAS is called IC-PAS, just like the left 
side of Figure 4(b) shows. 
[           A0           ] [        PP        ]
???3 ???7 ?8 ?10
de wei-zhiao-yun-cun
??5?4 ?6
dui yun-dong-yuan shi
?9 ?11
zui hao de
NN DEC NN
NP
P NN
PP
VC AD VA DEC
CP
VP
IP
??1
VV
biao-shi
VP
,2
PU
?0
PN
ta
?
PU
IP
DNP
[Pred] [      A1     ]  
Figure 3. The illustration of inside context (IC). The 
subscript in each word refers to its position in sen-
tence. 
Differently, Zhai et al (2012) attached the IC 
to its neighboring elements based on parse trees. 
For example, in Figure 3, they would attach the 
gap string ??(dui) ???(yun-dong-yuan)? to the 
PAS?s element ?Pred?, and then the span of 
?Pred? would become [6,8]. Consequently, the 
span [6,8] will be translated as a whole source 
element in the decoder. This results in a bad 
translation because the gap string ??(dui) ???
(yun-dong-yuan)? and predicate ??(shi)? should 
be translated separately, just as Figure 4(a) 
shows. Therefore, we can see that the attachment 
decision in (Zhai et al, 2012) is sometimes un-
reasonable and the IC also cannot be used for 
PAS disambiguation at all. In contrast, our meth-
                                                 
2 Here, two spans are neighboring means that the beginning 
of the latter span is the former span?s subsequent word in 
the sentence. For example, span [3,6] and [7,10] are neigh-
boring spans. 
1129
od of inside context integration is much flexible 
and beneficial for PAS disambiguation. 
(a)
(b)
[X1] [X2] [X4] [A0]1 [PP]2 [Pred]3 [A1]4 [X3] 
source-side PAS(?) target-side-like PAS
??? ??? ? ?
[            A0            ]1 [      A1     ]4[Pred]3
[the location of the olympic village]1 [for athletes]2[is]3 [the best]4
[         PP         ]2
de wei-zhiao-yun-cun
??? ?
dui yun-dong-yuan shi
? ?
zui hao de
 
Figure 4. Example of IC-PASTR. (a) The aligned 
span of each element of the PAS in Figure 3; (b) The 
extracted IC-PASTR from (a). 
Using the IC-PASs, we look for the aligned 
target span for each element of the IC-PAS. We 
demand that every element and its corresponding 
target span must be consistent with word align-
ment. Otherwise, we discard the IC-PAS. After-
wards, we can easily extract a rule for PAS trans-
formation, which we call IC-PASTR. As an ex-
ample, Figure 4(b) is the extracted IC-PASTR 
from Figure 4(a). 
Note that we only apply the source-side PAS 
and word alignment for IC-PASTR extraction. 
By contrast, Zhai et al (2012) utilized the result 
of bilingual SRL (Zhuang and Zong, 2010b). 
Generally, bilingual SRL could give a better 
alignment between bilingual elements. However, 
bilingual SRL usually achieves a really low re-
call on PASs, about 226,968 entries in our train-
ing set while it is 882,702 by using monolingual 
SRL system. Thus to get a high recall for PASs, 
we only utilize word alignment instead of captur-
ing the relation between bilingual elements. In 
addition, to guarantee the accuracy of IC-
PASTRs, we only retain rules with more than 5 
occurrences. 
4 Maximum Entropy PAS Disambigua-
tion (MEPD) Model 
In order to handle the role ambiguities, in this 
section, we concentrate on utilizing a maximum 
entropy model to incorporate the context infor-
mation for PAS disambiguation. Actually, the 
disambiguation problem can be considered as a 
multi-class classification task. That is to say, for 
a source-side PAS, every corresponding target-
side-like PAS can be considered as a label. For 
example, in Figure 1, for the source-side PAS 
?[A0]1[Pred]2[A1]3?, the target-side-like PAS 
?[X1] [X2] [X3]? in Figure 1(a) is thus a label and 
?[X2] [X3] [X1]? in Figure 1(b) is another label of 
this classification problem. 
The maximum entropy model is the classical 
way to handle this problem: 
exp( ( , , ( ), ( )))
( | , ( ), ( ))
exp( ( , , ( ), ( )))
i i i
tp i i i
h sp tp c sp c tp
P tp sp c sp c tp
h sp tp c sp c tp?
?
??
= ?
? ?
 
where sp and tp refer to the source-side PAS (not 
including the predicate) and the target-side-like 
PAS respectively. c(sp) and c(tp) denote the sur-
rounding context of sp and tp. hi is a binary fea-
ture function and ?i is the weight of hi. 
We train a maximum entropy classifier for 
each sp via the off-the-shelf MaxEnt toolkit 3 . 
Note that to avoid sparseness, sp does not in-
clude predicate of the PAS. Practically, the pred-
icate serves as a feature of the MEPD model. As 
an example, for the rule illustrated in Figure 4(b), 
we build a MEPD model for its source element 
list sp <[A0] [PP] [Pred] [A1]>, and integrate the 
predicate ??(shi)? into the MEPD model as a 
feature. 
In detail, we design a list of features for each 
pair <sp, tp> as follows: 
?   Lexical Features. These features include 
the words immediately to the left and right of sp, 
represented as w-1 and w+1. Moreover, the head 
word of each argument also serves as a lexical 
feature, named as hw(Ei). For example, Figure 3 
shows the context of the IC-PASTR in Figure 
4(b), and the extracted lexical features of the in-
stance are: w-1=? , w+1=? , hw([A0]1)=??
(wei-zhi), hw([A1]4)=?(hao). 
?   POS Features. These features are defined 
as the POS tags of the lexical features, p-1, p+1 
and phw(Ei) respectively. Thus, the correspond-
ing POS features of Figure 4 (b) are: p-1=PU, 
p+1=PU, phw([A0]1)=NN, phw([A1]4)=VA. 
?   Predicate Feature. It is the pair of source 
predicate and its corresponding target predicate. 
For example, in Figure 4(b), the source and tar-
get predicate are ??(shi)? and ?is? respectively. 
The predicate feature is thus ?PredF=?(shi)+is?. 
The target predicate is determined by: 
_ ( )
- arg max ( | - )j
j t range PAS
t pred p t s pred
?
=  
where s-pred is the source predicate and t-pred 
is the corresponding target predicate. 
                                                 
3http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.htm
l 
1130
t_range(PAS) refers to the target range covering 
all the words that are reachable from the PAS via 
word alignment.  tj refers to the jth word in 
t_range(PAS). The utilized lexical translation 
probabilities are from the toolkit in Moses 
(Koehn et al, 2007). 
?   Syntax Features. These features include 
st(Ei), i.e., the highest syntax tag for each argu-
ment, and fst(PAS) which is the lowest father 
node of sp in the parse tree. For example, for the 
rule shown in Figure 4(b), syntax features are 
st([A0]1)=NP, st([A1]4)=CP, and fst(PAS)=IP 
respectively.  
Using these features, we can train the MEPD 
model. We set the Gaussian prior to 1.0 and per-
form 100 iterations of the L-BFGS algorithm for 
each MEPD model. At last, we build 160 and 
215 different MEPD classifiers, respectively, for 
the PASTRs and IC-PASTRs. Note that since the 
training procedure of maximum entropy classifi-
er is really fast, it does not take much time to 
train these classifiers. 
5 Integrating into the PAS-based Trans-
lation Framework 
In this section, we integrate our method of PAS 
disambiguation into the PAS-based translation 
framework when translating each test sentence. 
For inside context integration, since the format 
of IC-PASTR is the same to PASTR4, we can 
use the IC-PASTR to substitute PASTR for 
building a PAS-based translation system directly. 
We use ?IC-PASTR? to denote this system. In 
addition, since our method of rule extraction is 
different from (Zhai et al, 2012), we also use 
PASTR to construct a translation system as the 
baseline system, which we call ?PASTR?. 
On the basis of PASTR and IC-PASTR, we 
further integrate our MEPD model into transla-
tion. Specifically, we take the score of the MEPD 
model as another informative feature for the de-
coder to distinguish good target-side-like PASs 
from bad ones. The weights of the MEPD feature 
can be tuned by MERT (Och, 2003) together 
with other translation features, such as language 
model. 
6 Related Work 
The method of PAS disambiguation for SMT is 
relevant to the previous work on context depend-
                                                 
4 The only difference between IC-PASTR and PASTR is 
that there are many syntactic labels in IC-PASTRs.  
ent translation. 
Carpuat and Wu (2007a, 2007b) and Chan et 
al. (2007) have integrated word sense disambig-
uation (WSD) and phrase sense disambiguation 
(PSD) into SMT systems. They combine rich 
context information to do disambiguation for 
words or phrases, and achieve improved transla-
tion performance. 
Differently, He et al (2008), Liu et al (2008) 
and Cui et al (2010) designed maximum entropy 
(ME) classifiers to do better rule section for hier-
archical phrase-based model and tree-to-string 
model respectively. By incorporating the rich 
context information as features, they chose better 
rules for translation and yielded stable improve-
ments on translation quality. 
Our work differs from the above work in the 
following two aspects: 1) in our work, we focus 
on the problem of disambiguates on PAS; 2) we 
define two kinds of PAS ambiguities: role 
ambiguity and gap ambiguity. 3) towards the two 
different ambiguities, we design two specific 
methods for PAS disambiguation: inside context 
integration and the novel MEPD model. 
In addition, Xiong et al (2012) proposed an 
argument reordering model to predict the relative 
position between predicates and arguments. They 
also combine the context information in the 
model. But they only focus on the relation be-
tween the predicate and a specific argument, ra-
ther than the entire PAS. Different from their 
work, we incorporate the context information to 
do PAS disambiguation based on the entire PAS. 
This is very beneficial for global reordering dur-
ing translation (Zhai et al, 2012). 
7 Experiment 
7.1 Experimental Setup  
We perform Chinese-to-English translation to 
demonstrate the effectiveness of our PAS disam-
biguation method. The training data contains 
about 260K sentence pairs5. To get accurate SRL 
results, we ensure that the length of each sen-
tence in the training data is among 10 and 30 
words. We run GIZA++ and then employ the 
grow-diag-final-and (gdfa) strategy to produce 
symmetric word alignments. The development 
set and test set come from the NIST evaluation 
test data (from 2003 to 2005). Similar to the 
training set, we also only retain the sentences 
                                                 
5 It is extracted from the LDC corpus. The LDC category 
number : LDC2000T50, LDC2002E18, LDC2003E07, 
LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10 
and LDC2005T34. 
1131
whose lengths are among 10 and 30 words. Fi-
nally, the development set includes 595 sentenc-
es from NIST MT03 and the test set contains 
1,786 sentences from NIST MT04 and MT05. 
We train a 5-gram language model with the 
Xinhua portion of English Gigaword corpus and 
target part of the training data. The translation 
quality is evaluated by case-insensitive BLEU-4 
with shortest length penalty. The statistical sig-
nificance test is performed by the re-sampling 
approach (Koehn, 2004). 
We perform SRL on the source part of the 
training set, development set and test set by the 
Chinese SRL system used in (Zhuang and Zong, 
2010b). To relieve the negative effect of SRL 
errors, we get the multiple SRL results by 
providing the SRL system with 3-best parse trees 
of Berkeley parser (Petrov and Klein, 2007), 1-
best parse tree of Bikel parser (Bikel, 2004) and 
Stanford parser (Klein and Manning, 2003). 
Therefore, at last, we can get 5 SRL result for 
each sentence. For the training set, we use these 
SRL results to do rule extraction respectively. 
We combine the obtained rules together to get a 
combined rule set. We discard the rules with 
fewer than 5 appearances. Using this set, we can 
train our MEPD model directly. 
As to translation, we match the 5 SRL results 
with transformation rules respectively, and then 
apply the resulting target-side-like PASs for de-
coding. As we mentioned in section 2.3, we use 
the state-of-the-art BTG system to translate the 
non-PAS spans. 
source-side PAS counts number of classes 
[A0] [Pred(?)] [A1] 245 6 
[A0] [Pred(?)] [A1] 148 6 
[A0] [AM-ADV] [Pred(?)] [A1] 68 20 
[A0] [Pred(??)] [A1] 66 6 
[A0] [Pred(?)] [A1] 42 6 
[A0] [Pred(??)] [A1] 32 4 
[A0] [AM-ADV] [Pred(?)] [A1] 32 19 
[A0] [Pred(??)] [A1] 29 4 
[AM-ADV] [Pred(?)] [A1] 26 6 
[A2] [Pred(?)] [A1] 16 5 
Table 1. The top 10 frequent source-side PASs in the 
dev and test set. 
7.2 Ambiguities in Source-side PASs 
We first give Table 1 to show some examples of 
role ambiguity. In the table, for instance, the se-
cond line denotes that the source-side PAS ?[A0] 
[Pred(?)] [A1]? appears 148 times in the devel-
opment and test set al together, and it corre-
sponds to 6 different target-side-like PASs in the 
training set. 
As we can see from Table 1, all the top 10 
PASs correspond to several different target-side-
like PASs. Moreover, according to our statistics, 
among all PASs appearing in the development 
set and test set, 56.7% of them carry gap strings. 
These statistics demonstrate the importance of 
handling the role ambiguity and gap ambiguity in 
the PAS-based translation framework. Therefore, 
we believe that our PAS disambiguation method 
would be helpful for translation. 
7.3 Translation Result  
We compare the translation result using PASTR, 
IC-PASTR and our MEPD model in this section. 
The final translation results are shown in Table 2. 
As we can see, after employing PAS for transla-
tion, all systems outperform the baseline BTG 
system significantly. This comparison verifies 
the conclusion of (Zhai et al, 2012) and thus also 
demonstrates the effectiveness of PAS. 
MT system Test set 
n-gram precision 
1 2 3 4 
BTG 32.75 74.39 41.91 24.75 14.91 
PASTR 33.24* 75.28 42.62 25.18 15.10 
PASTR+MEPD 33.78* 75.32 43.08 25.75 15.58 
IC-PASTR 33.95*# 75.62 43.36 25.92 15.58 
IC-PASTR+MEPD 34.19*# 75.66 43.40 26.15 15.92 
Table 2. Result of baseline system and the MT sys-
tems using our PAS-based disambiguation method. 
The ?*? and ?#? denote that the result is significantly 
better than BTG and PASTR respectively (p<0.01).  
Specifically, after integrating the inside con-
text information of PAS into transformation, we 
can see that system IC-PASTR significantly out-
performs system PASTR by 0.71 BLEU points. 
Moreover, after we import the MEPD model into 
system PASTR, we get a significant improve-
ment over PASTR (by 0.54 BLEU points). These 
comparisons indicate that both the inside context 
integration and our MEPD model are beneficial 
for the decoder to choose better target-side-like 
PAS for translation. 
On the basis of IC-PASTR, we further add our 
MEPD model into translation and get system IC-
PASTR+MEPD. We can see that this system 
further achieves a remarkable improvement over 
system PASTR (0.95 BLEU points).  
However, from Table 2, we find that system 
IC-PASTR+MEPD only outperforms system IC-
PASTR slightly (0.24 BLEU points). The result 
seems to show that our MEPD model is not such 
1132
useful after using IC-PASTR. We will explore 
the reason in section 7.5. 
7.4 Effectiveness of Inside Context Integra-
tion 
The method of inside context integration is used 
to combine the inside context (gap strings) into 
PAS for translation, i.e., extend the PASTR to 
IC-PASTR. In order to demonstrate the effec-
tiveness of inside context integration, we first 
give Table 3, which illustrates statistics on the 
matching PASs. The statistics are conducted on 
the combination of development set and test set. 
Transformation 
Rules 
Matching PAS 
None Gap PAS Gap PAS Total 
PASTR 1702 1539 3241 
IC-PASTR 1546 832 2378 
Table 3. Statistics on the matching PAS. 
In Table 3, for example, the line for PASTR 
means that if we use PASTR for the combined 
set, 3241 PASs (column ?Total?) can match 
PASTRs in total. Among these matching PASs, 
1539 ones (column ?Gap PAS?) carry gap strings, 
while 1702 ones do not (column ?None Gap 
PAS?). Consequently, since PASTR does not 
consider the inside context during translation, the 
Gap PASs, which account for 47% (1539/3241) 
of all matching PASs, might be handled inappro-
priately in the PAS-based translation framework. 
Therefore, integrating the inside context into 
PASTRs, i.e., using the proposed IC-PASTRs, 
would be helpful for translation. The translation 
result shown in Table 2 also demonstrates this 
conclusion. 
(a) reference
(c) translation result using IC-PASTR
[for economic recovery , especially of investment confidence is]
[  A0  ] [                              PP                               ] [Pred] [      A1      ]
? ? ? ? ???? ?? ?? ? ??? ?? ?? ??
[ a good sign ] [ for economic recovery , especially of investment confidence ]this is
? ? ? ? ??? ?? ?? ? ??? ?? ?? ??  ? 
[a good sign]this
(b) translation result using PASTR
[  A0  ] [                              PP                               ] [Pred] [      A1      ]
? ? ? ? ???? ?? ?? ? ??? ?? ?? ??
[a good sign]this is [for economic recovery and the restoration of investors ' confidence]
[  A0  ] [                            Pred                             ] [      A1      ]
 
Figure 5. Translation examples to verify the effec-
tiveness of inside context.  
From Table 3, we can also find that the num-
ber of matching PASs decreases after using IC-
PASTR. This is because IC-PASTR is more spe-
cific than PASTR. Therefore, for a PAS with 
specific inside context (gap strings), even if the 
matched PASTR is available, the matched IC-
PASTR might not. This indicates that comparing 
with PASTR, IC-PASTR is more capable of dis-
tinguishing different PASs. Therefore, based on 
this advantage, although the number of matching 
PASs decreases, IC-PASTR still improves the 
translation system using PASTR significantly. Of 
course, we believe that it is also possible to inte-
grate the inside context without decreasing the 
number of matching PASs and we plan this as 
our future work. 
We further give a translation example in Fig-
ure 5 to illustrate the effectiveness of our inside 
context integration method. In the example, the  
automatic SRL system ignores the long preposi-
tion phrase ?? ???? ???? ?? ???
?? for the PAS. Thus, the system using PASTRs 
can only attach the long phrase to the predicate 
??? according to the parse tree, and meanwhile, 
make use of a transformation rule as follows: 
[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1] 
source-side PAS(?) target-side-like PAS
 
In this way, the translation result is very bad, just 
as Figure 5(b) shows. The long preposition 
phrases are wrongly positioned in the translation. 
In contrast, after inside context integration, we 
match the inside context during PAS transfor-
mation. As Figure 5(c) shows, the inside context 
helps to selects a right transformation rule as fol-
lows and gets a good translation result finally. 
[X1] [X2] [X4] [A0]1 [PP]2 [Pred]3 [A1]4 [X3] 
source-side PAS(?) target-side-like PAS
 
7.5 Effectiveness of the MEPD Model 
The MEPD model incorporates various context 
features to select better target-side-like PAS for 
translation. On the basis of PASTR and IC-
PASTR, we build 160 and 215 different MEPD 
classifies, respectively, for the frequent source-
side PASs. 
In Table 2, we have found that our MEPD 
model improves system IC-PASTR slightly. We 
conjecture that this phenomenon is due to two 
possible reasons. On one hand, sometimes, many 
PAS ambiguities might be resolved by both in-
side context and the MEPD model. Therefore, 
the improvement would not be such significant 
1133
when we combine these two methods together. 
On the other hand, as Table 3 shows, the number 
of matching PASs decreases after using IC-
PASTR. Since the MEPD model works on PASs, 
its effectiveness would also weaken to some ex-
tent. Future work will explore this phenomenon 
more thoroughly. 
PASTR
Ref
PASTR 
+ MEPD
...  ,  [??]A0    [?]Pred    [? ?? ??]A1  ?
...  [the hague]     [is]      [the last leg]  .
...  ,  [??]    [?]    [? ?? ??]  ?
...  [the hague]   [is]   [his last stop]  .
...  ,  [??]A0    [?]Pred    [? ?? ??]A1  ?
...   [is]    [his last leg of]    [the hague] .
 
Figure 6. Translation examples to demonstrate the 
effectiveness of our MEPD model. 
Now, we give Figure 6 to demonstrate the ef-
fectiveness of our MEPD model. From the Fig-
ure, we can see that the system using PASTRs 
selects an inappropriate transformation rule for 
translation: 
[X1] [X3] [A0]1 [Pred]2 [A1]3 [X2] 
source-side PAS(?) target-side-like PAS
 
This rule wrongly moves the subject ???
(Hague)? to the end of the translation. We do not 
give the translation result of the BTG system 
here because it makes the same mistake. 
Conversely, by considering the context infor-
mation, the PASTR+MEPD system chooses a 
correct rule for translation: 
[X3] [X2] [A0]1 [Pred]2 [A1]3 [X1] 
source-side PAS(?) target-side-like PAS
 
As we can see, the used rule helps to keep the 
SVO structure unchanged, and gets the correct 
translation. 
8 Conclusion and Future Work 
In this paper, we focus on the problem of ambi-
guities for PASs. We first propose two ambigui-
ties: gap ambiguity and role ambiguity. Accord-
ingly, we design two novel methods to do effi-
cient PAS disambiguation: inside-context inte-
gration and a novel MEPD model. For inside 
context integration, we abstract the inside con-
text and combine them into the PASTRs for PAS 
transformation. Towards the MEPD model, we 
design a maximum entropy model for each ambi-
tious source-side PASs. The two methods suc-
cessfully incorporate the rich context information 
into the translation process. Experiments show 
that our PAS disambiguation methods help to 
improve the translation performance significantly.  
In the next step, we will conduct experiments 
on other language pairs to demonstrate the effec-
tiveness of our PAS disambiguation method. In 
addition, we also will try to explore more useful 
and representative features for our MEPD model. 
Acknowledgments 
The research work has been funded by the Hi-
Tech Research and Development Program (?863? 
Program) of China under Grant No. 
2011AA01A207, 2012AA011101, and 
2012AA011102 and also supported by the Key 
Project of Knowledge Innovation Program of 
Chinese Academy of Sciences under Grant 
No.KGZD-EW-501. We thank the anonymous 
reviewers for their valuable comments and sug-
gestions. 
References  
Wilker Aziz, Miguel Rios, and Lucia Specia. (2011). 
Shallow semantic trees for smt. In Proceedings of 
the Sixth Workshop on Statistical Machine Trans-
lation, pages 316?322, Edinburgh, Scotland, July. 
Daniel Bikel. (2004). Intricacies of Collins parsing 
model. Computational Linguistics, 30(4):480-511. 
David Chiang, (2007). Hierarchical phrase-based 
translation. Computational Linguistics, 33 (2):201?
228. 
Marine Carpuat and Dekai Wu. 2007a. How phrase-
sense disambiguation outperforms word sense dis-
ambiguation for statistical machine translation. In 
11th Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, pages 43?52. 
Marine Carpuat and Dekai Wu. 2007b. Improving 
statistical machine translation using word sense 
disambiguation. In Proceedings of EMNLP-CoNLL 
2007, pages 61?72. 
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 
2007. Word sense disambiguation improves statis-
tical machine translation. In Proc. ACL 2007, pag-
es 33?40. 
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou and 
Tiejun Zhao. A Joint Rule Selection Model for 
Hierarchical Phrase-Based Translation. In Proc. 
of ACL 2010. 
1134
Jason Eisner. (2003). Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003. 
Pascale Fung, Wu Zhaojun, Yang Yongsheng, and 
Dekai Wu. (2006). Automatic learning of chinese 
english semantic structure mapping. In IEEE/ACL 
2006 Workshop on Spoken Language Technology 
(SLT 2006), Aruba, December. 
Pascale Fung, Zhaojun Wu, Yongsheng Yang and 
Dekai Wu. (2007). Learning bilingual semantic 
frames: shallow semantic sarsing vs. semantic sole 
projection. In Proceedings of the 11th Conference 
on Theoretical and Methodological Issues in Ma-
chine Translation, pages 75-84. 
Qin Gao and Stephan Vogel. (2011). Utilizing target-
side semantic role labels to assist hierarchical 
phrase-based machine translation. In Proceedings 
of Fifth Workshop on Syntax, Semantics and Struc-
ture in Statistical Translation, pages 107?115, 
Portland, Oregon, USA, June 2011. Association for 
Computational Linguistics 
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexi-
calized rule selection. In Proc. of Coling 2008, 
pages 321?328. 
Franz Josef Och. (2003). Minimum error rate training 
in statistical machine translation. In Proc. of ACL 
2003, pages 160?167. 
Franz Josef Och and Hermann Ney. (2004). The 
alignment template approach to statistical machine 
translation. Computational Linguistics, 30:417?449. 
Dan Klein and Christopher D. Manning. (2003). Ac-
curate unlexicalized parsing. In Proc. of ACL-2003, 
pages 423-430. 
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 
(2003). Statistical phrase-based translation. In Pro-
ceedings of NAACL 2003, pages 58?54, Edmonton, 
Canada, May-June. 
Philipp Koehn. (2004). Statistical significance tests 
for machine translation evaluation. In Proceedings 
of EMNLP 2004, pages 388?395, Barcelona, Spain, 
July. 
P Koehn, H Hoang, A Birch, C Callison-Burch, M 
Federico, N Bertoldi, B Cowan, W Shen, C Moran 
and R Zens, (2007). Moses: Open source toolkit for 
statistical machine translation. In Proc. of ACL 
2007. pages 177?180, Prague, Czech Republic, 
June. Association for Computational Linguistics. 
Mamoru Komachi and Yuji Matsumoto. (2006). 
Phrase reordering for statistical machine translation 
based on predicate-argument structure. In Proceed-
ings of the International Workshop on Spoken 
Language Translation: Evaluation Campaign on 
Spoken Language Translation, pages 77?82. 
Ding Liu and Daniel Gildea. (2008). Improved tree-
to-string transducer for machine Translation. In 
Proceedings of the Third Workshop on Statistical 
Machine Translation, pages 62?69, Columbus, 
Ohio, USA, June 2008. 
Ding Liu and Daniel Gildea. (2010). Semantic role 
features for machine translation. In Proc. of Coling 
2010, pages 716?724, Beijing, China, August. 
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin. 
Maximum Entropy based Rule Selection Model for 
Syntax-based Statistical Machine Translation. In 
Proc. of EMNLP 2008.  
Yang Liu, Qun Liu and Shouxun Lin. (2006). Tree-to-
string alignment template for statistical machine 
translation. In Proc. of ACL-COLING 2006. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. (2006). SPMT: Statistical machine 
translation with syntactified target language 
phrases. In Proc. of EMNLP 2006, pages 44-52. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. (2002). Bleu: a method for automat-
ic evaluation of machine translation. In Proc. ACL 
2002, pages 311?318, Philadelphia, Pennsylvania, 
USA, July. 
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan 
Klein. (2006). Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 
21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440, Sydney, Australia, July. Association for Com-
putational Linguistics. 
Andreas Stolcke. (2002). Srilm ? an extensible lan-
guage modelling toolkit. In Proceedings of the 7th 
International Conference on Spoken Language 
Processing, pages 901?904, Denver, Colorado, 
USA, September. 
Dekai Wu and Pascale Fung. (2009a). Can semantic 
role labelling improve smt. In Proceedings of the 
13th Annual Conference of the EAMT, pages 218?
225, Barcelona, May. 
Dekai Wu and Pascale Fung. (2009b). Semantic roles 
for smt: A hybrid two-pass model. In Proc. NAACL 
2009, pages 13?16, Boulder, Colorado, June. 
ShuminWu and Martha Palmer. (2011). Semantic 
mapping using automatic word alignment and se-
mantic role labelling. In Proceedings of Fifth 
Workshop on Syntax, Semantics and Structure in 
Statistical Translation, pages 21?30, Portland, Or-
egon, USA, June 2011. 
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime 
Tsukada, and Masaaki Nagata. (2011). Extracting 
preordering rules from predicate-argument struc-
tures. In Proc. IJCNLP 2011, pages 29?37, Chiang 
Mai, Thailand, November.  
1135
Deyi Xiong, Qun Liu, and Shouxun Lin. (2006). Max-
imum entropy based phrase reordering model for 
statistical machine translation. In Proceedings of 
the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the 
Association for Computational Linguistics, pages 
521?528, Sydney, Australia, July. 
Deyi Xiong, Min Zhang, and Haizhou Li. (2012). 
Modelling the translation of predicate-argument 
structure for smt. In Proc. of ACL 2012, pages 
902?911, Jeju, Republic of Korea, 8-14 July 2012. 
Nianwen Xue. (2008). Labelling chinese predicates 
with semantic roles. Computational Linguistics, 
34(2): 225-255. 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing 
Zong. Machine Translation by Modeling Predicate- 
Argument Structure Transformation. In Proc. of 
COLING 2012. 
Hui Zhang, Min  Zhang, Haizhou Li and Eng Siong 
Chng. (2010). Non-isomorphic Forest Pair Transla-
tion. In Proceedings of EMNLP 2010, pages 440-
450, Massachusetts, USA, 9-11 October 2010.  
Tao Zhuang, and Chengqing Zong. (2010a). A mini-
mum error weighting combination strategy for chi-
nese semantic role labelling. In Proceedings of 
COLING-2010, pages 1362-1370. 
Tao Zhuang and Chengqing Zong. (2010b). Joint in-
ference for bilingual semantic role labelling. In 
Proceedings of EMNLP 2010, pages 304?314, 
Massachusetts, USA, 9-11 October 2010.  
1136
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1425?1434,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning a Phrase-based Translation Model from Mon-
olingual Data with Application to Domain Adaptation 
 
Jiajun Zhang and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences, Beijing, China 
{jjzhang, cqzong}@nlpr.ia.ac.cn 
 
 
 
Abstract 
Currently, almost all of the statistical ma-
chine translation (SMT) models are trained 
with the parallel corpora in some specific 
domains. However, when it comes to a lan-
guage pair or a different domain without 
any bilingual resources, the traditional SMT 
loses its power. Recently, some research 
works study the unsupervised SMT for in-
ducing a simple word-based translation 
model from the monolingual corpora. It 
successfully bypasses the constraint of 
bitext for SMT and obtains a relatively 
promising result. In this paper, we take a 
step forward and propose a simple but effec-
tive method to induce a phrase-based model 
from the monolingual corpora given an au-
tomatically-induced translation lexicon or a 
manually-edited translation dictionary. We 
apply our method for the domain adaptation 
task and the extensive experiments show 
that our proposed method can substantially 
improve the translation quality. 
1 Introduction 
During the last decade, statistical machine trans-
lation has made great progress. Novel translation 
models, such as phrase-based models (Koehn et 
a., 2007), hierarchical phrase-based models 
(Chiang, 2007) and linguistically syntax-based 
models (Liu et a., 2006; Huang et al, 2006; Gal-
ley, 2006; Zhang et al 2008; Chiang, 2010; 
Zhang et al, 2011; Zhai et al, 2011, 2012) have 
been proposed and achieved higher and higher 
translation performance. However, all of these 
state-of-the-art translation models rely on the 
parallel corpora to induce translation rules and 
estimate the corresponding parameters.  
It is unfortunate that the parallel corpora are 
very expensive to collect and are usually not 
available for resource-poor languages and for 
many specific domains even in a resource-rich 
language pair. 
Recently, more and more researchers concen-
trated on taking full advantage of the monolin-
gual corpora in both source and target languages, 
and proposed methods for bilingual lexicon in-
duction from non-parallel data (Rapp, 1995, 
1999; Koehn and Knight, 2002; Haghighi et al, 
2008; Daum? III and Jagarlamudi, 2011) and 
proposed unsupervised statistical machine trans-
lation (bilingual lexicon is a byproduct) with 
only monolingual corpora (Ravi and Knight, 
2011; Nuhn et al, 2012; Dou and Knight, 2012). 
In the bilingual lexicon induction (Koehn and 
Knight, 2002; Haghighi et al, 2008; Daum? III 
and Jagarlamudi, 2011), with the help of the or-
thographic and context features, researchers 
adopted an unsupervised method, such as canon-
ical correlation analysis (CCA) model, to auto-
matically induce the word translation pairs be-
tween two languages from non-parallel data only 
requiring that the monolingual data in each lan-
guage are from a fairly comparable domain. 
The unsupervised statistical machine transla-
tion method (Ravi and Knight, 2011; Nuhn et al, 
2012; Dou and Knight, 2012) viewed the trans-
lation task as a decipherment problem and de-
signed a generative model with the objective 
function to maximize the likelihood of the 
source language monolingual data. To tackle the 
large-scale vocabulary, they mainly considered 
the word-based model (e.g. IBM Model 3) and 
applied the Bayesian method with Gibbs sam-
pling or slice sampling. Finally, they used the 
learned translation model directly to translate 
unseen data (Ravi and Knight, 2011; Nuhn et al, 
2012) or incorporated the learned bilingual lexi-
con as a new in-domain translation resource into 
the phrase-based model which is trained with 
out-of-domain data to improve the domain adap-
tation performance in machine translation (Dou 
and Knight, 2012).  
We can easily see that these unsupervised 
methods can only induce the word-based transla-
tion rules (bilingual lexicon) at present. It is a 
big challenge that whether we can induce phrase 
1425
1, word reordering example:
?   ??   ?  ??   ?? ||| the purpose of the invention is to ||| 0-0 0-3 1-4 2-2 3-1 4-5 4-6
2, idiom example:
??   ??   ? ||| distinguish the true from the false ||| 0-0 1-2 1-5 2-1 2-4
3, unknown word translation:
??   ???   ??   ? ||| of the light-emitting diode chip ||| 0-2 1-2 2-4 3-0 3-1
 
Table 1: Examples of new translation knowledge learned with the proposed phrase pair induction method. For 
the three fields separated by ?|||?, the first two are respectively Chinese and English phrase, and the last one is 
the word alignment between these two phrases. 
 
level translation rules and learn a phrase-based 
model from the monolingual corpora. 
In this paper, we focus on exploring this di-
rection and propose a simple but effective meth-
od to induce the phrase-level translation rules 
from monolingual data. The main idea of our 
method is to divide the phrase-level translation 
rule induction into two steps: bilingual lexicon 
induction and phrase pair induction.  
Since many researchers have studied the bi-
lingual lexicon induction, in this paper, we 
mainly concentrate ourselves on phrase pair in-
duction given a probabilistic bilingual lexicon 
and two in-domain large monolingual data 
(source and target language). In addition, we 
will further introduce how to refine the induced 
phrase pairs and estimate the parameters of the 
induced phrase pairs, such as four standard 
translation features and phrase reordering feature 
used in the conventional phrase-based models 
(Koehn et al, 2007). The induced phrase-based 
model will be used to help domain adaptation 
for machine translation. 
In the rest of this paper, we first explain with 
examples to show what new translation 
knowledge can be learned with our proposed 
phrase pair induction method (Section 2), and 
then we introduce the approach for probabilistic 
bilingual lexicon acquisition in Section 3. In Sec-
tion 4 and 5, we respectively present our method 
for phrase pair induction and introduce an ap-
proach for phrase pair refinement and parameter 
estimation. Section 6 will show the detailed ex-
periments for the task of domain adaptation. We 
will introduce some related work in Section 7 
and conclude this paper in Section 8. 
2 What Can We Learn with Phrase 
Pair   Induction? 
Readers may doubt that if phrase pair induction 
is performed only using bilingual lexicon and 
monolingual data, what new translation 
knowledge can be learned? 
The bilingual lexicon can only express the 
translation equivalence between source- and tar-
get-side word pair and has little ability to deal 
with word reordering and idiom translation. In 
contrast, phrase pair induction can make up for 
this deficiency to some extent. Furthermore, our 
method is able to learn some unknown word 
translations. 
From the induced phrase pairs with our meth-
od, we have conducted a deep analysis and find 
that we can learn three kinds of new translation 
knowledge: 1) word reordering in a phrase pair; 
2) idioms; and 3) unknown word translations. 
Table 1 gives examples for each of the three 
kinds. For the first example, the source and tar-
get phrase are extracted respectively from mono-
lingual data, each word in the source phrase has 
a translation in the target phrase, but the word 
order is different. The word order encoded in a 
phrase pair is difficult to learn in a word-based 
SMT.  In the second example, the italic source 
word corresponds to two target words (in italic), 
and the phrase pair is an idiom which cannot be 
learned from word-based SMT. In the third ex-
ample, as we learn from the source and target 
monolingual text that the words around the italic 
ones are translations with each other, thus we 
cannot only extract a new phrase pair but also 
learn a translation pair of unknown words in 
italic. 
3 Probabilistic Bilingual Lexicon Ac-
quisition 
In order to induce the phrase pairs from the in-
domain monolingual data for domain adaptation, 
the probabilistic bilingual lexicon is essential. 
In this paper, we acquire the probabilistic bi-
lingual lexicon from two approaches: 1) build a 
bilingual lexicon from large-scale out-of-domain 
parallel data; 2) adopt a manually collected in-
domain lexicon. This paper uses Chinese-to-
English translation as a case study and electronic 
data is the in-domain data we focus on.  
1426
In Chinese-to-English translation, there are 
lots of parallel data on News. Here, we utilize 
about 2.08 million sentence pairs1 in News do-
main to learn a probabilistic bilingual lexicon. 
Basically, we can use GIZA++ (Och, 2003) to 
get the probabilistic lexicon. However, the prob-
lem is that each source-side word associates too 
many possible translations which contain much 
noise. For instance, in the lexicon obtained with 
GIZA++, each source-side word has about 13 
translations on average. The noise of the lexicon 
can influence the accuracy of the induced phrase 
pairs to a large extent. To learn a lexicon with a 
high precision, we follow Munteanu and Marcu 
(2006) to apply Log-Likelihood-Ratios (Dunning, 
1993; Melamed, 2000; Moore, 2004a, 2004b) to 
estimate how strong the association is between a 
source-side word and its aligned target-side word. 
We employ the same algorithm used in (Munte-
anu and Marcu, 2006) which first use the GI-
ZA++ (with grow-diag-final-and heuristic) to 
obtain the word alignment between source and 
target words, and then calculate the association 
strength between the aligned words. After using 
the log-likelihood-ratios algorithm2, we obtain a 
probabilistic bilingual lexicon with bidirectional 
translation probabilities from the out-of-domain 
data. In the final lexicon, the number of average 
translations is only 5. We call this lexicon LLR-
lex. 
   In the electronic domain, we manually collect-
ed a lexicon which contains about 140k entries. 
It should be noted that there is no translation 
probability in this lexicon. In order to assign 
probabilities to each entry, we apply the Corpus 
Translation Probability which used in (Wu et al, 
2008): given an in-domain source language 
monolingual data, we translate this data with the 
phrase-based model trained on the out-of-domain 
News data, the in-domain lexicon and the in-
domain target language monolingual data (for 
language model estimation).  With the source 
language data and its translation, we estimate the 
bidirectional translation probabilities for each 
entry in the original lexicon. For the entries 
whose translation probabilities are not estimated, 
we just assign a uniform probability. That is if a 
source word has n translations, then the transla-
tion probability of target word given the source 
word is 1/n. We call this lexicon Domain-lex. 
                                                 
1 LDC category numbers are: LDC2000T50, LDC2003E14, 
LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, 
LDC2005T10 and LDC2005T34. 
2  Following Moore (2004b), we use the threshold 10 on 
LLR to filter out unlikely translations. 
  We combine LLR-lex and Domain-lex to obtain 
the final probabilistic bilingual lexicon for phrase 
pair induction. 
4 Phrase Pair Induction Method 
Given a probabilistic bilingual lexicon and two 
monolingual data, we present a simple but effec-
tive method for phrase pair induction in this sec-
tion. 
 
 
Figure 1: a na?ve algorithm for phrase pair induction. 
4.1 A Na?ve Method 
We first introduce a relatively na?ve way to ex-
tract phrase pairs from the given resources. For a 
source phrase (word sequence), we can reorder 
the words in the phrase (permutation) first, and 
then obtain the target phrases with the bilingual 
lexicon (translation), and finally check if the tar-
get phrase is in the target monolingual data. The 
algorithm is given in Figure 1. 
Figure 1 shows that the na?ve algorithm is very 
easy to implement. However, the time complexi-
ty is too high. For each source phrase jis  (with 
? ?1 !j i? ?  permutations), suppose a source word 
has C translations on average and checking 
whether the target phrase ''jit  in T needs time 
? ?O T , then, phrase pair induction for a single 
source phrase needs time ? ?? ?1 1 !j iO C T j i? ? ? ?. 
It is very time consuming. One may design 
smarter algorithms. For example, one can collect 
distinct n-grams from source and target monolin-
gual data. Then, for a source-side phrase with 
length L, one can find the best translation candi-
date using the probabilistic bilingual lexicon 
from the target-side phrases with the same length 
L. The biggest disadvantage of these algorithms 
is that they can only induce phrase pair (with the 
Input:   Probabilistic bilingual lexicon V (each source word 
s maps a translation set V[s]) 
            Source language monolingual data S={sn} n=1...N 
            Target language monolingual data T={tm} m=1...M 
Output: Phrase pairs  P 
 
1: For each distinct source-side phrase jis  in S:  
2:       If each jk is s? in V: 
3: Collect [ ] jk k iV s ?  
4: For each permutation ''jis  of jis :  
5:        If ''jit  in T:     ' '[ ] ' [ , ]k kt V s k i j? ?  
6:  Add phrase pair ? ?'',j ji is t into P 
1427
same length) encoding word reordering, but can-
not learn phrase pairs in different length. Fur-
thermore, they cannot learn idioms and unknown 
word translations from monolingual data. Obvi-
ously, these kind of approaches is not optimal. 
4.2 Phrase Pair Induction with Inverted 
Index 
In order to make the phrase pair induction both 
effective and efficient, we propose a method 
using inverted index data structure which is usu-
ally a central component of a typical search en-
gine.  
The inverted index is employed to represent 
the target language monolingual data. For a tar-
get language word, the inverted index not only 
records the sentence position in monolingual 
data, but also records the word position in a sen-
tence. Some examples are shown in Table 2. By 
doing this, we do not need to iterate all the per-
mutations of source language phrase jis  to ex-
plore possible phrase pairs encoding word reor-
dering. Furthermore, it is possible to learn idiom 
translation and unknown word translations. We 
will elaborate how to induce phrase pairs with 
the help of inverted index. 
Target Language 
Word 
Position 
communication (2,5), (106,20), ?, (23022, 12) 
? ? 
zoom (90,2), (280,21), ?, (90239,15) 
Table 2: Some examples of inverted index for tar-
get language words, (2,5) means that ?communica-
tion? occurs at the 5th word of the 2nd sentence in the 
target monolingual data. 
The new algorithm for phrase pair induction is 
presented in Figure 2. Line 1 iterates all the dis-
tinct phrases in the source-side monolingual data. 
It can be implemented by collecting all the dis-
tinct n-grams in which n is the phrase length we 
are interested in (3 to 7 in this paper). For each 
distinct source-side phrase, Line 2-5 efficiently 
collects all the positions in the target monolin-
gual data for the translations of each word in the 
source phrase. Line 6 sorts the positions so that 
we can easily find the position sequence belong-
ing to a same sentence. Line 8-9 discards all the 
position sub-sequences that lack translations for 
more than one source-side words. That is to say 
we allow at most one unknown word in an in-
duced phrase pair in order to make the induction 
more accurate. Line 10 and Line 12 is the core 
of this algorithm. We first define a constraint 
before detailing the algorithm. 
Figure 2: Phrase pair induction using inverted index. 
Constraint: we require that there exists at 
most one phrase in a target sentence that is the 
translation of the source-side phrase. 
According to our analysis, it is not often to 
find that two phrases (length larger than 2) in a 
same sentence have the same meaning. Even if it 
happens, it is reasonable to keep the one with the 
highest probability. Given a position sequence 
belonging to a same sentence, Line 10 smoothes 
the probability of the single word gap according 
to the probabilities of the around words. Single 
word gap means that this word is not aligned but 
its left and right words are aligned with the 
words of the source-side phrase. Suppose the 
target sub-sequence is 
i i r jt t t?
 and 
i rt ?  is the 
only word that is not aligned with source-side 
words. We smooth the probability ? ?|i rp t null?  
as follows: 
? ?
? ? ? ?? ?
? ? ? ?
1 11 1
min | , |
, 1 1
2|
| |
,
2
i j
i r i r
i t j t
i r
i r t i r t
p t s p t s
if r or r j
p t null
p t s p t s
otherwise? ? ? ?
?
? ? ? ?
?
? ? ? ??
? ?
? ?
?
?
        (1) 
The above formula means that if the left or the 
right side only has one word, then the smoothed 
probability is one half of the minimum of the 
probabilities of the two neighbors, otherwise the 
smoothed probability is the average of the prob-
abilities of the two neighbors. This smoothing 
strategy encourages that if more words around 
the un-aligned word are translations of the 
source-side phrase, then the gap word is more 
likely to belong to the translations of the source-
side phrase. 
Input:   Probabilistic bilingual lexicon V (each source word s 
maps a translation set V[s]) 
            Source language monolingual data S={sn} n=1...N 
            Inverted index representing target language monolin-
gual data IMap 
Output: Phrase pairs P 
1: For each distinct source-side phrase jis  in S:  
2:      positionArray = [] 
3:      For each jk is s? : 
4:            For each [ ]kt V s? : 
5:       add  IMap[ t ]  into positionArray 
6:      Sort  positionArray 
7:      For each sequence in a same sentence in positionArray:  
8:              If more than 1 word in jis has no trans in the seq: 
9:                    Discard this seq and continue 
10:             Probability smoothing for single word gap 
11:             For all continuous position sub-sequence: 
12:                  Find the one kht  with maximum probability 
13:                 Add phrase pair ? ?,j ki hs t into P 
1428
After probability smoothing of the single gap 
word, we are ready to extract the candidate 
translation of the source-side phrase. Similar 
with Line 9 in Figure 2, we further filter the tar-
get continuous phrase if more than one word in 
source-side phrase has no translation in this tar-
get phrase. After that, we just choose the contin-
uous target phrase with the largest probability if 
two or more continuous target phrases exist in 
the same target sentence. The probability of a 
target-side phrase given the source-side phrase is 
computed similar to that of (Koehn et al, 2003) 
except that we impose length normalization: 
? ? ? ?? ? ? ?? ?
1
,1
1| , |
| ,
nn
lex i j
i j ai
p t s a p t s
j i j a ? ??
? ?
? ?? ? ??? ?
??
         (2) 
where the alignment a is produced using 
probabilistic bilingual lexicon. If a target word 
in t is a gap word, we suppose there is a word 
alignment between the target gap word and the 
source-side null.  
Similarly, we can compute the probability of 
source-side phrase given the target-side phrase 
? ?| ,lexp s t a . Then, we find the target-side phrase 
which has the biggest value of 
? ? ? ?| , | ,lex lexp t s a p s t a? . Line 13 in Figure 2 col-
lects the induced phrase pairs. 
For the time complexity, it depends on the 
length of positionArray, since the time complex-
ity of the core algorithm (Line 7-13) is propor-
tional to the length of positionArray. If posi-
tionArray contains almost all the positions in the 
target monolingual data T, then the worst time 
complexity will be ? ?logO T T  (for array sort). 
However, we find in the target monolingual data 
(1 million sentences) that each distinct word 
happens 110 times on average. Then, for a 
sources-side phrase with 7 words, the average 
length of positionArray will be 3850, since each 
source word has averagely 5 target translations 
(mentioned in Section 3). Therefore, the algo-
rithm is relatively efficient in the average case. 
5 Phrase Pair Refinement and Parame-
terization 
5.1 Phrase Pair Refinement 
Some of the phrase pairs induced in Section 4 
may contain noise. According to our analysis, 
we find that the biggest problem is that in the 
target-side of the phrase pair, there are two or 
more identical words aligned to the same source-
side word. For example, we extract a phrase pair 
as follows: 
?  ??  ??
of  business information of 
In the above phrase pair, there are two words 
?of? in the target side and the first one is redun-
dant. The phrase pair induction algorithm pre-
sented in Section 4 cannot deal with this situa-
tion. In this section, we propose a simple ap-
proach to handle this problem. For each entry in 
LLR-lex, such as (?, of), we can learn two kinds 
of information from the out-of-domain word-
aligned sentence pairs: one is whether the target 
translation is before or after the translation of the 
preceding source-side word (Order); the other is 
whether the target translation is adjacent with 
the translation of the preceding source-side word 
(Adjacency). If the source-side word is the be-
ginning of the phrase, we calculate the corre-
sponding information with the succeeding word 
instead of the preceding word. For the entries in 
Domain-lex, we constrain that the target transla-
tion should be adjacent with the translations of 
its source-side neighbors and translation order is 
the same with the source-side words. 
With the Order and Adjacency information, 
we first check the order information, and then 
check the adjacency information if the dupli-
cates cannot be handled using order information. 
For example, since (?, of) is an entry in LLR-
lex and we have learned that ?of? is much more 
likely to be behind the translation of the suc-
ceeding word. Thus, the first word ?of? can be 
discarded. This refinement can be applied before 
finding the phrase pair with maximum probabil-
ity (Line 12 in Figure 2) so that the duplicate 
words do not affect the calculation of translation 
probability of phrase pair. 
5.2 Translation Probability Estimation 
It is well known that in the phrase-based SMT 
there are four translation probabilities and the 
reordering probability for each phrase pair. 
   The translation probabilities in the traditional 
phrase-based SMT include bidirectional phrase 
translation probabilities and bidirectional lexical 
weights. For the lexical weights, we can use the 
? ?| ,lexp s t a  and ? ?| ,lexp t s a computed in the 
above section without length normalization. 
However, for the phrase-level probability, we 
cannot use maximum likelihood estimation since 
the phrase pairs are not extracted from parallel 
sentences. 
1429
 In this paper, we borrow and extend the idea of 
(Klementiev et al, 2012) to calculate the phrase-
level translation probability with context infor-
mation in source and target monolingual corpus. 
The value is calculated using a vector space 
model. With source and target vocabularies 
? ?1 2, , , Ns s s  and ? ?1 2, , , Mt t t , the source-side 
phrase s and target-side phrase t can be respec-
tively represented in an N- and M-dimensional 
vector. The k-th component of s?s contextual 
vector is computed using the method of (Fung 
and Yee, 1998) as follows: 
? ?? ?, maxlog / 1k s k kw n n n? ? ?               (3) 
where 
,s kn
and 
kn denotes the number of times ks  
occurs in the context of s and in the entire source 
language monolingual data, and 
maxn is the max-
imum number of occurrence of any source-side 
word in the source language monolingual data. 
The k-th element of t?s vector can be computed 
with the same method. We finally normalize 
these vectors with L2-norm. 
   With the s?s and t?s contextual vector represen-
tations, we calculate two similarities: 1) project 
s?s vector into target side t  with the lexical 
mapping p(t|s), and then get the similarity by 
computing the cosine of two angles between t?s 
and t ?s vectors; 2) project t?s vector into source 
side s  with the lexical mapping p(s|t), and then 
obtain the similarity between s?s and s ?s vectors. 
These two contextual similarities will serve as 
two phrase-level translation probabilities. 
5.3 Reordering Probability Estimation 
For the reordering probabilities of newly induced 
phrase pairs, we can also follow Klementiev et al 
(2012) to estimate these probabilities using 
source and target monolingual data. The method 
is to calculate six probabilities for monotone, 
swap or discontinuous cases. For the phrase pair 
(? ?? ?? , business information of), we 
find a source sentence containing ? ?? ??, 
and find a target sentence containing business 
information of. If there is another phrase pair 
? ?,s t ,  t  exactly follows business information of 
and s  occurs in the same source sentence with 
? ?? ??, then we compare the position 
relationship between s  and ? ?? ??. We 
increment the swap count if s  is just before ? 
?? ??. After counting, we finally use max-
imum likelihood estimation method to compute 
the reordering probabilities. 
6 Related Work 
As far as we know, few researchers study phrase 
pair induction from only monolingual data. 
   There are three research works that are most 
related with ours. One is using an in-domain 
probabilistic bilingual lexicon to extract sub-
sentential parallel fragments from comparable 
corpora (Munteanu and Marcu, 2006; Quirk et al, 
2007; Cettolo et al, 2010). Munteanu and Marcu 
(2006) first extract the candidate parallel sen-
tences from the comparable corpora and further 
extract the accurate sub-sentential bilingual 
fragments from the candidate parallel sentences 
using the in-domain probabilistic bilingual lexi-
con. Compared with their work, our focus is to 
induce phrase pairs directly from monolingual 
data rather than comparable data. Thus, finding 
the candidate parallel sentences is not possible in 
our situation. 
Another is to make full use of monolingual da-
ta with transductive learning (Ueffing et al, 2007; 
Schwenk, 2008; Wu et al, 2008; Bertoldi and 
Federico, 2009). For the target-side monolingual 
data, they just use it to train language model, and 
for the source-side monolingual data, they em-
ploy a baseline (word-based SMT or phrase-
based SMT trained with small-scale bitext) to 
first translate the source sentences, combining 
the source sentence and its target translation as a 
bilingual sentence pair, and then train a new 
phrase-base SMT with these pseudo sentence 
pairs. This method cannot learn idiom transla-
tions and unknown word translations. 
The third is to estimate the translation parame-
ters and reordering parameters using monolin-
gual data given the phrase pairs (Klementiev et 
al., 2012). Their work supposes the phrase pairs 
are already given and then corresponding param-
eters can be learned with monolingual data. Dif-
ferent from their work, we concentrate ourselves 
on inducing phrase pairs from monolingual data 
and then borrow some ideas from theirs for pa-
rameter estimation. Furthermore, we extend their 
contextual similarity between source and target 
phrases to both directions. 
7 Experiments 
7.1 Experimental Setup  
Our purpose is to induce phrase pairs to improve 
translation quality for domain adaptation. We 
have introduced the out-of-domain data and the 
electronic in-domain lexicon in Section 3. Here 
we introduce other information about the in-
1430
domain data. Besides the in-domain lexicon, we 
have collected respectively 1 million monolin-
gual sentences in electronic area from the web. 
They are neither parallel nor comparable because 
we cannot even extract a small number of paral-
lel sentence pairs from this monolingual data 
using the method of (Munteanu and Marcu, 
2006). We further employ experts to translate 
2000 Chinese electronic sentences into English. 
The first half is used as the tuning set (elec1000-
tune) and the second half is employed as the test-
ing set (elec1000-test). 
   We construct two kinds of phrase-based mod-
els using Moses (Koehn et al, 2007): one uses 
out-of-domain data and the other uses in-domain 
data. For the out-of-domain data, we build the 
phrase table and reordering table using the 2.08 
million Chinese-to-English sentence pairs, and 
we use the SRILM toolkit (Stolcke, 2002) to 
train the 5-gram English language model with 
the target part of the parallel sentences and the 
Xinhua portion of the English Gigaword. For the 
in-domain electronic data, we first consider the 
lexicon as a phrase table in which we assign a 
constant 1.0 for each of the four probabilities, 
and then we combine this initial phrase table and 
the induced phrase pairs to form the new phrase 
table. The in-domain reordering table is created 
for the induced phrase pairs. An in-domain 5-
gram English language model is trained with the 
target 1 million monolingual data. 
   We use BLEU (Papineni et al, 2002) score 
with shortest length penalty as the evaluation 
metric and apply the pairwise re-sampling ap-
proach (Koehn, 2004) to perform the signifi-
cance test. 
7.2 Experimental Results  
In this section, we first conduct experiments to 
figure out how the translation performance de-
grades when the domain changes. To better illus-
trate the comparison, we first use News data to 
evaluate the NIST evaluation tests and then use 
the same News data to evaluate the electronic 
test sets. For the NIST evaluation, we employ 
Chinese-to-English NIST MT03 as the tuning set 
and NIST MT05 as the test set. Table 3 gives the 
results. It is obvious that, it is relatively high 
when using the News training data to evaluate 
the same News test set. However, when the test 
domain is changed, the translation performance 
decreases to a large extent. 
Given the in-domain bilingual lexicon and two 
monolingual data, previous works also proposed 
some good methods to explore the potential of 
the given data to improve the translation quality. 
Here, we implement their approaches and use 
them as our strong baseline. Wu et al (2008) 
regards the in-domain lexicon with corpus trans-
lation probability as another phrase table and 
further use the in-domain language model be-
sides the out-of-domain language model. Table 4 
gives the results. We can see from the table that 
the domain lexicon is much helpful and signifi-
cantly outperforms the baseline with more than 
4.0 BLEU points. When it is enhanced with the 
in-domain language model, it can further im-
prove the translation performance by more than 
2.5 BLEU points. This method has made good 
use of in-domain lexicon and the target-side in-
domain monolingual data, but it does not take 
full advantage of the in-domain source-side 
monolingual data. 
In order to use source-side monolingual data, 
Ueffing et al (2007), Schwenk (2008), Wu et al 
(2008) and Bertoldi and Federico (2009) em-
ployed the transductive learning to first translate 
the source-side monolingual data using the best 
configuration (baseline+in-domain lexicon+in-
domain language model) and obtain 1-best trans-
lation for each source-side sentence. With the 
source-side sentences and their translations, the 
new phrase table and reordering table are built. 
Then, these resources are added into the best 
configuration. The experimental results are pre-
sented in the last low of Table 4. From the results, 
we see that transductive learning can further im-
prove the translation performance significantly 
by 0.6 BLEU points. 
In tranductive learning, in-domain lexicon and 
both-side monolingual data have been explored. 
However, this method does not take full ad-
vantage of both-side monolingual data because it 
uses source and target monolingual data individ-
ually. In our method, we explore fully the source 
and target monolingual data to induce translation 
equivalence on the phrase level. In order to make 
the phrase pair induction more efficient, we first 
sort all the sentences in the both-side monolin-
gual data according to the word hit rate in the 
bilingual lexicon. Then, we conduct six sets of 
experiments respectively on the first 100k, 200k, 
300k, 500k and whole 1m sentences. All the ex-
periments are run based on the configuration 
with BLEU 13.41 in Table 4, and we call this 
configuration BestConfig. Note that the unknown 
words are only allowed if the source-side of a 
phrase pair has more than 3 words. Table 5 
shows the results. 
1431
 Training Data Tune Data (NIST MT03) Test Data (NIST MT05) 
2.08M sentence pairs in News 
35.79 34.26 
Tune Data (elec1000-tune) Test Data (elec1000-test) 
7.93 6.69 
Table 3: Experimental results using News training data to test NIST evaluation data and electronic data (numbers 
denote BLEU score points in percent). 
 
Method Tune (elec1000-tune) Test (elec1000-test) 
Baseline 7.93 6.69 
baseline + in-domain lexicon 10.97 10.87 
baseline + in-domain lexicon + in-
domain language model 
13.72 13.41++ 
Transductive Learning 14.13 14.01* 
Table 4: Experimental results using News training data, in-domain lexicon, language model and transductive 
learning. Bold figures mean that the results are statistically significant better than the baseline with p<0.01, and 
?++? denotes the result is statistically significant better than baseline+in-domain lexicon. ?*? means that the 
result is statistically significant better than 13.41 with p<0.05. 
 
Method Tune (BLEU %) Test (BLEU %) 
BestConfig 13.72 13.41 
+phrase pair induction (100k) 14.23 14.06 
+phrase pair induction (200k) 14.45 14.24 
+phrase pair induction (300k) 14.76 14.83++ 
+phrase pair induction (500k) 14.98 15.16++ 
+phrase pair induction (1m) 15.11 15.30++ 
Table 5: Experimental results of our phrase pair induction method. Bold figures denotes the corresponding 
method significantly outperform the BestConfig with p<0.05. Bold and Italic figures means the results are sig-
nificantly better than that of BestConfig with p<0.01. ?++? denotes that the corresponding approach performs 
significantly better than Transductive Learning with p<0.01. 
 
Method Before Filtering After Filtering 
+phrase pair induction (100k) 72,615 8,724 
+phrase pair induction (200k) 108,948 12,328 
+phrase pair induction (300k) 136,529 17,505 
+phrase pair induction (500k) 150,263 19,862 
+phrase pair induction (1m) 169,172 21,486 
Table 6: the number of phrase pairs induced with different size of monolingual data. 
 
  We can see from the table that our method ob-
tains the best translation performance. When us-
ing the first 100k sentences for phrase pair induc-
tion, it obtains a significant improvement over 
the BestConfig by 0.65 BLEU points and can 
outperform the transductive learning method.  
When we use more monolingual data, the per-
formance becomes even better.  The method of 
phrase pair induction using 300k sentences per-
forms quite well. It outperforms the BestConfig 
significantly with an improvement of 1.42 BLEU 
points and it also performs much better than 
transductive learning method with gains of 0.82 
BLEU points. With the monolingual data larger 
and larger, the gains become smaller and smaller 
because the word hit rate gets lower and lower. 
These experimental results empirically show the 
effectiveness of our proposed phrase pair induc-
tion method. 
   A question remains that how many new phrase 
pairs are induced with different size of monolin-
gual data. Here, we give respectively the statis-
tics before and after filtering with the 1000 test 
sentences. Table 6 shows the statistics. We can 
see from the table that lots of new phrase pairs 
can be induced since the source and target mono-
lingual data is in the same domain. However, 
since the source and target monolingual data is 
1432
far from parallel, most of the phrase pairs are not 
long. For example, in the 108,948 distinct phrase 
pairs, we find that the phrase pair distribution 
according to source-side length is (3:50.6%, 
4:35.6%, 5:3.3%, 6:9.8%, 7:0.7%). It is easy to 
see that the phrase pairs whose source-side 
length longer than 4 account for only a very 
small part. 
8 Conclusion and Future Work 
This paper proposes a simple but effective meth-
od to induce phrase pairs from monolingual data. 
Given the probabilistic bilingual lexicon and 
both-side monolingual data in the same domain, 
the method employs inverted index structure to 
represent the target-side monolingual data, and 
induce the translations for each distinct source-
side phrase with the help of the bilingual lexicon. 
We further propose an approach to refine the re-
sult phrase pairs to make them more accurate. 
We also introduce how to estimate the translation 
and reordering parameters for the induced phrase 
pairs with monolingual data. Extensive experi-
ments on domain adaptation have shown that our 
method can significantly outperform previous 
methods which also focus on exploring the in-
domain lexicon and monolingual data. 
However, through the analysis we find that our 
induced phrase pairs still contain some noise, 
such as the words in source- and target-side of 
the phrase pair are all aligned but the target-side 
phrase expresses the different meaning. Further-
more, our proposed method cannot learn expres-
sions which are not lexical translations but are 
semantic ones. In the future, we will study fur-
ther on these phenomena and propose new meth-
ods to handle these problems. 
Acknowledgments 
The research work has been funded by the Hi-
Tech Research and Development Program (?863? 
Program) of China under Grant No. 
2011AA01A207, 2012AA011101 and 
2012AA011102, and also supported by the Key 
Project of Knowledge Innovation of Program of 
Chinese Academy of Sciences under Grant No. 
KGZD-EW-501. We would also like to thank the 
anonymous reviewers for their valuable sugges-
tions.  
References  
Nicola Bertoldi and Marcello Federico, 2009. Domain 
adaptation for statistical machine translation with 
monolingual resources. In Proc. of the Fourth 
Workshop on Statistical Machine Translation, 
pages 182-189. 
Mauro Cettolo, Marcello Federico and Nicola 
Bertoldi, 2010. Mining parallel fragments from 
comparable texts. In Proc. of the seventh 
International Workshop on Spoken Language 
Translation (IWSLT), pages 227-234. 
David Chiang, 2007. Hierarchical phrase-based 
translation. computational linguistics, 33 (2). 
pages 201-228. 
David Chiang, 2010. Learning to translate with source 
and target syntax. In Proc. of ACL 2010, pages 
1443-1452. 
Hal Daum? III and Jagadeesh Jagarlamudi, 2011. 
Domain adaptation for machine translation by 
mining unseen words. In Proc. of ACL-HLT 
2011. 
Qing Dou and Kevin Knight, 2012. Large Scale 
Decipherment for Out-of-Domain Machine 
Translation. In Proc. of EMNLP-CONLL 2012. 
Ted Dunning, 1993. Accurate methods for the 
statistics of surprise and coincidence. 
computational linguistics, 19 (1). pages 61-74. 
Pascale Fung and Lo Yuen Yee, 1998. An IR 
approach for translating new words from 
nonparallel, comparable texts. In Proc. of ACL-
COLING 1998., pages 414-420. 
Michel Galley, Jonathan Graehl, Kevin Knight, 
Daniel Marcu, Steve DeNeefe, Wei Wang and 
Ignacio Thayer, 2006. Scalable inference and 
training of context-rich syntactic translation 
models. In Proc. of COLING-ACL 2006, pages 
961-968. 
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick 
and Dan Klein, 2008. Learning bilingual 
lexicons from monolingual corpora. In Proc. of 
ACL-08: HLT, pages 771-779. 
Liang Huang, Kevin Knight and Aravind Joshi, 2006. 
A syntax-directed translator with extended 
domain of locality. In Proc. of AMTA 2006, 
pages 1-8. 
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch and David Yarowsky, 2012. Toward 
statistical machine translation without parallel 
corpora. In Proc. of EACL 2012., pages 130-140. 
Philipp Koehn, 2004. Statistical significance tests for 
machine translation evaluation. In Proc. of 
EMNLP 2004., pages 388-395, Barcelona, Spain, 
July 25th-26th, 2004. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, 
Marcello Federico, Nicola Bertoldi, Brooke 
Cowan, Wade Shen, Christine Moran, Richard 
Zens, Chris Dyer, Ond?ej Bojar, Alexandra 
Constantin and Evan Herbst, 2007. Moses: Open 
source toolkit for statistical machine translation. 
In Proc. of ACL on Interactive Poster and 
Demonstration Sessions 2007., pages 177-180, 
Prague, Czech Republic, June 27th-30th, 2007. 
Philipp Koehn and Kevin Knight, 2002. Learning a 
translation lexicon from monolingual corpora. In 
1433
Proc. of the ACL-02 workshop on Unsupervised 
lexical acquisition, pages 9-16. 
Yang Liu, Qun Liu and Shouxun Lin, 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. of COLING-ACL 2006, 
pages 609-616. 
I. Dan Melamed, 2000. Models of translational 
equivalence among words. computational 
linguistics, 26 (2). pages 221-249. 
Rorbert C. Moore, 2004a. Improving IBM word-
alignment model 1. In Proc. of ACL 2004. 
Rorbert C. Moore, 2004b. On log-likelihood-ratios 
and the significance of rare events. In Proc. of 
EMNLP 2004., pages 333-340. 
Dragos Stefan Munteanu and Daniel Marcu, 2006. 
Extracting parallel sub-sentential fragments from 
non-parallel corpora. In Proc. of ACL-COLING 
2006. 
Malte Nuhn, Arne Mauser and Hermann Ney, 2012. 
Deciphering Foreign Language by Combining 
Language Models and Context Vectors. In Proc. 
of ACL 2012. 
Franz Josef Och and Hermann Ney., 2003. A 
systematic comparison of various statistical 
alignment models. computational linguistics, 29 
(1). pages 19-51. 
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu, 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proc. of ACL 2002., pages 311-318. 
Chris Quirk, Raghavendra Udupa and Arul Menezes, 
2007. Generative models of noisy translations 
with applications to parallel fragment extraction. 
In Proc. of the Machine Translation Summit XI, 
pages 377-384. 
Reinhard Rapp, 1995. Identifying word translations in 
non-parallel texts. In Proc. of ACL 1995, pages 
320-322. 
Reinhard Rapp, 1999. Automatic identification of 
word translations from unrelated English and 
German corpora. In Proc. of ACL 1999, pages 
519-526. 
Sujith Ravi and Kevin Knight, 2011. Deciphering 
foreign language. In Proc. of ACL 2011., pages 
12-21. 
Holger Schwenk, 2008. Investigations on largescale 
lightly-supervised training for statistical machine 
translation. In Proc. of IWSLT 2008, pages 182-
189. 
Andreas Stolcke, 2002. SRILM-an extensible 
language modeling toolkit. In Proc. of 7th 
International Conference on Spoken Language 
Processing, pages 901-904, Denver, Colorado, 
USA, September 16th-20th, 2002. 
Nicola Ueffing, Gholamreza Haffari and Anoop 
Sarkar, 2007. Transductive learning for statistical 
machine translation. In Proc. of ACL 2007. 
Hua Wu, Haifeng Wang and Chengqing Zong, 2008. 
Domain adaptation for statistical machine 
translation with domain dictionary and 
monolingual corpora. In Proc. of COLING 2008., 
pages 993-1000. 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing 
Zong, 2011.  Simple but effective approaches to 
improving tree-to-tree model.  In Proc. of MT 
Summit XIII 2011, pages 261-268. 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing 
Zong, 2012. Tree-based translation without using 
parse trees. In Proc. of COLING 2012, pages 
3037-3054. 
Jiajun Zhang, Feifei Zhai and Chengqing Zong, 2011. 
Augmenting string-to-tree translation models 
with fuzzy use of the source-side syntax. In Proc. 
of EMNLP 2011, pages 204-215. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan and Sheng Li, 2008. A tree 
sequence alignment-based tree-to-tree translation 
model. In Proc. of ACL-08: HLT, pages 559-567. 
 
1434
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111?121,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Bilingually-constrained Phrase Embeddings for Machine Translation
Jiajun Zhang
1
, Shujie Liu
2
, Mu Li
2
, Ming Zhou
2
and Chengqing Zong
1
1
National Laboratory of Pattern Recognition, CASIA, Beijing, P.R. China
{jjzhang,cqzong}@nlpr.ia.ac.cn
2
Microsoft Research Asia, Beijing, P.R. China
{shujliu,muli,mingzhou}@microsoft.com
Abstract
We propose Bilingually-constrained Re-
cursive Auto-encoders (BRAE) to learn
semantic phrase embeddings (compact
vector representations for phrases), which
can distinguish the phrases with differ-
ent semantic meanings. The BRAE is
trained in a way that minimizes the seman-
tic distance of translation equivalents and
maximizes the semantic distance of non-
translation pairs simultaneously. After
training, the model learns how to embed
each phrase semantically in two languages
and also learns how to transform semantic
embedding space in one language to the
other. We evaluate our proposed method
on two end-to-end SMT tasks (phrase ta-
ble pruning and decoding with phrasal se-
mantic similarities) which need to mea-
sure semantic similarity between a source
phrase and its translation candidates. Ex-
tensive experiments show that the BRAE
is remarkably effective in these two tasks.
1 Introduction
Due to the powerful capacity of feature learn-
ing and representation, Deep (multi-layer) Neural
Networks (DNN) have achieved a great success in
speech and image processing (Kavukcuoglu et al,
2010; Krizhevsky et al, 2012; Dahl et al, 2012).
Recently, statistical machine translation (SMT)
community has seen a strong interest in adapting
and applying DNN to many tasks, such as word
alignment (Yang et al, 2013), translation confi-
dence estimation (Mikolov et al, 2010; Liu et al,
2013; Zou et al, 2013), phrase reordering predic-
tion (Li et al, 2013), translation modelling (Auli et
al., 2013; Kalchbrenner and Blunsom, 2013) and
language modelling (Duh et al, 2013; Vaswani et
al., 2013). Most of these works attempt to im-
prove some components in SMT based on word
embedding, which converts a word into a dense,
low dimensional, real-valued vector representation
(Bengio et al, 2003; Bengio et al, 2006; Collobert
and Weston, 2008; Mikolov et al, 2013).
However, in the conventional (phrase-based)
SMT, phrases are the basic translation units. The
models using word embeddings as the direct in-
puts to DNN cannot make full use of the whole
syntactic and semantic information of the phrasal
translation rules. Therefore, in order to success-
fully apply DNN to model the whole translation
process, such as modelling the decoding process,
learning compact vector representations for the ba-
sic phrasal translation units is the essential and
fundamental work.
In this paper, we explore the phrase embedding,
which represents a phrase (sequence of words)
with a real-valued vector. In some previous works,
phrase embedding has been discussed from differ-
ent views. Socher et al (2011) make the phrase
embeddings capture the sentiment information.
Socher et al (2013a) enable the phrase embed-
dings to mainly capture the syntactic knowledge.
Li et al (2013) attempt to encode the reordering
pattern in the phrase embeddings. Kalchbrenner
and Blunsom (2013) utilize a simple convolution
model to generate phrase embeddings from word
embeddings. Mikolov et al (2013) consider a
phrase as an indivisible n-gram. Obviously, these
methods of learning phrase embeddings either fo-
cus on some aspects of the phrase (e.g. reordering
pattern), or impose strong assumptions (e.g. bag-
of-words or indivisible n-gram). Therefore, these
phrase embeddings are not suitable to fully repre-
sent the phrasal translation units in SMT due to the
lack of semantic meanings of the phrase.
Instead, we focus on learning phrase embed-
dings from the view of semantic meaning, so
that our phrase embedding can fully represent the
phrase and best fit the phrase-based SMT. As-
suming the phrase is a meaningful composition
111
of its internal words, we propose Bilingually-
constrained Recursive Auto-encoders (BRAE) to
learn semantic phrase embeddings. The core idea
behind is that a phrase and its correct translation
should share the same semantic meaning. Thus,
they can supervise each other to learn their seman-
tic phrase embeddings. Similarly, non-translation
pairs should have different semantic meanings,
and this information can also be used to guide
learning semantic phrase embeddings.
In our method, the standard recursive auto-
encoder (RAE) pre-trains the phrase embedding
with an unsupervised algorithm by minimizing the
reconstruction error (Socher et al, 2010), while
the bilingually-constrained model learns to fine-
tune the phrase embedding by minimizing the se-
mantic distance between translation equivalents
and maximizing the semantic distance between
non-translation pairs.
We use an example to explain our model. As
illustrated in Fig. 1, the Chinese phrase on the
left and the English phrase on the right are trans-
lations with each other. If we learn the embedding
of the Chinese phrase correctly, we can regard it
as the gold representation for the English phrase
and use it to guide the process of learning English
phrase embedding. In the other direction, the Chi-
nese phrase embedding can be learned in the same
way. This procedure can be performed with an
co-training style algorithm so as to minimize the
semantic distance between the translation equiva-
lents
1
. In this way, the result Chinese and English
phrase embeddings will capture the semantics as
much as possible. Furthermore, a transformation
function between the Chinese and English seman-
tic spaces can be learned as well.
With the learned model, we can accurately mea-
sure the semantic similarity between a source
phrase and a translation candidate. Accordingly,
we evaluate the BRAE model on two end-to-
end SMT tasks (phrase table pruning and decod-
ing with phrasal semantic similarities) which need
to check whether a translation candidate and the
source phrase are in the same meaning. In phrase
table pruning, we discard the phrasal translation
rules with low semantic similarity. In decoding
with phrasal semantic similarities, we apply the
semantic similarities of the phrase pairs as new
features during decoding to guide translation can-
1
For simplicity, we do not show non-translation pairs
here.
source phrase embedding ps  
?? ? ??? France and Russia 
target phrase embedding pt  
Figure 1: A motivation example for the BRAE
model.
didate selection. The experiments show that up to
72% of the phrase table can be discarded without
significant decrease on the translation quality, and
in decoding with phrasal semantic similarities up
to 1.7 BLEU score improvement over the state-of-
the-art baseline can be achieved.
In addition, our semantic phrase embeddings
have many other potential applications. For in-
stance, the semantic phrase embeddings can be
directly fed to DNN to model the decoding pro-
cess. Besides SMT, the semantic phrase embed-
dings can be used in other cross-lingual tasks (e.g.
cross-lingual question answering) and monolin-
gual applications such as textual entailment, ques-
tion answering and paraphrase detection.
2 Related Work
Recently, phrase embedding has drawn more and
more attention. There are three main perspectives
handling this task in monolingual languages.
One method considers the phrases as bag-of-
words and employs a convolution model to trans-
form the word embeddings to phrase embeddings
(Collobert et al, 2011; Kalchbrenner and Blun-
som, 2013). Gao et al (2013) also use bag-of-
words but learn BLEU sensitive phrase embed-
dings. This kind of approaches does not take the
word order into account and loses much informa-
tion. Instead, our bilingually-constrained recur-
sive auto-encoders not only learn the composition
mechanism of generating phrases from words, but
also fine tune the word embeddings during the
model training stage, so that we can induce the full
information of the phrases and internal words.
Another method (Mikolov et al, 2013) deals
with the phrases having a meaning that is not a
simple composition of the meanings of its indi-
vidual words, such as New York Times. They first
find the phrases of this kind. Then, they regard
these phrases as indivisible units, and learn their
embeddings with the context information. How-
112
ever, this kind of phrase embedding is hard to cap-
ture full semantics since the context of a phrase
is limited. Furthermore, this method can only ac-
count for a very small part of phrases, since most
of the phrases are compositional. In contrast, our
method attempts to learn the semantic vector rep-
resentation for any phrase.
The third method views any phrase as the mean-
ingful composition of its internal words. The re-
cursive auto-encoder is typically adopted to learn
the way of composition (Socher et al, 2010;
Socher et al, 2011; Socher et al, 2013a; Socher
et al, 2013b; Li et al, 2013). They pre-train the
RAE with an unsupervised algorithm. And then,
they fine-tune the RAE according to the label of
the phrase, such as the syntactic category in pars-
ing (Socher et al, 2013a), the polarity in sentiment
analysis (Socher et al, 2011; Socher et al, 2013b),
and the reordering pattern in SMT (Li et al, 2013).
This kind of semi-supervised phrase embedding is
in fact performing phrase clustering with respect
to the phrase label. For example, in the RAE-
based phrase reordering model for SMT (Li et
al., 2013), the phrases with the similar reorder-
ing tendency (e.g. monotone or swap) are close
to each other in the embedding space, such as the
prepositional phrases. Obviously, this kind meth-
ods of semi-supervised phrase embedding do not
fully address the semantic meaning of the phrases.
Although we also follow the composition-based
phrase embedding, we are the first to focus on
the semantic meanings of the phrases and propose
a bilingually-constrained model to induce the se-
mantic information and learn transformation of the
semantic space in one language to the other.
3 Bilingually-constrained Recursive
Auto-encoders
This section introduces the Bilingually-
constrained Recursive Auto-encoders (BRAE),
that is inspired by two observations. First, the
recursive auto-encoder provides a reasonable
composition mechanism to embed each phrase.
And the semi-supervised phrase embedding
(Socher et al, 2011; Socher et al, 2013a; Li et
al., 2013) further indicates that phrase embedding
can be tuned with respect to the label. Second,
even though we have no correct semantic phrase
representation as the gold label, the phrases
sharing the same meaning provide an indirect but
feasible way.
x1 x2 x3 x4 
y1=f(W(1)[x1; x2]+b) 
y2=f(W(1)[y1; x3]+b) 
y3=f(W(1)[y2; x4]+b) 
Figure 2: A recursive auto-encoder for a four-
word phrase. The empty nodes are the reconstruc-
tions of the input.
We will first briefly present the unsupervised
phrase embedding, and then describe the semi-
supervised framework. After that, we introduce
the BRAE on the network structure, objective
function and parameter inference.
3.1 Unsupervised Phrase Embedding
3.1.1 Word Vector Representations
In phrase embedding using composition, the word
vector representation is the basis and serves as the
input to the neural network. After learning word
embeddings with DNN (Bengio et al, 2003; Col-
lobert and Weston, 2008; Mikolov et al, 2013),
each word in the vocabulary V corresponds to a
vector x ? R
n
, and all the vectors are stacked into
an embedding matrix L ? R
n?|V |
.
Given a phrase which is an ordered list of m
words, each word has an index i into the columns
of the embedding matrix L. The index i is used to
retrieve the word?s vector representation using a
simple multiplication with a binary vector e which
is zero in all positions except for the ith index:
x
i
= Le
i
? R
n
(1)
Note that n is usually set empirically, such as n =
50, 100, 200. Throughout this paper, n = 3 is used
for better illustration as shown in Fig. 1.
3.1.2 RAE-based Phrase Embedding
Assuming we are given a phrase w
1
w
2
? ? ?w
m
,
it is first projected into a list of vectors
(x
1
, x
2
, ? ? ? , x
m
) using Eq. 1. The RAE learns
the vector representation of the phrase by recur-
sively combining two children vectors in a bottom-
up manner (Socher et al, 2011). Fig. 2 illustrates
an instance of a RAE applied to a binary tree, in
113
which a standard auto-encoder (in box) is re-used
at each node. The standard auto-encoder aims at
learning an abstract representation of its input. For
two children c
1
= x
1
and c
2
= x
2
, the auto-
encoder computes the parent vector y
1
as follows:
p = f(W
(1)
[c
1
; c
2
] + b
(1)
) (2)
Where we multiply the parameter matrix W
(1)
?
R
n?2n
by the concatenation of two children
[c
1
; c
2
] ? R
2n?1
. After adding a bias term b
(1)
,
we apply an element-wise activation function such
as f = tanh(?), which is used in our experiments.
In order to apply this auto-encoder to each pair of
children, the representation of the parent p should
have the same dimensionality as the c
i
?s.
To assess how well the parent?s vector repre-
sents its children, the standard auto-encoder recon-
structs the children in a reconstruction layer:
[c
?
1
; c
?
2
] = f
(2)
(W
(2)
p+ b
(2)
) (3)
Where c
?
1
and c
?
2
are reconstructed children, W
(2)
and b
(2)
are parameter matrix and bias term for re-
construction respectively, and f
(2)
= tanh(?).
To obtain the optimal abstract representation of
the inputs, the standard auto-encoder tries to min-
imize the reconstruction errors between the inputs
and the reconstructed ones during training:
E
rec
([c
1
; c
2
]) =
1
2
||[c
1
; c
2
]? [c
?
1
; c
?
2
]||
2
(4)
Given y
1
= p, we can use Eq. 2 again to com-
pute y
2
by setting the children to be [c
1
; c
2
] =
[y
1
;x
3
]. The same auto-encoder is re-used until
the vector of the whole phrase is generated.
For unsupervised phrase embedding, the only
objective is to minimize the sum of reconstruction
errors at each node in the optimal binary tree:
RAE
?
(x) = argmin
y?A(x)
?
s?y
E
rec
([c
1
; c
2
]
s
) (5)
Where x is the list of vectors of a phrase, andA(x)
denotes all the possible binary trees that can be
built from inputs x. A greedy algorithm (Socher
et al, 2011) is used to generate the optimal binary
tree y. The parameters ? = (W, b) are optimized
over all the phrases in the training data.
3.2 Semi-supervised Phrase Embedding
The above RAE is completely unsupervised and
can only induce general representations of the
Reco nstr uctio n Erro r  Pred ictio n Erro r  
W (1)  
W (2)  W (label)  
Figure 3: An illustration of a semi-supervised
RAE unit. Red nodes show the label distribution.
multi-word phrases. Several researchers extend
the original RAEs to a semi-supervised setting so
that the induced phrase embedding can predict a
target label, such as polarity in sentiment analysis
(Socher et al, 2011), syntactic category in parsing
(Socher et al, 2013a) and phrase reordering pat-
tern in SMT (Li et al, 2013).
In the semi-supervised RAE for phrase embed-
ding, the objective function over a (phrase, label)
pair (x, t) includes the reconstruction error and the
prediction error, as illustrated in Fig. 3.
E(x, t; ?) = ?E
rec
(x, t; ?)+(1??)E
pred
(x, t; ?)
(6)
Where the hyper-parameter ? is used to balance
the reconstruction and prediction error. For label
prediction, the cross-entropy error is usually used
to calculate E
pred
. By optimizing the above ob-
jective, the phrases in the vector embedding space
will be grouped according to the labels.
3.3 The BRAE Model
We know from the semi-supervised phrase embed-
ding that the learned vector representation can be
well adapted to the given label. Therefore, we can
imagine that learning semantic phrase embedding
is reasonable if we are given gold vector represen-
tations of the phrases.
However, no gold semantic phrase embedding
exists. Fortunately, we know the fact that the
two phrases should share the same semantic rep-
resentation if they express the same meaning. We
can make inference from this fact that if a model
can learn the same embedding for any phrase pair
sharing the same meaning, the learned embedding
must encode the semantics of the phrases and the
corresponding model is our desire.
As translation equivalents share the same se-
mantic meaning, we employ high-quality phrase
translation pairs as training corpus in this
work. Accordingly, we propose the Bilingually-
constrained Recursive Auto-encoders (BRAE),
114
Sour ce Recons truction Err o r  
Sour ce Prediction Err o r  
W s (1)  
W s (2)  W s (label)  
Ta rg et Recons truction Err o r  
W t (1)  
W t (2)  
W t (label)  Ta rg et Prediction Err o r  
Source Language Phrase Target Language Phrase 
Figure 4: An illustration of the bilingual-
constrained recursive auto-encoders. The two
phrases are translations with each other.
whose basic goal is to minimize the semantic dis-
tance between the phrases and their translations.
3.3.1 The Objective Function
Unlike previous methods, the BRAE model jointly
learns two RAEs (Fig. 4 shows the network struc-
ture): one for source language and the other for
target language. For a phrase pair (s, t), two kinds
of errors are involved:
1. reconstruction errorE
rec
(s, t; ?): how well
the learned vector representations p
s
and p
t
repre-
sent the phrase s and t respectively?
E
rec
(s, t; ?) = E
rec
(s; ?) + E
rec
(t; ?) (7)
2. semantic error E
sem
(s, t; ?): what is the
semantic distance between the learned vector rep-
resentations p
s
and p
t
?
Since word embeddings for two languages are
learned separately and locate in different vector
space, we do not enforce the phrase embeddings
in two languages to be in the same semantic vector
space. We suppose there is a transformation be-
tween the two semantic embedding spaces. Thus,
the semantic distance is bidirectional: the distance
between p
t
and the transformation of p
s
, and that
between p
s
and the transformation of p
t
. As a re-
sult, the overall semantic error becomes:
E
sem
(s, t; ?) = E
sem
(s|t, ?) + E
sem
(t|s, ?) (8)
Where E
sem
(s|t, ?) = E
sem
(p
t
, f(W
l
s
p
s
+ b
l
s
))
means the transformation of p
s
is performed as
follows: we first multiply a parameter matrix W
l
s
by p
s
, and after adding a bias term b
l
s
we apply
an element-wise activation function f = tanh(?).
Finally, we calculate their Euclidean distance:
E
sem
(s|t, ?) =
1
2
||p
t
? f(W
l
s
p
s
+ b
l
s
)||
2
(9)
E
sem
(t|s, ?) can be calculated in exactly the same
way. For the phrase pair (s, t), the joint error is:
E(s, t; ?) = ?E
rec
(s, t; ?) + (1??)E
sem
(s, t; ?)
(10)
The hyper-parameter ? weights the reconstruction
and semantic error. The final BRAE objective over
the phrase pairs training set (S, T ) becomes:
J
BRAE
=
1
N
?
(s,t)?(S,T )
E(s, t; ?)+
?
2
||?||
2
(11)
3.3.2 Max-Semantic-Margin Error
Ideally, we want the learned BRAE model can
make sure that the semantic error for the positive
example (a source phrase s and its correct transla-
tion t) is much smaller than that for the negative
example (the source phrase s and a bad translation
t
?
). However, the current model cannot guarantee
this since the above semantic error E
sem
(s|t, ?)
only accounts for positive ones.
We thus enhance the semantic error with both
positive and negative examples, and the corre-
sponding max-semantic-margin error becomes:
E
?
sem
(s|t, ?) = max{0, E
sem
(s|t, ?)
? E
sem
(s|t
?
, ?) + 1}
(12)
It tries to minimize the semantic distance between
translation equivalents and maximize the semantic
distance between non-translation pairs simultane-
ously. Using the above error function, we need
to construct a negative example for each positive
example. Suppose we are given a positive exam-
ple (s, t), the correct translation t can be converted
into a bad translation t
?
by replacing the words
in t with randomly chosen target language words.
Then, a negative example (s, t
?
) is available.
3.3.3 Parameter Inference
Like semi-supervised RAE (Li et al, 2013), the
parameters ? in our BRAE model can also be di-
vided into three sets:
?
L
: word embedding matrix L for two lan-
guages (Section 3.1.1);
?
rec
: recursive auto-encoder parameter matrices
W
(1)
, W
(2)
, and bias terms b
(1)
, b
(2)
for two lan-
guages (Section 3.1.2);
?
sem
: transformation matrix W
l
and bias term
b
l
for two directions in semantic distance compu-
tation (Section 3.3.1).
115
To have a deep understanding of the parameters,
we rewrite Eq. 10:
E(s, t; ?) = ?(E
rec
(s; ?) + E
rec
(t; ?))
+ (1? ?)(E
?
sem
(s|t, ?) + E
?
sem
(t|s, ?))
= (?E
rec
(s; ?
s
) + (1? ?)E
?
sem
(s|t, ?
s
))
+ (?E
rec
(t; ?
t
) + (1? ?)E
?
sem
(t|s, ?
t
))
(13)
We can see that the parameters ? can be divided
into two classes: ?
s
for the source language and ?
t
for the target language. The above equation also
indicates that the source-side parameters ?
s
can be
optimized independently as long as the semantic
representation p
t
of the target phrase t is given to
compute E
sem
(s|t, ?) with Eq. 9. It is similar for
the target-side parameters ?
t
.
Assuming the target phrase representation p
t
is available, the optimization of the source-side
parameters is similar to that of semi-supervised
RAE. We apply the Stochastic Gradient Descent
(SGD) algorithm to optimize each parameter:
?
s
= ?
s
? ?
?J
s
??
s
(14)
In order to run SGD algorithm, we need to solve
two problems: one for parameter initialization and
the other for partial gradient calculation.
In parameter initialization, ?
rec
and ?
sem
for the
source language is randomly set according to a
normal distribution. For the word embedding L
s
,
there are two choices. First, L
s
is initialized ran-
domly like other parameters. Second, the word
embedding matrix L
s
is pre-trained with DNN
(Bengio et al, 2003; Collobert and Weston, 2008;
Mikolov et al, 2013) using large-scale unlabeled
monolingual data. We prefer to the second one
since this kind of word embedding has already
encoded some semantics of the words. In this
work, we employ the toolkit Word2Vec (Mikolov
et al, 2013) to pre-train the word embedding for
the source and target languages. The word em-
beddings will be fine-tuned in our BRAE model to
capture much more semantics.
The partial gradient for one instance is com-
puted as follows:
?J
s
??
s
=
?E(s|t, ?
s
)
??
s
+ ??
s
(15)
Where the source-side error given the target phrase
representation includes reconstruction error and
updated semantic error:
E(s|t, ?
s
) = ?E
rec
(s; ?
s
) + (1??)E
?
sem
(s|t, ?
s
)
(16)
Given the current ?
s
, we first construct the binary
tree (as illustrated in Fig. 2) for any source-side
phrase using the greedy algorithm (Socher et al,
2011). Then, the derivatives for the parameters in
the fixed binary tree will be calculated via back-
propagation through structures (Goller and Kuch-
ler, 1996). Finally, the parameters will be updated
using Eq. 14 and a new ?
s
is obtained.
The target-side parameters ?
t
can be optimized
in the same way as long as the source-side phrase
representation p
s
is available. It seems a para-
dox that updating ?
s
needs p
t
while updating ?
t
needs p
s
. To solve this problem, we propose an
co-training style algorithm which includes three
steps:
1. Pre-training: applying unsupervised phrase
embedding with standard RAE to pre-train the
source- and target-side phrase representations p
s
and p
t
respectively (Section 2.1.2);
2. Fine-tuning: with the BRAE model, us-
ing target-side phrase representation p
t
to update
the source-side parameters ?
s
and obtain the fine-
tuned source-side phrase representation p
?
s
, and
meanwhile using p
s
to update ?
t
and get the fine-
tuned p
?
t
, and then calculate the joint error over the
training corpus;
3. Termination Check: if the joint error
reaches a local minima or the iterations reach
the pre-defined number (25 is used in our exper-
iments), we terminate the training procedure, oth-
erwise we set p
s
= p
?
s
, p
t
= p
?
t
, and go to step
2.
4 Experiments
With the semantic phrase embeddings and the vec-
tor space transformation function, we apply the
BRAE to measure the semantic similarity between
a source phrase and its translation candidates in
the phrase-based SMT. Two tasks are involved in
the experiments: phrase table pruning that dis-
cards entries whose semantic similarity is very low
and decoding with the phrasal semantic similari-
ties as additional new features.
4.1 Hyper-Parameter Settings
The hyper-parameters in the BRAE model include
the dimensionality of the word embedding n in Eq.
1, the balance weight ? in Eq. 10, ?s in Eq. 11,
and the learning rate ? in Eq. 14.
For the dimensionality n, we have tried three
settings n = 50, 100, 200 in our experiments. We
116
empirically set the learning rate ? = 0.01. We
draw ? from 0.05 to 0.5 with step 0.05, and ?s
from {10
?6
, 10
?5
, 10
?4
, 10
?3
, 10
?2
}. The over-
all error of the BRAE model is employed to guide
the search procedure. Finally, we choose ? =
0.15, ?
L
= 10
?2
, ?
rec
= 10
?3
and ?
sem
= 10
?3
.
4.2 SMT Setup
We have implemented a phrase-based translation
system with a maximum entropy based reordering
model using the bracketing transduction grammar
(Wu, 1997; Xiong et al, 2006).
The SMT evaluation is conducted on Chinese-
to-English translation. Accordingly, our BRAE
model is trained on Chinese and English. The
bilingual training data from LDC
2
contains 0.96M
sentence pairs and 1.1M entity pairs with 27.7M
Chinese words and 31.9M English words. A 5-
gram language model is trained on the Xinhua por-
tion of the English Gigaword corpus and the En-
glish part of bilingual training data. The NIST
MT03 is used as the development data. NIST
MT04-06 and MT08 (news data) are used as the
test data. Case-insensitive BLEU is employed
as the evaluation metric. The statistical signif-
icance test is performed by the re-sampling ap-
proach (Koehn, 2004).
In addition, we pre-train the word embedding
with toolkit Word2Vec on large-scale monolingual
data including the aforementioned data for SMT.
The monolingual data contains 1.06B words for
Chinese and 1.12B words for English. To ob-
tain high-quality bilingual phrase pairs to train
our BRAE model, we perform forced decoding
for the bilingual training sentences and collect the
phrase pairs used. After removing the duplicates,
the remaining 1.12M bilingual phrase pairs (length
ranging from 1 to 7) are obtained.
4.3 Phrase Table Pruning
Pruning most of the phrase table without much
impact on translation quality is very important
for translation especially in environments where
memory and time constraints are imposed. Many
algorithms have been proposed to deal with this
problem, such as significance pruning (Johnson et
al., 2007; Tomeh et al, 2009), relevance prun-
ing (Eck et al, 2007) and entropy-based pruning
2
LDC category numbers: LDC2000T50, LDC2002L27,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T06,
LDC2005T10 and LDC2005T34.
(Ling et al, 2012; Zens et al, 2012). These algo-
rithms are based on corpus statistics including co-
occurrence statistics, phrase pair usage and com-
position information. For example, the signifi-
cance pruning, which is proven to be a very ef-
fective algorithm, computes the probability named
p-value, that tests whether a source phrase s and a
target phrase t co-occur more frequently in a bilin-
gual corpus than they happen just by chance. The
higher the p-value, the more likely of the phrase
pair to be spurious.
Our work has the same objective, but instead of
using corpus statistics, we attempt to measure the
quality of the phrase pair from the view of seman-
tic meaning. Given a phrase pair (s, t), the BRAE
model first obtains their semantic phrase represen-
tations (p
s
, p
t
), and then transforms p
s
into target
semantic space p
s
?
, p
t
into source semantic space
p
t
?
. We finally get two similarities Sim(p
s
?
, p
t
)
and Sim(p
t
?
, p
s
). Phrase pairs that have a low
similarity are more likely to be noise and more
prone to be pruned. In experiments, we discard
the phrase pair whose similarity in two directions
are smaller than a threshold
3
.
Table 1 shows the comparison results between
our BRAE-based pruning method and the signif-
icance pruning algorithm. We can see a common
phenomenon in both of the algorithms: for the first
few thresholds, the phrase table becomes smaller
and smaller while the translation quality is not
much decreased, but the performance jumps a lot
at a certain threshold (16 for Significance pruning,
0.8 for BRAE-based one).
Specifically, the Significance algorithm can
safely discard 64% of the phrase table at its thresh-
old 12 with only 0.1 BLEU loss in the overall
test. In contrast, our BRAE-based algorithm can
remove 72% of the phrase table at its threshold
0.7 with only 0.06 BLEU loss in the overall eval-
uation. When the two algorithms using a similar
portion of the phrase table
4
(35% in BRAE and
36% in Significance), the BRAE-based algorithm
outperforms the Significance algorithm on all the
test sets except for MT04. It indicates that our
BRAE model is a good alternative for phrase table
pruning. Furthermore, our model is much more in-
3
To avoid the situation that all the translation candidates
for a source phrase are pruned, we always keep the first 10
best according to the semantic similarity.
4
In the future, we will compare the performance by en-
forcing the two algorithms to use the same portion of phrase
table
117
Method Threshold PhraseTable MT03 MT04 MT05 MT06 MT08 ALL
Baseline 100% 35.81 36.91 34.69 33.83 27.17 34.82
BRAE
0.4 52% 35.94 36.96 35.00 34.71 27.77 35.16
0.5 44% 35.67 36.59 34.86 33.91 27.25 34.89
0.6 35% 35.86 36.71 34.93 34.63 27.34 35.05
0.7 28% 35.55 36.62 34.57 33.97 27.10 34.76
0.8 20% 35.06 36.01 34.13 33.04 26.66 34.04
Significance
8 48% 35.86 36.99 34.74 34.53 27.59 35.13
12 36% 35.59 36.73 34.65 34.17 27.16 34.72
16 25% 35.19 36.24 34.26 33.32 26.55 34.09
20 18% 35.05 36.09 34.02 32.98 26.37 33.97
Table 1: Comparison between BRAE-based pruning and Significance pruning of phrase table. Threshold
means similarity in BRAE and negative-log-p-value in Significance. ?ALL? combines the development
and test sets. Bold numbers denote that the result is better than or comparable to that of baseline. n = 50
is used for embedding dimensionality.
tuitive because it is directly based on the semantic
similarity.
4.4 Decoding with Phrasal Semantic
Similarities
Besides using the semantic similarities to prune
the phrase table, we also employ them as two in-
formative features like the phrase translation prob-
ability to guide translation hypotheses selection
during decoding. Typically, four translation prob-
abilities are adopted in the phrase-based SMT, in-
cluding phrase translation probability and lexical
weights in both directions. The phrase transla-
tion probability is based on co-occurrence statis-
tics and the lexical weights consider the phrase as
bag-of-words. In contrast, our BRAE model fo-
cuses on compositional semantics from words to
phrases. Therefore, the semantic similarities com-
puted using our BRAE model are complementary
to the existing four translation probabilities.
The semantic similarities in two directions
Sim(p
s
?
, p
t
) and Sim(p
t
?
, p
s
) are integrated into
our baseline phrase-based model. In order to in-
vestigate the influence of the dimensionality of the
embedding space, we have tried three different set-
tings n = 50, 100, 200.
As shown in Table 2, no matter what n is, the
BRAE model can significantly improve the trans-
lation quality in the overall test data. The largest
improvement can be up to 1.7 BLEU score (MT06
for n = 50). It is interesting that with dimen-
sionality growing, the translation performance is
not consistently improved. We speculate that us-
ing n = 50 or n = 100 can already distinguish
good translation candidates from bad ones.
4.5 Analysis on Semantic Phrase Embedding
To have a better intuition about the power of the
BRAE model at learning semantic phrase embed-
dings, we show some examples in Table 3. Given
the BRAE model and the phrase training set, we
search from the set the most semantically similar
English phrases for any new input English phrase.
The input phrases contain different number of
words. The table shows that the unsupervised
RAE can at most capture the syntactic property
when the phrases are short. For example, the
unsupervised RAE finds do not want for the in-
put phrase do not agree. When the phrase be-
comes longer, the unsupervised RAE cannot even
capture the syntactic property. In contrast, our
BRAE model learns the semantic meaning for
each phrase no matter whether it is short or rel-
atively long. This indicates that the proposed
BRAE model is effective at learning semantic
phrase embeddings.
5 Discussions
5.1 Applications of The BRAE model
As the semantic phrase embedding can fully rep-
resent the phrase, we can go a step further in the
phrase-based SMT and feed the semantic phrase
embeddings to DNN in order to model the whole
translation process (e.g. derivation structure pre-
diction). We will explore this direction in our fu-
ture work. Besides SMT, the semantic phrase em-
beddings can be used in other cross-lingual tasks,
such as cross-lingual question answering, since
the semantic similarity between phrases in differ-
ent languages can be calculated accurately.
In addition to the cross-lingual applications, we
believe the BRAE model can be applied in many
118
Method n MT03 MT04 MT05 MT06 MT08 ALL
Baseline 35.81 36.91 34.69 33.83 27.17 34.82
BRAE
50 36.43 37.64 35.35 35.53 28.59 35.84
+
100 36.45 37.44 35.58 35.42 28.57 36.03
+
200 36.34 37.35 35.78 34.87 27.84 35.62
+
Table 2: Experimental results of decoding with phrasal semantic similarities. n is the embedding dimen-
sionality. ?+? means that the model significantly outperforms the baseline with p < 0.01.
New Phrase Unsupervised RAE BRAE
military force
core force military power
main force military strength
labor force armed forces
at a meeting
to a meeting at the meeting
at a rate during the meeting
a meeting , at the conference
do not agree
one can accept do not favor
i can understand will not compromise
do not want not to approve
each people in this nation
each country regards every citizen in this country
each country has its all the people in the country
each other , and people all over the country
Table 3: Semantically similar phrases in the training set for the new phrases.
monolingual NLP tasks which depend on good
phrase representations or semantic similarity be-
tween phrases, such as named entity recognition,
parsing, textual entailment, question answering
and paraphrase detection.
5.2 Model Extensions
In fact, the phrases having the same meaning are
translation equivalents in different languages, but
are paraphrases in one language. Therefore, our
model can be easily adapted to learn semantic
phrase embeddings using paraphrases.
Our BRAE model still has some limitations.
For example, as each node in the recursive auto-
encoder shares the same weight matrix, the BRAE
model would become weak at learning the seman-
tic representations for long sentences with tens of
words. Improving the model to semantically em-
bed sentences is left for our future work.
6 Conclusions and Future Work
This paper has explored the bilingually-
constrained recursive auto-encoders in learning
phrase embeddings, which can distinguish phrases
with different semantic meanings. With the ob-
jective to minimize the semantic distance between
translation equivalents and maximize the semantic
distance between non-translation pairs simultane-
ously, the learned model can semantically embed
any phrase in two languages and can transform
the semantic space in one language to the other.
Two end-to-end SMT tasks are involved to test
the power of the proposed model at learning the
semantic phrase embeddings. The experimental
results show that the BRAE model is remarkably
effective in phrase table pruning and decoding
with phrasal semantic similarities.
We have also discussed many other potential ap-
plications and extensions of our BRAE model. In
the future work, we will explore four directions.
1) we will try to model the decoding process with
DNN based on our semantic embeddings of the
basic translation units. 2) we are going to learn
semantic phrase embeddings with the paraphrase
corpus. 3) we will apply the BRAE model in other
monolingual and cross-lingual tasks. 4) we plan to
learn semantic sentence embeddings by automati-
cally learning different weight matrices for differ-
ent nodes in the BRAE model.
Acknowledgments
We thank Nan Yang for sharing the baseline
code and anonymous reviewers for their valu-
able comments. The research work has been
partially funded by the Natural Science Founda-
tion of China under Grant No. 61333018 and
61303181, and Hi-Tech Research and Develop-
ment Program (863 Program) of China under
Grant No. 2012AA011102.
119
References
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In In-
novations in Machine Learning, pages 137?186.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
George E Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
IEEE Transactions on Audio, Speech, and Language
Processing, 20(1):30?42.
Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Ha-
jime Tsukada. 2013. Adaptation data selection us-
ing neural language models: Experiments in ma-
chine translation. In 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages 678?
683.
Matthias Eck, Stephen Vogal, and Alex Waibel. 2007.
Estimating phrase pair relevance for translation
model pruning. In MTSummit XI.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning semantic representations
for the phrase translation model. arXiv preprint
arXiv:1312.0482.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In IEEE Inter-
national Conference on Neural Networks, volume 1,
pages 347?352.
John Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation quality
by discarding most of the phrasetable. In Proceed-
ings of EMNLP.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700?1709.
Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,
Karol Gregor, Micha?el Mathieu, and Yann L Cun.
2010. Learning convolutional feature hierarchies for
visual recognition. In Advances in neural informa-
tion processing systems, pages 1090?1098.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Infor-
mation Processing Systems 25, pages 1106?1114.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recur-
sive autoencoders for itg-based translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Wang Ling, Joao Grac?a, Isabel Trancoso, and Alan
Black. 2012. Entropy-based pruning for phrase-
based machine translation. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 962?971.
Lemao Liu, Taro Watanabe, Eiichiro Sumita, and
Tiejun Zhao. 2013. Additive neural networks for
statistical machine translation. In 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 791?801.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of NIPS.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with composi-
tional vector grammars. In Proceedings of ACL.
120
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for
statistical machine translation. In Proceedings of
Summit XII, pages 144?151.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1387?1392.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of ACL-
COLING, pages 505?512.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-
hai Yu. 2013. Word alignment modeling with con-
text dependent deep neural network. In 51st Annual
Meeting of the Association for Computational Lin-
guistics.
Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
systematic comparison of phrase table pruning tech-
niques. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 972?983.
Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1393?1398.
121
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 779?784,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
RNN-based Derivation Structure Prediction for SMT
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{ffzhai, jjzhang, yzhou, cqzong}@nlpr.ia.ac.cn
Abstract
In this paper, we propose a novel deriva-
tion structure prediction (DSP) model
for SMT using recursive neural network
(RNN). Within the model, two steps are
involved: (1) phrase-pair vector represen-
tation, to learn vector representations for
phrase pairs; (2) derivation structure pre-
diction, to generate a bilingual RNN that
aims to distinguish good derivation struc-
tures from bad ones. Final experimental
results show that our DSP model can sig-
nificantly improve the translation quality.
1 Introduction
Derivation structure is important for SMT decod-
ing, especially for the translation model based
on nested structures of languages, such as BTG
(bracket transduction grammar) model (Wu, 1997;
Xiong et al, 2006), hierarchical phrase-based
model (Chiang, 2007), and syntax-based model
(Galley et al, 2006; Marcu et al, 2006; Liu et
al., 2006; Huang et al, 2006; Zhang et al, 2008;
Zhang et al, 2011; Zhai et al, 2013). In general,
derivation structure refers to the tuple that records
the used translation rules and their compositions
during decoding, just as Figure 1 shows.
Intuitively, a good derivation structure usually
yields a good translation, while bad derivations al-
ways result in bad translations. For example in
Figure 1, (a) and (b) are two different derivations
for Chinese sentence ??? ? ?9 ?1 
 ?
!?. Comparing the two derivations, (a) is more
reasonable and yields a better translation. How-
ever, (b) wrongly translates phrase ?? ?9? to
?and Sharon? and combines it with [??;Bush]
incorrectly, leading to a bad translation.
To explore the derivation structure?s potential
on yielding good translations, in this paper, we
propose a novel derivation structure prediction
(DSP) model for SMT decoding.
(a) (b)
??
Bush
??? ??
held a talk
? ??
with Sharon
?? ? ??
held a talk
? ??
with Sharon
??
Bush
?? ? ??
held a talk
? ??with Sharon
??
Bush
Bush and
? ??
Sharon
??
??
Bush
??? ??
held a talk
? ??
and Sharon
? ??
and Sharon
?? ? ??
held a talk
Figure 1: Two different derivation structures of
BTG translation model. In the structure, leaf
nodes denote the used translation rules. For each
node, the first line is the source string, while the
second line is its corresponding translation.
The proposed DSP model is built on recur-
sive neural network (RNN). Within the model,
two steps are involved: (1) phrase-pair vector
representation, to learn vector representations for
phrase pairs; (2) derivation structure prediction,
to build a bilingual RNN that aims to distinguish
good derivation structures from bad ones. Ex-
tensive experiments show that the proposed DSP
model significantly improves the translation qual-
ity, and thus verify the effectiveness of derivation
structure on indicating good translations.
We make the following contributions in this
work:
? We propose a novel RNN-based model to do
derivation structure prediction for SMT de-
coding. To our best knowledge, this is the
first work on this issue in SMT community;
? In current work, RNN has only been verified
to be useful on monolingual structure learn-
ing (Socher et al, 2011a; Socher et al, 2013).
We go a step further, and design a bilingual
RNN to represent the derivation structure;
? To train the RNN-based DSP model, we pro-
pose a max-margin objective that prefers gold
derivations yielded by forced decoding to
n-best derivations generated by the conven-
tional BTG translation model.
779
2 The DSP Model
The basic idea of DSP model is to represent the
derivation structure by RNN (Figure 2). Here, we
build the DSP model for BTG translation model,
which is naturally compatible with RNN. We be-
lieve that the DSP model is also beneficial to other
translation models. We leave them as our future
work.
2.1 Phrase-Pair Vector Representation
Phrase pairs, i.e., the used translation rules, are the
leaf nodes of derivation structure. Hence, to repre-
sent the derivation structure by RNN, we need first
to represent the phrase pairs. To do this, we use
two unsupervised recursive autoencoders (RAE)
(Socher et al, 2011b), one for the source phrase
and the other for the target phrase. We call the unit
of the two RAEs the Leaf Node Network (LNN).
Using n-dimension word embedding, RAE can
learn a n-dimension vector for any phrase. Mean-
while, RAE will build a binary tree for the phrase,
as Figure 2 (in box) shows, and compute a re-
construction error to evaluate the vector. We use
E(T
ph
) to denote the reconstruction error given by
RAE, where ph is the phrase and T
ph
is the corre-
sponding binary tree. In RAE, higher error corre-
sponds to worse vector. More details can be found
in (Socher et al, 2011b).
Given a phrase pair (sp, tp), we can use LNN
to generate two n-dimension vectors, representing
sp and tp respectively. Then, we concatenate the
two vectors directly, and get a vector r ? R
2n
to
represent phrase pair (sp, tp) (shown in Figure
2). The vector r is evaluated by combining the
reconstruction error on both sides:
E(T
sp
, T
tp
) =
1
2
[E(T
sp
) + E(T
tp
) ?
N
s
N
t
]
(1)
where T
sp
and T
tp
are the binary trees for sp and
tp. N
s
and N
t
denote the number of nodes in T
sp
and T
tp
. Note that in order to unify the errors on
the two sides, we use ratio N
s
/N
t
to eliminate the
influence of phrase length.
Then, according to Equation (1), we compute
an LNN score to evaluate the vector of all phrase
pairs, i.e., leaf nodes, in derivation d:
LNN(d) = ?
?
(sp,tp)
E(T
sp
, T
tp
)
(2)
where (sp, tp) is the used phrase pair in derivation
d. Obviously, the derivation with better phrase-
pair representations will get a higher LNN score.
??
? ?? with Sharon
?? ? ?? held a talk
Bush
Figure 2: Illustration of DSP model, based on the
derivation structure in Figure 1(a).
The LNN score will serve as part of the DSP
model for predicting good derivation structures.
2.2 Derivation Structure Prediction
Using the vector representations of phrase pairs,
we then build a Derivation Structure Network
(DSN) for prediction (Figure 2).
In DSN, the derivation structure is repre-
sented by repeatedly applying unit neural net-
work (UNN, Figure 3) at each non-leaf node. The
UNN receives two node vectors r
1
? R
2n
and
r
2
? R
2n
as input, and induces a vector p ? R
2n
to represent the parent node.
r1 r2
p
score
Figure 3: The unit neural network used in DSN.
For example, in Figure 2, node [? ?9; with
Sharon] serves as the first child with vector r
1
,
and node [?1
?!; held a talk] as the second
child with vector r
2
. The parent node vector p,
representing [? ?9 ?1 
 ?!; held a talk
with Sharon], is computed by merging r
1
and r
2
:
p = f(W
UNN
[r
1
; r
2
] + b
UNN
) (3)
where [r
1
; r
2
] ? R
4n?1
is the concatenation of r
1
and r
2
, W
UNN
? R
2n?4n
and b
UNN
? R
2n?1
are
the network?s parameter weight matrix and bias
term respectively. We use tanh(?) as function f .
Then, we compute a local score using a simple
inner product with a row vector W
score
UNN
? R
1?2n
:
s(p) = W
score
UNN
? p (4)
The score measures how well the two child nodes
r
1
and r
2
are merged into the parent node p.
As we all know, in BTG derivations, we have
two different ways to merge translation candi-
dates, monotone or inverted, meaning that we
780
merge two candidates in a monotone or inverted
order. We believe that different merging or-
der (monotone or inverted) needs different UNN.
Hence, we keep two different ones in DSN, one for
monotone order (with parameter W
mono
, b
mono
,
and W
score
mono
), and the other for inverted (with pa-
rameter W
inv
, b
inv
, and W
score
inv
). The idea is that
the merging order of the two candidates will de-
termine which UNN will be used to generate their
parent?s vector and compute the score in Equa-
tion (4). Using a set of gold derivations, we can
train the network so that correct order will receive
a high score by Equation (4) and incorrect one will
receive a low score.
Thus, when we merge the candidates of two ad-
jacent spans during BTG-based decoding, the lo-
cal score in Equation (4) is useful in two aspects:
(1) for the same merging order, it evaluates how
well the two candidates are merged; (2) for the dif-
ferent order, it compares the candidates generated
by monotone order and inverted order.
Further, to assess the entire derivation structure,
we apply UNN to each node recursively, until the
root node. The final score utilized for derivation
structure prediction is the sum of all local scores:
DSN(d) =
?
p
s(p) (5)
where d denotes the derivation structure and p is
the non-leaf node in d. Obviously, by this score,
we can easily assess different derivations. Good
derivations will get higher scores while bad ones
will get lower scores.
Li et al (2013) presented a network to predict
how to merge translation candidates, in monotone
or inverted order. Our DSN differs from Li?s work
in two points. For one thing, DSN can not only
predict how to merge candidates, but also evaluate
whether two candidates should be merged. For an-
other, DSN focuses on the entire derivation struc-
ture, rather than only the two candidates for merg-
ing. Therefore, the translation decoder will pursue
good derivation structures via DSN. Actually, Li?s
work can be easily integrated into our work. We
leave it as our future work.
3 Training
In this section, we present the method of training
the DSP model. The parameters involved in this
process include: word embedding, parameters of
the two unsupervised RAEs in LNN, and parame-
ters in DSN.
3.1 Max-Margin Framework
In DSP model, our goal is to assign higher scores
to gold derivations, and lower scores to bad ones.
To reach this goal, we adopt a max-margin frame-
work (Socher et al, 2010; Socher et al, 2011a;
Socher et al, 2013) for training.
Specifically, suppose we have a training data
like (u
i
,G(u
i
),A(u
i
)), where u
i
is the input
source sentence, G(u
i
) is the gold derivation set
containing all gold derivations of u
i
1
, and A(u
i
)
is the possible derivation set that contains all
possible derivations of u
i
. We want to minimize
the following regularized risk function:
J(?) =
1
N
N
?
i=1
R
i
(?) +
?
2
? ? ?
2
, where
R
i
(?) = max
?
d?A(u
i
)
(
s
(
?, u
i
,
?
d
)
+ ?
(
?
d,G(u
i
)
)
)
? max
d?G(u
i
)
(
s
(
?, u
i
, d
)
)
(6)
Here, ? is the model parameter. s(?, u
i
, d) is the
DSP score for sentence u
i
?s derivation d. It is
computed by summing LNN score (Equation (2))
and DSN score (Equation (5)):
s(?, u, d) = LNN
?
(d) +DSN
?
(d) (7)
?(
?
d,G(u
i
)) is the structure loss margin, which
penalizes derivation
?
d more if it deviates more
from gold derivations. It is formulated as:
?
(
?
d,G(u
i
)
)
=
?
pi?
?
d
?
s
?{pi 6? G(u
i
)}+ ?
t
Dist(y(
?
d), ref)
(8)
The margin includes two parts. For the first part,
pi is the source span in derivation
?
d, ? {?} is an
indicator function. We use the first part to count
the number of source spans in derivation
?
d, but
not in gold derivations. The second part is for
target side. Dist(y(
?
d), ref) computes the edit-
distance between the translation result y(
?
d) de-
fined by derivation
?
d and the reference translation
ref . Obviously, this margin can effectively esti-
mate the difference between derivation
?
d and gold
derivations, both on source side and target side.
Note that ?
s
and ?
t
are only two hyperparameters
for scaling. They are independent of each other,
and we set ?
s
= 0.1 and ?
t
= 0.1 respectively.
1
We investigate the general case here and suppose that
one sentence could have several different gold derivations.In
the experiment, we only use one gold derivation for simple
implementation.
781
3.2 Learning
As the risk function, Equation (6) is not differ-
entiable. We train the model via the subgradient
method (Ratliff et al, 2007; Socher et al, 2013).
For parameter ?, the subgriadient of J(?) is:
?J
??
=
1
N
?
i
?s(?, u
i
,
?
d
m
)
??
?
?s(?, u
i
, d
m
)
??
+??
where
?
d
m
is the derivation with the highest DSP
score, and d
m
denotes the gold derivation with the
highest DSP score. We adopt the diagonal vari-
ant of AdaGrad (Duchi et al, 2011; Socher et al,
2013) to minimize the risk function for training.
3.3 Training Instances Collection
In order to train the model, we need to collect the
gold derivation set G(u
i
) and possible derivation
set A(u
i
) for input sentence u
i
.
For G(u
i
) , we define it by force decoding
derivation (FDD). Basically, FDD refers to the
derivation that produces the exact reference trans-
lation (single reference in our training data). For
example, since ?Bush held a talk with Sharon? is
the reference of test sentence ??? ? ?9 ?
1
?!?, then Figure 1(a) is one of the FDDs.
As FDD can produce reference translation, we be-
lieve that FDD is of high quality, and take them as
gold derivations for training.
For A(u
i
), it should contain all possible deriva-
tions of u
i
. However, it is too difficult to obtain
all derivations. Thus, we use n-best derivations of
SMT decoding to simulate the complete derivation
space, and take them as the derivations in A(u
i
).
4 Integrating the DSP Model into SMT
To integrate the DSP model into decoding, we take
it (named DSP feature) as one of the features in the
log-linear framework of SMT. During decoding,
the DSP feature is distributed to each node in the
derivation structure. For the leaf node, the score
in Equation (2), i.e., LNN score, serves as the fea-
ture. For the non-leaf node, Equation (4) plays
the role. In order to give positive feature value to
the log-linear framework (for logarithm), we nor-
malize the DSP scores to [0,1] during decoding.
Due to the length limit, we ignore the specific nor-
malization methods here. We just preform some
simple transformations (such as adding a constant,
computing reciprocal), and convert the scores pro-
portionally to [0,1] at last.
5 Experiments
5.1 Experimental Setup
To verify the effectiveness of our DSP model, we
perform experiments on Chinese-to-English trans-
lation. The training data contains about 2.1M sen-
tence pairs with about 27.7M Chinese words and
31.9M English words
2
. We train a 5-gram lan-
guage model by the Xinhua portion of Gigaword
corpus and the English part of the training data.
We obtain word alignment by GIZA++, and adopt
the grow-diag-final-and strategy to generate the
symmetric alignment. We use NIST MT 2003 data
as the development set, and NIST MT04-08
3
as
the test set. We use MERT (Och, 2004) to tune pa-
rameters. The translation quality is evaluated by
case-insensitive BLEU-4 (Papineni et al, 2002).
The statistical significance test is performed by
the re-sampling approach (Koehn, 2004). The
baseline system is our in-house BTG system (Wu,
1997; Xiong et al, 2006; Zhang and Zong, 2009).
To train the DSP model, we first use Word2Vec
4
toolkit to pre-train the word embedding on large-
scale monolingual data. The used monolingual
data contains about 1.06B words for Chinese and
1.12B words for English. The dimensionality of
our vectors is 50. The detiled training process is
as follows:
(1) Using the BTG system to perform force de-
coding on FBIS part of the bilingual training data
5
,
and collect the sentences succeeded in force de-
coding (86,902 sentences in total)
6
. We then col-
lect the corresponding force decoding derivations
as gold derivations. Here, we only use the best
force decoding derivation for simple implementa-
tion. In future, we will try to use multiple force
decoding derivations for training.
(2) Collecting the bilingual phrases in the leaf
nodes of gold derivations. We train LNN by these
phrases via L-BFGS algorithm. Finally, we get
351,448 source phrases to train the source side
RAE and 370,948 target phrases to train the tar-
get side RAE.
2
LDC category number : LDC2000T50, LDC2002E18,
LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27,
LDC2005T10 and LDC2005T34.
3
For MT06 and MT08, we only use the part of news data.
4
https://code.google.com/p/word2vec/
5
Here we only use the high quality corpus FBIS to guar-
antee the quality of force decoding derivation.
6
Many sentence pairs fail in forced decoding due to many
reasons, such as reordering limit, noisy alignment, and phrase
length limit (Yu et al, 2013).
782
(3) Decoding the 86902 sentences by the BTG
system to get n-best translations and correspond-
ing derivations. The n-best derivations are used to
simulate the entire derivation space. We retain at
most 200-best derivations for each sentence.
(4) Leveraging force decoding derivations and
n-best derivations to train the DSP model. Note
that all parameters, including word embedding and
parameters in LNN and DSN, are tuned together in
this step. It takes about 15 hours to train the entire
network using a 16-core, 2.9 GHz Xeon machine.
5.2 Experimental Results
We compare baseline BTG system and the DSP-
augmented BTG system in this section. The final
translation results are shown in Table 1.
After integrating the DSP model into BTG sys-
tem, we get significant improvement on all test
sets, about 1.0 BLEU points over BTG system on
average. This comparison strongly demonstrates
that our DSP model is useful and will be a good
complement to current translation models.
Systems
BLEU(%)
MT04 MT05 MT06 MT08 Aver
BTG 36.91 34.69 33.83 27.17 33.15
BTG+DSP 37.41 35.77 35.08 28.42 34.17
Table 1: Final translation results. Bold numbers
denote that the result is significantly better than
baseline BTG system (p < 0.05). Column ?Aver?
gives the average BLEU points of the 4 test sets.
To have a better intuition for the effectiveness
of our DSP model, we give a case study in Figure
4. It depicts two derivations built by BTG system
and BTG+DSP system respectively.
From Figure 4(b), we can see that BTG system
yields a bad translation due to the bad derivation
structure. In the figure, BTG system makes three
mistakes. It attaches candidates [??; achieve-
ments], [? ? ; has reached] and [#\?;
singapore] to the big candidate [?UTransactions of the Association for Computational Linguistics, 1 (2013) 243?254. Action Editor: Philipp Koehn.
Submitted 12/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Unsupervised Tree Induction for Tree-based Translation 
 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing Zong 
National Laboratory of Pattern Recognition, Institute of Automation,  
Chinese Academy of Sciences, Beijing, China 
{ffzhai,jjzhang,yzhou,cqzong}@nlpr.ia.ac.cn 
  
 
 
 
 
Abstract 
In current research, most tree-based translation 
models are built directly from parse trees. In 
this study, we go in another direction and build 
a translation model with an unsupervised tree 
structure derived from a novel non-parametric 
Bayesian model. In the model, we utilize 
synchronous tree substitution grammars (STSG) 
to capture the bilingual mapping between 
language pairs. To train the model efficiently, 
we develop a Gibbs sampler with three novel 
Gibbs operators. The sampler is capable of 
exploring the infinite space of tree structures by 
performing local changes on the tree nodes. 
Experimental results show that the string-to-
tree translation system using our Bayesian tree 
structures significantly outperforms the strong 
baseline string-to-tree system using parse trees. 
1 Introduction 
In recent years, tree-based translation models1 are 
drawing more and more attention in the 
community of statistical machine translation 
(SMT). Due to their remarkable ability to 
incorporate context structure information and long 
distance reordering into the translation process, 
tree-based translation models have shown 
promising progress in improving translation 
quality (Liu et al, 2006, 2009; Quirk et al, 2005; 
Galley et al, 2004, 2006; Marcu et al, 2006; Shen 
et al, 2008; Zhang et al, 2011b). 
However, tree-based translation models always 
suffer from two major challenges: 1) They are 
usually built directly from parse trees, which are 
generated by supervised linguistic parsers. 
                                                          
1 A tree-based translation model is defined as a model 
using tree structures on one side or both sides. 
However, for many language pairs, it is difficult to 
acquire such corresponding linguistic parsers due 
to the lack of Tree-bank resources for training. 2) 
Parse trees are actually only used to model and 
explain the monolingual structure, rather than the 
bilingual mapping between language pairs. This 
indicates that parse trees are usually not the 
optimal choice for training tree-based translation 
models (Wang et al, 2010). 
Based on the above analysis, we can conclude 
that the tree structure that is independent from 
Tree-bank resources and simultaneously considers 
the bilingual mapping inside the bilingual sentence 
pairs would be a good choice for building tree-
based translation models. 
Therefore, complying with the above conditions, 
we propose an unsupervised tree structure for tree-
based translation models in this study. In the 
structures, tree nodes are labeled by combining the 
word classes of their boundary words rather than 
by syntactic labels, such as NP, VP. Furthermore, 
using these node labels, we design a generative 
Bayesian model to infer the final tree structure 
based on synchronous tree substitution grammars 
(STSG) 2 . STSG is derived from the word 
alignments and thus can grasp the bilingual 
mapping effectively. 
Training the Bayesian model is difficult due to 
the exponential space of possible tree structures for 
each training instance. We therefore develop an 
efficient Gibbs sampler with three novel Gibbs 
operators for training. The sampler is capable of 
exploring the infinite space of tree structures by 
performing local changes on the tree nodes. 
                                                          
2 We believe it is possible to design a model to infer the 
node label and tree structure jointly. We plan this as 
future work, and here, we focus only on inferring the 
tree structure in terms of the node labels derived from 
word classes. 
243
The tree structure formed in this way is 
independent from the Tree-bank resources and 
simultaneously exploits the bilingual mapping 
effectively. Experiments show that the proposed 
unsupervised tree (U-tree) is more effective and 
reasonable for tree-based translation than the parse 
tree. 
The main contributions of this study are as 
follows: 
1) Instead of the parse tree, we propose a 
Bayesian model to induce a U-tree for tree-
based translation. The U-tree exploits the 
bilingual mapping effectively and does not 
rely on any Tree-bank resources. 
2) We design a Gibbs sampler with three novel 
Gibbs operators to train the Bayesian model 
efficiently. 
The remainder of the paper is organized as 
follows. Section 2 introduces the related work. 
Section 3 describes the STSG generation process, 
and Section 4 depicts the adopted Bayesian model. 
Section 5 describes the Gibbs sampling algorithm 
and Gibbs operators. In Section 6, we analyze the 
achieved U-trees and evaluate their effectiveness. 
Finally, we conclude the paper in Section 7. 
2 Related Work 
In this study, we move in a new direction to build a 
tree-based translation model with effective 
unsupervised U-tree structures. 
For unsupervised tree structure induction, 
DeNero and Uszkoreit (2011) adopted a parallel 
parsing model to induce unlabeled trees of source 
sentences for syntactic pre-reordering. Our 
previous work (Zhai et al, 2012) designed an EM-
based method to construct unsupervised trees for 
tree-based translation models. This work differs 
from the above work in that we design a novel 
Bayesian model to induce unsupervised U-trees, 
and prior knowledge can be encoded into the 
model more freely and effectively. 
Blunsom et al (2008, 2009, 2010) utilized 
Bayesian methods to learn synchronous context 
free grammars (SCFG) from a parallel corpus. The 
obtained SCFG is further used in a phrase-based 
and hierarchical phrase-based system (Chiang, 
2007). Levenberg et al (2012) employed a 
Bayesian method to learn discontinuous SCFG 
rules. This study differs from their work because 
we concentrate on constructing tree structures for 
tree-based translation models. Our U-trees are 
learned based on STSG, which is more appropriate 
for tree-based translation models than SCFG. 
Burkett and Klein (2008) and Burkett et al 
(2010) focused on joint parsing and alignment. 
They utilized the bilingual Tree-bank to train a 
joint model for both parsing and word alignment. 
Cohn and Blunsom (2009) adopted a Bayesian 
method to infer an STSG by exploring the space of 
alignments based on parse trees. Liu et al (2012) 
re-trained the linguistic parsers bilingually based 
on word alignment. Burkett and Klein (2012) 
utilized a transformation-based method to learn a 
sequence of monolingual tree transformations for 
translation. Compared to their work, we do not rely 
on any Tree-bank resources and focus on 
generating effective unsupervised tree structures 
for tree-based translation models. 
Zollmann and Venugopal (2006) substituted the 
non-terminal X in hierarchical phrase-based model 
by extended syntactic categories. Zollmann and 
Vogel (2011) further labeled the SCFG rules with 
POS tags and unsupervised word classes. Our work 
differs from theirs in that we present a Bayesian 
model to learn effective STSG translation rules and 
U-tree structures for tree-based translation models, 
rather than designing a labeling strategy for 
translation rules. 
3 The STSG Generation Process 
In this work, we induce effective U-trees for the 
string-to-tree translation model, which is based on 
a synchronous tree substitution grammar (STSG) 
between source strings and target tree fragments. 
We take STSG as the generation grammar to match 
the translation model. Typically, such an STSG3 is 
a 5-tuple as follows: 
( , , , , )s t t tG N S P ? ?  
where: 
i s?  and t?  represent the set of source and 
target words, respectively, 
i tN  is the set of target non-terminals, 
i t tS N?  is the start root non-terminal, and 
i P  is the production rule set. 
                                                          
3 Generally, an STSG involves tree fragments on both 
sides. Here we only consider the special case where the 
source side is actually a string. 
244
Apart from the start non-terminal tS , we define 
all the other non-terminals in tN  by word classes. 
Inspired by (Zollmann and Vogel, 2011), we 
divide these non-terminals into three categories: 
one-word, two-word and multi-word non-terminals. 
The one-word non-terminal is a word class, such as 
C, meaning that it dominates a word whose word 
class is C. Two-word non-terminals are used to 
stand for two word strings. They are labeled in the 
form of C1+C2, where C1 and C2 are the word 
classes of the two words separately. Accordingly, 
multi-word non-terminals represent the strings 
containing more than two words. They are labeled 
as C1?Cn, demanding that the word classes of the 
leftmost word and the rightmost word are C1 and 
Cn, respectively. 
We use POS tag to play the role of word class4. 
For example, the head node of the rule in Figure 1 
is a multi-word non-terminal PRP?RB. It requires 
that the POS tags of the leftmost and rightmost 
word must be PRP and RB, respectively. Xiong et 
al. (2006) showed that the boundary word is an 
effective indicator for phrase reordering. Thus, we 
believe that combining the word class of boundary 
words can denote the whole phrase well. 
PRP...RB
we
PRP
VBP:x0 RB:x1
VBP+RB
??   x1   x0
wo-men
 
Figure 1. An example of an STSG production rule. 
Each production rule in P  consists of a source 
string and a target tree fragment. In the target tree 
fragment, each internal node is labeled with a non-
terminal in tN , and each leaf node is labeled with 
either a target word in t?  or a non-terminal in tN . 
The source string in a production rule comprises 
source words and variables. Each variable 
corresponds to a leaf non-terminal in the target tree 
fragment. In the STSG, the production rule is used 
to rewrite the root node into a string and a tree 
fragment. For example, in Figure 1, the rule 
rewrites the head node PRP?RB into the 
corresponding string and fragment. 
An STSG derivation refers to the process of 
generating a specific source string and target tree 
                                                          
4 The demand of a POS tagger impairs the independence 
from manual resources to some extent. In future, we 
plan to design a method to learn effective unsupervised 
labels for the non-terminals. 
structure by production rules. This process begins 
with the start non-terminal tS  and an empty source 
string. We repeatedly choose production rules to 
rewrite the leaf non-terminals and expand the 
string until no leaf non-terminal is left. Finally, we 
acquire a source string and a target tree structure 
defined by the derivation. The probability of a 
derivation is given as follows: 
  
1
( ) ( | )
n
i i
i
p d p r N
 
 ?  (1) 
where the derivation comprises a sequence of rules 
d=(r1,?,rn), and Ni represents the root node of rule 
ri. Hence, for a specific bilingual sentence pair, we 
can generate the best target-side tree structure 
based on the STSG, independent from the Tree-
bank resources. The STSG used in the above 
process is learned by the Bayesian model that is 
detailed in the next section. 
Actually, SCFG can also be used to build the U-
trees. We do not use SCFG because most of the 
tree-based models are based on STSG. In our 
Bayesian model, the U-trees are optimized through 
selecting a set of STSG rules. These STSG rules 
are consistent with the translation rules used in the 
tree-based models. 
Another reason is that STSG has a stronger 
expressive power on tree construction than SCFG. 
In a STSG-based U-tree or a STSG rule, although 
not linguistically informed, the nodes labeled by 
POS tags are also effective on distinguishing 
different ones. However, with SCFG, we have to 
discard all the internal nodes (i.e., flattening the U-
trees or rules) to express the same sequence, 
leading to a poor ability of distinguishing different 
U-trees and production rules. Thus, using STSG, 
we can build more specific U-trees for translation.  
In addition, we find that the Bayesian SCFG 
grammar cannot even significantly outperform the 
heuristic SCFG grammar (Blunsom et al 2009)5. 
This would indicate that the SCFG-based 
derivation tree as by-product is also not such good 
for tree-based translation models. Considering the 
above reasons, we believe that the STSG-based 
learning procedure would result in a better 
translation grammar for tree-based models. 
                                                          
5 In (Blunsom et al, 2009), for Chinese-to-English 
translation, the Bayesian SCFG grammar only 
outperform the heuristic SCFG grammar by 0.1 BLEU 
points on NIST MT 2004 and 0.6 BLEU points on NIST 
MT 2005 in the NEWS domain. 
245
4 Bayesian Model 
In this section, we present a Bayesian model to 
learn STSG defined in section 3. In the model, we 
use ?N to denote the probability distribution 
( | )p r N  in Equation (1). ?N follows a multinomial 
distribution and we impose a Dirichlet prior (DP) 
on it: 
  
0 0
| ~ ( )
| , ~ ( , ( | ) )
N
N N N
r N Multi
P DP P N
T
T D D <  (2) 
where 0 ( | )P N<  (base distribution) is used to assign 
prior probabilities to the STSG production rules. ?N 
controls the model?s tendency to either reuse 
existing rules or create new ones using the base 
distribution 0 ( | )P N< . 
Instead of denoting the multinomial distribution 
explicitly with a specific ?N, we integrate over all 
possible values of ?N to achieve the probabilities of 
rules. This integration results in the following 
conditional probability for rule ri given the 
previously observed rules r-i = r1 ,?, ri-1: 
 
0
0
( | )
( | , , , ) i
i
r N ii
i N i
N N
n P r N
p r r N P
n
DD D




   (3) 
Where n-i ri  denotes the number of ri in ir , and n
-i 
N  
represents the total count of rules rewriting non-
terminal N in ir . Thanks to the exchangeability of 
the model, all permutations of the rules are actually 
equiprobable. This means that we can compute the 
probability of each rule based on the previous and 
subsequent rules (i.e. consider each rule as the last 
one). This characteristic allows us to design an 
efficient Gibbs sampling algorithm to train the 
Bayesian model. 
4.1  Base Distribution 
The base distribution 0 ( | )P r N  is designed to 
assign prior probabilities to the STSG production 
rules. Because each rule r consists of a target tree 
fragment frag and a source string str in the model, 
we follow Cohn and Blunsom (2009) and 
decompose the prior probability 0 ( | )P r N  into two 
factors as follows: 
  0 ( | ) ( | ) ( | )P r N P frag N P str frag ?  (4) 
where ( | )P frag N  is the probability of 
producing the target tree fragment frag. To 
generate frag, Cohn and Blunsom (2009) used a 
geometric prior to decide how many child nodes to 
assign each node. Differently, we require that each 
multi-word non-terminal node must have two child 
nodes. This is because the binary structure has 
been verified to be very effective for tree-based 
translation (Wang et al, 2007; Zhang et al, 2011a).  
The generation process starts at root node N. At 
first, root node N is expanded into two child nodes. 
Then, each newly generated node will be checked 
to expand into two new child nodes with 
probability pexpand. This process repeats until all the 
new non-terminal nodes are checked. Obviously, 
pexpand controls the scale of tree fragments, where a 
large pexpand corresponds to large fragments
6. The 
new terminal nodes (words) are drawn uniformly 
from the target-side vocabulary, and the non-
terminal nodes are created by asking two questions 
as follows: 
1) What type is the node, one-word, two-
word or multi-word non-terminal? 
2) What tag is used to label the node? 
The answer to question 1) is chosen from a 
uniform distribution, i.e., the probability is 1/3 for 
each type of non-terminal. The entire generation 
process is in a top-down manner, i.e., generating a 
parent node first and then its children. 
With respect to question 2), because the father 
node has determined the POS tags of boundary 
words, we only need one POS tag to generate the 
label of the current node. For example, in Figure 1, 
as the father node PRP?RB demands that the POS 
tag of the rightmost word is RB, the right child of 
PRP?RB must also satisfy this condition. 
Therefore, we choose a POS tag VBP and obtain 
the label VBP+RB. The POS tag is drawn 
uniformly from the POS tag set. If the current node 
is a one-word non-terminal, question 2) is 
unnecessary. Similarly, with respect to the two-
word non-terminal node, questions 1) and 2) are 
both unnecessary for its two child nodes because 
they have already been defined by their father node. 
As an example of the generative process, the 
tree fragment in Figure 1 is created as follows: 
a. Determine that the left child of PRP?RB is 
a one-word non-terminal (labeled with PRP); 
b. Expand PRP and generate the word ?we? for 
PRP; 
                                                          
6 In our experiment, we set pexpand to 1/3 to encourage 
small tree fragments.  
246
c. Determine that the right child of PRP?RB is 
a two-word non-terminal; 
d. Utilize the predetermined RB and a POS tag 
VBP to form the tag of the two-word non-
terminal: VBP+RB; 
e. Expand VBP+RB (to VBP and RB); 
f. Do not expand VBP and RB. 
( | )P str frag  in Equation (4) is the probability of 
generating the source string, which contains 
several source words and variables. Inspired by 
(Blunsom et al, 2009) and (Cohn and Blunsom, 
2009), we define ( | )P str frag  as follows: 
 
var
1
1 1
( | ) ( ;1)
| |
poisson sw
sw
sw
c
c
s i
P str frag P c
c i 
 u u? ?  (5) 
where csw is the number of words in the source 
string. ?s means the source vocabulary set. Further, 
cvar denotes the number of variables, which is 
determined by the tree fragment frag. 
As shown in Equation(5), we first determine 
how many source words to generate using a 
Poisson distribution Ppoisson(csw;1), which imposes a 
stable preference for short source strings. Then, we 
draw each source word from a uniform distribution 
over ?s. Afterwards, we insert the variables into 
the string. The variables are inserted one at a time 
using a uniform distribution over the possible 
positions. This factor discourages more variables.  
For the example rule in Figure 1, the generative 
process of the source string is: 
a. Decide to generate one source word;  
b. Generate the source word ??? (wo-men) ?;  
c. Insert the first variable after the word;  
d. Insert the second variable between the word 
and the first variable. 
Intuitively, a good translation grammar should 
carry both small translation rules with enough 
generality and large rules with enough context 
information. DeNero and Klein (2007) proposed 
this statement, and Cohn and Blunsom (2009) has 
verified it in their experiments with parse trees. 
Our base distribution is also designed based on 
this intuition. Considering the two factors in our 
base distribution, we penalize both large target tree 
fragments with many nodes and long source strings 
with many words and variables. The Bayesian 
model tends to select both small and frequent 
STSG production rules to construct the U-trees. 
With these types of trees, we can extract small 
rules with good generality and simultaneously 
obtain large rules with enough context information 
by composition. We will show the effectiveness of 
our U-trees in the verification experiment. 
5 Model Training by Gibbs Sampling 
In this section, we introduce a collapsed Gibbs 
sampler, which enables us to train the Bayesian 
model efficiently. 
5.1 Initialization State 
At first, we use random binary trees to initialize the 
sampler. To get the initial U-trees, we recursively 
and randomly segment a sentence into two parts 
and simultaneously create a tree node to dominate 
each part. The created tree nodes are labeled by the 
non-terminals described in section 3. 
Using the initial target U-trees, source sentences 
and word alignment, we extract minimal GHKM 
translation rules7 in terms of frontier nodes (Galley 
et al, 2004). Frontier nodes are the tree nodes that 
can map onto contiguous substrings on the source 
side via word alignment. For example, the bold 
italic nodes with shadows in Figure 2 are frontier 
nodes. In addition, it should be noted that the word 
alignment is fixed8, and we only explore the entire 
space of tree structures in our sampler. Differently, 
Cohn and Blunsom (2009) designed a sampler to 
infer an STSG by fixing the tree structure and 
exploring the space of alignment. We believe that 
it is possible to investigate the space of both tree 
structure and alignment simultaneously. This 
subject will be one of our future work topics. 
For each training instance (a pair of source 
sentence and target U-tree structure), the extracted 
GHKM minimal translation rules compose a 
unique STSG derivation9. Moreover, all the rules 
developed from the training data constitute an 
initial STSG for the Gibbs sampler. 
                                                          
7 We attach the unaligned word to the lowest frontier 
node that can cover it in terms of word alignment. 
8 The sampler might reinforce the frequent alignment 
errors (AE), which would harm the translation model 
(TM). Actually, the frequent AEs also greatly impair the 
conventional TM. Besides, our sampler encourages the 
correct alignments and simultaneously discourages the 
infrequent AEs. Thus, compared with the conventional 
TMs, we believe that our final TM would not be worse 
due to AEs. Our final experiments verify this point and 
we will conduct a much detailed analysis in future. 
9 We only use the minimal GHKM rules (Galley et al, 
2004) here to reduce the complexity of the sampler. 
247
jin-tian jian-mianwo-men zai-ci
PRP+VBP
today
NN
we
PRP
meet
VBP
again
RB
?? ?? ?? ??
PRP...RB
NN...RB
 
Figure 2. Illustration of an initial U-tree structure. The 
bold italic nodes with shadows are frontier nodes. 
Under this initial STSG, the sampler modifies 
the initial U-trees (initial sample) to create a series 
of new ones (new samples) by the Gibbs operators. 
Consequently, new STSGs are created based on the 
new U-trees simultaneously and used for the next 
sampling operation. Repeatedly and after a number 
of iterations, we can obtain the final U-trees for 
building translation models. 
5.2 The Gibbs Operators 
In this section, we develop three novel Gibbs 
operators for the sampler. They explore the entire 
space of the U-tree structures by performing local 
changes on the tree nodes. 
For a U-tree of a given sentence, we define s-
node as the non-root node covering at least two 
words. Thus, the set of s-node contains all the tree 
nodes except the root node, the pre-terminal nodes 
and leaf nodes, which we call non-s-node. For 
example, in Figure 2, PRB?RB and PRP+VBP are 
s-nodes, while NN and NN?RB are non-s-nodes. 
Since the POS tag sequence of the sentence is 
fixed, all non-s-nodes would stay unchanged in all 
possible U-trees of the sentence. Based on this fact, 
our Gibbs operators work only on s-nodes. 
Further, we assign 3 descendant candidates (DC) 
for each s-node: its left child, right child and its 
sibling. For example, in Figure 3, the 3 DCs for the 
s-node are node PRP, VBP and RB respectively. 
According to the different DCs it governs, every s-
node might be in one of the two different states: 
1) Left state: as Figure 3(a) shows, the s-node 
governs the left two DCs, PRP and VBP, 
and is labeled PRP+VBP. 
2) Right state: as Figure 3(b) shows, the s-node 
governs the right two DCs, VBP and RB, and 
is labeled VBP+RB. 
For a specific U-tree, the states of s-nodes are fixed. 
Thus, by changing an s-node?s state, we can easily 
transform this U-tree to another one, i.e., from the 
current sample to a new one. 
To formulate the U-tree transformation process, 
we associate a binary variable ??{0,1} with each 
s-node, indicating whether the s-node is in the left 
?  or right state ?  Then we can change 
the U-tree by changing value of the ? parameters. 
Our first Gibbs operator, Rotate, just works by 
sampling value of the ?parameters, one at a time, 
and changing the U-tree accordingly. For example, 
in Figure 3(a), the s-node is currently in the left 
VWDWH? :HVDPSOHWKH?RIWKLVQRGHDQGLI
WKHVDPSOHGYDOXHRI?LVZHNHHSWKHVWUXFWXUH
unchanged, i.e., in the left state. Otherwise, we 
change its state to the right state ? , and 
transform the U-tree to Figure 3(b) accordingly. 
jian-mianwo-men zai-ci
s-node
we
PRP
meet
VBP
again
RB
?? ?? ??
PRP...RB
PRP+VBP
jian-mianwo-men zai-ci
s-node
we
PRP
meet
VBP
again
RB
?? ?? ??
PRP...RB
VBP+RB
(b) ?=1(a) ?=0
Rotate
 
Figure 3. Illustration of the Rotate operator. In the 
figure, (a) and (b) denote the s-node?s left state and right 
state respectively. The bold italic nodes with shadows in 
the figure are frontier nodes. 
Obviously, towards an s-node for sampling, the 
two values of ? would define two different U-trees. 
Using the GHKM algorithm (Galley et al 2004), 
we can get two different STSG derivations from 
the two U-trees based on the fixed word alignment. 
Each derivation carries a set of STSG rules (i.e., 
minimal GHKM translation rules) of its own. In 
the two derivations, the STSG rules defined by the 
two states include the one rooted at the s-node?s 
lowest ancestor frontier node, and the one rooted at 
the s-node if it is a frontier node. For instance, in 
Figure 3(a), as the s-node is not a frontier node, the 
left state (? ) defines only one rule: 
 
0 2 1
0 1 2
:
... ( ( : : ) : )
leftr x x x
PRP RB PRP VBP x PRP x VBP x RB
o
  
Differently, in Figure 3(b), the s-node is a 
frontier node and thus the right state (? 1) defines 
two rules: 
248
 
0 0 1 0 1
1 1 0 0 1
: ... ( : : )
: ( : : )
right
right
r x x PRP RB x PRP x VBP RB
r x x VBP RB x VBP x RB


o 
o   
Using these STSG rules, the two derivations are 
evaluated as follows (We use the value of ? to 
denote the corresponding STSG derivation): 
0 1
0 1 0
( 0) ( | )
( 1) ( , | )
( | ) ( | , )
left
right right
right right right
p p r r
p p r r r
p r r p r r r


 
 
  
<  v
<  v
 
 
Where r  refers to the conditional context, i.e., the 
set of all other rules in the training data. All the 
probabilities in the above formulas are computed 
by Equation(3). We then normalize the two scores 
and sample a value of ? based on them. With the 
Bayesian model described in section 4, the sampler 
ZLOOSUHIHUWKH?WKDWSURGXFHVVPDOODQGIUHTXHQW
STSG rules. This tendency results in more frontier 
nodes in the U-tree (i.e., the s-node tends to be in 
the state that is a frontier node), which will factor 
the training instance into more small STSG rules. 
In this way, the overall likelihood of the bilingual 
data is improved by the sampler. 
Theoretically, the Rotate operator is capable of 
arriving at any possible U-tree from the initial U-
tree. This is because we can first convert the initial 
U-tree to a left branch tree by the Rotate operator, 
and then transform it to any other U-tree. However, 
it may take a long time to do so. Thus, to speed up 
the structure transformation process, we employ a 
Two-level-Rotate operator, which takes a pair of s-
nodes in a parent-child relationship as a unit for 
sampling. Similar to the Rotate operator, we also 
assign a binary variable ??{0,1} to each unit and 
update the U-tree by sampling the value of ?. The 
method of sampling ? is similar to the one used for 
?. Figure 4 shows an example of the operator. As 
shown in Figure 4(a), the unit NN?VBP and 
PRP+VBP is in the left state (?=0), and governs 
the left three descendants: NN, PRP, and VBP. By 
the Two-level-Rotate operator, we can convert the 
unit to Figure 4(b), i.e., the ULJKWVWDWH?=1). Just as 
Figure 4(b) shows, the governed descendants of the 
unit are turned to PRP, VBP, and RB. 
It may be confusing when choosing the parent-
child s-node pair for sampling because the parent 
node always faces two choices: combining the left 
child or right child for sampling. To avoid 
confusion, we split the Two-level-Rotate operator 
into two operators: Two-level-left-Rotate operator, 
which works with the parent node and its left child, 
and Two-level-right-Rotate operator, which only 
considers the parent node and its right child 10 . 
Therefore, the operator used in Figure 4 is a Two-
level-right-Rotate operator. 
jin-tian jian-mianwo-men zai-ci
PRP+VBP
Today
NN
we
PRP
meet
VBP
again
RB
?? ?? ?? ??
NN...VBP
NN...RB
jin-tian jian-mianwo-men zai-ci
VBP+RB
Today
NN
we
PRP
meet
VBP
again
RB
?? ?? ?? ??
PRP...RB
NN...RB
(a) ?=0 (b) ?=1
Two-level-right-Rotate
 
Figure 4. Illustration of the Two-level-Rotate operator. 
The bold italic nodes with shadows in the Figure are 
frontier nodes. 
During sampling, for each training instance, the 
sampler first applies the Two-level-left-Rotate 
operator to all candidate pairs of s-nodes (parent s-
node and its left child s-node) in the U-tree. After 
that, the Two-level-right-Rotate operator is applied 
to all the candidate pairs of s-nodes (parent s-node 
and its right child s-node). Then, we use the Rotate 
operator on every s-node in the U-tree. By utilizing 
the operators separately, we can guarantee that our 
sampler satisfies detailed balance. We visit all the 
training instances in a random order (one iteration). 
After a number of iterations, we can obtain the 
final U-tree structures and build the tree-based 
translation model accordingly. 
6 Experiments 
6.1 Experimental Setup 
The experiments are conducted on Chinese-to-
English translation. The training data are the FBIS 
corpus with approximately 7.1 million Chinese 
words and 9.2 million English words. We obtain 
the bidirectional word alignment with GIZA++, 
and then adopt the grow-diag-final-and strategy to 
obtain the final symmetric alignment. We train a 5-
gram language model on the Xinhua portion of the 
English Gigaword corpus and the English part of 
                                                          
10 We can also take more nodes as a unit for sampling, 
but this would make the algorithm much more complex. 
249
the training data. For tuning and testing, we use the 
NIST MT 2003 evaluation data as the development 
set, and use the NIST MT04 and MT05 data as the 
test set. We use MERT (Och, 2004) to tune 
parameters. Since MERT is prone to search errors, 
we run MERT 5 times and select the best tuning 
parameters in the tuning set. The translation quality 
is evaluated by case-insensitive BLEU-4 with the 
shortest length penalty. The statistical significance 
test is performed by the re-sampling approach 
(Koehn, 2004). 
To create the baseline system, we use the open-
source Joshua 4.0 system (Ganitkevitch et al, 2012) 
to build a hierarchical phrase-based (HPB) system, 
and a syntax-augmented MT (SAMT) 11  system 
(Zollmann and Venugopal, 2006) respectively. 
The translation system used for testing the 
effectiveness of our U-trees is our in-house string-
to-tree system (abbreviated as s2t). The system is 
implemented based on (Galley et al, 2006) and 
(Marcu et al 2006). In the system, we extract both 
the minimal GHKM rules (Galley et al, 2004), and 
the rules of SPMT Model 1 (Galley et al, 2006) 
with phrases up to length L=5 on the source side. 
We then obtain the composed rules by composing 
two or three adjacent minimal rules. 
To build the above s2t system, we first use the 
parse tree, which is generated by parsing the 
English side of the bilingual data with the Berkeley 
parser (Petrov et al, 2006). Then, we binarize the 
English parse trees using the head binarization 
approach (Wang et al, 2007) and use the resulting 
binary parse trees to build another s2t system. 
For the U-trees, we run the Gibbs sampler for 
1000 iterations on the whole corpus. The sampler 
uses 1,087s per iteration, on average, using a single 
core, 2.3 GHz Intel Xeon machine. For the 
hyperparameters, we set ? to 0.1 and pexpand = 1/3 
to give a preference to the rules with small 
fragments. We built an s2t translation system with 
the achieved U-trees after the 1000th iteration. We 
only use one sample to extract the translation 
grammar because multiple samples would result in 
a grammar that would be too large. 
                                                          
11 From (Zollmann and Vogel, 2011), we find that the 
performance of SAMT system is similar with the 
method of labeling SCFG rules with POS tags. Thus, to 
be convenient, we only conduct experiments with the 
SAMT system. 
6.2 Analysis of The Gibbs Sampler 
To evaluate the effectiveness of the Gibbs sampler, 
we explore the change of the training data?s 
likelihood with increasing sampling iterations. 
1.239E+08
1.243E+08
1.247E+08
1.251E+08
1.255E+08
1.259E+08
100 200 300 400 500 600 700 800 900 1000
Number of Sampling Iterations 
N
e
g
a
ti
v
e
-L
o
g
 L
ik
e
li
h
o
o
d random 1
random 2
random 3
 
Figure 5. Histograms of the training data?s likelihood vs. 
the number of sampling iterations. In the figure, random 
1 to 3 refers to three independent runs of the sampler 
with different initial U-trees as initialization states. 
Figure 5 depicts the negative-log likelihood of 
the training data after several sampling iterations. 
The results show that the overall likelihood of the 
training data is improved by the sampler. Moreover, 
comparing the three independent runs, we see that 
although the sampler begins with different initial 
U-trees, the training data?s likelihood is always 
similar during sampling. This demonstrates that 
our sampler is not sensitive to the random initial 
U-trees and can always arrive at a good final state 
beginning from different initialization states. Thus, 
we only utilize the U-trees from random 1 for 
further analysis hereafter. 
1.035E+07
1.040E+07
1.045E+07
1.050E+07
1.055E+07
1.060E+07
100 200 300 400 500 600 700 800 900 1000
Number of Sampling Iterations 
T
o
ta
l 
N
u
m
b
e
r 
o
f 
F
ro
n
ti
e
r 
N
o
d
e
s
random 1
random 2
random 3
 
Figure 6. The total number of frontier nodes for the 
three independent runs. 
6.3 Analysis of the U-tree Structure 
Acquiring better U-trees for translation is our final 
purpose. However, are the U-trees achieved by the 
250
Gibbs sampler appropriate for the tree-based 
translation model? 
To answer this question, we first analyze the 
effect of the sampler on the U-trees. Figure 6 
shows the total number of frontier nodes in the 
training data during sampling. The results show 
that the number of frontier nodes increases with 
increased sampling. This tendency indicates that 
our sampler prefers the tree structure with more 
frontier nodes. Consequently, the final U-tree 
structures can always be factored into many small 
minimal translation rules. Just as we have argued 
in section 4.1, this is beneficial for a good 
translation grammar. 
To demonstrate the above analysis, Figure 7 
shows a visual comparison between our U-tree 
(from random 1) and the binary parse tree (found 
by head binarization). Because the traditional parse 
tree is not binarized, we do not consider it for this 
analysis. Figure 7 shows that whether it is the 
target tree fragment or the source string of the rule, 
our U-trees always tend to obtain the smaller 
ones12. This comparison verifies that our Bayesian 
tree induction model is effective in shifting the tree 
structures away from complex minimal rules, 
which tend to negatively affect translation. 
0
200k
400k
600k
800k
1000k
2 3 4 5 6 7 8 9 10 >=11
U-Tree
binary parse tree
Number of Nodes in the Target Tree Fragment
N
um
be
r 
of
 R
ul
es
Number of Words and Variables in the Source String
0
300k
600k
900k
1200k
1 2 3 4 5 6 7
N
um
be
r 
of
 R
ul
es
 
Figure 7. Histograms over minimal translation rule 
statistics comparing our U-trees and binary parse trees. 
                                                          
12 Binary parse trees get more tree fragments with two 
nodes than U-trees. This is because there are many 
unary edges in the binary parse trees, while no unary 
edge exists in our U-trees. 
Specifically, we show an example of a binary 
parse tree and our U-tree in Figure 8. The example 
U-tree is more conducive to extracting effective 
translation rules. For example, to translate the 
Chinese phrase ?? ??, we can extract a rule (R2 
in Figure 9) directly from the U-tree because the 
phrase ?? ?? is governed by a frontier node, i.e., 
node ?VBD+RB?. However, because no node 
governs ?? ?? in the binary parse tree, we can 
only obtain a rule (R1 in Figure 9) with many extra 
nodes and edges, such as node CD in R1. Due to 
these extra things, R1 is too large to show good 
generality. 
was
QP
dollarsUS1500only
VBD NNSNNPCDRB
NP
NP
? ???????
NP-COMP
(a) binary parse tree
(b) U-tree
was dollarsUS1500only
VBD NNSNNPCDRB
? ???????
VBD+RB NNP+NNS
CD...NNS
VBD...NNS
 
Figure 8. Example of different tree structures. The node 
NP-COMP is achieved by head binarization. The bold 
italic nodes with shadows denote frontier nodes. 
was QP
only
VBD
CD:x0RB
NP
NP
NP-COMP:x1
was only
VBD RB
? ?VBD+RB
? x1x0?
R1:
R2:
 
Figure 9. Example rules to translate the Chinese phrase 
??  ? .? R1 is extracted from Figure 8(a), i.e., the 
binary parse tree. R2 is from Figure 8(b), i.e., the U-tree. 
251
Based on the above analysis, we can conclude 
that our proposed U-tree structures are conducive 
to extracting small, minimal translation rules. This 
indicates that the U-trees are more consistent with 
the word alignment and are good at capturing 
bilingual mapping information. Therefore, because 
parse trees are always constrained by cross-lingual 
structure divergence, we believe that the proposed 
U-trees would result in a better translation 
grammar. We demonstrate this conclusion in the 
next sub-section. 
6.4 Final Translation Results 
The final translation results are shown in Table 1. 
In the table, lines 3-6 refer to the string-to-tree 
systems built with different types of tree structures. 
Table 1 shows that all our s2t systems 
outperform the Joshua (HPB) and Joshua (SAMT) 
system significantly. This comparison verifies the 
superiority of our in-house s2t system. Moreover, 
the results shown in Table 1 also demonstrate the 
effectiveness of head binarization, which helps to 
improve the s2t system using parse trees in all 
translation tasks. 
To test the effectiveness of our U-trees, we give 
the s2t translation system using the U-trees (from 
random 1). The results show that the system using 
U-trees achieves the best translation result from all 
of the systems. It surpasses the s2t system using 
parse trees by 1.47 BLEU points on MT04 and 
1.44 BLEU points on MT05. Moreover, even using 
the binary parse trees, the achieved s2t system is 
still lower than our U-tree-based s2t system by 
0.97 BLEU points on the combined test set. From 
the translation results, we can validate our former 
analysis that the U-trees generated by our Bayesian 
tree induction model are more appropriate for 
string-to-tree translation than parse trees. 
System MT04 MT05 All 
Joshua (HPB) 31.73 28.82 30.64 
Joshua (SAMT) 32.48 29.77 31.56 
s2t (parse-tree) 33.73* 30.25* 32.75* 
s2t (binary-parse-tree) 34.09* 30.99*# 32.92* 
s2t (U-tree) 35.20*# 31.69*# 33.89*# 
Table 1. Results (in case-insensitive BLEU-4 scores) of 
s2t systems using different types of trees. The ?*? and 
?#? denote that the results are significantly better than 
the Joshua (SAMT) system and the s2t system using 
parse trees (p<0.01). 
6.5 Large Data 
We also conduct an experiment on a larger 
bilingual training data from the LDC corpus13. The 
training corpus contains 2.1M sentence pairs with 
approximately 27.7M Chinese words and 31.9M 
English words. Similarly, we train a 5-gram 
language model using the Xinhua portion of the 
English Gigaword corpus and the English part of 
the training corpus. With the same settings as 
before, we run the Gibbs sampler for 1000 
iterations and utilize the final U-tree structure to 
build a string-to-tree translation system. 
The final BLEU score results are shown in Table 
2. In the scenario with a large data, the string-to-
tree system using our U-trees still significantly 
outperforms the system using parse trees. 
System MT04 MT05 All 
Joshua (HPB) 34.55 33.11 34.01 
Joshua (SAMT) 34.76 33.72 34.37 
s2t (parse-tree) 36.40* 34.53* 35.70* 
s2t (binary-parse-tree) 37.38*# 35.14*# 36.54*# 
s2t (U-tree) 38.02*# 36.12*# 37.34*# 
Table 2. Results (in case-insensitive BLEU-4 scores) for 
the large training data. The meaning of ?*? and ?#? are 
similar to Table 1. 
7 Conclusion and Future Work 
In this paper, we explored a new direction to build 
a tree-based model based on unsupervised 
Bayesian trees rather than supervised parse trees. 
To achieve this purpose, we have made two major 
efforts in this paper: 
(1) We have proposed a novel generative 
Bayesian model to induce effective U-trees for 
tree-based translation. We utilized STSG in the 
model to grasp bilingual mapping information. We 
further imposed a reasonable hierarchical prior on 
the tree structures, encouraging small and frequent 
minimal rules for translation. 
(2) To train the Bayesian tree induction 
model efficiently, we developed a Gibbs sampler 
with three novel Gibbs operators. The operators are 
designed specifically to explore the infinite space 
of tree structures by performing local changes on 
the tree structure. 
                                                          
13 LDC category number : LDC2000T50, LDC2002E18, 
LDC2003E07, LDC2004T07, LDC2005T06, 
LDC2002L27, LDC2005T10 and LDC2005T34. 
252
Experiments on the string-to-tree translation 
model demonstrated that our U-trees are better 
than the parse trees. The translation results verify 
that the well-designed unsupervised trees are 
actually more appropriate for tree-based translation 
than parse trees. Therefore, we believe that the 
unsupervised tree structure would be a promising 
research direction for tree-based translation. 
In future, we plan to testify our sampler with 
various initial trees, such as the tree structure 
formed by (Zhang et al, 2008). We also plan to 
perform a detailed empirical comparison between 
STST and SCFG under our settings. Moreover, we 
will further conduct experiments to compare our 
methods with other relevant works, such as (Cohn 
and Blunsom, 2009) and (Burkett and Klein, 2012). 
Acknowledgments 
We would like to thank Philipp Koehn and three 
anonymous reviewers for their valuable comments 
and suggestions. The research work has been 
funded by the Hi-Tech Research and Development 
Program (?863? Program) of China under Grant 
No. 2011AA01A207, 2012AA011101, and 
2012AA011102. 
References 
Phil Blunsom, Trevor Cohn, Miles Osborne. 2008. 
Bayesian synchronous grammar induction. In 
Advances in Neural Information Processing Systems, 
volume 21, pages 161-168. 
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles 
Osborne. 2009. A gibbs sampler for phrasal 
synchronous grammar induction. In Proc. of ACL 
2009, pages 782-790. 
Phil Blunsom and Trevor Cohn. 2010. Inducing 
synchronous grammars with slice sampling. In Proc. 
of NAACL 2010, pages 238-241. 
David Burkett and Dan Klein. 2008. Two languages are 
better than one (for syntactic Parsing). In Proc. of 
EMNLP 2008, pages 877-886. 
David Burkett, John Blitzer, and Dan Klein. 2010. Joint 
parsing and alignment with weakly synchronized 
grammars. In Proc. of NAACL 2010, pages 127-135.  
David Burkett and Dan Klein. 2012. Transforming trees 
to improve syntactic convergence. In Proc. of 
EMNLP 2012, pages 863-872. 
David Chiang. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33 (2). pages 
201-228. 
Dekai Wu. 1996. A polynomial-time algorithm for 
statistical machine translation. In Proc. of ACL 1996, 
pages 152-158. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23:377-404. 
Trevor Cohn and Phil Blunsom. 2009. A bayesian 
model of syntax-directed tree to string grammar 
induction. In Proc. of EMNLP 2009, pages 352-361. 
Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 
2010. Inducing tree-substitution grammars. Journal 
of Machine Learning Research, pages 3053-3096. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree 
translation. In Proc. of EMNLP 2006, pages 232-241. 
John DeNero and Dan Klein. 2007. Tailoring word 
alignments to syntactic machine translation. In Proc. 
of ACL 2007, pages 17-24. 
John DeNero and Jakob Uszkoreit. 2011. Inducing 
sentence structure from parallel corpora for 
reordering. In Proc. of EMNLP 2011, pages 193-203. 
Chris Dyer. 2010. Two monolingual parses are better 
than one (synchronous parse). In Proc. of NAACL 
2010, pages 263-266. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003, pages 205-208. 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What?s in a translation rule. In Proc. of 
HLT-NAACL 2004, pages 273?280. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer. 2006. Scalable inference and training of 
context-rich syntactic translation models. In Proc. of 
ACL-COLING 2006, pages 961-968. 
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post and Adam Lopez. 2011. Joshua 3.0: 
syntax-based machine translation with the thrax 
Grammar Extractor. In Proc of WMT11, pages 478-
484. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. A 
syntax-directed translator with extended domain of 
locality. In Proc. of AMTA 2006, pages 65-73. 
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.  
Statistical phrase-based translation, In Proc. of 
HLT/NAACL 2003, pages 48-54.  
253
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proc. of EMNLP 
2004, pages 388?395. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
RichDUG =HQV &KULV '\HU DQG 2QG?HM %RMDU. 2007. 
Moses: open source toolkit for statistical machine 
translation. In Proc. of ACL 2007, pages 177-180. 
Abby Levenberg, Chris Dyer and Phil Blunsom. 2012. 
A bayesian model for learning SCFGs with 
discontiguous Rules. In Proc. of EMNLP 2012, pages 
223-232. 
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri 
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, 
Wren N.G. Thornton, Jonathan Weese and Omar F. 
Zaidan. 2009. Joshua: An open source toolkit for 
parsing-based machine translation. In Proc. of ACL 
2009, pages 135-139. 
Shujie Liu, Chi-Ho Li, Mu Li, Ming Zhou. 2012. Re-
training monolingual parser bilingually for syntactic 
SMT. In Proc. of EMNLP 2012, pages 854-862. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. of ACL-COLING 2006, pages 
609-616. 
Yang Liu, Yajuan Lv and Qun Liu. 2009. Improving 
tree-to-tree translation with packed forests. In Proc. 
of ACL-IJCNLP 2009, pages 558-566. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language phrases. 
In Proc. of EMNLP 2006, pages 44-52. 
Franz Och, 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL 2003, 
pages 160-167. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic 
evaluation of machine translation. In Proc. of ACL 
2002, pages 311-318. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. 2006. Learning accurate, compact, and 
interpretable tree annotation. In Proc. of COLING-
ACL 2006, pages 433-440. 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: syntactically 
informed phrasal SMT. In Proc. of ACL 2005, pages 
271-279. 
Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A 
new string-to-dependency machine translation 
algorithm with a target dependency language model. 
In Proc. of ACL-08, pages 577-585. 
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. 
Binarizing syntax trees to improve syntax-based 
machine translation accuracy. In Proc. of EMNLP 
2007, pages 746-754. 
Wei Wang, Jonathan May, Kevin Knight, and Daniel 
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. 
Computational Linguistics, 36(2):247?277. 
Feifei Zhai, Jiajun Zhang, Yu Zhou and Chengqing 
Zong. 2012. Tree-based translation without using 
parse trees. In Proc. of COLING 2012, pages 3037-
3054. 
Hao Zhang, Liang Huang, Daniel Gildea and Kevin 
Knight. 2006. Synchronous binarization for machine 
translation. In Proc. of HLT-NAACL 2006, pages 
256-263. 
Hao Zhang, Daniel Gildea, and David Chiang. 2008. 
Extracting synchronous grammars rules from word 
level alignments in linear time. In Proc. of COLING 
2008, pages 1081-1088. 
Hao Zhang, Licheng Fang, Peng Xu, Xiaoyun Wu.  
2011a. Binarized forest to string translation. In Proc. 
of ACL 2011, pages 835-845. 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, Chew 
Lim Tan. 2009. Forest-based tree sequence to string 
translation model. In Proc. of ACL-IJCNLP 2009, 
pages 172-180. 
Jiajun Zhang, Feifei Zhai and Chengqing Zong. 2011b. 
Augmenting string-to-tree translation models with 
fuzzy use of source-side syntax. In Proc. of EMNLP 
2011, pages 204-215. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew 
Lim Tan and Sheng Li. 2007. A tree-to-tree 
alignment-based model for statistical Machine 
translation. MT-Summit-07. pages 535-542 
Min Zhang, Hongfei Jiang, Ai ti Aw, Haizhou Li, Chew 
Lim Tan and Sheng Li. 2008. A tree sequence 
alignment-based tree-to-tree translation model. In 
Proc. of ACL 2008, pages 559-567. 
Andreas Zollmann and Ashish Venugopal. 2006. Syntax 
augmented machine translation via chart parsing. In 
Proc. of Workshop on Statistical Machine 
Translation 2006, pages 138-141. 
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine 
translation. In Proc. of ACL 2011, pages 1-11. 
254
