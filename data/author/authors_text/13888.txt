N-gram-based Machine Translation
Jose? B. Marin?o?
Rafael E. Banchs?
Josep M. Crego?
Adria` de Gispert?
Patrik Lambert?
Jose? A. R. Fonollosa?
Marta R. Costa-jussa`?
Universitat Polite`cnica de Catalunya
This article describes in detail an n-gram approach to statistical machine translation. This ap-
proach consists of a log-linear combination of a translation model based on n-grams of bilingual
units, which are referred to as tuples, along with four specific feature functions. Translation
performance, which happens to be in the state of the art, is demonstrated with Spanish-to-English
and English-to-Spanish translations of the European Parliament Plenary Sessions (EPPS).
1. Introduction
The beginnings of statistical machine translation (SMT) can be traced back to the early
fifties, closely related to the ideas from which information theory arose (Shannon and
Weaver 1949) and inspired by works on cryptography (Shannon 1949, 1951) during
World War II. According to this view, machine translation was conceived as the problem
of finding a sentence by decoding a given ?encrypted? version of it (Weaver 1955).
Although the idea seemed very feasible, enthusiasm faded shortly afterward because of
the computational limitations of the time (Hutchins 1986). Finally, during the nineties,
two factors made it possible for SMT to become an actual and practical technology:
first, significant increment in both the computational power and storage capacity of
computers, and second, the availability of large volumes of bilingual data.
The first SMT systems were developed in the early nineties (Brown et al 1990, 1993).
These systems were based on the so-called noisy channel approach, which models the
probability of a target language sentence T given a source language sentence S as the
product of a translation-model probability p(S|T), which accounts for adequacy of trans-
lation contents, times a target language probability p(T), which accounts for fluency
of target constructions. For these first SMT systems, translation-model probabilities at
the sentence level were approximated from word-based translation models that were
trained by using bilingual corpora (Brown et al 1993). In the case of target language
probabilities, these were generally trained from monolingual data by using n-grams.
Present SMT systems have evolved from the original ones in such a way that
mainly differ from them in two respects: first, word-based translation models have been
? Department of Signal Theory and Communications, Campus Nord, Barcelona 08034, Spain.
Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for
publication: 5 July 2006
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 4
replaced by phrase-based translation models (Zens, Och, and Ney 2002; Koehn, Och,
and Marcu 2003) which are directly estimated from aligned bilingual corpora by consid-
ering relative frequencies, and second, the noisy channel approach has been expanded
to a more general maximum entropy approach in which a log-linear combination of
multiple feature functions is implemented (Och and Ney 2002).
As an extension of the machine translation problem, technological advances in the
fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it
possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron,
and Norvig 1992). According to this, SMT has also been approached from a finite-state
point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini,
and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi
2000). In this SMT approach, translation models are implemented by means of finite-
state transducers for which transition probabilities are learned from bilingual data.
As opposed to phrase-based translation models, which consider probabilities between
target and source units referred to as phrases, finite-state translation models rely on
probabilities among sequences of bilingual units, which are defined by the transitions
of the transducer.
The translation system described in this article implements a translation model that
has been derived from the finite-state perspective?more specifically, from the work of
Casacuberta (2001) and Casacuberta and Vidal (2004). However, whereas in this earlier
work the translation model is implemented by using a finite-state transducer, in the sys-
tem presented here the translation model is implemented by using n-grams. In this way,
the proposed translation system can take full advantage of the smoothing and consist-
ency provided by standard back-off n-gram models. The translation model presented
here actually constitutes a language model of a sort of ?bilanguage? composed of bilin-
gual units, which will be referred to as tuples (de Gispert and Marin?o 2002). An alterna-
tive approach, which relies on bilingual-unit unigram probabilities, was developed by
Tillmann and Xia (2003); in contrast, the approach presented here considers bilingual-
unit n-gram probabilities. In addition to the tuple n-gram translation model, the
translation system presented here implements four specific feature functions that are
log-linearly combined along with the translation model for performing the decoding
(Marin?o et al 2005).
This article is intended to provide a detailed description of the n-gram-based
translation system, as well as to demonstrate the system performance in a wide-
domain, large-vocabulary translation task. The article is structured as follows. First,
Section 2 presents a complete description of the n-gram-based translation model. Then,
Section 3 describes in detail the additional feature functions that, along with the trans-
lation model, compose the n-gram-based SMT system implemented. Section 4 describes
the European Parliament Plenary Session (EPPS) data, as well as the most relevant
details about the translation tasks considered. Section 5 presents and discusses the
translation experiments and their results. Finally, Section 6 presents some conclusions
and intended further work.
2. The Tuple N-gram Model
This section describes in detail the tuple n-gram translation model, which constitutes
the core model implemented by the n-gram-based SMT system. First, the bilingual unit
definition and model computation are presented in Section 2.1. Then, some important
refinements to the basic translation model are provided and discussed in Section 2.2.
Finally, Section 2.3 discusses issues related to n-gram-based decoding.
528
Marin?o et al N-gram-based Machine Translation
2.1 Tuple Extraction and Model Computation
As already mentioned, the translation model implemented by the described SMT sys-
tem is based on bilingual n-grams. This model actually constitutes a language model of
a particular bilanguage composed of bilingual units that are referred to as tuples. In this
way, the translation model probabilities at the sentence level are approximated by using
n-grams of tuples, such as described by the following equation:
p(T, S) ?
K
?
k=1
p((t, s)k|(t, s)k?1, (t, s)k?2, . . . , (t, s)k?n+1) (1)
where t refers to target, s to source, and (t, s)k to the kth tuple of a given bilingual
sentence pair. It is important to note that since both languages are linked up in tuples,
the context information provided by this translation model is bilingual.
Tuples are extracted from a word-to-word aligned corpus in such a way that a
unique segmentation of the bilingual corpus is achieved. Although in principle any
Viterbi alignment should allow for tuple extraction, the resulting tuple vocabulary
depends highly on the particular alignment set considered, and this impacts the trans-
lation results. According to our experience, the best performance is achieved when
the union of the source-to-target and target-to-source alignment sets (IBM models;
Brown et al [1993]) is used for tuple extraction (some experimental results regarding
this issue are presented in Section 4.2.2). Additionally, the use of the union can also
be justified from a theoretical point of view by considering that the union set typically
exhibits higher recall values than do other alignment sets such as the intersection and
source-to-target.
In this way, as opposed to other implementations, where one-to-one (Bangalore
and Riccardi 2000) or one-to-many (Casacuberta and Vidal 2004) alignments are used,
tuples are extracted from many-to-many alignments. This implementation produces
a monotonic segmentation of bilingual sentence pairs, which allows for simulta-
neously capturing contextual and reordering information into the bilingual translation
unit structures. This segmentation also allows for estimating the n-gram probabil-
ities appearing in (1). In order to guarantee a unique segmentation of the corpus,
tuple extraction is performed according to the following constraints (Crego, Marin?o,
and de Gispert 2004):
 a monotonic segmentation of each bilingual sentence pair is produced,
 no word inside the tuple is aligned to words outside the tuple, and
 no smaller tuples can be extracted without violating the previous
constraints.
Notice that, according to this, tuples can be formally defined as the set of shortest
phrases that provides a monotonic segmentation of the bilingual corpus. Figure 1
presents a simple example illustrating the unique tuple segmentation for a given pair of
sentences, as well as the complete phrase set.
The first important observation from Figure 1 is related to the possible occurrence
of tuples containing unaligned elements on the target side. This is the case for tuple 1.
Tuples of this kind should be handled in an alternative way for the system to be able
to provide appropriate translations for such unaligned elements. The problem of how
529
Computational Linguistics Volume 32, Number 4
Figure 1
Example of tuple extraction. Tuples are extracted from Viterbi alignments in such a way that the
set of shortest bilingual units that provide a monotonous segmentation of the bilingual sentence
pair is achieved.
to handle this kind of situation, which we refer to as involving source-nulled tuples, is
discussed in detail in Section 2.2.2.
Also, as observed from Figure 1, the total number of tuples is significantly lower
than the total number of phrases, and, in most of the cases, longer phrases can be
constructed by considering tuple n-grams, which is the case for phrases 2, 6, 7, 9, 10,
and 11. However, phrases 4 and 5 cannot be generated from tuples. In general, the tuple
representation is not able to provide translations for individual words that appear tied
to other words unless they occur alone in some other tuple. This problem, which we
refer to as embedded words, is discussed in detail in Section 2.2.1.
Another important observation from Figure 1 is that each tuple length is implicitly
defined by the word links in the alignment. As opposed to phrase-extraction proce-
dures, for which a maximum phrase length should be defined to avoid a vocabulary
explosion, tuple extraction procedures do not have any control over tuple lengths.
According to this, the tuple approach will strongly benefit from the structural similarity
between the languages under consideration. Then, for close language pairs, tuples are
expected to successfully handle those short reordering patterns that are included in
the tuple structure, as in the case of ?traducciones perfectas : perfect translations?
presented in Figure 1. On the other hand, in the case of distant pairs of languages, for
which a large number of long tuples are expected to occur, the approach will more easily
fail to provide a good translation model due to tuple sparseness.
2.2 Translation Model Refinements
The basic n-gram translation model, as defined in the previous section, exhibits some
important limitations that can be easily overcome by incorporating specific changes in
530
Marin?o et al N-gram-based Machine Translation
either the tuple vocabulary or the n-gram model. This section describes such limitations
and provides a detailed description of the implemented refinements.
2.2.1 Embedded Words. The first issue regarding the n-gram translation model is related
to the already mentioned problem of embedded words, which refers to the fact that
the tuple representation is not able to provide translations for individual words all the
time. Embedded words can become a serious drawback when they occur in relatively
significant numbers in the tuple vocabulary.
Consider for example the word translations in Figure 1. As seen from the figure, this
word appears embedded into tuple ?traducciones perfectas : perfect translations.? If a
similar situation is encountered for all other occurrences of that word in the training
corpus, then no translation probability for an independent occurrence of that word
will exist. A more relevant example would be the case of the embedded word perfect
since this adjective always moves relative to the noun it is modifying. In this case,
providing the translation system with a word-to-word translation probability for ?per-
fectas : perfect? only guarantees that the decoder will have a translation option for an
isolated occurrence of such words but does not guarantee anything about word order.
So, certainly, any adjective?noun combination including the word perfect, which has not
been seen during the training stage, will be translated in the wrong order. Accordingly,
the problem resulting from embedded words can be partially solved by incorporating a
bilingual dictionary able to provide word-to-word translation when required by the
translation system. A more complete treatment for this problem must consider the
implementation of a word-reordering strategy for the proposed SMT approach (as will
be discussed in Section 6, this constitutes one of the main concerns for our further
research).
In our n-gram-based SMT implementation, the following strategy for handling em-
bedded words is considered. First, one-word tuples for each detected embedded word
are extracted from the training data and their corresponding word-to-word translation
probabilities are computed by using relative frequencies. Then, the tuple n-gram model
is enhanced by including all embedded-word tuples as unigrams into the model. Since
a high-precision alignment set is desirable for extracting such one-word tuples and
estimating their probabilities, the intersection of both alignments, source to target and
target-to-source, is used instead of the union.
In the particular case of the EPPS tasks considered in this work, embedded words
do not constitute a real problem because of the great amount of training material and
the reduced size of the test data set (see Section 4.1 for a detailed description of the
EPPS data set). On the contrary, in other translation tasks with less available training
material, the embedded-word handling strategy described above has been very useful
(de Gispert, Marin?o, and Crego 2004).
2.2.2 Tuples with Empty Source Sides. The second important issue regarding the
n-gram translation model is related to tuples with empty source sides, hereinafter
referred to as source-nulled tuples. In the tuple n-gram model implementation, it fre-
quently happens that some target words linked to NULL end up producing tuples with
NULL source sides. Consider, for example, the first tuple of the example presented in
Figure 1. In this example, ?NULL : we? is a source-nulled tuple if Spanish is considered
to be the source language. Notice that tuples of this kind cannot be allowed since no
NULL is expected to occur in a translation input.
The classical solution to this problem in the finite-state transducer framework is
the inclusion of epsilon arcs (Knight and Al-Onaizan 1998; Bangalore and Riccardi
531
Computational Linguistics Volume 32, Number 4
2000). However, epsilon arcs significantly increase decoding complexity. In our n-gram
system implementation, this problem is easily solved by preprocessing the union set of
alignments before extracting tuples, in such a way that any target word that is linked
to NULL is attached to either its preceding word or its following word. In this way, no
target word remains linked to NULL, and source-nulled tuples will not occur during
tuple extraction.
Some different strategies for handling target words aligned to NULL have been
considered. In the simplest strategy, which will be referred to as the attach-to-right strat-
egy, target words aligned to NULL are always attached to their following word. This
simple strategy happens to provide better results, for English-to-Spanish and Spanish-
to-English translations, than the opposite one (attachment to the previous word), and
also better than a more sophisticated strategy that considers bigram probabilities for
deciding whether a given word should be attached to the following or to the pre-
vious one.
Notice that in the particular cases of Spanish and English, the attach-to-right strat-
egy can be justified heuristically. Indeed, when translating from Spanish to English,
most of the source-nulled tuples result from omitted verbal subjects, which is a very
common situation in Spanish. This is the case for the first tuple in Figure 1. Suppose,
for instance, that the attach-to-right strategy is used in Figure 1; in such a case, the
tuple ?quisie?ramos : would like? will be replaced by the new tuple ?quisie?ramos : we
would like,? which actually makes a better translation unit, at least from a grammatical
point of view. Similarly, some common situations can be identified for translations in
the English-to-Spanish direction, such as omitted determiners (e.g., ?I want information
about European countries : quiero informacio?n sobre los pa??ses Europeos?). Again,
the attach-to-right strategy for the unaligned Spanish determiner los seems to be the
best one.
Experimental results comparing the attach-to-right strategy to an additional strat-
egy based on a statistical translation lexicon are provided in Section 5.1.3.
2.2.3 Tuple Vocabulary Pruning. The third and last issue regarding the n-gram transla-
tion model is related to the computational costs resulting from the tuple vocabulary size
during decoding. The idea behind this refinement is to reduce both computation time
and storage requirements without degrading translation performance. In our n-gram-
based SMT system implementation, the tuple vocabulary is pruned by using histogram
counts. This pruning is performed by keeping the N most frequent tuples with common
source sides.
Notice that such a pruning, because it is performed before computing tuple n-gram
probabilities, has a direct impact on the translation model probabilities and then on
the overall system performance. For this reason, the pruning parameter N is critical
for efficient usage of the translation system. While a low value of N will significantly
decrease translation quality, on the other hand, a large value of N will provide the
same translation quality than a more adequate N, but with a significant increment in
computational costs. The optimal value for this parameter depends on data and should
be adjusted empirically for each considered translation task.
2.3 N-gram-based Decoding
Decoding for the n-gram-based translation model is slightly different from phrase-
based decoding. For this reason, a specific decoding tool had to be implemented. This
532
Marin?o et al N-gram-based Machine Translation
section briefly describes MARIE, the n-gram based search engine developed for our
SMT system (Crego, Marin?o, and de Gispert 2005a).
MARIE implements a beam-search strategy based on dynamic programming. The
decoding is performed monotonically and is guided by the source. During decoding,
partial-translation hypotheses are arranged into different stacks according to the total
number of source words they cover. In this way, a given hypothesis only competes with
those hypotheses that provide the same source-word coverage. At every translation
step, stacks are pruned to keep decoding tractable. MARIE allows for two different
pruning methods:
 Threshold pruning: for which all partial-translation hypotheses scoring
below a predetermined threshold value are eliminated.
 Histogram pruning: for which the maximum number of partial-translation
hypotheses to be considered is limited to the K-best ranked ones.
Additionally, MARIE allows for hypothesis recombination, which provides a more
efficient search. In the implemented algorithm, partial-translation hypotheses are re-
combined if they coincide exactly in both the present tuple and the tuple trigram history.
MARIE also allows for considering additional feature functions during decoding.
All these models are taken into account simultaneously, along with the n-gram trans-
lation model. In our SMT system implementation, four additional feature functions are
considered. These functions are described in detail in Section 3.2.
3. Feature Functions for the N-gram-based SMT System
This section describes in detail some feature functions that are implemented along with
the n-gram translation model for the complete translation system. First, in subsection
3.1, the log-linear combination framework and the implemented optimization proce-
dure are discussed. Then, four specific feature functions that constitute our SMT system
are detailed in Section 3.2.
3.1 Log-linear Combination Framework
As mentioned in the Introduction, in recent translation systems the noisy channel ap-
proach has been replaced by a more general approach, which is founded on the princi-
ples of maximum entropy (Berger, Della Pietra, and Della Pietra 1996). In this approach,
the corresponding translation for a given source language sentence S is defined by the
target language sentence that maximizes a log-linear combination of multiple feature
functions hi(S, T) (Och and Ney 2002), such as described by the following equation:
argmax
T
?
m
?mhm(S, T) (2)
where ?m represents the coefficient of the mth feature function hm(S, T), which ac-
tually corresponds to a log-scaled version of the mth-model probabilities. Optimal
values for the ?m coefficients are estimated via an optimization procedure by using a
development data set.
533
Computational Linguistics Volume 32, Number 4
3.2 Translation System Features
In addition to the tuple n-gram translation model, our n-gram-based SMT system
implements four feature functions: a target-language model, a word-bonus model, and
two lexicon models. These system features are described next.
3.2.1 Target-language Model. This feature provides information about the target lan-
guage structure and fluency. It favors those partial-translation hypotheses that are more
likely to constitute correctly structured target sentences over those that are not. The
model is implemented by using a word n-gram model of the target language, which is
computed according to the following expression:
hTL(T, S) = hTL(T) = log
K
?
k=1
p(wk|wk?1, wk?2, . . . , wk?n+1) (3)
where wk refers to the kth word in the considered partial-translation hypothesis. Notice
that this model only depends on the target side of the data, and can in fact be trained by
including additional information from other available monolingual corpora.
3.2.2 Word-bonus Model. This feature introduces a bonus that depends on the partial-
translation hypothesis length. This is done to compensate for the system preference for
short translations over large ones. The model is implemented through a bonus factor
that directly depends on the total number of words contained in the partial-translation
hypothesis, and it is computed as follows:
hWP(T, S) = hWP(T) = M (4)
where M is the number of words contained in the partial-translation hypothesis.
3.2.3 Source-to-Target Lexicon Model. This feature actually constitutes a complemen-
tary translation model. This model provides, for a given tuple, a translation probability
estimate between its source and target sides. This feature is implemented by using the
IBM-1 lexical parameters (Brown et al 1993; Och et al 2004). Accordingly, the source-
to-target lexicon probability is computed for each tuple according to the following
equation:
hLF(T, S) = log 1(I + 1)J
J
?
j=1
I
?
i=0
q(tnj |sni ) (5)
where sni and t
n
j are the ith and jth words in the source and target sides of tuple (t, s)n,
with I and J the corresponding total number of words in each side. In the equation,
q(.) refers to IBM-1 lexical parameters, which are estimated from alignments computed
in the source-to-target direction.
3.2.4 Target-to-Source Lexicon Model. Similar to the previous feature, this feature
function constitutes a complementary translation model too. It is computed in ex-
534
Marin?o et al N-gram-based Machine Translation
actly the same way the previous model is, with the only difference that IBM-1 lexical
parameters are estimated from alignments computed in the target-to-source direction
instead.
4. EPPS Translation Task
This section describes in detail the most relevant issues about the translation tasks con-
sidered. Section 4.1 describes the EPPS data set that is used, and Section 4.2 presents the
overall implementation details in regard to preprocessing, training, and optimization.
4.1 Corpus Description
The EPPS data set is composed of the official plenary session transcriptions of the Eu-
ropean Parliament, which are currently available in eleven different languages (Koehn
2002). However, in the case of the results presented here, we have used the Spanish and
English versions of the EPPS data that have been prepared by RWTH Aachen University
in the context of the European Project TC-STAR. The training, development, and test
data used include session transcriptions from April 1996 until September 2004, from
October 21 until October 28, 2004, and from November 15 until November 18, 2004,
respectively.
Table 1 presents the basic statistics for the training, development, and test data sets
for each considered language. More specifically, the statistics shown in Table 1 are the
number of sentences, the number of words, the vocabulary size (or number of distinct
words), the average sentence length in number of words, and the number of available
translation references.
As seen from Table 1, although the total number of words in the training set is
very similar for both languages, vocabulary sizes are substantially different. Indeed,
the Spanish vocabulary is approximately 60% larger than the English vocabulary. This
can be explained by the more inflected nature of Spanish, which is particularly evident
in the case of nouns, adjectives, and verbs, which may have many different forms de-
pending on gender, number, tense, and mode. As will be seen from results presented in
Section 5, this difference in vocabulary size has important consequences in translation
quality for the English-to-Spanish direction.
Regarding the development data set, only 1, 008 sentences were considered. Notice
from Table 1 that in this case, the Spanish vocabulary is 20% larger than the English
Table 1
Basic statistics for the training, development, and test data sets (M and k stand for millions and
thousands, respectively; Lmean refers to the average sentence length in number of words, and
Ref. to the number of available translation references).
Set Language Sentences Words Vocabulary Lmean Ref.
Train English 1.22 M 33.4 M 105 k 23.7 1
Spanish 1.22 M 34.8 M 169 k 28.4 1
Dev. English 1008 26.0 k 3.2 k 25.8 3
Spanish 1008 25.7 k 3.9 k 25.5 3
Test English 1094 26.8 k 3.9 k 24.5 2
Spanish 840 22.7 k 4.0 k 27.0 2
535
Computational Linguistics Volume 32, Number 4
vocabulary. Another important issue regarding the development data set is the number
of unseen words, that is, those words present in the development data that are not
present in the training data. In this case, 35 words (0.13%) out of the total number of
words in the English development set did not occur in the training data. From these 35
words, only 30 corresponded to different words. Similarly, 61 words (0.24%) out of the
total number of words in the Spanish development set were not in the training data. In
this case, 57 different words occurred.
Notice also in Table 1 that a different test set was used for each translation direction,
and although a different number of sentences is considered in each case, vocabulary
sizes are almost equivalent. Regarding unseen words, in this case, 112 words (0.42%) out
of the total number of words in the English test set did not occur in the training data.
From these 112 words, only 81 corresponded to different words. Similarly, 46 words
(0.20%) out of the total number of words in the Spanish test were not in the training
data. In this case, 40 different words occurred.
4.2 Preprocessing, Training, and System Optimization
This section presents the overall implementation details in regard to preprocessing,
training, and optimization of the translation system. Two languages, English and Span-
ish, and both translation directions between them are considered for several different
system configurations.
4.2.1 Preprocessing and Alignment. The training data are preprocessed by using stan-
dard tools for tokenizing and filtering. In the filtering stage, some sentence pairs are
removed from the training data to allow for a better performance of the alignment tool.
Sentence pairs are removed according to the following two criteria:
 Fertility filtering: removes sentence pairs with a word ratio larger than a
predefined threshold value.
 Length filtering: removes sentence pairs with at least one sentence of more
than 100 words in length. This helps to maintain bounded alignment
computational times.
After preprocessing, word-to-word alignments are performed in both directions,
source-to-target and target-to-source. In our system implementation, GIZA++ (Och and
Ney 2000) is used for computing the alignments. A total of five iterations for models
IBM-1 and HMM, and three iterations for models IBM-3 and IBM-4, are performed.
Then, the obtained alignment sets are used for computing the intersection and the
union of alignments from which tuples and embedded-word tuples are extracted,
respectively.
4.2.2 Tuple Extraction and Pruning. A tuple set for each translation direction is ex-
tracted from the union set of alignments while avoiding source-nulled tuples by using
the procedure described in Section 2.2.2. Then, the resulting tuple vocabularies are
pruned according to the procedure described in Section 2.2.3. In the case of the EPPS
data under consideration, pruning parameter values of N = 20 and N = 30 are used for
Spanish-to-English and English-to-Spanish, respectively.
In order to better justify such alignment set and pruning parameter selections,
Tables 2 and 3 present model sizes and translation accuracies for the tuple n-gram model
536
Marin?o et al N-gram-based Machine Translation
Table 2
Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and
translation accuracy when tuples are extracted from different alignment sets. Notice that
BLEU measurements in this table correspond to translations computed by using the tuple
n-gram model alone.
Direction Alignment set Tuple voc. Bigrams Trigrams BLEU
ES ? EN Source-to-target 1.920 6.426 2.353 0.4424
union 2.040 6.009 1.798 0.4745
refined 2.111 6.851 2.398 0.4594
EN ? ES Source-to-target 1.813 6.263 2.268 0.4152
union 2.023 6.092 1.747 0.4276
refined 2.081 6.920 2.323 0.4193
when tuples are extracted from different alignment sets and when different pruning
parameters are used, respectively. Translation accuracy is measured in terms of the
BLEU score (Papineni et al 2002), which is computed here for translations generated
by using the tuple n-gram model alone, in the case of Table 2, and by using the tuple
n-gram model along with the additional four feature functions described in Section 3.2,
in the case of Table 3. Both translation directions, Spanish to English (ES ? EN) and
English to Spanish (EN ? ES), are considered in each table.
In the case of Table 2, model size and translation accuracy are evaluated against
the type of alignment set used for extracting tuples. Three different alignment sets are
considered: source-to-target, the union of source-to-target and target-to-source, and the
?refined? alignment method described by Och and Ney (2003). For the results presented
in Table 2, a pruning parameter value of N = 20 was used for the Spanish-to-English
direction, while a value of N = 30 was used for the English-to-Spanish direction.
As can be clearly seen in Table 2, the union alignment set happens to be the most
favorable one for extracting tuples in both translation directions since it provides a
significantly better translation accuracy, in terms of BLEU score, than the other two
alignment sets considered. Notice also in Table 2 that the union set is the one providing
the smallest model sizes according to the number of bigrams and trigrams. This might
explain the improvement observed in translation accuracy, with respect to the other two
cases, in terms of model sparseness.
Table 3
Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and
translation accuracy for different pruning values and both translation directions. Notice that
BLEU measurements in this table correspond to translations computed by using the tuple
n-gram model along with the additional four feature functions described in Section 3.2.
Direction Pruning Tuple voc. Bigrams Trigrams BLEU
ES ? EN N = 30 2.109 6.233 1.805 0.5440
N = 20 2.040 6.009 1.798 0.5434
N = 10 1.921 5.567 1.759 0.5399
EN ? ES N = 30 2.023 6.092 1.747 0.4688
N = 20 1.956 5.840 1.733 0.4671
N = 10 1.843 5.342 1.677 0.4595
537
Computational Linguistics Volume 32, Number 4
In the case of Table 3, model size and translation accuracy are compared for three
different pruning conditions: N = 30, N = 20, and N = 10. For all the cases presented in
the table, tuples were extracted from the union set of alignments.
Notice in Table 3 how translation accuracy is clearly affected by pruning. In the
case of Spanish to English, values of N = 20 and N = 10, while providing tuple vo-
cabulary reductions of 3.27% and 8.91% with respect to N = 30, respectively, produce
a translation BLEU score reductions of 0.11% and 0.75%. On the other hand, in the
case of English to Spanish, values of N = 20 and N = 10 provide tuple vocabulary
reductions of 3.31% and 8.89% and a translation BLEU score reductions of 0.36% and
1.98% with respect to N = 30, respectively. According to these results, a similar tuple
vocabulary reduction seems to affect English-to-Spanish translations more than it af-
fects Spanish-to-English translations. For this reason, we finally adopted N = 20 and
N = 30 as the pruning parameter values for Spanish to English and English to Spanish,
respectively.
Another important observation derived from Table 3 is the higher BLEU score
values with respect to the ones presented in Table 2. This is because, as mentioned
above, the results presented in Table 3 were obtained by considering a full translation
system that implements the tuple n-gram model along with the additional four feature
functions described in Section 3.2. The relative impact of the described feature functions
on translation accuracy is studied in detail in Section 5.1.1.
4.2.3 Translation Model and Feature Function Training. After pruning, a tuple n-gram
model is trained for each translation direction by using the SRI Language Modeling
toolkit (Stolcke 2002). The options for Kneser?Ney smoothing (Kneser and Ney 1995)
and interpolation of higher and lower n-grams are used in these trainings. Then, each
tuple n-gram translation model is finally enhanced by including the unigram probabil-
ities for the embedded-word tuples such as described in Section 2.2.2.
Similarly, a word n-gram target language model is trained for each translation
direction by using the SRI Language Modeling toolkit. Again, as in the case of the
tuple n-gram model, Kneser?Ney smoothing and interpolation of higher and lower
n-grams are used. Extended target language models might also be obtained by adding
additional information from other available monolingual corpora. However, in the
translation tasks described here, target language models are estimated by using only
the information contained in the target side of the training data set.
In our SMT system implementation, trigram models are considered for both the
tuple translation model and the target language model. This selection is based on
perplexity measurements (over the development data set) obtained for n-gram models
computed from the EPPS training data by using different n-gram sizes. Table 4 presents
Table 4
Perplexity measurements for translation and target language models of different n-gram sizes.
Type of model Language Bigram Trigram 4-gram 5-gram
Translation ES ? EN 201.75 161.26 156.88 157.24
Translation EN ? ES 223.94 179.12 174.10 174.49
Language Spanish 81.98 52.49 48.03 47.54
Language English 78.91 50.59 46.22 45.59
538
Marin?o et al N-gram-based Machine Translation
perplexity values obtained for translation and target language models with different
n-gram sizes.
Although our system implements trigram models, the performance of translation
systems using different n-gram sized models is also evaluated. These results are pre-
sented and discussed in Section 5.1.2.
Finally, the source-to-target and target-to-source lexicon models are computed for
each translation direction according to the procedure described in Section 3.2.3. For each
considered lexicon model, either the alignment set in the source-to-target direction or
the alignment set in the target-to-source direction is used, accordingly.
4.2.4 System Optimization. Once the models are computed, a set of optimal log-linear
coefficients is estimated for each translation direction and system configuration via
an optimization procedure, which is described as follows. First, a development data
set that does not overlap either the training set or the test set is required. Then, trans-
lation quality over the development set is maximized by iteratively varying the set of
coefficients. In our SMT system implementation, this optimization procedure is per-
formed by using a tool developed in-house, which is based on a simplex method (Press
et al 2002), and the BLEU score (Papineni et al 2002) is used as a translation quality
measurement.
As will be described in the next section, several different system configurations
are considered in the experiments. For all these optimizations, the development data
described in Table 1 are used. As presented in the table, the development data included
three translation references for both English and Spanish, which are used to compute
the BLEU score at each iteration of the optimization procedures.
The same decoder settings are used for all system optimizations. These settings are
the following:
 decoding is performed monotonically, that is, no reordering capabilities
are used,
 decoding is guided by the source sentence to be translated,
 although available in the decoder, threshold pruning is not used, and
 a value of K = 50 for during-decoding histogram pruning is used.
5. Translation Experiments and Error Analysis
This section presents all translation experiments performed and a brief error analysis
of the obtained results. In order to evaluate the relative contributions of different
system elements to the overall performance of the n-gram-based translation system,
three different experimental settings are considered. The experiments and their re-
sults are described in Section 5.1, and a brief error analysis of results is presented in
Section 5.2. Finally, a comparison between n-gram-based SMT and state-of-the-art
phrase-based translation systems is presented in Section 5.3.
5.1 Translation Experiments and Results
As already mentioned, three experimental settings are considered. For each setting,
the impact on translation quality of a different system parameter is evaluated, namely,
539
Computational Linguistics Volume 32, Number 4
feature function, n-gram size, and the source-nulled tuple strategy. Evaluations in all
three experimental settings are performed with respect to the same standard system
configuration, which is defined in terms of the following parameters:
 Alignment set used for tuple extraction: UNION
 Tuple vocabulary pruning parameter: N = 20 for Spanish to English, and
N = 30 for English to Spanish
 N-gram size used in translation model: 3
 N-gram size used in target language model: 3
 Expanded translation model with embedded-word tuples: YES
 Source-nulled tuple handling strategy: attach-to-right
 Feature functions considered: target language, word-bonus,
source-to-target lexicon, and target-to-source lexicon
In the three experimental settings considered, which are presented in the following
subsections, a total of seven different system configurations are evaluated in both
translation directions, English to Spanish and Spanish to English. Thus, a total of 14
different translation experiments are performed. For each of these cases, the corre-
sponding test set is translated by using the corresponding estimated models and set
of optimal coefficients. The same decoder settings (which were previously described in
Section 4.2.4) that were used during the optimizations are used for all translation
experiments. Translation results are evaluated in terms of mWER and BLEU by using
the two references available for each language test set.
5.1.1 Feature Function Contributions. This experiment is designed to evaluate the
relative contribution of feature functions to the overall system performance. In this
section, four different systems are evaluated. These systems are:
 System A. This constitutes the basic n-gram translation system, which
implements the tuple trigram translation model alone, that is, no
additional feature function is used.
 System B. This is a target-reinforced system. In this system, the translation
model is used along with the target-language and word-bonus models.
 System C. This is a lexicon-reinforced system. In this system, the
translation model is used along with the source-to-target and
target-to-source lexicon models.
 System D. This constitutes the full system, that is, the translation model is
used along with all four additional feature functions. This system
corresponds to the standard system configuration that was defined at the
beginning of Section 5.1.
Table 5 summarizes the results of this evaluation, in terms of BLEU and mWER, for
the four systems considered. As can be seen from the table, both translation directions,
540
Marin?o et al N-gram-based Machine Translation
Table 5
Evaluation results for experiments on feature function contribution.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN A ? ? ? ? 39.71 0.4745
B 0.29 0.31 ? ? 39.51 0.4856
C ? ? 0.77 0.08 35.77 0.5356
D 0.49 0.30 0.94 0.25 34.94 0.5434
EN ? ES A ? ? ? ? 44.46 0.4276
B 0.33 0.27 ? ? 44.67 0.4367
C ? ? 0.29 0.15 41.69 0.4482
D 0.66 0.73 0.32 0.47 40.34 0.4688
Spanish to English and English to Spanish, are considered. Table 5 also presents the
optimized log-linear coefficients associated with the features considered in each system
configuration (the log-linear weight of the translation model has been omitted from the
table because its value is fixed to 1 in all cases).
As can be observed in Table 5, the inclusion of the four feature functions into
the translation system definitively produces a significant improvement in translation
quality in both translation directions. In particular, it becomes evident that the features
with the most impact on translation quality are the lexicon models. The target language
model and the word bonus also contribute to improving translation quality, but to a
lesser degree.
Also, although it is more evident in the English-to-Spanish direction than in the
opposite one, it can be noticed from the presented results that the contribution of
target-language and word-bonus models is more relevant when the lexicon mod-
els are used (full system). In fact, as seen from the ?lm values in Table 5, when
the lexicon models are not included, the target-language model contribution to the
overall translation system becomes much less significant. A comparative analysis of
the resulting translations suggests that including the lexicon models tends to favor
short tuples over long ones, so the target-language model becomes more important
for providing target context information when the lexicon models are used. How-
ever, more experimentation and research are required for fully understanding this
interesting result.
Another important observation, which follows from comparing results between
both translation directions, is that in all cases the Spanish-to-English translations are
consistently and significantly better than the English-to-Spanish translations. This is
clearly due to the more inflected nature of Spanish vocabulary. For example, the single
English word the can generate any of the four Spanish words el, la, los, and las. Similar
situations occur with nouns, adjectives, and verbs that may have many different forms
in Spanish. This would suggest that the English-to-Spanish translation task is more
difficult than the Spanish-to-English task.
5.1.2 Translation and Language N-gram Size. This experiment is designed to evaluate
the impact of translation- and language-model n-gram sizes on overall system perform-
ance. In this section, the full system (System D in the previous experiment) is com-
pared with two similar systems for which 4-grams are used for training the translation
541
Computational Linguistics Volume 32, Number 4
model and/or the target language model. More specifically, the three systems compared
in this experiment are:
 System D, which implements a tuple trigram translation model and a word
trigram target language model. This system corresponds to the standard
system configuration that was defined at the beginning of Section 5.1.
 System E, which implements a tuple trigram translation model and a word
4-gram target language model.
 System F, which implements a tuple 4-gram translation model and a word
4-gram target language model.
Table 6 summarizes the results of this evaluation for Systems E, F, and D. Again, both
translation directions are considered and the optimized coefficients associated with the
four feature functions are also presented for each system configuration.
As can be seen in Table 6, the use of 4-grams for model computation does not
provide a clear improvement in translation quality. This is more evident in the English-
to-Spanish direction for which System F happens to be the worst ranked one, while
System D is the one obtaining the best mWER score and system E is the one obtaining
the best BLEU score. On the other hand, in the Spanish-to-English direction, it seems
that a little improvement with respect to System D is achieved by using 4-grams.
However, it is not clear which system performs the best since System E obtains the
best BLEU score while System F obtains the best mWER score.
According to these results, more experimentation and research are required to fully
understand the interaction between the n-gram sizes of translation and target language
models. Notice that in the particular case of the n-gram SMT system described here,
such an interaction is not evident at all since the n-gram-based translation model itself
contains some of the target language model information.
5.1.3 Source-nulled Tuple Strategy Comparison. This experiment is designed to eval-
uate a different strategy for handling source-nulled tuples. In this section, the standard
system configuration (System D) presented at the beginning of Section 5.1, which imple-
ments the attach-to-right strategy described in Section 2.2.2, is compared with a similar
system (referred to as System G) implementing a more complex strategy for handling
those tuples with NULL source sides. More specifically, the latter system uses the
IBM-1 lexical parameters (Brown et al 1993) for computing the translation probabilities
of two possible new tuples: the one resulting when the null-aligned-word is attached to
Table 6
Evaluation results for experiments on n-gram size incidence.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN D 0.49 0.30 0.94 0.25 34.94 0.5434
E 0.50 0.54 0.66 0.45 34.66 0.5483
F 0.66 0.50 1.01 0.57 34.59 0.5464
EN ? ES D 0.66 0.73 0.32 0.47 40.34 0.4688
E 0.57 0.45 0.51 0.26 40.55 0.4714
F 1.24 1.07 0.99 0.57 40.91 0.4688
542
Marin?o et al N-gram-based Machine Translation
the previous word and the one resulting when it is attached to the following one. Then,
the attachment direction is selected according to the tuple with the highest translation
probability.
Table 7 summarizes the results of evaluation Systems D and G. Again, both trans-
lation directions are considered and the optimized coefficients associated with the four
feature functions are also presented for each system configuration.
As can be seen in Table 7, consistently better results are obtained in both translation
tasks when using IBM-1 lexicon probabilities to handle tuples with a NULL source
side. Even though slight improvements are achieved in both cases, especially with
the English-to-Spanish translation task, the results show how the initial attach-to-right
strategy is easily improved by making use of some bilingual knowledge.
5.2 Error Analysis
In this last section, we present a brief description of an error analysis performed
on some of the outputs provided by the standard system configuration that was de-
scribed in Section 5.1 (system D). More specifically, a detailed review of 100 trans-
lated sentences and their corresponding source sentences, in each direction, was
conducted. This analysis was very useful since it allowed us to identify the most com-
mon errors and problems related to our n-gram based SMT system in each translation
direction.
A detailed analysis of all the reviewed translations reveals that most translation
problems encountered are typically related to four basic different types of errors:
 Verbal forms: A significant number of wrong verbal tenses and auxiliary
forms were detected. This problem turned out to be the most common
one, reflecting the difficulty of the current statistical approach to capture
the linguistic phenomena that shape head verbs, auxiliary verbs, and
pronouns into full verbal forms in each language, especially given the
inflected nature of the Spanish language.
 Omitted translations: A large number of translations involving tuples with
NULL target sides were detected. Although in some cases these situations
corresponded to correct translations, most of the time they resulted in
omitted-word errors.
 Reordering problems: The two specific situations that most commonly
occurred were problems related to adjective?noun and subject?verb
structures.
Table 7
Evaluation results for experiments on strategies for handling source-nulled tuples.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN D 0.49 0.30 0.94 0.25 34.94 0.5434
G 0.49 0.45 0.78 0.39 34.15 0.5451
EN ? ES D 0.66 0.73 0.32 0.47 40.34 0.4688
G 0.96 0.93 0.53 0.44 40.12 0.4694
543
Computational Linguistics Volume 32, Number 4
 Concordance problems: Inconsistencies related to gender and number
were the most commonly found.
Table 8 presents the relative number of occurrences for each of the four types of errors
identified in both translation directions.
Notice in Table 8 that the most common errors in both translation directions are
those related to verbal forms. However, it is important to mention that 29.5% of verbal-
form errors in the English-to-Spanish direction actually correspond to verbal omissions.
Similarly, 12.8% of verbal-form errors in the Spanish-to-English direction are verbal
omissions. According to this, if errors due to omitted translations and to omitted verbal
forms are considered together, it is evident that errors involving omissions constitute
the most important group, especially in the case of English-to-Spanish translations. It
is also interesting to note that the Spanish-to-English direction exhibits more omitted-
translation errors that are not related to verbal forms than the English-to-Spanish
direction.
Also in Table 8, it can be seen that concordance errors affect more than twice as many
English-to-Spanish translations as Spanish-to-English ones. This result can be explained
by the more inflected nature of Spanish.
Finally, as an illustrative example, three Spanish-to-English translation outputs are
presented below. For each presented example, errors have been boldfaced and correct
translations are provided in brackets:
Example 1
The policy of the European Union on Cuba NULL must [must not] change.
Example 2
To achieve these purposes, it is necessary NULL for the governments to be allocated
[to allocate], at least, 60,000 million NULL dollars a year . . .
Example 3
In the UK we have NULL [already] laws enough [enough laws], but we want to encourage
NULL other States . . .
5.3 N-gram-based SMT Compared with Phrase-Based SMT
The n-gram-based translation system here described has been also evaluated and com-
pared to other phrase-based translation systems in the context of the European Project
Table 8
Percentage of occurrence for each type of error in English-to-Spanish and Spanish-to-English
translations that were studied.
Type of error English-to-Spanish Spanish-to-English
Verbal forms 31.3% 29.9%
Omitted translations 22.0% 26.1%
Reordering problems 15.9% 19.7%
Concordance problems 10.8% 4.6%
Other errors 20.0% 19.7%
544
Marin?o et al N-gram-based Machine Translation
TC-STAR. A detailed description of the first evaluation campaign (including the main
characteristics of every system) is available through the consortium?s Web site as a
progress report (Ney et al 2005).
Table 9 presents the four best BLEU results for the EPPS translation task in the
first TC-STAR?s evaluation campaign, where the results corresponding to our n-gram-
based translation system are provided in brackets. A total of six systems were evaluated
in this evaluation campaign. The task consisted of two translation directions: English
to Spanish and Spanish to English, and three different evaluation conditions: final
text edition, verbatim, and ASR output. The final text edition condition corresponds
to the official transcripts of the EPPS, so it is actually a written-language translation
condition. On the other hand, the other two conditions are spoken-language transla-
tion conditions. More specifically, the verbatim condition corresponds to literal tran-
scriptions of parliamentary speeches, which include hesitations, repeated words, and
other spontaneous speech effects; and the ASR output condition corresponds to the
output of an automatic speech recognition system, so it additionally includes speech-
recognition errors.
As can be seen in Table 9, performance of the n-gram-based translation system is
among the three best systems for the translation directions and conditions considered
in the first TC-STAR evaluation campaign.
Another independent comparison of the translation system proposed here with
other phrase-based translation systems is available through the results of the second
shared task of the ACL 2005 workshop on ?Building and using parallel texts: Data-
driven machine translation and beyond.? In this shared task, which was entitled ?Ex-
ploiting Parallel Texts for Statistical Machine Translation,? our n-gram-based translation
system was evaluated in four different translation directions: Spanish to English, French
to English, German to English, and Finish to English (Banchs et al 2005). The domain
of this task was also the European Parliament; however, the data set considered in this
evaluation was different from the one used in TC-STAR?s evaluation campaign. The
final text edition condition (official transcripts) was the only one considered here. A total
of twelve different systems participated in this shared task. Table 10 presents the four
best BLEU results for each of the four translation directions considered in the shared
task. Again, results corresponding to our n-gram-based translation system are provided
in brackets.
As can be seen in Table 10, the performance of the n-gram-based translation system
is among the three best systems for the four translation directions considered in the
ACL 2005 workshop shared task. The third system in Table 10 for ES to EN translation
Table 9
The four best BLEU results for the EPPS translation task in TC-STAR?s first evaluation campaign.
N-gram based system results are provided in brackets. All BLEU values presented here have
been taken from TC-STAR?s SLT Progress Report, available at: http://www.tc-star.org/.
Direction Condition First Second Third Fourth
ES ? EN Final text edition [53.3] 53.1 47.5 46.1
Verbatim 45.9 44.1 [42.1] 38.1
ASR output 41.5 39.7 [37.7] 34.7
EN ? ES Final text edition [46.2] 45.2 38.9 37.6
Verbatim 42.5 [38.1] 36.8 33.4
ASR output 38.7 34.3 [33.8] 33.0
545
Computational Linguistics Volume 32, Number 4
Table 10
The four best BLEU results for the four translation directions considered in the shared task
?Exploiting Parallel Texts for Statistical Machine Translation? (ACL 2005 workshop on
?Building and using parallel texts: Data-driven machine translation and beyond?). N-gram-
based system results are provided in brackets. All BLEU values presented here have been
taken from the shared task?s Web site: http://www.statmt.org/wpt05/mt-shared-task/.
Direction Condition First Second Third Fourth
FR ? EN Final text edition 30.27 [30.20] 29.53 28.89
ES ? EN Final text edition 30.95 [30.07] 29.84 29.08
DE ? EN Final text edition 24.77 [24.26] 23.21 22.91
FI ? EN Final text edition 22.01 20.95 [20.31] 18.87
deserves some comment. This system is a conventional phrase-based system sharing
the same decoder MARIE, IBM features, word bonus, and target-language model as the
n-gram-based system. The specific characteristics of the phrase-based system are direct
and inverse phrase conditional probabilities and phrase penalty. Additional compar-
isons between an n-gram system and a phrase-based system sharing a common decoder
and training and test framework can be found in Crego et al (2005c).
6. Conclusions and Further Work
As can be concluded from the results presented, the tuple n-gram translation model,
when used along with additional feature functions, provides state-of-the-art transla-
tions for the considered translation directions.
Another important result is that the quality of Spanish-to-English translations is
significantly and consistently better than those obtained in English-to-Spanish transla-
tions. Consequently, significant efforts should be dedicated towards properly exploiting
morphological analysis and synthesis methods for improving English-to-Spanish trans-
lation quality.
Additionally, four commonly occurring types of translation errors were identified
by reviewing a significant number of translated sentence pairs. This analysis has pro-
vided us with useful hints for future research and improvement of our SMT system.
However, more evaluation and discussion are required in this area in order to fully
understand these common translation failures and then implementing appropriate
solutions.
All the experiments presented in this work were performed using monotone de-
coding, and no reordering strategies were implemented. Although this system con-
figuration proved to provide state-of-the-art translations for the tasks presented, this
may not hold for tasks involving more distant language pairs for which reordering
capabilities must be implemented. Accordingly, along with other results obtained in
the present work, we consider that further research on n-gram SMT should focus on the
following issues:
 Reordering strategies, as well as non-monotonous decoding schemes, for
the proposed SMT system must be developed and tested. As mentioned
before, reordering problems specifically related to adjective?noun and
subject?verb structures occur very often in Spanish-to-English and
546
Marin?o et al N-gram-based Machine Translation
English-to-Spanish translations. Preliminary results concerning the use of
word class deterministic reordering and POS-tag-based reordering
patterns can be found in Costa-jussa`, Fonollosa, and Monte (2006) and
Crego and Marin?o (2006), respectively.
 An effective long-tuple unfolding strategy must be developed to avoid
the occurrence of long tuples resulting from long alignment links, which
happens to be a common situation when dealing with translations
between distant pairs of languages. This problem is closely related to
reordering, and some preliminary results have been presented by Crego,
Marin?o, and de Gispert (2005b).
 The definition of the tuple as a bilingual pair will be revised in order to
better handle unaligned words in both the source and the target sides. As
mentioned above, a better strategy for dealing with target words aligned
to NULL is required. Similarly, a better handling of NULLs in the target
side will result in fewer omitted-translation errors.
 The extension of the embedded-word concept to the more general idea of
embedded n-grams should be evaluated and implemented. Accordingly, a
translation probability should be estimated for those groups of words
that always occur embedded in tuples. This would guarantee that the
decoder will always have a translation option for any given word or word
combination previously seen in the training data. Further work is required
to determine the relative impact of these embedded n-grams on the
translation model, and the most appropriate strategy for handling them.
 Linguistic information must be used to cope with the observed
morphological problems in the English-to-Spanish translation direction,
as well as the more general problem of incorrect verbal form translations.
In this regard, ongoing research on linguistic tuples classification is
being done in order to improve translation results. Preliminary results
on detecting and classifying verb forms have been presented by
de Gispert (2005).
 A more detailed error analysis than the one presented in Section 5.2 is
required to fully understand the n-gram SMT system behavior and the
specific causes of each resulting type of error. It would be very useful for
improving our translation system performance to clearly identify whether
these errors are due to unseen information while training, to modeling
problems, or to decoding errors.
Acknowledgments
This work has been partly funded by the
European Union under the integrated project
TC-STAR (Technology and Corpora for
Speech to Speech Translation) (IST-2002-
FP6-506738, http://www.tc-star.org), the
Spanish Department of Education and
Science (MEC), the Department of
Universities, Research and Information
Society (Generalitat de Catalunya), and
the Universitat Polite`cnica de Catalunya.
References
Banchs, Rachel E., Josep Maria Crego,
Adria` de Gispert, Patrik Lambert, and
Jose? Bernardo Marin?o. 2005. Statistical
machine translation of Euparl data by
using bilingual n-grams. In ACL Workshop
on Data-Driven Machine Translation and
Beyond, pages 133?136, Ann Arbor, MI.
Bangalore, Srinivas and Giuseppe Riccardi.
2000. Stochastic finite-state models for
spoken language machine translation.
547
Computational Linguistics Volume 32, Number 4
In Proceedings of the Workshop on Embedded
Machine Translation Systems, pages 52?59,
Seattle, WA.
Berger, Adam, Stephen Della Pietra, and
Vincent Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39?71.
Brown, Peter, John Cocke, Stephen Della
Pietra, Vincent Della Pietra, Frederick
Jelinek, John Lafferty, Robert Mercer, and
Paul S. Roossin. 1990. A statistical
approach to machine translation.
Computational Linguistics, 16(2):79?85.
Brown, Peter, Stephen Della Pietra, Vincent
Della Pietra, and Robert Mercer. 1993.
The mathematics of statistical machine
translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Casacuberta, Francisco. 2001. Finite-state
transducers for speech input translation. In
Proceedings IEEE ASRU, pages 375?380,
Madonna di Campiglio, Italy.
Casacuberta, Francisco and Enrique Vidal.
2004. Machine translation with inferred
stochastic finite-state transducers.
Computational Linguistics, 30(2):205?225.
Costa-jussa`, Marta Ruiz, Jose? Adria?n
Rodriguez Fonollosa, and Enric Monte.
2006. Using reordering in statistical
machine translation based on alignment
block classification. Internal Report.
http://gps-tsc.upc.es/veu/personal/
mruiz/docs/br06.pdf.
Crego, Josep Maria, Jose? Bernardo
Marin?o, and Adria` de Gispert. 2004.
Finite-state-based and phrase-based
statistical machine translation. In
Proceedings of the 8th International
Conference on Spoken Language
Processing, pages 37?40, Jeju, Korea.
Crego, Josep Maria, Jose? Bernardo Marin?o,
and Adria` de Gispert. 2005a. An
Ngram-based statistical machine
translation decoder. In INTERSPEECH
2005, pages 3185?3188, Lisbon, Portugal.
Crego, Josep Maria, Jose? Bernardo Marin?o,
and Adria` de Gispert. 2005b. Reordered
search and tuple unfolding for Ngram-
based SMT. Proceedings of the Tenth
Machine Translation Summit, pages 283?289,
Phuket, Thailand.
Crego, Josep Maria, Marta Ruiz Costa-jussa`,
Jose? Bernardo Marin?o, and Jose? Adria?n
Rodriguez Fonollosa. 2005c. Ngram-
based versus phrase-based statistical
machine translation. In Proceedings of the
International Workshop on Spoken Language
Translation, pages 177?184, Pittsburgh, PA.
Crego, Josep Maria and Jose? Bernardo
Marin?o. 2006. Integration of POStag-based
source reordering into SMT decoding by
an extended search graph. In Proceedings of
the 7th Biennial Conference of the Association
for Machine Translation in the Americas,
Boston, MA.
de Gispert, Adria` and Jose? Bernardo Marin?o.
2002. Using X-grams for speech-to-
speech translation. In Proceedings of the
7th International Conference on Spoken
Language Processing, pages 1885?1888,
Denver, CO.
de Gispert, Adria`, Jose? Bernardo Marin?o, and
Josep Maria Crego. 2004. TALP:
Xgram-based spoken language translation
system. In Proceedings of the International
Workshop on Spoken Language Translation,
pages 85?90, Kyoto, Japan.
de Gispert, Adria`. 2005. Phrase linguistic
classification and generalization for
improving statistical machine translation.
In ACL?05 Student Workshop, pages 67?72,
Ann Arbor, MI.
Hutchins, John. 1986. Machine Translation:
Past, Present and Future. Ellis Horwood,
Chichester, England.
Kay, Martin, Jean Mark Gawron, and Peter
Norvig. 1992. Verbmobil: A Translation
System for Face-to-Face Dialog. CSLI.
Kneser, Reinhard and Hermann Ney. 1995.
Improved backing-off for m-gram
language modeling. In IEEE International
Conference on Acoustics, Speech and Signal
Processing, pages 49?52, Detroit, MI.
Knight, Kevin and Yaser Al-Onaizan.
1998. Translation with finite-state
devices. In AI Lecture Notes in Artificial
Intelligence, volume 1529, Springer-Verlag,
pages 421?437.
Koehn, Philippe, Franz Joseph Och,
and Daniel Marcu. 2003. Statistical
phrase-based translation. In Proceedings
of the 2003 Meeting of the North American
chapter of the ACL, pages 48?54, Edmonton,
Alberta, Canada.
Koehn, Philippe. 2002. Europarl: A
multilingual corpus for evaluation
of machine translation. Available
online at: http://people.csail.mit.edu/
people/koehn/publications/europarl/.
Marin?o, Jose? Bernardo, Rafael E. Banchs,
Josep Maria Crego, Adria` de Gispert,
Patrik Lambert, Jose? Adria?n Rodriguez
Fonollosa, and Marta Ruiz. 2005. Bilingual
N-gram statistical machine translation.
In Proceedings of the Tenth Machine
Translation Summit, pages 275?282,
Phuket, Thailand.
548
Marin?o et al N-gram-based Machine Translation
Ney, Hermann, Volker Steinbiss, Richard
Zens, Evgeny Matusov, Jorge Gonza?lez,
Young-suk Lee, Salim Roukos, Marcello
Federico, Muntsin Kolss, and Rafael
Banchs. 2005. SLT progress report.
TC-STAR Deliverable D5, European
Community project no. FP6-506738.
Available online at: http://www.
tc-star.org/pages/f documents.htm.
Och, Franz Joseph and Hermann Ney.
2000. Improved statistical alignment
models. In Proceedings of the 38th Annual
Meeting of the ACL, pages 440?447,
Hong Kong, China.
Och, Franz Joseph and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 295?302,
Philadelphia, PA.
Och, Franz Joseph and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Och, Franz Joseph, Daniel Gildea, Sanjeev
Khudanpur, Anoop Sarkar, Kenji Yamada,
Alexander Fraser, Shankar Kumar, Libin
Shen, David Smith, Katharine Eng, Viren
Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical
machine translation. In Proceedings of the
Human Language Technology Conference
NAACL, pages 161?168, Boston, MA, May.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Conference of the ACL,
pages 311?318, Philadelphia, PA.
Press, William H., Saul Teukolsky, William
Vetterling, and Brian P. Flannery.
2002. Numerical Recipes in C++: The
Art of Scientific Computing, Cambridge
University Press.
Riccardi, Giuseppe, Roberto Pieraccini, and
Enrico Bocchieri. 1996. Stochastic automata
for language modeling. Computer Speech
and Language, 10(4):265?293.
Shannon, Claude E. 1949. Communication
theory of secrecy systems. Bell System
Technical Journal, 28:656?715.
Shannon, Claude E. 1951. Prediction and
entropy of printed English. Bell System
Technical Journal, 30:50?64.
Shannon, Claude E. and Warren Weaver.
1949. The Mathematical Theory of
Communication, University of Illinois
Press, Urbana, IL.
Stolcke, Andreas 2002. SRLIM: An extensible
language modeling toolkit. In Proceedings
of the International Conference on Spoken
Language Processing, pages 901?904,
Denver, CO.
Tillmann, Christoph and Fei Xia. 2003. A
phrase-based unigram model for statistical
machine translation. In Proceedings of
HLT-NAACL - Short Papers, pages 106?108,
Edmonton, Alberta, Canada.
Vidal, Enrique. 1997. Finite-state speech-to-
speech translation. In Proceedings of 1997
IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 111?114,
Munich, Germany.
Weaver, Warren. 1955. Translation. In
William Locke and A. Donald Booth,
editors, Machine Translation of Languages:
Fourteen Essays. John Wiley & Sons, New
York, pages 15?23.
Zens, Richard, Franz Joseph Och, and
Hermann Ney. 2002. Phrase-based
statistical machine translation. In
25th German Conference on Artificial
Intelligence, pages 18?32, September.
Aachen, Springer Verlag.
549

Proceedings of NAACL HLT 2007, Companion Volume, pages 137?140,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Analysis and System Combination of Phrase- and N -gram-based
Statistical Machine Translation Systems
Marta R. Costa-jussa`1, Josep M. Crego1, David Vilar2
Jose? A. R. Fonollosa1, Jose? B. Marin?o1 and Hermann Ney2
1TALP Research Center (UPC), Barcelona 08034, Spain
{mruiz,jmcrego,adrian,canton}@gps.tsc.upc.edu
2RWTH Aachen University, Aachen D-52056, Germany
{vilar,ney}@i6.informatik.rwth-aachen.de
Abstract
In the framework of the Tc-Star project,
we analyze and propose a combination of
two Statistical Machine Translation sys-
tems: a phrase-based and an N -gram-based
one. The exhaustive analysis includes a
comparison of the translation models in
terms of efficiency (number of translation
units used in the search and computational
time) and an examination of the errors in
each system?s output. Additionally, we
combine both systems, showing accuracy
improvements.
1 Introduction
Statistical machine translation (SMT) has evolved
from the initial word-based translation models to
more advanced models that take the context sur-
rounding the words into account. The so-called
phrase-based and N -gram-based models are two ex-
amples of these approaches (Zens and Ney, 2004;
Marin?o et al, 2006).
In current state-of-the-art SMT systems, the
phrase-based or the N -gram-based models are usu-
ally the main features in a log-linear framework, rem-
iniscent of the maximum entropy modeling approach.
Two basic issues differentiate the N -gram-based
system from the phrase-based one: the training data
is sequentially segmented into bilingual units; and
the probability of these units is estimated as a bilin-
gual N -gram language model. In the phrase-based
model, no monotonicity restriction is imposed on the
segmentation and the probabilities are normally es-
timated simply by relative frequencies.
This paper extends the analysis of both systems
performed in (Crego et al, 2005a) by additionally
performing a manual error analysis of both systems,
which were the ones used by UPC and RWTH in the
last Tc-Star evaluation.
Furthermore, we will propose a way to combine
both systems in order to improve the quality of trans-
lations.
Experiments combining several kinds of MT sys-
tems have been presented in (Matusov et al, 2006),
based only on the single best output of each system.
Recently, a more straightforward approach of both
systems has been performed in (Costa-jussa` et al,
2006) which simply selects, for each sentence, one of
the provided hypotheses.
This paper is organized as follows. In section 2,
we briefly describe the phrase and the N -gram-based
baseline systems. In the next section we present the
evaluation framework. In Section 4 we report a struc-
tural comparison performed for both systems and, af-
terwards, in Section 5, we analyze the errors of both
systems. Finally, in the last two sections we rescore
and combine both systems, and the obtained results
are discussed.
2 Baseline Systems
2.1 Phrase-based System
The basic idea of phrase-based translation is to seg-
ment the given source sentence into units (here called
phrases), then translate each phrase and finally com-
pose the target sentence from these phrase transla-
tions.
In order to train these phrase-based models, an
alignment between the source and target training
sentences is found by using the standard IBM mod-
els in both directions (source-to-target and target-
to-source) and combining the two obtained align-
ments. Given this alignment an extraction of con-
tiguous phrases is carried out, specifically we extract
all phrases that fulfill the following restrictions: all
source (target) words within the phrase are aligned
only to target (source) words within the phrase.
The probability of these phrases is normally esti-
mated by relative frequencies, normally in both di-
rections, which are then combined in a log-linear way.
137
2.2 N-gram-based System
In contrast with standard phrase-based approaches,
the N -gram translation model uses tuples as bilin-
gual units whose probabilities are estimated as an
N -gram language model (Marin?o et al, 2006). This
model approximates the joint probability between
the source and target languages by using N -grams.
Given a word alignment, tuples define a unique
and monotonic segmentation of each bilingual sen-
tence, building up a much smaller set of units
than with phrases and allowing N -gram estimation
to account for the history of the translation pro-
cess (Marin?o et al, 2006).
2.3 Feature functions
Both baseline systems are combined in a log-linear
way with several additional feature functions: a tar-
get language model, a forward and a backward lex-
icon model and a word bonus are common features
for both systems. The phrase-based system also in-
troduces a phrase bonus model.
3 Evaluation framework
The translation models presented so far were the ones
used by UPC and RWTH in the second evaluation
campaign of the Tc-Star project. The goal of this
project is to build a speech-to-speech translation sys-
tem that can deal with real life data.
The corpus consists of the official version of the
speeches held in the European Parliament Plenary
Sessions (EPPS), as available on the web page of the
European Parliament. Table 1 shows some statistics.
The following tools have been used for building
both systems: Word alignments were computed us-
ing GIZA++ (Och, 2003), language models were es-
timated using the SRILM toolkit (Stolcke, 2002), de-
coding was carried out by the free available MARIE
decoder (Crego et al, 2005b) and the optimization
was performed through an in-house implementation
of the simplex method (Nelder and Mead, 1965).
Spanish English
Train Sentences 1.2M
Words 32M 31M
Vocabulary 159K 111K
Dev Sentences 1 122 699
Words 26K 21K
Test Sentences 1 117 894
Words 26K 26K
Table 1: Statistics of the EPPS Corpora.
4 Structural comparison
Both approaches aim at improving accuracy by in-
cluding word context in the model. However, the
implementation of the models are quite different and
may produce variations in several aspects.
Table 2 shows the effect on decoding time intro-
duced through different settings of the beam size.
Additionally, the number of available translation
units is shown, corresponding to number of avail-
able phrases for the phrase-based system and 1gram,
2gram and 3gram entries for the N -gram-based sys-
tem. Results are computed on the development set.
Task Beam Time(s) Units
50 2,677
es?en 10 852 537k
5 311
50 2,689
en?es 10 903 594k
5 329
50 1,264
es?en 10 281 104k 288k 145k
5 138
50 1,508
en?es 10 302 118k 355k 178k
5 155
Table 2: Impact on efficiency of the beam size in PB
(top) and NB system (bottom).
As it can be seen, the number of translation units
is similar in both tasks for both systems (537k ?
537k for Spanish to English and 594k ? 651k for
English to Spanish) while the time consumed in de-
coding is clearly higher for the phrase-based system.
This can be explained by the fact that in the phrase-
based approach, the same translation can be hypoth-
esized following several segmentations of the input
sentence, as phrases appear (and are collected) from
multiple segmentations of the training sentence pairs.
In other words, the search graph seems to be over-
populated under the phrase-based approach.
Table 3 shows the effect on translation accuracy
regarding the size of the beam in the search. Results
are computed on the test set for the phrase-based
and N -gram-based systems.
Results of the N -gram-based system show that de-
creasing the beam size produces a clear reduction
of the accuracy results. The phrase-based system
shows that accuracy results remain very similar un-
der the different settings. The reason is found on
how translation models are used in the search. In
the phrase-based approach, every partial hypothesis
138
Task Beam BLEU NIST mWER
50 51.90 10.53 37.54
es?en 10 51.93 10.54 37.49
5 51.87 10.55 37.47
50 47.75 9.94 41.20
en?es 10 47.77 9.96 41.09
5 47.86 10.00 40.74
50 51.63 10.46 37.88
es?en 10 51.50 10.45 37.83
5 51.39 10.45 37.85
50 47.73 10.08 40.50
en?es 10 46.82 9.97 41.04
5 45.59 9.83 41.04
Table 3: Impact on accuracy of the beam size in PB
(top) and NB system (bottom).
is scored uncontextualized, hence, a single score is
used for a given partial hypothesis (phrase). In the
N -gram-based approach, the model is intrinsically
contextualized, which means that each partial hy-
pothesis (tuple) depends on the preceding sequence
of tuples. Thus, if a bad sequence of tuples (bad
scored) is composed of a good initial sequence (well
scored), it is placed on top of the first stacks (beam)
and may cause the pruning of the rest of hypotheses.
5 Error analysis
In order to better asses the quality and the differ-
ences between the two systems, a human error anal-
ysis was carried out. The guidelines for this error
analysis can be found in (Vilar et al, 2006). We
randomly selected 100 sentences, which were evalu-
ated by bilingual judges.
This analysis reveals that both systems produce
the same kind of errors in general. However some dif-
ferences were identified. For the English to Spanish
direction the greatest problem is the correct genera-
tion of the right tense for verbs, with around 20% of
all translation errors being of this kind. Reordering
also poses an important problem for both phrase and
N-gram-based systems, with 18% or 15% (respec-
tively) of the errors falling into this category. Miss-
ing words is also an important problem. However,
most of them (approximately two thirds for both sys-
tems) are filler words (i.e. words which do not con-
vey meaning), that is, the meaning of the sentence
is preserved. The most remarkable difference when
comparing both systems is that the N -gram based
system produces a relatively large amount of extra
words (approximately 10%), while for the phrase-
based system, this is only a minor problem (2% of
the errors). In contrast the phrase-based system has
more problems with incorrect translations, that is
words for which a human can find a correspondence
in the source text, but the translation is incorrect.
Similar conclusions can be drawn for the inverse di-
rection. The verb generating problem is not so acute
in this translation direction due to the much simpli-
fied morphology of English. An important problem
is the generation of the right preposition.
The N -gram based system seems to be able to pro-
duce more accurate translations (reflected by a lower
percentage of translation errors). However, it gener-
ates too many additional (and incorrect words) in
the process. The phrase-based system, in contrast,
counteracts this effect by producing a more direct
correspondence with the words present in the source
sentence at the cost of sometimes not being able to
find the exact translation.
6 System Rescoring and
Combination
Integration of both output translations in the search
procedure is a complex task. Translation units of
both models are quite different and generation his-
tories pose severe implementation difficulties. We
propose a method for combining the two systems at
the level of N -best lists.
Some features that are useful for SMT are too com-
plex for including them directly in the search pro-
cess. A clear example are the features that require
the entire target sentence to be evaluated, as this is
not compatible with the pruning and recombination
procedures that are necessary for keeping the target
sentence generation process manageable. A possible
solution for this problem is to apply sentence level
re-ranking by using N -best lists.
6.1 Rescoring Criteria
The aim of the rescoring procedure is to choose the
best translation candidate out of a given set of N
possible translations. In our approach this transla-
tion candidates are produced independently by both
of the systems and then combined by a simple con-
catenation1. In order for the hypothesis to have a
comparable set of scores, we perform an additional
?cross-rescoring? of the lists.
Given an N -best list of the phrase-based (N -gram-
based) system, we compute the cost of each target
sentence of this N -best list for the N -gram-based
(phrase-based) system. However this computation
is not possible in all cases. Table 4 shows the per-
centage of target sentences that the N -gram-based
1With removal of duplicates.
139
(phrase-based) system is able to produce given an N -
best list of target sentences computed by the phrase-
based (N -gram-based) system. This percentage is
calculated on the development set.
The vocabulary of phrases is bigger than the vo-
cabulary of tuples, due to the fact that phrases are
extracted from multiple segmentations of the train-
ing sentence pairs. Hence, the number of sentences
reproduced by the N -gram-based system is smaller
than the number of sentences reproduced by the
phrase-based system. Whenever a sentence can not
be reproduced by a given system, the cost of the
worst sentence in the N -best list is assigned to it.
Task N -best % NB % PB
es?en 1000 37.5 57.5
en?es 1000 37.2 48.6
Table 4: Sentences (%) produced by each system.
6.2 Results
Table 5 shows results of the rescoring and system
combination experiments on the test set. The first
two rows include results of systems non-rescored and
PB (NB) rescored by NB (PB). The third row corre-
sponds to the system combination. Here, PB (NB)
rescored by NB (PB) are simply merged and ranked
by rescored score.
System N -best BLEU NIST mWER
Spanish-to-English
PB 1 51.90 10.54 37.50
PB 1000 52.55 10.61 37.12
NB 1 51.63 10.46 37.88
NB 1000 52.25 10.55 37.43
PB+NB 2 51.77 10.49 37.68
PB+NB 2000 52.31 10.56 37.32
English-to-Spanish
PB 1 47.75 9.94 41.2
PB 1000 48.46 10.13 39.98
NB 1 47.73 10.09 40.50
NB 1000 48.33 10.15 40.13
PB+NB 2 48.26 10.05 40.61
PB+NB 2000 48.54 10.16 40.00
Table 5: Rescoring and system combination results.
7 Discussion
The structural comparison has shown on the one
hand that the N -gram-based system outperforms
the phrase-based in terms of search time efficiency
by avoiding the overpopulation problem presented
in the phrase-based approach. On the other hand
the phrase-based system shows a better performance
when decoding under a highly constrained search.
A detailed error analysis has also been carried out
in order to better determine the differences in per-
formance of both systems. The N -gram based sys-
tem produced more accurate translations, but also a
larger amount of extra (incorrect) words when com-
pare to the phrase-based translation system.
In section 6 we have presented a system combina-
tion method using a rescoring feature for each SMT
system, i.e. the N -gram-based feature for the phrase-
based system and vice-versa. For both systems, con-
sidering the feature of the opposite system leads to
an improvement of BLEU score.
References
M.R. Costa-jussa`, J.M. Crego, A. de Gispert,
P. Lambert, M. Khalilov J.A.R. Fonollosa, J.B.
Marin?o, and R. Banchs. 2006. Talp phrase-based
statistical machine translation and talp system
combination the iwslt 2006. IWSLT06.
J. M. Crego, M. R. Costa-jussa`, J. Marin?o, and J. A.
Fonollosa. 2005a. N-gram-based versus phrase-
based statistical machine translation. IWSLT05,
October.
J.M. Crego, J. Marin?o, and A. de Gispert. 2005b.
An Ngram-based statistical machine translation
decoder. ICSLP05, April.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gis-
pert, P. Lambert, J.A.R. Fonollosa, and M.R.
Costa-jussa`. 2006. N-gram based machine trans-
lation. Computational Linguistics, 32(4):527?549.
E. Matusov, N. Ueffing, and H. Ney. 2006. Com-
puting consensus translation from multiple ma-
chine translation systems using enhanced hypothe-
ses alignment. EACL06, pages 33?40.
J.A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7:308?313.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language
modeling toolkit. Proc. of the 7th Int. Conf. on
Spoken Language Processing, ICSLP?02, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and
Hermann Ney. 2006. Error Analysis of Machine
Translation Output. In LREC06, pages 697?702,
Genoa, Italy, May.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In HLT04, pages 257?264, Boston, MA, May.
140
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 213?216,
Prague, June 2007. c?2007 Association for Computational Linguistics
Extending MARIE: an N -gram-based SMT decoder
Josep M. Crego
TALP Research Center
Universitat Polite`cnica de Catalunya
Barcelona, 08034
jmcrego@gps.tsc.upc.edu
Jose? B. Marin?o
TALP Research Center
Universitat Polite`cnica de Catalunya
Barcelona,08034
canton@gps.tsc.upc.edu
Abstract
In this paper we present several extensions of
MARIE1, a freely available N -gram-based sta-
tistical machine translation (SMT) decoder. The
extensions mainly consist of the ability to ac-
cept and generate word graphs and the intro-
duction of two new N -gram models in the log-
linear combination of feature functions the de-
coder implements. Additionally, the decoder is
enhanced with a caching strategy that reduces
the number of N -gram calls improving the over-
all search efficiency. Experiments are carried out
over the Eurpoean Parliament Spanish-English
translation task.
1 Introduction
Research on SMT has been strongly boosted in the last
few years, partially thanks to the relatively easy develop-
ment of systems with enough competence as to achieve
rather competitive results. In parallel, tools and tech-
niques have grown in complexity, which makes it diffi-
cult to carry out state-of-the-art research without sharing
some of this toolkits. Without aiming at being exhaus-
tive, GIZA++2, SRILM3 and PHARAOH4 are probably
the best known examples.
We introduce the recent extensions made to an N -
gram-based SMT decoder (Crego et al, 2005), which al-
lowed us to tackle several translation issues (such as re-
ordering, rescoring, modeling, etc.) successfully improv-
ing accuracy, as well as efficiency results.
As far as SMT can be seen as a double-sided prob-
lem (modeling and search), the decoder emerges as a key
component, core module of any SMT system. Mainly,
1http://gps-tsc.upc.es/soft/soft/marie
2http://www.fjoch.com/GIZA++.html
3http://www.speech.sri.com/projects/srilm/
4http://www.isi.edu/publications/licensed-sw/pharaoh/
any technique aiming at dealing with a translation prob-
lem needs for a decoder extension to be implemented.
Particularly, the reordering problem can be more effi-
ciently (and accurate) addressed when tightly coupled
with decoding. In general, the competence of a decoder
to make use of the maximum of information in the global
search is directly connected with the likeliness of suc-
cessfully improving translations.
The paper is organized as follows. In Section 2 we
and briefly review the previous work on decoding with
special attention to N -gram-based decoding. Section 3
describes the extended log-linear combination of feature
functions after introduced the two new models. Section
4 details the particularities of the input and output word
graph extensions. Experiments are reported on section 5.
Finally, conclusions are drawn in section 6.
2 Related Work
The decoding problem in SMT is expressed by the next
maximization: argmaxtI1?? P (t
I
1|sJ1 ), where sJ1 is the
source sentence to translate and tI1 is a possible transla-
tion of the set ? , which contains all the sentences of the
language of tI1.
Given that the full search over the whole set of tar-
get language sentences is impracticable (? is an infinite
set), the translation sentence is usually built incremen-
tally, composing partial translations of the source sen-
tence, which are selected out of a limited number of trans-
lation candidates (translation units).
The first SMT decoders were word-based. Hence,
working with translation candidates of single source
words. Later appeared the phrase-based decoders,
which use translation candidates composed of sequences
of source and target words (outperforming the word-
based decoders by introducing the word context). In the
last few years syntax-based decoders have emerged aim-
ing at dealing with pair of languages with different syn-
tactical structures for which the word context introduced
213
Figure 1: Generative process. Phrase-based (left) and N -gram-based (right) approaches.
in phrase-based decoders is not sufficient to cope with
long reorderings.
Like standard phrase-based decoders, MARIE em-
ploys translation units composed of sequences of source
and target words. In contrast, the translation con-
text is differently taken into account. Whereas phrase-
based decoders employ translation units uncontextual-
ized, MARIE takes the translation unit context into ac-
count by estimating the translation model as a standard
N -gram language model (N -gram-based decoder).
Figure 1 shows that both approaches follow the same
generative process, but they differ on the structure of
translation units. In the example, the units ?s1#t1? and
?s2 s3#t2 t3? of the N -gram-based approach are used
considering that both appear sequentially. This fact can
be understood as using a longer unit that includes both
(longer units are drawn in grey).
MARIE follows the maximum entropy framework,
where we can define a translation hypothesis t given a
source sentence s, as the target sentence maximizing a
log-linear combination of feature functions:
t?I1 = arg max
tI1
{ M
?
m=1
?mhm(sJ1 , tI1)
}
(1)
where ?m corresponds to the weighting coefficients of
the log-linear combination, and the feature functions
hm(s, t) to a logarithmic scaling of the probabilities of
each model. See (Marin?o et al, 2006) for further details
on the N -gram-based approach to SMT.
3 N-gram Feature Functions
Two language models (LM) are introduced in equation 1,
aiming at helping the decoder to find the right transla-
tions. Both are estimated as standard N -gram LM.
3.1 Target-side N -gram LM
The first additional N -gram LM is destinated to be ap-
plied over the target sentence (tagged) words. Hence,
as the original target LM (computed over raw words),
it is also used to score the fluency of target sentences,
but aiming at achieving generalization power through us-
ing a more generalized language (such as a language of
Part-of-Speech tags) instead of the one composed of raw
words. Part-Of-Speech tags have successfully been used
in several previous experiments. however, any other tag
can be applied.
Several sequences of target tags may apply to any given
translation unit (which are passed to the decoder before it
starts the search). For instance, regarding a translation
unit with the english word ?general? in its target side, if
POS tags were used as target tagged tags, there would ex-
ist at least two different tag options: noun and adjective.
In the search, multiple hypotheses are generated con-
cerning different target tagged sides (sequences of tags)
of a single translation unit. Therefore, on the one side, the
overall search is extended towards seeking the sequence
of target tags that better fits the sequence of target raw
words. On the other side, this extension is hurting the
overall efficiency of the decoder as additional hypotheses
appear in the search stacks while not additional transla-
tion hypotheses are being tested (only differently tagged).
This extended feature may be used toghether with a
limitation of the number of target tagged hypotheses per
translation unit. The use of a limited number of these
hypotheses implies a balance between accuracy and effi-
ciency.
3.2 Source-side N -gram LM
The second N -gram LM is applied over the input sen-
tence tagged words. Obviously, this model only makes
sense when reordering is applied over the source words
in order to monotonize the source and target word order.
In such a case, the tagged LM is learnt over the training
set with reordered source words.
Hence, the new model is employed as a reordering
model. It scores a given source-side reordering hypoth-
esis according to the reorderings made in the training
sentences (from which the tagged LM is estimated). As
for the previous extension, source tagged words are used
instead of raw words in order to achieve generalization
power.
Additional hypotheses regarding the same translation
unit are not generated in the search as all input sentences
are uniquely tagged.
Figure 2 illustrates the use of a source POS-tagged N -
214
gram LM. The probability of the sequence ?PRN VRB
NAME ADJ? is greater than the probability of the se-
quence ?PRN VRB ADJ NAME? for a model estimated
over the training set with reordered source words (with
english words following the spanish word order).
Figure 2: Source POS-tagged N -gram LM.
3.3 Caching N -grams
The use of several N -gram LM?s implies a reduction in
efficiency in contrast to other models that can be imple-
mented by means of a single lookup table (one access per
probability call). The special characteristics of Ngram
LM?s introduce additional memory access to account for
backoff probabilities and lower Ngrams fallings.
Many N -gram calls are requested repeatedly, produc-
ing multiple calls of an entry. A simple strategy to reduce
additional access consists of keeping a record (cache) for
those Ngram entries already requested. A drawback for
the use of a cache consists of the additional memory ac-
cess derived of the cache maintenance (adding new and
checking for existing entries).
Figure 3: Memory access derived of an N -gram call.
Figure 3 illustrates this situation. The call for a 3-gram
probability (requesting for the probability of the sequence
of tokens ?a b c?) may need for up to 6 memory access,
while under a phrase-based translation model the final
probability would always be reached after the first mem-
ory access. The additional access in the N -gram-based
approach are used to provide lower N -gram and backoff
probabilities in those cases that upper N -gram probabili-
ties do not exist.
4 Word Graphs
Word graphs are successfully used in SMT for several ap-
plications. Basically, with the objective of reducing the
redundancy of N -best lists, which very often convey se-
rious combinatorial explosion problems.
A word graph is here described as a directed acyclic
graph G = (V,E) with one root node n0 ? V .
Edges are labeled with tokens (words or translation units)
and optionally with accumulated scores. We will use
(ns(ne ??t?? s)), to denote an edge starting at node ns and
ending at node ne, with token t and score s. The file
format of word graphs coincides with the graph file for-
mat recognized by the CARMEL5 finite state automata
toolkit.
4.1 Input Graph
We can mainly find two applications for which word
graphs are used as input of an SMT system: the recog-
nition output of an automatic speech recognition (ASR)
system; and a reordering graph, consisting of a subset of
the whole word permutations of a given input sentence.
In our case we are using the input graph as a reorder-
ing graph. The decoder introduces reordering (distortion
of source words order) by allowing only for the distor-
tion encoded in the input graph. Though, the graph is
only allowed to encode permutations of the input words.
In other words, any path in the graph must start at node
n0, finish at node nN (where nN is a unique ending node)
and cover all the input words (tokens t) in whatever order,
without repetitions.
An additional feature function (distortion model) is in-
troduced in the log-linear combination of equation 1:
pdistortion(uk) ?
kI
?
i=k1
p(ni|ni?1) (2)
where uk refers to the kth partial translation unit covering
the source positions [k1, ..., kI ]. p(ni|ni?1) corresponds
to the edge score s encoded in the edge (ns(ne ??t?? s)),
where ni = ne and ni?1 = ns.
One of the decoding first steps consists of building
(for each input sentence) the set of translation units to be
used in the search. When the search is extended with re-
ordering abilities the set must be also extended with those
translation units that cover any sequence of input words
following any of the word orders encoded in the input
graph. The extension of the units set is specially relevant
when translation units are built from the tranining set with
reordered source words.
Given the example of figure 2, if the translation unit
?translations perfect # traducciones perfectas? is avail-
able, the decoder should not discard it, as it provides
a right translation. Notwithstanding that its source side
does not follow the original word order of the input sen-
tence.
4.2 Output Graph
The goal of using an output graph is to allow for further
rescoring work. That is, to work with alternative transla-
5http://www.isi.edu/licensed-sw/carmel/
215
tions to the single 1-best. Therefore, our proposed output
graph has some peculiarities that make it different to the
previously sketched intput graph.
The structure of edges remains the same, but obvi-
ously, paths are not forced to consist of permutations of
the same tokens (as far as we are interested into multiple
translation hypotheses), and there may also exist paths
which do not reach the ending node nN . These latter
paths are not useful in rescoring tasks, but allowed in or-
der to facilitate the study of the search graph. However,
a very easy and efficient algorithm (O(n), being n the
search size) can be used in order to discard them, before
rescoring work. Additionally, given that partial model
costs are needed in rescoring work, our decoder allows
to output the individual model costs computed for each
translation unit (token t). Costs are encoded within the
token s, as in the next example:
(0 (1 "o#or{1.5,0.9,0.6,0.2}" 6))
where the token t is now composed of the translation unit
?o#or?, followed by (four) model costs.
Multiple translation hypotheses can only be extracted
if hypotheses recombinations are carefully saved. As in
(Koehn, 2004), the decoder takes a record of any recom-
bined hypothesis, allowing for a rigorous N -best genera-
tion. Model costs are referred to the current unit while the
global score s is accumulated. Notice also that translation
units (not words) are now used as tokens.
5 Experiments
Experiments are carried out for a Spanish-to-English
translation task using the EPPS data set, corresponding
to session transcriptions of the European Parliament.
Eff. base +tpos +reor +spos
Beam size = 50
w/o cache 1, 820 2, 170 2, 970 3, 260
w/ cache ?50 ?110 ?190 ?210
Beam size = 100
w/o cache 2, 900 4, 350 5, 960 6, 520
w/ cache ?175 ?410 ?625 ?640
Table 1: Translation efficiency results.
Table 1 shows translation efficiency results (mea-
sured in seconds) given two different beam search sizes.
w/cache and w/o cache indicate whether the decoder em-
ploys (or not) the cache technique (section 3.3). Sev-
eral system configuration have been tested: a baseline
monotonous system using a 4-gram translation LM and
a 5-gram target LM (base), extended with a target POS-
tagged 5-gram LM (+tpos), further extended by allow-
ing for reordering (+reor), and finally using a source-side
POS-tagged 5-gram LM (+spos).
As it can be seen, the cache technique improves the ef-
ficiency of the search in terms of decoding time. Time
results are further decreased (reduced time is shown for
the w/ cache setting) by using more N -gram LM and al-
lowing for a larger search graph (increasing the beam size
and introducing distortion).
Further details on the previous experiment can be
seen in (Crego and Marin?o, 2006b; Crego and Marin?o,
2006a), where additionally, the input word graph and ex-
tended N -gram tagged LM?s are successfully used to im-
prove accuracy at a very low computational cost.
Several publications can also be found in bibliography
which show the use of output graphs in rescoring tasks
allowing for clear accuracy improvements.
6 Conclusions
We have presented several extensions to MARIE, a freely
available N -gram-based decoder. The extensions consist
of accepting and generating word graphs, and introducing
two N -gram LM?s over source and target tagged words.
Additionally, a caching technique is applied over the N -
gram LM?s.
Acknowledgments
This work has been funded by the European Union un-
der the integrated project TC-STAR - (IST-2002-FP6-
5067-38), the Spanish Government under the project
AVIVAVOZ - (TEC2006-13694-C03) and the Universitat
Polite`cnica de Catalunya under UPC-RECERCA grant.
References
J.M. Crego and J.B. Marin?o. 2006a. Integration of
postag-based source reordering into smt decoding by
an extended search graph. Proc. of the 7th Conf. of the
Association for Machine Translation in the Americas,
pages 29?36, August.
J.M. Crego and J.B. Marin?o. 2006b. Reordering experi-
ments for n-gram-based smt. 1st IEEE/ACL Workshop
on Spoken Language Technology, December.
J.M. Crego, J.B. Marin?o, and A. de Gispert. 2005. An
ngram-based statistical machine translation decoder.
Proc. of the 9th European Conference on Speech Com-
munication and Technology, Interspeech?05, pages
3193?3196, September.
Ph. Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. Proc. of the 6th Conf. of the Association for Ma-
chine Translation in the Americas, pages 115?124, Oc-
tober.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram based machine translation. Computa-
tional Linguistics, 32(4):527?549.
216
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 133?136,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Statistical Machine Translation of Euparl Data by using Bilingual N-grams
Rafael E. Banchs Josep M. Crego Adria` de Gispert
Department of Signal Theory and Communications
Universitat Polite`cnica de Catalunya, Barcelona 08034, Spain
{rbanchs,jmcrego,agispert,lambert,canton}@gps.tsc.upc.edu
Patrik Lambert Jose? B. Marin?o
Abstract
This work discusses translation results for
the four Euparl data sets which were made
available for the shared task ?Exploit-
ing Parallel Texts for Statistical Machine
Translation?. All results presented were
generated by using a statistical machine
translation system which implements a
log-linear combination of feature func-
tions along with a bilingual n-gram trans-
lation model.
1 Introduction
During the last decade, statistical machine transla-
tion (SMT) systems have evolved from the orig-
inal word-based approach (Brown et al, 1993)
into phrase-based translation systems (Koehn et al,
2003). Similarly, the noisy channel approach has
been expanded to a more general maximum entropy
approach in which a log-linear combination of mul-
tiple models is implemented (Och and Ney, 2002).
The SMT approach used in this work implements
a log-linear combination of feature functions along
with a translation model which is based on bilingual
n-grams. This translation model was developed by
de Gispert and Marin?o (2002), and it differs from the
well known phrase-based translation model in two
basic issues: first, training data is monotonously seg-
mented into bilingual units; and second, the model
considers n-gram probabilities instead of relative
frequencies. This model is described in section 2.
Translation results from the four source languages
made available for the shared task (es: Spanish, fr:
French, de: German, and fi: Finnish) into English
(en) are presented and discussed.
The paper is structured as follows. Section 2 de-
scribes the bilingual n-gram translation model. Sec-
tion 3 presents a brief overview of the whole SMT
procedure. Section 4 presents and discusses the
shared task results and other interesting experimen-
tation. Finally, section 5 presents some conclusions
and further work.
2 Bilingual N-gram Translation Model
As already mentioned, the translation model used
here is based on bilingual n-grams. It actually con-
stitutes a language model of bilingual units which
are referred to as tuples (de Gispert and Marin?o,
2002). This model approximates the joint probabil-
ity between source and target languages by using 3-
grams as it is described in the following equation:
p(T, S) ?
N
?
n=1
p((t, s)n|(t, s)n?2, (t, s)n?1) (1)
where t refers to target, s to source and (t, s)n to the
nth tuple of a given bilingual sentence pair.
Tuples are extracted from a word-to-word aligned
corpus according to the following two constraints:
first, tuple extraction should produce a monotonic
segmentation of bilingual sentence pairs; and sec-
ond, the produced segmentation is maximal in the
sense that no smaller tuples can be extracted with-
out violating the previous constraint (Crego et al,
2004). According to this, tuple extraction provides a
unique segmentation for a given bilingual sentence
pair alignment. Figure 1 illustrates this idea with a
simple example.
133
We would like to achieve perfect translations
NULL quisieramos lograr traducciones perfectas
t1 t2 t3 t4
Figure 1: Example of tuple extraction from an
aligned sentence pair.
Two important issues regarding this translation
model must be mentioned. First, when extracting
tuples, some words always appear embedded into tu-
ples containing two or more words, so no translation
probability for an independent occurrence of such
words exists. To overcome this problem, the tuple
3-gram model is enhanced by incorporating 1-gram
translation probabilities for all the embedded words
(de Gispert et al, 2004).
Second, some words linked to NULL end up pro-
ducing tuples with NULL source sides. This cannot
be allowed since no NULL is expected to occur in a
translation input. This problem is solved by prepro-
cessing alignments before tuple extraction such that
any target word that is linked to NULL is attached
to either its precedent or its following word.
3 SMT Procedure Description
This section describes the procedure followed for
preprocessing the data, training the models and op-
timizing the translation system parameters.
3.1 Preprocessing and Alignment
The Euparl data provided for this shared task (Eu-
parl, 2003) was preprocessed for eliminating all sen-
tence pairs with a word ratio larger than 2.4. As a
result of this preprocessing, the number of sentences
in each training set was slightly reduced. However,
no significant reduction was produced.
In the case of French, a re-tokenizing procedure
was performed in which all apostrophes appearing
alone were attached to their corresponding words.
For example, pairs of tokens such as l ? and qu ?
were reduced to single tokens such as l? and qu?.
Once the training data was preprocessed, a word-
to-word alignment was performed in both direc-
tions, source-to-target and target-to-source, by us-
ing GIZA++ (Och and Ney, 2000). As an approxi-
mation to the most probable alignment, the Viterbi
alignment was considered. Then, the intersection
and union of alignment sets in both directions were
computed for each training set.
3.2 Feature Function Computation
The considered translation system implements a to-
tal of five feature functions. The first of these mod-
els is the tuple 3-gram model, which was already de-
scribed in section 2. Tuples for the translation model
were extracted from the union set of alignments as
shown in Figure 1. Once tuples had been extracted,
the tuple vocabulary was pruned by using histogram
pruning. The same pruning parameter, which was
actually estimated for Spanish-English, was used for
the other three language pairs. After pruning, the
tuple 3-gram model was trained by using the SRI
Language Modeling toolkit (Stolcke, 2002). Finally,
the obtained model was enhanced by incorporating
1-gram probabilities for the embedded word tuples,
which were extracted from the intersection set of
alignments.
Table 1 presents the total number of running
words, distinct tokens and tuples, for each of the four
training data sets.
Table 1: Total number of running words, distinct to-
kens and tuples in training.
source running distinct tuple
language words tokens vocabulary
Spanish 15670801 113570 1288770
French 14844465 78408 1173424
German 15207550 204949 1391425
Finnish 11228947 389223 1496417
The second feature function considered was a tar-
get language model. This feature actually consisted
of a word 3-gram model, which was trained from the
target side of the bilingual corpus by using the SRI
Language Modeling toolkit.
The third feature function was given by a word
penalty model. This function introduces a sentence
length penalization in order to compensate the sys-
134
tem preference for short output sentences. More
specifically, the penalization factor was given by the
total number of words contained in the translation
hypothesis.
Finally, the fourth and fifth feature functions cor-
responded to two lexicon models based on IBM
Model 1 lexical parameters p(t|s) (Brown et al,
1993). These lexicon models were calculated for
each tuple according to the following equation:
plexicon((t, s)n) =
1
(I + 1)J
J
?
j=1
I
?
i=0
p(tin|sjn) (2)
where sjn and tin are the jth and ith words in the
source and target sides of tuple (t, s)n, being J and
I the corresponding total number words in each side
of it.
The forward lexicon model uses IBM Model 1 pa-
rameters obtained from source-to-target algnments,
while the backward lexicon model uses parameters
obtained from target-to-source alignments.
3.3 Decoding and Optimization
The search engine for this translation system was
developed by Crego et al (2005). It implements
a beam-search strategy based on dynamic program-
ming and takes into account all the five feature func-
tions described above simultaneously. It also allows
for three different pruning methods: threshold prun-
ing, histogram pruning, and hypothesis recombina-
tion. For all the results presented in this work the
decoder?s monotonic search modality was used.
An optimization tool, which is based on a simplex
method (Press et al, 2002), was developed and used
for computing log-linear weights for each of the fea-
ture functions described above. This algorithm ad-
justs the log-linear weights so that BLEU (Papineni
et al, 2002) is maximized over a given development
set. One optimization for each language pair was
performed by using the 2000-sentence development
sets made available for the shared task.
4 Shared Task Results
Table 2 presents the BLEU scores obtained for the
shared task test data. Each test set consisted of 2000
sentences. The computed BLEU scores were case
insensitive and used one translation reference.
Table 2: BLEU scores (shared task test sets).
es - en fr - en de - en fi - en
0.3007 0.3020 0.2426 0.2031
As can be seen from Table 2 the best ranked trans-
lations were those obtained for French, followed by
Spanish, German and Finnish. A big difference is
observed between the best and the worst results.
Differences can be observed from translation out-
puts too. Consider, for example, the following seg-
ments taken from one of the test sentences:
es-en: We know very well that the present Treaties are not
enough and that , in the future , it will be necessary to develop
a structure better and different for the European Union...
fr-en: We know very well that the Treaties in their current
are not enough and that it will be necessary for the future to
develop a structure more effective and different for the Union...
de-en: We very much aware that the relevant treaties are
inadequate and , in future to another , more efficient structure
for the European Union that must be developed...
fi-en: We know full well that the current Treaties are not
sufficient and that , in the future , it is necessary to develop the
Union better and a different structure...
It is evident from these translation outputs that
translation quality decreases when moving from
Spanish and French to German and Finnish. A
detailed observation of translation outputs reveals
that there are basically two problems related to this
degradation in quality. The first has to do with re-
ordering, which seems to be affecting Finnish and,
specially, German translations.
The second problem has to do with vocabulary. It
is well known that large vocabularies produce data
sparseness problems (Koehn, 2002). As can be con-
firmed from Tables 1 and 2, translation quality de-
creases as vocabulary size increases. However, it is
not clear yet, in which degree such degradation is
due to monotonic decoding and/or vocabulary size.
Finally, we also evaluated how much the full fea-
ture function system differs from the baseline tu-
ple 3-gram model alone. In this way, BLEU scores
were computed for translation outputs obtained for
the baseline system and the full system. Since the
English reference for the test set was not available,
we computed translations and BLEU scores over de-
135
velopment sets. Table 3 presents the results for both
the full system and the baseline.1
Table 3: Baseline- and full-system BLEU scores
(computed over development sets).
language pair baseline full
es - en 0.2588 0.3004
fr - en 0.2547 0.2938
de - en 0.1844 0.2350
fi - en 0.1526 0.1989
From Table 3, it is evident that the four additional
feature functions produce important improvements
in translation quality.
5 Conclusions and Further Work
As can be concluded from the presented results, per-
formance of the translation system used is much bet-
ter for French and Spanish than for German and
Finnish. As some results suggest, reordering and
vocabulary size are the most important problems re-
lated to the low translation quality achieved for Ger-
man and Finnish.
It is also evident that the bilingual n-gram model
used requires the additional feature functions to pro-
duce better translations. However, more experimen-
tation is required in order to fully understand each
individual feature?s influence on the overall log-
linear model performance.
6 Acknowledgments
This work has been funded by the European Union
under the integrated project TC-STAR - Technology
and Corpora for Speech to Speech Translation -(IST-
2002-FP6-506738, http://www.tc-star.org).
The authors also want to thank Jose? A. R. Fonol-
losa and Marta Ruiz Costa-jussa` for their participa-
tion in discussions related to this work.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. ?The mathemat-
1Differently from BLEU scores presented in Table 2, which
are case insensitive, BLEU scores presented in Table 3 are case
sensitive.
ics of statistical machine translation: parameter esti-
mation?. Computational Linguistics, 19(2):263?311.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2004. ?Finite-state-based and phrase-based statistical
machine translation?. Proc. of the 8th Int. Conf. on
Spoken Language Processing, :37?40, October.
Josep M. Crego, Jose? B. Marin?o, and Adria` de Gispert.
2005. ?A Ngram-based Statistical Machine Transla-
tion Decoder?. Submitted to INTERSPEECH 2005.
Adria` de Gispert, and Jose? B. Marin?o. 2002. ?Using X-
grams for speech-to-speech translation?. Proc. of the
7th Int. Conf. on Spoken Language Processing.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego.
2004. ?TALP: Xgram-based spoken language transla-
tion system?. Proc. of the Int. Workshop on Spoken
Language Translation, :85?90. Kyoto, Japan, October.
EUPARL: European Parliament Proceedings Parallel
Corpus 1996-2003. Available on-line at: http://
people.csail.mit.edu/people/koehn/public
ations/europarl/
Philipp Koehn. 2002. ?Europarl: A Multilingual Cor-
pus for Evaluation of Machine Translation?. Avail-
able on-line at: http://people.csail.mit.edu/
people/koehn/publications/europarl/
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
?Statistical phrase-based translation?. Proc. of the
2003 Meeting of the North American chapter of the
ACL, Edmonton, Alberta.
Franz J. Och and Hermann Ney. 2000. ?Improved statis-
tical alignment models?. Proc. of the 38th Ann. Meet-
ing of the ACL, Hong Kong, China, October.
Franz J. Och and Hermann Ney. 2002. ?Discriminative
training and maximum entropy models for statistical
machine translation?. Proc. of the 40th Ann. Meeting
of the ACL, :295?302, Philadelphia, PA, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. ?Bleu: a method for automatic eval-
uation of machine translation?. Proc. of the 40th Ann.
Conf. of the ACL, Philadelphia, PA, July.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++: the Art of Scientific Computing, Cambridge
University Press.
Andreas Stolcke. 2002. ?SRLIM: an extensible language
modeling toolkit?. Proc. of the Int. Conf. on Spoken
Language Processing :901?904, Denver, CO, Septem-
ber. Available on line at: http://www.speech.sr
i.com/projects/srilm/
136
Proceedings of the Workshop on Statistical Machine Translation, pages 1?6,
New York City, June 2006. c?2006 Association for Computational Linguistics
Morpho-syntactic Information for Automatic Error Analysis of Statistical
Machine Translation Output
Maja Popovic??
Hermann Ney?
Adria` de Gispert?
Jose? B. Marin?o?
Deepa Gupta?
Marcello Federico?
Patrik Lambert?
Rafael Banchs?
? Lehrstuhl fu?r Informatik VI - Computer Science Department, RWTH Aachen University, Aachen, Germany
? TALP Research Center, Universitat Polite`cnica de Catalunya (UPC), Barcelona, Spain
? ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy
{popovic,ney}@informatik.rwth-aachen.de {agispert,canton}@gps.tsc.upc.es
{gupta,federico}@itc.it {lambert,banchs}@gps.tsc.upc.es
Abstract
Evaluation of machine translation output
is an important but difficult task. Over the
last years, a variety of automatic evalua-
tion measures have been studied, some of
them like Word Error Rate (WER), Posi-
tion Independent Word Error Rate (PER)
and BLEU and NIST scores have become
widely used tools for comparing different
systems as well as for evaluating improve-
ments within one system. However, these
measures do not give any details about
the nature of translation errors. Therefore
some analysis of the generated output is
needed in order to identify the main prob-
lems and to focus the research efforts. On
the other hand, human evaluation is a time
consuming and expensive task. In this
paper, we investigate methods for using
of morpho-syntactic information for auto-
matic evaluation: standard error measures
WER and PER are calculated on distinct
word classes and forms in order to get a
better idea about the nature of translation
errors and possibilities for improvements.
1 Introduction
The evaluation of the generated output is an impor-
tant issue for all natural language processing (NLP)
tasks, especially for machine translation (MT). Au-
tomatic evaluation is preferred because human eval-
uation is a time consuming and expensive task.
A variety of automatic evaluation measures have
been proposed and studied over the last years, some
of them are shown to be a very useful tool for com-
paring different systems as well as for evaluating
improvements within one system. The most widely
used are Word Error Rate (WER), Position Indepen-
dent Word Error Rate (PER), the BLEU score (Pap-
ineni et al, 2002) and the NIST score (Doddington,
2002). However, none of these measures give any
details about the nature of translation errors. A rela-
tionship between these error measures and the actual
errors in the translation outputs is not easy to find.
Therefore some analysis of the translation errors is
necessary in order to define the main problems and
to focus the research efforts. A framework for hu-
man error analysis and error classification has been
proposed in (Vilar et al, 2006), but like human eval-
uation, this is also a time consuming task.
The goal of this work is to present a framework
for automatic error analysis of machine translation
output based on morpho-syntactic information.
2 Related Work
There is a number of publications dealing with
various automatic evaluation measures for machine
translation output, some of them proposing new
measures, some proposing improvements and exten-
sions of the existing ones (Doddington, 2002; Pap-
ineni et al, 2002; Babych and Hartley, 2004; Ma-
tusov et al, 2005). Semi-automatic evaluation mea-
sures have been also investigated, for example in
(Nie?en et al, 2000). An automatic metric which
uses base forms and synonyms of the words in or-
der to correlate better to human judgements has been
1
proposed in (Banerjee and Lavie, 2005). However,
error analysis is still a rather unexplored area. A
framework for human error analysis and error clas-
sification has been proposed in (Vilar et al, 2006)
and a detailed analysis of the obtained results has
been carried out. Automatic methods for error anal-
ysis to our knowledge have not been studied yet.
Many publications propose the use of morpho-
syntactic information for improving the perfor-
mance of a statistical machine translation system.
Various methods for treating morphological and
syntactical differences between German and English
are investigated in (Nie?en and Ney, 2000; Nie?en
and Ney, 2001a; Nie?en and Ney, 2001b). Mor-
phological analysis has been used for improving
Arabic-English translation (Lee, 2004), for Serbian-
English translation (Popovic? et al, 2005) as well as
for Czech-English translation (Goldwater and Mc-
Closky, 2005). Inflectional morphology of Spanish
verbs is dealt with in (Popovic? and Ney, 2004; de
Gispert et al, 2005). To the best of our knowledge,
the use of morpho-syntactic information for error
analysis of translation output has not been investi-
gated so far.
3 Morpho-syntactic Information and
Automatic Evaluation
We propose the use of morpho-syntactic informa-
tion in combination with the automatic evaluation
measures WER and PER in order to get more details
about the translation errors.
We investigate two types of potential problems for
the translation with the Spanish-English language
pair:
? syntactic differences between the two lan-
guages considering nouns and adjectives
? inflections in the Spanish language considering
mainly verbs, adjectives and nouns
As any other automatic evaluation measures,
these novel measures will be far from perfect. Pos-
sible POS-tagging errors may introduce additional
noise. However, we expect this noise to be suffi-
ciently small and the new measures to be able to give
sufficiently clear ideas about particular errors.
3.1 Syntactic differences
Adjectives in the Spanish language are usually
placed after the corresponding noun, whereas in En-
glish is the other way round. Although in most cases
the phrase based translation system is able to han-
dle these local permutations correctly, some errors
are still present, especially for unseen or rarely seen
noun-adjective groups. In order to investigate this
type of errors, we extract the nouns and adjectives
from both the reference translations and the sys-
tem output and then calculate WER and PER. If the
difference between the obtained WER and PER is
large, this indicates reordering errors: a number of
nouns and adjectives is translated correctly but in the
wrong order.
3.2 Spanish inflections
Spanish has a rich inflectional morphology, espe-
cially for verbs. Person and tense are expressed
by the suffix so that many different full forms of
one verb exist. Spanish adjectives, in contrast to
English, have four possible inflectional forms de-
pending on gender and number. Therefore the er-
ror rates for those word classes are expected to be
higher for Spanish than for English. Also, the er-
ror rates for the Spanish base forms are expected to
be lower than for the full forms. In order to investi-
gate potential inflection errors, we compare the PER
for verbs, adjectives and nouns for both languages.
For the Spanish language, we also investigate differ-
ences between full form PER and base form PER:
the larger these differences, more inflection errors
are present.
4 Experimental Settings
4.1 Task and Corpus
The corpus analysed in this work is built in the
framework of the TC-Star project. It contains more
than one million sentences and about 35 million run-
ning words of the Spanish and English European
Parliament Plenary Sessions (EPPS). A description
of the EPPS data can be found in (Vilar et al, 2005).
In order to analyse effects of data sparseness, we
have randomly extracted a small subset referred to
as 13k containing about thirteen thousand sentences
and 370k running words (about 1% of the original
2
Training corpus: Spanish English
full Sentences 1281427
Running Words 36578514 34918192
Vocabulary 153124 106496
Singletons [%] 35.2 36.2
13k Sentences 13360
Running Words 385198 366055
Vocabulary 22425 16326
Singletons [%] 47.6 43.7
Dev: Sentences 1008
Running Words 25778 26070
Distinct Words 3895 3173
OOVs (full) [%] 0.15 0.09
OOVs (13k) [%] 2.7 1.7
Test: Sentences 840 1094
Running Words 22774 26917
Distinct Words 4081 3958
OOVs (full) [%] 0.14 0.25
OOVs (13k) [%] 2.8 2.6
Table 1: Corpus statistics for the Spanish-English
EPPS task (running words include punctuation
marks)
corpus). The statistics of the corpora can be seen in
Table 1.
4.2 Translation System
The statistical machine translation system used in
this work is based on a log-linear combination of
seven different models. The most important ones are
phrase based models in both directions, additionally
IBM1 models at the phrase level in both directions
as well as phrase and length penalty are used. A
more detailed description of the system can be found
in (Vilar et al, 2005; Zens et al, 2005).
4.3 Experiments
The translation experiments have been done in both
translation directions on both sizes of the corpus. In
order to examine improvements of the baseline sys-
tem, a new system with POS-based word reorderings
of nouns and adjectives as proposed in (Popovic? and
Ney, 2006) is also analysed. Adjectives in the Span-
ish language are usually placed after the correspond-
ing noun, whereas for English it is the other way
round. Therefore, local reorderings of nouns and ad-
Spanish?English WER PER BLEU
full baseline 34.5 25.5 54.7
reorder 33.5 25.2 56.4
13k baseline 41.8 30.7 43.2
reorder 38.9 29.5 48.5
English?Spanish WER PER BLEU
full baseline 39.7 30.6 47.8
reorder 39.6 30.5 48.3
13k baseline 49.6 37.4 36.2
reorder 48.1 36.5 37.7
Table 2: Translation Results [%]
jective groups in the source language have been ap-
plied. If the source language is Spanish, each noun is
moved behind the corresponding adjective group. If
the source language is English, each adjective group
is moved behind the corresponding noun. An adverb
followed by an adjective (e.g. ?more important?) or
two adjectives with a coordinate conjunction in be-
tween (e.g. ?economic and political?) are treated as
an adjective group. Standard translation results are
presented in Table 2.
5 Error Analysis
5.1 Syntactic errors
As explained in Section 3.1, reordering errors due
to syntactic differences between two languages have
been measured by the relative difference between
WER and PER calculated on nouns and adjectives.
Corresponding relative differences are calculated
also for verbs as well as adjectives and nouns sep-
arately.
Table 3 presents the relative differences for the
English and Spanish output. It can be seen that
the PER/WER difference for nouns and adjectives
is relatively high for both language pairs (more than
20%), and for the English output is higher than for
the Spanish one. This corresponds to the fact that
the Spanish language has a rather free word order:
although the adjective usually is placed behind the
noun, this is not always the case. On the other hand,
adjectives in English are always placed before the
corresponding noun. It can also be seen that the
difference is higher for the reduced corpus for both
outputs indicating that the local reordering problem
3
English output 1? PERWER
full nouns+adjectives 24.7
+reordering 20.8
verbs 4.1
adjectives 10.2
nouns 20.1
13k nouns+adjectives 25.7
+reordering 20.1
verbs 4.6
adjectives 8.4
nouns 19.1
Spanish output 1? PERWER
full nouns+adjectives 21.5
+reordering 20.3
verbs 3.3
adjectives 5.6
nouns 16.9
13k nouns+adjectives 22.9
+reordering 19.8
verbs 3.9
adjectives 5.4
nouns 19.3
Table 3: Relative difference between PER and
WER [%] for different word classes
is more important when only small amount of train-
ing data is available. As mentioned in Section 3.1,
the phrase based translation system is able to gen-
erate frequent noun-adjective groups in the correct
word order, but unseen or rarely seen groups intro-
duce difficulties.
Furthermore, the results show that the POS-based
reordering of adjectives and nouns leads to a de-
crease of the PER/WER difference for both out-
puts and for both corpora. Relative decrease of the
PER/WER difference is larger for the small corpus
than for the full corpus. It can also be noted that the
relative decrease for both corpora is larger for the
English output than for the Spanish one due to free
word order - since the Spanish adjective group is not
always placed behind the noun, some reorderings in
English are not really needed.
For the verbs, PER/WER difference is less than
5% for both outputs and both training corpora, in-
dicating that the word order of verbs is not an im-
English output PER
full verbs 44.8
adjectives 27.3
nouns 23.0
13k verbs 56.1
adjectives 38.1
nouns 31.7
Spanish output PER
full verbs 61.4
adjectives 41.8
nouns 28.5
13k verbs 73.0
adjectives 50.9
nouns 37.0
Table 4: PER [%] for different word classes
portant issue for the Spanish-English language pair.
PER/WER difference for adjectives and nouns is
higher than for verbs, for the nouns being signifi-
cantly higher than for adjectives. The reason for this
is probably the fact that word order differences in-
volving only the nouns are also present, for example
?export control = control de exportacio?n?.
5.2 Inflectional errors
Table 4 presents the PER for different word classes
for the English and Spanish output respectively. It
can be seen that all PERs are higher for the Spanish
output than for the English one due to the rich in-
flectional morphology of the Spanish language. It
can be also seen that the Spanish verbs are espe-
cially problematic (as stated in (Vilar et al, 2006))
reaching 60% of PER for the full corpus and more
than 70% for the reduced corpus. Spanish adjectives
also have a significantly higher PER than the English
ones, whereas for the nouns this difference is not so
high.
Results of the further analysis of inflectional er-
rors are presented in Table 5. Relative difference
between full form PER and base form PER is sig-
nificantly lower for adjectives and nouns than for
verbs, thus showing that the verb inflections are the
main source of translation errors into the Spanish
language.
Furthermore, it can be seen that for the small cor-
4
Spanish output 1? PERbPERf
full verbs 26.9
adjectives 9.3
nouns 8.4
13k verbs 23.7
adjectives 15.1
nouns 6.5
Table 5: Relative difference between PER of base
forms and PER of full forms [%] for the Spanish
output
pus base/full PER difference for verbs and nouns is
basically the same as for the full corpus. Since nouns
in Spanish only have singular and plural form as in
English, the number of unseen forms is not partic-
ularly enlarged by the reduction of the training cor-
pus. On the other hand, base/full PER difference of
adjectives is significantly higher for the small corpus
due to an increased number of unseen adjective full
forms.
As for verbs, intuitively it might be expected that
the number of inflectional errors for this word class
also increases by reducing the training corpus, even
more than for adjectives. However, the base/full
PER difference is not larger for the small corpus,
but even smaller. This is indicating that the problem
of choosing the right inflection of a Spanish verb ap-
parently is not related to the number of unseen full
forms since the number of inflectional errors is very
high even when the translation system is trained on
a very large corpus.
6 Conclusion
In this work, we presented a framework for auto-
matic analysis of translation errors based on the use
of morpho-syntactic information. We carried out a
detailed analysis which has shown that the results
obtained by our method correspond to those ob-
tained by human error analysis in (Vilar et al, 2006).
Additionally, it has been shown that the improve-
ments of the baseline system can be adequately mea-
sured as well.
This work is just a first step towards the devel-
opment of linguistically-informed evaluation mea-
sures which provide partial and more specific infor-
mation of certain translation problems. Such mea-
sures are very important to understand what are the
weaknesses of a statistical machine translation sys-
tem, and what are the best ways and methods for
improvements.
For our future work, we plan to extend the pro-
posed measures in order to carry out a more de-
tailed error analysis, for example examinating dif-
ferent types of inflection errors for Spanish verbs.
We also plan to investigate other types of translation
errors and other language pairs.
Acknowledgements
This work was partly supported by the TC-STAR
project by the European Community (FP6-506738)
and partly by the Generalitat de Catalunya and the
European Social Fund.
References
Bogdan Babych and Anthony Hartley. 2004. Extending
bleu mt evaluation method with frequency weighting.
In Proc. of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Barcelona,
Spain, July.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for mt evaluation with improved
correlation with human judgements. In 43rd Annual
Meeting of the Assoc. for Computational Linguistics:
Proc. Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, pages 65?72,
Ann Arbor, MI, June.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego.
2005. Improving statistical machine translation by
classifying and generalizing inflected verb forms. In
Proc. of the 9th European Conf. on Speech Commu-
nication and Technology (Interspeech), pages 3185?
3188, Lisbon, Portugal, September.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology, pages 128?132, San Diego.
Sharon Goldwater and David McClosky. 2005. Improv-
ing stastistical machine translation through morpho-
logical analysis. In Proc. of the Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
Vancouver, Canada, October.
Young-suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In Proc. 2004 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), Boston, MA, May.
5
Evgeny Matusov, Gregor Leusch, Oliver Bender, and
Hermann Ney. 2005. Evaluating machine transla-
tion output with automatic sentence segmentation. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 148?154, Pitts-
burgh, PA, October.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In COLING
?00: The 18th Int. Conf. on Computational Linguistics,
pages 1081?1085, Saarbru?cken, Germany, July.
Sonja Nie?en and Hermann Ney. 2001a. Morpho-
syntactic analysis for reordering in statistical machine
translation. In Proc. MT Summit VIII, pages 247?252,
Santiago de Compostela, Galicia, Spain, September.
Sonja Nie?en and Hermann Ney. 2001b. Toward hier-
archical models for statistical machine translation of
inflected languages. In Data-Driven Machine Trans-
lation Workshop, pages 47?54, Toulouse, France, July.
Sonja Nie?en, Franz J. Och, Gregor Leusch, and Her-
mann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research. In
Proc. Second Int. Conf. on Language Resources and
Evaluation (LREC), pages 39?45, Athens, Greece,
May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Maja Popovic? and Hermann Ney. 2004. Towards the use
of word stems & suffixes for statistical machine trans-
lation. In Proc. 4th Int. Conf. on Language Resources
and Evaluation (LREC), pages 1585?1588, Lissabon,
Portugal, May.
Maja Popovic? and Hermann Ney. 2006. POS-based
word reorderings for statistical machine translation. In
Proc. of the Fifth Int. Conf. on Language Resources
and Evaluation (LREC), Genova, Italy, May.
Maja Popovic?, David Vilar, Hermann Ney, Slobodan
Jovic?ic?, and Zoran ?Saric?. 2005. Augmenting a small
parallel text with morpho-syntactic language resources
for Serbian?English statistical machine translation. In
43rd Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Building and Using
Parallel Texts: Data-Driven Machine Translation and
Beyond, pages 41?48, Ann Arbor, MI, June.
David Vilar, Evgeny Matusov, Sas?a Hasan, Richard Zens,
and Hermann Ney. 2005. Statistical machine transla-
tion of european parliamentary speeches. In Proc. MT
Summit X, pages 259?266, Phuket, Thailand, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error analysis of statistical machine
translation output. In Proc. of the Fifth Int. Conf. on
Language Resources and Evaluation (LREC), page to
appear, Genova, Italy, May.
Richard Zens, Oliver Bender, Sas?a Hasan, Shahram
Khadivi, Evgeny Matusov, Jia Xu, Yuqi Zhang, and
Hermann Ney. 2005. The RWTH phrase-based statis-
tical machine translation system. In Proceedings of the
International Workshop on Spoken Language Transla-
tion (IWSLT), pages 155?162, Pittsburgh, PA, October.
6
Proceedings of the Workshop on Statistical Machine Translation, pages 142?145,
New York City, June 2006. c?2006 Association for Computational Linguistics
TALP Phrase-based statistical translation system for European language
pairs
Marta R. Costa-jussa`
Patrik Lambert
Jose? B. Marin?o
Josep M. Crego
Maxim Khalilov
Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,jmcrego,agispert,lambert,khalilov,canton,adrian, rbanchs)@gps.tsc.upc.edu
Adria` de Gispert
Rafael E. Banchs
Abstract
This paper reports translation results for
the ?Exploiting Parallel Texts for Statis-
tical Machine Translation? (HLT-NAACL
Workshop on Parallel Texts 2006). We
have studied different techniques to im-
prove the standard Phrase-Based transla-
tion system. Mainly we introduce two re-
ordering approaches and add morphologi-
cal information.
1 Introduction
Nowadays most Statistical Machine Translation
(SMT) systems use phrases as translation units. In
addition, the decision rule is commonly modelled
through a log-linear maximum entropy framework
which is based on several feature functions (in-
cluding the translation model), hm. Each feature
function models the probability that a sentence e in
the target language is a translation of a given sen-
tence f in the source language. The weights, ?i,
of each feature function are typically optimized to
maximize a scoring function. It has the advantage
that additional features functions can be easily in-
tegrated in the overall system.
This paper describes a Phrase-Based system
whose baseline is similar to the system in Costa-
jussa` and Fonollosa (2005). Here we introduce
two reordering approaches and add morphological
information. Translation results for all six trans-
lation directions proposed in the shared task are
presented and discussed. More specifically, four
different languages are considered: English (en),
Spanish (es), French (fr) and German (de); and
both translation directions are considered for the
pairs: EnEs, EnFr, and EnDe. The paper is orga-
nized as follows: Section 2 describes the system;
0This work has been supported by the European Union
under grant FP6-506738 (TC-STAR project) and the TALP
Research Center (under a TALP-UPC-Recerca grant).
Section 3 presents the shared task results; and, fi-
nally, in Section 4, we conclude.
2 System Description
This section describes the system procedure fol-
lowed for the data provided.
2.1 Alignment
Given a bilingual corpus, we use GIZA++ (Och,
2003) as word alignment core algorithm. During
word alignment, we use 50 classes per language
estimated by ?mkcls?, a freely-available tool along
with GIZA++. Before aligning we work with low-
ercase text (which leads to an Alignment Error
Rate reduction) and we recover truecase after the
alignment is done.
In addition, the alignment (in specific pairs of
languages) was improved using two strategies:
Full verb forms The morphology of the verbs
usually differs in each language. Therefore, it is
interesting to classify the verbs in order to address
the rich variety of verbal forms. Each verb is re-
duced into its base form and reduced POS tag as
explained in (de Gispert, 2005). This transforma-
tion is only done for the alignment, and its goal
is to simplify the work of the word alignment im-
proving its quality.
Block reordering (br) The difference in word
order between two languages is one of the most
significant sources of error in SMT. Related works
either deal with reordering in general as (Kanthak
et al, 2005) or deal with local reordering as (Till-
mann and Ney, 2003). We report a local reorder-
ing technique, which is implemented as a pre-
processing stage, with two applications: (1) to im-
prove only alignment quality, and (2) to improve
alignment quality and to infer reordering in trans-
lation. Here, we present a short explanation of the
algorithm, for further details see Costa-jussa` and
Fonollosa (2006).
142
Figure 1: Example of an Alignment Block, i.e. a
pair of consecutive blocks whose target translation
is swapped
This reordering strategy is intended to infer the
most probable reordering for sequences of words,
which are referred to as blocks, in order to mono-
tonize current data alignments and generalize re-
ordering for unseen pairs of blocks.
Given a word alignment, we identify those pairs
of consecutive source blocks whose translation is
swapped, i.e. those blocks which, if swapped,
generate a correct monotone translation. Figure 1
shows an example of these pairs (hereinafter called
Alignment Blocks).
Then, the list of Alignment Blocks (LAB) is
processed in order to decide whether two consec-
utive blocks have to be reordered or not. By using
the classification algorithm, see the Appendix, we
divide the LAB in groups (Gn, n = 1 . . . N ). In-
side the same group, we allow new internal com-
bination in order to generalize the reordering to
unseen pairs of blocks (i.e. new Alignment Blocks
are created). Based on this information, the source
side of the bilingual corpora are reordered.
In case of applying the reordering technique for
purpose (1), we modify only the source training
corpora to realign and then we recover the origi-
nal order of the training corpora. In case of using
Block Reordering for purpose (2), we modify all
the source corpora (both training and test), and we
use the new training corpora to realign and build
the final translation system.
2.2 Phrase Extraction
Given a sentence pair and a corresponding word
alignment, phrases are extracted following the cri-
terion in Och and Ney (2004). A phrase (or
bilingual phrase) is any pair of m source words
and n target words that satisfies two basic con-
straints: words are consecutive along both sides
of the bilingual phrase, and no word on either side
of the phrase is aligned to a word out of the phrase.
We limit the maximum size of any given phrase to
7. The huge increase in computational and storage
cost of including longer phrases does not provide
a significant improvement in quality (Koehn et al,
2003) as the probability of reappearance of larger
phrases decreases.
2.3 Feature functions
Conditional and posterior probability (cp, pp)
Given the collected phrase pairs, we estimate the
phrase translation probability distribution by rela-
tive frequency in both directions.
The target language model (lm) consists of an
n-gram model, in which the probability of a trans-
lation hypothesis is approximated by the product
of word n-gram probabilities. As default language
model feature, we use a standard word-based 5-
gram language model generated with Kneser-Ney
smoothing and interpolation of higher and lower
order n-grams (Stolcke, 2002).
The POS target language model (tpos) con-
sists of an N-gram language model estimated over
the same target-side of the training corpus but us-
ing POS tags instead of raw words.
The forward and backwards lexicon mod-
els (ibm1, ibm1?1) provide lexicon translation
probabilities for each phrase based on the word
IBM model 1 probabilities. For computing the
forward lexicon model, IBM model 1 probabili-
ties from GIZA++ source-to-target algnments are
used. In the case of the backwards lexicon model,
target-to-source alignments are used instead.
The word bonus model (wb) introduces a sen-
tence length bonus in order to compensate the sys-
tem preference for short output sentences.
The phrase bonus model (pb) introduces a con-
stant bonus per produced phrase.
2.4 Decoding
The search engine for this translation system is de-
scribed in Crego et al (2005) which takes into ac-
count the features described above.
Using reordering in the decoder (rgraph) A
highly constrained reordered search is performed
by means of a set of reordering patterns (linguisti-
cally motivated rewrite patterns) which are used to
143
extend the monotone search graph with additional
arcs. See the details in Crego et al (2006).
2.5 Optimization
It is based on a simplex method (Nelder and
Mead, 1965). This algorithm adjusts the log-
linear weights in order to maximize a non-linear
combination of translation BLEU and NIST: 10 ?
log10((BLEU ? 100) + 1) + NIST. The max-
imization is done over the provided development
set for each of the six translation directions under
consideration. We have experimented an improve-
ment in the coherence between all the automatic
figures by integrating two of these figures in the
optimization function.
3 Shared Task Results
3.1 Data
The data provided for this shared task corresponds
to a subset of the official transcriptions of the
European Parliament Plenary Sessions, and it
is available through the shared task website at:
http://www.statmt.org/wmt06/shared-task/.
The development set used to tune the system
consists of a subset (500 first sentences) of the
official development set made available for the
Shared Task.
We carried out a morphological analysis of the
data. The English POS-tagging has been carried
out using freely available TNT tagger (Brants,
2000). In the Spanish case, we have used the
Freeling (Carreras et al, 2004) analysis tool
which generates the POS-tagging for each input
word.
3.2 Systems configurations
The baseline system is the same for all tasks and
includes the following features functions: cp, pp,
lm, ibm1, ibm1?1, wb, pb. The POStag target
language model has been used in those tasks for
which the tagger was available. Table 1 shows the
reordering configuration used for each task.
The Block Reordering (application 2) has been
used when the source language belongs to the Ro-
manic family. The length of the block is lim-
ited to 1 (i.e. it allows the swapping of single
words). The main reason is that specific errors are
solved in the tasks from a Romanic language to
a Germanic language (as the common reorder of
Noun + Adjective that turns into Adjective +
Noun). Although the Block Reordering approach
Task Reordering Configuration
Es2En br2
En2Es br1 + rgraph
Fr2En br2
En2Fr br1 + rgraph
De2En -
En2De -
Table 1: Additional reordering models for each
task: br1 (br2) stands for Block Reordering ap-
plication 1 (application 2); and rgraph refers to
the reordering integrated in the decoder
does not depend on the task, we have not done
the corresponding experiments to observe its ef-
ficiency in all the pairs used in this evaluation.
The rgraph has been applied in those cases
where: we do not use br2 (there is no sense in
applying them simultaneously); and we have the
tagger for the source language model available.
In the case of the pair GeEn, we have not exper-
imented any reordering, we left the application of
both reordering approaches as future work.
3.3 Discussion
Table 2 presents the BLEU scores evaluated on the
test set (using TRUECASE) for each configuration.
The official results were slightly better because a
lowercase evaluation was used, see (Koehn and
Monz, 2006).
For both, Es2En and Fr2En tasks, br helps
slightly. The improvement of the approach de-
pends on the quality of the alignment. The better
alignments allow to extract higher quality Align-
ment Blocks (Costa-jussa` and Fonollosa, 2006).
The En2Es task is improved when adding both
br1 and rgraph. Similarly, the En2Fr task seems to
perform fairly well when using the rgraph. In this
case, the improvement of the approach depends on
the quality of the alignment patterns (Crego et al,
2006). However, it has the advantage of delay-
ing the final decision of reordering to the overall
search, where all models are used to take a fully
informed decision.
Finally, the tpos does not help much when trans-
lating to English. It is not surprising because it was
used in order to improve the gender and number
agreement, and in English there is no need. How-
ever, in the direction to Spanish, the tpos added
to the corresponding reordering helps more as the
Spanish language has gender and number agree-
ment.
144
Task Baseline +tpos +rc +tpos+rc
Es2En 29.08 29.08 29.89 29.98
En2Es 27.73 27.66 28.79 28.99
Fr2En 27.05 27.06 27.43 27.23
En2Fr 26.16 - 27.80 -
De2En 21.59 21.33 - -
En2De 15.20 - - -
Table 2: Results evaluated using TRUECASE on
the test set for each conguration: rc stands for
Reordering Conguration and refers to Table 1.
The bold results were the congurations submit-
ted.
4 Conclusions
Reordering is important when using a Phrase-
Based system. Although local reordering is sup-
posed to be included in the phrase structure, per-
forming local reordering improves the translation
quality. In fact, local reordering, provided by the
reordering approaches, allows for those general-
izations which phrases could not achieve. Re-
ordering in the DeEn task is left as further work.
References
T. Brants. 2000. Tnt - a statistical part-of-speech tag-
ger. Proceedings of the Sixth Applied Natural Lan-
guage Processing.
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An open-source suite of language analyz-
ers. 4th Int. Conf. on Language Resources and Eval-
uation, LREC?04.
M. R. Costa-jussa` and J.A.R. Fonollosa. 2005. Im-
proving the phrase-based statistical translation by
modifying phrase extraction and including new fea-
tures. Proceedings of the ACL Workshop on Build-
ing and Using Parallel Texts: Data-Driven Machine
Translation and Beyond.
M. R. Costa-jussa` and J.A.R. Fonollosa. 2006. Using
reordering in statistical machine translation based on
alignment block classification. Internal Report.
J.M. Crego, J. Marin?o, and A. de Gispert. 2005.
An Ngram-based statistical machine translation de-
coder. Proc. of the 9th Int. Conf. on Spoken Lan-
guage Processing, ICSLP?05.
J. M. Crego, A. de Gispert, P. Lambert, M. R.
Costa-jussa`, M. Khalilov, J. Marin?o, J. A. Fonol-
losa, and R. Banchs. 2006. Ngram-based smt
system enhanced with reordering patterns. HLT-
NAACL06 Workshop on Building and Using Paral-
lel Texts: Data-Driven Machine Translation and Be-
yond, June.
A. de Gispert. 2005. Phrase linguistic classification for
improving statistical machine translation. ACL 2005
Students Workshop, June.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H.
Ney. 2005. Novel reordering approaches in phrase-
based statistical machine translation. Proceedings
of the ACL Workshop on Building and Using Par-
allel Texts: Data-Driven Machine Translation and
Beyond, pages 167?174, June.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between european
languages. June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. Proc. of the Human Lan-
guage Technology Conference, HLT-NAACL?2003,
May.
J.A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7:308?313.
F.J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449, December.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language mod-
eling toolkit. Proc. of the 7th Int. Conf. on Spoken
Language Processing, ICSLP?02, September.
C. Tillmann and H. Ney. 2003. Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 29(1):97?133, March.
A Appendix
Here we describe the classification algorithm used
in Section 1.
1. Initialization: set n? 1 and LAB ? ? LAB.
2. Main part: while LAB ? is not empty do
? Gn = {(?k, ?k)} where (?k, ?k) is any
element of LAB ?, i.e. ?k is the first
block and ?k is the second block of the
Alignment Block k of the LAB ?.
? Recursively, move elements (?i, ?i)
from LAB? to Gn if there is an element
(?j , ?j) ? Gn such that ?i = ?j or
?i = ?j
? Increase n (i.e. n? n + 1)
3. Ending: For each Gn, construct the two sets
An and Bn which consists on the first and
second element of the pairs in Gn, respec-
tively.
145
Proceedings of the Workshop on Statistical Machine Translation, pages 162?165,
New York City, June 2006. c?2006 Association for Computational Linguistics
N-gram-based SMT System Enhanced with Reordering Patterns
Josep M. Crego
Marta R. Costa-jussa`
Jose? B. Marin?o
Adria` de Gispert
Maxim Khalilov
Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
{jmcrego,agispert,lambert,mruiz,khalilov,rbanchs,canton,adrian}@gps.tsc.upc.edu
Patrik Lambert
Rafael E. Banchs
Abstract
This work presents translation results for
the three data sets made available in the
shared task ?Exploiting Parallel Texts for
Statistical Machine Translation? of the
HLT-NAACL 2006 Workshop on Statisti-
cal Machine Translation. All results pre-
sented were generated by using the N-
gram-based statistical machine translation
system which has been enhanced from the
last year?s evaluation with a tagged target
language model (using Part-Of-Speech
tags). For both Spanish-English transla-
tion directions and the English-to-French
translation task, the baseline system al-
lows for linguistically motivated source-
side reorderings.
1 Introduction
The statistical machine translation approach used
in this work implements a log-linear combination
of feature functions along with a translation model
which is based on bilingual n-grams (de Gispert and
Marin?o, 2002).
This translation model differs from the well
known phrase-based translation approach (Koehn
et al, 2003) in two basic issues: first, training data
is monotonously segmented into bilingual units; and
second, the model considers n-gram probabilities in-
stead of relative frequencies. This translation ap-
proach is described in detail in (Marin?o et al, 2005).
For those translation tasks with Spanish or En-
glish as target language, an additional tagged (us-
ing POS information) target language model is used.
Additionally a reordering strategy that includes POS
information is described and evaluated.
Translation results for all six translation directions
proposed in the shared task are presented and dis-
cussed. Both translation directions are considered
for the pairs: English-Spanish, English-French,
and English-German.
The paper is structured as follows: Section 2
briefly outlines the baseline system. Section 3 de-
scribes in detail the implemented POS-based re-
ordering strategy. Section 4 presents and discusses
the shared task results and, finally, section 5 presents
some conclusions and further work.
2 Baseline N-gram-based SMT System
As already mentioned, the translation model used
here is based on bilingual n-grams. It actually con-
stitutes a language model of bilingual units, referred
to as tuples, which approximates the joint probabil-
ity between source and target languages by using
bilingual n-grams (de Gispert and Marin?o, 2002).
Tuples are extracted from a word-to-word aligned
corpus according to the following two constraints:
first, tuple extraction should produce a monotonic
segmentation of bilingual sentence pairs; and sec-
ond, no smaller tuples can be extracted without vi-
olating the previous constraint. See (Crego et al,
2004) for further details.
For all experiments presented here, the translation
model consisted of a 4-gram language model of tu-
ples. In addition to this bilingual n-gram translation
model, the baseline system implements a log linear
combination of five feature functions.
162
These five additional models are:
? A target language model. 5-gram of the target
side of the bilingual corpus.
? A word bonus. Based on the number of tar-
get words in the partial-translation hypothesis,
to compensate the LM preference for short sen-
tences.
? A Source-to-target lexicon model. Based on
IBM Model 1 lexical parameters(Brown et al,
1993), providing a complementary probability
for each tuple in the translation table. These
parameters are obtained from source-to-target
alignments.
? A Target-to-source lexicon model. Analo-
gous to the previous feature, but obtained from
target-to-source alignments.
? A Tagged (POS) target language model. This
feature implements a 5-gram language model
of target POS-tags. In this case, each trans-
lation unit carried the information of its target
side POS-tags, though this is not used for trans-
lation model estimation (only in order to eval-
uate the target POS language model at decod-
ing time). Due to the non-availability of POS-
taggers for French and German, it was not pos-
sible to incorporate this feature in all transla-
tion tasks considered, being only used for those
translation tasks with Spanish and English as
target languages.
The search engine for this translation system is
described in (Crego et al, 2005) and implements
a beam-search strategy based on dynamic program-
ming, taking into account all feature functions de-
scribed above, along with the bilingual n-gram trans-
lation model. Monotone search is performed, in-
cluding histogram and threshold pruning and hy-
pothesis recombination.
An optimization tool, which is based on a down-
hill simplex method was developed and used for
computing log-linear weights for each of the feature
functions. This algorithm adjusts the weights so that
a non-linear combination of BLEU and NIST scores
is maximized over the development set for each of
the six translation directions considered.
This baseline system is actually very similar to
the system used for last year?s shared task ?Exploit-
ing Parallel Texts for Statistical Machine Transla-
tion? of ACL?05 Workshop on Building and Us-
ing Parallel Texts: Data-Driven Machine Translation
and Beyond (Banchs et al, 2005), whose results
are available at: http://www.statmt.org/wpt05/
mt-shared-task/. A more detailed description of
the system can be found in (2005).
The tools used for POS-tagging were Freel-
ing (Carreras et al, 2004) for Spanish and
TnT (Brants, 2000) for English. All language mod-
els were estimated using the SRI language mod-
eling toolkit. Word-to-word alignments were ex-
tracted with GIZA++. Improvements in word-to-
word alignments were achieved through verb group
classification as described in (de Gispert, 2005).
3 Reordering Framework
In this section we outline the reordering framework
used for the experiments (Crego and Marin?o, 2006).
A highly constrained reordered search is performed
by means of a set of reordering patterns (linguisti-
cally motivated rewrite patterns) which are used to
extend the monotone search graph with additional
arcs.
To extract patterns, we use the word-to-word
alignments (the union of both alignment directions)
and source-side POS tags. The main procedure con-
sists of identifying all crossings produced in the
Figure 1: Reordering patterns are extracted using
word-to-word alignments. The generalization power
is achieved through the POS tags. Three instances of
different patterns are extracted using the sentences
in the example.
163
word-to-word alignments. Once a crossing has been
detected, its source POS tags and alignments are
used to account for a new instance of pattern. The
target side of a pattern (source-side positions after
reordering), is computed using the original order
of the target words to which the source words are
aligned. See figure 1 for a clarifying example of
pattern extraction.
The monotone search graph is extended with re-
orderings following the patterns found in training.
The procedure identifies first the sequences of words
in the input sentence that match any available pat-
tern. Then, each of the matchings implies the ad-
dition of an arc into the search graph (encoding the
reordering learnt in the pattern). However, this ad-
dition of a new arc is not performed if a translation
unit with the same source-side words already exists
in the training. Figure 2 shows an example of the
procedure.
Figure 2: Three additional arcs have been added
to the original monotone graph (bold arcs) given
the reordering patterns found matching any of the
source POS tags sequence.
Once the search graph is built, the decoder tra-
verses the graph looking for the best translation.
Hence, the winner hypothesis is computed using
all the available information (the whole SMT mod-
els). The reordering strategy is additionally sup-
ported by a 5-gram language model of reordered
source POS-tags. In training, POS-tags are re-
ordered according with the extracted reordering pat-
terns and word-to-word links. The resulting se-
quence of source POS-tags are used to train the n-
gram LM.
Notice that this reordering framework has only
been used for some translation tasks (Spanish-
to-English, English-to-Spanish and English-to-
French). The reason is double: first, because we
did not have available a French POS-tagger. Second,
because the technique used to learn reorderings (de-
tailed below) does not seem to apply for language
pairs like German-English, because the agglutina-
tive characteristic of German (words are formed by
joining morphemes together).
Table 1: BLEU, NIST and mWER scores (com-
puted using two reference translations) obtained for
both translation directions (Spanish-to-English and
English-to-Spanish).
Conf BLEU NIST mWER
Spanish-to-English
base 55.23 10.69 34.40
+rgraph 55.59 10.70 34.23
+pos 56.39 10.75 33.75
English-to-Spanish
base 48.03 9.84 41.18
+rgraph 48.53 9.81 41.15
+pos 48.91 9.91 40.29
Table 1 shows the improvement of the original
baseline system described in section 2 (base), en-
hanced using reordering graphs (+rgraph) and pro-
vided the tagged-source language model (+pos).
The experiments in table 1 were not carried out over
the official corpus of this shared task. The Spanish-
English corpus of the TC-Star 2005 Evaluation was
used. Due to the high similarities between both cor-
pus (this shared task corpus consists of a subset of
the whole corpus used in the TC-Star 2005 Evalua-
tion), it makes sense to think that comparable results
would be obtained.
It is worth mentioning that the official corpus of
the shared task (HLT-NAACL 2006) was used when
building and tuning the present shared task system.
4 Shared Task Results
The data provided for this shared task corresponds
to a subset of the official transcriptions of the Euro-
pean Parliament Plenary Sessions. The development
set used to tune the system consists of a subset (500
first sentences) of the official development set made
available for the Shared Task.
164
Table 2 presents the BLEU, NIST and mWER
scores obtained for the development-test data set.
The last column shows whether the target POS lan-
guage model feature was used or not. Computed
scores are case sensitive and compare to one refer-
ence translation. Tasks in bold were conducted al-
lowing for the reordering framework. For French-
to-English task, block reordering strategy was used,
which is described in (Costa-jussa` et al, 2006). As it
can be seen, for the English-to-German task we did
not use any of the previous enhancements.
Table 2: Translation results
Task BLEU NIST mWER tPOS
en ? es 29.50 7.32 58.95 yes
es ? en 30.29 7.51 57.72 yes
en ? fr 30.23 7.40 59.76 no
fr ? en 30.21 7.61 56.97 yes
en ? de 17.40 5.61 71.18 no
de ? en 23.78 6.70 65.83 yes
Important differences can be observed between
the German-English and the rest of translation tasks.
They result from the greater differences in word
order present in this language pair (the German-
English results are obtained under monotone decod-
ing conditions). Also because the greater vocabulary
of words of German, which increases sparseness in
any task where German is envolved. As expected,
differences in translation accuracy between Spanish-
English and French-English are smaller.
5 Conclusions and Further Work
As it can be concluded from the presented results,
although in principle some language pairs (Spanish-
English-French) seem to have very little need for re-
orderings (due to their similar word order), the use
of linguistically-based reorderings proves to be use-
ful to improve translation accuracy.
Additional work is to be conducted to allow for
reorderings when translating from/to German.
6 Acknowledgments
This work was partly funded by the European Union
under the integrated project TC-STAR1: Technology
and Corpora for Speech to Speech Translation (IST-
2002-FP6-506738) and the European Social Fund.
1http://www.tc-star.org
References
R. E. Banchs, J. M. Crego, A. de Gispert, P. Lambert, and
J. B. Marin?o. 2005. Statistical machine translation of
euparl data by using bilingual n-grams. Proc. of the
ACL Workshop on Building and Using Parallel Texts
(ACL?05/Wkshp), pages 67?72, June.
T. Brants. 2000. TnT ? a statistical part-of-speech tag-
ger. In Proc. of the Sixth Applied Natural Language
Processing (ANLP-2000), Seattle, WA.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine transla-
tion. Computational Linguistics, 19(2):263?311.
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An open-source suite of language analyzers.
4th Int. Conf. on Language Resources and Evaluation,
LREC?04, May.
M.R. Costa-jussa`, J.M. Crego, A. de Gispert, P. Lam-
bert, M. Khalilov, R. Banchs, J.B. Marin?o, and J.A.R.
Fonollosa. 2006. Talp phrase-based statistical transla-
tion system for european language pairs. Proc. of the
HLT/NAACL Workshop on Statistical Machine Trans-
lation, June.
J. M. Crego and J. Marin?o. 2006. A reordering frame-
work for statistical machine translation. Internal Re-
port.
J. M. Crego, J. Marin?o, and A. de Gispert. 2004. Finite-
state-based and phrase-based statistical machine trans-
lation. Proc. of the 8th Int. Conf. on Spoken Language
Processing, ICSLP?04, pages 37?40, October.
J. M. Crego, J. Marin?o, and A. Gispert. 2005. An ngram-
based statistical machine translation decoder. Proc. of
the 9th European Conference on Speech Communica-
tion and Technology, Interspeech?05, September.
A. de Gispert and J. Marin?o. 2002. Using X-grams
for speech-to-speech translation. Proc. of the 7th
Int. Conf. on Spoken Language Processing, ICSLP?02,
September.
A. de Gispert. 2005. Phrase linguistic classification and
generalization for improving statistical machine trans-
lation. Proc. of the ACL Student Research Workshop
(ACL?05/SRW), June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. Proc. of the Human
Language Technology Conference, HLT-NAACL?2003,
May.
J.B. Marin?o, R Banchs, J.M. Crego, A. de Gispert,
P. Lambert, M. R. Costa-jussa`, and J.A.R. Fonollosa.
2005. Bilingual n?gram statistical machine transla-
tion. Proc. of the MT Summit X, September.
165
Proceedings of the Second Workshop on Statistical Machine Translation, pages 167?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Ngram-based statistical machine translation enhanced with multiple
weighted reordering hypotheses
Marta R. Costa-jussa`, Josep M. Crego, Patrik Lambert, Maxim Khalilov
Jose? A. R. Fonollosa, Jose? B. Marin?o and Rafael E. Banchs
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,jmcrego,lambert,khalilov,adrian,canton,rbanchs)@gps.tsc.upc.edu
Abstract
This paper describes the 2007 Ngram-based sta-
tistical machine translation system developed at
the TALP Research Center of the UPC (Uni-
versitat Polite`cnica de Catalunya) in Barcelona.
Emphasis is put on improvements and extensions
of the previous years system, being highlighted
and empirically compared. Mainly, these include
a novel word ordering strategy based on: (1) sta-
tistically monotonizing the training source cor-
pus and (2) a novel reordering approach based
on weighted reordering graphs. In addition, this
system introduces a target language model based
on statistical classes, a feature for out-of-domain
units and an improved optimization procedure.
The paper provides details of this system par-
ticipation in the ACL 2007 SECOND WORK-
SHOP ON STATISTICAL MACHINE TRANSLA-
TION. Results on three pairs of languages are
reported, namely from Spanish, French and Ger-
man into English (and the other way round) for
both the in-domain and out-of-domain tasks.
1 Introduction
Based on estimating a joint-probability model between
the source and the target languages, Ngram-based SMT
has proved to be a very competitive alternatively to
phrase-based and other state-of-the-art systems in previ-
ous evaluation campaigns, as shown in (Koehn and Monz,
2005; Koehn and Monz, 2006).
Given the challenge of domain adaptation, efforts have
been focused on improving strategies for Ngram-based
SMT which could generalize better. Specifically, a novel
reordering strategy is explored. It is based on extending
the search by using precomputed statistical information.
Results are promising while keeping computational ex-
penses at a similar level as monotonic search. Addition-
ally, a bonus for tuples from the out-of-domain corpus is
introduced, as well as a target language model based on
statistical classes. One of the advantages of working with
statistical classes is that they can easily be used for any
pair of languages.
This paper is organized as follows. Section 2 briefly
reviews last year?s system, including tuple definition and
extraction, translation model and feature functions, de-
coding tool and optimization criterion. Section 3 delves
into the word ordering problem, by contrasting last year
strategy with the novel weighted reordering input graph.
Section 4 focuses on new features: both tuple-domain
bonus and target language model based on classes. Later
on, Section 5 reports on all experiments carried out for
WMT 2007. Finally, Section 6 sums up the main conclu-
sions from the paper and discusses future research lines.
2 Baseline N-gram-based SMT System
The translation model is based on bilingual n-grams. It
actually constitutes a language model of bilingual units,
referred to as tuples, which approximates the joint proba-
bility between source and target languages by using bilin-
gual n-grams.
Tuples are extracted from a word-to-word aligned cor-
pus according to the following two constraints: first, tu-
ple extraction should produce a monotonic segmentation
of bilingual sentence pairs; and second, no smaller tuples
can be extracted without violating the previous constraint.
For all experiments presented here, the translation
model consisted of a 4-gram language model of tuples.
In addition to this bilingual n-gram translation model, the
baseline system implements a log linear combination of
four feature functions. These four additional models are:
a target language model (a 5-gram model of words);
a word bonus; a source-to-target lexicon model and a
target-to-source lexicon model, both features provide a
complementary probability for each tuple in the transla-
tion table.
The decoder (called MARIE) for this translation sys-
167
tem is based on a beam search 1.
This baseline system is actually the same system used
for the first shared task ?Exploiting Parallel Texts for Sta-
tistical Machine Translation? of the ACL 2005 Work-
shop on Building and Using Parallel Texts: Data-Driven
Machine Translation and Beyond. A more detailed de-
scription of the system can be found in (Marin?o et al,
2006).
3 Baseline System Enhanced with a
Weighted Reordering Input Graph
This section briefly describes the statistical machine re-
ordering (SMR) technique. Further details on the archi-
tecture of SMR system can be found on (Costa-jussa` and
Fonollosa, 2006).
3.1 Concept
The SMR system can be seen as a SMT system which
translates from an original source language (S) to a re-
ordered source language (S?), given a target language
(T). The SMR technique works with statistical word
classes (Och, 1999) instead of words themselves (partic-
ularly, we have used 200 classes in all experiments).
Figure 1: SMR approach in the (A) training step (B) in
the test step (the weight of each arch is in brackets).
3.2 Using SMR technique to improve SMT training
The original source corpus S is translated into the re-
ordered source corpus S? with the SMR system. Fig-
ure 1 (A) shows the corresponding block diagram. The
reordered training source corpus and the original training
target corpus are used to build the SMT system.
The main difference here is that the training is com-
puted with the S?2T task instead of the S2T original task.
Figure 2 (A) shows an example of the alignment com-
puted on the original training corpus. Figure 2 (B) shows
the same links but with the source training corpus in a
different order (this training corpus comes from the SMR
output). Although, the quality in alignment is the same,
the tuples that can be extracted change (notice that the
tuple extraction is monotonic). We are able to extract
1http://gps-tsc.upc.es/veu/soft/soft/marie/
smaller tuples which reduces the translation vocabulary
sparseness. These new tuples are used to build the SMT
system.
Figure 2: Alignment and tuple extraction (A) original
training source corpus (B) reordered training source cor-
pus.
3.3 Using SMR technique to generate multiple
weighted reordering hypotheses
The SMR system, having its own search, can generate ei-
ther an output 1-best or an output graph. In decoding, the
SMR technique generates an output graph which is used
as an input graph by the SMT system. Figure 1 (B) shows
the corresponding block diagram in decoding: the SMR
output graph is given as an input graph to the SMT sys-
tem. Hereinafter, this either SMR output graph or SMT
input graph will be referred to as (weighted) reordering
graph. The monotonic search in the SMT system is ex-
tended with reorderings following this reordering graph.
This reordering graph has multiple paths and each path
has its own weight. This weight is added as a feature
function in the log-linear framework. Figure 3 shows the
weighted reordering graph.
The main difference with the reordering technique for
WMT06 (Crego et al, 2006) lies in (1) the tuples are ex-
tracted from the word alignment between the reordered
source training corpus and the given target training cor-
pus and (2) the graph structure: the SMR graph provides
weights for each reordering path.
4 Other features and functionalities
In addition to the novel reordering strategy, we consider
two new features functions.
4.1 Target Language Model based on Statistical
Classes
This feature implements a 5-gram language model of tar-
get statistical classes (Och, 1999). This model is trained
by considering statistical classes, instead of words, for
168
Figure 3: Weighted reordering input graph for SMT sys-
tem.
the target side of the training corpus. Accordingly, the tu-
ple translation unit is redefined in terms of a triplet which
includes: a source string containing the source side of
the tuple, a target string containing the target side of the
tuple, and a class string containing the statistical classes
corresponding to the words in the target strings.
4.2 Bonus for out-of-domain tuples
This feature adds a bonus to those tuples which comes
from the training of the out-of-domain task. This feature
is added when optimizing with the development of the
out-of-domain task.
4.3 Optimization
Finally, a n-best re-ranking strategy is implemented
which is used for optimization purposes just as pro-
posed in http://www.statmt.org/jhuws/. This procedure
allows for a faster and more efficient adjustment of model
weights by means of a double-loop optimization, which
provides significant reduction of the number of transla-
tions that should be carried out. The current optimization
procedure uses the Simplex algorithm.
5 Shared Task Framework
5.1 Data
The data provided for this shared task corresponds to a
subset of the official transcriptions of the European Par-
liament Plenary Sessions 2. Additionally, there was avail-
able a smaller corpus called News-Commentary. For all
tasks and domains, our training corpus was the catenation
of both.
2http://www.statmt.org/wmt07/shared-task/
5.2 Processing details
Word Alignment. The word alignment is automati-
cally computed by using GIZA++ 3 in both directions,
which are symmetrized by using the union operation. In-
stead of aligning words themselves, stems are used for
aligning. Afterwards case sensitive words are recovered.
Spanish Morphology Reduction. We implemented a
morphology reduction of the Spanish language as a pre-
processing step. As a consequence, training data sparse-
ness due to Spanish morphology was reduced improving
the performance of the overall translation system. In par-
ticular, the pronouns attached to the verb were separated
and contractions as del or al are splited into de el or a
el. As a post-processing, in the En2Es direction we used
a POS target language model as a feature (instead of the
target language model based on classes) that allowed to
recover the segmentations (de Gispert, 2006).
Language Model Interpolation. In other to better
adapt the system to the out-of-domain condition, the
target language model feature was built by combining
two 5-gram target language models (using SRILM 4).
One was trained from the EuroParl training data set, and
the other from the available, but much smaller, news-
commentary data set. The combination weights for the
EuroParl and news-commentary language models were
empirically adjusted by following a minimum perplexity
criterion. A relative perplexity reduction around 10-15%
respect to original EuroParl language model was achieved
in all the tasks.
5.3 Experiments and Results
The main difference between this year?s and last year?s
systems are: the amount of data provided; the word align-
ment; the Spanish morphology reduction; the reordering
technique; the extra target language model based on sta-
tistical classes (except for the En2Es); and the bonus for
the out-of-domain task (only for the En2Es task).
Among them, the most important is the reordering
technique. That is why we provide a fair comparison be-
tween the reordering patterns (Crego and Marin?o, 2006)
technique and the SMR reordering technique. Table 1
shows the system described above using either reorder-
ing patterns or the SMR technique. The BLEU calcula-
tion was case insensitive and sensitive to tokenization.
Table 2 presents the BLEU score obtained for the 2006
test data set comparing last year?s and this year?s systems.
The computed BLEU scores are case insensitive, sensi-
tive to tokenization and uses one translation reference.
The improvement in BLEU results shown from UPC-jm
3http://www.fjoch.com/GIZA++.html
4http://www.speech.sri.com/projects/srilm/
169
Task Reordering patterns SMR technique
es2en 31.21 33.34
en2es 31.67 32.33
Table 1: BLEU comparison: reordering patterns vs. SMR
technique.
Task UPC-jm 2006 UPC 2007
in-d out-d in-d out-d
es2en 31.01 27.92 33.34 32.85
en2es 30.44 25.59 32.33 33.07
fr2en 30.42 21.79 32.44 26.93
en2fr 31.75 23.30 32.30 27.03
de2en 24.43 17.57 26.54 21.63
en2de 17.73 10.96 19.74 15.06
Table 2: BLEU scores for each of the six translation di-
rections considered (computed over 2006 test set) com-
paring last year?s and this year?s system results (in-
domain and out-domain).
2006 Table 2 and reordering patterns Table 1 in the En-
glish/Spanish in-domain task comes from the combina-
tion of: the additional corpora, the word alignment, the
Spanish morphology reduction and the extra target lan-
guage model based on classes (only in the Es2En direc-
tion).
6 Conclusions and Further Work
This paper describes the UPC system for the WMT07
Evaluation. In the framework of Ngram-based system, a
novel reordering strategy which can be used for any pair
of languages has been presented and it has been showed
to significantly improve translation performance. Ad-
ditionally two features has been added to the log-lineal
scheme: the target language model based on classes and
the bonus for out-of-domain translation units.
7 Acknowledgments
This work has been funded by the European Union un-
der the TC-STAR project (IST-2002-FP6-506738) and
the Spanish Government under grant TEC2006-13964-
C03 (AVIVAVOZ project).
References
M.R. Costa-jussa` and J.A.R. Fonollosa. 2006. Statistical
machine reordering. In EMNLP, pages 71?77, Sydney,
July. ACL.
J.M. Crego and J.B. Marin?o. 2006. Reordering experi-
ments for n-gram-based smt. In SLT, pages 242?245,
Aruba.
Josep M. Crego, Adria` de Gispert, Patrik Lambert,
Marta R. Costa-jussa`, Maxim Khalilov, Rafael Banchs,
Jose? B. Marin?o, and Jose? A. R. Fonollosa. 2006. N-
gram-based smt system enhanced with reordering pat-
terns. In WMT, pages 162?165, New York City, June.
ACL.
Adria` de Gispert. 2006. Introducing Linguistic Knowl-
edge in Statistical Machine Translation. Ph.D. thesis,
Universitat Polite`cnica de Catalunya, December.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between european lan-
guages. In WMT, pages 119?124, Michigan, June.
ACL.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In WMT, pages 102?121, New
York City, June. ACL.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram based machine translation. Computa-
tional Linguistics, 32(4):527?549, December.
F.J. Och. 1999. An efficient method for determin-
ing bilingual word classes. In EACL, pages 71?76,
Bergen, Norway, June.
170
Proceedings of the Third Workshop on Statistical Machine Translation, pages 127?130,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The TALP-UPC Ngram-based statistical machine translation system for
ACL-WMT 2008
Maxim Khalilov, Adolfo Hern?ndez H., Marta R. Costa-juss?,
Josep M. Crego, Carlos A. Henr?quez Q., Patrik Lambert,
Jos? A. R. Fonollosa, Jos? B. Mari?o and Rafael E. Banchs
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(khalilov, adolfohh, mruiz, jmcrego, carloshq, lambert, adrian, canton, rbanchs)@gps.tsc.upc.edu
Abstract
This paper reports on the participation of the TALP
Research Center of the UPC (Universitat Polit?cnica
de Catalunya) to the ACL WMT 2008 evaluation
campaign.
This year?s system is the evolution of the one we em-
ployed for the 2007 campaign. Main updates and
extensions involve linguistically motivated word re-
ordering based on the reordering patterns technique.
In addition, this system introduces a target language
model, based on linguistic classes (Part-of-Speech),
morphology reduction for an inflectional language
(Spanish) and an improved optimization procedure.
Results obtained over the development and test sets
on Spanish to English (and the other way round)
translations for both the traditional Europarl and
a challenging News stories tasks are analyzed and
commented.
1 Introduction
Over the past few years, the Statistical Machine Transla-
tion (SMT) group of the TALP-UPC has been develop-
ing the Ngram-based SMT system (Mari?o et al, 2006).
In previous evaluation campaigns the Ngram-based ap-
proach has proved to be comparable with the state-of-
the-art phrase-based systems, as shown in Koehn and
Monz(2006), Callison-Burch et al (2007).
We present a summary of the TALP-UPC Ngram-
based SMT system used for this shared task. We dis-
cuss the system configuration and novel features, namely
linguistically motivated reordering technique, which is
applied on the decoding step. Additionally, the reorder-
ing procedure is supported by an Ngram language model
(LM) of reordered source Part-of-Speech tags (POS).
In this year?s evaluation we submitted systems for
Spanish-English and English-Spanish language pairs for
the traditional (Europarl) and challenging (News) tasks.
In each case, we used only the supplied data for each lan-
guage pair for models training and optimization.
This paper is organized as follows. Section 2 briefly
outlines the 2008 system, including tuple definition and
extraction, translation model and additional feature mod-
els, decoding tool and optimization procedure. Section 3
describes the word reordering problem and presents the
proposed technique of reordering patterns learning and
application. Later on, Section 4 reports on the experi-
mental setups of the WMT 2008 evaluation campaign. In
Section 5 we sum up the main conclusions from the pa-
per.
2 Ngram-based SMT System
Our translation system implements a log-linear model in
which a foreign language sentence fJ1 = f1, f2, ..., fJ
is translated into another language eI1 = f1, f2, ..., eI by
searching for the translation hypothesis e?I1 maximizing a
log-linear combination of several feature models (Brown
et al, 1990):
e?I1 = argmax
eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
where the feature functions hm refer to the system models
and the set of ?m refers to the weights corresponding to
these models.
The core part of the system constructed in that way
is a translation model, which is based on bilingual n-
grams. It actually constitutes an Ngram-based LM of
bilingual units (called tuples), which approximates the
joint probability between the languages under consider-
ation. The procedure of tuples extraction from a word-
to-word alignment according to certain constraints is ex-
plained in detail in Mari?o et al (2006).
The Ngram-based approach differs from the phrase-
based SMT mainly by distinct representating of the bilin-
gual units defined by word alignment and using a higher
127
order HMM of the translation process. While regular
phrase-based SMT considers context only for phrase re-
ordering but not for translation, the N-gram based ap-
proach conditions translation decisions on previous trans-
lation decisions.
The TALP-UPC 2008 translation system, besides the
bilingual translation model, which consists of a 4-gram
LM of tuples with Kneser-Ney discounting (estimated
with SRI Language Modeling Toolkit1), implements a
log-linear combination of five additional feature models:
? a target language model (a 4-gram model of words,
estimated with Kneser-Ney smoothing);
? a POS target language model (a 4-gram model of
tags with Good-Turing discounting (TPOS));
? a word bonus model, which is used to compensate
the system?s preference for short output sentences;
? a source-to-target lexicon model and a target-to-
source lexicon model, these models use word-to-
word IBM Model 1 probabilities (Och and Ney,
2004) to estimate the lexical weights for each tuple
in the translation table.
Decisions on the particular LM configuration and
smoothing technique were taken on the minimal-
perplexity and maximal-BLEU bases.
The decoder (called MARIE), an open source tool2,
implementing a beam search strategy with distortion ca-
pabilities was used in the translation system.
Given the development set and references, the log-
linear combination of weights was adjusted using a sim-
plex optimization method (with the optimization criteria
of the highest BLEU score ) and an n-best re-ranking
just as described in http://www.statmt.org/jhuws/. This
strategy allows for a faster and more efficient adjustment
of model weights by means of a double-loop optimiza-
tion, which provides significant reduction of the number
of translations that should be carried out.
3 Reordering framework
For a great number of translation tasks a certain reorder-
ing strategy is required. This is especially important
when the translation is performed between pairs of lan-
guages with non-monotonic word order. There are var-
ious types of distortion models, simplifying bilingual
translation. In our system we use an extended monotone
reordering model based on automatically learned reorder-
ing rules. A detailed description can be found in Crego
and Mari?o (2006).
1http://www.speech.sri.com/projects/srilm/
2http://gps-tsc.upc.es/veu/soft/soft/marie/
Apart from that, tuples were extracted by an unfold-
ing technique: this means that the tuples are broken into
smaller tuples, and these are sequenced in the order of the
target words.
3.1 Reordering patterns
Word movements are realized according to the reordering
rewrite rules, which have the form of:
t1, ..., tn 7? i1, ..., in
where t1, ..., tn is a sequence of POS tags (relating a
sequence of source words), and i1, ..., in indicates which
order of the source words generate monotonically the tar-
get words.
Patterns are extracted in training from the crossed links
found in the word alignment, in other words, found in
translation tuples (as no word within a tuple can be linked
to a word out of it (Crego and Mari?o, 2006)).
Having all the instances of rewrite patterns, a score for
each pattern on the basis of relative frequency is calcu-
lated as shown below:
p(t1, ..., tn 7? i1, ..., in) =
N(t1, ..., tn 7? i1, ..., in)
NN(t1, ..., tn)
3.2 Search graph extension and source POS model
The monotone search graph is extended with reorderings
following the patterns found in training. Once the search
graph is built, the decoder traverses the graph looking for
the best translation. Hence, the winning hypothesis is
computed using all the available information (the whole
SMT models).
Figure 1: Search graph extension. NC, CC and AQ stand re-
spectively for name, conjunction and adjective.
The procedure identifies first the sequences of words
in the input sentence that match any available pattern.
Then, each of the matchings implies the addition of an arc
into the search graph (encoding the reordering learned in
the pattern). However, this addition of a new arc is not
128
Task BL BL+SPOS
Europarl News Europarl News
es2en 32.79 36.09 32.88 36.36
en2es 32.05 33.91 32.10 33.63
Table 1: BLEU comparison demonstrating the impact of the
source-side POS tags model.
performed if a translation unit with the same source-side
words already exists in the training. Figure 1 shows how
two rewrite rules applied over an input sentence extend
the search graph given the reordering patterns that match
the source POS tag sequence.
The reordering strategy is additionally supported by
a 4-gram language model (estimated with Good-Turing
smoothing) of reordered source POS tags (SPOS). In
training, POS tags are reordered according with the ex-
tracted reordering patterns and word-to-word links. The
resulting sequence of source POS tags is used to train the
Ngram LM.
Table 1 presents the effect of the source POS LM in-
troduction to the reordering module of the Ngram-based
SMT. As it can be seen, the impactya le h of the source-
side POS LM is minimal, however we decided to consider
the model aiming at improving it in future. The reported
results are related to the Europarl and News Commen-
tary (News) development sets. BLEU calculation is case
insensitive and insensitive to tokenization. BL (baseline)
refers to the presented Ngram-based system considering
all the features, apart from the target and source POS
models.
4 WMT 2008 Evaluation Framework
4.1 Corpus
An extraction of the official transcriptions of the 3rd re-
lease of the European Parliament Plenary Sessions3 was
provided for the ACL WMT 2008 shared translation task.
About 40 times smaller corpus from news domain (called
News Commentary) was also available. For both tasks,
our training corpus was the catenation of the Europarl and
News Commentary corpora.
TALP UPC participated in the constraint to the
provided training data track for Spanish-English and
English-Spanish translation tasks. We used the same
training material for the traditional and challenging tasks,
while the development sets used to tune the system were
distinct (2000 sentences for Europarl task and 1057
for News Commentary, one reference translation for
each of them). A brief training and development corpora
statistics is presented in Table 2.
3http://www.statmt.org/wmt08/shared-task.html
Spanish English
Train
Sentences 1.3 M 1.3 M
Words 38.2 M 35.8 K
Vocabulary 156 K 120 K
Development Europarl
Sentences 2000 2000
Words 61.8 K 58.7 K
Vocabulary 8 K 6.5 K
Development News Commentary
Sentences 1057 1057
Words 29.8 K 25.8 K
Vocabulary 5.4 K 4.9 K
Table 2: Basic statistics of ACL WMT 2008 corpus.
4.2 Processing details
The training data was preprocessed by using provided
tools for tokenizing and filtering.
POS tagging. POS information for the source and the
target languages was considered for both translation tasks
that we have participated. The software tools available
for performing POS-tagging were Freeling (Carreras et
al., 2004) for Spanish and TnT (Brants, 2000) for En-
glish. The number of classes for English is 44, while
Spanish is considered as a more inflectional language,
and the tag set contains 376 different tags.
Word Alignment. The word alignment is automati-
cally computed by using GIZA++4(Och and Ney, 2000)
in both directions, which are symmetrized by using the
union operation. Instead of aligning words themselves,
stems are used for aligning. Afterwards case sensitive
words are recovered.
Spanish Morphology Reduction. We implemented a
morphology reduction of the Spanish language as a pre-
processing step. As a consequence, training data sparse-
ness due to Spanish morphology was reduced improving
the performance of the overall translation system. In par-
ticular, the pronouns attached to the verb were separated
and contractions as del or al were splitted into de el or
a el. As a post-processing, in the En2Es direction we
used a POS target LM as a feature (instead of the target
language model based on classes) that allowed to recover
the segmentations (de Gispert, 2006).
4.3 Experiments and Results
In contrast to the last year?s system where statistical
classes were used to train the target-side tags LM, this
year we used linguistically motivated word classes
4http://code.google.com/p/giza-pp/
129
Task BL+SPOS BL+SPOS+TPOS
(UPC 2008)
Europarl News Europarl News
es2en 32.88 36.36 32.89 36.31
en2es 31.52 34.13 30.72 32.72
en2es "clean"5 32.10 33.63 32.09 35.04
Table 3: BLEU scores for Spanish-English and English-Spanish
2008 development corpora (Europarl and News Commentary).
Task UPC 2008
Europarl News
es2en 32.80 19.61
en2es 31.31 19.28
en2es "clean"5 32.34 20.05
Table 4: BLEU scores for official tests 2008.
(POS) which were considered to train the POS target LM
and extract the reordering patterns. Other characteristics
of this year?s system are:
? reordering patterns technique;
? source POS model, supporting word reordering;
? no LM interpolation. For this year?s evaluation, we
trained two separate LMs for each domain-specific
corpus (i.e., Europarl and News Commentary tasks).
It is important to mention that 2008 training material is
identical to the one provided for the 2007 shared transla-
tion task.
Table 3 presents the BLEU score obtained for the 2008
development data sets and shows the impact of the target-
side POS LM introduction, which can be characterized as
highly corpus- and language-dependent feature. BL refers
to the same system configuration as described in subsec-
tion 3.2. The computed BLEU scores are case insensitive,
insensitive to tokenization and use one translation refer-
ence.
After submitting the systems we discovered a bug re-
lated to incorrect implementation of the target LMs of
words and tags for Spanish, it caused serious reduction
of translation quality (1.4 BLEU points for development
set in case of English-to-Spanish Europarl task and 2.3
points in case of the corresponding News Commentary
task). The last raw of table 3 (en2es "clean") repre-
sents the results corresponding to the UPC 2008 post-
evaluation system, while the previous one (en2es) refers
to the "bugged" system submitted to the evaluation.
The experiments presented in Table 4 correspond to the
2008 test evaluation sets.
5Corrected post-evaluation results (see subsection 4.3.)
5 Conclusions
In this paper we introduced the TALP UPC Ngram-based
SMT system participating in the WMT08 evaluation.
Apart from briefly summarizing the decoding and opti-
mization processes, we have presented the feature mod-
els that were taken into account, along with the bilingual
Ngram translation model. A reordering strategy based on
linguistically-motivated reordering patterns to harmonize
the source and target word order has been presented in
the framework of the Ngram-based system.
6 Acknowledgments
This work has been funded by the Spanish Government
under grant TEC2006-13964-C03 (AVIVAVOZ project).
The authors want to thank Adri? de Gispert (Cambridge
University) for his contribution to this work.
References
T. Brants. 2000. TnT ? a statistical part-of-speech tagger. In
Proceedings of the 6th Applied Natural Language Processing
(ANLP-2000).
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek,
J. D. Lafferty, R. Mercer, and P. S. Roossin. 1990. A sta-
tistical approach to machine translation. Computational Lin-
guistics, 16(2):79?85.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine trans-
lation. In Proceedings of the ACL 2007 Workshop on Statis-
tical and Hybrid methods for Machine Translation (WMT),
pages 136?158.
X. Carreras, I. Chao, L. Padr?, and M. Padr?. 2004. Freeling:
An open-source suite of language analyzers. In Proceedings
of the 4th Int. Conf. on Language Resources and Evaluation
(LREC?04).
J. M. Crego and J. B. Mari?o. 2006. Improving statistical MT
by coupling reordering and decoding. Machine Translation,
20(3):199?215.
A. de Gispert. 2006. Introducing linguistic knowledge into
statistical machine translation. Ph.D. thesis, Universitat
Polit?cnica de Catalunya, December.
P. Koehn and C. Monz. 2006. Manual and automatic eval-
uation of machine translation between european languages.
In Proceedings of the ACL 2006 Workshop on Statistical and
Hybrid methods for Machine Translation (WMT), pages 102?
121.
J. B. Mari?o, R. E. Banchs, J. M. Crego, A. de Gispert, P. Lam-
bert, J. A. R. Fonollosa, and M. R. Costa-juss?. 2006. N-
gram based machine translation. Computational Linguistics,
32(4):527?549, December.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proceedings of the the 38th Annual Meeting
on Association for Computational Linguistics (ACL), pages
440?447.
F. Och and H. Ney. 2004. The alignment template approach to
statistical machine translation. 30(4):417 ? 449, December.
130
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85?89,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The TALP-UPC phrase-based translation system for EACL-WMT 2009
Jos? A.R. Fonollosa and Maxim Khalilov and Marta R. Costa-juss? and
Jos? B. Mari?o and Carlos A. Henr?quez Q. and Adolfo Hern?ndez H. and
Rafael E. Banchs
TALP Research Center
Universitat Polit?cnica de Catalunya, Barcelona 08034
{adrian,khalilov,mruiz,canton,carloshq,adolfohh,rbanchs}@talp.upc.edu
Abstract
This study presents the TALP-UPC sub-
mission to the EACL Fourth Worskhop
on Statistical Machine Translation 2009
evaluation campaign. It outlines the ar-
chitecture and configuration of the 2009
phrase-based statistical machine transla-
tion (SMT) system, putting emphasis on
the major novelty of this year: combina-
tion of SMT systems implementing differ-
ent word reordering algorithms.
Traditionally, we have concentrated on
the Spanish-to-English and English-to-
Spanish News Commentary translation
tasks.
1 Introduction
TALP-UPC (Center of Speech and Language
Applications and Technology at the Universitat
Polit?cnica de Catalunya) is a permanent par-
ticipant of the ACL WMT shared translations
tasks, traditionally concentrating on the Spanish-
to-English and vice versa language pairs. In this
paper, we describe the 2009 system?s architecture
and design describing individual components and
distinguishing features of our model.
This year?s system stands aside from the
previous years? configurations which were per-
formed following an N -gram-based (tuple-based)
approach to SMT. By contrast to them, this
year we investigate the translation models (TMs)
interpolation for a state-of-the-art phrase-based
translation system. Inspired by the work pre-
sented in (Schwenk and Est?ve, 2008), we attack
this challenge using the coefficients obtained for
the corresponding monolingual language models
(LMs) for TMs interpolation.
On the second step, we have performed
additional word reordering experiments, com-
paring the results obtained with a statisti-
cal method (R. Costa-juss? and R. Fonollosa,
2009) and syntax-based algorithm (Khalilov and
R. Fonollosa, 2008). Further the outputs of
the systems were combined selecting the trans-
lation with the Minimum Bayes Risk (MBR) al-
gorithm (Kumar, 2004) that allowed significantly
outperforming the baseline configuration.
The remainder of this paper is organized as
follows: Section 2 presents the TALP-UPC?09
phrase-based system, along with the translation
models interpolation procedure and other minor
novelties of this year. Section 3 reports on the ex-
perimental setups and outlines the results of the
participation in the EACL WMT 2009 evaluation
campaign. Section 4 concludes the paper with dis-
cussions.
2 TALP-UPC phrase-based SMT
The system developed for this year?s shared
task is based on a state-of-the-art SMT sys-
tem implemented within the open-source MOSES
toolkit (Koehn et al, 2007). A phrase-based trans-
lation is considered as a three step algorithm:
(1) the source sequence of words is segmented
in phrases, (2) each phrase is translated into tar-
get language using translation table, (3) the target
phrases are reordered to be inherent in the target
language.
A bilingual phrase (which in the context of SMT
do not necessarily coincide with their linguistic
analogies) is any pair of m source words and n
target words that satisfies two basic constraints:
(1) words are consecutive along both sides of the
bilingual phrase and (2) no word on either side of
the phrase is aligned to a word outside the phrase.
Given a sentence pair and a corresponding word-
to-word alignment, phrases are extracted follow-
ing the criterion in (Och and Ney, 2004). The
probability of the phrases is estimated by relative
frequencies of their appearance in the training cor-
pus.
85
Classically, a phrase-based translation system
implements a log-linear model in which a foreign
language sentence fJ1 = f1, f2, ..., fJ is trans-
lated into another language eI1 = e1, e2, ..., eI by
searching for the translation hypothesis e?I1 maxi-
mizing a log-linear combination of several feature
models (Brown et al, 1990):
e?I1 = argmaxeI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
where the feature functions hm refer to the system
models and the set of ?m refers to the weights cor-
responding to these models.
2.1 Translation models interpolation
We implemented a TM interpolation strategy fol-
lowing the ideas proposed in (Schwenk and Es-
t?ve, 2008), where the authors present a promis-
ing technique of target LMs linear interpolation;
in (Koehn and Schroeder, 2007) where a log-linear
combination of TMs is performed; and specifi-
cally in (Foster and Kuhn, 2007) where the authors
present various ways of TM combination and ana-
lyze in detail the TM domain adaptation.
In the framework of the evaluation campaign,
there were two Spanish-to-English parallel train-
ing corpora available: Europarl v.4 corpus (about
50M tokens) and News Commentary (NC) corpus
(about 2M tokens). The test dataset provided by
the organizers this year was from the news do-
main, so we considered the Europarl training cor-
pus as "out-of-domain" data and the News Com-
mentary as "in-domain" training material. Unfor-
tunately, the in-domain corpus is much smaller in
size, however the Europarl corpus can be also used
to increase the final translation and reordering ta-
bles in spite of its different nature.
A straightforward approach to the TM interpo-
lation would be an iterative TM reconstruction ad-
justing scale coefficients on each step of the loop
with use of the highest BLEU score as a maxi-
mization criterion.
However, we did not expect a significant gain
from this time-consumption strategy and we de-
cided to follow a simpler approach. In the pre-
sented results, we obtained the best interpola-
tion weight following the standard entropy-based
optimization of the target-side LM. We adjust
the weight coefficient ?Europarl (?NC = 1 ?
?Europarl) of the linear interpolation of the target-
side LMs:
P (w) = ?Europarl ? PwEuroparl + ?NC ? PwNC (1)
where PwEuroparl and PwNC are probabilities as-
signed to the word sequence w by the LM esti-
mated on Europarl and NC data, respectively.
The scale factor values are automatically opti-
mized to obtain the lowest perplexity ppl(w) pro-
duced by the interpolated LM P (w). We used the
standard script compute ? best ? mix from the
SRI LM package (Stolcke, 2002) for optimization.
On the next step, the optimized coefficients
?Europarl and ?NC are generalized on the interpo-
lated translation and reordering models. In other
words, reordering and translation models are in-
terpolated using the same weights which yield the
lowest perplexity for LM interpolation.
The word-to-word alignment was obtained from
the joint (merged) database (Europarl + NC).
Then, we separately computed the translation and
reordering tables corresponding to the in- and out-
of-domain parts of the joint alignment. The final
tables, as well as the final target LM were obtained
using linear interpolation. The weights were se-
lected using a minimum perplexity criterion esti-
mated on the corresponding interpolated combina-
tion of the target-side LMs.
The optimized coefficient values are: for Span-
ish: NC weight = 0.526, Europarl weight = 0.474;
for English: NC weight = 0.503, Europarl weight
= 0.497. The perplexity results obtained using
monolingual LMs and the 2009 development set
(English and Spanish references) can be found in
Table 1, while the corresponding improvement in
BLEU score is presented in Section 3.3 and sum-
mary of the obtained results (Table 4).
Europarl NC Interpolated
English 463.439 489.915 353.305
Spanish 308.802 347.092 246.573
Table 1: Perplexity results obtained on the Dev
2009 corpus and the monolingual LMs.
Note that the corresponding reordering models
are interpolated with the same weights.
2.2 Statistical Machine Reordering
The idea of the Statistical Machine Reordering
(SMR) stems from the idea of using the power-
ful techniques developed for SMT and to translate
86
the source language (S) into a reordered source
language (S?), which more closely matches the
order of the target language. To infer more re-
orderings, it makes use of word classes. To cor-
rectly integrate the SMT and SMR systems, both
are concatenated by using a word graph which of-
fers weighted reordering hypotheses to the SMT
system. The details are described in (?).
2.3 Syntax-based Reordering
Syntax-based Reordering (SBR) approach deals
with the word reordering problem and is based on
non-isomorphic parse subtree transfer as described
in details in (Khalilov and R. Fonollosa, 2008).
Local and long-range word reorderings are
driven by automatically extracted permutation pat-
terns operating with source language constituents.
Once the reordering patterns are extracted, they
are further applied to monotonize the bilingual
corpus in the same way as shown in the previ-
ous subsection. The target-side parse tree is con-
sidered as a filter constraining reordering rules to
the set of patterns covered both by the source- and
target-side subtrees.
2.4 System Combination
Over the past few years the MBR algorithm uti-
lization to find the best consensus outputs of dif-
ferent translation systems has proved to improve
the translation accuracy (Kumar, 2004). The sys-
tem combination is performed on the 200-best
lists which are generated by the three systems:
(1) MOSES-based system without pre-translation
monotonization (baseline), (2) MOSES-based
SMT enhanced with SMR monotonization and (3)
MOSES-based SMT augmented with SBR mono-
tonization. The results presented in Table 4 show
that the combined output significantly outperforms
the baseline system configuration.
3 Experiments and results
We followed the evaluation baseline instructions 1
to train the MOSES-based translation system.
In some experiments we used MBR decod-
ing (Kumar and Byrne, 2004) with the smoothed
BLEU score as a similarity criteria, that al-
lowed gaining 0.2 BLEU points comparing to the
standard procedure of outputting the translation
with the highest probability (HP). We applied the
Moses implementation of this algorithm to the list
1http://www.statmt.org/wmt09/baseline.html
of 200 best translations generated by the TALP-
UPC system. The results obtained over the official
2009 Test dataset can be found in Table 2.
Task HP MBR
EsEn 24.48 24.62
EnEs 23.46 23.64
Table 2: MBR versus MERT decoding.
The "recase" script provided within the base-
line was supplemented with and additional mod-
ule, which restore the original case for unknown
words (many of them are proper names and loos-
ing of case information leads to a significant per-
formance degradation).
3.1 Language models
The target-side language models were estimated
using the SRILM toolkit (Stolcke, 2002). We tried
to use all the available in-domain training mate-
rial: apart from the corresponding portions of the
bilingual NC corpora we involved the following
monolingual corpora:
? News monolingual corpus (49M tokens for
English and 49M for Spanish)
? Europarl monolingual corpus (about 504M
tokens for English and 463M for Spanish)
? A collection of News development and test
sets from previous evaluations (151K tokens
for English and 175K for Spanish)
? A collection of Europarl development and
test sets from previous evaluations (295K to-
kens for English and 311K for Spanish)
Five LMs per language were estimated on the
corresponding datasets and interpolated follow-
ing the maximum perplexity criteria. Hence, the
larger LMs incorporating in- and out-of-domain
data were used in decoding.
3.2 Spanish enclitics separation
For the Spanish portion of the corpus we imple-
mented an enclitics separation procedure on the
preprocessing step, i.e. the pronouns attached to
the verb were separated and contractions as del
or al were splitted into de el or a el. Conse-
quently, training data sparseness due to Spanish
morphology was reduced improving the perfor-
mance of the overall translation system. As a
87
post-processing, the segmentation was recovered
in the English-to-Spanish direction using target-
side Part-of-Speech tags (de Gispert, 2006).
3.3 Results
The automatic scores provided by the WMT?09
organizers for TALP-UPC submissions calculated
over the News 2009 dataset can be found in Ta-
ble 3. BLEU and NIST case-insensitive (CI) and
case-sensitive (CS) metrics are considered.
Task Bleu CI Bleu CS NIST CI NIST CS
EsEn 25.93 24.54 7.275 7.017
EnEs 24.85 23.37 6.963 6.689
Table 3: BLEU and NIST scores for preliminary
official test dataset 2009 (primary submission)
with 500 sentences excluded.
The TALP-UPC primary submission was
ranked the 3rd among 28 presented translations
for the Spanish-to-English task and the 4th for the
English-to-Spanish task among 9 systems.
The following system configurations and the in-
ternal results obtained are reported:
? Baseline: Moses-based SMT, as proposed
on the web-page of the evaluation campaign
with Spanish enclitics separation and modi-
fied version of ?recase? tool,
? Baseline+TMI: Baseline enhanced with TM
interpolation as described in subsection 2.1,
? Baseline+TMI+MBR: the same as the latter
but with MBR decoding,
? Baseline+TMI+SMR: the same as Base-
line+TMI but with SMR technique applied to
monotonize the source portion of the corpus,
as described in subsection 2.2,
? Baseline+SBR: the same as Baseline but with
SBR algorithm applied to monotonize the
source portion of the corpus, as described in
subsection 2.3,
? System Combination: a combined output of
the 3 previous systems done with the MBR
algorithm, as described in subsection 2.4.
Impact of TM interpolation and MBR decod-
ing is more significant for the English-to-Spanish
translation task, for which the target-side mono-
lingual corpus is smaller than for the Spanish-to-
English translation.
We did not have time to meet the evalua-
tion deadline for providing the system combi-
nation output. Nevertheless, during the post-
evaluation period we performed the experiments
reported in the last three lines of Table 4 (Base-
line+TMI+SMR, Baseline+SBR and System com-
bination).
Note that the results presented in Table 4 differ
from the ones which can be found the Table 3 due
to selective conditions of preliminary evaluation
done by the Shared Task organizers.
System News 2009 Test CI News 2009 Test CS
Spanish-to-English
Baseline 25.82 24.37
Baseline+TMI 25.84 24.47
Baseline+TMI+MBR (Primary) 26.04 24.62
Baseline+SMR 24.95 23.62
Baseline+SBR 24.24 22.89
System combination 26.44 25.00
English-to-Spanish
Baseline 24.56 23.05
Baseline+TMI 25.01 23.41
Baseline+TMI+MBR (Primary) 25.16 23.64
Baseline+SMR 24.09 22.65
Baseline+SBR 23.52 22.05
System combination 25.39 23.86
Table 4: Experiments summary.
88
4 Conclusions
In this paper, we present the TALP-UPC phrase-
based translation system developed for the EACL-
WMT 2009 evaluation campaign. The major nov-
elties of this year are translation models interpola-
tion done in linear way and combination of SMT
systems implementing different word reordering
algorithms. The system was ranked pretty well for
both translation tasks in which our institution has
participated.
Unfortunately, the promising reordering tech-
niques and the combination of their outputs were
not applied within the evaluation deadline, how-
ever we report the obtained results in the paper.
5 Acknowledgments
This work has been funded by the Spanish Gov-
ernment under grant TEC2006-13964-C03 (AVI-
VAVOZ project).
References
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra,
F. Jelinek, J.D. Lafferty, R. Mercer, and P.S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?
85.
A. de Gispert. 2006. Introducing linguistic knowledge
into Statistical Machine Translation. Ph.D. thesis,
Universitat Polit?cnica de Catalunya, December.
G. Foster and R. Kuhn. 2007. Mixture-model adap-
tation for SMT. In In Annual Meeting of the Asso-
ciation for Computational Linguistics: Proc. of the
Second Workshop on Statistical Machine Transla-
tion (WMT), pages 128?135, Prague, Czech Repub-
lic, June.
M. Khalilov and J. R. Fonollosa. 2008. A new subtree-
transfer approach to syntax-based reordering for sta-
tistical machine translation. Technical report, Uni-
versitat Polit?cnica de Catalunya.
Ph. Koehn and J. Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation.
In In Annual Meeting of the Association for Compu-
tational Linguistics: Proc. of the Second Workshop
on Statistical Machine Translation (WMT), pages
224?227, Prague, Czech Republic, June.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: open-source
toolkit for statistical machine translation. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL) 2007, pages 177?180.
Sh. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In In
HLTNAACL?04, pages 169?176.
Sh. Kumar. 2004. Minimum Bayes-Risk Techniques in
Automatic Speech Recognition and Statistical Ma-
chine Translation. Ph.D. thesis, Johns Hopkins Uni-
versity.
F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 3(4):417?449, December.
M. R. Costa-juss? and J. R. Fonollosa. 2009. An
Ngram reordering model. Computer Speech and
Language. ISSN 0885-2308, accepted for publica-
tion.
H. Schwenk and Y. Est?ve. 2008. Data selection and
smoothing in an open-source system for the 2008
nist machine translation evaluation. In Proceedings
of the Interspeech?08, pages 2727?2730, Brisbane,
Australia, September.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proceedings of the Int. Conf.
on Spoken Language Processing, pages 901?904.
89
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 98?102,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Using collocation segmentation to augment the phrase table
Carlos A. Henr?quez Q.
?
, Marta R. Costa-juss?
?
, Vidas Daudaravicius?
Rafael E. Banchs
?
, Jos? B. Mari?o
?
?
TALP Research Center, Universitat Polit?cnica de Catalunya, Barcelona, Spain
{carlos.henriquez,jose.marino}@upc.edu
?
Barcelona Media Innovation Center, Barcelona, Spain
{marta.ruiz,rafael.banchs}@barcelonamedia.org
?Faculty of Informatics, Vytautas Magnus University, Kaunas, Lithuania
vidas@donelaitis.vdu.lt
Abstract
This paper describes the 2010 phrase-based
statistical machine translation system de-
veloped at the TALP Research Center of
the UPC
1
in cooperation with BMIC
2
and
VMU
3
. In phrase-based SMT, the phrase
table is the main tool in translation. It is
created extracting phrases from an aligned
parallel corpus and then computing trans-
lation model scores with them. Performing
a collocation segmentation over the source
and target corpus before the alignment
causes that different and larger phrases
are extracted from the same original doc-
uments. We performed this segmentation
and used the union of this phrase set with
the phrase set extracted from the non-
segmented corpus to compute the phrase
table. We present the configurations con-
sidered and also report results obtained
with internal and official test sets.
1 Introduction
The TALP Research Center of the UPC
1
in coop-
eration with BMIC
2
and VMU
3
participated in the
Spanish-to-English WMT task. Our primary sub-
mission was a phrase-based SMT system enhanced
with POS tags and our contrastive submission was
an augmented phrase-based system using colloca-
tion segmentation (Costa-juss? et al, 2010), which
mainly is a way of introducing new phrases in the
translation table. This paper presents the descrip-
tion of both systems together with the results that
we obtained in the evaluation task and is organized
as follows: first, Section 2 and 3 present a brief de-
scription of a phrase-based SMT, followed by a gen-
eral explanation of collocation segmentation. Sec-
tion 4 presents the experimental framework, corpus
used and a description of the different systems built
for the translation task; the section ends showing
the results we obtained over the official test set. Fi-
nally, section 5 presents the conclusions obtained
from the experiments.
1
Universitat Polit?cnica de Catalunya
2
Barcelona Media Innovation Center
3
Vytautas Magnus University
2 Phrase-based SMT
This approach to SMT performs the translation
splitting the source sentence in segments and as-
signing to each segment a bilingual phrase from
a phrase-table. Bilingual phrases are translation
units that contain source words and target words,
e.g. < unidad de traduccio?n | translation unit >,
and have different scores associated to them. These
bilingual phrases are then sorted in order to max-
imize a linear combination of feature functions.
Such strategy is known as the log-linear model
(Och and Ney, 2003) and it is formally defined as:
e? = arg max
e
[
M?
m=1
?mhm (e, f)
]
(1)
where hm are different feature functions with
weights ?m. The two main feature functions
are the translation model (TM) and the target
language model (LM). Additional models include
POS target language models, lexical weights, word
penalty and reordering models among others.
3 Collocation segmentation
Collocation segmentation is the process of de-
tecting boundaries between collocation segments
within a text (Daudaravicius and Marcinkeviciene,
2004). A collocation segment is a piece of text be-
tween boundaries. The boundaries are established
in two steps using two different measures: the Dice
score and a Average Minimum Law (AML).
The Dice score is used to measure the associa-
tion strength between two words. It has been used
before in the collocation compiler XTract (Smadja,
1993) and in the lexicon extraction system Cham-
pollion (Smadja et al, 1996). It is defined as fol-
lows:
Dice (x; y) =
2f (x, y)
f (x) + f (y)
(2)
where f (x, y) is the frequency of co-occurrence of
x and y, and f (x) and f (y) the frequencies of
occurrence of x and y anywhere in the text. It gives
high scores when x and y occur in conjunction.
The first step then establishes a boundary between
98
two adjacent words when the Dice score is lower
than a threshold t = exp (?8). Such a threshold
was established following the results obtained in
(Costa-juss? et al, 2010), where an integration of
this technique and a SMT system was performed
over the Bible corpus.
The second step of the procedure uses the AML.
It defines a boundary between words xi?1 and xi
when:
Dice (xi?2;xi?1) +Dice (xi;xi+1)
2
> Dice (xi?1;xi)
(3)
That is, the boundary is set when the Dice value
between words xi and xi?1 is lower than the aver-
age of preceding and following values.
4 Experimental Framework
All systems were built using Moses (Koehn et al,
2007), a state-of-the-art software for phrase-based
SMT. For preprocessing Spanish, we used Freeling
(Atserias et al, 2006), an open source library of
natural language analyzers. For English, we used
TnT (Brants, 2000) and Moses' tokenizer. The
language models were built using SRILM (Stolcke,
2002).
4.1 Corpus
This year, the translation task provided four dif-
ferent sources to collect corpora for the Spanish-
English pair. Bilingual corpora included version 5
of the Europarl Corpus (Koehn, 2005), the News
Commentary corpus and the United Nations cor-
pus. Additional English corpora was available from
the News corpus. The organizers also allowed the
use of the English Gigaword Third and Fourth Edi-
tion, released by the LDC. As for development
and internal test, the test sets from 2008 and 2009
translation tasks were available.
For our experiments, we selected as training data
the union of the Europarl and the News Commen-
tary. Development was performed with a section
of the 2008 test set and the 2009 test set was se-
lected as internal test. We deleted all empty lines,
removed pairs that were longer than 40 words, ei-
ther in Spanish or English; and also removed pairs
whose ratio between number of words were bigger
than 3.
As a preprocess, all corpora were lower-cased
and tokenized. The Spanish corpus was tokenized
and POS tags were extracted using Freeling, which
split clitics from verbs and also separated words
like del into de el. In order to build a POS tar-
get language model, we also obtained POS tags
from the English corpus using the TnT tagger.
Statistics of the selected corpus can be seen in Ta-
ble 1.
Corpora Spanish English
Training sent 1, 180, 623 1, 180, 623
Running words 26, 454, 280 25, 291, 370
Vocabulary 118, 073 89, 248
Development sent 1, 729 1, 729
Running words 37, 092 34, 774
Vocabulary 7, 025 6, 199
Internal test sent 2, 525 2, 525
Running words 69, 565 65, 595
Vocabulary 10, 539 8, 907
Official test sent 2, 489 -
Running words 66, 714 -
Vocabulary 10, 725 -
Table 1: Statistics for the training, development
and test sets.
Internal test Official test
Adjectives 137 72
Common nouns 369 188
Proper nouns 408 2, 106
Verbs 213 128
Others 119 168
Total 1246 2662
Table 2: Unknown words found in internal and
official test sets
It is important to notice that neither the United
Nations nor the Gigaword corpus were used for
bilingual training. Nevertheless, the English part
from the United Nations and the monolingual
News corpus were used to build the language model
of our systems.
4.1.1 Unknown words
We analyzed the content from the internal and of-
ficial test and realized that they both contained
many words that were not seen in the training data.
Table 2 shows the number of unknown words found
in both sets, classified according to their POS.
In average, we may expect an unknown word
every two sentences in the internal test and more
than one per sentence in the official test set. It can
also be seen that most of those unknown words are
proper nouns, representing 32% and 79% of the
unknown sets, respectively. Common nouns were
the second most frequent type of unknown words,
followed by verbs and adjectives.
4.2 Systems
We submitted two different systems for the trans-
lation task. First a baseline using the training data
mentioned before; and then an augmented system,
where the baseline-extracted phrase list was ex-
tended with additional phrases coming from a seg-
mented version of the training corpus.
We also considered an additional system built
99
with two different decoding path, a standard path
from words to words and POS and an alternative
path from stems to words and POS in the target
side. At the end, we did not submit this system
to the translation task because it did not provide
better results than the previous two in our internal
test.
The set of feature functions used include: source-
to-target and target-to-source relative frequen-
cies, source-to-target and target-to-source lexical
weights, word and phrase penalties, a target lan-
guage model, a POS target language model, and a
lexicalized reordering model (Tillman, 2004).
4.2.1 Considering stems as an alternate
decoding path.
Using Moses' framework for factored translation
models we defined a system with two decoding
paths: one decoding path using words and the
other decoding path using stems in the source lan-
guage and words in the target language. Both de-
coding paths only had a single translation step.
The possibility of using multiple alternative decod-
ing path was developed by Birch et. al. (2007).
This system tried to solve the problem with the
unknown words. Because Spanish is morphologi-
cally richer than English, this alternative decoding
path allowed the decoder translate words that were
not seen in the training data and shared the same
root with other known words.
4.2.2 Expanding the phrase table using
collocation segmentation.
In order to build the augmented phrase table with
the technique mentioned in section 3, we seg-
mented each language of the bilingual corpus in-
dependently and then, using the collocation seg-
ments as words, we aligned the corpus and ex-
tracted the phrases from it. Once the phrases were
extracted, the segments of each phrase were split
again in words to have standard phrases. Finally,
we use the union of this phrases and the phrases
extracted from the baseline system to compute the
final phrase table. A diagram of the whole proce-
dure can be seen in figure 1.
The objective of this integration is to add new
phrases in the translation table and to enhance
the relative frequency of the phrases that were ex-
tracted from both methods.
4.2.3 Language model interpolation.
Because SMT systems are trained with a bilingual
corpus, they ended highly tied to the domain the
corpus belong to. Therefore, when the documents
we want to translate belong to a different domain,
additional domain adaptation techniques are rec-
ommended to build the system. Those techniques
usually employ additional corpora that correspond
to the domain we want to translate from.
internal test
baseline 24.25
baseline+stem 23.45
augmented 23.9
Table 3: Internal test results.
test testcased?detok
baseline 26.1 25.1
augmented 26.1 25.1
Table 4: Results from translation task
The test set for this translation task comes from
the news domain, but most of our bilingual cor-
pora belonged to a political domain, the Europarl.
Therefore we use the additional monolingual cor-
pus to adapt the language model to the news do-
main.
The strategy used followed the experiment per-
formed last year in (R. Fonollosa et al, 2009).
We used SRILM during the whole process. All
language models were order five and used modi-
fied Kneser-Ney discount and interpolation. First,
we build three different language models accord-
ing to their domain: Europarl, United Nations and
news; then, we obtained the perplexity of each lan-
guage model over the News Commentary develop-
ment corpus; next, we used compute-best-mix to
obtain weights for each language model that di-
minish the global perplexity. Finally, the models
were combined using those weights.
In our experiments all systems used the resulting
language model, therefore the difference obtained
in our results were cause only by the translation
model.
4.3 Results
We present results from the three systems devel-
oped this year. First, the baseline, which included
all the features mentioned in section 4.2; then, the
system with an alternative decoding path, called
baseline+stem; and finally the augmented system,
which integrated collocation segmentation to the
baseline. Internal test results can be seen in table
3. Automatic scores provided by the WMT 2010
organizers for the official test can be found in ta-
ble 4. All BLEU scores are case-insensitive and
tokenized except for the official test set which also
contains case-sensitive and non-tokenized score.
We obtained a BLEU score of 26.1 and 25.1 for
our case-insensitive and sensitive outputs, respec-
tively. The highest score was obtained by Uni-
versity of Cambridge, with 30.5 and 29.1 BLEU
points.
100
Figure 1: Example of the expansion of the phrase table using collocation segmentation. New phrases
added by the collocation-based system are marked with a ??.
4.3.1 Comparing systems
Once we obtained the translation outputs from the
baseline and the augmented system, we performed
a manual comparison of them. Even though we
did not find any significant advantages of the aug-
mented system over the baseline, the collocation
segmentation strategy chose a better morphologi-
cal structures in some cases as can be seen in Table
5 (only sentence sub-segments are shown):
5 Conclusion
We presented two different submissions for the
Spanish-English language pair. The language
model for both system was built interpolating two
big out-of-domain language models and one smaller
in-domain language model. The first system was a
baseline with POS target language model; and the
second one an augmented system, that integrates
the baseline with collocation segmentation. Re-
sults over the official test set showed no difference
in BLEU between these two, even though internal
results showed that the baseline obtained a better
score.
We also considered adding an additional decod-
ing path from stems to words in the baseline but
internal tests showed that it did not improve trans-
lation quality either. The high number of unknown
words found in Spanish suggested us that consider-
ing in parallel the simple form of stems could help
us achieve better results. Nevertheless, a deeper
study of the unknown set showed us that most
of those words were proper nouns, which do not
have inflection and therefore cannot benefited from
stems.
Finally, despite that internal test did not showed
an improvement with the augmented system, we
submitted it as a secondary run looking for the
effect these phrases could have over human evalu-
ation.
Acknowledgment
The research leading to these results has received
funding from the European Community's Seventh
Framework Programme (FP7/2007-2013) under
grant agreement number 247762, from the Span-
ish Ministry of Science and Innovation through the
Buceador project (TEC2009-14094-C04-01) and
the Juan de la Cierva fellowship program. The
authors also wants to thank the Barcelona Media
Innovation Centre for its support and permission
to publish this research.
References
Jordi Atserias, Bernardino Casas, Elisabet
Comelles, Meritxell Gonz?lez, Llu?s Padr?, and
Muntsa Padr?. 2006. FreeLing 1.3: Syntactic
and semantic services in an open-source NLP
101
Original: sabiendo que est? recibiendo el premio
Baseline: knowing that it receive the prize
Augmented: knowing that he is receiving the prize
Original: muchos de mis amigos prefieren no separarla.
Baseline: many of my friends prefer not to separate them.
Augmented: many of my friends prefer not to separate it.
Original: Los estadounidenses contar?n con un tel?fono m?vil
Baseline: The Americans have a mobile phone
Augmented: The Americans will have a mobile phone
Original: es plenamente consciente del camino m?s largo que debe emprender
Baseline: is fully aware of the longest journey must undertake
Augmented: is fully aware of the longest journey that need to be taken
Table 5: Comparison between baseline and augmented outputs
library. In Proceedings of the fifth interna-
tional conference on Language Resources and
Evaluation (LREC 2006), ELRA, Genoa, Italy,
May.
Alexandra Birch, Miles Osborne, and Philipp
Koehn. 2007. Ccg supertags in factored statis-
tical machine translation. In StatMT '07: Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, pages 916, Morristown,
NJ, USA. Association for Computational Lin-
guistics.
Thorsten Brants. 2000. TnT  a statistical part-
of-speech tagger. In Proceedings of the Sixth
Applied Natural Language Processing (ANLP-
2000), Seattle, WA.
Marta R. Costa-juss?, Vidas Daudaravicius, and
Rafael E. Banchs. 2010. Integration of statisti-
cal collocation segmentations in a phrase-based
statistical machine translation system. In 14th
Annual Conference of the European Association
for Machine Translation.
Vidas Daudaravicius and Ruta Marcinkeviciene.
2004. Gravity counts for the boundaries of col-
locations. International Journal of Corpus Lin-
guistics, 9:321348(28).
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ond?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In ACL '07: Proceedings of
the 45th Annual Meeting of the ACL on Interac-
tive Poster and Demonstration Sessions, pages
177180, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Machine
Translation Summit.
Franz Josef Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical Align-
ment Models. Computational Linguistics, 29:19
51.
Jos? A. R. Fonollosa, Maxim Khalilov, Marta R.
Costa-juss?, Jos? B. Mari?o, Carlos A. Hen-
r?quez Q., Adolfo Hern?ndez H., and Rafael E.
Banchs. 2009. The TALP-UPC phrase-based
translation system for EACL-WMT 2009. In
Proceedings of the Fourth Workshop on Statis-
tical Machine Translation, pages 8589, Athens,
Greece, March. Association for Computational
Linguistics.
Frank A. Smadja, Kathleen McKeown, and
Vasileios Hatzivassiloglou. 1996. Translating
collocations for bilingual lexicons: A statistical
approach. Computational Linguistics, 22(1):1
38.
Frank Smadja. 1993. Retrieving collocations from
text: Xtract. Comput. Linguist., 19(1):143177.
Andreas Stolcke. 2002. SRILM  an extensible
language modeling toolkit. pages 901904.
Christoph Tillman. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In
HLT-NAACL.
102
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 275?282,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The TALP-UPC phrase-based translation systems for WMT12: Morphologysimplification and domain adaptationLlu??s Formiga, Carlos A. Henr??quez Q., Adolfo Herna?ndez,Jose? B. Marin?o, Enric Monte and Jose? A. R. Fonollosa
TALP Research Centre
Universitat Polite`cnica de Catalunya
Barcelona, Spain
{lluis.formiga,carlos.henriquez,adolfo.hernandezjose.marino,enric.monte,jose.fonollosa}@upc.eduAbstract
This paper describes the UPC participation in
the WMT 12 evaluation campaign. All sys-
tems presented are based on standard phrase-
based Moses systems. Variations adopted sev-
eral improvement techniques such as mor-
phology simplification and generation and do-
main adaptation. The morphology simpli-
fication overcomes the data sparsity prob-
lem when translating into morphologically-
rich languages such as Spanish by translat-
ing first to a morphology-simplified language
and secondly leave the morphology gener-
ation to an independent classification task.
The domain adaptation approach improves the
SMT system by adding new translation units
learned from MT-output and reference align-
ment. Results depict an improvement on TER,
METEOR, NIST and BLEU scores compared
to our baseline system, obtaining on the of-
ficial test set more benefits from the domain
adaptation approach than from the morpho-
logical generalization method.1 Introduction
TALP-UPC (Center of Speech and Language
Applications and Technology at the Universitat
Polite`cnica de Catalunya) has participated in the
WMT12 shared task translating across two direc-
tions: English to Spanish and Spanish to English
tasks.
For the Spanish to English task we submitted a
baseline system that uses all parallel training data
and a combination of different target language mod-
els (LM) and Part-Of-Speech (POS) language mod-
els. A similar configuration was submitted for the
English to Spanish task as baseline. Our main ap-
proaches enriched the latter baseline in two indepen-
dent ways: morphology simplification and domain
adaptation by deriving new units into the phrase-
table. Furthermore, additional specific strategies
have been addressed on all systems to deal with well
known linguistic phenomena in Spanish such as cli-
tics and contractions.
The paper is presented as follows. Section 2
presents the main rationale for the phrase-based sys-
tem and the main pipeline of our baseline system.
Section 3 presents the approaches taken to improve
the baseline system on the English to Spanish task.
Section 4 presents the obtained results on internal
and official test sets while conclusions and further
work are presented in Section 5.2 Baseline system: Phrase-Based SMT
Classically, a phrase-based translation system im-
plements a log-linear model in which a foreign lan-
guage sentence f j1 = f1, f2, . . . , fj is translated into
another language sentence eI1 = e1, e2, . . . , eI by
searching for the translation hypothesis that max-
imizes a log-linear combination of feature models
(Brown et al, 1990):
e?I1 = argmaxeI1 ( MXm=1 mhm  eI1, fJ1  ) (1)
where the separate feature functions hm refer to
the system models and the set of  m refers to the
weights corresponding to these models. As fea-
ture functions we used the standard models available
275
the$ NATO$ mission$ officially$ ended$
la$ misi?n$ de$ la$ OTAN$ termin?$ oficialmente$
DAFS$ NCFS$ SPS$ DAFS$ NP$ VMIS3S0$ RG$
Figure 1: Factored phrase-based MT based on trans-
lation from surface to surface and Part-of-Speech
on Moses, i.e., relative frequencies, lexical weights,
word and phrase penalty, wbe-msd-bidirectional-fe
reordering models and two language models, one for
surface and one for POS tags. Phrase scoring was
computed using Good-Turing discounting (Foster et
al., 2006).
The tuning process was done using MERT (Och,
2003) with Minimum Bayes-Risk decoding (MBR)
(Kumar and Bryne, 2004) on Moses and focusing on
minimizing the BLEU score (Papineni et al, 2002)
of the development set. Final translations were also
computed using MBR decoding.
Additionally to the settings mentioned before, we
worked with a factored version of the corpus. Fac-
tored corpora augments surface forms with addi-
tional information, such as POS tags or lemmas as
shown in Figure 1. In that case, factors other than
surface (e.g. POS) are usually less sparse, allowing
to build factor-specific language models with higher-
order n-grams. These higher-order language models
usually help to obtain more syntactically correct out-
put. Concretely we map input source surfaces to tar-
get surfaces and POS tags.2.1 Corpus used
The baseline system was trained using all paral-
lel corpora, i.e. the European Parliament (EPPS)
(Koehn, 2005), News Commentary and United Na-
tions. Table 1 shows the statistics of the training data
after the cleaning process described later on Subsec-
tion 2.2.
Regarding the monolingual data, there was also
more News corpora separated by years for Spanish
and English and there was the Gigaword monolin-
gual corpus for English. All data can be found on
the Translation Task?s website1. We used all News
corpora (and Gigaword for English) to build the lan-
1http://www.statmt.org/wmt12/translation-task.html
Corpus Sent. Words Vocab. avg.len.
EPPS
Eng
1.90 M
49.40 M 124.03 k 26.05
Spa 52.66 M 154.67 k 27.28
News.Com
Eng
0.15 M
3.73 M 62.70 k 24.20
Spa 4.33 M 73.97 k 28.09
UN
Eng
8.38 M
205.68 M 575.04 k 24.54
Spa 239.40 M 598.54 k 28.56
Table 1: English-Spanish corpora statistics for
NAACL-WMT 2012 after cleaning process
guage model. Initially, a LM was built for every cor-
pus and then they were combined to produce de final
LM. Table 2 presents the statistics of each corpora,
again after the cleaning process.
Corpus Sent. Words Vocab.
EPPS
Eng 2.22 M 59.88 M 144.03 k
Spa 2.12 M 61.97 M 174.92 k
News.Com.
Eng 0.21 M 5.08 M 72.55 k
Spa 0.18 M 5.24 M 81.56 k
UN
Eng 11.20 M 315.90 M 767.12 k
Spa 11.20 M 372.21 M 725.73 k
News.07
Eng 3.79 M 90.25 M 711.55 k
Spa 0.05 M 1.33 M 64.10 k
News.08
Eng 13.01 M 308.82 M 1555.53 k
Spa 1.71 M 49.97 M 377.56 k
News.09
Eng 14.75 M 348.24 M 1648.05 k
Spa 1.07 M 30.57 M 287.81 k
News.10
Eng 6.81 M 158.15 M 915.14 k
Spa 0.69 M 19.58 M 226.76 k
News.11
Eng 13.46 M 312.50 M 1345.79 k
Spa 5.11 M 151.06 M 668.63 k
Giga Eng 22.52 M 657.88 M 3860.67 k
Table 2: Details of monolingual corpora used for
building language-models.
For internal testing we used the News 2011?s data
and concatenated the remaining three years of News
data as a single parallel corpus for development. Ta-
ble 3 shows the statistics for these two sets and in-
cludes in the last rows the statistics of the official test
set for this year?s translation task.2.2 Corpus processing
All corpora were processed in order to remove or
normalize ambiguous or special characters such as
quotes and spaces. Among other TALP-UPC spe-
cific scripts, we used a modified version of the
normalized-punctuation script provided by the orga-
nizers in order to skip the reordering rules which in-
volved quotes and stop punctuation signs.
276
Corpus Sent. Words Vocab. avg.len.
dev
Eng
7.57 k
189.01 k 18.61 k 24.98
Spa 202.80 k 21.75 k 26.80
test11
Eng
3.00 k
74.73 k 10.82 k 24.88
Spa 81.01 k 12.16 k 26.98
test12
Eng
3.00 k
72.91 k 10.24 k 24.28
Spa 80.38 k 12.02 k 26.77
Table 3: Detail of development and test corpora used
to tune and test the system.
POS-Tagging and tokenization for both Spanish
and English data sets were obtained using FreeLing
(Padro? et al, 2010). Freeling tokenization is able
to deal with contractions (?del? ! ?de el?) and cli-
tics separation (?co?mpramelo? ! ?compra me lo?)
in Spanish and English. Stemming was performed
using Snowball (Porter, 2001).
Surface text was lowercased conditionally based
on the POS tagging: proper nouns and adjectives
were separated from other POS categories to deter-
mine if a string should be fully lowercased (no spe-
cial property), partially lowercased (proper noun or
adjective) or not lowercased at all (acronym).
Bilingual corpora were cleaned with clean-
corpus-n script of Moses (Koehn et al, 2007) re-
moving all sentence pair with more than 70 words
in any language, considering the already tokenized
data. That script also ensures a maximum length
ratio below of nine (9) words between source and
target sentences.
Postprocessing in both languages consisted of a
recasing step using Moses recaser script. Further-
more we built an additional script in order to check
the casing of output names with respect to source
sentence names and case them accordingly, with ex-
ception of names placed at beginning of the sen-
tence. After recasing, a final detokenization step
was performed using standard Moses tools. Span-
ish postprocessing also included two special scripts
to recover contractions and clitics.2.3 Language Model and alignmentconfiguration
Word alignment was performed at stem level with
GIZA++ toolkit (Och and Ney, 2003) and grow-
diag-final-and joint alignment.
Language models were built from the monolin-
gual data provided covering different domains: Eu-
roparl, News and UN. We built them using Kneser-
Ney algorithm (Chen and Goodman, 1999), inter-
polation in order to avoid over-fitting and consider-
ing unknown words. First we built a 5-gram lan-
guage model for each corpus; then, the final LM
was obtained interpolating them all towards the de-
velopment set. We used SRI Language Model (Stol-
cke, 2002) toolkit, which provides compute-best-mix
script for the interpolation.
The POS language model was built analogously
to the surface language with some variants: it was a7-gram LM, without discounting nor interpolation.3 Improvement strategies3.1 Motivations
In order to improve the baseline system we present
two different strategies. First we present an im-
provement strategy based on morphology simplifi-
cation plus generation to deal with the problems
raised by morphological rich languages such as
Spanish. Second we present a domain adaptation
strategy that consists in deriving new units into the
phrase-table.3.2 Morphology simplification
The first improvement strategy is based on morphol-
ogy simplification when translating from English to
Spanish.
The problems raised when translating from a lan-
guage such as English into richer morphology lan-
guages are well known and are a research line of
interest nowadays (Popovic and Ney, 2004; Koehn
and Hoang, 2007; de Gispert and Marin?o, 2008;
Toutanova et al, 2008; Clifton and Sarkar, 2011). In
that direction, inflection causes a very large target-
language lexicon with a significant data sparsity
problem. In addition, system output is limited only
to the inflected phrases available in the parallel train-
ing corpus. Hence, SMT systems cannot gener-
ate proper inflections unless they have learned them
from the appropriate phrases. That would require to
have a parallel corpus containing all possible word
inflections for all phrases available, which it is an
unfeasible task.
The morphology related problems in MT have
been addressed from different approaches and may
277
????????????? ????????????????
??????????????????????? ???????????? ??????
??????????????????????????????
???????
???????
??????????????????????????
????????????????????????????????? ?? ???????????? ?? ??
????!???"#???$??%&%?$?%?'
????????????????????????
??????????????????????????????
????????????????????
%')???')?'??'????????????????? ?$?*
??)???
??????????????????????
?+$?,$???"?'??$?%?'?$??,?$?#
??$%'%'"???? ??????
Figure 2: Above, flow diagram of the training of simplified morphology translation models. Below, Spanish
morphology generation as an independent classification task.Type Text
PLAIN la Comisio?n puede llegar
TARGET: a paralizar el programa
TARGET+PoS la Comisio?n VMIP3S0[poder]
(Gen. Sur.): llegar a paralizar el programa
TARGET+PoS la Comisio?n VMIPpn0[poder]
(Simpl. PoS): llegar a paralizar el programa
Table 4: Example of morphology simplification
steps taken for Spanish verbs.
be summarized in four categories: i) factored mod-
els (Koehn and Hoang, 2007), enriched input mod-
els (Avramidis and Koehn, 2008; Ueffing and Ney,
2003), segmented translation (Virpioja et al, 2007)
and morphology generation (Toutanova et al, 2008;
de Gispert and Marin?o, 2008).
Our strategy for dealing with morphology gener-
ation is based in the latter approach (de Gispert and
Marin?o, 2008) (Figure 2). We center our strategy in
simplifying only verb forms as previous studies in-
dicate that they contribute to the main improvement
(Ueffing and Ney, 2003; de Gispert and Marin?o,
2008). That strategy makes clear the real impact
of morphology simplification by providing an upper
bound oracle for the studied scenarios.
The approach is as follows: First, target verbs
are simplified substituting them with their sim-
plified forms (Table 4). In this example, the
verb form ?puede? (he can) is transformed into
?VMIPpn0[poder]?, indicating simplified POS and
base form (lemma); where ?p? and ?n? represent any
person and number once simplified (from 3rd per-
son singular). Secondly, standard MT models are
obtained from English into simplified morphology
Spanish. Morphology prediction acts as a black box,
with its models estimated over a simplified morphol-
ogy parallel texts (including target language model
and lexicon models).
Generation is implemented by Decision Directed
Acyclic Graphs (DDAG) (Platt et al, 2000) com-
pound of binary SVM classifiers. In detail, a DDAG
combines many two-class classifiers to a multi-
classification task (Herna?ndez et al, 2010).3.3 Domain adaptation
Depending on the available resources, different do-
main adaptation techniques are possible. Usually,
the baseline system is built with a large out-of-
domain corpus (in our case the European Parlia-
ment) and we aim to adapt to another domain that
has limited data, either only monolingual or hope-
fully bilingual as well. The WMT Translation Task
focuses on adapting the system to a news domain,
offering an in-domain parallel corpus to work with.
In case of additional target monolingual data, pre-
vious works have focused on language model inter-
polations (Bulyko et al, 2007; Mohit et al, 2009;
Wu et al, 2008). When parallel in-domain data
is available, the latest researches have focused on
mixture model adaptation of the translation model
(Civera and Juan, 2007; Foster and Kuhn, 2007; Fos-
ter et al, 2010). Our work is closer to the latest ap-
278
proaches. We used the in-domain parallel data to
adapt the translation model, but focusing on the de-
coding errors that the out-of-domain baseline system
made while translating the in-domain corpus. The
idea is to detect where the system made its mistakes
and use the in-domain data to teach it how to correct
them.
Our approach began with a baseline system built
with the Parliament and the United Nations parallel
corpora but without the News parallel corpus. The
rest of the configuration remained the same for the
baseline. With this alternative baseline system, we
translated the source side of the News parallel cor-
pus to obtain a revised corpus of it, as defined in
(Henr??quez Q. et al, 2011). The revised corpus con-
sists of the source side, the output translation and the
target side, also called the target correction. The out-
put translation and its reference are then compare to
detect possible mistakes that the system caused dur-
ing decoding.
The translation was used as a pivot to find a word-
to-word alignment between the source side and the
target correction. The word-to-word alignment be-
tween source side and translation was provided by
Moses during decoding. The word-to-word align-
ment between the output translation and target cor-
rection was obtained following these steps:
1. Translation Edit Rate (Snover et al, 2006) be-
tween each output translation and target correc-
tion sentence pair was computed to obtain its
edit path and detect which words do not change
between sentences. Words that did not change
were directly linked
2. Going from left to right, for each unaligned
word wout on the output translation sentence
and each word wtrg on the target correction
sentence, a similarity function was computed
between them and wout got aligned with the
word wtrg that maximized this similarity.
The similarity function was defined as a linear
combination of features that considered if the words
wout and wtrg were identical, if the previous or fol-
lowing word of any of them were aligned with each
other and a lexical weight between them using the
bilingual lexical features from the baseline as refer-
ences.
With both word-to-word alignments computed for
a sentence pair, we linked source word wsrc with tar-
get word wtrg is and only if exists a output transla-
tion word wout such that there is a link between wsrc
and wout and a link between wout and wtrg.
After aligning the corpus, we built the transla-
tion and reordering model of it, using the baseline
settings. We called these translation and reorder-
ing models, revised models. They include phrases
found in the baseline that were correctly chosen dur-
ing decoding and also new phrases that came from
the differences between the output translation and its
correction.
Finally, the revised translation model features
were linearly combined with their corresponding
baseline features to build the final translation model,
called the derived translation model. The combina-
tion was computed in the following way:
hid(s, t) = ?hib(s, t) + (1  ?)hir(s, t) (2)
where hid(s, t) is the derived feature function i for
the bilingual phrase (s, t), hib(s, t) is the baseline
feature function of and hir(s, t) the revised feature
function. A value of ? = 0.60 was chosen after de-
termining it was the one that maximized the BLEU
score of the development set during tuning. Differ-
ent values for ? were considered, between 0.50 and0.95 with increments of 0.05 between them.
Regarding the reordering model, we added the un-
seen phrases from the revised reordering model into
the baseline reordering model, leaving the remaining
baseline phrase reordering weights intact.4 Results4.1 Language Model perplexities
LM
Perplexity
Surface POS
Baseline 205.36 13.23
Simplified 193.66 12.66
Table 6: Perplexities obtained across baseline and
morphology simplification.
Before evaluating translation performance, we
studied to what extent the morphology simplifica-
279
EN!ES
BLEU NIST TER METEOR
CS CI CS CI CS CI
test11
Baseline 30.7 32.53 7.820 8.120 57.19 55.05
Morph. Oracle 31.56 33.35 7.949 8.233 56.44 ?
Morph. Gen. 31.03 32.85 7.866 8.163 56.95 55.39
Adaptation 31.16 32.93 7.857 8.155 56.88 55.19
test12
Baseline 31.21 32.74 7.981 8.244 55.76 55.48
Morph. Oracle 32 33.41 8.090 8.339 55.15 ?
Morph. Gen. 31.46 32.98 8.010 8.274 55.62 55.66
Adaptation 31.73 33.24 8.037 8.294 55.37 55.82
(a) English!Spanish
ES!EN
BLEU NIST TER METEOR
CS CI CS CI CS CI
test11
Baseline
28.81 30.29 7.670 7.933 59.01 51.09
test12 32.27 33.81 8.014 8.282 56.26 53.96
(b) Spanish!English
Table 5: Automatic scores for English$Spanish translations. CS and CI indicate Case-Sensitive or Case-
Insensitive evaluations.
tion strategy may help decreasing the language mod-
els perplexity.
In table 6 we can see the effects of simplification.
Perplexity is computed from the corresponding in-
ternal test sets to the baseline or simplified language
models.
In general terms, the simplification process is
slightly effective, yielding an averaged improvement
of  5.02%.4.2 Translation performance
Evaluations were performed with different transla-
tion quality measures: BLEU, NIST, TER and ME-
TEOR (Denkowski and Lavie, 2011) which evalu-
ate distinct aspects of the quality of the translations.
First we evaluated the WMT11 test (test11) as an
internal indicator of our systems. Later we did the
same analysis with the WMT12 official test files.
Table 5 presents the obtained results. Experi-
ments began building the baseline system, which
included the special treatment for clitics, contrac-
tions and casing as described in Section 2.2. Once
the baseline was set, we proceeded with two paral-
lel lines, one for morphology simplification and the
other for domain adaptation.
For morphology generation approach (Table 5)
oracles (Morph. Oracle) represent how much gain
we could expect with a perfect generation module
and generation (Morph. Gen.) represent the actual
performance combining simplification and the gen-
eration strategies. Oracles achieve a promising av-
eraged improvement of +1.79% (depending on the
metric or the test set) with respect to the baseline.
However, generation only improves the baseline by
a +0.61%, encouraging us to keep working on that
strategy.
Regarding the domain adaptation approach, we
evaluated the internal test set (test11). As we can
see again on Table 5a the adaptation strategy outper-
forms the baseline on all quality measures starting
with an averaged gain of +0.94%.
Comparing the two approaches, we can see that
the domain adaptation method was better in terms of
BLEU score and TER than the morphology genera-
tion but the latter was better on NIST and METEOR
on our internal test set. This made us decided for the
latter as the primary system submitted, leaving the
domain adaptation approach system as a contrastive
submission. Additionally to the automatic quality
measures, we are particularly interested in the man-
ual evaluation results, as we believe the morphology
generation will be more sensitive to this type of eval-
280
uation than to automatic metrics.
Official results (test12) can be found on Table 5b.
Surprisingly, this time the domain adaptation ap-
proach performed better than the morphology sim-
plification on all metrics: BLEU, NIST, TER and
METEOR, with an averaged gain of +1.04% over
the baseline system, which ranks our submissions
second and third in terms of BLEU scores (con-
trastive and primary respectively) when compared
with all other submissions for the WMT12 transla-
tion task.5 Conclusions and further work
This papers describes the UPC participation during
the 2012 WMT?s Translation Task. We have partici-
pated with a baseline system for Spanish-to-English,
a baseline system for English-to-Spanish and two in-
dependent enhancements to the baseline system for
English-to-Spanish as well.
Our primary submission applied morphology sim-
plification and generation with the objective of ease
the translation process when dealing with rich mor-
phology languages like Spanish, deferring the mor-
phology generation as an external post-process clas-
sification task.
The second approach focused on domain adapta-
tion. Instead of concatenating the training News par-
allel data together with the European Parliament and
United Nations, a preliminary system was built with
the latter two and separated translation and reorder-
ing models were computed using the News parallel
data. These models were then added to the prelimi-
nary models in order to build the adapted system.
Results showed that both approaches performed
better than the baseline system, being the domain
adaptation configuration the one that performed bet-
ter for 2012 test in terms of all automatic quality
indicators: BLEU, NIST, TER and METEOR. We
look forward the the manual evaluation results as we
believe our primary system may be more sensitive to
this type of human evaluation.
Future work should focus on combining the two
approaches, applying first morphological general-
ization to the training data and then using the domain
adaptation technique on the resulting corpora in or-
der to determine the joined benefits of both strate-
gies.
Acknowledgments
This work has been partially funded by the Spanish
Government (Buceador, TEC2009-14094-C04-01)
and the European Community?s FP7 program under
grant agreement number 247762 (FAUST, FP7-ICT-
2009-4-247762).References
E. Avramidis and P. Koehn. 2008. Enriching morpho-
logically poor languages for statistical machine trans-
lation. Proceedings of ACL-08: HLT, pages 763?770.
P.F. Brown, J. Cocke, S.A.D. Pietra, V.J.D. Pietra, F. Je-
linek, J.D. Lafferty, R.L. Mercer, and P.S. Roossin.
1990. A statistical approach to machine translation.
Computational linguistics, 16(2):79?85.
Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, Long
Nguyen, and John Makhoul. 2007. Language model
adaptation in machine translation from speech. Test,
4:117?120.
S.F. Chen and J. Goodman. 1999. An empirical study of
smoothing techniques for language modeling. Com-
puter Speech & Language, 13(4):359?393.
Jorge Civera and Alfons Juan. 2007. Domain adaptation
in statistical machine translation with mixture mod-
elling. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, StatMT ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
A. Clifton and A. Sarkar. 2011. Combining morpheme-
based machine translation with post-processing mor-
pheme prediction. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies. Portland, OR,
USA.
Adria` de de Gispert and Jose? B. Marin?o. 2008. On the
impact of morphology in English to Spanish statistical
MT. Speech Communication, 50(11-12):1034?1046.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proceed-
ings of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation.
George Foster and Roland Kuhn. 2007. Mixture-Model
Adaptation For SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, StatMT
?07, pages 128?135, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
281
EMNLP ?06, pages 53?61, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Carlos A. Henr??quez Q., Jose? B. Marin?o, and Rafael E.
Banchs. 2011. Deriving translation units using small
additional corpora. In Proceedings of the 15th Confer-
ence of the European Association for Machine Trans-
lation.
Adolfo Herna?ndez, Enric Monte, and Jose? B. Marin?o.
2010. Multiclass classification for Morphology gener-
ation in statistical machine translation. In Proceedings
of the VI Jornadas en Tecnolog??a del Habla? and II
Iberian SLTech Workshop, pages 179?182, November.
http://fala2010.uvigo.es.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868?876, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177?
180. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Transla-
tion Summit.
Shankar Kumar and William Bryne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proceedings of the Human Language Technology
and North American Association for Computational
Linguistics Conference (HLT/NAACL), Boston,MA,
May 27-June 1.
Behrang Mohit, Frank Liberato, and Rebecca Hwa.
2009. Language Model Adaptation for Difficult to
Translate Phrases. In Proceedings of the 13th Annual
Conference of the EAMT.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL).
Llu??s Padro?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castello?n. 2010. Freeling 2.1:
Five years of open-source language processing tools.
In Proceedings of 7th Language Resources and Evalu-
ation Conference (LREC 2010), La Valletta, MALTA,
May. ELRA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL).
John C. Platt, Nello Cristianini, and John Shawe-taylor.
2000. Large margin DAGs for multiclass classifica-
tion. In Advances in Neural Information Processing
Systems, pages 547?553. MIT Press.
Maja Popovic and Hermann Ney. 2004. Towards the
use of word stems and suffixes for statistical machine
translation. In Proceedings of the 4th International
Conference on Language Resources and Evaluation,
LREC?04, pages 1585?1588, May.
M. Porter. 2001. Snowball: A language for stemming
algorithms.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas.
A. Stolcke. 2002. Srilm-an extensible language mod-
eling toolkit. In Seventh International Conference on
Spoken Language Processing.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08: HLT,
pages 514?522, Columbus, Ohio, June. Association
for Computational Linguistics.
Nicola Ueffing and Hermann Ney. 2003. Using pos in-
formation for statistical machine translation into mor-
phologically rich languages. In Proceedings of the
tenth conference on European chapter of the Associa-
tion for Computational Linguistics - Volume 1, EACL
?03, pages 347?354, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
S. Virpioja, J.J. Va?yrynen, M. Creutz, and M. Sadeniemi.
2007. Morphology-aware statistical machine transla-
tion based on morphs induced in an unsupervised man-
ner. Machine Translation Summit XI, 2007:491?498.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08,
pages 993?1000, Stroudsburg, PA, USA. Association
for Computational Linguistics.
282
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 134?140,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The TALP-UPC Phrase-based Translation Systems for WMT13:
System Combination with Morphology Generation,
Domain Adaptation and Corpus Filtering
Llu??s Formiga?, Marta R. Costa-jussa`?, Jose? B. Marin?o?
Jose? A. R. Fonollosa?, Alberto Barro?n-Ceden?o??, Llu??s Ma`rquez?
?TALP Research Centre ?Facultad de Informa?tica
Universitat Polite`cnica de Catalunya Universidad Polite?cnica de Madrid
Barcelona, Spain Madrid, Spain
{lluis.formiga,marta.ruiz,jose.marino,jose.fonollosa}@upc.edu
{albarron, lluism}@lsi.upc.edu
Abstract
This paper describes the TALP participa-
tion in the WMT13 evaluation campaign.
Our participation is based on the combi-
nation of several statistical machine trans-
lation systems: based on standard phrase-
based Moses systems. Variations include
techniques such as morphology genera-
tion, training sentence filtering, and do-
main adaptation through unit derivation.
The results show a coherent improvement
on TER, METEOR, NIST, and BLEU
scores when compared to our baseline sys-
tem.
1 Introduction
The TALP-UPC center (Center for Language and
Speech Technologies and Applications at Univer-
sitat Polite`cnica de Catalunya) focused on the En-
glish to Spanish translation of the WMT13 shared
task.
Our primary (contrastive) run is an internal
system selection comprised of different train-
ing approaches (without CommonCrawl, unless
stated): (a) Moses Baseline (Koehn et al,
2007b), (b) Moses Baseline + Morphology Gener-
ation (Formiga et al, 2012b), (c) Moses Baseline
+ News Adaptation (Henr??quez Q. et al, 2011),
(d) Moses Baseline + News Adaptation + Mor-
phology Generation , and (e) Moses Baseline +
News Adaptation + Filtered CommonCrawl Adap-
tation (Barro?n-Ceden?o et al, 2013). Our sec-
ondary run includes is the full training strategy
marked as (e) in the previous description.
The main differences with respect to our last
year?s participation (Formiga et al, 2012a) are: i)
the inclusion of the CommonCrawl corpus, using
a sentence filtering technique and the system com-
bination itself, and ii) a system selection scheme
to select the best translation among the different
configurations.
The paper is organized as follows. Section 2
presents the phrase-based system and the main
pipeline of our baseline system. Section 3 de-
scribes the our approaches to improve the baseline
system on the English-to-Spanish task (special at-
tention is given to the approaches that differ from
last year). Section 4 presents the system combi-
nation approach once the best candidate phrase of
the different subsystems are selected. Section 5
discusses the obtained results considering both in-
ternal and official test sets. Section 6 includes con-
clusions and further work.
2 Baseline system: Phrase-Based SMT
Our contribution is a follow up of our last year par-
ticipation (Formiga et al, 2012a), based on a fac-
tored Moses from English to Spanish words plus
their Part-of-Speech (POS). Factored corpora aug-
ments words with additional information, such as
POS tags or lemmas. In that case, factors other
than surface (e.g. POS) are usually less sparse, al-
lowing the construction of factor-specific language
models with higher-order n-grams. Such language
models can help to obtain syntactically more cor-
rect outputs.
We used the standard models available in Moses
as feature functions: relative frequencies, lexi-
cal weights, word and phrase penalties, wbe-msd-
bidirectional-fe reordering models, and two lan-
guage models (one for surface and one for POS
tags). Phrase scoring was computed using Good-
Turing discounting (Foster et al, 2006).
As aforementioned, we developed five factored
Moses-based independent systems with different
134
approaches. We explain them in Section 3. As
a final decision, we applied a system selection
scheme (Formiga et al, 2013; Specia et al, 2010)
to consider the best candidate for each sentence,
according to human trained quality estimation
(QE) models. We set monotone reordering of
the punctuation signs for the decoding using the
Moses wall feature.
We tuned the systems using the Moses
MERT (Och, 2003) implementation. Our focus
was on minimizing the BLEU score (Papineni et
al., 2002) of the development set. Still, for ex-
ploratory purposes, we tuned configuration (c) us-
ing PRO (Hopkins and May, 2011) to set the ini-
tial weights at every iteration of the MERT algo-
rithm. However, it showed no significant differ-
ences compared to the original MERT implemen-
tation.
We trained the baseline system using all
the available parallel corpora, except for
common-crawl. That is, European Parlia-
ment (EPPS) (Koehn, 2005), News Commentary,
and United Nations. Regarding the monolingual
data, there were more News corpora organized
by years for Spanish. The data is available at
the Translation Task?s website1. We used all
the News corpora to busld the language model
(LM). Firstly, a LM was built for every corpus
independently. Afterwards, they were combined
to produce de final LM.
For internal testing we used the News 2011 and
News 2012 data and concatenated the remaining
three years of News data as a single parallel corpus
for development.
We processed the corpora as in our participa-
tion to WMT12 (Formiga et al, 2012a). Tok-
enization and POS-tagging in both Spanish and
English was obtained with FreeLing (Padro? et al,
2010). Stemming was carried out with Snow-
ball (Porter, 2001). Words were conditionally case
folded based on their POS: proper nouns and ad-
jectives were separated from other categories to
determine whether a string should be fully folded
(no special property), partially folded (noun or ad-
jective) or not folded at all in (acronym).
Bilingual corpora was filtered with the clean-
corpus-n script of Moses (Koehn et al, 2007a), re-
moving those pairs in which a sentence was longer
than 70. For the CommonCrawl corpus we used a
more complex filtering step (cf. Section 3.3).
1http://www.statmt.org/wmt13/translation-task.html
Postprocessing included two special scripts to
recover contractions and clitics. Detruecasing was
done forcing the capitals after the punctuation
signs. Furthermore we used an additional script in
order to check the casing of output names with re-
spect to the source. We reused our language mod-
els and alignments (with stems) from WMT12.
3 Improvement strategies
We tried three different strategies to improve the
baseline system. Section 3.1 shows a strategy
based on morphology simplification plus genera-
tion. Its aim is dealing with the problems raised
by morphology-rich languages, such as Spanish.
Section 3.2 presents a domain?adaptation strategy
that consists of deriving new units. Section 3.3
presents an advanced strategy to filter the good bi-
sentences from the CommonCrawl corpus, which
might be useful to perform the domain adaptation.
3.1 Morphology generation
Following the success of our WMT12 participa-
tion (Formiga et al, 2012a), our first improve-
ment is based on the morphology generalization
and generation approach (Formiga et al, 2012b).
We focus our strategy on simplifying verb forms
only.
The approach first translates into Spanish sim-
plified forms (de Gispert and Marin?o, 2008). The
final inflected forms are predicted through a mor-
phology generation step, based on the shallow
and deep-projected linguistic information avail-
able from both source and target language sen-
tences.
Lexical sparseness is a crucial aspect to deal
with for an open-domain robust SMT when trans-
lating to morphology-rich languages (e.g. Span-
ish) . We knew beforehand (Formiga et al, 2012b)
that morphology generalization is a good method
to deal with generic translations and it provides
stability to translations of the training domain.
Our morphology prediction (generation) sys-
tems are trained with the WMT13 corpora (Eu-
roparl, News, and UN) together with noisy data
(OpenSubtitles). This combination helps to obtain
better translations without compromising the qual-
ity of the translation models. These kind of mor-
phology generation systems are trained with a rel-
atively short amount of parallel data compared to
standard SMT training corpora.
Our main enhancement to this strategy is the
135
addition of source-projected deep features to the
target sentence in order to perform the morphol-
ogy prediction. These features are Dependency
Features and Semantic Role Labelling, obtained
from the source sentence through Lund Depen-
dency Parser2. These features are then projected
to the target sentence as explained in (Formiga et
al., 2012b).
Projected deep features are important to pre-
dict the correct verb morphology from clean and
fluent text. However, the projection of deep fea-
tures is sentence-fluency sensitive, making it un-
reliable when the baseline MT output is poor. In
other words, the morphology generation strategy
becomes more relevant with high-quality MT de-
coders, as their output is more fluent, making the
shallow and deep features more reliable classifier
guides.
3.2 Domain Adaptation through pivot
derived units
Usually the WMT Translation Task focuses on
adapting a system to a news domain, offering an
in-domain parallel corpus to work with. How-
ever this corpus is relatively small compared to
the other corpora. In our previous participation
we demonstrated the need of performing a more
aggressive domain adaptation strategy. Our strat-
egy was based on using in-domain parallel data to
adapt the translation model, but focusing on the
decoding errors that the out-of-domain baseline
system makes when translating the in-domain cor-
pus.
The idea is to identify the system mistakes and
use the in-domain data to learn how to correct
them. To that effect, we interpolate the transla-
tion models (phrase and lexical reordering tables)
with a new adapted translation model with derived
units. We obtained the units identifying the mis-
matching parts between the non-adapted transla-
tion and the actual reference (Henr??quez Q. et al,
2011). This derivation approach uses the origi-
nal translation as a pivot to find a word-to-word
alignment between the source side and the target
correction (word-to-word alignment provided by
Moses during decoding).
The word-to-word monolingual alignment be-
tween output translation target correction was ob-
tained combining different probabilities such as
i)lexical identity, ii) TER-based alignment links,
2http://nlp.cs.lth.se/software/
Corpus Sent. Words Vocab. avg.len.
Original EN 1.48M 29.44M 465.1k 19.90ES 31.6M 459.9k 21.45
Filtered EN 0.78M 15.3M 278.0k 19.72ES 16.6M 306.8k 21.37
Table 1: Commoncrawl corpora statistics for
WMT13 before and after filtering.
iii) lexical model probabilities, iv) char-based Lev-
enshtein distance between tokens and v) filtering
out those alignments from NULL to a stop word
(p = ??).
We empirically set the linear interpolation
weight as w = 0.60 for the baseline translation
models and w = 0.40 for the derived units trans-
lations models. We applied the pivot derived units
strategy to the News domain and to the filtered
Commoncrawl corpus (cf. Section 5). The proce-
dure to filter out the Commoncrawl corpus is ex-
plained next.
3.3 CommonCrawl Filtering
We used the CommonCrawl corpus, provided for
the first time by the organization, as an impor-
tant source of information for performing aggres-
sive domain adaptation. To decrease the impact
of the noise in the corpus, we performed an auto-
matic pre-selection of the supposedly more correct
(hence useful) sentence pairs: we applied the au-
tomatic quality estimation filters developed in the
context of the FAUST project3. The filters? pur-
pose is to identify cases in which the post-editions
provided by casual users really improve over auto-
matic translations.
The adaptation to the current framework is as
follows. Example selection is modelled as a bi-
nary classification problem. We consider triples
(src, ref , trans), where src and ref stand for the
source-reference sentences in the CommonCrawl
corpus and trans is an automatic translation of the
source, generated by our baseline SMT system. A
triple is assigned a positive label iff ref is a bet-
ter translation from src than trans. That is, if the
translation example provided by CommonCrawl is
better than the output of our baseline SMT system.
We used four feature sets to characterize the
three sentences and their relationships: sur-
face, back-translation, noise-based and similarity-
based. These features try to capture (a) the simi-
larity between the different texts on the basis of
3http://www.faust-fp7.eu
136
diverse measures, (b) the length of the different
sentences (including ratios), and (c) the likelihood
of a source or target text to include noisy text.4
Most of them are simple, fast-calculation and
language-independent features. However, back-
translation features require that trans and ref are
back-translated into the source language. We did
it by using the TALP es-en system from WMT12.
Considering these features, we trained lin-
ear Support Vector Machines using SVMlight
(Joachims, 1999). Our training collection was the
FFF+ corpus, with +500 hundred manually anno-
tated instances (Barro?n-Ceden?o et al, 2013). No
adaptation to CommonCrawl was performed. To
give an idea, classification accuracy over the test
partition of the FFF+ corpus was only moderately
good (?70%). However, ranking by classification
score a fresh set of over 6,000 new examples, and
selecting the top ranked 50% examples to enrich a
state-of-the-art SMT system, allowed us to signifi-
cantly improve translation quality (Barro?n-Ceden?o
et al, 2013).
For WMT13, we applied these classifiers to
rank the CommonCrawl translation pairs and then
selected the top 53% instances to be processed by
the domain adaptation strategy. Table 1 displays
the corpus statistics before and after filtering.
4 System Combination
We approached system combination as a system
selection task. More concretely, we applied Qual-
ity Estimation (QE) models (Specia et al, 2010;
Formiga et al, 2013) to select the highest qual-
ity translation at sentence level among the trans-
lation candidates obtained by our different strate-
gies. The QE models are trained with human
supervision, making use of no system-dependent
features.
In a previous study (Formiga et al, 2013),
we showed the plausibility of building reliable
system-independent QE models from human an-
notations. This type of task should be addressed
with a pairwise ranking strategy, as it yields bet-
ter results than an absolute quality estimation ap-
proach (i.e., regression) for system selection. We
also found that training the quality estimation
models from human assessments, instead of au-
tomatic reference scores, helped to obtain better
4We refer the interested reader to (Barro?n-Ceden?o et al,
2013) for a detailed description of features, process, and eval-
uation.
models for system selection for both i) mimicking
the behavior of automatic metrics and ii) learning
the human behavior when ranking different trans-
lation candidates.
For training the QE models we used the data
from the WMT13 shared task on quality estima-
tion (System Selection Quality Estimation at Sen-
tence Level task5), which contains the test sets
from other WMT campaigns with human assess-
ments. We used five groups of features, namely:
i) QuestQE: 17 QE features provided by the Quest
toolkit6; ii) AsiyaQE: 26 QE features provided by
the Asiya toolkit for MT evaluation (Gime?nez and
Ma`rquez, 2010a); iii) LM (and LM-PoS) perplex-
ities trained with monolingual data; iv) PR: Clas-
sical lexical-based measures -BLEU (Papineni et
al., 2002), NIST (Doddington, 2002), and ME-
TEOR (Denkowski and Lavie, 2011)- computed
with a pseudo-reference approach, that is, using
the other system candidates as references (Sori-
cut and Echihabi, 2010); and v) PROTHER: Ref-
erence based metrics provided by Asiya, including
GTM, ROUGE, PER, TER (Snover et al, 2008),
and syntax-based evaluation measures also with a
pseudo-reference approach.
We trained a Support Vector Machine ranker by
means of pairwise comparison using the SVMlight
toolkit (Joachims, 1999), but with the ?-z p? pa-
rameter, which can provide system rankings for
all the members of different groups. The learner
algorithm was run according to the following pa-
rameters: linear kernel, expanding the working set
by 9 variables at each iteration, for a maximum of
50,000 iterations and with a cache size of 100 for
kernel evaluations. The trade-off parameter was
empirically set to 0.001.
Table 2 shows the contribution of different fea-
ture groups when training the QE models. For
evaluating performance, we used the Asiya nor-
malized linear combination metric ULC (Gime?nez
and Ma`rquez, 2010b), which combines BLEU,
NIST, and METEOR (with exact, paraphrases and
synonym variants). Within this scenario, it can
be observed that the quality estimation features
(QuestQE and AsiyaQE) did not obtain good re-
sults, perhaps because of the high similarity be-
tween the test candidates (Moses with different
configurations) in contrast to the strong differ-
ence between the candidates in training (Moses,
5http://www.quest.dcs.shef.ac.uk/wmt13 qe.html
6http://www.quest.dcs.shef.ac.uk
137
Features Asiya ULCWMT?11 WMT?12 AVG WMT?13
QuestQE 60.46 60.64 60.55 60.06
AsiyaQE 61.04 60.89 60.97 60.29
QuestQE+AsiyaQE 60.86 61.07 60.96 60.42
LM 60.84 60.63 60.74 60.37
QuestQE+AsiyaQE+LM 60.80 60.55 60.67 60.21
QuestQE+AsiyaQE+PR 60.97 61.12 61.05 60.54
QuestQE+AsiyaQE+PR+PROTHER 61.05 61.19 61.12 60.69
PR 61.24 61.08 61.16 61.04
PR+PROTHER 61.19 61.16 61.18 60.98
PR+PROTHER+LM 61.11 61.29 61.20 61.03
QuestQE+AsiyaQE+PR+PROTHER+LM 60.70 60.88 60.79 60.14
Table 2: System selection scores (ULC) obtained using QE models trained with different groups of
features. Results displayed for WMT11, WMT12 internal tests, their average, and the WMT13 test
EN?ES BLEU TER
wmt13 Primary 29.5 0.586
wmt13 Secondary 29.4 0.586
Table 4: Official automatic scores for the WMT13
English?Spanish translations.
RBMT, Jane, etc.). On the contrary, the pseudo-
reference-based features play a crucial role in the
proper performance of the QE model, confirming
the hypothesis that PR features need a clear dom-
inant system to be used as reference. The PR-
based configurations (with and without LM) had
no big differences between them. We choose the
best AVG result for the final system combination:
PR+PROTHER+LM, which it is consistent with
the actual WMT13 evaluated afterwards.
5 Results
Evaluations were performed considering different
quality measures: BLEU, NIST, TER, and ME-
TEOR in addition to an informal manual analy-
sis. This manifold of metrics evaluates distinct as-
pects of the translation. We evaluated both over
the WMT11 and WMT12 test sets as internal in-
dicators of our systems. We also give our perfor-
mance on the WMT13 test dataset.
Table 3 presents the obtained results for the
different strategies: (a) Moses Baseline (w/o
commoncrawl) (b) Moses Baseline+Morphology
Generation (w/o commoncrawl) (c) Moses Base-
line+News Adaptation through pivot based align-
ment (w/o commoncrawl) (d) Moses Baseline +
News Adaptation (b) + Morphology Generation
(c) (e) Moses Baseline + News Adaptation (b) +
Filtered CommonCrawl Adaptation.
The official results are in Table 4. Our primary
(contrastive) run is the system combination strat-
egy whereas our secondary run is the full training
strategy marked as (e) on the system combination.
Our primary system was ranked in the second clus-
ter out of ten constrained systems in the official
manual evaluation.
Independent analyzes of the improvement
strategies show that the highest improvement
comes from the CommonCrawl Filtering + Adap-
tation strategy (system e). The second best strat-
egy is the combination of the morphology pre-
diction system plus the news adaptation system.
However, for the WMT12 test the News Adap-
tation strategy contributes to main improvement
whereas for the WMT13 this major improvement
is achieved with the morphology strategy. Analyz-
ing the distance betweem each test set with respect
to the News and CommonCrawl domain to further
understand the behavior of each strategy seems an
interesting future work. Specifically, for further
contrasting the difference in the morphology ap-
proach, it would be nice to analyze the variation in
the verb inflection forms. Hypothetically, the per-
son or the number of the verb forms used may have
a higher tendency to be different in the WMT13
test set, implying that our morphology approach is
further exploited.
Regarding the system selection step (internal
WMT12 test), the only automatic metric that has
an improvement is TER. However, TER is one of
138
EN?ES BLEU NIST TER METEOR
wmt12 Baseline 32.97 8.27 49.27 49.91
wmt12 + Morphology Generation 33.03 8.29 49.02 50.01
wmt12 + News Adaptation 33.22 8.31 49.00 50.16
wmt12 + News Adaptation + Morphology Generation 33.29 8.32 48.83 50.29
wmt12 + News Adaptation + Filtered CommonCrawl Adaptation 33.61 8.35 48.82 50.52
wmt12 System Combination 33.43 8.34 48.78 50.44
wmt13 Baseline 29.02 7.72 51.92 46.96
wmt13 Morphology Generation 29.35 7.73 52.04 47.04
wmt13 News Adaptation 29.19 7.74 51.91 47.07
wmt13 News Adaptation + Morphology Generation 29.40 7.74 51.96 47.12
wmt13 News Adaptation + Filtered CommonCrawl Adaptation 29.47 7.77 51.82 47.22
wmt13 System Combination 29.54 7.77 51.76 47.34
Table 3: Automatic scores for English?Spanish translations.
the most reliable metrics according to human eval-
uation. Regarding the actual WMT13 test, the sys-
tem selection step is able to overcome all the auto-
matic metrics.
6 Conclusions and further work
This paper described the TALP-UPC participa-
tion for the English-to-Spanish WMT13 transla-
tion task. We applied the same systems as in last
year, but enhanced with new techniques: sentence
filtering and system combination.
Results showed that both approaches performed
better than the baseline system, being the sentence
filtering technique the one that most improvement
reached in terms of all the automatic quality indi-
cators: BLEU, NIST, TER, and METEOR. The
system combination was able to outperform the
independent systems which used morphological
knowledge and/or domain adaptation techniques.
As further work would like to focus on further
advancing on the morphology-based techniques.
Acknowledgments
This work has been supported in part by
Spanish Ministerio de Econom??a y Competitivi-
dad, contract TEC2012-38939-C03-02 as well
as from the European Regional Development
Fund (ERDF/FEDER) and the European Commu-
nity?s FP7 (2007-2013) program under the fol-
lowing grants: 247762 (FAUST, FP7-ICT-2009-
4-247762), 29951 (the International Outgoing
Fellowship Marie Curie Action ? IMTraP-2011-
29951) and 246016 (ERCIM ?Alain Bensoussan?
Fellowship).
References
Alberto Barro?n-Ceden?o, Llu??s Ma`rquez, Carlos A.
Henr??quez Q, Llu??s Formiga, Enrique Romero, and
Jonathan May. 2013. Identifying Useful Hu-
man Correction Feedback from an On-line Machine
Translation Service. In Proceedings of the Twenty-
Third International Joint Conference on Artificial
Intelligence. AAAI Press.
Adria` de de Gispert and Jose? B. Marin?o. 2008. On the
impact of morphology in English to Spanish statis-
tical MT. Speech Communication, 50(11-12):1034?
1046.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, HLT ?02, pages 138?145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Lluis Formiga, Carlos A. Henr??quez Q., Adolfo
Herna?ndez, Jose? B. Marin?o, Enric Monte, and Jose?
A. R. Fonollosa. 2012a. The TALP-UPC phrase-
based translation systems for WMT12: Morphol-
ogy simplification and domain adaptation. In Pro-
ceedings of the Seventh Workshop on Statistical
Machine Translation, pages 275?282, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
Llu??s Formiga, Adolfo Herna?ndez, Jose? B. Marin?, and
Enrique Monte. 2012b. Improving english to
spanish out-of-domain translations by morphology
generalization and generation. In Proceedings of
139
the AMTA Monolingual Machine Translation-2012
Workshop.
Llu??s Formiga, Llu??s Ma`rquez, and Jaume Pujantell.
2013. Real-life translation quality estimation for mt
system selection. In Proceedings of 14th Machine
Translation Summit (MT Summit), Nice, France,
September. EAMT.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?06, pages 53?61, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77?86.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010b. Linguistic
measures for automatic machine translation evalu-
ation. Machine Translation, 24(3-4):209?240, De-
cember.
Carlos A. Henr??quez Q., Jose? B. Marin?o, and Rafael E.
Banchs. 2011. Deriving translation units using
small additional corpora. In Proceedings of the 15th
Conference of the European Association for Ma-
chine Translation.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods
? Support Vector Learning, chapter Making large-
Scale SVM Learning Practical. MIT Press.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007a. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007b. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Trans-
lation Summit.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Llu??s Padro?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castello?n. 2010. Freeling
2.1: Five years of open-source language processing
tools. In Proceedings of 7th Language Resources
and Evaluation Conference (LREC 2010), La Val-
letta, MALTA, May. ELRA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
M. Porter. 2001. Snowball: A language for stemming
algorithms.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and Translation Model Adaptation
using Comparable Corpora. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation Versus Quality Es-
timation. Machine Translation, 24:39?50, March.
140
