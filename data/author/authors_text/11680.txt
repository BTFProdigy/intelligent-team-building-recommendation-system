Proceedings of NAACL HLT 2009: Short Papers, pages 177?180,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
 
 
Towards Effective Sentence Simplification for  
Automatic Processing of Biomedical Text  
 
Siddhartha Jonnalagadda*, Luis Tari**, J?rg Hakenberg**, Chitta Baral**, Graciela Gonzalez* 
*Department of Biomedical Informatics, Arizona State University, Phoenix, AZ 85004, USA. 
**Department of Computer Science and Engineering, Arizona State University, Tempe, AZ 85281, USA. 
 Corresponding author: ggonzalez@asu.edu 
 
 
 
 
Abstract 
The complexity of sentences characteristic to 
biomedical articles poses a challenge to natu-
ral language parsers, which are typically 
trained on large-scale corpora of non-technical 
text. We propose a text simplification process, 
bioSimplify, that seeks to reduce the complex-
ity of sentences in biomedical abstracts in or-
der to improve the performance of syntactic 
parsers on the processed sentences. Syntactic 
parsing is typically one of the first steps in a 
text mining pipeline. Thus, any improvement 
in performance would have a ripple effect 
over all processing steps. We evaluated our 
method using a corpus of biomedical sen-
tences annotated with syntactic links. Our em-
pirical results show an improvement of 2.90% 
for the Charniak-McClosky parser and of 
4.23% for the Link Grammar parser when 
processing simplified sentences rather than the 
original sentences in the corpus. 
1 Introduction 
It is typical that applications for biomedical text 
involve the use of natural language syntactic pars-
ers as one of the first steps in processing. Thus, the 
performance of the system as a whole is largely 
dependent on how well the natural language syn-
tactic parsers perform.  
One of the challenges in parsing biomedical text is 
that it is significantly more complex than articles in 
typical English text. Different analysis show other 
problematic characteristics, including inconsistent 
use of nouns and partial words (Tateisi & Tsujii, 
2004), higher perplexity measures (Elhadad, 2006), 
greater lexical density, plus increased number of 
relative clauses and prepositional phrases (Ge-
moets, 2004), all of which correlate with dimi-
nished comprehension and higher text difficulty.  
These characteristics also lead to performance 
problems in terms of computation time and accura-
cy for parsers that are trained on common English 
text corpus. 
  We identified three categories of sentences: 1) 
normal English sentences, like in Newswire text, 
2) normal biomedical English sentences ? those 
sentences which can be parsed without a problem 
by Link Grammar-, and 3) complex biomedical 
English sentences ? those sentences which can?t be 
parsed by Link Grammar. Aside from the known 
characteristics mentioned before, sentences in the 
third group tended to be longer (18% of them had 
more than 50 words, while only 8% of those in 
group 2 and 2% of those in group 1 did). It has 
been observed that parsers perform well with sen-
tences of reduced length (Chandrasekar & Srini-
vas, 1997; Siddharthan, 2006).  
  In this paper, we explore the use of text simplifi-
cation as a preprocessing step for general parsing 
to reduce length and complexity of biomedical sen-
tences in order to enhance the performance of the 
parsers.  
2 Methods  
There are currently many publicly available corpo-
ra of biomedical texts, the most popular among 
them being BioInfer, Genia, AImed, HPRD 50, 
IEPA, LLL and BioCreative1-PPI. Among these 
corpora, BioInfer includes the most comprehensive 
collection of sentences and careful annotation for 
links of natural parser, in both the Stanford and 
Link Grammar schemes. Therefore, we chose the 
BioInfer corpus, version 1.1.0 (Pyysalo et al, 
2007), containing 1100 sentences for evaluating 
the effectiveness of our simplification method on 
177
  
the performance of syntactic parsers. The method 
includes syntactic and non-syntactic transforma-
tions, detailed next. 
2.1 Non-syntactic transformation 
We group here three steps of our approach: 1. pre-
processing through removal of spurious phrases; 2. 
replacement of gene names; 3. replacement of 
noun phrases. 
  To improve the correctness of the parsing, each 
biomedical sentence is first preprocessed to re-
move phrases that are not essential to the sentence. 
This includes removal of section indicators, which 
are phrases that specify the name of the section at 
the beginning of the sentence, plus the removal of 
phrases in parentheses (such as citations and num-
bering in lists). Also, partially hyphenated words 
are transformed by combining with the nearest 
word that follows or precedes the partial hyphe-
nated word to make a meaningful word. For in-
stance, the phrase ?alpha- and beta-catenin? is 
transformed into ?alpha-catenin and beta-catenin?. 
  Occurrences of multi-word technical terms and 
entity names involved in biomedical processes are 
common in biomedical text. Such terms are not 
likely to appear in the dictionary of a parser (per-
plexity is high), and will force it to use morpho-
guessing and unknown word guessing. This is time 
consuming and prone to error. Thus, unlike typical 
text simplification that emphasizes syntactic trans-
formation of sentences, our approach utilizes a 
named entity recognition engine, BANNER  
(Leaman & Gonzalez, 2008), to replace multi-word 
gene names with single-word placeholders.  
 Replacement of gene names with single elements 
is not enough, however, and grammatical category 
(i.e. singular or plural) of the element has to be 
considered. Lingpipe (Alias-i, 2006), a shallow 
parser for biomedical text, identifies noun phrases 
and replaces them with single elements. A single 
element is considered singular when the following 
verb indicates a third-person singular verb or the 
determiner preceded by the element is either ?a? or 
?an?. Otherwise it is considered as plural and an 
?s? is attached to the end of the element. 
2.2 Syntactic transformation 
The problem of simplifying long sentences in 
common English text has been studied before, not-
ably by Chandrasekar & Srinivas (1997) and Sidd-
harthan (2006). However, the techniques used in 
these studies might not totally solve the issue of 
parsing biomedical sentences. For example, using 
Siddharthan?s approach, the biological finding 
?The Huntington's disease protein interacts with 
p53 and CREB-binding protein and represses tran-
scription?, and assuming multi-word nouns such as 
?CREB-binding protein? do not present a problem, 
would be simplified to: 
?The Huntington's disease protein interacts with 
p53. The Huntington's disease protein interacts 
with CREB-binding protein. The Huntington's 
disease protein represses transcription.?  
 Our method transforms it to ?GENE1 interacts 
with GENE2 and GENE3 and represses transcrip-
tion.? Both decrease the average sentence length, 
but the earlier distorts the biological meaning 
(since the Huntington?s disease protein might not 
repress transcription on its own), while the latter 
signifies it. 
  While replacement of gene names and noun 
phrases can reduce the sentence length, there are 
Figure 1 ? Linkages after simplification of the original sentence 
? GENE1: human CREB binding protein 
? GENE2: CBP 
? GENE3s: CBP 
? REPNP1s: RTS patients 
Original sentence ST: The gene for the human CREB binding protein, the transcriptional coactivator CBP,   
is included in the RT1 cosmid, and mutations in CBP have recently been identified in RTS patients. 
ST1: 
ST2: 
c1 
c3 c4 
c2 
178
  
cases when the sentences are still too complex to 
be parsed efficiently. We developed a simple algo-
rithm that utilizes linkages (specific grammatical 
relationships between pairs of words in a sentence) 
of the Link Grammar parser (Sleator, 1998) and 
punctuations for splitting sentences into clauses. 
An example in Figure 1 illustrates the main part of 
the algorithm. Each linkage has a primary link type 
in CAPITAL followed by secondary link type in 
short. The intuition behind the algorithm is to try to 
identify independent clauses from complex sen-
tences. The first step is to split the sentence ST into 
clauses c1, c2, c3 and c4 based on commas. c1 is 
parsed using the Link Grammar parser, but c1 can-
not be a sentence as there is no ?S? link in the lin-
kage of c1. c2 is then attached to c1 and the linkage 
of ?c1, c2? does not contain a ?S? link as well. ?c1, 
c2, c3.? is recognized as a sentence, since the lin-
kage contains an ?S? link, indicating that it is a 
sentence, as well as the linkage of c4. So the algo-
rithm returns two sentences ST1 and ST2 for ST. 
3 Results 
Our method has the greatest impact on the perfor-
mance of Link Grammar (LG), which lies at the 
core of BioLG (Pyysalo et al, 2006). However, it 
also has a significant impact on the self-training 
biomedical parser by McClosky & Charniak (CM), 
which is currently the best parser available for 
biomedical text.  
3.1 Rudimentary statistics of the results of sim-
plification: After the simplification algorithm was 
tested on the 1100 annotated sentences of the Bio-
Infer corpus, there were 1159 simplified sentences 
because of syntactic transformation (section 2.2). 
The number of words per sentence showed a sharp 
drop of 20.4% from 27.0 to 21.5. The Flesh-
Kincaid score for readability dropped from 17.4 to 
14.2. The Gunning Fog index also dropped by 
18.3% from 19.7 to 16.1. 
Pre-
processing 
Replacement 
of gene names 
Replacement of 
noun phrases 
Syntactic  
Simplification 
359  1082  915  91 
Table 1: Sentences processed in each stage 
 
3.2 Impact of simplification on the efficiency of 
parsing: We inputted the BioInfer corpus to LG 
and CM. If LG cannot find a complete linkage, it 
invokes its panic mode, where sentences are re-
turned with considerably low accuracy. Out of the 
1100 original sentences in the corpus, 219 went 
into panic mode. After processing, only 39 out of 
1159 simplified sentences triggered panic mode (a 
16.4% improvement in efficiency). The average 
time for parsing a sentence also dropped from 7.36 
secs to 1.70 secs after simplification.  
 
3.3 Impact of simplification on the accuracy of 
parsing: Let ?g, ?o and ?s, respectively be the 
sets containing the links of the gold standard, the 
output generated by the parser on original sen-
tences and the output generated by the parser on 
simplified sentences. We denote a link of type ? 
between the tokens ?1 and ?2 by (?,?1,?2). In the 
case of the original sentences, the tokens ?1and ?2 
are single-worded. So, (?,?1,?2) is a true positive 
iff (?,?1,?2) belongs to both ?g and ?o, false posi-
tive iff it only belongs to ?o and false negative iff 
it only belongs to ?g. In the case of simplified sen-
tences, the tokens ?1and ?2 can have multiple 
words. So, (?,?1,?2) which belongs to ?s is a true 
positive iff (?,??1,??2) belongs to ?g where ??1 
and ??2 are respectively one of the words in ?1 and 
?2. Additionally, (?,?1,?2) which belongs to ?g is 
not a false negative if ?1 and ?2 are parts of a sin-
gle token of a simplified sentence. For measuring 
the performance of a parser, the nature of linkage 
is most relevant in the context of the sentence in 
consideration. So, we calculate precision and recall 
for each sentence and average them over all sen-
tences to get the respective precision and recall for 
the collection.  
 
 Precision Recall f-measure 
CM 77.94% 74.08% 75.96% 
BioSimplify + 
CM 
82.51% 75.51% 78.86% 
Improvement   4.57% 1.43% 2.90% 
    
LG 72.36% 71.65% 72.00% 
BioSimplify + 
LG 
78.30% 74.27% 76.23% 
Improvement   5.94% 2.62% 4.23% 
Table 2: Accuracy of McClosky & Charniak (CM) and 
Link Grammar (LG) parsers based on Stanford depen-
dencies, with and without simplified sentences.  
 
  In order to compare the effect of BioSimplify on 
the two parsers, a converter from Link Grammar to 
Stanford scheme was used (Pyysalo et al 2007: 
precision and recall of 98% and 96%). Results of 
179
  
this comparision are shown in Table 2. On CM and 
LG, we were able to achieve a considerable im-
provement in the f-measures by 2.90% and 4.23% 
respectively. CM demonstrated an absolute error 
reduction of 4.1% over its previous best on a dif-
ferent test set. Overall, bioSimplify leverages pars-
ing of biomedical sentences, increasing both the 
efficiency and accuracy. 
4 Related work 
During the creation of BioInfer, noun phrase ma-
cro-dependencies were determined using a simple 
rule set without parsing. Some of the problems re-
lated to parsing noun phrases were removed by 
reducing the number of words by more than 20%. 
BioLG enhances LG by expansion of lexicons and 
the addition of morphological rules for biomedical 
domain. Our work differs from BioLG not only in 
utilizing a gene name recognizer, a specialized 
shallow parser and syntactic transformation, but 
also in creating a preprocessor that can improve the 
performance of any parser on biomedical text. 
  The idea of improving the performance of deep 
parsers through the integration of shallow and deep 
parsers has been reported in (Crysmann et al, 
2002; Daum et al, 2003; Frank et al, 2003) for 
non-biomedical text. In BioNLP, extraction sys-
tems (Jang et al, 2006; Yakushiji et al, 2001) used 
shallow parsers to enhance the performance of 
deep parsers. However, there is a lack of evalua-
tion of the correctness of the dependency parses, 
which is crucial to the correctness of the extracted 
systems. We not only evaluate the correctness of 
the links, but also go beyond the problem of rela-
tionship extraction and empower future researchers 
in leveraging their parsers (and other extraction 
systems) to get better results. 
5 Conclusion and Future work 
We achieved an f-measure of 78.86% using CM on 
BioInfer Corpus which is a 2.90% absolute reduc-
tion in error. We achieved a 4.23% absolute reduc-
tion in error using LG. According to the measures 
described in section 3.1, the simplified sentences 
of BioInfer outperform the original ones by more 
than 18%. Our method can also be used with other 
parsers. As future work, we will demonstrate the 
impact of our simplification method on other text 
mining tasks, such as relationship extraction. 
Acknowledgments 
We thank Science Foundation Arizona (award 
CAA 0277-08 Gonzalez) for partly supporting this 
research. SJ also thanks Bob Leaman and Anoop 
Grewal for their guidance. 
References  
Chandrasekar, R., & Srinivas, B. (1997). Automatic 
induction of rules for text simplification. Knowledge-
Based Systems, 10, 183-190.  
Crysmann, B., Frank, A., Kiefer, B., Muller, S., Neu-
mann, G., et al (2002). An integrated architecture for 
shallow and deep processing. ACL'02. 
Daum, M., Foth, K., & Menzel, W. (2003). Constraint 
based integration of deep and shallow parsing tech-
niques. EACL'03.  
Elhadad, No?mie (2006) User-Sensitive Text Summari-
zation: Application to the Medical Domain. 
Ph.D. Thesis, Columbia University. Available at 
www.dbmi.columbia.edu/noemie/papers/thesis.pdf 
Frank, A., Becker M, et al,. (2003). Integrated shallow 
and deep parsing: TopP meets HPSG. ACL'03. 
Gemoets, D., Rosemblat, G., Tse, T., Logan, R., Assess-
ing Readability of Consumer Health Information: An 
Exploratory Study. MEDINFO 2004. 
Jang, H., Lim, J., Lim, J.-H., Park, S.-J., Lee, K.-C. and 
Park, S.-H. (2006) Finding the evidence for protein-
protein interactions from PubMed abstracts, Bioin-
formatics, 22, e220-226. 
Leaman, R., & Gonzalez, G. (2008). BANNER: An 
executable survery of advances in biomedical named 
entity recognition. 652-663.  
Manning, C. D., & Sch?tze, H. (1999). Foundations of 
statistical natural language processing MIT Press. 
McClosky, D., & Charniak, E. (2008) Self-training for 
biomedical parsing. ACL?08. 
Pyysalo, S., Ginter, F., Haverinen, K., Heimonen, J., 
Salakoski, T., & Laippala, V. (2007) On the unifica-
tion of syntactic annotations under the stanford de-
pendency scheme.. ACL?07. 
Pyysalo, S., Salakoski, T., Aubin, S., & Nazarenko, A. 
(2006). Lexical adaptation of link grammar to the 
biomedical sublanguage: A comparative evaluation 
of three approaches. BMC Bioinformatics, 7, S2.  
Siddharthan, A. (2006). Syntactic simplification and 
text cohesion. Res  Lang  Comput, 4(1), 77-109.  
Sleator, D. (1998) Link Grammar Documentation 
Tateisi, Y., & Tsujii, J. (2004). Part-of-speech annota-
tion of biology research abstracts. LREC, 1267-1270.  
Yakushiji, A., Tateisi, Y., Miyao, Y., & Tsujii, J. 
(2001). Event extraction from biomedical papers using a 
full parser. PSB'01, 6, 408-419. 
180
Proceedings of the Workshop on BioNLP: Shared Task, pages 86?94,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Molecular event extraction from Link Grammar parse trees
Jo?rg Hakenberg1, Ille?s Solt2, Domonkos Tikk2,3, Luis Tari1,
Astrid Rheinla?nder3, Quang Long Ngyuen3, Graciela Gonzalez1, and Ulf Leser3
1Arizona State University, Tempe, AZ 85283, USA,
2Budapest University of Technology and Economics, 1117 Budapest, Hungary,
3Humboldt Universita?t zu Berlin, 10099 Berlin, Germany.
Abstract
We present an approach for extracting molec-
ular events from literature based on a deep
parser, using in a query language for parse
trees. Detected events range from gene ex-
pression to protein localization, and cover a
multitude of different entity types, including
genes/proteins, binding sites, and locations.
Furthermore, our approach is capable of rec-
ognizing negation and the speculative char-
acter of extracted statements. We first parse
documents using Link Grammar (BioLG) and
store the parse trees in a database. Events are
extracted using a newly developed query lan-
guage with traverses the BioLG linkages be-
tween trigger terms, arguments, and events.
The concrete queries are learnt from an an-
notated corpus. On BioNLP Shared Task data,
we achieve an overall F1-measure of 29.6%.
1 Introduction
Biomedical text mining aims at making the wealth
of information available in publications available for
systematic, automatic studies. An important area of
biomedical text mining is concerned with the ex-
traction of relationships between biological entities,
especially the extraction of protein?protein inter-
actions from PubMed abstracts (Krallinger et al,
2008). The BioNLP?09 Shared Task addresses the
problem of extracting nine different types of molec-
ular events (Kim et al, 2009) and thus targets a
problem that is considerable less-well studied than
protein-protein interactions. Such molecular events
include statements about the expression level of
genes, the binding sites of proteins, and the up/down
regulation of genes, among others. All events fo-
cus on genes/proteins and may include only a single
protein (e.g., protein catabolism), multiple proteins
(e.g., binding), and other arguments (e.g., phospho-
rylation site; protein location). The most complex
type of event considered in the task are regulations,
which may refer to other events (negative regulation
of gene expression) and may also include causes as
arguments. The task also addresses the problem that
experimental findings often are described in a defen-
sive manner (?Our results suggest ...?) or may appear
in negated context. This meta-information about an
extracted event should be taken into account when
text mining results are used in automated analysis
pipelines, but recognizing the degree of confidence
that can be put into an event adds further complex-
ity to the task. Overall, the three tasks in BioNLP?09
are: 1) event detection and characterization, 2) event
argument recognition, and 3) recognition of nega-
tions and speculations.
The approach we present in this paper addresses
all three tasks. Essentially, our system consists of
three components: A deep parser, a query language
for parse trees, and a set of queries that extract spe-
cific events from parse trees. First, we use the Bio-
LG parser (Pyysalo et al, 2006) for parsing sen-
tences into a graph-like structure. Essentially, Bio-
LG recognizes the syntactic structure of a sentence
and represents this information in a tree. It adds links
between semantically connected elements, such as
the links between a verb and its object and sub-
ject. Second, we store the result of BioLG in a re-
lational database. This information is accessed by a
special-purpose query language (Tu et al, 2008) that
86
 


	

 






	





Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 117?125,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Towards Internet-Age Pharmacovigilance: Extracting Adverse Drug
Reactions from User Posts to Health-Related Social Networks
Robert Leaman1, Laura Wojtulewicz2, Ryan Sullivan2
Annie Skariah2, Jian Yang1, Graciela Gonzalez2
1School of Computing, Informatics and Decision Systems Engineering
2Department of Biomedical Informatics
Arizona State University, Tempe, Arizona, USA
{robert.leaman, whitz, rpsulli, annie.skariah,
jian.yang, graciela.gonzalez}@asu.edu
Abstract
Adverse reactions to drugs are among the
most common causes of death in industri-
alized nations. Expensive clinical trials are
not sufficient to uncover all of the adverse
reactions a drug may cause, necessitating
systems for post-marketing surveillance,
or pharmacovigilance. These systems
have typically relied on voluntary report-
ing by health care professionals. However,
self-reported patient data has become an
increasingly important resource, with ef-
forts such as MedWatch from the FDA al-
lowing reports directly from the consumer.
In this paper, we propose mining the re-
lationships between drugs and adverse re-
actions as reported by the patients them-
selves in user comments to health-related
websites. We evaluate our system on a
manually annotated set of user comments,
with promising performance. We also re-
port encouraging correlations between the
frequency of adverse drug reactions found
by our system in unlabeled data and the
frequency of documented adverse drug re-
actions. We conclude that user comments
pose a significant natural language pro-
cessing challenge, but do contain useful
extractable information which merits fur-
ther exploration.
1 Introduction
It is estimated that approximately 2 million pa-
tients in the United States are affected each year by
severe adverse drug reactions, resulting in roughly
100,000 fatalities. This makes adverse drug re-
actions the fourth leading cause of death in the
U.S, following cancer and heart diseases (Giaco-
mini et al, 2007). It is estimated that $136 bil-
lion is spent annually on treating adverse drug re-
actions in the U.S., and other nations face simi-
lar difficulties (van Der Hooft et al, 2006; Leone
et al, 2008). Unfortunately, the frequency of ad-
verse drug reactions is often under-estimated due
to a reliance on voluntary reporting (Bates et al,
2003; van Der Hooft et al, 2006).
While severe adverse reactions have received
significant attention, less attention has been di-
rected to the indirect costs of more common
adverse reactions such as nausea and dizziness,
which may still be severe enough to motivate the
patient to stop taking the drug. The literature
shows, however, that non-compliance is a major
cause of the apparent failure of drug treatments,
and the resulting economic costs are estimated to
be quite significant (Urquhart, 1999; Hughes et al,
2001). Thus, detecting and characterizing adverse
drug reactions of all levels of severity is critically
important, particularly in an era where the demand
for personalized health care is high.
1.1 Definitions
An adverse drug reaction is generally defined as
an unintended, harmful reaction suspected to be
caused by a drug taken under normal conditions
(World Health Organization, 1966; Lee, 2006).
This definition is sufficiently broad to include such
conditions as allergic reactions, drug tolerance,
addiction or aggravation of the original condition.
A reaction is considered severe if it ?results in
death, requires hospital admission or prolonga-
tion..., results in persistent or significant disabil-
ity/incapacity, or is life-threatening,? or if it causes
a congenital abnormality (Lee, 2006).
117
1.2 Pharmacovigilance
The main sources of adverse drug reaction in-
formation are clinical trials and post-marketing
surveillance instruments made available by the
Food and Drug Administration (FDA), Centers
for Disease Control and Prevention (CDC) in the
United States, and similar governmental agencies
worldwide. The purpose of a clinical trial, how-
ever, is only to determine whether a product is
effective and to detect common serious adverse
events. Clinical trials, by their nature and pur-
pose, are focused on a limited number of par-
ticipants selected by inclusion/exclusion criteria
reflecting specific subject characteristics (demo-
graphic, medical condition and diagnosis, age).
Thus, major uncertainties about the safety of the
drug remain when the drug is made available to
a wider population over longer periods of time,
in patients with co-morbidities and in conjunction
with other medications or when taken for off-label
uses not previously evaluated.
Recently, the regulatory bodies of both the U.S.
and the U.K. have begun programs for patient re-
porting of adverse drug reactions. Studies have
shown that patient reporting is of similar qual-
ity to that of health professionals, and there is
some evidence that patients are more likely to
self-report adverse drug reactions when they be-
lieve the health professionals caring for them have
not paid sufficient attention to an adverse reaction
(Blenkinsopp et al, 2007). In general, however,
the FDA advocates reporting only serious events
through MedWatch.
Self-reported patient information captures a
valuable perspective that might not be captured in
a doctor?s office, clinical trial, or even in the most
sophisticated surveillance software. For this rea-
son, the International Society of Drug Bulletins
asserted in 2005 that ?patient reporting systems
should periodically sample the scattered drug ex-
periences patients reported on the internet.?
1.3 Social Networks
Social networks focusing on health related topics
have seen rapid growth in recent years. Users in
an online community often share a wide variety
of personal medical experiences. These interac-
tions can take many forms, including blogs, mi-
croblogs and question/answer discussion forums.
For many reasons, patients often share health ex-
periences with each other rather than in a clini-
cal research study or with their physician (Davi-
son et al, 2000). Such social networks bridge
the geographical gap between people, allowing
them to connect with patients who share similar
conditions?something that might not be possible
in the real world.
In this paper we propose and evaluate automat-
ically extracting relationships between drugs and
adverse reactions in user posts to health-related
social network websites. We anticipate this tech-
nique will provide valuable additional confirma-
tion of suspected associations between drugs and
adverse reactions. Moreover, it is possible this
technique may eventually provide the ability to
detect novel associations earlier than with current
methods.
2 Related Work
In the work closest in purpose to this study, two
reviewers manually analyzed 1,374 emails to the
BBC and 862 messages on a discussion forum re-
garding a link between the drug paroxetine and
several adverse reactions including withdrawal
symptoms and suicide (Medawara et al, 2002).
The authors concluded that the user reports con-
tained clear evidence of linkages that the voluntary
reporting system then in place had not detected.
Not much work has been done to automatically
extract adverse reactions from text, other than the
SIDER side effect resource, which was created by
mining drug insert literature (Kuhn et al, 2010).
There is, however, significant literature support for
mining more general concepts, such as diseases.
MetaMap is a primarily lexical system for map-
ping concepts in biomedical text to concepts in
the UMLS Metathesaurus (Aronson, 2001). The
ConText system categorizes findings in clinical
records as being negated, hypothetical, or histor-
ical (Harkema et al, 2009).
Most of the work on finding diseases concerns
either biomedical text or clinical records. A no-
table exception is the BioCaster system, which de-
tects infectious disease outbreaks by mining news
reports posted to the web (Collier et al, 2008).
Health social networks have become a popular
way for patients to share their health related expe-
riences. A considerable amount of research has
been devoted to this area (Moturu et al, 2008),
but most of this work has focused on the study of
social interactions and quality evaluation instead
of text mining. Automated information extrac-
118
tion from health social network websites remains
largely unexplored.
3 Data Preparation
We used the DailyStrength1 health-related social
network as the source of user comments in this
study. DailyStrength allows users to create pro-
files, maintain friends and join various disease-
related support groups. It serves as a resource for
patients to connect with others who have similar
conditions, many of whom are friends solely on-
line. As of 2007, DailyStrength had an average
of 14,000 daily visitors, each spending 82 minutes
on the site and viewing approximately 145 pages
(comScore Media Metrix Canada, 2007).
3.1 Data Acquisition
To efficiently gather user comments about spe-
cific drugs from the DailyStrength site, we im-
plemented a highly parallelized automatic web
crawler. All data was scraped from the raw
HTML using regular expressions since the site has
no open API. Users indicate a specific treatment
when posting comments to DailyStrength, how-
ever we filter treatments which are not drugs. For
each user comment we extracted the user ID, dis-
ease name, drug name, and comment text. While
more information about each user is available at
the site (gender, age, self-declared location, and
length of membership at the site), we limited our
data usage to just the comment data. The Dai-
lyStrength Privacy Policy states that comments
made by users will be publicly available. All
data was gathered in accordance with the Dai-
lyStrength Terms of Service, and to respect fair
use the data will not be made publicly available
without permission from the site.
3.2 Preparing the Lexicon
To enable finding adverse reactions in the user
comments, we created a lexicon by combining
terms and concepts from four resources.
The UMLS Metathesaurus is a resource con-
taining many individual biomedical vocabularies
(National Library of Medicine, 2008). We utilized
a subset limited to the COSTART vocabulary cre-
ated by the U.S. Food and Drug Administration for
post-marketing surveillance of adverse drug reac-
tions, which contains 3,787 concepts.
1http://www.dailystrength.org
The SIDER side effect resource contains 888
drugs linked with 1,450 adverse reaction terms
extracted from pharmaceutical insert literature
(Kuhn et al, 2010). We used the raw term found
in the literature and the associated UMLS concept
identifier (CUI).
The Canada Drug Adverse Reaction Database,
or MedEffect2, contains associations between
10,192 drugs and 3,279 adverse reactions, which
we used to create a list of adverse reaction terms.
We found many adverse reaction terms with very
similar meanings, for example ?appetite exagger-
ated,? and ?appetite increased,? which we grouped
together manually.
We also included a small set of colloquial
phrases we located manually in a subset of the
DailyStrength comments and mapped to UMLS
CUIs. This list is available3, and includes the
terms ?throw up,? meaning vomit, ?gain pounds,?
meaning weight gain, and ?zonked out,? meaning
somnolence.
We considered all terms which are associated
with the same UMLS concept identifier (CUI) as
synonymous and grouped them into a single con-
cept. We also merged all concepts containing a
term in common into a single unified concept. Our
lexicon contains 4,201 unified concepts, each con-
taining between one and about 200 terms.
4 Annotation
We annotated comments relating to the following
4 drugs: carbamazepine, olanzapine, trazodone,
and ziprasidone. These drugs were chosen be-
cause they are known to cause adverse reactions
and we could verify our results with close collabo-
rators. We retained but did not annotate comments
for the drugs aspirin and ciprofloxacin; these com-
ments are used during evaluation. Our data con-
tains a total of 6,890 comment records. User com-
ments were selected for annotation randomly and
were independently annotated by two annotators.
Annotator 1 has a BS in biology, 10 years nurs-
ing experience in the behavioral unit of a long term
care facility, and has dispensed all of the drugs an-
notated. Annotator 2 has a BS and an MS in neuro-
science, and has work experience in data manage-
ment for pharmaceutical-related clinical research
and post-marketing drug surveillance.
2http://www.hc-sc.gc.ca/dhp-mps/medeff/index-eng.php
3http://diego.asu.edu/downloads/adrs
119
Concept Definition
Adverse
effect
A reaction to the drug experienced by the
patient, which the user considered nega-
tive
Beneficial
effect
A reaction to the drug experienced by the
patient, which the user considered posi-
tive
Indication The condition for which the patient is tak-
ing the drug
Other A disease or reaction related term not
characterizable as one of the above
Table 1: The concepts annotated in this study and
their definitions.
4.1 Concepts Annotated
Each comment was annotated for mentions of ad-
verse effects, beneficial effects, indications and
other terms, as defined in table 1. Each annota-
tion included the span of the mention and the name
of the concept found, using entries from the lexi-
con described in section 3.2. Each annotation also
indicates whether it refers to an adverse effect, a
beneficial effect, an indication or an other term,
which we shall call its characterization.
4.2 Annotation Practices
There are four aspects which require careful con-
sideration when characterizing mentions. First,
the stated concept may or may not be actually
experienced by the patient; mentions of concepts
not experienced by the patient were categorized as
other. Second, the user may state that the con-
cept is the reason for taking the drug. If so, the
mention was categorized as an indication. Third,
the concept may be an effect caused by the drug.
In this case, the mention is categorized as either
an adverse effect or a beneficial effect based on
whether the user considers the effect a positive
one. This requires some judgment regarding what
people normally view as positive ? while sleepi-
ness is normally an adverse effect, someone suf-
fering from insomnia would consider it a benefi-
cial effect, regardless of whether insomnia is the
primary reason for taking the drug. Mentions of
concepts which were experienced by the patient
but neither an effect of the drug nor the reason for
taking it were also categorized as other. Concepts
were characterized as an adverse effect unless the
context indicated otherwise.
Comments not containing a mention or that only
indicated the presence of an adverse effect (?Gave
me weird side effects?) were discarded. If more
than one mention occurred in a comment, then
each mention was annotated separately.
Some comments clearly mentioned an adverse
reaction, but the reaction itself was ambiguous.
For example, in the comment ?It did the job when
I was really low. However, I BALLOONED on
it,? the annotator could infer ?BALLOONED? to
mean either weight gain or edema. A frequent ex-
ample is colloquial terms such as ?zombie,? which
could be interpreted as a physiological effect (e.g.
fatigue) or a cognitive effect (e.g. mental dull-
ness). In such cases, each mention was annotated
by using both the context of the mention and an-
notator?s knowledge of the effects of the drug.
Spans were annotated by choosing the mini-
mum span of characters from the comment that
would maintain the meaning of the term. Lo-
cating the mention boundaries was straightfor-
ward in many cases, even when descriptive words
were in the middle of the term (?It works bet-
ter than the other meds ive taken but I am
gaining some weight?). However some com-
ments were not as simple (?it works but the
pounds are packing on?).
4.3 Corpus Description
A total of 3,600 comments were annotated, a sam-
ple of which can be seen in table 2. We reserved
450 comments for system development. The an-
notators found 1,260 adverse effects, 391 indica-
tions, 157 beneficial effects and 78 other, for a to-
tal of 1,886 annotations.
We measured the agreement between annotators
by calculating both kappa (?) (Cohen, 1960) and
inter-annotator agreement (IAA). For ?, we con-
sidered agreement to mean that the concept terms
were in the same unified concept from the lexicon
and the characterization of the mentions matched,
since there is no standard method for calculating
? which includes the span. For IAA, we added
the constraint that the annotation spans must over-
lap, since discussions of IAA typically include the
span. Using these definitions, ? was calculated to
be 85.6% and IAA to be 85.3%4.
5 Text Mining
Since the drug name is specified by the user when
the comment is submitted to DailyStrength, no ex-
4?>IAA here due to the different definitions of agree-
ment.
120
Sample Comments Annotations
hallucinations and weight gain ?hallucinations? - hallucinations: adverse effect; ?weight gain?
- weight gain: adverse effect
This has helped take the edge off of my constant sorrow.
It has also perked up my appetite. I had lost a lot of
weight and my doctor was concerned.
?constant sorrow? - depression: indication; ?perked up my ap-
petite? - appetite increased: beneficial effect; ?lost a lot of
weight? - weight loss: other
It worked well, but doctor didn?t asked for the treatment
to continue once my husband was doing well again.
none
ARGH! Got me nicely hypomanic for two weeks, then
pooped out on me and just made me gain a half pound
a day so I had to stop.
?hypomanic? - hypomania: beneficial effect; ?pooped out? -
tolerance: adverse effect; ?gain a half a pound a day? - weight
gain: adverse effect
Works to calm mania or depression but zonks me and
scares me about the diabetes issues reported.
?mania? - mania: indication; ?depression? - depression: indi-
cation; ?zonks me? - somnolence: adverse effect; ?diabetes? -
diabetes: other
Works for my trigeminal neuralgia. Increasing to see if
it helps stabalize mood. Fatigue!
?trigeminal neuralgia? - trigeminal neuralgia: indication; ?sta-
balize mood? - emotional instability: indication; ?Fatigue? -
fatigue: adverse effect
Take for seizures and bipolar works well ?seizures? - seizures: indication; ?bipolar? - bipolar disorder:
indication
fatty patti! ?fatty? - weight gain: adverse effect
Table 2: An illustrative selection of uncorrected comments submitted to the DailyStrength health-related
social networking website, and their associated annotations.
traction was necessary for drug names. To ex-
tract the adverse drug reactions from the user
comments, we implemented a primarily lexical
method, utilizing the lexicon discussed in section
3.2.
5.1 Methods Used
Each user comment was split into sentences using
the Java sentence breaker, tokenized by splitting at
whitespace and punctuation, and tagged for part-
of-speech using the Hepple tagger (Hepple, 2000).
Stop-words were removed from both user com-
ments and lexical terms5. Tokens were stemmed
using the Snowball implementation of the Porter2
stemmer6.
Terms from the lexicon were found in the user
comments by comparing a sliding window of to-
kens from the comment to each token in the lexical
term. The size of the window is configurable and
set to 5 for this study since that is the number of
tokens in the longest term found by the annotators.
Using a sliding window allows the tokens to be in
different orders and for there to be irrelevant to-
kens between the relevant ones, as in weight gain
and ?gained a lot of weight.?
Since user comments contain many spelling er-
rors, we used the Jaro-Winkler measurement of
string similarity to compare the individual tokens
5http://ir.dcs.gla.ac.uk/resources/linguistic utils/stop words
6http://snowball.tartarus.org
(Winkler, 1999). We scored the similarity between
the window of tokens in the user comment and the
tokens in the lexical term by pairing them as an
assignment problem (Burkard et al, 2009). We
then summed the similarities of the individual to-
kens and normalized the result by the number of
tokens in the lexical term. This score is calculated
for both the original tokens and the stemmed to-
kens in the window, and the final score is taken to
be the higher of the two scores. The lexical term is
considered to be present in a user comment if the
final score is greater than a configurable threshold.
We noted that most mentions could be cate-
gorized by using the closest verb to the left of
the mention, as in ?taking for seizures.? As this
study focuses on adverse effects, we implemented
a filtering method to remove indications, benefi-
cial effects, and other mentions on a short list of
verbs we found to indicate them. Verbs on this
list include ?helps,? ?works,? and ?prescribe? all
of which generally denote indications. The com-
plete list is available7.
5.2 Text Mining Results
We first evaluated the system against the 3,150 an-
notated comments not reserved for system devel-
opment. Because our purpose is to find adverse
drug reactions, we limited our evaluation to ad-
7http://diego.asu.edu/downloads/adrs
121
verse effects. We used a strict definition of true
positive, requiring the system to label the mention
with a term from the same unified concept as the
annotators. The results of this study are 78.3%
precision and 69.9% recall, for an f-measure of
73.9%.
Since the purpose of this study is to determine
if mining user comments is a valid way to find ad-
verse reactions, we ran our system on all avail-
able comments and compared the frequencies of
adverse reactions found against their documented
incidence. We calculated the frequency that each
adverse effect was found in the user comments
for each of the drugs studied in this experiment.
We then determined the most commonly found ad-
verse reactions for each drug and compared them
against the most common documented adverse re-
actions for the drug. Since the four drugs we chose
for annotation all act primarily on the central ner-
vous system, we added aspirin and ciprofloxacin
for this study. The results of this evaluation con-
tain encouraging correlations that are summarized
in table 3.
6 Discussion
6.1 Error Analysis
We performed an analysis to determine the pri-
mary sources of error for our extraction system.
We randomly selected 100 comments and deter-
mined the reason for the 24 false positives (FPs)
and 29 false negatives (FNs) found.
The largest source of error (17% of FPs and
55% of FNs) was the use of novel adverse re-
action phrases (?liver problem?) and descriptions
(?burn like a lobster?). This problem is due in part
to idiomatic expressions, which may be handled
by creating and using a specialist lexicon. This
problem might also be partially relieved by the ap-
propriate use of semantic analysis. However, this
source of error is also caused by the users delib-
erately employing a high degree of linguistic cre-
ativity (?TURNED ME INTO THE SPAWN OF
SATAN!!!?) which may require deep background
knowledge to correctly recognize.
The next largest source of error was poor ap-
proximate string matching (46% of FPs and 17%
of FNs). While users frequently misspelled words,
making lexical analysis difficult, the approximate
string matching technique used also introduced
many FPs. We note that spelling unfamiliar med-
ical terminology is particularly difficult for users.
Correcting this important source of error will re-
quire improved modeling of the spelling errors
made by users.
Ambiguous terms accounted for 8% of the FPs
and 7% of the FNs. While this is frequently
a problem with colloquial phrases (?brain fog?
could refer to mental dullness or somnolence),
there are some terms which are ambiguous on their
own (?numb? may refer to loss of sensation or
emotional indifference). These errors can be cor-
rected by improving the analysis of the context
surrounding each mention.
Surprisingly, miscategorizations only ac-
counted for 4% of the FPs. This small percentage
seems to indicate that the simple filtering tech-
nique employed is reasonably effective. However
this source of error can be seen more prominently
in the frequency analysis, as seen in table 3. For
example, one of the most frequent effects found in
comments about trazodone was insomnia, which
is one of its most common off-label uses. Other
examples included depression with olanzapine,
mania with ziprasidone, and stroke with aspirin.
We note that since conditions not being experi-
enced by the patient are always categorized as
other, our techniques should profit somewhat
from an extension to handle negation.
6.2 Analysis of Documented vs. Found
Adverse Reactions
The experiment comparing the documented inci-
dence of adverse reactions to the frequency they
are found contained some interesting correlations
and differences. We begin by noting that the ad-
verse reaction found most frequently for all 6 of
the drugs corresponded to a documented adverse
reaction. There were also similarities in the less
common reactions, such as diabetes with olanzap-
ine and bleeding with aspirin. In addition, many of
the adverse reactions found corresponded to docu-
mented, but less common, reactions to the drug.
Examples of this included edema with olanzap-
ine, nightmares with trazodone, weight gain with
ziprasidone, tinnitus with aspirin, and yeast infec-
tion with ciprofloxacin.
One interesting difference is the relative fre-
quency of ?hangover? in the comments for ziprasi-
done. Since the users were not likely referring to
a literal hangover, they were probably referring
to the fatigue, headache, dry mouth and nausea
that accompany a hangover, all of which are doc-
122
Drug name
(Brand name)
Primary Indi-
cations
Documented Adverse Effects
(Frequency)
Adverse Effects Found in User Comments (Fre-
quency)
carbamazepine
(Tegretol)
epilepsy,
trigeminal
neuralgia
dizziness, somnolence or fa-
tigue, unsteadiness, nausea,
vomiting
somnolence or fatigue (12.3%), allergy (5.2%),
weight gain (4.1%), rash (3.5%), depression (3.2%),
dizziness (2.4%), tremor/spasm (1.7%), headache
(1.7%), appetite increased (1.5%), nausea (1.5%)
olanzapine
(Zyprexa)
schizophrenia,
bipolar
disorder
weight gain (65%), alteration
in lipids (40%), somnolence
or fatigue (26%), increased
cholesterol (22%), diabetes
(2%)
weight gain (30.0%), somnolence or fatigue
(15.9%), appetite increased (4.9%), depression
(3.1%), tremor (2.7%), diabetes (2.6%), mania
(2.3%), anxiety (1.4%), hallucination (0.7%), edema
(0.6%)
trazodone
(Oleptro)
depression somnolence or fatigue (46%),
headache (33%), dry mouth
(25%), dizziness (25%), nausea
(21%)
somnolence or fatigue (48.2%), nightmares (4.6%),
insomnia (2.7%), addiction (1.7%), headache
(1.6%), depression (1.3%), hangover (1.2%), anxi-
ety attack (1.2%), panic reaction (1.1%), dizziness
(0.9%)
ziprasidone
(Geodon)
schizophrenia somnolence or fatigue (14%),
dyskinesia (14%), nausea
(10%), constipation (9%),
dizziness (8%)
somnolence or fatigue (20.3%), dyskinesia (6.0%),
mania (3.7%), anxiety attack (3.5%), weight gain
(3.2%), depression (2.4%), allergic reaction (1.9%),
dizziness (1.2%), panic reaction (1.2%)
aspirin pain, fever,
reduce blood
clotting
nausea, vomiting, ulcers,
bleeding, stomach pain or
upset
ulcers (4.5%), sensitivity (3.8%), stroke (3.1%),
bleeding time increased (2.8%), somnolence or fa-
tigue (2.7%), malaise (2.1%), weakness (1.4%),
numbness (1.4%), bleeding (1.0%), tinnitus (0.7%)
ciprofloxacin
(Cipro)
bacterial infec-
tion
diarrhea (2.3%), vomiting
(2.0%), abdominal pain
(1.7%), headache (1.2%),
restlessness (1.1%)
abdominal pain (8.8%), malaise (4.4%), nau-
sea (3.8%), allergy (3.1%), somnolence or fatigue
(2.5%), dizziness (1.9%), weakness (1.6%), tolerance
(1.5%), rash (1.3%), yeast infection (1.1%)
Table 3: List of drugs included in the subset for analysis, with their indications and 5 most common
adverse effects together with their frequency of incidence in adults taking the drug over the course of
one year, as listed in the FDA online drug library, http://www.accessdata.fda.gov/scripts/cder/drugsatfda
(some frequency data is not available). Also the 10 most frequent adverse effects found in the the
DailyStrength data using our automated system. Correlations are highlighted in bold.
umented adverse reactions to the drug.
Users frequently commented on weight gain
and fatigue while ignoring other reactions such as
increased cholesterol. While this may be because
users are more conscious of issues they can di-
rectly observe, this hypothesis would not explain
why other directly observable reactions such as
nausea and constipation are not always reported.
Determining the general trends in the differences
between clinical and user reports is an important
area for future work.
6.3 Limitations
The present study has some limitations. We
did not analyze the demographics of the users
whose comments we mined, though it is likely
that they are predominantly from North America
and English-speaking. In future work we intend
to expand the range of users and compare their
demographics against clinical studies of adverse
reactions. Also, the drugs we annotated oper-
ate primarily on the central nervous system and
therefore have different adverse reaction profiles
than would other drugs with substantially different
mechanisms. While the inclusion of aspirin and
ciprofloxacin does provide some evidence these
techniques are more generally applicable, we also
intend to expand the range of drugs studied in fu-
ture work.
6.4 Opportunities for Further Study
In addition to our current classification for ad-
verse reactions, there are additional dimensions
along which each user comment could be studied.
For example, many comments describe the degree
of the adverse reaction, which can be straight-
forward (?extremely?) or more creative (?like a
pig?). Also, many users explicitly state whether
they are still taking the drug, typically indicating
whether their physician took them off or whether
they took themselves off (non-compliance), and
whether adverse reactions were the reason. User
123
comments can also be categorized as medically
non-descriptive (?I took one tablet and could?nt
get out of bed for days and felt like I got hit
by a truck?), somewhat medically descriptive
(?My kidneys were not functioning properly?),
or medically sound (?I ended up with severe leg
swelling?). Comments also typically indicate
whether the user is the patient or a caretaker by be-
ing phrased in either the first person or third person
narrative. Finally, users also frequently describe
whether they thought the benefits of the drug out-
weighed the adverse effects. We believe these ad-
ditional dimensions represent a fertile area for fur-
ther research.
7 Conclusion
In summary, we have shown that user comments
to health related social networks do contain ex-
tractable information relevant to pharmacovigi-
lance. We believe this approach should be eval-
uated for the ability to detect novel relationships
between drugs and adverse reactions.
In addition to the improvements discussed in
section 6, we plan in future work to increase the
scale of the study (additional drugs, additional
data sources, more user comments), improve the
characterization of reactions using rule-based pat-
terns, and evaluate the improved system with re-
spect to all characterizations.
Acknowledgments
The authors would like to thank Dr. Diana Pe-
titti for her early support and suggestions, Tasnia
Tahsin for reviewing an earlier version, Skatje My-
ers for locating mergeable reaction concepts, and
the anonymous reviewers for many useful sugges-
tions. The authors are grateful for support from
Science Foundation Arizona grant CAA 0277-
08, the Arizona Alzheimers Disease Data Man-
agement Core under NIH Grant NIA P30 AG-
19610, and the Arizona Alzheimers Consortium
pilot grant.
References
Alan R. Aronson. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: the MetaMap
program. In Proceedings of the AMIA Symposium,
page 17. American Medical Informatics Associa-
tion.
D.W. Bates, R.S. Evans, H. Murff, P.D. Stetson,
L. Pizziferri, and G. Hripcsak. 2003. Detecting ad-
verse events using information technology. Journal
of the American Medical Informatics Association,
10(2):115?128.
A. Blenkinsopp, M. Wang, P. Wilkie, and P. A. Rout-
ledge. 2007. Patient reporting of suspected adverse
drug reactions: a review of published literature and
international experience. British Journal of Clinical
Pharmacology, 63(2):148?156.
Rainer Burkard, Mauro Dell?Amico, and Silvano
Martello. 2009. Assignment Problems. Society for
Industrial and Applied Mathematics.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46.
Nigel Collier, Son Doan, Ai Kawazoe, Reiko Matsuda
Goodwin, Mike Conway, Yoshio Tateno, Quoc-
Hung Ngo, Dinh Dien, Asanee Kawtrakul, Koichi
Takeuchi, Mika Shigematsu, and Kiyosu Taniguchi.
2008. BioCaster: detecting public health rumors
with a Web-based text mining system. Bioinformat-
ics, 24(24):2940?2941.
comScore Media Metrix Canada. 2007. Key Measures
Report - Health.
K. P. Davison, J. W. Pennebaker, and S. S. Dickerson.
2000. Who talks? The social psychology of ill-
ness support groups. The American Psychologist,
55(2):205?217.
K.M. Giacomini, R.M. Krauss, D.M. Roden,
M. Eichelbaum, M.R. Hayden, and Y. Naka-
mura. 2007. When good drugs go bad. Nature,
446(7139):975?977.
Henk Harkema, John N. Dowling, Tyler Thornblade,
and Wendy W. Chapman. 2009. ConText: An al-
gorithm for determining negation, experiencer, and
temporal status from clinical reports. Journal of
Biomedical Informatics, 42(5):839851.
Mark Hepple. 2000. Independence and commit-
ment: Assumptions for rapid training and execution
of rule-based POS taggers. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 277?278.
Dyfrig A. Hughes, Adrian Bagust, Alan Haycox,
and Tom Walley. 2001. The impact of non-
compliance on the cost-effectiveness of pharmaceu-
ticals: a review of the literature. Health Economics,
10(7):601?615.
International Society Of Drug Bulletins. 2005. Berlin
Declaration on Pharmacovigilance.
Michael Kuhn, Monica Campillos, Ivica Letunic,
Lars Juhl Jensen, and Peer Bork. 2010. A side ef-
fect resource to capture phenotypic effects of drugs.
Molecular Systems Biology, 6:343?348.
Anne Lee, editor. 2006. Adverse Drug Reactions.
Pharmaceutical Press, second edition.
124
Roberto Leone, Laura Sottosanti, Maria Luisa Iorio,
Carmela Santuccio, Anita Conforti, Vilma Sabatini,
Ugo Moretti, and Mauro Venegoni. 2008. Drug-
Related Deaths: An Analysis of the Italian Sponta-
neous Reporting Database. Drug Safety, 31(8):703?
713.
Charles Medawara, Andrew Herxheimer, Andrew Bell,
and Shelley Jofre. 2002. Paroxetine, Panorama
and user reporting of ADRs: Consumer intelligence
matters in clinical practice and post-marketing drug
surveillance. The International Journal of Risk and
Safety in Medicine, 15(3):161169.
S. T. Moturu, H. Liu, and W. G. Johnson. 2008. Trust
evaluation in health information on the World Wide
Web. In 30th Annual International Conference of
the IEEE Engineering in Medicine and Biology So-
ciety, pages 1525?1528.
National Library of Medicine. 2008. UMLS Knowl-
edge Sources.
John Urquhart. 1999. Pharmacoeconomic conse-
quences of variable patient compliance with pre-
scribed drug regimens. PharmacoEconomics,
15(3):217?228.
Cornelis S. van Der Hooft, Miriam C. J. M. Sturken-
boom, Kees van Grootheest, Herre J. Kingma, and
Bruno H. Ch. Stricker. 2006. Adverse drug
reaction-related hospitalisations: a nationwide study
in The Netherlands. Drug Safety, 29(2):161?168.
William E. Winkler. 1999. The state of record linkage
and current research problems.
World Health Organization. 1966. International Drug
Monitoring: The Role of the Hospital.
125
Proceedings of BioNLP Shared Task 2011 Workshop, pages 153?154,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Double Layered Learning for Biological Event Extraction from Text
Ehsan Emadzadeh, Azadeh Nikfarjam, Graciela Gonzalez
Arizona State University / Tempe, AZ 85283, USA
ehsan.emadzadeh@asu.edu, azadeh.nikfarjam@asu.edu
graciela.gonzalez@asu.edu
Abstract
This paper presents our approach (referred to
as BioEvent) for protein-level complex event
extraction, developed for the GENIA task
(Kim et al, 2011b) of the BioNLP Shared
Task 2011 (Kim et al, 2011a). We devel-
oped a double layered machine learning ap-
proach which utilizes a state-of-the-art mini-
mized feature set for each of the event types.
We improved the best performing system
of BioNLP 2009 overall, and ranked first
amongst 15 teams in finding ?Localization?
events in 201112. BioEvent is available at
http://bioevent.sourceforge.net/
1 Introduction
A biological event refers to a specific kind of inter-
action between biological entities. Events consist
of two parts: event triggers and event arguments.
Event extraction can be very challenging when deal-
ing with complex events with multiple or nested ar-
guments; for example, events themselves can be an
argument for other events.
2 Methods
In general, to detect an event mentioned in text, the
event trigger should be identified first, then comple-
mented with event arguments. We divided the train-
ing and testing tasks into two phases: trigger detec-
tion and argument detection.
1Using the ?Approximate Span without Event Trigger
Matching/Approximate Recursive? metric
2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/-
SharedTask/evaluation.shtml
2.1 Event Trigger Detection
The trigger detection problem can be modeled as
a multi-class classification of a word or combina-
tion of words (phrase). Instead of using all possible
phrases in the training text as examples for the clas-
sifier, we only included those that were known trig-
gers in the training set. For the official shared task
submission we used SVM light (Joachims, 1999).
Detailed explanation of the trigger detection process
includes three main steps: pre-processing, training
of the SVM models, and combining SVM results.
Pre-processing. All tokenized documents pro-
vided by the shared task organizers (Stenetorp et al,
2011) were converted to database records. Then dif-
ferent sets of attributes were defined and calculated
for words, sentences and documents.
Training SVM models and Combining Results.
We trained 9 different binary SVM models using
one-vs-many approach. One of the challenging tasks
was to compare the results of different SVM models,
given that each had different feature sets and their
confidence values were not directly comparable and
needed to be calibrated properly before comparing.
We tried three approaches: 1) selecting the SVM re-
sult with highest positive distance to hyperplane, 2)
using a trained decision tree and 3) using another
SVM trained for voting. Model J48 from the WEKA
library (Hall et al, 2009) was trained based on SVM
distances for the training set examples and expected
outputs. In the third approach, we tried SVM for
voting, which generated better results than the deci-
sion tree. Last two approaches consist of two layers
of classifiers which first layer includes event types
classifiers and second layer generates final decision
153
Event type Bioevent Turku09
Gene expression 71.88 70.84
Transcription 47.62 47.14
Protein catabolism 60.87 60.87
Phosphorylation 75.14 73.39
Localization 61.49 59.68
Binding 34.42 35.97
Regulation 24.03 22.26
Positive regulation 33.41 31.84
Negative regulation 18.89 18.58
ALL-TOTAL 44.69 43.54
Table 1: F-Value from our BioEvent system compared to
Turku09 (Bjorne et al, 2009) results, using Approximate
Span/Approximate Recursive matching
based on first layer outputs.
2.2 Arguments detection and Post-processing
Similar to trigger detection, argument detection can
be modeled for a classification task by assigning an
argument type label to each possible combination
of an event trigger and a biological entity in a sen-
tence. We obtained entities from a1 files, as well as
the supportive analysis data provided by the shared
task organizers (Bjorne et al, 2009). After gener-
ating events using SVM classification, we merged
them with the output from the Turku system to gen-
erate the final result. For common events (detected
by both systems) we used the arguments detected by
the Turku system.
3 Results
Since we tried to improve upon the best performing
system in the 2009 competition (Turku09), we com-
pare the results of our system and Turku09?s on the
2011 test set. Table 1 shows the performance of our
proposed system and that of Turku09. We see that
Binding was our worst event (negative change), Lo-
calization the most improved, no change for Protein
Catabolism, and only a slight improvement in Neg-
ative Regulation.
4 Conclusion and future work
In this research we focused on event trigger detec-
tion by applying a SVM-based model. SVM is very
sensitive to parameters and further tuning of param-
eters can improve the overall result. Furthermore,
we want to evaluate our method independently and
find the contribution of each modification to the fi-
nal result. Our method is generalizable to other do-
mains by using proper train-set and finding useful
attributes for new event types.
Acknowledgments
The authors would like to thank Ryan Sullivan
for his helps during this research. EE and GG
acknowledge partial funding from NLM Contract
HHSN276201000031C.
References
Jari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting Complex Biological Events with Rich Graph-
Based Feature Sets. Computational Linguistics,
(June):10?18.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
T. Joachims. 1999. Making large scale SVM learn-
ing practical. Advances in Kernel Methods - Support
Vector Learnin, (B. Scho?lkopf and C. Burges and A.
Smola (ed.)).
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
154
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 214?222,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Automatic Approaches for Gene-Drug Interaction Extraction  
from Biomedical Text: Corpus and Comparative Evaluation 
 
 
Nate Sutton, Laura Wojtulewicz, Neel Mehta, Graciela Gonzalez 
Department of Biomedical Informatics 
Arizona State University, Tempe, Arizona, USA 
{nate.sutton, whitz, nbmehta2, graciela.gonzalez}@asu.edu  
 
  
Abstract 
Publications that report genotype-drug inte-
raction findings, as well as manually curated 
databases such as DrugBank and PharmGKB 
are essential to advancing pharmacogenomics, 
a relatively new area merging pharmacology 
and genomic research. Natural language 
processing (NLP) methods can be very useful 
for automatically extracting knowledge such 
as gene-drug interactions, offering researchers 
immediate access to published findings, and 
allowing curators a shortcut for their work.   
We present a corpus of gene-drug interac-
tions for evaluating and training systems to 
extract those interactions.  The corpus in-
cludes 551 sentences that have a mention of a 
drug and a gene from about 600 journals 
found to be relevant to pharmacogenomics 
through an analysis of gene-drug relationships 
in the PharmGKB knowledgebase.  
We evaluated basic approaches to auto-
matic extraction, including gene and drug co-
occurrence, co-occurrence plus interaction 
terms, and a linguistic pattern-based method.  
The linguistic pattern method had the highest 
precision (96.61%) but lowest recall (7.30%), 
for an f-score of 13.57%. Basic co-occurrence 
yields 68.99% precision, with the addition of 
an interaction term precision increases slightly 
(69.60%), though not as much as could be ex-
pected. Co-occurrence is a reasonable base-
line method, with pattern-based being a prom-
ising approach if enough patterns can be gen-
erated to address recall. The corpus is availa-
ble at http://diego.asu.edu/index.php/projects 
1 Introduction 
Pharmacogenomics is a relatively new area of 
biomedical research that merges pharmacology and 
molecular genomics, among other disciplines, and 
focuses on studying the effects of genetic variabili-
ty on drug toxicity and efficacy, on the discovery 
of novel genomic targets for drug development, 
and on the identification and functional characteri-
zation of polymorphisms relevant to drug action.  
Thus, publications that report genotype-drug find-
ings and manually curated databases that collect 
such findings, like PharmGKB and DrugBank 
(Hewett et al, 2002; Wishart, 2006) are of para-
mount importance to the field.  However, manual 
curation is expensive and time consuming, and 
cannot keep up with the ever increasing number of 
publications.  Natural language processing (NLP) 
methods can be very useful for automatically ex-
tracting such gene-drug interactions, offering re-
searchers immediate access to published findings, 
and allowing curators a shortcut for their work. 
 Consider for example a sentence contain-
ing an interaction NLP can help extract:  ?Only the 
epsilon4 allele of APOE was found to make a sig-
nificant (P = 0.002) but small contribution to war-
farin dose requirement.? (PMID: 16847429).  We 
can easily see that in the sentence, an APOE allele 
interacts with the drug warfarin in its dose re-
quirement.   Furthermore, at a higher level of ab-
straction, the sentence can help researchers infer 
that APOE affects the metabolic processes targeted 
by the drug warfarin. 
 NLP researchers attacking an interaction 
extraction project such as this one, will usually 
start by identifying the entities involved in the ex-
tractions and the terms that indicate such interac-
tions.  Assuming named entity recognition (NER) 
systems exist for the entities in question (or a dic-
tionary is available for direct match), the main 
concern becomes extracting true interactions.  A 
gold standard corpus would then need to be identi-
fied or created in order to evaluate and develop 
interaction extraction approaches, starting with the 
214
simplest ones.  We aim to support advancement in 
the area of gene-drug interaction extraction 
through the construction of a corpus for that task 
that offers advantages not available in another sim-
ilar corpus.  Also for that support we report on a 
study of the capabilities of different methods for 
that form of extraction. 
To achieve our aim, we describe a new 
corpus of gene-drug interactions, and compare the 
performance of two basic approaches plus the re-
implementation of a more advanced pattern-based 
approach measured against this corpus.  We do not 
seek in this publication to advance the extraction 
methods themselves, but allow a side-to-side com-
parison of approaches on a single corpus. 
 The sentences in the corpus (a total of 551) 
were randomly selected from sentences that in-
clude both a gene and a drug mention from the ab-
stracts published on a selection of journals that 
have articles relevant to pharmacogenomics. In 
general, annotations include interactions evident 
from the sentence, if any, also noting when men-
tioned genes or drugs are not involved in interac-
tions.  All sentences were annotated by the main 
author, with a second and third annotator verifying 
26% of the corpus.  The corpus is publicly availa-
ble online along with other supplementary mate-
rials including the annotation guide1.  
 The extraction methods evaluated include 
co-occurrence of a gene and a drug, co-occurrence 
of a gene and a drug plus a recognized interaction 
term, and one that uses specific linguistic patterns 
for classification based on (Coulet, Shah, Garten, 
Musen, & Altman, 2010).  The linguistic pattern 
method had the highest precision (96.61%) but 
lowest recall (7.30%), for an f-score of 13.57%. 
Basic co-occurrence yields 68.99% precision, with 
the addition of an interaction term increasing pre-
cision slightly (69.60%), though not as much as 
could be expected.  Analysis of our results show 
that performance could be immediately improved 
by improving the fundamental entity-recognition 
of drugs and genes.  
2 Related Work 
A good portion of the work presented here follows 
prior approaches to high quality protein-protein 
interaction (PPI) corpora development and extrac-
                                                          
1 http://diego.asu.edu/index.php/projects 
tion.  Given that our corpus contains genes and 
proteins as entities, procedures used to create PPI 
corpora were a useful resource.  A variety of anno-
tation decisions made were informed by the work 
of Pyysalo et. al. on their BioInfer corpus (Pyysalo 
et al, 2007).  A detailed annotation guide used in 
their work was referenced in annotation rules in 
this work.  Other corpora, such as the ones used in 
Biocreative challenges, have also made valuable 
contributions to PPI extraction progress (Haken-
berg et al, 2010; Krallinger, Leitner, Rodriguez-
Penagos, & Valencia, 2008). 
 Unlike for PPI interaction extraction, there 
are very limited currently available corpora that 
can be used for automatic gene-drug interaction 
extraction system development and evaluation.  
One corpus that contains those interactions is a 300 
sentence corpus by Ahlers et al (Ahlers, Fiszman, 
Demner-Fushman, Lang, & Rindflesch, 2007).  
The Ahlers et. al. corpus include the biological 
interaction categories of inhibit, and stimulate in 
addition to interaction annotations for genes and 
drugs.  Our corpus does not contain those addition-
al categories directly, but the interaction words that 
are annotated in our corpus can indicate such cate-
gories as well as others.  All in all, our focus was 
on creating a corpus that could be used for evalua-
tion of basic as well as complex approaches, and 
allow machine-learning based systems to be 
trained on it. 
Current systems for extracting gene-drug 
interactions are based on entity co-occurrence and 
some include matching of relationship terms.  
Those systems commonly use statistical formulas 
for ranking the relevance of results.  Polysearch, 
Pharmspresso, and others are examples of such 
systems (Cheng et al, 2008; Garten & Altman, 
2009).  Some systems integrate linguistic patterns 
into their methods, such as those by Coulet et. al. 
and Tari et. al. (Luis Tari, J?rg Hakenberg, Gracie-
la Gonzalez, & Baral, 2009).  The system by Cou-
let et al explores the value of dependency graph 
information for relationship extraction.  Another 
result of Coulet et. al.'s work was the Phare ontol-
ogy that includes concepts relevant to those rela-
tionships, which we utilize in this work. The value 
of such collections of interaction-indicating terms 
has been highlighted before in the biomedical rela-
tionship extraction context (Bui, Nuall?in, Bouch-
er, & Sloot, 2010; Chowdhary, Zhang, & Liu, 
2009). 
215
3 Materials and Methods  
3.1  Corpus design. 
The purpose for the creation of the new corpus was 
to create a resource that NLP developers can use to 
train and test gene-drug interaction extraction sys-
tems.  The corpus was based on articles from jour-
nals that are known to contain pharmacogenomic 
relationships.  Genes and drugs were automatically 
tagged and then 551 sentences that contain both a 
gene and drug were randomly selected for annota-
tion.  The corpus and sentence selection process is 
described in the following subsections.  
 
Journal Selection.  A list of journals relevant to 
pharmacogenomics was generated by extracting 
the journal names from articles that have been cu-
rated in PharmGKB as containing evidence of 
gene-drug relationships. This list was generated 
from their downloadable ?relationships? file, 
which contains the abstract IDs of articles with 
manually curated gene-drug relationships.  591 
journal names were obtained this way.  The goal of 
using only those journals is to make the corpus 
representative of typical sentences containing a 
gene and drug from literature known to report 
pharmacogenomic findings. 
 
Sentence processing. All abstracts in PubMed from 
the relevant journal names were downloaded. A 
sentence splitter program from OpenNLP was used 
to extract sentences from the abstracts (?The 
OpenNLP Homepage,? n.d.).  A total of 
22,601,402 sentences were processed.  
 
Identification of entites.  Previous work in pharma-
cogenomics relationship extraction has shown ef-
fective results by classifying relationships after 
identifying sentences with entities of interest 
through dictionary matching techniques (Garten & 
Altman, 2009; Rebholz-Schuhmann et al, 2007).  
Our work takes a similar approach, but utilizes a 
machine-learning based method, BANNER, for 
gene recognition, as it was shown to have better 
performance than a dictionary-based method 
(Leaman & Gonzalez, 2008). Drugs were recog-
nized through the use of dictionary matching.  The 
dictionaries used for drugs were based on drug 
names available at DrugBank.  Exact full token 
matching of drug terms was used to identify them 
in sentences. Although incorrectly tagged (false 
entity) genes and drugs were corrected by annota-
tors, they did not add entities missed by NER rec-
ognition. A second round of annotation will correct 
this when we shift focus to NER. 
 Terms indicative of an interaction for add-
ing to basic co-occurrence relationship extraction 
were extracted from the Phare ontology.  The 
terms acquired were from rdfs labeled text in the 
?object properties? in the ontology.  Object proper-
ties are elements of the ontology that describe rela-
tionships between classes such as gene and drugs, 
yielding 168 unique terms after stemming. 
 
Sentence selection.  The initial annotation effort 
that is the focus of this paper was aimed at com-
pleting around 500 sentences as a proof of concept, 
with a total of 1,500 to be completed in the second 
phase of this project.  Random selection of sen-
tences that include a gene and a drug, in contrast to 
balanced positive and negative selection, was used 
to make the corpus reflect typical sentences poten-
tially containing an interaction that can be easily 
extracted from the source articles after simple 
(drug and gene) concept tagging, which is the most 
basic approach to interaction extraction.  The ran-
domized ratio of positive and negative interactions 
in the corpus is useful for training classification 
systems that operate on similarly pre-processed 
sentences to account for that naturally occurring 
ratio. 
 
3.2  Annotation. 
An annotation tool named STAV was used to 
create annotations (?stav,? n.d.).  Customization of 
the tool was performed to match the types of anno-
tations needed for the corpus.  The identified enti-
ties were formatted for use with the tool.  Annota-
tions created with the tool were stored in the Bi-
oNLP shared task file format. That format is com-
patible with a variety of existing systems for rela-
tionship extraction.  
 
Annotation guidelines. Based on a review of litera-
ture on related annotation guidelines for relation-
ships such as PPIs, an initial annotation guideline 
was created based on a small sample of sentences.  
The guide was iteratively refined through annota-
tion of additional sentences, until considered suffi-
ciently stable for release to additional annotators.   
The guideline was refined to achieve a bal-
ance of complexity and clarity to assist annotators.  
216
Only a few (5-10) example sentences per annotator 
have been discussed in person.  The explicit writ-
ten instructions in the guide were relied on more 
than in-person example sentence discussions to 
train annotators to handle the complicated content 
of the corpus and avoid over-influencing the anno-
tators, as noted that is possible with the overuse of 
those examples (Hovy & Lavid, 2008). 
 The first annotator, a student with a Bache-
lor of Science (BS) in Biology, was the main anno-
tator and author of the guidelines. The second and 
third annotators are PhD students in Biomedical 
Informatics, the second with a BS in Biology and 
10 years nursing experience, and the other with a 
Bachelor of Technology in Bioinformatics.  Week-
ly annotation meetings were done on individual 
bases.  A short checklist of things to look for in 
annotations was distributed in addition to the 
guidelines. 
 
Annotations.  The following describes major anno-
tation categories and subcategories in the corpus:  
 
? Interaction  Genes and drugs are annotated 
simply as ?having an interaction? broadly un-
derstood as having an ?action, effect, or influ-
ence? on each other.  All gene-drug interac-
tions annotated must have at least one interac-
tion term that helps explain the interaction.  
Additional properties that were annotated and 
a brief explanation of their purpose include: 
o Direct/Indirect:  Describes the complexi-
ty in the interaction statements. An ?indi-
rect? interaction is one where the presence 
of an intermediary entity is needed for se-
mantic understanding of the interaction. 
o Explicit/Inferred:  Records if an infe-
rence had to be made on whether the inte-
raction was present because an interaction 
was not explicitly stated. 
? Non-interaction 
o Shared Entity:  An entity connected to 
both a gene and a drug that don't interact 
with each other.  In contrast to an interme-
diary entity. 
? Interaction Term  Terms that are descriptive 
of the interaction (as defined earlier).  These 
terms are helpful for capturing more specifical-
ly the type of interaction present.  
? Intermediary Entity  These are non-gene, 
non-drug entities that are closely connected to 
the interaction.  They are entities that are 
needed for understanding of the full semantic 
meaning of gene-drug interactions.  These enti-
ties are not annotated themselves but they are 
used to determine the indirectness property. 
 
Examples of these categories can be seen in the 
sentence: ?Using standard steady-state kinetic 
analysis, it was demonstrated that paclitaxel was a 
possible uncompetitive inhibitor to NAT activity in 
cytosols based on the decrease in apparent values 
of K(m) and V(max).? (PMID: 11955677).  This 
sentence includes an interaction between the drug 
paclitaxel and gene NAT.  An interaction term that 
helps establish that the interaction is present is ?in-
hibitor?.  ?Cytosols? is where the NAT inhibition 
activity can occur and represents an intermediary 
entity that is needed in the semantic meaning of the 
interaction. 
 The broad definition of interaction was 
used to make progress toward annotations includ-
ing, and in turn being representative of, the most 
general form of gene-drug interaction that is de-
scribed in the source abstracts.  We chose to first 
concentrate on getting good inter-annotator agree-
ment using the general definition before consider-
ing additionally annotating specific biological inte-
raction types.  Annotated interactions are required 
to have at least one annotated interaction term (al-
though terms do not have to be from the predefined 
list) to ensure that specific and identifiable lan-
guage is present that justifies the annotation.   
 The subcategories included were added to 
record the linguistic complexity in which the inte-
ractions and non-interactions are described.  Re-
cording that complexity can help system develop-
ers handle its presence when trying to automatical-
ly recognize interaction statements.  Additionally, 
the annotation properties of speculation, negation, 
and nesting were allowed but not separately anno-
tated in interaction annotations.  
 Each annotator reported annotation time 
estimates.  Total time spent on annotations includ-
ing meetings but not other work (e.g. guideline 
development) was approximately 80 hours for the 
primary annotator and 20 hours combined for other 
annotators.  Hard sentences to annotate required 
research into source articles and entities described.   
 
217
Evaluation of the Corpus. Around 26% of the cor-
pus was annotated by a second and third annotator.  
A program was created for IAA scoring, account-
ing for nested entities and equivalent entities in-
cluding abbreviations.  Manual review was used to 
verify the program?s scores.   Example sentences 
from the corpus discussed with annotators were not 
used for IAA scoring. 
 
3.3  Relationship Extraction methods. 
Three basic methods for extracting interactions 
were implemented for evaluation. The basic me-
thod, co-occurrence, is inherent to the corpus as all 
sentences are selected based on both entities being 
present in them. Thus, in co-occurrence, any men-
tion of a gene and a drug together in a sentence 
represents an interaction between those entities. 
Co-occurrence plus interaction terms, the 
second method tried, identifies that interactions are 
present only when sentences contain an interaction 
word from a predefined list.  The list of interaction 
terms obtained from the Phare ontology was fil-
tered by removing common stop words. Also, a 
filter was applied to only use terms greater than 
two letters in size.  Those filters were used to avoid 
unneeded matches from common words. 
 The linguistic pattern based extraction me-
thod developed for this evaluation was based on 
the work by Coulet et. al.  Specific linguistic pat-
terns described in that work were used to classify 
the presence of interactions between genes and 
drugs.  A program named Graph Spider was used 
to match the specified patterns within sentences 
(Shepherd & Clegg, 2008).  The Stanford Parser 
was used to generate dependency graphs for use 
with the pattern recognition in Graph Spider.   
 The dependency rules designed by Coulet. 
et. al. were entered into Graph Spider using the 
metapattern language (MPL) designed by the 
Graph Spider authors.  MPL is a pattern formalism 
that can be used to match dependency subgraph 
patterns in dependency parsed text.  After depen-
dency graphs were generated for processing in 
Graph Spider, text representing genes and drugs in 
the graphs were converted to general tags for those 
entity types.  Those conversions were made to al-
low the patterns in MPL to be generalizable.  
 Java programs were created to reformat 
and score the subgraph pattern match results made 
by Graph Spider.  Scoring used text character posi-
tions (spans) of entities included in annotations.  
True positives were recorded when pairs of entity 
spans in Graph Spider subgraph results matched 
annotated pairs of entity spans labeled as having 
interactions.  False positives and false negatives 
were similarly assessed using entity spans.  A ma-
nual evaluation of pattern matched output com-
pared to annotations was performed to ensure ac-
curacy. 
 A condition applied in the pattern based 
system was that the patterns can match up to four 
modifier words for each individual gene and drug 
in interaction pattern matches.  Those words are 
additional words that modify the meaning of the 
gene or drug in the interaction.  The limit was in-
cluded for practical reasons, as hand coding of pat-
terns in MPL is complex.  The rules described by 
Coulet et. al. did not specify any limit on modifier 
words but the difference in results by including a 
realistic limit is predicted to be negligible. 
4 Results  
A total of 551 sentences are annotated, with 781 
interactions present in them. There are 351 in-
stances of non-interactive entities in the same set.   
The average length of sentences is 28.1 words.  
Table 1 describes further properties of the corpus.   
 
Annotation Analysis. The inter-annotator agree-
ment scores are reported as accuracy and Cohen?s 
kappa.  Kappa was chosen due to its widespread 
use and therefore comparability with other work in 
corpus creation.  Accuracy is found by the number 
of instances agreed on divided by the total in-
stances annotated.  A total of 144 sentences were 
used for the scoring.  Annotators 1 and 2, 1 and 3, 
and 2 and 3 were compared using 92, 52, and 61 
sentences respectively.  IAA results with the main 
categories of interaction vs. non-interaction are 
shown in Table 2. 
 
 
Sentences Tokens (with 
punctuation) 
Words (tokens with 
no punctuation) 
551 18,585 15,464 
Table 1.  Statistics describing corpus properties. 
 1 & 2 1 & 3 2 & 3 
Accuracy 81.1% 74.2% 73.0% 
Kappa 45.7% 30.5% 11.4% 
Table 2.  Inter-annotator agreement results. 
218
 IAA scores were found for all annotated 
subcategories.  Those subcategories are DirectEx-
plicit, IndirectExplicit, IndirectInferred for interac-
tions and SharedEntity for non-interactions.  Their 
ranges of scores with all annotator pair groups us-
ing accuracy scores are 72-79%, 40-69%, 62-82%, 
50-60% and kappa scores are 31-58%, 1-27%, -4-
31%, 0-4% respectively.  Those scores are created 
by selecting main category inter-annotator matches 
(e.g. interaction) and calculating the IAA between 
the annotated subcategories. 
In some sentences, annotators missed 
doing annotations for gene-drug instances that the 
other annotator added. IAA scores did not include 
annotations made by only one annotator.  Confir-
mation with annotators was made that annotations 
not made were not intended to represent non-
interactions.  The percentage of missed inter-
annotator instances was approximately 20%.  Fu-
ture work will be to improve the inter-annotator 
annotation process so that those instances are not 
missed for IAA scoring.  While some annotations 
were missed in IAA scoring, annotations by the 
primary annotator that are included in the corpus 
contain all instances (none missed) from the source 
text to our knowledge.
 
I
D 
Contents Agree
ment 
Sentence text 
A One direct expli-
cit interaction 
Y This suggests that galantamine (GAL), a cholinesterase inhibitor, could be 
effective when seeking to prolong abstinence in recently detoxified alcohol-
ics. (PMID: 16328375) 
B One indirect ex-
plicit and four 
shared entity 
non-interactions  
Y They are widely distributed and mediate all of the known biologic effects of 
angiotensin II (AngII) through a variety of signal transduction systems, in-
cluding activation of phospholipases C and A2, inhibition of adenylate cyc-
lase, opening of calcium channels, and activation of tyrosine kinases. (PM-
ID: 9892138) 
C One indirect ex-
plicit interaction 
N The results of studies of perfused rat hearts with completely inhibited crea-
tine kinase show significantly decreased work capacity and respectively, 
energy fluxes, in these hearts in spite of significant activation of adenylate 
kinase system (Dzeja et al this volume). (PMID: 9746326) 
Table 3.  Example sentences from the corpus.
 
Table 4.  Extraction system performances.    
Note that sentences were selected based on co-
occurrence of a gene and a drug, thus recall is 
100% for that method, as it essentially defines 
the corpus. 
 
 
The scoring methods used were instance 
level scoring instead of sentence level scoring.  In 
the instance level scoring each gene-drug instance 
counted in performance scores.   
 A caveat about the pattern-based system 
scoring should be noted.  That caveat was that the 
Graph Spider software used was unable to process 
approximately 10% (around 50) of the sentences in 
the corpus due to errors.  The pattern-based system 
is likely to have scored slightly higher if it could 
have processed those sentences. 
5 Discussion 
5.1  Analyses of interaction extraction methods 
performance. 
Interaction  
Extractor Type 
Precision 
(TP/TP+FP) 
Recall 
(TP/TP+
FN) 
F1-Score 
(2*((P*R)/(
P+R))) 
Co-occurrence 68.99%  
(781/1132) 
100.00% 
(781/781) 
 
81.65% 
Co-occurrence 
plus int. terms 
69.60% 
(664/954) 
85.02% 
(664/781) 
76.54% 
Pattern-based 96.61% 
(57/59)  
7.30% 
(57/781) 
13.57% 
 
219
The f-score of co-occurrence with and without in-
teraction terms showed better performance than the 
pattern-based interaction extractions, which was 
expected. Pattern based methods, particularly those 
where the patterns were manually created, are typi-
cally very high in precision and very low in recall, 
as they are highly dependant on the specific pat-
terns included for recognition. Although recall was 
low, users who want very high confidence interac-
tion predictions or interactions of a very specific 
type can benefit from the pattern-based system?s 
demonstrated high precision. Co-occurrence can 
suit users who want to focus on recall. 
Coulet et al reported their system scored a 
precision of 70% for exact match and 87.7% for 
exact or incomplete match but true classification.  
Our results are similar to their 87.7% results in 
both percentage and scoring method.  The method 
that allows incompleteness accepts matches that 
accurately identify core pharmacogenomic rela-
tionships but don?t need to correctly match modifi-
er words.  Our scoring is similar in not needing to 
match modifier words.  The similarity in results 
indicates that we correctly implemented the system 
that Coulet et al designed.  That indication does 
have the limitation that the 10% of sentences una-
ble to be processed may have affected the results. 
An example of a more complex interaction 
that was matched by co-occurrence with an inte-
raction term but not the pattern-based method was 
?Moreover, S-nitrosylation of thioredoxin was also 
significantly augmented after atorvastatin treat-
ment.? (PMID: 15289372).  In that sentence, an 
interaction occurred where thioredoxin's (gene) S-
nitrosylation was augmented by atorvastatin 
(drug).  Analysis of the dependency graphs used by 
the pattern-based system revealed some reasons 
why it was unable to identify the interaction.  
 The pattern-based system uses a rule that 
applies to that sentence: a potential pattern se-
quence match can be ?interrupted? by a dependen-
cy that does not fit accepted patterns.  In the non-
classified sentence, the entities ?was? and ?aug-
mented? were terms that caused the pattern match-
ing to be interrupted.  Both ?was? and ?aug-
mented? are not nouns or prepositions.  They both 
also are needed in the dependency subgraph that 
connects the gene and drug together.  Those parts 
of speech are not allowed to be chained together in 
the pattern-based system's patterns.  That deviation 
from the allowed patterns caused the system to 
miss that interaction. 
Adding patterns with more diversity in al-
lowed parts of speech in series of interaction terms 
that connect genes and drugs in interactions can 
improve recall performance.  A review of parts of 
speech (POS) in missed matches showed that some 
misses were due to no verb POS tags being present 
in interaction descriptions.  That can occur when 
verbs are in their nominalized form or other situa-
tions.  Mining the corpus for both part of speech 
and dependency graph patterns can identify pat-
terns that are able to correct those misses.  Also, 
the POS tagger included with the parser mis-
tagged a variety of words.  Using a higher perfor-
mance tagger or one trained on biomedical text 
may help with pattern matches.  
Ahlers et. al. also reported relationship ex-
traction performance from a new system with their 
gene-drug corpus.  That system achieved a preci-
sion of 73% and recall of 50% extracting an anno-
tation category including gene-drug relationships.  
The system is built upon an earlier system and an 
important part of its capabilities comes from spe-
cialized linguistic rules it uses.  The corpus in-
cluded in this work can be useful for further devel-
opment of systems that integrate such rules with 
other methods to improve extraction performances. 
Some characteristics were notable about 
the results of the methods using co-occurrence 
with and without interaction terms.  The perfor-
mances found of those methods may be specific to 
an increased amount of gene-drug interactions 
found in the journals used compared to other jour-
nals.  Also, the use of interaction terms from the 
Phare ontology was expected to increase precision 
because they were found from predicted pharma-
cogenomic relationships.  The co-occurrence with 
interaction terms method resulted in only approx-
imately equaling the precision of basic co-
occurrence.  One possible reason for that is the 
terms were originally found partly with disease 
relationships.  They therefore can be less relevant 
to gene-drug interactions.   
 
5.2  Analyses of annotations 
Table 2 includes that the general interaction anno-
tations had the kappa values 46%, 30%, 11% 
which are considered only moderate to low scores 
by common rating methods.  Some IAA scores, 
such as kappa, include a correction for chance 
220
agreement probability.  An intentional design 
choice was made in the corpus to allow an unba-
lanced but natural ratio of interactions to non-
interactions.  That imbalance increased kappa?s 
correction.  Although our reasonably high IAA 
scores with accuracy helped increase the kappa 
score, they were not enough to offset the correction 
and bring kappa above the moderate score.   
 An article by Strijbos et. al. states that 
kappa can have a strict chance agreement correc-
tion in the case of few categories (Strijbos, Mar-
tens, Prins, & Jochems, 2006).  Given that general 
interaction scores were only based on the catego-
ries of present or absent, kappa may have been 
overly strict with the correction.  If that correction 
in our data is not strict, but justified, than that indi-
cates how further improving our annotation 
process can be valuable.  Further investigation will 
go into understanding what statistics may be useful 
for scoring given the corpus properties.  Explora-
tion will also continue with talking to annotator s 
about what may be causing disagreement.  That 
exploration will help reveal ways to improve IAA. 
 Subcategories showed mixed results in 
their IAA performances.  The subcategories with 
the highest IAA scores may indicate that those 
subcategories are more clearly defined than others 
in the annotation guide. 
 Reviewing some annotated sentences can 
help clarify how the IAA results occurred.  All an-
notators agreed the drug galantamine has a direct 
explicit interaction with cholinesterase in sentence 
A in Table 3.  Such an interaction description is 
simply described and an annotator has reported 
that type of interaction being the easiest to identify.   
 Agreement was found with all annotators 
for annotations in sentence B in Table 3.  It was 
readily understandable to annotators that calcium 
and other signal transduction systems do not have 
an interaction simply for all being a part of those 
types of systems. 
 An example of a sentence with annotator 
disagreement was sentence C in table 3. Although 
endogenously produced in this case, the nested 
entity creatine was considered a drug due to being 
relevant to creatine in its exogenous drug form. 
 The occurrence of multiple properties, 
such as inhibition and effects on hearts can make it 
difficult to follow the logic of the interaction be-
tween creatine and adenylate kinase (enzyme).  
The interaction annotation can be hard for annota-
tors to find due to that complexity and the subtle-
ness of the ?in spite of? phrase describing the ne-
gated effect between the drug and gene.  The inte-
raction is negated but that still is considered an 
interaction by the annotation rules used. 
   
5.3  Future Work 
As mentioned before, the corpus will grow from 
around 500 sentences that it has right now to 
around 1,500.  The larger the corpus expands to be, 
the more representative it will become of gene-
drug interactions.  Other future work includes work 
with more advanced interaction extraction systems. 
Along with this publication, a version of 
the corpus with high confidence in annotations will 
be released.  Given that this is an initial work, a 
relatively modest amount of annotation revisions 
may occur with a few periodic later version releas-
es of the corpus to improve its quality. 
 Unfortunately no tagger is perfect so as 
annotations proceed, drugs or genes that were 
missed by the tagger can be investigated to further 
understand why that occurred.  An example of a 
commonly missed drug was acetylcholine.  Ace-
tylcholine was picked up as a drug if it was spelled 
out, but not if it was abbreviated as ACh and it is 
commonly abbreviated.   
6 Conclusion 
The extraction results indicated that the systems 
tested can be utilized and built upon according to 
user preferences in precision, recall, or specific 
interaction terms.  The corpus presented here offers 
valuable utility to system developers working to-
ward achieving favorable balances of precision and 
recall in gene-drug interaction extractions.  The 
growth of that corpus will also increasingly benefit 
the developers working on those extractions.  That 
type of extraction is important to advancing work 
in pharmacogenomics by retrieving knowledge for 
individuals working in the field.  
Acknowledgements 
The authors wish to thank Ehsan Emadzadeh for 
his help with the annotation tool and Robert Lea-
man for his help with annotation methods. 
 
 
 
221
References  
Ahlers, C., Fiszman, M., Demner-Fushman, D., 
Lang, F.-M., & Rindflesch, T. (2007). Extracting 
semantic predications from Medline citations for 
pharmacogenomics. Pacific Symposium on Bio-
computing. Pacific Symposium on Biocomputing, 
209?220. 
Bui, Q.-C., Nuall?in, B. O., Boucher, C. A., & 
Sloot, P. M. A. (2010). Extracting causal relations 
on HIV drug resistance from literature. BMC Bio-
informatics, 11, 101. doi:10.1186/1471-2105-11-
101 
Cheng, D., Knox, C., Young, N., Stothard, P., Da-
maraju, S., & Wishart, D. S. (2008). PolySearch: a 
web-based text mining system for extracting rela-
tionships between human diseases, genes, muta-
tions, drugs and metabolites. Nucleic Acids Re-
search, 36(Web Server issue), W399?405. 
doi:10.1093/nar/gkn296 
Chowdhary, R., Zhang, J., & Liu, J. S. (2009). 
Bayesian inference of protein-protein interactions 
from biological literature. Bioinformatics (Oxford, 
England), 25(12), 1536?1542. 
doi:10.1093/bioinformatics/btp245 
Coulet, A., Shah, N. H., Garten, Y., Musen, M., & 
Altman, R. B. (2010). Using text to build semantic 
networks for pharmacogenomics. Journal of Bio-
medical Informatics, 43(6), 1009?1019. 
doi:10.1016/j.jbi.2010.08.005 
Garten, Y., & Altman, R. B. (2009). Pharmspresso: 
a text mining tool for extraction of pharmacoge-
nomic concepts and relationships from full text. 
BMC Bioinformatics, 10 Suppl 2, S6. 
doi:10.1186/1471-2105-10-S2-S6 
Hakenberg, J., Leaman, R., Vo, N. H., Jonnalagad-
da, S., Sullivan, R., Miller, C., Tari, L., et al 
(2010). Efficient extraction of protein-protein inte-
ractions from full-text articles. IEEE/ACM Trans-
actions on Computational Biology and Bioinfor-
matics / IEEE, ACM, 7(3), 481?494. 
doi:10.1109/TCBB.2010.51 
Hewett, M., Oliver, D. E., Rubin, D. L., Easton, K. 
L., Stuart, J. M., Altman, R. B., & Klein, T. E. 
(2002). PharmGKB: The Pharmacogenetics Know-
ledge Base. Nucleic Acids Research, 30(1), 163?
165. doi:10.1093/nar/30.1.163 
Krallinger, M., Leitner, F., Rodriguez-Penagos, C., 
& Valencia, A. (2008). Overview of the protein-
protein interaction annotation extraction task of 
BioCreative II. Genome Biology, 9 Suppl 2, S4. 
doi:10.1186/gb-2008-9-s2-s4 
Leaman, R., & Gonzalez, G. (2008). BANNER: an 
executable survey of advances in biomedical 
named entity recognition. Pacific Symposium on 
Biocomputing. Pacific Symposium on Biocomput-
ing, 652?663. 
Luis Tari, J?rg Hakenberg, Graciela Gonzalez, & 
Baral, C. (2009). Querying parse tree database of 
medline text to synthesize user-specific biomolecu-
lar networks. CiteSeerX. Retrieved from 
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=
10.1.1.140.8574 
Pyysalo, S., Ginter, F., Heimonen, J., Bj?rne, J., 
Boberg, J., J?rvinen, J., & Salakoski, T. (2007). 
BioInfer: a corpus for information extraction in the 
biomedical domain. BMC Bioinformatics, 8, 50. 
doi:10.1186/1471-2105-8-50 
Rebholz-Schuhmann, D., Kirsch, H., Arregui, M., 
Gaudan, S., Riethoven, M., & Stoehr, P. (2007). 
EBIMed?text Crunching to Gather Facts for Pro-
teins from Medline. Bioinformatics, 23(2), e237?
e244. doi:10.1093/bioinformatics/btl302 
Sconce, E. A., Daly, A. K., Khan, T. I., Wynne, H. 
A., & Kamali, F. (2006). APOE genotype makes a 
small contribution to warfarin dose requirements. 
Pharmacogenetics and Genomics, 16(8), 609?611. 
doi:10.1097/01.fpc.0000220567.98089.b5 
Shepherd, A. J., & Clegg, A. B. (2008). Syntactic 
pattern matching with GraphSpider and MPL. Pro-
ceedings of the Third International Symposium on 
Semantic Mining in Biomedicine SMBM 2008 Tur-
ku Finland, 129?132. 
stav. (n.d.).GitHub. Retrieved March 26, 2012, 
from https://github.com/TsujiiLaboratory/stav 
Strijbos, J.-W., Martens, R. L., Prins, F. J., & Jo-
chems, W. M. G. (2006). Content analysis: What 
are they talking about? Computers & Education, 
46(1), 29?48. doi:10.1016/j.compedu.2005.04.002 
T1.pdf. (n.d.). Retrieved from http://www.lrec-
conf.org/proceedings/lrec2008/workshops/T1.pdf 
The OpenNLP Homepage. (n.d.). Retrieved March 
26, 2012, from 
http://opennlp.sourceforge.net/projects.html 
Wishart, D. S. (2006). DrugBank: a comprehensive 
resource for in silico drug discovery and explora-
tion. Nucleic Acids Research, 34(90001), D668?
D672. doi:10.1093/nar/gkj067 
 
222
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 1?9,
Baltimore, Maryland USA, June 26-27 2014. c?2014 Association for Computational Linguistics
Natural Language Processing Methods for Enhancing Geographic 
Metadata for Phylogeography of Zoonotic Viruses 
Tasnia Tahsin 
Department of Biomedical 
Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
 ttahsin@asu.edu 
Rachel Beard 
Department of Biomedical  
Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
rachel.beard@asu.edu 
Robert Rivera 
Department of Biomedical 
Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
 rdriver1@asu.edu 
   
Rob Lauder 
Department of Biomedical 
Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
 rlauder@asu.edu 
Davy Weissenbacher 
Department of Biomedical  
Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
dweissen@asu.edu 
Garrick Wallstrom 
Department of Biomedical 
Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
 gwallstrom@asu.edu 
 
Matthew Scotch 
Department of Biomedical Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
 mscotch@asu.edu 
Graciela Gonzalez 
Department of Biomedical Informatics 
Arizona State University 
13212 E Shea Blvd 
Scottsdale, AZ 85259 
 Graciela.gonzalez@asu.edu 
  
Abstract 
Zoonotic viruses, viruses that are trans-
mittable between animals and humans, 
represent emerging or re-emerging patho-
gens that pose significant public health 
threats throughout the world. It is there-
fore crucial to advance current surveil-
lance mechanisms for these viruses 
through outlets such as phylogeography. 
Phylogeographic techniques may be ap-
plied to trace the origins and geographical 
distribution of these viruses using se-
quence and location data, which are often 
obtained from publicly available data-
bases such as GenBank. Despite the abun-
dance of zoonotic viral sequence data in 
GenBank records, phylogeographic anal-
ysis of these viruses is greatly limited by 
the lack of adequate geographic metadata. 
Although more detailed information may 
often be found in the related articles refer-
enced in these records, manual extraction 
of this information presents a severe bot-
tleneck. In this work, we propose an auto-
mated system for extracting this infor-
mation using Natural Language Pro-
cessing (NLP) methods. In order to vali-
date the need for such a system, we first 
determine the percentage of GenBank rec-
ords with ?insufficient? geographic 
metadata for seven well-studied zoonotic 
viruses. We then evaluate four different 
named entity recognition (NER) systems 
which may help in the automatic extrac-
tion of information from related articles 
that can be used to improve the GenBank 
geographic metadata. This includes a 
novel dictionary-based location tagging 
system that we introduce in this paper.  
1
1 Introduction 
Zoonotic viruses, viruses that are transmittable 
between animals and humans, have become in-
creasingly prevalent in the last century leading to 
the rise and re-emergence of a variety of diseases 
(Krauss, 2003). In order to enhance currently 
available surveillance systems for these viruses, a 
better understanding of their origins and transmis-
sion patterns is required. This need has led to a 
greater amount of research in the field of phylo-
geography, the study of geographical lineages of 
species (Avise, 2000). Population health agencies 
frequently apply phylogeographic techniques to 
trace the evolutionary changes within viral line-
ages that affect their diffusion and transmission 
among animal and human hosts (Ciccozzi et al., 
2013; Gray and Salemi, 2012; Weidmann et al., 
2013). Prediction of virus migration routes en-
hances the chances of isolating the viral strain for 
vaccine production. In addition, if the source of 
the strain is identified, intervention methods may 
be applied to block the virus at the source and 
limit outbreaks in other areas.  
Phylogeographic analysis depends on the utili-
zation of both the sequence data and the location 
of collection of specific viral sequences.  Re-
searchers often use publicly available databases 
such as GenBank for retrieving this information. 
For instance, Wallace and Fitch (2008) used data 
from GenBank records to study the migration of 
the H5N1 virus in various animal hosts over Eu-
rope, Asia and Africa, and were able to identify 
the Guangdong province in China as the source of 
the outbreak.  However, the extent of phylogeo-
graphic modeling is highly dependent on the spec-
ificity of available geospatial information and the 
lack of geographic data more specific than the 
state or province level may limit phylogeographic 
analysis and distort results. In the previous exam-
ple, Wallace and Fitch (2008) had to use town-
level information to identify the source of the 
H5N1 outbreak; without specific location data, 
they would not have been able to identify the 
Guangdong province as the source. Unfortu-
nately, while there is an abundance of sequence 
data in GenBank records, many of them lack suf-
ficient geographic metadata that would enable 
specific identification of the isolate?s location of 
collection. A prior study conducted by Scotch et 
al. (2011) showed that the geographic information 
of 80% of the GenBank records associated with 
single or double stranded RNA viruses within tet-
rapod hosts is less specific than 1st level adminis-
trative boundaries (ADM1) such as state or prov-
ince.  
Though many of the records lack specific geo-
graphic metadata, more detailed information is of-
ten available within the journal articles referenced 
in them. However, manual extraction of this infor-
mation is time-consuming and cumbersome and 
presents a severe bottleneck on phylogeographic 
analysis. In this work, we investigate the potential 
of NLP techniques to enhance the geographic data 
available for phylogeographic studies of zoonotic 
viruses using NER systems. In addition to geo-
graphic metadata and sequence information, Gen-
Bank records also contain several other forms of 
metadata such as host, collection date and gene for 
each isolate. Journal articles that are referenced in 
these records often mention the location of isola-
tion for the viral sample in conjunction with re-
lated metadata (Figure 1 provides an example of 
such a case). Therefore, by allowing identification 
of location mentions along with mentions of re-
lated GenBank metadata in these articles, we be-
lieve that NER systems may help to accurately 
link each GenBank record to its corresponding lo-
cation of isolation and distinguish it from other lo-
cation mentions.  
Previously Scotch et al. (2011) evaluated the 
performance of BANNER (Leaman and Gonza-
lez, 2008) and the Stanford NER tool (Finkel et 
al., 2005) for automated identification of gene and 
location mentions respectively, in 10 full-text 
PubMed articles, each related to a specific Gen-
Bank record. They were both found to achieve f-
scores of less than 0.45, thereby establishing the 
need for NER systems with better performance 
and/or a larger test corpus (Scotch et al, 2011). In 
this study, we start by evaluating the state of geo-
graphic insufficiency for zoonotic viruses in Gen-
Bank records using a new automated approach. 
Next, we further expand upon the work done by 
Scotch et al. (2011) by building our own diction-
ary-based location-tagging system and evaluating 
its performance on a larger corpus corresponding 
to over 8,500 GenBank records for zoonotic vi-
ruses. In addition, we also evaluate the perfor-
mance of three other state-of-the-art NER tools 
for tagging gene, date and species mentions in this 
corpus. We believe that identification of these en-
tities will be useful for the future development of 
a system for extracting the location of collection 
of viral isolates from articles related to their re-
spective GenBank records.   
 
 
2
 
                                GenBank Record 
 
                
 
 
Figure 1. Example of how the date, gene, and strain metadata within a GenBank record may be 
used to differentiate between two potential locations in a related article 
2 Methods 
The process undertaken to complete this study can 
be divided into three distinct stages: selection of 
the zoonotic viruses and extraction of relevant 
GenBank data related to each virus, computation 
of ?sufficiency? statistics on the extracted data, 
and development/evaluation of NER systems for 
tagging location, gene, date and species mentions 
in full-text PubMed Central articles. A detailed 
description of each phase is given below. 
2.1 Virus Selection and GenBank Data Ex-
traction 
The domain of this study has been limited to zo-
onotic viruses that are most consistently docu-
mented and tracked by public health, agriculture 
and wildlife state departments within the United 
States. These viruses include influenza, rabies, 
hantavirus, western equine encephalitis (WEE), 
eastern equine encephalitis (EEE), St. Louis en-
cephalitis (SLE), and West Nile virus (WNV). 
The Entrez Programming Utilities (E-Utilities) 
was used to download the following fields from 
59,595 GenBank records associated with these vi-
ruses: GenBank Accession ID, PubMed Central 
ID, Strain name, Collection date and Country. 
These records were the result of a query per-
formed to retrieve all accession numbers related 
to the selected viruses which had at least one ref-
erence to a PubMed Central article. The results 
                                                 
1 Iso.org. [Internet]. Gen?ve. c2013. Available from 
http://www.iso.org/iso/home/standards/country_codes.htm 
 
from the query was retrieved on August 22nd, 
2013.  
2.2 Sufficiency Analysis 
Database Integration: The data extracted from 
Genbank was used to compute the percentage of 
GenBank records that had insufficient geographic 
information for each of the selected viruses. In or-
der to perform this computation, we used data 
from the ISO 3166-1 alpha-2 1  table and the 
GeoNames database. The ISO 3166-1 alpha-2 is 
the International Standard for representing coun-
try names using two-letter codes. The GeoNames2 
database contains a variety of geospatial data for 
over 10 million locations on earth, including the 
ISO 3166-1 alpha-2 code for the country of each 
location and a feature code that can be used to de-
termine the administrative level of each location. 
To allow for efficient querying, we downloaded 
the main GeoNames table and the ISO alpha-2 
country codes table from their respective websites 
and stored them in a local SQL database. Prior to 
adding the ISO data to the database, some com-
monly used country names and their correspond-
ing country codes were added to the table since it 
only included a single title for each country. For 
example, the ISO table included the country name 
?United States? but not alternate names such as 
?USA?, ?United States of America?, or ?US?. Us-
ing the created database in conjunction with a par-
ser written in Java, we were able to retrieve most 
2 Geonames.org. [Internet]. Egypt. c2013. [updated 2013 
Apr 30] Available from http://www.geonamesorg/EG/ad-
ministrative-division-egypt.html 
Related PubMed Article 
3
of the geographic information present within the 
records and classify each of them as sufficient or 
insufficient.   
 
 
 
Figure 2. Sufficiency Criteria 
 
Sufficiency Criteria: For the purpose of this 
project, we considered any geographical bound-
ary more specific than ADM1 to be ?sufficient?. 
Based on this criterion, a feature code in 
GeoNames was categorized as sufficient only if it 
was absent from the following list of feature 
codes: ADM1, ADM1H, ADMD, ADMDH, PCL, 
PCLD, PCLF, PCLH, PCLI and PCLS. Evalua-
tion of the geographical sufficiency of a GenBank 
record was dependent upon whether the record in-
cluded a country name. A GenBank record with a 
country mention was called sufficient if the geo-
graphic information extracted from that record in-
cluded another place mention whose feature code 
fell within the class of sufficient feature codes and 
whose ISO country code matched that of the re-
trieved country. For instance, a GenBank record 
with the geographic metadata ?Orange County, 
United States? will be called sufficient since the 
place ?Orange County? has a sufficient feature 
code of ?ADM2? and a country code of ?US? 
which matches the country code of the retrieved 
country, ?United States?. Place mentions with 
matching country codes often had several differ-
ent feature codes in GeoNames. Such places were 
only called sufficient if all feature codes corre-
sponding to the given pair of place name and 
country code were classified as sufficient. In cases 
where the GenBank record had no country men-
tion, the record was called sufficient only if all 
matching GeoNames entries for any of the places 
mentioned in it had sufficient feature codes. The 
sufficiency criteria were designed to ensure that a 
geographic location is only called sufficient if its 
administrative level was found to be more specific 
than ADM1 without any form of ambiguity. Fig-
ure 3 illustrates the pathways of geographical suf-
ficiency for GenBank records in a diagram. 
Sufficiency Computation: In order to obtain 
the geographic information for each Genbank rec-
ord, we used a Java parser which automatically 
extracted data from the ?country? field of each 
record.  Since the ?country? field typically con-
tained multiple place mentions divided by a set of 
delimiters consisting of comma, colon and hy-
phen, we first split this field using these delimit-
ers.  We then checked each string obtained 
through this process against the ISO country code 
table to determine whether it was a potential coun-
try name for the record?s location.  If the query 
returned no results, then the locally stored 
GeoNames table was searched and for each match 
found, the corresponding ISO country code and 
feature code were extracted.  Figure 4 shows a di-
agram of this process. 
 
 
 
Figure 3. Sufficiency Calculation Example 
 
In cases where no sufficient location data was 
found from the ?country? field of a GenBank rec-
ord, the Java parser searched through its ?strain? 
field. This was done because some viral strains 
such as influenza include their location of origin 
integrated into their names. For example, the in-
fluenza strain ?A/duck/Alberta/35/76? indicates 
that the geographic origin of the strain is Alberta. 
The different sections of a strain field are sepa-
rated by either forward slash, parenthesis, comma, 
colon, hyphen or underscore and so we used a set 
of delimiters consisting of these characters to split 
this field. Each string thus retrieved was queried 
as before on the ISO country code table and the 
GeoNames table. GeoNames often returned 
matches for strings like ?raccoons? and ?chicken? 
which were actually meant to be names of host 
species within the ?strain? field, and so a list of 
4
 
 
Figure 4. Example of annotation including all four entities  
 
some of the most frequently seen host name men-
tions in these records was manually created and 
filtered out before querying GeoNames.  
Some of the place mentions contained very spe-
cific location information which resulted in 
GeoNames not finding a match for them. A list 
was created for strings like ?north?, ?south-east?, 
?governorate? etc. which when removed from a 
place mention may produce a match. In cases of 
potential place mentions which contained any one 
of these strings and for which GeoNames returned 
no matching result, a second query was performed 
after removal of the string. 
Evaluation of Sufficiency Computation: We 
manually annotated 10% of all influenza records 
in GenBank which reference at least one PubMed 
Central article as sufficient or insufficient based 
on our sufficiency criteria (5731 records). We 
then ran our program on these records and com-
pared system results with annotated results. 
2.3 Development/Evaluation of NER sys-
tems 
Creation of Gold Standard Corpus: We created 
a gold standard corpus consisting of twenty-seven 
manually-annotated full-text PubMed Central ar-
ticles in order to evaluate the performance of NER 
systems for tagging location, gene, species and 
date mentions in text. The articles corresponded to 
over 8,500 GenBank records and were randomly 
sampled using the subset of extracted GenBank 
records which contained a link to PubMed Central 
articles and had insufficient geographic metadata.  
Three annotators tagged the following four en-
tities in each article using the freely available an-
notation tool, BRAT (Stenetorp et al., 2012): gene 
names, locations, dates and species. Figure 4 pro-
vides an example of the manual annotation in 
BRAT. We annotated all mentions of each entity 
type, not only those relevant to zoonotic viruses, 
in order to evaluate system performance. A total 
of over 19,000 entities were annotated within this 
corpus. The number of tokens annotated was 
about 24,000. A set of annotation guidelines was 
created for this process (available upon request). 
Before creating the guidelines, each annotator in-
dividually annotated six common articles and 
compared and discussed their results to devise a 
reasonable set of rules for annotating each entity. 
After discussion, the annotators re-annotated the 
common articles based on the guidelines and di-
vided the remaining articles amongst themselves. 
The inter-annotator agreement was calculated for 
each pair of annotators. The annotated corpus will 
be made available at diego.asu.edu/downloads. 
Development of Automated Location Tag-
ger: We developed a dictionary-based NER sys-
tem using the GeoNames database for automated 
identification of location mentions in text. The 
dictionary used by this system, which we will 
hereby refer to as GeoNamer, was created by re-
trieving distinct place names from the GeoNames 
table and filtering out commonly used words from 
the retrieved set. Words filtered out include stop 
words such as ?are? and ?the?, generic place names 
such as ?cave? and ?hill?, numbers like ?one? and 
?two?, domain specific words such as ?biology? 
and ?DNA?, most commonly used surnames like 
?Garcia?, commonly used animal names such as 
?chicken? and ?fox? and other miscellaneous 
words such as ?central?. This was a crucial step 
since the GeoNames database contains a wide ar-
ray of commonly used English words which may 
cause a large volume of false positives if not re-
moved. The final dictionary consists of 5,396,503 
entries. In order to recognize place mentions in a 
5
given set of text files, GeoNamer first builds a Lu-
cene index on the contents of the files. It then con-
structs a phrase query for every entry in the 
Geonames dictionary and runs each query on the 
Lucene index. The document id, query text, start 
offset and end offset for every match found is 
written to an output file. We chose this approach 
because of its simplicity and efficiency.  
Evaluation of NER Systems: Four different 
NER systems for identifying species, gene, date 
and location mentions in text were evaluated us-
ing the created gold standard. The evaluated sys-
tems include LINNEAUS (Gerner et al., 2010), 
BANNER, Stanford SUTime (Chang and Man-
ning, 2012) and GeoNamer. LINNEAUS, BAN-
NER and Stanford SUTime are widely-used, 
state-of-the-art open source NER systems for 
recognition of species, gene and temporal expres-
sions respectively. GeoNamer is the system we 
developed in this work for the purpose of tagging 
locations, as described earlier.  
3 Results 
3.1 Sufficiency Analysis 
The system for classifying records as sufficient or 
insufficient was found to have an accuracy of 72% 
as compared to manual annotation.  98% of the 
errors was due to insufficient records being called 
sufficient. The results of the sufficiency analysis 
are given in Table 1. 64% of all GenBank records 
extracted for this project contained insufficient 
geographic information. Amongst the seven stud-
ied viruses, WEE had the highest and EEE had the 
lowest percentage of insufficient records.  
 
Virus 
Type  
Number of 
Entries  
% Insuffi-
cient  
WEE  67  90  
Rabies  4450  85  
WNV  1084  79  
SLE  141  74  
Hanta  1745  66  
Influenza  51734  62  
EEE  374  51  
All  59595  64  
 
Table 1. Percentage of GenBank records with in-
sufficient geographic information for each zoon-
otic virus studied in this project 
3.2 Gold Standard Corpus 
The results for the comparison of the annota-
tions performed by our three annotators on 6 com-
mon papers can be found in Table 2. We used the 
F-score between each pair of annotators as a 
measure of inter-rater agreement and had over 
90% agreement with overlap matching and over 
86% agreement with exact matching in all cases. 
The final gold standard corpus contained approx-
imately 19,000 entities corresponding to approxi-
mately 24,000 tokens. 
 
Entity F-score 
(A,B) 
(Exact; 
Overlap) 
F-score 
(?,?) 
(Exact; 
Overlap) 
F-score  
(?,?) 
(Exact; 
Overlap) 
Date .975; 
.978 
.979; 
.987 
.962; 
.973 
Gene .914; 
.926 
.913; 
.932 
.911; 
.954 
Location .945; 
.961 
.907; 
.931 
.914; 
.935 
Species .909; 
.956 
.874; 
.940 
.915; 
.959 
Virus .952; 
.958 
.947; 
.966 
.947; 
.955 
Mean .939; 
.956 
.924; 
.951 
.930; 
.955 
 
Table 2. Frequency of Annotated Entities for 6 
common annotated papers 
3.3 Performance Analysis of NER Systems 
The performance metrics for the NER systems 
at tagging the desired entities in the test set are 
listed in Table 3. The highest performance was 
achieved by Stanford SUTime for date tagging. 
Tagging of genes had the lowest performance. 
 
Entity  Precision  
(Exact; 
Overlap)  
Recall 
(Exact; 
Overlap)  
F-score  
(Exact; 
Overlap)  
BAN-
NER  
0.070; 
0.239  
0.114; 
0.395  
0.087; 
0.297  
Geo-
Namer 
0.452; 
0.626  
0.658; 
0.783  
0.536; 
0.696  
LIN-
NEAUS  
0.853; 
0.962  
0.563; 
0.658  
0.678; 
0.781  
Stanford 
SUTime 
0.800; 
0853  
0.681; 
0.727  
0.736; 
0.785  
 
Table 3. Performance Statistics of NER 
 
6
4 Discussion 
Based on our analysis, at least half of the Gen-
Bank records for each of the studied zoonotic vi-
ruses lack sufficient geographic information, and 
the proportion of insufficient records can be as 
high as 90%. Our automated system for classify-
ing records as insufficient or sufficient was found 
to have an accuracy of 72% with 98% of the errors 
being a result of insufficient records being called 
sufficient. Therefore, our computed estimate of 
insufficiency is very likely to be an underestima-
tion of the actual problem. The virus with the 
highest level of sufficiency, EEE, had a large 
number of records with county level information 
in the ?country? field. However, the insufficient 
records for this virus typically contained no place 
mention, not even at the country level. A key rea-
son for our calculated percentage of sufficient 
GenBank records being higher for these seven vi-
ruses than what has been previously computed by 
Scotch et al. (2011) was the inclusion of the 
?strain? field. The ?strain? field often contained 
specific location information which, when com-
bined with place mentions present within the 
?country? field, made the record geographically 
sufficient. The virus for which the inclusion of 
?strain? field had the greatest impact on boosting 
the sufficiency percentage was influenza. Most of 
the GenBank records associated with this virus 
had structured ?strain? fields from which the par-
ser could easily separate place mentions using 
GeoNames. 
Although the sufficiency classifications pro-
duced by our system were correct most of the 
time, there were a few cases where a record got 
incorrectly labeled as insufficient even when it 
contained detailed geographic information. This 
typically happened because GeoNames failed to 
return matching results for these places. For in-
stance, the country field ?India: Majiara,WB? was 
not found to be sufficient even though Majiara is 
a city in India because GeoNames has no entry for 
it. In some cases the lack of matching result was 
due to spelling variations of the place name. For 
instance the country field ?Indonesia: Yogjakarta? 
was called insufficient since ?Yogjakarta? is 
spelled as ?Yogyakarta? in GeoNames. Some-
times the database simply did not contain the ex-
act string present in the GenBank record. For in-
stance, it does not have any entry for the place 
?south Kalimantan? but it contains the place name 
?kalimantan?. The number of sufficient records 
which were called insufficient by our system due 
to inexact matching were greatly mitigated by re-
moving strings such as ?south? from the place 
mention, as described in the ?Methods? section. 
Most of the NER systems performed signifi-
cantly better with overlap measures than with ex-
act-match measures. This is because our annota-
tion guidelines typically involved tagging the 
longest possible match for each entity and the au-
tomated systems frequently missed portions of 
each annotation. Stanford SUTime had the best 
overlap f-measure of 0.785, closely followed by 
LINNEAUS with an overlap f-measure of 0.781. 
Although Stanford SUTime was fairly effective at 
finding date mentions in text, it tagged all four-
digit-numbers such as ?1012? and ?2339? as 
years, leading to a number of false positives. The 
poor recall of LINNEAUS was mostly caused be-
cause the dictionary used by LINNEAUS tagged 
only species mentions in text while we tagged ge-
nus and family mentions as well. It also missed a 
lot of commonly used animal names such as mon-
key, bat, badger and wolf. GeoNamer was the 
third best performer with the highest recall but 
second lowest precision. This is because the 
GeoNames dictionary contains an extensively 
large list of location names, many of which are 
commonly used words such as ?central?. Even 
though we filtered out a vast majority of these 
words, it still produced false positives such as 
?wizard?. However, its performance was consid-
erably better than that of the Stanford location tag-
ger used by Scotch et al. (2011) which was found 
to have a recall, precision and f-score of 0.26, 0.81 
and 0.39 respectively. The improved performance 
was achieved because of the higher recall of our 
system. The GeoNames dictionary provides an 
extensive coverage of all location mentions in the 
world and the Stanford NER system, which is a 
CRF classifier trained on a different dataset, was 
not able to recognize many of the place mentions 
present in full-text PMC articles related to Gen-
Bank records.  
BANNER showed the poorest performance 
amongst all the entity taggers evaluated in this pa-
per. In fact, the f-score we achieved for BANNER 
in this study was much lower that its past f-score 
of 0.42 within the domain of articles related to 
GenBank records for viral isolates (Scotch et al., 
2011).  As mentioned by Scotch et al. (2011), a 
key reason for BANNER?s poor performance in 
this domain is the difference between the data set 
used to train the BANNER model and the annota-
tion corpus used to test this system. The version 
of BANNER used in these two studies was trained 
on the training set for the BioCreative 2 Gene 
7
Mention task, which comprised of 15,000 sen-
tences from PubMed abstracts. These abstracts of-
ten contained the full names for gene and protein 
mentions while the full-text articles we used 
mostly contained the abbreviated forms of gene 
names, which BANNER tended to miss. The arti-
cles also contained abbreviated forms of several 
entities such as viral strain name (e.g. H1N1) and 
species name (e.g. VEEV) which look similar to 
abbreviated gene names. Therefore, BANNER of-
ten misclassified these entities as gene mentions. 
A possible reason for BANNER having a much 
lower performance in this study than in the previ-
ous study conducted by Scotch et al (2011) is the 
presence of a large number of tables in the journal 
articles we selected. BANNER is a machine learn-
ing system based on conditional random fields 
which uses orthographic, morphological and shal-
low syntax features extracted from sentences to 
identify gene mentions in text. Such features do 
not help greatly for extraction from tables. There-
fore, BANNER was often not able to identify the 
gene mentions in the tables present within our cor-
pus, thereby producing false negatives. Moreover, 
it tagged several entries within the table as a single 
gene name, thereby producing false positives as 
well. This reduced both the recall and precision of 
BANNER. 
Although this study explores the problem of in-
sufficient geographic information in GenBank 
more thoroughly than past studies, the number of 
papers annotated as the gold standard is still lim-
ited. Thus, the performance of the taggers re-
ported can be construed as a preliminary estimate 
at best. The set of taggers and their performance 
seem to be adequate for a large-scale application, 
with the exception of BANNER. However, we did 
not make any changes to the BANNER system 
(specifically, re-training) since changes to it are 
not possible until sufficient data is annotated for 
retraining. 
5 Conclusions and Future Work 
It can be concluded that the majority of Gen-
Bank records for zoonotic viruses do not contain 
sufficient geographic information concerning 
their origin. In order to enable phylogeographic 
analysis of these viruses and thereby monitor their 
spread, it is essential to develop an efficient mech-
anism for extracting this information from pub-
lished articles. Automated NER systems may help 
accelerate this process significantly. Our results 
indicate that the NER systems LINNEAUS, Stan-
ford SUTime and GeoNamer produce satisfactory 
performance in this domain and thus can be used 
in the future for linking GenBank records with 
their corresponding geographic information. 
However, the current version of BANNER is not 
well-suited for this task. We will need to train 
BANNER specifically for this purpose before in-
corporating it within our system. 
We are currently altering the component of our 
program which classifies records as sufficient or 
insufficient in order to reduce the number of errors 
due to insufficient records being called sufficient. 
We are also manually looking through GenBank 
records for zoonotic viruses with insufficient geo-
graphic metadata and linking them to the location 
mentions in related articles which we deem to be 
the most likely location of collection for the given 
viral isolate. The resulting annotated corpus will 
be used to train and evaluate an automated system 
for populating GenBank geographic metadata. 
We have already covered all GenBank records re-
lated to Encephalitis viruses and close to 10% of 
all records related to Influenza which are linked to 
PubMed Central articles. The annotation process 
has revealed that a large proportion of the infor-
mation allowing linkage of GenBank records to 
geographic metadata is often present in tables 
within the articles in addition to textual sentences. 
Therefore, we have developed a Python parser for 
automatically linking GenBank records to loca-
tion mentions using tables from the HTML ver-
sion of the PubMed Central articles.  Future work 
will include further expansion of this annotation 
corpus and the development of an integrated sys-
tem for enhancing GenBank geographic metadata 
for phylogeographic analysis of zoonotic viruses.  
Acknowledgement 
Research reported in this publication was sup-
ported by the NIAID of the NIH under Award 
Number R56AI102559 to MS and GG. The con-
tent is solely the responsibility of the authors and 
does not necessarily represent the official views 
of the National Institutes of Health 
 
 
 
 
 
 
 
 
8
 References 
Avise, John C. (2000). Phylogeography : the history 
and formation of species Cambridge, Mass.: Harvard 
University Press. 
Chang, Angel X., and Christopher Manning. "SUTime: 
A library for recognizing and normalizing time ex-
pressions." LREC. 2012. 
Ciccozzi M, et al. Epidemiological history and phylo-
geography of West Nile virus lineage 2. Infection, 
Genetics and Evolution. 2013:17;46-50. 
Finkel JR, Grenager T, Manning C. Incorporating non-
local information into information extraction sys-
tems by Gibbs sampling. In: Proceedings of the 43rd 
annual meeting of the association for computational 
linguistics (ACL 2005); 2005. p. 363?70. 
Gerner M, Nenadic G, and Bergman CM. LINNAEUS: 
A species name identification system for biomedical 
literature. BMC Bioinformatics. 2010;11(85). 
Gray RR, and Salemi M. Integrative molecular phylo-
geography in the context of infectious diseases on 
the human-animal interface. Parasitology-Cam-
bridge. 2012;139:1939-1951 
Krauss, H. (2003). Zoonoses: infectious diseases trans-
missible from animals to humans (3rd ed.). Wash-
ington, D.C.: ASM Press. 
Leaman R and Gonzalez G. BANNER: An executable 
survey of advances in biomedical named entity 
recognition. Pacific Symposium on Biocomputing. 
2008;13:652-663. 
Scotch, Matthew, et al. Enhancing phylogeography by 
improving geographical information from GenBank. 
Journal of biomedical informatics. 2011;44:S44-
S47. 
Stenetorp P, et al. BRAT: A Web-based Tool for NLP-
Assisted Text Annotation. EACL '12 Proceedings 
Wallace, R.G. and W.M. Fitch, Influenza A H5N1 im-
migration is filtered out at some international bor-
ders. PLoS One, 2008. 3(2): p. e1697. 
Weidmann M, et al. Molecular phylogeography of tick-
borne encephalitis virus in Central Europe. Journal 
of General Virology. 2013;94:2129-2139. 
9
