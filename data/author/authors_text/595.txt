Improving Text Segmentation Using Latent
Semantic Analysis: A Reanalysis of Choi,
Wiemer-Hastings, and Moore (2001)
Yves Bestgen?
FNRS - Universite? Catholique
de Louvain
Choi, Wiemer-Hastings, and Moore (2001) proposed to use Latent Semantic Analysis (LSA) to
extract semantic knowledge from corpora in order to improve the accuracy of a text segmentation
algorithm. By comparing the accuracy of the very same algorithm, depending on whether or not
it takes into account complementary semantic knowledge, they were able to show the benefit
derived from such knowledge. In their experiments, semantic knowledge was, however, acquired
from a corpus containing the texts to be segmented in the test phase. If this hyper-specificity
of the LSA corpus explains the largest part of the benefit, one may wonder if it is possible to
use LSA to acquire generic semantic knowledge that can be used to segment new texts. The two
experiments reported here show that the presence of the test materials in the LSA corpus has an
important effect, but also that the generic semantic knowledge derived from large corpora clearly
improves the segmentation accuracy.
1. Improving Text Segmentation by Using Complementary Semantic Knowledge
For the last ten years, many methods have been proposed for the segmentation of texts
in topically related units on the basis of lexical cohesion. The major distinction between
these methods is in the contrast between the approaches based exclusively on the
information contained in the text to be segmented, such as lexical repetition (e.g., Choi
2000; Hearst 1997; Heinonen 1998; Kehagias, Pavlina, and Petridis 2003; Utiyama and
Isahara 2001), and those approaches that rest on complementary semantic knowledge
extracted from dictionaries and thesauruses (e.g., Kozima 1993; Lin et al 2004; Morris
and Hirst 1991), or from collocations collected in large corpora (Bolshakov and Gelbukh
2001; Brants, Chen, and Tsochantaridis 2002; Choi et al 2001; Ferret 2002; Kaufmann
1999; Ponte and Croft 1997). According to their authors, methods that use additional
knowledge allow for a solution to problems encountered when sentences belonging to
a unique topic do not share common words due to the use of hyperonyms or synonyms
and allow words that are semantically related to be taken as positive evidence for
topic continuity. Empirical arguments in favor of these methods have been provided
recently by Choi et al (2001) in a study using Latent Semantic Analysis (Latent Semantic
Indexing, Deerwester et al 1990) to extract a semantic space from a corpus allowing
determination of the similarity of meanings of words, sentences, or paragraphs. By
? Center for Text and Discourse Studies, PSOR, Place du Cardinal Mercier 10, B-1348 Louvain-la-Neuve
Belgium
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 1
comparing the accuracy of the very same algorithm according to whether or not it takes
into account complementary semantic knowledge, they were able to show the benefit
derived from such knowledge.
However, implications of Choi et al?s study for text segmentation and for the use
of LSA in natural language processing are unclear due to the methodology employed.
In their experiments, semantic knowledge was acquired from a corpus containing the
materials to be segmented in the test phase. One could speculate whether the largest
part of the benefit obtained thanks to the addition of semantic knowledge was not due
to this hyper-specificity of the LSA corpus (i.e., the inclusion of the test materials). If
this were the case, it would call into question the possibility of using LSA to acquire
generic semantic knowledge that can be used to segment new texts. A priori, the prob-
lem does not seem serious for at least two reasons. First, Choi et al?s segmentation
procedure does not rely on supervised learning in which a system learns how to
efficiently segment a text from training data. The LSA corpus only intervenes in an
indirect manner by allowing the extraction of semantic proximities between words that
are then used to compute similarities between parts of the text to segment (see Section 2
for details). Second, Choi et al employed a large number of small test samples to
evaluate their algorithm, each making up?on average?0.15% of the LSA corpus. The
present study shows, however, that the presence of the test materials in the LSA corpus
has an important effect, but also that the generic semantic knowledge derived from
large corpora clearly improves the segmentation accuracy. This conclusion is drawn
from two experiments in which the presence or absence of the test materials in the
LSA corpus is manipulated. The first experiment is based on the original materials
from Choi et al, which consisted of a small corpus (1,000,000 words). The second
experiment is based on a much larger corpus (25,000,000 words). Before reporting
these experiments, Choi?s algorithm and the use of LSA within this framework are
described.
2. The Two Versions of Choi?s Algorithm
The segmentation algorithm proposed by Choi (2000) is made up of the three steps
usually found in any segmentation procedure based on lexical cohesion. Firstly, the
document to be segmented is divided into minimal textual units, usually sentences.
Then, a similarity index between every pair of adjacent units is calculated. Each raw
similarity value is cast on an ordinal scale by taking the proportion of neighboring
values that are smaller than it. Lastly, the document is segmented recursively according
to the boundaries between the units that maximize the sum of the average similarities
inside the segments thus comprised (divisive clustering).
The step of greatest interest here is the one that calculates the inter-sentence sim-
ilarities. The procedure initially proposed by Choi (2000), C99, rests exclusively on
the information contained in the text to be segmented. According to the vector space
model, each sentence is represented by a vector of word frequency count, and the
similarity between two sentences is calculated by means of the cosine measure between
the corresponding vectors. In a first evaluation based on the procedure described below,
Choi showed that its algorithm outperforms several other approaches such as TextTiling
(Hearst 1997) and Segmenter (Kan, Klavans, and McKeown 1998).
Choi et al (2001) claimed that it was possible to improve the inter-sentence similar-
ities index by taking into account the semantic proximities between words estimated on
the basis of Latent Semantic Analysis (LSA). Briefly stated, LSA rests on the thesis that
6
Bestgen Improving Text Segmentation
analyzing the contexts in which words occur permits an estimation of their similarity in
meaning (Deerwester et al 1990; Landauer and Dumais 1997). The first step in the analy-
sis is to construct a lexical table containing an information-theoretic weighting of the
frequencies of the words occurrence in each document (i.e. sentence, paragraph, or text)
included in the corpus. This frequency table undergoes a Singular Value Decomposition
that extracts the most important orthogonal dimensions, and, consequently, discards the
small sources of variability in term usage. After this step, every word is represented by
a vector of weights indicating its strength of association with each of the dimensions.
This makes it possible to measure the semantic proximity between any two words by
using, for instance, the cosine measure between the corresponding vectors. Proximity
between any two sentences (or any other textual units), even if these sentences were
not present in the original corpus, can be estimated by computing a vector for each
of these units?which corresponds to the weighted sum of the vectors of the words
that compose it?and then by computing the cosine between these vectors (Deerwester
et al 1990). Choi et al (2001) have shown that using this procedure to compute the
inter-sentence similarities results in the previous version of the algorithm (based solely
on word repetition) being outperformed.
3. Experiment 1
The aim of this experiment is to determine the impact of the presence of the test
materials in the LSA corpus on the results obtained by Choi et al (2001). Does semantic
knowledge acquired from a corpus that does not include the test materials also improve
the segmentation accuracy?
3.1 Method
This experiment was based on the procedure and test materials designed by Choi (2000),
which was also used by several authors as a benchmark for comparing segmentation
systems (Brants et al 2002; Ferret 2002; Kehagias et al 2003; Utiyama and Isahara
2001). The task consists in finding the boundaries between concatenated texts. Each
test sample is a concatenation of ten text segments. Each segment consisted in the first
n sentences of a randomly selected text from two sub-sections of the Brown corpus.
For the present experiment, I used the most general test materials built by Choi (2000),
in which the size of the segments within each sample varies randomly from 3 to 11
sentences. It is composed of 400 samples.
The analysis related to the comparison between the accuracy of the algorithm when
the test materials were included in the LSA corpus (Within) and when it was not
(Without). One Within semantic space, which corresponds to the one used by Choi et al,
was built using the entire Brown corpus as the LSA corpus. Four hundred different
Without spaces were built, one for each test sample, by each time removing from the
Brown corpus only the sentences that make this sample.
To extract the LSA space and to apply the segmentation algorithm, a series of
parameters had to be set. First of all, paragraphs were used as documents for building
the lexical tables because Choi et al observed that such middle-sized units were more
effective than shorter units (i.e., sentences). The words on Choi?s stoplist were removed,
as were those that appeared only once in the whole corpus. Words were not stemmed,
as in Choi et al (2001). To build the LSA space, the singular value decomposition was
realized using the program SVDPACKC (Berry 1992; Berry et al 1993), and the first
7
Computational Linguistics Volume 32, Number 1
Table 1
Error rates and variance (in parentheses) for the Within and the Without conditions.
Pk WindowDiff
Within 0.084 (0.005) 0.090 (0.005)
Without 0.120 (0.006) 0.126 (0.006)
300 singular vectors were retained. Concerning the segmentation algorithm, I used the
version in which the number of boundaries to be found is imposed, and thus fixed at
nine. An 11 ? 11 rank mask was used for the ordinal transformation, as recommended
by Choi (2000).
3.2 Results
The segmentation accuracy was evaluated by means of the index reported by Choi et al
(2001): the Pk measure of segmentation inaccuracy (Beeferman, Berger, and Lafferty
1999), which gives the proportion of sentences that are wrongly predicted to belong to
the same segment or wrongly predicted to belong to different segments. I also report,
for potential future comparison, Pevzner and Hearst?s (2002) WindowDiff index, which
remedies several problems in the Pk measure.
Results are provided in Table 1.1 Compared with the Within condition, the perform-
ance in the Without condition is definitely worse, as confirmed by t tests for paired
sample (each test sample being used as an observation) that are significant for an alpha
smaller than 0.0001. The C99 algorithm, which does not employ LSA to estimate the
similarities between the sentences, produces a Pk of 0.13 (Choi et al 2001, Table 3, line 3:
No stemming). It appears that although the Without condition is still better than C99, the
benefit is very small.
Before concluding that the presence of the test materials in the LSA corpus strongly
modified the semantic space, an alternative explanation must be considered. The loss
of accuracy in the Without condition could potentially be due to the fact that the words
indexed in the corresponding LSA spaces are systematically slightly fewer than those
present in the Within space. Removing each test sample led to the loss?on average?
of 23 different words out of the 25,847 words that are indexed in the Within space. In
the Without spaces, these words are no longer available to estimate the similarity of
the sentences, whereas they are employed in the Within space. In order to determine
whether this factor can explain the difference in performance, a complementary analysis
was carried out on the Within space in which, for each test sample, only the words
present in the corresponding Without space were taken into account. In this manner,
only the semantic relations can come into play. Compared with the complete Within
space, almost no drop in performance was observed: the Pk error rate went from 0.084
to 0.085 in the new analysis. This result indicates that it is not the words selected for
the calculation of the proximities that matter, but the semantic relations in the spaces
extracted from the word co-occurrences by the Singular Value Decomposition.
1 The error rate is in fact slightly better than that reported by Choi et al (2001). The difference could be
due to several factors, such as the pre-processing of the Brown corpus (e.g., tokenization and paragraph
identification) or the scaling function applied to the raw frequencies, which was here the standard
information-theoretic weighting described in Landauer, Foltz, and Laham (1998).
8
Bestgen Improving Text Segmentation
4. Experiment 2
Experiment 1 was conducted on the Choi et al (2001) LSA corpus, a 1,000,000-word
collection of texts from very different genres and with varied themes. The smallness of
the corpus and diversity of the texts could have affected the results at two levels. First,
removing a few sentences of a text should have less impact if the corpus contains a
lot of texts on similar topics. Second, a larger corpus would probably also permit the
extraction of a more stable and efficient semantic space. This could produce a greater
difference between the LSA version of the algorithm and the version that does not
use additional semantic knowledge (C99). For these reasons, a second experiment was
conducted on the basis of a much larger corpus consisting of the articles published
during 1997 and 1998 in the Belgian French-speaking newspaper Le Soir (roughly 52,000
articles and 26,000,000 words). In this corpus, the test materials from each sample
account for?on average?0.0066% of the complete corpus. This second experiment
also made it possible to compare the Within and Without spaces with a Former space
composed of articles published in the same newspaper, but during the years 1995 and
1996 (roughly 50,000 articles and more than 22,000,000 words). This condition will show
the possibility of using LSA to build even more generic semantic knowledge, since the
LSA corpus is earlier than the text to segment.
4.1 Method
The test materials were extracted from the 1997?1998 corpus following the guidelines
given in Choi (2000). It is composed of 400 samples of ten segments, of which the length
varies randomly from 3 to 11 sentences. Three types of LSA space were composed. The
Within space is based on the whole 1997?1998 corpus. Four hundred different Without
spaces were built as described in Experiment 1. Finally, a Former space was built from the
1995?1996 corpus. The parameters employed to build the semantic spaces are identical
to those used in Experiment 1 with one exception: in order to reduce the size of the
lexical tables the whole articles and not the paragraphs were used as documents.
4.2 Results
Although the results are mostly similar to those obtained in Experiment 1, Table 2 shows
some interesting differences. The discrepancy between the Within and Without condition
is much smaller, even if it remains statistically significant (p < 0.0001). Using a corpus
from the same source, but with earlier years, still returns a poorer performance (p <
0.0001). The C99 algorithm, which is not based on LSA, produces a Pk error rate of
0.150, a value definitely worse than those obtained with the Without and Former spaces.
This confirms the usefulness of semantic knowledge acquired from large corpora in
estimating inter-sentence similarities.
5. Conclusion
The two experiments showed that the presence of the test materials in the LSA corpus
increases the algorithm accuracy even when a corpus of more than 25,000,000 words is
used. They also showed that the use of independent semantic knowledge improves the
segmentation accuracy and that this can be observed even when the semantic knowl-
edge is extracted from former years of the same source. This observation underlines
9
Computational Linguistics Volume 32, Number 1
Table 2
Error rates and variance (in parentheses) for the Within, Without and Former conditions.
Pk WindowDiff
Within 0.069 (0.004) 0.073 (0.004)
Without 0.080 (0.004) 0.085 (0.005)
Former 0.097 (0.005) 0.101 (0.005)
the possibility of building relatively generic semantic knowledge; that is, knowledge
which could be employed to process new linguistic data, as has been recently proposed
in a anaphora resolution algorithm, in a continuous speech recognition system, or
in machine translation (Bellegarda 2000; Klebanov and Wiemer-Hastings 2002; Kim,
Chang, and Zhang 2003). A question the present study does not answer concerns
the possibility of employing a corpus drawn from another source, such as another
newspaper. Bellegarda (2000) observed in speech recognition tasks that such a semantic
space is definitely less effective. It is nevertheless possible that evaluating the semantic
proximity between two sentences is less affected by the style of composition of the
source than predicting the next word of a statement.
Recently, several authors have proposed segmentation algorithms, based mainly
on dynamic programming, that equal or even outperform Choi?s results (Ji and Zha
2003, Kehagias et al 2003; Utiyama and Isahara 2001). These algorithms do not rest
on additional semantic knowledge. According to the results of the present study, they
could still be improved by taking into account such knowledge.
Finally, this study allows a more general conclusion about the use of LSA for nat-
ural language processing. If one?s objective is to analyze a linguistic phenomenon in a
large corpus such as for instance the factors determining the use of causal connectives
(Degand, Spooren, and Bestgen 2004), it is preferable to extract the semantic space from
the corpus at hand. The two experiments did indeed show that such specific corpora
allow the extraction of a more efficient semantic space. However, if the objective is to
test the effectiveness of an algorithm intended to process new linguistic data on the basis
of a semantic space built beforehand, one must avoid including the material to analyze
in the LSA corpus since that would produce an over-estimate of the effectiveness of the
procedure.
Acknowledgments
Yves Bestgen is research fellow of the Belgian
National Fund for Scientific Research
(FNRS). This work was supported by grant
FRFC 2.4535.02 and by a grant (Action de
Recherche concerte) of the government of the
French-language community of Belgium.
A previous version was presented at TALN
2005 (Traitement Automatique des
Langues Naturelles, Dourdan, France).
Thanks to the anonymous reviewers for
their valuable comments.
References
Beeferman, Doug, Adam Berger, and John
Lafferty. 1999. Statistical models for text
segmentation. Machine Learning,
34(1?3):177?210.
Bellegarda, Jerome R. 2000. Large vocabulary
speech recognition with multispan
statistical language models. IEEE
Transactions on Speech and Audio Processing,
8(1):78?84.
Berry, Michael W. 1992. Large scale singular
value computations. International
Journal of Supercomputer Applications,
6(1):13?49.
Berry, Michael W., Theresa Do, Gavin
O?Brien, Vijay Krishna, and Sowmini
Varadhan. 1993. SVDPACKC: version 1.0
user?s guide. Tech. Rep. CS-93-194,
University of Tennessee, Knoxville, TN,
October 1993.
10
Bestgen Improving Text Segmentation
Bolshakov, Igor A. and Alexander Gelbukh.
2001. Text segmentation into paragraphs
based on local text cohesion. In Proceedings
of Text, Speech and Dialogue (TSD-2001).
Springer-Verlag, Berlin, pages 158?166.
Brants, Thorsten, Francine Chen, and
Ioannis Tsochantaridis. 2002. Topic-
based document segmentation with
probabilistic latent semantic analysis.
In Proceedings of CIKM?02, McLean, VA,
pages 211?218.
Choi, Freddy Y. Y. 2000. Advances in domain
independent linear text segmentation. In
Proceedings of NAACL-00, Seattle, WA,
pages 26?33.
Choi, Freddy Y. Y., Peter Wiemer-Hastings,
and Johanna Moore. 2001. Latent semantic
analysis for text segmentation. In
Proceedings of NAACL?01, Pittsburgh, PA,
pages 109?117.
Deerwester, Scott, Susan T. Dumais, George
W. Furnas, Thomas K. Landauer, and
Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the
American Society for Information Science,
41(6):391?407.
Degand, Liesbeth, Wilbert Spooren, and
Yves Bestgen. 2004. On the use of
automatic tools for large scale semantic
analyses of causal connectives. In
Proceedings of ACL 2004 Workshop
on Discourse Annotation, Barcelona,
Spain, pages 25?32.
Ferret, Olivier. 2002. Using collocations
for topic segmentation and link detection.
In Proceedings of COLING 2002, Taipei,
Taiwan, pages 260?266.
Hearst, Marti. 1997. TextTiling: Segmenting
text into multi-paragraph subtopic
passages. Computational Linguistics,
23(1):33?64.
Heinonen, Oskari. 1998. Optimal
multi-paragraph text segmentation by
dynamic programming. In Proceedings
of 17th International Conference on
Computational Linguistics (COLING-
ACL?98), Montreal, Canada,
pages 1484?1486.
Ji, Xiang and Hongyuan Zha. 2003.
Domain-independent text segmentation
using anisotropic diffusion and dynamic
programming. In Proceedings of the
26th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, Toronto, Canada,
pages 322?329.
Kan, Min-Yen, Judith L. Klavans, and
Kathleen R. McKeown. 1998. Linear
segmentation and segment significance.
In Proceedings of the 6th International
Workshop of Very Large Corpora, Montreal,
Canada, pages 197?205.
Kaufmann, Stefan. 1999. Cohesion and
collocation: Using context vectors in text
segmentation. In Proceedings of ACL?99,
College Park, MD, pages 591?595.
Kehagias, Athanasios, Fragkou Pavlina,
and Vassilios Petridis. 2003. Linear
text segmentation using a dynamic
programming algorithm. In Proceedings
of the 10th Conference of the European
Chapter of the Association for Computational
Linguistics, Budapest, Hungary,
pages 171?178.
Kim, Yu-Seop, Jeong-Ho Chang, and
Byoung-Tak Zhang. 2003. An empirical
study on dimensionality optimization in
text mining for linguistic knowledge
acquisition. In Lecture Notes in Computer
Science, vol. 2637. Springer-Verlag, Berlin,
pages 111?116.
Klebanov, Beata and Peter M.
Wiemer-Hastings. 2002. Using LSA
for pronominal anaphora resolution.
In Proceedings of the Third International
Conference on Computational Linguistics
and Intelligent Text Processing,
Lecture Notes in Computer Science,
vol. 2276, Springer-Verlag, Berlin,
pages 197?199.
Kozima, Hideki. 1993. Text segmentation
based on similarity between words. In
Proceedings of the 31st Annual Meeting of the
Association for Computational Linguistics,
Columbus, OH, pages 286?288.
Landauer, Thomas K. and Susan T. Dumais.
1997. A solution to Plato?s problem: The
latent semantic analysis theory of
acquisition, induction and representation
of knowledge. Psychological Review,
104(2):211?240.
Landauer, Thomas K., Peter W. Foltz, and
Darrell Laham. 1998. An Introduction to
latent semantic analysis. Discourse
Processes, 25(2?3):259?284.
Lin, Ming, Jay F. Nunamaker, Michael Chau,
and Hsinchun Chen. 2004. Segmentation
of lecture videos based on text: A
method combining multiple linguistic
features. In Proceedings of the 37th Hawaii
International Conference on System Sciences
(HICSS-37), Big Island, HI, Track 1,
Volume 1, pages 10003.
Morris, Jane and Graeme Hirst. 1991. Lexical
cohesion computed by thesaural
relations as an indicator of the structure
of text. Computational Linguistics,
17(1):21?48.
11
Computational Linguistics Volume 32, Number 1
Pevzner, Lev and Marti Hearst. 2002. A
critique and improvement of an evaluation
metric for text segmentation. Computational
Linguistics, 28(1):19?36.
Ponte, Jay M. and W. Bruce Croft. 1997. Text
segmentation by topic. In Proceedings of the
1st European Conference on Research and
Advanced Technology for Digital Libraries,
Lecture Notes in Computer Science,
vol. 1324, Springer-Verlag, Berlin,
pages 120?129.
Utiyama, Masao and Hitoshi Isahara.
2001. A statistical model for domain-
independent text segmentation. In
Proceedings of ACL?2001, Toulouse,
France, pages 491?498.
12
On the use of automatic tools for large scale semantic analyses of causal 
connectives 
Liesbeth Degand 
Universit? catholique de Lou-
vain 
Institute of Linguistics 
degand@lige.ucl.ac.be 
 
Wilbert Spooren 
Vrije Universiteit, Amsterdam 
Language & Communication 
w.spooren@let.vu.nl 
Yves Bestgen 
Universit? catholique de Lou-
vain 
Faculty of Psychology 
Yves.bestgen@psp.ucl.
ac.be 
 
 
Abstract 
In this paper we describe the (annotation) tools underly-
ing two automatic techniques to analyse the meaning 
and use of backward causal connectives in large Dutch 
newspaper corpora. With the help of these techniques, 
Latent Semantic Analysis and Thematic Text Analysis, 
the contexts of more than 14,000 connectives were stud-
ied. We will focus here on the methods of analysis and 
on the fairly straightforward (annotation) tools needed 
to perform the semantic analyses, i.e. POS-tagging, lem-
matisation and a thesaurus-like thematic dictionary. 
1 Introduction 
In ongoing work, we explore the possibility to 
make use of large corpora to test hypotheses con-
cerning linguistic factors determining the meaning 
and use of connectives. Of course, corpus-based 
approaches of connectives are not new, but classi-
cally they consist of either fully analysed but rela-
tively small corpora, or of large corpora of which a 
random set is analysed. The reason for this quanti-
tative restriction is clear: The data-analyses are 
completely hand-based.  While these empirical 
studies are useful from a qualitative point of view, 
they all suffer from the same quantitative draw-
back, namely the relatively small number of data 
(rarely more than 100 occurrences are analysed, 
mostly only 50).  In addition, most of these analy-
ses are still too analyst-dependent, making gener-
alizations and replications difficult. Changing this 
situation includes handling exhaustively large cor-
pora (with hundreds and even thousands of occur-
rences of the same linguistic phenomenon) and 
implementing the analytic procedures to make 
them analyst-independent. In this paper, we test 
such a methodology for which we used a number 
of linguistic hypotheses found in the literature on 
the semantics of causal connectives and tried to 
replicate the results. The linguistic material we 
worked on are four Dutch backward causal con-
nectives: aangezien ('since'), doordat ('because of 
the fact that'), omdat ('because') and, want ('be-
cause'). This choice was motivated by the fact that 
there has already been quite some linguistic work 
on this topic, mainly empirically based (Degand, 
2001; Degand and Pander Maat, 2003; Pit, 2003).1 
We have shown elsewhere how linguistic hypothe-
ses concerning the scaling of these connectives in 
terms of subjectivity and their thematic behaviour 
could be supported (Bestgen et al, 2003). Since 
these first results are very encouraging, we would 
like to focus here on the methods of automatic 
analysis ? Latent Semantic Analysis and Thematic 
Text Analysis - and on the fairly straightforward 
(annotation) tools needed to perform the semantic 
analyses, i.e. POS-tagging, lemmatisation and a 
thesaurus-like thematic dictionary. We illustrate 
how the combination of the two techniques of 
automatic analysis permit to gain deeper insight 
into the semantic constraints on the use of the con-
nectives studied. Doing so, we test a number of 
new hypotheses concerning the perspectivizing and 
polyphonic nature of connectives that remain un-
confirmed in the linguistic literature. We also dis-
cuss the robustness of the techniques and their re-
usability in other contexts and other languages.  
                                                           
1
 For lack of space we will not present the linguistic analyses here but will 
consider them as given. 
2 Techniques and Tools 
The techniques used have to fulfil two tasks: they 
are needed to extract the relevant linguistic mate-
rial from the corpus, that is to say the four connec-
tives with their context of use; and they are used to 
analyse the retrieved elements in order to test a 
number of linguistic hypotheses concerning the 
meaning and use of these connectives.  Our main 
objective is to show that with the use of these tech-
niques only fairly straightforward annotation tools 
are needed to perform quite profound semantic 
analyses on massive quantitative data. 
2.1 POS-Tagging and the identification of 
the causal segments 
The extraction of the relevant linguistic material 
was fulfilled by automatic syntactic analysis tech-
niques. As a basis for our analyses we worked with 
the first six months of a Dutch newspaper corpus 
of more than 30 million words2. This material was 
POS-tagged using MBT (Memory Based Tagger) 
(Daelemans et al,1996). We then discarded the 
items with few content words: sports results, tele-
vision programs, crosswords and puzzles, stock 
exchange reports, service information from the 
newspaper editor, etc. We also ?cleaned? the cor-
pus material of irregularities caused by the incom-
patibility between the source file and the tagging 
program (mostly nonsense words generated by the 
program). This eventually led to a data set of ap-
proximately 16,500,000 words.  
The POS-tagging permitted to segment the cor-
pus in sentences and to label the words grammati-
cally.  Second, POS-tagging allowed us to locate 
and extract the connectives from the sentences in 
which they occurred. Concretely, we extracted all 
sentence-length segments on the basis of the tag 
<UT> (?utterance?). We then did a search on the 
four connectives tagged as <conj> by the parser.   
Table 1 displays the frequencies of the retrieved 
connectives. These figures do not include a number 
of sentences that were eliminated because they 
were potentially problematic for the analysis. This 
was for instance the case for sentences containing 
more than one connective out of our list of four.3 
 
                                                           
2
 We used the year 1997 of "De Volkskrant" a Dutch national daily newspaper. 
The corpus is distributed on CD-rom. 
2
 These cases were eliminated in order to be sure of the exact influence of the 
connective and about the exact contribution of  the context. 
Connective Raw frequency Relative frequency  
(per million words) 
aangezien 248 30 
Doordat 826 101 
Omdat 7689 938 
Want 5621 686 
Table 1: Frequencies of the causal connectives in the 
data set 
The extracted sentences were then analysed in 
terms of a series of heuristics to identify the CAUSE 
(P) and CONSEQUENCE segments (Q)4.  From a 
syntactic point of view, the connectives doordat, 
omdat and aangezien can occur in two basic types 
of causal constructions: medial (Q CONNECTIVE P), 
see example (1), and preposed ones (CONNECTIVE 
P, Q), see example (2).  The connective want only 
appears in medial constructions.   
(1) Een gezamenlijk beleid is 
nodig omdat in het najaar 
in het Japanse Kyoto 
wereldwijd wordt onderhan-
deld over het klimaat. 
?A common policy is necessary 
because worldwide negotia-
tions will take place in the 
autumn in the Japanese city 
of Kyoto.? 
(2) Iedere strenge winter 
heeft gevolgen voor de 
kerkorgels', zegt dr. A.J. 
Gierveld van de Gerefor-
meerde Organisten-
vereniging. Doordat het 
hout krimpt, kunnen er 
kieren ontstaan waardoor 
lucht ontsnapt. 
?Every hard winter has conse-
quences for the church or-
gans?, Dr. A.J. Gierveld of 
the Reformed Organists Union 
says. Because the wood 
shrinks, crocks may show, 
through which air escapes.? 
 
The heuristics to identify the CAUSE (P) and 
CONSEQUENCE (Q) segments were primarily based 
on  
                                                           
4
 The connectives under investigation are all so-called backward causal connec-
tives, i.e. they express an underlying causal relation of the type CONSEQUENCE ? 
CAUSE, in which the connective introduces the CAUSE segment.   
a) the position of the connective in the 
sentence (number and type of words 
preceding the connective),  
b) the number, position and order of finite 
verbs in the segment,  
c) the presence or absence of punctuation 
markers, especially commas.  
For example, a sentence beginning with the con-
nective omdat can either be preposed (P-Q) (ex-
ample 3), or medial (Q-P), if Q and P are given in 
different sentences (example 4).  
(3) Omdat de verdachte niet 
eerder was veroordeeld, 
bleef de gevangenisstraf 
geheel voorwaardelijk. 
?Because the suspect had not 
been convicted before, the 
sentence was entirely proba-
tional.? 
(4) Maar er zijn meer pro-
gramma's die de moeite 
waard zijn en die toch 
niet worden bekeken. Omdat 
ze onvindbaar zijn tussen 
de ramsj. 
?But there are more [TV] pro-
grammes that are worth watch-
ing and still are not being 
watched. Because they are 
hard to trace among the rub-
bish.? 
 
To extract these segments correctly, a number of 
rules enter into play. For example,  
 
a) If CONN = omdat, doordat or aangezien; and 
b) If CONN in initial position, look for first fi-
nite verb [vf], if vf appears in segment 
<?vf, vf ?> or <? vf vf ?>,  then cut be-
fore second vf, and segment containing 
CONN is P, the other one is Q. 
c) If CONN in initial position and there is only 
one vf, then segment containing CONN is P, 
and previous sentence is Q. 
Other rules are used to determine whether the 
CONN is in initial position or not. In addition to 
examples (2-3), example (5) also illustrates a case 
of initial connective, even though a word precedes 
the connective.  
(5) En omdat in Nederland de 
voertaal nog steeds het 
Nederlands is, worden de 
meeste schoolvakken ook in 
die taal gedoceerd. 
?And because Dutch is still 
the main language in the 
Netherlands, most subjects 
are taught in that language.? 
 
This resulted in 21 heuristic rules, the adequacy of 
which was hand-checked on large samples of the 
data. In the end, 1.4% of the data were lost because 
one of the segments was missing or because none 
of the procedures could work out the identification 
of P and Q.  Ultimately  we were able to identify 
the causal segments for 14181 sentences. Four syn-
tactic environments can be distinguished, involving 
a preposed construction <Conn P Q.> as in exam-
ples (2, 3, 5) above, and three types of medial con-
structions:  
a)  <Q conn P.> corresponds to a construction 
in which Q and P are linked by a connective 
within the same sentence (example 1);  
b) <Q. Conn P.> corresponds to constructions 
in which the previous sentence functions as 
Q (examples 4); and  
c) <Prev. Q conn P.> corresponds to construc-
tions for which the Q-segment is anaphoric 
with the preceding sentence, thus requiring 
this previous sentence for the semantic in-
terpretation, as in example (6), in which the 
Q ?dat komt? (litt. ?that comes?) picks up the 
semantic information from the previous sen-
tence and links it to the P-segment intro-
duced by the connective. 
(6) De Europese economie 
raakt hopeloos achterop 
bij de Amerikaanse en 
Japanse. Dat komt door-
dat Europa niet meedoet 
op nieuwe groeimarkten. 
?The European economy is 
falling hopelessly behind 
the American and Japanese 
economy. This is because 
Europe is not participating 
in new growth markets.? 
 
Actually, only 7.1% of the sentences investigated 
belong to the preposed construction type. How-
ever, important divergences exist between the con-
nectives: want is never used in preposed position, 
omdat in 10.41% of the cases, and doordat in 
14.32% of the cases, a figure which rises to 43.5% 
of the cases for aangezien. It is interesting to point 
out that this is in total agreement with previous 
small-scale corpus research on this matter. 
2.2 Lemmatisation and the construction of 
the LSA semantic space 
The first automatic technique that will be presented 
is Latent Semantic Analysis (LSA), a mathematical 
technique for extracting a very large ?semantic 
space? from large text corpora on the basis of the 
statistical analysis of the set of co-occurrences in a 
text corpus. Landauer et al (1998) stress that this 
technique can be viewed from two sides.  At a 
theoretical level, it is meant to be used to develop 
simulations of the cognitive processes running dur-
ing language comprehension, including, for in-
stance, a computational model of metaphor 
treatment (Kintsch, 2000 ; Lemaire et al, 2001), 
but also to analyse the coherence of texts (Foltz et 
al., 1998 ; Pi?rard et al, 2004). At a more applied 
level, it is a technique which enables to infer and to 
represent the meaning of words on the basis of 
their actual use in text so that the similarity of the 
meaning of words, sentences or paragraphs can be 
estimated (Bestgen, 2002; Choi et al, 2001). It is 
this latter aspect which draws our attention here. 
The point of departure of the analysis is a lexical 
table (Lebart and Salem, 1992) containing the fre-
quencies of every word in each of the documents 
included in the text material, a document being a 
text, a paragraph, or a sentence. To derive semantic 
relations between words from the lexical table the 
analysis of mere co-occurrences will not do,  the 
major problem being that even in a large corpus 
most words are relatively rare.  Consequently the 
co-occurrences of words are even rarer.  This fact 
makes such co-occurrences very sensitive to arbi-
trary variations (Burgess et al, 1998 ; Kintsch, 
2001).  LSA resolves this problem by replacing the 
original frequency table by an approximation pro-
ducing a kind of smoothening effect on the asso-
ciations. To this end, the frequency table 
undergoes a singular value decomposition and it is 
then recomposed on the basis of only a fraction of 
the information it contains. Thus, the thousands of 
words from the documents have been substituted 
by linear combinations or ?semantic dimensions? 
with respect to which the original words can be 
situated again. Contrary to a classical factor analy-
sis the extracted dimensions are very numerous 
and non-interpretable.   
All original words and segments can then be 
placed into this semantic space. The meaning of 
each word is represented by a vector, thus indicat-
ing the exact location of the word in this multidi-
mensional semantic space. To calculate the 
semantic proximity between two words, the cosine 
between the two vectors that represent them is cal-
culated.  The more two words are semantically 
similar, the more their vectors point in the same 
direction, and consequently, the closer their cosine 
will be to 1 (coinciding vectors).  A cosine of 0 
shows an absence of similarity, since the corre-
sponding vectors point in orthogonal directions.  It 
is also possible to calculate the similarity between 
?higher order? elements, i.e. between sentences, 
paragraphs, and entire documents, or combinations 
of those, even if this higher order element isn?t by 
itself an analysed element. The vector in question 
corresponds to the centroid of the words compos-
ing the segment under investigation.  The centroid 
results from the weighted sum of the vectors of 
these words (Deerwester et al, 1990). This makes 
it possible to calculate the semantic proximity be-
tween any two sentences, viz. whether present in 
the original corpus or not, whether the original 
corpus had been segmented in sentence length 
documents or not.  
To perform the LSA analyses, we used the 
Dutch newspaper corpus to build the semantic 
space. To this end, the data set, which had been 
lemmatised with MBLEM (Memory Based Lem-
matiser) (Van den Bosch & Daelemans, 1999), was 
cut into article-length segments.  Elimination of all 
digits, special characters, punctuation marks, and 
of a list of 222 stopwords (words occurring in 
?any? context, like determiners, auxiliaries, con-
junctions, ?), brought the total number of words 
back to approximately 6.5 million. For the input 
lexical table, the documents were articles of mini-
mally 24 words and maximally 523 words, i.e. all 
articles minus the 10% shortest and minus the 10% 
longest ones. As to the words, we kept all those 
that occurred at least ten times in the data set. 
Overall this resulted in a matrix of 36630 terms in 
28640 documents.  To build the semantic space 
proper, the singular value decomposition was real-
ized with the program SVDPACKC (Berry, 1992; 
Berry et al, 1993), and the 300 first singular vec-
tors were retained.  In the present research we will 
use this technique to evaluate the semantic prox-
imity between P& Q, and between the causal seg-
ments and the prior or subsequent sentences.  
2.3 Dictionaries and lexical categorisation 
The second technique used to test the linguistic 
hypotheses is alternatively called ?word count 
strategy? (Pennebaker et al, 2003), automatic iden-
tification of linguistic features (Biber, 1988) or 
thematic text analysis (Popping, 2000; Stone, 
1997), the aim of which is to determine whether 
some categories of words (e.g., words of opinion, 
fact, attitude, etc.) or some grammatical categories 
(e.g. personal pronouns) occur more often in a 
given type of text segment. The first step in this 
kind of analysis is to build a dictionary that con-
tains the categories to be investigated and the cor-
responding (lemmatised) lexical entries that signal 
their occurrence. The categories may correspond to 
grammatical classes, but also to thematic word 
grouping. The following step consists in searching 
all the text segments containing these lexical en-
tries in order to account for the frequency of each 
category in each text segment. These data are put 
into a matrix that has one row for each text seg-
ment and one column for each category, each cell 
containing the frequency of the respective category 
in the respective text segment. Finally, this matrix 
is analysed to determine whether some categories 
occur more often in a given type of text segment. 
To illustrate this technique, let us assume that we 
want to test the hypothesis that (nominative) per-
sonal pronouns occur more frequent in text seg-
ments connected by want than by the other 
backward causal connectives. In the first step the 
"Personal-Pronoun" dictionary is built, containing 
the corresponding lexical entries: ik, jij, je, hij, zij, 
ze, wij, we, jullie, u. All the text segments contain-
ing these lexical entries are then searched in order 
to account for the frequency of the concept "Per-
sonal-Pronoun" in each text segment. These data 
are put into a matrix which is analysed to deter-
mine whether the concept "Personal-Pronoun" oc-
curs more often with want-segments than with the 
other causal segments.  
The two main difficulties we are confronted 
with when using this technique in the present stud-
ies are (i) the reduced size of the analysed text 
segments (one sentence or even less), and (ii) the 
difficulty, or even impossibility, to build an ex-
haustive list of words belonging to a category like 
fact, opinion, attitude, etc. With respect to the first 
difficulty, we believe that the reduced size of the 
segments will be compensated by the large number 
of segments of each type being analysed.  The sec-
ond difficulty is addressed below where we pro-
pose a number of ways to extend the category lists 
automatically.  
3 Combining LSA and TTA: an applica-
tion 
3.1 Perspective shift 
There are a number of claims in the literature that 
some connectives co-occur with perspective shifts 
between the causal segments, while others do not. 
Perspectivisation accounts for the fact that there 
are more sources of information than the speaker 
alone. In relation to our connectives, perspectivisa-
tion has been claimed to play a role in the meaning 
differences between want (introducing a perspec-
tive shift) and omdat (no perspective shift). How-
ever, the various corpus studies on this matter have 
not univocally confirmed this hypothesis (Degand, 
2001; Oversteegen, 1997).  We would like to ex-
plore this matter further by comparing the semantic 
tightness of the segments related by our connec-
tives.  This will be done by calculating the seman-
tic proximity between Q and P for each of the 
connectives. Our hypotheses are as follows: 
Hypothesis 1: The cosine between Q and P re-
lated by monophonic connectives (omdat) 
should be higher than the cosine between Q and 
P related by polyphonic connectives (want). 
Hypothesis 2: The cosine between the prior sen-
tence and the subsequent sentence should be 
higher for monophonic connectives than for 
polyphonic connectives. 
 
Cos. Q & P Cos. Prior Subse-
quent  
 
Mean SD Mean SD 
aangezien 
(N = 200) 
0.143 0.17 0.207 0.21 
doordat (N 
= 644) 
0.154 0.17 0.187 0.19 
omdat (N = 
5691) 
0.137 0.17 0.182 0.20 
want (N = 
3974) 
0.120 0.17 0.150 0.19 
Table 2: Mean Cosine per connective between the 
causal segments, and between the prior and subsequent 
sentences 
 
Table 2 displays the cosines resulting from the 
LSA-analysis. Two ANOVAs  were performed. 
The first one had the connectives as independent 
variable and the semantic proximity between the 
causal segments as dependent variable. It shows 
that hypothesis 1 is borne out (F(3, 10505) = 
11.36, p < 0.0001): the causal segments related by 
the (monophonic) connective omdat are semanti-
cally closer than the segments related by the (poly-
phonic) connective want.  The results furthermore 
show that doordat and aangezien should be de-
scribed in terms of  monophonic connectives.  The 
second ANOVA, with the connectives as inde-
pendent variable and the semantic proximity be-
tween the prior and subsequent sentences as 
dependent variable, confirms hypothesis 2 (F(3, 
10505) = 25.75, p < 0.0001): the monophonic con-
nectives aangezien, doordat and omdat go along 
with topic continuity (or at least semantic prox-
imity) between the prior and subsequent sentence 
to the causal construction, while this is less the 
case for the connective want.  
To confirm that these results are indeed related 
to the issue of perspectivisation, this LSA-analysis 
was completed with a thematic text analysis to test 
for the presence vs. absence of perspective indica-
tors. To this end we built a "Perspective" diction-
ary of perspective-indicating elements (Spooren, 
1989) such as intensifiers, emphasisers, attitudinal 
nouns and adjuncts, etc. (Caenepeel, 1989). The 
dictionary was composed of two subcategories: 
a) communication markers, like (non-
ambiguous) verbs and adverbs of saying 
and thinking, e.g. report, tell, confirm, re-
quire,  according to,? 
b) markers of the speaker's attitude, like lin-
guistic elements expressing an expectation 
or a denial of expectation, intensifiers and 
attitudinals, and evaluative words, e.g. 
probably, must, horrible, fantastic, ?  
To build the dictionary, we used a Dutch thesaurus 
(Brouwers, 1997) and extracted all (unambiguous) 
lemmas corresponding to one of the above-
mentioned categories.  Multi-word expressions or 
separable verbs were not included in the lists. The 
lists were composed on two native speaker's 
judgements with a good knowledge of the litera-
ture on perspectivisation.  
The idea of the thematic text analysis was to 
confirm that the break in semantic tightness occur-
ring with want-segments, as revealed by the LSA-
analysis, could indeed be interpreted in terms of a 
perspective shift.  We would therefore expect that 
the causal segments related by the connective want 
show diverging perspectivisation patterns, and that 
this will not be the case for the segments related by 
omdat, doordat, aangezien. This is reformulated in 
hypothesis 3. 
Hypothesis 3:  If the causal segments are re-
lated by the connective want, the Q-segment 
contains perspective signals, the P-segment 
does not.  The causal segments related by the 
connectives omdat, doordat, aangezien do not 
present such a shift. 
Communication 
markers 
Attitude markers   
Mean Q Mean P Mean Q Mean P 
aangezien 
(N = 139) 
0.173 
SD: 0.38 
0.115 
SD: 0.32 
0.360 
SD: 0.48 
0.273 
SD: 0.45 
doordat (N 
= 699) 
0.129 
SD: 0.33 
0.104 
SD: 0.31 
0.305 
SD: 0.46 
0.326 
SD: 0.47 
omdat (N = 
6747) 
0.179 
SD: 0.38 
0.162 
SD: 0.37 
0.312 
SD: 0.46 
0.312 
SD: 0.46 
want (N = 
5589) 
0.175 
SD: 0.38 
0.181 
SD: 0.38 
0.442 
SD: 0.50 
0.394 
SD: 0.49 
Table 3: Mean number of perspective markers in P & Q  
 
The results displayed in Table 3 show that the hy-
pothesis is borne out for the subcategory of attitu-
dinal markers: want-segments display a higher 
amount of attitudinal markers in Q than in P (F(1, 
5588) = 26.84, p < 0.0001). For the other connec-
tives this is not the case. For the communication 
markers, the hypothesis is not borne out.  Actually, 
only omdat displays a higher amount of communi-
cation markers in Q (F(1, 6746) = 6.53, p < 0.01).  
While this latter result might seem counter to ex-
pectation, it actually goes in the direction of prior 
observations that omdat-relations frequently dis-
play the explicit introduction of speech acts (De-
gand, 2001; Pit 2003). 
All together, these results offer new interesting 
insights into the discourse environment of (Dutch) 
causal connectives.  On the one hand, we have 
shown with the LSA analysis that the proximity 
between Q and P is lower for want-relations than 
for the other connectives and that this is also the 
case for the semantic proximity between the sen-
tences prior and subsequent to the causal relations.  
We therefore concluded that the connective want is 
a marker of  thematic shift.  On the other hand, the 
TTA analysis revealed that the Q-segments in 
want-relations display a higher amount of attitudi-
nal markers. In our view, the presence of these 
markers leads to the conclusion that the connective 
want is indeed a marker of perspective shift, i.e. 
the break in semantic tightness should be inter-
preted as a perspective break, as has often been 
suggested in the literature. Furthermore, the addi-
tional results for want (absence of communication 
markers in Q) also suggest that markers expressing 
the speaker's attitude should be clearly distin-
guished from those that explicit the speaker's 
speech act (verbs of saying) or designate him/her 
explicitly as the source of the speech act (adverbs 
like aldus, volgens, ? 'according to'). 
The polyphony/monophony distinction overlaps 
with the coordination/subordination distinction 
between want vs. the other connectives.  The ques-
tion arises which of those two factors is responsi-
ble for the results obtained.  One route to follow is 
to compare our results with a language like English 
in which a same connective (because) has both 
monophonic and polyphonic uses, or with a lan-
guage like French where a polyphonic connective 
like puisque is subordinating. The latter topic is 
object of ongoing research. 
4 Discussion 
In this paper we have presented a method for the 
linguistic investigation of a discourse phenomenon, 
viz. connectives, giving very satisfying results 
without necessitating heavy, work-intensive (hand-
based) discourse annotation.  The research pre-
sented is important to the corpus study of discourse 
phenomena for a number of reasons. The first is 
that it makes it possible to test linguistic hypothe-
ses about the use of causal connectives on a large 
scale basis, whereas previous tests were based on 
only small corpora and small amount of data. The 
second is that the analysis is mostly fully auto-
matic, especially with respect to the coding of the 
fragments. It is especially this latter feature that 
should appeal to the linguistic community, and 
makes our method more robust. The intercoder 
reliability is a constant concern of everyone work-
ing with corpora to test linguistic hypotheses (Car-
letta, 1996), and the more so when one is coding 
for semanto-pragmatic interpretations, as in the 
case of the analysis of connectives. A third reason 
is that our method combines two techniques of 
automatic text analysis, which allows us to formu-
late our hypotheses to be tested more fine-grained 
than possible with either one separately. Moreover, 
hypothesis formulation and testing goes further: 
We can use the methodology to formulate new hy-
potheses. An interesting possibility is to use LSA 
to find neighbours of terms in the dictionary, thus 
extending the dictionary. A further interesting 
venue is to test the linguistic hypotheses for differ-
ent genres. This brings us to a further possibility, 
namely to reuse the semantic space for different 
types of linguistic research. A final possibility is to 
use the present semantic space for comparative 
research: How do the present results compare to a 
similar analysis of French connectives? 
Acknowledgements 
L. Degand and Y. Bestgen are research fellows of 
the Belgian National Fund for Scientific Research.  
This research was supported by grant n? FRFC 
2.4535.02 and by a grant (?Action de Recherche 
concert?e?) of the government of the French-
language community of Belgium. 
References 
Berry, M.W. (1992). Large scale singular value compu-
tation, International journal of Supercomputer Appli-
cation,  6: 13-49. 
Berry, M., Do, T., O'Brien, G., Krishna, V. and Varad-
han, S. (1993). SVDPACKC: Version 1.0 User's 
Guide, Tech. Rep. CS-93-194, University of Tennes-
see, Knoxville, TN, October 1993. 
Burgess C., Livesay K., Lund K., " Explorations in Con-
text Space : Words, Sentences, Discourse ", Dis-
course Processes, Vol. 25, 1998, p. 211-257.   
Bestgen, Y. (2002). D?termination de la valence affec-
tive de termes dans de grands corpus de textes. Actes 
du Colloque International sur la Fouille de Texte 
CIFT'02 (pp. 81-94). Nancy : INRIA. 
Bestgen, Y., Degand, L. & Spooren, W. (2003). On the 
use of automatic techniques to determine the seman-
tics of connectives in large newspaper corpora: an 
exploratory study. Lagerwerf L., Spooren W., De-
gand L. (Eds). Determination of Information and 
Tenor in Texts: MAD 2003, Stichting Neerlandistiek 
VU Amsterdam & Nodus Publikationen M?nster, 
179-188. 
Biber, D. (1998). Variation across speech and writing. 
Cambridge: Cambridge University Press. 
Brouwers, L. (1997). Het juiste woord, betekeniswoor-
denboek. 6th ed. (ed. by F. Claes). Antwerpen etc.: 
Standaard. 
Caenepeel, M. (1989). Aspect, Temporal Ordering and 
Perspective in Narrative Fiction. Doctoral Disserta-
tion University of Edinburgh.  
Carletta, J. (1996). Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics 22 (2), 249-254. 
Choi,  F., Wiemer-Hastings P., & Moore J. (2001) La-
tent Semantic Analysis for Text Segmentation. In L. 
Lee & D. Harman (Eds.), Proceedings of the 2001 
Conference on Empirical Methods in Natural Lan-
guage Processing , 109-117.  
Daelemans, W., Zavrel, J., Berck, P., & Gillis, S. 
(1996). MBT: A Memory-Based Part of Speech Tag-
ger-Generator.  In E. Ejerhed & I. Dagan (Eds.), Pro-
ceedings of the Fourth Workshop on Very Large 
Corpora (pp. 14-27). Copenhagen, Denmark. 
Deerwester S., Dumais S.T., Furnas G.W., Landauer 
T.K., Harshman R., Indexing by Latent Semantic 
Analysis, Journal of the American Society for Infor-
mation Science, Vol. 41, 1990,  391-407. 
Degand, L. (2001). Form and Function of Causation. A 
theoretical and empirical investigation of causal 
constructions in Dutch, Peeters, Leuven, Paris, Ster-
ling. 
Degand, L.  & Pander Maat, H. (2003) A contrastive 
study of Dutch and French causal connectives on the 
Speaker Involvement Scale, A. Verhagen & J. van de 
Weijer (eds.) Usage based approaches to Dutch (pp. 
175-199). Utrecht: LOT.  
Foltz, P.W., Kintsch, W.,  & Landauer T.K. (1998). The 
measurement of textual coherence with Latent Se-
mantic Analysis. Discourse Processes, 25, 285-307. 
Kintsch, W. (2000). Metaphor comprehension: A com-
putational theory. Psychonomic Bulletin and Review, 
7, 257-266. 
Kintsch W., (2001).Predication, Cognitive Science 25, 
173-202. 
Landauer, T.K., Foltz, P.W., and Laham, D. (1998).  An 
introduction to Latent Semantic Analysis. Discourse 
Processes, 25 (2, 3), 259-284. 
Lebart, L., Salem, A., and Berry, L. (1998). Exploring 
Textual Data. Kluwer Academic Publisher. 
Lemaire, B., Bianco, M., Sylvestre, E., & Noveck, I. 
(2001). Un mod?le de compr?hension de textes fond? 
sur l'analyse de la s?mantique latente. In H. Paugam 
Moisy, V. Nyckees, J. Caron-Pargue (Eds.), La Co-
gnition entre Individu et Soci?t? : Actes du Colloque 
de l'ARCo (pp. 309-320). Paris: Herm?s.  
Oversteegen, L. (1997). On the pragmatic nature of 
causal and contrastive connectives. Discourse 
Processes, 24, 51-86. 
Pennebaker, J.W., , Mehl, M.R., & Niederhoffer, K.G. 
(2003). Psychological aspects of natural language 
use: Our words, our selves. Annual Review of  Psy-
chology, 54, 547-577. 
Pi?rard, S., Degand, L., & Bestgen Y. (2004). Vers une 
recherche automatique des marqueurs de la segmen-
tation du discours. Actes des 7es Journ?es internatio-
nales d?Analyse statistique des Donn?es Textuelles. 
Louvain-la-Neuve. 
Pit, M. (2003). How to Express Yourself with a Causal 
Connective. Subjectivity and Causal Connectives in 
Dutch, German and French. Amsterdam : Rodopi.  
Popping, R. (2000). Computer-assisted text analysis. 
London: SAGE. 
Spooren, W.P.M.S (1989). Some Aspects of the Form 
and Interpretation of Global Contrastive Coherence 
Relations. Unpublished Dissertation, K.U. Nijmegen. 
Stone, P.J. (1997). Thematic text analysis: New agendas 
for analyzing text content. In C.W. Roberts (Eds.). 
Text Analysis for the Social Sciences: Methods for 
Drawing Statistical Inferences from Texts and Tran-
scripts (pp.35-54). Mahwah, NJ: Erlbaum. 
van den Bosch, A., & Daelemans, W. (1999). Memory-
based morphological analysis. In Proceedings of the 
37th Annual Meeting of the Association for Compu-
tational Linguistics, ACL'99 (pp. 285-292). New 
Brunswick, NJ: ACL 
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 160?165,
Dublin, Ireland, August 23-24, 2014.
CECL: a New Baseline and a Non-Compositional Approach for the Sick
Benchmark
Yves Bestgen
Centre for English Corpus Linguistics
Universit?e catholique de Louvain
yves.bestgen@uclouvain.be
Abstract
This paper describes the two procedures
for determining the semantic similarities
between sentences submitted for the Se-
mEval 2014 Task 1. MeanMaxSim, an
unsupervised procedure, is proposed as a
new baseline to assess the efficiency gain
provided by compositional models. It out-
performs a number of other baselines by
a wide margin. Compared to the word-
overlap baseline, it has the advantage of
taking into account the distributional simi-
larity between words that are also involved
in compositional models. The second
procedure aims at building a predictive
model using as predictors MeanMaxSim
and (transformed) lexical features describ-
ing the differences between each sentence
of a pair. It finished sixth out of 17 teams
in the textual similarity sub-task and sixth
out of 19 in the textual entailment sub-
task.
1 Introduction
The SemEval-2014 Task 1 (Marelli et al., 2014a)
was designed to allow a rigorous evaluation
of compositional distributional semantic models
(CDSMs). CDSMs aim to represent the meaning
of phrases and sentences by composing the dis-
tributional representations of the words they con-
tain (Baroni et al., 2013; Bestgen and Cabiaux,
2002; Erk and Pado, 2008; Grefenstette, 2013;
Kintsch, 2001; Mitchell and Lapata, 2010); they
are thus an extension of Distributional Semantic
Models (DSMs), which approximate the meaning
of words with vectors summarizing their patterns
of co-occurrence in a corpus (Baroni and Lenci,
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
2010; Bestgen et al., 2006; Kintsch, 1998; Lan-
dauer and Dumais, 1997). The dataset for this
task, called SICK (Sentences Involving Composi-
tional Knowledge), consists of almost 10,000 En-
glish sentence pairs annotated for relatedness in
meaning and entailment relation by ten annotators
(Marelli et al., 2014b).
The rationale behind this dataset is that ?un-
derstanding when two sentences have close mean-
ings or entail each other crucially requires a com-
positional semantics step? (Marelli et al., 2014b),
and thus that annotators judge the similarity be-
tween the two sentences of a pair by first build-
ing a mental representation of the meaning of each
sentence and then comparing these two represen-
tations. However, another option was available
to the annotators. They could have paid atten-
tion only to the differences between the sentences,
and assessed the significance of these differences.
Such an approach could have been favored by the
dataset built on the basis of a thousand sentences
modified by a limited number of (often) very
specific transformations, producing sentence pairs
that might seem quite repetitive. An analysis con-
ducted during the training phase of the challenge
brought some support for this hypothesis. The
analysis focused on pairs of sentences in which the
only difference between the two sentences was the
replacement of one content word by another, as in
A man is singing to a girl vs. A man is singing to
a woman, but also in A man is sitting in a field
vs. A man is running in a field. The material
was divided into two parts, 3500 sentence pairs
in the training set and the remaining 1500 in the
test set. First, the average similarity score for each
pair of interchanged words was calculated on the
training set (e.g., in this sample, there were 16 sen-
tence pairs in which woman and man were inter-
changed, and their mean similarity score was 3.6).
Then, these mean scores were used as the similar-
ity scores of the sentence pairs of the test sample
160
in which the same words were interchanged. The
correlation between the actual scores and the pre-
dicted score was 0.83 (N=92), a value that can be
considered as very high, given the restrictions on
the range in which the predicted similarity scores
vary (min=3.5 and max=5.0; Howell, 2008, pp.
272-273). It is important to note that this observa-
tion does not prove that the participants have not
built a compositional representation, especially as
it only deals with a very specific type of trans-
formation. It nevertheless suggests that analyz-
ing only the differences between the sentences of
a pair could allow the similarity between them to
be effectively estimated.
Following these observations, I opted to try
to determine the degree of efficacy that can be
achieved by two non-compositional approaches.
The first approach, totally unsupervised, is pro-
posed as a new baseline to evaluate the efficacy
gains brought by compositional systems. The sec-
ond, a supervised approach, aims to capitalize on
the properties of the SICK benchmark. While
these approaches have been developed specifically
for the semantic relatedness sub-task, the second
has also been applied to the textual entailment sub-
task. This paper describes the two proposed ap-
proaches, their implementation in the context of
SemEval 2014 Task 1, and the results obtained.
2 Proposed Approaches
2.1 A New Baseline for CDSM
An evident baseline in the field of CDSM is based
on the proportion of common words in two sen-
tences after the removal (or retaining) of stop
words (Cheung and Penn, 2012). Its main weak-
ness is that it does not take into account the seman-
tic similarities between the words that are com-
bined in the CDSM models. It follows that a com-
positional approach may seem significantly better
than this baseline, even if it is not compositionality
that matters but only the distributional part. At first
glance, this problem can be circumvented by using
as baseline a simple compositional model like the
additive model. The analyses below show that this
model is much less effective for the SILK dataset
than the distributional baseline proposed here.
MeanMaxSim, the proposed baseline, is an ex-
tension of the classic measure based on the pro-
portion of common words, taking advantage of the
distributional similarity but not of compositional-
ity. It corresponds to the mean, calculated using all
the words of the two sentences, of the maximum
semantic similarity between each word in a sen-
tence and all the words of the other sentence. More
formaly, given two sentences a = (a
1
, .., a
n
) and
b = (b
1
, ..b
m
),
MMS =
(
?
i
max
j
sim(a
i
,b
j
)+
?
j
max
i
sim(a
i
,b
j
))
n+m
In this study, the cosine between the word distri-
butional representations was used as the measure
of semantic similarity, but other measures may be
used. The common words of the two sentences
have an important impact on MeanMaxSim, since
their similarity with themselves is equal to the
maximum similarity possible. Their impact would
be much lower if the average similarity between
a word and all the words in the other sentence
were employed instead of the maximum similar-
ity. Several variants of this measure can be used,
for example not taking into account every instance
where a word is repeated in a sentence or not al-
lowing any single word to be the ?most similar? to
several other words.
2.2 A Non-Compositional Approach Based
on the Differences Between the Sentences
The main limitation of the first approach in the
context of this challenge is that it is completely
unsupervised and therefore does not take advan-
tage of the training set provided by the task orga-
nizers. The second approach addresses this limi-
tation. It aims to build a predictive model, using
as predictors MeanMaxSim but also lexical fea-
tures describing the differences between each sen-
tence of a pair. For the extraction of these fea-
tures, each pair of sentences of the whole dataset
(training and testing sets) is analyzed to iden-
tify all the lemmas that are not present with the
same frequency in both sentences. Each of these
differences is encoded as a feature whose value
corresponds to the unsigned frequency difference.
This step leads to a two-way contingency table
with sentence pairs as rows and lexical features
as columns. Correspondence Analysis (Blasius
and Greenacre, 1994; Lebart et al., 2000), a sta-
tistical procedure available in many off-the-shelf
software like R (Nenadic and Greenacre, 2006), is
then used to decompose this table into orthogonal
dimensions ordered according to the correspond-
ing part of associations between rows and columns
they explain. Each row receives a coordinate on
these dimensions and these coordinates are used as
predictors of the relatedness scores of the sentence
161
pairs. In this way, not only are the frequencies of
lexical features transformed into continuous pre-
dictors, but these predictors also take into account
the redundancy between the lexical features. Fi-
nally, a predictive model is built on the basis of
the training set by means of multiple linear regres-
sion with stepwise selection of the best predictors.
For the textual entailment sub-task, the same pro-
cedure was used except that the linear regression
was replaced by a linear discriminant analysis.
3 Implementation Details
This section describes the steps and additional
resources used to implement the proposed ap-
proaches for the SICK challenge.
3.1 Preprocessing of the Dataset
All sentences were tokenized and lemmatized by
the Stanford Parser (de Marneffe et al., 2006;
Toutanova et al., 2003).
3.2 Distributional Semantics
Latent Semantic Analysis (LSA), a classical DSM
(Deerwester et al., 1991; Landauer et al., 1998),
was used to gather the semantic similarity between
words from corpora. The starting point of the anal-
ysis is a lexical table containing the frequencies of
every word in each of the text segments included
in the corpus. This table is submitted to a singu-
lar value decomposition, which extracts the most
significant orthogonal dimensions. In this seman-
tic space, the meaning of a word is represented by
a vector and the semantic similarity between two
words is estimated by the cosine between their cor-
responding vectors.
Three corpora were used to estimate these simi-
larities. The first one, the TASA corpus, is com-
posed of excerpts, with an approximate average
length of 250 words, obtained by a random sam-
pling of texts that American students read (Lan-
dauer et al., 1998). The version to which T.K.
Landauer (Institute of Cognitive Science, Univer-
sity of Colorado, Boulder) provided access con-
tains approximately 12 million words.
The second corpus, the BNC (British National
Corpus; Aston and Burnard, 1998) is composed
of approximately 100 million words and covers
many different genres. As the documents included
in this corpus can be of up to 45,000 words, they
were divided into segments of 250 words, the last
segment of a text being deleted if it contained
fewer than 250 words.
The third corpus (WIKI, approximately 600
million words after preprocessing) is derived from
the Wikipedia Foundation database, downloaded
in April 2011. It was built using WikiExtractor.py
by A. Fuschetto. As for the BNC, the texts were
cut into 250-word segments, and any segment of
fewer than 250 words was deleted.
All these corpora were lemmatized by means
of the TreeTagger (Schmid, 1994). In addition, a
series of functional words were removed as well
as all the words whose total frequency in the cor-
pus was lower than 10. The resulting (log-entropy
weighted) matrices of co-occurrences were sub-
mitted to a singular value decomposition (SVD-
PACKC, Berry et al., 1993) and the first 300 eigen-
vectors were retained.
3.3 Unsupervised Approach Details
Before estimating the semantic similarity between
a pair of sentences using MeanMaxSim, words (in
their lemmatized forms) considered as stop words
were filtered out. This stop word list (n=82), was
built specifically for the occasion on the basis of
the list of the most frequent words in the training
dataset.
3.4 Supervised Approach Details
To identify words not present with the same fre-
quency in both sentences, all the lemmas (includ-
ing those belonging to the stop word list) were
taken into account. The optimization of the param-
eters of the predictive model was performed using
a three-fold cross-validation procedure, with two
thirds of the 5000 sentence pairs for training and
the remaining third for testing. The values tested
by means of an exhaustive search were:
? Minimum threshold frequency of the lexical
features in the complete dataset: from 10 to
70 by step of 10.
? Number of dimensions retained from the CA:
from 10 to the total number of dimensions
available by step of 10.
? P-value threshold to enter or remove predic-
tors from the model: 0.01 and from 0.05 to
0.45 by step of 0.05.
This cross-validation procedure was repeated
five times, each time changing the random distri-
bution of sentence pairs in the samples. The fi-
nal values of the three parameters were selected
162
on the basis of the average correlation calculated
over all replications. For the relatedness sub-task,
the selected values were a minimum threshold fre-
quency of 40, 140 dimensions and a p-value of
0.20. For the entailment sub-task, they were a
minimum threshold frequency of 60, 100 dimen-
sions and a p-value of 0.25.
4 Results
4.1 Semantic Relatedness Sub-Task
The main measure of performance selected by the
task organizers was the Pearson correlation, calcu-
lated on the test set (4927 sentence pairs), between
the mean values of similarity according to the an-
notators and the values predicted by the automatic
procedures.
Unsupervised Approach: MeanMaxSim. Ta-
ble 1 shows the results obtained by MeanMaxSim,
based on the three corpora, and by three other
baselines:
? WO: The word-overlap baseline proposed by
the organizers of the task, computed as the
number of distinct tokens in both sentences
divided by the number of distinct tokens in
the longer sentence, optimizing the number
of the most frequent words stripped off the
sentences on the test set.
? SWL: The word-overlap baseline computed
as in WO but using lemmas instead of words
and the stop words list.
? ADD: The simple additive compositional
model, in which each sentence is represented
by the sum of the vectors of the lemmas that
compose it (stripping off stop words and us-
ing the best performing corpus) and the simi-
larity is the cosine between these two vectors
(Bestgen et al., 2010; Guevara, 2011) .
MeanMaxSim r Baseline r
TASA 0.696 WO 0.627
BNC 0.698 SWL 0.613
WIKI 0.696 ADD 0.500
Table 1: Pearson?s correlation for MeanMaxSim
and several other baselines on the test set.
MeanMaxSim produces almost identical results
regardless of the corpus used. The lack of differ-
ence between the three corpora was unexpected.
It could be related to the type of vocabulary used
in the SICK materials, seemingly mostly frequent
and concrete words whose use could be relatively
similar in the three corpora. MeanMaxSim per-
formance is clearly superior to all other baselines;
among these, the additive model is the worst. This
result is important because it shows that this com-
positional model is not, for the SICK benchmark,
the most interesting baseline to assess composi-
tional approaches. In the context of the best per-
formance of the other teams, MeanMaxSim is
(hopefully) well below the most effective proce-
dures, which reached correlations above 0.80.
Supervised Approach. The supervised ap-
proach resulted in a correlation of 0.78044, a value
well above all baselines reported above. This cor-
relation ranked the procedure sixth out of 17, tied
with another team (0.78019). The three best teams
scored significantly higher, with correlations be-
tween 0.826 and 0.828.
4.2 Textual Entailment Sub-Task
Only the supervised approach was used for this
sub-task. The proposed procedure achieved an ac-
curacy of 79.998%, which ranks it sixth again, but
out of 19 teams, still at a respectable distance from
the best performance (84.575%).
5 Conclusion
The main contribution of this research seems to be
the proposal of MeanMaxSim as baseline for eval-
uating CDSM. It outperforms a number of other
baselines by a wide margin and is very easy to
calculate. Compared to the word-overlap base-
line, it has the advantage of taking into account
the distributional similarity between words that are
also involved in compositional models. The su-
pervised approach proposed achieved an accept-
able result (sixth out of 17) and it could easily be
improved, for example by replacing standard lin-
ear regression by a procedure less sensitive to the
risk of overfit due to the large number of predictors
such as Partial Least Squares regression (Guevara,
2011). However, since this approach is not com-
positional and its efficacy (compared to others) is
limited, it is not obvious that trying to improve it
would be very useful.
Acknowledgements
Yves Bestgen is Research Associate of the Belgian
Fund for Scientific Research (F.R.S-FNRS).
163
References
Aston Guy, and Burnard Lou (1998). The BNC Hand-
book: Exploring the British National Corpus with
SARA. Edinburgh: Edinburgh University Press.
Baroni, Marco, and Lenci Alessandro (2010) Distri-
butional memory: A general framework for corpus-
based semantics, Computational Linguistics, 36,
673-721.
Baroni, Marco, Bernardi, Raffaella, and Zamparelli,
Roberto (2013). Frege in space: a program for com-
positional distributional semantics. In Annie Zae-
nen, Bonnie Webber and Martha Palmer. Linguistic
Issues in Language Technologies (LiLT), CSLI Pub-
lications.
Berry, Michael, Do, Theresa, O?Brien, Gavin, Krishna,
Vijay, and Varadhan, Sowmini (1993). Svdpackc:
Version 1.0 user?s guide, Technical Report Num-
ber CS-93-194, University of Tennessee, Knoxville,
TN.
Bestgen, Yves, and Cabiaux, Anne-Franoise (2002).
L?analyse s?emantique latente et l?identification des
m?etaphores. In Actes de la 9me Conf?erence annuelle
sur le traitement automatique des langues naturelles
(pp. 331-337). Nancy : INRIA.
Bestgen, Yves, Degand, Liesbeth, and Spooren,
Wilbert (2006). Towards automatic determination
of the semantics of connectives in large newspaper
corpora. Discourse Processes, 41, 175-193.
Bestgen, Yves, Lories, Guy, and Thewissen, Jennifer
(2010). Using latent semantic analysis to measure
coherence in essays by foreign language learners?
In Sergio Bolasco, Isabella Chiari and Luca Giu-
liano (Eds.), Proceedings of 10th International Con-
ference on Statistical Analysis of Textual Data, 385-
395. Roma: LED.
Blasius, Jorg, and Greenacre, Michael (1994). Com-
putation of Correspondence Analysis. In Michael
Greenacre and Jorg Blasius (eds.), Correspondence
Analysis in the Social Sciences, pp. 53-75. Academic
Press, London.
Cheung, Jackie, and Penn, Gerald (2012). Evaluating
distributional models of semantics for syntactically
invariant inference. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, 33-43, Avignon, France.
de Marneffe, Marie-Catherine, MacCartney, Bill, and
Manning, Christopher (2006). Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the 5th Edition of the Language Re-
sources and Evaluation Conference. Genoa, Italy.
Deerwester, Scott, Dumais, Susan, Furnas, George,
Landauer, Thomas and Harshman, Richard (1990).
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41, 391-
407.
Erk, Katrin, and Pado, Sebastian (2008). A structured
vector space model for word meaning in context. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, 897-906,
Honolulu, Hawaii.
Grefenstette, Edward (2013). Category-theoretic
quantitative compositional distributional models of
natural language semantics. PhD Thesis, Univer-
sity of Oxford, UK.
Guevara, Emiliano (2011). Computing semantic com-
positionality in distributional semantics. In Pro-
ceedings of the Ninth International Conference on
Computational Semantics, 135-144, Oxford, UK.
Howell, David (2008). M?ethodes statistiques en sci-
ences humaines. Bruxelles, Belgique: De Boeck
Universit?e.
Kintsch, Walter (1998). Comprehension: A
Paradigme for Cognition. New York: Cambridge
University Press.
Kintsch, Walter (2001). Predication. Cognitive Sci-
ence, 25(2), 173-202.
Landauer, Thomas, and Dumais, Susan, (1997). A so-
lution to Plato?s problem: The latent semantic anal-
ysis theory of acquisition, induction and representa-
tion of knowledge. Psychological Review, 104(2),
211-240.
Landauer, Thomas, Foltz, Peter, and Laham, Darrell
(1998). An introduction to latent semantic analysis,
Discourse Processes, 25, 259-284.
Lebart, Ludovic, Piron Marie, et Morineau Alain
(2000). Statistique exploratoire multidimension-
nelle (3e ?edition), Paris: Dunod.
Marelli, Marco, Bentivogli, Luisa, Baroni, Marco,
Bernardi, Raffaella, Menini, Stefano, and Zampar-
elli, Roberto (2014a). Semeval-2014 task 1: Evalu-
ation of compositional distributional semantic mod-
els on full sentences through semantic relatedness
and textual entailment. In Proceedings of SemEval-
2014: Semantic Evaluation Exercises. Dublin, Ire-
land.
Marelli, Marco, Menini, Stefano, Baroni, Marco, Ben-
tivogli, Luisa, Bernardi, Raffaella, and Zamparelli,
Roberto (2014b). A SICK cure for the evaluation
of compositional distributional semantic models. In
Proceedings of the 9th Edition of the Language Re-
sources and Evaluation Conference, Reykjavik, Ice-
land.
Mitchell, Jeff, and Lapata, Mirella (2010). Composi-
tion in distributional models of semantics. Cognitive
Science, 34, 1388-1429.
Nenadic, Oleg, and Greenacre, Michael (2007). Cor-
respondence analysis in R, with two- and three-
dimensional graphics: the CA package, Journal of
Statistical Software, 20(3), 1-13.
164
Schmid, Helmut (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
1994 International Conference on New Methods in
Language Processing, 44-49, Manchester, UK.
Toutanova, Kristina, Klein, Dan, Manning, Christo-
pher, and Singer, Yoram (2003). Feature-rich part-
of-speech tagging with a cyclic dependency net-
work. In Proceddings of the Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Linguistic
2003, 252-259, Edmonton, Canada.
165
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 111?118,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Maximizing Classification Accuracy in Native Language Identification   Scott Jarvis Yves Bestgen Steve Pepper Ohio University Universit? catholique de Louvain Department of Linguistic Department of Linguistics Centre for English Corpus Linguistics Aesthetic and Literary Studies Athens, OH, USA Louvain-la-Neuve, Belgium University of Bergen, Norway jarvis@ohio.edu yves.bestgen@uclouvain.be pepper.steve@gmail.com       Abstract 
This paper reports our contribution to the 2013 NLI Shared Task. The purpose of the task was to train a machine-learning system to identify the native-language affiliations of 1,100 texts written in English by nonnative speakers as part of a high-stakes test of gen-eral academic English proficiency. We trained our system on the new TOEFL11 corpus, which includes 11,000 essays written by nonnative speakers from 11 native-language backgrounds. Our final system used an SVM classifier with over 400,000 unique features consisting of lexical and POS n-grams occur-ring in at least two texts in the training set. Our system identified the correct native-language affiliations of 83.6% of the texts in the test set. This was the highest classification accuracy achieved in the 2013 NLI Shared Task. 1 Introduction The problem of automatically identifying a writer?s or speaker?s first language on the basis of features found in that person?s language production is a relatively new but quickly expanding line of in-quiry. It seems to have begun in 2001, but most of the studies published in this area have appeared in just the past two years. Although the practical ap-plications of native-language identification (NLI) are numerous, most of the existing research seems to be motivated by one or the other of two types of questions: (1) questions about the nature and extent of native-language influence in nonnative speak-ers? speech or writing, and (2) questions about the 
maximum levels of NLI classification accuracy that are achievable, which includes questions about the technical details of the systems that achieve the best results. Our previous work in this area has been motivated primarily by the former (see the multiple studies in Jarvis and Crossley, 2012), but in the present study we conform to the goals of the 2013 NLI Shared Task (Tetreault et al, 2013) in a pursuit of the latter. 2 Related Work The first published study to have performed an NLI analysis appears to have been Mayfield Tomokiyo and Jones (2001). The main goal of the study was to train a Na?ve Bayes system to identify native versus nonnative speakers of English on the basis of the lexical and part-of-speech (POS) n-grams found in their speech. The nonnative speak-ers in the study included six Chinese speakers and 31 Japanese speakers, and as a secondary goal, the researchers trained the system to identify the nonnative speakers by their native language (L1) backgrounds. The highest NLI accuracy they achieved was 100%. They achieved this result us-ing a model made up of a combination of lexical 1-grams and 2-grams in which nouns (and only nouns) were replaced with a POS identifier (=N). As far as we are aware, an NLI accuracy of 100% has not been achieved since Mayfield Tomokiyo and Jones (2001), but the NLI tasks that researchers have engaged in since then have been a great deal more challenging than theirs. This is true primarily in the sense that no other NLI study we are aware of has had such a high baseline accuracy, which is the accuracy that would be achieved if all 
111
cases were classified as belonging to the largest group. Because 31 of the 37 participants in the Mayfield Tomokiyo and Jones study were Japa-nese speakers, the baseline accuracy was already 83.8%. To avoid such a bias and to provide a greater challenge to their systems, researchers in recent years have engaged in NLI tasks that have involved more equally balanced groups with a far larger number of L1s. Most of these studies have focused on the identification of the L1s of nonnative writers who produced the texts included in the International Corpus of Learner English (ICLE) (Granger et al, 2009). NLI studies that have focused on the ICLE in-clude but are not limited to, in chronological order, Koppel et al (2005), Tsur and Rappoport (2007), Jarvis (2011), Bestgen et al (2012), Jarvis and Paquot (2012), Bykh and Meuers (2012), and Tetreault et al (2012). The highest NLI accuracy achieved in any of these studies was 90.1%, which was reported by Tetreault et al (2012). The re-searchers in this study used a system involving the LIBLINEAR instantiation of Support Vector Ma-chines (SVM) with the L1-regularized logistic re-gression solver and default parameters. The features in their model included character n-grams, function words, parts of speech, spelling errors and features of writing quality, such as grammatical errors, style markers, and so forth. They used spe-cialized software to extract error counts, grammar fragments, and counts of basic dependencies. They also created language model perplexity scores that reflected the lexical 5-grams most representative of each L1 in the corpus. This combination of fea-tures is more comprehensive than that used in any other NLI study, but the authors reported that their success was not due simply to the combination of features, but also because of the ensemble classifi-cation method they used. The ensemble method involved the creation of separate classifier models for each category of features; the L1 affiliations of individual texts were later predicted by the com-bined probabilities produced by the different clas-sifier models. The authors pointed out that combining all features into a single classifier gave them an NLI accuracy of only 82.6%, which is far short of the 90.1% they achieved through the en-semble method. The number of L1s represented in the study by Tetreault et al (2012) was seven, and it is notewor-thy that they achieved a higher NLI accuracy than 
any of the previous NLI studies that had examined the same number (Bykh and Meurers, 2012) or even a smaller number of L1s in the ICLE (e.g., Koppel et al, 2005, Tsur and Rappoport, 2007; Bestgen et al, 2012). The only NLI studies we know of that have examined more than seven L1s in the ICLE are Jarvis (2011) and Jarvis and Paquot (2012). Both studies examined 12 L1s in the ICLE, and both used a combination of features that included only lexical n-grams (1-grams, 2-grams, 3-grams, and 4-grams). Jarvis (2011) com-pared 20 different NLI systems to determine which would provide the highest classification accuracy for this particular task, and he found that LDA per-formed best with an NLI accuracy of 53.6%. This is the system that was then adopted for the Jarvis and Paquot (2012) study. It is important to note that the primary goal for Jarvis and Paquot was not to maximize NLI accuracy per se, but rather to use NLI as a means for assisting in the identification of specific instances and types of lexical influence from learners? L1s in their English writing. As noted by Bestgen et al (2012), Jarvis and Paquot (2012), and Tetreault et al (2012), there are certain disadvantages to using the ICLE for NLI research. One problem made especially clear by Bestgen et al is that the language groups repre-sented in the ICLE are not evenly balanced in terms of their levels of English proficiency. This creates an artificial sampling bias that allows an NLI system to distinguish between L1 groups on the basis of proficiency-related features without creating a classification model that accurately re-flects the influences of the learners? language backgrounds. Another problem mentioned by these and other authors is that writing topics are not evenly distributed across the L1 groups in the ICLE. That is, learners from some L1 groups tend-ed to write their essays in response to certain writ-ing prompts, whereas learners from other L1 groups tended to write in response to other writing prompts. Tetreault et al took extensive measures to remove as much of the topic bias as possible before running their analyses, but they also intro-duced a new corpus of nonnative English writing that is much larger and better balanced than the ICLE in terms of the distribution of topics across L1 groups. The new corpus is the TOEFL11, which will be described in detail in Section 3. Prior to the 2013 NLI Shared Task, the only NLI study to have been conducted on the TOEFL11 
112
corpus was Tetreault et al (2012). As described earlier, they performed an NLI analysis on a sub-sample of the ICLE representing seven L1 back-grounds. They also used the same system (including an identical set of features) in an NLI analysis of the TOEFL11. The fact that the TOEFL11 is better balanced than the ICLE is ad-vantageous in terms of the strength of the NLI classification model that it promotes, but this also makes the classification task itself more challeng-ing because it gives the system fewer cues (i.e., fewer systematic differences across groups) to rely on. The fact that the TOEFL11 includes 11 L1s, as opposed to the seven L1s in the subsample of the ICLE the authors examined, also makes the NLI task more challenging. For these reasons, NLI ac-curacy is bound to be higher for the ICLE than for the TOEFL11. This is indeed what the authors found. The NLI accuracy they reported for the TOEFL11 was nearly 10% lower than for the ICLE (80.9% vs. 90.1%). Nevertheless, their result of 80.9% accuracy was still remarkable for a task in-volving 11 L1s. Tetreault et al have thus set a very high benchmark for the 2013 NLI Shared Task. 3 Data The present study tests the effectiveness of our own NLI system for identifying the L1s represent-ed in the TOEFL11 (Blanchard et al, 2013). The TOEFL11 is a corpus of texts consisting of 11,000 essays written by nonnative English speakers as part of a high-stakes test of general proficiency in academic English. The essays were written by learners from the following 11 L1 backgrounds: Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, and Turkish. The corpus is perfectly balanced in terms of its number of essays per L1 group (i.e., 1,000 per L1), and it is also fairly well balanced in relation to the topics written about. The essays in the TOEFL11 were written in response to any of eight different writing prompts, and all eight prompts are reflected in all 11 L1 groups. Within four of the L1 groups, all prompts are almost equally represented with a proportion of approximately 12.5% per prompt (i.e., 100% ? 8 prompts = 12.5%). In other groups, there is more variability. The Italian group shows the largest discrepancies, with one prompt repre-senting only 1.2% of the essays, and another prompt representing 17.2% of the group?s essays. 
  English Proficiency L1  Low Medium High ARABIC Count 274 545 181 % 27.4% 54.5% 18.1% CHINESE Count 90 662 248 % 9.0% 66.2% 24.8% FRENCH Count 60 526 414 % 6.0% 52.6% 41.4% GERMAN Count 14 371 615 % 1.4% 37.1% 61.5% HINDI Count 25 399 576 % 2.5% 39.9% 57.6% ITALIAN Count 145 569 286 % 14.5% 56.9% 28.6% JAPANESE Count 207 617 176 % 20.7% 61.7% 17.6% KOREAN Count 154 617 229 % 15.4% 61.7% 22.9% SPANISH Count 73 502 425 % 7.3% 50.2% 42.5% TELUGU Count 86 595 319 % 8.6% 59.5% 31.9% TURKISH Count 73 561 366 % 7.3% 56.1% 36.6%  Table 1: Distribution of English Proficiency Levels  The distribution of learners? proficiency levels (low, medium, high) is even more variable across groups. Ideally, 33% of each group would fall into each proficiency level, but Table 1 shows that the distribution of proficiency levels does not come close to this in any L1 group. The distribution is especially skewed in the case of the German speakers, where only 1.4% of the participants fall into the low proficiency category whereas 61.5% fall into the high proficiency category. In any case, in nine of the 11 groups, the bulk of participants falls into the medium proficiency category, and in seven of those nine groups, the proportion of high-proficiency learners is greater than the proportion of low-proficiency learners. Clearly, the TOEFL11 
113
is not a perfectly balanced corpus, but it is much larger than the ICLE and involves fewer prompts, which are more evenly distributed across L1 groups. Another advantage of the TOEFL11 is that each text is associated with a proficiency level that has been determined by assessment experts using a consistent rating procedure for the entire corpus. This fact may allow researchers to isolate the ef-fects of learners? proficiency levels and to adjust their systems accordingly.  The TOEFL11 data were distributed to the 2013 NLI Shared Task participants in three stages. The initial distribution was a training set consisting of 9,900 of the 11,000 texts in the TOEFL11. The training set was made up of 900 texts from each L1 group. Later, a development set was made availa-ble. This included the remaining 1,100 texts in the TOEFL11, with 100 texts per L1. Finally, a test set was also provided to the teams participating in the 2013 NLI Shared Task. The test set consisted of 1,100 texts representing the same 11 L1s that are found in the TOEFL11. The test set included in-formation about the prompt that each text was writ-ten in response to, as well as information about the writer?s proficiency level, but did not include in-formation about the writer?s L1. 4 System Although our previous work has used NLI as a means toward exploring and identifying the effects of crosslinguistic influence in language learners? written production (see Jarvis and Crossley, 2012), in the present study we approached NLI exclusive-ly as a classification task, in keeping with the goals of the NLI Shared Task (Tetreault et al 2013). In order to maximize classification accuracy for the present study, we chose a system that would allow for the inclusion of thousands of features without violating statistical assumptions. Due to the unre-stricted number of features it allows and the high levels of classification accuracy it has achieved in previous research, such as in the study by Tetreault et al (2012), we chose to use linear Support Vector Machines (SVM) via the LIBLINEAR software package (Fan et al, 2008). The software allows the user to choose among the following types of solv-ers: a: L2-regularized L1-loss SVM (dual) b: L2-regularized L2-loss SVM (dual) c: L2-regularized logistic regression (primal) 
d: L1-regularized L2-loss SVM e: L1-regularized logistic regression f: L2-regularized L1-loss SVM (primal) g: L2-regularized L2-loss SVM (primal) h: Multi-class SVM by Crammer and Singer Although Tetreault et al (2012) used the Type e solver, we found Type b to be the most efficient in terms of both speed and accuracy. LIBLINEAR implements SVM via a multi-class classification strategy that juxtaposes each class (i.e., each L1) against all others. It also optimizes a cost parame-ter (Parameter C) using a grid search that relies on a crossvalidation criterion. The software iterates over multiple values of C until it arrives at an op-timal value. Although LIBLINEAR has a built-in program for optimizing C, we used our own opti-mization program in order to have more flexibility in choosing values of C to test. 4.1 Features Used The features we tried represented three broad cate-gories: words, characters, and complex features. The word category included lexemes, lemmas, and POS tags, as well as n-grams consisting of lex-emes, lemmas, and POS tags. Lexemes were de-fined as the observed forms of words, numbers, punctuation marks, and even symbols that were encountered in the TOEFL11. Lemmas were de-fined as the dictionary forms of lexemes, and we used the TreeTagger software package (Schmid, 1995) to automate the task of converting lexemes to lemmas. TreeTagger is unable to determine lemmas for rare words, misspelled words, and newly borrowed or coined words, and in such cas-es, it outputs ?unknown? in place of a lemma. We also used TreeTagger to automate the identification of the parts of speech (POS) associated with indi-vidual words. TreeTagger can only estimate the POS for unknown words, and it is also not perfect-ly accurate in determining the correct POS for words that it does recognize. Nevertheless, Schmid (1995) found that its POS tagging accuracy tends to be between 96% and 98%, which we consider to be adequate for present purposes. We included in our system all 1-grams, 2-grams, 3-grams, and 4-grams of lexemes, lemmas, and POS tags that oc-curred in at least two texts in the training set. Our character n-grams included all character n-grams from one character to nine characters in length that occurred in at least two texts in the 
114
training set. Finally, our complex features included nominalization suffixes (e.g., -tion, -ism), number of tokens per essay, number of types, number of sentences, number of characters, mean sentence length, mean length of lexemes, and a measure of lexical variety (i.e., type-token ratio). 5 Results We applied the system described in the previous section to the TOEFL11 corpus. We did this in multiple stages, first by training the system on the original training set of 9,900 texts while using LIBLINEAR?s built-in 5-fold crossvalidation. With the original training set, we tried multiple combinations of features in order to arrive at an optimal model. We found that our complex fea-tures contributed very little to any model we tested, and that we could achieve higher levels of NLI accuracy by excluding them altogether. We also found that models made up of optimal sets of lexi-cal features gave us roughly the same levels of NLI accuracy as models made up of optimal sets of character n-grams. However, models made up of a combination of lexical features and character fea-tures together performed worse than models made up of just one or the other. Our best performing model, by a small margin, was a model consisting of 1-grams, 2-grams, and 3-grams involving lex-emes, lemmas, and POS tags. The results of our comparison of multiple lexical models is shown in 
Table 2, with the best performing model represent-ed as Model A. Table 2 shows that Model A consists of all 1-gram, 2-gram, and 3-gram lexemes, lemmas, and POS tags that occur in at least two texts, using a log-entropy weighting schema and normalizing each text to unit length. It is noteworthy that nor-malizing each text vector, but also using a log-entropy weighting schema clearly improves the model accuracy. Normalizing each text vector as recommended by Fan et al (2008), but also using a log-entropy weighting schema (Dumais, 1991; Bestgen, 2012) clearly improves the model accura-cy. The total number of unique features in Model A is over 400,000. Our initial run of this model on the training set gave us a 5-fold cross-validated NLI accuracy of 82.53%.  We then attempted to determine whether these results could be replicated using other test materials. We first applied the best performing models displayed in Table 2 to the development set?using the development set as a test set?and achieved an NLI accuracy of over 86% for Model A, which remained the most accurate one. Then we applied these models to our own test set built to be evenly balanced in terms of the strat-ification of both L1s and prompts. We built this test set because we discovered large differences when we compared the distribution of prompts across L1 groups in the official test set for the 2013 Model Lexemes Lemmas Parts of Speech (POS tag) Frequency cut-off Weighting schema Normalization (to 1 per text) Accuracy (5-fold)  1g 2g 3g 1g 2g 3g 1g 2g 3g     A x x x x x x x x x ?2 LE Yes 82.53 B x x x x x x x x x ?5 LE Yes 82.52 C x x x x x x x x x ?10 LE Yes 82.48 D x x x x x x x x x ?2 LE No 80.46 E x x x x x x x x x ?2 Bin Yes 79.13 F x x x x x x x x x ?2 LFreq Yes 79.12 G x x  x x  x x  ?2 LE Yes 82.49 H x   x   x   ?2 LE Yes 76.42 I x x x x x x    ?2 LE Yes 82.09 J x x x    x x x ?2 LE Yes 81.24 K    x x x x x x ?2 LE Yes 80.92 L x x x       ?2 LE Yes 81.57 M    x x x    ?2 LE Yes 81.02 N       x x x ?2 LE Yes 54.95 Weighting schema: LE = Log-Entropy, Bin = Binary, LFreq = log of the raw frequencies  Table 2: Feature Combinations 
115
NLI Shared Task versus both the training set and development set. To build it, we combined the training set and development set into a single cor-pus (i.e., the full TOEFL11), and then divided the TOEFL11 into a double-stratified set of cells cross-tabulated by L1 and prompt. This resulted in 11 x 8 = 88 cells, and we randomly selected 10 texts per cell for the test set. This gave us a test set of 880 texts. We used the remaining 10,120 texts as a training set. However, the new division of training and test sets did not strongly modify our results, so we retained the previous Model A as our final model. In preparation for the final task of identifying the L1 affiliations of the 1,100 texts included in the official test set for the 2013 NLI Shared Task, we used the entire TOEFL11 corpus of 11,000 texts as our training set?with the features in Model A?in order to select the final values for the cost parame-ter (C) of our SVM system. By means of a 10-fold 
crossvalidation (CV) procedure on this dataset, the C parameter was set to 3200. The results of a 10-fold CV (using the fold split-ting of Tetreault et al, 2012) of the system?s per-formance with the TOEFL11 are shown in Table 3. The total number of texts per L1 group is consist-ently 1000, which makes the raw frequencies in the table directly interpretable as percentages. The lowest rate of accurate identification for any L1 in the 10-fold CV was 78.6%, and this was for Telu-gu. For all other L1s, the NLI accuracy rate ex-ceeded 80%, and in the case of German, it reached 96.5%. The overall NLI accuracy for the 10-fold CV was 84.5%. For the final stage of the analysis, we applied our system to the official test set in order to deter-mine how well it can identify writers? L1s in texts it has not yet encountered. The results of the final analysis are shown in Table 4. The classification accuracy (or recall) for individual L1s in the final   Predicted L1  Actual L1 ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Total ARA 802 16 41 14 28 11 9 12 47 8 12 1000 CHI 6 894 5 6 15 2 20 31 7 3 11 1000 FRE 24 11 856 28 11 25 4 4 33 1 3 1000 GER 2 4 6 965 5 3 1 2 9 0 3 1000 HIN 10 6 1 7 803 0 1 1 11 155 5 1000 ITA 3 3 26 24 8 890 3 1 35 1 6 1000 JPN 10 29 3 11 3 0 810 108 9 4 13 1000 KOR 5 51 3 8 7 1 98 802 12 1 12 1000 SPA 20 9 40 24 10 65 5 5 807 5 10 1000 TEL 5 0 2 1 200 0 1 2 1 786 2 1000 TUR 22 11 16 20 18 5 7 14 17 5 865 1000 Accuracy = 84.5% Table 3: 10-Fold Crossvalidation Results   ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Prec. F ARA 75 0 5 2 2 1 1 2 7 3 2 82.4 78.5 CHI 1 89 0 1 1 0 4 2 0 0 2 82.4 85.6 FRE 2 1 86 6 2 1 0 0 2 0 0 86.0 86.0 GER 0 0 1 96 0 0 0 0 2 0 1 83.5 89.3 HIN 1 0 0 0 81 0 0 0 4 13 1 74.3 77.5 ITA 0 1 3 4 0 90 0 0 2 0 0 90.9 90.5 JPN 2 3 0 1 1 2 85 3 2 0 1 85.9 85.4 KOR 0 10 1 0 1 0 8 76 1 2 1 87.4 81.3 SPA 4 0 4 2 3 3 0 1 81 0 2 78.6 79.8 TEL 1 1 0 1 18 0 0 0 0 79 0 81.4 80.2 TUR 5 3 0 2 0 2 1 3 2 0 82 89.1 85.4 Accuracy = 83.6% Table 4: Final NLI Results  
116
analysis ranges from 75% (Arabic) to 96% (Ger-man), and precision ranges from 74.3% (Hindi) to 90.9% (Italian). Our overall accuracy in identifying the L1s in the test set was 83.6%. 6 Conclusion Our system turned out to be the most successful system in the 2013 NLI Shared Task. Our 10-fold crossvalidated accuracy of 84.5% is also higher than the result of 80.9% previously achieved by Tetreault et al (2012) in their earlier NLI analysis of the TOEFL11. We find this to be both interest-ing and unexpected given that Tetreault et al used more complex measures than we did, such as 5-gram language models, and they also used an en-semble method of classification. Accordingly, we interpret the success of our model as an indication that the most reliable L1 specificity in the TOEFL11 is to be found simply in the words, word forms, sequential word combinations, and sequen-tial POS combinations that the nonnative writers produced. Tetreault et al emphasized the useful-ness of features that reflect L1-specific language models, but we believe that the multiple binary class comparisons that SVM makes might already take full advantage of L1 specificity as long as all of the relevant features are fed into the system. As for the ensemble method of classification used by Tetreault et al, their results clearly indi-cate that this method enhanced their NLI accuracy not only for the TOEFL11, but also for three addi-tional learner corpora, including the ICLE. Our own study did not compare our single-model sys-tem with the use of an ensemble method, but we are naturally curious about whether our own results could have been enhanced through the use of an ensemble method. As mentioned earlier, our pre-liminary attempts to construct a model based on character n-grams produced nearly as high levels of NLI accuracy as our final model involving lexi-cal and POS n-grams. Although we found that combining lexical and character n-grams worsened our results, we believe that a fruitful avenue for future research would be to test whether an ensem-ble of separate models based on character versus lexical n-grams could improve classification accu-racy. Importantly, however, a useful ensemble method generally needs to include more than two models unless it is based on probabilities rather 
than on the majority-vote method (cf. Jarvis, 2011; Tetreault et al, 2012). Our original interest in NLI began with a curios-ity about the evidence it can provide for the pres-ence of crosslinguistic influence in nonnative speakers? speech and writing. We believe that NLI strongly supports investigations of L1 influence, but in the case of the present results, we do not believe that L1 influence is solely responsible for the 83.6% NLI accuracy our system has achieved. Other factors are certainly also at play, such as the educational systems and cultures that the nonnative speakers come from. Apparent effects of cultural and/or educational background can be seen in the misclassification results in Table 4. Note, for ex-ample, that when Hindi speakers are miscatego-rized, they are overwhelmingly identified as Telugu speakers and vice versa. Importantly, Hindi and Telugu are both languages of India, but they belong to separate language families. Thus, L1 in-fluence appears to overlap with other background variables that, together, allow texts to be grouped reliably. To the extent that this is true, the term NLI might be somewhat misleading. Clearly, NLI research has the potential to contribute a great deal to the understanding of crosslinguistic influence, but it of course also needs to be combined with other types of evidence that demonstrate L1 influ-ence (see Jarvis, 2012). Acknowledgments The authors wish to thank the organizers of the 2013 NLI Shared Task for putting together this valuable event and for promptly responding to all questions and concerns raised throughout the pro-cess. We also wish to acknowledge the support of the Belgian Fund for Scientific Research (F.R.S-FNRS); Yves Bestgen is a Research Associate with the F.R.S-FNRS. Additionally, we acknowledge the support of the University of Bergen?s ASKeladden Project, which is funded by the Nor-wegian Research Council (NFR). References  Yves Bestgen. 2012. DEFT2009 : essais d'optimisation d'une proc?dure de base pour la t?che 1. In Cyril Grouin and Dominic Forest (Eds.), Exp?rimentations et ?valuations en fouille de textes : un panorama des campagnes DEFT (pp. 135?151). Hermes Lavoisier, Paris, France. 
117
Yves Bestgen, Sylviane Granger, and Jennifer Thewis-sen. 2012. Error patterns and automatic L1 identifica-tion. In Scott Jarvis and Scott Crossley (Eds.), Approaching Language Transfer through Text Clas-sification: Explorations in the Detection-based Ap-proach (pp. 127?153). Multilingual Matters, Bristol, UK. Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife Cahill, and Martin Chodorow. 2013. TOEFL11: A Corpus of Non-Native English. Educa-tional Testing Service, Princeton, NJ. Serhiy Bykh and Detmar Meurers. 2012. Native lan-guage identification using recurring n-grams?Investigating abstraction and domain dependence. Proceedings of COLING 2012: Technical Papers (pp. 425-440). Susan Dumais 1991. Improving the retrieval of infor-mation from external sources. Journal Behavior Re-search Methods, Instruments, & Computers, 23:229?236. Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Ma-chine Learning Research, 9:1871?1874. (LIBLINEAR available at http://www.csie.ntu.edu.tw/~cjlin/liblinear). Sylviane Granger, Estelle Dagneaux, and Fanny Meunier. 2009. The International Corpus of Learner English: Handbook and CD-ROM, version 2. Presses Universitaires de Louvain, Louvain-la-Neuve, Bel-gium. Scott Jarvis. 2011. Data mining with learner corpora: Choosing classifiers for L1 detection. In Fanny Meunier, Sylvie De Cock, Ga?tanelle Gilquin, and Magali Paquot (Eds.), A Taste for Corpora: In Honor of Sylviane Granger (pp. 127?154). Benjamins, Am-sterdam. Scott Jarvis. 2012. The detection-based approach: An overview. In Scott Jarvis and Scott Crossley (Eds.), Approaching Language Transfer through Text Clas-sification: Explorations in the Detection-based Ap-proach (pp. 1?33). Multilingual Matters, Bristol, UK. Scott Jarvis and Scott Crossley. 2012. Approaching Language Transfer through Text Classification: Ex-plorations in the Detection-based Approach. Multi-lingual Matters, Bristol, UK. Scott Jarvis and Magali Paquot. 2012. Exploring the role of n-grams in L1 identification. In Scott Jarvis and Scott Crossley (Eds.), Approaching Language Transfer through Text Classification: Explorations in the Detection-based Approach (pp. 71?105). Multi-lingual Matters, Bristol, UK.  Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005. Automatically determining an anonymous au-thor?s native language. ISI (pp. 209?217). 
Helmut Schmid. 1995. Improvements in part-of-speech tagging with an application to German. Proceedings of the ACL SIGDAT-Workshop. Dublin, Ireland. Laura Mayfield Tomokiyo and Rosie Jones. 2001. You?re not from ?round here, are you? Na?ve Bayes detection of non-native utterance text. Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Lingusitics (NAACL ?01). The Association for Computational Linguistics, Cambridge, MA. Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow. 2012. Native tongues, lost and found: Resources and empirical evaluations in native lan-guage identification. Proceedings of COLING 2012: Technical Papers (pp. 2585?2602). Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. Summary report on the first shared task on na-tive language identification. Proceedings of the Eight Workshop on Building Educational Applications Us-ing NLP. Association for Computational Linguistics, Atlanta, GA. Oren Tsur and Ary Rappoport. 2007. Using classifier features for studying the effect of native language on the choice of written second language words. Pro-ceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition (pp. 9?16). As-sociation for Computational Linguistics, Prague, Czech Republic. 
118
