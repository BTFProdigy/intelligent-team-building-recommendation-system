Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 19?24,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Combining Source and Target Language Information for 
Name Tagging of Machine Translation Output 
 
 
Shasha Liao 
New York University 
715 Broadway, 7th floor 
New York, NY 10003 USA 
  liaoss@cs.nyu.edu 
 
 
 
Abstract 
A Named Entity Recognizer (NER) generally 
has worse performance on machine translated 
text, because of the poor syntax of the MT 
output and other errors in the translation. As 
some tagging distinctions are clearer in the 
source, and some in the target, we tried to 
integrate the tag information from both source 
and target to improve target language tagging 
performance, especially recall. 
In our experiments with Chinese-to-English 
MT output, we first used a simple merge of the 
outputs from an ET (Entity Translation) system 
and an English NER system, getting an absolute 
gain of 7.15% in F-measure, from 73.53% to 
80.68%. We then trained an MEMM module to 
integrate them more discriminatively, and got a 
further average gain of 2.74% in F-measure, 
from 80.68% to 83.42%.  
1 Introduction 
Because of the growing multilingual environment 
for NLP, there is an increasing need to be able to 
annotate and analyze the output of machine 
translation (MT) systems.  But treating this task as 
one of processing ?ordinary text? can lead to poor 
results.  We examine this problem with respect to 
the name tagging of English text.   
A Named Entity Recognizer (NER) trained on 
an English corpus does not have the same 
performance when applied to machine-translated 
text. From our experiments on NIST 05 Chinese-
to-English MT evaluation data, when we used the 
same English NER to tag the reference translation 
and the MT output, the F-measure was 81.38% for 
the reference but only 73.53% for the MT output. 
There are two primary reasons for this. First, the 
performance of current translation systems is not 
very good, and so the output is quite different from 
Standard English text. The fluency of the translated 
text will be poor, and the context of a named entity 
may be weird. Second, the translated text has some 
foreign names which are hard for the English NER 
to recognize, even if they are well translated by the 
MT system, because such names appear very 
infrequently in the English training corpus.  
Training an NER on MT output does not seem 
to be an attractive solution. It may take a lot of 
time to manually annotate a large amount of 
training data, and this labor may have to be 
repeated for a new MT system or even a new 
version of an existing MT system.  Furthermore, 
the resulting system may still not work well, in so 
far as the translation is not good and information is 
somehow distorted. In fact, sometimes the 
meanings of the translated sentences are hard to 
decipher unless we check the source language or 
get a human translated document as reference. As a 
result, we need source language information to aid 
the English NER. 
However, it is also not enough to rely entirely 
on the source language NE results and map them 
onto the translated English text.  First, the word 
alignment from source language to English 
generated by the MT system may not be accurate, 
leading to problems in mapping the Chinese name 
tags. Second, the translated text is not exactly same 
as the source language because there may be 
information missed or added. For example, the 
Chinese phrase ????
?
?, which is not a name 
in Chinese, and should be literally translated as 
19
?the subway in Hong Kong?, may end up being 
translated to ?mtrc?, the abbreviation of ?The Mass 
Transit Railway Corporation?, which is an 
organization in Hong Kong (and so should get a 
name tag in English).  
If we can use the information from both the 
source language and the translated text, we cannot 
only find the named entities missed by the English 
NER, but also modify incorrect boundaries in the 
English results which are caused by the bad 
content. However, using word alignment to map 
the source language information into the English 
text is problematic, for two reasons: First, the word 
alignment produced by machine translation is 
typically not very good, with a Chinese-English 
AER (alignment error rate) of about 40% (Deng 
and William 2005). So just using word alignment 
to map the information would introduce a lot of 
noise. Second, in the case of function words in 
English which have no corresponding realization in 
Chinese, traditional word alignment would align 
the function word with another Chinese 
constituent, such as a name, which could lead to 
boundary errors in tagging English names. We 
have therefore used an alternative method to fetch 
the source language information for information 
extraction, which is called Entity Translation and is 
described in Section 3. 
2 Motivation 
When we use the English NER to annotate the 
translated text, we find that the performance is not 
as good as English texts. This is due to several 
types of problems. 
2.1 Bad name contexts 
Producing correct word order is very hard for a 
phrase-based MT system, particularly when 
translating between two such disparate languages, 
and there are still a lot of Chinese syntax structures 
left in translated text, which are usually not regular 
English expressions. As a result, it is hard for the 
English NER to detect names in these contexts.1 
Ex. 1. annan said, "kumaratunga president 
personally against him to areas under guerrilla 
control field visit because it feared the rebels 
will use his visit as a political chip" 
                                                 
1
 The MT system we used generates monocase translations, so 
we show all the translations in lower case. 
It is hard to recognize from this example that 
kumaratunga is a person name unless we are 
already familiar with this name or realize this is a 
normal Chinese expression structure, although not 
an English one. 
Ex. 2. A reporter from shantou <ORG2> 
university school of medicine</ORG>, faculty 
of medicine, university of <GPE>hong 
kong</GPE>, <ORG>influenza research 
center</ORG> was informed that ?... 
Here source language information can help fix 
incorrect name boundaries assigned by the English 
NER, especially from a messy context. In Example 
3, the source language tagger can tell us that 
?shantou university? and ?university of hong 
kong? are two named entities, allowing us to fix 
the wrong name boundaries of the English NER. 
2.2 Bad translations 
There are cases where the MT system does not 
recognize there is a name and translates it as 
something else, and if we do not refer to the source 
language, we sometimes cannot understand the 
sentence, or annotate it. 
Ex. 3. xinhua shanghai , january 1 
(<ORG>feng yizhen su lofty</ORG>) snow , 
frozen , and the shanghai airport staff in snow 
and inalienable . 
The translation system does not output the names 
correctly, and only when we look at the Chinese 
sentence can we know that there are two person 
names here, one is ?feng yizhen?, and the other is 
?su lofty?, where the second one is translated 
incorrectly. English NER treats the whole as an 
ORGANIZATION as there is no punctuation to 
separate the two names. 
2.3 Unknown foreign names 
There are many Chinese GPE and PERSON names 
which are missed because they appear rarely in 
English text, especially city, county or even 
province names, and so are hard for English NER 
to detect or classify. However, on the Chinese side, 
they may be common names and so easily tagged. 
                                                 
2
 We use the entity types of ACE (the Automatic Content 
Extraction evaluation) for name types.  Here ORG = 
?ORGANIZATION? is the tag for an organization; GPE = 
?Geo-Political Entity? is the tag for a location with a 
government; other locations (e.g., ?Sahara Desert?) are tagged 
as LOCATION. 
20
Ex. 4. At present, shishi city in the province to 
achieve a village public transportation, village 
water ; village of cable television . 
The city names in examples 4 are famous in 
Chinese but do not appear much in English text, 
and so are missed by the English NER; however, a 
Chinese NER would be able to tag them as named 
entities. 
3 Entity Translation System 
The MT pipeline we employ begins with an Entity 
Translation (ET) system which identifies and 
translates the names in the text (Heng Ji et al, 
2007).  This system runs a source-language NER 
(based on an HMM) and then uses a variety of 
strategies to translate the names it identifies.  One 
strategy, for example, uses a corpus-trained name 
transliteration component coupled with a target 
language model to select the best transliteration.  
The source text, annotated with name translations, 
is then passed to a statistical, phrase-based MT 
system (Zens and Ney, 2004). Depending on its 
phrase table and language model, this name-aware 
MT system would decide whether to accept the 
translation provided by ET. Experiments show that 
the MT system with ET pre-processing can 
produce better translations than the MT system 
alone, with 17% relative error reduction on overall 
name translation. 
The strategy combining multiple transliterations 
and selection based on a language model is 
particularly effective for foreign (non-Chinese) 
person names rendered in Chinese.  If these names 
did not appear in the bilingual training material, 
they would be mistranslated by an MT system 
without ET.  These names are often also difficult 
for the English tagger, so ET can benefit both 
translation and name recognition. 
For each name tagged by ET, we see if the 
translation string proposed by ET appears in the 
translation produced by the MT system.  If so, we 
use the ET output to assign an ?ET name type? to 
that string in the translation.  This approach avoids 
the problems of using word alignments from the 
MT system; in particular, the alignment of function 
words in English with names in Chinese. 
4 Integrating source and target 
information 
We first try a very simple merge method to see 
how much gain can be gotten by simply combining 
the two sources. After that, we describe a corpus-
trained model which addresses some of the tag 
conflict situations and gets additional gains.  
4.1 Results from English NER and ET 
First, we analyzed the English NER and ET output 
to see the named entity distribution of the two 
sources. We focus on the differences between them 
because when they agree, we can expect little 
improvement from using source language 
information.  In the nist05 data, we find 1893 
named entities in the English NER output (target 
language part) and 1968 named entities in the ET 
output (source language part); 1171 of them are the 
same. This means that 38.14% of the names tagged 
in the target language and 40.5% of those in the 
source language do not have a corresponding tag in 
the other language, which suggests that the source 
and target NER may have different strengths on 
name tagging.   
  We checked the names which are tagged 
differently, and there are 347 correct names from 
ET missed by English NER and 418 from English 
NER missed by ET.  
4.2 Simple Merge 
First, in order to see if the ET system can really 
help the English NER, we do a simple merge 
experiment, which just adds the named entities 
extracted from the ET system into the English 
NER results, so long as there is no conflict 
between them (i.e., so long as the ET-tagged name 
does not overlap an English NE-tagged name). 
Our experiments show that this simple method 
can improve the English NER result substantially 
(Table 5-1), especially for recall, confirming our 
intuition.  
We checked the errors produced by this simple 
merge method, and divided them into four types. 
1. Missed by both sources. 
2. Missed by one source and erroneously tagged 
by the other 
3. Erroneously tagged by both sources 
4. Conflict situations where the English NE-
tagged name is wrong but the ET-tagged name 
is correct. 
21
Although there is not much we can do for the first 
three error types, we can address the last error type 
by some intelligent learning method. In NIST05 
data, there are 261 names which have conflicts, 
and we can get more gains here. 
There are two kinds of conflicts: A type conflict 
which occurs when the ET and English NER tag 
the same named entity but give it different types; 
and a boundary conflict which occurs when there is 
a tag overlap between English NER and ET. We 
treat these two kinds of conflict differently by 
using different features to indicate them.  
4.3 Maximum Entropy Markov Model 
We use a MEMM (Maximum Entropy Markov 
Model) as our tagging model. An MEMM is a 
variation on traditional Hidden Markov Models 
(HMM). Like an HMM, it attempts to characterize 
a string of tokens as a most likely set of transitions 
through a Markov model. The MEMM allows 
observations to be represented as arbitrary 
overlapping features (such as word, capitalization, 
formatting, part-of-speech), and defines the 
conditional probability of state sequences given 
observation sequences. It does this by using the 
maximum entropy framework to fit a set of 
exponential models that represent the probability 
of a state given an observation and the previous 
state (McCallum et al 2000).  
In our experiment, we train the maximum 
entropy framework at the token level, and use the 
BIO types as the states to be predicted. There are 
four entity types: PERSON, ORGANIZATION, 
GPE and LOCATION, and so a total of 9 states. 
4.4 Feature Sets for MEMM 
In our experiment, we are interested not only in 
training a module, but also in measuring the 
different performance for different scales of 
training corpora. If a small annotated corpus can 
get reasonable gain, this method for combining 
taggers will be much more practical.  
As a result, we first build a small feature set and 
enlarge it by adding more features, expecting that 
the small feature set may get better performance 
with a small training corpus. 
 
Set 1: Features Focusing on Current Tag and 
Previous State Information 
We first try to use few features to see how much 
gain we can get if we only consider the tag 
information from ET and English NER, and the 
previous state. These features are: 
F1: current token?s type in ET  
F2: current token?s type in English NER 
F3: Feature1+Feature2 
F4: if there is a type conflict + ET type + 
English NER type 
F5: if there is a type conflict +ET type 
confidence + English NER confidence 
F6: if there is a boundary conflict + ET type + 
English NER type 
F7: if there is a boundary conflict + ET token 
confidence + English NER confidence 
F8: state for the previous token 
F4 and F5 are used to help resolve the type 
conflicts, and F6 and F7 to resolve boundary 
conflicts. When there is a conflict, we need the 
confidence information from both ET and English 
NER to indicate which side to choose.  
The English NER reports a margin, which can 
be used to gauge tag confidence. The margin is the 
difference in log probability between the top 
tagging hypothesis and a hypothesis which assigns 
the name a different NE tag, or no NE tag. We use 
this as the confidence of English NER output. 
For ET output, the situation is more 
complicated. We use different confidence methods 
for type and boundary conflicts. For type conflicts, 
we use the source of the ET translation as the ?type 
confidence?, for example, if the ET result comes 
from a person name list, the output is probably 
correct. For boundary conflicts, as the ET system 
uses some pruning strategy to fix the boundary 
errors in word alignment, and the translation 
procedure contains several disparate components 
which produce different kind of confidence 
measure, it is not reasonable to use Chinese NER 
confidence as the confidence estimate. As a result, 
we check if the token is capitalized in ET 
translation, and treat it as the ?token confidence?. 
 
Set 2: Set 1 + Current Token Information  
F9: current token + ET type+ English NER 
type 
Token information can be used to predict the result 
when there is a conflict, as the conflict reason 
varies and in some cases without knowing the 
token itself, it is hard to know the right choice. As 
a result, we add the current token feature but this is 
the only place we use token information. 
 
22
Set 3: Set2 + Sequence Information 
Our experiments showed some performance gain 
with only the current token features and the 
previous state, but we still wanted to see if 
additional features ? such as information on the 
previous and following tokens ? would help. To 
this end, we added such features, while still 
retaining our focus on the ET and English NER 
information: 
F10: English NER result of the current token + 
that of the previous token  
F11: ET result of the current token + ET result 
of the previous token. 
F12: English NER result of the current token + 
that of the next token. 
F13: ET result of the current token + that of 
the next token. 
5 Experiment 
The experiment was carried out on the Chinese 
part of the NIST 05 machine translation evaluation 
(NIST05) and NIST 04 machine translation 
evaluation (NIST04) data, where NISTT05 
contains 100 documents and NIST04 contains 200 
documents. We annotated all the data in NIST05 
and 120 documents for NIST04 for our 
experiment.  
The ET system used a Chinese HMM-based 
NER trained on 1,460,648 words; the English 
name tagger was also HMM-based and trained on 
450,000 words.  
First, we want to see the result with very small 
training data, and so divided the NIST05 data into 
5 subsets, each containing 20 documents. We ran a 
cross validation experiment on this small corpus, 
with 4 subsets as training data and 1 as testing 
data. We refer to this configuration as Corpus13. 
Second, to see whether increasing the training 
data would appreciably influence the result, we 
added the annotated NIST04 data into the training 
corpus, and we call this configuration Corpus2. 
                                                 
3
 We conducted some experiments with a small corpus in 
which we relied on the alignment information from the MT 
system, but the results were much worse than using the ET 
output.  Simple merge using alignment yielded a name tagger 
F score of 73.34% (1.42% worse than the baseline, 75.76%), 
while ET F score of 81.23%; MEMM with minimal features 
using alignment yielded an improvement of 1.7% (vs. 7.9% 
using ET). 
 
Figure 1. Flow chart of our system 
5.1 Simple Merge Result 
The simple merge method gets a significant F-
measure gain of 7.15% from the English NER 
baseline, which confirms our intuition that some 
named entities are easy to tag in source language 
and others in target language. This represents 
primarily a significant recall improvement, 14.37%. 
 
 NER baseline Simple Merge 
P 
85.68 82.70 
R 
64.39 78.76 
F 
73.53 80.68 
Table 1. Simple merge method on Corpus1 (100 documents) 
5.2 Integrating Results on Corpus1 
On this small training corpus, we test each subset 
with other subsets as training data, and calculate 
the total performance on the whole corpus. The 
best result comes from Set2 instead of Set3, 
presumably because the training data is too small 
to handle the richer model of Set3. Our experiment 
shows that we can get 1.9% gain over simple 
merge method with Set 2 using 80 documents as 
training data. 
 
 Simple Merge Set1 Set2 Set3 
P 
82.70 
84.73 84.72 84.48 
R 
78.76 
78.01 80.55 80.15 
F 
80.68 
81.23 82.58 82.26 
English NE 
Integration 
Procedure 
ET 
Chinese NE 
English Text 
Final Tagged Text 
ET-Tagged Text 
NE-Tagged Text 
Chinese Text 
MT 
23
Table 2. Results on Corpus1, which contains 100 documents, 
with 80 documents used for training at each fold. 
5.3 Integrating Results on Corpus2 
On this corpus, every training data set contains 200 
documents, and we can get a gain of 2.74% over 
the simple merge method.  With the larger training 
set, the richer model (Set 3) now outperforms the 
others. 
 
 Simple Merge Set1 Set2 Set3 
P 
82.70 
85.04 85.15 85.78 
R 
78.76 
78.09 80.59 81.18 
F 
80.68 
81.42 82.81 83.42 
Table 3. Result on Corpus2 (220 documents), with 200 
documents used for training at each fold of cross-validation. 
 
On corpus2, Using a Wilcoxon Matched-Pairs test, 
with a 10-fold division, all the sets perform 
significantly better (in F-measure) than the simple 
merge at a 95% confidence level. 
6 Prior Work 
Huang and Vogel (2002) describe an approach to 
extract a named entity translation dictionary from a 
bilingual corpus while concurrently improving the 
named entity annotation quality. They use a 
statistical alignment model to align the entities and 
iteratively extract the name pairs with higher 
alignment probability and treat them as global 
information to improve the monolingual named 
entity annotation quality for both languages. Using 
this iterative method, they get a smaller but cleaner 
named entity translation dictionary and improve 
the annotation F-measure from 70.03 to 78.15 for 
Chinese and 73.38 to 81.46 in English.  This work 
is similar in using information from the source 
language (in this case mediated by the word 
alignment) to improve the target language tagging.  
However, they used bi-texts (with hand-translated, 
relatively high-quality English) and so did not 
encounter the problems, mentioned above, which 
arise with MT output. 
7 Conclusion 
We present an integrated approach to extract the 
named entities from machine translated text, using 
name entity information from both source and 
target language. Our experiments show that with a 
combination of ET and English NER, we can get a 
considerably better NER result than would be 
possible with either alone, and in particular, a large 
improvement in name identification recall. 
MT output poses a challenge for any type of 
language analysis, such as relation or event 
recognition or predicate-argument analysis.  Even 
though MT is improving, this problem is likely to 
be with us for some time.  The work reported here 
indicates how source language information can be 
brought to bear on such tasks. 
The best F-measure in our experiments exceeds 
the score of the English NER on reference text, 
which reflects the intuition that even for well 
translated text, we can still benefit from source 
language information. 
Acknowledgments 
This material is based upon work supported by the 
Defense Advanced Research Projects Agency 
under Contract No. HR0011-06-C-0023, and the 
National Science Foundation under Grant NO. IIS-
0534700. Any opinions, findings and conclusions 
expressed in this material are those of the author 
and do not necessarily reflect the views of the U. S. 
Government. 
References 
Yonggang Deng, Byrne and William J. 2005. HMM 
Word and Phrase Alignment for Statistical Machine 
Translation. Proc. Human Language Technology 
Conference and Empirical Methods in Natural 
Language Processing. 
Fei Huang and Vogel, S. 2002. Improved named entity 
translation and bilingual named entity 
extraction. Proc. Fourth IEEE Int'l. Conf. on 
Multimodal Interfaces. 
A. McCallum, D. Freitag and F. Pereira. 2000. 
Maximum entropy Markov models for information 
extraction and segmentation. Proc. 17th 
International Conf. on Machine Learning. 
Heng Ji, Matthias Blume, Dayne Freitag,Ralph 
Grishman, Shahram Khadivi and Richard Zens. 
2007. NYU-Fair Isaac-RWTH Chinese to English 
Entity Translation 07 System. Proceedings of ACE 
ET 2007 PI/Evaluation Workshop. Washington.  
Richard Zens and Hermann Ney. 2004. Improvements in 
phrase-based statistical Machine Translation. In 
Proc. HLT/NAACL,Boston 
 
24
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 146?154,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Recognition of Logical Relations for English, Chinese and
Japanese in the GLARF Framework
Adam Meyers?, Michiko Kosaka?, Nianwen Xue?, Heng Ji?, Ang Sun?, Shasha Liao? and Wei Xu?
? New York University, ?Monmouth University, ?Brandeis University, ? City University of New York
Abstract
We present GLARF, a framework for repre-
senting three linguistic levels and systems for
generating this representation. We focus on a
logical level, like LFG?s F-structure, but com-
patible with Penn Treebanks. While less fine-
grained than typical semantic role labeling ap-
proaches, our logical structure has several ad-
vantages: (1) it includes all words in all sen-
tences, regardless of part of speech or seman-
tic domain; and (2) it is easier to produce ac-
curately. Our systems achieve 90% for En-
glish/Japanese News and 74.5% for Chinese
News ? these F-scores are nearly the same as
those achieved for treebank-based parsing.
1 Introduction
For decades, computational linguists have paired a
surface syntactic analysis with an analysis represent-
ing something ?deeper?. The work of Harris (1968),
Chomsky (1957) and many others showed that one
could use these deeper analyses to regularize differ-
ences between ways of expressing the same idea.
For statistical methods, these regularizations, in ef-
fect, reduce the number of significant differences be-
tween observable patterns in data and raise the fre-
quency of each difference. Patterns are thus easier
to learn from training data and easier to recognize in
test data, thus somewhat compensating for the spare-
ness of data. In addition, deeper analyses are often
considered semantic in nature because conceptually,
two expressions that share the same regularized form
also share some aspects of meaning. The specific de-
tails of this ?deep? analysis have varied quite a bit,
perhaps more than surface syntax.
In the 1970s and 1980s, Lexical Function Gram-
mar?s (LFG) way of dividing C-structure (surface)
and F-structure (deep) led to parsers such as (Hobbs
and Grishman, 1976) which produced these two lev-
els, typically in two stages. However, enthusiasm
for these two-stage parsers was eclipsed by the ad-
vent of one stage parsers with much higher accu-
racy (about 90% vs about 60%), the now-popular
treebank-based parsers including (Charniak, 2001;
Collins, 1999) and many others. Currently, many
different ?deeper? levels are being manually anno-
tated and automatically transduced, typically using
surface parsing and other processors as input. One
of the most popular, semantic role labels (annota-
tion and transducers based on the annotation) char-
acterize relations anchored by select predicate types
like verbs (Palmer et al, 2005), nouns (Meyers et
al., 2004a), discourse connectives (Miltsakaki et al,
2004) or those predicates that are part of particular
semantic frames (Baker et al, 1998). The CONLL
tasks for 2008 and 2009 (Surdeanu et al, 2008;
Hajic? et al, 2009) has focused on unifying many of
these individual efforts to produce a logical structure
for multiple parts of speech and multiple languages.
Like the CONLL shared task, we link surface lev-
els to logical levels for multiple languages. How-
ever, there are several differences: (1) The logical
structures produced automatically by our system can
be expected to be more accurate than the compara-
ble CONLL systems because our task involves pre-
dicting semantic roles with less fine-grained distinc-
tions. Our English and Japanese results were higher
than the CONLL 2009 SRL systems. Our English F-
scores range from 76.3% (spoken) to 89.9% (News):
146
the best CONLL 2009 English scores were 73.31%
(Brown) and 85.63% (WSJ). Our Japanese system
scored 90.6%: the best CONLL 2009 Japanese score
was 78.35%. Our Chinese system 74.5%, 4 points
lower than the best CONLL 2009 system (78.6%),
probably due to our system?s failings, rather than the
complexity of the task; (2) Each of the languages
in our system uses the same linguistic framework,
using the same types of relations, same analyses of
comparable constructions, etc. In one case, this re-
quired a conversion from a different framework to
our own. In contrast, the 2009 CONLL task puts
several different frameworks into one compatible in-
put format. (3) The logical structures produced by
our system typically connect all the words in the sen-
tence. While this is true for some of the CONLL
2009 languages, e.g., Czech, it is not true about
all the languages. In particular, the CONLL 2009
English and Chinese logical structures only include
noun and verb predicates.
In this paper, we will describe the GLARF frame-
work (Grammatical and Logical Representation
Framework) and a system for producing GLARF
output (Meyers et al, 2001; Meyers, 2008). GLARF
provides a logical structure for English, Chinese and
Japanese with an F-score that is within a few per-
centage points of the best parsing results for that
language. Like LFG?s (LFG) F-structure, our log-
ical structure is less fine-grained than many of the
popular semantic role labeling schemes, but also has
two main advantages over these schemes: it is more
reliable and it is more comprehensive in the sense
that it covers all parts of speech and the resulting
logical structure is a connected graph. Our approach
has proved adequate for three genetically unrelated
natural languages: English, Chinese and Japanese.
It is thus a good candidate for additional languages
with accurate parsers.
2 The GLARF framework
Our system creates a multi-tiered representation in
the GLARF framework, combining the theory un-
derlying the Penn Treebank for English (Marcus et
al., 1994) and Chinese (Xue et al, 2005) (Chom-
skian linguistics of the 1970s and 1980s) with: (2)
Relational Grammar?s graph-based way of repre-
senting ?levels? as sequences of relations; (2) Fea-
ture structures in the style of Head-Driven Phrase
Structure Grammar; and (3) The Z. Harris style goal
of attempting to regularize multiple ways of saying
the same thing into a single representation. Our
approach differs from LFG F-structure in several
ways: we have more than two levels; we have a
different set of relational labels; and finally, our ap-
proach is designed to be compatible with the Penn
Treebank framework and therefore, Penn-Treebank-
based parsers. In addition, the expansion of our the-
ory is governed more by available resources than by
the underlying theory. As our main goal is to use
our system to regularize data, we freely incorporate
any analysis that fits this goal. Over time, we have
found ways of incorporating Named Entities, Prop-
Bank, NomBank and the Penn Discourse Treebank.
Our agenda also includes incorporating the results of
other research efforts (Pustejovsky et al, 2005).
For each sentence, we generate a feature structure
(FS) representing our most complete analysis. We
distill a subset of this information into a dependency
structure governed by theoretical assumptions, e.g.,
about identifying functors of phrases. Each GLARF
dependency is between a functor and an argument,
where the functor is the head of a phrase, conjunc-
tion, complementizer, or other function word. We
have built applications that use each of these two
representations, e.g., the dependency representation
is used in (Shinyama, 2007) and the FS represen-
tation is used in (K. Parton and K. R. McKeown
and R. Coyne and M. Diab and R. Grishman and
D. Hakkani-Tu?r and M. Harper and H. Ji and W. Y.
Ma and A. Meyers and S. Stolbach and A. Sun and
G. Tu?r and W. Xu and S. Yarman, 2009).
In the dependency representation, each sentence
is a set of 23 tuples, each 23-tuple characterizing up
to three relations between two words: (1) a SUR-
FACE relation, the relation between a functor and an
argument in the parse of a sentence; (2) a LOGIC1
relation which regularizes for lexical and syntac-
tic phenomena like passive, relative clauses, deleted
subjects; and (3) a LOGIC2 relation corresponding
to relations in PropBank, NomBank, and the Penn
Discourse Treebank (PDTB). While the full output
has all this information, we will limit this paper to
a discussion of the LOGIC1 relations. Figure 1 is
a 5 tuple subset of the 23 tuple GLARF analysis of
the sentence Who was eaten by Grendel? (The full
147
L1 Surf L2 Func Arg
NIL SENT NIL Who was
PRD PRD NIL was eaten
COMP COMP ARG0 eaten by
OBJ NIL ARG1 eaten Who
NIL OBJ NIL by Grendel
SBJ NIL NIL eaten Grendel
Figure 1: 5-tuples: Who was eaten by Grendel
Who
eaten
was
by
PRD
S?OBJ
L?OBJ
ARG1
COMP
ARG0
S?SENT
L?SBJ
Grendel
Figure 2: Graph of Who was eaten by Grendel
23 tuples include unique ids and fine-grained lin-
guistic features). The fields listed are: logic1 label
(L1), surface label (Surf), logic2 label (L2), func-
tor (Func) and argument (Arg). NIL indicates that
there is no relation of that type. Figure 2 repre-
sents this as a graph. For edges with two labels,
the ARG0 or ARG1 label indicates a LOGIC2 re-
lation. Edges with an L- prefix are LOGIC1 la-
bels (the edges are curved); edges with S-prefixes
are SURFACE relations (the edges are dashed); and
other (thick) edges bear unprefixed labels represent-
ing combined SURFACE/LOGIC1 relations. Delet-
ing the dashed edges yields a LOGIC1 representa-
tion; deleting the curved edges yields a SURFACE
representation; and a LOGIC2 consists of the edges
labeled ARGO and ARG1 relations, plus the sur-
face subtrees rooted where the LOGIC2 edges ter-
minate. Taken together, a sentence?s SURFACE re-
lations form a tree; the LOGIC1 relations form a
directed acyclic graph; and the LOGIC2 relations
form directed graphs with some cycles and, due to
PDTB relations, may connect sentences to previous
ones, e.g., adverbs like however, take the previous
sentence as one of their arguments.
LOGIC1 relations (based on Relational Gram-
mar) regularize across grammatical and lexical al-
ternations. For example, subcategorized verbal ar-
guments include: SBJect, OBJect and IND-OBJ (in-
direct Object), COMPlement, PRT (Particle), PRD
(predicative complement). Other verbal modifiers
include AUXilliary, PARENthetical, ADVerbial. In
contrast, FrameNet and PropBank make finer dis-
tinctions. Both PP arguments of consulted in John
consulted with Mary about the project bear COMP
relations with the verb in GLARF, but would have
distinct labels in both PropBank and FrameNet.
Thus Semantic Role Labeling (SRL) should be more
difficult than recognizing LOGIC1 relations.
Beginning with Penn Treebank II, Penn Treebank
annotation includes Function tags, hyphenated addi-
tions to phrasal categories which indicate their func-
tion. There are several types of function tags:
? Argument Tags such as SBJ, OBJ, IO (IND-
OBJ), CLR (COMP) and PRD?These are lim-
ited to verbal relations and not all are used in
all treebanks. For example, OBJ and IO are
used in the Chinese, but not the English tree-
bank. These labels can often be directly trans-
lated into GLARF LOGIC1 relations.
? Adjunct Tags such as ADV, TMP, DIR, LOC,
MNR, PRP?These tags often translate into a
single LOGIC1 tag (ADV). However, some of
these also correspond to LOGIC1 arguments.
In particular, some DIR and MNR tags are re-
alized as LOGIC1 COMP relations (based on
dictionary entries). The fine grained seman-
tic distinctions are maintained in other features
that are part of the GLARF description.
In addition, GLARF treats Penn?s PRN phrasal
category as a relation rather than a phrasal category.
For example, given a sentence like, Banana ketchup,
the agency claims, is very nutritious, the phrase
the agency claims is analyzed as an S(entence) in
GLARF bearing a (surface) PAREN relation to the
main clause. Furthermore, the whole sentence is a
COMP of the verb claims. Since PAREN is a SUR-
FACE relation, not a LOGIC1 relation, there is no
LOGIC1 cycle as shown by the set of 5-tuples in
Figure 3? a cycle only exists if you include both
SURFACE and LOGIC1 relations in a single graph.
Another important feature of the GLARF frame-
work is transparency, a term originating from N.
148
L1 Surf L2 Func Arg
NIL SBJ ARG1 is ketchup
PRD PRD ARG2 is nutritious
SBJ NIL NIL nutritious Ketchup
ADV ADV NIL nutritious very
N-POS N-POS NIL ketchup Banana
NIL PAREN NIL is claims
SBJ SBJ ARG0 claims agency
Q-POS Q-POS NIL agency the
COMP NIL ARG1 claims is
Figure 3: 5-tuples: Banana Ketchup, the agency claims,
is very nutritious
L1 Surf L2 Func Arg
SBJ SBJ ARG0 ate and
OBJ OBJ ARG1 ate box
CONJ CONJ NIL and John
CONJ CONJ NIL and Mary
COMP COMP NIL box of
Q-POS Q-POS NIL box the
OBJ OBJ NIL of cookies
Figure 4: 5-tuples: John and Mary ate the box of cookies
Sager?s unpublished work. A relation between two
words is transparent if: the functor fails to character-
ize the selectional properties of the phrase (or sub-
graph in a Dependency Analysis), but its argument
does. For example, relations between conjunctions
(e.g., and, or, but) and their conjuncts are transparent
CONJ relations. Thus although and links together
John and Mary, it is these dependents that deter-
mine that the resulting phrase is noun-like (an NP
in phrase structure terminology) and sentient (and
thus can occur as the subject of verbs like ate). An-
other common example of transparent relations are
the relations connecting certain nouns and the prepo-
sitional objects under them, e.g., the box of cookies
is edible, because cookies are edible even though
boxes are not. These features are marked in the
NOMLEX-PLUS dictionary (Meyers et al, 2004b).
In Figure 4, we represent transparent relations, by
prefixing the LOGIC1 label with asterisks.
The above description most accurately describes
English GLARF. However, Chinese GLARF has
most of the same properties, the main exception be-
ing that PDTB arguments are not currently marked.
For Japanese, we have only a preliminary represen-
tation of LOGIC2 relations and they are not derived
from PropBank/NomBank/PDTB.
2.1 Scoring the LOGIC1 Structure
For purposes of scoring, we chose to focus on
LOGIC1 relations, our proposed high-performance
level of semantics. We scored with respect to: the
LOGIC1 relational label, the identity of the functor
and the argument, and whether the relation is trans-
parent or not. If the system output differs in any of
these respects, the relation is marked wrong. The
following sections will briefly describe each system
and present an evaluation of its results.
The answer keys for each language were created
by native speakers editing system output, as repre-
sented similarly to the examples in this paper, al-
though part of speech is included for added clar-
ity. In addition, as we attempted to evaluate logi-
cal relation (or dependency) accuracy independent
of sentence splitting. We obtained sentence divi-
sions from data providers and treebank annotation
for all the Japanese and most of the English data, but
used automatic sentence divisions for the English
BLOG data. For the Chinese, we omitted several
sentences from our evaluation set due to incorrect
sentence splits. The English and Japanese answer
keys were annotated by single native speakers ex-
pert in GLARF. The Chinese data was annotated by
several native speakers and may have been subject
to some interannotator agreement difficulties, which
we intend to resolve in future work. Currently, cor-
recting system output is the best way to create an-
swer keys due to certain ambiguities in the frame-
work, some of which we hope to incorporate into fu-
ture scoring procedures. For example, consider the
interpretation of the phrase five acres of land in Eng-
land with respect to PP attachment. The difference
in meaning between attaching the PP in England
to acres or to land is too subtle for these authors?
we have difficulty imagining situations where one
statement would be accurate and the other would
not. This ambiguity is completely predictable be-
cause acres is a transparent noun and similar ambi-
guities hold for all such cases where a transparent
noun takes a complement and is followed by a PP
attachment. We believe that a more complex scor-
ing program could account for most of these cases.
149
Similar complexities arise for coordination and sev-
eral other phenomena.
3 English GLARF
We generate English GLARF output by applying a
procedure that combines:
1. The output of the 2005 version of the Charniak
parser described in (Charniak, 2001), which
label precision and recall scores in the 85%
range. The updated version of the parser seems
to perform closer to 90% on News data and per-
form lower on other genres. That performance
would reflect reports on other versions of the
Charniak parser for which statistics are avail-
able (Foster and van Genabith, 2008).
2. Named entity (NE) tags from the JET NE sys-
tem (Ji and Grishman, 2006), which achieves
F-scores ranging 86%-91% on newswire for
both English and Chinese (depending on
Epoch). The JET system identifies seven
classes of NEs: Person, GPE, Location, Orga-
nization, Facility, Weapon and Vehicle.
3. Machine Readable dictionaries: COMLEX
(Macleod et al, 1998), NOMBANK dictio-
naries (from http://nlp.cs.nyu.edu/
meyers/nombank/) and others.
4. A sequence of hand-written rules (citations
omitted) such that: (1) the first set of rules con-
vert the Penn Treebank into a Feature Structure
representation; and (2) each rule N after the
first rule is applied to an entire Feature Struc-
ture that is the output of rule N ? 1.
For this paper, we evaluated the English output for
several different genres, all of which approximately
track parsing results for that genre. For written
genres, we chose between 40 and 50 sentences.
For speech transcripts, we chose 100 sentences?we
chose this larger number because a lot of so-called
sentences contained text with empty logical de-
scriptions, e.g., single word utterances contain no
relations between pairs of words. Each text comes
from a different genre. For NEWS text, we used 50
sentences from the aligned Japanese-English data
created as part of the JENAAD corpus (Utiyama
Genre Prec Rec F
NEWS 731815 = 89.7%
715
812 = 90.0% 89.9%
BLOG 704844 = 83.4%
704
899 = 78.3% 80.8%
LETT 392434 = 90.3%
392
449 = 87.3% 88.8%
TELE 472604 = 78.1%
472
610 = 77.4% 77.8%
NARR 732959 = 76.3%
732
964 = 75.9% 76.1%
Table 1: English Aggregate Scores
Corpus Prec Rec F Sents
NEWS 90.5% 90.8% 90.6% 50
BLOG 84.1% 79.6% 81.7% 46
LETT 93.9% 89.2% 91.4% 46
TELE 81.4% 83.2% 84.9% 103
NARR 77.1% 78.1% 79.5% 100
Table 2: English Score per Sentence
and Isahara, 2003); the web text (BLOGs) was
taken from some corpora provided by the Linguistic
Data Consortium through the GALE (http:
//projects.ldc.upenn.edu/gale/) pro-
gram; the LETTer genre (a letter from Good Will)
was taken from the ICIC Corpus of Fundraising
Texts (Indiana Center for Intercultural Communi-
cation); Finally, we chose two spoken language
transcripts: a TELEphone conversation from
the Switchboard Corpus (http://www.ldc.
upenn.edu/Catalog/readme_files/
switchboard.readme.html) and one NAR-
Rative from the Charlotte Narrative and Conversa-
tion Collection (http://newsouthvoices.
uncc.edu/cncc.php). In both cases, we
assumed perfect sentence splitting (based on Penn
Treebank annotation). The ICIC, Switchboard
and Charlotte texts that we used are part of the
Open American National Corpus (OANC), in
particular, the SIGANN shared subcorpus of the
OANC (http://nlp.cs.nyu.edu/wiki/
corpuswg/ULA-OANC-1) (Meyers et al, 2007).
Comparable work for English includes: (1) (Gab-
bard et al, 2006), a system which reproduces the
function tags of the Penn Treebank with 89% accu-
racy and empty categories (and their antecedents)
with varying accuracies ranging from 82.2% to
96.3%, excluding null complementizers, as these are
theory-internal and have no value for filling gaps.
(2) Current systems that generate LFG F-structure
150
such as (Wagner et al, 2007) which achieve an F
score of 91.1 on the F-structure PRED relations,
which are similar to our LOGIC1 relations.
4 Chinese GLARF
The Chinese GLARF program takes a Chinese
Treebank-style syntactic parse and the output of a
Chinese PropBanker (Xue, 2008) as input, and at-
tempts to determine the relations between the head
and its dependents within each constituent. It does
this by first exploiting the structural information
and detecting six broad categories of syntactic rela-
tions that hold between the head and its dependents.
These are predication, modification, complementa-
tion, coordination, auxiliary, and flat. Predication
holds at the clause level between the subject and the
predicate, where the predicate is considered to be
the head and the subject is considered to the depen-
dent. Modification can also hold mainly within NPs
and VPs, where the dependents are modifiers of the
NP head or adjuncts to the head verb. Coordination
holds almost for all phrasal categories where each
non-punctuation child within this constituent is ei-
ther conjunction or a conjunct. The head in a co-
ordination structure is underspecified and can be ei-
ther a conjunct or a conjunction depending on the
grammatical framework. Complementation holds
between a head and its complement, with the com-
plement usually being a core argument of the head.
For example, inside a PP, the preposition is the head
and the phrase or clause it takes is the dependent. An
auxiliary structure is one where the auxiliary takes
a VP as its complement. This structure is identi-
fied so that the auxiliary and the verb it modifies can
form a verb group in the GLARF framework. Flat
structures are structures where a constituent has no
meaningful internal structure, which is possible in a
small number of cases. After these six broad cate-
gories of relations are identified, more fine-grained
relation can be detected with additional information.
Figure 5 is a sample 4-tuple for a Chinese translation
of the sentence in figure 3.
For the results reported in Table 3, we used the
Harper and Huang parser described in (Harper and
Huang, Forthcoming) which can achieve F-scores
as high as 85.2%, in combination with informa-
tion about named entities from the output of the
Figure 5: Agency claims, Banana Ketchup is very have
nutrition DE.
JET Named Entity tagger for Chinese (86%-91% F-
measure as per section 3). We used the NE tags to
adjust the parts of speech and the phrasal boundaries
of named entities (we do the same with English).
As shown in Table 3, we tried two versions of the
Harper and Huang parser, one which adds function
tags to the output and one that does not. The Chinese
GLARF system scores significantly (13.9% F-score)
higher given function tagged input, than parser out-
put without function tags. Our current score is about
10 points lower than the parser score. Our initial er-
ror analysis suggests that the most common forms
of errors involve: (1) the processing of long NPs;
(2) segmentation and POS errors; (3) conjunction
scope; and (4) modifier attachment.
5 Japanese GLARF
For Japanese, we process text with the KNP parser
(Kurohashi and Nagao, 1998) and convert the output
into the GLARF framework. The KNP/Kyoto Cor-
pus framework is a Japanese-specific Dependency
framework, very different from the Penn Treebank
framework used for the other systems. Process-
ing in Japanese proceeds as follows: (1) we pro-
cess the Japanese with the Juman segmenter (Kuro-
151
Type Prec Rec F
No Function Tags Version
Aggr 8431374 = 61.4%
843
1352 = 62.4% 61.8%
Aver 62.3% 63.5% 63.6%
Function Tags Version
Aggr 10311415 = 72.9%
1031
1352 = 76.3% 74.5%
Aver 73.0% 75.3% 74.9%
Table 3: 53 Chinese Newswire Sentences: Aggregate and
Average Sentence Scores
hashi et al, 1994) and KNP parser 2.0 (Kurohashi
and Nagao, 1998), which has reported accuracy of
91.32% F score for dependency accuracy, as re-
ported in (Noro et al, 2005). As is standard in
Japanese linguistics, the KNP/Kyoto Corpus (K)
framework uses a dependency analysis that has some
features of a phrase structure analysis. In partic-
ular, the dependency relations are between bun-
setsu, small constituents which include a head word
and some number of modifiers which are typically
function words (particles, auxiliaries, etc.), but can
also be prenominal noun modifiers. Bunsetsu can
also include multiple words in the case of names.
The K framework differentiates types of dependen-
cies into: the normal head-argument variety, coor-
dination (or parallel) and apposition. We convert
the head-argument variety of dependency straight-
forwardly into a phrase consisting of the head and
all the arguments. In a similar way, appositive re-
lations could be represented using an APPOSITIVE
relation (as is currently done with English). In the
case of bunsetsu, the task is to choose a head and
label the other constituents?This is very similar to
our task of labeling and subdividing the flat noun
phrases of the English Penn Treebank. Conjunction
is a little different because the K analysis assumes
that the final conjunct is the functor, rather than a
conjunction. We automatically changed this analy-
sis to be the same as it is for English and Chinese.
When there was no actual conjunction, we created a
theory-internal NULL conjunction. The final stages
include: (1) processing conjunction and apposition,
including recognizing cases that the parser does not
recognize; (2) correcting parts of speech; (3) label-
ing all relations between arguments and heads; (4)
recognizing and labeling special constituent types
Figure 6: It is the state?s duty to protect lives and assets.
Type Prec Rec F
Aggr 764843 = 91.0%
764
840 = 90.6% 90.8%
Aver 90.7% 90.6% 90.6%
Table 4: 40 Japanese Sentences from JENAA Corpus:
Aggregate and Average Sentence Scores
such as Named Entities, double quote constituents
and number phrases (twenty one); (5) handling com-
mon idioms; and (6) processing light verb and cop-
ula constructions.
Figure 6 is a sample 4-tuple for a Japanese
sentence meaning It is the state?s duty to protect
lives and assets. Conjunction is handled as dis-
cussed above, using an invisible NULL conjunction
and transparent (asterisked) logical CONJ relations.
Copulas in all three languages take surface subjects,
which are the LOGIC1 subjects of the PRD argu-
ment of the copula. We have left out glosses for the
particles, which act solely as case markers and help
us identify the grammatical relation.
We scored Japanese GLARF on forty sentences of
the Japanese side of the JENAA data (25 of which
are parallel with the English sentences scored). Like
the English, the F score is very close to the parsing
scores achieved by the parser.
152
6 Concluding Remarks and Future Work
In this paper, we have described three systems
for generating GLARF representations automati-
cally from text, each system combines the out-
put of a parser and possibly some other processor
(segmenter, Named Entity Recognizer, PropBanker,
etc.) and creates a logical representation of the sen-
tence. Dictionaries, word lists, and various other
resources are used, in conjunction with hand writ-
ten rules. In each case, the results are very close to
parsing accuracy. These logical structures are in the
same annotation framework, using the same labeling
scheme and the same analysis for key types of con-
structions. There are several advantages to our ap-
proach over other characterizations of logical struc-
ture: (1) our representation is among the most accu-
rate and reliable; (2) our representation connects all
the words in the sentence; and (3) having the same
representation for multiple languages facilitates run-
ning the same procedures in multiple languages and
creating multilingual applications.
The English system was developed for the News
genre, specifically the Penn Treebank Wall Street
Journal Corpus. We are therefore considering
adding rules to better handle constructions that ap-
pear in other genres, but not news. The experi-
ments describe here should go a long way towards
achieving this goal. We are also considering ex-
periments with parsers tailored to particular genres
and/or parsers that add function tags (Harper et al,
2005). In addition, our current GLARF system uses
internal Propbank/NomBank rules, which have good
precision, but low recall. We expect that we achieve
better results if we incorporate the output of state
of the art SRL systems, although we would have to
conduct experiments as to whether or not we can im-
prove such results with additional rules.
We developed the English system over the course
of eight years or so. In contrast, the Chinese and
Japanese systems are newer and considerably less
time was spent developing them. Thus they cur-
rently do not represent as many regularizations. One
obstacle is that we do not currently use subcate-
gorization dictionaries for either language, while
we have several for English. In particular, these
would be helpful in predicting and filling relative
clause and others gaps. We are considering auto-
matically acquiring simple dictionaries by recording
frequently occurring argument types of verbs over
a larger corpus, e.g., along the lines of (Kawahara
and Kurohashi, 2002). In addition, existing Japanese
dictionaries such as the IPAL (monolingual) dictio-
nary (technology Promotion Agency, 1987) or previ-
ously acquired case information reported in (Kawa-
hara and Kurohashi, 2002).
Finally, we are investigating several avenues for
using this system output for Machine Translation
(MT) including: (1) aiding word alignment for other
MT system (Wang et al, 2007); and (2) aiding the
creation various MT models involving analyzed text,
e.g., (Gildea, 2004; Shen et al, 2008).
Acknowledgments
This work was supported by NSF Grant IIS-
0534700 Structure Alignment-based MT.
References
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The
Berkeley FrameNet Project. In Coling-ACL98, pages
86?90.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL 2001, pages 116?123.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Foster and J. van Genabith. 2008. Parser Evaluation
and the BNC: 4 Parsers and 3 Evaluation Metrics. In
LREC 2008, Marrakech, Morocco.
R. Gabbard, M. Marcus, and S. Kulick. 2006. Fully pars-
ing the penn treebank. In NAACL/HLT, pages 184?
191.
D. Gildea. 2004. Dependencies vs. Constituents for
Tree-Based Alignment. In EMNLP, Barcelona.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue, and
Y. Zhang. 2009. The CoNLL-2009 shared task:
Syntactic and semantic dependencies in multiple lan-
guages. In CoNLL-2009, Boulder, Colorado, USA.
M. Harper and Z. Huang. Forthcoming. Chinese Statis-
tical Parsing. In J. Olive, editor, Global Autonomous
Language Exploitation. Publisher to be Announced.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, L. Yung, A. Krasnyan-
skaya, and R. Stewart. 2005. Parsing and Spoken
153
Structural Event. Technical Report, The John-Hopkins
University, 2005 Summer Research Workshop.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley-Interscience, New York.
J. R. Hobbs and R. Grishman. 1976. The Automatic
Transformational Analysis of English Sentences: An
Implementation. International Journal of Computer
Mathematics, 5:267?283.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Sydney,
Australia.
K. Parton and K. R. McKeown and R. Coyne and M. Diab
and R. Grishman and D. Hakkani-Tu?r and M. Harper
and H. Ji and W. Y. Ma and A. Meyers and S. Stol-
bach and A. Sun and G. Tu?r and W. Xu and S. Yarman.
2009. Who, What, When, Where, Why? Comparing
Multiple Approaches to the Cross-Lingual 5W Task.
In ACL 2009.
D. Kawahara and S. Kurohashi. 2002. Fertilization
of Case Frame Dictionary for Robust Japanese Case
Analysis. In Proc. of COLING 2002.
S. Kurohashi and M. Nagao. 1998. Building a Japanese
parsed corpus while improving the parsing system. In
Proceedings of The 1st International Conference on
Language Resources & Evaluation, pages 719?724.
S. Kurohashi, T. Nakamura, Y. Matsumoto, and M. Na-
gao. 1994. Improvements of Japanese Morpholog-
ical Analyzer JUMAN. In Proc. of International
Workshop on Sharable Natural Language Resources
(SNLR), pages 22?28.
C. Macleod, R. Grishman, and A. Meyers. 1998. COM-
LEX Syntax. Computers and the Humanities, 31:459?
481.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Proceed-
ings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004a. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
A. Meyers, R. Reeves, Catherine Macleod, Rachel Szeke-
ley, Veronkia Zielinska, and Brian Young. 2004b. The
Cross-Breeding of Dictionaries. In Proceedings of
LREC-2004, Lisbon, Portugal.
A. Meyers, N. Ide, L. Denoyer, and Y. Shinyama. 2007.
The shared corpora working group report. In Pro-
ceedings of The Linguistic Annotation Workshop, ACL
2007, pages 184?190, Prague, Czech Republic.
A. Meyers. 2008. Using treebank, dictionaries and
glarf to improve nombank annotation. In Proceedings
of The Linguistic Annotation Workshop, LREC 2008,
Marrakesh, Morocco.
E. Miltsakaki, A. Joshi, R. Prasad, and B. Webber. 2004.
Annotating discourse connectives and their arguments.
In A. Meyers, editor, NAACL/HLT 2004 Workshop:
Frontiers in Corpus Annotation, pages 9?16, Boston,
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
T. Noro, C. Koike, T. Hashimoto, T. Tokunaga, and
Hozumi Tanaka. 2005. Evaluation of a Japanese CFG
Derived from a Syntactically Annotated corpus with
Respect to Dependency Measures. In 2005 Workshop
on Treebanks and Linguistic theories, pages 115?126.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
J. Pustejovsky, A. Meyers, M. Palmer, and M. Poe-
sio. 2005. Merging PropBank, NomBank, TimeBank,
Penn Discourse Treebank and Coreference. In ACL
2005 Workshop: Frontiers in Corpus Annotation II:
Pie in the Sky.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In ACL 2008.
Y. Shinyama. 2007. Being Lazy and Preemptive at
Learning toward Information Extraction. Ph.D. the-
sis, NYU.
M. Surdeanu, R. Johansson, A. Meyers, Ll. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Dependen-
cies. In Proceedings of the CoNLL-2008 Shared Task,
Manchester, GB.
Information technology Promotion Agency. 1987. IPA
Lexicon of the Japanese Language for Computers
IPAL (Basic Verbs). (in Japanese).
M. Utiyama and H. Isahara. 2003. Reliable Measures
for Aligning Japanese-English News Articles and Sen-
tences. In ACL-2003, pages 72?79.
J. Wagner, D. Seddah, J. Foster, and J. van Genabith.
2007. C-Structures and F-Structures for the British
National Corpus. In Proceedings of the Twelfth In-
ternational Lexical Functional Grammar Conference,
Stanford. CSLI Publications.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
EMNLP-CoNLL 2007, pages 737?745.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese TreeBank: Phrase Structure Annotation
of a Large Corpus. Natural Language Engineering,
11:207?238.
N. Xue. 2008. Labeling Chinese Predicates with Seman-
tic roles. Computational Linguistics, 34:225?255.
154
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 116?120,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Transducing Logical Relations from Automatic and Manual GLARF
Adam Meyers?, Michiko Kosaka?, Heng Ji?, Nianwen Xue?,
Mary Harper?, Ang Sun?, Wei Xu? and Shasha Liao?
? New York Univ., ?Monmouth Univ., ?Brandeis Univ,, ?City Univ. of New York, ?Johns
Hopkins Human Lang. Tech. Ctr. of Excellence & U. of Maryland, College Park
Abstract
GLARF relations are generated from tree-
bank and parses for English, Chinese and
Japanese. Our evaluation of system out-
put for these input types requires consid-
eration of multiple correct answers.1
1 Introduction
Systems, such as treebank-based parsers (Char-
niak, 2001; Collins, 1999) and semantic role la-
belers (Gildea and Jurafsky, 2002; Xue, 2008), are
trained and tested on hand-annotated data. Evalu-
ation is based on differences between system out-
put and test data. Other systems use these pro-
grams to perform tasks unrelated to the original
annotation. For example, participating systems in
CONLL (Surdeanu et al, 2008; Hajic? et al, 2009),
ACE and GALE tasks merged the results of sev-
eral processors (parsers, named entity recognizers,
etc.) not initially designed for the task at hand.
This paper discusses differences between hand-
annotated data and automatically generated data
with respect to our GLARFers, systems for gen-
erating Grammatical and Logical Representation
Framework (GLARF) for English, Chinese and
Japanese sentences. The paper describes GLARF
(Meyers et al, 2001; Meyers et al, 2009) and
GLARFers and compares GLARF produced from
treebank and parses.
2 GLARF
Figure 1 includes simplified GLARF analyses for
English, Chinese and Japanese sentences. For
each sentence, a GLARFer constructs both a Fea-
ture Structure (FS) representing a constituency
analysis and a set of 31-tuples, each representing
1Support includes: NSF IIS-0534700 & IIS-0534325
Structure Alignment-based MT; DARPA HR0011-06-C-
0023 & HR0011-06-C-0023; CUNY REP & GRTI Program.
This work does not necessarily reflect views of sponsors.
up to three dependency relations between pairs of
words. Due to space limitations, we will focus on
the 6 fields of the 31-tuple represented in Figure 1.
These include: (1) a functor (func); (2) the de-
pending argument (Arg); (3) a surface (Surf) la-
bel based on the position in the parse tree with no
regularizations; (4) a logic1 label (L
?
1) for a re-
lation that reflects grammar-based regularizations
of the surface level. This marks relations for fill-
ing gaps in relative clauses or missing infinitival
subjects, represents passives as paraphrases as ac-
tives, etc. While the general framework supports
many regularizations, the relations actually repre-
sented depends on the implemented grammar, e.g.,
our current grammar of English regularizes across
passives and relative clauses, but our grammars
of Japanese and Chinese do not currently.; (5) a
logic2 label (L2) for Chinese and English, which
represents PropBank, NomBank and Penn Dis-
course Treebank relations; and (6) Asterisks (*)
indicate transparent relations, relations where the
functor inherits semantic properties of certain spe-
cial arguments (*CONJ, *OBJ, *PRD, *COMP).
Figure 1 contains several transparent relations.
The interpretation of the *CONJ relations in the
Japanese example, include not only that the nouns
[zaisan] (assets) and [seimei] (lives) are con-
joined, but also that these two nouns, together
form the object of the Japanese verb [mamoru]
(protect). Thus, for example, semantic selection
patterns should treat these nouns as possible ob-
jects for this verb. Transparent relations may serve
to neutralize some of the problematic cases of at-
tachment ambiguity. For example, in the English
sentence A number of phrases with modifiers are
not ambiguous, there is a transparent *COMP re-
lation between numbers and of and a transpar-
ent *OBJ relation between of and phrases. Thus,
high attachment of the PP with modifiers, would
have the same interpretation as low attachment
since phrases is the underlying head of number of
116
Figure 1: GLARF 5-tuples for 3 languages
phrases. In this same example, the adverb not can
be attached to either the copula are or the pred-
icative adjective, with no discernible difference in
meaning?this factor is indicated by the transparent
designation of the relations where the copula is a
functor. Transparent features also provide us with
a simple way of handling certain function words,
such as the Chinese word De which inherits the
function of its underlying head, connecting a vari-
ety of such modifiers to head nouns (an adjective
in the Chinese example.). For conjunction cases,
the number of underlying relations would multi-
ply, e.g., Mary and John bought and sold stock
would (underlyingly) have four subject relations
derived by pairing each of the underlying subject
nouns Mary and John with each of the underlying
main predicate verbs bought and sold.
3 Automatic vs. Manual Annotation
Apart from accuracy, there are several other ways
that automatic and manual annotation differs. For
Penn-treebank (PTB) parsing, for example, most
parsers (not all) leave out function tags and empty
categories. Consistency is an important goal for
manual annotation for many reasons including: (1)
in the absence of a clear correct answer, consis-
tency helps clarify measures of annotation quality
(inter-annotator agreement scores); and (2) consis-
tent annotation is better training data for machine
learning. Thus, annotation specifications use de-
faults to ensure the consistent handling of spurious
ambiguity. For example, given a sentence like I
bought three acres of land in California, the PP in
California can be attached to either acres or land
with no difference in meaning. While annotation
guidelines may direct a human annotator to prefer,
for example, high attachment, systems output may
have other preferences, e.g., the probability that
land is modified by a PP (headed by in) versus the
probability that acres can be so modified.
Even if the manual annotation for a particular
corpus is consistent when it comes to other factors
such as tokenization or part of speech, developers
of parsers sometimes change these guidelines to
suit their needs. For example, users of the Char-
niak parser (Charniak, 2001) should add the AUX
category to the PTB parts of speech and adjust
their systems to account for the conversion of the
word ain?t into the tokens IS and n?t. Similarly, to-
kenization decisions with respect to hyphens vary
among different versions of the Penn Treebank, as
well as different parsers based on these treebanks.
Thus if a system uses multiple parsers, such differ-
ences must be accounted for. Differences that are
not important for a particular application should
be ignored (e.g., by merging alternative analyses).
For example, in the case of spurious attachment
ambiguity, a system may need to either accept both
as right answers or derive a common representa-
tion for both. Of course, many of the particular
problems that result from spurious ambiguity can
be accounted for in hind sight. Nevertheless, it
is precisely this lack of a controlled environment
which adds elements of spurious ambiguity. Us-
ing new processors or training on new treebanks
can bring new instances of spurious ambiguity.
4 Experiments and Evaluation
We ran GLARFers on both manually created tree-
banks and automatically produced parses for En-
glish, Chinese and Japanese. For each corpus, we
created one or more answer keys by correcting
117
system output. For this paper, we evaluate solely
on the logic1 relations (the second column in fig-
ure 1.) Figure 2 lists our results for all three lan-
guages, based on treebank and parser input.
As in (Meyers et al, 2009), we generated 4-
tuples consisting of the following for each depen-
dency: (A) the logic1 label (SBJ, OBJ, etc.), (B)
its transparency (True or False), (C) The functor (a
single word or a named entity); and (D) the argu-
ment (a single word or a named entity). In the case
of conjunction where there was no lexical con-
junction word, we used either punctuation (com-
mas or semi-colons) or the placeholder *NULL*.
We then corrected these results by hand to produce
the answer key?an answer was correct if all four
members of the tuple were correct and incorrect
otherwise. Table 2 provides the Precision, Recall
and F-scores for our output. The F-T columns
indicates a modified F-score derived by ignoring
the +/-Transparent distinction (resulting changes
in precision, recall and F-score are the same).
For English and Japanese, an expert native
speaking linguist corrected the output. For Chi-
nese, several native speaking computational lin-
guists shared the task. By checking compatibil-
ity of the answer keys with outputs derived from
different sources (parser, treebank), we could de-
tect errors and inconsistencies. We processed the
following corpora. English: 86 sentence article
(wsj 2300) from the Wall Street Journal PTB test
corpus (WSJ); 46 sentence letter from Good Will
(LET), the first 100 sentences of a switchboard
telephone transcript (TEL) and the first 100 sen-
tences of a narrative from the Charlotte Narra-
tive and Conversation (NAR). These samples are
taken from the PTB WSJ Corpus and the SIGANN
shared subcorpus of the OANC. The filenames are:
110CYL067, NapierDianne and sw2014. Chi-
nese: a 20 sentence sample of text from the
Penn Chinese Treebank (CTB) (Xue et al, 2005).
Japanese: 20 sentences from the Kyoto Corpus
(KYO) (Kurohashi and Nagao, 1998)
5 Running the GLARFer Programs
We use Charniak, UMD and KNP parsers (Char-
niak, 2001; Huang and Harper, 2009; Kurohashi
and Nagao, 1998), JET Named Entity tagger (Gr-
ishman et al, 2005; Ji and Grishman, 2006)
and other resources in conjunction with language-
specific GLARFers that incorporate hand-written
rules to convert output of these processors into
a final representation, including logic1 struc-
ture, the focus of this paper. English GLAR-
Fer rules use Comlex (Macleod et al, 1998a)
and the various NomBank lexicons (http://
nlp.cs.nyu.edu/meyers/nombank/) for
lexical lookup. The GLARF rules implemented
vary by language as follows. English: cor-
recting/standardizing phrase boundaries and part
of speech (POS); recognizing multiword expres-
sions; marking subconstituents; labeling rela-
tions; incorporating NEs; regularizing infiniti-
val, passives, relatives, VP deletion, predica-
tive and numerous other constructions. Chi-
nese: correcting/standardizing phrase boundaries
and POS, marking subconstituents, labeling rela-
tions; regularizing copula constructions; incorpo-
rating NEs; recognizing dates and number expres-
sions. Japanese: converting to PTB format; cor-
recting/standardizing phrase boundaries and POS;
labeling relations; processing NEs, double quote
constructions, number phrases, common idioms,
light verbs and copula constructions.
6 Discussion
Naturally, the treebank-based system out-
performed parse-based system. The Charniak
parser for English was trained on the Wall Street
Journal corpus and can achieve about 90% accu-
racy on similar corpora, but lower accuracy on
other genres. Differences between treebank and
parser results for English were higher for LET and
NAR genres than for the TEL because the system
is not currently designed to handle TEL-specific
features like disfluencies. All processors were
trained on or initially designed for news corpora.
Thus corpora out of this domain usually produce
lower results. LET was easier as it consisted
mainly of short simple sentences. In (Meyers et
al., 2009), we evaluated our results on 40 Japanese
sentences from the JENAAD corpus (Utiyama
and Isahara, 2003) and achieved a higher F-score
(90.6%) relative to the Kyoto corpus, as JENAAD
tends to have fewer long complex sentences.
By using our answer key for multiple inputs, we
discovered errors and consequently improved the
quality of the answer keys. However, at times we
were also compelled to fork the answer keys?given
multiple correct answers, we needed to allow dif-
ferent answer keys corresponding to different in-
puts. For English, these items represent approxi-
mately 2% of the answer keys (there were a total
118
Treebank Parser
ID % Prec % Rec F F-T % Prec % Rec F F-T
WSJ 12381491 = 83.0
1238
1471 = 84.2 83.6 87.1
1164
1452 = 80.2
1164
1475 = 78.9 79.5 81.8
LET 419451 = 92.9
419
454 = 92.3 92.6 93.3
390
434 = 89.9
390
454 = 85.9 87.8 87.8
TEL 478627 = 76.2
478
589 = 81.2 78.6 82.2
439
587 = 74.8
439
589 = 74.5 74.7 77.4
NAR 8171013 = 80.7
817
973 =84.0 82.3 84.1
724
957 = 75.7
724
969 = 74.7 75.2 76.1
CTB 351400 = 87.8
351
394 = 89.1 88.4 88.7
352
403 = 87.3
352
438 = 80.4 83.7 83.7
KYO 525575 = 91.3
525
577 = 91.0 91.1 91.1
493
581 = 84.9
493
572 = 86.2 85.5 87.8
Figure 2: Logic1 Scores
Figure 3: Examples of Answer Key Divergences
of 74 4-tuples out of a total of 3487). Figure 3 lists
examples of answer key divergences that we have
found: (1) alternative tokenizations; (2) spurious
differences in attachment and conjunction scope;
and (3) ambiguities specific to our framework.
Examples 1 and 2 reflect different treatments of
hyphenation and contractions in treebank specifi-
cations over time. Parsers trained on different tree-
banks will either keep hyphenated words together
or separate more words at hyphens. The Treebank
treatment of can?t regularizes so that (can need
not be differentiated from ca), whereas the parser
treatment makes maintaining character offsets eas-
ier. In example 3, the Japanese parser recognizes
a single word whereas the treebank divides it into
a prefix plus stem. Example 4 is a case of differ-
ences in character encoding (zero).
Example 5 is a common case of spurious attach-
ment ambiguity for English, where a transparent
noun takes an of PP complement?nouns such as
form, variety and thousands bear the feature trans-
parent in the NOMLEX-PLUS dictionary (a Nom-
Bank dictionary based on NOMLEX (Macleod et
al., 1998b)). The relative clause attaches either
to the noun thousands or people and, therefore,
the subject gap of the relative is filled by either
thousands or people. This ambiguity is spurious
since there is no meaningful distinction between
these two attachments. Example 6 is a case of
attachment ambiguity due to a support construc-
tion (Meyers et al, 2004). The recipient of the
gift will be Goodwill regardless of whether the
PP is attached to give or gift. Thus there is not
much sense in marking one attachment more cor-
rect than the other. Example 7 is a case of conjunc-
tion ambiguity?the context does not make it clear
whether or not the pearls are part of a necklace or
just the beads are. The distinction is of little con-
sequence to the understanding of the narrative.
Example 8 is a case in which our grammar han-
dles a case ambiguously: the prenominal adjective
can be analyzed either as a simple noun plus ad-
jective phrase meaning various businesses or as a
noun plus relative clause meaning businesses that
are varied. Example 9 is a common case in Chi-
nese where the verb/noun distinction, while un-
clear, is not crucial to the meaning of the phrase ?
under either interpretation, 5 billion was exported.
7 Concluding Remarks
We have discussed challenges of automatic an-
notation when transducers of other annotation
schemata are used as input. Models underly-
ing different transducers approximate the origi-
nal annotation in different ways, as do transduc-
ers trained on different corpora. We have found
it necessary to allow for multiple correct answers,
due to such differences, as well as, genuine and
spurious ambiguities. In the future, we intend to
investigate automatic ways of identifying and han-
dling spurious ambiguities which are predictable,
including examples like 5,6 and 7 in figure 3 in-
volving transparent functors.
119
References
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In ACL 2001, pages 116?123.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28:245?288.
R. Grishman, D. Westbrook, and A. Meyers. 2005.
Nyu?s english ace 2005 system description. In ACE
2005 Evaluation Workshop.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In CoNLL-2009, Boulder, Col-
orado, USA.
Z. Huang and M. Harper. 2009. Self-training PCFG
Grammars with Latent Annotations across Lan-
guages. In EMNLP 2009.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Syd-
ney, Australia.
S. Kurohashi and M. Nagao. 1998. Building a
Japanese parsed corpus while improving the pars-
ing system. In Proceedings of The 1st International
Conference on Language Resources & Evaluation,
pages 719?724.
C. Macleod, R. Grishman, and A. Meyers. 1998a.
COMLEX Syntax. Computers and the Humanities,
31:459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998b. Nomlex: A lexicon of nominal-
izations. In Proceedings of Euralex98.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Pro-
ceedings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, and Catherine Macleod. 2004.
NP-External Arguments: A Study of Argument
Sharing in English. In The ACL 2004 Workshop
on Multiword Expressions: Integrating Processing,
Barcelona, Spain.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao,
and W. Xu. 2009. Automatic Recognition of Log-
ical Relations for English, Chinese and Japanese in
the GLARF Framework. In SEW-2009 at NAACL-
HLT-2009.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Depen-
dencies. In Proceedings of the CoNLL-2008 Shared
Task, Manchester, GB.
M. Utiyama and H. Isahara. 2003. Reliable Mea-
sures for Aligning Japanese-English News Articles
and Sentences. In ACL-2003, pages 72?79.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase Structure Annota-
tion of a Large Corpus. Natural Language Engi-
neering.
N. Xue. 2008. Labeling Chinese Predicates with Se-
mantic roles. Computational Linguistics, 34:225?
255.
120
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 680?688,
Beijing, August 2010
Filtered Ranking for Bootstrapping in Event Extraction 
Shasha Liao Dept. of Computer Science New York University liaoss@cs.nyu.edu 
Ralph Grishman Dept. of Computer Science New York University  grishman@cs.nyu.edu 
 Abstract Several researchers have proposed semi-supervised learning methods for adapting event extraction systems to new event types. This paper investigates two kinds of bootstrapping methods used for event extraction: the document-centric and similarity-centric approaches, and proposes a filtered ranking method that combines the advantages of the two. We use a range of extraction tasks to compare the generality of this method to previous work. We analyze the results using two evaluation metrics and observe the effect of different training corpora. Experiments show that our new ranking method not only achieves higher performance on different evaluation metrics, but also is more stable across different bootstrapping corpora. 1 Introduction The goal of event extraction is to identify instances of a class of events in text, along with the arguments of the event (the participants, place, and time). In this paper we shall focus on the sub-problem of identifying the events themselves. Event extraction systems from the early and mid 90s relied primarily on hand-coded rules, which must be written anew for every task. Since then, supervised and semi-supervised methods have been developed in order to build systems for new scenarios more easily. Supervised methods can perform quite well with enough training data, but annotating sufficient data may require months of labor. 
Semi-supervised methods aim to reduce the annotated data required, ideally to a small set of seeds. Most semi-supervised event extractors seek to learn sets of patterns consisting of a predicate and some lexical or semantic constraints on its arguments. The semi-supervised learning was based primarily on one of two assumptions: the document-centric approach, which assumes that relevant patterns should appear more frequently in relevant documents (Riloff 1996; Yangarber et al 2000; Yangarber 2003; Surdeanu et al2006); and the similarity-centric approach, which assumes that relevant patterns should have lexically related terms (Stevenson and Greenwood 2005, Greenwood and Stevenson 2006). An effective semi-supervised extractor will have good performance over a range of extraction tasks and corpora. However, many of the learning procedures just cited have been tested on only one or two extraction tasks, so their generality is uncertain. To remedy this, we have tested learners based on both assumptions, targeting both a MUC (Message Understanding Conference) scenario and several ACE (Automatic Content Extraction) event types. We identify shortcomings of the prior bootstrapping methods, propose a more effective and stable ranking method, and consider the effect of different corpora and evaluation metrics. 2 Related Work The basic assumption of the document-centric approach is that documents containing a large number of patterns already identified as relevant to a particular IE scenario are likely to contain further relevant patterns. Riloff (1996) initiated 
680
this approach and claimed that if a corpus can be divided into documents involving a certain event type and those not involving that type, patterns can be evaluated based on their frequency in relevant and irrelevant documents. Yangarber et al (2000) incorporated Riloff?s metric into a bootstrapping procedure, which started with several seed patterns but required no manual document classification or corpus annotation.  The seed patterns were used to identify some relevant documents, and the top-ranked patterns (based on their distribution in relevant and irrelevant documents) were added to the seed set. This process was repeated, assigning a relevance score to each document based on the relevance of the patterns it contains and gradually growing the set of relevant patterns. This approach was further refined by Surdeanu et al (2006), who used a co-training strategy in which two classifiers seek to classify documents as relevant to a particular scenario. Patwardhan and Riloff (2007) presented an information extraction system that find relevant regions of text and applies extraction patterns within those regions. They created a self-trained relevant sentence classifier to identify relevant regions, and use a semantic affinity measure to automatically learn domain-relevant extraction patterns. They also distinguish primary patterns from secondary patterns and apply the patterns selectively in the relevant regions. Stevenson and Greenwood (2005) (henceforth ?S&G?) suggested an alternative method for ranking the candidate patterns. Their approach relied on the assumption that useful patterns will have similar lexical items to the patterns that have already been accepted. They used WordNet to calculate word similarity. They chose to represent each pattern as a vector consisting of the lexical items and used a version of the cosine metric to determine the similarity between pairs of patterns. Later, Greenwood and Stevenson (2006) introduced a structural similarity measure that could be applied to extraction patterns consisting of linked dependency chains. 3 Ranking Methods in Bootstrapping Most semi-supervised event extraction systems are based on patterns with variables which have semantic type constraints. A simple example is ?organization appoints person as position?; if 
this pattern matches a passage in a test document, a hiring event will be instantiated with the items matching the variables being the arguments of the event. So training an event extractor becomes primarily a task of acquiring these patterns. In a semi-supervised setting, this involves ranking candidate patterns and accepting the top-ranked patterns at each iteration.  Our goal was to create a more robust learner through improved pattern ranking. 3.1 Problems of Document-centric Bootstrapping Document-centric bootstrapping tries to find patterns with high frequency in relevant documents and low frequency in irrelevant documents. The assumption is that descriptions of the same event or the same type of event may occur multiple times in a document, and so a document containing a relevant pattern is more likely to contain more such patterns. This approach may end up extracting patterns for related events; for example, start-position often comes with end-position events. This effect may be salutary if the extraction scenario includes these related events (as in MUC-6), but will pose a problem if the goal is to extract individual event types. Also, because an extra corpus for bootstrapping is needed, different corpora might perform quite differently (see Figure 2). 3.2 Problems of Similarity-centric Bootstrapping Similarity-centric bootstrapping tries to find patterns with high lexical similarities. The most crucial issue is how to evaluate the similarity of two patterns, which is based on the similarity of two words. In this strategy, no extra corpus is needed, which eliminates the effort to find a good bootstrapping corpus, but a semantic dictionary that can provide word similarity is required. S&G used WordNet1 to provide word similarity information. However, in the similarity-centric approach, lexical polysemy can lead the bootstrapping down false paths. For example, for start-position (hire) events, ?name? and ?charge? are in the same Synset as appoint, but including these words is quite dangerous because they contain other common senses                                                            1http://wordnet.princeton.edu/ 
681
unrelated to start-position events. For die events, we might have words like ?go? and ?pass?, which are also used in very specific contexts when they refer to ?die?. If similarity-centric ranking extracts patterns including these words, performance will deteriorate very quickly, because most of the time, these words do not predicate the proper event, and more and more wrong patterns will be extracted. 3.3 Our Approach We propose a new ranking method, which constrains the document-centric and similarity-centric assumptions, and makes a more restricted assumption: patterns that appear in relevant documents and are lexically similar are most likely to be relevant. This method limits the effect of ambiguous patterns by narrowing the search to relevant documents, and limits irrelevant patterns in relevant documents by word similarity restriction.  For example, although ?charge? has high word similarity to ?appoint?, its document relevance score is very low, and we will not include this word in bootstrapping starting from ?appoint?. Many different combinations are possible; we propose one that uses the word similarity as a filter. The document relevance score is first applied to rank the patterns in relevant documents, then the patterns with lexical similarity scores below a similarity threshold will be removed from the ranking; only patterns above threshold will be added to the seeds. However, if in the current iteration, no pattern meets the threshold, the threshold will be lowered until new patterns can be found. We call this ranking method filtered ranking2: 
? 
Filter(p) =
Yangarber(p) Stevenson(p) >= t
0 otherwise
?? 
?? 
??  where t is the threshold, which is initialized to 0.9 in our experiments. 4 System Description Our approach is similar to that for document-centric bootstrapping, but the ranking                                                            2 We also tried using the product of the document relevance score and word similarity score, and found the results to be quite similar. Due to space limitations, we do not report these results here.  
function is changed to incorporate lexical similarity information. For our experiments bootstrapping was terminated after a fixed number of iterations; in practice, we would monitor performance on a held-out (dev-test) sample and stop when it declines for k iterations. 4.1 Pre-processing Instead of limiting ourselves to surface syntactic relations, we want to get more general and meaningful patterns. To this end, we used semantic role labeling (Gildea and Jurafsky, 2002) to generate the logical grammatical and predicate-argument representation automatically from a parse tree (Meyers et al 2009). The output of the semantic labeling is the dependency representation of the text, where each sentence is a graph consisting of nodes (corresponding to words) and arcs. Each arc captures up to three relations between two words: (1) a SURFACE relation, the relation between a predicate and an argument in the parse tree of a sentence; (2) a LOGIC1 (grammatical logical) relation which regularizes for lexical and syntactic phenomena like passive, relative clauses, and deleted subjects; and (3) a LOGIC2 (predicate-argument) relation corresponding to relations in PropBank (Palmer et al 2005) and NomBank  In constructing extraction patterns from this graph, we take each dependency link along with its predicate-argument role; if that role is null, we use its logical grammatical role, and finally, its surface role. For example, for the sentence: John is hit by Tom?s brother. we generate the patterns: <Arg1 hit John> <Arg0 hit brother> <T-pos brother Tom> where the first two represent LOGIC2 relations and the third a SURFACE relation.  To reduce data sparseness, all inflected words are changed to their root form (e.g. ?attackers???attacker?), and all names are replaced by their ACE type (person, organization, location, etc.), so the first pattern would become <Arg1 hit PERSON> 4.2 Document-based Ranking The document-centric method employs a 
682
re-implementation of the procedure described in (Yangarber et al 2000), using the disjunctive voting scheme for document relevance.  At each iteration i we compute a precision score Preci(p) for each pattern p and a relevance score Reli(d) for each document d.  Initially the seed patterns have precision 1 and all other patterns precision 0.  These are updated by 
? 
Re l
i
(d) =1? (1?Prec
i
(p))
p?K (d )
?
 
where K(d) is the set of accepted patterns  that match document d, and 
? 
Prec
i+1
(p) =
1
| H(p) |
? Re l
i
(d)
d ?H (p )
?
 
where H(p) is the set of documents matching pattern p.  Patterns are then ranked by 
? 
RankFun
Yangarber
(p) =
Sup(p)
H(p)
* logSup(p)  
where  (a generalization of Yangarber?s metric), and the top-ranked candidates are added to the set of accepted patterns. 4.3 Pattern Similarity  For two words, there are several ways to measure their similarity using WordNet, which can be roughly divided into two categories: distance-based, including Leacock and Chodorow (1998), Wu and Palmer (1994); and information content based, including Resnik (1995), Lin (1998), and Jiang and Conrath (1997). We follow S&G (2005)?s method and use the semantic similarity of concepts based on Information Content (IC). Every pattern consists of a predicate and a constraint (?argument?) on its local syntactic context, and so the similarity of two patterns depends on the similarity of the predicates and the similarity of the arguments.  We modified S&G?s structural similarity measure to reflect some differences in pattern structure: first, S&G only focus on patterns headed by verbs, while we include verbs, nouns and adjectives; second, they only record the subject and object to a verb, while we record all argument relations; third, 
our patterns only contain a predicate and a single constraint (argument), while their pattern might contain two arguments, subject and object. With two arguments, many more patterns are possible and the vector similarity calculation over all patterns in a large corpus becomes very time consuming. We do not limit ourselves to verb patterns because nouns and (occasionally) adjectives can also represent an event. For example, ?Stevenson?s promotion is a signal ?? expresses a start-position event. Moreover, in our pattern, we assume that the predicate is more important than constraint, because it is the root (head) of the pattern in the semantic graph structure, and place different weights on predicate and constraint. Finally, the similarity of two patterns p1 and p2 is computed as follows: 
 where ?+?=1, f represents a predicate, r represent a role, and a represent an argument. In our experiment, ? is set to 0.6 and ? is set to 0.4. The role similarity is 1 for identical roles and for roles which generally correspond at the syntactic and predicate-argument level (arg0 ? subj; arg1 ? obj); selected other role pairs are assigned a small positive similarity (0.1 or 0.2), and others 0. As with the document-centric method, bootstrapping begins by accepting a set of seed patterns. At each iteration, the procedure computes the similarity between all patterns in the training corpus and the currently accepted patterns and accepts the most similar pattern(s). In S&G?s experiments the evaluation corpus also served as the training corpus. 5 Experiments There have been two types of event extraction tasks. One involved several ?elementary? event types, such as ?attack?, ?die?, ?injure? etc.; for example, the ACE 2005 evaluation3 used a set of 33 event types and subtypes. The other type involved a scenario ? a set of related events, like ?attacks and the damage, injury, and death they cause?, or ?arrest, trial, sentencing etc.?. The                                                            3See http://projects.ldc.upenn.edu/ace/docs/English-Events- Guidelines_v5.4.3.pdf for a description of this task. 
683
MUC evaluations included two scenarios that have been the subject of considerable research on learning methods: terrorist incidents (MUC-3/4) and executive succession (MUC-6). We conducted experiments on the MUC-6 task to make a comparison to previous work. We also did experiments on ACE 2005 data, because it provides many distinct event types; we conducted experiments on three disparate event types: attack, die, and start-position. Note that MUC-6 identifies a scenario while ACE identifies specific event types, and types which are in the same MUC scenario might represent different ACE events. For example, the executive succession scenario (MUC-6) includes the start-position and end-position events in ACE.  5.1 Data Description There are four corpora used in the experiments: MUC-6 corpora ? Bootstrapping: pre-selected data from the Reuters corpus (Rose et al 2002) from 1996 and 1997, including 3000 related documents and 3000 randomly chosen unrelated documents ? Evaluation: MUC-6 annotated data, including 200 documents (official training and test). We were guided by the MUC-6 key file in annotating every document and sentence as relevant or irrelevant. ACE corpora ? Bootstrapping: untagged data from the Gigaword corpus from January 2006, including 14,171 English newswire articles from Agence France-Presse (AFP). ? Evaluation: ACE 2005 annotated (training) data, including 589 documents 5.2 Parameters used in Experiments In our bootstrapping process, we only extract patterns appearing more than 2 times in the corpus, and the similarity filter threshold is originally set to 0.9. If no patterns are found, it is reduced by 0.1 until new patterns are found.  In each iteration, the top 3 patterns in the ranking function will be added to the seeds. For the similarity-centric method, only patterns appearing more than 2 times and in less than 30% of the documents will be extracted, which is the same as S&G?s approach. 
5.3 MUC-6 Experiments Our overall goal was to demonstrate that filtered ranking was in all cases competitive with and in at least some cases clearly superior to the earlier methods, over a range of extraction tasks and bootstrapping corpora. We began with the MUC-6 task, where the efficacy of the earlier methods had already been demonstrated.  < Arg0 resign Person > < Arg1 appoint Person > < Arg0 appoint Org_commercial> <Arg1 succeed Person > Table 1. Seeds for MUC-6 evaluation  For MUC-6 evaluation, we follow S&G?s approach and assess extraction patterns by their ability to identify event-relevant sentences.4 The system treats a sentence as relevant if it matches an extraction pattern. Bootstrapping starts from four seeds which yield 80% precision and 24% recall for sentence filtering.  To compare with previous work, we tested the filtered ranking method on two corpora: the first is the Reuters corpus used in S&G?s recreation of Yangarber?s experiment (Filter1), to compare with their results for the document-centric method; the second uses the test corpus as S&G did (Filter2), to compare with their results for the similarity-centric method. We compare methods based on peak F score; in practice, this would mean controlling the bootstrapping using a held-out test sample.   
 Figure 1. F score for different ranking methods on MUC-6 evaluation  Figure 1 showed that the filtered ranking                                                            4 We also tried the document filtering evaluation introduced by Yangarber but, as S&G observed, this metric is too insensitive because over 50% of the documents in the MUC-6 test set are relevant. 
684
methods edge out both document and similarity-centric methods.  Our scores are comparable to S&G?s, although they report somewhat better performance for similarity-centric than for document-centric (55 vs. 51) whereas document-centric did better for us. This difference may reflect differences in pattern generation (discussed above) and possibly differences in the specific corpora used. However, document-centric bootstrapping needs an extra corpus for bootstrapping; S&G used a pre-selected corpus that contains approximately same number of relevant and irrelevant documents5. We wanted to check if such a corpus is essential for the document-centric method, and if the need for pre-selection can be reduced through filtered ranking. Thus, we set up another experiment to see if the document-centric method is stable or sensitive to different corpora. We used two additional corpora for MUC-6 evaluation: one is a subset of the Wall Street Journal (WSJ) 1991 corpus, which contains 18,734 untagged documents; the other is the Gigaword AFP corpus described in section 5.1. Both corpora are much larger than the Reuters corpus, and while we do not have precise information about relevant document density, the WSJ contains quite a few start-position events because it is primarily business news; the Gigaword corpus (AFP newswire) has fewer start-position events because it contains a wider variety of news.   
 Figure 2. Document-centric and Filtered ranking results on different corpora for MUC-6   Figure 2 showed that the document-centric method performs quite differently on different corpora, which indicates that a pre-selected corpus plays an important role in                                                            5 The pre-selection of relevant and irrelevant documents is based on document meta-data provided as part of the Reuters Corpus Volume I (Rose et al, 2002). 
document-centric ranking. It suggests that the percentage of relevant documents may be more important than the overall corpus size. The figure also shows that filtered ranking is much more stable across different corpora. Richer corpora still have better peak performance, but the difference is not quite as great; also, peak performance on a given corpus is consistently better than the document-centric method. From the above experiments, we conclude that our filtering method is better in two aspects: first, bootstrapping on the same corpus performs better than either document or similarity-centric methods; second, if we can not get a corpus with an assured high density of relevant documents, it is safer to use filtered ranking because it is more stable across different corpora. 5.4 ACE2005 Experiments The ACE2005 corpus includes annotations for 33 different event types and subtypes, offering us an opportunity to assess the generality of our methods across disparate event types. We selected 3 event types to report on here: ? Die: ?occurs whenever the life of a PERSON Entity ends. It can be accidental, intentional or self-inflicted.? This event appears 535 times in the corpus. ? Attack: ?is defined as a violent physical act causing harm or damage.? Attack events include a variety of sub-events like ?person attack person?, ?country invade country?, and ?weapons attack locations?. This event type appears 1120 times. ? Start-Position: ?occurs whenever a PERSON Entity begins working for (or changes offices within) an ORGANIZATION or GPE. This includes government officials starting their terms, whether elected or appointed?. It appears 116 times in the corpus. We choose these three event types because they reflect the diversity of events ACE annotated: die events appear frequently in the ACE corpus and its definition is very clear; attack events also appear frequently, but its definition is rather complicated and contains several different sub-events; start-position?s definition is clear, but it is relatively infrequent in the corpus. Based on the observations from the MUC-6 corpus, we eschewed corpus pre-selection for 
685
two reasons: first, building a different corpus for training each event type is an extra burden in developing a system for handling multiple events; second, we want to demonstrate that filtered ranking would work without pre-selection, while the document-centric method does not. As a result, we used the Gigaword AFP corpus for all event types. In the ACE 2005 corpus, for every event, the annotators recorded a trigger, which is the main word that most clearly expresses an event occurrence. This added information allowed us to conduct dual evaluations: one based on sentence relevance - following S&G - presented in section 5.4.2, and one based on trigger identification, presented in section 5.4.3. 5.4.1 ACE2005 Supervised Model To provide a benchmark for our semi-supervised learners, we built a very simple pattern-based supervised learning model. For training, for every pattern, we count how many times it contains an event trigger and how many times it does not. If more than 50% of the time it contains an event trigger, we treat it as a positive pattern.  For sentence level evaluation, if there is a positive pattern in a sentence, we tag this sentence as relevant; otherwise not. For word level evaluation, if the word is the predicator of a positive pattern, we tag it as a trigger; otherwise not6.  We did a 5-fold cross-validation on the ACE 2005 data, report the average results and compare it to the semi-supervised learning method (see figure 3 & 4). 5.4.2 Sentence level ACE Event Evaluation7 Different event types have quite different performance (see figure 3): for the die event, the peak performance of all methods is quite good, and quite close to the supervised result; for the attack event, filtered ranking performs much better than both document and similarity-centric                                                            6For word-level evaluation, we only consider trigger words with at least one semantic argument such as subject, object or a preposition; for that reason the performance is quite different from sentence level evaluation. We did the same for the word-level evaluation of semi-supervised learning.  7 We do not list Attack seed patterns here as there are 34 patterns used. 
methods, but still worse than the supervised method; for start-position events, the semi-supervised method beats the supervised method. The reason might be as follows: Die events appear frequently in ACE 2005, and most instances correspond to a small number of forms, so it is easy to find the correct patterns both from WordNet or related documents. As a result, filtered ranking provides no apparent benefit.  Attack is a more complicated event including several sub-events, which also have a lot of related events like die and injure. As a result, the document-centric method?s performance goes down much faster, because patterns for related event types get drawn in; while the similarity-centric method performs worse than filtered ranking because some ambiguous words are introduced. For example, ?hit? is an attack trigger, but words in the same Synset, such as ?reach?, ?make?, ?attain?, ?gain? are quite dangerous because most of the time, these words do not refer to an attack event. Start-position events do not appear frequently in ACE 2005, and supervised learning cannot achieve good performance because it can?t collect enough training samples. The similarity-centric and Filter2 methods, which also depend on the ACE 2005 corpus, do not perform well either. Filter1 performs quite well because the Gigaword AFP corpus is quite large and contains more relevant documents, although the percentage is very small. This confirms our assumption that filtered ranking can achieve reasonable performance on a large unselected corpus, which is especially useful when the event is rare in the evaluation corpus.  <Arg1 kill Person> <Arg1 slay Person> <Arg1 death Person> Table 2. Seeds for Ace 2005 Die evaluation  <Arg0 hire ORG> <Arg1 hire Person> <Arg1 appoint Person> <Arg0 appoint ORG> Table 3. Seeds for Ace 2005 Start-Position evaluation  
686
 Figure 3. Performance on different ranking methods on ACE2005 sentence level evaluation  
 Figure 4. Performance on different ranking methods on ACE2005 word level evaluation  5.4.3 Word-level ACE Event Evaluation Word-level evaluation is different from sentence-level evaluation because patterns which appear around an event but do not predicate an event are penalized in this evaluation. For example, the pattern <Sbj chairman PERSON>, which arises from a phrase like ?PERSON was the chairman of COMPANY?, appears much more in relevant start-position sentences than irrelevant sentences, and adding this pattern to the seeds will improve performance using the relevant-sentence metric. We would prefer a metric which discounted such patterns. As noted above, ACE event annotations contain triggers, which are more specific event locators than a sentence, and we use this as the basis for a more specific evaluation. Extracted patterns are used to identify event triggers instead of identifying relevant sentences. For every word w in the ACE corpus, we extract all the patterns whose predicate is w. If the event extraction patterns include one of these patterns, we tag w as a trigger.  In word level evaluation, document-centric performs worse than the other methods. The reason is that some patterns appear often in the 
context of an event and are positive patterns for sentence level evaluation, but they do not actually predicate an event and are negative patterns in word level evaluation. In this situation, the document-centric method performs worse than the similarity-centric method, because it extracts many such patterns. For example, of the sentences which contain die events, 29% also contain attack events.  Thus in word level evaluation, filtered ranking continues to outperform either document- or similarity-centric methods, and its advantage over document-centric methods is accentuated. 6 Conclusions In this paper, we propose a new ranking method in bootstrapping for event extraction and investigate the performance on different bootstrapping corpora with different ranking methods. This new method can block some irrelevant patterns coming from relevant documents, and, by preferring patterns from relevant documents, can eliminate some lexical ambiguity. Experiments show that this new ranking method performs better than previous ranking methods and is more stable across different corpora.  
687
References  D. Gildea and D. Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics, 28:245?288. MA Greenwood, M. Stevenson. 2006. Improving semi-supervised acquisition of relation extraction patterns. Proceedings of the Workshop on Information Extraction Beyond the Document, pages 29?35. Jay J. Jiang and David W. Conrath. 1997. Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy. In Proceedings of International Conference Research on Computational Linguistics (ROCLING X), Taiwan C. Leacock and M. Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In C. Fellbaum, editor, WordNet: An electronic lexical database, pages 265?283. MIT Press. D. Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the International Conference on Machine Learning, Madison, August. A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S.  Liao and W. Xu. 2009. Automatic Recognition of Logical Relations for English, Chinese and Japanese in the GLARF Framework. In SEW-2009 (Semantic Evaluations Workshop) at NAACL HLT-2009 MUC. 1995. Proceedings of the Sixth Message Understanding Conference (MUC-6), San Mateo, CA. Morgan Kaufmann. Martha Palmer, Dan Gildea, and Paul Kingsbury, The Proposition Bank: A Corpus Annotated with Semantic Roles, Computational Linguistics, 31:1, 2005. Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2003. Using measures of semantic relatedness for word sense disambiguation. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, Mexico City, Mexico.  Patwardhan, S. and Riloff, E. 2007. Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions. Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP-07) T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. In Proceedings of the Nineteenth 
National Conference on Artificial Intelligence (Intelligent Systems Demonstrations), pages 1024-1025, San Jose, CA, July 2004. P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448?453, Montreal, August. Ellen Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proc. Thirteenth National Conference on Artificial Intelligence (AAAI-96), 1996, pp. 1044-1049. T. Rose, M. Stevenson, and M. Whitehead. 2002. The Reuters Corpus Volume 1 - from Yesterday?s news to tomorrow?s language resources. In LREC-02, pages 827?832, La Palmas, Spain. M. Stevenson and M. Greenwood. 2005. A Semantic Approach to IE Pattern Induction. Proceedings of ACL 2005. Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006. A Hybrid Approach for the Acquisition of Information Extraction Patterns. Proceedings of the EACL 2006 Workshop on Adaptive Text Extraction and Mining (ATEM 2006) Z. Wu and M. Palmer. 1994. Verb semantics and lexical selection. In 32nd Annual Meeting of the Association for Computational Linguistics, pages 133?138, Las Cruces, New Mexico. Roman Yangarber; Ralph Grishman; Pasi Tapanainen; Silja Huttunen. 2000. Automatic Acquisition of Domain Knowledge for Information Extraction. Proc. COLING 2000. Roman Yangarber. 2003. Counter-Training in Discovery of Semantic Patterns. Proceedings of ACL2003  
688
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 789?797,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Using Document Level Cross-Event Inference  to Improve Event Extraction 
  Shasha Liao New York University 715 Broadway, 7th floor New York, NY 10003 USA liaoss@cs.nyu.edu  
Ralph Grishman New York University 715 Broadway, 7th floor New York, NY 10003 USA grishman@cs.nyu.edu      Abstract 
Event extraction is a particularly challenging type of information extraction (IE). Most current event extraction systems rely on local information at the phrase or sentence level. However, this local context may be insufficient to resolve ambiguities in identifying particular types of events; information from a wider scope can serve to resolve some of these ambiguities. In this paper, we use document level information to improve the performance of ACE event extraction. In contrast to previous work, we do not limit ourselves to information about events of the same type, but rather use information about other types of events to make predictions or resolve ambiguities regarding a given event. We learn such relationships from the training corpus and use them to help predict the occurrence of events and event arguments in a text. Experiments show that we can get 9.0% (absolute) gain in trigger (event) classification, and more than 8% gain for argument (role) classification in ACE event extraction. 1 Introduction The goal of event extraction is to identify instances of a class of events in text. The ACE 2005 event extraction task involved a set of 33 generic event types and subtypes appearing frequently in the news. In addition to identifying the event itself, it also identifies all of the participants and attributes of each event; these are the entities that are involved in that event.  Identifying an event and its participants and attributes is quite difficult because a larger field of view is often needed to understand how facts 
tie together. Sometimes it is difficult even for people to classify events from isolated sentences. From the sentence: (1) He left the company. it is hard to tell whether it is a Transport event in ACE, which means that he left the place; or an End-Position event, which means that he retired from the company. However, if we read the whole document, a clue like ?he planned to go shopping before he went home? would give us confidence to tag it as a Transport event, while a clue like ?They held a party for his retirement? would lead us to tag it as an End-Position event. Such clues are evidence from the same event type. However, sometimes another event type is also a good predictor. For example, if we find a Start-Position event like ?he was named president three years ago?, we are also confident to tag (1) as End-Position event. Event argument identification also shares this benefit. Consider the following two sentences: (2) A bomb exploded in Bagdad; seven people died while 11 were injured.  (3) A bomb exploded in Bagdad; the suspect got caught when he tried to escape.  If we only consider the local context of the trigger ?exploded?, it is hard to determine that ?seven people? is a likely Target of the Attack event in (2), or that the ?suspect? is the Attacker of the Attack event, because the structures of (2) and (3) are quite similar. The only clue is from the semantic inference that a person who died may well have been a Target of the Attack event, and the person arrested is probably the Attacker of the Attack event. These may be seen as 
789
examples of a broader textual inference problem, and in general such knowledge is quite difficult to acquire and apply. However, in the present case we can take advantage of event extraction to learn these rules in a simpler fashion, which we present below. Most current event extraction systems are based on phrase or sentence level extraction.  Several recent studies use high-level information to aid local event extraction systems. For example, Finkel et al (2005), Maslennikov and Chua (2007), Ji and Grishman (2008), and Patwardhan and Riloff (2007, 2009) tried to use discourse, document, or cross-document information to improve information extraction.  However, most of this research focuses on single event extraction, or focuses on high-level information within a single event type, and does not consider information acquired from other event types. We extend these approaches by introducing cross-event information to enhance the performance of multi-event-type extraction systems. Cross-event information is quite useful: first, some events co-occur frequently, while other events do not. For example, Attack, Die, and Injure events very frequently occur together, while Attack and Marry are less likely to co-occur. Also, typical relations among the arguments of different types of events can be helpful in predicting information to be extracted. For example, the Victim of a Die event is probably the Target of the Attack event. As a result, we extend the observation that ?a document containing a certain event is likely to contain more events of the same type?, and base our approach on the idea that ?a document containing a certain type of event is likely to contain instances of related events?. In this paper, automatically extracted within-event and cross-event information is used to aid traditional sentence level event extraction. 2 Task Description Automatic Content Extraction (ACE) defines an event as a specific occurrence involving participants1, and it annotates 8 types and 33 subtypes of events. We first present some ACE terminology to understand this task more easily: ? Entity: an object or a set of objects in one of the semantic categories of interest, referred to in the document by one or more                                                            1 See http://projects.ldc.upenn.edu/ace/docs/English-Events- Guidelines_v5.4.3.pdf for a description of this task. 
(coreferential) entity mentions. ? Entity mention: a reference to an entity (typically, a noun phrase) ? Timex: a time expression including date, time of the day, season, year, etc. ? Event mention: a phrase or sentence within which an event is described, including trigger and arguments. An event mention must have one and only one trigger, and can have an arbitrary number of arguments. ? Event trigger: the main word that most clearly expresses an event occurrence. An ACE event trigger is generally a verb or a noun. ? Event mention arguments (roles)2 : the entity mentions that are involved in an event mention, and their relation to the event. For example, event Attack might include participants like Attacker, Target, or attributes like Time_within and Place. Arguments will be taggable only when they occur within the scope of the corresponding event, typically the same sentence. Consider the sentence: (4) Three murders occurred in France today, including the senseless slaying of Bob Cole and the assassination of Joe Westbrook. Bob was on his way home when he was attacked?    Event extraction depends on previous phases like name identification, entity mention classification and coreference. Table 1 shows the results of this preprocessing. Note that entity mentions that share the same EntityID are coreferential and treated as the same object.  Entity(Timex) mention head word Entity ID Entity type 0001-1-1 France 0001-1 GPE 0001-T1-1 Today 0001-T1 Timex 0001-2-1 Bob Cole 0001-2 PER 0001-3-1 Joe Westbrook 0001-3 PER 0001-2-2 Bob 0001-2 PER 0001-2-3 He 0001-2 PER Table 1. An example of entities and entity mentions and their types                                                            2 Note that we do not deal with event mention coreference in this paper, so each event mention is treated as a separate event. 
790
There are three Die events, which share the same Place and Time roles, with different Victim roles. And there is one Attack event sharing the same Place and Time roles with the Die events.  Role Event type Trigger Place Victim Time Die murder 0001-1-1  0001-T1-1 Die death 0001-1-1 0001-2-1 0001-T1-1 Die killing 0001-1-1 0001-3-1 0001-T1-1 Role Event type Trigger Place Target Time Attack attack 0001-1-1 0001-2-3 0001-T1-1 Table2. An example of event trigger and roles  In this paper, we treat the 33 event subtypes as separate event types and do not consider the hierarchical structure among them. 3 Related Work Almost all the current ACE event extraction systems focus on processing one sentence at a time (Grishman et al, 2005; Ahn, 2006; Hardy et al 2006). However, there have been several studies using high-level information from a wider scope: Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context to refine the performance of relation extraction. They claimed that discourse information could filter noisy dependency paths as well as increasing the reliability of dependency path extraction. Finkel et al (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. They used this technique to augment an information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. Ji and Grishman (2008) were inspired from the hypothesis of ?One Sense Per Discourse? (Yarowsky, 1995); they extended the scope from a single document to a cluster of topic-related documents and employed a rule-based approach 
to propagate consistent trigger classification and event arguments across sentences and documents. Combining global evidence from related documents with local decisions, they obtained an appreciable improvement in both event and event argument identification. Patwardhan and Riloff (2009) proposed an event extraction model which consists of two components: a model for sentential event recognition, which offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event; and a model for recognizing plausible role fillers, which identifies phrases as role fillers based upon the assumption that the surrounding context is discussing a relevant event. This unified probabilistic model allows the two components to jointly make decisions based upon both the local evidence surrounding each phrase and the ?peripheral vision?. Gupta and Ji (2009) used cross-event information within ACE extraction, but only for recovering implicit time information for events. 4 Motivation We analyzed the sentence-level baseline event extraction, and found that many events are missing or spuriously tagged because the local information is not sufficient to make a confident decision. In some local contexts, it is easy to identify an event; in others, it is hard to do so. Thus, if we first tag the easier cases, and use such knowledge to help tag the harder cases, we might get better overall performance. In addition, global information can make the event tagging more consistent at the document level. Here are some examples. For trigger classification: The pro-reform director of Iran's biggest-selling daily newspaper and official organ of Tehran's municipality has stepped down following the appointment of a conservative ?it was founded a decade ago ? but a conservative city council was elected in the February 28 municipal polls ? Mahmud Ahmadi-Nejad, reported to be a hardliner among conservatives, was appointed mayor on Saturday ?Founded by former mayor Gholamhossein Karbaschi, Hamshahri?    
791
  Figure 1. Conditional probability of the other 32 event types in documents where a Die event appears  
 Figure 2. Conditional probability of the other 32 event types in documents where a Start-Org event appears   The sentence level baseline system finds event triggers like ?founded? (trigger of Start-Org), ?elected? (trigger of Elect), and ?appointment? (trigger of Start-Position), which are easier to identify because these triggers have more specific meanings. However, it does not recognize the trigger ?stepped? (trigger of End-Position) because in the training corpus ?stepped? does not always appear as an End-Position event, and local context does not provide enough information for the MaxEnt model to tag it as a trigger. However, in the document that contains related events like Start-Position, ?stepped? is more likely to be tagged as an End-Position event. For argument classification, the cross-event evidence from the document level is also useful: British officials say they believe Hassan was a blindfolded woman seen being shot in the head by a hooded militant on a video obtained but not aired by the Arab television station Al-Jazeera. She would be the first foreign woman to die in the wave of kidnappings in Iraq?she's been killed by 
(men in pajamas), turn Iraq upside down and find them. From this document, the local information is not enough for our system to tag ?Hassan? as the target of an Attack event, because it is quite far from the trigger ?shot? and the syntax is somewhat complex. However, it is easy to tag ?she? as the Victim of a Die event, because it is the object of the trigger ?killed?. As ?she? and ?Hassan? are co-referred, we can use this easily tagged argument to help identify the harder one. 4.1 Trigger Consistency and Distribution Within a document, there is a strong trigger consistency: if one instance of a word triggers an event, other instances of the same word will trigger events of the same type3.  There are also strong correlations among event types in a document. To see this we calculated the conditional probability (in the ACE corpus) of a certain event type appearing in a document when another event type appears in the same document.                                                            3 This is true over 99.4% of the time in the ACE corpus. 
792
    
 Figure 3. Conditional probability of all possible roles in other event types for entities that are the Targets of Attack events (roles with conditional probability below 0.002 are omitted)   Event Cond. Prob. Attack 0.714 Transport 0.507 Injure 0.306 Meet 0.164 Arrest-Jail 0.153 Sentence 0.126 Phone-Write 0.111 End-Position 0.116 Trial-Hearing 0.105 Convict 0.100 Table 3. Events co-occurring with die events with conditional probability > 10%  As there are 33 subtypes, there are potentially 33?32/2=528 event pairs. However, only a few of these appear with substantial frequency. For example, there are only 10 other event types that occur in more than 10% of the documents in which a die event appears. From Table 3, we can see that Attack, Transport and Injure events appear frequently with Die. We call these the related event types for Die (see Figure 1 and Table 3).  The same thing happens for Start-Org events, although its distribution is quite different from Die events. For Start-Org, there are more related events like End-Org, Start-Position, and End-Position (Figure 2). But there are 12 other event types which never appear in documents containing Start-Org events.  From the above, we can see that the distributions of different event types are quite different, and these distributions might be good predictors for event extraction. 
4.2 Role Consistency and Distribution Normally one entity, if it appears as an argument of multiple events of the same type in a single document, is assigned the same role each time.4 There is also a strong relationship between the roles when an entity participates in different types of events in a single document. For example, we checked all the entities in the ACE corpus that appear as the Target role for an Attack event, and recorded the roles they were assigned for other event types. Only 31 other event-role combinations appeared in total (out of 237 possible with ACE annotation), and 3 clearly dominated. In Figure 3, we can see that the most likely roles for the Target role of the Attack event are the Victim role of the Die or Injure event and the Artifact role of the Transport event. The last of these corresponds to troop movements prior to or in response to attacks. 5 Cross-event Approach In this section we present our approach to using document-level event and role information to improve sentence-level ACE event extraction.  Our event extraction system is a two-pass system where the sentence-level system is first applied to make decisions based on local information. Then the confident local information is collected and gives an approximate view of the content of the document. The document level system is finally applied to deal with the cases which the local 
                                                           4 This is true over 97% of the time in the ACE corpus. 
793
system can?t handle, and achieve document consistency. 5.1 Sentence-level Baseline System We use a state-of-the-art English IE system as our baseline (Grishman et al 2005). This system extracts events independently for each sentence, because the definition of event mention argument constrains them to appear in the same sentence. The system combines pattern matching with statistical models. In the training process, for every event mention in the ACE training corpus, patterns are constructed based on the sequences of constituent heads separating the trigger and arguments. A set of Maximum Entropy based classifiers are also trained: ? Argument Classifier: to distinguish arguments of a potential trigger from non-arguments; ? Role Classifier: to classify arguments by argument role.  ? Reportable-Event Classifier (Trigger Classifier): Given a potential trigger, an event type, and a set of arguments, to determine whether there is a reportable event mention. In the test procedure, each document is scanned for instances of triggers from the training corpus. When an instance is found, the system tries to match the environment of the trigger against the set of patterns associated with that trigger. This pattern-matching process, if successful, will assign some of the mentions in the sentence as arguments of a potential event mention. The argument classifier is applied to the remaining mentions in the sentence; for any argument passing that classifier, the role classifier is used to assign a role to it. Finally, once all arguments have been assigned, the reportable-event classifier is applied to the potential event mention; if the result is successful, this event mention is reported.5 5.2 Document-level Confident Information Collector To use document-level information, we need to collect information based on the sentence-level baseline system. As it is a statistically-based model, it can provide a value that indicates how likely it is that this word is a trigger, or that the mention is an argument and has a particular role.                                                            5 If the event arguments include some assigned by the pattern-matching process, the event mention is accepted unconditionally, bypassing the reportable- event classifier. 
We want to see if this value can be trusted as a confidence score. To this end, we set different thresholds from 0.1 to 1.0 in the baseline system output, and only evaluate triggers, arguments or roles whose confidence score is above the threshold. Results show that as the threshold is raised, the precision generally increases and the recall falls. This indicates that the value is consistent and a useful indicator of event/argument confidence (see Figure 4).6  
 Figure 4. The performance of different confidence thresholds in the baseline system  on the development set  To acquire confident document-level information, we only collect triggers and roles tagged with high confidence. Thus, a trigger threshold t_threshold and role threshold r_threshold are set to remove low confidence triggers and arguments. Finally, a table with confident event information is built. For every event, we collect its trigger and event type; for every argument, we use co-reference information and record every entity and its role(s) in events of a certain type.  To achieve document consistency, in cases where the baseline system assigns a word to triggers for more than one event type, if the margin between the probability of the highest and the second highest scores is above a threshold m_threshold, we only keep the event type with highest score and record this in the confident-event table. Otherwise (if the margin is smaller) the event type assignments will be recorded in a separate conflict table. The same strategy is applied to argument/role conflicts. We will not use information in the conflict table to infer the event type or argument/roles for other event mentions, because we cannot                                                            6 The trigger classification curve doesn?t follow the expected recall/precision trade-off, particularly at high thresholds.  This is due, at least in part, to the fact that some events bypass the reportable-event classifier (trigger classifier) (see footnote 5). At high thresholds this is true of the bulk of the events. 
794
confidently resolve the conflict. However, the event type and argument/role assignments in the conflict table will be included in the final output because the local confidence for the individual assignments is high.  As a result, we finally build two document-level confident-event tables: the event type table and the argument (role) table. A conflict table is also built but not used for further predictions (see Table 4).  Confident table Event type table Trigger Event Type Met Meet Exploded Attack Went Transport   Injured Injure Attacked Attack Died Die Argument role table Entity ID Event type Role 0004-T2 Die Time Within 0004-6 Die Place 0004-4 Die Victim 0004-7 Die Agent 0004-11 Attack Target 0004-T3 Attack Time Within 0004-12 Attack Place 0004-10 Attack Attacker Conflict table Entity ID Event type Roles 0004-8 Attack Victim, Agent Table 4. Example of document-level confident-event table (event type and argument role entries) and conflict table  5.3 Statistical Cross-event Classifiers To take advantage of cross-event relationships, we train two additional MaxEnt classifiers ? a document-level trigger and argument classifier ? and then use these classifiers to infer additional events and event arguments. In analyzing new text, the trigger classifier is first applied to tag an event, and then the argument (role) classifier is applied to tag possible arguments and roles of this event.  5.3.1 Document Level Trigger Classifier From the document-level confident-event table, we have a rough view of what kinds of events 
are reported in this document. The trigger classifier predicts whether a word is the trigger of an event, and if so of what type, given the information (from the confident-event table) about other types of events in the document. Each feature of this classifier is the conjunction of: ? The base form of the word ? An event type ? A binary indicator of whether this event type is present elsewhere in the document (There are 33 event types and so 33 features for each word).  5.3.2 Document Level Argument (Role) Classifier The role classifier predicts whether a given mention is an argument of a given event and, if so, what role it takes on, again using information from the confident-event table about other events. As noted above, we assume that the role of an entity is unique for a specific event type, although an entity can take on different roles for different event types. Thus, if there is a conflict in the document level table, the collector will only keep the one with highest confidence, or discard them all. As a result, every entity is assigned a unique role with respect to a particular event type, or null if it is not an argument of a certain event type. Each feature is the conjunction of: ? The event type we are trying to assign an argument/role to. ? One of the 32 other event types ? The role of this entity with respect to the other event type elsewhere in the document, or null if this entity is not an argument of that type of event  5.4 Document Level Event Tagging At this point, the low-confidence triggers and arguments (roles) have been removed and the document-level confident-event table has been built; the new classifiers are now used to augment the confident tags that were previously assigned based on local information. For trigger tagging, we only apply the classifier to the words that do not have a confident local labeling; if the trigger is already in the document level confident-event table, we will not re-tag it.   
795
           performance system/human Trigger classification Argument classification Role classification  P R F P R F P R F Sentence-level baseline system 67.56 53.54 59.74 46.45 37.15 41.29 41.02 32.81 36.46 Within-event-type rules 63.03 59.90 61.43 48.59 46.16 47.35 43.33 41.16 42.21 Cross-event statistical model 68.71 68.87 68.79 50.85 49.72 50.28 45.06 44.05 44.55 Human annotation1 59.2 59.4 59.3 60.0 69.4 64.4 51.6 59.5 55.3 Human annotation2 69.2 75.0 72.0 62.7 85.4 72.3 54.1 73.7 62.4 Table 5. Overall performance on blind test data  The argument/role tagger is then applied to all events?those in the confident-event table and those newly tagged. For argument tagging, we only consider the entity mentions in the same sentence as the trigger word, because by the ACE event guidelines, the arguments of an event should appear within the same sentence as the trigger. For a given event, we re-tag the entity mentions that have not already been assigned as arguments of that event by the confident-event or conflict table. 6 Experiments We followed Ji and Grishman (2008)?s evaluation and randomly select 10 newswire texts from the ACE 2005 training corpora as our development set, which is used for parameter tuning, and then conduct a blind test on a separate set of 40 ACE 2005 newswire texts. We use the rest of the ACE training corpus (549 documents) as training data for both the sentence-level baseline event tagger and document-level event tagger.  To compare with previous work on within-event propagation, we reproduced Ji and Grishman (2008)?s approach for cross-sentence, within-event-type inference (see ?within-event-type rules? in Table 5). We applied their within-document inference rules using the cross-sentence confident-event information. These rules basically serve to adjust trigger and argument classification to achieve document-wide consistency. This process treats each event type separately: information about events of a given type is used to infer information about other events of the same type. We report the overall Precision (P), Recall (R), and F-Measure (F) on blind test data. In addition, we also report the performance of two human 
annotators on 28 ACE newswire texts (a subset of the blind test set).7 From the results presented in Table 5, we can see that using the document level cross-event information, we can improve the F score for trigger classification by 9.0%, argument classification by 9.0%, and role classification by 8.1%. Recall improved sharply, demonstrating that cross-event information could recover information that is difficult for the sentence-level baseline to extract; precision also improved over the baseline, although not as markedly. Compared to the within-event-type rules, the cross-event model yields much more improvement for trigger classification: rule-based propagation gains 1.7% improvement while the cross-event model achieves a further 7.3% improvement. For argument and role classification, the cross-event model also gains 3% and 2.3% above that obtained by the rule-based propagation process. 7 Conclusion and Future Work We propose a document-level statistical model for event trigger and argument (role) classification to achieve document level within-event and cross-event consistency. Experiments show that document-level information can improve the performance of a sentence-level baseline event extraction system.  The model presented here is a simple two-stage recognition process; nonetheless, it has proven sufficient to yield substantial improvements in event recognition and event                                                            7 The final key was produced by review and adjudication of the two annotations by a third annotator, which indicates that the event extraction task is quite difficult and human agreement is not very high. 
796
argument recognition. Richer models, such as those based on joint inference, may produce even greater gains. In addition, extending the approach to cross-document information, following (Ji and Grishman 2008), may be able to further improve performance. References  David Ahn. 2006. The stages of event extraction. In Proc. COLING/ACL 2006 Workshop on Annotating and Reasoning about Time and Events. Sydney, Australia.  J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proc. 43rd Annual Meeting of the Association for Computational Linguistics, pages 363?370, Ann Arbor, MI, June. Ralph Grishman, David Westbrook and Adam Meyers. 2005. NYU?s English ACE 2005 System Description. In Proc. ACE 2005 Evaluation Workshop, Gaithersburg, MD. Prashant Gupta, Heng Ji. 2009. Predicting Unknown Time Arguments based on Cross-Event Propagation. In Proc. ACL-IJCNLP 2009. Hilda Hardy, Vika Kanchakouskaya and Tomek Strzalkowski. 2006. Automatic Event Classification Using Surface Text Features. In Proc. AAAI06 Workshop on Event Extraction and Synthesis. Boston, MA. H. Ji and R. Grishman. 2008. Refining Event Extraction through Cross-Document Inference. In Proc. ACL-08: HLT, pages 254?262, Columbus, OH, June. M. Maslennikov and T. Chua. 2007. A Multi resolution Framework for Information Extraction from Free Text. In Proc. 45th Annual Meeting of the Association of Computational Linguistics, pages 592?599, Prague, Czech Republic, June.  S. Patwardhan and E. Riloff. 2007. Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions. In Proc. Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 2007, pages 717?727, Prague, Czech Republic, June. Patwardhan, S. and Riloff, E. 2009. A Unified Model of Phrasal and Sentential Evidence for Information Extraction. In Proc. Conference on Empirical Methods in Natural Language Processing 2009, (EMNLP-09). David Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proc. ACL 1995. Cambridge, MA.  
797
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 260?265,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Can Document Selection Help Semi-supervised Learning?  
A Case Study On Event Extraction 
 
 
Shasha Liao Ralph Grishman 
Computer Science Department 
New York University 
liaoss@cs.nyu.edu grishman@cs.nyu.edu 
 
 
 
 
 
 
Abstract 
Annotating training data for event 
extraction is tedious and labor-intensive. 
Most current event extraction tasks rely 
on hundreds of annotated documents, but 
this is often not enough. In this paper, we 
present a novel self-training strategy, 
which uses Information Retrieval (IR) to 
collect a cluster of related documents as 
the resource for bootstrapping. Also, 
based on the particular characteristics of 
this corpus, global inference is applied to 
provide more confident and informative 
data selection. We compare this approach 
to self-training on a normal newswire 
corpus and show that IR can provide a 
better corpus for bootstrapping and that 
global inference can further improve 
instance selection. We obtain gains of 
1.7% in trigger labeling and 2.3% in role 
labeling through IR and an additional 
1.1% in trigger labeling and 1.3% in role 
labeling by applying global inference. 
1 Introduction 
The goal of event extraction is to identify 
instances of a class of events in text. In addition 
to identifying the event itself, it also identifies 
all of the participants and attributes of each 
event; these are the entities that are involved in 
that event. The same event might be presented 
in various expressions, and an expression might 
represent different events in different contexts. 
Moreover, for each event type, the event 
participants and attributes may also appear in 
multiple forms and exemplars of the different 
forms may be required. Thus, event extraction is 
a difficult task and requires substantial training 
data. However, annotating events for training is 
a tedious task. Annotators need to read the 
whole sentence, possibly several sentences, to 
decide whether there is a specific event or not, 
and then need to identify the event participants 
(like Agent and Patient), and attributes (like 
place and time) to complete an event annotation. 
As a result, for event extraction tasks like 
MUC4, MUC6 (MUC 1995) and ACE2005, 
from one to several hundred annotated 
documents were needed. 
In this paper, we apply a novel self-training 
process on an existing state-of-the-art baseline 
system. Although traditional self-training on 
normal newswire does not work well for this 
specific task, we managed to use information 
retrieval (IR) to select a better corpus for 
bootstrapping. Also, taking advantage of 
properties of this corpus, cross-document 
inference is applied to obtain more 
?informative? probabilities. To the best of our 
knowledge, we are the first to apply information 
retrieval and global inference to semi-supervised 
learning for event extraction. 
2 Task Description 
Automatic Content Extraction (ACE) defines an 
event as a specific occurrence involving 
260
participants 1 ; it annotates 8 types and 33 
subtypes of events.2 We first present some ACE 
terminology to understand this task more easily: 
? Event mention3: a phrase or sentence within 
which an event is described, including one 
trigger and an arbitrary number of arguments.  
? Event trigger: the main word that most 
clearly expresses an event occurrence. 
? Event mention arguments (roles): the entity 
mentions that are involved in an event 
mention, and their relation to the event.  
Here is an example: 
(1) Bob Cole was killed in France today; 
he was attacked?    
Table 1 shows the results of the preprocessing, 
including name identification, entity mention 
classification and coreference, and time 
stamping. Table 2 shows the results for event 
extraction. 
 
Mention 
ID 
Head  Ent.ID Type 
E1-1 France E-1 GPE 
T1-1 today T1 Timex 
E2-1 Bob Cole E-2 PER 
E2-2 He E-2 PER 
 
Table 1. An example of entities and entity 
mentions and their types 
 
Event 
type 
Trigger Role 
Place Victim Time 
Die killed E1-1 E2-1 T1-1 
  Place Target Time 
Attack attacked E1-1 E2-2 T1-1 
 
Table 2. An example of event triggers and roles 
                                                          
1http://projects.ldc.upenn.edu/ace/docs/English-Event
s-Guidelines_v5.4.3.pdf 
2  In this paper, we treat the event subtypes 
separately, and no type hierarchy is considered. 
3  Note that we do not deal with event mention 
coreference in this paper, so each event mention is 
treated separately.  
3 Related Work 
Self-training has been applied to several natural 
language processing tasks. For event extraction, 
there are several studies on bootstrapping from a 
seed pattern set. Riloff (1996) initiated the idea of 
using document relevance for extracting new 
patterns, and Yangarber et al (2000, 2003) 
incorporated this into a bootstrapping approach, 
extended by Surdeanu et al (2006) to co-training. 
Stevenson and Greenwood (2005) suggested an 
alternative method for ranking the candidate 
patterns by lexical similarities. Liao and 
Grishman (2010b) combined these two 
approaches to build a filtered ranking algorithm. 
However, these approaches were focused on 
finding instances of a scenario/event type rather 
than on argument role labeling. Starting from a 
set of documents classified for relevance, 
Patwardhan and Riloff (2007) created a 
self-trained relevant sentence classifier and 
automatically learned domain-relevant extraction 
patterns. Liu (2009) proposed the BEAR system, 
which tagged both the events and their roles. 
However, the new patterns were boostrapped 
based on the frequencies of sub-pattern mutations 
or on rules from linguistic contexts, and not on 
statistical models. 
The idea of sense consistency was first 
introduced and extended to operate across related 
documents by (Yarowsky, 1995). Yangarber et 
al. (Yangarber and Jokipii, 2005; Yangarber, 
2006; Yangarber et al, 2007) applied 
cross-document inference to correct local 
extraction results for disease name, location and 
start/end time. Mann (2007) encoded specific 
inference rules to improve extraction of 
information about CEOs (name, start year, end 
year). Later, Ji and Grishman (2008) employed a 
rule-based approach to propagate consistent 
triggers and arguments across topic-related 
documents. Gupta and Ji (2009) used a similar 
approach to recover implicit time information for 
events. Liao and Grishman (2010a) use a 
statistical model to infer the cross-event 
information within a document to improve event 
extraction.  
261
4 Event Extraction Baseline System 
We use a state-of-the-art English IE system as 
our baseline (Grishman et al 2005). This system 
extracts events independently for each sentence, 
because the definition of event mention 
arguments in ACE constrains them to appear in 
the same sentence. The system combines pattern 
matching with statistical models. In the training 
process, for every event mention in the ACE 
training corpus, patterns are constructed based on 
the sequences of constituent heads separating the 
trigger and arguments. A set of Maximum 
Entropy based classifiers are also trained: 
? Argument Classifier: to distinguish 
arguments of a potential trigger from 
non-arguments. 
? Role Classifier: to classify arguments by 
argument role. We use the same features as 
the argument classifier. 
? Reportable-Event Classifier (Trigger 
Classifier): Given a potential trigger, an 
event type, and a set of arguments, to 
determine whether there is a reportable 
event mention. 
In the test procedure, each document is 
scanned for instances of triggers from the 
training corpus. When an instance is found, the 
system tries to match the environment of the 
trigger against the set of patterns associated with 
that trigger. If this pattern-matching process 
succeeds, the argument classifier is applied to the 
entity mentions in the sentence to assign the 
possible arguments; for any argument passing 
that classifier, the role classifier is used to assign 
a role to it. Finally, once all arguments have been 
assigned, the reportable-event classifier is 
applied to the potential event mention; if the 
result is successful, this event mention is 
reported. 
5 Our Approach 
In self-training, a classifier is first trained with a 
small amount of labeled data. The classifier is 
then used to classify the unlabeled data. 
Typically the most confident unlabeled points, 
together with their predicted labels, are added to 
the training set. The classifier is re-trained and 
the procedure repeated. As a result, the criterion 
for selecting the most confident examples is 
critical to the effectiveness of self-training. 
To acquire confident samples, we need to first 
decide how to evaluate the confidence for each 
event. However, as an event contains one trigger 
and an arbitrary number of roles, a confident 
event might contain unconfident arguments. 
Thus, instead of taking the whole event, we select 
a partial event, containing one confident trigger 
and its most confident argument, to feed back to 
the training system.  
For each mention mi, its probability of filling a 
role r in a reportable event whose trigger is t is 
computed by: 
? 
PRoleOfTrigger(mi,r,t) = PArg(mi) ? PRole(mi,r) ? PEvent (t) 
 where PArg(mi) is the probability from the 
argument classifier, PRole(mi,r) is that from the 
role classifier, and PEvent(t) is that from the 
trigger classifier. In each iteration, we added the 
most confident <role, trigger> pairs to the 
training data, and re-trained the system. 
5.1 Problems of Traditional Self-training 
(ST) 
However, traditional self-training does not 
perform very well (see our results in Table 3). 
The newly added samples do not improve the 
system performance; instead, its performance 
stays stable, and even gets worse after several 
iterations.  
We analyzed the data, and found that this is 
caused by two common problems of traditional 
self-training. First, the classifier uses its own 
predictions to train itself, and so a classification 
mistake can reinforce itself. This is particularly 
true for event extraction, due to its relatively poor 
performance, compared to other NLP tasks, like 
Named Entity Recognition, parsing, or 
part-of-speech tagging, where self-training has 
been more successful. Figure 1 shows that the 
precision using the original training data is not 
very good: while precision improves with 
increasing classifier threshold, about 1/3 of the 
roles are still incorrectly tagged at a threshold of 
0.90. 
 
262
0.35 
0.4 
0.45 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
Trigger Labeling 
Argument Labeling 
Role Labeling 
 
 
Figure 1. Precision on the original training data 
with different thresholds (from 0.0 to 0.9) 
 
Another problem of self-training is that 
nothing ?novel? is added because the most 
confident examples are those frequently seen in 
the training data and might not provide ?new? 
information. Co-training is a form of 
self-training which can address this problem to 
some extent. However, it requires two views of 
the data, where each example is described using 
two different feature sets that provide different, 
complementary information. Ideally, the two 
views are conditionally independent  and each 
view is sufficient (Zhu, 2008). Co-training has 
had some success in training (binary) semantic 
relation extractors for some relations, where the 
two views correspond to the arguments of the 
relation and the context of these arguments 
(Agichtein and Gravano 2000).  However, it has 
had less success for event extraction because 
event arguments may participate in multiple 
events in a corpus and individual event instances 
may omit some arguments. 
5.2 Self-training on Information Retrieval 
Selected Corpus (ST_IR) 
To address the first problem (low precision of 
extracted events), we tried to select a corpus 
where the baseline system can tag the instances 
with greater confidence. (Ji and Grishman 2008) 
have observed that the events in a cluster of 
documents on the same topics as documents in 
the training corpus can be tagged more 
confidently. Thus, we believe that bootstrapping 
on a corpus of topic-related documents should 
perform better than a regular newswire corpus. 
We followed Ji and Grishman (2008)?s 
approach and used the INDRI retrieval system4 
(Strohman et al, 2005) to obtain the top N  
                                                          
4 http://www.lemurproject.org/indri/ 
related documents for each annotated document 
in the training corpus. The query is event-based 
to insure that related documents contain the same 
events. For each training document, we construct 
an INDRI query from the triggers and arguments. 
For example, for sentence (1) in section 2, we use 
the keywords ?killed?, ?attacked?, ?France?, 
?Bob Cole?, and ?today? to extract related 
documents. Only names and nominal arguments 
will be used; pronouns appearing as arguments 
are not included. For each argument we also add 
other names coreferential with the argument. 
5.3 Self-training using Global Inference 
(ST_GI) 
Although bootstrapping on related documents 
can solve the problem of ?confidence? to some 
extent, the ?novelty? problem still remains:  the 
top-ranked extracted events will be too similar to 
those in the training corpus. To address this 
problem, we propose to use a simple form of 
global inference based on the special 
characteristics of related-topic documents. 
Previous studies pointed out that information 
from wider scope, at the document or 
cross-document level, could provide non-local 
information to aid event extraction (Ji and 
Grishman 2008, Liao and Grishman 2010a). 
There are two common assumptions within a 
cluster of related documents (Ji and Grishman 
2008): 
? Trigger Consistency Per Cluster: if one 
instance of a word triggers an event, other 
instances of the same word will trigger events 
of the same type. 
? Role Consistency Per Cluster: if one entity 
appears as an argument of multiple events of 
the same type in a cluster of related 
documents, it should be assigned the same 
role each time. 
Based on these assumptions, if a trigger/role 
has a low probability from the baseline system, 
but a high one from global inference, it means 
that the local context of this trigger/role tag is not 
frequently seen in the training data, but the tag is 
still confident. Thus, we can confidently add it to 
the training data and it can provide novel 
information which the samples confidently 
tagged by the baseline system cannot provide. 
263
To start, the baseline system extracts a set of 
events and estimates the probability that a 
particular instance of a word triggers an event of 
that type, and the probability that it takes a 
particular argument. The global inference 
process then begins by collecting all the 
confident triggers and arguments from a cluster 
of related documents.5 For each trigger word and 
event type, it records the highest probability 
(over all instances of that word in the cluster) that 
the word triggers an event of that type.  For each 
argument, within-document and cross-document 
coreference6 are used to collect all instances of 
that entity; we then compute the maximum 
probability (over all instances) of that argument 
playing a particular role in a particular event 
type. These maxima will then be used in place of 
the locally-computed probabilities in computing 
the probability of each trigger-argument pair in 
the formula for PRoleOfTrigger given above.
7  For 
example, if the entity ?Iraq? is tagged confidently 
(probability > 0.9) as the ?Attacker? role 
somewhere in a cluster, and there is another 
instance where from local information it is only 
tagged with 0.1 probability to be an ?Attacker? 
role, we use probability of 0.9 for both instances. 
In this way, a trigger pair containing this 
argument is more likely to be added into the 
training data through bootstrapping, because we 
have global evidence that this role probability is 
high, although its local confidence is low. In this 
way, some novel trigger-argument pairs will be 
chosen, thus improving the baseline system. 
6 Results 
We randomly chose 20 newswire texts from the 
ACE 2005 training corpora (from March to May 
of 2003) as our evaluation set, and used the 
                                                          
5 In our experiment, only triggers and roles with 
probability higher than 0.9 will be extracted. 
6 We use a statistical within-document coreference 
system (Grishman et al 2005), and a simple 
rule-based cross-document coreference system, 
where entities sharing the same names will be treated 
as coreferential across documents. 
7 If a word or argument has multiple tags (different 
event types or roles) in a cluster, and the difference 
in the probabilities of the two tags is less than some 
threshold, we treat this as a ?conflict? and do not use 
the conflicting information for global inference. 
remaining newswire texts as the original training 
data (83 documents). For self-training, we picked 
10,000 consecutive newswire texts from the 
TDT5 corpus from 20038 for the ST experiment. 
For ST_IR and ST_GI, we retrieved the best N 
(using N = 25, which (Ji and Grishman 2008) 
found to work best) related texts for each training 
document from the English TDT5 corpus 
consisting of 278,108 news texts (from April to 
September of 2003). In total we retrieved 1650 
texts; the IR system returned no texts or fewer 
than 25 texts for some training documents. In 
each iteration, we extract 500 trigger and 
argument pairs to add to the training data. 
Results (Table 3) show that bootstrapping on 
an event-based IR corpus can produce 
improvements on all three evaluations, while 
global inference can yield further gains.  
 
 Trigger 
labeling 
Argument 
labeling 
Role 
labeling 
Baseline 54.1 39.2 35.4 
ST 54.2 40.0 34.6 
ST_IR 55.8 42.1 37.7 
ST_GI 56.9 43.8 39.0 
 
Table 3. Performance (F score) with different 
self-training strategies after 10 iterations 
7 Conclusions and Future Work 
We proposed a novel self-training process for 
event extraction that involves information 
retrieval (IR) and global inference to provide 
more accurate and informative instances. 
Experiments show that using an IR-selected 
corpus improves trigger labeling F score 1.7%, 
and role labeling 2.3%. Global inference can 
achieve further improvement of 1.1% for trigger 
labeling, and 1.3% for role labeling. Also, this 
bootstrapping involves processing a much 
                                                          
8  We selected all bootstrapping data from 2003 
newswire, with the same genre and time period as 
ACE 2005 data to avoid possible influences of 
variations in the genre or time period on the 
bootstrapping. Also, we selected 10,000 documents 
because this size of corpus yielded a set of 
confidently-extracted events (probability > 0.9) 
roughly comparable in size to those extracted from 
the IR-selected corpus; a larger corpus would have 
slowed the bootstrapping. 
264
smaller but more closely related corpus, which is 
more efficient. Such pre-selection of documents 
may benefit bootstrapping for other NLP tasks as 
well, such as name and relation extraction. 
Acknowledgments 
We would like to thank Prof. Heng Ji for her kind 
help in providing IR data and useful suggestions. 
References  
Eugene Agichtein and Luis Gravano.  2000.  
Snowball:  Extracting relations from large 
plain-text collections. In Proceedings of 5th ACM 
International Conference on Digital Libraries. 
Ralph Grishman, David Westbrook and Adam 
Meyers. 2005. NYU?s English ACE 2005 System 
Description. In Proc. ACE 2005 Evaluation 
Workshop, Gaithersburg, MD. 
Prashant Gupta and Heng Ji. 2009. Predicting 
Unknown Time Arguments based on Cross-Event 
Propagation. In Proceedings of ACL-IJCNLP 
2009. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction through Cross-Document Inference. In 
Proceedings of ACL-08: HLT, pages 254?262, 
Columbus, OH, June. 
Shasha Liao and Ralph Grishman. 2010a. Using 
Document Level Cross-Event Inference to 
Improve Event Extraction. In Proceedings of ACL 
2010. 
Shasha Liao and Ralph Grishman. 2010b. Filtered 
Ranking for Bootstrapping in Event Extraction. In 
Proceedings of COLING 2010. 
Ting Liu. 2009. Bootstrapping events and relations 
from text. Ph.D. thesis, State University of New 
York at Albany. 
Gideon Mann. 2007. Multi-document Relationship 
Fusion via Constraints on Probabilistic Databases. 
In Proceedings of HLT/NAACL 2007. Rochester, 
NY, US. 
MUC. 1995. Proceedings of the Sixth Message 
Understanding Conference (MUC-6), San Mateo, 
CA. Morgan Kaufmann. 
S. Patwardhan and E. Riloff. 2007. Effective 
Information Extraction with Semantic Affinity 
Patterns and Relevant Regions. In Proceedings of 
the 2007 Conference on Empirical Methods in 
Natural Language Processing (EMNLP-07). 
Ellen Riloff. 1996. Automatically Generating 
Extraction Patterns from Untagged Text. In 
Proceedings of Thirteenth National Conference on 
Artificial Intelligence (AAAI-96), pp. 1044-1049. 
M. Stevenson and M. Greenwood. 2005. A Semantic 
Approach to IE Pattern Induction. In Proceedings 
of ACL 2005. 
Trevor Strohman, Donald Metzler, Howard Turtle 
and W. Bruce Croft. 2005. Indri: A 
Language-model based Search Engine for 
Complex Queries (extended version). Technical 
Report IR-407, CIIR, UMass Amherst, US. 
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 
2006. A Hybrid Approach for the Acquisition of 
Information Extraction Patterns. In Proceedings of 
the EACL 2006 Workshop on Adaptive Text 
Extraction and Mining (ATEM 2006). 
Roman Yangarber, Ralph Grishman, Pasi 
Tapanainen, and Silja Huttunen. 2000. Automatic 
Acquisition of Domain Knowledge for 
Information Extraction. In Proceedings of 
COLING 2000. 
Roman Yangarber. 2003. Counter-Training in 
Discovery of Semantic Patterns. In Proceedings of 
ACL2003. 
Roman Yangarber and Lauri Jokipii. 2005. 
Redundancy-based Correction of Automatically 
Extracted Facts. In Proceedings of HLT/EMNLP 
2005. Vancouver, Canada. 
Roman Yangarber. 2006. Verification of Facts across 
Document Boundaries.  In Proceedings of 
International Workshop on Intelligent Information 
Access. Helsinki, Finland. 
Roman Yangarber, Clive Best, Peter von Etter, Flavio 
Fuart, David Horby and Ralf Steinberger. 2007. 
Combining Information about Epidemic Threats 
from Multiple Sources. In Proceedings of RANLP 
2007 workshop on Multi-source, Multilingual 
Information Extraction and Summarization. 
Borovets, Bulgaria. 
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proceedings of ACL 1995. Cambridge, MA.  
Xiaojin Zhu. 2008 Semi-Supervised Learning 
Literature Survey. http:// pages.cs.wisc.edu/ 
~jerryzhu/research/ssl/semireview.html 
 
265
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60?68,
Beijing, August 2010
Large Corpus-based Semantic Feature Extraction  for Pronoun Coreference  
Shasha Liao Dept. of Computer Science New York University liaoss@cs.nyu.edu 
Ralph Grishman Dept. of Computer Science New York University  grishman@cs.nyu.edu 
 Abstract 
Semantic information is a very important factor in coreference resolution. The combination of large corpora and ?deep? analysis procedures has made it possible to acquire a range of semantic informa-tion and apply it to this task. In this pa-per, we generate two statistically-based semantic features from a large corpus and measure their influence on pronoun coreference. One is contextual compati-bility, which decides if the antecedent can be used in the anaphor?s context; the other is role pair, which decides if the ac-tions asserted of the antecedent and the anaphor are likely to apply to the same entity. We apply a semantic labeling sys-tem and a baseline coreference system to a large corpus to generate semantic pat-terns and convert them into features in a MaxEnt model. These features produce an absolute gain of 1.5% to 1.7% in reso-lution accuracy (a 6% reduction in er-rors). To understand the limitations of these features, we also extract patterns from the test corpus, use these patterns to train a coreference model, and examine some of the cases where coreference still fails. We also compare the performance of patterns extracted from semantic role labeling and syntax. 1 Introduction Coreference resolution is the task of determining whether two phrases refer to the same entity. 
Coreference is critical to most NLP tasks, yet even the sub-problem of pronoun coreference remains very challenging. In principle, we need several types of information to identify the right antecedent. First, number and gender agreement constraints can narrow the candidate set.  If mul-tiple candidates remain, we would next use some sequence or syntactic features, like position, word, word salience and discourse focus. For example, whether an antecedent is in subject po-sition might be helpful because the subject is more likely to be referred to; or an entity that has been referred to repeatedly is more likely to be referred to again. However, these features do not suffice to pick the correct antecedent, and some-times similar syntactic structures might have quite different coreference solutions. For exam-ple, for the following two sentences: (1) The terrorist shot a 13-year-old boy; he was arrested after the attack. (2) The terrorist shot a 13-year-old boy; he was fatally wounded in the attack. it is likely that ?he? refers to ?terrorist? in (1) and ?boy? in (2). However, we cannot get the right antecedent using the features we mentioned above because the examples share the same ante-cedent words and syntactic structure.  People can still resolve these correctly because ?terrorist? is more likely to be arrested than ?boy?, and be-cause the one shooting is more likely to be ar-rested than the one being shot. In such cases, semantic constraints and prefer-ences are required for correct coreference resolu-tion. Methods for acquiring and using such knowledge are receiving increasing attention in 
60
recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all ex-plored this task.  However, this task is difficult because it re-quires the acquisition of a large amount of se-mantic information. Furthermore, there is not universal agreement on the value of these seman-tic preferences for pronoun coreference. Kehler et al (2004) reported that such information did not produce apparent improvement in overall pronoun resolution.  In this paper, we will extract semantic features from a semantic role labeling system instead of a parse tree, and explore whether pronoun corefer-ence resolution can benefit from such knowledge, which is automatically extracted from a large corpus. We studied two features: the contextual compatibility feature which has been demon-strated to work at the syntactic level by previous work; and the role pair feature, which has not previously been applied to general domain pro-noun co-reference. In addition, to obtain a rough upper bound on the benefits of our approach and understand its limitations, we conducted a second experiment in which the semantic knowledge is extracted from the evaluation corpus.  We will use the term mention to describe an individual referring phrase. For most studies of coreference, mentions are noun phrases and may be headed by a name, a common noun, or a pro-noun.  We will use the term entity to refer to a set of coreferential mentions. 2 Related Work Contextual compatibility features have long been studied for pronoun coreference: Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution. It determined the preference of candidates based on predicate-argument fre-quencies. Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution. They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge. They got substantial gains on articles in two spe-cific domains, terrorism and natural disasters.  Yang et al (2005) use statistically-based se-mantic compatibility information to improve 
pronoun resolution. They use corpus-based and web-based extraction strategies, and their work shows that statistically-based semantic compati-bility information can improve coreference reso-lution. In contrast, Kehler et al (2004) claimed that the contextual compatibility feature does not help much for pronoun coreference: existing learning-based approaches already performed well; such statistics are simply not good predictors for pro-noun interpretation; data is sparse in the collected predicate-argument statistics. The role pair feature has not been studied for general, broad-domain pronoun co-reference, but it has been used for other tasks: Pekar (2006) built pairs of 'templates' which share an 'anchor' argument; these correspond closely to our role pairs.  Association statistics of the template pairs were used to acquire verb entailments. Abe et al (2008) looked for pairs appearing in specific syn-tactic patterns in order to acquire finer-grained event relations.  Chambers and Jurafsky (2008) built narrative event chains, which are partially ordered sets of events related by a common pro-tagonist. They use high-precision hand-coded rules to get coreference information, extract predicate arguments that link the mentions to verbs, and link the arguments of the coreferred mentions to build a verb entailment model.  Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.  They use these features to improve coreference resolution for two domain-specific corpora in-volving terrorism and natural disasters. Their result raises the natural question as to whether the approach (which may capture domain-specific pairs such as ?kidnap?release? in the terrorism domain) can be successfully extended to a general news corpus.  We address this ques-tion in the experiments reported here. 3 Corpus Analysis In order to extract semantic features from our large training corpus, we apply a sequence of analyzers. These include name tagging, parsing, a baseline coreference analyzer, and, most im-portant, a semantic labeling system that can gen-erate the logical grammatical and predicate-argument representation automatically from a 
61
parse tree (Meyers et al 2009). We use semantic labeling because it provides more general and meaningful patterns, with a ?deeper? analysis than parsed text. The output of the semantic la-beling is the dependency representation of the text, where each sentence is a graph consisting of nodes (corresponding to words) and arcs. Each arc captures up to three relations between two words: (1) a SURFACE relation, the relation be-tween a predicate and an argument in the parse of a sentence; (2) a LOGIC1 (grammatical logical) relation which regularizes for lexical and syntac-tic phenomena like passive, relative clauses, and deleted subjects; and (3) a LOGIC2 (predicate-argument) relation corresponding to relations in PropBank and NomBank. It is designed to be compatible with the Penn TreeBank (Marcus et al, 1994) framework and therefore, Penn Tree-Bank-based parsers, while incorporating Named Entities, PropBank, and NomBank.  Because nouns and verbs provide the most relevant contexts and capture the events in which the entities participate, we generate semantic pat-terns (triples) only for those arcs with verb or noun heads.  We use the following relations: ? Logic2 relations:  We use in particular the Arg0 relation (which corresponds roughly to agent) and Arg1 relation (which corresponds roughly to patient).  ? Logic1 relations: We use in particular the Sbj and Obj relations, representing the logical subject and object of a verb (regularizing passive, relative clauses, deleted subjects) ? Surface relations: T-pos relation is particu-larly used, which captures the head noun ? determiner relation for possessive constructs such as ?bomber?s attack? and ?his responsi-bility?. For example, for the sentence: John is hit by Tom?s brother. we generate the semantic patterns  <Arg1 hit John> <Arg0 hit brother> <T-pos brother Tom>  We apply this labeling system to all the data we use, and to generate the semantic pattern, we take first its predicate-argument role; if that is 
null, we take its logical grammatical role; if both are null, we take its surface role.    To reduce data sparseness, all inflected words are changed to their base form (e.g. ?attack-ers???attacker?). All names are replaced by their ACE types (person, organization, location, etc.). Only patterns with noun arguments are ex-tracted because we only consider noun phrases as possible antecedents. 4 Semantic Features 4.1 Contextual Compatibility Patterns Pronouns, especially neutral pronouns (?it?, ?they?), carry little semantics of their own, so examining the compatibility of the context of a pronoun and its candidate antecedents is a good way to improve antecedent selection. Specifi-cally, we want to determine whether the predi-cate, which is applied to the anaphor, can be ap-plied to the antecedents.  We take the semantic pattern with the anaphor in third position. Then, each candidate antecedent is substituted for the anaphor to see if it is suitable for the context. For example, consider the sentence The company issued a statement that it  bought G.M. which would generate the semantic patterns  <Arg0 issue company> <Arg1 issue statement> <Arg0 buy it> <Arg1 buy Organization>  (here ?G.M? is a name of type organization and so is replaced by the token Organization).  The relevant context of the anaphor is the semantic pattern <Arg0 buy it>.  Suppose there are two candidate antecedents for ?it?: ?company? and ?statement?. We would generate the two seman-tic patterns <Arg0 buy company> and <Arg0 buy statement>. Assuming <Arg0 buy company> is more highly ranked than <Arg0 buy statement>, we can infer that the anaphor is more likely to refer to ?company?. (We describe the specific metric we use for ranking below, in section 4.3.)  As further examples consider:  (3) The suspect's lawyer, Chifumu Banda, told the court he had advised Chiluba not to ap-pear in court Friday. 
62
(4) Foreign military analysts said it would be highly unusual for an accident to kill a whole submarine crew and they suggested possible causes to a disaster? For (3), if we know that a lawyer is more likely to give advice than a suspect, we could link ?he? to ?lawyer? instead of ?suspect? in the first sentence. For (4), if we know that analysts are more likely to ?suggest? than crew, we can link ?they? to ?analysts? in the second sentence. 4.2 Role Pair Patterns The role pair pattern is a new feature in general pronoun co-reference.  The original intuition for introducing it into coreference is that there are pairs of actions involving the same entity that are much more likely to occur together than would be true if one assumed statistical independence.  The second action may be a rephrasing or elabo-ration of the first, or the two might be actions that are part of a common ?script?.  For example: (5) Prime Minister Mahathir Mohamad sacked the former deputy premier in 1998, who was sentenced to a total of 15 years in jail after being convicted of corruption and sodomy.   He was released after four years because?.  (6) The robber attacked the boy with a knife; he was bleeding heavily and died in the hospital the next day. For (5), if we know that the person who was sentenced is more likely to be released than the person who sacked others, we would know ?he? refers to ?deputy premier? instead of ?prime min-ister?. And in (6), because someone being at-tacked is more likely to die than the attacker, we can infer that ?he? refers to ?boy?. To acquire such information, we need to iden-tify those pairs of predicates which are likely to apply to the same entity.  We collect this data from a large corpus. The basic process is: apply a baseline coreference system to produce mentions and entities for a large corpus. For every entity, record the predicates for every mention, and then the pairs of predicates for successive mentions within each entity.  Although the performance of the baseline coreference is not very high, and individual documents may yield many idiosyncratic pairs, we can gather many significant role pairs by col-
lecting statistics from a large corpus and filtering out the low frequency patterns; this process can eliminate much of the noise due to coreference errors.  Here is an example of the extracted role pairs involving ?attack?:   Obj volley  x Arg0 bombard  x Obj barrage  x Arg0 snatch  x Sbj attack  x Arg0 pound  x Obj reoccupy x Arg1 halt  x Arg0 assault  x 
Arg0 attack x  ? 
Arg1 bombard  x Table1. Top 10 role pairs associated with  ?Arg0 attack x? 4.3 Contextual Compatibility Scores To properly compare the patterns involving al-ternative candidate antecedents, we need to nor-malize the raw frequencies first.  We followed Yang et al (2005)?s idea, which normalizes the pattern frequency by the frequency of the candi-dates, and use a relative score that is normalized by the maximum score of all its candidates: 
        
? 
CompScore(P
context,Cand
)
=
CompFreq(Pcontext,Cand)
Max
Ci?Set(cands)
CompFreq(Pcontext,Ci)
 
and        
? 
CompFreq(P
context,Cand
) =
freq(P
context ,Cand
)
freq(Cand)
 
where 
? 
P
context,Cand
 is the contextual compatibility pattern built from the context of the pronoun and the base form of the candidate.  In contrast to Yang?s work, which used con-textual compatibility on the mention level, we consider the contextual compatibility of an entity to an anaphor:  we calculate the contextual in-formation of all the mentions and choose the one with highest score as the contextual compatibility score for this entity1: 
                                                1 Note that all the mentions in the entity are generated by the overall coreference system. Also, the ACE entity type of names is determined by the system.  No key annotations are considered in the entire coreference phase. 
63
? 
freq(context,entity)
= Max
mention
i
?entity
freq(P
context,mention
i
)
 
4.4 Role Pair Scores Unlike the contextual compatibility feature, we only take the role pair of the successive mentions in the candidate entity and the anaphor, because they are more reliably coreferential than arbitrary pairs of mentions within an entity:  
where  and  are the contextual pat-terns of the anaphor and the last mention in the candidate entity.  For a set of possible candidates, we compute a relative score: 
? 
PairScore(p
ana
, p
cand
)
=
PairFreq(p
ana
, p
cand
)
Max
pi?Set(cands)
PairFreq(p
ana
, pi)  Both scores are quantized (binned) in intervals of 0.1 for use as MaxEnt features.  5 Experiment Our coreference solution system uses ACE anno-tated data and follows the ACE 2005 English entity guidelines.2 The baseline coreference sys-tem to compare with is the same one used for extracting semantic features from the large cor-pus. It employs an entity-mention (rather than a mention-pair) model.  Besides entity and mention information, which (as mentioned above) is system output, the se-mantic information is also automatically ex-tracted by a semantic labeling system. As a result, we report results in section 5.4 which involve no information from the reference (key) annotation. 5.1 Baseline System Description The baseline system first applies processes like parsing, semantic labeling, name tagging, and entity mention tagging, producing a set of men-tions to which coreference analysis is then ap-plied. The coreference phase deals with corefer-ence among mentions that might be pronouns,                                                 2 Automatic Content Extraction evaluation, http://projects.ldc.upenn.edu/ace/ 
names or proper nouns, and generates entities when it is finished. The whole is a one-pass process, resolving coreference in the order in which mentions appear in the document. In the pronoun coreference process, every pronoun mention is assigned to one of the candidate enti-ties.   Features Description Hobbs_    Distance Hobbs distance between the last mention in the entity and the anaphor Head_Pro Combined word features of the head of the last mention in the entity and anaphor Is_Subject True if the last mention in the entity is a subject of the sentence Last_Cat Whether the last mention in the entity is a noun phrase, a pronoun or a name Co_Prior Number of prior references to this entity Table 2. Features used in baseline system  The baseline co-reference system has separate, quite elaborate, primarily rule-based systems to handle names, nominals, headless NP's, and ad-verbs ("here", "there") which may be anaphoric, as well as first- and second-person pronouns. The MaxEnt model under study in this paper is only responsible for third-person pronouns.  Also, gender, number, and human/non-human are han-dled separately outside of the MaxEnt model, and the model only resolves mentions that satisfy these constraints.3 In the MaxEnt model, 5 basic features (described in table 2) are used. Thus, while the set of features used in the model is relatively small in comparison to many current statistically based reference resolvers, these are the primary features relevant to the limited task 
                                                3 Gender information is obtained from a dictionary of gen-der-specific nouns and from first-name lists from the US Census.  Number information comes from large syntactic dictionaries, corpus annotation of collective nouns (syntac-tically singular nouns which may take plural anaphors), and name tagger information (some organizations and political entities may take plural anaphors). 
64
of the MaxEnt model, and its performance is still competitive4.  5.2 Corpus Description There are two kinds of corpora used in our ex-periment, a small coreference-annotated corpus used for training and evaluating (in cross-validation) the pronoun coreference model, and a large raw-text corpus for extracting semantic in-formation. For model training and evaluation, we assem-bled two small corpora from the available ACE data. One consists of news articles (460 docu-ments) from ACE 2005 (330 documents) and ACE 2003 (130 documents), which together con-tain 3934 pronouns. The other is the full ACE 2005 training set (592 documents), which in-cludes newswire, broadcast news, broadcast con-versations (interviews and discussions), web logs, web forums, and Fisher telephone transcripts, and contains 5659 pronouns. In evaluation, we consider a pronoun to be correctly resolved if its antecedent in the system output (the most recent prior mention in the en-tity to which the pronoun is assigned) matches the antecedent in the key. We report accuracy (percentage of pronouns which are correctly re-solved). We used a large corpus to extract semantic in-formation, consisting of five years of AFP newswire from the LDC English Gigaword cor-pus (1996, 2002, 2004, 2005 and 2006), a total of 907,368 documents. We omit news articles writ-ten in 1998, 2000 and 2003 to insure there is no overlap between the ACE data and Gigaword data. We pre-processed each document (parsing, name identification, and semantic labeling) and ran the baseline coreference system, which automatically identified all the mentions (includ-ing name mentions and nominal mentions) and built a set of entities for each document.  
                                                4For example, among papers reporting a pronoun accuracy metric, Kehler et al (2004), testing on a 2002 ACE news corpus, get a pronoun accuracy (without semantic features) of 75.7%; (Yang et  al. 2005), testing on the MUC corefer-ence corpora (also news) get for  their single-candidate baseline (without semantic features) 75.1%  pronoun accu-racy. Although the testing conditions in each case are  dif-ferent, these are comparable to our baseline performance. 
5.3 Semantic Information Extraction from Large Corpus In order to remove noise, we only keep contex-tual compatibility patterns that appear more than 5 times; and only keep role pair patterns which appear more than 15 times, and appear in more than three different years to avoid random pairs extracted from repeated stories. We automati-cally extracted 626,008 contextual compatibility patterns and 4,736,359 role pairs.  Note that we extract fewer patterns than Yang (2005), who extracted in total 2,203,203 contextual compati-bility patterns, from a much smaller corpus (173,252 Wall Street Journal articles). This might be for two reasons: first, we pruned low frequency patterns; second, we used a semantic labeling system instead of shallow parsing. Sec-tion 5.6 gives a comparison of pattern extraction based on different levels of analysis.  5.4 Results  News Corpus 2005 Corpus  Accu SignTest (p <=) Accu SignTest (p <=) baseline 75.54  72.04  context 76.59 0.025 73.35 0.002 role pair 76.28 0.031 73.03 0.003 combine 77.02 0.0005 73.72 0.0015 Table 3. Accuracy of 5-fold cross-validation with sta-tistics-based semantic features  We did a 5-fold cross validation to test the con-tribution from statistically-based semantic fea-tures, and report an average accuracy. All the mentions and their features are obtained from system output; as a result, if the correct antece-dent is not correctly discovered and analyzed from the previous phases, we will not be able to co-refer the pronoun correctly.  Experiments on the news articles show that each feature provides approximately 1% gain by itself, and contributes to a substantial overall gain of 1.45%. For the 2005 corpus, the baseline is lower because the documents come from different genres, and we get more gain from each semantic feature. We also computed the significance over the baseline using the sign test5.                                                  5 In applying the sign test, we treated each pronoun as an independent sample, which is either correctly resolved or incorrectly resolved. Where the individual observations are 
65
5.5 Self-Extracted Bound To better understand the potential maximum con-tribution of our semantic features, we constructed an approximation to the most favorable possible semantic features for each test set. We did this by using perfect coreference knowledge and by col-lecting patterns for each test set from the test set itself. For each corpus used for cross-validation, we first collect all the contextual compatibility and role pair patterns corresponding to the cor-rect antecedents (we ignore the patterns corre-sponding to the wrong antecedents, because we can not get this negative information when we extract them from a large corpus), and score these patterns to produce semantic features for the MaxEnt Model, both training and testing.  We then use these features in the model and do a cross-validation as before.  Also, as before, we rely on system output to identify and analyze potential antecedents; if the prior phases do not do so correctly, coreference analysis may well fail.  This experiment shows that we can get about 3 to 4% gain from each feature type sepa-rately; 4.5 to 5.5% gain is achieved from the two features together.   News Corpus 2005 Corpus  Accu SignTest (p <=) Accu SignTest (p <=) baseline 75.54  72.04  context 79.23 7e-14 76.04 9e-27 role pair 78.85 6e-13 75.95 1e-26 combine 79.97 4e-16 77.50 2e-38 Table 4. Accuracy of 5-fold cross-validation with self-extracted semantic features  5.6 Comparison between Semantic and Syntax Patterns To better understand the difference between se-mantic role labeling and syntactic relations, we did a comparison between patterns extracted from the syntax level and those extracted from semantic role labeling: Experiments show that using semantic roles (such as Arg0 and Arg1) works better. This may                                                                        (changes in) binary outcomes, the sign test provides a suita-bly sensitive significance test. (In particular, it is compara-ble to performing a paired t-test over counts of correct reso-lutions, aggregated over documents.) 
be because the "deeper" representation provides more generalization of relations. For example, the phrases ?weapon?s use? and ?use weapon? share the same semantic relation <Arg1 use weapon>, while they yield different grammatical relations: <T-pos use weapon> and <Obj use weapon>.   News Corpus 2005 Corpus  semantic syntax semantic syntax baseline 75.54  72.04  context 79.23 77.73 76.04 75.83 role pair 78.85 76.87 75.95 74.17 combine 79.97 78.42 77.50 76.76 Table 5. Accuracy of 5-fold cross-validation with self-extracted semantic features based on different levels of syntactic/semantic relations 5.7 Error Analysis We analyzed the errors in the self-extracted re-sults, to see why such corpus-specific semantic features do not produce an even greater reduction in errors. For the contextual compatibility feature, we find cases where an incorrect candidate is equally compatible with the context of the ana-phor; for example, if all the candidates are person names, they will share the same context feature because they generate the same ACE type. In other cases, the context does not provide enough information. For example, in a context tuple <Arg0 get x>, x can be almost any noun, because ?get? is too vague to predicate the compatible subjects. There are similar limitations with the role pair feature; for example, <Arg0 get they> can be associated with a lot of other actions. To quantify this problem, we counted the pat-terns that appear in both positive examples (cor-rect antecedents) and negative examples (incor-rect antecedents). For contextual compatibility patterns, 39.5% of the patterns which appear with positive examples also appear in the negative sample, while for role pair patterns, 19% of the patterns which appear with positive examples also appear in the negative sample.   So we see that, even with a pattern set highly tuned to the test set, many patterns do not by themselves serve to distinguish correct from incorrect coreference. We analyzed some of the cases where the se-mantic information does not help, or even harms the analysis.   In some cases all the antecedent 
66
scores are very low, either because the patterns are very rare or the antecedent is a common word that appears in a lot of patterns.  In other cases, several antecedents have a high compatibility score but the correct one does not have the top score. In these cases, the contextual compatibility is not reliable, as was pointed out by Kehler et al (2004): (7) The model for a republic, adopted over bitter objections from those advocating direct elec-tion of a president, is for presidential nomi-nations to be made with public input and the winning candidate decided by a two-thirds majority of Parliament. Former prime minis-ter Paul Keating, who put the republic issue in the spotlight in his unsuccessful 1996 campaign for re-election, welcomed the re-sult. Here adding semantic features leads ?his? to be incorrectly resolved to ?president? rather than the entity with mentions ?prime minister? and ?Paul Keating?; all the relevant patterns are common, but the score for <Arg0 campaign president> is higher (around 0.0012) than for <Arg0 campaign minister> (0.0004) or <Arg0 campaign Person> (0.0006). Another problem is that the patterns do not capture enough context information, for example: (8) The U.S. administration has been pressing the Security Council to adopt a statement condemning Pyongyang for failing to meet its obligations. If we can get the semantic context of ?fail to meet its obligations? instead of ?its obligations?, we might get better solutions for (8).  The role pair information raises similar prob-lems. Some verbs are very vague, like ?get?, ?take?, ?have?, and role pairs with these verbs might not be very useful. Here is an example: (9) The retired Greek officer tried to get Ocalan to the Netherlands, home to a large Kurdish community. He claimed he had been ma-nipulated by the government. In this sentence, the role pair information is very vague and it is hard to select a proper ante-cedent by connecting the subject of ?try? or ?get? or the object of ?get? to the subject of ?claim?. 
5.8 Limitations of Semantic Features The availability of very large corpora coupled with improved pre-processing (e.g., faster pars-ers, accurate semantic labelers) is making it eas-ier to extract large sets of semantic patterns. However, results on ?perfect? semantic informa-tion show that even if we can get very good se-mantic features, there are at least two concerns to address: ? How to best capture the context information: larger context patterns may suffer from data sparseness; simple patterns may be insuffi-ciently selective, appearing in both positive and negative samples.  ? In some cases, the baseline features are suffi-cient to select the antecedent and the semantic features only do harm.  If we are able to better gauge our confidence in the decisions based on the baseline features and on the semantic features, we may be able to combine these two sources more effectively. 6 Conclusions and Future Work We have presented two ways to incorporate se-mantic features into a MaxEnt model-based pro-noun coreference system, where these features have been extracted from a large corpus using a baseline IE (Information Extraction) system and a semantic labeling system, with no specific do-main information.  We also estimated the maximal benefit of these features and did some error analysis to identify cases where this semantic knowledge did not suffice. Our experiments show the value of these semantic features for pronoun coreference, but also the limitations of our current context representation and reference resolution models.  Last, we compared the features extracted from different levels of analysis, and showed that 'deeper' representations worked better. References Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008. Two-phased event relation acquisition:  coupling the relation-oriented and argument-oriented ap-proaches.  Proc. 22nd Int'l Conf. on Computational Linguistics (COLING 2008). David Bean and Ellen Riloff. 2004. Unsupervised Learning of Contextual Role Knowledge for 
67
Coreference Resolution. Proc. HLT-NAACL 2004. Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised Learning of Narrative Event Chains. Proc. ACL-08:  HLT. I. Dagan and A. Itai. 1990. Automatic processing of large corpora for the resolution of anaphora references. Proc. 13th International Conference on Computational Linguistics (COLING  1990). J. Hobbs. 1978. Resolving pronoun references. Lingua, 44:339?352. A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The (non)utility of predicate-argument fre-quencies for pronoun interpretation. Proc. HLT-NAACL 2004. A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S.  Liao and W. Xu. 2009. Automatic Recognition of Logi-cal Relations for English, Chinese and Japanese in the GLARF Framework. In SEW-2009 (Semantic Evaluations Workshop) at NAACL HLT-2009 Viktor Pekar.  2006.  Acquisition of verb entailment from text.  Proc. HLT-NAACL 2006. Simone Paolo Ponzetto and Michael Strube. 2006 Exploiting semantic Role Labeling, WordNet and Wikipedia for Coreference Resolution. Proc. HLT-NAACL 2006. X. Yang, J. Su, G. Zhou, and C. Tan. 2004. An NP-cluster approach to coreference resolution. Proc. 20th International Conference on Computa-tional Linguistics (COLING 2004). Xiaofeng Yang, Jian Su, Chew Lim Tan.2005.  Im-proving Pronoun Resolution Using Statistics-Based Semantic Compatibility Information. Proc. 43rd Annual Meeting of the Assn. for Com-putational Linguistics. Xiaofeng Yang and Jian Su. 2007. Coreference Resolution Using Semantic Relatedness In-formation from Automatically Discovered Pat-terns. Proc. 45th Annual Meeting of the Assn. for Computational Linguistics.  
68
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 88?97,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Improving MT Word Alignment Using Aligned Multi-Stage Parses
Adam Meyers?, Michiko Kosaka?, Shasha Liao? and Nianwen Xue?
? New York University, ?Monmouth University, ?Brandeis University
Abstract
We use hand-coded rules and graph-aligned
logical dependencies to reorder English text
towards Chinese word order. We obtain a
1.5% higher F-score for Giza++ compared to
running with unprocessed text. We describe
this research and its implications for SMT.
1 Introduction
Some statistical machine translation (SMT) systems
use pattern-based rules acquired from linguistically
processed bitexts. They acquire these rules through
the alignment of a parsed structure in one language
with a raw string in the other language (Yamada and
Knight, 2001; Shen et al, 2008) or the alignment
of source/target language parse trees (Zhang et al,
2008; Cowan, 2008). This paper shows that ma-
chine translation (MT) can also benefit by aligning a
?deeper? level of analysis than parsed text, which in-
cludes semantic role labeling, regularization of pas-
sives and wh constructions, etc. We create GLARF
representations (Meyers et al, 2009) for English and
Chinese sentences, in the form of directed acyclic
graphs. We describe two graph-based techniques
for reordering English sentences to be closer to that
of corresponding Chinese sentences. One technique
is based on manually created rules and the other is
based on an automatic alignment of GLARF repre-
sentations of Chinese/English sentences. After re-
ordering, we align words of the reordered English
with the words of the Chinese, using the Giza++
word aligner(Och and Ney, 2003). For both tech-
niques, the resulting alignment has a higher F-score
than Giza++ on raw text (a 0.7% to 1.5% absolute
improvement). In principle, our reordered text can
be used to improve any Chinese/English SMT sys-
tem for which Giza++ (or other word aligners) are
part of the processing pipeline.
These experiments are a first step in using
GLARF-style analyses for MT, potentially improv-
ing systems that already perform well with aligned
text lacking large gaps in surface alignment. We hy-
pothesize that SMT systems are most likely to ben-
efit from deep analysis for structures where source
and target language word order differs the most. We
propose using deep analysis to reorder such struc-
tures in one language to more closely reflect the
word order of the other language. The text would be
reordered at two stages in an SMT system: (1) prior
to acquiring a translation model; and (2) either prior
to translation (if source text is reordered) or after
translation (if target text is reordered). Our system
moves large constituents (e.g., noun post-modifiers)
to bring English word order closer to that of parallel
Chinese sentences. This improves word alignment
and is likely to improve SMT.
For this work we use two English/Chinese bitext
corpora developed by the Linguistic Data Consor-
tium (LDC): the Tides FBIS corpus and the GALE
Y1 Q4 Chinese/English Word-Alignment corpus.
We used 2300 aligned sentences from FBIS for de-
velopment purposes. We divided the GALE corpus
into into a 3407 sentence development subcorpus
(DEV) and a 1505 sentence test subcorpus (TEST).
We used the LDC?s manual alignments of the FBIS
corpus to score these data.
88
2 Related Work in SMT
Four papers stand out as closely related to the
present study. (Collins et al, 2005; Wang et al,
2007) describe experiments which use manually cre-
ated parse-tree-based rules to reorder one side of
a bitext: German/English in (Collins et al, 2005)
and English/Chinese in (Wang et al, 2007). Both
achieve BLEU score improvements for SMT: 25.2%
to 26.8% for (Collins et al, 2005) and 28.52 to 30.86
for (Wang et al, 2007). (Wang et al, 2007) uses
rules very similar to our own as they use the same
language pair, although they reorder the Chinese,
whereas we reorder the English. The most signifi-
cant differences between our research and (Collins
et al, 2005; Wang et al, 2007) are: (1) our manual
rules benefit from a level of representation ?deeper?
than a surface parse; and (2) In addition to the hand-
coded rules, we also use automatic alignment-based
rules. (Wu and Fung, 2009) uses PropBank role la-
bels (Palmer et al, 2005) as the basis of a second
pass filter over an SMT system to improve the BLEU
score from 42.99 to 43.51. The main similarity to
the current study is the use of a level of represen-
tation that is ?deeper? than a surface parse. How-
ever, our application of linguistic structure is more
like that of (Wang et al, 2007) and our ?deep? level
connects all predicates and arguments in the sen-
tence, regardless of part of speech, rather than just
connecting verbs to their arguments. (Bryl and van
Genabith, 2010) describes an open source LFG F-
structure alignment tool with an algorithm similar to
our previous work. They evaluate their alignment
output on 20 manually-aligned German and English
F-structures. They leave the impact of their work on
MT to future research.
In addition to these papers, there has also been
some work on rule-based reordering preprocessors
to word alignment based on shallower linguistic in-
formation. For example (Crego and Marin?o, 2006)
reorders based on patterns of POS tags. We hypoth-
esize that this is similar to the above approaches in
that patterns of POS tags are likely to simulate pars-
ing or chunking.
3 Preparing the Data
The two stage parsers of previous decades (Hobbs
and Grishman, 1976) generated a syntactic repre-
sentation analogous to the (more accurate) output
of current treebank-based parsers (Charniak, 2001)
and an additional second stage output that regular-
ized constructions (passive, active, relative clauses)
to representations similar to active clauses with no
gaps, e.g., The book was read by Mary was given a
representation similar to that of Mary read the book.
Treating the active clause as canonical provides a
way to reduce variation in language and thus, mak-
ing it easier to acquire and apply statistical informa-
tion from corpora?there is more evidence for partic-
ular statistical patterns when applications learn pat-
terns and patterns more readily match data.
Two-stage parsers were influenced by linguistic
theories (Harris, 1968; Chomsky, 1957; Bresnan and
Kaplan, 1982) which distinguish a ?surface? and a
?deep? level. The deep level neutralizes differences
between ways to express the same meaning?a pas-
sive like The cheese was eaten by rats was analyzed
in terms of the active form Rats ate the cheese. Cur-
rently ?semantic parsing? refers to a similar repre-
sentation, e.g., (Wagner et al, 2007) or our own
GLARF (Meyers et al, 2009). However, the term is
also used for semantic role labelers (Gildea and Ju-
rafsky, 2002; Xue, 2008), systems which typically
label semantic relations between verbs and their ar-
guments and rarely cover arguments of other parts
of speech. Second stage semantic parsers like our
own, connect all the tokens in the sentence. Aligned
text processed in this way can (for example) repre-
sent differences in English/Chinese noun modifier
order, including relative clauses. In contrast, few
role labelers handle noun modifiers and none han-
dle relative clauses. Below, we describe the GLARF
framework and our system for generating GLARF
representations of English and Chinese sentences.
For each language, we combine several types of
information which may include: named entity (NE)
tagging, date/number regularization, recognition of
multi-word expressions (the preposition with respect
to, the noun hand me down and the verb ad lib),
role labels for predicates of all parts of speech, regu-
larizing passives and other constructions, error cor-
rection, among other processes into a single typed
feature structure (TFS) representation. This TFS
is converted into a set of 25-tuples representing
dependency-style relations between pairs of words
in the sentence. Three types of dependencies are
89
n1
know
SBJ OBJ
n3
of
n5 OBJ
SBJ OBJ
N?POS
COMP
n4
the
Q?POS
n2? n3?
n6?
n1?
I rules
tennis
n6
n2
 
 
  
Figure 1: Word-Aligned Logic1 Dependencies
represented: surface dependencies (close to the level
of the parser), logic1 dependencies (reflecting var-
ious regularizations) and logic2 dependencies (re-
flecting the output of a PropBanker, NomBanker
and Penn Discourse Treebank transducer).(Palmer
et al, 2005; Xue and Palmer, 2003; Meyers et al,
2004; Miltsakaki et al, 2004) The surface depen-
dency graph is a tree; The logic1 dependency graph
is an directed acyclic graph; and The logic2 depen-
dency graph is a directed graph with cycles, cover-
ing only a subset of the tokens in the sentence. For
these experiments, we focus on the logic1 relations,
but will sometimes use the surface relations as well.
Figure 1 is a simple dependency-based logic1 repre-
sentation of I know the rules of tennis and its Chi-
nese translation. The edge labels name the relations
between heads and dependents, e.g., I is the SBJ of
know and the dashed lines indicate word level corre-
spondences. Each node is labeled with both a word
and a unique node identifier (n1, n1?, etc.)
The English system achieves F-scores for logic1
dependencies on parsed news text in the 80?90%
range and the Chinese system achieves F-scores in
the 74?84% range, depending on the complexity of
the text. The English system has been created over
the course of about 9 years, and consequently is
more extensive than the Chinese system, which has
been created over the past 3 years. The systems are
described in more detail in (Meyers et al, 2009).
The GLARF representations are created in a se-
ries of steps involving several processors. The En-
glish pipeline includes: (1) dividing text into sen-
tences; (2) running the JET NE tagger (Ji and Gr-
ishman, 2006); (3) running scripts that clean up data
(to prevent parser crashes); (4) running a parser (cur-
rently Charniak?s 2005 parser based on (Charniak,
2001)); (5) running filters that: (a) correct com-
mon parsing errors; (b) merge NE information with
the parse, resolving conflicts in constituent bound-
aries by hand-coded rules; (c) regularize numbers,
dates, times and holidays; (d) identify heads and
label relations between constituents; (e) regularize
text grammatically (filling empty subjects, resolv-
ing relative clause and Wh gaps, etc.); (f) mark con-
junction scope; (g) identify transparent constituents
(e.g., recognizing, that A variety of different peo-
ple has the semantic features of people (human), not
those of variety, the syntactic head of the phrase.);
among other aspects. The Chinese pipeline is simi-
lar, except that it includes the LDC word segmenter
and a PropBanker (Xue, 2008). Also, the regulariza-
tion routines are not as completely developed, e.g.,
relative clause gaps and passives are not handled
yet. The Chinese system currently uses the Berke-
ley parser (Petrov and Klein, 2007). Each of these
pipelines derives typed feature structure representa-
tions, which are then converted into the 25 tuple rep-
resentation of 3 types of dependencies between pairs
of tokens: surface, logic1 and logic2.
To insure that the logic1 graphs are acyclic, we as-
sume that certain edges are surface only and that the
resulting directed acyclic graphs can have multiple
roots. It turns out that the multiple rooted cases are
mostly limited to a few constructions, the most com-
mon being parenthetical clauses and relative clauses.
A parenthetical clause takes the main clause as an
argument. For example, in The word ?potato?, he
claimed, is spelled with a final ?e?., the verb claimed,
takes the entire main clause as an argument, we as-
sume that he claimed is a dependent on the main
verb (is) spelled labeled PARENTHETICAL in our
surface dependency structure, but that the main verb
(is) spelled is a dependent of the verb claimed in
our logic1 structure, labeled COMPLEMENT. Thus
the logic1 surface dependency structure have dis-
tinct roots. In a relative clause, such as the book that
I read?, we assume that the clause that I read is a de-
pendent on the noun book in our surface dependency
structure with the label RELATIVE, but book is a de-
pendent on the verb read in our logic1 dependency
structure, with the label OBJ. This, means that our
logic1 dependency graphs for sentences containing
relative clauses are multi-rooted. One of the roots is
the same as the root of the surface tree and the other
root is the root of the relative clause graph (a rela-
90
tive pronoun or a main verb). Furthermore, there is
a surface path connecting the relative clause root to
the rest of the graph. Noncyclic graph traversal is
possible, provide that: (1) we use the surface path to
enter the graph representing the relative clause ? oth-
erwise, the traversal would skip the relative clause;
and (2) we halt the traversal if we reach this path a
second time ? this avoids traversing down an end-
less path. The parenthetical and relative clause are
representative of the handful of cases in which naive
representations would introduce loops. All cases of
which we are aware have the essential properties of
one of these two cases: (1) either introducing a dif-
ferent single root of the clause; or (2) introducing an
additional root that can be bridged by a surface path.
4 Manual Reordering Rules
We derived manual rules for making the English
Word Order more like the Chinese by manually in-
specting the data. We inspected the first 100-200
sentences of the DEV corpus by first transliterating
the Chinese into English ? replaced each Chinese
word with the aligned English counterpart. Several
patterns emerged which were easy to formalize into
rules in the GLARF framework. These patterns were
verified and sometimes generalized through discus-
sions with native Chinese speakers and linguists.
Our rules, similar to those of (Wang et al, 2007) are
as follows (results are discussed in section 6): (1)
Front a post-nominal PP headed by a preposition in
the list {of, in, with, about)}. (2) Front post-nominal
relative clause that begins with that or does not have
any relative pronoun, such that the main predicate is
not a copula plus adjective construction. (3) Front
post-nominal relative clause that begins with that or
has no relative pronoun if the main predicate is a
copula+adjective construction which is not negated
by a word from the set {no neither nor never not
n?t}. (4) Front post-nominal reduced relative in the
form of a passive or adjectival phrase. (5) Move ad-
verbials more than and less than after numbers that
they modify. (6) Move PPs that post-modify adjec-
tives to the position before the adjective. (7) Move
subordinate conjunctions before and after to the end
of the clause that they introduce. (8) Move an ini-
tial one-word-long title (Mr., Ms., Dr., President) to
the end of the name. (9) Move temporal adverbials
(adverb, PP, subordinate clause that is semantically
temporal) to pre-verb position.
5 Automatic Node Alignment and its
Application for Word Alignment
In this experiment, we automatically derive re-
orderings of the English sentences from an align-
ment between nodes in logic1 dependency graphs
for the English (source) and Chinese (target) sen-
tences. Source/Target designations are for conve-
nience, since the direction of MT is irrelevant.
We define an alignment as a partial function from
the nodes in the source graph and the nodes in the
target graph. We, furthermore, assume that this map-
ping is 1 to 1 for most node pairs, but can be n to 1
(or 1 to n). Furthermore, we allow some nodes, in
effect, to represent multiple tokens. These are iden-
tified as part of the GLARF analysis of a particular
sentence string and reflect language-specific rules.
Thus, for our purposes, a mapping between a source
and target node, each representing a multi-word ex-
pression is 1 to 1, rather than N to N.
We identify the following types of multi-word ex-
pressions for this purpose: (a) idiomatic expressions
from our monolingual lexicons, (b) dates, (c) times
(d) numbers and (e) ACE (Grishman, 2000) NEs.
Dates, holidays and times are regularized using ISO-
TimeML, e.g., January 3, 1977 becomes 1977-03-01
and numbers are converted to Arabic numbers.
5.1 ALIGN-ALG1
This work uses a modified version of ALIGN-
ALG1, a graph alignment algorithm we previously
used to align 1990s-style two-stage parser output for
MT experiments. ALIGN-ALG1 is an O(n2) algo-
rithm, n is the maximum number of nodes in the
source and target graphs (Meyers et al, 1996; Mey-
ers et al, 1998). Given Source Tree T and Target
Tree T ?, an alignment(T, T ?) is a partial function
from nodes N in T to nodes N ? in T ?. An exhaus-
tive search of possible alignments would consider all
non-intersecting combinations of the T ?T ? pairs of
source/target nodes ? There are at most T ! such pair-
ings where T >= T ?.1 However, ALIGN-ALG1 as-
sumes that some of these pairings are unlikely, and
1This ignores N to 1 matches, which we allow, although rel-
atively rarely.
91
favors pairings that assume the structure of the trees
correspond more closely. In particular, it is assumed
that ancestor nodes are more likely to match if most
of their descendant nodes match as well.
ALIGN-ALG1 finds the highest scoring align-
ment, where the score of an alignment is the sum
of the scores of the node pairs in the partial func-
tion. The score for each node pair (n, n?) partially
depends on the scores of a mapping from the chil-
dren of n to the children of n?. While the process
of calculating the scores is recursive, it can be made
efficient using dynamic programming.
ALIGN-ALG1 assumes that we align r and r?,
the roots of T and T ?. Calculating the scores for r
and r?, entails calculating the scores of pairs of their
children, and by extension all mappings from N to
N ? that obey the dominance preserving constraint:
Given nodes n1 and n2 in N and nodes n?1 and n?2
in N ?, where all 4 nodes are part of the alignment,
it cannot be the case that: n1 dominates n2, but
n?1 does not dominate n?2. Here, dominates means
is an ancestor in the dependency graph. ALIGN-
ALG1 scores each pair of nodes using the formula:
Score(n, n?) = Lex(n, n?) + ChildV al(n, n?),
where Lex(n, n?) is a score based on matching the
words labeling nodes n and n?, e.g., the score is 1 if
the pair is found in a bilingual dictionary and 0 oth-
erwise. Given n has children c0, . . . , ci and n? has
children c?0, . . . , c?j , to calculate ChildVal: (1) Cre-
ate Child-Matrix, a (i+ 1)? (j + 1) matrix (2) Fill
every position (1 <= x <= i, 1 <= x? <= j)
with Score(x, x?) (3) Fill every position (i+1, 1 <=
x? <= j) with Score(n, x?) minus a penalty (e.g.,
- .1) for collapsing an edge. This treats n? and x?
as a single unit, matched to n.2 (4) Fill every po-
sition (1 <= x <= i, j+1) with Score(x, n?) mi-
nus a penalty for collapsing an edge. Thus n + x is
paired with n?. (5) Set (i+1,j+1) to ??. Collapsing
both source and target edges is not permitted. (6) For
all sets of positions in the matrix such that no node
or column is repeated, select the set with the high-
est aggregate score. The aggregate score is the nu-
meric value of ChildV al(n, n?). If (n,n?) is part of
the alignment that is ultimately chosen, this choice
of node pairs is also part of the alignment. There
2The slight penalty represents that collapsing edges compli-
cate the analysis and is thus disfavored (Occam?s Razor).
are at most max(i + 1, j + 1)! possible pairings.
Rather than calculating them all, a greedy heuristic
can reduce the calculation time with minimal effect
on accuracy: the highest scoring cell in the matrix is
chosen first, conflicting cells are eliminated, the next
highest scoring cell is chosen, etc.
Consider the example in Figure 1, assum-
ing the dashed lines connect lexical matches
(the function LEX returns 1 for these node
pairs). Where n1 and n1? are the roots,
Score(n1, n1?) = 1 + ChildV al(n1, n1?). Cal-
culating ChildV al(n1, n1?) requires a recursive
descent down the pairs of nodes, until the bot-
tom most pair is scored. Score(n6, n6?) = 1.
Score(n5, n6?) = 0 + .9 (derived by collaps-
ing an edge and subtracting a penalty of .1).
Score(n3, n3?) = 1 + .9 = 1.9. Score(n2, n2?) =
1. ChildV al(n1, n1?) = 1 + 1.9 = 2.9. Thus
Score(n1, n1?) = 3.9. The alignment includes:
(n1, n1?), (n2, n2?), (n3, n3?), (n5, n6?), (n6, n6?).
The collapsing of edges helps recognize cases
where multiple predicates form substructures, e.g.,
take a walk, is angry, etc. in one tree can map to sin-
gle verbs in the other tree, allowing outgoing edges
from walk or angry to map to outgoing edges of the
corresponding verb, e.g., the agent and goal of John
walked to the store could map to the agent and goal
of John took a walk to the store.
In practice, ALIGN-ALG1 falls short because:
(1) Our translation dictionary does not have suffi-
cient coverage for the algorithm to perform well; (2)
The assumption that the roots of both graphs should
be aligned is often false. Parallel text often reflects
a dynamic, rather than a literal translation. In one
pair of aligned sentences in the FBIS corpus, the
English phrase the above mentioned requests cor-
responds to: meaning these re-
quests of Chen Shui-bian ? Chen Shui-bian has no
counterpart in the English. Parts of translations can
be omitted due to: (a) the discretion of the trans-
lators, (b) the expected world knowledge of partic-
ular language communities, (c) the cultural impor-
tance of particular information, etc.; (3) Violations
of the dominance-preserving constraint exist. The
most common type that we have observed consists
of sequences of transparent nouns and of (e.g., se-
ries of) in English corresponding to quantifiers in
92
Chinese ( ). Thus the head of the English con-
struction corresponds to the dependent of the Chi-
nese construction and vice versa.
5.2 Lexical Resources
Our primary bilingual Chinese/English dictionary
(LEX1) had insufficient coverage for ALIGN-ALG1
to be effective. LEX1 is a merger between:
The LDC 2002 Chinese-English Dictionary and
HowNet. In addition, we manually added additional
translations of units of measure from English. We
also used NEDICT, a name translation dictionary (Ji
et al, 2009) and AUTODICT, English/Chinese word
to word pairs with high similarity scores taken from
MT phase tables created as part of the (Zhang et al,
2007) system. The NEDICT was used both for pre-
cise matches and partial matches (since, NEs can
often be synonymous with substrings of NEs). In
addition, we used some WordNet (Fellbaum, 1998)
synonyms of English to expand the coverage of all
the dictionaries, allowing English words to match
Chinese word translations of their synonyms. We
allowed additional matches of function words that
served similar functions in the two languages includ-
ing: copulas, pronouns and determiners.
Finally, we use a mutual information (MI) based
approach to find further lexical information. We run
our alignment program over the corpus two times,
the first time, we acquire statistical information
useful for generating a MI-based score. This score
is used as a lexical score on the second pass for
items that do not match any of the dictionaries. On
the first pass, we tally the frequency of each pair
of source/target words s and t, such that neither
s, nor t are matched lexically to any other item
in the sentence. We, furthermore, keep track of
the number of times each word appears in the
corpus and the number of times each word appeared
unaligned in the corpus. We tally MI as follows:
pair?frequency2
1+(source?word?frequency?target?word?frequency)
One is added to the denominator as a variation on
add-one smoothing (Laplace, 1816), intended to
penalize low frequency scores. We calculate this
score in two ways: (a) using the global frequencies
of the source and target words; and (b) using the
frequency these words were unaligned. The larger
of the two scores is the one that is actually use.
Different lexicons are given different weights.
Matches between words in the hand-coded transla-
tion dictionary and NEDICT are given a score of
1.0. Matches in other dictionaries are allotted lower
scores to represent that these are based on automati-
cally acquired information, which we assume is less
reliable than manually coded information.3
5.3 ALIGN-ALG2
With ALIGN-ALG2, we partially address two lim-
itations of ALIGN-ALG1: (1) the assumption that
the roots of source and target graph are aligned;
and (2) the dominance-preserving constraint. Ba-
sically, we assume that structural similarity is fa-
vored, but not necessarily at the global level. Thus
it is likely that many subparts of corresponding trees
correspond closely, but not necessarily the highest
nodes in the trees.
We use ALIGN-ALG1 to align every possible pair
of S source nodes and T target nodes. Then we look
for P , the highest scoring node pair of all SXT
pairs. P and all the pairs of descendants that are
used to derive this score (the highest scoring pairs
of children, grand children, etc.) become the initial
output. Then we find all unmatched source and tar-
get children, and look up the highest scoring pair of
these nodes, and we repeat the process, adding the
resulting node pairs to the output. We continue to
repeat this process until either all the nodes are in-
cluded in the output or there is no remaining pair
with a score above a threshold score (we leave au-
tomatic methods of tuning this score to future work
and preliminarily have set this parameter to .3). This
means that: 1) some parts of the graphs are left un-
aligned (the alignment is a partial mapping); 2) the
alignment is more resilient to misalignment caused
by differences in graph structure, regardless of the
reason; and 3) the alignment may be between pair
of unconnected graphs, each containing subsets of
nodes and edges in the source and target graphs.
While more complex than ALIGN-ALG1, ALIGN-
ALG2 performs relatively quickly. After one itera-
tion using ALIGN-ALG1, scores are looked up, not
recalculated.
3Current informal weights of .2 to .6 may be replaced with
automatically tuned weights (hill-climbing, etc.) in future work.
93
5.4 Treating Multiple Tokens as One
In some cases, parsing and segmentation of text
can be corrected through minor modifications to our
alignment routine. Similarly, we use bilingual lex-
ical information to determine that certain other ad-
jacent tokens should be treated as single words for
purposes of alignment.
Given a language for which segmentation is a
common source of processing error (Chinese), if a
token is unaligned, we check to see whether subdi-
viding the token into two sub-tokens would allow
one or both of these sub-tokens to be alignable with
unaligned tokens in the other language. We iter-
ate through the string one token at a time, trying
all partitions. Given a source token ABC, consist-
ing of segments A, B and C, we test the two pairs of
subsequences {A, BC} and {AB, C}, to see which
of the two partitions (if any) could be aligned with
unaligned target tokens and we compare the scores
of both, selecting the highest score. Unless no par-
tition yields further source/target matches, we then
choose the highest scoring partition and add the re-
sulting node pairings to our alignment. In a similar
way, if there are a pair of aligned names consisting
of source tokens sj . . . sk and target tokens tj . . . tk,
we look for adjacent unaligned source nodes (a se-
quence of nodes ending in sj?1 or beginning with
sk+1) and/or adjacent target language nodes, such
that adding these nodes to the name sequence would
produce at least as high a lexical score. The lexi-
con can also be used to match two adjacent items to
the same word. We use a similar routine that checks
our lexicons for words that are adjacent to matching
words. This is particularly meaningful for the entries
automatically acquired by means of MI, as our cur-
rent method for acquiring MI would not distinguish
between 1 to 1 and N to 1 cases. Thus MI scores
for adjacent items typically does mean that an N to
1 match is appropriate. For example, the Chinese
word had high MI with every word
in the sequence (except and): ambassador extraor-
dinary and plenipotentiary (example is from FBIS).
This routine was able to cause our procedure to treat
this English sequence as a single token.
5.5 Using Node Alignment for Reordering
Given a node alignment, we can attempt to reorder
the source language so that words associated with
aligned nodes reflect the order of the words label-
ing the corresponding target nodes. Specifically,
we reorder our surface phrase structure-based repre-
sentation of the source language (English) and then
print out all the words yielded from the resulting
reordered tree. Reordering takes place in a bottom
up fashion as follows: for each phrase P with chil-
dren c0 . . . cn, reorder the structure beneath the child
nodes first. Then build the new-constituent right
to left, one child at a time from cn . . . c0. Start-
ing with an empty sequence, each item is put in
its proper place among the constituents in the se-
quence so far. At each step, place some ci after some
cj in ci+1 . . . cn, such that cj align precedes ci
and cj is after every ck in ci+1 . . . cn such that
ci align precedes ck. If cj does not exist, ci is
placed at the beginning of the sequence so far.
Definition of X align precedes Y , where X and
Y are nodes sharing the same parent: (1) Let pairsX
be the set of source/target pairs in the alignment such
that some (leaf node) descendant of X is the source
node in the pair; (2) Let pairsY be the set of pairs
in the alignment such that some descendant of Y is
the source node in the pair; (3) let Xtmax be the last
target member of a pair in pairsX , where the or-
der is determined by the word order of the target
words labeling the nodes; (4) let Ytmin be the first
target member of a pair in pairsY , where the order
is determined the same way; (5) let Xsmin be the
first source member of a pair in pairsx, according
to the source sentence word order; (6) let Ysmax be
the last source word in a pair in pairsY ordered the
same way. (7) X align precedes Y if: Xtmax pre-
cedes Ytmin and there is no source/target pair Q,R
in the alignment such that: (A) R precedes, Ytmin;
(B) Xtmax precedes R; (C) Q either precedes Xsmin
or follows Ysmax; (D) If Q precedes Ysmax, then R
does not precede Ytmin.
Essentially, the align precedes operator pro-
vides a conservative way to order the source sub-
trees S1 and S2 by their aligned target sub-tree coun-
terparts T1 and T2. The idea is that if T1 and T2
are ordered in an opposite manner to S1 and S2,
the source subtrees should trade places. However,
94
System DEV TEST
BASELINE 53.1% 49.9%
MANUAL 54.0% 50.6%
(p < .01) (not significant)
ALIGN 53.5% 51.1%
(p < .05) (p < .01)
ALIGN+MI 53.8% 51.4%
(p < .01) (p < .01)
Table 1: F Scores for Reordering Rules
a source/target pair Bs, Bt can block this reorder-
ing if doing so would upset the order of the moved
constituents relative to Bs and Bt e.g., if before the
move, Bs precedes S2 and Bt precedes T2, but af-
ter the move S2 would precede Bs. This reordering
proceeds from right to left, halting after placing c0.
6 Results
The results summarized in table 1, provide F-scores
(the harmonic mean of precision and recall) of the
word alignment resulting from running GIZA++
with and without our reordering rules, using the
LDC?s manually created word alignments for our
DEV and TEST corpora.4 Giza++ is run with En-
glish as source and Chinese as target. Our baseline
is the result of running Giza++ on the raw text. The
statistical significance of differences from the base-
line are provided in parentheses, next to each non-
baseline score(rounded to 2 significant digits). We
divided both corpora into 20 parts and ran all ver-
sions of the program on each section. We compared
the system output for each section against the base-
line and used the sign test to calculate statistical sig-
nificance. All system output except one5 achieved
at least p < .05 and most systems achieved signifi-
cance well below p < .01.
Informally, we observe that the rules reordering
common noun modifiers produce most of the total
4We used F-scores, which (Fraser and Marcu, 2007) show to
correlate well with improvements in BLEU. We weighted pre-
cision and recall evenly since we do not currently have BLEU
scores for MT that use these alignments and therefore cannot
tune the weights. Our results also showed improvements in
alignment error rate (AER) (Och and Ney, 2000), which incor-
porate the ?possible? and ?sure? portions of the manual align-
ment into F-score, but do not seem to correlate well with BLEU.
5When run on the test corpus, the manual system outper-
formed the baseline system on only 13 out of 20 sections.
improvement. However, space limitations prevent a
detailed exploration of these differences. The results
show that for both DEV and TEST corpora, both re-
ordering approaches improve F-scores of GIZA++
over the baseline. The manual rules (MANUAL)
seem to suffer somewhat from overtraining on the
DEV corpus, as they were designed based on DEV
corpus examples, whereas the alignment based ap-
proaches (ALIGN and subsequent entries in the ta-
ble) seem resilient to these effects. The use of Mu-
tual Information (ALIGN+MI) seems to further im-
prove the F-score.
The two approaches worked for many of the same
phenomena, e.g., they fronted many of the same
noun post-modifiers. The advantage of the hand-
coded rules seems to be that they cover reordering
of words which we cannot align. For example, a
rule that fronts post-nominal of phrases operates re-
gardless of dictionary coverage. Thus the rule-based
version fronted the of phrase in the NP the govern-
ment of the Guangxi Zhuangzu Autonomous Region
in our DEV corpus, due to the absolute application
of the rule. However, the alignment-based version
did not front the PP because the name was not found
in NEDICT. On the other hand, exceptions to this
rule were better handled by the alignment-based sys-
tem. For example, if series of aligns with the quan-
tifier , the PP would be incorrectly fronted
by the manual, but not the alignment-based system.
Also, the alignment-based method can handle cases
not covered by our rules with minimal labor. Thus,
the automatic system, but not the manual-rule sys-
tem fronted the locative PP in Guangxi to the po-
sition between been and quite in the sentence: for-
eign businessmen have been quite actively investing
in Guangxi. This is closer to the Chinese, but may
have been difficult to predict with an automatic rule
for several reasons, e.g., it is not clear if all post-
verbal locative phrases should front.
We further analyzed the DEV ALIGN+MI run to
determine both how often nodes were combined to-
gether by our algorithm to produce N to 1 align-
ments and the number of reorderings undertaken. It
turns out that out of the 59,032 pairs of nodes were
aligned for 3076 sentence pairs:6 55,391 alignments
6When sentences were misparsed in one language or the
other they were not reordered by the program.
95
were 1 to 1 (93.8% of the total) , 3443 alignments
were 2 to 1 (5.8% of the total) and 203 alignments
were N to 1, where N is greater than 2 (0.3% of the
total). The reordering program moved 1597 single
tokens; 2140 blocks 2 or 3 tokens long; 1203 blocks
of 4 or 5 tokens; 610 blocks of 6 or 7 tokens, 419
blocks of 8, 9 or 10 tokens, and 383 blocks of more
than 10 tokens.
7 Concluding Remarks
We have demonstrated that deep level linguistic
analysis can be used to improve word alignment re-
sults. It is natural to consider whether or not these
reorderings are likely to improve MT results. Both
the manual and alignment-based systems moved
post-nominal English modifiers to pre-nominal po-
sition, to reflect Chinese word order ? other move-
ments were much less frequent. In principle, these
selective reorderings may help SMT systems iden-
tify phrases of English that correspond to phrases of
Chinese, thus improving the quality of the phrase ta-
bles, especially when large chunks are moved. We
would also expect that the precision of our system to
be more important than the recall, since our system
would not yield an improvement if it produced too
much noise. Further experiments with current MT
systems are needed to assess whether this is actually
the case. We are considering such tests for future re-
search, using the Moses SMT system (Koehn et al,
2007).
Our representation had several possible advan-
tages over pure parse-based methods. We used se-
mantic features such as temporal, locative and trans-
parent (whether a low-content words inherits its se-
mantics) to help guide our alignment. The regu-
larized structure, also, helped identify long-distance
dependency relationships. We are also consider-
ing several improvements for our alignment-based
rules: (1) using additional dictionary resources such
as CATVAR (Habash and Dorr, 2003), so that cross-
part-of speech alignments can be more readily rec-
ognized; (2) finding more optimal orderings for
unaligned source language words. For example,
the alignment-based method reordered a bright star
arising from China?s policy to a bright arising from
China ?s policy star, separating bright from star,
even though bright star function as a unit; (3) incor-
porating and using multi-word bilingual dictionary
entries.; (4) automatic methods for tuning parame-
ters of our system that are currently hand-coded; (5)
training MI on a much larger corpus; (6) investigat-
ing possible ways to merge the manual-rules with
the alignment-based approach; and (7) performing
similar experiments with English/Japanese bitexts.
We would expect both parse-based approaches
and our system to handle mismatches that cover
large distances better than more shallow approaches
to reordering, e.g., (Crego and Marin?o, 2006) in the
same way that a full-parse handles constituent struc-
ture more completely than a chunker. In addition,
we would expect our approach to work best in lan-
guages where there are large differences in word or-
der, as these are exactly the cases that all predicate-
argument structure is designed to handle well (they
reduce apparent variation in structure). Towards this
end we are currently working on a Japanese/English
system. Obviously, the cost of developing GLARF
(or similar) systems are high, require linguistic ex-
pertise and may not be possible for resource-poor
languages. Nevertheless, we maintain that such sys-
tems are useful for many purposes and are there-
fore worth the cost. The GLARF system for En-
glish is available for download at http://nlp.
cs.nyu.edu/meyers/GLARF.html.
Acknowledgments
This work was supported by NSF Grant IIS-
0534700 Structure Alignment-based MT.
References
J. Bresnan and R. M. Kaplan. 1982. Syntactic Represen-
tation: Lexical-Functional Grammar: A Formal The-
ory for Grammatical Representation. In J. Bresnan,
editor, The Mental Representation of Grammatical Re-
lations. The MIT Press, Cambridge.
A. Bryl and J. van Genabith. 2010. f-align: An Open-
Source Alignment Tool for LFG f-Structures. In Pro-
ceedings of AMTA 2010.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL 2001, pages 116?123.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause
Restructuring for Statistical Machine Translation. In
ACL 2005.
96
B. A. Cowan. 2008. A Tree-to-Tree Model for Statistical
Machine Translation. Ph.D. thesis, MIT.
J. M. Crego and J. B. Marin?o. 2006. Integration of POS-
tag-based source reordering into SMT decoding by an
extended search graph. In AMTA?06.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. The MIT Press, Cambridge.
A. Fraser and D. Marcu. 2007. Measuring Word
Alignment Quality for Statistical Machine Translation.
Computational Linguistics, 33:293?303.
D. Gildea and D. Jurafsky. 2002. Automatic Labeling of
Semantic Roles. Computational Linguistics, 28:245?
288.
R. Grishman. 2000. Entity Annotation Guidelines.
ftp://jaguar.ncsl.nist.gov/ace/phase1/edt phase1 v2.2.pdf.
N. Habash and B. Dorr. 2003. CatVar: A Database of
Categorial Variations for English. In Proceedings of
the MT Summit, pages 471?474, New Orleans.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley-Interscience, New York.
J. R. Hobbs and R. Grishman. 1976. The Automatic
Transformational Analysis of English Sentences: An
Implementation. International Journal of Computer
Mathematics, 5:267?283.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Sydney,
Australia.
H. Ji, R. Grishman, D. Freitag, M. Blume, J. Wang,
S. Khadivi, R. Zens, and H. Ney. 2009. Name Transla-
tion for Distillation. In Global Autonomous Language
Exploitation. Springer.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In ACL 2007 Demon-
stration Session, Prague.
P. Laplace. 1816. Essai philosophique sur les probabil-
its. Courcier Imprimeur, Paris.
Adam Meyers, Roman Yangarber, and Ralph Grishman.
1996. Alignment of Shared Forests for Bilingual Cor-
pora. In Proceedings of Coling 1996: The 16th In-
ternational Conference on Computational Linguistics,
pages 460?465.
Adam Meyers, Roman Yangarber, Ralph Grishman,
Catherine Macleod, and Antonio Moreno-Sandoval.
1998. Deriving Transfer Rules from Dominance-
Preserving Alignments. In Proceedings of Coling-
ACL98: The 17th International Conference on Com-
putational Linguistics and the 36th Meeting of the As-
sociation for Computational Linguistics.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. Annotating
Noun Argument Structure for NomBank. In Proceed-
ings of LREC-2004, Lisbon, Portugal.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao,
and W. Xu. 2009. Automatic Recognition of Logi-
cal Relations for English, Chinese and Japanese in the
GLARF Framework. In SEW-2009 at NAACL-HLT-
2009.
E. Miltsakaki, A. Joshi, R. Prasad, and B. Webber. 2004.
Annotating discourse connectives and their arguments.
In A. Meyers, editor, NAACL/HLT 2004 Workshop:
Frontiers in Corpus Annotation, pages 9?16, Boston,
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
F. J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In ACL 2000.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
S. Petrov and D. Klein. 2007. Improved Inference for
Unlexicalized Parsing. In HLT-NAACL 2007.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In ACL 2008.
J. Wagner, D. Seddah, J. Foster, and J. van Genabith.
2007. C-Structures and F-Structures for the British
National Corpus. In Proceedings of the Twelfth In-
ternational Lexical Functional Grammar Conference,
Stanford. CSLI Publications.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
EMNLP-CoNLL 2007, pages 737?745.
D. Wu and P. Fung. 2009. Semantic roles for smt: A
hybrid two-pass model. In HLT-NAACL-2009, pages
13?16, Boulder, Colorado, June. Association for Com-
putational Linguistics.
N. Xue and M. Palmer. 2003. Annotating the Proposi-
tions in the Penn Chinese Treebank. In The Proceed-
ings of the 2nd SIGHAN Workshop on Chinese Lan-
guage Processing, Sapporo.
N. Xue. 2008. Labeling Chinese Predicates with Seman-
tic roles. Computational Linguistics, 34:225?255.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL, pages 523?530.
Y. Zhang, R. Zens, and H. Ney. 2007. Chunk-Level
Reordering of Source Language Sentences with Auto-
matically Learned Rules for Statistical Machine Trans-
lation. In Proc. of NAACL/HLT 2007.
M. Zhang, H. Jiang, A. Aw, H. Li, C. L. Tan, and S. Li.
2008. A Tree Sequence Alignment-based Tree-to-Tree
Translation Model. In ACL 2008.
97
Proceedings of SADAATL 2014, pages 11?20,
Dublin, Ireland, August 24, 2014.
Jargon-Term Extraction by Chunking
Adam Meyers
?
, Zachary Glass
?
, Angus Grieve-Smith
?
, Yifan He
?
,
Shasha Liao
?
and Ralph Grishman
?
New York University
?
, Google
?
meyers/angus/yhe/grishman@cs.nyu.edu, zglass@alumni.princeton.edu
Abstract
NLP definitions of Terminology are usually application-dependent. IR terms are noun sequences
that characterize topics. Terms can also be arguments for relations like abbreviation, definition or
IS-A. In contrast, this paper explores techniques for extracting terms fitting a broader definition:
noun sequences specific to topics and not well-known to naive adults. We describe a chunking-
based approach, an evaluation, and applications to non-topic-specific relation extraction.
1 Introduction
Webster?s II New College Dictionary (Houghton Mifflin Company, 2001, p.1138) defines terminology
as: The vocabulary of technical terms and usages appropriate to a particular field, subject, science,
or art. Systems for automatically extracting instances of terminology (terms) usually assume narrow
operational definitions that are compatible with particular tasks. Terminology, in the context of Infor-
mation Retrieval (IR) (Jacquemin and Bourigault, 2003) refers to keyword search terms (microarray,
potato, genetic algorithm), single or multi-word (mostly nominal) expressions collectively representing
topics of documents that contain them. These same terms are also used for creating domain-specific
thesauri and ontologies (Velardi et al., 2001). We will refer to these types of terms as topic-terms and
this type of terminology topic-terminology. In other work, types of terminology (genes, chemical names,
biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al.,
2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information
Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus
et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may
not be terminology-specific, but have terms as arguments, e.g., (Schwartz and Hearst, 2003; Jin et al.,
2013) detect abbreviation and definition relations respectively and the arguments are terms. In contrast
to this previous work, we have built a system that extracts a larger set of terminology, which we call
jargon-terminology. Jargon-terms may include ultracentrifuge, which is unlikely to be a topic-term of a
current biology article, but will not include potato, a non-technical word that could be a valid topic-term.
We aim to find all the jargon-terms found in a text, not just the ones that fill slots for specific relations.
As we show, jargon-terminology closely matches the notional (e.g., Webster?s) definition of terminol-
ogy. Furthermore, the important nominals in technical documents tend to be jargon-terms, making them
likely arguments of a wide variety of possible IE relations (concepts or objects that are invented, two
nominals that are in contrast, one object that is ?better than? another, etc.). Specifically, the identification
of jargon-terms lays the ground for IE tasks that are not genre or task dependent. Our approach which
finds all instances of terms (tokens) in text is conducive to these tasks. In contrast, topic-term detection
techniques find smaller sets of terms (types), each term occurring multiple times and the set of terms
collectively represents a topic, in a similar way that a set of documents can represent a topic.
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organisers. License details: http://creativecommons.org/licenses/by/4.0/
11
This paper describes a system for extracting jargon-terms in technical documents (patents and journal
articles); the evaluation of this system using manually annotated documents; and a set of information
extraction (IE) relations which take jargon-terms as arguments. We incorporate previous work in termi-
nology extraction, assuming that terminology is restricted to noun groups (minus some left modifiers)
(Justeson and Katz, 1995);
1
and we use both topic-term extraction techniques (Navigli and Velardi,
2004) and relation-based extraction techniques (Jin et al., 2013) in components of our system. Rather
than looking at the distribution of noun groups as a whole for determining term-hood, we refine the
classes used by the noun group chunker itself, placing limitations on the candidate noun groups pro-
posed and then filtering the output by setting thresholds on the number and quality of the ?jargon-like?
components of the phrase. The resulting system admits not only topic-terms, but also other non-topic
instances of terminology. Using the more inclusive set of jargon-terms (rather than just topic-terms) as
arguments of the IE relations in section 6, we are able to detect a larger and more informative set of rela-
tion. Furthermore, these relations are salient for a wide variety of genres (unlike those in (BioCreAtIvE,
2006)) ? a genre-neutral definition of terminology makes this possible. For example, the CONTRAST
relation between the two bold face terms in necrotrophic effector system
A1
that is an exciting contrast
to the biotrophic effector models
A2
. would be applicable in most academic genres. Our jargon-terms
also contrast with the tactic of filling terminology slots in relations with any noun-group (Justeson and
Katz, 1995), as such a strategy overgenerates, lowering precision.
2 Topic-term Extraction
Topic-term extractors (Velardi et al., 2001; Tomokiyo and Hurst, 2003) collect candidate terms (N-grams,
noun groups, words) that are more representative of a foreground corpus (documents about a specific
topic) than they are of a background corpus (documents about a wide range of topics), using statistical
measures such as
Term Frequency
Inverse Document Frequency
(TFIDF), or a variation thereof. Due to the metrics used
and cutoffs assumed, the list of terms selected is usually no more than a few hundred distinct terms,
even for a large set of foreground documents and tend to be especially salient to that topic. The terms
can be phrases that lay people would not know (e.g., microarray, genetic algorithm) or common topics
for that document set (e.g., potato, computer). Such systems rank all candidate terms, using cutoffs
(minimum scores or percentages of the list) to separate out the highest-ranked terms as output. Thus
sets of topic-terms, derived this way, are dependent on the foreground and background assumed, and the
publication dates. So a precise definition would include such information, e.g., topic-terms(biomedical-
patents, random-patents, 1990?1999) would refer to those topic-terms that differentiate a foreground of
biomedical patents from the 1990s from a background of diverse patents from the same epoch. Narrower
topics are possible (e.g., comparing DNA-microarray patents to the same background); or broader ones
(e.g., if a diverse corpus including news articles, fiction and travel writing are the background set instead
of patents, then patent terms such as national stage application may be highly ranked in the output).
Thus topic-terms generated by these methods model a relationally based definition and are relative to the
chosen foregrounds, backgrounds and dates.
Topic-terms can include words/phrases like potato, wheat, rat, monkey, which may be common sub-
jects of some set of biomedical documents, but are not specific to a technical field. In contrast, jargon-
terms would include words (like ultracentrifuge, theorem, graduated cylinder) that are specific to tech-
nical language, but don?t tend to be topics of any current document of interest. Jargon-terms, like topic-
terms, can be defined relative to a particular foreground (which can also be represented as a set of
documents), but there is the implicit assumption that they all share the same background set: non-genre-
specific language (or simply a very diverse set of documents). It is also possible to refer to terminology in
general as the union of jargon-terms with respect to the set of specialized knowledge areas as foregrounds
and all sharing the same background of non-genre-specific language. Jargon-terms, like topic-terms, are
also time dependent, since some terms will eventually be absorbed into the common lexicon, e.g., com-
puter. However, we can make the simplifying assumption that we are talking about jargon in the present
1
We restrict our scope to nominal terminology, but acknowledge the importance of non-nominal terminology, e.g., event
verb terms (calcify, coactivate) which are crucial to IE.
12
time. Furthermore, jargon-term status is somewhat less time sensitive than topic-term status because ter-
minology is absorbed very sparingly (and very slowly) into the popular lexicon, whereas topics go in and
out of fashion quickly within a literature that is meant for an expert audience. Ignoring the potato type
cases, topic-terms are a proper subset of jargon-terms and, thus, the set of jargon-terms is larger than the
set of topic-terms. Finally, topic terms are ranked with respect to how well they can serve as keywords,
i.e., how specific they are to a particular document set, whereas +/-jargon-term is a binary distinction.
We built a topic term extractor that combines several metrics together in an ensemble including:
TFIDF, KL Divergence (Cover and Thomas, 1991; Hisamitsu et al., 1999) and a combination of Do-
main Relevance and Document Consensus (DRDC) based on (Navigli and Velardi, 2004). Furthermore,
we filtered the output by requiring that each term would be recognized as a term by the jargon-term chun-
ker described below in section 3. We manually scored the top 100 terms generated for two classes of
biology patents (US patent classes 435 and 436) and achieved accuracies of 85% and 76% respectively.
We also manually evaluated the top 100 terms taken from biology articles, yielding an accuracy of about
88%. As discussed, we use the output of this system for our jargon-term extraction system.
3 Jargon-term Extraction by Chunking
(Justeson and Katz, 1995) uses manual rules to detect noun groups (sequences of nouns and adjectives
ending in a noun) with the goal of detecting instances of topic-terms. They filter out those noun groups
that occur only once in the document on the theory that the multiply used noun groups are more likely to
be topics. They manually score their output from two computer science articles and one biotechnology
article, with 146, 350 and 834 instances of terms and achieve accuracies of 96%, 86% and 77%. (Frantzi
et al., 2000) uses linguistic rules similar to noun chunking to detect candidate terms; filters the results
using a stop list and other linguistic constraints; uses statistical filters to determine whether substrings
are likely to be terms as well; and uses statistical filters based on neighboring words (context). (Frantzi et
al., 2000) ranks their terms by scores and achieve about 75% accuracy for the top 40 terms ? their system
is tested on medical records (quite a different corpus form ours). Our system identifies all instances
of terminology (not just topic terms) and identifies many more instances per document (919, 1131 and
2166) than (Justeson and Katz, 1995) or (Frank, 2000). As we aim to find all instances of jargon-terms,
we evaluate for both precision and recall rather than just accuracy (section 5). Two of the documents
that we test on are patents, which have a very different word distribution than articles. In fact, due to
both the amount of repetition in patents and the presence of multiple types of terminology (legal terms
as well as topic-related terms), it is hard to imagine that eliminating terms occurring below a frequency
threshold (as with (Justeson and Katz, 1995)) would be an effective method of filtering. Furthermore,
(Frank, 2000) used a very different corpus than we did and they focused on a slightly different problem
(e.g., we did not attempt to find the highest-ranked terms and we did not attempt to find both long terms
and substrings which were terms). Thus while it is appropriate to compare our methodology, it is difficult
to compare our results.
We have implemented a hand-crafted term extractor, which we will call a jargon-term chunker because
it functions in much the same way as a noun group chunker. It uses a deterministic finite state machine,
based on parts of speech (POS) and a fine-tuned set of lexical categories. We observed that jargon-terms
are typically noun groups, minus some left modifiers, and normally include words that are not in standard
vocabulary or belong to certain other classes of words (e.g., nominalizations). While topic-term tech-
niques factor the distribution of whole term sequences into the choice of topic-terms, our method focuses
on the distribution of words within topic-term sequences. The primary function of POS classification is
to cluster words distributionally in a language. A POS tag reflects the syntactic distribution of the word
in the sense that words with the same POS should be able to replace each other in sentences. Morpholog-
ically, POSs are subject to the same morphological variation (prefixes, suffixes, tense, gender, number,
etc.). For example, the English word duck belongs to the POS noun because it tends to occur: after a
determiner, after an adjective, and ending a unit that can be the subject of a verb: nouns are substitutable
for each other. Furthermore, it has a plural form resulting from an -s or -es suffix, etc. Similarly, we
hold that the presence of particular classes of words within a noun group affects its potential to function
13
as a jargon-term. As will become evident, we can use topic-term-like metrics to identify some of these
word classes. Furthermore, given our previous assertion that topic-terms are a subset of jargon-terms,
we assume that the most saliently ranked topic-terms are also jargon-terms and words that are commonly
parts of topic-terms tend to be parts of jargon-terms. There are also ?morphological properties? that are
indicative of subsets of jargon-terms: allCap acronyms, chemical formulas, etc.
Our system classifies each word using POS tags, manually created dictionaries and the output of our
own topic-term system. These classifications are achieved in four stages. In the first stage we divide
the text into smaller segments using coordinate conjunctions (and, or, as well as, . . .) and punctuation
(periods, left/right parentheses and brackets, quotation marks, commas, colons, semi-colons). These
segments are typically smaller than the level of the sentence, but larger than most noun groups. These
segments are good units to process because they are larger than jargon-terms (substrings of noun groups)
and smaller than sentences (and thus provide a smaller search space). In the second stage, potential
jargon-term (PJs) are generated by processing tokens from left to right and classifying them using a
finite state machine (FSM). The third stage filters the PJs generated with a set of manually constructed
constraints, yielding a set of jargon-terms. A final filter (stage 4) identifies named entities and separates
them out from the true jargon-terms: it turns out that many named entities have similar phrase-internal
properties as jargon-terms.
The FSM (that generates PJs) in the second stage includes the following states (Ramshaw and Marcus,
1995): START (S) (marking the beginning of a segment), Begin Term (B-T), Inside Term (I-T), End
Term (E-T), and Other (O). A PJ is a sequence consisting of: (a) a single E-T; or (b) exactly one B-T,
followed by zero or more instances of I-T, followed by zero or one instances of E-T. Each transition to
a new state is conditioned on: (a) the (extended) POS tag of the current word; (b) the extended POS
tag of the previous word; and (c) the previous state. The extended POSs are derived from the output of
a Penn-Treebank-based POS tagger and refinements based on machine readable dictionaries, including
COMLEX Syntax (Macleod et al., 1997), NOMLEX (Macleod et al., 1998), and some manually encoded
dictionaries created for this project. Table 1 describes the transitions in the FSM (unspecified entries
mean no restriction). ELSE indicates that in all cases other than those listed, the FST goes to state O.
Extended POS tags are classified as follows.
Adjectives, words with POS tags JJ, JJR or JJS, are subdivided into:
STAT-ADJ: Words in this class are marked adjective in our POS dictionaries and found as the first word
in one of the top ranked topic-terms (for the topic associated with the input document).
TECH-ADJ: If an adjective ends in a suffix indicating (-ic, -cous, -xous, and several others) it is a
technical word, but it is not found in our list of exceptions, it is marked TECH-ADJ.
NAT-ADJ: An adjective, usually capitalized, that is the adjectival form of a country, state, city or conti-
nent, e.g., European, Indian, Peruvian, . . .
CAP-ADJ: An adjective such that the first letter is capitalized (but is not marked NAT-ADJ).
ADJ: Other adjectives
Nouns are marked NN or NNS by the POS tagger and are the default POS for out of vocabulary (OOV)
words. POS tags like NNP, NNPS and FW (proper nouns and foreign nouns) are not reliable for our POS
tagger (trained on news) when applied to patents and technical articles. So NOUN is also assumed for
these. Subclasses include:
O-NOUN: (Singular or plural) nouns not found in any of our dictionaries (COMLEX plus some person
names) or nouns found in lists of specialized vocabulary which currently include chemical names.
PER-NOUN: Nouns beginning with a capital that are in our dictionary of first and last names.
PLUR-NOUN: Nouns with POS NNS nouns that are not marked O-NOUN or PER-NOUN.
C-NOUN: Nouns with POS NN that are not marked O-NOUN or PER-NOUN.
Verbs Only ING-VERBs (VBG) and ED-VERBs (VBN and VBD) are needed for this task (other
verbs trigger state O). Finally, we use the following additional POS tags:
POSS: POS for ?s, split off from a possessive noun.
PREP: All prepositions (POS IN and TO)
ROM-NUM: Roman numerals (I, II, . . ., MMM)
14
Previous Current Previous New
POS POS State State
DET, PREP, POSS, VERB O
O-NOUN, C-NOUN, PLUR-NOUN ROM-NUM B-T, I-T E-T
PLUR-NOUN B-T,I-T I-T
ADJ, CAP-ADJ I-T I-T
C-NOUN, PER-NOUN, O-NOUN B-T, I-T I-T
O-NOUN CAP-ADJ, TECH-ADJ, B-T, I-T I-T
STAT-ADJ, NAT-ADJ
CAP-ADJ, TECH-ADJ, NAT-ADJ, E-T, O, S B-T
ING-VERB, ED-VERB, STAT-ADJ
C-NOUN, O-NOUN, PER-NOUN
TECH-ADJ, NAT-ADJ TECH-ADJ, NAT-ADJ B-T, I-T I-T
ADJ, CAP-ADJ ADJ, CAP-ADJ
ELSE O
Table 1: Transition Table
A potential jargon-term (PJ) is an actual jargon-term unless it is filtered out as follows. First, a jargon
term J must meet all of these conditions:
1. J must contain at least one noun.
2. J must be more than one character long, not counting a final period.
3. J must contain at least one word consisting completely of alphabetic characters.
4. J must not end in a common abbreviation from a list (e.g., cf., etc.)
5. J must not contain a word that violates a morphological filter, designed to rule out numeric identi-
fiers (patent numbers), mathematical formulas and other non-words. This rules out tokens beginning
with numbers that include letters; tokens including plus signs, ampersands, subscripts, superscripts;
tokens containing no alphanumeric characters at all, etc.
6. J must not contain a word that is a member of a list of common patent section headings.
Secondly, a jargon-term J must satisfy at least one of the following additional conditions:
1. J = highly ranked topic-term or a substring of J is a highly ranked topic-term.
2. J contains at least one O-NOUN.
3. J consists of at least 4 words, at least 3 of which are either nominalizations (C-NOUNs found in
NOMLEX-PLUS (Meyers et al., 2004; Meyers, 2007)) or TECH-ADJs.
4. J = nominalization at least 11 characters long.
5. J = multi-word ending in a common noun and containing a nominalization.
A final stage aims to distinguish named entities from jargon-terms. It turns out that named entities, like
jargon terms, include many out of vocabulary words. Thus we look for NEs among those PJs that remain
after stage 3 and contain capitalized words (a single capital letter followed by lowercase letters). These
NE filters are based on manually collected lists of named entities and nationality adjectives, as well as
common NE endings. Dictionary lookup is used to assign GPE (ACE?s Geopolitical Entity) to New York
or American; LOC(ation) to Aegean Sea and Ural Mountains; and FAC(ility) to Panama Canal and Suez
Canal. Plurals of nationality words, e.g., Americans are filtered out as non-terms. PJs are filtered by
endings typically associated with non-terms, e.g., et al signals PJs as citations to articles and honorifics
(Esq, PhD, Jr, Snr) signal PER(son) named entities. Finally, if at least one of the words in a multi-word
term is a first or last person name, we can further filter them by endings, where ORGanization endings
15
include Agency, Association, College and more than 65 others; GPE endings include Heights, Township,
Park; LOC(ation) endings include Street, Avenue and Boulevard. It turns out that 2 word capitalized
structures including at least one person name are usually either ORG or GPE in our patent corpus, and
we maintain this ambiguity, but mark them as non-terms.
We have described a first implementation of a jargon-term chunker based on a combination of prin-
ciples previously implemented in noun group chunking and topic-term extraction systems. The chunker
can use essentially the same algorithms as previous noun group chunkers, though in this case we used
a manual-rule based FSM. The extended POSs are defined according to conventional POS (represent-
ing substitutability, morphology, etc.), statistical topic-term extraction, OOV status (absence from our
dictionary) or presence in specialized dictionaries (NOMLEX, dictionary of chemicals, etc.). We use
topic-term extraction to identify both particular noun sequences (high-ranked topic-terms) and some of
their components (STAT-ADJ), and could extend this strategy to other components, e.g., common head
nouns. We approximated the concept of ?rare word? by noting which words were not found in our
standard dictionary (O-NOUN). As is well-known, ?noun? and ?adjective? are the first and second most
frequent POS for OOV words and both POSs are typically found as part of noun groups. Furthermore,
rare instances of O-NOUN (and OOV adjectives) are typically parts of jargon-terms. This approximation
is fine-tuned by the addition of word lists (e.g., chemicals). In future work, we can use more distribu-
tional information to fine-tune these categories, e.g., we can use topic-term techniques to identify single
topic words (nouns and adjectives) and experiment with these additional POS (instead of or in addition
to the current POS classes).
4 The Annotator Definition of Jargon-Term
For purposes of annotation, we defined jargon-term as a word or multi-word nominal expression that is
specific to some technical sublanguage. It need not be a proper noun, but it should be conventionalized
in one of the following two ways:
1. The term is defined early (possibly by being abbreviated) in the document and used repeatedly
(possibly only in its abbreviated form).
2. The term is special to a particular field or subfield (not necessarily the field of the document being
annotated). It is not enough if the document contains a useful description of an object of interest
? there must be some conventional, definable term that can be used and reused. Thus multi-word
expressions that are defined as jargon terms must be somewhat word-like ? mere descriptions that
are never reused verbatim are not jargon terms. (Justeson and Katz, 1995) goes further than we do:
they require that terms be reused within the document being annotated, whereas we only require
that they be reused (e.g., frequent hits in a web search).
Criterion 2 leaves open the question of how specific to a genre an expression must be to be considered a
jargon-term. At an intuitive level, we would like to exclude words like patient, which occur frequently
in medical texts, but are also commonly found in non-expert, everyday language. By contrast, we would
like to include words like tumor and chromosome, which are more intrinsic to technical language insofar
as they have specialized definitions and subtypes within medical language. To clarify, we posited that a
jargon-term must be sufficiently specialized so that a typical naive adult should not be expected to know
the meaning of the term. We developed 2 alternative models of a naive adult:
1. Homer Simpson, an animated TV character who caricatures the typical naive adult?the annotators
invoke the question: Would Homer Simpson know what this means?
2. The Juvenile Fiction sub-corpus of the COCA: The annotators go to http://corpus.byu.
edu/coca/ and search under FIC:Juvenile ? a single occurrence of an expression in this corpus
suggests that it is probably not a jargon-term.
In addition, several rules limited the span of terms to include the head and left modifiers that collocate
with the heads. Decisions about which modifiers to include in a term were difficult. However, as this
16
Strict Sloppy
Doc Terms Matches Pre Rec F Matches Pre Rec F
Annot 1
SRP 1131 798 70.8% 70.6% 70.7% 1041 92.5% 92.0% 92.2%
SUP 2166 1809 87.5% 83.5% 85.5% 1992 96.3% 92.0% 94.1%
VVA 919 713 90.9% 77.6% 83.7% 762 97.2% 82.9% 89.5%
Annot 2
SRP 1131 960 98.4% 84.9% 91.1% 968 99.2% 85.6% 91.9%
SUP 2166 1999 95.5% 92.3% 93.8% 2062 98.5% 95.2% 96.8%
VVA 919 838 97.4% 91.2% 94.2% 855 99.4% 93.0% 96.1%
Base 1
SRP 1131 602 24.3% 53.2% 33.4% 968 44.2% 96.8% 60.7%
SUP 2166 1367 36.5% 63.1% 46.2% 1897 50.6% 87.6% 64.2%
VVA 919 576 28.5% 62.7% 39.2% 887 44.0% 96.5% 60.4%
Base 2:
SRP 1131 66 24.9% 5.8% 9.5% 151 57.0% 13.4% 21.6%
SUP 2166 771 52.3% 35.6% 42.4% 1007 68.4% 46.5% 55.3%
VVA 919 270 45.8% 29.4% 35.8% 392 66.5% 42.6% 51.9%
System SRP 1131 932 39.0% 82.4% 53.0% 1121 46.9% 99.1% 63.7%
Without SUP 2166 1475 39.7% 68.1% 50.2% 1962 52.8% 90.6% 66.7%
Filter VVA 919 629 27.8% 68.4% 39.5% 900 39.8% 97.9% 56.6%
System
SRP 1131 669 69.0% 59.2% 63.7% 802 82.8% 70.9% 76.4%
SUP 2166 1193 64.7% 55.1% 59.5% 1526 82.8% 70.5% 76.1%
VVA 919 581 62.1% 63.2% 62.7% 722 77.2% 78.6% 77.9%
Table 2: Evaluation of Annotation, Baseline and Complete System Against Adjudicated Data
evaluation task came on the heels of the relation extraction task described in section 6, we based our
extent rules on the definitions and the set of problematic examples that were discussed and cataloged
during that project. This essentially formed the annotation equivalent of case-law for extents. We will
make our annotation specifications available on-line, along with discussions of these cases.
5 Evaluation
For evaluation purposes, we annotated all the instances of jargon-terms in a speech recognition patent
(SRP), a sunscreen patent (SUP) and an article about a virus vaccine (VVA). Each document was an-
notated by 2 people and then adjudicated by Annotator 2 after discussing controversial cases. Table 2
scores the system, annotator 1 and annotator 2, by comparing each against the answer key providing:
number of terms in the answer key, number of matches, precision, recall and F-measure. The ?strict?
scores are based on exact matches between system terms and answer key terms, whereas the ?sloppy?
scores count as correct instances where part of a system term matches part of an answer key term (span
errors). As the SRP document was annotated first, some of specification agreement process took place
after annotation and the scores for annotators are somewhat lower than for the other documents. How-
ever, Annotator 1?s scores for SUP and VVA are good approximations of how well a human being should
be expected to perform and the system?s scores should be compared to Annotator 1 (i.e., accounting for
the adjudicator?s bias).
There are 4 system results: two baseline systems and two stages of the system described in section 3.
Baseline 1 assumes terms derived by removing determiners from noun groups ? we used an MEMM
chunker using features from the GENIA corpus (Kim et al., 2003). That system has relatively high recall,
but overgenerates, yielding a lower precision and F-measure than our full system ? it is also inaccurate
at determining the extent of terms. Baseline 2 restricts the noun groups from this same chunker to those
with O-NOUN heads. This improves the precision at a high cost to recall. Similarly, we first ran our
system without filtering the potential jargon-terms, and then we ran the full system. Clearly our more
complex strategy performs better than these baselines and the linguistic filters increase precision more
than they reduce recall, resulting in higher F-measures (though low-precision high-recall output may be
better for some applications).
17
6 Relations with Jargon-Terms
(Meyers et al., 2014) describes the annotation of 200 PubMed articles from and 26 patents with several
relations, as well as a system for automatically extracting relations. It turned out that the automatic
system depended on the creation of a jargon-term extraction system and thus that work was the major
motivating factor for the research described here. Choosing topic-terms as potential arguments would
have resulted in low recall. In contrast, allowing any noun-group to be an argument would have lowered
precision, e.g., diagram, large number, accordance and first step are unlikely to be valid arguments of
relations. In the example: The resequencing pathogen microarray
A2
in the diagram is a promising new
technology., we can detect that the authors of the articles view pathogen microarray as significant, and
not the NG diagram. By selecting jargon-terms as potential arguments we are selecting the most probable
noun group arguments for our relations. For the current system (which does not use a parser), the system
performs best if non-jargon-terms are not considered as potential relation arguments at all. However, one
could imagine a wider coverage (and slower) system incorporating a preference for jargon-terms (like a
selection restriction) with dependency-based constraints.
We will only describe a few of these relations due to space considerations. Our relations include:
(1) ABBREVIATE, a relation between two terms that are equivalent. In the normal case, one term
is clearly a shorthand version of the other, e.g., ?The D. melanogaster gene Muscle LIM protein at
84B
A1
(abbreviated as Mlp84B
A2
)?. However, in the special case (ABBREVIATE:ALIAS) neither
term is a shorthand for the other. For example in ?Silver behenate
A1
, also known as CH3-(CH2)20-
COOAg
A2
?, the chemical name establishes that this substance is a salt, whereas the formula provides
the proportions of all its constituent elements; (2) ORIGINATE, the relation between an ARG1 (person,
organization or document) and an ARG2 (a term), such that the ARG1 is an inventor, discoverer, manu-
facturer, or distributor of the ARG2 and some of these roles are differentiated as subtypes of the relation.
Examples include the following: ?Eagle
A1
?s minimum essential media
A2
and DOPG
A2
was obtained
from Avanti Polar Lipids
A1
?. (3) EXEMPLIFY, an IS-A relation (Hearst, 1992) between terms so
that ARG1 is an instance of ARG2, e.g., ?Cytokines
A2
, for instance interferon
A1
?; and ?proteins
A2
such as insulin
A1
?; (4) CONTRAST relations, e.g., ?necrotrophic effector system
A1
that is an ex-
citing contrast to the biotrophic effector models
A2
?; (5) BETTER THAN relations, e.g., ?Bayesian
networks
A1
hold a considerable advantage over pairwise association tests
A2
?; and (6) SIGNIFICANT
relations, e.g., ?Anaerobic SBs
A2
are an emerging area of research and development? (ARG1, the author
of the article, is implicit). These relations are applicable to most technical genres.
7 Concluding Remarks
We have described a method for extracting instances of jargon-terms with an F-measure of between
62% and 77% (strict vs sloppy), about 73% to 84% of human performance. We expect this work to
facilitate the extraction of a wide reange of relations from technical documents. Previous work has
focused on generating topic-terminology or term types, extracted over sets of documents. In contrast, we
describe an effective method of extracting term tokens, which represent a larger percent of the instances
of terminology in documents and constitute arguments of many more potential relations. Our work on
relation extraction yielded very low recalls until we adopted this methodology. Consequently, we have
obtained recall of over 50% for many relations (with precision ranging from 70% for OPINION relations
like Significant to 96% for Originate.).
Acknowledgments
Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Inte-
rior National Business Center contract number D11PC20154. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation
thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should
not be interpreted as necessarily representing the official policies or endorsements, either expressed or
implied, of IARPA, DoI/NBC, or the U.S. Government.
18
References
M. Bada, L. E. Hunter, M. Eckert, and M. Palmer. 2010. An overview of the craft concept annotation guidelines.
In The Linguistic Annotation Workshop, ACL 2010, pages 207?211.
BioCreAtIvE. 2006. Biocreative ii.
M. Bundschus, M. Dejori, M. Stetter, V Tresp, and H. Kriegel. 2008. Extraction of semantic biomedical relations
from text using conditional random fields. BMC Bioinformatics, 9.
P. Corbett, C. Batchelor, and S. Teufel. 2007. Annotation of chemical named entities. In BioNLP 2007, pages
57?64.
Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. Wiley-Interscience, New York.
A. Frank. 2000. Automatic F-Structure Annotation of Treebank Trees. In Proceedings of The LFG00 Conference,
Berkeley.
K. Frantzi, S. Ananiadou, and H. Mima. 2000. Automatic recognition of multi-word terms:. the C-value/NC-value
method. International Journal on Digital Libraries, 3(2):115?130.
C. Giuliano, A. Lavelli, and L. Romano. 2006. Exploiting shallow linguistic information for relation extraction
from biomedical literature. In EACL 2006, pages 401?408, Trento.
M. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In ACL 1992, pages 539?545.
T. Hisamitsu, Y. Niwa, S. Nishioka, H. Sakurai, O. Imaichi, M. Iwayama, and A. Takano. 1999. Term extraction
using a new measure of term representativeness. In Proceedings of the First NTCIR Workshop on Research in
Japanese Text Retrieval and Term Recognition.
Houghton Mifflin Company. 2001. Webster?s II New College Dictionary. Houghton Mifflin Company.
C. Jacquemin and D. Bourigault. 2003. Term Extraction and Automatic Indexing. In R. Mitkov, editor, Handbook
of Computational Linguistics. Oxford University Press, Oxford.
Y. Jin, M. Kan, J. Ng, and X. He. 2013. Mining scientific terms and their definitions: A study of the acl anthology.
In EMNLP-2013.
J. S. Justeson and S. M. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for
identification in text. Natural Language Engineering, 1(1):9?27.
J. D. Kim, T. Ohta, Y. Tateisi, and J. I. Tsujii. 2003. Genia corpus?a semantically annotated corpus for bio-
textmining. Bioinformatics, 19 (suppl 1):i180?i182.
C. Macleod, R. Grishman, and A. Meyers. 1997. COMLEX Syntax. Computers and the Humanities, 31:459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and R. Reeves. 1998. Nomlex: A lexicon of nominalizations. In
Proceedings of Euralex98.
A. Meyers, R. Reeves, C. Macleod, R. Szekeley, V. Zielinska, and B. Young. 2004. The Cross-Breeding of
Dictionaries. In Proceedings of LREC-2004, Lisbon, Portugal.
A. Meyers, G. Lee, A. Grieve-Smith, Y. He, and H. Taber. 2014. Annotating Relations in Scientific Articles. In
LREC-2014.
A. Meyers. 2007. Those Other NomBank Dictionaries ? Manual for Dictionaries that Come with NomBank.
http:nlp.cs.nyu.edu/meyers/nombank/nomdicts.pdf.
R. Navigli and P. Velardi. 2004. Learning Domain Ontologies from Document Warehouses and Dedicated Web
Sites. Computational Linguistics, 30.
L. A. Ramshaw and M. P. Marcus. 1995. Text Chunking using Transformation-Based Learning. In ACL Third
Workshop on Very Large Corpora, pages 82?94.
A. Schwartz and M. Hearst. 2003. A simple algorithm for identifying abbreviation definitions in biomedical text.
In Pacific Composium on Biocomputing.
T. Tomokiyo and M. Hurst. 2003. A language model approach to keyphrase extraction. In ACL 2003 Workshop
on Multiword Expressions: Analysis, Acquisition and Treatment.
19
P. Velardi, M. Missikoff, and R. Basili. 2001. Identification of relevant terms to support the construction of domain
ontologies. In Workshop on Human Language Technology and Knowledge Management - Volume 2001, pages
5:1?5:8.
20
