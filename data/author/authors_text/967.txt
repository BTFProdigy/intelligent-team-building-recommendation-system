Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 201?204,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Word Segmentation based on an Approach of Maximum Entropy
Modeling
Yan Song1 Jiaqing Guo1 Dongfeng Cai2
Natural Language Processing Lab
Shenyang Institute of Aeronautical Engineering
Shenyang, 110034, China
1.{mattsure,guojiaqing}@gmail.com
2.cdf@ge-soft.com
Abstract
In this paper, we described our Chinese
word segmentation system for the 3rd
SIGHAN Chinese Language Processing
Bakeoff Word Segmentation Task. Our
system deal with the Chinese character se-
quence by using the Maximum Entropy
model, which is fully automatically gen-
erated from the training data by analyz-
ing the character sequences from the train-
ing corpus. We analyze its performance
on both closed and open tracks on Mi-
crosoft Research (MSRA) and University
of Pennsylvania and University of Col-
orado (UPUC) corpus. It is shown that we
can get the results just acceptable without
using dictionary. The conclusion is also
presented.
1 Introduction
In the 3rd SIGHAN Chinese Language Process-
ing Bakeoff Word Segmentation Task, we partici-
pated in both closed and open tracks on Microsoft
Research corpus (MSRA for short) and University
of Pennsylvania and University of Colorado cor-
pus (UPUC for short). The following sections de-
scribed how our system works and presented the
results and analysis. Finally, the conclusion is pre-
sented with discussions of the system.
2 System Overview
Using Maximum Entropy approach for Chinese
Word Segmentation is not a fresh idea, some pre-
vious works (Xue and Shen, 2003; Low, Ng and
Guo, 2005) have got good performance in this
field. But what we consider in the process of
Segmentation is another way. We treat the input
text which need to be segmented as a sequence of
the Chinese characters, The segment process is, in
fact, to find where we should split the character se-
quence. The point is to get the segment probability
between 2 Chinese characters, which is different
from dealing with the character itself.
In this section, training and segmentation pro-
cess of the system is described to show how our
system works.
2.1 Pre-Process of Training
For the first step we find the Minimal Segment
Unit (MSU for short) of a text fragment in the
training corpus. A MSU is a character or a string
which is the minimal unit in a text fragment that
cannot be segmented any more. According to the
corpus, all of the MSUs can be divided into 5
type classes: ?C? - Chinese Character (such as
/\0 and /?0), ?AB? - alphabetic string
(such as ?SIGHAN?), ?EN? - digit string (such
as ?1234567?), ?CN? - Chinese number string
(such as /?z?0) and ?P? - punctua-
tion (/?0,/"0,/?0, etc). Besides the
classes above, we define a tag ?NL? as a special
MSU, which refers to the beginning or ending of a
text fragment. So, any MSU u can be described
as: u?C?AB?EN?CN?P?{NL}. In order
to check the capability of the pure Maximum En-
tropy model, in closed tracks, we didn?t have any
type of classes, the MSU here is every character
of the text fragment, u?C ??{NL}. For instance,
/???\
SIGHAN2006?c?m"0
is segmented into these MSUs: /?/?/?/\/

/S/I/G/H/A/N/2/0/0/6/?/c/?/m/"0.
Once we get al the MSUs of a text fragment,
we can get the value of the Nexus Coefficient (NC
for short) of any 2 adjacent MSUs according to
the training corpus. The set of NC value can be
201
described as: NC ? {0, 1}, where 0 means those
2 MSUs are segmented and 1 means they are not
segmented (Roughly, we appoint r = 0 if either
one of the 2 adjacent MSUs is NL). For example,
the NC value of these 2 MSUs/\0 and/?0
in the text fragment /\?0 is 0 since these 2
characters is segmented according to the training
corpus.
2.2 Training
Since the segmentation is to obtain NC value of
any 2 adjacent MSUs (here we call the interspace
of the 2 adjacent MSUs a check point, illustrated
below),
. . .U?3 U?2 U?1 U+1 U+2 U+3 . . .
6
Check Point of U?1 and U+1
we built a tool to extract the feature as follows:
(?) U?3, U?2, U?1, U+1, U+2, U+3
(?) U?1U+1
(?) r?2r?1
(?) U?3r?2, U?2r?1
() r?2U?2, r?1U?1
In these features above, U+n (U?n) refers to
the following (previous) n MSU of the check
point with the information of relative position
(Intuitively, We consider the same MSU has
different effect on the NC value of the check point
when its relative position is different to check
point). And U?1U+1 is the 2 adjacent MSUs of
the check point. r?2r?1 is the NC value of the
previous 2 check points. Similarly, the (?) and ()
features represent the MSUs with their adjacent
r. For instance, in the sentence?????I<,
we can extract these features for the check point
between the MSU? and?:
(?) NL?3,NL?2,??1,?+1,?+2,?+3,
(?)??1?+1
(?) 00 (because ? is the boundary of the sen-
tence)
(?) NL?30,NL?20
() 0NL?2,0??1
and also these features for the check point be-
tween the MSU? and?:
(?)??3,??2,??1,?+1,I+2,<+3
(?)??1?+1
Figure 1: MSRA training curve
Figure 2: UPUC training curve
(?) 01 (for UPUC corpus, here the value is 00
since ?? is segmented into 2 characters, but in
MSRA corpus,?? is treated as a word)
(?)??30,??21
() 0??2,1??1
After the extraction of the features, we use the
ZhangLe?s Maximum Entropy Toolkit1 to train the
model with a feature cutoff of 1. In order to get
the best number of iteration, 9/10 of the training
data is used to train the model, and the other 1/10
portion of the training data is used to evaluate the
model. Figure 1 and 2 show the results of the eval-
uation on MSRA and UPUC corpus.
From the figures we can see the best iteration
number range from 555 to 575 for MSRA corpus,
and 360 to 375 for UPUC corpus. So we decide
the iteration for 560 rounds for MSRA tracks and
365 rounds for UPUC tracks, respectively.
2.3 Segmentation
As we mentioned in the beginning of this section,
the segmentation is the process to obtain the value
1Download from http://maxent.sourceforge.net
202
of every NC in a text fragment. This process is
similar to the training process. Firstly, We scan
the text fragment from start to end to get al of
the MSUs. Then we can extract all of the features
from the text fragment and decide which check
point we should tag as r = 0 by this equation:
p(r|c) =
1
Z
K?
j=1
?
fj(r|c)
j (1)
where K is the number of features, Z is the nor-
malization constant used to ensure that a probabil-
ity distribution results, and c represents the con-
text of the check point. ?j is the weight for fea-
ture fj , here {?1?2 . . . ?K} is generated by the
training data. We then compute P (r = 0|c) and
P (r = 1|c) by the equation (1).
After one check point is treated with value of
r, the system shifts backward to the next check
point until all of the check point in the whole text
fragment are treated. And by calculating:
P =
n?1?
i=1
p(ri|ci) =
n?1?
i=1
1
Z
K?
j=1
?fk(ri|ci)j (2)
to get an r sequence which can maximize P . From
this process we can see that the sequence is, in fact,
a second-order Markov Model. Thus it is easily to
think about more tags prior to the check point (as
an nth-order Markov Model) to get more accuracy,
but in this paper we only use the previous 2 tags
from the check point.
2.4 Identification of New words
We perform the new word(s) identification as a
post-process by check the word formation power
(WFP) of characters. The WFP of a character is
defined as: WFP (c) = Nwc/Nc, where Nwc is
the number of times that the character c appears
in a word of at least 2 characters in the training
corpus, Nc is the number of times the character c
occurs in the training corpus. After a text fragment
is segmented by our system, we extract all consec-
utive single characters. If at least 2 consecutive
characters have the WFP larger than our threshold
of 0.88, we polymerize them together as a word.
For example,/???0 is a new word which is
segmented as /?/?/?0 by our system, WFP
of these 3 characters is 0.9517,0.9818 and 1.0 re-
spectively, then they are polymerized as one word.
Besides the WFP, during the experiments, we
find that the Maximum Entropy model can poly-
merize someMSUs as a newword (We call it poly-
merization characteristic of the model), such as?
?? in the training corpus, we can extract ?
? as the previous context feature of the check
point after, in another stringS,?, we can
extract the backward context? of the check point
after with r = 1. Then in the test, a new word
??? is recognized by the model since ?
? and ? are polymerized if ? appears to-
gether a large number of times in the training cor-
pus.
3 Performance analysis
Here Table 1 illustrates the results of all 4 tracks
we participate. The first column is the track name,
and the 2nd column presents the Recall (R), the
3rd column the Precision (P), the 4th column is
F-measure (F). The Roov refers to the recall of the
out-of-vocabulary words and the Riv refers to the
recall of the words in training corpus.
Track R P F Roov Riv
MSRA Closed 0.923 0.929 0.926 0.554 0.936
MSRA Open 0.938 0.946 0.942 0.706 0.946
UPUC Closed 0.902 0.887 0.895 0.568 0.934
UPUC Open 0.926 0.906 0.917 0.660 0.954
Table 1: Results of our system in 4 tracks.
3.1 Closed tracks
For all of the closed tracks, we perform the seg-
mentation as we mentioned in the section above,
without any class defined. Every MSU we extract
from the training data is a character, which may be
a Chinese character, an English letter or a single
digit. We extract the features based on this kind of
MSUs to generate the models. The results show
these models are not precise.
For the UPUC closed track, the official released
training data is rather small. Then the capability
of the model is limited, this is the most reasonable
negative effect on our F-measure 0.895.
3.2 Open tracks
The primary change between open tracks and
closed tracks is that we have classified 5 classes
(?C?,?AB?,?EN?,?CN? and ?P?) to MSUs in or-
der to improve the accuracy of the model. The
classification really works and affects the perfor-
mance of the system in a great deal. As this text
fragment 1998c can be recognized as (EN)(C),
which can also presents 1644c, thus 1644c can
203
be easily recognized though there is no 1664c in
the training data.
The training corpus we used in UPUC open
track is the same as in UPUC closed track. With
those 5 classes, it is easily seen that the F-measure
increased by 2.2% in the open tracks.
For the MSRA open track, we adjust the class
?P? by removing the punctuation ?!? from the
class, because in the MSRA corpus, ?!? can be
a part of a organization name, such as ?!? in
/?l?!???u??
?0. Besides,
we add the Microsoft Research training data of
SIGHAN bakeoff 2005 as extended training cor-
pus. The larger training data cooperate with the
classification method, the F-measure of the open
track increased to 0.942 as comparison with 0.926
of closed track.
3.3 Discussion of the tracks
Through the tracks, we tested the performance by
using the pure Maximum Entropy model in closed
tracks and run with the improved model with clas-
sified MSUs in open tracks. It is shown that the
pure model without any additional methods can
hardly make us satisfied, for the open tracks, the
model with classes are just acceptable in segmen-
tation.
In both closed and open tracks, we use the
same new word identification process, and with
the polymerization characteristic of the model, we
find the Roov is better than we expected.
On the other hand, in our system, there is no dic-
tionary used as we described in the sections above,
the Riv of each track shows that affects the system
performance.
Another factor affects our system in the UPUC
tracks is the wrongly written characters. Consider
that our system is based on the sequence of char-
acters, this kind of mistake is fatal. For example,
in the sentence ???#u??Off<ff{?,
where{? is written as{?. The model cannot
recognize it since {? didn?t occur in the train-
ing corpus. In the step of new word identification,
the WFPs of the 2 characters {?? are 0.8917
and 0.8310, thus they are wrongly segmented into
2 single characters while they are treated as a word
in the gold standard corpus. Therefore, we believe
the results can increase if there are no such mis-
takes in the test data.
4 Conclusion
We propose an approach to Chinese word seg-
mentation by using Maximum Entropy model,
which focuses on the nexus relationship of any
2 adjacent MSUs in a text fragment. We tested
our system with pure Maximum Entropy models
and models with simplex classification method.
Compare with the pure models, the models with
classified MSUs show us better performances.
However, the Maximum Entropy models of our
system still need improvement if we want to
achieve higher performance. In future works,
we will consider using more training data and
add some hybrid methods with pre- and post-
processes to improve the system.
Acknowledgements
We would like to thank all the colleagues of our
Lab. Without their encouragement and help, this
work cannot be accomplished in time.
This is our first time to participate such an in-
ternational bakeoff. There are a lot of things we
haven?t experienced ever before, but with the en-
thusiastic help from the organizers, we can come
through the task. Especially, We wish to thank
Gina-Anne Levow for her patience and immediate
reply for any of our questions, and we also thank
Olivia Kwong for the advice of paper submission.
References
Nianwen Xue and Libin Shen. 2003. Chinese Word
Segmentation as LMR tagging. In Proceedings of
the Second SIGHAN Workshop on Chinese Lan-
guage Processing, p176-179.
Maosong Sun, Ming Xiao, B K Tsou. 2004. Chinese
Word Segmentation without Using Dictionary Based
on Unsupervised Learning Strategy. Chinese Jour-
nal of Computers, Vol.27, #6, p736-742.
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo.
2005. A Maximum Entropy Approach to Chinese
Word Segmentation. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, p161-164.
204
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 506?513,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Zhou qiaoli: A divide-and-conquer strategy for  
semantic dependency parsing 
 
 
Qiaoli Zhou Ling Zhang Fei Liu Dongfeng 
Cai 
Guiping 
Zhang 
Knowledge Engineering  
Research Center Shenyang Aerospace University 
No.37 Daoyi South Avenue 
Shenyang, Liaoning, China 
Zhou_qiao_li@
hotmail.com 
710138892@qq.
com 
fei_l2011@
163.com 
caidf@vip.16
3.com 
zgp@ge-
soft.com 
 
 
 
 
 
 
Abstract 
We describe our SemEval2012 shared Task 5 
system in this paper. The system includes 
three cascaded components: the tagging se-
mantic role phrase, the identification of se-
mantic role phrase, phrase and frame semantic 
dependency parsing. In this paper, semantic 
role phrase is tagged automatically based on 
rules, and takes Conditional Random Fields 
(CRFs) as the statistical identification model 
of semantic role phrase. A projective graph-
based parser is used as our semantic depend-
ency parser. Finally, we gain Labeled At-
tachment Score (LAS) of 61.84%, which 
ranked the first position. At present, we gain 
the LAS of 62.08%, which is 0.24% higher 
than that ranked the first position in the task 5. 
1 System Architecture  
To solve the problem of low accuracy of long dis-
tance dependency parsing, this paper proposes a 
divide-and-conquer strategy for semantic depend-
ency parsing. Firstly, Semantic Role (SR) phrase in 
a sentence are identified; next, SR phrase can be 
replaced by their head or SR of head. Therefore, 
the original sentence is divided into two kinds of 
parts, which can be parsed separately. The first 
kind is SR phrase parsing; the second kind is  
parsing the sentence in which the SR phrases are 
replaced by their head or SR of head. Finally, the 
paper takes graph-based parser as the semantic de-
pendency parser for all parts. They are described in 
Section 2 and Section 4. Their experimental results 
are shown in Section5. Section 6 gives our conclu-
sion and future work. 
2 SR Phrase Tagging and Frame  
To identify SR phrase, SR phrase of train corpus 
are tagged. SR phrase is tagged automatically 
based on rules in this paper. A phrase of the sen-
tence is called Semantic Role phrase (SR phrase) 
when the parent of only one word of this phrase is 
out of this phrase. The word with the parent out of 
the phrase is called Head of Phrase (HP). The 
shortest SR phrase is one word, while the longest 
SR phrase is a part of the sentence. In this paper, 
the new sequence in which phrases are replaced by 
their head or SR of head is defined as the frame. In 
this paper, firstly, SR phrases of the sentence are 
identified; secondly, the whole sentence is divided 
into SR phrases and frame; thirdly, SR phrase and 
frame semantic dependency are parsed; finally, the 
dependency parsing results of all components are 
combined into the dependency parsing result of the 
whole sentence. 
SR of HP is used as the type of this phrase. Only 
parts of types of SR phrases are tagged. In this pa-
per, the tagged SR phrases are divided into two 
506
types: Main Semantic Role (MSR) phrase and 
Preposition Semantic Role (PSR) phrase. 
2.1 MSR Phrase Tagging  
In this paper, MSR phrase includes: OfPart, agent, 
basis, concerning, content, contrast, cost, existent, 
experiencer, isa, partner, patient, possession, pos-
sessor, relevant, scope and whole. MSR phrase 
tagging rules are shown in figure1&2. 
  
Figure1: Tagging Rule of the Last Word of MSR Phrase 
Figure 1 shows the rule for identification of the 
last word of MSR phrase. If the SR of the current 
word is MSR and its POS is not VV, VE, VC or 
VA, it is the last word of phrase. 
As shown in the figure 2, the first word of 
phrase is found based on the last word of phrase. 
The child with the longest distance from the last 
word of phrase is used as the current word, and if 
the current word has no child, it is the first word of 
phrase; otherwise, the child of the current word is 
found recursively. If the first word of phrase POS 
is preposition and punctuation, and its parent is the 
last word, the word following the first word serves 
as the first word of phrase. 
 
Figure2: Tagging Rule of the First Word of MSR Phrase 
 
 
Figure3: Example of the Tagging MSR Phrase 
As shown in the figure 3, the first column is 
word ID and the seventh column is parent ID of 
word. SR of ID40 is content, so ID40 is the last 
word of phrase. Its children include ID39 and ID37, 
thus ID37 with the longest distance from ID40 is 
the current word. The child of ID37 is ID33, the 
child of ID33 is ID32, ID32 has no child, and ID32 
is the first word of SR phrase. 
The tagged result in the above figure 3 is as fol-
lows: ?/CC ?/VC ??/VV content[ ??/JJ ?
? /NN ? /CC ?? /NR ? /ETC ?? /NN ?
/DEG ??/NN ??/NN ]  
Input: wi: word index (ID) in a given sentence. 
           N: the number of words. 
          Mi: MSR list. 
          Vi: POS tags list 
Output: the last word ID of MSR phrase 
Function: Findmainsemanticword(wi): return word 
ID when wi of semantic belongs to Mi. 
Otherwise return 0. 
Function: FindPOSword(wi): return true when wi 
of POS tagging not belongs to Vi. Oth-
erwise return 0. 
Function Findlastword(wi) 
For i?1 to N do begin 
             If (Findmainsemanticword(wi)&& 
FindPOSword(wi)) 
               { 
                   return wi; 
} 
else { 
                          i++; 
} 
       end 
return 0; 
29  ?  ?  CC  CC  _  30  aux-depend  _  _ 
30  ?  ?  VC  VC  _  58 s-succession  _  _ 
31  ?? ??  VV  VV  _  54  s-succession _  _ 
32  ??  ??  JJ   JJ  _  33  d-attribute  _  _ 
33  ??  ??  NN  NN  _  37  s-coordinate  _  _ 
34  ?  ?  CC  CC  _  37  aux-depend  _  _ 
35  ??  ??  NR  NR  _  37  d-member  _  _ 
36  ?  ?  ETC  ETC  _  35  aux-depend  _  _ 
37  ??  ??  NN  NN  _  40  d-genetive  _  _ 
38  ?  ?  DEG  DEG  _  37  aux-depend  _  _ 
39  ??  ??  NN  NN  _  40  s-coordinate  _ _ 
40  ?? ??  NN  NN  _  31  content  _  _ 
Input: Lword: the last word ID of MSR phrase. 
Output: Fword: the first word ID of MSR phrase. 
Function: Findmaxlenchild (w): return child ID 
with the longest distance from w when w 
has child. Otherwise returns 0. 
Fuction: FindPOSword(w): return POS of w. 
Fuction:Findparent(w): return parent ID of w. 
Function Findfirstword(Lword) 
     If(Findmaxlenchild (Lword)= =0) 
      { 
         return Lword; 
} 
Else { 
Fword=Findmaxlenchildword(Lword); 
If(findPOSword(Fword)==P||  
findPOSword(Fword)= =PU) 
{ 
    If (findparent(Fword)= =Lword) 
        Return Fword +1; 
} 
Findfirstword(Fword); 
} 
507
After phrases are tagged, a new sequence gener-
ated by replacing the phrase with HP is called 
MSR frame. 
MSR frame: ?/CC ?/VC ??/VV ??/NN  
Example of sentences with nested phrases: 
?/P ??/JJ ??/NN ?/PU ??/NT exis-
tent[ ? /P ?? /NR ?? /NN ?? /VV con-
tent[ ??/NN ] ?/DEC ??/NN ???/NN ] 
?/AD ?/VE ?????/CD ?/M  
After phrases are tagged, a new sequence gener-
ated by replacing the phrase with HP is called 
MSR frame. 
MSR frame: ?/P ??/JJ ??/NN ?/PU ??
/NT???/NN ?/AD ?/VE ?????/CD ?
/M 
2.2 PSR Phrase Tagging  
In this paper, SR phrase containing preposition is 
defined as PSR phrase. If the POS tags of the cur-
rent word is Preposition (P), the first word and the 
last word of PSR phrase are found based on the 
current word. PSR phrase tagging rule as figure 4 
& 5. 
 
Figure 4: Tagging Rule of the First Word of PSR Phrase 
As shown in the figure 4, the child with the 
longest distance from the current word is the first 
word of phrase. If the prep has no child, then it is 
PSR phrase. 
As shown in the figure 5, firstly, the parent of 
the prep is found; next, the parent is taken as the 
current word, and the child with the longest dis-
tance from the current word is found recursively. If 
no child is found, the current word is the last word 
of PSR phrase. If preposition of SR is root or par-
ent of preposition is root, and proposition is PSR. 
If ID of preposition is larger than ID of parent of 
preposition, and preposition is PSR. 
 
Figure5: Tagging Rule of the Last Word of PSR Phrase 
 
 
Figure6: Example of the Tagging PSR Phrase 
As shown in the figure6, ID4 is prep, and it has 
no child, so the first word is ID4. The parent of 
Input: Pword: the word ID that word POS tags is P. 
Output: Fword: the first word ID of PSR phrase. 
Function: Findmaxlenchildword(w): return word ID 
with the longest distance from w when w 
has child. Otherwise returns 0. 
Function Findfirstword(Pword) 
        If(Findmaxlenchildword(Pword)= =0) 
          { 
             return Pword; 
} 
Else { 
return Fwrod= 
 Findmaxlenchildword(Pword); 
} 
Input: Pword: the word ID that word POS tags is P. 
Output: Lword: the last word ID of PSR phrase. 
Function: Findmaxchild (w): return word ID that 
length is max with w when w has child. 
Otherwise return 0. 
Function: Findparent (w): return word ID when w of 
parent is not root. Otherwise return 0.  
Function: Findroot(w): return 1 when w of semantic 
role is root. Other wise return 0. 
Function Findlastword(Pword) 
Var cword: parent ID 
     If(Findparentsword(Pword)= =0|| 
 findroot(Pword)= =1)  { 
             return Pword; 
} 
else { cword=Findparent (Pword) ) 
 If(Pword>cword){ 
return Pword; 
} 
else { 
                   if(Findmaxchild (cword)= =0) { 
                               return cword; 
} 
else{  
Lword= 
Findmaxchild (cword); 
Findlastword(Lword); 
} 
                           } 
}
1  ??  ??  NN  NN  _  2  j-agent  _  _ 
2  ??  ??  NN  NN  _  3  r-patient  _  _ 
3  ??  ??  NN  NN  _  11  agent  _  _ 
4  ?  ?  P P  _ 5  prep-depend  _ first word 
5  ??  ??  VV  VV  _  11 duration _ head_ 
6  ?? ?? NR  NR _ 8  d-genetive  _ _ 
7  ?? ?? NN  NN _  8 r-patient _ _ 
8  ??  ?? NN  NN _ 9 d-host _  _ 
9  ??  ?? NN  NN _ 5 patient  _  _ 
10  ?  ? LC  LC  _ 5  aux-depend _ last word_ 
11  ??  ?? VV VV  _  0  ROOT _  _ 
12  ?  ?  AS  AS  _ 11 aspect  _  _ 
13 ??  ?? JJ  JJ  _ 14 d-attribute  _  _ 
14  ?? ??  NN NN  _  11 content  _  _ 
15  ?  ?  PU  PU  _ 11  PU  _  _ 
508
ID4 is ID5, the child with the longest distance from 
ID5 is ID10, and ID10 with no child is the last 
word of phrase. 
The tagged result in the above figure 6 is as fol-
lows: ??/NN ??/NN ??/NN duration[?/P 
??/VV ??/NR ??/NN ??/NN ?/LC] ?
?/VV ?/AS ??/JJ ??/NN ?/PU 
The position of HP in PSR phrase is not fixed. 
After phrases are tagged, a new sequence gener-
ated by replacing the phrase with SR of HP is 
called PSR frame. 
PSR frame: ??/NN ??/NN ??/NN dura-
tion/duration ?? /VV ? /AS ?? /JJ ??
/NN ?/PU 
Examples of sentences with nested phrases: 
s-cause[ ??/P ??/NR s-purpose[ ?/P ?
?/VV ???/NT ]  ?/MSP ??/VV ??/VV 
?/DT ?/M ??/NN ??/NN ],/PU ??/AD 
?? /NN ?? /NN ? /VV ? /VV ????
/VV ?/PU 
PSR frame: s-cause/s-cause ,/PU ??/AD ??
/NN ??/NN ?/VV ?/VV ????/VV ?/PU 
2.3 SR Phrase Tagging Performance 
If the parent of only one word of the tagged phrase 
is out of this phrase, this phrase is tagged correctly. 
If each word in the generated frame has one parent 
(i.e. words out of the phrase are dependent on HP 
instead of other words of the phrase), the frame is 
correct. 
 Phrase Frame 
MSR 99.99% 100% 
PSR 99.98% 99.70% 
Table 1. Tagging Performance (P-score) 
 
As shown in the table 1, tagging results were of 
very high accuracy. The wrong results were not 
contained in phrase and frame train corpus of de-
pendency parsing. 
3 SR Phrase Identification  
In this paper, we divide SR phrase into two classes: 
Max SR phrase and Base SR phrase. Max SR 
phrase refers to SR phrase is not included in any 
other SR phrase in a sentence. Base SR phrase re-
fers to SR phrase does not include any other SR 
phrase in a SR phrase. Therefore, MSR phrase is 
divided into two classes: Max MSR (MMSR) 
phrase and Base MSR (BMSR) phrase. PSR phrase 
was divided into two classes: Max PSR (MPSR) 
phrase and Base PSR (BPSR) phrase. 
3.1 MMSR Phrase Identification based on 
Cascaded Conditional Random Fields 
Reference (Qiaoli Zhou, 2010) is selected as our 
approach of MMSR phrase identification. The 
MMSR identifying process is conceptually very 
simple. The MMSR identification first performs 
identifying BMSR phrase, and converts the identi-
fied phrase to head. It then performs identifying for 
the updated sequence and converts the newly rec-
ognized phrases into head. The identification re-
peats this process until the whole sequence has no 
phrase, and the top-level phrase are the MMSR 
phrases. A common approach to the phrase identi-
fication problem is to convert the problem into a 
sequence tagging task by using the ?BIEO? (B for 
beginning, I for inside, E for ending, and O for 
outside) representation. If the phrase has one word, 
the tag is E. This representation enables us to use 
the linear chain CRF model to perform identifying, 
since the task is simply assigning appropriate la-
bels to sequence. 
There are two differences between our feature 
set and Qiaoli (2010)?s: 
1) We use dependency direction of word as iden-
tification feature, while Qiaoli (2010) did not 
use. 
2) We do not use scoring algorithm which is used 
by Qiaoli (2010). 
Direction Unigrams D-3,D-2 ,D-1 , D0 , D+1 ,D+2 ,D+3
Direction Bigrams D-2D-1, D-1D0, D0D+1, D+1D+2,  
Word & Direction W0D0
Table 2. Feature Templates of MMSR Phrase 
 
Table 2 is additional new feature templates 
based on Qiaoli (2010). W represents a word, and 
D represents dependency direction of the word. 
With this approach, nested MSR phrases are identi-
fied, and the top-level MSR phrase is the MMSR 
that we obtained. 
corpus P R F 
dev 81.41% 75.40% 78.29% 
test 81.23% 73.04% 76.92% 
Table 3.  MMSR Identification Performance 
509
3.2 BMSR Phrase Identification based on 
CRFs  
We use the tag set ?BIEO? the same as that used 
for MMSR identification. 
Word Unigrams W-3, W-2, W-1, W0, W+1, W+2, W+3
Word Bigrams 
W-3W-2, W-2W-1, W-1W0, W0W+1, 
W+1W+2, W+2W+3
POS Unigrams P-3 , P-2, P-1, P0, P+1, P+2, P+3
POS Bigrams 
P-3P-2, P-2P-1, P-1P0, P0P+1,  
P+1P+2, P+2P+3
Word_X X0
Word_Y Y0
Word_D D0
Word_S S-3, S-2 , S-1 , S0, S+1, S+2, S+3
Word & POS W-1P-1, W0P0, W+1P+1
Word & Word_X W-3X0
Word & Word_D 
W0D0, W-3W-2D0, W-2W-1D0,  
W-1W0D0, W0W+1D0, W+1W+2D0, 
W+2W+3D0
Word & Word_S W-1S-1, W0S0, W+1S+1, W+2S+2
Word_X & Word_Y X0Y0
POS & Word_D 
P0D0, P-3P-2D0, P-2P-1D0, P-1P0D0, 
P0P+1D0, P+1P+2D0, P+2P+3D0
POS & Word_S 
P-1S-1, P-2S-2, P-3S-3, P0S0, 
 P+1S+1, P+2S+2, P+3S+3
Word_D & Word_S 
D-1S-1, D-2S-2, D-3S-3, D0S0, 
 D+1S+1, D+2S+2, D+3S+3
Word & POS & 
Word_D 
W-1P-1D0, W0P0D0, W+1P+1D0
Word & POS & 
Word_D & Word_S 
W-3P-3D-3S-3, W-2P-2D-2S-2,  
W-1P-1D-1S-1, W0P0D0S0, W1P1D1S1, 
W2P2D2S2, W3P3D3S3
Table 4. Feature Templates of BMSR Phrase 
 
In table 4, ?W? represents a word, ?P? repre-
sents the part-of-speech of the word, ?X? repre-
sents the fourth word following the current word, 
?Y? represents the fifth word following the current 
word, ?D? represents the dependency direction of 
the current word, and ?S? represents the paired 
punctuation feature. ?S? consists of ?RLIO? (R for 
the right punctuation, L for the left punctuation, I 
for the part between the paired punctuation and O 
for outside). 
 
corpus P R F 
dev 79.32% 80.65% 79.98% 
test 79.22% 79.96% 79.59% 
Table 5.  BMSR Identification Performance (F-score) 
3.3 MPSR Phrase Identification Based on 
Collection  
Reference (Dongfeng, 2011) is selected as our ap-
proach of MPSR phrase identification. The posi-
tion of HP in PSR phrase is not fixed. Not only 
PSR phrase is identified, but also PSR phrase type 
is identified.  
There are two major differences between our 
feature set and Dongfeng (2011)?s: 
1) We take the PSR phrase type (the SR of HP) 
as tag.  
2)  We use ?S-type? represents that the PSR 
phrase is the single preposition. ?Type? represents 
SR of the preposition. 
For example: ???/NN location [?/P ??
/NR ??/NR] ??/VV 
O|W POS
Dongfeng 
(2011) Tag 
Our Tag 
*|??? NN O O 
*|? P O O 
?|?? NR I I 
?|?? NR E Location-E
?|?? VV N N 
Table 6. Example of PSR Phrase Tag Set  
 
In table 6, Dongfeng(2011) takes ?E? as the tag 
of last word of PSR phrase, but we take ?Location-
E? as the tag of last word of PSR phrase  (Location 
is type of  PSR phrase). 
With this approach, nested PSR phrases are 
identified, and the top-level PSR phrase is the 
MPSR that we obtained. 
corpus MPSR phrase MPSR phrase & type
dev 84.00% 54.23% 
test 83.78% 51.60% 
Table 7. MPSR Identification Performance (F-score) 
3.4 Combined Identification of MSR Phrase 
and PSR Phrase 
Identification process: MSR phrase and PSR 
phrase are respectively identified in one sentence, 
and the results are combined in accordance with 
this rule: if phrases are nested, only the top-level 
phrase is tagged; if phrases are same, only the PSR 
510
phrase is tagged; if phrases are overlapped, only 
PSR phrase is tagged. 
There are two combinations in this paper:  
1) MMSR phrase and MPSR phrase combined 
result is defined as MMMP phrase. For exam-
ple as follow (?[ ]?represents MMSR, 
?{}?represents MPSR): 
Example A: [ ??/NN ] ?/VC [ ??/VV ?
?/NR ?/DEC ?/CD ?/M ??/JJ ??/NN ?
?/NN ] ?/PU ??/DT ?/M ?/VE [ ??/CD 
?/M ??/NN ??/NN ?/PU ???/CD ?/M 
??/NN ??/NN ] ??/VV location{ ?/P ?
/DT ?/M ??/NN ?/LC } ?/PU  
MMMP  frame: [ ??/NN ] ?/VC ??/NN ?
/PU ??/DT ?/M ?/VE ??/NN ??/VV 
location/location ?/PU 
2) BMSR phrase and MPSR phrase combined 
result is defined as BMMP phrase. 
Example B: [ ??/NN ] ?/VC ??/VV [ ??
/NR ] ?/DEC ?/CD ?/M ??/JJ ??/NN ?
?/NN ?/PU ??/DT ?/M ?/VE [ ??/CD ?
/M ??/NN ??/NN ?/PU ???/CD ?/M ?
?/NN ??/NN ] ??/VV location{ ?/P ?/DT 
?/M ??/NN ?/LC } ?/PU 
BMMP  frame: ??/NN ?/VC ??/VV ??
/NR ?/DEC ?/CD ?/M ??/JJ ??/NN ??
/NN ?/PU ??/DT ?/M ?/VE ??/NN ??
/VV location/location ?/PU 
corpus phrase P R F 
BMMP 79.48% 81.60% 80.53%
dev 
MMMP 80.00% 76.79% 78.36%
BMMP 80.14% 82.48% 81.30%
test 
MMMP 80.19% 78.53% 79.35%
Table 8.  Combination Phrase Identification 
Performance 
3.5 Phrase and Frame Length Distribution   
We count phrases, frame and Original Sentence 
(OS) length distribution in training set and dev set. 
 BMMP MMMP MMSR BMSR OS 
[0,5) 80.07% 71.36% 75.36% 85.74% 9.07%
[5,10) 16.15% 21.63% 18.93% 12.33% 8.30%
[10,20) 3.35% 6.13% 5.05% 1.80% 17.23%
20? 0.43% 0.88% 0.66% 0.13% 65.40%
Table 9.  Length Distribution of Phrases and OS 
 
Table 9 shows, about 95% of phrases have less 
than 10 words, but about 65% of OS has more than 
20 words. 
 BMMP MMMP MMSR BMSR OS 
[0,5) 16.00% 18.70% 16.43% 14.36% 9.07%
[5,10) 18.87% 24.91% 19.41% 14.11% 8.30%
[10,20) 34.26% 35.42% 33.94% 30.68% 17.23%
20? 30.87% 20.97% 30.22% 40.85% 65.40%
Table 10.  Length Distribution of Frames and OS 
 
Table 10 shows, about 70% of frames have less 
than 20 words, especially 80% of MMMP frame 
has less than 20 words, but about 65% of OS has 
more than 20 words. 
 BMMP MMMP BMSR MMSR OS 
phrase 3.07 3.83 2.53 3.44 30.07
frame 16.00 13.21 19.16 15.79 30.07
Table 11. Average Length 
 
We count phrases, frame and Original Sentence 
(OS) Average Length (AL) in training set and dev 
set. Table 11 shows phrase of AL accounted for 
10% of OS of AL, and frame of AL accounted for 
50% of OS of AL. The AL shows that the semantic 
dependency paring unit length of OS is greatly re-
duced after dividing an original sentence into SR 
phrases and frame.  
As shown in tables 9, 10 and 11, the length dis-
tribution indicates that the divide-and-conquer 
strategy reduces the complexity of sentences sig-
nificantly. 
4 Semantic Dependency Parsing  
Graph-based parser is selected as our basic seman-
tic dependency parser. It views the semantic de-
pendency parsing as problem of finding maximum 
spanning trees (McDonald, 2006) in directed 
graphs. In this paper, phrase and frame semantic 
dependency parsing result was obtained by Graph-
based parser. Training set of phrase comes from 
phrases, and training set of frame comes from 
frames. 
5 Experiments  
5.1 Direction of Identification  
511
Dependency direction serves as feature of SR 
phrase identification, so we need to identify de-
pendency direction of word. We use tag set is {B, 
F}, B represents backward dependence, F repre-
sents forward dependence. The root?s dependency 
direction in sentence is B. Dependency direction 
identification p-score has reached 94.87%. 
Word Unigrams W-4, W-3, W-2, W-1, W0, W+1,  
W+ 2, W+ 3, W+ 4
Word Bigrams W-3W-2, W-2W-1, W-1W0, W0W+1, 
W+1W+2, W+2W+3
Word Trigrams W-1W 0W+1
Word Four-grams W-2W-1W0 W +1, W0W+1W+2W+3
Word Five-grams W- 4W-3W-2W-1W0,  
W0W+1W+2W+3W+ 4
POS Unigrams P-4, P-3, P-2, P-1, P0, P+1, P+2, P+3, P+ 4
POS Bigrams P-3P-2, P-2P-1, P-1P0, P0P+1, 
 P+1P+2, P +2P+3
POS Trigrams P-1P0P+1
POS Four-grams P-2P-1P0P+1, P0P+1P+2P+3
POS Five-grams P-4P-3P-2P-1P0, P0P+1P+2P+3P+4
Word & POS W-2 P-2, W-1P-1, W0P0, W+1P+1, 
W+2P+2
Table 12.  Feature Templates of Dependency Direction 
In table12, w represents word, p represents POS. 
5.2 System and Model  
For a sentence for which phrases has been identi-
fied, if phrases can be identified, then the whole 
sentence semantic dependency parsing result is 
obtained by phrase parsing model and frame pars-
ing model. Therefore, in this paper, the sentence is 
divided into the following types based on the 
phrase identification results: (1) SentMMMP indi-
cates MMSR phrase and MPSR phrase identified 
in a sentence; (2) SentBMMP indicates BMSR 
phrase and MPSR phrase identified in a sentence; 
(3) SentMMSR indicates only MMSR phrase iden-
tified in a sentence; (4) SentMPSR indicates only 
MPSR phrase identified in a sentence; (5) 
SentBMSR indicates only BMSR phrase identified 
in a sentence; (6) SentNone indicates no phrase 
identified in a sentence. 
Sentence type Phrase parsing Model 
Frame parsing
Model 
SentMMMP MMMP phrase MMMP frame
SentBMMP BMMP phrase BMMP frame
SentMMSR MMSR phrase MMSR frame
SentMPSR MPSR phrase MPSR frame 
SentBMSR BMSR phrase BMSR frame
SentNone Sentence model 
Table 13.  Type of Sentence and Parsing Model 
Table 13 shows types of sentence, and parsing 
models for every type of sentence. For example, 
parsing SentMMMP needs MMMP phrase parsing 
model and MMMP frame paring model 
The corpus contains the sentence type deter-
mined by the phrase identification strategy. 
Strategy of phrase 
identification Sentence type in the corpus
Strategy MMMP SentMMMP, SentMMSR, SentMPSR, SentNone 
Strategy BMMP SentBMMP, SentMPSR, SentBMSR, SentNone 
Strategy BMSR SentBMSR, SentNone 
Table 14.  Sentence Types in the Corpus 
 
As shown in table 14, Strategy MMMP indicates 
that MMMP phrase in the corpus was identified, 
and sentences in the corpus were divided into 
SentMMMP, SentMMSR, SentMPSR and Sent-
None. Strategy BMMP indicates that BMMP 
phrase in the corpus was identified, and sentences 
in the corpus were divided into SentBMMP, 
SentBMSR, SentMPSR and SentNone. Strategy 
BMSR indicates that BMSR phrase in the corpus 
was identified, and sentences in the corpus were 
divided into SentBMSR and SentNone. 
5.3 Comparative Experiments  
In this paper, we carry out comparative experi-
ments of parsing for the test set by 3 systems. 
1) System1 represents strategy MMMP in the 
table 14. 
2) System2 represents strategy BMMP in the ta-
ble 14. 
3) System3 represents strategy BMSR in the table 
14. 
 Dev Test 
G-parser 62.31% 61.68% 
System1(MMMP) 61.98% 61.84% 
System2(BMMP) 62.7% 62.08% 
System3(BMSR) 62.22% 61.15% 
Table 15.  Comparative Experiments 
 
As shown in the table 15, system2 result is more 
accurate than system1, because BMMP phrase 
identification is more accurate than MMMP as 
shown in the table 8. Although, BMSR phrase 
identification is more accurate than MMMP phrase 
as shown in the table 5 & 8, system 3 result is less 
accurate than systm1. Compared with BMSR iden-
512
tification, MMMP identification reduces the com-
plexity of sentences significantly, because the table 
11 shows that the AL of MMMP frame is about 
30% less than that of BMSR frame. G-parser is 
graph-based parser (Wangxiang Che, 2008). 
6 Conclusion and Future Work  
To solve the problem of low accuracy of long dis-
tance dependency parsing, this paper proposes a 
divide-and-conquer strategy for semantic depend-
ency parsing. We present our SemEval2012 shared 
Task 5 system which is composed of three cas-
caded components: the tagging of SR phrase, the 
identification of Semantic-role- phrase and seman-
tic dependency parsing.  
Divide-and-conquer strategy is influenced by 
two factors: one is identifying the type of phrase 
will greatly reduce the sentence complexity; the 
other is phrase identifying precision results in cas-
caded errors. The topic of this evaluation is seman-
tic dependency parsing, and word and POS contain 
less semantic information. If we can make seman-
tic label on words, then it will be more helpful for 
semantic dependency parsing. In the future, we 
will study how to solve the long distance depend-
ency parsing problem. 
Acknowledgments 
The authors would like to thank the reviewers for 
their helpful comments. This work was supported 
by National Natural Science Foundation of China 
(NSFC) via grant 61073123 and Natural Science 
Foundation of Liaoning province via grant 
20102174. 
References  
Dongfeng Cai, Ling Zhang, Qiaoli Zhou and Yue Zhao. 
A Collocation Based Approach for Prepositional 
Phrase Identification. IEEE NLPKE, 2011. 
McDonald, Ryan. 2006. Discriminative Learning and 
Spanning Tree Algorithms for Dependency Parsing. 
Ph.D. thesis, University of Pennsylvania. 
Guiping Zhang, Wenjing Lang, Qiaoli Zhou and Dong-
feng Cai. 2010. Identification of Maximal-Length 
Noun Phrases Based on Maximal-Length Preposition 
Phrases in Chinese, 2010 International Conference 
on Asian Language Processing, pages 65-68. 
Qiaoli Zhou, Wenjing Lang, Yingying Wang, Yan 
Wang, Dongfeng Cai. 2010.  The SAU Report for the 
1st CIPS-SIGHAN-ParsEval-2010, Proceedings of 
the First CIPS-SIGHAN Joint Conference on Chi-
nese Language Processing, pp:304-311. 
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang 
Li,Bing Qin, Ting Liu, and Sheng Li. 2008. A cas-
caded syntactic and semantic dependency parsing 
system. In CoNLL-2008. 
513
Bigram HMM with Context Distribution Clustering for Unsupervised
Chinese Part-of-Speech tagging
Lidan Zhang
Department of Computer Science
the University of Hong Kong
Hong Kong
lzhang@cs.hku.hk
Kwok-Ping Chan
Department of Computer Science
the University of Hong Kong
Hong Kong
kpchan@cs.hku.hk
Abstract
This paper presents an unsupervised
Chinese Part-of-Speech (POS) tagging
model based on the first-order HMM.
Unlike the conventional HMM, the num-
ber of hidden states is not fixed and will
be increased to fit the training data. In
favor of sparse distribution, the Dirich-
let priors are introduced with variational
inference method. To reduce the emis-
sion variables, words are represented by
their contexts and clustered based on the
distributional similarities between con-
texts. Experiment results show the out-
put state sequence of HMM are highly
correlated to the latent annotations of
gold POS tags, in context of clustering
similarity measures. The other exper-
iments on a real application, unsuper-
vised dependency parsing, reveal that the
output sequence can replace the manu-
ally annotated tags without loss of accu-
racies.
1 Introduction
Recently latent variable model has shown great
potential in recovering the underlying structures.
For example, the task of POS tagging is to re-
cover the appropriate sequence structure given
the input word sequence (Goldwater and Grif-
fiths, 2007). One of the most popular exam-
ple of latent models is Hidden Markov Model
(HMM), which has been extensively studied for
many years (Rabiner, 1989). The key problem
of HMM is how to find an optimal hidden state
number and the topology appropriately.
In most cases, the topology of HMM is pre-
defined by exploiting the domain or empirical
knowledge. This topology will be fixed during
the whole process. Therefore how to select the
optimal topology for a certain application or a set
of training data is still a problem, because many
researches show that varying the size of the state
space greatly affects the performance of HMM.
Generally there are two ways to adjust the state
number: top-down and bottom-up methods. In
the bottom-up methods (Brand, 1999), the state
number is initialized with a relatively large num-
ber. During the training, the states are merged or
trimmed and ended with a small set of states. On
the other hand, the top-down methods (Siddiqi et
al., 2007) start from a small state set and split one
or some states until no further improvement can
be obtained. The bottom-up approaches require
huge computational cost in deciding the states to
be merged, which makes it impractical for appli-
cations with large state space. In this paper, we
focus on the latter approaches.
Another problem in HMM is that EM algo-
rithm might yield local maximum value. John-
son (2007) points out that training HMM with
EM gives poor results because it leads to a fairly
flat distribution of hidden states when the empiri-
cal distribution is highly skewed. A multinomial
prior, which favors sparse distribution, is a good
choice for natural language tasks. In this paper,
we proposed a new procedure for inferring the
HMM topology and estimating its parameters si-
multaneously. Gibbs sampling has been used in
infinite HMM (iHMM) (Beal et al, 2001; Fox et
al., 2008; Van Gael et al, 2008) for inference.
Unfortunately Gibbs sampling is slow and diffi-
cult to be converged. In this paper, we proposed
the variational Bayesian inference for the adap-
tive HMMmodel with Dirichlet prior. It involves
a modification to the Baum-Welch algorithm. In
each iteration, we replaced only one hidden state
with two new states until convergence.
To reduce the number of observation vari-
ables, the words are pre-clustered and repre-
sented by the exemplar within the same clus-
ter. It is a one-to-many clustering, because the
same word play different roles under different
contexts. We evaluate the similarity between the
distribution of contexts, with the assumption that
the context distribution implies syntactic pattern
of the given word (Zelling, 1968; Weeds and
Weir, 2003). With this clustering, more contex-
tual information can be considered without in-
creasing the model complexity. A relatively sim-
ple model is important for unsupervised task in
terms of computational burden and data sparse-
ness. This is the reason why we do not increase
the order of HMM(Kaji and Kitsuregawa, 2008;
Headden et al, 2008).
With unsupervised algorithms, there are two
aspects to be evaluated (Van Gael et al, 2009).
Fist one is how good the outcome clusters are.
We compare the HMM results with the manu-
ally POS tags and report the similarity measures
based on information theory. On the other hand,
we test how good the outputs act as an interme-
diate results. In many natural language tasks, the
inputs are word class, not the actual lexical item,
for reason of sparsity. In this paper, we choose
the unsupervised dependency parsing as the ap-
plication to investigate whether our clusters can
replace the manual labeled tags or not.
The paper is organized as below: in section 2,
we describe the definition of HMM and its vari-
ance inference. We present our dynamic HMM
in section 3. To overcome the context limitation
in the first-order HMM, we present our distribu-
tional similarity clustering in section 4. In sec-
tion 5, we reported the results of the mentioned
experiments while section 6 concludes the paper.
2 Terminology
The task of POS tagging is to assign a syntac-
tic category sequence to the input words. Let
S be defined as the set of all possible hidden
states, which are expected to be highly correlated
to POS tags. ? represents the set of all words.
Therefore the task is to find a sequence of tag
sequence S = s1...sn ? S given a sequence of
words (i.e. a sentence, W = w1...wn ? ?). The
optimal tags is to maximize the conditional prob-
ability p(S |W), which is equal to:
max
S
p(S |W) = max
S
p(S )p(W |S )
= max
S
p(W, S )
(1)
In this paper,we consider the first-order HMM,
where the POS tags are regarded as hidden states
and words as observed variables. According to
the Markov assumption, the best sequence of
tags S for a given sequence of words W is done
by maximizing (with s0 = 0) the joint probabil-
ity:
p(W, S ) =
n
?
i=1
p(si|si?1)p(wi|si) (2)
where w0 is the special boundary marker of sen-
tences.
2.1 Variational Inference for HMM
Let the HMM be modeled with parameter ? =
(A, B, pi), where A = {ai j} = {P(st = j|st?1 = i)}
is the transition matrix governing the dynamic of
the HMM. B = {bt(i)} = {P(wt = i|st}) is the state
emission matrix and pi = {pii} = {P(s1 = i)} as-
signs the initial probabilities to all hidden states.
In favor of sparse distributions, a natural choice
is to encode Dirichlet prior into parameters p(?).
In particular, we have:
p(A) =
N
?
i=1
Dir({ai1, ..., aiN} |u(A))
p(B) =
N
?
i=1
Dir({bi1, ..., biN} |u(B))
p(pi) = Dir({pi1, ..., piN} |u(pi))
(3)
where the Dirichlet distribution of order N with
hyperparameter vector u is defined as:
Dir(x|u) =
?(
?N
i=1 ui)
?N
i=1 ?(ui)
N
?
i=1
xui?1i . (4)
In this paper, we consider the symmetric
Dirichlet distribution with a fixed length, i.e.
u = [
?N
i=1 ui/N, ...,
?N
i=1 ui/N].
In the Bayesian framework, the model param-
eters are also regarded as hidden variables. The
marginal likelihood can be calculated by sum-
ming up all hidden variables. According to the
Jensen?s inequality, the lower bound of marginal
likelihood is defined as:
ln p(W) = ln
?
?
S
p(?)p(W, S |?)d?
?
?
?
S
q(?, S ) ln
p(W, S , ?)
q(?, S )
d?
= F
(5)
Generally, Variational Bayesian Inference
aims to find a tractable distribution q(?, s) that
maximizes the lower bound F . To make infer-
ence flexible, the posterior distribution can be
assumed to be factorized according to the mean-
field assumption. We have:
p(W, S , ?) ? q(S , ?) = q
?
(?)qS (S ) (6)
Then an extension of EM algorithm (called
Baum-Welch algorithm) can be used to alter-
nately optimize the qS and q?. The EM process
is described as follows:
? E Step: Forward-Backward algorithm to
find the optimal state sequence S (t+1) =
argmax p(S (t)|W, ?(t))
? M Step: The parameters ?(t+1) are re-
estimated given the optimal state S (t+1)
The E and M steps are repeated until a conver-
gence criteria is satisfied. Beal (2003) proved
that only need to do minor modifications in M
step (in 1) is needed, when Dirichlet prior is in-
troduced.
3 Adaptive Hidden Markov Model
As aforementioned, the key problem of HMM is
how to initialize the number of hidden states and
select the topology of HMM. In this paper, we
use the top-down scheme: starting from a small
number of states, only one state is chosen in each
step and splitted into two new states. This binary
split scheme is described in Figure 1.
Algorithm 1 Outline of our adpative HMM
Initialization: Initialize: t = 0, N(t)
repeat
Optimization: Find the optimal parameters
for current Nt
Candidate Generation: Split states and
generate candidate HMMs
Candidate Selection: Select the optimal
HMM from the candidates, whose hidden
state number is Nt+1
untilNo further improvement can be achieved
after splitting
In the following, we will discuss the details of
each step one by one.
3.1 Candidate Generation
Let N(t) represent the number of hidden states at
timestep t. The problem is how to choose the
states for splitting. A straightforward way is to
select all states and generate N(t) + 1 candidate
HMMs, including the original un-splitted one.
Obviously the exhaustive search is inefficient es-
pecially for large state space. To make the algo-
rithm more efficient, some constraints must be
set to narrow the search space.
Intuitively entropy implies uncertainty. So
hidden states with large conditional entropies are
desirable to be splitted. We can define the con-
ditional entropy of the state sequences given ob-
servation W as:
H(S |W) = ?
?
S
[P(S |W) log P(S |W)] (8)
Our assumption is the state to be splitted must
be the states sequence with the highest condi-
tional entropy value. This entropy can be recur-
sively calculated with complexity O(N2T ) (Her-
nando et al, 2005). Here N is the number of
A(t+1) = {a(t+1)i j } = exp[?(?
(A)
i j ) ? ?(
N
?
j=1
?
(A)
i j )] ; ?
(A)
i j = u
(A)
j + Eq(s)[ni j]
B(t+1) = {b(t+1)ik } = exp[?(?
(B)
ik ) ? ?(
T
?
k=1
?
(B)
ik )] ; ?
(B)
ik = u
(B)
k + Eq(s)[n
?
ik]
pi
(t+1)
= {pi(t+1)i } = exp[?(?
(pi)
i ) ? ?(
N
?
i=1
?
(pi)
j )]; ?
(pi)
i = u
(pi)
i + Eq(s)[n
??
i ]
(7)
Figure 1: Parameters update equations in M-step. Here E is the expectation with respect to the
model parameters. And ni j is the expected number of transition from state si to state s j; n?ik is the
expected number of times word wk occurs with state si; n??i is the occurrence of s0 = i
states and T is the length of sequence. Using
this entropy constraint, the size of candidate state
set is always smaller than the minimal value be-
tween N and T .
3.2 Candidate Selection
Given the above candidate set, the parameters of
each HMM are to be updated. Note that we just
update the parameters related to the split state,
whilst keep the others fixed. Suppose the i-th
hidden state is replaced by two new states. First
the transition matrix is enlarged from N(t) ? N(t)
dimension to (N(t) + 1) ? (N(t) + 1) dimension,
by inserting one column and row after the i-th
column and row. In the process of update, we
only change the items in the two (i and i + 1)
rows and columns. The other elements irrelevant
to the split state are not involved in the update
procedure. Similarly EM algorithm is used to
find the optimal parameters. Note that most of
the calculations can be skipped by making use
of the forward and backward probability matrix
achieved in the previous step. Therefore the con-
vergence is fast.
Given the candidate selection, we can use a
modified Baum-Welch algorithm to find optimal
states and parameters. Here we use the algorithm
in (Siddiqi et al, 2007) with some modifications
for the Dirichlet prior. In particular, in E step,
we follow their partial Forward-Background al-
gorithm to calculate E[ni j] and E[n?ik], if si or s j
is candidate state to be splitted. Then in M-step,
only rows and columns related to the candidate
state are updated according to equation (7). The
detailed description is given as appendix.
Finally it is natural to use variational bound
of marginal likelihood in equation (5) for model
scoring and convergence criterion.
4 Distributional Clustering
To reduce the number of observation variables,
the words are clustered before HMM training.
Intuitively, the words share the similar contexts
have similar syntactic property. The categories
of many words are varied in different contexts.
In other words, the cluster of a given word is
heavily dependent on the context it appears. For
example,?? can be a noun (meaning: discov-
ery) if it acts as an object, or a verb (meaning: to
discover) if it is followed with a noun. Further-
more the introduction of context can overcome
the limited context in the first-order HMM.
The underlying hypothesis of clustering based
on distributional similarity is that the words oc-
curring in similar contexts behave as similar syn-
tactic roles. In this work, the context of a word
is a trigram consist of the word immediately pre-
ceding the target and the word immediately fol-
lowing it. The similarity between two words
is measured by Pointwise Mutual Information
(PMI) between the context pair in which they ap-
pear:
PMI(wi,w j) = log
P(ci, c j)
P(ci)P(c j)
(9)
where ci denotes the context of wi. P(ci, c j) is
the co-occurrence probability of ci and c j, and
P(ci) =
?
j P(ci, c j) is the occurrence probabil-
ity of ci. In our experiments, the cutoff context
count is set to 10, which means the frequency
less than the threshold is labeled as the unknown
context.
The above distributional similarity can be
used as a distance measure. Hence any cluster-
ing algorithm can be adopted. In this paper, we
use the affinity propagation algorithm (Frey and
Dueck, 2007). Its parameter ?dampfact? is set
to 0.9, and the other parameters are set as de-
fault. After running the clustering algorithm, the
contexts are clustered into 1869 clusterings. It
is noted that one word might be classified into
several clusters , if its contexts are clustered into
several clusters.
5 Experiments
As aforementioned, the outputs of our HMM
model are evaluated in two ways, clustering met-
ric and parsing performance. The data used in all
experiments are the Chinese data set in CoNLL-
2007 shared task. The number of tokens in
training, development and test sets are 609,060,
49,620 and 73,153 respectively. We use all train-
ing data set for training the model, whose maxi-
mum length is 242.
The hyper parameters of Dirichlet priors are
initialized in a homogeneous way. The initial
hidden state is set to 40 in all experiments. After
several iterations, the hidden states number con-
verged to 247, which is much larger than the size
of the manually defined POS tags. Our expec-
tation is the refinement variables can reveal the
deep granularity of the POS tags.
5.1 Clustering Evaluation
In this paper, we use information theoretic based
metrics to quantify the information shared by
two clusters. The most common information-
based clustering metric is the variational of In-
formation (VI)(Meila?, 2007). Given the cluster-
ing resultCr and the gold clusteringCg, VI sums
up the conditional entropy of one cluster distri-
bution given the other one:
VI(Cr,Cg) = H(Cr) + H(Cg) ? 2I(Cr,Cg)
= H(Cr |Cg) + H(Cg|Cr)
(10)
where H(Cr) is the entropy associated with the
clustering Cr, and mutual information I(Cr,Cg)
quantifies the mutual dependence between two
clusterings, or say the shared information be-
tween two variables. It is easy to see that
VI? [0, log(N)], where N is the number of data
points. However, the standard VI is not normal-
ized, which favors clusterings with a small num-
ber of clusters. It can be normalized by divid-
ing by log(N), because the number of training
instances are fixed. However the normalized VI
score is misleadingly large, if the N is very large
which is the case in our task. In this paper only
un-normalized VI scores are reported to show the
score ranking.
To standardize the measures to have fixed
bounds, (Strehl and Ghosh, 2003) defined the
normalized Mutual Information (NMI) as:
NMI(Cr,Cg) =
I(Cr,Cg)
?
H(Cr)H(Cg)
(11)
NMI takes its lower bound of 0 if no information
is shared by two clusters and the upper bound
of 1 if two clusterings are identical. The NMI
however, still has problems, whose variation is
sensitive to the choice of the number of clusters.
Rosenberg and Hirschberg (2007) proposed
V-measure to combine two desirable properties
of clustering: homogeneity (h) and completeness
(c) as follows:
h = 1 ? H(Cg|Cr)/H(Cg)
c = 1 ? H(Cr |Cg)/H(Cr)
V = 2hc/(h + c)
(12)
Generally homogeneity and completeness
runs in opposite way, whose harmonic mean (i.e.
V-measure) is a comprise score, just like F-score
for the precision and recall.
Let us first examine the contextual word clus-
tering performance. The VI score between dis-
tributional word categories and gold standard is
2.39. The NMI and V-measure score are 0.53
and 0.48, respectively.
The clustering performance of the HMM out-
puts are reported in Figure 2. The best VI
score achieved was 3.9524, while V-measure
was 62.09% and NMI reached 0.8051. Previous
40 60 80 100 120 140 160 180 200 220 2403.8
4
4.2
4.4
4.6
4.8
5
(a) VI score
40 60 80 100 120 140 160 180 200 220 2400
0.1
0.2
0.3
0.4
0.5
0.6
0.7
 
 
NMIhomogeneitycompletenessV?measure
(b) normalized scores
Figure 2: Clustering evaluation metrics against number of hidden states
work of Chinese tagging focuses on the tagging
accuracies, e.g. Wang (Wang and Schuurmans,
) and Huang et al (Huang et al, 2007). To
our knowledge, this is the first work to report
the distributional clustering similarity measures
based on informatics view for Chinese . Simi-
lar works can be found on English of WSJ cor-
pus (Van Gael et al, 2009). Their best results of
VI, V-measure, achieved with Pitman-Yor prior,
were 3.73 and 59%. We believe the Chinese re-
sults are not good as English correspondences
because of the rich unknown words in Chinese
(Tseng et al, 2005).
5.2 Dependency Parsing Evaluation
The next experiment is to test the goodness of the
outcome states of our model in the context of real
tasks. In this work, we consider unsupervised
dependency parsing for a fully unsupervised sys-
tem. The dependency parsing is to extract the
dependency graph whose nodes are the words of
the given sentence. The dependency graph is a
directed acyclic graph in which every edge links
from a head word to its dependent. Because we
work on unsupervised methods in this paper, we
choose a simple generative head-outward model
(DependencyModel with Valence, DMV) (Klein
and Manning, 2004; Headden III et al, 2009) for
parsing. The data through the experiment is re-
stricted to the sentences up to length 10 (exclud-
ing punctuation).
Because the main purpose is to test the HMM
output rather than to improve the parsing perfor-
mance, we select the original DMV model with-
out extensions or modifications. Starting from
the root, DMV generates the head, and then each
head recursively generates its left and right de-
pendents. In each direction, the possible depen-
dents are repeatedly chosen until a STOP marker
is seen. DMV use inside-outside algorithm for
re-estimation. We choose the ?harmonic? ini-
tializer proposed in (Klein and Manning, 2004)
for initialization. The valence information is the
simplest binary value indicating the adjacency.
For different HMM candidates with varied hid-
den state number, we directly use the outputs as
the input of the DMV and trained a set of models.
Performing test on these individual models, we
report the directed dependency accuracies (the
fraction of words assigned the correct parent) in
Figure 3.
40 60 80 100 120 140 160 180 200 220 24035
40
45
50
55
Figure 3: Directed accuracies for different hid-
den states
It is noted that the accuracy monotonically
increases when the number of states increases.
The most drastic increase happened when state
changes from 40 to 120. The accuracy increased
from 38.56% to 50.60%. If the state number is
larger than 180, the increase is not obvious. The
final best accuracy is 54.20%, which improve the
standard DMV model by 5.6%. Therefore we
can see that the introduction of more annotations
can help the parsing results. However, the im-
provement is limited and stable when the num-
ber of state number is large. To further improve
the parsing performance, one might turn to the
extension of DMV model, e.g. introducing more
knowledge (prior or lexical information) or more
sophistical smoothing techniques. However, the
development of parser is not the focus of this pa-
per.
6 Conclusion and Future Work
This paper works on the unsupervised Chinese
POS tagging based on the first-order HMM. Our
contributions are: 1). The number of hidden
states can be adjusted to fit the data. 2). For in-
ference, we use the variational inference, which
is faster and is guaranteed theoretically to con-
vergence. 3). To overcome the context limitation
in HMM, the words are clustered based on dis-
tributional similarities. It is a 1-to-many cluster-
ing, which means one word might be classified
into different clusters under different contexts.
Finally, experiments show the hidden states are
correlated to the latent annotations of the stan-
dard POS tags.
The future work includes to improve the per-
formance by incorporating a small amount of su-
pervision. The typical supervision used before
is dictionary extracted from a large corpus like
Chinese Gigaword. Another interesting idea is
to select some exemplars (Haghighi and Klein,
2006).
References
Beal, Matthew J., Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2001. The infinite hidden
markov model. In NIPS, pages 577?584.
Beal, M. J. 2003. Variational algorithms for
approximate bayesian inference. Phd Thesis.
Gatsby Computational Neuroscience Unit, Uni-
versity College London.
Brand, Matthew. 1999. An entropic estimator for
structure discovery. In Proceedings of the 1998
conference on Advances in neural information pro-
cessing systems II, pages 723?729, Cambridge,
MA, USA. MIT Press.
Fox, Emily B., Erik B. Sudderth, Michael I. Jordan,
and Alan S. Willsky. 2008. An hdp-hmm for sys-
tems with state persistence. In ICML ?08: Pro-
ceedings of the 25th international conference on
Machine learning.
Frey, Brendan J. and Delbert Dueck. 2007. Clus-
tering by passing messages between data points.
Science, 315:972?976.
Goldwater, Sharon and Tom Griffiths. 2007. A
fully bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 744?751, Prague, Czech Republic,
June. Association for Computational Linguistics.
Haghighi, Aria and Dan Klein. 2006. Prototype-
driven learning for sequence models. In Pro-
ceedings of the main conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 320?327.
Headden, III, William P., David McClosky, and Eu-
gene Charniak. 2008. Evaluating unsupervised
part-of-speech tagging for grammar induction. In
COLING ?08: Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics,
pages 329?336, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Headden III, William P., Mark Johnson, and David
McClosky. 2009. Improving unsupervised depen-
dency parsing with richer contexts and smoothing.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 101?109, Boulder, Colorado,
June. Association for Computational Linguistics.
Hernando, D., V. Crespi, and G. Cybenko. 2005. Ef-
ficient computation of the hidden markov model
entropy for a given observation sequence. vol-
ume 51, pages 2681?2685.
Huang, Zhongqiang, Mary Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and dis-
criminative reranking. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
1093?1102, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Johnson, Mark. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the 2007
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
296?305, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Kaji, Nobuhiro and Masaru Kitsuregawa. 2008. Us-
ing hidden markov random fields to combine dis-
tributional and pattern-based word clustering. In
COLING ?08: Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics,
pages 401?408, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Klein, Dan and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of
dependency and constituency. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages
478?485, Barcelona, Spain, July.
Meila?, Marina. 2007. Comparing clusterings?an in-
formation based distance. volume 98, pages 873?
895.
Rabiner, Lawrence R. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. In Proceedings of the IEEE, pages
257?286.
Rosenberg, Andrew and Julia Hirschberg. 2007.
V-measure: A conditional entropy-based exter-
nal cluster evaluation measure. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 410?420.
Siddiqi, Sajid, Geoffrey Gordon, and Andrew Moore.
2007. Fast state discovery for hmm model selec-
tion and learning. In Proceedings of the Eleventh
International Conference on Artificial Intelligence
and Statistics (AI-STATS).
Strehl, Alexander and Joydeep Ghosh. 2003. Clus-
ter ensembles ? a knowledge reuse framework
for combining multiple partitions. Journal of Ma-
chine Learning Research, 3:583?617.
Tseng, Huihsin, Daniel Jurafsky, and Christopher
Manning. 2005. Morphological features help pos
tagging of unknown words across language vari-
eties. pages 32?39.
Van Gael, Jurgen, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for
the infinite hidden markov model. In ICML ?08:
Proceedings of the 25th international conference
on Machine learning.
Van Gael, Jurgen, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 678?687, Singapore, Au-
gust. Association for Computational Linguistics.
Wang, Qin Iris and Dale Schuurmans. Improved es-
timation for unsupervised part-of-speech tagging.
page 2005, Wuhan, China.
Weeds, Julie and David Weir. 2003. A general
framework for distributional similarity. In Pro-
ceedings of the 2003 conference on Empirical
methods in natural language processing, pages
81?88, Morristown, NJ, USA. Association for
Computational Linguistics.
Zelling, Harris. 1968. Mathematical sturcture of lan-
guage. NewYork:Wiley.
APPENDIX
Pseudo-code of the extended Baum-Welch Al-
gorithm in our dynamic HMM
Input: Time step t:
State Candidate: k ? (k(1), k(2)) ;
Sate Number: Nt;
Model Parameter: ?(t) = (A(t), B(t), pi(t));
Initialize
u(l)[k(1), k(2)]? [ u
(l)[k]
2 ,
u(l)[k]
2 ], l ? {A, B, pi}
pik(1) ? 12pik; pik(2) ?
1
2pik
ak?k(i) ? 12ak?k(i) ; ak(i)k? ? ak(i)k?;
ak(i)k( j) ? 12ak(i)k( j) , here i, j ? 1, 2, k? , k
repeat
E step:
update forward: ?t(k(1)) and ?t(k(2))
backward: ?t(k(1)) and ?t(k(2))
update ?t(i, j) and ?t(i); if i, j ? {k(1), k(2)}
update E[ni j] =
?
t ?t(i, j)/
?
t ?t(i)
E[nik] =
?
t,wt=k ?t( j)/
?
t ?t( j)
M step:
update ?(t+1) using equation (7)
until (4F < ?)
Output: ?(t+1), F
Automatic Identification of Predicate Heads in Chinese Sentences 
 
Xiaona Rena   Qiaoli Zhoua   Chunyu Kitb   Dongfeng Caia 
Knowledge Engineering Research Centera 
Shenyang Aerospace University 
Department of Chinese, Translation and Linguisticsb 
City University of Hong Kong 
rxn_nlp@163.com    ctckit@cityu.edu.hk 
 
  
Abstract 
We propose an effective approach to auto-
matically identify predicate heads in Chinese 
sentences based on statistical pre-processing 
and rule-based post-processing. In the pre-
processing stage, the maximal noun phrases in 
a sentence are recognized and replaced by 
?NP? labels to simplify the sentence structure. 
Then a CRF model is trained to recognize the 
predicate heads of this simplified sentence. In 
the post-processing stage, a rule base is built 
according to the grammatical features of 
predicate heads. It is then utilized to correct 
the preliminary recognition results. Experi-
mental results show that our approach is feasi-
ble and effective, and its accuracy achieves 
89.14% on Tsinghua Chinese Treebank. 
1 Introduction 
It is an important issue to identify predicates in 
syntactic analysis. In general, a predicate is con-
sidered the head of a sentence. In Chinese, it 
usually organizes two parts into a well-formed 
sentence, one with a subject and its adjunct, and 
the other with an object and/or complement (Luo 
et al, 1994). Accurate identification of predicate 
head is thus critical in determining the syntactic 
structure of a sentence. Moreover, a predicate 
head splitting a long sentence into two shorter 
parts can alleviate the complexity of syntactic 
analysis to a certain degree. This is particularly 
useful when long dependency relations are in-
volved. Without doubt, this is also a difficult task 
in Chinese dependency parsing (Cheng et al, 
2005). 
Predicate head identification also plays an im-
portant role in facilitating various tasks of natural 
language processing. For example, it enhances 
shallow parsing (Sun et al, 2000) and head-
driven parsing (Collins, 1999), and also improves 
the precision of sentence similarity computation 
(Sui et al, 1998a). There is reason to expect it to 
be more widely applicable to other tasks, e.g. 
machine translation, information extraction, and 
question answering. 
In this paper, we propose an effective ap-
proach to automatically recognize predicate 
heads of Chinese sentences based on a preproc-
essing step for maximal noun phrases 1(MNPs). 
MNPs usually appear in the location of subject 
and object in a sentence. The proper identifica-
tion of them is thus expected to assist the analy-
sis of sentence structure and/or improve the ac-
curacy of predicate head recognition. 
In the next section, we will first review some 
related works and discuss their limitations, fol-
lowed by a detailed description of the task of 
recognizing predicate heads in Section 3. Section 
4 illustrates our proposed approach and Section 5 
presents experiments and results. Finally we 
conclude the paper in Section 6. 
2 Related Works 
There exist various approaches to identify predi-
cate heads in Chinese sentences. Luo and Zheng 
(1994) and Tan (2000) presented two rule-based 
methods based on contextual features and part of 
speeches. A statistical approach was presented in 
Sui and Yu (1998b), which utilizes a decision 
tree model. Gong et al (2003) presented their 
hybrid method combining both rules and statis-
tics. These traditional approaches only make use 
of the static and dynamic grammatical features of 
the quasi-predicates to identify the predicate 
heads. On this basis, Li and Meng (2005) pro-
posed a method to further utilize syntactic rela-
tions between the subject and the predicate in a 
sentence. Besides the above monolingual pro-
posals, Sui and Yu (1998a) discussed a bilingual 
strategy to recognize predicate heads in Chinese 
                                                 
1 Maximal noun phrase is the noun phrase which is not con-
tained by any other noun phrases.  
sentences with reference to those in their coun-
terpart English sentences. 
Nevertheless, these methods have their own 
limitations. The rule-based methods require ef-
fective linguistic rules to be formulated by lin-
guists according to their own experience. Cer-
tainly, this is impossible to cover all linguistic 
situations concerned, due to the complexity of 
language and the limitations of human observa-
tion. In practice, we also should not underesti-
mate the complexity of feature application, the 
computing power demanded and the difficulties 
in handing irregular sentence patterns. For in-
stance, a sentence without subject may lead to an 
incorrect recognition of predicate head. For cor-
pus-based approaches, they rely on language data 
in huge size but the available data may not be 
adequate. Those bilingual methods may first en-
counter the difficulty of determining correct sen-
tence alignment in the case that the parallel data 
consist of much free translation. 
Our method proposed here focuses on a simple 
but effective means to help identify predicate 
heads, i.e., MNP pre-processing. At present, 
there has some substantial progress in automatic 
recognition of MNP. Zhou et al (2000) proposed 
an efficient algorithm for identifying Chinese 
MNPs by using their structure combination, 
achieving an 85% precision and an 82% recall. 
Dai et al (2008) presented another method based 
on statistics and rules, reaching a 90% F-score on 
HIT Chinese Treebank. Jian et al (2009) em-
ployed both left-right and right-left sequential 
labeling and developed a novel ?fork position? 
based probabilistic algorithm to fuse bidirec-
tional results, obtaining an 86% F-score on the 
Penn Chinese Treebank. Based on these previous 
works, we have developed an approach that first 
identifies the MNPs in a sentence, which are then 
used in determining the predicate heads in the 
next stage. 
3 Task  Description 
The challenge of accurate identification of predi-
cate heads is to resolve the problem of quasi-
predicate heads in a sentence. On the one hand, 
the typical POSs of predicate heads in Chinese 
sentences are verbs, adjectives and descriptive 
words 2 . Each of them may have multiple in-
stances in a sentence. On the other hand, while a 
simple sentence has only one predicate head, a 
complex sentence may have multiple ones. The 
                                                 
2 We only focus on Verbs and adjectives in this work. 
latter constitutes 8.25% in our corpus. Thus, the 
real difficulty lies in how to recognize the true 
predicate head of a sentence among so many 
possibilities. 
Take a simple sentence as example: 
?/rN ?/qN ?/v ??/a ??/n ?
/uJDE ?/a ?/n ??/v ??/aD ?
/uJDE ??/v ?/n ?/cC ??/v ?
/n ?/wE 
The quasi-predicate heads (verbs and adjectives) 
include ?/v, ??/a, ?/a, ??/v, ??/v, 
and ??/v. However, there are two MNPs in 
this sentence, namely, ??/rN ?/qN ?/v ??
/a ??/n ?/uJDE ?/a ?/n? and ???/aD 
?/uJDE ??/v ?/n ?/cC ??/v ?/n?. 
These two MNPs cover most quasi-predicate 
heads in the sentence, except ??/v, the true 
predicate head that we want. 
An MNP is a complete semantic unit, and its 
internal structure may include different kinds of 
constituents (Jian et al, 2009). Therefore, the 
fundamental structure of a sentence can be made 
clear after recognizing its MNPs. This can help 
filter out those wrong quasi-predicates for a bet-
ter shortlist of good candidates for the true predi-
cate head in a sentence. 
In practice, the identification of predicate head 
begins with recognizing MNPs in the same sen-
tence. It turns the above example sentence into: 
[ ?/rN ?/qN ?/v ??/a ??/n ?
/uJDE ?/a ?/n ] ??/v [ ??/aD 
?/uJDE ??/v ?/n ?/cC ??/v ?
/n ] ?/wE 
These MNPs are then replaced with the conven-
tional label ?NP? for noun phrase, resulting in a 
simplified sentence structure as follows. 
NP/NP  ??/v  NP/NP ?/wE 
This basic sentence structure can largely allevi-
ates the complexity of the original sentence and 
narrows down the selection scope of quasi-
predicates for the true head. In this particular 
example, the only verb left in the sentence after 
MNP recognition is the true predicate head. 
4 Predicate Head Identification  
This section describes the process of identifying 
predicate heads in sentences. As illustrated in 
Figure 1 below, it can be divided into three steps: 
Step 1: recognize the MNPs in a sentence and 
replace the MNPs with ?NP? label to simplify 
the sentence structure. 
Step 2: recognize the predicate heads in the 
resulted simplified structure. 
Step 3: post-process the preliminary results to 
correct the wrong predicate heads according to 
heuristics in a rule base. 
4.1 MNP Recognition 
The MNP recognition is performed via a trained 
CRF model on unlabeled data. We adopt the 
method in Dai et al (2008), with modified tem-
plates for the different corpus. Each feature is 
composed of the words and POS tags surround-
ing the current word i, as well as different com-
bination of them. The context window of tem-
plate is set to size 3. Table 1 shows the feature 
template we use.  
Type Features 
Unigram Wordi Posi 
Bigram Wordi/Posi  
Surrounding Wordi-1/Wordi Posi-1/Posi 
 Wordi/Wordi+1 Posi/Posi+1 
 Wordi-2/Posi-2 Posi-2/Posi-1 
 Posi-2/Posi-1/Posi Posi-3/Posi-2 
 Posi-1/Posi/Posi+1 Wordi+3/Posi+3 
 Posi+1/Posi+2/Posi+3 Wordi+2/Wordi+3
 
Table 1: Feature Template 
 
Test data Final results 
Preliminar
 
 
Figure 1: Flow Chart of Predicate Head Identification 
 
The main effective factors for MNPs recogni-
tion are the lengths of MNPs and the complexity 
of sentence in question. We analyze the length 
distribution of MNPs in TCT 3  corpus, finding 
that their average length is 6.24 words and the 
longest length is 119 words. Table 2 presents this 
distribution in detail. 
 
Length of MNP Occurrences Percentage (%)
len?5 3260 48.82 
5?len?10 2348 35.17 
len?10 1069 16.01 
 
Table 2: Length Distribution of MNPs in TCT Corpus 
 
The MNPs longer than 5 words cover 50% of 
total occurrences, indicating the relatively high 
complexity of sentences. We trained a CRF 
model using this data set, which achieves an F-
score of 83.7% on MNP recognition. 
4.2 Predicate Head Identification 
After the MNPs in a sentence are recognized, 
they are replaced by ?NP? label to rebuild a sim-
plified sentence structure. It largely reduces the 
difficulty in identifying predicate heads from this 
simplified structure.  
We evaluate our models by their precision in 
the test set, which is formulated as 
                                                 
3 Tsinghua Chinese Treebank ver1.0. 
_
100%
_
right sentences
Precision
Sum sentences
= ?      (1) 
The right_sentences refer to the number of sen-
tences whose predicate heads are successfully 
identified, and the sum_sentences to the total 
number of sentences in the test set. We count a 
sentence as right_sentence if and only if all its 
predicate heads are successfully identified, in-
cluding those with multiple predicate heads. 
For each predicate head, we need an appropri-
ate feature representation f (i, j). We test the 
model performance with different context win-
dow sizes of template. The results are shown in 
Table 3 as follows. 
 
Template Context window size Precision (%) 
Temp1 2 79.27 
Temp2 3 82.59 
Temp3 4 81.37 
 
Table 3: Precisions of Predicate Heads Recognition under 
Different Context Window Sizes 
 
It shows that the window size of 3 words gives 
the highest precision (82.59%). Therefore we 
apply this window size, together with other fea-
tures in our CRF model, including words, POSs, 
phrase tags and their combinations. There are 24 
template types in total. 
4.3 Post-processing 
The post-processing stage is intended to correct 
errors in the preliminary identification results of 
MNP recognition MNP replacement Predicate head recognition y results 
Predicate head recognition model Rule base MNP recognition model 
predicate heads, by applying linguistic rules for-
mulated heuristically. We test each rule to see if 
it improves the recognition accuracy, so as to 
retrieve a validated rule base. The labeling of 
predicate heads follows the standard of TCT and 
a wrong labeling is treated as an error. 
There are three main types of error, according 
to our observation. The first is that no predicate 
head is identified. The second is that the whole 
sentence is recognized as an MNP, such that no 
predicate head is recognized. The third is that the 
predicate head is incorrectly identified, such as  
??? in the expression ???????, where the 
correct answer is ???? according to the TCT 
standard.  
 
Error types Percentage Improved  percentage 
No predicate head 17.50% 2.44% 
a sentence as an MNP 10.63% 1.11% 
??????? 8.75% 0.56% 
Others 63.12% 2.77% 
 
Table 4: Types of Error  
 
Table 4 lists different types of error, together 
with their percentage in all sentences whose 
predicate heads have been mistakenly identified, 
and the improvement in percentage after the 
post-processing. To correct these errors, a num-
ber of rules for post-processing are formulated. 
The main rules are the followings: 
? If no predicate head is recognized in a sen-
tence, we label the first verb as the predi-
cate head. 
Error sample??/p [ ????/m ?/qT ?
???/nR ] ?/f ?/wP [ ??/nS ??/d 
??/v ????/b ???/b ??/n ] ?
/wE 
Corrected??/p [ ????/m ?/qT ??
??/nR ] ?/f ?/wP [ ??/nS ??/d 
??/v ????/b ???/b ??/n ] ?
/wE 
? If the whole sentence is recognized as an 
MNP, such that no predicate head is identi-
fied, we label the first verb as the predicate 
head. 
 Error sample?[ ??/n ??/v ?/n ?
/cC ?/n ?/m ??/n ] ?/wE 
Corrected?[ ??/n ??/v ?/n ?/cC ?
/n ?/m ??/n ] ?/wE 
? For expression ???????, we label ??
?? as the predicate head. 
Error sample?[ ?/rB ?/m ?/qN ??/n ] 
??/v ???/n ?/vC [ ?/d ?/v ??
??/n ?/cC ????/n ??/n ?/uJDE 
???/b ??/n ] ?/wE 
Corrected?[ ?/rB ?/m ?/qN ??/n ] 
??/v ???/n ?/vC [ ?/d ?/v ??
??/n ?/cC ????/n ??/n ?/uJDE 
???/b ??/n ] ?/wE 
There are also other rules in the rule base be-
sides the above ones. For example, if the first 
word of a sentences is ??? or ????, it is la-
beled as the predicate head. 
5 Experiments 
5.1 Data Sets 
Our experiments are carried out on the Tsinghua 
Chinese Treebank (TCT). Every constituent of a 
sentence in TCT is labeled by human expert. We 
randomly extract 5000 sentences from TCT and 
remove those sentences that do not have predi-
cate head. Finally, our data set contains 4613 
sentences, in which 3711 sentences are randomly 
chosen as training data and 902 sentences as test-
ing data. The average length of these sentences 
in training set is 20 words. 
The number of quasi-predicate heads in a sen-
tence is a critical factor to determine the per-
formance of predicate head recognition. Reduc-
ing the number of quasi-predicate heads can im-
prove the recognition precision. Table 5 shows 
the percentage of quasi-predicate heads in train-
ing data before and after MNP replacement. 
 
Number of 
quasi-
predicates 
Percentage before 
MNP replace-
ment(%) 
Percentage after 
MNP replace-
ment(%) 
1 12.50 49.69 
2 19.62 27.22 
3 20.37 12.37 
>3 47.51 10.72 
 
Table 5: The Percentage of Quasi-predicate Heads Before 
and After MNP Replacement 
 
From Table 5, we can see that almost half sen-
tences contain more than three quasi-predicate 
heads. Only 12.5% of sentences have only one 
quasi-predicate head before MNP replacement. 
However, after MNPs are replaced with the ?NP? 
label, only 10.72% contain more than three 
quasi-predicate heads and nearly 50% contain 
only one quasi-predicate head. We have evidence 
that MNP pre-processing can reduce the number 
of quasi-predicate heads and lower the complex-
ity of sentence structures. 
5.2 Results and Discussion 
For comparison purpose, we developed four dif-
ferent models for predicate head recognition. 
Models 1 and 2 are CRF models, the former rec-
ognizing predicate heads directly and the later 
recognizing MNPs at the same time. Model 3 
recognizes predicate heads based on MNP pre-
processing. Model 4 is based on model 3, includ-
ing the post-processing stage. Table 6 shows the 
recognition performance of each model using the 
best context window size. 
 
Model Context window size 
Number of cor-
rect sentences 
Preci-
sion(%) 
model 1 4 680 75.39 
model 2 4 687 76.16 
model 3 3 745 82.59 
model 4 3 804 89.14 
 
Table 6: Performance of Different Models 
 
Comparing these models, we can see that the 
additional feature in model 2 leads to 1% im-
provement in precision over model 1. Moreover, 
the MNP pre-processing in model 3 results in a 
large increase in accuracy, compared to model 1. 
It indicates that the MNP pre-processing does 
improve the precision of recognition. Compared 
with model 3, model 4 achieves a precision even 
6.55% higher, indicating that the post-processing 
is also an effective step for recognition. 
As shown, the performance is affected by the 
effect of MNP recognition. There are three kinds 
of relation between the predicate heads and the 
types of MNP recognition error: 
Relation 1: The whole sentence is recognized 
as an MNP. 
Relation 2: The boundaries of an MNP are in-
correctly recognized and the MNP does not con-
tain the predicate head. 
Relation 3: The boundaries of an MNP are in-
correctly recognized and the MNP contains the 
predicate head. Table 7 shows the distribution of 
these three relations in the recognition errors. 
 
Relation Number of sentences Percentage(%)
Relation 1 17 5.47 
Relation 2  281 90.35 
Relation 3 13 4.18 
 
Table 7: Distribution of the Three Relations in 
Recognition Errors 
In our approach, the errors of relation 1 and 
relation 3 can be solved by the post-processing, 
as presented in Section 4.3. Relation 2 holds the 
largest proportion among the three. But the error 
rate of predicate head recognition only reaches 
31.67% in this case. That is to say, although the 
MNP boundaries are incorrectly recognized, the 
accuracy of predicate head recognition can still 
reach 68.33%. 
Chen (2007) proposed a probabilistic model 
(model 5) for recognizing predicate heads in Chi-
nese sentences. The probabilities of quasi-
predicates are estimated by maximum likelihood 
estimation. A discounted model is used to 
smooth parameters. We compare his model with 
our model 3 using different contextual features 
on TCT corpus. Table 8 shows the comparison 
results.  
The highest precision of model 3 is 82.59% 
when the context window size is set to 3. For 
model 5, it is 70.62% at a context window size of 
4. Experimental results show that the precision of 
our method is about 12% higher than Chen?s. 
 
Context window size Model Precision (%) 
model 5 69.18 2 
model 3 79.27 
model 5 70.18 
3 
model 3 82.59 
model 5 70.62 
4 
model 3 81.37 
 
Table 8: Comparison between model 3 and Chen?s model 
 
Beside Chen?s method, the Stanford Parser 
can also recognize the predicate heads in simple 
Chinese sentences. The root node of dependency 
tree is the predicate head. For a comparison, we 
randomly extract two hundred simple sentences 
in our test data to compare it with the outputs of 
our model 3. We also train a model of predicate 
head recognition (model 6), which assumes that 
all MNPs are successfully identified. The com-
parison is shown in Table 9. We can see that the 
precision of model 6 is 8.35% higher than model 
3. This means that our method still has a certain 
room for further improvement. 
 
Stanford Parser model 3 model6 
78.17% 83.15% 91.5% 
 
Table 9: Comparison between model 3 and Stanford 
Parser 
5.3 Error Analysis 
As shown above, the post-processing can correct 
most errors in the recognition of predicate heads. 
But we also observe some errors that cannot be 
corrected this way. For example, 
???/n?/p ???/n ??/v [ ??/n 
??/n ] ??/v ?/wE 
The predicate head here is ????, but usually 
???? is recognized as the predicate head. This 
is because ???? can be used either as a verb or 
a noun. There are many verbs of this kind in Chi-
nese, such as ??? ? and ??? ?. Mistakes 
caused by the flexibility of Chinese verb and the 
ambiguity of sentence structure appear to deserve 
more of our effort. Meanwhile, there are also 
some other unusual cases that cannot be properly 
solved with statistical methods. 
6 Conclusion 
Identification of predicate heads is important to 
syntactic parsing. In this paper, we have pre-
sented a novel method that combines both statis-
tical and rule-based approaches to identify predi-
cate heads based on MNP pre-processing and 
rule-based post-processing. We have had a series 
of experiments to show that this method achieves 
a significant improvement over some state-of-
the-art approaches. Furthermore, it also provides 
a simple structure of sentence that can be utilized 
for parsing. 
In the future, we will study how semantic in-
formation can be applied to further improve the 
precision of MNP recognition and predicate head 
identification. It is also very interesting to ex-
plore how this approach can facilitate parsing, 
including shallow parsing. 
Acknowledgments 
We would like to thank the anonymous review-
ers for their helpful comments and suggestions. 
We also thank Billy Wong of City University of 
Hong Kong for his much-appreciated input dur-
ing the writing process. 
References  
Zhiqun Chen. 2007. Study on recognizing predicate of 
Chinese sentences. Computer Engineering and 
Applications, 43(17): 176-178. 
Yuchang Cheng, Asahara Masayuki, and Matsumoto 
Yuji. 2005. Chinese deterministic dependency ana-
lyzer: examining effects of global features and root 
node finder. In Proceedings of the Fourth 
SIGHAN Wordshop on Chinese Language 
Processing, pp. 17-24. 
Cui Dai, Qiaoli Zhou, and Dongfeng Cai. 2008. 
Automatic recognition of Chinese maximal-length 
noun phrase based on statistics and rules. Journal 
of Chinese Information Processing, 22(6): 110-
115. 
Xiaojin Gong, Zhensheng Luo, and Weihua Luo. 
2003. Recognizing the predicate head of Chinese 
sentences. Journal of Chinese Information 
Processing, 17(2): 7-13. 
Ping Jian, and Chengqing Zong. 2009. A new ap-
proach to identifying Chinese maximal-length 
phrase using bidirectional labeling. CAAI Trans-
actions on Intelligent Systems, 4(5): 406-413. 
Guochen Li, and Jing Meng. 2005. A method of iden-
tifying the predicate head based on the correpon-
dence between the subject and the predicate. Jour-
nal of Chinese Information Processing, 19(1): 
1-7. 
Zhensheng Luo, and Bixia Zheng. 1994. An approach 
to the automatic analysis and frequency statistics of 
Chinese sentence patterns. Journal of Chinese 
Information Processing, 8(2): 1-9. 
Zhifang Sui, and Shiwen Yu. 1998a. The research on 
recognizing the predicate head of a Chinese simple 
sentence in EBMT. Journal of Chinese Informa-
tion Processing, 12(4): 39-46. 
Zhifang Sui, and Shiwen Yu. 1998b. The acquisition 
and application of the knowledge for recognizing 
the predicate head of a Chinese simple sentence. 
Journal of Peking University (Science Edition), 
34(2-3): 221-229. 
Honglin Sun, and Shiwen Yu. 2000. Shallow parsing: 
an overview. Contemporary Linguistics, 2(2): 
74-83. 
Hui Tan. 2000. Center predicate recognization for 
scientific article. Journal of WuHan University 
(Natural Science Edition), 46(3): 1-3. 
Qiang Zhou, Maosong Sun, and Changning Huang. 
2000. Automatically identify Chinese maximal 
noun phrase. Journal of Software, 11(2): 195-201. 
Michael Collins. 1999. Head-driven statistical 
models for natural language parsing. Ph. D. 
Thesis, University of Pennsylvania. 
Active Learning Based Corpus Annotation 
Hongyan Song1 and Tianfang Yao2
Shanghai Jiao Tong University 
Department of Computer Science and Engineering 
Shanghai, China 200240 
1songhongyan@sjtu.org 
2yao-tf@cs.sjtu.edu.cn 
Abstract
Opinion Mining aims to automatically acquire 
useful opinioned information and knowledge 
in subjective texts. Research of Chinese Opin-
ioned Mining requires the support of annotated 
corpus for Chinese opinioned-subjective texts. 
To facilitate the work of corpus annotators, 
this paper implements an active learning based 
annotation tool for Chinese opinioned ele-
ments which can identify topic, sentiment, and 
opinion holder in a sentence automatically. 
1 Introduction 
Opinion Mining is a novel and important re-
search topic, aiming to automatically acquire 
useful opinioned information and knowledge in 
subjective texts (Liu et al 2008). This technique 
has wide and many real world applications, such 
as e-commerce, business intelligence, informa-
tion monitoring, public opinion poll, e-learning, 
newspaper and publication compilation, and 
business management. For instance, a typical 
opinion mining system produces statistical re-
sults from online product reviews, which can be 
used by potential customers when deciding 
which model to choose, by manufacturers to find 
out the possible areas of improvement, and by 
dealers for sales plan evaluation (Yao et al 
2008).
   According to Kim and Hovy (2004), an opin-
ion is composed of four parts, namely, topic, 
holder, sentiment, and claim, in which the holder 
expresses the claim including positive or nega-
tive sentiment towards the topic. For example, in 
the sentence I like this car, I is the holder, like is 
the positive sentiment, car is the topic, and the 
whole sentence is the claim. 
   Research on Chinese opinion mining technol-
ogy requires the support of annotated corpus for 
Chinese opinioned-subjective text. Since the cor-
pus includes deep level information related to 
word segmentation, part-of-speech, syntax, se-
mantics, opinioned elements, and some other 
information, the finished annotation is very com-
plicated. Hence, it is necessary to develop an 
automatic tool to facilitate the work of annotators 
so that the efficiency and accuracy of annotation 
can be improved. 
   When developing the automatic annotation tool, 
we find it is most difficult for the tool to annotate 
opinioned elements automatically. Because 
unlike other elements such as part-of-speech, and 
dependency relationship that needed to be anno-
tated in the corpus, there is no available tool that 
can identify opinioned elements automatically. 
Special classifiers should be constructed to solve 
this problem. 
   In traditional supervised learning tasks, train-
ing process consumes all the available annotated 
training instances, so a classifier with high classi-
fication accuracy might be constructed. When 
training a classifier for opinioned elements, it is 
very expensive and time-consuming to get anno-
tated instances. On the other hand, unannotated 
instances are abundant in this case, because all 
the texts in the corpus can be regarded as unan-
notated instances before being annotated. This 
scenario is very appropriate for active learning 
application. An active learning algorithm picks 
up the instances which will improve the per-
formance of the classifier to the largest extent 
into the training set, and often produce classifier 
with higher accuracy using less training instances. 
   Active learning algorithm is featured with 
smaller training set size, less influence from un-
balanced training data and better classification 
performance comparing to classical learning al-
gorithm. This paper experimentally demonstrates 
the validity of active learning algorithm when 
used for opinioned elements identification and 
proposes a computational method for overall sys-
tem performance evaluation which consists of F-
measure, training time, and number of training 
instances.
2 Related Work 
Common active learning algorithms can be di-
vided into two classes, membership query and 
selective sampling (Dagan and Engelson, 1995).
For membership query, algorithm constructs 
learning instances by itself according to the 
knowledge learnt, and submits the instances for 
human processing (Angluin, 1988) (Sammut and 
Banerji, 1986) (Shapiro, 1982). Although this 
method has proved high learning efficiency (Da-
gan and Engelson, 1995), it can be applied in 
fewer scenarios. Since constructing meaningful 
training instance without the knowledge of target 
concept is rather difficult. As to selective sam-
pling, algorithm picks up training instances 
which can improve the performance of the classi-
fier to the largest extent from a large variety of 
available instances. Algorithm in this class can 
be further divided into stream-based algorithm 
and pool-based algorithm according to how in-
stances are saved (Long et al 2008). For stream-
based algorithm (Engelson and Dagon, 1999) 
(Freund et al 1997), unannotated instances are 
submitted to the system successively. All the 
instances not selected by the algorithms will be 
discarded. As to pool-based algorithm (Muslea et 
al, 2006) (McCallum and Nigam, 1998) (Lewis
and Gail, 1994), the algorithm choose the most 
appropriate training instances from all the avail-
able instances. Instance not selected might have 
chance to be picked up in the next round. Though 
its computational complexity is higher, selective 
sampling is widely used as an active learning 
method for no prior knowledge of the target con-
cept is required. 
Although much research has been made in 
the field, we found no case which deals with 
multi-classification problem in active learning. 
Besides, there is no available method to evaluate 
the performance of active learning in information 
extraction.
3 Active Learning Based Corpus Anno-
tation
3.1 System Structure 
The pool-based active learning algorithm is 
composed of two main parts: a learning engine 
and a selecting engine (Figure 1). The learning 
engine uses instances in the training set to im-
prove the performance of the classifier. The se-
lecting engine picks up unannotated instances 
according to preset rules, submits these instances 
for human annotation, and incorporates these 
instances into the training set after the annotation 
is completed. The learning engine and the select-
ing engine work in turns. The performance of the 
classifier tends to improve with the increasing of 
the training set size. When the preset condition is 
met, the training process will finish. 
Figure 1 System Workflow 
For our active learning based annotation tool, 
the workflow is as follows. 
1. Convert raw texts into the format which 
the algorithm can deal with. 
2. Selecting engine picks up instances which 
are expected to improve the performance of the 
classifier to the largest extent. 
3. Annotate these instances manually. 
4. Learning engine incorporate these anno-
tated instances into the training set, and use the 
new training set to train the classifier. 
5. Find out whether the performance of the 
classifier satisfies the preset standard. If not, go 
to step 2. 
6. Use the classifier to identify the opinioned 
element in the unannotated dataset. 
7. Convert the result into the required format. 
3.2 Learning Engine 
The learning engine maintains the classifier by 
iteratively training classifiers with new training 
sets. The classifier adopted determines the up 
limit of the system performance. We use Support 
Vector Machine (SVM) (Vapnik, 1995) (Boser et
al, 1992) (Chang and Lin, 1992) as the classifier 
for our system for its high generalization per-
formance even with feature vectors of high di-
mension and its ability to manage kernel func-
tions that map input data to higher dimensional 
space without increasing computational com-
plexity. 
3.3 Selecting Engine 
In our system, selecting engine picks up in-
stances for human annotation, and puts the anno-
tated instance into the training set. The strategy 
adopted when selecting training instance is criti-
cal to the overall performance of the active learn-
ing algorithm. A good strategy will more likely 
to produce a classifier with high accuracy from 
less training instances. 
The strategy we adopted here is to choose the 
instances which the classifier is most unsure 
about which class they belong to. For a linear bi-
classification SVM, these instances are the ones 
closest to the separating hyper plane. That means, 
the selecting engine will choose training in-
stances according to their geometric distances to 
the hyper plane. The instance with least distance 
will be selected as the next instance to be added 
into the training set while the other instances will 
be saved for future reference. 
The computational complexity of getting the 
distance between an instance and the hyper plane 
is low. However, this method can not be applied 
to SVM with non-linear kernel for geometric 
distances are meaningless in these cases. We use 
radial basis function, which is non-linear, as the 
kernel function in our system for it outperforms 
linear kernel in the experiment. Hence, we must 
find another method to pick up training instances. 
Non-linear SVM decides the class an in-
stance belongs to according to its decision func-
tion value.
S
( ) ( )
s
s s s
x
y x y K x x bD
?
 ??&
& & &               (1)       
The instance will be classified into one cer-
tain class if , or the other class 
if . However, it will be difficult to clas-
sify the instance according to SVM theory 
if
( ) 0y x !&
( ) 0y x &
( ) 0y x  & . Hence, we may deduce that SVM is 
most unsure when classifying an instance with 
least absolute decision function value. 
We define the Predict Value (PV) as the 
value based on which selecting engine picks up 
training instances. 
For bi-classification SVM, we have PV 
equals to the absolute decision function value, 
namely, 
PV( ) ( )x
&
y x
&
                                       (2) 
Instances with the minimum PV will be selected 
into the training set before other instances. 
For example, if we want to identify all the 
topics in the sentence,  
I like this car very much, but the price is a little 
bit too high. 
????????????????
The PV of each instance in the sentences are 
listed in Table 1. They are calculated from the 
decision function of the SVM gained from the 
last round of iteration.  
Instances PV
?   I 0.260306643320642 
?   very 0.553855024703612 
?? like 0.427269428974918 
?   this 0.031682276068012 
?   type 0.366598504697780 
?   car 0.095961213527654 
? 0.178633448748979 
?? but 0.092571306234562 
?? price 0.052164989563922 
?   high 0.539913276317129 
?   (auxiliary word) 0.458036102580422 
?   a little bit 0.439936293288062 
? 0.375263535139242 
Table 1 Example of 2-Classification SVM 
Predict Value 
Suppose all the instances in this sentence 
have not been added into the training set. This
(0.0316), price (0.0521), and but (0.0925) will be 
selected into the training set successively for 
they have the minimal PVs. 
For multi-classification SVM, it will be more 
complicated to find the training instances. Be-
cause common multi-classification SVM is im-
plemented by voting process (Hsu and Lin, 2002),
there are
1
( 1)
2
t t?  decision function values in t-
classification SVM. 
In our system, we need to classify instances 
into 4 classes, namely, topic, holder, sentiment
and other. So a 4-classification SVM is adopted. 
Suppose for an instance, we get 6 Decision Func-
tion Values from 6 bi-classification SVMs as in 
Table 2. 
No. Classification Decision Function Value Result
1 Class 0 Vs Class 1 1.00032792289507 0
2 Class 0 Vs Class 2 0.999999993721249 0
3 Class 0 Vs Class 3 1.00032792289507 0
4 Class 1 Vs Class 2 0.106393804825973 1
5 Class 1 Vs Class 3 -5.20417042793042E-18 3
6 Class 2 Vs Class 3 -0.106393804825973 3
Table 2 Example of 4-Classification SVM Decision 
Process
For each bi-classification SVM, the class in-
stance belongs to is determined by whether the 
decision function value is greater than or less 
than zero. The instance in Table 2 belongs to 
Class 0 since there 3 votes out of 6 votes for 
Class 0. When deciding which class an instance 
belongs to, only the decision function values 
from bi-classification SVMs with correct votes 
will work on the certainty of the final result. 
Hence, we define Predict Value for multi-
classification SVMs as the arithmetic mean value 
of the absolute decision function value of every 
bi-classification SVM with correct vote, 
          
^
t
1, bi classification SVMs with correct votes
1
( ) y ( )
k
t t `
x x
k  ?
?
&

39  
&
ir
      (3) 
For the instance in Table2, the value is calculated 
from the decision function values from bi-
classification SVMs numbered 1, 2, and 3. 
3.4 Experiments 
To prove the validity of active learning algorithm 
and find out the relations between the perform-
ance of the classifiers and the way the classifiers 
are trained, we carried out batches of experi-
ments.
In most information extraction tasks, a word 
and its context are considered a learning sample, 
and encoded as feature vectors. In our experi-
ments, context data includes the part-of-speech 
tag, dependency relation, word semantic mean-
ing, and word disambiguation information of the 
word being classified, its neighboring words and 
its parent word in dependency grammar. Part-of-
speech tag and dependency relation are common 
features for Chinese Natural Language Process-
ing (NLP) tasks1. We get word semantic mean-
ing from HowNet, which is an online common-
sense knowledge base unveiling inter-conceptual
relations and inter-attribute relations of concepts
as connoting in lexicons of the Chinese and the
English equivalents (Zhendong Dong and Qiang 
Dong, 1999). Given an occurrence of a word in 
natural language text, word sense disambiguation 
is the process of identifying which sense of the 
word is intended if the word has a number of dis-
tinct senses. According to Song and Yao (2009), 
this information may help in Chinese NLP tasks 
such as topic identification. 
Lack of explicit boundary between training 
instances and testing instances is a great differ-
ence between common machine learning algo-
rithm and learning algorithm designed for corpus 
annotation. For common machine learning algo-
rithm such as human face recognition, the quan-
tity of training instances is limited while the test-
ing instances could be infinite. It is unnecessary 
and impossible to annotate all the testing in-
stances. However, when annotating a corpus, all 
the texts need to be annotated are decided be-
forehand. Although tools automated part of the 
annotation process, the results still need to be 
reviewed for several times to ensure the quality 
of annotation. That means in an annotation sce-
nario, all the data to be processed are available 
during the training stage. 
The raw texts used in our experiments are 
taken from forums of chinacars.com. These texts 
include explicit subjective opinion and informal 
network language, which are necessary for opin-
ion mining research. Most of them are comments 
composed of one or more sentences on certain 
type of vehicle. The detailed opinion elements 
distributions are showed in table 3. 
We use all the texts as testing data set and a 
subset of it as a training data set. First of all, we 
pick up 10 instances for each class, and train a 
simple classification model with them. Then, the 
baseline system picks up k instances in sequence 
and adds them into the training data set to train a 
new classification model iteratively until the 
training data set is as large as the testing data set, 
1 We use Language Technology Platform (LTP), developed 
by Center for Information Retrieval, Harbin Institute of 
Technology, for part-of-speech tagging, dependency rela-
tionship analysis and word sense disambiguation in our 
experiment.
while the active learning system picks up in-
stances according to the strategy in Chapter 3.3.  
Type No. of Instances
Topic 638
Sentiment 769
Holder 46
Other 1500
Total 2953
Table 3 Detailed Information of the Data Set 
We use three bi-classification model to test 
the performance of the active learning system on 
topic, sentiment, and holder identification sepa-
rately and a four-classification model to identify 
the three opinion elements simultaneously. The 
results of the experiments are illustrated in Fig-
ure 2, 3, 4, and 5 respectively. Table 4, 5, and 6 
provide the detailed F-measure trends while dif-
ferent numbers of instances are added into the 
training data set in each rounds. For each ex-
periment, we try to compare the performances 
when we add different number of instances into 
the training data set in each round of iteration. 
Figure 2 Topic Identification 
Figure 3 Sentiment Identification 
Figure 4 Holder Identification 
Figure 5 All Opinion Elements Identification 
As are illustrated in the figures, the active 
learning system can always achieve better or at 
least no worse performance than baseline system. 
For example, when adding 200 instances in each 
round for topic identification task (Figure2 and 
Table 4), the active learning system reaches its 
peak value in F-measure (0.8644) with only 600 
training instances. This F-measure value is even 
higher than the value the baseline system get 
(0.8604) after taking all the 2953 training in-
stances.
The active learning system outperforms the 
baseline system greatly especially when dealing 
with unbalanced data set (Figure 4 and Table 4). 
In opinion holder identification task, the baseline 
system can not find any holder until 1600 train-
ing instances are taken while the active learning 
system reaches its peak F-measure value (0.8810) 
with only 600 training instances. That means 
when using active learning algorithm, it is possi-
ble for us to save some time for optimizing the 
parameters when dealing with unbalanced data. 
The number of instances added to the training 
data set in each round (k) influences the perform-
ance of the active learning algorithm in a large 
extent. When a smaller value is assigned to k, the 
active learning system will tend to achieve better 
F-measure (Table 4) with less training instances 
comparing to the baseline system. Advantages of 
the active learning system will be diminished by 
the increase in k (Table 6). 
4 Evaluation of Active Learning Algo-
rithm
For active learning algorithm based on member-
ship query, its training process will probably take 
longer time by the time the optimum classier is 
found, since the training process consists of sev-
eral rounds of iteration. At the beginning of the 
iteration, the classification speed of the model is 
much faster due to less training instances are 
used and the model is simple. With more and 
more training instances are added into the train-
ing data set, the model will become more com-
plex and more time will be needed for classifica-
? 
Topic Sentiment Holder All Three Elements No. of    
Instances Baseline
Active 
Learning 
Baseline
Active 
Learning
Baseline
Active 
Learning
Baseline 
Active 
Learning
200 0.7118 0.6221  0.6481 0.0103 0.0000 0.0000 0.6968  0.3874 
400 0.8072 0.8287  0.7344 0.6239 0.0000 0.0000 0.7691  0.7336 
600 0.8237 0.8644  0.7845 0.7860 0.0000 0.8810 0.7907  0.7979 
800 0.8250 0.8625  0.7876 0.8133 0.0000 0.8810 0.8020  0.8240 
1000 0.8386 0.8613  0.7878 0.8189 0.0000 0.8810 0.8101  0.8378 
1200 0.8389 0.8588  0.7992 0.8153 0.0000 0.8810 0.8128  0.8377 
1400 0.8489 0.8588  0.8011 0.8141 0.0000 0.8810 0.8178  0.8471 
1600 0.8450 0.8581  0.8033 0.8150 0.0426 0.8810 0.8211  0.8468 
1800 0.8521 0.8581  0.8059 0.8183 0.1224 0.8810 0.8271  0.8479 
2000 0.8528 0.8585  0.8169 0.8197 0.6857 0.8810 0.8348  0.8481 
2200 0.8560 0.8583  0.8109 0.8200 0.8101 0.8810 0.8372  0.8468 
2400 0.8592 0.8592  0.8186 0.8195 0.8395 0.8810 0.8404  0.8474 
2600 0.8620 0.8610  0.8165 0.8205 0.8675 0.8810 0.8440  0.8463 
2800 0.8578 0.8610  0.8138 0.8177 0.8810 0.8810 0.8464  0.8443 
2953 0.8604 0.8604  0.8183 0.8183 0.8810 0.8810 0.8446  0.8446 
Table 4 F-measure Trends when k=200 

Topic Sentiment Holder All Three Elements No. of    
Instances Baseline
Active 
Learning 
Baseline
Active 
Learning
Baseline
Active 
Learning
Baseline 
Active 
Learning
500 0.8198 0.7730  0.7616 0.1369 0.0000 0.0000 0.7831  0.5173 
1000 0.8386 0.8508  0.7878 0.7566 0.0000 0.8837 0.8101  0.7776 
1500 0.8468 0.8592  0.8039 0.8175 0.0833 0.8810 0.8194  0.8398 
2000 0.8528 0.8610  0.8169 0.8183 0.6857 0.8810 0.8348  0.8484 
2500 0.8626 0.8583  0.8168 0.8205 0.8395 0.8810 0.8427  0.8463 
2953 0.8604 0.8604  0.8183 0.8183 0.8810 0.8810 0.8446  0.8446 

Table 5  F-measure Trends when k=500

Topic Sentiment Holder All Three Elements No. of    
Instances Baseline
Active 
Learning 
Baseline
Active 
Learning
Baseline
Active 
Learning
Baseline 
Active 
Learning
1000 0.8386 0.8335  0.7878 0.3514 0.0000 0.0000 0.8101  0.7534 
2000 0.8528 0.8581  0.8169 0.8170 0.6857 0.8810 0.8348  0.8376 
2953 0.8604 0.8604  0.8183 0.8183 0.8810 0.8810 0.8446  0.8446 

Table 6  F-measure Trends when k=1000
tion. On account of the features of active learn-
ing algorithm, we believe it is necessary to find a 
way to balance the performance of the classifier 
and the time it take in training process for a thor-
ough evaluation of the algorithm. 
We define the measurement for time as: 
k
T
C
                                                 (4) 
where C is the number of all the possible training 
instances available, k is the number of training 
instances added into the training data set in each 
round of iteration. T is the approximate value of 
the inverse ratio of the time it takes for training 
process. T will have a greater value if the training 
process takes less time. Its range is (0, 1] just 
similar to F-measure. 
We define the measurement for the training 
instances used as: 
(1 )
n
K
C
  (5) 
where n is the number of the training instances 
actually used. K will have a greater value if less 
training instances are used in the training process. 
The range of K is [0, 1). 
To judge the overall performance of an active 
learning algorithm, we consider the F-measure 
(F) of the classifier, the time it takes during the 
training process, and the training instances used. 
We define the Active Learning Performance 
(ALP) as the harmonic mean of the three aspects: 
1
( )
(6)
( ) ( )
ALP
K F T
F k C n
F C k k C n F C C n
D E J
D E J
 
 
? ?  
? ? ?  ?   ? ? 
where + + =1D E J , and > @, , 0,1D E J ? . They 
are the weights for the three measurements. The 
greater the value of a certain weight is, the more 
important the measurement is in the overall per-
formance. The greater the value of the ALP is, 
the better the performance of the active learning 
algorithm. For instance, when training a classi-
fier for sentiment identification using active 
learning algorithm, we get a classifier with F-
measure of 0.8189 using 1000 training instances 
and a classifier with F-measure of 0.8200 using 
2200 training instances (Table 4). Sup-
pose
1
= = =
3
D E J , we calculate the value of ALP
for the two cases according to equation (6) and 
get 0.1714 and 0.1507 as results respectively. 
That means a people with no preference among 
F-measure, the number of training instances 
adopted and the time used during training proc-
ess will choose to get a classifier with less train-
ing instances, less training time and less F-
measure value. 
5 Conclusion
This paper experimentally demonstrates the va-
lidity of active learning algorithm when used for 
opinioned elements identification and proposed a 
computational method for overall system per-
formance evaluation which consists of F-
measure, training time, and number of training 
instances. According to our tests, active learning 
algorithm outperforms the base line system in 
most of the cases especially when fewer in-
stances are added into the training data set in 
each round of iteration. However, the method 
could extent the training time in a large scale. To 
balance the pros and cons of active learning algo-
rithm, it might be helpful to adjust the number of 
training instances added in each round dynami-
cally in the training process. For instance, add 
less training instances at the beginning of the 
training process to ensure a high peak value of F-
measure could be achieved and add more train-
ing instances later so that time spent on training 
process could be reduced. 
Acknowledgments  
The author of this paper would like to thank In-
formation Retrieval Lab, Harbin Institute of 
Technology for providing the tool (LTP) used in 
experiments. This research was supported by 
National Natural Science Foundation of China 
Grant No.60773087.  
References
Andrew K. McCallum, Kamal Nigam. 1998. Employ-
ing EM in Pool-based Active Learning for Text 
Classification. In Proceedings of the 15th Interna-
tional Conference on Machine Learning.
Bernhard E. Boser, Isabelle M. Guyon, and Vladimir 
N. Vapnik. 1992. A Training Algorithm for Opti-
mal Margin Classifiers. In Proceedings of the Fifth 
Annual Workshop on Computational Learning 
Theory.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: 
a library for support vector machines. Software 
available at http://www.csie.ntu.edu.tw/~cjlin/ 
libsvm 
Chih-Wei Hsu and Chih-Jen Lin. 2002. A Compari-
son of Methods for Multi-class Support Vector 
Machines. IEEE Transactions on Neural Networks.
Claude Sammut and Ranan B. Banerji. 1986. Learn-
ing Concepts by Asking Questions. Machine 
Learning: An Artificial Intelligence Approach,
1986, 2: 167-191 
Dana Angluin. 1988. Queries and Concept Learning. 
Machine Learning, 1988, 2(4): 319-342 
David D. Lewis, William A. Gail. 1994. A Sequential 
Algorithm for Training Text Classifiers. In Pro-
ceedings of the 17th Annual International ACM 
SIGIR Conference on Research and Development 
in Information Retrieval.
Ehud Y. Shapiro. 1982. Algorithmic Program Debug-
ging. M.I.T. Press. 
Ido Dagan, Sean P. Engelson. 1995. Committee-
Based Sampling for Training Probabilistic Classi-
fiers. In Proceedings of the International Confer-
ence on Machine Learning.
Ion Muslea, Steven Minton, Craig A. Knoblock. 2006. 
Active Learning with Multiple Views. Journal of 
Artificial Intelligence Research, 2006, 27(1): 203-
233. 
Quansheng Liu, Tianfang Yao, Gaohui Huang, Jun 
Liu, Hongyan Song. 2008. A Survey of Opinion 
Mining for Texts. Journal of Chinese Information 
Processing. 2008, 22(6):63-68. 
Jun Long, Jianping Yin, En Zhu, and Wentao Zhao. A 
Survey of Active Learning. 2008. Journal of Com-
puter Research and Development, 2008, 45(z1): 
300-304. 
Shlomo A. Engelson, Ido Dagon. 1999. Committee-
based Sample Selection for Probabilistic Classifi-
ers. Journal of Artificial Intelligence Research,
1999, 11: 335-360. 
Hongyan Song, Jun Liu, Tianfang Yao, Quansheng 
Liu, Gaohui Huang. 2009. Construction of an An-
notated Corpus for Chinese Opinioned-Subjective 
Texts. Journal of Chinese Information Processing,
2009, 23(2): 123-128. 
Hongyan Song and Tianfang Yao. 2009. Improving 
Chinese Topic Extraction Using Word Sense Dis-
ambiguation Information. In Proceedings of the 4th 
International Conference on Innovative Computing, 
Information and Control.
Soo-Min Kim and Eduard Hovy. 2004. Determining 
the Sentiment of Opinions. In Proceedings of the 
Conference on Computational Linguistics: 1367-
1373. 
Tianfang Yao, Xiwen Cheng, Feiyu Xu, Hans 
Uszkoreit, and Rui Wang. 2008. A Survey of Opin-
ion Mining for Texts. Journal of Chinese Informa-
tion Processing, 2008, 22(3): 71-80. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer.  
Yoav Freund, H.Sebastian Seung, Eli Shamir, Naftali 
Tishby. 1997. Selective Sampling Using the Query 
by Committee Algorithm. Machine Learning,
28(2-3): 133-168 
Zhendong Dong and Qiang Dong. 1999. HowNet. 
http://www.keenage.com 
The SAU Report for the 1st CIPS-SIGHAN-ParsEval-2010 
Qiaoli Zhou Wenjing 
Lang 
Yingying 
Wang 
Yan Wang Dongfeng Cai
Knowledge Engineering Research Center,Shenyang Aerospace 
University,Shenyang,China 
Qiaoli_z@yahoo.com.cn 
 
Abstract 
This paper presents our work for 
participation in the 2010 CIPS-SIGHAN 
evaluation on two tasks which are Event 
Description Sub-sentence (EDSs) 
Analysis and Complete Sentence (CS) 
Parsing in Chinese Parsing. The paper 
describes the implementation of our 
system as well as the results we have 
achieved and the analysis. 
1 Introduction 
The paper describes the parsing system of SAU 
in 1st CLPS-SIGHAN evaluation task 2. We 
participate in two tasks - EDS Analysis and CS 
Parsing. The testing set only provides 
segmentation results, therefore, we divide our 
system into the following subsystems: (1) Part-
of-Speech (POS) tagging system, we mainly 
make use of Conditional Random Fields (CRFs) 
model for POS tagging; (2) parsing system, the 
paper adopts divide-and-conquer strategy to 
parsing, which uses CCRFs model for parsing 
and adopts searching algorithm to build trees in 
decoding; (3) head recognition system, which 
also makes use of CCRFs model. 
The rest of the paper is organized as follows: 
Section 2 describes the POS tagging system; 
Section 3 describes the structure of our parsing 
system; Section 4 describes head recognition 
system in parsing tree; Section 5 presents the 
results of our system and the analysis; Section 6 
concludes the paper. 
2 Part-of-Speech Tagging 
We use CRFs model and post-processing 
method for POS tagging. In the first step, we tag 
POS based on CRFs. The second step is the 
post-processing after tagging, which is 
correcting by using dictionary drawn from 
training set. The system architecture of POS 
tagging is shown in Figure 1. 
2.1 Features 
Feature selection significantly influences the 
performance of CRFs. We use the following 
features in our system. 
Atom Template 
word(-2) , word(-1) , word(0) , word(1) , word(2) 
prefix( word (0) ) ,suffix( word(0) ) 
includeDot1(word ( 0 )) 
includeDot2(word ( 0 )) 
Complex Template 
word(-1)& word(0) ? word(0)& word(1) 
word(0)& prefix( word (0) ) 
word(0)& suffix( word(0) ) 
word(0)& includeDot1(word ( 0 )) 
word(0)& includeDot2(word ( 0 )) 
Table 1: Feature templates used in POS tagger. 
word(i) represents the ith word, prefix( word (i) ) 
represents the first character of the ith word, 
suffix( word (i) ) represents the last character of  
the ith word, ncludeDot1(word ( i)) represents 
the ith word containing ?? ? or not, and 
includeDot2(word ( i)) represnts the ith word 
containing ?.? or not. 
2.2 Post-processing 
The post-processing module adopts the 
following processing by analyzing the errors 
from tagging result based on CRFs. We firstly 
need to build two dictionaries which are single 
class word dictionary and ambiguity word 
dictionary before the post-processing. The 
single class word dictionary and ambiguity 
word dictionary are built by drawing from 
training set. 
 
The single class word is the word having 
single POS in training set, and the ambiguity 
word is the word having multi POS in training 
set. Besides, we build rules for words with 
distinctive features aiming at correcting errors, 
such as ???, numbers and English characters, 
etc. 
Figure 2 shows the post-processing step after 
POS tagging by CRFs model. As shown in 
Figure 2, we respectively post-process single 
class words and ambiguity words according to 
CRF score. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(1) Single class word processing module 
The post-processing of single class words 
consults the single class word dictionary and 
CRFs score. When the score from CRFs is 
higher than 0.9, we take the POS from CRFs as 
the final POS; otherwise, POS of the word is 
corrected by the POS in the single class word 
dictionary. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
2 
3 
1 
N 
CRF Primary result 
Word class? 
Ambiguity word 
Single class word 
Unknown word 
End 
Rule base 
Figure2: Post-processing architecture after CRF labeling 
Single class word 
processing module 
Ambiguity word 
processing module 
Unknown word 
processing module 
Training 
corpus 
Features selection 
Parameter estimation 
CRF model
Testing 
corpus 
POS tagger based 
on CRF 
Primary reco- 
gnition result 
Post-processing 
POS 
Result 
Figure 1: System architecture of POS tagging
(2) Ambiguity word processing module 
The post-processing of ambiguity words 
consults the ambiguity word dictionary and 
CRFs score. When the POS from CRFs belongs 
to the POS of the word in the ambiguity word 
dictionary, we take the POS from CRFs as the 
final POS; otherwise, we examine the score of 
CRF, if the score is less than 0.4, the final POS 
of the word is the POS who has the highest 
score (has highest frequency), or else taking 
POS from CRF as the final POS. 
(3) Unknown word processing module 
The unknown words are the words not in 
training set. By analyzing the examples, we find 
that there are great deals of person names, 
location names, organization names and 
numbers, etc. And the words have 
characteristics when building word, therefore, 
we set up rules for processing. 
2.3 Experiment results 
 Table 2 shows the comparative experimental 
results of POS tagging using two methods. 
Table 2: Comparative POS tagging results 
3 Parsing system 
The paper uses divide-and-conquer strategy 
(Shiuan 1996 et al, Braun 2000 et al, Lyon 
1997 et al)for parsing. Firstly, we recognize 
MNP for an input sentence, which divide the 
sentence into two kinds of parts. One kind is 
MNPs, and the other one is frame which is a 
new sentence generating by replacing MNP 
using its head word. Secondly, we use parsing 
approach based on chunking (Abney, 1991, Erik 
Tjong and Kim Sang, 2001) and a searching 
algorithm in decoding. Thirdly, we combine the 
parsing trees of MNPs and frame, which obtains 
the full parsing tree of the original sentence. 
Figure 3 shows the architecture of paring 
system. 
3.1 MNP recognition 
Maximal Noun Phrase (MNP) is the noun 
phrase which is not contained by any other noun 
phrases. We use Berkeley parser (2009 1.0) for 
MNP recognition. We first use Berkeley parser 
to parse sentences after POS tagging, and then 
we tag MNPs from the parsing results. As the 
following example: 
Berkeley parser result: dj[ ??/nS vp[ ??/v 
vp[ ??/v np[ pp[ ?/p np[ ??/nS ??/n ] ] ?
/uJDE ??/n ] ] ] ] 
MNP recognition result: ??/nS ??/v ??
/v np[ ?/p ??/nS ??/n ?/uJDE ??/n ]  
The results of MNP recognition EDSs 
analysis and CS parsing are as table3: 
 
 P R F 
EDSs 85.3202% 85.998% 85.6578% 
CS 77.7102% 79.2782% 78.4864% 
Table 3: Results of MNP recognition 
3.2 Head recognition of MNP and 
generation of frame 
 In this paper, the new sentence in which MNPs 
are replaced by their head word is defined as the 
sentence?s frame. The head of MNPs is 
identified after MNP recognition and then they 
are used to replace the original MNP, and 
finally the sentence?s frame is formed. We use 
the rules to recognize the head of MNP. Usually, 
the last word of MNP is the head of the phrase, 
which can represent the MNP in function. For 
example: ?[?/r ??/n] ??/ad ??/v ??/v 
[?? /v ?? /v ? /u ?? /n]? ? In this 
sentence? ? /r?? /n? and ? ??/v ??/v 
?/u ??/n? are MNPs. If we omit the 
modifier in MNP, for example ?[??/n] ??
/ad ??/v ??/v [??/n]??, the meaning of 
the sentence will not be changed. Because the 
head can represent the syntax function of MNP, 
we can use the head for parsing, which can 
avoid the effect of the modifier of MNP on 
parsing and reduce the complexity of parsing. 
Method EDSs precision 
CS 
precision
CRF 92.83% 89.42% 
CRF +  
post-processing 93.96% 91.05% 
However, the components of MNP are 
complicated, not all of the last word of MNP 
can be the head of MNP. The paper shows that 
if MNP has parentheses, we can use the last 
word before parentheses as the head. When the 
last word of MNP is ???, we use the second last 
word as the head.  
3.3 Chunking with CRFs 
The accuracy of chunk parsing is highly 
dependent on the accuracy of each level of  
 
chunking. This section describes our approach 
to the chunking task. A common approach to 
the chunking problem is to convert the problem 
into a sequence tagging task by using the 
?BIEO? (B for beginning, I for inside, E for 
ending, and O for outside) representation. 
This representation enables us to use the 
linear chain CRF model to perform chunking, 
since the task is simply assigning appropriate 
labels to sequence. 
3.3.1 Features 
Table 4 shows feature templates used in the 
whole levels of chunking. In the whole levels of 
chunking, we can use a rich set of features 
because the chunker has access to the 
information about the partial trees that have 
been already created (Yoshimasa et al, 2009). It 
uses the words and POS tags around the edges 
of the covered by the current non-terminal 
symbol. 
Table 4: Feature templates used in parsing system.  
W represents a word, P represents the part-of-speech 
of the word, C represents the sum of the chunk 
containing the word, F represents the first word of 
the chunk containing the word, L represents the last 
word of the chunk containing the word, S represents 
that the word is a non-terminal symbol or not. Wj is 
the current word; Wj-1 is the word preceding Wj, Wj+1 
is the word following Wj. 
 
 
 
 
 
 
 
 
3.4 Searching for the Best Parse 
The probability for an entire parsing tree is 
computed as the product of the probabilities 
output by the individual CRF chunkers: 
0
(y / )
h
i i
i
score p x
=
=?  
We use a searching algorithm to find the highest 
probability derivation. CRF can score each 
chunker result by A* search algorithm, 
therefore, we use the score as the probability of 
each chunker. We do not give pseudo code, but 
the basic idea is as figure 4. 
 
 
1: inti parser(sent) 
2: Parse(sent, 1, 0) 
    3: 
    4: function Parse(sent, m, n) 
    5:  if sent is chunked as a complete sentence 
    6:     return m 
    7:  H = Chunking(sent, m/n) 
    8:   for h?H do 
    9:    r = m * h.probability 
    10:     if r?n then 
    11:        sent2 = Update(sent, h) 
    12:        s = Parse(sent2, r, n) 
    13:        if s?n then n = s 
    14:    return n 
15: function Chunking(sent, t) 
    16: perform chunking with a CRF chunker and 
return a set of chunking hypotheses whose  
17: probabilities are greater than t. 
18: function Update(sent, h) 
19:  update sequence sent according to chunking 
hypothesis h and return the updated sequence. 
Figure 4: Searching algorithm for the best parse  
 
It is straightforward to introduce beam search 
in this search algorithm?we simply limit the 
number of hypotheses generated by the CRF 
chunker. We examine how the width of the 
beam affects the parsing performance in the 
Word Unigrams W-2 , W-1, W0, W1, W2,
Word Bigrams W-2W-1, W-1W0, W0W1, 
W1W2, W0W-2, W0W2,  
Word Trigrams W0W-1W-2, W0W1W2
POS Unigrams P-3, P-2 , P-1 , P0 , P1, P2, P3,
POS Bigrams P-3P-2, P-2P-1, P-1P0, P0P1, 
P1P2, P2P3, P0P-2, P0P2,
POS Trigrams P-3P-2P-1, P-2P-1P0, P-1P0P1, 
P0P1P2, P1P2P3
Word & POS W0P0, W0P-1, W0P1,
Word & WordCount W0C0
Word & FirstWord W0F0 , W-1F0
Word & LastWord W0L0, W1L0
Word & Symbol W0S0
Chunk Model
 frame 
MNPs
sentence 
MNP Recognition parsing tree
Search
CRF Chunker
Figure3: Parsing system architecture 
experiments. We experiment beam width and 
we adopt the beam width of 4 at last. 
3.5 Head Finding 
Head finding is a post process after parsing in 
our system. The paper uses method combining 
statistics and rules to find head. The selected 
statistical method is CRF model. The first step 
is to train a CRF classifier to classify each 
context-free production into several categories. 
Then a rule-based method is used to post 
process the identification results and gets the 
final recognition results. The rule-based post-
processing module mainly uses rule base and 
case base to carry out post-processing. 
3.6 Head finding based on CRFs 
The head finding procedure proceeds in the 
bottom-up fashion, so that the head words of 
productions in lower layers could be used as 
features for the productions of higher layers 
(Xiao chen et al 2009). 
 
Atom template Definition 
CurPhraseTag The label of the current word 
LCh_Word The left most child 
RCh_Word The right most child 
LCh_Pos The POS of the left most child
MCh_Pos The POS of the middle child 
RCh_Pos The POS of the right most child
NumCh The number of children 
CurPhraseTag 1 ? The labels of the former phrase and the latter 
Table 5: Atom templates for Head finding 
 
Table 6: Complex templates for Head finding 
 
The atom templates are not sufficient for 
labeling context; therefore, we use some 
complex templates by combining the upper 
atom templates for more effectively describing 
context. When the feature function is fixed, the 
atom templates in complex templates are 
instantiated, which will generate features. 
The final feature templates are composed of 
the atom templates and the complex templates. 
The feature templates of the head recognition in 
phrases contain 24 types. 
3.7 Head Finding based on rules 
Through the analysis of error examples, we 
found that some CRFs recognition results are 
clearly inconsistent with the actual situation; we 
can use rules to correct these errors, thus 
forming a rule base. Example-base is a chunk-
based library built through analysis and 
processing on the training corpus. The 
Example-base is composed of all the bottom 
chunk and high-level chunk in training corpus. 
High-level phrases are the bottom chunk 
replaced by heads. 
3.8 Experiment results of head finding 
Table 7 shows the comparative experiment 
results of head recognition. 
 
Table7: Comparative results of head recognition 
4 Experiment of parsing system 
We perform experiments on the training set and 
testing set of Tsinghua Treebank provided by 
CIPS-SIGHAN-ParsEval-2010. For the direct 
fluence of parsing result by the length of 
sentence, we count the length distribution of 
corpus. 
in
Table 8 shows that the length of training set 
and testing set of EDSs is mostly less than 20 
words. The length of training set of CS is evenly 
distributed, while the length of testing set is 
between 30 and 40 words. 
Complex Template 
CurPhraseTag/ NumCh, CurPhraseTag/ LCh_Word, 
CurPhraseTag/LCh_Pos, 
CurPhraseTag/LCh_Pos/RCh_Pos, 
CurPhraseTag/NumCh/LCh_Pos/ RCh_Pos, 
CurPhraseTag/NumCh/LCh_Word/LCh_Pos/MCh_
Pos/RCh_Word/RCh_Pos,  
LCh_Word/LCh_Pos, CurPhraseTag/MCh_Pos, 
NumCh/LCh_Pos/ MCh_Pos/ RCh_Pos, 
 CurPhraseTag/ NumCh/ MCh_Pos, 
CurPhraseTag/LCh_Word/LCh_Pos/MCh_Pos/RCh
_Word/RCh_Pos,  
LCh_Word/ LCh_Pos, LCh_Pos/ MCh_Pos, 
 CurPhraseTag/NumCh, RCh_Word/RCh_Pos,  
NumCh/LCh_Word/LCh_Pos/MCh_Pos/RCh_Word
/RCh_Pos 
 Total Num 
Wrong 
Num Precision 
CRFs 7035 93 98.68% 
CRFs + 
rule-base+ 
case-base 
7035 74 98.95% 
The paper adopts divide-and-conquer strategy 
to parsing; therefore, we conduct the 
frame whose length is less than 5 words, the frame 
length distribution of training set is 9.17% higher 
than the testing set; for the frame whose length is 
more than 5 words and less than 10 words, the 
training set is 7.65% lower than testing; and for the 
frame whose length is between 10 words and 20 
words, the testing set is 20.09% higher compared 
with the training set. From another aspect, in 
testing set, CS is 46.2% lower compared with 
EDSs for frame whose length is less than 5. 
Therefore, the complexity of frame in CS is higher 
than in EDSs. 
comparative experiment of MNP parsing and 
frame parsing. In addition, the results of MNP 
parsing and frame parsing depend on the length 
largely, so we list the length distribution of 
MNP and frame of EDSs and CS as table 9 and 
table 10. 
 
As shown in Table 8, 9 and 10, the length 
distribution of testing set shows that the paring unit 
length of EDSs is reduced to less than 10 from less 
than 20 in original sentence and CS is reduced to 
less than 20 from between 30 and 40 after dividing 
an original sentence into MNPs parts and frame 
part. The above data indicate the divide-and-
conquer strategy reduces the complexity of 
sentences significantly. 
Table 8: Length distribution of EDSs and CS 
 EDSs CS 
length training set 
testing 
set 
training 
set 
testing 
set 
[0, 10) 50.68% 64.30% 10.59% 0 
[10,20) 37.27% 29.50% 27.55% 0 
[20,30) 8.64% 5.40% 26.37% 79.9%
[30,40) 2.31% 0.60% 16.63% 20.1%
40? 1.10% 0.20% 18.86% 0 
 
We define Simple MNP (SMNP) whose 
length is less than 5 words and Complete MNP 
(CMNP) whose length is more than 5 words. 
 We can conclude that the parsing result of CS 
is lower than EDSs from Table 11, which is due 
to the higher complexity of MNP and frame in CS 
compared with EDSs from the results of Table 9 
and Table 10. In addition, we obtain about 1% 
improvement compared with Berkeley parser in 
MNP and Frame parsing result in EDSs from 
Table 11 and Table 12, which indicates that our 
method is effective for short length parsing units. In 
particular, Table 12 shows that our result is 1.8% 
higher than Berkeley parser in the frame parsing of 
CS. Due to the non-consistent frame length 
distribution of training set and testing set in CS 
from Table 10, we find that Berkeley parser largely 
depends on training set compared with our method. 
Table 9: Length distribution of MNP  
 EDSs CS 
length training set 
testing 
set 
training 
set 
testing 
set 
[0,5) 55.30% 62.46% 55.42% 59.45%
[5,10) 32.66% 29.69% 32.57% 30.77%
[10,20) 10.03% 6.75% 10.03% 8.65%
20? 2.00% 1.09% 1.98% 1.12%
 
Table 9 shows the length distribution of MNP 
in training set and testing set of sub-sentence is 
consistent in basic, but the SMNP distribution 
of EDSs is 3.01% less than CS, which 
illuminates the complexity of MNP in CS is 
higher than in EDSs. 
 
 EDSs CS 
length training set 
testing 
set 
training 
set 
testing 
set 
[0,5) 45.84% 47.20% 10.17% 1.00%
[5,10) 43.58% 44.00% 24.14% 10.80%
[10,20) 9.98% 8.70% 41.31% 62.20%
20? 0.60% 0.10% 24.38% 26.00%
To more fairly compare the performance of 
our proposed method, the comparative results 
are shown as Table 13, the first one (Model01) 
is combination method of MNP pre-processing 
and chunk-based, and the chunk-based result 
which adopts CCRFs method with searching 
algorithm; the second one (Berkeley) is the 
parsing result of Berkeley parser; the third one 
(Model02) also is combination method of MNP 
pre-processing and chunk-based, and the chunk-
based result which adopts CCRFs method only; 
and the lase one (Model03) is the chunk-based 
result which adopts CCRFs method with 
searching algorithm. 
Table 10: Length distribution of frame 
 
Table 10 shows the length distribution of frame 
in training set and testing set of EDSs is consistent 
in basic, while the CS is non-consistent. For the 
  
    
 method P R F 
Berkeley 87.5746% 87.8365% 87.7053% 
EDSs 
Proposed Method 88.5752% 88.6341% 88.6047% 
Berkeley 84.4755% 84.9182% 84.6963% CS 
Proposed Method 84.7535% 85.046% 84.8995% 
Table 11: Comparative results of MNP parsing 
 
 method P R F 
Berkeley 91.3411% 91.1823% 91.2617% 
EDSs 
Proposed Method 92.4669% 92.0765% 92.2713% 
Berkeley 85.4388% 85.3023% 85.3705% 
CS 
Proposed Method 87.3357% 87.0357% 87.1854% 
Table12: Comparative results of Frame parsing 
 
 P R F 
Model 01 85.42% 85.35% 85.39%
Berkeley 84.56% 84.62% 84.59%
Models 02 85.31% 85.30% 85.31%
Models 03 83.99% 83.77% 83.88%
Table13: Comparative results of EDSs 
 
dj constituent fj constituent overall F 
P R P R F F F 
Model 01 78.64% 78.73% 78.69% 70.22% 71.62% 70.91% 74.80% 
Berkeley 78.37% 78.16% 78.26% 69.43% 72.42% 70.89% 74.58% 
Models 02 78.18% 78.30% 78.24% 70.20% 70.98% 70.59% 74.41% 
Models 03 77.38% 77.41% 77.39% 70.39% 70.01% 70.24% 73.82% 
Table14: Comparative results of CS 
 
From Table 13, we can see that Model01 
performance in EDSs is improved by 0.08% 
than Model02, and the searching algorithm 
helps little in EDSs analysis. From Table 14, we 
can see that Model01 performance in CS is 
improved by 0.4% than Model02, better than 
Berkeley parser result with search algorism. 
Overall, in EDSs analysis, Model01 
performance is improved by 0.8% than 
Berkeley parser, and in overall F-measure of CS, 
Model01 performance is 0.22% higher than 
Berkeley parser. From Table 13 and 14, We can 
see that Model01 performance in EDSs is 
improved by 1.51% than Model03 and the 
Model01 in CS is improved by 0.98% than 
Model03, and the MNP pre-processing helps. 
5 Conclusions 
We participate in two tasks - EDS Analysis 
and CS Parsing in CLPS-SIGHAN- ParsEval-
2010. We use divide-and-conquer strategy for 
parsing and a chunking-based discriminative 
approach to full parsing by using CRF for 
chunking. As we all know, CRF is effective for  
chunking task. However, the chunking result in 
the current level is based on the upper level in 
the chunking-based parsing approach, which 
will enhance ambiguity problems when the 
input of the current level contains non-terminal 
symbols, therefore, the features used in 
chunking is crucial. This paper, for effectively 
using the information of partial trees that have 
been already created, keeps the terminal 
symbols in the node containing non-terminal 
symbols for features. Our experiments show 
that these features are effective for ambiguity 
problems. 
We suppose that MNP pre-processing before 
statistical model can significantly simplify the 
analysis of complex sentences, which will have 
more satisfatory results compared with using 
statistical model singly. The current results 
show that the MNP pre-processing does 
simplify the complex sentences. However, the 
performance of MNP recognition and the 
parsing of MNP need to be improved, which 
will be our next work. 
References 
Yoshimasa Tsuruoka, Jun?ichi Tsujii, Sophia 
Anaiakou. 2009. Fast Full Parsing by Linear-
Chain Conditional Random Fields. In 
Proceedings of EACL?09, pages 790-798.  
Xiao chen, Changning Huang, Mu li, Chunyu Kit. 
2009. Better Parser Combination. In CIPS-
ParsEval-2009, pages 81-90. 
Abney, S.. 1991. Parsing by chunks, Principle-Based 
Parsing, Kluwer Academic Publishers. 
Erik Tjong, Kim Sang. 2000. Transforming a 
chunker to a parser. In J.Veenstra W.daelemans, 
K Sima? an and J. Zavrek, editors, Computational 
Linguistics in the Netherlands 2000, Rodopi, page 
177-188.  
P.L. Shiuan, C.T.H. Ann. 1996. A Divided-and-
Conquer Strategy for Parsing. In Proc. of the 
ACL/SIGPARSE 5th International Workshop on 
Parsing Technologies. Santa Cruz, USA, 1996, 
pages 57-66 
C. Braun, G. Neumann, J, Piskorski. 2000. A Divide-
and-Conquer Strategy for Shallow Parsing of 
German Free Texts. In Proc. of ANLP-2000. 
Seattle, Washington, 2000, pages 239-246. 
C.Lyon, B.Dickerson. 1997. Reducing the 
Complexity of Parsing by a Method of 
Decomposition International Workshop on 
Parsing Technology, 1997, pages 215-222. 
Qiaoli Zhou, Xin Liu, Xiaona Ren, Wenjing Lang, 
Dongfeng Cai. 2009. Statistical parsing based on 
Maximal Noun Phrase pre-processing. In CIPS-
ParsEval-2209. 
P.L. Shiuan, C.T.H. Ann. A Divide-and-Conquer 
Strategy for Parsing. In: Proc. of the 
ACL/SIGPARSE 5th International Workshop on 
Parsing Technologies. Santa Cruz, USA, 1996. 
57-66. 
C. Braun, G. Neumann, J. Piskorski. A Divide-and-
Conquer Strategy for Shallow Parsing of German 
Free Texts. In: Proc. of ANLP-2000. Seattle, 
Washington, 2000. 239-246. 
C. Lyon, B. Dickerson. Reducing the Complexity of 
Parsing by a Method of Decomposition. 
International Workshop on Parsing Technology. 
1997. 215-222. 
