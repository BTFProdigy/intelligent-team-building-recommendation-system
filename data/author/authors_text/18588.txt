Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1710?1720,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning Biological Processes with Global Constraints
Aju Thalappillil Scaria?, Jonathan Berant?, Mengqiu Wang and Christopher D. Manning
Stanford University, Stanford
Justin Lewis and Brittany Harding
University of Washington, Seattle
Peter Clark
Allen Institute for Artificial Intelligence, Seattle
Abstract
Biological processes are complex phenom-
ena involving a series of events that are re-
lated to one another through various relation-
ships. Systems that can understand and rea-
son over biological processes would dramat-
ically improve the performance of semantic
applications involving inference such as ques-
tion answering (QA) ? specifically ?How??
and ?Why?? questions. In this paper, we
present the task of process extraction, in
which events within a process and the rela-
tions between the events are automatically ex-
tracted from text. We represent processes by
graphs whose edges describe a set of temporal,
causal and co-reference event-event relations,
and characterize the structural properties of
these graphs (e.g., the graphs are connected).
Then, we present a method for extracting rela-
tions between the events, which exploits these
structural properties by performing joint in-
ference over the set of extracted relations.
On a novel dataset containing 148 descrip-
tions of biological processes (released with
this paper), we show significant improvement
comparing to baselines that disregard process
structure.
1 Introduction
A process is defined as a series of inter-related
events that involve multiple entities and lead to an
end result. Product manufacturing, economical de-
velopments, and various phenomena in life and so-
cial sciences can all be viewed as types of processes.
Processes are complicated objects; consider for ex-
ample the biological process of ATP synthesis de-
scribed in Figure 1. This process involves 12 en-
tities and 8 events. Additionally, it describes rela-
tions between events and entities, and the relation-
ship between events (e.g., the second occurrence of
the event ?enter?, causes the event ?changing?).
?Both authors equally contributed to the paper
Automatically extracting the structure of pro-
cesses from text is crucial for applications that re-
quire reasoning, such as non-factoid QA. For in-
stance, answering a question on ATP synthesis, such
as ?How do H+ ions contribute to the production
of ATP?? requires a structure that links H+ ions
(Figure 1, sentence 1) to ATP (Figure 1, sentence
4) through a sequence of intermediate events. Such
?How?? questions are common on FAQ websites
(Surdeanu et al, 2011), which further supports the
importance of process extraction.
Process extraction is related to two recent lines
of work in Information Extraction ? event extrac-
tion and timeline construction. Traditional event ex-
traction focuses on identifying a closed set of events
within a single sentence. For example, the BioNLP
2009 and 2011 shared tasks (Kim et al, 2009; Kim
et al, 2011) consider nine events types related to
proteins. In practice, events are currently almost al-
ways extracted from a single sentence. Process ex-
traction, on the other hand, is centered around dis-
covering relations between events that span multiple
sentences. The set of possible event types in process
extraction is also much larger.
Timeline construction involves identifying tem-
poral relations between events (Do et al, 2012; Mc-
Closky and Manning, 2012; D?Souza and Ng, 2013),
and is thus related to process extraction as both fo-
cus on event-event relations spanning multiple sen-
tences. However, events in processes are tightly cou-
pled in ways that go beyond simple temporal order-
ing, and these dependencies are central for the pro-
cess extraction task. Hence, capturing process struc-
ture requires modeling a larger set of relations that
includes temporal, causal and co-reference relations.
In this paper, we formally define the task of
process extraction and present automatic extraction
methods. Our approach handles an open set of event
types and works over multiple sentences, extract-
ing a rich set of event-event relations. Furthermore,
1710
7/5/13 7:01 PMbrat
Page 1 of 1http://127.0.0.1:8001/index.xhtml#/examples/emnlp2013/p66
H+ ions flowing down their gradient enter a half channel in a stator, which is anchored in the membrane.
H+ ions enter binding sites within a rotor, changing the shape of each subunit so that the rotor spins within the membrane.
Spinning of the rotor causes an internal rod to spin as well.
Turning of the rod activates catalytic sites in the knob that can produce ATP from ADP and P_i.
Entity Event Entity Event Entity Entity
cotemp
prev
Entity Event Entity Entity Event Entity Entity Event Entity
same
causes causes
prev
Event Entity Entity Event
same
same causes
Event Entity Event Entity Event Entity Entity
resultsame
raw-materialcauses causes
1
2
3
4
Figure 1: Partial annotation of the ATP synthesis process. Most of the semantic roles have been removed for simplicity.
we characterize a set of global properties of process
structure that can be utilized during process extrac-
tion. For example, all events in a process are some-
how connected to one another. Also, processes usu-
ally exhibit a ?chain-like? structure reflecting pro-
cess progression over time. We show that incor-
porating such global properties into our model and
performing joint inference over the extracted rela-
tions significantly improves the quality of process
structures predicted. We conduct experiments on a
novel dataset of process descriptions from the text-
book ?Biology? (Campbell and Reece, 2005) that
were annotated by trained biologists. Our method
does not require any domain-specific knowledge and
can be easily adapted to non-biology domains.
The main contributions of this paper are:
1. We define process extraction and characterize
processes? structural properties.
2. We model global structural properties in pro-
cesses and demonstrate significant improve-
ment in extraction accuracy.
3. We publicly release a novel data set of 148
fully annotated biological process descrip-
tions along with the source code for our sys-
tem. The dataset and code can be down-
loaded from http://nlp.stanford.edu/
software/bioprocess/.
2 Process Definition and Dataset
We define a process description as a paragraph or
sequence of tokens x = {x1, . . . x|x|} that describes
a series of events related by temporal and/or causal
relations. For example, in ATP synthesis (Figure 1),
the event of rotor spinning causes the event where
an internal rod spins.
We model the events within a process and their
relations by a directed graph P = (V,E), where
the nodes V = {1, . . . , |V |} represent event men-
tions and labeled edges E correspond to event-event
relations. An event mention v ? V is defined by a
trigger tv, which is a span of words xi, xi+1, . . . , xj ;
and by a set of argument mentions Av, where each
argument mention av ? Av is also a span of words
labeled by a semantic role l taken from a set L. For
example, in the last event mention of ATP synthesis,
tv = produce, and one of the argument mentions is
av = (ATP, RESULT). A labeled edge (u, v, r) in the
graph describes a relation r ? R between the event
mentions u and v. The task of process extraction is
to extract the graph P from the text x.1
A natural way to break down process extraction
into sub-parts is to first perform semantic role label-
ing (SRL), that is, identify triggers and predict ar-
gument mentions with their semantic role, and then
extract event-event relations between pairs of event
mentions. In this paper, we focus on the second
step, where given a set of event triggers T , we find
all event-event relations, where a trigger represents
the entire event. For completeness, we now describe
the semantic roles L used in our dataset, and then
1Argument mentions are also related by coreference rela-
tions, but we neglect that since it is not central in this paper.
1711
present the set of event-event relationsR.
The setL contains standard semantic roles such as
AGENT, THEME, ORIGIN, DESTINATION and LO-
CATION. Two additional semantic roles were em-
ployed that are relevant for biological text: RESULT
corresponds to an entity that is the result of an event,
and RAW-MATERIAL describes an entity that is used
or consumed during an event. For example, the last
event ?produce? in Figure 1, has ?ATP? as the RE-
SULT, and ?ADP? as the RAW-MATERIAL.
The event-event relation set R contains the fol-
lowing (assuming a labeled edge (u, v, r)):
1. PREV denotes that u is an event immediately
before v. Thus, the edges (u, v, PREV) and
(v, w, PREV), preclude the edge (u,w, PREV).
For example, in ?When a photon strikes
. . . energy is passed . . . until it reaches . . . ?,
there is no edge (strikes, reaches, PREV) due
to the intervening event ?passed?.
2. COTEMP denotes that events u and v overlap in
time (e.g., the first two event mentions flowing
and enter in Figure 1).
3. SUPER denotes that event u includes event
v. For instance, in ?During DNA replica-
tion, DNA polymerases proofread each nu-
cleotide. . . ? there is an edge (DNA replication,
proofread, SUPER).
4. CAUSES denotes that event u causes event v
(e.g., the relation between changing and spins
in sentence 2 of Figure 1).
5. ENABLES denotes that event u creates precon-
ditions that allow event v to take place. For
example, the description ?. . . cause cancer cells
to lose attachments to neighboring cells. . . , al-
lowing them to spread into nearby tissues? has
the edge (lose, spread, ENABLES). An in-
tuitive way to think about the difference be-
tween Causes and Enables is the following: if
u causes v this means that if u happens, then
v happens. If u enables v, then if u does not
happen, then v does not happen.
6. SAME denotes that u and v both refer to the
same event (spins and Spinning in Figure 1).
Early work on temporal logic (Allen, 1983) con-
tained more temporal relations than are used in our
Avg Min Max
# of sentences 3.80 1 15
# of tokens 89.98 19 319
# of events 6.20 2 15
# of non-NONE relations 5.64 1 24
Table 1: Process statistics over 148 process descriptions.
NONE is used to indicate no relation.
relation set R. We chose a relation set R that cap-
tures the essential aspects of temporal relations be-
tween events in a process, while keeping the annota-
tion as simple as possible. For instance, we include
the SUPER relation that appears in temporal anno-
tations such as the Timebank corpus (Pustejovsky
et al, 2003) and Allen?s work, but in practice was
not considered by many temporal ordering systems
(Chambers and Jurafsky, 2008; Yoshikawa et al,
2009; Do et al, 2012). Importantly, our relation set
also includes the relations CAUSES and ENABLES,
which are fundamental to modeling processes and
go beyond simple temporal ordering.
We also added event coreference (SAME) to R.
Do et al (2012) used event coreference information
in a temporal ordering task to modify probabilities
provided by pairwise classifiers prior to joint infer-
ence. In this paper, we simply treat SAME as an-
other event-event relation, which allows us to easily
perform joint inference and employ structural con-
straints that combine both coreference and temporal
relations simultaneously. For example, if u and v are
the same event, then there can exist no w, such that
u is before w, but v is after w (see Section 3.3)
We annotated 148 process descriptions based on
the aforementioned definitions. Further details on
annotation and data set statistics are provided in Sec-
tion 4 and Table 1.
Structural properties of processes Coherent pro-
cesses exhibit many structural properties. For ex-
ample, two argument mentions related to the same
event cannot overlap ? a constraint that has been
used in the past in SRL (Toutanova et al, 2008). In
this paper we focus on three main structural prop-
erties of the graph P . First, in a coherent pro-
cess, all events mentioned are related to one another,
and hence the graph P must be connected. Sec-
ond, processes tend to have a ?chain-like? structure
where one event follows another, and thus we expect
1712
Deg. Gold Local Global
0 0 29 0
1 219 274 224
2 369 337 408
3 46 14 17
? 4 22 2 7
Table 2: Node degree distribution for event mentions on
the training set. Predictions for the Local and Global
models were obtained using 10-fold cross validation.
nodes? degree to generally be ? 2. Indeed, 90% of
event mentions have degree ? 2, as demonstrated
by the Gold column of Table 2. Last, if we consider
relations between all possible triples of events in a
process, clearly some configurations are impossible,
while others are common (illustrated in Figure 2).
In Section 3.3, we show that modeling these proper-
ties using a joint inference framework improves the
quality of process extraction significantly.
3 Joint Model for Process Extraction
Given a paragraph x and a trigger set T , we wish
to extract all event-event relations E. Similar to Do
et al (2012), our model consists of a local pairwise
classifier and global constraints. We first introduce
a classifier that is based on features from previous
work. Next, we describe novel features specific for
process extraction. Last, we incorporate global con-
straints into our model using an ILP formulation.
3.1 Local pairwise classifier
The local pairwise classifier predicts relations be-
tween all event mention pairs. In order to model
the direction of relations, we expand the set R to
include the reverse of four directed relations: PREV-
NEXT, SUPER- SUB, CAUSES-CAUSED, ENABLES-
ENABLED. After adding NONE to indicate no rela-
tion, and including the undirected relations COTEMP
and SAME,R contains 11 relations. The classifier is
hence a function f : T ? T ? R. As an example,
f(ti, tj) = PREV iff f(tj , ti) = NEXT. Let n be the
number of triggers in a process, and ti be the i-th
trigger in its description. Since f(ti, tj) completely
determines f(tj , ti), it suffices to consider only pairs
with i < j. Note that the process graph P is undi-
rected under the new definition ofR.
Table 3 describes features from previous
Feature Description
POS Pair of POS tags
Lemma Pair of lemmas
Prep? Preposition lexeme, if in a prepositional phrase
Sent. count Quantized number of sentences between triggers
Word count Quantized number of words between triggers
LCA Least common ancestor on constituency tree, if exists
Dominates? Whether one trigger dominates other
Share Whether triggers share a child on dependency tree
Adjacency Whether two triggers are adjacent
Words btw. For adjacent triggers, content words between triggers
Temp. btw. For adjacent triggers, temporal connectives (from a
small list) between triggers
Table 3: Features extracted for a trigger pair (ti, tj). As-
teriks (*) indicate features that are duplicated, once for
each trigger.
work (Chambers and Jurafsky, 2008; Do et al,
2012) extracted for a trigger pair (ti, tj). Some
features were omitted since they did not yield
improvement in performance on a development set
(e.g., lemmas and part-of-speech tags of context
words surrounding ti and tj), or they require gold
annotations provided in TimeBank, which we do
not have (e.g., tense and aspect of triggers). To
reduce sparseness, we convert nominalizations into
their verbal forms when computing word lemmas,
using WordNet?s (Fellbaum, 1998) derivation links.
3.2 Classifier extensions
A central source of information to extract event-
event relations from text are connectives such as af-
ter, during, etc. However, there is variability in the
occurrence of these connectives as demonstrated by
the following two sentences (connectives in bold-
face, triggers in italics):
1. Because alleles are exchanged during gene flow, ge-
netic differences are reduced.
2. During gene flow, alleles are exchanged, and genetic
differences are hence reduced.
Even though both sentences express the same re-
lation (exchanged, reduced,CAUSES), the connec-
tives used and their linear position with respect to the
triggers are different. Also, in sentence 1, gene flow
intervenes between exchanged and reduced. Since
our dataset is small, we wish to identify the trig-
gers related to each connective, and share features
between such sentences. We do this using the syn-
tactic structure and by clustering the connectives.
1713
tj
ti tk
(a) SAME transitivity
SAMESAME
SAME
tj
ti tk
(b) CAUSE-COTEMP
CAUSES
CAUSES COTEMP
tj
ti tk
(c) COTEMP transitivity
COTEMPCOTEMP
COTEMP / SAME
tj
ti tk
(d) SAME contradiction
PREVPREV
SAME
tj
ti tk
(e) PREV contradiction
PREVPREV
PREV
Figure 2: Relation triangles (a)-(c) are common in the gold standard while (d)-(e) are impossible.
Sentence 1 presents a typical case where by walk-
ing up the dependency tree from the marker because,
we can find the triggers related by this marker:
because
mark
???? exchanged
advcl
???? reduced. When-
ever a trigger is the head of an adverbial clause and
marked by a mark dependency label, we walk on the
dependency tree and look for a trigger in the main
clause that is closest to the root (or the root itself
in this example). By utilizing the syntactic struc-
ture, we can correctly spot that the trigger gene flow
is not related to the trigger exchanged through the
connective because, even though they are linearly
closer. In order to reduce sparseness of connectives,
we created a hand-made clustering of 30 connectives
that maps words into clusters2 (e.g., because, since
and hence to a ?causality? cluster). After locating
the relevant pair of triggers, we use these clusters
to fire the same feature for connectives belonging to
the same cluster. We perform a similar procedure
whenever a trigger is part of a prepositional phrase
(imagine sentence 1 starting with ?due to allele ex-
change during gene flow . . . ?) by walking up the
constituency tree, but details are omitted for brevity.
In sentence 2, the connective hence is an adverbial
modifier of the trigger reduced. We look up the clus-
ter for the connective hence and fire the same feature
for the adjacent triggers exchanged and reduced.
We further extend our features to handle the rich
relation set necessary for process extraction. The
first event of a process is often expressed as a nom-
inalization and includes subsequent events (SUPER
relation), e.g., ?The Calvin cycle begins by incor-
porating...?. To capture this, we add a feature that
fires when the first event of the process description
is a noun. We also add two features targeted at the
2The full set of connectives and their clustering are provided
as part of our publicly released package.
SAME relation: one indicating if the lemmas of ti
and tj are the same, and another specifying the de-
terminer of tj , if it exists. Certain determiners in-
dicate that an event trigger has already been men-
tioned, e.g., the determiner this hints a SAME rela-
tion in ?The next steps decompose citrate back to
oxaloacetate. This regeneration makes . . . ?. Last,
we add as a feature the dependency path between ti
and tj , if it exists, e.g., in ?meiosis produces cells
that divide . . . ?, the feature
dobj
???
rcmod
???? is fired for
the trigger pair produces and divide. In Section 4.1
we empirically show that our extensions to the local
classifier substantially improve performance.
For our pairwise classifier, we train a maximum
entropy classifier that computes a probability pijr
for every trigger pair (ti, tj) and relation r. Hence,
f(ti, tj) = arg maxr pijr.
3.3 Global Constraints
Naturally, pairwise classifiers are local models that
can violate global properties in the process structure.
Figure 3 (left) presents an example for predictions
made by the pairwise classifier, which result in two
triggers (deleted and dupcliated) that are isolated
from the rest of the triggers. In this section, we dis-
cuss how we incorporate constraints into our model
to generate coherent global process structures.
Let ?ijr be the score for a relation r between the
trigger pair (ti, tj) (e.g., ?ijr = log pijr), and yijr be
the corresponding indicator variable. Our goal is to
find an assignment for the indicators y = {yijr | 1 ?
i < j ? n, r ? R}. With no global constraints this
can be formulated as the following ILP:
1714
arg max
y
?
ijr
?ijryijr (1)
s.t.?i,j
?
r
yijr = 1
where the constraint ensures exactly one relation be-
tween each event pair. We now describe constraints
that result in a coherent global process structure.
Connectivity Our ILP formulation for enforcing
connectivity is a minor variation of the one sug-
gested by Martins et al (2009) for dependency pars-
ing. In our setup, we want P to be a connected undi-
rected graph, and not a directed tree. However, an
undirected graph P is connected iff there exists a
directed tree that is a subgraph of P when edge di-
rections are ignored. Thus the resulting formulation
is almost identical and is based on flow constraints
which ensure that there is a path from a designated
root in the graph to all other nodes.
Let R? be the set R \ NONE. An edge (ti, tj) is
in E iff there is some non-NONE relation between
ti and tj , i.e. iff yij :=
?
r?R? yijr is equal to 1.
For each variable yij we define two auxiliary binary
variables zij and zji that correspond to edges of the
directed tree that is a subgraph of P . We ensure that
the edges in the tree exist also in P by tying each
auxiliary variable to its corresponding ILP variable:
?i<j zij ? yij , zji ? yij (2)
Next, we add constraints that ensure that the graph
structure induced by the auxiliary variables is a tree
rooted in an arbitrary node 1 (The choice of root
does not affect connectivity). We add for every i 6= j
a flow variable ?ij which specifies the amount of
flow on the directed edge zij .
?
i
zi1 = 0, ?j 6=1
?
i
zij = 1 (3)
?
i
?1i = n? 1 (4)
?j 6=1
?
i
?ij ?
?
k
?jk = 1 (5)
?i 6=j ?ij ? n ? zij (6)
Equation 3 says that all nodes in the graph have
exactly one parent, except for the root that has no
parents. Equation 4 ensures that the outgoing flow
from the root is n?1, and Equation 5 states that each
of the other n ? 1 nodes consume exactly one unit
of flow. Last, Equation 6 ties the auxiliary variables
to the flow variables, making sure that flow occurs
only on edges. The combination of these constraints
guarantees that the graph induced by the variables
zij is a directed tree and consequently the graph in-
duced by the objective variables y is connected.
Chain structure A chain is a connected graph
where the degree of all nodes is ? 2. Table 2
presents nodes? degree and demonstrates that indeed
process graphs are close to being chains. The fol-
lowing constraint bounds nodes? degree by 2:
?j(
?
i<j
yij +
?
j<k
yjk ? 2) (7)
Since graph structures are not always chains, we
add this as a soft constraint, that is, we penalize the
objective for each node with degree > 2. The chain
structure is one of the several soft constraints we
enforce. Thus, our modified objective function is
?
ijr ?ijryijr +
?
k?K ?kCk, where K is the set of
soft constraints, ?k is the penalty (or reward for de-
sirable structures), and Ck indicates whether a con-
straint is violated (or satisfied). Note that under this
formulation our model is simply a constrained con-
ditional model (wei Chang et al, 2012). The param-
eters ?k are tuned on a development set (see Sec-
tion 4).
Relation triads A relation triad (or a re-
lation triangle) for any three triggers ti, tj
and tk in a process is a 3-tuple of relations
(f(ti, tj), f(tj , tk), f(ti, tk)). Clearly, some triads
are impossible while others are quite common. To
find triads that could improve process extraction, the
frequency of all possible triads in both the training
set and the output of the pairwise classifier were
found, and we focused on those for which the clas-
sifier and the gold standard disagree. We are inter-
ested in triads that never occur in training data but
are predicted by the classifier, and vice versa. Fig-
ure 2 illustrates some of the triads found and Equa-
1715
tions 8-12 provide the corresponding ILP formula-
tions. Equations 8-10 were formulated as soft con-
straints (expanding the setK) and were incorporated
by defining a reward ?k for each triad type.3 On
the other hand, Equations 11-12 were formulated as
hard constraints to prevent certain structures.
1. SAME transitivity (Figure 2a, Eqn. 8): Co-
reference transitivity has been used in past
work (Finkel and Manning, 2008) and we in-
corporate it by a constraint that encourages tri-
ads that respect transitivity.
2. CAUSE-COTEMP (Figure 2b, Eqn. 9): If ti
causes both tj and tk, then often tj and tk are
co-temporal. E.g, in ?genetic drift has led to
a loss of genetic variation and an increase in
the frequency of . . .?, a single event causes two
subsequent events that occur simultaneously.
3. COTEMP transitivity (Figure 2c, Eqn. 10): If
ti is co-temporal with tj and tj is co-temporal
with tk, then usually ti and tk are either co-
temporal or denote the same event.
4. SAME contradiction (Figure 2d, Eqn. 11): If
ti is the same event as tk, then their tempo-
ral ordering with respect to a third trigger tj
may result in a contradiction, e.g., if tj is af-
ter ti, but before tk. We define 5 temporal
categories that generate
(5
2
)
possible contradic-
tions, but for brevity present just one represen-
tative hard constraint. This constraint depends
on prediction of temporal and co-reference re-
lations jointly.
5. PREV contradiction (Figure 2e, Eqn. 12): As
mentioned (Section 3.3), if ti is immediately
before tj , and tj is immediately before tk, then
ti cannot be immediately before tk.
yijSAME + yjkSAME + yikSAME ? 3 (8)
yijCAUSES + yikCAUSES + yjkCOTEMP ? 3 (9)
yijCOTEMP + yjkCOTEMP + yikCOTEMP+
yikSAME ? 3 (10)
yijPREV + yjkPREV + yikSAME ? 2 (11)
yijPREV + yjkPREV ? yikNONE ? 1 (12)
3We experimented with a reward for certain triads or a
penalty for others and empirically found that using rewards re-
sults in better performance on the development set.
We used the Gurobi optimization package4 to
find an exact solution for our ILP, which contains
O(n2|R|) variables and O(n3) constraints. We also
developed an equivalent formulation amenable to
dual decomposition (Sontag et al, 2011), which is a
faster approximation method. But practically, solv-
ing the ILP exactly with Gurobi was quite fast (av-
erage/median time per process: 0.294 sec/0.152 sec
on a standard laptop).
4 Experimental Evaluation
We extracted 148 process descriptions by going
through chapters from the textbook ?Biology? and
marking any contiguous sequence of sentences that
describes a process, i.e., a series of events that lead
towards some objective. Then, each process descrip-
tion was annotated by a biologist. The annotator was
first presented with annotation guidelines and anno-
tated 20 descriptions. The annotations were then
discussed with the authors, after which all process
descriptions were annotated. After training a sec-
ond biologist, we measured inter-annotator agree-
ment ? = 0.69, on 30 random process descriptions.
Process descriptions were parsed with Stanford
constituency and dependency parsers (Klein and
Manning, 2003; de Marneffe et al, 2006), and 35
process descriptions were set aside as a test set
(number of training set trigger pairs: 1932, number
of test set trigger pairs: 906). We performed 10-
fold cross validation over the training set for feature
selection and tuning of constraint parameters. For
each constraint type (connectivity, chain-structure,
and five triad constraints) we introduced a param-
eter and tuned the seven parameters by coordinate-
wise ascent, where for hard constraints a binary pa-
rameter controls whether the constraint is used, and
for soft constraints we attempted 10 different re-
ward/penalty values. For our global model we de-
fined ?ijr = log pijr, where pijr is the probability at
edge (ti, tj) for label r, given by the pairwise clas-
sifier.
We test the following systems: (a) All-Prev: Since
the most common process structure was chain-like,
we simply predict PREV for every two adjacent trig-
gers in text. (b) Localbase: A pairwise classifier with
features from previous work (Section 3.1) (c) Local:
4www.gurobi.com
1716
Temporal Full
P R F1 P R F1
All-Prev 58.4 54.8 56.6 34.1 32.0 33.0
Localbase 61.5 51.8 56.2 52.1 43.9 47.6
Local 63.2 55.7? 59.2 54.7 48.3? 51.3
Chain 64.5 60.5?? 62.4? 56.1 52.6?? 54.3?
Global 63.9 61.4?? 62.6?? 56.2 54.0?? 55.0??
Table 4: Test set results on all experiments. Best number
in each column is bolded. ? and ? denote statistical signif-
icance (p < 0.01) against Localbase and Local baselines,
respectively.
A pairwise classifier with all features (Section 3.2)
(d) Chain: For every two adjacent triggers, choose
the non-NONE relation with highest probability ac-
cording to Local. This baseline heuristically com-
bines our structural assumptions with the pairwise
classifier. We deterministically choose a connected
chain structure, and then use the classifier to label
the edges. (e) Global: Our full model that uses ILP
inference.
To evaluate system performance we compare the
set of predictions on all trigger pairs to the gold stan-
dard annotations and compute micro-averaged pre-
cision, recall and F1. We perform two types of eval-
uations: (a) Full: evaluation on our full set of 11
relations (b) Temporal: Evaluation on temporal re-
lations only, by collapsing PREV, CAUSES, and EN-
ABLES to a single category and similarly for NEXT,
CAUSED, and ENABLED (inter-annotator agreement
? = 0.75). We computed statistical significance
of our results with the paired bootstrap resampling
method of 2000 iterations (Efron and Tibshirani,
1993), where the units resampled are trigger-trigger-
relation triples.
4.1 Results
Table 4 presents performance of all systems. We see
that using global constraints improves performance
almost invariably on all measures in both full and
temporal evaluations. Particularly, in the full eval-
uation Global improves recall by 12% and overall
F1 improves significantly by 3.7 points against Lo-
cal (p < 0.01). Recall improvement suggests that
modeling connectivity allowed Global to add cor-
rect relations in cases where some events were not
connected to one another.
The Local classifier substantially outperforms
Localbase. This indicates that our novel features
(Section 3.2) are important for discriminating be-
tween process relations. Specifically, in the full eval-
uation Local improves precision more than in the
temporal evaluation, suggesting that designing syn-
tactic and semantic features for connectives is useful
for distinguishing PREV, CAUSES, and ENABLES
when the amount of training data is small.
The Chain baseline performs only slightly worse
than our global model. This demonstrates the strong
tendency of processes to proceed linearly from one
event to the other, which is a known property of dis-
course structure (Schegloff and Sacks, 1973). How-
ever, since the structure is deterministically fixed,
Chain is highly inflexible and does not allow any
extensions or incorporation of other structural con-
straints or domain knowledge. Thus, it can be used
as a simple and efficient approximation but is not a
good candidate for a real system. Further support
for the linear nature of process structure is provided
by the All-Prev baseline, which performs poorly in
the full evaluation, but in temporal evaluation works
reasonably well.
Table 2 presents the degree distribution of Local
and Global on the development set comparing to the
gold standard. The degree distribution of Global is
more similar to the gold standard than Local. In par-
ticular, the connectivity constraint ensures that there
are no isolated nodes and shifts mass from nodes
with degree 0 and 1 to nodes with degree 2.
Table 5 presents the order in which constraints
were introduced into the global model using coor-
dinate ascent on the development set. Connectivity
is the first constraint to be introduced, and improves
performance considerably. The chain constraint, on
the other hand, is included third and the improve-
ment in F1 score is relatively smaller. This can be
explained by the distribution of degrees in Table 2
which shows that the predictions of Local does not
have many nodes with degree > 2. As for triad con-
straints, we see that four constraints are important
and are included in the model, but one is discarded.
Last, we examined the results of Global when
macro-averaging over processes, i.e., assigning each
process the same weight by computing recall, pre-
cision and F1 for each process and averaging those
scores. We found that results are quite similar
(with a slight improvement): in the full evalua-
1717
Order Parameter name Value (?) F1 score
? Local model ? 49.9
1 Connectivity constraint ? 51.2
2 SAME transitivity 0.5 52.9
3 Chain constraint -0.5 53.3
4 CAUSE-COTEMP 1.0 53.7
6 PREV contradiction ? 53.8
7 SAME contradiction ? 53.9
Table 5: Order by which constraint parameters were set
using coordinate ascent on the development set. For each
parameter, the value chosen and F1 score after including
the constraint are provided. Negative values correspond
to penalties, positive values to rewards, and a value of?
indicates a hard constraint.
tion Global obtains R/P/F1 of 56.4/55.0/55.7, and
in the temporal evaluation Global obtains R/P/F1 of
63.8/62.3/63.1.
4.2 Qualitative Analysis
Figure 3 shows two examples where global con-
straints corrected the predictions of Local. In Fig-
ure 3, left, Local failed to predict the causal rela-
tions skipped-deleted and used-duplicated, possibly
because they are not in the same sentence and are not
adjacent to one another. By enforcing the connectiv-
ity constraint, Global correctly adds the correct re-
lations and connects deleted and duplicated to the
other triggers in the process.
In Figure 3, right, Local predicts a structure that
results in a ?SAME contradiction? structure. The
triggers bind and binds cannot denote the same event
if a third trigger secrete is temporally between them.
However, Local predicts they are the same event, as
they share a lemma. Global prohibits this structure
and correctly predicts the relation as NONE.
To better understand the performance of Local,
we analyzed the confusion matrix generated based
on its predictions. Although this is a challenging
11-class classification task, most of the mass is con-
centrated on the matrix diagonal, as desired. Error
analysis reveals that 17.5% of all errors are con-
fusions between NONE and PREV, 11.1% between
PREV and CAUSES, and 8.6% between PREV and
COTEMP. This demonstrates that distinguishing the
classes PREV, CAUSES and COTEMP is challenging
for Local. Our current global constraints do not ad-
dress this type of error, and thus an important direc-
tion for future work is to improve the local model.
The global model depends on the predictions of
the local classifier, and so enforcing global con-
straints does not guarantee improvement in perfor-
mance. For instance, if Local produces a graph that
is disconnected (e.g., deleted in Figure 3, left), then
Global will add an edge. However, the label of the
edge is determined by scores computed based on
the local classifier, and if this prediction is wrong,
we will now be penalized for both the false nega-
tive of the correct class (just as before), and also for
the false positive of the predicted class. Despite that
we see that Global improves overall performance by
3.7 F1 points on the test set.
5 Related Work
A related line of work is biomedical event extrac-
tion in recent BioNLP shared tasks (Kim et al,
2009; Kim et al, 2011). Earlier work employed a
pipeline architecture where first events are found,
and then their arguments are identified (Miwa et al,
2010; Bjo?rne et al, 2011). Subsequent methods pre-
dicted events and arguments jointly using Markov
logic (Poon and Vanderwende, 2010) and depen-
dency parsing algorithms (McClosky et al, 2011).
Riedel and McCallum (2011) further improved per-
formance by capturing correlations between events
and enforcing consistency across arguments.
Temporal event-event relations have been ex-
tensively studied (Chambers and Jurafsky, 2008;
Yoshikawa et al, 2009; Denis and Muller, 2011;
Do et al, 2012; McClosky and Manning, 2012;
D?Souza and Ng, 2013), and we leverage such
techniques in our work (Section 3.1). However,
we extend beyond temporal relations alone, and
strongly rely on dependencies between process
events. Chambers and Jurafsky (2011) learned event
templates (or frames), where events that are related
to one another and their semantic roles are extracted.
Recently, Cheung et al (2013) proposed an unsuper-
vised generative model for inducing such templates.
A major difference in our work is that we do not
learn typical event relations from a large and redun-
dant corpus, but are given a paragraph and have a
?one-shot? chance to extract the process structure.
We showed in this paper that global structural
properties lead to significant improvements in ex-
traction accuracy, and ILP is an effective framework
1718
shifts
skippedCAUSESCAUSES
usedCAUSESCAUSES
deletedCAUSESCAUSES
duplicatedCAUSESCAUSES bind
secreteCOTEMPPREV bindsSAMENONE
PREVENABLES
Figure 3: Process graph fragments. Black edges (dotted) are predictions of Local, green (solid) are predictions of
Global, and gold (dashed) are gold standard edges. To reduce clutter, we present the predictions of Global only when
it disagrees with Local. In all other cases, the predictions of Global and Local are identical. Original text, Left: ?... the
template shifts . . . , and a part of the template strand is either skipped by the replication machinery or used twice as a
template. As a result, a segment of DNA is deleted or duplicated.? Right: ?Cells of mating type A secrete a signaling
molecule, which can bind to specific receptor proteins on nearby cells. At the same time, cells secrete factor, which
binds to receptors on A cells.?
for modeling global constraints. Similar observa-
tions and techniques have been proposed in other
information extraction tasks. Reichart and Barzi-
lay (2012) tied information from multiple sequence
models that describe the same event by using global
higher-order potentials. Berant et al (2011) pro-
posed a global inference algorithm to identify entail-
ment relations. There is an abundance of examples
of enforcing global constraints in other NLP tasks,
such as in coreference resolution (Finkel and Man-
ning, 2008), parsing (Rush et al, 2012) and named
entity recognition (Wang et al, 2013).
6 Conclusion
Developing systems that understand process de-
scriptions is an important step towards building ap-
plications that require deeper reasoning, such as bi-
ological process models from text, intelligent tutor-
ing systems, and non-factoid QA systems. In this
paper we have presented the task of process extrac-
tion, and developed methods for extracting relations
between process events. Processes contain events
that are tightly coupled through strong dependen-
cies. We have shown that exploiting these structural
dependencies and performing joint inference over all
event mentions can significantly improve accuracy
over several baselines. We have also released a new
dataset containing 148 fully annotated descriptions
of biological processes. Though the models we built
were trained on biological processes, they do not en-
code domain specific information, and hence should
be extensible to other domains.
In this paper we assumed that event triggers are
given as input. In future work, we want to perform
trigger identification jointly with extraction of event-
event relations. As explained in Section 4.2, the
performance of our system is confined by the per-
formance of the local classifier, which is trained on
relatively small amounts of data. Since data annota-
tion is expensive, it is important to improve the lo-
cal classifier without increasing the annotation bur-
den. For example, one can use unsupervised meth-
ods that learn narrative chains (Chambers and Ju-
rafsky, 2011) to provide some prior on the typical
order of events. Alternatively, we can search on the
web for redundant descriptions of the same process
and use this redundancy to improve classification.
Last, we would like to integrate our method into QA
systems and allow non-factoid questions that require
deeper reasoning to be answered by matching the
questions against the learned process structures.
Acknowledgments
The authors would like to thank Roi Reichart for
fruitful discussion and the anonymous reviewers for
their constructive feedback. This work was partially
funded by Vulcan Inc. The second author was spon-
sored by a Rothschild fellowship.
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26(11):832?843.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Learning entailment relations by global graph
structure optimization. Journal of Computational Lin-
guistics, 38(1).
1719
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2011. Extract-
ing contextualized complex biological events with rich
graph-based feature sets. Computational Intelligence,
27(4):541?557.
Neil Campbell and Jane Reece. 2005. Biology. Ben-
jamin Cummings.
Nathanael Chambers and Daniel Jurafsky. 2008. Jointly
combining implicit constraints improves temporal or-
dering. In Proceedings of EMNLP.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
ACL, pages 976?986.
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of NAACL-HLT.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC.
Pascal Denis and Philippe Muller. 2011. Predicting
globally-coherent temporal structures from texts via
endpoint inference and graph decomposition. In Pro-
ceedings of IJCAI.
Quang Do, Wei Lu, and Dan Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of EMNLP-CoNLL.
Jennifer D?Souza and Vincent Ng. 2013. Classifying
temporal relations with rich linguistic knowledge. In
Proceedings of NAACL-HLT.
Bradley Efron and Robert Tibshirani. 1993. An introduc-
tion to the bootstrap, volume 57. CRC press.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2009. Overview of
BioNLP 09 shared task on event extraction. In Pro-
ceedings of BioNLP.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Junichi Tsujii. 2011. Overview of BioNLP
shared task 2011. In Proceedings of BioNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Andre? L. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL/IJCNLP.
David McClosky and Christopher D. Manning. 2012.
Learning constraints for consistent timeline extraction.
In Proceedings of EMNLP-CoNLL, pages 873?882.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency pars-
ing. In Proceedings of ACL, pages 1626?1635.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010. Event extraction with complex event
classification using rich features. J. Bioinformatics
and Computational Biology, 8(1).
Hoifung Poon and Lucy Vanderwende. 2010. Joint in-
ference for knowledge extraction from biomedical lit-
erature. In Proceedings of HLT-NAACL.
James Pustejovsky, Jose? M. Castan?o, Robert Ingria,
Roser Sauri, Robert J. Gaizauskas, Andrea Setzer,
Graham Katz, and Dragomir R. Radev. 2003.
TimeML: Robust specification of event and temporal
expressions in text. In New Directions in Question An-
swering.
Roi Reichart and Regina Barzilay. 2012. Multi-event ex-
traction guided by global constraints. In Proceedings
of HLT-NAACL.
Sebastian Riedel and Andrew McCallum. 2011. Fast and
robust joint models for biomedical event extraction. In
Proceedings of EMNLP.
Alexander M. Rush, Roi Reichert, Michael Collins, and
Amir Globerson. 2012. Improved parsing and POS
tagging using inter-sentence consistency constraints.
In Proceedings of EMNLP.
Emanuel A Schegloff and Harvey Sacks. 1973. Opening
up closings. Semiotica, 8(4):289?327.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2011. Introduction to dual decomposition for in-
ference. In Suvrit Sra, Sebastian Nowozin, and
Stephen J. Wright, editors, Optimization for Machine
Learning. MIT Press.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2).
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191.
Mengqiu Wang, Wanxiang Che, and Christopher D. Man-
ning. 2013. Effective bilingual constraints for semi-
supervised learning of named entity recognizers. In
Proceedings of AAAI.
Ming wei Chang, Lev Ratinov, and Dan Roth. 2012.
Structured learning with constrained conditional mod-
els. Machine Learning, 88(3):399?431, 6.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with Markov logic. In Proceedings
of ACL/IJCNLP.
1720
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1499?1510,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Modeling Biological Processes for Reading Comprehension
Jonathan Berant
?
, Vivek Srikumar
?
, Pei-Chun Chen, Brad Huang and Christopher D. Manning
Stanford University, Stanford
Abby Vander Linden and Brittany Harding
University of Washington, Seattle
Abstract
Machine reading calls for programs that
read and understand text, but most current
work only attempts to extract facts from
redundant web-scale corpora. In this pa-
per, we focus on a new reading compre-
hension task that requires complex reason-
ing over a single document. The input is
a paragraph describing a biological pro-
cess, and the goal is to answer questions
that require an understanding of the re-
lations between entities and events in the
process. To answer the questions, we first
predict a rich structure representing the
process in the paragraph. Then, we map
the question to a formal query, which is
executed against the predicted structure.
We demonstrate that answering questions
via predicted structures substantially im-
proves accuracy over baselines that use
shallower representations.
1 Introduction
The goal of machine reading is to develop pro-
grams that read text to learn about the world
and make decisions based on accumulated knowl-
edge. Work in this field has focused mostly on
macro-reading, i.e., processing large text collec-
tions and extracting knowledge bases of facts (Et-
zioni et al., 2006; Carlson et al., 2010; Fader et al.,
2011). Such methods rely on redundancy, and are
thus suitable for answering common factoid ques-
tions which have ample evidence in text (Fader et
al., 2013). However, reading a single document
(micro-reading) to answer comprehension ques-
tions that require deep reasoning is currently be-
yond the scope of state-of-the-art systems.
In this paper, we introduce a task where given
a paragraph describing a process, the goal is to
?
Both authors equally contributed to the paper.
answer reading comprehension questions that test
understanding of the underlying structure. In par-
ticular, we consider processes in biology text-
books such as this excerpt and the question that
follows:
?. . . Water is split, providing a source of elec-
trons and protons (hydrogen ions, H
+
) and giv-
ing off O
2
as a by-product. Light absorbed by
chlorophyll drives a transfer of the electrons
and hydrogen ions from water to an acceptor
called NADP
+
. . . ?
Q What can the splitting of water lead to?
a Light absorption
b Transfer of ions
This excerpt describes a process in which a com-
plex set of events and entities are related to one
another. A system trying to answer this ques-
tion must extract a rich structure spanning multi-
ple sentences and reason that water splitting com-
bined with light absorption leads to transfer of
ions. Note that shallow methods, which rely on
lexical overlap or text proximity, will fail. Indeed,
both answers are covered by the paragraph and the
wrong answer is closer in the text to the question.
We propose a novel method that tackles this
challenging problem (see Figure 1). First, we train
a supervised structure predictor that learns to ex-
tract entities, events and their relations describing
the biological process. This is a difficult prob-
lem because events have complex interactions that
span multiple sentences. Then, treating this struc-
ture as a small knowledge-base, we map ques-
tions to formal queries that are executed against
the structure to provide the answer.
Micro-reading is an important aspect of natural
language understanding (Richardson et al., 2013;
Kushman et al., 2014). In this work, we focus
specifically on modeling processes, where events
and entities relate to one another through com-
plex interactions. While we work in the biology
1499
?. . . Water is split, providing a source of elec-
trons and protons (hydrogen ions, H
+
) and
giving off O
2
as a by-product. Light ab-
sorbed by chlorophyll drives a transfer of
the electrons and hydrogen ions from water
to an acceptor called NADP+ . . . ?
Q What can the splitting of water lead to?
a Light absorption
b Transfer of ions
water
split
THEME
absorb
light
THEME
transfer ions
THEME
ENABLE
CAUSE
water
split
absorb
light
THEME
(CAUSE|ENABLE)
+
THEME
water
split
transfer ions
THEME
(CAUSE|ENABLE)
+
THEME
Step 1
Step 2
Step 3: Answer = b
Figure 1: An overview of our reading comprehension system. First, we predict a structure from the input paragraph (the
top right portion shows a partial structure skipping some arguments for brevity). Circles denote events, squares denote argu-
ments, solid arrows represent event-event relations, and dashed arrows represent event-argument relations. Second, we map
the question paired with each answer into a query that will be answered using the structure. The bottom right shows the query
representation. Last, the two queries are executed against the structure, and a final answer is returned.
domain, processes are abundant in domains such
as chemistry, economics, manufacturing, and even
everyday events like shopping or cooking, and our
model can be applied to these domains as well.
The contributions of this paper are:
1. We propose a reading comprehension task
which requires deep reasoning over struc-
tures that represent complex relations be-
tween multiple events and entities.
2. We present PROCESSBANK, a new dataset
consisting of descriptions of biological pro-
cesses, fully-annotated with rich process
structures, and accompanied by multiple-
choice questions.
3. We present a novel method for answer-
ing questions, by predicting process struc-
tures and mapping questions to queries. We
demonstrate that by predicting structures we
can improve reading comprehension accu-
racy over baselines that do not exploit the un-
derlying structure.
The data and code for this paper are avail-
able at http://www-nlp.stanford.edu/
software/bioprocess.
2 Task Definition and Setup
This section describes the reading comprehension
task we address and the accompanying dataset.
We will use the example in Figure 1 as our run-
ning example throughout the paper.
Our goal is to tackle a complex reading com-
prehension setting that centers on understanding
the underlying meaning of a process description.
We target a multiple-choice setting in which each
input consists of a paragraph of text describing a
biological process, a question, and two possible
answers. The goal is to identify the correct answer
using the text (Figure 1, left). We used the 148
paragraphs from the textbook Biology (Campbell
and Reece, 2005) that were manually identified by
Scaria et al. (2013). We extended this set to 200
paragraphs by including additional paragraphs that
describe biological processes. Each paragraph in
the collection represents a single biological pro-
cess and describes a set of events, their partici-
pants and their interactions.
Because we target understanding of paragraph
meaning, we use the following desiderata for
building the corpus of questions and answers:
1. The questions should focus on the events and
entities participating in the process described
in the paragraph, and answering the questions
should require reasoning about the relations
between those events and entities.
2. Both answers should have similar lexical
overlap with the paragraph. Moreover, names
of entities and events in the question and an-
swers should appear as in the paragraph and
not using synonyms. This is to ensure that the
task revolves around reading comprehension
rather than lexical variability.
1
A biologist created the question-answer part of
1
Lexical variability is an important problem in NLP, but
is not the focus of this task.
1500
the corpus comprising of 585 questions spread
over the 200 paragraphs. A second annotator val-
idated 326 randomly chosen questions and agreed
on the correct answer with the first annotator in
98.1% of cases. We provide the annotation guide-
lines in the supplementary material.
Figure 1 (left) shows an excerpt of a paragraph
describing a process and an example of a ques-
tion based on it. In general, questions test an un-
derstanding of the interactions between multiple
events (such as causality, inhibition, temporal or-
dering), or between events and entities (i.e., roles
of entities in events), and require complex reason-
ing about chains of event-event and event-entity
relations.
3 The Structure of Processes
A natural first step for answering reading compre-
hension questions is to identify a structured rep-
resentation of the text. In this section, we define
this structure. We broadly follow the definition of
Scaria et al. (2013), but modify important aspects,
highlighted at the end of this section.
A paragraph describing a process is a sequence
of tokens that describes events, entities and their
relations (see Figure 1, top right). A process is
a directed graph (T ,A, E
tt
, E
ta
), where the nodes
T are labeled event triggers, the nodes A are ar-
guments, E
tt
are labeled edges describing event-
event relations, and E
ta
are labeled edges from
triggers to arguments denoting semantic roles (see
Figure 1 top right for a partial structure of the run-
ning example). The goal of process extraction is
to generate the process graph given the input para-
graph.
Triggers and arguments A trigger is a token
span denoting the occurrence of an event. In Fig-
ure 1, split, absorbed and transfer are event trig-
gers. In rare cases, a trigger denotes the non-
occurrence of an event. For example, in ?sym-
patric speciation can occur when gene flow is
blocked?, sympatric speciation occurs if gene flow
does not happen. Thus, nodes in T are labeled as
either a T-YES or T-NO to distinguish triggers of
events that occur from triggers of events that do
not occur. Arguments are token spans denoting
entities that participate in the process (such as wa-
ter, light and ions in Figure 1).
Semantic roles The edges E
ta
from triggers
to arguments are labeled by the semantic roles
AGENT, THEME, SOURCE, DESTINATION, LO-
CATION, RESULT, and OTHER for all other roles.
Our running example shows three THEME seman-
tic roles for the three triggers. For brevity, the fig-
ure does not show the RESULT of the event split,
namely, both source of electrons and protons (hy-
drogen ions, H
+
) and O
2
.
Event-event relations The directed edges E
tt
between triggers are labeled by one of eight pos-
sible event-event relations. These relations are
central to answering reading comprehension ques-
tions, which test understanding of the depen-
dencies and causal relations between the process
events. We first define three relations that express
a dependency between two event triggers u and v.
1. CAUSE denotes that u starts before v, and if
u happens then v happens (Figure 1).
2. ENABLE denotes that u creates conditions
necessary for the occurrence of v. This
means that u starts before v and v can only
happen if u happens (Figure 1).
2
3. PREVENT denotes that u starts before v and
if u happens, then v does not happen.
In processes, events sometimes depend on more
than one other event. For example, in Figure 1
(right top) transfer of ions depends on both water
splitting as well as light absorption. Conversely,
in Figure 2, the shifting event results in either one
of two events but not both. To express both con-
junctions and disjunctions of related events we
add the relations CAUSE-OR, ENABLE-OR and
PREVENT-OR, which express disjunctions, while
the default CAUSE, ENABLE, and PREVENT ex-
press conjunction (Compare the CAUSE-OR rela-
tions in Figure 2 with the relations in Figure 1).
We define the SUPER relation to denote that
event u is part of event v. (In Figure 2, slip-
page is a sub-event of replication.) Last, we use
the event coreference relation SAME to denote two
event mentions referring to the same event.
Notice that the assignments of relation labels in-
teract across different pairs of events. As an ex-
ample, if event u causes event v, then v can not
cause u. Our inference algorithm uses such struc-
tural constraints when predicting process structure
(Section 4).
2
In this work, we do not distinguish causation from facil-
itation, where u can help v but is not absolutely required. We
instructed the annotators to ignore the inherent uncertainty in
these cases and use CAUSE.
1501
Figure 2: Partial example of a process, as annotated in our dataset.
Avg Min Max
# of triggers 7.0 2 18
# of arguments 11.3 1 36
# of relation 7.9 1 37
Table 1: Statistics of triggers, arguments and rela-
tions over the 200 annotated paragraphs.
Three biologists annotated the same 200 para-
graphs described in Section 2 using the brat anno-
tation tool (Stenetorp et al., 2012). For each para-
graph, one annotator annotated the process, and
a second validated its correctness. Importantly,
the questions and answers were authored sepa-
rately by a different annotator, thus ensuring that
the questions and answers are independent from
the annotated structures. Table 1 gives statistics
over the dataset. The annotation guidelines are in-
cluded in the supplementary material.
Relation to Scaria et al. (2013) Scaria et al.
(2013) also defined processes as graphs where
nodes are events and edges describe event-event
relations. Our definition differs in a few important
aspects.
First, the set of event-event relations in that
work included temporal relations in addition to
causal ones. In this work, we posit that because
events in a process are inter-related, causal depen-
dencies are sufficient to capture the relevant tem-
poral ordering between them. Figure 1 illustrates
this phenomenon, where the temporal ordering be-
tween the events of water splitting and light ab-
sorption is unspecified. It does not matter whether
one happens before, during, or after the other. Fur-
thermore, the incoming causal links to transfer im-
ply that the event should happen after splitting and
absorption.
A second difference is that Scaria et al. (2013)
do not include disjunctions and conjunctions of
events in their formulation. Last, Scaria et al.
(2013) predict only relations given input triggers,
while we predict a full process structure.
4 Predicting Process Structures
We now describe the first step of our algorithm.
Given an input paragraph we predict events, their
arguments and event-event relations (Figure 1,
top). We decompose this into three sub-problems:
1. Labeling trigger candidates using a multi-
class classifier (Section 4.1).
2. For each trigger, identifying an over-
complete set of possible arguments, using a
classifier tuned for high recall (Section 4.2).
3. Jointly assigning argument labels and rela-
tion labels for all trigger pairs (Section 4.3).
The event-event relations CAUSE, ENABLE,
CAUSE-OR and ENABLE-OR, form a semantic
cluster: If (u, v) is labeled by one of these, then
the occurrence of v depends on the occurrence of
u. Since our dataset is small, we share statistics by
collapsing all four labels to a single ENABLE la-
bel. Similarly, we collapse the PREVENT and
PREVENT-OR labels, overall reducing the number
of relations to four.
For brevity, in what follows we only provide
a flavor of the features we extract, and refer the
reader to the supplementary material for details.
4.1 Predicting Event Triggers
The first step is to identify the events in the pro-
cess. We model the trigger detector as a multi-
class classifier that labels all content words in
the paragraph as one of T-YES, T-NO or NOT-
TRIGGER (Recall that a word can trigger an event
that occurred, an event that did not occur, or not
be a trigger at all). For simplicity, we model trig-
gers as single words, but in the gold annotation
about 14% are phrases (such as gene flow). Thus,
we evaluate trigger prediction by taking heads of
gold phrases. To train the classifier, we extract
1502
the lemma and POS tag of the word and adja-
cent words, dependency path to the root, POS
tag of children and parent in the dependency tree,
and clustering features from WordNet (Fellbaum,
1998), Nomlex (Macleod et al., 1998), Levin verb
classes (Levin, 1993), and a list of biological pro-
cesses compiled from Wikipedia.
4.2 Filtering Argument Candidates
Labeling trigger-argument edges is similar to se-
mantic role labeling. Following the standard ap-
proach (Punyakanok et al., 2008), for each trigger
we collect all constituents in the same sentence to
build an over-complete set of plausible candidate
arguments. This set is pruned with a binary classi-
fier that is tuned for high recall (akin to the argu-
ment identifier in SRL systems). On the develop-
ment set we filter more than half of the argument
candidates, while achieving more than 99% recall.
This classifier is trained using argument identifica-
tion features from Punyakanok et al. (2008).
At the end of this step, each trigger has a set of
candidate arguments which will be labeled during
joint inference. In further discussion, the argument
candidates for trigger t are denoted by A
t
.
4.3 Predicting Arguments and Relations
Given the output of the trigger classifier, our goal
is to jointly predict event-argument and event-
event relations. We model this as an integer linear
program (ILP) instance described below. We first
describe the inference setup assuming a model that
scores inference decisions and defer description of
learning to Section 4.4. The ILP has two types of
decision variables: arguments and relations.
Argument variables These variables capture
the decision that a candidate argument a, belong-
ing to the set A
t
of argument candidates, takes a
label A (from Section 3). We denote the Boolean
variables by y
t,a,A
, which are assigned a score
b
t,a,A
by the model. We include an additional label
NULL-ARG, indicating that the candidate is not an
argument for the trigger.
Event-event relation variables These variables
capture the decision that a pair of triggers t
1
and
t
2
are connected by a directed edge (t
1
, t
2
) labeled
by the relation R. We denote these variables by
z
t
1
,t
2
,R
, which are associated with a score c
t
1
,t
2
,R
.
Again, we introduce a label NULL-REL to indicate
triggers that are not connected by an edge.
Name Description
Unique labels Every argument candidate and trigger pair has ex-
actly one label.
Argument overlap Two arguments of the same trigger cannot overlap.
Relation symmetry The SAME relation is symmetric. All other rela-
tions are anti-symmetric, i.e., for any relation la-
bel other than SAME, at most one of (t
i
, t
j
) or
(t
j
, t
i
) can take that label and the other is assigned
the label NULL-REL.
Max arguments per
trigger
Every trigger can have no more than two arguments
with the same label.
Max triggers per ar-
gument
The same span of text can not be an argument for
more than two triggers.
Connectivity The triggers must form a connected graph, framed
as flow constraints as in Magnanti and Wolsey
(1995) and Martins et al. (2009).
Shared arguments If the same span of text is an argument of two trig-
gers, then the triggers must be connected by a rela-
tion that is not NULL-REL. This ensures that trig-
gers that share arguments are related.
Unique parent For any trigger, at most one outgoing edge can be
labeled SUPER.
Table 2: Constraints for joint inference.
Formulation Given the two sets of variables,
the objective of inference is to find a global as-
signment that maximizes the score. That is, the
objective can be stated as follows:
max
y,z
?
t,a?A
t
,A
b
t,a,A
? y
t,a,A
+
?
t
1
,t
2
,R
c
t
1
,t
2
,R
? z
t
1
,t
2
,R
Here, y and z refer to all the argument and rela-
tion variables respectively.
Clearly, all possible assignments to the infer-
ence variables are not feasible and there are both
structural as well as prior knowledge constraints
over the output space. Table 2 states the con-
straints we include, which are expressed as linear
inequalities over output variables using standard
techniques (e.g., (Roth and Yih, 2004)).
4.4 Learning in the Joint Model
We train both the trigger classifier and the argu-
ment identifier using L
2
-regularized logistic re-
gression. For the joint model, we use a linear
model for the scoring functions, and train jointly
using the structured averaged perceptron algo-
rithm (Collins, 2002).
Since argument labeling is similar to semantic
role labeling (SRL), we extract standard SRL fea-
tures given the trigger and argument from the syn-
tactic tree for the corresponding sentence. In ad-
dition, we add features extracted from an off-the-
shelf SRL system. We also include all feature con-
junctions. For event relations, we include the fea-
tures described in Scaria et al. (2013), as well as
context features for both triggers, and the depen-
dency path between them, if one exists.
1503
5 Question Answering via Structures
This section describes our question answering sys-
tem that, given a process structure, a question and
two answers, chooses the correct answer (steps 2
and 3 in Figure 1).
Our strategy is to treat the process structure as
a small knowledge-base. We map each answer
along with the question into a structured query that
we compare against the structure. The query can
prove either the correctness or incorrectness of the
answer being considered. That is, either we get a
valid match for an answer (proving that the cor-
responding answer is correct), or we get a refu-
tation in the form of a contradicted causal chain
(thus proving that the other answer is correct).
This is similar to theorem proving approaches sug-
gested in the past for factoid question answering
(Moldovan et al., 2003).
The rest of this section is divided into three
parts: Section 5.1 defines the queries we use, Sec-
tion 5.2 describes a rule-based algorithm for con-
verting a question and an answer into a query and
finally, 5.3 describes the overall algorithm.
5.1 Queries over Processes
We model a query as a directed graph path with
regular expressions over edge labels. The bot-
tom right portion of Figure 1 shows examples of
queries for our running example. In general, given
a question and one of the answer candidates, one
end of the path is populated by a trigger/argument
found in the question and the other is populated
with a trigger/ argument from the answer.
We define a query to consist of three parts:
1. A regular expression over relation labels, de-
scribing permissible paths,
2. A source trigger/argument node, and
3. A target trigger/argument node.
For example, the bottom query in Figure 1 looks
for paths labeled with CAUSE or ENABLE edges
from the event split to the event transfer.
Note that the representation of questions as di-
rected paths is a modeling choice and did not influ-
ence the authoring of the questions. Indeed, while
most questions do fit this model, there are rare
cases that require a more complex query structure.
5.2 Query Generation
Mapping a question and an answer into a query
involves identifying the components of the query
listed above. We do this in two phases: (1) In the
alignment phase, we align triggers and arguments
in the question and answer to the process structure
to give us candidate source and target nodes. (2)
In the query construction phase, we identify the
regular expression and the direction of the query
using the question, the answer and the alignment.
We identify three broad categories of QA pairs
(see Table 3) that can be identified using simple
lexical rules: (a) Dependency questions ask which
event or argument depends on another event or ar-
gument, (b) Temporal questions ask about tempo-
ral ordering of events, and (c) True-false questions
ask whether some fact is true. Below, we describe
the two phases of query generation primarily in the
context of dependency questions with a brief dis-
cussion about temporal and true-false questions at
the end of the section.
Alignment Phase We align triggers in the struc-
ture to the question and the answer by matching
lemmas or nominalizations. In case of multiple
matches, we use the context to disambiguate and
resolve ties using the highest matching candidate
in the syntactic dependency tree.
We align arguments in the question and the an-
swer in a similar manner. Since arguments are
typically several words long, we prefer maximal
spans. Additionally, if a question (or an answer)
contains an aligned trigger, we prefer to align
words to its arguments.
Query Construction Phase We construct a
query using the aligned question and answer trig-
gers/arguments. We will explain query construc-
tion using our running example (reproduced as the
dependency question in Table 3).
First, we identify the source and the target of
the query. We select either the source or the tar-
get to be a question node and populate the other
end of the query path with an answer node. To
make the choice between source or target for the
question node, we use the main verb in the ques-
tion, its voice and relative position of the question
word with respect to the main verb. In our exam-
ple, the main verb lead to is in active voice and the
question word what is not in subject position. This
places the trigger from the question as the source
of the query path (see both queries in the bottom
right portion of the running example). In contrast,
had the verb been require, the trigger would be the
target of the query. We construct two verb clusters
that indicate query direction using a small seed set
1504
Type Example # (%)
Dependency Q: What can the splitting of water lead to? 407 (69.57%)
a: Light absorption
b: Transfer of ions
Temporal Q: What is the correct order of events? 57 (9.74%)
a: PDGF binds to tyrosine kinases, then cells divide, then wound healing
b: Cells divide, then PDGF binds to tyrosine kinases, then wound healing
True-False Q: Cdk associates with MPF to become cyclin 121 (20.68%)
a: True
b: False
Table 3: Examples and statistics for each of the three coarse types of questions.
Is main verb trigger?
Condition Regular Exp.
Wh- word subjective? AGENT
Wh- word object? THEME
Condition Regular Exp.
default (ENABLE|SUPER)
+
DIRECT (ENABLE|SUPER)
PREVENT (ENABLE|SUPER)
?
PREVENT(ENABLE|SUPER)
?
Yes No
Figure 3: Rules for determining the regular expressions for queries concerning two triggers. In each table, the condition
column decides the regular expression to be chosen. In the left table, we make the choice based on the path from the root to
the Wh- word in the question. In the right table, if the word directly modifies the main trigger, the DIRECT regular expression
is chosen. If the main verb in the question is in the synset of prevent, inhibit, stop or prohibit, we select the PREVENT regular
expression. Otherwise, the default one is chosen. We omit the relation label SAME from the expressions, but allow going
through any number of edges labeled by SAME when matching expressions to the structure.
that we expand using WordNet.
The final step in constructing the query is to
identify the regular expression for the path con-
necting the source and the target. Due to paucity
of data, we do not map a question and an answer
to arbitrary regular expressions. Instead, we con-
struct a small set of regular expressions, and build
a rule-based system that selects one. We used the
training set to construct the regular expressions
and we found that they answer most questions (see
Section 6.4). We determine the regular expression
based on whether the main verb in the sentence is
a trigger and whether the source and target of the
path are triggers or arguments. Figure 3 shows the
possible regular expressions and the procedure for
choosing one when both the source and target are
triggers. If either of them are argument nodes, we
append the appropriate semantic role to the regu-
lar expression, based on whether the argument is
the source or the target of the path (or both).
True-false questions are treated similarly, ex-
cept that both source and target are chosen from
the question. For temporal questions, we seek to
identify the ordering of events in the answers. We
use the keywords first, then, or simultaneously to
identify the implied order in the answer. We use
the regular expression SUPER
+
for questions ask-
ing about simultaneous events and ENABLE
+
for
those asking about sequential events.
5.3 Answering Questions
We match the query of an answer to the process
structure to identify the answer. In case of a match,
the corresponding answer is chosen. The matching
path can be thought of as a proof for the answer.
If neither query matches the graph (or both do),
we check if either answer contradicts the struc-
ture. To do so, we find an undirected path from
the source to the target. In the event of a match, if
the matching path traverses any ENABLE edge in
the incorrect direction, we treat this as a refutation
for the corresponding answer and select the other
one. In our running example, in addition to the
valid path for the second query, for the first query
we see that there is an undirected path from split
to absorb through transfer that matches the first
query. This tells us that light absorption cannot
be the answer because it is not along a causal path
from split.
Finally, if none of the queries results in a match,
we look for any unlabeled path between the source
and the target, before backing off to a dependency-
based proximity baseline described in Section 6.
When there are multiple aligning nodes in the
question and answer, we look for any proof or
refutation before backing off to the baselines.
1505
6 Empirical Evaluation
In this section we aim to empirically evaluate
whether we can improve reading comprehension
accuracy by predicting process structures. We first
provide details of the experimental setup.
6.1 Experimental setup
We used 150 processes (435 questions) for train-
ing and 50 processes (150 questions) as the test
set. For development, we randomly split the train-
ing set 10 times (80%/20%), and tuned hyper-
parameters by maximizing average accuracy on
question answering. We preprocessed the para-
graphs with the Stanford CoreNLP pipeline ver-
sion 3.4 (Manning et al., 2014) and Illinois SRL
(Punyakanok et al., 2008; Clarke et al., 2012). We
used the Gurobi optimization package
3
for infer-
ence.
We compare our system PROREAD to baselines
that do not have access to the process structure:
1. BOW: For each answer, we compute the
proportion of content word lemmas covered
by the paragraph and choose the one with
higher coverage. For true-false questions, we
compute the coverage of the question state-
ment, and answer ?True? if it is higher than a
threshold tuned on the development set.
2. TEXTPROX: For dependency questions, we
align content word lemmas in both the ques-
tion and answer against the text and select the
answer whose aligned tokens are closer to the
aligned tokens of the question. For tempo-
ral questions, we return the answer for which
the order of events is identical to their order
in the paragraph. For true-false questions, we
return ?True? if the number of bigrams from
the question covered in the text is higher than
a threshold tuned on the development set.
3. SYNTPROX: For dependency questions, we
use proximity as in TEXTPROX, except that
distance is measured using dependency tree
edges. To support multiple sentences we con-
nect roots of adjacent sentences with bidi-
rectional edges. For temporal questions this
baseline is identical to TEXTPROX. For true-
false questions, we compute the number of
dependency tree edges in the question state-
ment covered by edges in the paragraph (an
edge has a source lemma, relation, and target
lemma), and answer ?True? if the coverage is
3
http://www.gurobi.com/
Method Depen. Temp. True-
false
All
PROREAD 68.1 80.0 55.6 66.7
SYNTPROX 61.9 70.0 48.1 60.0
TEXTPROX 58.4 70.0 33.3 54.7
BOW 47.8 40.0 44.4 46.7
GOLD 77.9 80.0 70.4 76.7
Table 4: Reading comprehension test set accuracy. The All
column shows overall accuracy across all questions. The first
three columns show accuracy for each coarse type.
higher than a threshold tuned on the training
set.
To separate the contribution of process struc-
tures from the performance of our structure pre-
dictor, we also run our QA system given manually
annotated gold standard structures (GOLD).
4
6.2 Reading Comprehension Task
We evaluate our system using accuracy, i.e., the
proportion of questions answered correctly. Ta-
ble 4 presents test set results, where we break
down questions by their coarse-type.
PROREAD improves accuracy compared to the
best baseline by 6.7 absolute points (last column).
Most of the gain is due to improvement on de-
pendency questions, which are the most common
question type. The performance of BOW indicates
that lexical coverage alone does not distinguish the
correct answer from the wrong answer. In fact,
guessing the answer with higher lexical overlap
results in performance that is slightly lower than
random. Text proximity and syntactic proximity
provide a stronger cue, but exploiting predicted
process structures substantially outperforms these
baselines.
Examining results using gold information high-
lights the importance of process structures inde-
pendently of the structure predictor. Results of
GOLD demonstrate that given gold structures we
can obtain a dramatic improvement of almost 17
points compared to the baselines, using our sim-
ple deterministic QA system.
Results on true-false questions are low for
PROREAD and all the baselines. True-false ques-
tions are harder for two main reasons. First, in
dependency and temporal questions, we create a
query for both answers, and can find a proof or
a refutation for either one of them. In true-false
4
We also ran an experiment where gold triggers are
given and arguments and relations are predicted. We found
that this results in slightly higher performance compared to
PROREAD.
1506
Precision Recall F
1
Triggers 75.4 73.9 74.6
Arguments 43.4 34.4 38.3
Relations 27.0 22.5 24.6
Table 5: Structured prediction test set results.
questions we must determine given a single state-
ment whether it holds. Second, an analysis of true-
false questions reveals that they focus less on re-
lations between events and entities in the process,
and require modeling lexical variability.
5
6.3 Structure Prediction Task
Our evaluation demonstrates that gold structures
improve accuracy substantially more than pre-
dicted structures. To examine this, we now di-
rectly evaluate the structure predictor by com-
paring micro-average precision, recall and F
1
be-
tween predicted and gold structures (Table 5).
While performance for trigger identification is
reasonable, performance on argument and relation
prediction is low. This explains the higher perfor-
mance obtained in reading comprehension given
gold structures. Note that errors in trigger predic-
tion propagate to argument and relation prediction
? a relation cannot be predicted correctly if either
one of the related triggers is not previously identi-
fied. One reason for low performance is the small
size of the dataset. Thus, training process predic-
tors with less supervision is an important direction
for future work. Furthermore, the task of process
prediction is inherently difficult, because often re-
lations are expressed only indirectly in text. For
example, in Figure 1 the relation between water
splitting and transfer of ions is only recoverable
by understanding that water provides the ions that
need to be transferred.
Nevertheless, we find that questions can often
be answered correctly even if the structure con-
tains some errors. For example, the gold structure
for the sentence ?Some . . . radioisotopes have
long half-lives, allowing . . . ?, contains the trigger
long half-lives, while we predict have as a trigger
and long half-lives as an argument. This is good
enough to answer questions related to this part of
the structure correctly, and overall, to improve per-
formance using predicted structures.
5
The low performance of TEXTPROX and SYNTPROX on
true-false questions can also be attributed to the fact that we
tuned a threshold parameter on the training set, and this did
not generalize well to the test set.
Reason GOLD PROREAD
Alignment 35% 15%
Missing from annotation 25% 10%
Entity coreference 20% 10%
Missing regular expression 10%
Lexical variability 5% 10%
Error in predicted structure 55%
Other 5%
Table 6: Error analysis results. An explanation of the vari-
ous categories are in the body of the paper.
6.4 Error Analysis
This section presents the results of an analysis of
20 sampled errors of GOLD (gold structures), and
20 errors of PROREAD (predicted structures). We
have categorized the primary reason for error in
Table 6.
As expected, the main problem when using pre-
dicted structures, is structure errors which account
for more than half of the errors.
Errors in GOLD are distributed across various
categories, which we briefly describe. Alignment
errors occur due to multiple words aligning to mul-
tiple triggers and arguments. For example, in the
question ?What is the result of gases being pro-
duced in the lysosome??, the answer ?engulfed
pathogens are poisoned? is incorrectly aligned to
the trigger engulfed rather than to poisoned.
Another reason for errors are cases where ques-
tions are asked about parts of the paragraph that
are missing from annotation. This is possible since
questions were authored independently of struc-
ture annotation. Two other causes for errors are
entity coreference errors, where a referent for an
entity is missing from the structure, and lexical
variability, where the author of questions uses
names for triggers or arguments that are missing
from the paragraph, and so alignment fails.
Last, in 10% of the cases in GOLD we found
that the answer could not be retrieved using the set
of regular expressions that are currently used by
our QA system.
7 Discussion
This work touches on several strands of work in
NLP including information extraction, semantic
role labeling, semantic parsing and reading com-
prehension.
Event and relation extraction have been studied
via the ACE data (Doddington et al., 2004) and
related work. The BioNLP shared tasks (Kim et
al., 2009; Kim et al., 2011; Riedel and McCal-
1507
lum, 2011) focused on biomedical data to extract
events and their arguments. Event-event relations
have been mostly studied from the perspective of
temporal ordering; e.g., (Chambers and Jurafsky,
2008; Yoshikawa et al., 2009; Do et al., 2012; Mc-
Closky and Manning, 2012). The process struc-
ture predicted in this work differs from these lines
of work in two important ways: First, we predict
events, arguments and their interactions from mul-
tiple sentences, while most earlier work focused
on one or two of these components. Second, we
model processes, and thus target causal relations
between events, rather than temporal order only.
Our semantic role annotation is similar to ex-
isting SRL schemes such as PropBank (Palmer et
al., 2005), FrameNet (Ruppenhofer et al., 2006)
and BioProp (Chou et al., 2006). However, in con-
trast to PropBank and FrameNet, we do not allow
all verbs to trigger events and instead let the an-
notators decide on biologically important triggers,
which are not restricted to verbs (unlike BioProp,
where 30 pre-specified verbs were selected for an-
notation). Like PropBank and BioProp, the argu-
ment labels are not trigger specific.
Mapping questions to queries is effectively a se-
mantic parsing task. In recent years, several lines
of work addressed semantic parsing using vari-
ous formalisms and levels of supervision (Zettle-
moyer and Collins, 2005; Wong and Mooney,
2006; Clarke et al., 2010; Berant et al., 2013).
In particular, Krishnamurthy and Kollar (2013)
learned to map natural language utterances to ref-
erents in an image by constructing a KB from the
image and then mapping the utterance to a query
over the KB. This is analogous to our process of
constructing a process structure and performing
QA by querying that structure. In our work, we
parse questions into graph-based queries, suitable
for modeling processes, using a rule-based heuris-
tic. Training a statistical semantic parser that will
replace the QA system is an interesting direction
for future research.
Multiple choice reading comprehension tests
are a natural choice for evaluating machine read-
ing. Hirschman et al. (1999) presented a bag-of-
words approach to retrieving sentences for read-
ing comprehension. Richardson et al. (2013) re-
cently released the MCTest reading comprehen-
sion dataset that examines understanding of fic-
tional stories. Their work shares our goal of ad-
vancing micro-reading, but they do not focus on
process understanding.
Developing programs that perform deep reason-
ing over complex descriptions of processes is an
important step on the road to fulfilling the higher
goals of machine reading. In this paper, we present
an end-to-end system for reading comprehen-
sion of paragraphs which describe biological pro-
cesses. This is, to the best of our knowledge, the
first system to both predict a rich structured rep-
resentation that includes entities, events and their
relations, and utilize this structure for answering
reading comprehension questions. We also created
a new dataset, PROCESSBANK, which contains
200 paragraphs that are both fully-annotated with
process structure, as well as accompanied by ques-
tions. We empirically demonstrated that model-
ing biological processes can substantially improve
reading comprehension accuracy in this domain.
Acknowledgments
The authors would like to thank Luke
Amuchastegui for authoring the multiple-choice
questions, and also the anonymous reviewers for
their constructive feedback. We thank the Allen
Institute for Artificial Intelligence for assistance
in funding this work.
References
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of EMNLP.
Neil Campbell and Jane Reece. 2005. Biology. Ben-
jamin Cummings.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of AAAI.
Nathanael Chambers and Daniel Jurafsky. 2008.
Jointly combining implicit constraints improves
temporal ordering. In Proceedings of EMNLP.
Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan
Su, Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu.
2006. A semi-automatic method for annotating a
biomedical proposition bank. In Proceedings of the
Workshop on Frontiers in Linguistically Annotated
Corpora 2006, July.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In Proceedings of CoNLL.
James Clarke, Vivek Srikumar, Mark Sammons, and
Dan Roth. 2012. An NLP Curator (or: How I
1508
Learned to Stop Worrying and Love NLP Pipelines).
In Proceedings of LREC.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of ACL.
Quang Do, Wei Lu, and Dan Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of EMNLP-CoNLL.
George R. Doddington, Alexis Mitchell, Mark A. Przy-
bocki, Lance A. Ramshaw, Stephanie Strassel, and
Ralph M. Weischedel. 2004. The Automatic Con-
tent Extraction (ACE) Program-Tasks, Data, and
Evaluation. In Proceedings of LREC.
Oren Etzioni, Michele Banko, and Michael J. Cafarella.
2006. Machine reading. In Proceedings of AAAI.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-Driven Learning for Open Ques-
tion Answering. In Proceedings of ACL.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.
Lynette Hirschman, Marc Light, Eric Breck, and
John D. Burger. 1999. Deep read: A reading com-
prehension system. In Proceedings of ACL.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2009. Overview of
BioNLP 09 shared task on event extraction. In Pro-
ceedings of BioNLP.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Junichi Tsujii. 2011. Overview of
BioNLP shared task 2011. In Proceedings of
BioNLP.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: Connect-
ing natural language to the physical world. TACL,
1:193?206.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. In Proceedings of
ACL.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago Press.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Proceedings of EU-
RALEX.
Thomas L. Magnanti and Laurence A. Wolsey. 1995.
Optimal trees. Handbooks in operations research
and management science, 7:503?615.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of ACL:
System Demonstrations.
Andr?e L. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL/IJCNLP.
David McClosky and Christopher D. Manning. 2012.
Learning constraints for consistent timeline extrac-
tion. In Proceedings of EMNLP-CoNLL.
Dan Moldovan, Christine Clark, Sanda Harabagiu, and
Steve Maiorano. 2003. Cogex: A logic prover
for question answering. In Proceedings of NAACL-
HLT.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).
Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. MCTest: A challenge dataset for
the open-domain machine comprehension of text. In
Proceedings of EMNLP.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of EMNLP.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNLL.
Josef Ruppenhofer, Michael Ellsworth, Miriam RL
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: Extended theory and
practice. Berkeley FrameNet Release, 1.
Aju Thalappillil Scaria, Jonathan Berant, Mengqiu
Wang, Peter Clark, Justin Lewis, Brittany Harding,
and Christopher D. Manning. 2013. Learning bi-
ological processes with global constraints. In Pro-
ceedings of EMNLP.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. Brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the demonstra-
tions at EACL.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of HLT-NAACL.
1509
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly identi-
fying temporal relations with Markov logic. In Pro-
ceedings of ACL/IJCNLP.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of UAI.
1510
