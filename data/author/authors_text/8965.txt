Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 427?434, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Chinese Named Entity Recognition Based on Multiple Features 
 
 
Youzheng Wu, Jun Zhao, Bo Xu Hao Yu 
National Laboratory of Pattern Recognition Fujitsu R&D Center Co., Ltd 
Institute of Automation, CAS Beijing 100016, China 
Beijing, 100080, China yu@frdc.fujitsu.com 
(yzwu,jzhao,bxu)@nlpr.ia.ac.cn  
 
 
 
 
Abstract 
This paper proposes a hybrid Chinese 
named entity recognition model based on 
multiple features. It differentiates from 
most of the previous approaches mainly 
as follows. Firstly, the proposed Hybrid 
Model integrates coarse particle feature 
(POS Model) with fine particle feature 
(Word Model), so that it can overcome 
the disadvantages of each other. Secondly, 
in order to reduce the searching space and 
improve the efficiency, we introduce heu-
ristic human knowledge into statistical 
model, which could increase the perform-
ance of NER significantly. Thirdly, we 
use three sub-models to respectively de-
scribe three kinds of transliterated person 
name, that is, Japanese, Russian and 
Euramerican person name, which can im-
prove the performance of PN recognition. 
From the experimental results on People's 
Daily testing data, we can conclude that 
our Hybrid Model is better than the mod-
els which only use one kind of features. 
And the experiments on MET-2 testing 
data also confirm the above conclusion, 
which show that our algorithm has consis-
tence on different testing data. 
1 Introduction 
Named Entity Recognition (NER) is one of the key 
techniques in the fields of Information Extraction, 
Question Answering, Parsing, Metadata Tagging in 
Semantic Web, etc. In MET-2 held in conjunction 
with the Seventh Message Understanding Confer-
ence (MUC-7), the task of NER is defined as rec-
ognizing seven sub-categories entities: person (PN), 
location (LN), organization (ON), time, date, cur-
rency and percentage. As for Chinese NEs, we fur-
ther divide PN into five sub-classes, that is, 
Chinese PN (CPN), Japanese PN (JPN), Russian 
PN (RPN), Euramerican PN (EPN) and abbrevi-
ated PN (APN) like "???/Mr. Wu". Similarly, 
LN is split into common LN (LN) like "???
/Zhongguancun" and abbreviated LN (ALN) such 
as "?/Beijing", "?/Shanghai". The recognition of 
time (TM) and numbers (NM) is comparatively 
simpler and can be implemented via finite state 
automata. Therefore, our research focuses on the 
recognition of CPN, JPN, RPN, EPN, APN, LN, 
ALN and ON. 
Compared to English NER, Chinese NER is 
more difficult. We think that the main differences 
between Chinese NER and English NER lie in: (1) 
Unlike English, Chinese lacks the capitalization 
information which can play very important roles in 
identifying named entities. (2) There is no space 
between words in Chinese, so we have to segment 
the text before NER. Consequently, the errors in 
word segmentation will affect the result of NER. 
In this paper, we proposes a hybrid Chinese 
NER model based on multiple features which em-
phasizes on (1) combining fine particle features 
(Word Model) with coarse particle features (POS 
Model); (2) integrating human knowledge into sta-
tistical model; (3) and using diverse sub-models 
for different kinds of entities. Especially, we divide 
transliterated person name into three sub-classes 
according to their characters set, that is, JPN, RPN 
and EPN. In order to deduce the complexity of the 
model and the searching space, we divide the rec-
427
ognition process into two steps: (1) word segmen-
tation and POS tagging; (2) named entity recogni-
tion based on the first step. 
Trained on the NEs labeled corpus of five-
month People's Daily corpus and tested on one-
month People's Daily corpus, the Hybrid Model 
achieves the following performance. The precision 
and the recall of PN (including CPN, JPN, RPN, 
EPN, AP N), LN (including ALN) and ON are re-
spectively (94.06%, 95.21%), (93.98%, 93.48%), 
and (84.69%, 86.86%). From the experimental re-
sults on People's Daily testing data, we can con-
clude that our Hybrid Model is better than other 
models which only use one kind of features. And 
the experiments on MET-2 testing data also con-
firm the above conclusion, which show that our 
algorithm has consistence on different testing data. 
2 Related Work 
On the impelling of international evaluations like 
MUC, CoNLL, IEER and ACE, the researches on 
English NER have achieved impressive results. For 
example, the best English NER system[Chinchor. 
1998] in MUC7 achieved 95% precision and 92% 
recall. However, Chinese NER is far from mature. 
For example, the performance (precision, recall) of 
the best Chinese NER system in MET-2 is (66%, 
92%), (89%, 91%), (89%, 88%) for PN, LN and 
ON respectively.  
Recently, approaches for NER are a shift away 
from handcrafted rules[Grishman, et al 1995] 
[Krupka, et al 1998][Black et al 1998] towards 
machine learning algorithms, i.e. unsupervised 
model like DL-CoTrain, CoBoost[Collins, 1999, 
2002], supervised learning like Error-driven [Ab-
erdeen, et al 1995], Decision Tree [Sekine, et al 
1998], HMM[Bikel, et al 1997] and Maximum 
Entropy[Borthwick, et al 1999][Mikheev, et 
al.1998].  
Similarly, the models for Chinese NER can also 
be divided into two categories: Individual Model 
and Integrated Model.  
Individual Model[Chen, et al 1998][Sun, et al 
1994][Zheng, et al 2000] consists of several sub-
models, each of them deals with a kind of entities. 
For example, the recognition of PN may be statis-
tical-based model, while LN and ON may be rule-
based model like [Chen, et al 1998]. Integrated 
Model[Sun, et al 2002] [Zhang, et al 2003][Yu, et 
al. 1998][Chua, et al 2002] deals with all kinds of 
entities in a unified statistical framework. Most of 
these integrated models can be viewed as a HMM 
model. The differences among them are the defini-
tion of state and the features used in entity model 
and context model.  
In fact, a NER model recognizes named entities 
through mining the intrinsic features in the entities 
and the contextual features around the entities. 
Most of existing approaches employ either coarse 
particle features, like POS and ROLE[Zhang, et al 
2003], or fine particle features like word. The data 
sparseness problem is serious if only using fine 
particle features, and coarse particle features will 
lose much important information though without 
serious data sparseness problem. Our idea is that 
coarse particle features should be integrated into 
fine particle features to overcome the disadvan-
tages of them. However, most systems do not com-
bine them and especially ignore the impact of POS. 
Inspired by the algorithms of identifying 
BaseNP and Chunk[Xun, et al 2000], we propose 
a hybrid NER model which emphasizes on com-
bining coarse particle features (POS Model) with 
fine particle features (Word Model). Though the 
Hybrid Model can overcome the disadvantages of 
the Word Model and the POS Model, there are still 
some problems in such a framework. Data sparse-
ness still exists and very large searching space in 
decoding will influence efficiency. Our idea is that 
heuristic human knowledge can not only improve 
the time efficiency, but also solve the data sparse-
ness problem to some extent by restricting the gen-
eration of entity candidates. So we intend to 
incorporate human knowledge into the statistical 
model to improve efficiency and effectivity of the 
Hybrid Model.  
Similarly, for capturing intrinsic features in dif-
ferent types of entities, we design several sub-
models for each kind of entities. For example, we 
divide transliterated person name into three sub-
classes according to their characters sets, that is, 
JPN, RPN and EPN. 
3 Chinese NER with Multiple Features 
Chinese NEs have very distinct word features in 
their composition and contextual information. For 
example, about 365 highest frequently used sur-
names cover 99% Chinese surnames[Sun, et al 
1994]. Similarly the characters used for transliter-
ated names are also limited. LNs and ONs often 
428
end with the specific words like "?/province" and 
"??/company". However, data sparseness is very 
serious when using word features. So we try to 
introduce coarse particle feature to overcome the 
data sparseness problem. POS features are simplest 
and easy to obtain. Therefore, our hybrid model 
combines word feature with POS feature to recog-
nize Chinese NEs. 
Given a word/pos sequence as equation (1): 
nnii twtwtwTW //// 11 LL=                    (1) 
where n is the number of words and ti is the POS 
of word wi. The task of Chinese NE identification 
is to find the optimal sequence WC*/ TC* by split-
ting, combining and classifying the sequence of (1). 
mmii21 tc/wctc/wctc/wc*TC/*WC LL=     (2) 
where [ ]ljji wwwc += L , [ ]ljji tttc += L , nm ? . 
Note that the definition of words in {wi} set is 
that each kind of NEs (including PN, APN, LN, 
ALN, ON, TM, NM) is defined as a word and all 
the other words in the vocabulary are also defined 
as individual words. Consequently, {wi} set has 
|V|+7 words, where |V| is the size of vocabulary. 
The size of {ti} set is 48 which include PKU POS 
tagging set1 and each kind of NEs. 
Obviously, we could obtain the optimal se-
quence WC*/TC* through the following three 
models: the Word Model, the POS Model and the 
Hybrid Model.  
The Word Model employs word features for 
NER, which is introduced by [Sun, et al 2002]. 
The POS Model employs POS features for NER. 
This paper proposes a Hybrid Model which com-
bines word features with POS features.  
We will describe these models in detail in fol-
lowing section. 
3.1 The Hybrid Model 
For the convenience of description, we take apart 
equation (1) into two components: word sequence 
as equation (3) and POS sequence as (4).  
ni21 wwwwW LL=                                     (3) 
ni21 ttttT LL=                                          (4) 
The Word Model estimates the probability of 
generating a NE from the viewpoint of word se-
quence, which can be expressed in equation (5).  
                                                          
1 http://icl.pku.edu.cn/nlp-tools/catetkset.html 
( ) ( )WC|WPWCPargmax*WC wc=                  (5) 
The POS Model estimates the probability of 
generating a NE from the viewpoint of POS se-
quence, which can be expressed in equation (6). 
( ) ( )TC|TPTCPargmax*TC TC=                      (6) 
Our proposed Hybrid Model combines the Word 
Model with the POS Model, which can be ex-
pressed in the equation (7). 
( )
( ) ( )
( ) ( ) ( )
( ) ( )
( ) ( ) ( ) ( ) ( ) ?]TCPTC|T[PWCPWC|WPargmax
W,TWC,TC,Pargmax
T,WPW,TWC,TC,Pargmax
W,T|WC,TCPargmax
*TC*,WC
TCWC,
TCWC,
TCWC,
TCWC,
?
=
=
=
  (7) 
where factor ? > 0 is to balance the Word Model 
and the POS Model. 
Therefore, the Hybrid Model consists of four 
sub-models: word context model P(WC), POS con-
text model P(TC), word entity model P(W|WC) 
and POS entity model P(T|TC). 
3.2 Context Model 
The word context model and the POS context 
model estimate the probability of generating a 
word or a POS given previous context. P(WC) and 
P(TC) can be estimated according to (8) and (9) 
respectively.  
( ) ( )?
=
=
m
1i
1i2ii wcwc|wcPWCP                       (8) 
( ) ( )?
=
=
m
1i
1i2ii tctc|tcPTCP                             (9) 
3.3 Word Entity Model  
Different types of NEs have different structures 
and intrinsic characteristics. Therefore, a single 
model can't capture all types of entities. Typical, 
character-based model is more appropriate for PNs, 
whereas, word-based model is more competent for 
LNs and ONs. Especially, we divided transliterated 
PN into three categories such as JPN, RPN and 
EPN.  
For the sake of estimating the probability of 
generating a NE, we define 19 sub-classes shown 
as Table 1 according to their position in NEs. 
 
429
Tag Description 
Sur Surname of CPN 
Dgb First character of Given Name of CPN 
Dge Last character of Give Name of CPN 
Bfn First character of EPN 
Mfn Middle character of EPN 
Efn Last character of EPN 
RBfn First character of RPN 
RMfn Middle character of RPN 
REfn Last character of RPN 
JBfn surname of JPN 
JMfn Middle character of JPN 
JEfn Last character of JPN 
Bol First word of LN 
Mol Middle word of LN 
Eol Last word of LN 
Aloc Single character LN 
Boo First word of ON 
Moo Middle word of ON 
Eoo Last word of ON 
Table 1 Sub-classes in Entity Model 
3.3.1 Word Entity Model for PN 
For the class of PN (including CPN, APN, JPN, 
RPN and EPN), the word entity model is a charac-
ter-based trigram model which can be expressed in 
equation (10). ( )
( ) ( )( )
( )( )1kiik
1liil1i
ik1i
ik1i
wcwc
1k
2l
wcwcwc
2k
wcwc
iwcwc
w,ENe|wP
w,MNe|wPBNe|wP
ENeMNeMNeBNe|wwP
wc|wwP
?
?
?
??
???
?
???
?=
??
=
? 448476
LL
L
       (10) 
where, BNe, MNe and ENe denotes the first, mid-
dle and last characters respectively. 
The word entity models for PN are estimated 
with Chinese, Japanese, Russian and Euramerican 
names lists which contain 15.6 million, 0.15 mil-
lion, 0.44 million, 0.4 million entities respectively. 
3.3.2 Word Entity Model for LN and ON 
For the class of LN and ON, the word entity model 
is a word-based trigram model. The model can be 
expressed by (11). 
( )
( ) ( )
( )( ) ( )
( )( ) ( )ikwcwcwcwc
wcwcwc
1k
2l
1liil
wcendwcstartwcwc
2k
wcwcwc
iendwcstartwc
wc|wwPwc,ENe|wcP
wc|wwPwc,MNe|wcP
wc|w..wPBNe|wcP
ENeMNeMNeBNe|wcwcwcP
wc|wwP
denikstartik1kiik
ilendilstartil
1i1i1i1i
ikil1i
ii
?
L
L
448476
LLL
L
?
?
=
???
?
???
?=
=
 (11) 
The word entity models and the POS entity 
model for LN and ON are estimated with LN and 
ON names lists which respectively contain 0.44 
mil-lion and 3.2 million entities. 
3.3.3 Word Entity Model for ALN 
For the class of ALN, we use word-based bi-gram 
model. The entity model for ALN can be expressed 
by equation (12). 
( ) ( )
)LocA(C
ocAL,wC
ocAL|wP ii =                           (12) 
where wi is the ALN which includes single and 
multiple characters ALN. 
3.4 POS Entity Model 
But for the class of PN, it's very difficult to obtain 
the corpus to train POS Entity Model. For the sake 
of simplification, we use word entity model shown 
in equation (10) to replace the POS entity model. 
For the class of LN and ON, POS entity model 
can be expressed by equation (13). ( )
( ) ( )
( )( ) ( )
( )( ) ( )iktctctctc
tctctc
1k
2l
1liil
tcendwcstarttctc
2k
tctctc
iendtcstarttc
tc|ttPtc,ENe|tcP
tc|ttPtc,MNe|tcP
tc|t..tPBNe|tcP
ENeMNeMNeBNe|tctctcP
tc|ttP
denikstartik1kiik
ilendilstartil
1i1i1i1i
ikil1i
ii
?
L
L
448476
LLL
L
?
?
=
???
?
???
?=
=
    (13) 
While for the class of ALN, POS entity model is 
shown as equation (14). 
( ) ( )
)ocAL(C
ocAL,tiCocAL|tP i =                               (14) 
4 Heuristic Human Knowledge 
In this section, we will introduce heuristic human 
knowledge that is used for Chinese NER and the 
430
method of how to incorporate them into statistical 
model which are shown as follows. 
1. CPN surname list (including 476 items) and 
JPN surnames list (including 9189 items): Only 
those characters in the surname list can trigger per-
son name recognition. 
2. RPN and EPN characters lists: Only those 
consecutive characters in the transliterated charac-
ter list form a candidate transliterated name. 
3. Entity Length Restriction: Person name can-
not span any punctuation and the length of CN 
cannot exceed 8 characters while the length of TN 
is unrestrained. 
4. Location keyword list (including 607 items):  
If the word belongs to the list, 2~6 words before 
the salient word are accepted as candidate LNs. 
5. General word list (such as verbs and preposi-
tions): Words in the list usually is followed by a 
location name, such as "?/at", "?/go". If the cur-
rent word is in the list, 2~6 words following it are 
accepted as candidate LNs. 
6. ALN name list (including 407 items): If the 
current word belongs to the list, we accept it as a 
candidate ALN. 
7. Organization keyword list (including 3129 
items): If the current word is in organization key-
word list, 2~6 words before keywords are accepted 
as the candidate ONs. 
8. An organization name template list: We 
mainly use organization name templates to recog-
nize the missed nested ONs in the statistical model. 
Some of these templates are as follows: 
ON-->LN D* OrgKeyWord 
ON-->PN D* OrgKeyWord 
ON-->ON OrgKeyWord 
D and OrgKeyWord denote words in the middle 
of ONs and ONs keywords. D* means repeating 
zero or more times. 
5 Back-off Model to Smooth 
Data sparseness problem still exists. As some pa-
rameters were never observed in training corpus, 
the model will back off to a less powerful model. 
The escape probability[Black, et al 1998] was ad-
opted to smooth the statistical model shown as (15). 
00N11N2N1N
1N1NN1N1N
^
p)W(p)WWW(p
)WWW(p)WWW(p
???
?
+++
+=
LL
LL  (15) 
where NN e1? = , Ni0,e)e1(?
N
1ik
kii <<=
+=
? , and ei 
is the escape probability which can be estimated by 
equation (16). 
)WWW(f
)WWW(q
e
1N21
1N21
N L
L=                           (16) 
q(w1w2?wN-1) in (16) denotes the number of dif-
ferent symbol wN that have directly followed the 
word sequence w1w2?wN-1. 
6 Experiments 
In this chapter, we will conduct experiments to 
answer the following questions.  
Will the Hybrid Model be more effective than 
the Word Model and the POS Model? To answer 
this question, we will compare the performances of 
models with different parameter ? and find the best 
value of ? in equation (7). 
Will the conclusion from different testing sets be 
consistent? To answer this question, we evaluate 
models on the MET-2 test data and compare the 
performances of the Word Model, the POS Model 
and the Hybrid Model. 
Will the performance be improved significantly 
after combining human knowledge? To answer this 
question, we compare two models with and with-
out human knowledge.  
In our evaluation, only NEs with correct 
boundaries and correct categories are considered as 
the correct recognition. We conduct evaluations in 
terms of precision, recall and F-Measure. Note that 
PNs in experiments includes all kinds of PNs and 
LNs include ALNs. 
6.1 Will the Hybrid Model be More Effective 
Than the Word Model and POS Model? 
The parameter ? in equation (7) denotes the balanc-
ing factor of the Word Model and the POS Model. 
The larger ?, the larger contribution of the POS 
Model. The smaller ?, the larger contribution of the 
Word Model. So the task of this experiment is to 
find the best value of ?. In this experiment, the 
training corpus is from five-month's People's Daily 
tagged with NER tags and the testing set is from 
one-month's People's Daily. 
With the change of ?, the performances of rec-
ognizing PNs are shown in Fig.1.  
Note that the left, middle and right point in ab-
scissa respectively denote the performance of the 
431
Word Model, the Hybrid Model and the POS 
Model. 
0 1.6 3.2 4.8 6.4 8 9.6
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.96
Lamda
%
Precision
Recall
F?Measure
 
Fig.1 Performance of Recognizing LNs Impacted 
by ? 
From Fig.1, we can find that the performances 
of recognizing PNs are improved with the increas-
ing of ? in the beginning stage but decline in the 
ending. This experiment shows that the Word 
Model and the POS Model can overcome their dis-
advantages, and it is a feasible approach to inte-
grate the Word Model and the POS Model in order 
to improve the performance PNs recognition.  
With the change of ?, the performances of rec-
ognizing LNs are shown in Fig.2. 
0 1.6 3.2 4.8 6.4 8 9.6
0.9
0.91
0.92
0.93
0.94
Lamda
%
Precision
Recall
F?Measure
 
Fig.2 Performance of Recognizing LNs Impacted 
by ? 
As the Fig.2 shows, the precision and recall of 
LNs are improved with the increasing of ? and de-
creased in the later stage. This phenomenon also 
proves that the Hybrid Model is better for recog-
nizing LN than either the Word Model or the POS 
Model. 
Similarly, with the change of ?, the perform-
ances of recognizing ONs are shown in Fig.3. 
 
0 1.6 3.2 4.8 6.4 8 9.6
0.7
0.75
0.8
0.85
Lamda
%
Precision
Recall
F?Measure
 
Fig.3 Performance of Recognizing LNs Impacted 
by ? 
Comparing Fig.3 with Fig.1 and Fig.2, we find 
that the POS Model has different impact on recog-
nizing ONs from that on recognizing PNs and LNs. 
Especially, the POS Model has obvious side-effect 
on the recall. We speculate that the reasons may be 
that the probability of generating POS sequence by 
POS entity model is lower than that by POS con-
text model. 
According to Fig.1~Fig.3, we choose the best 
value ? = 2.8. And the performances of different 
models are shown in Table 2 in detail. 
 P(%) R(%) F(%) 
PN 94.06 95.21 94.63 
LN 93.98 93.48 93.73 
Hybrid 
Model 
(?= 2.8) 
ON 84.69 86.86 85.76 
 
PN 88.24 90.11 89.16 
LN 91.50 93.17 92.32 
Word 
Model 
ON 78.85 88.77 83.52 
    
PN 93.44 95.11 94.27 
LN 89.97 92.20 91.07 
POS 
Model 
ON 80.90 69.29  74.65 
Table 2 Performance of the Hybrid Model, the 
Word Model and the POS Model 
From Table 2, we find that the F-Measures of 
the Hybrid Model for PN, LN, ON are improved 
by 5.4%, 1.4%, 2.2% respectively in comparison 
with the Word Model, and these F-Measures are 
improved by 0.4%, 2.7%, 11.1% respectively in 
comparison with the POS Model. 
432
Conclusion 1: The experimental results validate 
our idea that the Hybrid Model can improve the 
performance of both the Word Model and the POS 
Model. However, the improvements for PN, LN 
and ON are different. That is, the POS Model has 
obvious side-effect on the recall of ON recognition 
at all times, while the recalls for PN and ON rec-
ognition are improved in the beginning but de-
creased in the ending with the increasing of ?. 
6.2 Will the Conclusion from Different Test-
ing Sets be Consistent? 
We also conduct experiments on the MET-2 test-
ing corpus to validate our conclusion from Exp.1, 
that is, the Hybrid Model could achieve better per-
formance than either the Word Model or the POS 
Model alone. The experimental results (F-Measure) 
on MET-2 are shown in Table 3. 
Model Word Model 
Hybrid 
Model 
POS 
Model 
PN 75.21% 80.77% 76.61% 
LN 89.78% 90.95% 89.81% 
ON 76.30% 80.21% 76.83% 
Table 3 F-Measure on MET-2 test corpus  
Comparing Table 3 with Table 2, we find that 
the performances of models on MET-2 are not as 
good as that on People Daily's testing data. The 
main reason lies in that the NE definitions in Peo-
ple Daily's corpus are different from that in MET-2. 
However, Table 3 can still validate our conclude 1, 
that is, the Hybrid Model is better than both the 
Word Model and the POS Model. For example, the 
F-Measures of the Hybrid Model for PN, LN and 
ON are improved by 5.6%, 1.2% and 3.9% respec-
tively in comparison with the Word Model, and 
these F-Measures are improved by 4.2%, 3.1% and 
3.4% respectively in comparison with the POS 
Model. 
Conclusion 2: Though the performances of the 
Hybrid Model on MET-2 are not as good as that 
on People's Daily corpus, the experimental results 
also support conclusion 1, i.e. the Hybrid Model 
which combining the Word Model with the POS 
Model can achieve better performance than either 
the Word Model or the POS Model. 
6.3 Will the Performance be Improved Sig-
nificantly after Incorporating Human 
Knowledge?  
One of our ideas in this paper is that human 
knowledge can not only reduce the search space, 
but also improve the performance through avoiding 
generating the noise NEs. This experiment will be 
conducted to validate this idea. Table 4 shows the 
performances of models with and without human 
knowledge.  
 P(%) R(%) F(%) 
PN 91.81 70.65 79.85 
LN 79.47 88.83 83.89 Model I 
ON 64.95 80.63 71.95 
 
PN 94.06 95.21 94.63 
LN 93.98 93.48 93.73 Model II 
ON 84.69 86.86 85.76 
Table 4 Performances Impacted by Human Know-
ledge 
From Table 4, we find that F-Measure of model 
with human knowledge (Model II) is improved by 
14.8%?9.8%?13.8% for PN, LN and ON respec-
tively compared with that of the model without 
human knowledge (Model I). 
Conclusion 3: From this experiment, we learn 
that human knowledge can not only reduce the 
search space, but also significantly improve the 
performance of pure statistical model. 
7 Conclusion 
In this paper, we propose a hybrid Chinese NER 
model which combines multiple features. The main 
contributions are as follows: ? The proposed Hy-
brid Model emphasizes on integrating coarse parti-
cle feature (POS Model) with fine particle feature 
(Word Model), so that it can overcome the disad-
vantages of each other; ? In order to reduce the 
search space and improve the efficiency of model, 
we incorporate heuristic human knowledge into 
statistical model, which could increase the per-
formance of NER significantly; ? For capturing 
intrinsic features in different types of entities, we 
design several sub-models for different entities. 
Especially, we divide transliterated person name 
into three sub-classes according to their characters 
set, that is, CPN JPN, RPN and EPN. 
There is a lack of effective recognition strategy 
for abbreviated ONs such as ????(Kunming 
Machine Tool Co.,Ltd), ? ? ? ? (Phoenix 
Photonics Ltd) in this paper. And most of mis-
433
recognized ONs in current system belong to them. 
So in the future work, we will be focusing more on 
recognizing abbreviated ONs. 
8 Acknowledgements 
This research is carried out as part of the coopera-
tive project with Fujitsu R&D Center Co., Ltd. We 
would like to thank Yingju Xia, Fumihito Nisino 
for helpful feedback in the process of developing 
and implementing. This work was supported by the 
Natural Sciences Foundation of China under grant 
No. 60372016 and 60272041, the Natural Science 
Foundation of Beijing under grant No. 4052027. 
References  
N.A. Chinchor: Overview of MUC-7/MET-2. In: Pro-
ceedings of the Seventh Message Understanding 
Conference (MUC-7), April. (1998). 
Youzheng Wu, Jun Zhao, Bo Xu: Chinese Named En-
tity Recognition Combining Statistical Model with 
Human Knowledge. In: The Workshop attached with 
41st ACL for Multilingual and Mix-language Named 
Entity Recognition, Sappora, Japan. (2003) 65-72. 
Endong Xun, Changning Huang, Ming Zhou: A Unified 
Statistical Model for the Identification of English 
BaseNP. In: Proceedings of ACL-2000, Hong Kong. 
(2000). 
Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou, 
Changning Huang: Chinese Named Entity Identifica-
tion Using Class-based Language Model. In: 
COLING 2002. Taipei, August 24-25. (2002). 
Huaping Zhang, Qun Liu, Hongkui Yu, Xueqi Cheng, 
Shuo Bai: Chinese Named Entity Recognition Using 
Role Model. In: the International Journal of Compu-
tational Linguistics and Chinese Language Process-
ing, vol.8, No.2. (2003) 29-60. 
D.M. Bikel, Scott Miller, Richard Schwartz, Ralph 
Weischedel: Nymble: a High-Performance Learning 
Name-finder. In: Fifth Conference on Applied Natu-
ral Language Processing, (published by ACL). (1997) 
194-201. 
Borthwick .A: A Maximum Entropy Approach to 
Named Entity Recognition. PhD Dissertation. (1999). 
Mikheev A., Grover C. and Moens M: Description of 
the LTG System Used for MUC-7. In: Proceedings of 
7th Message Understanding Conference (MUC-7), 
1998. 
Sekine S., Grishman R. and Shinou H: A decision tree 
method for finding and classifying names in Japanese 
texts. In: Proceedings of the Sixth Workshop on Very 
Large Corpora, Canada, 1998. 
Aberdeen, John, et al MITRE: Description of the 
ALEMBIC System Used for MUC-6. In: Proceedings 
of the Sixth Message Understanding Conference 
(MUC-6), November. (1995) 141-155. 
Ralph Grishman and Beth Sundheim: Design of the 
MUC-6 evaluation. In: 6th Message Understanding 
Conference, Columbia, MD. (1995) 
Krupka, G. R. and Hausman, K. IsoQuest: Inc.: Descrip-
tion of the NetOwl TM Extractor System as Used for 
MUC-7. In Proceedings of the MUC-7, 1998. 
Black, W.J.; Rinaldi, F, Mowart, D: FACILE: Descrip-
tion of the NE System Used for MUC-7. In Proceed-
ings of the MUC-7, 1998. 
Michael Collins, Yoram Singer: Unsupervised models 
for named entity classification. In Proceedings of 
EMNLP. (1999) 
Michael Collins: Ranking Algorithms for Named Entity 
Extraction: Boosting and the Voted Perceptron. In: 
Proceeding of ACL-2002. (2002) 489-496. 
S.Y.Yu, et al Description of the Kent Ridge Digital 
Labs System Used for MUC-7. In: Proceedings of the 
Seventh Message Understanding Conference, 1998. 
H.H. Chen, et al Description of the NTU System Used 
for MET2. In: Proceedings of the Seventh Message 
Understanding Conference. 
Tat-Seng Chua, et al Learning Pattern Rules for Chi-
nese Named Entity Extraction. In: Proceedings of 
AAAI'02. (2002) 
Maosong Sun, et al Identifying Chinese Names in Un-
restricted Texts. Journal of Chinese Information 
Processing. (1994). 
Jiahen Zheng, Xin Li, Hongye Tan: The Research of 
Chinese Names Recognition Methods Based on Cor-
pus. In: Journal of Chinese Information Processing. 
Vol.14 No.1. (2000). 
CoNLL. http://cnts.uia.ac.be/conll2004/ 
IEER. http://www.nist.gov/speech/tests/ie-er/er99/er99. 
htm 
ACE. http://www.itl.nist.gov/iad/894.01/tests/ace/ 
 
434
Chinese Named Entity Recognition Combining a Statistical Model with 
Human Knowledge 
Youzheng WU Jun ZHAO Bo XU 
National Laboratory of Pattern Recognition 
Institute of Automation Chinese Academy of Sciences 
No.95 Zhongguancun East Road, 100080, Beijing, China 
(yzwu, jzhao,boxu)@nlpr.ia.ac.cn 
 
 
Abstract 
Named Entity Recognition is one of the 
key techniques in the fields of natural 
language processing, information retrieval, 
question answering and so on. 
Unfortunately, Chinese Named Entity 
Recognition (NER) is more difficult for 
the lack of capitalization information and 
the uncertainty in word segmentation. In 
this paper, we present a hybrid algorithm 
which can combine a class-based 
statistical model with various types of 
human knowledge very well. In order to 
avoid data sparseness problem, we 
employ a back-off model and?????
?/TONG YI CI CI LIN? , a Chinese 
thesaurus, to smooth the parameters in the 
model. The F-measure of person names, 
location names, and organization names 
on the newswire test data for the 1999 
IEER evaluation in Mandarin is 86.84%, 
84.40% and 76.22% respectively. 
1 Introduction 
The NER task was first introduced as Message 
Understanding Conference (MUC) subtask in 1995 
(MUC-6). Named Entities were defined as entity 
names (organizations, persons and locations), 
temporal expressions (dates and times) and number 
expressions (monetary values and percentages). 
Compared with the entity name recognition, the 
recognition of temporal and number expressions is 
simpler. So, our research focuses on the 
recognition of person, location and organization 
names. 
The Multilingual NE task first started in 
1995(MET-1), including Chinese, Japanese, and 
Spanish in that year, and continued for Chinese, 
Japanese in 1998(MET-2). Compared with English 
NER, Chinese NER is more difficult. We think the 
main differences between Chinese NER and 
English NER lie in:  
First, unlike English, Chinese lacks the 
capitalization information that plays an important 
role in signaling named entities.  
Second, there is no space between words in 
Chinese, and we have to segment the text before 
NER. However, the errors in word segmentation 
will affect the result of NER. 
Third, Different types of named entities have 
different structures, especially for abbreviative 
entities. Therefore, a single unified model can?t 
capture all the types of entities. Typical structures 
of Chinese person name (CN), location name (LN) 
and organization name (ON) are as follows: 
CN--><surname> <given name> 
LN--><name part>* <a salient word> 
ON-->{[person name] [organization name] [place 
name] [kernel name] }*  [organization type] <a 
salient word> 
Here <>* means repeating one or several times. 
{}* means selecting at least one of items. 
Fourth, there are few openly available resources 
for Chinese NER. Thus we have to resort to the 
algorithm that doesn?t rely on large NER-tagged 
text corpus. 
Based on the above analysis, we present a 
hybrid algorithm that incorporating various types 
of human knowledge into a statistical model. The 
innovative points of our paper are as follows. 
First, the hybrid algorithm can make the best 
use of existing limited resources to develop an 
effective NER system. These resources include 
one-month?s Chinese People?s Daily tagged with 
NER tags by Peking University (which contains 
about two-million Chinese characters) and various 
types of human knowledge. 
Second, in order to compensate for the lack of 
labeled corpus, we use several types of human 
knowledge, such as??????/TONG YI CI 
CI LIN? [Mei.J.J, et al 1983], a general location 
names list, the list of the salient words in location 
name, the list of the salient words in organization 
names, a Chinese surnames list, the list of Chinese 
characters that could be included in transliterated 
person names, and so on. 
Third, we emphasize that human knowledge 
and statistical information should be combined 
very well. For example, a general LN list and a 
general famous ON list are used in our system. 
However, we only accept words in the lists as 
entity candidates with a probability. Whether it is 
a LN or ON depends on the context. This is 
different from other systems which accept them as 
a LN or ON once the system meets them. More 
details refer to section 4. 
This paper will be organized as follows. Section 
2 is the background of NER. Section 3 describes 
the class-based statistical baseline Chinese NER 
model. Section 4 describes different types of 
human knowledge for different named entities 
recognitions and how to combine them with a 
statistical model organically in details. Section 5 is 
the evaluation and section 6 is the conclusion. 
2 Backgroud 
The researches on English NER have made 
impressive achievement. The best NER system 
[Mikheev, et al 1999] in MUC7 achieved 95% 
precision and 92% recall. Recent methods for 
English NER focus on machine-learning 
algorithms such as DL-CoTrain, CoBoost [Collins 
and Singer 1999], HMM [Daniel M. Bikel 1997], 
maximum entropy model [Borthwick, et al 1999] 
and so on. 
However, Chinese NER is still at its immature 
phase. Typical Chinese NER systems are as 
follows. 
NTU system [Hsin-His Chen, et al 1997] relied 
on a statistical model when recognizing person 
names, but rules when recognizing location and 
organization names. In the formal run of MET-2, 
the total F-measure is 79.61%. As a result, they 
may miss the person names whose probability is 
lower than the threshold, the location and 
organization names may also be missed for those 
which don?t accord with the rules. 
[Yu et al 1998] uses both a contextual model 
and a morphological model. However, their system 
requires information of POS tags, semantic tags 
and NE lists. The system obtains 86.38% F-
measure. 
[CHUA et al 2000] employs a combination of 
template-based rules supplemented by the default-
exception trees and decision tree that obtains over 
91% F-measure on MET-2 test data. It also uses 
HowNet [Dong & Dong 2000] to cluster 
semantically related words. 
[Jian Sun, 2002] presents a class-based 
language model for Chinese NER which achieves 
81.79% F-measure on MET-2 test set and 78.75% 
F-measure on IEER test data. However, the model 
heavily depends on statistical information, and 
must be trained on large labeled corpus. 
For Chinese NER, we can?t achieve satisfactory 
performance if we use only a statistical model or 
handcrafted heuristic rules. Therefore, we have to 
resort to the algorithm that can incorporate human 
knowledge into a statistical model. 
In the following sections, we will introduce a 
statistical Chinese NER model first, and then 
incorporate various types of human knowledge into 
the statistical model in order to show the power of 
human knowledge for Chinese NER. 
3 The Baseline Class-based Statistical 
Model 
We regard NER as a tagging problem. Given a 
sequence of Chinese string nwwwW L21= , the task 
of NER is to find the most likely sequence of class 
sequence ( )nmcccC m <== L21*  that maximizes 
the probability ( )WCP | . We use Bayes? Rule to 
rewrite ( )WCP |  as equation (3.1): 
( ) ( )( )
( )
( )WP
CPCWP
WP
WCPWCP ?== )|(,|             (3.1) 
So, the class-based baseline model can be 
expressed as equation (3.2). 
( ) ( )( )
( ) ( )( )
( ) ( )???
????
? ??
?=
?=
?
=
?
m
i
iiiiji
C
mmn
C
C
ccPcwwP
cccPcccwwwP
CPCWPC
1
11
212121
||maxarg
|maxarg
|maxarg*
L
LLL (3.2) 
We call ( )CP  as the contextual model and 
( )CWP |  as the morphological model. Formally, we 
can regard such a class-based statistical model as 
HMM. The classes used in our model are shown in 
Table 1, where |V| means the size of vocabulary 
used for word segmentation. 
Class Description 
PN Person Name 
LN Location Name 
ON Organization Name 
TM Time Name 
NM Number Name 
Other One word is on Class 
Total |V| + 5 
Table 1 Classes used in our model 
3.1 Contextual Model 
Due to our small-sized labeled corpus, we use a 
statistical bi-gram language model as the 
contextual model. This model can be described as 
equation (3.3). 
( ) ( )?=
=
??
mi
i
ii ccPCP
1
1|                                       (3.3) 
Theoretically, trigram is more powerful for 
NER than bi-gram, however when training corpus 
is small, trigram can?t work effectively. Using bi-
gram model, we still need ( )25+V  transmission 
probabilities, some of which can?t be observed in 
our small-sized labeled corpus and some of which 
are unauthentic. That is, data sparseness is still 
serious. We will explain how to resolve data 
sparseness problem in details in section 3 and 4. 
3.2 Morphological Model 
Recognition of Person Names 
The model of person names recognition 
(including Chinese person names abbreviated to 
CN and Transliterated person names abbreviated to 
TN) is a character-based tri-states unigram model. 
In principle, Chinese person name is composed 
of a surname (including single-character surname 
like "?/wu" and double-character surname like"?
? /Ouyang") and a given name (one or two 
characters like "?/peng" or "??/youzheng"). So 
we divide Chinese name words into three parts as 
the surname (surCN), the middle name (midCN) 
and the end name (endCN), which means the 
probability of a specific character used in different 
position in person names isn?t equal. For example, ( ) ( )
( )endCNc|?/wu
CNsc|?/wusurCNc|?/wu
j
jj
=?
=?=
P
ecPP         (3.4) 
The model for three-character-CN recognition 
is described as equation (3.5). ( )
( ) ( )
( )endCNcwP
midCNcwPsurCNcwP
CNcwwwP
jj
jjjj
jjjj
=?
=?=?
=
|
||
|
3
21
321
        (3.5) 
The model for two-character-CN recognition is 
described as equation (3.6). ( )
( ) ( )endCNwPsurCNwP
CNcwwP
jj
jjj
||
|
21
21
??
=        (3.6) 
where ( )CNcwwwP jjjj =|321  means the probability 
of emitting the candidate person name 321 jjj www  
under the state of CN. 
For TN, we don?t divide transliterated name 
words into several different parts. That is, the 
probability of a word used in different position in 
TN is same. The model is as follows. 
( ) ( )?=
=
=?=
ki
i
jjijjkjj TNcwPTNcwwwP
1
21 ||L (3.7) 
Must be mentioned is that all these probabilities 
are estimated from labeled corpus using maximum 
likelihood estimation. 
Recognition of Location Names 
For location names recognition, we use a word-
based bi-state unigram model, and divide words 
used in the location name into two parts: location-
end-words (LE) and non-location-end words 
(NLE). That means the probability of the word 
used in the end position of location name is 
different from that of in other position. 
The model for location name recognition is 
shown in equation (3.8). ( )
( ) ( )LEcwPNLEcwP
LNcwwwP
jjk
ki
i
jji
jjkjj
=?=
?=
??=
=
||
|
1
1
21 L
           (3.8) 
The parameters in equation (3.8) are also 
estimated from labeled training corpus. 
Recognition of Organization Names 
For the model of organization names 
recognition, we use bi-state unigram that is similar 
to the location morphological model shown as 
equation (3.9): ( )
( ) ( )NOEcwPOEcwP
ONcwwwP
jjk
ki
i
jji
jjkjj
=?=
==
??=
=
||
|
1
1
21 L
           (3.9) 
where OE means the word used in the end position 
of organization name, while NOE is not. 
The parameters in equation (3.9) are also 
estimated from the labeled training corpus. 
Back-off Models to Smooth 
Data sparseness problem still exists. As some 
parameters were never observed in trained corpus, 
the model will back off to a less-powerful model. 
We employ escape probability to smooth the 
statistical model [Teahan, et al 1999]. 
An escape probability is the probability that a 
previously unseen character will occur. There is no 
theoretical basis for choosing the escape 
probability optimally. Here we estimate the escape 
probability in a particular context as: 
n
d5.0=?                                                      (3.10) 
The probability of a word ci that has occurred c 
times in that context ci-1 is: 
( )
n
cccP ii
5.0| 1
?=?                                        (3.11) 
While the probability of a word that has never 
occurred in that context is: 
( ) ( )iii cPccP ?=? ?1|                                     (3.12) 
where n is the number of times that context has 
appeared and d is the number of different symbols 
that have directly followed it. 
As a example, if we observe the bi-gram "A B" 
once in training corpus and ?A C" three times, and 
nowhere else did we see the word "A", then 
( )
31
5.03| +
?=ACP , while the escape probability 
31
25.0
+
?=?  and unseen transition probability of 
( ) ( )DPADP ?= ?| . 
The Evaluation for the Baseline 
The baseline model was evaluated in terms of 
precision (P), recall (R) and F-measure (F) metrics.  
responsesofnumber
responsescorrectofnumberP =  
NEallofnumber
responsescorrectofnumberR =  
( )
( ) RP
RPF +?
??+= ?
? 12                                       (3.13) 
where ?  is a weighted constant often set to 1. 
We test the baseline system on the newswire 
test data for the 1999 IEER evaluation in Mandarin 
(http://www.nist.gov/speech/tests/ie-r/er_99/er_ 99. 
htm). Table 2 in section 4 summarizes the result of 
baseline model. 
 Precision Recall F-measure
PN 80.23% 89.55% 84.63% 
LN 45.05% 66.96% 53.86% 
ON 42.98% 61.45% 50.58% 
Total 52.61% 71.53% 60.63% 
Table 2 The Performance of The Baseline 
4 The Hybrid Model Incorporating 
Human Knowledge into the Baseline 
From table 1, we find that the performance of 
the above statistical baseline model isn?t 
satisfactory. The problems mainly lie in: 
? Data sparseness is still serious though we 
only use bi-gram contextual model, unigram 
morphological model and smooth the parameters 
with a back-off model.  
? In order to recognize the named entities, 
we have to estimate the probability of every word 
in text as named entities. Thus redundant 
candidates not only enlarge search space but also 
result in many unpredictable errors.  
? Abbreviative named entities especially 
organization abbreviation can?t be resolved by the 
baseline model. Because abbreviations have weak 
statistical regularities, so can?t be captured by such 
a baseline model. 
We try to resolve these problems by 
incorporating human knowledge. In fact, human 
being usually uses prior knowledge when 
recognizing named entities. In this section, we 
introduce the human knowledge that is used for 
NER and the method of how to incorporate them 
into the baseline model.  
Given a sequence of Chinese characters, the 
recognition process after combined with human 
knowledge consists of the five steps shown in Figure1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1 Recognition Process of the Hybrid Model 
 
4.1 Incorporate Knowledge for Person Name 
Recognition 
Chinese person names are composed of a 
surname and a given name. Usually the characters 
used for Chinese person names are limited. 
[Maosong Sun, Changning Huang, 1994] presents 
365 most high frequently used surnames cover 
99% Chinese surnames. 1141 most high frequently 
used characters cover 99% Chinese given names. 
Similarly the characters used for transliterated 
names are also limited. We extract about 476 
transliterated characters from the training corpus. 
The following is the human knowledge used for 
person name recognition and the method of how to 
incorporate them into the baseline. 
? A Chinese single and plural surname list: 
Only those characters in the surname list can 
trigger person name recognition. 
? A list of person title list: Only when the 
current character belongs to the surname list and 
the next word is in the title list, candidates are 
accepted. 
? A transliterated character list: Only 
those consecutive characters in the transliterated 
character list form a candidate transliterated name. 
? Person name can?t span any punctuation 
and the length of CN can?t exceed 8 characters 
while the length of TN is unrestrained. 
All these knowledge are used for restricting 
search space. 
4.2 Incorporate Knowledge for Location 
Name Recognition 
A complete location name is composed of the 
name part and a salient word. For the location 
name "???/Beijing City", the name part is "?
? /Beijing" and the salient word is "? /city". 
Unfortunately, the salient word is omitted in many 
occasions. So it is unfeasible to trigger LN 
recognition only depending on the salient words in 
location name. In order to improve the precision 
and recall of LN recognition, we use the following 
human knowledge. The method of incorporating 
them is also explained. 
? A general location name list: The list 
includes the names of Chinese provinces and 
counties, foreign country and its capitals, some 
famous geographical names and foreign cities. If 
the current word is in the list, we accept it as a 
candidate LN. 
? A location salient word list: If the word 
wi belongs to the list, 2~6 words before the salient 
word are accepted as candidate LNs. 
? A general word list (such as verbs and 
prepositions) which usually is followed by a 
location name, such as "? /at", "? /go". If the 
word wi is in the list, 2~6 words following it are 
accepted as candidate LNs. 
? An abbreviative location name list: If the 
current word is in the list, we accept it as a 
candidate LN such as "?/China", "?/America". 
PN and LN 
Generate 
NE Candidates
Recognition Nested 
Organization Names
Named Entities
Human Knowledge 
TONG YI CI CI LIN 
Extract 
Organization Kernel 
Word Segmentation
Nested Organization 
Name Templates
NE Pools
Text
Search the Max. 
P(C|W)
? Coordinate LN recognition: If wi-2 is a 
candidate LN and wi-1 is "? "(a punctuation 
denoting coordinate relation), LN recognition is 
triggered at the position of word wi. 
? Location name can?t span punctuations and 
its length couldn?t exceed 6 words. 
Knowledge ?, ?, ?, ?, ? can restrict 
search space while knowledge ? deals with 
abbreviative location name. 
4.3 Incorporate Knowledge for Organization 
Name Recognition 
The organization names recognition is the most 
difficult task. The reasons lie in nested ONs and 
abbreviative ONs especially. 
Nested ON means there are one or more 
location names, person names and/or organization 
names embedded in organization name. Typical 
structure of ON has been given in section 1. We 
can capture most of the nested organization names 
by several ON templates mentioned in the 
following section. 
Abbreviative ONs include continuous and 
discrete abbreviation which omits some words in 
the full name. Take "????????????
" as example, abbreviative ON of it may omit LN "
?? /Shanghai", organization types like"??
/supermarket", "??/stock", "??/limited", and 
salient word like "??/company" from full names 
but usually remains organization kernel "??
/Hualian". Table 3 lists some examples of 
abbreviative ONs. 
?????? 
?????? 
Shanghai Hualian 
Co.,Ltd 
????
Shanghai
Hualian Continuous Abbreviation 
???? 
Tsinghua niversity 
?? 
Tsinghua
??????? 
Shanghai Stock 
Exchange 
?? 
Shanghai 
Stock Discrete Abbreviation ???? 
Peking University 
?? 
Bei Da 
Table 3 Nest Organization Full Names and Its 
Abbreviative Names 
So it is important to extract organization kernel 
from the full name in order to recognize 
abbreviative ON like "????". Moreover, an 
organization's abbreviative names usually occur 
after its' full name, unless it is a well-known 
organization. So this strategy for abbreviation 
organization name recognition is effective. 
The following is the human knowledge used for 
ON recognition and the method of how to 
incorporate them. 
? An organization salient word (OrgSws) 
list: If the current word wi is in OrgSws list, 2~6 
words before OrgSw are accepted as the candidate 
ONs. 
? A general famous organization name list: 
If the current word is in the list, we accept it as a 
candidate ON such as "???/ State Department", 
"???/ U.N. ". 
? An organization names template list: We 
mainly use organization name templates to 
recognize the nested ONs. Some of these templates 
are as follows: 
ON-->LN D* OrgSw 
ON-->PN D* OrgSw 
ON-->ON OrgSw 
D means words used in the middle of organization 
names. D* means repeating zero or more times. 
This component runs in the end stage of 
recognition process shown in Figure 1. 
? An organization type list: The list is used 
to extract organization kernels from recognized 
ONs. We have a pool which memorizes ONs 
recognized in current paragraph and its kernel. If 
the current word belongs to organization kernel in 
pool, we accept it as a candidate ON. The idea is 
effective especially in financial domain which 
contains many stocks such as"????/Shanghai 
Hualian", "????/Changjiang Technology". 
Knowledge ?, ?, ? restrict search space 
while knowledge ? deals with abbreviative 
organization name. 
4.4 Semantic Similarity Computation for 
Data Sparseness 
??????/TONG YI CI CI LIN?classifies 
the words in terms of semantic similarity. Here we 
use it to resolve data sparseness problem. If current 
transmission probability doesn?t exist, we resort to 
its synonym transmission. In statistical sense, 
synonym transmissions are approximate. Take an 
example, the probability of P(A|B) doesn?t exist, 
but there has P(C|B), meanwhile, the word A and 
C are thesaurus according to ??????/TONG 
YI CI CI LIN?, then we use P(C|B) to replace 
P(A|B). 
5 Results of Evaluation 
We also test our hybrid model on IEER-99 neswire 
test data. The performance is shown in Table 4. 
 Precision Recall F-measure
PN 83.30% 92.28% 87.56% 
LN 88.31% 84.69% 86.47% 
ON 84.49% 71.08% 77.21% 
Total 86.09% 83.18% 84.61% 
Table 4 The Performance of the Hybrid Model 
Comparing Table 1 with 4, we find that the 
performance of the hybrid model increases 
remarkably. More specifically, the precision and 
the recall of PNs increase from 80.23% to 83.30% 
and from 89.55% to 92.28% respectively. The 
precision and recall of LNs increase from 45.05% 
to 82.18% and from 66.96% to 86.74% 
respectively. The precision and recall of ONs 
increase from 42.98% to 80.86% and from 61.45% 
to 72.09% respectively. The reason that the 
improvement of PNs is slighter than that of ONs 
and LNs is that the statistical information 
estimated from labeled corpus for PNs is good 
enough but not for LNs and ONs. 
Must be mentioned is that, in our evaluation, 
only NEs with both correct boundary and correct 
type label are considered as the correct 
recognitions, which is a little different from other 
evaluation systems. 
We also test our system on data set of sport, 
finance, news and entertainment domains. These 
test data are downloaded from Internet shown in 
Table 4.  
Number of NE 
Domain 
PN LN ON
File 
size 
Sport(S) 954 510 609 91K
Finance(F) 212 406 461 80K
News(N) 526 961 437 76K
Entertainment(E) 1016 511 133 100K
Total 2708 2388 1640 247K
Table 4 Statistic of Multi-field Test Data 
The results are shown in Table 5. 
 Precision Recall F-measure
S 80.17% 91.10% 85.28% 
F 61.35% 94.34% 74.35% 
N 88.66% 83.27% 85.88% 
PN 
E 82.20% 82.28% 82.24% 
S 82.90% 81.76% 82.33% 
F 83.72% 81.03% 82.35% 
N 91.95% 91.56% 91.75% 
LN
E 81.64% 87.87% 84.64% 
S 73.43% 67.16% 70.15% 
F 65.88% 60.30% 62.97% 
N 92.52% 84.70% 88.44% 
ON
E 78.30% 62.41% 69.46% 
Total 81.01% 81.24% 81.12% 
Table 5 Results on different domain 
Table 5 shows that the performance on financial 
domain is much lower. The reason is that, in 
financial domain, there are many stock names 
which are the abbreviation of organization names. 
Moreover, organization full name never appear in 
the text. So the system can?t recognize them as an 
organization name. However, on many occasions, 
they are recognized as person names. As a result, 
the precision of PNs declines, meanwhile, the 
precision and recall of ONs can?t be high. 
Based on the above analysis, we find that the 
main sources of errors in our system are as follows. 
First, we still have not found a good strategy for 
the abbreviation location names and organization 
names. Because abbreviative LNs and ONs 
sometimes appear before full LN, sometimes not, 
so the pool strategy can?t work well. 
Second, some famous organization names that 
always appear in the shape of abbreviation can?t be 
recognized as ON because the full name never 
appear such as ?? /GaoTong, ?? /Xinlang. 
However, these ONs are often recognized as PNs. 
Such errors are especially serious in finance 
domain shown Table 5. 
Third, many words can?t be found in ????
??/TONG YI CI CI LIN?. 
6 Conclusions 
Chinese NER is a more difficult task than English 
NER. Though many approaches have been tried, 
the result is still not satisfactory. In this paper, we 
present a hybrid algorithm of incorporating human 
knowledge into statistical model. Thus we only 
need a relative small-sized labeled corpus (one-
month?s Chinese People?s Daily tagged with NER 
tags at Peking University) and human knowledge, 
but can achieve better performance. The main 
contribution of this paper is putting forward an 
approach which can make up for the limitation of 
using the statistical model or human knowledge 
purely by combining them organically. 
Our lab was mainly devoted to cross-language 
information processing and its application. So in 
the future we will shift our algorithm to other 
languages. And fine-tune to a specific domain such 
as sports. 
ACKNOWLEDGEMENT 
This paper is supported by the National ?973? 
project G1998030501A-06 and the Natural Science 
Foundation of China 60272041. 
References 
Jian Sun, et al 2002. Chinese Named Entity 
Identification Using Class-based Language Model. 
Proceedings of the 19th International Conference on 
Computational Linguistics 
Hsin-His Chen, et al 1997. Description of the NTU 
System Used for MET2. Proceedings of the Seventh 
Message Understanding Conference 
Tat-Seng Chua, et al 2002. Learning Pattern Rules for 
Chinese Named Entity Extraction. Proceedings of 
AAAI?02 
W.J.Teahan, et al 1999. A Compression-based 
Algorithm for Chinese Word Segmentation. 
Computational Linguistic 26(2000) 375-393 
Maosong Sun, et al 1994. Identifying Chinese Names in 
Unrestricted Texts. Journal of Chinese Information 
Processing. 1994,8(2) 
Collins, Singer. 1999. Unsupervised Models for Named 
Entity Classification. Proceedings of 1999 Joint 
SIGDAT Conference on Empirical Methods in NLP 
and Very Large Corpora 
Daniel M. Bikel, et al 1997. Nymble: a High-
Performance Learning Name-finder. Proceedings of 
ANLP-97, page 194-201, 1997 
Yu et al 1998. Description of the Kent Ridge Digital 
Labs System Used for MUC-7. Proceedings of the 
Seventh Message Understanding Conference 
Silviu Cucerzan, David Yarowsky. 1999. Language 
Independent Named Entity Recognition Combining 
Morphological and Contextual Evidence. 
Proceedings 1999 Joint SIGDAT Conference on 
EMNLP and VLC 
Peter F.Brown, et al 1992. Class-Based n-gram Model 
of Natural Language. 1992 Association for 
Computational Linguistics 
A.Mikheev, M.Moens, and C.Grover. 1999. Named 
entity recognition without gazetteers. Proceedings of 
the Ninth Conference of the European Chapter of the 
Association for Computational Linguistics. Bergen, 
Norway 
Borthwich. A. 1999. A Maximum Entropy Approach to 
Named Entity Recognition. PhD Dissertation 
Dong & Dong. 2000. Hownet. At: http://www.keenage. 
com 
Yu.S.W. 1999. The Specification and Manual of 
Chinese Word Segmentation and Part of Speech 
Tagging. At: http://www.icl.pku.edu.cn/Introduction/ 
corpustagging. htm 
Mei.J.J, et al 1983. ??????/TONG YI CI CI 
LIN?. Shanghai CISHU Press 
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 56?63,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Cluster-based Language Model for Sentence Retrieval in Chinese 
Question Answering 
 
 
Youzheng Wu                               Jun Zhao                               Bo Xu 
National Laboratory of Pattern Recognition 
Institute of Automation Chinese Academy of Sciences 
No.95 Zhongguancun East Road, 100080, Beijing, China 
(yzwu, jzhao,boxu)@nlpr.ia.ac.cn 
 
  
 
Abstract 
Sentence retrieval plays a very important 
role in question answering system. In this 
paper, we present a novel cluster-based 
language model for sentence retrieval in 
Chinese question answering which is mo-
tivated in part by sentence clustering and 
language model. Sentence clustering is 
used to group sentences into clusters. 
Language model is used to properly rep-
resent sentences, which is combined with 
sentences model, cluster/topic model and 
collection model. For sentence clustering, 
we propose two approaches that are One-
Sentence-Multi-Topics and One-
Sentence-One-Topic respectively. From 
the experimental results on 807 Chinese 
testing questions, we can conclude that 
the proposed cluster-based language 
model outperforms over the standard lan-
guage model for sentence retrieval in 
Chinese question answering. 
1 Introduction 
To facilitate the answer extraction of question 
answering, the task of retrieval module is to find 
the most relevant passages or sentences to the 
question. So, the retrieval module plays a very 
important role in question answering system, 
which influences both the performance and the 
speed of question answering. In this paper, we 
mainly focus on the research of improving the 
performance of sentence retrieval in Chinese 
question answering. 
Many retrieval approaches have been pro-
posed for sentence retrieval in English question 
answering. For example, Ittycheriach [Ittycheriah, 
et al 2002] and H. Yang [Hui Yang, et al 2002] 
proposed vector space model. Andres [Andres, et 
al. 2004] and Vanessa [Vanessa, et al 2004] pro-
posed language model and translation model re-
spectively. Compared to vector space model, 
language model is theoretically attractive and a 
potentially very effective probabilistic frame-
work for researching information retrieval prob-
lems [Jian-Yun Nie. 2005]. 
However, language model for sentence re-
trieval is not mature yet, which has a lot of diffi-
cult problems that cannot be solved at present. 
For example, how to incorporate the structural 
information, how to resolve data sparseness 
problem. In this paper, we mainly focus on the 
research of the smoothing approach of language 
model because sparseness problem is more seri-
ous for sentence retrieval than for document re-
trieval. 
At present, the most popular smoothing ap-
proaches for language model are Jelinek-Mercer 
method, Bayesian smoothing using Dirichlet pri-
ors, absolute discounting and so on [C. Zhai, et al 
2001]. The main disadvantages of all these 
smoothing approaches are that each document 
model (which is estimated from each document) 
is interpolated with the same collection model 
(which is estimated from the whole collection) 
through a unified parameter. Therefore, it does 
not make any one particular document more 
probable than any other, on the condition that 
neither the documents originally contains the 
query term. In other word, if a document is rele-
vant, but does not contain the query term, it is 
still no more probable, even though it may be 
topically related. 
As we know, most smoothing approaches of 
sentence retrieval in question answering are 
learned from document retrieval without many 
adaptations. In fact, question answering has some 
56
characteristics that are different from traditional 
document retrieval, which could be used to im-
prove the performance of sentence retrieval. 
These characteristics lie in: 
1. The input of question answering is natural 
language question which is more unambiguous 
than query in traditional document retrieval. 
For traditional document retrieval, it?s difficult 
to identify which kind of information the users 
want to know. For example, if the user submit 
the query {??/invent, ??/telephone}, search 
engine does not know what information is 
needed, who invented telephone, when telephone 
was invented, or other information. On the other 
hand, for question answering system, if the user 
submit the question {???????/who in-
vented the telephone?}, it?s easy to know that the 
user want to know the person who invented the 
telephone, but not other information. 
2. Candidate answers extracted according to 
the semantic category of the question?s answer 
could be used for sentence clustering of question 
answering. 
Although the first retrieved sentences are re-
lated to the question, they usually deal with one 
or more topics. That is, relevant sentences for a 
question may be distributed over several topics. 
Therefore, treating the question?s words in re-
trieved sentences with different topics equally is 
unreasonable. One of the solutions is to organize 
the related sentences into several clusters, where 
a sentence can belong to about one or more clus-
ters, each cluster is regarded as a topic. This is 
sentence clustering. Obviously, cluster and topic 
have the same meaning and can be replaced each 
other. In the other word, a particular entity type 
was expected for each question, and every spe-
cial entity of that type found in a retrieved sen-
tence was regarded as a cluster/topic.  
In this paper, we propose two novel ap-
proaches for sentence clustering. The main idea 
of the approaches is to conduct sentence cluster-
ing according to the candidate answers which are 
also considered as the names of the clusters.  
For example, given the question {?????
??/who invented telephone?}, the top ten re-
trieved sentences and the corresponding candi-
date answers are shown as Table 1. Thus, we can 
conduct sentence clustering according to the 
candidate answers, that are, {??/Bell, ???
/Siemens, ???/Edison,??/Cooper, ???
/Stephen}.
 
ID Top 10 Sentences Candidate Answer 
S1 1876? 3? 10???????/Bell invented telephone on Oct. 3th, 1876. ??/Bell 
S2 
????????????????????????
/ Bell, Siemens and Edison invented telephone, electromo-
tor and electric light respectively. 
???/ Siemens 
??/Bell 
???/ Edison 
S3 
??? ??????? ????????????
/Recently, the public paid a great deal of attention to Cooper 
who is Father of Mobile Phone. 
??/Cooper 
S4 1876 ????????????? /In 1876, Bell in-vented telephone. ??/Bell 
S5 
???1876 ???????????????1879 ?
??????????????/Subsequently, American 
scientist Bell invented the phone in 1876; Edison invented 
the electric light in 1879. 
??/Bell 
???/Edison 
S6 1876 ? 3 ? 7 ???????????????/On March 7th, 1876, Bell became the patentee of telephone. ??/Bell 
S7 
????????????????????????
???/Bell not only invented telephone, but also estab-
lished his own company for spreading his invention. 
??/Bell 
S8 
??????????? 30 ???????????
????????????????/Thirty years after 
the invention of first mobile phone, Cooper still anticipated 
the date of the realization of future phone?s technology. 
??/Cooper 
57
S9 
????????????????????????
????????????????????????
???/Cooper said, he was surprised at the speed that the 
consumers switched to mobile phones; but the populariza-
tion of mobile phone isn?t omnipresent, which made him a 
little bit disappointed. 
??/Cooper 
S10 
????????????????????????
???????????/England inventor Stephen de-
signed the paper-clicked CMOS chip which included all 
electronic components. 
???/Stephen 
Table 1 The Top 10 Retrieved Sentences and its Candidate Answers 
Based on the above analysis, this paper pre-
sents cluster-based language model for sentence 
retrieval of Chinese question answering. It dif-
fers from most of the previous approaches 
mainly as follows. 1. Sentence Clustering is con-
ducted according to the candidate answers ex-
tracted from the top 1000 sentences. 2. The in-
formation of the cluster of the sentence, which is 
also called as topic, is incorporated into language 
model through aspect model. For sentence clus-
tering, we propose two novel approaches that are 
One-Sentence-Multi-Topics and One-Sentence-
One-Topic respectively. The experimental results 
show that the performances of cluster-based lan-
guage model for sentence retrieval are improved 
significantly. 
The framework of cluster-based language 
model for sentence retrieval is shown as Figure 1. 
 
Figure 1 The Framework of Cluster-based Language Model for Sentence Retrieval 
2 Language Model for Information Re-
trieval 
Language model for information retrieval is pre-
sented by Ponte & Croft in 1998[J. Ponte, et al 
1998] which has more advantages than vector 
space model. After that, many improved models 
are proposed like J.F. Gao [J.F Gao, et al 2004], 
C. Zhai [C. Zhai, et al 2001], and so on. In 1999, 
Berger & Lafferty [A. Berger, et al 1999] pre-
sented statistical translation model for informa-
tion retrieval. 
The basic approach of language model for in-
formation retrieval is to model the process of 
generating query Q. The approach has two steps. 
1. Constructing document model for each docu-
ment in the collection; 2. Ranking the documents 
according to the probabilities p(Q|D). A classical 
unigram language model for IR could be ex-
pressed in equation (1). 
( ) ( )?
Qw
i
i
D|wpD|Qp
?
=                                   (1) 
where, wi is a query term, p(wi|D) is document 
model which represents terms distribution over 
document. Obviously, estimating the probability 
p(wi|D) is the key of document model. To solve 
the sparseness problem, Jelinek-Mercer is com-
monly used which could be expressed by equa-
tion (2). 
( ) ( ) ( ) ( )C|wp?1D|wp?D|wp MLML ?+?= -   (2) 
where, pML(w|D) and pML(w|C) are document 
model and collection model respectively esti-
mated via maximum likelihood. 
Question 
Document 
Retrieval 
Sentence 
Splitter 
Candidate An-
swer Extraction
Language Model 
for Sentence Re-
trieval
Sentence Clus-
tering 
Results 
Cluster-based Lan-
guage Model for 
Sentence Retrieval
Question 
Analyzer 
58
As described above, the disadvantages of 
standard language model is that it does not make 
any one particular document any more probable 
than any other, on the condition that neither the 
documents originally contain the query term. In 
the other word, if a document is relevant, but 
does not contain the query term, it is still no 
more probable, even though it may be topically 
related. Thus, the smoothing approaches based 
on standard language model are improper. In this 
paper, we propose a novel cluster-based lan-
guage model to overcome it. 
3 Cluster-based Language Model for 
Sentence Retrieval 
Note that document model p(w|D) in document 
retrieval is replace by p(w|S) called sentence 
model in sentence retrieval. 
The assumption of cluster-based language 
model for retrieval is that topic-related sentences 
tend to be relevant to the same query. So, incor-
porating the topic of sentences into language 
model can improve the performance of sentence 
retrieval based on standard language model. 
The proposed cluster-based language model is 
a mixture model of three components, that are 
sentence model pML(w|S), cluster/topic model 
p_topicML(w|T) and collection model pML(w|C). 
We can formulate our model as equation (3). 
( ) ( ) ( )
( ) ( ) ( )( )Cwp?1Twp_topic?
?1Swp?S|wp
MLML
ML
|?+|?
?+|?=
-
-          (3) 
In fact, the cluster-based language model can 
also be viewed as a two-stage smoothing ap-
proach. The cluster model is first smoothed using 
the collection model, and the sentence model is 
then smoothed with the smoothed cluster model. 
In this paper, the cluster model is in the form 
of term distribution over cluster/topic, associated 
with the distribution of clusters/topics over sen-
tence, which can be expressed by equation (4).  
( ) ( ) ( )?
?Tt
StptwpTwp_topic ||=|                     (4) 
where, T is the set of clusters/topics. p_topic(w|T) 
is cluster model. p(t|S) is topic sentence distribu-
tion which means the distribution of topic over 
sentence. And p(w|t) is term topic distribution 
which means the term distribution over topics. 
Before estimating the sentence model p(w|S), 
topic-related sentences should be organized into 
clusters/topics to estimate p(t|S) and p(w|t) prob-
abilities. For sentence clustering, this paper pre-
sents two novel approaches that are One-
Sentence-Multi-Topics and One-Sentence-One-
Topic respectively. 
3.1 One-Sentence-Multi-Topics 
The main idea of One-Sentence-Multi-Topics 
can be summarized as follows. 
1. If a sentence includes M different candidate 
answers, then the sentence consists of M different 
topics. 
For example, the sentence S5 in Table 1 includes 
two topics which are ???????/Bell in-
vented telephone? and ????????/Edison 
invented electric light? respectively. 
2. Different sentences have the same topic if two 
candidate answers are same. 
For example, the sentence S4 and S5 in Table 1 
have the same topic ??????? /Bell in-
vented telephone? because both of sentences 
have the same candidate answer ???/Bell?. 
Based on the above ideas, the result of sen-
tence clustering based on One-Sentence-Multi-
Topics is shown in Table 2. 
Name of Clusters Sentences 
??/Bell S1 S2 S4 S5 S6 S7 S8 
???/Siemens S2 
???/Edison S2 S5 
??/Cooper S3 S8 S9 
???/Stephen S10 
Table 2 The Result of One-Sentence-Multi-
Topics Sentence Clustering 
So, we could estimate term topic distribution 
using equation (5). 
( ) ( )( )?
w'
t,w'n
twn
twp
,=|                                         (5) 
Topic sentence distribution can be estimated 
using equation (6) and (7). 
( )
? /
/=|
t
st
st
kl1
kl1
Stp                                            (6) 
( ) ( ) ( )( )?w ML
ML
MLst t|wp
swp
logs|wptsKLkl
|?=||=    (7) 
where, klst means the Kullback-Leibler diver-
gence between the sentence with the cluster/topic. 
k denotes the number of cluster/topic. The main 
idea of equation (6) is that the closer the Kull-
back-Leibler divergence, the larger the topic sen-
tence probability p(t|S). 
3.2 One-Sentence-One-Topic 
The main idea of One-Sentence-One-Topic also 
could be summarized as follows. 
59
1. A sentence only has one kernel candidate an-
swer which represents the kernel topic no matter 
how many candidate answers is included. 
For example, the kernel topic of sentence S5 in 
Table 1 is ???????/Bell invented tele-
phone? though it includes three different candi-
date answers. 
2. Different sentences have the same topic if two 
kernel candidate answers are same. 
For example, the sentence S4 and S5 in Table 1 
have the same topic ??????? /Bell in-
vented telephone?. 
3. The kernel candidate answer has shortest av-
erage distance to all query terms. 
Based on the above ideas, the result of sen-
tence clustering based on One-Sentence-One-
Topic is shown in Table 3. 
Name of Clusters Sentences 
??/Bell S1 S2 S4 S5 S6 S7
??/Cooper S3 S8 S9 
???/Stephen S10 
Table 3 The Result of One-Sentence-One-Topic 
Sentence Clustering 
Equation (8) and (9) can be used to estimate 
the kernel candidate answer and the distances of 
candidate answers respectively. Term topic dis-
tribution in One-Sentence-One-Topic can be es-
timated via equation (5). And topic sentence dis-
tribution is equal to 1 because a sentence only 
belongs to one cluster/topic. { }
i
i
a
a
*
i SemDis  a argmin=                               (8) 
( )
N
q,aSemDis
SemDis j
ji
ai
?
=
                         (9) 
( )
ji qaji PositionPositionqaSemDis -=,           (10) 
where, ai* is the kernel candidate answer. ai is 
the i-th candidate answer, 
iaSemDis is the average 
distance of i-th candidate answer. qj is the j-th 
query term, N is the number of all query terms. 
jqPosition and iaPosition  mean the position of 
query term qj and candidate answer ai. 
4 Experiments and Analysis 
Research on Chinese question answering, is still 
at its early stage. And there is no public evalua-
tion platform for Chinese question answering. So 
in this paper, we use the evaluation environment 
presented by [Youzheng Wu, et al 2004] which 
is similar to TREC question answering track 
[Ellen. M. Voorhees. 2004]. The documents col-
lection is downloaded from Internet which size is 
1.8GB. The testing questions are collected via 
four different approaches which has 7050 Chi-
nese questions currently. 
In this section, we randomly select 807 testing 
questions which are fact-based short-answer 
questions. Moreover, the answers of all testing 
questions are named entities identified by 
[Youzheng Wu, et al 2005]. Figure 2 gives the 
details. Note that, LOC, ORG, PER, NUM and 
TIM denote the questions which answer types 
are location, organization, person, number and 
time respectively, SUM means all question types. 
165
311
28
168
135
0
100
200
300
400
PER LOC ORG TIM NUM
 
Figure 2 The Distribution of Various Question 
Types over Testing Questions 
Chinese question answering system is to re-
turn a ranked list of five answer sentences per 
question and will be strictly evaluated (unsup-
ported answers counted as wrong) using mean 
reciprocal rank (MRR). 
4.1 Baseline: Standard Language Model for 
Sentence Retrieval 
Based on the standard language model for infor-
mation retrieval, we can get the baseline per-
formance, as is shown in Table 4, where ? is the 
weight of document model. 
? 0.6 0.7 0.8 0.9 
LOC 49.95 51.50 52.63 54.54
ORG 53.69 51.01 50.12 51.01
PER 63.10 64.42 65.94 65.69
NUM 48.43 49.86 51.78 53.26
TIM 56.97 58.38 58.77 61.49
SUM 53.98 55.28 56.40 57.93
Table 4 The Baseline MRR5 Performance 
60
In the following chapter, we conduct experi-
ments to answer two questions.  
1. Whether cluster-based language model for 
sentence retrieval could improve the perform-
ance of standard language model for sentence 
retrieval? 
2. What are the performances of sentence clus-
tering for various question types? 
4.2 Cluster-based Language Model for Sen-
tence Retrieval 
In this part, we will conduct experiments to vali-
date the performances of cluster-based language 
models which are based on One-Sentence-Multi-
Topics and One-Sentence-One-Topic sentence 
clustering respectively. In the following experi-
ments, ? = 0.9. 
4.2.1 Cluster-based Language Model Based 
on One-Sentence-Multi-Topics 
The experimental results of cluster-based lan-
guage model based on One-Sentence-Multi-
Topics sentence clustering are shown in Table 5. 
The relative improvements are listed in the 
bracket. 
? 0.6 0.7 0.8 0.9 
LOC 55.57 (+11.2) 
55.61 
(+7.98) 
56.59 
(+7.52) 
57.70 
(+5.79)
ORG 59.05 (+9.98) 
59.46 
(+16.6) 
59.46 
(+18.6) 
59.76 
(+17.2)
PER 67.73 (+7.34) 
68.03 
(+5.60) 
67.71 
(+2.68) 
67.45 
(+2.68)
NUM 52.79 (+9.00) 
53.90 
(+8.10) 
54.45 
(+5.16) 
55.51 
(+4.22)
TIM 60.17 (+5.62) 
60.63 
(+3.85) 
62.33 
(+6.06) 
61.68 
(+0.31)
SUM 58.14 (+7.71) 
58.63 
(+6.06) 
59.30 
(+5.14) 
59.54 
(+2.78)
Table 5 MRR5 Performance of Cluster-based 
Language Model Based on One-Sentence-Multi-
Topics 
From the experimental results, we can find 
that by integrating the clusters/topics of the sen-
tence into language model, we can achieve much 
improvement at each stage of ?. For example, the 
largest and smallest improvements for all types 
of questions are about 7.7% and 2.8% respec-
tively. This experiment shows that the proposed 
cluster-based language model based on One-
Sentence-Multi-Topics is effective for sentence 
retrieval in Chinese question answering.  
4.2.2 Cluster-based Language Model Based 
on One-Sentence-One-Topic 
The performance of cluster-based language 
model based on One-Sentence-One-Topic sen-
tence clustering is shown in Table 6. The relative 
improvements are listed in the bracket. 
? 0.6 0.7 0.8 0.9 
LOC 53.02 (+6.15)
54.27 
(+5.38) 
56.14 
(+6.67) 
56.28 
(+3.19)
ORG 58.75 (+9.42)
58.75 
(+17.2) 
59.46 
(+18.6) 
59.46 
(+16.6)
PER 66.57 (+5.50)
67.07 
(+4.11) 
67.44 
(+2.27) 
67.29 
(+2.44)
NUM 49.95 (+3.14)
50.87 
(+2.02) 
52.15 
(+0.71) 
53.51 
(+0.47)
TIM 59.75 (+4.88)
60.65 
(+3.89) 
62.71 
(+6.70) 
62.20 
(+1.15)
SUM 56.48 (+4.63)
57.65 
(+4.29) 
58.82 
(+4.29) 
59.22 
(+2.23)
Table 6 MRR5 Performance of Cluster-based 
Language Model Based on One-Sentence-One-
Topic 
In Comparison with Table 5, we can find that 
the improvement of cluster-based language 
model based on One-Sentence-One-Topic is 
slightly lower than that of cluster-based language 
model based on One-Sentence-Multi-Topics. The 
reasons lie in that Clusters based on One-
Sentence-One-Topic approach are very coarse 
and much information is lost. But the improve-
ments over baseline system are obvious. 
Table 7 shows that MRR1 and MRR20 scores 
of cluster-based language models for all question 
types. The relative improvements over the base-
line are listed in the bracket. This experiment is 
to validate whether the conclusion based on dif-
ferent measurements is consistent or not. 
 One-Sentence-Multi-Topics 
One-Sentence-
One-Topic 
? MRR1 MRR20 MRR1 MRR20
0.6 50.00 (+14.97)
59.60 
(+7.66) 
48.33 
(+10.37) 
57.70 
(+4.23)
0.7 50.99 (+13.36)
60.03 
(+6.12) 
49.44 
(+9.92) 
58.62 
(+3.62)
0.8 51.05 (+8.99) 
60.68 
(+5.06) 
51.05 
(+8.99) 
60.01 
(+3.90)
0.9 51.92 (+5.81) 
61.05 
(+2.97) 
51.30 
(+4.54) 
60.25 
(+1.62)
Table 7 MRR1 and MRR20 Performances of 
Two Cluster-based Language Models 
61
Table 7 also shows that the performances of 
two cluster-based language models are higher 
than that of the baseline system under different 
measurements. For MRR1 scores, the largest 
improvements of cluster-based language models 
based on One-Sentence-Multi-Topics and One-
Sentence-One-Topic are about 15% and 10% 
respectively. For MRR20, the largest improve-
ments are about 7% and 4% respectively. 
Conclusion 1: The experiments show that the 
proposed cluster-based language model can im-
prove the performance of sentence retrieval in 
Chinese question answering under the various 
measurements. Moreover, the performance of 
clustering-based language model based on One-
Sentence-Multi-Topics is better than that based 
on One-Sentence-One-Topic. 
4.3 The Analysis of Sentence Clustering for 
Various Question Types 
The parameter ? in equation (3) denotes the bal-
ancing factor of the cluster model and the collec-
tion model. The larger ?, the larger contribution 
of the cluster model. The small ?, the larger con-
tribution of the collection model. If the perform-
ance of sentence retrieval decreased with the in-
creasing of ?, it means that there are many noises 
in sentence clustering. Otherwise, sentence clus-
tering is satisfactory for cluster-based language 
model. So the task of this experiment is to find 
the performances of sentence clustering for vari-
ous question types, which is helpful to select the 
most proper ? to obtain the best performance of 
sentence retrieval. 
With the change of ? and the fixed ? (? = 0.9), 
the performances of cluster-based language 
model based on One-Sentence-Multi-Topics are 
shown in Figure 3. 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
SUM
LOC
ORG
PER
NUM
TIM
 
Figure 3 MRR5 Performances of Cluster-based 
Language Model Based on One-Sentence-Multi-
Topics with the Change of ? 
In Figure 3, the performances of TIM and 
NUM type questions decreased with the increas-
ing of the parameter ? (from 0.6 to 0.9), while 
the performances of LOC, PER and ORG type 
questions increased. This phenomenon showed 
that the performance of sentence clustering based 
on One-Sentence-Multi-Topics for TIM and 
NUM type questions is not as good as that for 
LOC, PER and ORG type questions. This is in 
fact reasonable. The number and time words fre-
quently appeared in the sentence, which does not 
represent a cluster/topic when they appear. While 
PER, LOC and ORG entities can represent a 
topic when they appeared in the sentence. 
Similarly, with the change of ? and the fixed ? 
(?=0.9), the performances of cluster-based lan-
guage model based on One-Sentence-One-Topic 
are shown in Figure 4. 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
SUM
LOC
ORG
PER
NUM
TIM
 
Figure 4 MRR5 Performance of Cluster-based 
Language Model Based on One-Sentence-One-
Topic with the Change of ? 
In Figure 4, the performances of TIM, NUM, 
LOC and SUM type questions decreased with the 
increasing of ? (from 0.6 to 0.9). This phenome-
non shows that the performances of sentence 
clustering based on One-Sentence-One-Topic are 
not satisfactory for most of question types. But, 
compared to the baseline system, the cluster-
based language model based on this kind of sen-
tence clustering can still improve the perform-
ances of sentence retrieval in Chinese question 
answering. 
Conclusion 2: The performance of the pro-
posed sentence clustering based on One-
Sentence-Multi-Topics for PER, LOC and ORG 
type questions is higher than that for TIM and 
NUM type questions. Thus, for PER, LOC and 
ORG questions, we should choose the larger ? 
value (about 0.9) in cluster-based language 
model based on One-Sentence-Multi-Topics. 
While for TIM and NUM type questions, the 
62
value of ? should be smaller (about 0.5). But, the 
performance of sentence clustering based on 
One-Sentence-One-Topic for all questions is not 
ideal, so the value for cluster-based language 
model based on One-Sentence-One-Topic should 
be smaller (about 0.5) for all questions. 
5 Conclusion and Future Work 
The input of a question answering system is 
natural language question which contains richer 
information than the query in traditional docu-
ment retrieval. Such richer information can be 
used in each module of question answering sys-
tem. In this paper, we presented a novel cluster-
based language model for sentence retrieval in 
Chinese question answering which combines the 
sentence model, the cluster/topic model and the 
collection model. 
For sentence clustering, we presented two ap-
proaches that are One-Sentence-Multi-Topics 
and One-Sentence-One-Topic respectively. The 
experimental results showed that the proposed 
cluster-based language model could improve the 
performances of sentence retrieval in Chinese 
question answering significantly. 
However, we only conduct sentence clustering 
for questions, which have the property that their 
answers are named entities in this paper. In the 
future work, we will focus on all other type ques-
tions and improve the performance of the sen-
tence retrieval by introducing the structural, syn-
tactic and semantic information into language 
model. 
Reference 
J. Ponte, W. Bruce Croft. A Language Modeling Ap-
proach to Information Retrieval. In the Proceedings 
of ACM SIGIR 1998, pp 275-281, 1998. 
C. Zhai, J. Lafferty. A Study of Smoothing Tech-
niques for Language Modeling Applied to ad hoc 
Information Retrieval. In the Proceedings of the 
ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, 2001. 
Ittycheriah, S. Roukos. IBM's Statistical Question 
Answering System-TREC 11. In the Eleventh Text 
Retrieval Conference (TREC 2002), Gaithersburg, 
Maryland, November 2002. 
Hui Yang, Tat-Seng Chua. The Integration of Lexical 
Knowledge and External Resources for Question 
Answering. In the Proceedings of the Eleventh 
Text REtrieval Conference (TREC?2002), Mary-
land, USA, 2002, page 155-161. 
Andres Corrada-Emmanuel, W.Bruce Croft, Vanessa 
Murdock. Answer Passage Retrieval for Question 
Answering. In the Proceedings of the 27th Annual 
International Conference on Research and Devel-
opment in Information Retrieval, pp. 516 ? 517, 
2004. 
Ellen M. Voorhees. Overview of the TREC 2004 
Question Answering Track. In Proceedings of the 
Twelfth Text REtrieval Conference (TREC 2004), 
2004. 
Vanessa Murdock, W. Bruce Croft. Simple Transla-
tion Models for Sentence Retrieval in Factoid 
Question Answering. In the Proceedings of the 
SIGIR 2004 Workshop on Information Retrieval 
for Question Answering, pp.31-35, 2004. 
Thomas Hofmann. Probabilistic Latent Semantic In-
dexing. In the Proceedings of the Twenty-Second 
Annual International SIGIR Conference on Re-
search and Development in Information Retrieval, 
1999. 
A. Berger and J. Lafferty. Information Retrieval as 
Statistical Translation. In the Proceedings of ACM 
SIGIR-1999, pp. 222?229, Berkeley, CA, August 
1999. 
A. Echihabi and D.Marcu. A noisy-channel approach 
to question answering. In the Proceeding of the 
41st Annual Meeting of the Association for Com-
putational Linguistics, Sappora, Japan, 2003. 
Leif Azzopardi, Mark Girolami and Keith van 
Rijsbergen. Topic Based Language Models for ad 
hoc Information Retrieval. In the Proceeding of 
IJCNN 2004 & FUZZ-IEEE 2004, July 25-29, 
2004, Budapest, Hungary. 
Jian-Yun Nie. Integrating Term Relationships into 
Language Models for Information Retrieval. Re-
port at ICT-CAS. 
Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu and 
Guihong Cao. 2004b. Dependence language model 
for information retrieval. In SIGIR-2004. Sheffield, 
UK, July 25-29. 
Youzheng Wu, Jun Zhao, Bo Xu. Chinese Named 
Entity Recognition Model Based on Multiple Fea-
tures. In the Proceeding of HLT/EMNLP 2005, 
Vancouver, B.C., Canada, pp.427-434, 2005. 
Youzheng Wu, Jun Zhao, Xiangyu Duan  and Bo Xu. 
Building an Evaluation Platform for Chinese Ques-
tion Answering Systems. In Proceeding of the First 
National Conference on Information Retrieval and 
Content Security. Shanghai, China, December, 
2004.(In Chinese) 
 
63
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 113?120
Manchester, August 2008
Learning Reliable Information for Dependency Parsing Adaptation
Wenliang Chen
?
, Youzheng Wu
?
, Hitoshi Isahara
?
?
Language Infrastructure Group
?
Spoken Language Communication Group, ATR
?
Machine Translation Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, youzheng.wu, isahara}@nict.go.jp
Abstract
In this paper, we focus on the adaptation
problem that has a large labeled data in the
source domain and a large but unlabeled
data in the target domain. Our aim is to
learn reliable information from unlabeled
target domain data for dependency pars-
ing adaptation. Current state-of-the-art sta-
tistical parsers perform much better for
shorter dependencies than for longer ones.
Thus we propose an adaptation approach
by learning reliable information on shorter
dependencies in an unlabeled target data
to help parse longer distance words. The
unlabeled data is parsed by a dependency
parser trained on labeled source domain
data. The experimental results indicate
that our proposed approach outperforms
the baseline system, and is better than cur-
rent state-of-the-art adaptation techniques.
1 Introduction
Dependency parsing aims to build the dependency
relations between words in a sentence. There
are many supervised learning methods for training
high-performance dependency parsers(Nivre et al,
2007), if given sufficient labeled data. However,
the performance of parsers declines when we are in
the situation that a parser is trained in one ?source?
domain but is to parse the sentences in a second
?target? domain. There are two tasks(Daum?e III,
2007) for the domain adaptation problem. The
first one is that we have a large labeled data in the
source domain and a small labeled data in target
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
domain. The second is similar, but instead of hav-
ing a small labeled target data, we have a large but
unlabeled target data. In this paper, we focus on
the latter one.
Current statistical dependency parsers perform
worse while the distance of two words is becoming
longer for domain adaptation. An important char-
acteristic of parsing adaptation is that the parsers
perform much better for shorter dependencies than
for longer ones (the score at length l is much higher
than the scores at length> l ).
In this paper, we propose an approach by using
the information on shorter dependencies in auto-
parsed target data to help parse longer distance
words for adapting a parser. Compared with the
adaptation methods of Sagae and Tsujii (2007) and
Reichart and Rappoport (2007), our approach uses
the information on word pairs in auto-parsed data
instead of using the whole sentences as newly la-
beled data for training new parsers. It is difficult
to detect reliable parsed sentences, but we can find
relative reliable parsed word pairs according to de-
pendency length. The experimental results show
that our approach significantly outperforms base-
line system and current state of the art techniques.
2 Motivation and prior work
In dependency parsing, we assign head-dependent
relations between words in a sentence. A simple
example is shown in Figure 1, where the arc be-
tween a and hat indicates that hat is the head of a.
Current statistical dependency parsers perform
better if the dependency lengthes are shorter (Mc-
Donald and Nivre, 2007). Here the length of the
dependency from word w
i
to word w
j
is simply
equal to |i ? j|. Figure 2 shows the results (F
1
113
The  boy  saw    a       red       hat    .
Figure 1: An example for dependency relations.
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  2  4  6  8  10  12  14  16  18  20
F1
Dependency Length
sameDomain
diffDomain
Figure 2: The scores relative to dependency length.
?SameDomain? refers to training and testing in the
same domain, and ?diffDomain? refers to training
and testing in two domains (domain adaptation).
score)
1
on our testing data, provided by a deter-
ministic parser, which is trained on labeled source
data. Comparing two curves at the figure, we find
that the scores of diffDomain decreases muchmore
sharply than the scores of sameDomain, when de-
pendency length increases. The score decreases
from about 92% at length 1 to 50% at 7. When
lengthes are larger than 7, the scores are below
50%. We also find that the score at length l is much
higher (around 10%) than the score at length l + 1
from length 1 to 7. There is only one exception that
the score at length 4 is a little less than the score at
length 5. But this does not change so much and the
scores at length 4 and 5 are much higher than the
one at length 6.
Two words (word w
i
and word w
j
) having a
dependency relation in one sentence can be adja-
cent words (word distance = 1), neighboring words
(word distance = 2), or the words with distance >
2 in other sentences. Here the distance of word
pair (word w
i
and word w
j
) is equal to |i ? j|. For
example, ?a? and ?hat? has dependency relation in
the sentence at Figure 1. They can also be adjacent
words in the sentence ?The boy saw a hat.? and
the words with distance = 3 in ?I see a red beauti-
ful hat.?. This makes it possible for the word pairs
with different distances to share the information.
According to the above observations, we present
1
F
1
= 2 ? precision ? recall/(precision + recall) where
precision is the percentage of predicted arcs of length d that
are correct and recall is the percentage of gold standard arcs
of length d that are correctly predicted.
an idea that the information on shorter depen-
dencies in auto-parsed target data is reliable for
parsing the words with longer distance for do-
main adaptation. Here, ?shorter? is not exactly
short. That is to say, the information on depen-
dency length l in auto-parsed data can be used to
help parse the words whose distances are longer
than l when testing, where l can be any number.
We do not use the dependencies whose lengthes
are too long because the accuracies of long depen-
dencies are very low.
In the following content, we demonstrate our
idea with an example. The example shows how to
use the information on length 1 to help parse two
words whose distance is longer than 1. Similarly,
the information on length l can also be used to help
parse the words whose distance is longer than l.
Figure 2 shows that the dependency parser per-
forms best at tagging the relations between adja-
cent words. Thus, we expect that dependencies of
adjacent words in auto-parsed target data can pro-
vide useful information for parsing words whose
distances are longer than 1. We suppose that our
task is Chinese dependency parsing adaptation.
Here, we have two words ???JJ(large-scale)?
and ???NN(exhibition)?. Figure 3 shows
the examples in which word distances of these
two words are different. For the sentences in
the bottom part, there is a ambiguity of ?JJ
+ NN1 + NN2? at ?? ?JJ(large-scale)/?
?NN(art)/??NN(exhibition)?, ???JJ(large-
scale)/? ?NN(culture)/? ?NN(art)/?
?NN(exhibition)? and ???JJ(large-scale)/?
?NR(China)/? ?NN(culture)/? ?NN(art)/?
?NN(exhibition)?. Both NN1 and NN2 could be
the head of JJ. In the examples in the upper part,
???JJ(large-scale)? and ???NN(exhibition)?
are adjacent words, for which current parsers
can work well. We use a parser to parse the
sentences in the upper part. ???(exhibition)? is
assigned as the head of ???(large-scale)?. Then
we expect the information from the upper part
can help parse the sentences in the bottom part.
Now, we consider what a learning model could
do to assign the appropriate relation between ??
?(large-scale)? and ???(exhibition)? in the
bottom part. We provide additional information
that ???(exhibition)? is the possible head of ??
?(large-scale)? in the auto-parsed data (the upper
part). In this way, the learning model may use this
information to make correct decision.
114
A1)?	
///?
A2)?///Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 33?41, Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Unsupervised SVM Classifier for Answer Selection in Web
Question Answering
Youzheng Wu, Ruiqiang Zhang, Xinhui Hu, and Hideki Kashioka
National Institute of Information and Communications Technology (NICT),
ATR Spoken Language Communication Research Labs.
2-2-2 Hikaridai ?Keihanna Science City? Kyoto 619-0288 Japan
{Youzheng.wu,Ruiqiang.zhang,Xinhui.hu,Hideki.kashioka}@atr.jp
Abstract
Previous machine learning techniques for
answer selection in question answering
(QA) have required question-answer train-
ing pairs. It has been too expensive and
labor-intensive, however, to collect these
training pairs. This paper presents a novel
unsupervised support vector machine (U-
SVM) classifier for answer selection, which
is independent of language and does not re-
quire hand-tagged training pairs. The key
ideas are the following: 1. unsupervised
learning of training data for the classifier by
clustering web search results; and 2. select-
ing the correct answer from the candidates
by classifying the question. The compara-
tive experiments demonstrate that the pro-
posed approach significantly outperforms
the retrieval-based model (Retrieval-M), the
supervised SVM classifier (S-SVM), and the
pattern-based model (Pattern-M) for answer
selection. Moreover, the cross-model com-
parison showed that the performance rank-
ing of these models was: U-SVM > Pattern-
M > S-SVM > Retrieval-M.
1 Introduction
The purpose of answer selection in QA is to se-
lect the exact answer to the question from the ex-
tracted candidate answers. In recent years, many
supervised machine learning techniques for answer
selection in open-domain question answering have
been investigated in some pioneering studies [Itty-
cheriah et al 2001; Ng et al 2001; Suzuki et al
2002; Sasaki, et al 2005; and Echihabi et al 2003].
Compared with retrieval-based [Yang et al 2003],
pattern-based [Ravichandran et al 2002 and Soub-
botin et al 2002], and deep NLP-based [Moldovan
et al 2002, Hovy et al 2001; and Pasca et al 2001]
answer selection, machine learning techniques are
more effective in constructing QA components from
scratch. These techniques suffer, however, from the
problem of requiring an adequate number of hand-
tagged question-answer training pairs. It is too ex-
pensive and labor intensive to collect such training
pairs for supervised machine learning techniques.
To tackle this knowledge acquisition bottleneck,
this paper presents an unsupervised SVM classifier
for answer selection, which is independent of lan-
guage and question type, and avoids the need for
hand-tagged question-answer pairs. The key ideas
are as follows:
1. Regarding answer selection as a kind of classi-
fication task and adopting an SVM classifier;
2. Applying unsupervised learning of pseudo-
training data for the SVM classifier by cluster-
ing web search results;
3. Training the SVM classifier by using three
types of features extracted from the pseudo-
training data; and
4. Selecting the correct answer from the candidate
answers by classifying the question. Note that
this means classifying a question into one of
the clusters learned by clustering web search
results. Therefore, our classifying the question
33
Figure 1: Web Question Answering Architecture
is different from conventional question classifi-
cation (QC) [Li et al 2002] that determines the
answer type of the question.
The proposed approach is fully unsupervised and
starts only from a user question. It does not require
richly annotated corpora or any deep linguistic tools.
To the best of our knowledge, no research on this
kind of study we discuss here has been reported.
Figure 1 illustrates the architecture of our web QA
approach. The S-SVM and Pattern-M models are
included for comparison.
Because the focus of this paper just evaluates the
answer selection part, our approach requires knowl-
edge of the answer type to the question in order to
find candidate answers, and that the answer must be
a NE for convenience in candidate extraction. Ex-
periments using Chinese versions of the TREC 2004
and 2005 test data sets show that our approach sig-
nificantly outperforms the S-SVM for answer selec-
tion, with a top 1 score improvement of more than
20%. Results obtained with the test data set in [Wu
et al 2004] show that the U-SVM increases the
top 1/mrr 5/top 5 scores by 5.95%/6.06%/8.68%
as compared with the Pattern-M. Moreover, our
cross-model comparison demonstrates that the per-
formance ranking of all models considered is: U-
SVM > Pattern-M > S-SVM > Retrieval-M.
2 Comparison among Models
Related researches on answer selection in QA can be
classified into four categories. The retrieval-based
model [Yang et al 2003] selects a correct answer
from the candidates according to the distance be-
tween a candidate and all question keywords. This
model does not work, however, if the question and
the answer-bearing sentences do not match on the
surface. The pattern-based model [Ravichandran
et al 2002 and Soubbotin et al 2002] first clas-
sifies the question into predefined categories, and
then extracts the exact answer by using answer pat-
terns learned off-line. Although the pattern-based
model can obtain high precision for some prede-
fined types of questions, it is difficult to define ques-
tion types in advance for open-domain question an-
swering. Furthermore, this model is not suitable for
all types of questions. The deep NLP-based model
[Moldovan et al 2002; Hovy et al 2001; and Pasca
et al 2001] usually parses the user question and an
answer-bearing sentence into a semantic represen-
tation, and then semantically matches them to find
the answer. This model has performed very well at
TREC workshops, but it heavily depends on high-
performance NLP tools, which are time consuming
and labor intensive for many languages. Finally, the
machine learning-based model has also been inves-
tigated. current models of this type are based on su-
pervised approaches [Ittycheriah et al 2001; Ng et
al. 2001; Suzuki et al 2002; and Sasaki et al 2005]
that are heavily dependent on hand-tagged question-
answer training pairs, which not readily available.
In response to this situation, this paper presents
the U-SVM for answer selection in open-domain
web question answering system. Our U-SVM has
the following advantages over supervised machine
learning techniques. First, the U-SVM classifies
questions into a question-dependent set of clusters,
and the answer is the name of a question cluster.
In contrast, most previous models have classified
candidates into positive and negative. Second, the
U-SVM automatically learns the unique question-
dependent clusters and the pseudo-training for each
34
Table 1: Comparison of Various Machine Learning Techniques
System Model Key Idea Training Data
[Ittycheriah et al 2001] ME Classifier Classifying candidates into positive
and negative
5,000 English
Q-A pairs
[Suzuki et al 2002] SVM Classifier Classifying candidates into positive
and negative
1358 Japanese
Q-A pairs
[Echihabi et al 2003] N-C Model Selecting correct answer by aligning
question with sentences
90,000 English
Q-A pairs
[Sasaki et al 2005] ME Classifier Classifying words in sentences into an-
swer and non-answer words
2,000 Japanese
Q-A pairs
Our U-SVM Model SVM Classifier Classifying question into a set of
question-dependent clusters
No Q-A pairs
question. This differs from the supervised tech-
niques, in which a large number of hand-tagged
training pairs are shared by all of the test ques-
tions. In addition, supervised techniques indepen-
dently process the answer-bearing sentences, so the
answers to the questions may not always be ex-
tractable because of algorithmic limitations. On the
other hand, the U-SVM can use the interdependence
between answer-bearing sentences to select the an-
swer to a question.
Table 1 compares the key idea and training data
used in the U-SVM with those used in the supervised
machine learning techniques. Here, ME means the
maximum entropy model, and N-C means the noisy-
channel model.
3 The U-SVM
The essence of the U-SVM is to regard answer selec-
tion as a kind of text categorization-like classifica-
tion task, but with no training data available. In the
U-SVM, the steps of ?clustering web search results?,
?classifying the question?, and ?training SVM clas-
sifier? play very important roles.
3.1 Clustering Web Search Results
Web search results, such as snippets returned by
Google, usually include a mixture of multiple
subtopics (called clusters in this paper) related to
the user question. To group the web search results
into clusters, we assume that the candidate answer in
each Google snippet can represent the ?signature? of
its cluster. In other words, the Google snippets con-
taining the same candidate are regarded as aligned
snippets, and thus belong to the same cluster. Web
search results are clustered in two phases.
? A first-stage Google search (FGS) is ap-
plied to extract n candidate answers
{c1, c2, . . . , cn} from the top m Google
snippets {s1, s2, . . . , sm} by a NER tool
[Wu et al 2005]. Those snippets containing
the candidates {ci} and at least one ques-
tion keyword {qi} are retained. Finally,
the retained snippets {s1, s2, . . . , sm} are
clustered into n clusters {C1, C2, . . . , Cn}
by clustering web search results, that is,
If a snippet includes L different candidates,
the snippet belongs to L different clusters.
If the candidates of different snippets are
the same, these snippets belong to the same
clusters.
Consequently, the number of clusters {Ci} is
fully determined by the number of candidates
{ci}, and the cluster name of a cluster Ci is the
candidate answer ci. Up to this point, we have
obtained clusters and sample snippets for each
cluster that will be used as training data for the
SVM classifier. Because this training data is
learned automatically, rather than hand-tagged,
we call it pseudo-training data.
? A second-stage Google search (SGS) is ap-
plied to resolve data sparseness in the pseudo-
training samples learned through the FGS. The
FGS data may have very few training snip-
pets in some clusters, so more snippets must
be collected. Note that this step just learns new
35
Google snippets into the clusters learned by the
FGS, but does not add new clusters.
For each candidate answer ci:
Combine the original query q = {qi} and
the candidate ci to form a new query q? =
{q, ci}.
Submit q? to Google and download the top
50 Google snippets.
Retain the snippets containing the candi-
date ci and at least one keyword qi.
Group the retained snippets into n clusters
to form the new pseudo-training data.
End
Here, we give an example illustrating the prin-
ciple of clustering web search results in the
FGS. In submitting TREC 2004 test question 1.1
?when was the first Crip gang started?? to Google
(http://www.google.com/apis), we extract n(= 8)
different candidates from the top m(= 30) Google
snippets. The Google snippets containing the same
candidates are aligned snippets, and thus the 12 re-
tained snippets are grouped into 8 clusters, as listed
in Table 2. This table roughly indicates that the snip-
pets with the same candidate answers contain the
same sub-meanings, so these snippets are considered
as aligned snippets. For example, all Google snip-
pets that contain the candidate answer 1969 express
the time of establishment of ?the first Crip gang?.
In summary, the U-SVM uses the result of ?clus-
tering web search results? as the pseudo-training
data of the SVM classifier, and then classifies user
question into one of the clusters for answer selec-
tion. On the one hand, the clusters and their names
are based on candidate answers to question; on the
other hand, candidates are dependent on question.
Therefore, the clusters are question-dependent.
3.2 Classifying Question
Using the pseudo-training data obtained by cluster-
ing web search results to train the SVM classifier,
we classify user questions into a set of question-
dependent clusters and assume that the correct an-
swer is the name of the question cluster that is as-
signed by the trained U-SVM classifier. For the
above example, if the U-SVM classifier, trained on
the pseudo-training data listed in Table 2, classifies
the above test question into a cluster whose name is
1969, then the cluster name 1969 is the answer to
the question.
This paper selects LIBSVM toolkit1 to implement
the SVM classifier. The kernel is the radical basis
function with the parameter ? = 0.001 in the exper-
iments.
3.3 Feature Extraction
To classify the question into a question-dependent
set of clusters, the U-SVM classifier extracts three
types of features.
? A similarity-based feature set (SBFS) is
extracted from the Google snippets. The SBFS
attempts to capture the word overlap between
a question and a snippet. The possible values
range from 0 to 1.
SBFS Features
percentage of matched keywords (KWs)
percentage of mismatched KWs
percentage of matched bi-grams of KWs
percentage of matched thesauruses
normalized distance between candidate and
KWs
To compute the matched thesaurus feature, we
adopt TONGYICICILIN 2 in the experiments.
? A Boolean match-based feature set (BMFS) is
also extracted from the Google snippets. The
BMFS attempts to capture the specific key-
word Boolean matches between a question and
a snippet. The possible values are true or false.
BMFS Features
person names are matched or not
location names are matched or not
organization names are matched or not
time words are matched or not
number words are matched or not
root verb is matched or not
candidate has or does not have bi-gram in
snippet matching bi-gram in question
candidate has or does not have desired
named entity type
? A window-based word feature set (WWFS)
is a set of words consisting of the words
1http://www.csie.ntu.edu.tw/ cjlin/libsvm/
2A Chinese Thesaurus Lexicon
36
Table 2: Clustering Web Search Results
Cluster Name Google Snippet
1969 It is believed that the first Crip gang was formed in late 1969. During this time in
Los Angeles there were ...
... the first Bloods and Crips gangs started forming in Los Angeles in late 1969, the
Island Bloods sprung up in north Pomona ...
... formed by 16 year old Raymond Lee Washington in 1969. Williams joined
Washington in 1971 ... had come to be called the Crips. It was initially started to
eliminate all street gangs ...
August 8, 2005 High Country News ? August 8, 2005: The Gangs of Zion
2004 2004 main 1 Crips 1.1 FACTOID When was the first Crip gang started? 1.2 FAC-
TOID What does the name mean or come...
1972 One of the first-known and publicized killings by Crip gang members occurred at
the Hollywood Bowl in March 1972.
1971 Williams joined Washington in 1971, forming the westside faction of what had
come to be called the Crips.
The Crips gang formed as a kind of community watchdog group in 1971 after the
demise of the Black Panthers. ...
... formed by 16 year old Raymond Lee Washington in 1969. Williams joined
Washington in 1971 ... had come to be called the Crips. It was initially started to
eliminate all street gangs ...
1982 Oceanside police first started documenting gangs in 1982, when five known gangs
were operating in the city: the Posole Locos...
mid-1990s Street Locos; Deep Valley Bloods and Deep Valley Crips. By the mid-1990s, gang
violence had ...
1970s The Blood gangs started up as opposition to the Crips gangs, also in the 1970s, and
the rivalry stands to this day ...
preceding {wi?5, . . . , wi?1} and following
{wi+1, . . . , wi+5} the candidate answer. The
WWFS features can be regarded as a kind of
relevant snippets-based question keywords ex-
pansion. By extracting the WWFS feature set,
the feature space in the U-SVM becomes ques-
tion dependent, which may be more suitable for
classifying the question. The number of classi-
fication features in the S-SVM must be fixed,
however, because all questions share the same
training data. This is one difference between
the U-SVM and the supervised SVM classifier
for answer selection. Each word feature in the
WWFS is weighted using its ISF value.
ISF (wj , Ci) =
N(wj , Ci) + 0.5
N(wj) + 0.5
(1)
where N(wj) is the total number of the
snippets containing word feature wj , and
N(wj , Ci) is the number of snippets in cluster
Ci containing word feature wj .
When constructing question vector, we assume
that the question is an ideal question that con-
tains all the extracted WWFS words. There-
fore, the values of the WWFS word features in
question vector are 1. Similarly, the values of
the SBFS and BMFS features in question vec-
tor are also estimated by self-similarity calcu-
lation.
4 Experiments
4.1 Data Sets
For the experiments, no English named entity recog-
nition (NER) tool is in our hand at the time of
the experiments; therefore, we validate the U-SVM
37
in terms of Chinese web QA using three test data
sets, which will be published with this paper3. Al-
though the U-SVM is independent of the question
types, for convenience in candidate extraction, only
those questions whose answers are named entities
are selected. The three test data sets are CTREC04,
CTREC05 and CTEST05. CTREC04 is a set of
178 Chinese questions translated from TREC 2004
FACTOID testing questions. CTREC05 is a set of
279 Chinese questions translated from TREC 2005
FACTOID testing questions. CTEST05 is a set of
178 Chinese questions found in [Wu et al 2004]
that are similar to TREC testing questions except
that they are written in Chinese. Figure 2 breaks
down the types of questions (manually assigned) in
the CTREC04 and CTREC05 data sets. Here, PER,
LOC, ORG, TIM, NUM, and CR refer to questions
whose answers are a person, location, organization,
time, number, and book or movie, respectively.
Figure 2: Statistics of CTEST05
To collect the question-answer training data for
the S-SVM, we submitted 807 Chinese questions to
Google and extracted the candidates for each ques-
tion from the top 50 Google snippets. We then man-
ually selected the snippets containing the correct
answers as positive snippets, and designated all of
the other snippets as negative snippets. Finally, we
collected 807 hand-tagged Chinese question-answer
pairs as the training data of S-SVM called CTRAIN-
DATA.
4.2 Evaluation Method
In the experiments, the top m(= 50) Google snip-
pets are adopted to extract candidates by using a
3Currently no public testing question set for simplified Chi-
nese QA is available.
Chinese NER tool [Wu et al 2005]. The number of
the candidates extracted from the top m(= 50) snip-
pets, n, is adaptive for different questions but it does
not exceed 30. The results are evaluated in terms
of two scores, top n and mrr 5. Here, top n is the
rate at which at least one correct answer is included
in the top n answers, while mrr 5 is the average re-
ciprocal rank (1/n) of the highest rank n(n ? 5) of
a correct answer to each question.
4.3 U-SVM vs. Retrieval-M
The Retrieval-M selects the candidate with the short-
est distances to all question keywords as the cor-
rect answer. In this experiment, the Retrieval-M
is implemented based on the snippets returned by
Google, while the U-SVM is based on the SGS data,
the SBFS and BMFS feature. Table 3 summarizes
the comparative performance.
Table 3: Comparison of Retrieval-M and U-SVM
Retrieval-M U-SVM
top 1 27.84% 53.61%
CTREC04 mrr 5 43.67% 66.25%
top 5 71.13% 88.66%
top 1 34.00% 50.00%
CTREC05 mrr 5 48.20% 62.38%
top 5 71.33% 82.67%
The table shows that the U-SVM greatly improves
the performance of the Retrieval-M: the top 1 im-
provements for CTREC04 and CTREC05 are about
25.8% and 16.0%, respectively. This experiment
demonstrates that the assumptions used here in clus-
tering web search results and in classifying the ques-
tion are effective in many cases, and that the U-SVM
benefits from these assumptions.
4.4 U-SVM vs. S-SVM
To explore the effectiveness of our unsupervised
model as compared with the supervised model, we
conduct a cross-model comparison of the S-SVM
and the U-SVM with the SBFS and BMFS feature
sets. The U-SVM results are compared with the S-
SVM results for CTREC04 and CTREC05 in Ta-
bles 4 and 5, respectively. The S-SVM is trained
on CTRAINDATA.
These tables show the following:
38
Table 4: Comparison of U-SVM and S-SVM on
CTREC04
FGS SGS
top 1 S-SVM 30.93% 39.18%
U-SVM 45.36% 53.61%
mrr 1 S-SVM 45.36% 53.54%
U-SVM 57.44% 66.25%
top 5 S-SVM 71.13% 79.38%
U-SVM 76.29% 88.66%
Table 5: Comparison of U-SVM and S-SVM on
CTREC05
FGS SGS
top 1 S-SVM 30.00% 33.33%
U-SVM 48.00% 50.00%
mrr 1 S-SVM 45.59% 48.67%
U-SVM 58.01% 62.38%
top 5 S-SVM 72.00% 74.67%
U-SVM 75.33% 82.67%
? The proposed U-SVM significantly outper-
forms the S-SVM for all measurements and
all test data sets. For the CTREC04 test data
set, the top1 improvements for the FGS and
SGS data are about 14.5% and 14.4%, respec-
tively. For the CTREC05 test data set, the top1
score for the FGS data increases from 30.0%
to 48.0%, and the top 1 score for the SGS data
increases from 33.3% to 50.0%. Note that the
SBFS and BMFS features here is fewer than the
features in [Ittycheriah et al 2001; Suzuki et
al. 2002], but the comparison is still effective
because the models are compared in terms of
the same features. In the S-SVM, all questions
share the same training data, while the U-SVM
uses the unique pseudo-training data for each
question. This is the main reason why the U-
SVM performs better than the S-SVM does.
? The SGS data is greatly helpful for both
the U-SVM and the S-SVM. Compared with
the FGS data, the top 1/mrr 5/top 5 im-
provements for the S-SVM and the U-SVM
on CTREC04 are 8.25%/8.18%/8.25% and
7.25%/8.81%/12.37%. The SGS can be re-
garded as a kind of query expansion. The rea-
sons for this improvement are: the data sparse-
ness in FGS data is partially resolved; and the
use of the Web to introduce data redundancy
is helpful. [Clarke et al 2001; Magnini et al
2002; and Dumais et al 2002].
In the S-SVM, all of the test questions share the
same hand-tagged training data, so the WWFS fea-
tures cannot be easily used [Ittycheriah et al 2002;
Suzuki, et al 2002]. Tables 6 and 7 compare
the performances of the U-SVM with the (SBFS +
BMFS) features, the WWFS features, and combina-
tion of three types of features for the CTREC04 and
CTREC05 test data sets, respectively.
Table 6: Performances of U-SVM for Different Fea-
tures on CTREC04
SBFS+BMFS WWFS Combination
top 1 53.61% 46.39% 60.82%
mrr 5 66.25% 59.19% 71.31%
top 5 88.66% 81.44% 88.66%
Table 7: Performances of U-SVM for Different Fea-
tures on CTREC05
SBFS+BMFS WWFS Combination
top 1 50.00% 49.33% 57.33%
mrr 5 62.38% 59.26% 65.61%
top 5 82.67% 74.00% 80.00%
These tables report that combining three types
of features can improve the performance of
the U-SVM. Using a combination of features
with the CTREC04 test data set results in the
best performances: 60.82%/71.31%/88.66% for
top 1/mrr 5/top 5. Similarly, as compared with
using the (SBFS + BMFS) and WWFS features, the
improvements from using a combination of features
with the CTREC05 test data set are 7.33%/3.23%/-
2.67% and 8.00%/6.35%/6.00%, respectively. The
results also demonstrate that the (SBFS + BMFS)
features are more important than the WWFS fea-
tures.
These comparative experiments indicate that the
U-SVM performs better than the S-SVM does, even
though the U-SVM is an unsupervised technique and
no hand-tagged training data is provided. The aver-
39
age top 1 improvements for both test data sets are
both more than 20%.
4.5 U-SVM vs. Pattern-M vs. S-SVM
To compare the U-SVM with the Pattern-M and
the S-SVM, we use the CTEST05 data set, shown
in Figure 3. The CTEST05 includes 14 different
question types, for example, Inventor Stuff (with
question like ?Who invented telephone??), Event-
Day (with question like ?when is World Day for Wa-
ter??), and so on. The Pattern-M uses the depen-
dency syntactic answer patterns learned in [Wu et
al. 2007] to extract the answer, and named entities
are also used to filter noise from the candidates.
Figure 3: Statistics of CTEST05
Table 8 summarizes the performances of the U-
SVM, Pattern-M, and S-SVM models on CTEST05.
Table 8: Comparison of U-SVM, Pattern-M and S-
SVM on CTEST05
S-SVM Pattern-M U-SVM
top 1 44.89% 53.14% 59.09%
mrr 5 56.49% 61.28% 67.34%
top 5 74.43% 73.14% 81.82%
The results in the table show that the U-SVM
significantly outperforms the S-SVM and Pattern-
M, while the S-SVM underperforms the Pattern-
M. Compared with the Pattern-M, the U-SVM in-
creases the top 1/mrr 5/top 5 scores by 5.95%/
6.06%/8.68%, respectively. The reasons may lie in
the following:
? The Chinese dependency parser influences de-
pendency syntactic answer-pattern extraction,
and thus degrades the performance of the
Pattern-M model.
? The imperfection of Google snippets affects
pattern matching, and thus adversely influences
the Pattern-M model. From the cross-model
comparison, we conclude that the performance
ranking of these models is: U-SVM > Pattern-
M > S-SVM > Retrieval-M.
5 Conclusion and Future Work
This paper presents an unsupervised machine learn-
ing technique (called the U-SVM) for answer selec-
tion that is validated in Chinese open-domain web
QA. Regarding answer selection as a kind of classifi-
cation task, the U-SVM automatically learns clusters
and pseudo-training data for each cluster by cluster-
ing web search results. It then selects the correct
answer from the candidates according to classifying
the question. The contribution of this paper is that
it presents an unsupervised machine learning tech-
nique for web QA that starts with only a user ques-
tion. The results of our experiments with three test
data sets are encouraging. As compared with the
S-SVM, the top 1 performances of the U-SVM for
the CTREC04 and CTREC05 data sets are signifi-
cantly improved, at more than 20%. Moreover, the
U-SVM performs better than the Retrieval-M and
the Pattern-M.
These experiments have only validated the U-
SVM on named entity types of questions that ac-
count for about 82% of all TREC2004 and 2005
FACTOID test questions. In fact, our technique is
independent of question types only if the candidates
can be extracted. In the future, we will explore the
effectiveness of our technique for the other types of
questions. The web search results clustering in the
U-SVM defines that a candidate in a Google snip-
pet can represent the ?signature? of its cluster. This
definition, however, is not always effective. To fil-
ter noise in the pseudo-training data, we will extract
relations between the candidates and the keywords
as the cluster signatures of Google snippets. More-
over, applying the U-SVM to QA systems in other
languages, like English and Japanese, will also be
included in our future work.
40
References
Abdessamad Echihabi, and Daniel Marcu. 2003. A
Noisy-Channel Approach to Question Answering. In
Proc. of ACL-2003, Japan.
Abraham Ittycheriah, Salim Roukos. 2002. IBM?s Sta-
tistical Question Answering System-TREC 11. In Proc.
of TREC-11, Gaithersburg, Maryland.
Bernardo Magnini, Matteo Negri, Roberto Prevete,
Hristo Tanev. 2002. Is It the Right Answer? Exploit-
ing Web Redundancy for Answer Validation. In Proc.
of ACL-2002, Philadelphia, pp. 425 432.
Charles L. A. Clarke, Gordon V. Cormack, Thomas R.
Lynam. Exploiting Redundancy in Question Answer-
ing In Proc. of SIGIR-2001, pp 358?365, 2001.
Christopher Pinchak, Dekang Lin. 2006. A Probabilistic
Answer Type Model. In Proc. of EACL-2006, Trento,
Italy, pp. 393-400.
Dan Moldovan, Sanda Harabagiu, Roxana Girju, et al
2002. LCC Tools for Question Answering. NIST Spe-
cial Publication: SP 500-251, TREC-2002.
Deepak Ravichandran, Eduard Hovy. 2002. Learning
Surface Text Patterns for a Question Answering Sys-
tem. In Proc. of the 40th ACL, Philadelphia, July
2002.
Eduard Hovy, Ulf Hermjakob, Chin-Yew Lin. 2001. The
Use of External Knowledge of Factoid QA. In Proc.
of TREC 2001, Gaithersburg, MD, U.S.A., November
13-16, 2001.
Hui Yang, Tat-Seng Chua. 2003. QUALIFIER: Question
Answering by Lexical Fabric and External Resources.
In Proc. of EACL-2003, page 363-370.
Hwee T. Ng, Jennifer L. P. Kwan, and Yiyuan Xia. 2001.
Question Answering Using a Large Text Database: A
Machine Learning Approach. In Proc. of EMNLP-
2001, pp66-73 (2001).
Jun Suzuki, Yutaka Sasaki, Eisaku Maeda. 2002. SVM
Answer Selection for Open-Domain Question Answer-
ing. In Proc. of Coling-2002, pp. 974 980 (2002).
Marius Pasca. 2001. A Relational and Logic Represen-
tation for Open-Domain Textual Question Answering.
In Proc. of ACL (Companion Volume) 2001: 37-42.
Martin M. Soubbotin, Sergei M. Soubbotin. 2002. Use of
Patterns for Detection of Likely Answer Strings: A Sys-
tematic Approach. In Proc. of TREC-2002, Gaithers-
burg, Maryland, November 2002.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andre Ng. Web Question Answering: Is More
Always Better?. In Proc. SIGIR-2002, pp 291?298,
2002.
Xin Li, and Dan Roth. 2002. Learning Question Classi-
fication. In Proc. of the 19th International Conference
on Computational Linguistics, Taibai, 2002.
Youzheng Wu, Hideki Kashioka, Jun Zhao. 2007. Us-
ing Clustering Approaches to Open-domain Question
Answering. In Proc. of CICLING-2007, Mexico City,
Mexico, pp506 517, 2007.
Youzheng Wu, Jun Zhao and Bo Xu. 2005. Chinese
Named Entity Recognition Model Based on Multiple
Features. In Proc. of HLT/EMNLP-2005, Vancouver,
Canada, pp.427-434.
Youzheng Wu, Jun Zhao, Xiangyu Duan and Bo Xu.
2004. Building an Evaluation Platform for Chinese
Question Answering Systems. In Proc. of the First
NCIRCS, China, December, 2004.
Yutaka Sasaki. 2005. Question Answering as Question-
Biased Term Extraction: A New Approach toward
Multilingual QA. In Proc. of ACL-2005, pp.215-222.
41
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1908?1917, Dublin, Ireland, August 23-29 2014.
Recurrent Neural Network-based Tuple Sequence Model
for Machine Translation
Youzheng Wu, Taro Watanabe, Chiori Hori
National Institute of Information and Communications Technology (NICT), Japan
erzhengcn@gmail.com
{taro.watanabe, chiori.hori}@nict.go.jp
Abstract
In this paper, we propose a recurrent neural network-based tuple sequence model (RNNTSM)
that can help phrase-based translation model overcome the phrasal independence assumption.
Our RNNTSM can potentially capture arbitrary long contextual information during estimating
probabilities of tuples in continuous space. It, however, has severe data sparsity problem due
to the large tuple vocabulary coupled with the limited bilingual training data. To tackle this
problem, we propose two improvements. The first is to factorize bilingual tuples of RNNTSM
into source and target sides, we call factorized RNNTSM. The second is to decompose phrasal
bilingual tuples to word bilingual tuples for providing fine-grained tuple model. Our extensive
experimental results on the IWSLT2012 test sets
1
showed that the proposed approach essentially
improved the translation quality over state-of-the-art phrase-based translation systems (baselines)
and recurrent neural network language models (RNNLMs). Compared with the baselines, the
BLEU scores on English-French and English-German tasks were greatly enhanced by 2.1%-
2.6% and 1.8%-2.1%, respectively.
1 Introduction
The phrase-based translation systems (Koehn et al., 2003) rely on language model and lexicalized re-
ordering model to capture lexical dependencies that span phrase boundaries. Their translation models,
however, do not explicitly model context dependencies between translation units. To address this limi-
tation, Marino et al. (2006) and Crego and Yvon (2010) proposed n-gram-based translation systems to
capture dependencies across phrasal boundaries. The n-gram translation models have been shown to be
effective in helping the phrase-based translation models overcome the phrasal independence assumption
(Durrani et al., 2013; Zhang et al., 2013). Most of the n-gram translation models (Marino et al., 2006;
Durrani et al., 2013; Zhang et al., 2013) employed Markov (n-gram) model over sequence of bilingual
tuples also known as minimal translation units (MTUs).
Recently, some pioneer studies (Schwenk et al., 2007; Son et al., 2012) proposed feed-forward neural
networks with factorizations to model bilingual tuples in a continuous space. Although the authors
reported some gains over the n-gram model in machine translation tasks, these models can only capture
a limited amount of context and remain a kind of n-gram model. In language modeling, experimental
results in (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013) showed that recurrent
neural networks (RNNs) outperform feed-forward neural networks in both perplexity and word error rate
in speech recognition even though it is harder to train properly.
Therefore, in this paper we take the advantages of RNN and tuple sequence model and propose re-
current neural network-based tuple sequence models (RNNTSMs) to improve phrase-based translation
system. Our RNNTSMs are capable of modeling long-span context and have better generalization. Com-
pared with such related studies as (Schwenk et al., 2006; Son et al., 2012), our main contributions can
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The IWSLT workshop aims at translating TED speeches (http://www.ted.com), a collection of public lectures cov-
ering a variety of topics.
1908
be summarized as: (i) our models can be regarded as deep neural network translation models because
they can capture arbitrary-length context potentially, which are proven to estimate more accurate proba-
bilities of bilingual tuples; (ii) we extend the conventional RNNTSM to factorized RNNTSMs that can
significantly overcome the data sparseness problem caused by the large vocabularies of bilingual tuples
by incorporating the factors from the source and the target sides in addition to bilingual tuples; (iii) we
investigate heuristic rules to decompose phrasal bilingual tuples to word bilingual tuples for reducing
the out-of-tuple-vocabulary rate and providing fine-grained tuple sequence model; (iv) we integrate the
proposed models into the state-of-the-art phrase-based translation system (MOSES) as a supplement of
the work in (Son et al., 2012) that is a complete n-gram translation system.
2 Related Work
The n-gram translation model (Marino et al., 2006) is a Markov model over phrasal bilingual tuples and
can improve the phrase-based translation system (Koehn et al., 2003) by providing contextual depen-
dencies between phrase pairs. To further improve the n-gram translation model, Crego and Yvon (2010)
explored factored bilingual n-gram language models. Durrani et al. (2011) proposed a joint sequence
model for the translation and reordering probabilities. Zhang et al. (2013) explored multiple decomposi-
tion structures as well as dynamic bidirectional decomposition. Since neural networks advance the state
of the art in the fields of image processing, acoustic modeling (Seide et al., 2011), language modeling
(Bengio et al., 2003), natural language processing (Collobert et al., 2011; Socher et al., 2013), machine
transliteration (Deselaers et al., 2009), etc, some prior studies have been done on neural network-based
translation models (NNTMs).
One kind of the NNTMs relies on word-to-word alignment information or phrasal bilingual tuples. For
example, Schwenk et al. (2007) investigated feed-forward neural networks to model bilingual tuples in
continuous space. Son et al. (2012) improved this idea by decomposing tuple units, i.e., distinguishing the
source and target sides of the tuple units, to address data sparsity issues. Although the authors reported
some gains over the n-gram model in the BLEU scores on some tasks, these models can only capture
a limited amount of context and remain a kind of n-gram model. In addition, a feed-forward neural
network independent from bilingual tuples was proposed (Schwenk, 2012), which can infer meaningful
translation probabilities for phrase pairs not seen in the training data.
Another kind of the NNTMs do not rely on alignment. Auli et al. (2013) and Kalchbrenner and
Blunsom (2013) proposed joint language and translation model with recurrent neural networks, in which
latent semantic analysis and convolutional sentence model were used to model source-side sentence.
Potentially, they can exploit an unbounded history of both source and target words thanks to recurrent
connections. However, they only modestly observed gains over the recurrent neural network language
model. Previous studies (Wu and Wang, 2007; Yang et al., 2013) showed that the performance of word
alignment (alignment error rate) is nearly 80%. That means explicit word alignment may be more reliable
as a way to represent the corresponding bilingual sentences compared with an implicit compressed vector
representation (Auli et al., 2013).
Our RNNTSM takes the advantages of the above NNTMs, that is, RNN enables our model to cap-
ture long-span contextual information, while tuple sequence model uses word alignment without much
information loss. Furthermore, factorized RNN and word bilingual tuples are proposed to address data
sparsity issue. To the best of our knowledge, few studies have been done on this aspect.
3 Tuple Sequence Model
In tuple sequence model, bilingual tuples are translation units extracted from word-to-word alignment.
They are composed of source phrases and their aligned target phrases that are also known as minimal
translation units (MTUs) and thus cannot be broken down any further without violating the constrains
of the translation rules. This condition results in a unique segmentation of the bilingual sentence pair
given its alignment. In our implementation, GIZA++ with grow-diag-final-and setting is used
to conduct word-to-word alignments in both directions, source-to-target and target-to-source (Och and
1909
DZRUGWRZRUGDOLJQHGUHVXOWIURP*,=$ZLWKJURZGLDJILQDODQG
FRPSRVHUVDQG
RQW ?W? LQWHUURJ?V
DIULFDQ DPHULFDQ PXVLFLDQV
OHV musiciens DIURDP?ULFDL
LQWHUYLHZHGZHUH
HW FRPSRVLWHXUV
X
PXVLFLDQV
OHVmusiciensW
V
EVHTXHQFHRISKUDVDOELOLQJXDOWXSOHV
DIULFDQDPHULFDQ
DIURDP?ULFDL
V
X
W
X
DQG
HW
V
W
X
FRPSRVHUV
FRPSRVLWHXUV
V
W
X
ZHUHLQWHUYLHZHG
RQW?W?LQWHUURJ?V
V
W
DIULFDQ
DIURDP?ULFDL
V
X X X X XExploiting Social Q&A Collection in Answering Complex Questions
Youzheng Wu Hisashi Kawai
Spoken Language Communication Group, MASTAR Project
National Institute of Information and Communications Technology
2-2-2 Hikaridai, Keihanna Science City, Kyoto 619-0288, Japan
{youzheng.wu, hisashi.kawai}@nict.go.jp
Abstract
This paper investigates techniques to au-
tomatically construct training data from
social Q&A collections such as Yahoo!
Answer to support a machine learning-
based complex QA system1. We extract
cue expressions for each type of question
from collected training data and build
question-type-specific classifiers to im-
prove complex QA system. Experiments
on 10 types of complex Chinese ques-
tions verify that it is effective to mine
knowledge from social Q&A collections
for answering complex questions, for in-
stance, the F3 improvement of our sys-
tem over the baseline and translation-
based model reaches 7.9% and 5.1%, re-
spectively.
1 Introduction
Research on the topic of QA systems has mainly
concentrated on answering factoid, definitional,
reason and opinion questions. Among the ap-
proaches proposed to answer these questions,
machine learning techniques have been found
more effective in constructing QA components
from scratch. Yet these supervised techniques re-
quire a certain scale of (question, answer), short
for Q&A, pairs as training data. For example,
(Echihabi et al, 2003) and (Sasaki, 2005) con-
structed 90,000 English Q&A pairs and 2,000
Japanese Q&A pairs, respectively for their fac-
toid QA systems. (Cui et al, 2004) constructed
1Complex questions cannot be answered by simply ex-
tracting named entities. In this paper complex questions do
not include definitional questions.
76 term-definition pairs for their definitional QA
systems. (Stoyanov et al, 2005) required a
known subjective vocabulary for their opinion
QA. (Higashinaka and Isozaki, 2008) used 4,849
positive and 521,177 negative examples in their
reason QA system. Among complex QA sys-
tems, many other types of questions have not
been well studied, apart from reason and defi-
nitional questions. Appendix A lists 10 types of
complex Chinese questions and their examples
we discussed in this paper.
According to the related studies on QA, su-
pervised machine-learning technique may be ef-
fective for answering these questions. To em-
ploy the supervised approach, we need to re-
construct training Q&A pairs for each type of
question, though this is an extremely expensive
and labor-intensive task. To deal with the ac-
quisition problem of training Q&A pairs, we in-
vestigate techniques to automatically construct
training data by utilizing social Q&A collections
crawled from the Web, which contains millions
of user-generated Q&A pairs. Many studies
(Surdeanu et al, 2008) (Duan et al, 2008) have
been done on retrieving similar Q&A pairs from
social QA websites as answers to test questions.
Our study, however, regards social Q&A web-
sites as a knowledge repository and aims to mine
knowledge from them for synthesizing answers
to questions from multiple documents. There is
very little literature on this aspect. Our work can
be seen as a kind of query-based summarization
(Dang, 2006) (Harabagiu et al, 2006) (Erkan
and Radev, 2004), and can also be employed to
answer questions that have not been answered in
social Q&A websites.
This paper mainly focuses on the following three
steps: (1) automatically constructing question -
type-specific training Q&A pairs from the so-
cial Q&A collection; (2) extracting cue expres-
sions for each type of question from the col-
lected training data, and (3) building question-
type-specific classifiers to filer out noise sen-
tences before using a state-of-the-art IR formula
to select answers.
We evaluate our system on 10 types of Chi-
nese questions by using the Pourpre evalua-
tion tool (Lin and Demner-Fushman, 2006).
The experimental results show the effectiveness
of our system, for instance, the F3/NR im-
provement of our system over the baseline and
translation-based model reaches 7.9%/11.1%,
and 5.1%/5.6%, respectively.
2 Social Q&A Collection
Recently launched social QA websites such as
Yahoo! Answer2 and Baidu Zhidao3 provide
an interactive platform for users to post ques-
tions and answers. After questions are answered
by users, the best answer can be chosen by the
asker or nominated by the community. The num-
ber of Q&A pairs on such sites has risen dra-
matically. These pairs could collectively form a
source of training data that is required in super-
vised machine-learning-based QA systems.
In this paper we aim to explore such user-
generated Q&A collections to automatically col-
lect Q&A training data. However, social col-
lections have two salient characteristics: tex-
tual mismatch between questions and answers
(i.e., question words are not necessarily used
in answers); and user-generated spam or flip-
pant answers, which are unfavorable factors in
our study. Thus, we only crawl questions and
their best answers to form Q&A pairs, wherein
the best answers are longer than the empiri-
cal threshold. Finally, 60.0 million Q&A pairs
were crawled from Chinese social QA websites.
These pairs will be used as the source of training
data required in our study.
2http://answers.yahoo.com/
3http://zhidao.baidu.com/
3 Our Complex QA System
The typical complex QA system architecture is
a cascade of three modules. The Question Ana-
lyzer analyzes test questions and identifies an-
swer types of questions. The Document Re-
triever & Answer Candidate Extractor retrieves
documents related to questions from the given
collection (Xinhua and Lianhe Zaobao newspa-
pers from 1998-2001 were used in this study) for
consideration, and segments the documents into
sentences as answer candidates. The Answer Ex-
traction module applies state-of-the-art IR for-
mulas (e.g., KL-divergence language model) to
directly estimate similarities between sentences
(1,024 sentences were used in our case) and
questions, and selects the most similar sentences
as the final answers. Given three answer candi-
dates, s1 = ?Solutions to global warming range
from changing a light bulb to engineering giant
reflectors in space ...?, s2 = ?Global warming
will bring bigger storms and hurricanes that will
hold more water ...?, and s3 = ?nuclear power
is the relatively low emission of carbon diox-
ide (CO2), one of the major causes of global
warming,? to the question of ?What are the haz-
ards of global warming??, however, it is hard for
this architecture to select the correct answer, s2,
because the three candidates contain the same
question words ?global warming?.
According to our observation, answers to a
type of question usually contain some type-
of-question dependent cue expressions (?will
bring? in this case). This paper argues that
the above QA system can be improved by us-
ing such question-type-specific cue expressions.
For each test question, we perform the follow-
ing three steps. (1) Collecting question-type-
specific Q&A pairs from the social Q&A collec-
tion which question types are same as the test
question to form positive training data. Sim-
ilarly, negative Q&A pairs are also collected
which question types are different from the
test question. (2) Extracting and weighting
question-type-specific cue expressions from the
collected Q&A pairs. (3) Building a question-
type-specific classifier by employing the cue ex-
pressions and the collected Q&A pairs, which re-
moves noise sentences from answer candidates
before using the Answer Extraction module.
3.1 Collecting Q&A Pairs
We first introduce the notion of the answer type
informer of the question as follows. In a ques-
tion, a short subsequence of tokens (typically 1-
3 words) that are adequate for question classi-
fication is considered an answer-type informer,
e.g., ?hazard? in the question of ?What are the
hazards of global warming?? This paper makes
the following assumption: type of complex ques-
tion is determined by its answer type informer.
For example, the question of ?What are the haz-
ards of global warming?? belongs to hazard-type
question, because its answer type informer is
?hazard?. Therefore, the task of recognizing
question-types is shifted to identifying answer
type informer of question.
In this paper, we regard answer-type informer
recognition as a sequence tagging problem and
adopt conditional random fields (CRFs) because
many work has shown that CRFs have a con-
sistent advantage in sequence tagging. We
manually label 3,262 questions with answer-
type informers to train a CRF, which classi-
fies each question word into a set of tags O =
{IB , II , IO}: IB for a word that begins an in-
former, II for a word that occurs in the mid-
dle of an informer, and IO for a word that
is outside of an informer. In the following
feature templates used in the CRF model, wn
and tn, refer to word and PoS, respectively;
n refers to the relative position from the cur-
rent word n=0. The feature templates in-
clude the following four types: unigrams of
wn and tn, where n=?2,?1, 0, 1, 2; bigrams
of wnwn+1 and tntn+1, where n=?1, 0; tri-
grams of wnwn+1wn+2 and tntn+1tn+2, where
n=?2,?1, 0; and bigrams of OnOn+1, where
n=?1, 0.
The trained CRF model is then employed to
recognize answer-type informers from questions
of social Q&A pairs. Finally, we recognized 103
answer-type informers in which frequencies are
larger than 10,000. Moreover, the numbers of
answer type informers for which frequencies are
larger than 100, 1,000, and 5,000 are 2,714, 807,
and 194, respectively.
Based on answer-type informers of questions
recognized, we can collect training data for each
type of question as follows: (1) Q&A pairs are
grouped together in cases in which the answer-
type informers X of their questions are the same,
and (2) Q&A pairs clustered by informers X
are regarded as the positive training data of
X-type questions. For instance, 10,362 Q&A
pairs grouped via informer X (=?hazard?) are
regarded as positive training data of answering
hazard-type questions. Table 1 lists some ques-
tions, which, together with their best answers,
are employed as the training data of the corre-
sponding type of questions. For each type of
question, we also randomly select some Q&A
pairs that do not contain informers in questions
as negative training data. Preprocessing of the
training data, including word segmentation, PoS
tagging, and named entity (NE) tagging (Wu et
al., 2005), is conducted. We also replace each
NE with its tag type.
Qtype Questions of Q&A pairs
Hazard-
type
What are the hazards of the tro-
jan.psw.misc.kah virus?
What are the hazards of RMB appreciation
on China?s economy?
Hazards of smoke
What are the hazards of contact lenses?
What are the hazards of waste accumula-
tion?
Casualty-
type
What were the casualties on either side from
the U.S.-Iraq war?
What were the casualties of the Sino-French
War?
What were the casualties of the Sichuan
earthquake in 2008?
What were the casualties of highway acci-
dents over the years?
What were the casualties of the Ryukyu Is-
lands tsunami?
Reason-
type
What are the main reasons of China?s water
shortage?
What are the reasons of asthma?
What are the reasons of blurred photos?
What are the reasons of air pollution?
The reasons for the soaring prices!
Table 1: Questions (translated from Chinese) of
social Q&A pairs (words in bold denote answer-
type informers of questions). These questions
and their best answers are regarded as positive
training data for hazard-type question.
3.2 Cue Expressions
We extract lexical and PoS-based n-grams as cue
expressions from the collected training data. To
reduce the dimensionality of the cue expression
space, we first select the top 3,000 lexical un-
igrams using the formula: scorew = tfw ?
log(idfw), where tf(w) denotes the frequency of
word w, and idf(w) represents the inverted doc-
ument frequency of w that indicates its global
importance. Table 2 shows some of the learned
unigrams. The top 300 unigrams are then used as
seeds to learn lexical bigrams and trigrams iter-
atively. Only lexical bigrams and trigrams that
contain seed unigrams with frequencies larger
than the thresholds are retained as lexical fea-
tures. Moreover, we extract PoS-based unigrams
and bigrams as cue expressions.
Further, we assign each extracted feature si a
weight calculated using the equation weightsi =
csi1 /(c
si
1 + c
si
2 ), where, c
si
1 and c
si
2 denote its fre-
quencies in positive and negative training Q&A
pairs, respectively.
Qtype Top Unigrams
Hazard-type ?3/hazard s?/lead to ?/cause
Z?/give rise to ?	/bring about k
//influence?3/damage
Casualty-type ?}/casualty ?}/death I?/hurt
/missing ?
/wrecked j}/die
in battle??/wounded
Table 2: Top unigrams learned from hazard-type
and casualty-type Q&A pairs
3.3 Classifiers
As mentioned above, we use the extracted cue
expressions and the collected Q&A pairs to build
question-type-specific classifiers, which is used
to remove noise sentences from answer candi-
dates. For classifiers, we employ multivariate
classification SVMs (Thorsten Joachims, 2005)
that can directly optimize a large class of perfor-
mance measures like F1-Score, prec@k (preci-
sion of a classifier that predicts exactly k = 100
examples to be positive) and error-rate (percent-
age of errors in predictions). Instead of learn-
ing a univariate rule that predicts the label of a
single example in conventional SVMs (Vapnik,
1998), multivariate SVMs formulate the learn-
ing problem as a multivariate prediction of all
examples in the data set. Considering hypothe-
ses h that map a tuple x of n feature vectors
x = (x1, ...,xn) to a tuple y of n labels y =
(y1, ..., yn), multivariate SVMs learn a classifier
hw(x) = argmaxy??Y {wT?(x, y?)} (1)
by solving the following optimization problem.
minw,??0
1
2?w?
2 +C? (2)
s.t. : ?y? ? Y \y : wT [?(x, y) ? ?(x, y?)]
? ?(y?, y) ? ?
(3)
where, w is a parameter vector, ? is a function
that returns a feature vector describing the match
between (x1, ...,xn) and (y?1, ..., y?n), ? denotes
types of multivariate loss functions, and ? is a
slack variable.
4 Experiments
The NTCIR 2008 test data set (Mitamura et al,
2008) contains 30 complex questions4 we dis-
cussed here. However, a small number of test
questions are included for some question types,
e.g.; it contains only 1 hazard-type, 1 scale-type,
and 3 significance-type questions. To form a
more complete test set, we create another 65 test
questions5 . Therefore, the test data used in this
paper includes 95 complex questions.
For each test question we also provide a list
of weighted nuggets, which are used as the gold
standard answers for evaluation. The evaluation
is conducted by employing Pourpre v1.0c (Lin
and Demner-Fushman, 2006), which uses the
standard scoring methodology for TREC other
questions (Voorhees, 2003), i.e., answer nugget
recall NR, nugget precision NP , and a combi-
nation score F3 of NR and NP . For better un-
derstanding, we evaluate the systems when out-
putting the top N sentences as answers.
4Because definitional, biography, and relationship ques-
tions in the NTCIR 2008 test set are not discussed here.
5The approach of creating test data is same as that in the
NTCIR 2008.
F3 (%) NR (%) NP (%)
N = 1 N = 5 N = 10 N = 1 N = 5 N = 10 N = 1 N = 5 N = 10
Baseline 9.82 18.18 21.95 9.44 19.85 27.64 34.35 25.32 18.96
TransM 9.76 20.47 24.76 9.44 19.85 33.10 31.96 21.73 13.57
Ourslin 10.92 22.61 25.74 10.49 25.95 34.70 34.98 23.40 15.11
Ourserrorrate 12.37 23.10 27.74 12.05 26.98 37.03 33.22 26.48 18.67
Ourspre@k 8.96 22.85 29.85 8.72 25.67 38.78 26.28 28.82 20.45
Table 3: Overall performance for the test data
4.1 Overall Results
Table 3 summarizes the evaluation results for
several N values. The baseline refers to the con-
ventional method introduced in Section 3, which
does not employ question-type-specific classi-
fiers before the Answer Extraction. The baseline
can be expressed by the formula:
sim(q, s) = ?Vq ? Vs??Vq? ? ?Vs?
(4)
where, Vq and Vs are the vectors of the ques-
tion and candidate answer. The TransM de-
notes a translation model for QA (Xue, et al,
2008) (Bernhard et al, 2009), which uses Q&A
pairs as the parallel corpus, with questions to the
?source? language and answers corresponding to
the ?target? language. This model can be ex-
pressed by:
P (q|S) =
?
w?q
((1 ? ?)Pmx(w|S) + ?Pml(w|C))
Pmx(w|S) = (1 ? ?)Pml(w|S)+
?
?
t?S
P (w|t)Pml(t|S)
(5)
where, q is the question, S the sentence, P (w|t)
the probability of translating a sentence term t to
the question term w, which is obtained by using
the GIZA++ toolkit (Och and Ney, 2003). We
use six million Q&A pairs to train IBM model 1
for obtaining word-to-word probability P (w|t).
Ourserrorrate and Ourspre@k denote our models
that are based on classifiers optimizing perfor-
mance measure error-rate and prec@k, respec-
tively. Ourslin, a linear interpolation model, that
combines scores of classifiers and the baseline,
which is similar to (Mori et al, 2008) and can be
expressed by the equation:
sim(q, s)? = sim(q, s) + ? ? ?(s) (6)
where, ?(s) is the score calculated by classi-
fiers (Thorsten Joachims, 2005) and ? denotes
the weight of the score.
This experiment shows that: (1) Question-
type-specific classifiers can greatly outperform
the baseline; for example, the F3 improvements
of Ourserrorrate and Ourspre@k over the base-
line in terms of N=10 are 5.8% and 7.9%,
respectively. (2) Ourserrorrate is better than
Ourspre@k when N < 10. The average num-
bers of sentences retained in Ourserrorrate and
Ourspre@k are 130, and 217, respectively. That
means the precision of the classifier optimiz-
ing errorrate is superior to the classifier optimiz-
ing prec@k, while the recall is relatively infe-
rior. (3) Ourslin is worse than Ourserrorrate and
Ourspre@k, which indicates that using question-
type-specific classifiers by classification is better
than using it by interpolation like (Mori et al,
2008). (4) Our models also outperform TransM,
e.g.; the F3 improvement is 5.1% when N is
set to 10. TransM exploits the social Q&A col-
lection without consideration of question types,
while our models select and exploit the social
Q&A pairs of the same question types. Thereby,
this experiment also indicates that it is better to
exploit social Q&A pairs by type of question.
The performance ranking of these models when
N=10 is: Oursprec@k > Ourserrorrate > Ourslin
> TransM > Baseline.
4.2 Impact of Features
In order to evaluate the contributions of indi-
vidual features to our models, this experiment
is conducted by gradually adding them. Table
4 summarizes the performance of Ourprec@k on
different set of features, L and P represent lex-
ical and PoS-based features, respectively. This
table demonstrates that all the lexical and PoS
features can positively impact Ourprec@k, espe-
cially, the contribution of the PoS-based features
is largest.
Features F3 NR NP
Lunigram 23.44 31.23 17.32
+Lbigram +Ltrigram 25.34 33.15 18.87
+Punigram 28.24 36.27 20.18
+Pbigram 29.85 38.78 20.45
Table 4: Impact of features on Ourprec@k.
4.3 Improvement
As discussed in Section 2, the writing style of
social Q&A collections slightly differs from that
of our complex QA system, which is an unfavor-
able circumstance in utilizing social Q&A col-
lections. For better understanding we randomly
select 100 Q&A training pairs of each type of
question acquired in Section 3, and manually
classify each Q&A pair into NON-NOISE and
NOISE6 categories. Figure 1 reports the percent-
age of NON-NOISE. This figure indicates that
71% of the training pairs of the scale-type ques-
tions are noises, which may lead to a small im-
provement.
0.87
0.79
0.86
0.5 0.51
0.79
0.54 0.58
0.85
0.29
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 1: Percentage of NON-NOISE pairs by
type of questions.
To further improve the performance, we em-
6NOISE means that the Q&A pair is not useful in our
study.
ploy k-fold cross validation to remove noises
from the collected training data in Section 3.1.
Specifically, the collected training data are first
divided into k (= 5) sets. Secondly, k-1 sets are
used to train classifiers that are applied to clas-
sify the Q&A pairs in the remaining set. Finally,
part of the Q&A pairs classified as negative pairs
are removed7. According to Figure 1, we re-
move 20% of the training data from the nega-
tive pairs for the hazard-type, impact-type, and
function-type questions, and 40% of the train-
ing data for significance-type, event-type, and
reason-type questions. Because the sizes of the
training pairs of the other four types of ques-
tions are small, we do not use this approach on
them. Table 5 shows the results of Ourspre@k on
the above six types of questions. The numbers
in brackets indicate absolute improvements over
the system based on the data without removing
noises. N is the number of answer sentences to a
question. The experiment shows that the perfor-
mance is generally improved by removing noise
in the training Q&A pairs using k-fold cross-
validation.
F3 (%) NR (%) NP (%)
N = 1 9.6+2.1 9.3+2.0 30.8+7.4
N = 5 21.6+0.7 24.9+1.2 26.0?1.3
N = 10 28.6+0.9 37.9+1.7 19.2?0.2
Table 5: Performance of Ourspre@k after remov-
ing noises in the training Q&A pairs.
4.4 Subjective evaluation
Pourpre v1.0c evaluation is based on n-gram
overlap between the automatically produced an-
swers and the human generated reference an-
swers. Thus, it is not able to measure concep-
tual equivalent. In subjective evaluation, the an-
swer sentences returned by systems are labeled
by a native Chinese assessor. Figure 2 shows the
distribution of the ranks of the first correct an-
swers for all questions. This figure demonstrates
that the Ourspre@k answers 57 questions which
7We do not remove all negative Q&A pairs to ensure
the coverage of training data because the classifiers have
relatively lower recall, as mentioned in Section 3.3.
first answers are ranked in top 3, which is larger
than that of the baseline, i.e., 49. Moreover,
the Ourspre@k contains only 11.5% of questions
which answers are ranked after top 10, while this
number of the baseline is 20.7%.
26
16
15
6
0
3
2
3 2 4
30
9 10
1
6
8
2
1 2 00
5
10
15
20
25
30
35
1 2 3 4 5 6 7 8 9 10
Ourprec@k
Baseline
Figure 2: Distribution of the ranks of first an-
swers.
5 Related Work
Recently, some pioneering studies on the social
Q&A collection have been conducted. Among
them, much of the research aims to retrieve an-
swers to queried questions from the social Q&A
collection. For example, (Surdeanu et al, 2008)
proposed an answer ranking engine for non-
factoid questions by incorporating textual fea-
tures into a machine learning approach. (Duan
et al, 2008) proposed searching questions se-
mantically equivalent or close to the queried
question for a question recommendation sys-
tem. (Agichtein et al, 2008) investigated tech-
niques of finding high-quality content in the so-
cial Q&A collection, and indicated that 94% of
answers to questions with high quality have high
quality. (Xue, et al, 2008) proposed a retrieval
model that combines a translation-based lan-
guage model for the question part with a query
likelihood approach for the answer part.
Another category of study regards the social
Q&A collection as a kind of knowledge reposi-
tory and aims to mine knowledge from it for gen-
erating answers to questions. To the best of our
knowledge, there is very limited work reported
on this aspect. This paper is similar to (Mori et
al., 2008), but different from it as follows. (1)
(Mori et al, 2008) collects training data for each
test question using 7-grams for which centers are
interrogatives, while this paper collects training
data for each type of question using answer type
informers. (2) About the knowledge learned,
we extract lexical/class-based, PoS-based uni-
grams, bigrams, and trigrams. (Mori et al, 2008)
only extracts lexical bigrams. (3) They incor-
porated knowledge learned by interpolating with
the baseline. However, we utilize the learned
knowledge to train a binary classifier, which can
remove noise sentences before answer selection.
6 Conclusion
This paper investigated a technique for mining
knowledge from social Q&A websites for im-
proving a sentence-based complex QA system.
More specifically, it explored a social Q&A col-
lection to automatically construct training data,
and created question-type-specific classifier for
each type of question to filter out noise sentences
before answer selection.
The experiments on 10 types of complex Chi-
nese questions show that the proposed approach
is effective; e.g., the improvement in F3 reaches
7.9%. In the future, we will endeavor to reduce
NOISE pairs in the training data, and to extract
type-of-question dependent features. Future re-
search tasks also include adapting the QA system
to a topic-based summarization system, which,
for example, summarizes accidents according to
?casualty?, ?reason?, and summarizes events ac-
cording to ?reason?, ?measure,? ?impact?, etc.
Appendix A. Examples of 10 Types of Ques-
tions.
References
Abdessamad Echihabi and Daniel Marcu. 2003. A
Noisy-Channel Approach to Question Answering.
In Proc. of ACL 2003, Japan.
Delphine Bernhard and Iryna Gurevych. 2009. Com-
bining Lexical Semantic Resources with Question
& Answer Archives for Translation-based Answer
Finding. In Proc. of ACL-IJCNLP 2009, Singa-
pore, pp728-736.
Ellen M. Voorhees. 2003 Overview of the TREC
2003 Question Answering Track. In Proc. of
TREC 2003, pp54-68, USA.
Qtype Examples
? 3/Hazard-
type
\E?#F{?34??What
are the hazards of global warming?
*~/Function-
type
?\){*~4??What are the
functions of the United Nations?
k //Impact-
type
??911/G??){k/List
the impact of the 911 attacks on the
United States.
?B/ ???)?WTO{?B
Significance-
type
List the significance of China?s acces-
sion to the WTO.
? ?/Attitude-
type
???)??1?B{??List
the attitudes of other countries toward
the Israeli-Palestinian conflict.
D/Measure-
type
????>\0?fR?
?JD?What measures have
been taken for energy-saving and
emissions-reduction in Japan?
? O/Reason-
type
\E?#F{?O4??What
are the reasons for global warming?
?}/Casualty-
type
??b.8
{?}List the
casualties of the Lockerbie Air Disas-
ter.
/ G/Event-
type
????}Z?Z??g/
GList the events in the Northern
Ireland peace process.
 ?/Scale-
type
??f?-??2F??{
?Give information about the scale
of the Kunming World Horticulture
Exposition.
Eugene Agichtein, Carlos Castillo, Debora Donato.
2008 Finding High-Quality Content in Social Me-
dia. In Proc. of WSDM 2008, California, USA.
Franz J. Och and Hermann Ney. 2003. A system-
atic Comparison of Various Statistical Alignment
Models. In Computational Linguistics, 29(1):19-
51.
Gunes Erkan and Dragomir Radev. 2004. LexRank:
Graph-based Lexical Centrality as Salience in
Text. In Journal of Artificial Intelligence
Research,22:457-479.
Hang Cui, Min Yen Kan, and Tat Seng Chua. 2004.
Unsupervised Learning of Soft Patterns for Defini-
tion Question Answering. In Proc. of WWW 2004.
Hoa Trang Dang. 2006. Overview of DUC 2006. In
Proc. of TREC 2006.
Huizhong Duan, Yunbo Cao, Chin Yew Lin, and
Yong Yu. 2008. Searching Questions by Identify-
ing Question Topic and Question Focus. In Proc.
of ACL 2008, Canada, pp 156-164.
Jimmy Lin and Dina Demner-Fushman. 2006. Will
Pyramids Built of Nuggets Topple Over. In Proc.
of HLT/NAACL2006, pp 383-390.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to Rank Answers on
Large Online QA Collections. In Proc. of ACL
2008, Ohio, USA, pp 719-727.
Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based Question Answering for why-
Questions. In Proc. of IJCNLP 2008, pp 418-425.
Tatsunori Mori, Takuya Okubo, and Madoka Ish-
ioroshi. 2008. A QA system that can answer any
class of Japanese non-factoid questions and its ap-
plication to CCLQA EN-JA task. In Proc. of NT-
CIR2008, Tokyo, pp 41-48.
Sanda Harabagiu, Finley Lacatusu, Andrew Hickl.
2006. Answering Complex Questions with Ran-
dom Walk Models. In Proc. of the 29th SIGIR, pp
220-227, ACM.
Ves Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-Perspective Question Answering Us-
ing the OpQA Corpus. In Proc. of HLT/EMNLP
2005, Canada, pp 923-930.
Teruko Mitamura, Eric Nyberg, Hideki Shima,
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin,
Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai,
Donghong Ji and Noriko Kando. 2008. Overview
of the NTCIR-7 ACLIA Tasks: Advanced Cross-
Lingual Information Access. In Proc. of NTCIR
2008.
Thorsten Joachims. 2005. A Support Vector Method
for Multivariate Performance Measures. In Proc.
of ICML2005, pp 383-390.
Vladimir Vapnik 1998. Statistical learning theory.
John Wiley.
Xiaobing Xue, Jiwoon Jeon, W.Bruce Croft. 2008.
Retrieval Models for Question and Answer
Archives. In Proc. of SIGIR 2008, pp 475-482.
Yutaka Sasaki. 2005. Question Answering as
Question-biased Term Extraction: A New Ap-
proach toward Multilingual QA. In Proc. of ACL
2005, pp 215-222.
Youzheng Wu, Jun Zhao, Bo Xu, and Hao Yu. 2005.
Chinese Named Entity Recognition Model based
on Multiple Features. In Proc. of HLT/EMNLP
2005, Canada, pp 427-434.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin,
Dingyi Han, Yong Yu. 2008. Understanding
and Summarizing Answers in Community-Based
Question Answering Services. In Proc. of COL-
ING 2008, Manchester, pp 497-504.
