Constructivist Development of
Grounded Construction Grammars
Luc Steels
University of Brussels (VUB AI Lab)
SONY Computer Science Lab - Paris
6 Rue Amyot, 75005 Paris steels@arti.vub.ac.be
Abstract
The paper reports on progress in building com-
putational models of a constructivist approach to
language development. It introduces a formalism
for construction grammars and learning strategies
based on invention, abduction, and induction. Ex-
amples are drawn from experiments exercising the
model in situated language games played by embod-
ied artificial agents.
1 Introduction
The constructivist approach to language learning
proposes that ?children acquire linguistic compe-
tence (...) only gradually, beginning with more
concrete linguistic structures based on particular
words and morphemes, and then building up to
more abstract and productive structures based on
various types of linguistic categories, schemas, and
constructions.? (TomaselloBrooks, 1999), p. 161.
The approach furthermore assumes that language
development is (i) grounded in cognition because
prior to (or in a co-development with language)
there is an understanding and conceptualisation of
scenes in terms of events, objects, roles that objects
play in events, and perspectives on the event, and
(ii) grounded in communication because language
learning is intimately embedded in interactions with
specific communicative goals. In contrast to the
nativist position, defended, for example, by Pinker
(Pinker, 1998), the constructivist approach does not
assume that the semantic and syntactic categories
as well as the linking rules (specifying for example
that the agent of an action is linked to the subject
of a sentence) are universal and innate. Rather, se-
mantic and syntactic categories as well as the way
they are linked is built up in a gradual developmen-
tal process, starting from quite specific ?verb-island
constructions?.
Although the constructivist approach appears to
explain a lot of the known empirical data about child
language acquisition, there is so far no worked out
model that details how constructivist language de-
velopment works concretely, i.e. what kind of com-
putational mechanisms are implied and how they
work together to achieve adult (or even child) level
competence. Moreover only little work has been
done so far to build computational models for han-
dling the sort of ?construction grammars? assumed
by this approach. Both challenges inform the re-
search discussed in this paper.
2 Abductive Learning
In the constructivist literature, there is often the im-
plicit assumption that grammatical development is
the result of observational learning, and several re-
search efforts are going on to operationalise this ap-
proach for acquiring grounded lexicons and gram-
mars (see e.g. (Roy, 2001)). The agents are given
pairs with a real world situation, as perceived by the
sensori-motor apparatus, and a language utterance.
For example, an image of a ball is shown and at the
same time a stretch of speech containing the word
?ball?. Based on a generalisation process that uses
statistical pattern recognition algorithms or neural
networks, the learner then gradually extracts what
is common between the various situations in which
the same word or construction is used, thus progres-
sively building a grounded lexicon and grammar of
a language.
The observational learning approach has had
some success in learning words for objects and ac-
quiring simple grammatical constructions, but there
seem to be two inherent limitations. First, there is
the well known poverty of the stimulus argument,
widely accepted in linguistics, which says that there
is not enough data in the sentences normally avail-
able to the language learner to arrive at realistic
lexicons and grammars, let alne learn at the same
time the categorisations and conceptualisations of
the world implied by the language. This has lead
many linguists to adopt the nativist position men-
tioned earlier. The nativist position could in princi-
ple be integrated in an observational learning frame-
work by introducing strong biases on the generali-
sation process, incorporating the constraints of uni-
versal grammar, but it has been difficult to identify
and operationalise enough of these constraints to do
concrete experiments in realistic settings. Second,
observational learning assumes that the language
system (lexicon and grammar) exists as a fixed static
system. However, observations of language in use
shows that language users constantly align their lan-
guage conventions to suit the purposes of specific
conversations (ClarkBrennan, 1991). Natural lan-
guages therefore appear more to be like complex
adaptive systems, similar to living systems that con-
stantly adapt and evolve. This makes it difficult
to rely exclusively on statistical generalisation. It
does not capture the inherently creative nature of
language use.
This paper explores an alternative approach,
which assumes a much more active stance from lan-
guage users based on the Peircian notion of abduc-
tion (Fann, 1970). The speaker first attempts to
use constructions from his existing inventory to ex-
press whatever he wants to express. However when
that fails or is judged unsatisfactory, the speaker
may extend his existing repertoire by inventing new
constructions. These new constructions should be
such that there is a high chance that the hearer may
be able to guess their meaning. The hearer also
uses as much as possible constructions stored in
his own inventory to make sense of what is being
said. But when there are unknown constructions,
or the meanings do not fit with the situation being
talked about, the hearer makes an educated guess
about what the meaning of the unknown language
constructions could be, and adds them as new hy-
potheses to his own inventory. Abductive construc-
tivist learning hence relies crucially on the fact that
both agents have sufficient common ground, share
the same situation, have established joint attention,
and share communicative goals. Both speaker and
hearer use themselves as models of the other in or-
der to guess how the other one will interpret a sen-
tence or why the speaker says things in a particular
way.
Because both speaker and hearer are taking risks
making abductive leaps, a third activity is needed,
namely induction, not in the sense of statistical gen-
eralisation as in observational learning but in the
sense of Peirce (Fann, 1970): A hypothesis arrived
at by making educated guesses is tested against
further data coming from subsequent interactions.
When a construction leads to a successful interac-
tion, there is some evidence that this construction
is (or could become) part of the set of conventions
adopted by the group, and language users should
therefore prefer it in the future. When the construc-
tion fails, the language user should avoid it if alter-
natives are available.
Implementing these visions of language learn-
ing and use is obviously an enormous challenge for
computational linguistics. It requires not only cog-
nitive and communicative grounding, but also gram-
mar formalisms and associated parsing and produc-
tion algorithms which are extremely flexible, both
from the viewpoint of getting as far as possible
in the interpretation or production process despite
missing rules or incompatibilities in the inventories
of speaker and hearer, and from the viewpoint of
supporting continuous change.
3 Language Games
The research reported here uses a methodological
approach which is quite common in Artificial Life
research but still relatively novel in (computational)
linguistics: Rather than attempting to develop sim-
ulations that generate natural phenomena directly,
as one does when using Newton?s equations to sim-
ulate the trajectory of a ball falling from a tower,
we engage in computational simulations and robotic
experiments that create (new) artificial phenomena
that have some of the characteristics of natural phe-
nomena and hence are seen as explaining them.
Specifically, we implement artificial agents with
components modeling certain cognitive operations
(such as introducing a new syntactic category, com-
puting an analogy between two events, etc.), and
then see what language phenomena result if these
agents exercise these components in embodied situ-
ated language games. This way we can investigate
very precisely what causal factors may underly cer-
tain phenomena and can focus on certain aspects of
(grounded) language use without having to face the
vast full complexity of real human languages. A
survey of work which follows a similar methodol-
ogy is found in (CangelosiParisi, 2003).
The artificial agents used in the experiments driv-
ing our research observe real-world scenes through
their cameras. The scenes consist of interactions
between puppets, as shown in figure 1. These
scenes enact common events like movement of peo-
ple and objects, actions such as push or pull, give
or take, etc. In order to achieve the cognitive
grounding assumed in constructivist language learn-
ing, the scenes are processed by a battery of rela-
tively standard machine vision algorithms that seg-
ment objects based on color and movement, track
objects in real-time, and compute a stream of low-
level features indicating which objects are touch-
ing, in which direction objects are moving, etc.
These low-level features are input to an event-
recognition system that uses an inventory of hier-
archical event structures and matches them against
the data streaming in from low-level vision, similar
to the systems described in (SteelsBaillie, 2003).
Figure 1: Scene enacted with puppets so that typical
interactions between humans involving agency can
be perceived and described.
In order to achieve the communicative ground-
ing required for constructivist learning, agents go
through scripts in which they play various language
games, similar to the setups described in (Steels,
2003). These language games are deliberately quite
similar to the kind of scenes and interactions used in
a lot of child language research. A language game
is a routinised interaction between two agents about
a shared situation in the world that involves the ex-
change of symbols. Agents take turns playing the
role of speaker and hearer and give each other feed-
back about the outcome of the game. In the game
further used in this paper, one agent describes to
another agent an event that happened in the most
recently experienced scene. The game succeeds if
the hearer agrees that the event being described oc-
curred in the recent scene.
4 The Lexicon
Visual processing and event recognition results in
a world model in the form of a series of facts de-
scribing the scene. To play the description game, the
speaker selects one event as the topic and then seeks
a series of facts which discriminate this event and its
objects against the other events and objects in the
context. We use a standard predicate calculus-style
representation for meanings. A semantic structure
consists of a set of units where each unit has a ref-
erent, which is the object or event to which the unit
draws attention, and a meaning, which is a set of
clauses constraining the referent. A semantic struc-
ture with one unit is for example written down as
follows:
[1] unit1   ev1   fall(ev1,true), fall-1(ev1,obj1), ball(obj1)
where unit1 is the unit, ev1 the referent, and fall(ev1,
true), fall-1(ev1,obj1), ball(obj1) the meaning. The
different arguments of an event are decomposed
into different predicates. For example, for ?John
gives a book to Mary?, there would be four clauses:
give(ev1,true) for the event itself, give-1(ev1, John),
for the one who gives, give-2(ev1,book1), for the ob-
ject given, and give-3(ev1,Mary), for the recipient.
This representation is more flexible and makes it
possible to add new components (like the manner
of an event) at any time.
Syntactic structures mirror semantic structures.
They also consist of units and the name of units
are shared with semantic structures so that cross-
reference between them is straightforward. The
form aspects of the sentence are represented in a
declarative predicate calculus style, using the units
as arguments. For example, the following unit is
constrained as introducing the string ?fall?:
[2] unit1   string(unit1, ?fall?)
The rule formalism we have developed uses ideas
from several existing formalisms, particularly
unification grammars and is most similar to the
Embodied Construction Grammars proposed in
(BergenChang, 2003). Lexical rules link parts of
semantic structure with parts of syntactic structure.
All rules are reversable. When producing, the
left side of a rule is matched against the semantic
structure and, if there is a match, the right side is
unified with the syntactic structure. Conversely
when parsing, the right side is matched against the
syntactic structure and the left side unified with the
semantic structure. Here is a lexical entry for the
word ?fall?.
[3] ?unit   ?ev   fall(?ev,?state), fall-1(?ev,?obj)
 ?unit   string(?unit,?fall?)
It specifies that a unit whose meaning is
fall(?ev,?state), fall-1(?ev,?obj) is expressed with
the string ?fall?. Variables are written down with a
question mark in front. Their scope is restricted to
the structure or rule in which they appear and rule
application often implies the renaming of certain
variables to take care of the scope constraints. Here
is a lexical entry for ?ball?:
[4] ?unit   ?obj   ball(?obj)
 ?unit   string(?unit,?ball?)
Lexicon lookup attempts to find the minimal set
of rules that covers the total semantic structure.
New units may get introduced (both in the syntactic
and semantic structure) if the meaning of a unit
is broken down in the lexicon into more than one
word. Thus, the original semantic structure in [1]
results after the application of the two rules [3]
and [4] in the following syntactic and semantic
structures:
[5] unit1   ev1   fall(ev1,true), fall-1(ev1,obj1)
unit2   obj1   ball(obj1)
??
unit1   string(unit1, ?fall?)
unit2   string(unit2, ?ball?)
If this syntactic structure is rendered, it produces
the utterance ?fall ball?. No syntax is implied yet.
In the reverse direction, the parser starts with the
two units forming the syntactic structure in [5]
and application of the rules produces the following
semantic structure:
[6] unit1   ?ev   fall(?ev,?state), fall-1(?ev,?obj)
unit2   ?obj1   ball(?obj1)
The semantic structure in [6] now contains variables
for the referent of each unit and for the various
predicate-arguments in their meanings. The inter-
pretation process matches these variables against
the facts in the world model. If a single consistent
series of bindings can be found, then interpretation
is successful. For example, assume that the facts in
the meaning part of [1] are in the world model then
matching [6] against them results in the bindings:
[7] ?ev/ev1, ?state/true, ?obj/obj1, ?obj1/obj1
When the same word or the same meaning is
covered by more than one rule, a choice needs
to be made. Competing rules may develop if an
agent invented a new word for a particular meaning
but is later confronted with another word used by
somebody else for the same meaning. Every rule
has a score and in production and parsing, rules
with the highest score are preferred.
When the speaker performs lexicon lookup and
rules were found to cover the complete semantic
structure, no new rules are needed. But when some
part is uncovered, the speaker should create a new
rule. We have experimented so far with a simple
strategy where agents lump together the uncovered
facts in a unit and create a brand new word, consist-
ing of a randomly chosen configuration of syllables.
For example, if no word for ball(obj1) exists yet to
cover the semantic structure in [1], a new rule such
as [4] can be constructed by the speaker and subse-
quently used. If there is no word at all for the whole
semantic structure in [1], a single word covering the
whole meaning will be created, giving the effect of
holophrases.
The hearer first attempts to parse as far as pos-
sible the given sentence, and then interprets the re-
sulting semantic structure, possibly using joint at-
tention or other means that may help to find the in-
tended interpretation. If this results in a unique set
of bindings, the language game is deemed success-
ful. But if there were parts of the sentence which
were not covered by any rule, then the hearer can
use abductive learning. The first critical step is to
guess as well as possible the meaning of the un-
known word(s). Thus suppose the sentence is ?fall
ball?, resulting in the semantic structure:
[8] unit1   ?ev   fall(?ev,?state), fall-1(?ev,?obj)
If this structure is matched, bindings for ?ev and
?obj are found. The agent can now try to find the
possible meaning of the unknown word ?ball?. He
can assume that this meaning must somehow help
in the interpretation process. He therefore concep-
tualises the same way as if he would be the speaker
and constructs a distinctive description that draws
attention to the event in question, for example by
constraining the referent of ?obj with an additional
predicate. Although there are usually several ways
in which obj1 differs from other objects in the con-
text. There is a considerable chance that the pred-
icate ball is chosen and hence ball(?obj) is abduc-
tively inferred as the meaning of ?ball? resulting in
a rule like [4].
Agents use induction to test whether the rules
they created by invention and abduction have been
adopted by the group. Every rule has a score, which
is local to each agent. When the speaker or hearer
has success with a particular rule, its score is in-
creased and the score of competing rules is de-
creased, thus implementing lateral inhibition. When
there is a failure, the score of the rule that was used
is decreased. Because the agents prefer rules with
the highest score, there is a positive feedback in
the system. The more a word is used for a partic-
ular meaning, the more success that word will have.
Figure 2: Winner-take-all effect in words competing
for same meaning. The x-axis plots language games
and the y-axis the use frequency.
Scores rise in all the agents for these words and so
progressively we see a winner-take-all effect with
one word dominating for the expression of a par-
ticular meaning (see figure 2). Many experiments
have by now been performed showing that this kind
of lateral inhibition dynamics allows a population
of agents to negotiate a shared inventory of form-
meaning pairs for content words (Steels, 2003).
5 Syntactisation
The reader may have noticed that the semantic
structure in [6] resulting from parsing the sentence
?fall ball?, includes two variables which will both
get bound to the same object, namely ?obj, intro-
duced by the predicate fall-1(?ev,?obj), and ?obj1, in-
troduced by the predicate ball(?obj1). We say that in
this case ?obj and ?obj1 form an equality. Just from
parsing the two words, the hearer cannot know that
the object involved in the fall event is the same as
the object introduced by ball. He can only figure
this out when looking at the scene (i.e. the world
model). In fact, if there are several balls in the
scene and only one of them is falling, there is no
way to know which object is intended. And even if
the hearer can figure it out, it is still desirable that
the speaker should provide extra-information about
equalities to optimise the hearer?s interpretation ef-
forts.
A major thesis of the present paper is that resolv-
ing equivalences between variables is the main mo-
tor for the introduction of syntax. To achieve it, the
agents could, as a first approximation, use rules like
the following one, to be applied after all lexical rules
have been applied:
[9] ?unit1   ?ev1   fall-1(?ev1,?obj2)
?unit2   ?obj2   ball(?obj2)

?unit1   string(?unit1, ?fall?)
?unit2   string(?unit2, ?ball?)
This rule is formally equivalent to the lexical rules
discussed earlier in the sense that it links parts of
a semantic structure with parts of a syntactic struc-
ture. But now more than one unit is involved. Rule
[9] will do the job, because when unifying its right
side with the semantic structure (in parsing) ?obj2
unifies with the variables ?obj (supplied by ?fall?)
and ?obj1 (supplied by ?ball?) and this forces them
to be equivalent. Note that ?unit1 in [9] only con-
tains those parts of the original meaning that involve
the variables which need to be made equal.
The above rule works but is completely specific to
this case. It is an example of the ad hoc ?verb-island?
constructions reported in an early stage of child lan-
guage development. Obviously it is much more de-
sirable to have a more general rule, which can be
achieved by introducing syntactic and semantic cat-
egories. A semantic category (such as agent, perfec-
tive, countable, male) is a categorisation of a con-
ceptual relation, which is used to constrain the se-
mantic side of grammatical rules. A syntactic cate-
gory (such as noun, verb, nominative) is a categori-
sation of a word or a group of words, which can
be used to constrain the syntactic side of grammati-
cal rules. A rule using categories can be formed by
taking rule [9] above and turning all predicates or
content words into semantic or syntactic categories.
[10] ?unit1   ?ev1   semcat1(?ev1,?obj2)
?unit2   ?obj2   semcat2(?obj2)

?unit1   syncat1 (?unit1)
?unit2   syncat2(?unit2)
The agent then needs to create sem-rules to cate-
gorise a predicate as belonging to a semantic cate-
gory, as in:
[11] ?unit1   ?ev1   fall-1(?ev1,?obj2)


?unit1   ?ev1   semcat1(?ev1,?obj1)
and syn-rules to categorise a word as belonging to a
syntactic category, as in:
[12] ?unit1   string(?unit1,?fall?)


?unit1   ?ev1   syncat1(?unit1)
These rules have arrows going only in one direction
because they are only applied in one way.1 During
production, the sem-rules are applied first, then the
lexical rules, next the syn-rules and then the gram-
1Actually if word morphology is integrated, syn-rules need
to be bi-directional, but this topic is not discussed further here
due to space limitations.
matical rules. In parsing, the lexical rules are ap-
plied first (in reverse direction), then the syn-rules
and the sem-rules, and only then the grammatical
rules (in reverse direction). The complete syntactic
and semantic structures for example [9] look as fol-
lows:
[13] unit1   ?ev1   fall(?ev1,?state), fall-1(?ev1,?obj),
semcat1(?ev1,?obj)
unit2   ?obj1   ball(?obj1), semcat2(?obj1)
??
unit1   string(unit1, ?fall?), syncat-1(unit1)
unit2   string(unit2, ?ball?), syncat-2(unit2)
The right side of rule [10] matches with this syntac-
tic structure, and if the left side of rule [10] is unified
with the semantic structure in [13] the variable ?obj2
unifies with ?obj and ?obj1, thus resolving the equal-
ity before semantic interpretation (matching against
the world model) starts.
How can language users develop such rules? The
speaker can detect equalities that need to be re-
solved by re-entrance: Before rendering a sentence
and communicating it to the hearer, the speaker re-
parses his own sentence and interprets it against the
facts in his own world model. If the resulting set
of bindings contains variables that are bound to the
same object after interpretation, then these equali-
ties are candidates for the construction of a rule and
new syntactic and semantic categories are made as
a side effect. Note how the speaker uses himself as
a model of the hearer and fixes problems that the
hearer might otherwise encounter. The hearer can
detect equalities by first interpreting the sentence
based on the constructions that are already part of
his own inventory and the shared situation and prior
joint attention. These equalities are candidates for
new rules to be constructed by the hearer, and they
again involve the introduction of syntactic and se-
mantic categories. Note that syntactic and semantic
categories are always local to an agent. The same
lateral inhibition dynamics is used for grammatical
rules as for lexical rules, and so is also a positive
feedback loop leading to a winner-take-all effect for
grammatical rules.
6 Hierarchy
Natural languages heavily use categories to tighten
rule application, but they also introduce additional
syntactic markings, such as word order, function
words, affixes, morphological variation of word
forms, and stress or intonation patterns. These
markings are often used to signal to which category
certain words belong. They can be easily incorpo-
rated in the formalism developed so far by adding
additional descriptors of the units in the syntactic
structure. For example, rule [10] can be expanded
with word order constraints and the introduction of
a particle ?ba?:
[14] ?unit1   ?ev1   semcat1(?ev1,?obj2)
?unit2   ?obj2   semcat2(?obj2)

?unit1   syncat1 (?unit1)
?unit2   syncat2(?unit2)
?unit3   string (?unit3, ?ba?)
?unit4   syn-subunits (  ?unit1, ?unit2, ?unit3  ),
preceeds(?unit2, ?unit3)
Note that it was necessary to introduce a superunit
?unit4 in order to express the word order constraints
between the ba-particle and the unit that introduces
the object. Applying this rule as well as the syn-
rules and sem-rules discussed earlier to the seman-
tic structure in [5] yields:
[13] unit1   ev1   fall(ev1,true), fall-1(ev1,obj),
semcat1(ev1,obj)
unit2   obj1   ball(obj1), semcat2(obj1)
??
unit1   string(unit1, ?fall?), syncat-1(unit1)
unit2   string(unit2, ?ball?), syncat-2(unit2)
unit3   string(unit3, ?ba?)
unit4   syn-subunits(  unit1,unit2,unit3  ),
preceeds(unit2,unit3)
When this syntactic structure is rendered, it pro-
duces ?fall ball ba?, or equivalently ?ball ba fall?,
because only the order between ?ball? and ?ba? is
constrained.
Obviously the introduction of additional syntac-
tic features makes the learning of grammatical rules
more difficult. Natural languages appear to have
meta-level strategies for invention and abduction.
For example, a language (like Japanese) tends to use
particles for expressing the roles of objects in events
and this usage is a strategy both for inventing the ex-
pression of a new relation and for guessing what the
use of an unknown word in the sentence might be.
Another language (like Swahili) uses morphologi-
cal variations similar to Latin for the same purpose
and thus has ended up with a rich set of affixes. In
our experiments so far, we have implemented such
strategies directly, so that invention and abduction
is strongly constrained. We still need to work out
a formalism for describing these strategies as meta-
rules and research the associated learning mecha-
nisms.
Figure 3: The graph shows the dependency structure
as well as the phrase-structure emerging through the
application of multiple rules
When the same word participates in several
rules, we automatically get the emergence of
hierarchical structures. For example, suppose that
two predicates are used to draw attention to obj1 in
[5]: ball and red. If the lexicon has two separate
words for each predicate, then the initial semantic
structure would introduce different variables so that
the meaning after parsing ?fall ball ba red? would
be:
[15] fall(?ev,?state), fall-1(?ev,?obj), ball (?obj),
red(?obj2)
To resolve the equality between ?obj and ?obj2, the
speaker could create the following rule:
[14] ?unit1   ?obj   semcat3(?obj)
?unit2   ?obj   semcat4(?obj)

?unit1   syncat3(?unit1)
?unit2   syncat4(?unit2)
?unit3   syn-subunits (  unit1,unit2  ), pre-
ceeds(unit1,unit2)
The predicate ball is declared to belong to semcat4
and the word ?ball? to syncat4. The predicate red
belongs to semcat3 and the word ?red? to syncat3.
Rendering the syntactic structure after application
of this rule gives the sentence ?fall red ball ba?. A
hierarchical structure (figure 3) emerges because
?ball? participates in two rules.
7 Re-use
Agents obviously should not invent new conven-
tions from scratch every time they need one, but
rather use as much as possible existing categorisa-
tions and hence existing rules. This simple economy
principle quickly leads to the kind of syntagmatic
and paradigmatic regularities that one finds in natu-
ral grammars. For example, if the speaker wants to
express that a block is falling, no new semantic or
syntactic categories or linking rules are needed but
block can simply be declared to belong to semcat4
and ?block? to syncat3 and rule [14] applies.
Re-use should be driven by analogy. In one of
the largest experiments we have carried out so far,
agents had a way to compute the similarity between
two event-structures by pairing the primitive opera-
tions making up an event. For example, a pick-up
action is decomposed into: an object moving into
the direction of another stationary object, the first
object then touching the second object, and next the
two objects moving together in (roughly) the oppo-
site direction. A put-down action has similar sub-
events, except that their ordering is different. The
roles of the objects involved (the hand, the object
being picked up) are identical and so their gram-
matical marking could be re-used with very low risk
of being misunderstood. When a speaker reuses a
grammatical marking for a particular semantic cate-
gory, this gives a strong hint to the hearer what kind
of analogy is expected. By using these invention
and abduction strategies, semantic categories like
agent or patient gradually emerged in the artificial
grammars. Figure 4 visualises the result of this ex-
periment (after 700 games between 2 agents taking
turns). The x-axis (randomly) ranks the different
predicate-argument relations, the y-axis their mark-
ers. Without re-use, every argument would have its
own marker. Now several markers (such as ?va? or
?zu?) cover more than one relation.
Figure 4: More compact grammars result from re-
use based on semantic analogies.
8 Conclusions
The paper reports significant steps towards the com-
putational modeling of a constructivist approach to
language development. It has introduced aspects of
a construction grammar formalism that is designed
to handle the flexibility required for emergent de-
veloping grammars. It also proposed that invention,
abduction, and induction are necessary and suffi-
cient for language learning. Much more technical
work remains to be done but already significant ex-
perimental results have been obtained with embod-
ied agents playing situated language games. Most
of the open questions concern under what circum-
stances syntactic and semantic categories should be
re-used.
Research funded by Sony CSL with additional fund-
ing from ESF-OMLL program, EU FET-ECAgents and
CNRS OHLL.
References
Bergen, B.K. and N.C. Chang. 2003. Embod-
ied Construction Grammar in Simulation-Based
Language Understanding. TR 02-004, ICSI,
Berkeley.
Cangelosi, and D. Parisi 2003. Simulating the Evo-
lution of Language. Springer-Verlag, Berlin.
Clark, H. and S. Brennan 1991. Grounding in com-
munication. In: Resnick, L. J. Levine and S.
Teasley (eds.) Perspectives on Socially Shared
Cognition. APA Books, Washington. p. 127-149.
Fann, K.T. 1970. Peirce?s Theory of Abduction
Martinus Nijhoff, The Hague.
Roy, D. 2001. Learning Visually Grounded Words
and Syntax of Natural Spoken Language. Evolu-
tion of communication 4(1).
Pinker, S. 1998. Learnability and Cognition: The
acquisition of Argument Structure. The MIT
Press, Cambridge Ma.
Steels, L. 2003 Evolving grounded communication
for robots. Trends in Cognitive Science. Volume
7, Issue 7, July 2003 , pp. 308-312.
Steels, L. and J-C. Baillie 2003. Shared Ground-
ing of Event Descriptions by Autonomous Robots.
Journal of Robotics and Autonomous Systems
43, 2003, pp. 163-173.
Tomasello, M. and P.J. Brooks 1999. Early syntac-
tic development: A Construction Grammar ap-
proach In: Barrett, M. (ed.) (1999) The Develop-
ment of Language Psychology Press, London. pp.
161-190.
Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 73?80,
New York City, June 2006. c?2006 Association for Computational Linguistics
A (very) Brief Introduction to Fluid Construction Grammar
Luc Steels(1,2) and Joachim de Beule(1)
(1) University of Brussels (VUB AI Lab)
(2) SONY Computer Science Lab - Paris
steels@arti.vub.ac.be
Abstract
Fluid Construction Grammar (FCG) is a
new linguistic formalism designed to ex-
plore in how far a construction gram-
mar approach can be used for handling
open-ended grounded dialogue, i.e. dia-
logue between or with autonomous em-
bodied agents about the world as experi-
enced through their sensory-motor appa-
ratus. We seek scalable, open-ended lan-
guage systems by giving agents both the
ability to use existing conventions or on-
tologies, and to invent or learn new ones
as the needs arise. This paper contains a
brief introduction to the key ideas behind
FCG and its current status.
1 Introduction
Construction grammar is receiving growing atten-
tion lately, partly because it has allowed linguists
to discuss a wide range of phenomena which were
difficult to handle in earlier frameworks (Goldberg,
1995; OstmanFried, 2005; Croft, 2001), and partly
because it has allowed psychologists to describe in
a more satisfactory way early language develop-
ment (TomaselloBrooks, 1999). There were already
some attempts to formalise construction grammar
(KayFillmore, 1999) and build a computational im-
plementation (BergenChang, 2003), but many open
problems remain and at this early stage of fun-
damental research, it makes sense to explore al-
ternative approaches. In our team, we focus on
open-ended grounded dialogue, in other words how
it is possible for a speaker to formulate an utter-
ance about the world and for a hearer to under-
stand what is meant (ClarkBrennan, 1991). The
present paper briefly reports on the formalisation
of construction grammar called Fluid Construc-
tion Grammar (FCG) that we have developed for
this research. Although the formalism is novel in
several fundamental aspects, it also builds heav-
ily on the state of the art in formal and computa-
tional linguistics, particularly within the tradition of
unification-based feature structure grammars such as
HPSG (PollardSag, 1994). FCG has been under de-
velopment from around 2001 and an implementa-
tion on a LISP substrate has been released through
http://arti.vub.ac.be/FCG/ in 2005. The FCG core
engine (for parsing and production) is fully opera-
tional and has already been used in some large-scale
experiments in language grounding (SteelsLoetzsch,
2006). We do not claim to have a complete solu-
tion for all linguistic issues that arise in construction
grammar, and neither do we claim that the solutions
we have adopted so far are final. On the contrary, we
are aware of many difficult technical issues that still
remain unresolved and welcome any discussion that
would bring us forward.
2 Motivations
FCG grew out of efforts to understand the creative
basis of language. Language creativity is more than
the application of an existing set of rules (even if
the rules are recursive and thus allow an infinite set
of possible sentences). Human language users often
stretch and expand rules whenever the need arises,
73
Figure 1: Typical experimental setup. The bot-
tom shows two robots moving around in an envi-
ronment that contains balls and boxes. The robots
are equiped with a complex sensory-motor system,
able to detect the objects and build an analog world
model of their location and trajectories (as shown in
the right top corner).
and occasionally invent totally new ones. So we
need to understand how new aspects of language
(new concepts and conceptualisations, new lexical
items, new syntactic and semantic categories, new
grammatical constructions, new interaction patterns)
may arise and spread in a population, the same way
biologists try to understand how new life forms may
arise (Steels, 2003).
This motivation leads immediately to some require-
ments. First of all we always use multi-agent sim-
ulations so that we can investigate the spreading of
conventions in a population. Agents take turns be-
ing speaker and hearer and build up competences in
conceptualisation and verbalisation (for production)
and parsing and interpretation (for understanding).
They must be able to store an inventory of rules and
apply them in either processing direction, and they
must be able to expand their inventories both by in-
venting new constructions if necessary and by adopt-
ing those used by others. Second, the agents must
have something to talk about. We are interested in
grounded language, which means dialogue about ob-
jects and events in the world as perceived through a
sensory-motor apparatus. We take embodiment lit-
erally. Our experiments use physical robots (Sony
AIBOs) located in a real world environment (see fig-
ure 1 from (SteelsLoetzsch, 2006)) Third, the agents
must be motivated to say and learn something. We
achieve this by programming the robots with scripts
to play language games. A language game sets up
a joint attentional frame so that robots share gen-
eral motives for interaction, a specific communica-
tive goal (for example draw attention to an object),
and give feedback to enable repair of miscommu-
nication (for example through pointing). We typi-
cally perform experiments in which a population of
agents starts with empty conceptual and linguistic
repertoires and then builds from scratch a communi-
cation system that is adequate for a particular kind of
language game. Agents seek to maximise commu-
nicative success while minimising cognitive effort.
One advantage of grounded language experiments
is that we can clearly monitor whether the capaci-
ties given to the agents are adequate for bootstrap-
ping a language system and how efficient and suc-
cessful they are. By starting from scratch, we can
also test whether our objective of understanding lan-
guage creativity has been achieved. Of course such
experiments will never spontaneously lead to the
emergence of English or any other human language,
but we can learn a great deal about the processes that
have given rise and are still shaping such languages.
3 Meaning
The information about an utterance is organized in
a semantic and a syntactic structure. The seman-
tic structure is a decomposition of the utterance?s
meaning and contains language-specific semantic
re-categorisations (for example a put-event is cate-
gorised as a cause-move-location with an agent, a
patient and a location). The syntactic structure is
a decomposition of the form of the utterance into
constituents and morphemes and contains additional
syntactic categorisations such as syntactic features
(like number and gender), word order constraints,
etc.
We follow a procedural semantics approach, in the
sense that the meaning of an utterance is a program
that the hearer is assumed to execute (Winograd,
1972; Johnson-Laird, 1997). Hence conceptualisa-
tion becomes a planning process (to plan the pro-
gram) and interpretation becomes the execution of
a program. For example, the meaning of a phrase
like ?the box? is taken to be a program that in-
volves the application of an image schema to the
flow of perceptual images and anchor it to a partic-
74
ular physical object in the scene. So we do not as-
sume some pre-defined or pre-processed logic-style
fact base containing the present status of the world
(as this is extremely difficult to extract and main-
tain from real world perception in a noisy and fast
changing world) but view language as playing an
active role in how the world is perceived and cate-
gorised. It is in principle possible to use many dif-
ferent programming languages, but we have opted
for constraint based processing and designed a new
constraint programming language IRL (Incremental
Recruitment Language) and implemented the neces-
sary planning, chunking and execution mechanisms
of constraint networks (SteelsBleys, 2005). A sim-
ple example of a constraint network for ?the box? is
as follows1:
1. (equal-to-context ?s)
2. (filter-set-prototype ?r ?s ?p)
3. (prototype ?p [box])
4. (select-element ?o ?r ?d)
5. (determiner ?d [single-unique])
Equal-to-context, select-element,
etc. are primitive constraints that implement funda-
mental cognitive operators. Equal-to-context
grabs the set of elements in the current context
and binds it to ?s. Filter-set-prototype
filters this set with a prototype ?p which is bound
in (3) to [box]. Select-element selects an
element ?o from ?r according to the determiner
?d which is bound to [single-unique] in
(5), meaning that ?r should be a singleton. The
constraints are powerful enough to be used both in
interpretation, when semantic objects such as pro-
totypes, determiners, categories, relations, etc. are
supplied through language and values need to be
found for other variables, and in conceptualisation,
when these values are known but the objective is
to find the semantic objects. Moreover, during
conceptualization the constraints may extend the
repertoire of semantic objects (e.g. introducing a
new prototype) if needed, allowing the agents to
progressively build up their ontologies.
1We use prefix notation. Order does not play a role as the
constraint interpreter cycles through the network until all vari-
ables are bound or until no further progress can be made. Sym-
bols starting with a question mark represent variables.
Figure 2: Left: decomposition of the constraint pro-
gram for ?the ball? in the semantic structure. Right:
related syntactic structure. In reality both structures
contain a lot more information.
4 Syntactic and Semantic Structures
As mentioned, FCG organises the information about
an utterance in feature structures, similar to other
feature-structure based formalisms (as first intro-
duced by Kay (Kay, 1984)) but with some impor-
tant differences. An FCG feature structure contains
units which correspond (roughly) to words (more
precisely morphemes) and constituents.
A unit has a name and a set of features. Hierarchical
structure is not implicitly represented by embedding
one unit in another one, but explicitly by the fea-
tures syn-subunits (for the syntactic structure) and
sem-subunits (for the semantic structure). There is a
strong correspondence between the syntactic and se-
mantic structure built up for the same utterance (see
figure 2) although there can be units which only ap-
pear in the syntactic structure (for example for gram-
matical function words) and vice versa. The cor-
respondence is maintained by using the same unit
names in both the semantic and syntactic structure.
Units in syntactic structures have three features: (1)
syn-subunits, (2) syn-cat which contains the syn-
tactic categories, and (3) form containing the form
associated with the unit. Units in semantic struc-
tures have four features: (1) sem-subunits, (2) sem-
cat containing the semantic categories, (3) meaning
which is the part of the utterance?s meaning covered
by the unit, and (4) context which contains variables
that occur in the meaning but are ?external? in the
sense that they are linked to variables occurring in
the meaning of other units. An example semantic
structure (in list-notation) for the left structure in
figure 2 is shown in figure 3. FCG is a completely
open-ended formalism in the sense that all linguistic
75
Figure 3: Semantic structure in list-notation.
categories (syntactic or semantic) are open and in
principle language-specific (as in radical construc-
tion grammar (Croft, 2001).) Thus the set of lexical
categories (noun, verb, adjective, etc.), of possible
semantic roles (agent, patient, etc.), of syntactic fea-
tures (number, gender, politeness, etc.), and so on,
are all open. The value of the syn-cat and sem-cat
features consists of a conjunction of predicates (each
possibly having arguments.) New categories can be
introduced at any time and used as (part of) a pred-
icate. The form of the utterance is described in a
declarative manner, using predicates like precedes or
meets which define linear ordering relations among
the form of units or any other aspect of surface form
including prosodic contour or stress.
5 Rules
A rule (also called template) typically expresses
constraints on possible meaning-form mappings.
Each rule has a score which reflects the success
that the agent has had in using it. All else be-
ing equal, agents prefer rules with higher scores,
thus reflecting frequency effects. A rule has two
poles. A left pole which typically contains con-
straints on semantic structure formulated as a fea-
ture structure with variables, and a right pole which
typically contains constraints on syntactic structure
again formulated as a feature structure with vari-
ables. Rules are divided into rule subsets which
help constrain the order of rule-application and de-
sign large-scale grammars. Thus we make a distinc-
tion between morph-rules, which decompose a word
into a stem and pending morphemes and introduce
syntactic categories; lex-stem-rules, which associate
meaning with the stem as well as valence informa-
tion and a role-frame; con-rules, which correspond
to grammatical constructions that associate parts of
semantic structure with parts of syntactic structure;
and sem and syn-rules which perform inference over
semantic or syntactic categories to expand semantic
or syntactic structure.
All rules are bi-directional. Typically, during pro-
duction, the left pole is ?unified? with the semantic
structure under construction, possibly yielding a set
of bindings. If successful, the right pole is ?merged?
with the syntactic structure under construction. The
merge operation can be understood as a partial uni-
fication, but extending the structure with those parts
of the pole that were missing. During parsing, the
right pole is unified with the syntactic structure and
parts of the left pole are added to the semantic
structure. The unification phase is thus used to see
whether a rule is triggered and the merge phase rep-
resents the actual application of the rule. The FCG
Unify and Merge operators are defined in great for-
mal detail in (SteelsDeBeule, 2006). During pro-
duction lex-stem-rules are applied before the con-
rules and the morph-rules. During parsing the lex-
stem-rules are applied right after the morph-rules.
The con-rules then build higher order structure. It
is enormously challenging to write rules that work
in both directions but this strong constraint is very
helpful to achieve a compact powerful grammar.
6 Building Hierarchy
One of the innovative aspects of FCG is the way it
handles hierarchy. Both the left-pole and the right-
pole of a construction can introduce hierarchical
structure with the J-operator (DeBeuleSteels, 2005).
This way, the semantic pole of constructions (lexical
or grammatical) can decompose the meaning to be
expressed (which originally resides in the top node
of the semantic structure) and the syntactic pole can
group units together into a larger constituent. Con-
straints governed by the J-operator do not have to
match during the unification phase. Instead they are
used to build additional structure during the merge
76
Figure 4: Example lexical entry for ?put? and illus-
tration of the J-operator.
phase. This may include the construction of a new
unit as well as pending from an existing unit and ab-
sorbing some other units.
Figure 4 shows an example which will be used fur-
ther in the next section. It is a lexical rule prepar-
ing a resultative construction (GoldbergJackendoff,
2004). The semantic pole of the rule combines
some stretch of meaning (the introduction of an
event-type, namely a put-event) with a frame (cause-
move-location with roles for agent, patient and loca-
tion). These are associated with a lexical stem ?put?
in the right pole which also adds a valence frame
SVOL (triggering the subject-verb-object-location
construction). In production, this rule triggers when
a ?put? event-type is part of the meaning (?==? means
?includes but may also contain additional expres-
sions?). When merging the semantic pole with the
semantic structure, a new unit hanging from ?top is
created and the specified value of the meaning fea-
ture copied down. The new unit also receives the
context and sem-cat features as specified by the J-
operator. At the same time, the syntactic pole is
merged with the syntactic structure and so the ?new-
unit (which is already bound) is added as a subunit
of ?top in the syntactic structure as well. The J-
operator will then add stem and valence informa-
tion. Thus the semantic structure of figure 5 will
be transformed into the one of figure 6. And the cor-
responding syntactic structure becomes as in figure
7. In parsing, an existing syntactic unit with stem
((unit-2
(meaning
( ..
(event-type ev-type1
(put (put-1 o1) (put-2 o11)
(put-3 o22))) ... ))))
Figure 5: Semantic structure triggering the rule in
figure 4 in production.
((unit-2
(sem-subunits (... unit-3 ...)))
(unit-3
(meaning
((event-type
ev-type1
(put (put-1 o1) (put-2 o11)
(put-3 o22)))))
(context ((link ev-type1)))
(sem-cat
((sem-event-type
ev-type1
(cause-move-location
(agent o1) (patient o11)
(location o22))))))
... )
Figure 6: Resulting semantic structure after apply-
ing the rule in figure 4 to the semantic structure of
figure 5.
((unit-2
(syn-subunits (... unit-3 ...)))
(unit-3
(form ((stem unit-3 "put")))
(syn-cat ((valence SVOL))))
... )
Figure 7: Resulting syntactic structure after apply-
ing the rule in figure 4.
?put? is required to trigger the rule. If found, the
rule will add the valence information to it and on
77
the semantic side the meaning as well as the seman-
tic categorisation in terms of a cause-move-location
frame are added.
7 Implementing Constructions
Lexical constructions provide frame and valence in-
formation for word stems and parts of meaning.
Grammatical constructions bind all this together.
Figure 8 shows an example of a grammatical con-
struction. It also uses the J-operator to build hier-
archy, both on the semantic side (to decompose or
add meaning) and on the syntactic side (to group
constituents together.) An example of a SVOL-
construct is Mary puts the milk in the refrigerator.
Before application of the construction, various units
should already group together the words making up
a nounphrase for the subject (which will be bound
to ?subject-unit), a nounphrase for the direct object
(bound to the ?object-unit) and a prepositional noun-
phrase (bound to ?oblique-unit). Each of these units
also will bind variables to their referents, commu-
nicated as context to the others. On the semantic
side the cause-move-location frame with its various
roles aids to make sure that all the right variable
bindings are established. On the syntactic side the
construction imposes word-order constraints (ex-
pressed with the meets-predicate), the valence of the
verb, and specific types of constituents (nounphrase,
verbphrase, prepositional nounphrase). The SVOL
construction operates again in two directions. In
production it is triggered when the semantic struc-
ture built so far unifies with the semantic pole, and
then the syntactic structure is expanded with the
missing parts from the syntactic pole. Constraints
on the syntactic pole (e.g. valence) may prevent
the application of the construction. In parsing, the
SVOL construction is triggered when the syntactic
structure built so far unifies with the syntactic pole
and the semantic structure is then expanded with the
missing parts from the semantic pole. Again ap-
plication may be constrained when semantic con-
straints in the construction prevent it.
8 Fluidity, Conventionalisation and
Meta-grammars
Although FCG must become adequate for dealing
with the typical phenomena that we find in human
natural languages, our main target is to make scien-
tific models of the processes that underly the origins
of language, in other words of the creative process
by which language users adapt or invent new forms
to express newmeanings that unavoidably arise in an
open world and negotiate tacitly the conventions that
they adopt as a group. We have already carried out
a number of experiments in this direction and here
only a brief summary can be given (for more dis-
cussion see: (Steels, 2004; DeBeuleBergen, 2006;
SteelsLoetzsch, 2006)).
In our experiments, speaker and hearer are cho-
sen randomly from a population to play a language
game as part of a situated embodied interaction that
involves perception, joint attention and feedback.
When the speaker conceptualizes the scene, he may
construct new semantic objects (for example new
categories) or recruit new constraint networks in
order to achieve the communicative goal imposed
by the game. Also when the speaker is trying to
verbalise the constraint network that constitutes the
meaning of an utterance, there may be lexical items
missing or new constructions may have to be built.
We use a meta-level architecture with reflection to
organise this process. The speaker goes through the
normal processing steps, using whatever inventory is
available. Missing items may accumulate and then
the speaker moves to a meta-level, trying to repair
the utterance by stretching existing constructions,
re-using them by analogy for new purposes, or in-
troducing other linguistic items. The speaker also
engages in self-monitoring by re-entering the utter-
ance and comparing what he meant to say to inter-
pretations derived by parsing his own utterance. The
speaker can thus detect potential problems for the
listener such as combinatorial explosions in pars-
ing, equalities among variables which were not ex-
pressed, etc. and these problems can be repaired by
the introduction of additional rules.
The hearer receives an utterance and tries to go as
far as possible in the understanding process. The
parser and interpreter are not geared towards check-
ing for grammaticality but capable to handle utter-
ances even if a large part of the rules are missing.
The (partial) meaning is then used to arrive at an in-
terpretation, aided by the fact that the context and
communicative goals are restricted by the language
game. If possible, the hearer gives feedback on how
78
he understood the utterance and whether an interpre-
tation was found. If there is failure or miscommuni-
cation the hearer will then repair his inventory based
on extra information provided by the speaker. This
can imply the introduction of new concepts extend-
ing the ontology, storing new lexical items, intro-
ducing new constructions, assigning certain words
to new syntactic classes, etc. Speaker and hearer
also update the scores of all rules and concepts. In
case of success, scores go up of the items that were
used and competitors are decreased to achieve lat-
eral inhibition and hence a positive feedback loop
between success and use. In case of failure, scores
go down so that the likelihood of using the failing
solution diminishes. In our simulations, games are
played consecutively by members of a population
and we have been able to show ?so far for relatively
simple forms of language? that shared communi-
cation systems can emerge from scratch in popula-
tions. Much work remains to be done in researching
the repair strategies needed and when they should be
triggered. The repair strategies themselves should
also be the subject of negotiation among the agents
because they make use of a meta-grammar that de-
scribes in terms of rules (with the same syntax and
processing as the FCG rules discussed here) how re-
pairs are to be achieved.
9 Conclusions
FCG is a tool offered to the community of re-
searchers interested in construction grammar. It al-
lows the precise formal definition of constructions in
a unification-based feature structure grammar style
and contains the necessary complex machinery for
building an utterance starting from meaning and
reconstructing meaning starting from an utterance.
FCG does not make linguistic theorising superflu-
ous, on the contrary, the formalism is open to any
framework of linguistic categories or organisation
of grammatical knowledge as long as a construction
grammar framework is adopted. There is obviously
a lot more to say, not only about how we handle
various linguistic phenomena (such as inheritance
of properties by a parent phrasal unit from its head
subunit) but also what learning operators can pro-
gressively build fluid construction grammars driven
by the needs of communication. We refer the reader
to the growing number of papers that provide more
details on these various aspects.
10 Acknowledgement
This research was conducted at the Sony Computer
Science Laboratory in Paris and the University of
Brussels VUB Artificial Intelligence Laboratory. It
is partially sponsored by the EU ECAgents project
(FET IST-1940). FCG and the experiments in lan-
guage evolution are team work and major contribu-
tions were made by Joris Bleys, Martin Loetzsch,
Nicolas Neubauer, Wouter Van den Broeck, Remy
Van Trijp, and Pieter Wellens.
References
Bergen, B.K. and N.C. Chang. (2003) Embodied Con-
struction Grammar in Simulation-Based Language
Understanding. Technical Report 02-004, Interna-
tional Computer Science Institute, Berkeley.
Clark, H. and S. Brennan (1991) Grounding in com-
munication. In: Resnick, L. J. Levine and S. Teasley
(eds.) Perspectives on Socially Shared Cognition. APA
Books, Washington. p. 127-149.
Croft, William A. (2001). Radical Construction Gram-
mar; Syntactic Theory in Typological Perspective. Ox-
ford: Oxford University Press.
De Beule, J. and L. Steels (2005) Hierarchy in Fluid
Construction Grammar. In: Furbach, U. (eds) (2005)
Proceedings of KI-2005. Lecture Notes in AI 3698.
Springer-Verlag, Berlin. p.1-15.
De Beule, J. and B. Bergen (2006) On the Emergence
of Compositionality. Proceedings of the Evolution of
Language Conference VI, Rome.
Goldberg, A.E. (1995) Constructions.: A Construction
Grammar Approach to Argument Structure. Univer-
sity of Chicago Press, Chicago
Goldberg, A. and R. Jackendoff (2004) The English Re-
sultative as a Family of Constructions. Language 80
532-568.
Johnson-Laird, P.N. (1997) Procedural Semantics. Cog-
nition, 5 (1977) 189-214.
Goldberg, A. (2003) Constructions: A new theoretical
approach to language Trends in Cognitive Science.
Volume 7, Issue 5, May 2003 , pp. 219-224.
Kay, P. and C. Fillmore (1999) Grammatical construc-
tions and linguistic generalizations: the Whats X do-
ing Y? construction. Language 75(1), 133.
79
Kay, M. (1984) Functional unification grammar: A for-
malism for machine translation. Proceedings of the
International Conference of Computational Linguis-
tics.
Pollard, C.and I. Sag (1994) Head-driven Phrase Struc-
ture Grammar. CSLI Stanford Univ, Calif.
Ostman, Jan-Ola and Mirjam Fried (eds.) (2005) Con-
struction Grammars: Cognitive grounding and theo-
retical extensions. W. Benjamins, Amsterdam.
Steels, L. (2003) Evolving grounded communication for
robots. Trends in Cognitive Science. Volume 7, Issue
7, July 2003 , pp. 308-312.
Steels, L. (2004) Constructivist Development of
Grounded Construction Grammars. In D. Scott, W.
Daelemans and M. Walker (Eds.), Proceedings Annual
Meeting of Association for Computational Linguistics
Conference. Barcelona: ACL , (pp. 9-16).
Steels, L. and J. De Beule (2006) Unify and Merge in
FCG. In: Vogt, P. et.al. (eds.) Proceedings of EELC III.
Lecture Notes in Computer Science. Springer-Verlag,
Berlin.
Steels, L. and J. Bleys (2005) PlanningWhat To Say: Sec-
ond Order Semantics for Fluid Construction Gram-
mars. In: Proceedings of CAEPIA 2005. Santiago de
Compostella.
Steels, L. and M. Loetzsch (2006) Perspective Alignment
in Spatial Language. In: Coventry, K., J. Bateman
and T. Tenbrink (2006) Spatial Language in Dialogue.
Oxford University Press. Oxford.
Tomasello, M. and P.J. Brooks (1999) Early syntactic
development: A Construction Grammar approach In:
Barrett, M. (ed.) (1999) The Development of Language
Psychology Press, London. pp. 161-190.
Winograd, T. (1972) Understanding natural language.
New York, Academic Press.
(def-con-rule SVOL-Phrase
((?top
(sem-subunits
(== ?subject-unit ?verb-unit
?object-unit ?oblique-unit)))
(?subject-unit
(context (== (link ?subject))))
(?verb-unit
(context (== (link ?event ?event-type)))
(sem-cat
(== (sem-event-type ?event-type
(cause-move-location
(agent ?subject)
(patient ?object)
(location ?oblique))))))
(?object-unit
(context (== (link ?object))))
(?oblique-unit
(context (== (link ?oblique))))
((J ?new-unit ?top
(?subject-unit ?verb-unit
?object-unit ?oblique-unit))
(context (== (link ?event)))))
<-->
((?top
(form
(==
(meets ?subject-unit ?verb-unit)
(meets ?verb-unit ?object-unit)
(meets ?object-unit
?oblique-unit)))
(syn-subunits
(== ?subject-unit ?verb-unit
?object-unit ?oblique-unit)))
(?subject-unit
(syn-cat
(== (constituent NounPhrase))))
(?verb-unit
(syn-cat
(== (constituent VerbPhrase)
(valence SVOL))))
(?object-unit
(syn-cat
(== (constituent NounPhrase))))
(?oblique-unit
(syn-cat
(== (constituent PrepNounPhrase))))
((J ?new-unit ?top
(?subject-unit ?verb-unit
?object-unit ?oblique-unit))
(syn-cat
(== (constituent sentence))))))
Figure 8: A resultative construction.
80
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 63?68,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Fluid Construction Grammar:
The New Kid on the Block
Remi van Trijp1, Luc Steels1,2, Katrien Beuls3, Pieter Wellens3
1Sony Computer Science 2ICREA Institute for 3 VUB AI Lab
Laboratory Paris Evolutionary Biology (UPF-CSIC) Pleinlaan 2
6 Rue Amyot PRBB, Dr Aiguidar 88 1050 Brussels (Belgium)
75005 Paris (France) 08003 Barcelona (Spain) katrien|pieter@
remi@csl.sony.fr steels@ai.vub.ac.be ai.vub.ac.be
Abstract
Cognitive linguistics has reached a stage
of maturity where many researchers are
looking for an explicit formal grounding
of their work. Unfortunately, most current
models of deep language processing incor-
porate assumptions from generative gram-
mar that are at odds with the cognitive
movement in linguistics. This demonstra-
tion shows how Fluid Construction Gram-
mar (FCG), a fully operational and bidi-
rectional unification-based grammar for-
malism, caters for this increasing demand.
FCG features many of the tools that were
pioneered in computational linguistics in
the 70s-90s, but combines them in an inno-
vative way. This demonstration highlights
the main differences between FCG and re-
lated formalisms.
1 Introduction
The ?cognitive linguistics enterprise? (Evans
et al 2007) is a rapidly expanding research dis-
cipline that has so far avoided rigorous formal-
izations. This choice was wholly justified in the
70s-90s when the foundations of this scientific
movement were laid (Rosch, 1975; Lakoff, 1987;
Langacker, 1987), and it remained so during the
past two decades while the enterprise worked on
getting its facts straight through empirical stud-
ies in various subfields such as language acqui-
sition (Tomasello, 2003; Goldberg et al 2004;
Lieven, 2009), language change and grammati-
calization (Heine et al 1991; Bar?dal and Chel-
liah, 2009), and corpus research (Boas, 2003; Ste-
fanowitsch and Gries, 2003). However, with nu-
merous textbooks on the market (Lee, 2001; Croft
and Cruse, 2004; Evans and Green, 2006), cogni-
tive linguistics has by now established itself as a
serious branch in the study of language, and many
cognitive linguists are looking for ways of explic-
itly formalizing their work through computational
models (McClelland, 2009).
Unfortunately, it turns out to be very difficult
to adequately formalize a cognitive linguistic ap-
proach to grammar (or ?construction grammar?)
using the tools for precision-grammars developed
in the 70s-90s such as unification (Kay, 1979;
Carpenter, 1992), because these tools are typi-
cally incorporated in a generative grammar (such
as HPSG; Ginzburg and Sag, 2000) whose as-
sumptions are incompatible with the foundations
of construction grammar. First, cognitive linguis-
tics blurs the distinction between ?competence?
and ?performance?, which means giving up the
sharp distinction between declarative and proce-
dural representations. Next, construction gram-
marians argue for a usage-based approach (Lan-
gacker, 2000), so the constraints on features may
change and features may emerge or disappear
from a grammar at any given time.
This demonstration introduces Fluid Construc-
tion Grammar (FCG; Steels, 2011, 2012a), a
novel unification-based grammar formalism that
addresses these issues, and which is available as
open-source software at www.fcg-net.org.
After more than a decade of development, FCG
is now ready to handle sophisticated linguistic
issues. FCG revisits many of the technologies
developed by computational linguists and intro-
duces several key innovations that are of inter-
est to anyone working on deep language process-
ing. The demonstration illustrates these innova-
tions through FCG?s interactive web interface.
63
semantic 
pole
syntactic 
pole
transient structure
semantic 
pole
syntactic 
pole
construction
matching phase
first 
merging 
phase
second 
merging 
phase
semantic 
pole
syntactic 
pole
transient structure
semantic 
pole
syntactic 
pole
construction
second
merging
phase
first 
merging 
phase
matching phase
Figure 1: FCG allows the implementation of efficient and strongly reversible grammars. Left: In production,
conditional units of the semantic pole of a construction are matched against a transient structure, before additional
semantic constraints and the syntactic pole are merged with the structure. Right: In parsing, the same algorithm
applies but in the opposite direction.
2 Strong and Efficient Reversibility
Reversible or bidirectional grammar formalisms
can achieve both production and parsing (Strza-
lkowski, 1994). Several platforms, such as the
LKB (Copestake, 2002), already achieve bidirec-
tionality, but they do so through separate algo-
rithms for parsing and production (mainly for effi-
ciency reasons). One problem with this approach
is that there may be a loss of coherence in gram-
mar engineering. For instance, the LKB parser
can handle a wider variety of structures than its
generator.
FCG uses one core engine that handles both
parsing and production with a single linguistic
inventory (see Figure 1). When processing, the
FCG-system builds a transient structure that con-
tains all the information concerning the utterance
that the system has to parse or produce, divided
into a semantic and syntactic pole (both of whom
are feature structures). Grammar rules or ?con-
structions? are coupled feature structures as well
and thus contain a semantic and syntactic pole.
When applying constructions, the FCG-system
goes through three phases. In production, FCG
first matches all feature-value pairs of the seman-
tic pole of a construction with the semantic pole
of the transient structure, except fv-pairs that are
marked for being attributed by the construction
(De Beule and Steels, 2005). Matching is a more
strict form of unification that resembles a sub-
sumption test (see Steels and De Beule, 2006).
If matching is successful, all the marked fv-pairs
of the semantic pole are merged with the tran-
sient structure in a first merge phase, after which
the whole syntactic pole is merged in a second
phase. FCG-merge is equivalent to ?unification?
in other formalisms. The same three-phase algo-
rithm is applied in parsing as well, but this time in
the opposite direction: if the syntactic pole of the
construction matches with the transient structure,
the attributable syntactic fv-pairs and the seman-
tic pole are merged.
3 WYSIWYG Grammar Engineering
Most unification grammars use non-directional
linguistic representations that are designed to be
independent of any model of processing (Sag
and Wasow, 2011). Whereas this may be de-
sirable from a ?mathematical? point-of-view, it
puts the burden of efficient processing on the
shoulders of computational linguists, who have to
find a balance between faithfulness to the hand-
written theory and computational efficiency (Mel-
nik, 2005). For instance, there is no HPSG imple-
mentation, but rather several platforms that sup-
port the implementation of ?HPSG-like? gram-
mars: ALE (Carpenter and Penn, 1995), ALEP
(Schmidt et al 1996), CUF (D?rre and Dorna,
64
top
cxn-applied
top
nominal-adjectival-cxn
sem-subunits  
footprints  
args  
sem-cat  
nominal-adjectival-phrase-1
(word-ballon-1 
word-rouge-1)
(nominal-adjectival-cxn)
(red-ball-15 context-19)
((sem-function 
identifier))
word-
ballon-
1
word-
rouge-
1
word-le-1
sem syn
form  
syn-subunits  
syn-cat  
footprints  
nominal-adjectival-phrase-1
((meets 
word-ballon-1 
word-rouge-1))
(word-ballon-1 
word-rouge-1)
((number singular) 
(gender masculine) 
(syn-function nominal))
(nominal-adjectival-cxn)
word-
rouge-
1
word-
ballon-
1
word-le-1
Figure 2: FCG comes equipped with an interactive web interface for inspecting the linguistic inventory, con-
struction application and search. This Figure shows an example construction where two units are opened up for
closer inspection of their feature structures.
1993), LIGHT (Ciortuz, 2002), LKB (Copestake,
2002), ProFIT (Erbach, 1995), TDL (Krieger and
Sch?fer, 1994), TFS (Emele, 1994), and others
(see Bolc et al 1996, for a survey). Unfortu-
nately, the optimizations and technologies devel-
oped within these platforms are often considered
by theoretical linguists as engineering solutions
rather than scientific contributions.
FCG, on the other hand, adheres to the cogni-
tive linguistics assumption that linguistic perfor-
mance is equally important as linguistic compe-
tence, hence processing becomes a central notion
in the formalism. FCG representations therefore
offer a ?what you see is what you get? approach
to grammar engineering where the representations
have a direct impact on processing and vice versa.
For instance, a construction?s division between a
semantic and syntactic pole is informative with re-
spect to how the construction is applied.
Some grammarians may object that this design
choice forces linguists to worry about process-
ing, but that is entirely the point. It has already
been demonstrated in other unification-based for-
malisms that different grammar representations
have a significant impact on processing efficiency
(Flickinger, 2000). Moreover, FCG-style repre-
sentations can be directly implemented and tested
without having to compromise on either faithful-
ness to a theory or computational efficiency.
Since writing grammars is highly complex,
however, FCG also features a ?design level? on top
of its operational level (Steels, 2012b). On this
level, grammar engineers can use templates that
build detailed constructions. The demonstration
shows how to write a grammar in FCG, switch-
ing between its design level, its operational level
and its interactive web interface (see Figure 2).
The web interface allows FCG-users to inspect the
linguistic inventory, the search tree in processing,
and so on.
4 Robustness and Learning
Unification-based grammars have the reputation
of being brittle when it comes to processing nov-
elty or ungrammatical utterances (Tomuro, 1999).
Since cognitive linguistics adheres to a usage-
based view on language (Langacker, 2000), how-
ever, an adequate formalization must be robust
and open-ended.
A first requirement is that there can be differ-
ent degrees of ?entrenchment? in the grammar:
while some features might still be emergent, oth-
ers are already part of well-conventionalized lin-
guistic patterns. Moreover, new features and con-
structions may appear (or disappear) from a gram-
mar at any given time. These requirements are
hard to reconcile with the type hierarchy approach
of other formalisms, so FCG does not imple-
ment typed feature structures. The demonstra-
tion shows how FCG can nevertheless prevent
over-licensing of linguistic structures through its
matching phase and how it captures generaliza-
tions through its templates ? two benefits typically
associated with type hierarchies.
Secondly, FCG renders linguistic processing
fluid and robust through a meta-level architec-
ture, which consists of two layers of processing,
as shown in Figure 3 (Beuls et al 2012). There
is a routine layer in which constructional process-
ing takes place. At the same time, a meta-layer
65
!"!"
routine processing 
diagnostic 
problem repair 
diagnostic diagnostic diagnostic 
problem 
repair meta-layer processing 
Figure 3: There are two layers of processing in FCG. On the routine level, constructional processing takes place.
At the same time, a meta-layer of diagnostics and repairs try to detect and solve problems that occur in the routine
layer.
is active that runs diagnostics for detecting prob-
lems in routine processing, and repairs for solving
those problems. The demonstration shows how
the meta-layer is used for solving common prob-
lems such as missing lexical entries and coercion
(Steels and van Trijp, 2011), and how its archi-
tecture offers a uniform way of implementing the
various solutions for robustness already pioneered
in the aforementioned grammar platforms.
5 Efficiency
Unification is computationally expensive, and
many technical solutions have been proposed for
efficient processing of rich and expressive fea-
ture structures (Tomuro, 1999; Flickinger, 2000;
Callmeier, 2001). In FCG, however, research
on efficiency takes a different dimension because
performance is considered to be an integral part of
the linguistic theory that needs to be operational-
ized. The demonstration allows conference par-
ticipants to inspect the following research results
on the interplay between grammar and efficiency:
? In line with construction grammar, there is
no distinction between the lexicon and the
grammar. Based on language usage, the lin-
guistic inventory can nevertheless organize
itself in the form of dependency networks
that regulate which construction should be
considered when in processing (Wellens and
De Beule, 2010; Wellens, 2011).
? There is abundant psycholinguistic evidence
that language usage contains many ready-
made language structures. FCG incorporates
a chunking mechanism that is able to cre-
ate such canned phrases for faster processing
(Stadler, 2012).
? Morphological paradigms, such as the Ger-
man case system, can be represented in the
form of ?feature matrices?, which reduce
syntactic and semantic ambiguity and hence
speed up processing efficiency and reliability
(van Trijp, 2011).
? Many linguistic domains, such as spatial lan-
guage, are known for their high degree of
polysemy. By distinguishing between actual
and potential values, such polysemous struc-
tures can be processed smoothly (Spranger
and Loetzsch, 2011).
6 Conclusion
With many well-developed unification-based
grammar formalisms available to the community,
one might wonder whether any ?new kid on the
block? can still claim relevance today. With this
demonstration, we hope to show that Fluid Con-
struction Grammar allows grammar engineers to
unchart new territory, most notably in the relation
between linguistic competence and performance,
and in modeling usage-based approaches to lan-
guage.
66
References
Johanna Bar?dal and Shobhana Chelliah, edi-
tors. The Role of Semantic, Pragmatic and
Discourse Factors in the Development of Case.
John Benjamins, Amsterdam, 2009.
Katrien Beuls, Remi van Trijp, and Pieter
Wellens. Diagnostics and repairs in Fluid Con-
struction Grammar. In Luc Steels, editor, Com-
putational Issues in Fluid Construction Gram-
mar. Springer Verlag, Berlin, 2012.
Hans C. Boas. A Constructional Approach to Re-
sultatives. Stanford Monograph in Linguistics.
CSLI, Stanford, 2003.
Leonard Bolc, Krzysztof Czuba, Anna
Kups?c?, Malgorzata Marciniak, Agnieszka
Mykowiecka, and Adam Przepi?rkowski. A
survey of systems for implementing HPSG
grammars. Research Report 814 of IPI
PAN (Institute of Computer Science, Polish
Academy of Sciences), 1996.
Ulrich Callmeier. Efficient parsing with large-
scale unification grammars. Master?s thesis,
Universit?t des Saarlandes, 2001.
Bob Carpenter. The Logic of Typed Feature Struc-
tures. Cambridge UP, Cambridge, 1992.
Bob Carpenter and Gerald Penn. The Attribute
Logic Engine (Version 2.0.1). Pittsburgh, 1995.
Liviu Ciortuz. LIGHT ? a constraint language and
compiler system for typed-unification gram-
mars. In Proceedings of The 25th German Con-
ferences on Artificial Intelligence (KI 2002),
volume 2479 of LNAI, pages 3?17, Berlin,
2002. Springer-Verlag.
Ann Copestake. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stan-
ford, 2002.
William Croft and D. Alan Cruse. Cognitive Lin-
guistics. Cambridge Textbooks in Linguistics.
Cambridge University Press, Cambridge, 2004.
J. De Beule and L. Steels. Hierarchy in fluid con-
struction grammar. In U. Furbach, editor, Pro-
ceedings of the 28th Annual German Confer-
ence on Artificial Intelligence, volume 3698 of
Lecture Notes in Artificial Intelligence, pages
1?15, Berlin, Germany, 2005. Springer Verlag.
Jochen D?rre and Michael Dorna. CUF ? a
formalism for linguistic knowledge represen-
tation. In Jochen D?rre, editor, Computa-
tional Aspects of Constraint Based Linguistic
Descriptions, volume I, pages 1?22. DYANA-2
Project, Amsterdam, 1993.
Martin C. Emele. The typed feature structure rep-
resentation formalism. In Proceedings of the
International Workshop on Sharable Natural
Language Resources, Ikoma, Nara, 1994.
Gregor Erbach. ProFIT: Prolog with features,
inheritance and templates. In Proceedings of
EACL-95, 1995.
Vyvyan Evans and Melanie Green. Cognitive Lin-
guistics: An Introduction. Lawrence Erlbaum
Associates / Edinburgh University Press, Hills-
dale, NJ/Edinburgh, 2006.
Vyvyan Evans, Benjamin K. Bergen, and J?rg
Zinken. The cognitive linguistics enterprise:
An overview. In V. Evans, B.K. Bergen, and
J. Zinken, editors, The Cognitive Linguistics
Reader. Equinox Publishing, London, 2007.
Daniel P. Flickinger. On building a more efficient
grammar by exploiting types. Natural Lan-
guage Engineering, 6(1):15?28, 2000.
Jonathan Ginzburg and Ivan A. Sag. Interroga-
tive Investigations: the Form, the Meaning, and
Use of English Interrogatives. CSLI Publica-
tions, Stanford, 2000.
Adele E. Goldberg, Devin M. Casenhiser, and
Nitya Sethuraman. Learning argument struc-
ture generalizations. Cognitive Linguistics, 15
(3):289?316, 2004.
Bernd Heine, Ulrike Claudi, and Friederike H?n-
nemeyer. Grammaticalization: A Concep-
tual Framework. University of Chicago Press,
Chicago, 1991.
Martin Kay. Functional grammar. In Proceedings
of the Fifth Annual Meeting of the Berkeley Lin-
guistics Society, pages 142?158. Berkeley Lin-
guistics Society, 1979.
Hans-Ulrich Krieger and Ulrich Sch?fer. TDL ?
a type description language for HPSG. part 1:
Overview. In Proceedings of the 15th Interna-
tional Conference on Computational Linguis-
tics, pages 893?899, Kyoto, 1994.
George Lakoff. Women, Fire, and Danger-
ous Things: What Categories Reveal about
the Mind. The University of Chicago Press,
Chicago, 1987.
67
Ronald W. Langacker. Foundations of Cognitive
Grammar: Theoretical Prerequisites. Stanford
University Press, Stanford, 1987.
Ronald W. Langacker. A dynamic usage-based
model. In Michael Barlow and Suzanne Kem-
mer, editors, Usage-Based Models of Lan-
guage, pages 1?63. Chicago University Press,
Chicago, 2000.
David Lee. Cognitive Linguistics: An Introduc-
tion. Oxford University Press, Oxford, 2001.
Elena Lieven. Developing constructions. Cogni-
tive Linguistics, 20(1):191?199, 2009.
James L. McClelland. The place of modeling in
cognitive science. Topics in Cognitive Science,
1:11?38, 2009.
Nurit Melnik. From ?hand-written? to computa-
tionally implemented HPSG theories. In Ste-
fan M?ller, editor, Proceedings of the HPSG05
Conference, Stanford, 2005. CSLI Publica-
tions.
Eleanor Rosch. Cognitive representations of se-
mantic categories. Journal of Experimental
Psychology: General, 104:192?233, 1975.
Ivan A. Sag and Thomas Wasow. Performance-
compatible competence grammar. In Robert D.
Borsley and Kersti B?rjars, editors, Non-
Transformational Syntax: Formal and Explicit
Models of Grammar, pages 359?377. Wiley-
Blackwell, 2011.
Paul Schmidt, Sibylle Rieder, Axel Theofilidis,
and Thierry Declerck. Lean formalisms, lin-
guistic theory, and applications. grammar de-
velopment in ALEP. In Proceedings of the
16th International Conference on Computa-
tional Linguistics (COLING-96), pages 286?
291, Copenhagen, 1996.
Michael Spranger and Martin Loetzsch. Syntac-
tic indeterminacy and semantic ambiguity: A
case study for German spatial phrases. In Luc
Steels, editor, Design Patterns in Fluid Con-
struction Grammar. John Benjamins, Amster-
dam, 2011.
Kevin Stadler. Chunking constructions. In
Luc Steels, editor, Computational Issues in
Fluid Construction Grammar. Springer Verlag,
Berlin, 2012.
Luc Steels, editor. Design Patterns in Fluid Con-
struction Grammar. John Benjamins, Amster-
dam, 2011.
Luc Steels, editor. Computational Issues in
Fluid Construction Grammar. Springer, Berlin,
2012a.
Luc Steels. Design methods for Fluid Construc-
tion Grammar. In Luc Steels, editor, Computa-
tional Issues in Fluid Construction Grammar.
Springer Verlag, Berlin, 2012b.
Luc Steels and Joachim De Beule. Unify and
merge in Fluid Construction Grammar. In
P. Vogt, Y. Sugita, E. Tuci, and C. Nehaniv,
editors, Symbol Grounding and Beyond., LNAI
4211, pages 197?223, Berlin, 2006. Springer.
Luc Steels and Remi van Trijp. How to make con-
struction grammars fluid and robust. In Luc
Steels, editor, Design Patterns in Fluid Con-
struction Grammar, pages 301?330. John Ben-
jamins, Amsterdam, 2011.
Anatol Stefanowitsch and Stefan Th. Gries. Col-
lostructions: Investigating the interaction of
words and constructions. International Journal
of Corpus Linguistics, 2(8):209?243, 2003.
Tomek Strzalkowski, editor. Reversible Grammar
in Natural Language Processing. Kluwer Aca-
demic Publishers, Boston, 1994.
Michael Tomasello. Constructing a Language. A
Usage Based Theory of Language Acquisition.
Harvard University Press, 2003.
Noriko Tomuro. Left-Corner Parsing Algorithm
for Unification Grammars. PhD thesis, DePaul
University, Chicago, 1999.
Remi van Trijp. Feature matrices and agree-
ment: A case study for German case. In Luc
Steels, editor, Design Patterns in Fluid Con-
struction Grammar, pages 205?236. John Ben-
jamins, Amsterdam, 2011.
Pieter Wellens. Organizing constructions in net-
works. In Luc Steels, editor, Design Patterns in
Fluid Construction Grammar. John Benjamins,
Amsterdam, 2011.
Pieter Wellens and Joachim De Beule. Prim-
ing through constructional dependencies: A
case study in Fluid Construction Grammar.
In A. Smith, M. Schouwstra, Bart de Boer,
and K. Smith, editors, The Evolution of Lan-
guage (EVOLANG8), pages 344?351, Singa-
pore, 2010. World Scientific.
68
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 127?132,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Fluid Construction Grammar for Historical and Evolutionary Linguistics
Pieter Wellens1, Remi van Trijp2, Katrien Beuls1, Luc Steels2,3
1VUB AI Lab 2Sony Computer Science 3 ICREA Institute for
Pleinlaan 2 Laboratory Paris Evolutionary Biology (UPF-CSIC)
1050 Brussels (Belgium) 6 Rue Amyot PRBB, Dr Aiguidar 88
pieter|katrien@ 75005 Paris (France) 08003 Barcelona (Spain)
ai.vub.ac.be remi@csl.sony.fr steels@ai.vub.ac.be
Abstract
Fluid Construction Grammar (FCG) is an
open-source computational grammar for-
malism that is becoming increasingly pop-
ular for studying the history and evolution
of language. This demonstration shows
how FCG can be used to operationalise the
cultural processes and cognitive mecha-
nisms that underly language evolution and
change.
1 Introduction
Historical linguistics has been radically trans-
formed over the past two decades by the ad-
vent of corpus-based approaches. Ever increas-
ing datasets, both in size and richness of anno-
tation, are becoming available (Yuri et al, 2012;
Davies, 2011), and linguists now have more pow-
erful tools at their disposal for uncovering which
changes have taken place. In this demonstration,
we present Fluid Construction Grammar (Steels,
2011, FCG), an open-source grammar formalism
that makes it possible to also address the question
of how these changes happened by uncovering the
cognitive mechanisms and cultural processes that
drive language evolution.
FCG combines the expressive power of fea-
ture structures and unification with the adaptiv-
ity and robustnes of machine learners. In sum,
FCG aims to be an open instrument for de-
veloping robust and open-ended models of lan-
guage processing that can be used for both pars-
ing and production. FCG can be downloaded at
http://www.fcg-net.org.
2 Design Philosophy
Fluid Construction Grammar is rooted in a
cognitive-functional approach to language, which
is quite different from a generative grammar such
as HPSG (Pollard and Sag, 1994). A genera-
tive grammar is a model of language competence
that licenses well-formed structures and rejects ill-
formed utterances. Such grammars often decide
on the well- or ill-formedness of utterances by us-
ing a strong type system that defines a set of fea-
tures and possible values for those features. The
burden of efficient and robust language process-
ing with a generative grammar largely rests on the
shoulders of the language processor.
A cognitive-functional grammar, on the other
hand, functions more like a transducer between
meaning and form. In parsing, such a grammar
tries to uncover as much meaning as possible from
a given utterance rather than deciding on its gram-
maticality. In the other direction, the grammar
tries to produce intelligible utterances, which are
well-formed as a side-effect if the grammar ad-
equately captures the conventions of a particular
language. A cognitive-functional grammar can
best be implemented without a strong type system
because the set of possible features and values for
them is assumed to be open-ended. Efficient and
robust language processing also becomes a joint
responsibility of the grammar and the linguistic
processor.
3 Reversible Language Processing
As a construction grammar, FCG represents all
linguistic knowledge as pairings of function and
form (called constructions). This means that any
linguistic item, be it a concrete lexical item (see
Figure 1) or a schematic construction, shares the
same fundamental representation in FCG.
Each construction consists of two poles (a se-
mantic/functional one and a syntactic/form one),
each represented as a feature structure. By using a
separate semantic and syntactic pole, FCG allows
the same construction to be efficiently parsed and
produced by the same processing engine by sim-
ply changing the direction of application.
127
reset
tag ?meaning-849 
footprints  
?top-unit-1611
(meaning
(==
(identify-person
?kim-1
 
?context-243
?person-119)
(bind
 
person
?person-119
 
[kim])))
(==0
 
kim-lex
 
lex)
footprints  
tag ?form-946 
?top-unit-1611
kim-lex (lex)
?top-unit-1611
(==0
 
kim-lex
 
lex)
(form
(
==
 
(string
 
?word-kim-1
 
"Kim")))
?top-unit-1611
sem syn
args  
sem-cat  
footprints  
?word-kim-1
? ?meaning-849
(?kim-1)
((sem-function
referring)
(sem-class
 
person))
(==1
 
kim-lex
 
lex)
footprints  
syn-cat  
?word-kim-1
? ?form-946
(==1
 
kim-lex
 
lex)
((lex-cat
proper-noun)
(syn-function
nominal))
Babel web interface http://localhost:8000/
1 of 1 12/6/12 11:08 PM
Figure 1: Lexical construction for the proper
noun ?Kim? as shown in the FCG web interface.
All constructions are mappings between semantic
(left) and syntactic feature structures (right).
FCG processing uses two different kinds of uni-
fication called match and merge. The match phase
is a conditional phase which checks for applicabil-
ity of the construction. The merge operation most
closely resembles classical (yet untyped) unifica-
tion. In production (i.e. going from meaning to
form), the processor will consider a construction?s
semantic pole as a set of conditions that need to be
satisfied, and the syntactic pole as additional infor-
mation that can be contributed by the construction.
In parsing (i.e. going from form to meaning), the
roles of the poles are reversed.
Since FCG pays a lot of attention to the inter-
action between linguistic knowledge and process-
ing, it makes it possible to investigate the conse-
quences of particular aspects of grammar with re-
gard to representation, production, parsing, learn-
ing and propagation (in a population of language
users). For example, a small case system may be
easier to represent and produce than a large sys-
tem, but it might also lead to increased ambigu-
ity in parsing and learning that the larger system
would avoid. Fluid Construction Grammar can
bring these differences to the surface for further
computational analysis.
It is exactly this ability to monitor the impact of
grammatical choices, that has sparked the interest
of an increasingly wide audience of historical and
evolutionary linguists. With FCG, different histor-
ical stages can be implemented (which addresses
questions about representation and processing) but
FCG also comes bundled with a reflective learn-
ing framework (Beuls et al, 2012) for learning the
key constructions of each stage. That same archi-
tecture has proven to be adequately powerful to
implement processes of grammaticalization so that
Linguistic system 1
Reconstruction
Individual Learning
Population 
Alignment
Grammaticalization
Linguistic system 2
Reconstruction
Individual Learning
Population 
Alignment
1.
2.
3.
1.
2.
3.
4.
Figure 2: Schematic overview of the experimental
methodology for historical and evolutionary lin-
guists. The example here shows only two linguis-
tic stages but there could be more.
actual linguistic change over time can be modeled
(van Trijp, 2010; Beuls and Steels, 2013; Wellens
and Loetzsch, 2012).
4 How to set up an evolutionary
linguistics experiment in FCG?
As the FCG processor can both produce and
parse utterances it is possible to instantiate not
one but a set or population of FCG processors
(or FCG agents) that can communicatively inter-
act with each other. Experiments in historical or
evolutionary linguistics make use of this multi-
agent approach where all agents engage in situated
pairwise interactions (language games) (Steels,
2012b).
In this systems demo we will focus on a re-
cent experiment in the emergence of grammatical
agreement (Beuls and Steels, 2013). The language
game consists of two agents in which one agent
(the speaker) has to describe one or more (max
three) objects in a scene to the other agent (the
hearer). Each object can be described by one or
more words. It follows that without any grammat-
ical marking it would be difficult (often impossi-
ble) for the hearer to figure out which words de-
scribe the same object and thus to arrive at a suc-
cessful interpretation. The hypothesis is that the
introduction of agreement markers helps solve this
ambiguity.
Next to setting up a language game script the
methodology consists of operationalizing the lin-
guistic strategies required for a population to boot-
strap and maintain a particular linguistic system (in
this case nominal agreement). Examples of lin-
128
!"!"
routine processing 
diagnostic 
problem repair 
diagnostic diagnostic diagnostic 
problem 
repair meta-layer processing 
Figure 3: Reflective meta-layer architecture oper-
ating as part of an FCG agent/processor.
guistic systems already investigated include Ger-
man case (van Trijp, 2012a; van Trijp, 2013),
the grammatical expression of space (Spranger
and Steels, 2012), the emergence of quantifiers
(Pauw and Hilferty, 2012) and the expression of
aspect in Russian (Gerasymova et al, 2012) [for
an overview see (Steels, 2011; Steels, 2012a)].
An experiment generally investigates multi-
ple linguistic systems of increasing complexity
where each system can, but need not, map to a
stage along an attested grammaticalization path-
way. Most often a stage is introduced in order
to gradually increase the complexity of the emer-
gent dynamics. In this demo we posit four sys-
tems/strategies, (1) a baseline purely lexical strat-
egy, (2) a strategy to bootstrap and align formal
(meaningless) agreement markers, (3) a strategy to
bootstrap and align meaningful agreement mark-
ers, and finally (4) a strategy that allows re-use
of existing lexical constructions as markers (gram-
maticalization).
Implementing and linking together all the com-
ponents involved in a single system is a highly
non-trivial undertaking and our methodology pre-
scribes the following four steps to undertake for
each system (see also Figure 2).
Reconstruction: A full operationalization of all
the constructions (lexical and grammatical)
involved in the chosen linguistic phenom-
ena. When multiple agents are initialized
with these constructions they should be able
to communicate successfully with each other.
This stage serves primarily to test and verify
intuitions about the different linguistic sys-
tems.
Individual Learning: Implementation of learn-
ing algorithms (or re-use of existing ones)
Figure 4: Meaningful marker strategy.
so that one agent can learn the constructions
based on the input of another agent. These
learning operations are generally divided into
diagnostics and repair strategies (see Fig-
ure 3). Diagnostics continually monitor FCG
processing for errors or inefficiencies and
generate problems if they are found. Repair
strategies then act on these problems by al-
tering the linguistic inventory (e.g. adding,
removing or changing constructions).
Population Alignment: There exists a large gap
between the cognitive machinary needed for
learning an existing linguistic system (step 2)
and bootstrapping, aligning and maintaining
a complete linguistic system from scratch. In
this step individual learning operators are ex-
tended with alignment strategies.
Grammaticalization: Moving from one linguis-
tic system to another is the final step of the
experiment. The challenge is to find and im-
plement the mechanisms that drive grammat-
icalization (Heine and Kuteva, 2007) in line
with observed grammaticalization pathways.
As an example we?ll give a short sketch of one
possible game as played in the meaningful marker
strategy as schematically shown in Figure 4. The
sketch shows a context of four objects (O1 to O4),
each described by three features. The speaker
chooses topic O1 + O2 which, given his vocab-
ulary (shown top right), results in uttering ?shuq-
fon sizhic zabu?. Words ?shuqfon? and ?sizhic?
both describe parts of O1 and ?zabu? of O2. In
order to explicitly communicate this linking the
speaker attaches the markers ?-ti? and ?-ta? so that
their meaning is compatible with the objects they
are linking as shown in the Figure. This allows
129
Figure 5: A network of constructions. Diamond shaped nodes represent lexical constructions, egg shaped
nodes represent grammatical constructions and rectangular nodes represent semantic categories. Arrows
can be read as ?primes?. For example the preposition between [BETWEEN.PREP] primes the category
LOCATIVE RELATION which in turn primes both the [LOCATIVE RELATION] and [SPATIAL PHRASE]
constructions. Both of these constructions also require a semantic category [REFERENT].
the hearer to arrive at a single non-ambiguous in-
terpretation. For more details we refer the reader
to (Beuls and Steels, 2013) and the web demo at
http://ai.vub.ac.be/materials/plos-agreement/.
5 Features of FCG
A number of key features of FCG have already
been introduced. Reversible bidirectional process-
ing, a single data representation for all linguistic
knowledge, a reflective meta-layer architecture for
learning and a multi-agent component for manag-
ing multiple interacting FCG instances. Other fea-
tures, some of which are unique to FCG, include,
but are not limited to:
Web interface: FCG comes with a rich
HTML/AJAX based web interface (Loet-
zsch, 2012) where it can show fine-grained
information to the user in a user-friendly
manner through the use of expandable
elements. See Figure 6.
Customizable processing: Linguistic process-
ing is implemented as a search process
(Bleys et al, 2011). The user has easy
access to the most important parameters
influencing this process. Examples of these
are the heuristics and the tests that determine
whether a node represents an acceptable
solution. FCG comes bundled with a library
of heuristics and goal tests and with a bit
of programming skills users can add new
primitives easily.
Customizable construction inventory: By de-
fault, FCG stores all constructions in one
large set. FCG however supplies a num-
ber of different taxonomies, both for concep-
tual and efficiency reasons. One popular op-
tion is to organize constructions in smaller
subsets (Beuls, 2011) like lexical, morpho-
logical, functional, etc. Another option is
to use networks (Wellens, 2011) that can
learn co-occurrence relations between con-
structions and ?prime? constructions when
they are likely to apply (see Figure 5).
Interfaces to external repositories: FCG
can connect to external repositories like
Framenet (Baker et al, 1998) and Wordnet
(Miller, 1995) to load thousands of lexical
entries (Micelli et al, 2009; Wellens and
Beule, 2010).
Robustness: FCG continues operation as far as
it can get even if some constructions do not
apply (Steels and van Trijp, 2011). Sup-
plied with appropriate diagnostics and repair
strategies FCG can even recover from errors
(van Trijp, 2012b).
Open source: Best of all, FCG is freely down-
loadable and open source (http://www.fcg-
net.org). It is written in Common Lisp
(CLOS) and compatible with most popu-
lar lisp implementations (SBCL, CCL, Lisp-
works, ...).
130
top
top
Parsing "block"
Applying construction set (70)  in direction 
Found a solution
initialstructure top
applicationprocess
appliedconstructions
resultingstructure
top
Meaning:
((apply-class ?ref-2 ?src-2 ?class-1) (bind object-class ?class-1 block))
sem syn
initial
top
top
cxn-applied
application result
status cxn-applied
sourcestructure top
appliedconstruction
resultingstructure top
resultingbindings ((?form-84 form ((string block-83 "block"))) (?block-unit-2 . block-83) (?top-39 . top))
added infirst merge block-83
added insecondmerge
block-83
cxn supplier :ordered-by-label
remaining labels (cat  gram)
remaining cxns (right-lex speaker-lex unique-lex hearer-lex)
block-morph (morph t)
sem syn
block-morph (morph t)
sem syn block-83 block-lex(lex t)
noun-
cat
(cat t)
noun-cat (cat t) block-lex (lex t) block-morph (morph t)
noun-unit-273
footprints  
meaning  
ref  
sem-cat  
block-83
(block-lex)
((bind object-class ?class-1 block))
?class-1
((sem-function ((value ?sem-function-value-4) (valence (identifier))))(class (object-class)))
sem syn noun-unit-273 block-83
expanded search tree node
expanded unit
Figure 6: An example of parsing the noun ?Block? as shown in the FCG web interface. Users can click
on nearly every element to show an expanded version.
The reader is encouraged to take a look at
http://www.fcg-net.org/projects/design-patterns-
in-fluid-construction-grammar for a selection of
demonstrations of Fluid Construction Grammar.
6 Conclusion
Fluid Construction Grammar is a mature technol-
ogy that can be used by computational linguists
to complement more traditional corpus-based ap-
proaches. FCG builds on many existing and
proven technologies and adds new innovations to
the mix resulting in a user friendly, yet powerful
and extensible framework for in-depth investiga-
tions in natural language phenomena.
Acknowledgments
The FCG formalism is being developed at the Ar-
tificial Intelligence Laboratory of the Vrije Uni-
versiteit Brussel and the Sony Computer Science
Laboratory in Paris. Pieter Wellens has been
supported by the ESF EuroUnderstanding project
DRUST funded by FWO and by the Vrije Uni-
versiteit Brussel. Katrien Beuls received fund-
ing from a strategic basic research grant from the
agency for Innovation by Science and Technol-
ogy (IWT). Remi van Trijp is funded by the Sony
Computer Science Laboratory Paris. We would
also like to thank Michael Spranger for his con-
tributions to the FCG formalism.
131
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Katrien Beuls and Luc Steels. 2013. Agent-based
models of strategies for the emergence and evo-
lution of grammatical agreement. PLoS ONE,
8(3):e58960, 03.
Katrien Beuls, Remi van Trijp, and Pieter Wellens.
2012. Diagnostics and repairs in Fluid Construc-
tion Grammar. In Luc Steels and Manfred Hild, ed-
itors, Language Grounding in Robots. Springer Ver-
lag, Berlin.
Katrien Beuls. 2011. Construction sets and unmarked
forms: A case study for Hungarian verbal agree-
ment. In Luc Steels, editor, Design Patterns in Fluid
Construction Grammar, pages 237?264. John Ben-
jamins, Amsterdam.
Joris Bleys, Kevin Stadler, and Joachim De Beule.
2011. Search in linguistic processing. In Luc Steels,
editor, Design Patterns in Fluid Construction Gram-
mar, pages 149?179. John Benjamins, Amsterdam.
Mark Davies. 2011. N-grams and word frequency
data from the corpus of historical american english
(coha).
Kateryna Gerasymova, Michael Spranger, and Katrien
Beuls. 2012. A language strategy for aspect: En-
coding aktionsarten through morphology. In Luc
Steels, editor, Experiments in Cultural Language
Evolution, pages 257 ? 276. John Benjamins.
Bernd Heine and Tania Kuteva. 2007. The Genesis
of Grammar: A Reconstruction. Oxford University
Press, October.
Martin Loetzsch. 2012. Tools for grammar engineer-
ing. In Luc Steels, editor, Computational Issues
in Fluid Construction Grammar. Springer Verlag,
Berlin.
V. Micelli, R. van Trijp, and J. De Beule. 2009. Fram-
ing fluid construction grammar. In N.A. Taatgen and
H. van Rijn, editors, the 31th Annual Conference
of the Cognitive Science Society, pages 3023?3027.
Cognitive Science Society.
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38:39?41, November.
Simon Pauw and Joseph Hilferty. 2012. The emer-
gence of quantifiers. In Luc Steels, editor, Experi-
ments in Cultural Language Evolution, pages 277 ?
304. John Benjamins.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press, Chicago.
Michael Spranger and Luc Steels. 2012. Emergent
functional grammar for space. In Luc Steels, editor,
Experiments in Cultural Language Evolution, pages
207 ? 232. John Benjamins, Amsterdam.
Luc Steels and Remi van Trijp. 2011. How to make
construction grammars fluid and robust. In Luc
Steels, editor, Design Patterns in Fluid Construction
Grammar, pages 301?330. John Benjamins, Ams-
terdam.
Luc Steels, editor. 2011. Design Patterns in Fluid
Construction Grammar. John Benjamins.
Luc Steels, editor. 2012a. Computational Issues in
Fluid Construction Grammar, volume 7249 of Lec-
ture Notes in Computer Science. Springer, Berlin.
Luc Steels, editor. 2012b. Experiments in Cultural
Language Evolution. John Benjamins, Amsterdam.
Remi van Trijp. 2010. Grammaticalization and seman-
tic maps: Evidence from artificial language evolu-
tion. Linguistic Discovery, 8:310?326.
Remi van Trijp. 2012a. Not as awful as it seems : Ex-
plaining german case through computational exper-
iments in fluid construction grammar. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 829?839.
Remi van Trijp. 2012b. A reflective architecture for
language processing and learning. In Luc Steels,
editor, Computational Issues in Fluid Construction
Grammar. Springer Verlag, Berlin.
Remi van Trijp. 2013. Linguistic assessment crite-
ria for explaining language change: A case study on
syncretism in German definite articles. Language
Dynamics and Change, 3(1).
Pieter Wellens and Joachim De Beule. 2010. Priming
through constructional dependencies: a case study
in fluid construction grammar. In The Evolution
of Language ( EVOLANG8), pages 344?351. World
Scientific.
Pieter Wellens and Martin Loetzsch. 2012. Multi-
dimensional meanings in lexicon formation. In Luc
Steels, editor, Experiments in Cultural Language
Evolution, pages 143?166. John Benjamins, Ams-
terdam.
Pieter Wellens. 2011. Organizing constructions in net-
works. In Luc Steels, editor, Design Patterns in
Fluid Construction Grammar, pages 181?201. John
Benjamins, Amsterdam.
Lin Yuri, Michel Jean-Baptiste, Lieberman Aiden Erez,
Orwant Jon, Brockman Will, and Slav Petrov. 2012.
Syntactic annotations for the google books ngram
corpus. In ACL (System Demonstrations). The As-
sociation for Computer Linguistics.
132
