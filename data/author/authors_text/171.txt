Learning Dependency Translation Models 
as Collections of Finite-State Head 
Transducers 
Hiyan Alshawi* 
Shannon Laboratory, AT&T Labs 
Shona Douglas* 
Shannon Laboratory, AT&T Labs 
Srinivas Bangalore* 
Shannon Laboratory, AT&T Labs 
The paper defines weighted head transducers,finite-state machines that perform middle-out string 
transduction. These transducers are strictly more expressive than the special case of standard left- 
to-right finite-state transducers. Dependency transduction models are then defined as collections 
of weighted head transducers that are applied hierarchically. A dynamic programming search 
algorithm is described for finding the optimal transduction of an input string with respect to a 
dependency transduction model. A method for automatically training a dependency transduc- 
tion model from a set of input-output example strings is presented. The method first searches 
for hierarchical alignments of the training examples guided by correlation statistics, and then 
constructs the transitions of head transducers that are consistent with these alignments. Experi- 
mental results are given for applying the training method to translation from English to Spanish 
and Japanese. 
1. Introduction 
We will define a dependency transduction model in terms of a collection of weighted 
head transducers. Each head transducer is a finite-state machine that differs from 
"standard" finite-state transducers in that, instead of consuming the input string left 
to right, it consumes it "middle out" from a symbol in the string. Similarly, the output 
of a head transducer is built up middle out at positions relative to a symbol in the 
output string. The resulting finite-state machines are more expressive than standard 
left-to-right transducers. In particular, they allow long-distance movement with fewer 
states than a traditional finite-state ransducer, a useful property for the translation task 
to which we apply them in this paper. (In fact, finite-state head transducers are capable 
of unbounded movement with a finite number of states.) In Section 2, we introduce 
head transducers and explain how input-output positions on state transitions result 
in middle-out transduction. 
When applied to the problem of translation, the head transducers forming the de- 
pendency transduction model operate on input and output strings that are sequences 
of dependents of corresponding headwords in the source and target languages. The 
dependency transduction model produces ynchronized dependency trees in which 
each local tree is produced by a head transducer. In other words, the dependency 
* 180 Park Avenue, Florham Park, NJ 07932 
t 180 Park Avenue, Florham Park, NJ 07932 
180 Park Avenue, Florham Park, NJ 07932 
@ 2000 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 1 
model applies the head transducers ecursively, imposing a recursive decomposition 
of the source and target strings. A dynamic programming search algorithm finds op- 
timal (lowest total weight) derivations of target strings from input strings or word 
lattices produced by a speech recognizer. Section 3 defines dependency transduction 
models and describes the search algorithm. 
We construct the dependency transduction models for translation automatically 
from a set of unannotated examples, each example comprising a source string and a 
corresponding target string. The recursive decomposition of the training examples 
results from an algorithm for computing hierarchical alignments of the examples, 
described in Section 4.2. This alignment algorithm uses dynamic programming search 
guided by source-target word correlation statistics as described in Section 4.1. 
Having constructed a hierarchical alignment for the training examples, a set of 
head transducer t ansitions are constructed from each example as described in Sec- 
tion 4.3. Finally, the dependency transduction model is constructed by aggregating the 
resulting head transducers and assigning transition weights, which are log probabili- 
ties computed from the training counts by simple maximum likelihood estimation. 
We have applied this method of training statistical dependency transduction mod- 
els in experiments on English-to-Spanish and English-to-Japanese translations of tran- 
scribed spoken utterances. The results of these experiments are described in Section 5; 
our concluding remarks are in Section 6. 
2. Head Transducers 
2.1 Weighted Finite-State Head Transducers 
In this section we describe the basic structure and operation of a weighted head trans- 
ducer. In some respects, this description is simpler than earlier presentations (e.g., 
Alshawi 1996); for example, here final states are simply a subset of the transducer 
states whereas in other work we have described the more general case in which final 
states are specified by a probability distribution. The simplified escription is adequate 
for the purposes of this paper. 
Formally, aweighted head transducer is a 5-tuple: an alphabet W of input symbols; 
an alphabet V of output symbols; a finite set Q of states q0 . . . . .  qs; a set of final states 
F c Q; and a finite set T of state transitions. A transition from state q to state q' has 
the form 
(q,q',w,v,o~,fl, cl
where w is a member of W or is the empty string c; v is a member of V or ?; the integer 
o~ is the input position; the integer fl is the output position; and the real number c is 
the weight or cost of the transition. A transition in which oz = 0 and fl = 0 is called a 
head transition. 
The interpretation f q, q', w, and v in transitions i similar to left-to-right transduc- 
ers, i.e., in transitioning from state q to state qt, the transducer "reads" input symbol 
w and "writes" output symbol v, and as usual if w (or v) is e then no read (respec- 
tively write) takes place for the transition. The difference lies in the interpretation f 
the read position c~ and the write position ft. To interpret the transition positions as 
transducer actions, we consider notional input and output apes divided into squares. 
On such a tape, one square is numbered 0,and the other squares are numbered 1,2 . . . .  
rightwards from square 0, and -1 , -2  . . . .  leftwards from square 0 (Figure 1). 
A transition with input position ~ and output position fl is interpreted as reading 
w from square c~ on the input tape and writing v to square fl of the output tape; if 
square fl is already occupied, then v is written to the next empty square to the left of 
46 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
<q, q' ,  w, v, a, fl,, c> 
@ o p -@ 
C 
I lw l  w0 I I 
-4 -3a,=--2-1 0 1 2 3 4 
-4 -3 -2 -1 0 1 2 ,0=3 4 
Figure 1 
Transition symbols and positions. 
fl if fl < 0, or to the right of fl if fl > 0, and similarly, if input was already read from 
position a, w is taken from the next unread square to the left of a if a < 0 or to the 
right of c~ if a ~ 0. 
The operation of a head transducer is nondeterministic. It starts by taking a head 
transition 
{q, q', w0, v0, 0, 0, c} 
where w0 is one of the symbols (not necessarily the leftmost) in the input string. (The 
valid initial states are therefore implicitly defined as those with an outgoing head 
transition.) w0 is considered to be at square 0 of the input tape and v0 is output at 
square 0 of the output tape. Further state transitions may then be taken until a final 
state in F is reached. For a derivation to be valid, it must read each symbol in the 
input string exactly once. At the end of a derivation, the output string is formed by 
taking the sequence of symbols on the target ape, ignoring any empty squares on this 
tape. 
The cost of a derivation of an input string to an output string by a weighted 
head transducer is the sum of the costs of transitions taken in the derivation. We can 
now define the string-to-string transduction function for a head transducer to be the 
function that maps an input string to the output string produced by the lowest-cost 
valid derivation taken over all initial states and initial symbols. (Formally, the function 
is partial in that it is not defined on an input when there are no derivations or when 
there are multiple outputs with the same minimal cost.) 
In the transducers produced by the training method described in this paper, the 
source and target positions are in the set {-1,0,1},  though we have also used hand- 
coded transducers (Alshawi and Xia 1997) and automatically trained transducers (A1- 
shawl and Douglas 2000) with a larger range of positions. 
2.2 Relationship to Standard FSTs 
The operation of a traditional eft-to-right ransducer can be simulated by a head 
transducer by starting at the leftmost input symbol and setting the positions of the 
first transition taken to a = 0 and fl = 0, and the positions for subsequent transitions 
to o~ = 1 and fl = 1. However, we can illustrate the fact that head transducers are more 
47 
Computational Linguistics Volume 26, Number 1 
a:a  
a:a ~ b:b 
0:0 
Figure 2 
Head transducer to reverse an input string of arbitrary length in the alphabet {a, b}. 
expressive than left-to-right transducers by the case of a finite-state head transducer 
that reverses a string of arbitrary length. (This cannot be performed by a traditional 
transducer with a finite number of states.) 
For example, the head transducer described below (and shown in Figure 2) with 
input alphabet {a, b} will reverse an input string of arbitrary length in that alphabet. 
The states of the example transducer are Q = {ql, q2} and F = {q2}, and it has the 
following transitions (costs are ignored here): 
{ql, q2,a,a,O,O} 
<ql, q2, b, b, 0, 0> 
<q2,q2,a,a,-1,1} 
(q2, q2, b, b, -1,1} 
The only possible complete derivations of the transducer read the input string right 
to left, but write it left to right, thus reversing the string. 
Another similar example is using a finite-state head transducer to convert a palin- 
drome of arbitrary length into one of its component halves. This clearly requires the 
use of an empty string on some of the output transitions. 
3. Dependency Transduction Models 
3.1 Dependency Transduction using Head Transducers 
In this section we describe dependency transduction models, which can be used for 
machine translation and other transduction tasks. These models consist of a collection 
of head transducers that are applied hierarchically. Applying the machines hierarchi- 
cally means that a nonhead transition is interpreted not simply as reading an input- 
output pair (w, v), but instead as reading and writing a pair of strings headed by (w, v) 
according to the derivation of a subnetwork. 
For example, the head transducer shown in Figure 3 can be applied recursively in 
order to convert an arithmetic expression from infix to prefix (Polish) notation (as noted 
by Lewis and Stearns \[1968\], this transduction cannot be performed by a pushdown 
transducer). 
In the case of machine translation, the transducers derive pairs of dependency 
trees, a source language dependency tree and a target dependency tree. A dependency 
tree for a sentence, in the sense of dependency grammar (for example Hays \[1964\] and 
Hudson \[1984\]), is a tree in which the words of the sentence appear as nodes (we do 
not have terminal symbols of the kind used in phrase structure grammar). In such a 
tree, the parent of a node is its head and the child of a node is the node's dependent. 
The source and target dependency trees derived by a dependency transduction 
model are ordered, i.e., there is an ordering on the nodes of each local tree. This 
48 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
b b ~C~ b:b b:b 
Figure 3 
Dependency transduction network mapping bracketed arithmetic expressions from infix to 
prefix notation. 
I I want to make a collect call I 
? , 
\[ quiero hac~ una llamada de cobr~ I 
Figure 4 
Synchronized dependency trees derived for transducing I want to make a collect call into quiero 
hacer una llamada de cobrar. 
means, in particular, that the target sentence can be constructed directly by a simple 
recursive traversal of the target dependency tree. Each pair of source and target rees 
generated is synchronized in the sense to be formalized in Section 4.2. An example is 
given in Figure 4. 
Head transducers and dependency transduction models are thus related as fol- 
lows: Each pair of local trees produced by a dependency transduction derivation is the 
result of a head transducer derivation. Specifically, the input to such a head transducer 
is the string corresponding to the flattened local source dependency tree. Similarly, the 
output of the head transducer derivation is the string corresponding to the flattened 
local target dependency tree. In other words, the head transducer is used to convert 
a sequence consisting of a headword w and its left and right dependent words to a 
sequence consisting of a target word v and its left and right dependent words (Fig- 
ure 5). Since the empty string may appear in a transition in place of a source or target 
symbol, the number of source and target dependents can be different. 
The cost of a derivation produced by a dependency transduction model is the 
sum of all the weights of the head transducer derivations involved. When applying a 
dependency transduction model to language translation, we choose the target string 
obtained by flattening the target ree of the lowest-cost dependency derivation that 
also generates the source string. 
We have not yet indicated what weights to use for head transducer t ansitions. 
The definition of head transducers as such does not constrain these. However, for a 
dependency transduction model to be a statistical model for generating pairs of strings, 
we assign transition weights that are derived from conditional probabilities. Several 
49 
Computational Linguistics Volume 26, Number 1 
Iw1 ..- wk.ll ~?1 ..-'~nl 
Iv ,  
Figure 5 
Head transducer converts the sequences of left and right dependents (wl ... wk-l/ and 
(wk+i ? ? ? w,) of w into left and right dependents (vl... vj-1) and {Vj+I... Vp) of v. 
probabilistic parameterizations can be used for this purpose including the following 
for a transition with headwords w and v and dependent words w' and v': 
P(q', w', v', fllw, v, q). 
Here q and q' are the from-state and to-state for the transition and a and fl are the 
source and target positions, as before. We also need parameters P(q0, ql\]w, v) for the 
probability of choosing a head transition 
(qo, ql, w,v,O,O) 
given this pair of headwords. To start the derivation, we need parameters 
P(roots(wo, vo)) for the probability of choosing w0,v0 as the root nodes of the two 
trees. 
These model parameters can be used to generate pairs of synchronized epen- 
dency trees starting with the topmost nodes of the two trees and proceeding recur- 
sively to the leaves. The probability of such a derivation can be expressed as: 
P( oots(wo, vo) )P(Dwo,vo) 
where P(Dw,v) is the probability of a subderivation headed by w and v, that is 
P(Dw,v) = P(qo, qllw, v) H P(qi+l, Wi, Vi,~i, fli\]w,v, qi)P(Dwi,vl) 
1K i ln  
for a derivation in which the dependents of w and v are generated by n transitions. 
3.2 Transduction Algorithm 
To carry out translation with a dependency transduction model, we apply a dynamic 
programming search to find the optimal derivation. This algorithm can take as input 
either word strings, or word lattices produced by a speech recognizer. The algorithm 
is similar to those for context-free parsing such as chart parsing (Earley 1970) and 
the CKY algorithm (Younger 1967). Since word string input is a special case of word 
lattice input, we need only describe the case of lattices. 
We now present a sketch of the transduction algorithm. The algorithm works 
bottom-up, maintaining a set of configurations. A configuration has the form 
In1, n2, w, v, q, c, t\] 
corresponding to a bottom-up artial derivation currently in state q covering an input 
sequence between nodes nl and n2 of the input lattice, w and v are the topmost 
50 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
nodes in the source and target derivation trees. Only the target ree t is stored in the 
configuration. 
The algorithm first initializes configurations for the input words, and then per- 
forms transitions and optimizations to develop the set of configurations bottom-up: 
Initialization: For each word edge between odes n and n ~ in the lattice 
with source word w0, an initial configuration is constructed for any head 
transition of the form 
(q, q', w0, v0, 0, 0, c} 
Such an initial configuration has the form: 
\[n, n t, w0, v0, q~, c, v0\] 
Transition: We show the case of a transition in which a new configuration 
results from consuming a source dependent wl to the left of a headword 
w and adding the corresponding target dependent Vl to the right of the 
target head v. Other cases are similar. The transition applied is: 
(q, q~, Wl, Vl, -1,1, c'} 
It is applicable when there are the following head and dependent 
configurations: 
\[n2,n3,w,v,q,c,t\] 
\[nl, n2, Wl, Vl, qf, Cl, tl\] 
where the dependent configuration is in a final state qf. The result of 
applying the transition is to add the following to the set of 
configurations: 
In1, n3, w, v, q', c + Cl q- C', t'\] 
where Y is the target dependency tree formed by adding tl as the 
rightmost dependent of t. 
Optimization: We also require a dynamic programming condition to 
remove suboptimal (sub)derivations. Whenever there are two 
configurations 
\[n, n', w, v, q, Cl, tl\] 
\[n, n', w, v, q, C2, t2\] 
and c2 > Cl, the second configuration is removed from the set of 
configurations. 
If, after all applicable transitions have been taken, there are configurations span- 
ning the entire input lattice, then the one with the lowest cost is the optimal derivation. 
When there are no such configurations, we take a pragmatic approach in the trans- 
lation application and simply concatenate the lowest costing of the minimal length 
sequences of partial derivations that span the entire lattice. A Viterbi-like search of 
the graph formed by configurations i used to find the optimal sequence of deriva- 
tions. One of the advantages ofmiddle-out transduction is that robustness i improved 
through such use of partial derivations when no complete derivations are available. 
51 
Computational Linguistics Volume 26, Number 1 
4. Training Method 
Our training method for head transducer models only requires a set of training exam- 
ples. Each example, or bitext, consists of a source language string paired with a target 
language string. In our experiments, the bitexts are transcriptions of spoken English 
utterances paired with their translations into Spanish or Japanese. 
It is worth emphasizing that we do not necessarily expect he dependency repre- 
sentations produced by the training method to be traditional dependency structures 
for the two languages. Instead, the aim is to produce bilingual (i.e., synchronized, see 
below) dependency representations that are appropriate to performing the translation 
task for a specific language pair or specific bilingual corpus. For example, headwords 
in both languages are chosen to force a synchronized alignment (for better or worse) 
in order to simplify cases involving so-called head-switching. This contrasts with one 
of the traditional approaches (e.g., Dorr 1994; Watanabe 1995) to posing the transla- 
tion problem, i.e., the approach in which translation problems are seen in terms of 
bridging the gap between the most natural monolingual representations underlying 
the sentences of each language. 
The training method has four stages: (i) Compute co-occurrence statistics from the 
training data. (ii) Search for an optimal synchronized hierarchical alignment for each 
bitext. (iii) Construct a set of head transducers that can generate these alignments with 
transition weights derived from maximum likelihood estimation. 
4.1 Computing Pairing Costs 
For each source word w in the data set, assign a cost, the translation pairing cost 
c(w, v) for all possible translations v into the target language. These translations of the 
source word may be zero, one, or several target language words (see Section 4.4 for 
discussion of the multiword case). The assignment of translation pairing costs (effec- 
tively a statistical bilingual dictionary) may be done using various statistical measures. 
For this purpose, a suitable statistical function needs to indicate the strength of co- 
occurrence correlation between source and target words, which we assume is indicative 
of carrying the same semantic ontent. Our preferred choice of statistical measure for 
assigning the costs is the ~ correlation measure (Gale and Church 1991). We apply 
this statistic to co-occurrence of the source word with all its possible translations in 
the data set examples. We have found that, at least for our data, this measure leads to 
better performance than the use of the log probabilities of target words given source 
words (cf. Brown et al 1993). 
In addition to the correlation measure, the cost for a pairing includes a distance 
measure component that penalizes pairings proportionately to the difference between 
the (normalized) positions of the source and target words in their respective sentences. 
4.2 Computing Hierarchical Alignments 
As noted earlier, dependency transduction models are generative probabilistic models; 
each derivation generates a pair of dependency trees. Such a pair can be represented 
as a synchronized hierarchical alignment of two strings. A hierarchical alignment 
consists of four functions. The first two functions are an alignment mapping f from 
source words w to target words f(w) (which may be the empty string ~), and an 
inverse alignment mapping from target words v to source words fr(v). The inverse 
mapping is needed to handle mapping of target words to ~; it coincides wi thf  for pairs 
without source ~. The other two functions are a source head-map g mapping source 
dependent words w to their heads g(w) in the source string, and a target head-map 
h mapping target dependent words v to their headwords h(v) in the target string. An 
52 
Alshawi, Bangalore, and Douglas Leaning Dependency Translation Models 
g 
show me nonstop flights to boston 
muestreme los vuelos sin escalas a boston 
g 
show me z ' \  
muestr~me 
nonstop flights to boston 
los vuelos sin escalas a boston 
Figure 6 
A hierarchical alignment: alignment mappings f and f', and head-maps g and h. 
example hierarchical alignment is shown in Figure 6 (f and f '  are shown separately 
for clarity). 
A hierarchical alignment is synchronized (i.e., it corresponds to synchronized e- 
pendency trees) if these conditions hold: 
Nonover lap :  If wl # w2, thenf(wl) f(w2), and similarly, if Vl  V2, thenf'(vl) # 
d'(v2). 
Synchron izat ion :  if f (w) = v and v # e, then f(g(w)) = h(v), and f'(v) = w. 
Similarly, ifd'(v) = w and w # e, thend'(h(v)) = g(w), andf(w) = v. 
Phrase  cont igu i ty :  The image under f of the maximal substring dominated by a 
headword w is a contiguous egment of the target string. 
(Here w and v refer to word tokens not symbols (types). We hope that the context of 
discussion will make the type-token distinction clear in the rest of this article.) The 
hierarchical alignment in Figure 6 is synchronized. 
Of course, translations of phrases are not always transparently related by a hier- 
archical alignment. In cases where the mapping between a source and target phrase is 
unclear (for example, one of the phrases might be an idiom), then the most reasonable 
choice of hierarchical alignment may be for f and f '  to link the heads of the phrases 
only, all the other words being mapped to e, with no constraints on the monolingual 
head mappings h and g. (This is the approach we take to compound lexical pairings, 
discussed in Section 4.4.) 
In the hierarchical alignments produced by the training method described here, 
the source and target strings of a bitext are decomposed into three aligned regions, 
as shown in Figure 7: a head region consisting of headword w in the source and its 
corresponding targetf(w) in the target string, a left substring region consisting of the 
source substring to the left of w and its projection under f on the target string, and 
a right substring region consisting of the source substring to the right of w and its 
projection under f  on the target string. The decomposition is recursive in that the left 
substring region is decomposed around a left headword wl, and the right substring 
53 
Computational Linguistics Volume 26, Number 1 
\[ 
Figure 7 
Decomposing source and target strings around heads w and f(w). 
region is decomposed around a right headword Wr. This process of decomposition 
continues for each left and right substring until it only contains a single word. 
For each bitext there are, in general, multiple such recursive decompositions that 
satisfy the synchronization constraints for hierarchical alignments. We wish to find 
such an alignment hat respects the co-occurrence statistics of bitexts as well as the 
phrasal structure implicit in the source and target strings. For this purpose we define 
a cost function on hierarchical alignments. The cost function is the sum of three terms. 
The first term is the total of all the translation pairing costs c(w,f(w)) of each source 
word w and its translation f(w) in the alignment; the second term is proportional to 
the distance in the source string between dependents wd and their heads g(wa); and the 
third term is proportional to the distance in the target string between target dependent 
words va and their heads h(va). 
The hierarchical alignment hat minimizes this cost function is computed using 
a dynamic programming procedure. In this procedure, the pairing costs are first re- 
trieved for each possible source-target pair allowed by the example. Adjacent source 
substrings are then combined to determine the lowest-cost subalignments for suc- 
cessively larger substrings of the bitext satisfying the constraints tated above. The 
successively larger substrings eventually span the entire source string, yielding the 
optimal hierarchical alignment for the bitext. This procedure has O(n 6) complexity 
in the number of words in the source (or target) sentence. In Alshawi and Douglas 
(2000) we describe a version of the alignment algorithm in which heads may have 
an arbitrary number of dependents, and in which the hierarchical alignments for the 
training corpus are refined by iterative reestimation. 
4.3 Constructing Transducers 
Building a head transducer involves creating appropriate head transducer states and 
tracing hypothesized head transducer transitions between them that are consistent 
with the hierarchical alignment of a bitext. 
The main transitions that are traced in our construction are those that map heads, 
wl and Wr, of the right and left dependent phrases of w to their translations as indi- 
cated by the alignment function f in the hierarchical alignment. The positions of the 
dependents in the target string are computed by comparing the positions off(wt) and 
f(Wr) to the position of v = f(w). 
In order to generalize from instances in the training data, some model states aris- 
ing from different raining instances are shared. In particular, in the construction de- 
scribed here, for a given pair (w, v) there is only one final state. (We have also tried 
using automatic word-clustering techniques to merge states further, but for the lim- 
ited domain corpora we have used so far, the results are inconclusive.) To specify 
54 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
? \oo 
Figure 8 
+1 :+1 -1 :+ 1 
States and transitions constructed for the "swapping" decomposition shown in Figure 7. 
the sharing of states we make use of a one-to-one state-naming function ? from se- 
quences of strings to transducer states. The same state-naming function is used for 
all examples in the data set, ensuring that the transducer fragments recorded for 
the entire data set will form a complete collection of head transducer transition et- 
works. 
Figure 7 shows a decomposition i which w has a dependent to either side, v 
has both dependents to the right, and the alignment is "swapping" (f(wl) is to the 
right off(wr)). The construction for this decomposition case is illustrated in Figure 8 
as part of a finite-state transition diagram, and described in more detail below. (The 
other transition arrows shown in the diagram will arise from other bitext alignments 
containing (w,f(w)) pairings.) Other cases covered by our algorithm (e.g., a single left 
source dependent but no right source dependent, or target dependents on either side 
of the target head) are simple variants. 
The detailed construction is as follows: 
1. Construct a transition from sl = ?(initial) to S 2 = O ' (w, f (w) ,  head) mapping 
the source headword w to the target head f(w) at position 0 in source 
and target. (In our training construction there is only one initial state sl.) 
2. Since the target dependentf(wr) is to the left of target dependentf(wl) 
(and we are restricting positions to {-1, 0, +1}) the Wr transition is 
constructed first in order that the target dependent nearest he head is 
output first. 
Construct a transition from s2 to s3 = c~(w,f(w), swapping, Wr,f(Wr) 
mapping the source dependent Wr at position +1 to the target dependent 
f(Wr) at position +1. 
3. Construct a transition from s3 to s4 = cr(w,f(w),final) mapping the source 
dependent wl at position -1 to the target dependentf(wl) at position +1. 
If instead the alignment had been as in Figure 9, in which the source dependents 
are mapped to target dependents in a parallel rather than swapping configuration 
(the configuration of sin escalas and Boston around flights:los vuelos in Figure 6), the 
construction is the same, except for the following differences: 
. 
. 
Since the target dependentf(wl) is to the left of target dependentf(Wr), 
the wl transition is constructed first in order that the target dependent 
nearest he head is output first. 
The source and target positions are as shown in Figure 10. Instead of 
s ta te  s3, we use a different state ss = ?(w,f(w),parallel, wl,f(wl)). 
55 
Computational Linguistics Volume 26, Number 1 
\[ "'" l \ [ \ ] \ [  ..-4..- \] 
j J  
Figure 9 
Decomposing source and target strings around heads w and f(w)--"parallel'. 
w :f(w) ?\oo 
Figure 10 
-1 :+1 
w / f (w , ) 
+1 :+ 1 
States and transitions constructed for the "parallel" decomposition shown in Figure 9. 
Other states are the same as for the first case. The resulting states and transitions are 
shown in Figure 10. 
After the construction described above is applied to the entire set of aligned bi- 
texts in the training set, the counts for transitions are treated as event observation 
counts of a statistical dependency transduction model with the parameters described 
in Section 3.1. More specifically, the negated logs of these parameters are used as the 
weights for transducer t ansitions. 
4.4 Mult iword Pairings 
In the translation application, source word w and target word v are generalized so 
they can be short substrings (compounds) of the source and target strings. Exam- 
ples of such multiword pairs are show me:muestrdme and nonstop:sin escalas in Fig- 
ure 6. The cost for such pairings still uses the same ~ statistic, now taking the ob- 
servations to be the co-occurrences of the substrings in the training bitexts. How- 
ever, in order that these costs can be comparable to the costs for simple pairings, 
they are multiplied by the number of words in the source substring of the pair- 
ing. 
The use of compounds in pairings does not require any fundamental changes to 
the hierarchical lignment dynamic programming algorithm, which simply produces 
dependency trees with nodes that may be compounds. In the transducer construction 
phase of the training method, one of the words of a compound is taken to be the pri- 
mary or "real" headword. (In fact, we take the least common word of a compound to 
be its head.) An extra chain of transitions i constructed to transduce the other words 
of compounds, if necessary using transitions with epsilon strings. This compilation 
means that the transduction algorithm is unaffected by the use of compounds when 
aligning training data, and there is no need for a separate compound identification 
phase when the transduction algorithm is applied to test data. Some results for dif- 
ferent choices of substring lengths can be found in Alshawi, Bangalore, and Douglas 
(1998). 
56 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
5. Experiments 
5.1 Evaluation Method 
In order to reduce the time required to carry out training evaluation experiments, 
we have chosen two simple, string-based evaluation metrics that can be calculated 
automatically. These metrics, simple accuracy and translation accuracy, are used to 
compare the target string produced by the system against a reference human transla- 
tion from held-out data. 
Simple accuracy is computed by first finding a transformation f one string into 
another that minimizes the total weight of insertions, deletions, and substitutions. (We 
use the same weights for these operations as in the NIST ASR evaluation software 
\[National Institute of Standards and Technology 1997\].) Translation accuracy includes 
transpositions (i.e., movement) of words as well as insertions, deletions, and substi- 
tutions. We regard the latter metric as more appropriate for evaluation of translation 
systems because the simple metric would count a transposition as two errors: an in- 
sertion plus a deletion. (This issue does not arise for speech recognizers because these 
systems do not normally make transposition errors.) 
For the lowest edit-distance transformation between the reference translation and 
system output, if we write I for the number of insertions, D for deletions, S for substi- 
tutions, and R for number of words in the reference translation string, we can express 
simple accuracy as 
simple accuracy = 1 - ( I  + D + S) /R .  
Similarly, if T is the number of transpositions in the lowest weight transformation 
including transpositions, we can express translation accuracy as 
translation accuracy = 1 - ( I  ~ + D ~ + S + T) /R .  
Since a transposition corresponds toan insertion and a deletion, the values of I ~ and D ~ 
for translation accuracy will, in general, be different from I and D in the computation of
simple accuracy. For Spanish, the units for string operations in the evaluation metrics 
are words, whereas for Japanese they are Japanese characters. 
5.2 English-to-Spanish 
The training and test data for the English-to-Spanish experiments were taken from 
a set of transcribed utterances from the Air Travel Information System (ATIS) corpus 
together with a translation of each utterance to Spanish. An utterance is typically asin- 
gle sentence but is sometimes more than one sentence spoken in sequence. Alignment 
search and transduction training was carried out only on bitexts with sentences up 
to length 20, a total of 13,966 training bitexts. The test set consisted of 1,185 held-out 
bitexts at all lengths. Table 1 shows the word accuracy percentages ( ee Section 5.1) 
for the trained model, e2s, against he original held-out ranslations at various source 
sentence l ngths. Scores are also given for a "word-for-word" baseline, sww, in which 
each English word is translated by the most highly correlated Spanish word. 
5.3 English-to-Japanese 
The training and test data for the English-to-Japanese experiments was a set of tran- 
scribed utterances of telephone service customers talking to AT&T operators. These 
utterances, collected from real customer-operator interactions, tend to include frag- 
mented language, restarts, etc. Both training and test partitions were restricted to bi- 
texts with at most 20 English words, giving 12,226 training bitexts and 3,253 held-out 
test bitexts. In the Japanese text, we introduce "word" boundaries that are convenient 
57 
Computational Linguistics Volume 26, Number 1 
Table 1 
Simple accuracy/translation accuracy (percent) for the trained 
English-to-Spanish model (e2s) against he word-for-word baseline 
(sww). 
Length < 5 < 10 G 15 < 20 All 
sww 45.1/45.8 46.7/48.6 46.5/48.2 45.5/47.1 45.2/46.9 
e2s 75.4/75.8 76.3/78.0 75.4/77.0 74.4/76.0 73.3/75.0 
Table 2 
Simple accuracy/translation accuracy as percentages of Japanese 
characters, for the trained English-to-Japanese model (e2j) and the 
word-for-word baseline (jww). 
Length G 5 < 10 G 15 ~ 20 All 
jww 75.8/78.0 45.2/50.4 40.0/45.4 37.2/42.8 37.2/42.8 
e2j 89.2/89.7 74.0/76.6 68.6/72.2 66.4/70.1 66.4/70.1 
for the training process. These word boundaries are parasitic on the word boundaries 
in the English transcriptions: the translators are asked to insert such a word boundary 
between any two Japanese characters that are taken to have arisen from the translation 
of distinct English words. This results in bitexts in which the number of multichar- 
acter Japanese "words" is at most the number of English words. However, as noted 
above, evaluation of the Japanese output is done with Japanese characters, i.e., with 
the Japanese text in its natural format. Table 2 shows the Japanese character accuracy 
percentages for the trained English-to-Japanese model, e2j, and a baseline model, jww, 
which gives each English word its most highly correlated translation. 
5.4 Note on Experimental Setting 
The vocabularies in these English-Spanish and English-Japanese experiments are only 
a few thousand words; the utterances are fairly short (an average of 7.3 words per utter- 
ance) and often contain errors typical of spoken language. So while the domains may 
be representative of task-oriented ialogue settings, further experimentation would 
be needed to assess the effectiveness of our method in situations uch as translat- 
ing newspaper articles. In terms of the training data required, Tsukada et al (1999) 
provide indirect empirical evidence suggesting accuracy can be further improved by 
increasing the size of our training sets, though also suggesting that the learning curve 
is relatively shallow beyond the current size of corpus. 
6. Concluding Remarks 
Formalisms for finite-state and context-free transduction have a long history (e.g., 
Lewis and Stearns 1968; Aho and Ullman 1972), and such formalisms have been ap- 
plied to the machine translation problem, both in the finite-state case (e.g., Vilar et al 
1996) and the context-free case (e.g., Wu 1997). In this paper we have added to this 
line of research by providing a method for automatically constructing fully lexicalized 
statistical dependency transduction models from training examples. 
Automatically training a translation system brings important benefits in terms of 
maintainability, robustness, and reducing expert coding effort as compared with tra- 
58 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
ditional rule-based translation systems (a number of which are described in Hutchins 
and Somers \[1992\]). The reduction of effort results, in large part, from being able 
to do without artificial intermediate representations of meaning; we do not require 
the development of semantic mapping rules (or indeed any rules) or the creation of 
a corpus including semantic annotations. Compared with left-to-right ransduction, 
middle-out ransduction also aids robustness because, when complete derivations are 
not available, partial derivations tend to have meaningful headwords. 
At the same time, we believe our method has advantages over the approach de- 
veloped initially at IBM (Brown et al 1990; Brown et al 1993) for training translation 
systems automatically. One advantage is that our method attempts to model the nat- 
ural decomposition of sentences into phrases. Another is that the compilation of this 
decomposition i to lexically anchored finite-state head transducers produces imple- 
mentations that are much more efficient han those for the IBM model. In particular, 
our search algorithm finds optimal transductions of test sentences in less than "real 
time" on a 300MHz processor, that is, the time to translate an utterance is less than 
the time taken to speak it, an important consideration for our speech translation ap- 
plication. 
References 
Aho, Alfred V. and Jeffrey D. Ullman. 1972. 
The Theory o/Parsing, Translation, and 
Compiling. Prentice-Hall, Englewood 
Cliffs, NJ. 
Alshawi, H. 1996. Head automata for 
speech translation. In Proceedings ofthe 
International Conference on Spoken Language 
Processing, pages 2360-2364, Philadelphia, 
PA. 
Alshawi, H., S. Bangalore, and S. Douglas. 
1998. Learning phrase-based head 
transduction models for translation of 
spoken utterances. In Proceedings ofthe 
International Conference on Spoken Language 
Processing, pages 2767-2770, Sydney, 
Australia. 
Alshawi, H. and S. Douglas. 2000. Learning 
dependency transduction models from 
unannotated examples. Philosophical 
Transactions ofthe Royal Society (Series A: 
Mathematical, Physical and Engineering 
Sciences). To appear. 
Alshawi, Hiyan and Fei Xia. 1997. 
English-to-Mandarin speech translation 
with head transducers. In Proceedings of
the Workshop on Spoken Language 
Translation, Madrid, Spain. 
Brown, P. J., J. Cocke, S. A. Della Pietra, 
V. J. Della Pietra, J. Lafferty, R. L. Mercer, 
and P. Rossin. 1990. A statistical approach 
to machine translation. Computational 
Linguistics, 16(2):79-85. 
Brown, P. J., S. A. Della Pietra, V. J. Della 
Pietra, and R. L. Mercer. 1993. The 
mathematics ofmachine translation: 
Parameter estimation. Computational 
Linguistics, 16(2):263-312. 
Dorr, B. J. 1994. Machine translation 
divergences: A formal description and 
proposed solution. Computational 
Linguistics, 20(4):597-634. 
Earley, J. 1970. An efficient context-free 
parsing algorithm. Communications of the 
ACM, 13(2):94-102. 
Gale, W. A. and K. W. Church. 1991. 
Identifying word correspondences in 
parallel texts. In Proceedings ofthe Fourth 
DARPA Speech and Natural Language 
Processing Workshop, ages 152-157, Pacific 
Grove, CA. 
Hays, D. G. 1964. Dependency theory: A 
formalism and some observations. 
Language, 40:511-525. 
Hudson, R. A. 1984. Word Grammar. 
Blackwell, Oxford. 
Hutchins, W. J. and H. L. Somers. 1992. An 
Introduction to Machine Translation. 
Academic Press, New York. 
Lewis, P. M. and R. E. Stearns. 1968. 
Syntax-directed transduction. Journal of the 
Association for Computing Machinery, 
15(3):465-488. 
National Institute of Standards and 
Technology. 1997. Spoken Natural 
Language Processing Group Web page. 
http://www.itl.nist.gov/div894. 
Tsukada, Hajime, Hiyan Alshawi, Shona 
Douglas, and Srinivas Bangalore. 1999. 
Evaluation of machine translation system 
based on a statistical method by using 
spontaneous speech transcription. In
Proceedings ofthe Fall Meeting of the 
Acoustical Society of Japan, pages 115-116, 
September. 
Vilar, J. M., V. M. Jim~nez, J. C. Amengual, 
A. Castellanos, D. Llorens, and E. Vidal. 
1996. Text and speech translation by 
59 
Computational Linguistics Volume 26, Number 1 
means of subsequential transducers. 
Natural Language Engineering, 2(4):351-354. 
Watanabe, Hideo. 1995. A model of a 
bi-directional transfer mechanism using 
rule combination. Machine Translation, 
10(4):269-291. 
Wu, Dekai.1997. Stochastic inversion 
transduction grammars and bilingual 
parsing of parallel corpora. Computational 
Linguistics, 23(3):377-404. 
Younger, D. 1967. Recognition and Parsing 
of Context-Free Languages in Time n 3. 
Information and Control, 10:189-208. 
60 
Effective Utterance Classification with Unsupervised Phonotactic Models
Hiyan Alshawi
AT&T Labs - Research
Florham Park, NJ 07932, USA
hiyan@research.att.com
Abstract
This paper describes a method for utterance
classification that does not require manual
transcription of training data. The method
combines domain independent acoustic models
with off-the-shelf classifiers to give utterance
classification performance that is surprisingly
close to what can be achieved using conven-
tional word-trigram recognition requiring man-
ual transcription. In our method, unsupervised
training is first used to train a phone n-gram
model for a particular domain; the output of
recognition with this model is then passed to
a phone-string classifier. The classification ac-
curacy of the method is evaluated on three dif-
ferent spoken language system domains.
1 Introduction
A major bottleneck in building data-driven speech pro-
cessing applications is the need to manually transcribe
training utterances into words. The resulting corpus of
transcribed word strings is then used to train application-
specific language models for speech recognition, and in
some cases also to train the natural language components
of the application. Some of these speech processing ap-
plications make use of utterance classification, for exam-
ple when assigning a call destination to naturally spoken
user utterances (Gorin et al, 1997; Carpenter and Chu-
Carroll, 1998), or as an initial step in converting speech
to actions in spoken interfaces (Alshawi and Douglas,
2001).
In this paper we present an approach to utterance clas-
sification that avoids the manual effort of transcribing
training utterances into word strings. Instead, only the
desired utterance class needs to be associated with each
sample utterance. The method combines automatic train-
ing of application-specific phonotactic models together
with token sequence classifiers. The accuracy of this
phone-string utterance classification method turns out to
be surprisingly close to what can be achieved by conven-
tional methods involving word-trigram language mod-
els that require manual transcription. To quantify this,
we present empirical accuracy results from three differ-
ent call-routing applications comparing our method with
conventional utterance classification using word-trigram
recognition.
Previous work at AT&T on utterance classification
without words used information theoretic metrics to dis-
cover ?acoustic morphemes? from untranscribed utter-
ances paired with routing destinations (Gorin et al, 1999;
Levit et al, 2001; Petrovska-Delacretaz et al, 2000).
However, that approach has so far proved impractical:
the major obstacle to practical utility was the low run-
time detection rate of acoustic morphemes discovered
during training. This led to a high false rejection rate (be-
tween 40% and 50% for 1-best recognition output) when
a word-based classification algorithm (the one described
by Wright et. al (1997)) was applied to the detected se-
quence of acoustic morphemes.
More generally, previous work using phone string (or
phone-lattice) recognition has concentrated on tasks in-
volving retrieval of audio or video (Jones et al, 1996;
Foote et al, 1997; Ng and Zue, 1998; Choi et al, 1999).
In those tasks, performance of phone-based systems was
not comparable to the accuracy obtainable from word-
based systems, but rather the rationale was avoiding the
difficulty of building wide coverage statistical language
models for handling the wide range of subject matter that
a typical retrieval system, such as a system for retrieving
news clips, needs to cover. In the work presented here, the
task is somewhat different: the system can automatically
learn to identify and act on relatively short phone subse-
quences that are specific to the speech in a limited domain
of discourse, resulting in task accuracy that is comparable
to word-based methods.
                                                               Edmonton, May-June 2003
                                                                 Main Papers , pp. 1-7
                                                         Proceedings of HLT-NAACL 2003
In section 2 we describe the utterance classification
method. Section 3 describes the experimental setup and
the data sets used in the experiments. Section 4 presents
the main comparison of the performance of the method
against a ?conventional? approach using manual tran-
scription and word-based models. Section 5 gives some
concluding remarks.
2 Utterance Classification Method
2.1 Runtime Operation
The runtime operation of our utterance classification
method is simple. It involves applying two models
(which are trained as described in the next subsection): A
statistical n-gram phonotactic model and a phone string
classification model. At runtime, the phonotactic model
is used by an automatic speech recognition system to con-
vert a new input utterance into a phone string which is
mapped to an output class by applying the classification
model. (We will often refer to an output class as an ?ac-
tion?, for example transfer to a specific call-routing des-
tination). The configuration at runtime is as shown in
Figure 1. More details about the specific recognizer and
classifier components used in our experiments are given
in the Section 3.
Newutterance
Phonen -gram
recognizer
Predictedaction,Confidencescore
Classifier
runtimeprediction
Phonestring
Prompt
Phonestring
classification
model
Phonen -gram
model Nmax
Figure 1: Utterance classifier runtime operation
The classifier can optionally make use of more infor-
mation about the context of an utterance to improve the
accuracy of mapping to actions. As noted in Figure 1,
in the experiments presented here, we use a single addi-
tional feature as a proxy for the utterance context, specif-
ically, the identity of the spoken prompt that elicited the
utterance. It should be noted, however, that inclusion of
such additional information is not central to the method:
Whether, and how much, context information to include
to improve classification accuracy will depend on the ap-
plication. Other candidate aspects of context may include
the dialog state, the day of week, the role of the speaker,
and so on.
2.2 Training Procedure
Training is divided into two phases. First, train a phone
n-gram model using only the training utterance speech
files and a domain-independent acoustic model. Second,
train a classification model mapping phone strings and
prompts (the classifier inputs) to actions (the classifier
outputs).
The recognition training phase is an iterative proce-
dure in which a phone n-gram model is refined succes-
sively: The phone strings resulting from the current pass
over the speech files are used to construct the phone n-
gram model for the next iteration. In other words, this
is a ?Viterbi re-estimation? or ?1-best re-estimation? pro-
cess. We currently only re-estimate the n-gram model, so
the same general-purpose HMM acoustic model is used
for ASR decoding in all iterations. Other more expen-
sive n-gram re-estimation methods can be used instead,
including ones in which successive n-gram models are
re-estimated from n-best or lattice ASR output. Candi-
dates for the initial model used in this procedure are an
unweighted phone loop or a general purpose phonotactic
model for the language being recognized.
The steps of the training process are as follows. (The
procedure is depicted in Figure 2.)
Training
utteranceaudiofiles
Phone-loop
recognizer Phonestrings0Phonen -gram
model1
Actions Phonestringclassification
model
sequence
classifiertraining
algorithm
Phonen -gram
recognizer PhonestringsNmax
Phonen -gram
model Nmax
? ?
Prompts
Figure 2: Utterance classifier training procedure
1. Set the phone string model G to an initial phone
string model. Initialize the n-gram order N to 1.
(Here ?order? means the size of the n-grams, so for
example 2 means bi-grams.)
2. Set S to the set of phone strings resulting from rec-
ognizing the training speech files with G (after pos-
sibly adjusting the insertion penalty, as explained
below).
3. Estimate an n-gram model G? of order N from the
set of strings S.
4. If N < Nmax, set N ? N + 1 and G? G? and go
to step 2, otherwise continue with step 5.
5. For each recognized string s ? S, construct a clas-
sifier input pair (s, r) where r is the prompt that
elicited the utterance recognized as s.
6. Train a classification model M to generalize the
training function f : (s, r) ? a, where a is the
action associated with the utterance recognized as s.
7. Return the classifier model M and the final n-gram
model G? as the results of the training procedure.
Instead of increasing the order N of the phone n-gram
model during re-estimation, an alternative would be to
iterate Nmax times with a fixed n-gram order, possibly
with successively increased weight being given to the lan-
guage model vs. the acoustic model in ASR decoding.
One issue that arises in the context of unsupervised
recognition without transcription is how to adjust recog-
nition parameters that affect the length of recognized
strings. In conventional training of recognizers from
word transcriptions, a ?word insertion penalty? is typ-
ically tuned after comparing recognizer output against
transcriptions. To address this issue, we estimate the ex-
pected speaking rate (in phones per second) for the rele-
vant type of speech (human-computer interaction in these
experiments). The token insertion penalty of the recog-
nizer is then adjusted so that the speaking rate for auto-
matically detected speech in a small sample of training
data approximates the expected speaking rate.
3 Experimental Setup
3.1 Data
Three collections of utterances from different domains
were used in the experiments. Domain A is the one stud-
ied in previously cited experiments (Gorin et al, 1999;
Levit et al, 2001; Petrovska-Delacretaz et al, 2000). Ut-
terances for domains B and C are from similar interactive
spoken natural language systems.
Domain A. The utterances being classified are the cus-
tomer side of live English conversations between AT&T
residential customers and an automated customer care
system. This system is open to the public so the num-
ber of speakers is large (several thousand). There were
40106 training utterances and 9724 test utterances. The
average length of an utterance was 11.29 words. The split
between training and test utterances was such that the ut-
terances from a particular call were either all in the train-
ing set or all in the test set. There were 56 actions in
this domain. Some utterances had more than one action
associated with them, the average number of actions as-
sociated with an utterance being 1.09.
Domain B. This is a database of utterances from an in-
teractive spoken language application relating to product
line information. There were 10470 training utterances
and 5005 test utterances. The average length of an utter-
ance was 3.95 words. There were 54 actions in this do-
main. Some utterances had more than one action associ-
ated with them, the average number of actions associated
with an utterance being 1.23.
Domain C. This is a database of utterances from an
interactive spoken language application relating to con-
sumer order transactions (reviewing order status, etc.) in
a limited domain. There were 14355 training utterances
and 5000 test utterances. The average length of an utter-
ance was 8.88 words. There were 93 actions in this do-
main. Some utterances had more than one action associ-
ated with them, the average number of actions associated
with an utterance being 1.07.
3.2 Recognizer
The same acoustic model was used in all the experiments
reported here, i.e. for experiments with both the phone-
based and word-based utterance classifiers. This model
has 42 phones and uses discriminatively trained 3-state
HMMs with 10 Gaussians per state. It uses feature space
transformations to reduce the feature space to 60 fea-
tures prior to discriminative maximum mutual informa-
tion training. This acoustic model was trained by Andrej
Ljolje and is similar to the baseline acoustic model used
for experiments with the Switchboard corpus, an earlier
version of which is described by Ljolje et al (2000).
(Like the model used here, the baseline model in those
experiments does not involve speaker and environment
normalizations.)
The n-gram phonotactic models used were represented
as weighted finite state automata. These automata (with
the exception of the initial unweighted phone loop) were
constructed using the stochastic language modeling tech-
nique described by Riccardi et al (1996). This modeling
technique, which includes a scheme for backing off to
probability estimates for shorter n-grams, was originally
designed for language modeling at the word level.
3.3 Classifier
Different possible classification algorithms can be used in
our utterance classification method. For the experiments
reported here we use the BoosTexter classifier (Schapire
and Singer, 2000). Among the alternatives are decision
trees (Quinlan, 1993) and support vector machines (Vap-
nik, 1995). BoosTexter was originally designed for text
categorization. It uses the AdaBoost algorithm (Freund
and Schapire, 1997; Schapire, 1999), a wide margin ma-
chine learning algorithm. At training time, AdaBoost
selects features from a specified space of possible fea-
tures and associates weights with them. A distinguishing
characteristic of the AdaBoost algorithm is that it places
more emphasis on training examples that are difficult to
classify. The algorithm does this by iterating through a
number of rounds: at each round, it imposes a distribu-
tion on the training data that gives more probability mass
to examples that were difficult to classify in the previ-
ous round. In our experiments, 500 rounds of boosting
were used; each round allows the selection of a new fea-
ture and the adjustment of weights associated with exist-
ing features. In the experiments, the possible features are
identifiers corresponding to prompts, and phone n-grams
or word n-grams (for the phone and word-based methods
respectively) up to length 4.
3.4 Experimental Conditions
Three experimental conditions are considered. The suf-
fixes (M and H) in the condition names refer to whether
the two training phases (i.e. training for recognition and
classification respectively) use inputs produced by ma-
chine (M) or human (H) processing.
PhonesMM This experimental condition is the method
described in this paper, so no human transcriptions
are used. Unsupervised training from the training
speech files is used to build a phone recognition
model. The classifier is trained on the phone strings
resulting from recognizing the training speech files
with this model. At runtime, the classifier is ap-
plied to the results of recognizing the test files with
this model. The initial recogition model for the un-
supervised recognition training process was an un-
weighted phone loop. The final n-gram order used
in the recognition training procedure (Nmax in sec-
tion 2) was 5.
WordsHM Human transcriptions of the training speech
files are used to build a word trigram model. The
classifier is trained on the word strings resulting
from recognizing the training speech files with this
word trigram model. At runtime, the classifier is ap-
plied to the results of recognizing the test files with
the word trigram model.
Learned phone Corresponding
sequence words
b ih l ih billing
k ao l z calls
n ah m b number
f aa n phone
r ey t rate
k ae n s cancel
aa p ax r operator
aw t m ay what my
ch eh k check
m ay b my bill
p ae n ih company
s w ih ch switch
er n ae sh international
v ax k w have a question
l ih ng p billing plan
r ey t s rates
k t uw p like to pay
ae l ax n balance
m er s er customer service
r jh f ao charge for
Table 1: Example phone sequences learned by the train-
ing procedure from domain A training speech files.
WordsHH Human transcriptions of the training speech
files are used to build a word trigram model. The
classifier is trained on the human transcriptions of
the speech training files. At runtime, the classifier
is applied to the results of recognizing the test files
with the word trigram model.
For all three conditions, median recognition and classi-
fication time for test data was less than real time (i.e. the
duration of test speech files) on current micro-processors.
As noted earlier, the acoustic model, the number of boost-
ing rounds, and the use of prompts as an additional clas-
sification feature, are the same for all experimental con-
ditions.
3.5 Example learned phone sequences
To give an impression of the kind of phone sequences
resulting from the automatic training procedure and ap-
plied by the classifier at runtime, see Table 1. The table
lists some examples of such phone strings learned from
domain A training speech files, together with English
words, or parts of words (shown in bold type), they may
correspond to. (Of course, the words play no part in the
method and are only included for expository purposes.)
The phone strings are shown in the DARPA phone alpha-
bet.
Rejection PhoneMM WordHM WordHH
rate (%) accuracy accuracy accuracy
0 74.6 76.2 77.0
10 79.5 81.1 81.5
20 84.4 85.8 86.2
30 89.4 90.5 90.9
40 94.1 94.7 94.4
50 97.2 97.3 96.7
Table 2: Phone-based and word-based utterance classifi-
cation accuracy for domain A
4 Classification Accuracy
In this section we compare the accuracy of our phone-
string utterance classification method (PhonesMM) with
methods (WordsHM and WordsHH) using manual tran-
scription and word string models.
Accuracy Metric
The results are presented as utterance classification rates,
specifically the percentage of utterances in the test set for
which the predicted action is valid. Here a valid predic-
tion means that the predicted action is the same as one of
the actions associated with the test utterance by a human
labeler. (As noted in section 3, the average number of
actions associated with an utterance was 1.09, 1.23, and
1.07 for domains A, B, and C, respectively.) In this met-
ric we only take into account a single action predicted
by the classifier, i.e. this is ?rank 1? classification ac-
curacy, rather than the laxer ?rank 2? classification ac-
curacy (where the classifier is allowed to make two pre-
dictions) reported by Gorin et. al (1999) and Petrovska
et. al (2000).
In practical applications of utterance classification,
user inputs are rejected if the confidence of the classifier
in making a prediction falls below a threshold appropri-
ate to the application. After rejection, the system may,
for example, route the call to a human or reprompt the
user. We therefore show the accuracy of classifying ac-
cepted utterances at different rejection rates, specifically
0% (all utterances accepted), 10%, 20%, 30%, 40%, and
50%. Following Schapire and Singer (2000), the con-
fidence level, for rejection purposes, assigned to a pre-
diction is taken to be the difference between the scores
assigned by BoosTexter to the highest ranked action (the
predicted action) and the next highest ranked action.
Accuracy Results
Utterance classification accuracy rates, at various rejec-
tion rates, for domain A are shown in Table 2 for the
three experimental conditions described in section 3.4.
The corresponding results for domains B and C are shown
in Tables 3 and 4.
Rejection PhoneMM WordHM WordHH
rate (%) accuracy accuracy accuracy
0 80.8 81.6 81.0
10 86.0 86.7 85.3
20 90.0 90.6 89.5
30 93.9 93.7 92.3
40 96.3 96.8 94.7
50 97.5 97.7 96.4
Table 3: Phone-based and word-based utterance classifi-
cation accuracy for domain B
Rejection PhoneMM WordHM WordHH
rate (%) accuracy accuracy accuracy
0 68.2 68.9 69.9
10 73.3 73.7 74.9
20 78.9 79.2 80.2
30 84.8 84.7 85.5
40 89.7 89.3 90.2
50 94.1 93.3 94.5
Table 4: Phone-based and word-based utterance classifi-
cation accuracy for domain C
The utterances in domain A are on average longer and
more complex than in domain B; this may partly explain
the higher classification rates for domain B. The gener-
ally lower classification accuracy rates for domain C may
reflect the larger set of actions for this domain (92 ac-
tions, compared with 56 and 54 actions for domains A
and B). Another difference between the domains was that
the recording quality for domain B was not as high as
for domains A and C. Despite these differences between
the domains, there is a consistent pattern for the compar-
ison of most interest to this paper, i.e. the relative per-
formance of utterance classification methods requiring or
not requiring transcription.
Perhaps the most surprising outcome of these ex-
periments is that the phone-based method with short
?phrasal? contexts (up to four phones) has classifica-
tion accuracy that is so close to that provided by the
longer phrasal contexts of trigram word recognition and
word-string classification. Of course, the re-estimation
of phone n-grams employed in the phone-based method
means that two-word units are implicitly modeled since
the phone 5-grams modeled in recognition, and 4-grams
in classification, can straddle word boundaries.
The experiments suggest that if transcriptions are
available (i.e. the effort to produce them has already
been expended), then they can be used to slightly improve
performance over the phone-based method (PhonesMM)
not requiring transcriptions. For domains A and C, this
would give an absolute performance difference of about
2%, while for domain B the difference is around 1%.
Nmax Recog. Classif.
accuracy accuracy
0 54.2 70.0
1 56.6 70.6
2 59.1 71.2
3 59.5 71.5
4 60.0 73.2
5 62.3 74.6
Table 5: Phone recognition accuracy and phone string
classification accuracy (PhoneMM with no rejection) for
increasing values of Nmax for domain A.
Nmax Recog. Classif.
accuracy accuracy
0 27.9 69.2
1 38.3 70.7
2 48.6 74.7
3 53.3 77.6
4 55.1 79.2
5 55.7 80.8
Table 6: Phone recognition accuracy and phone string
classification accuracy (PhoneMM with no rejection) for
increasing values of Nmax for domain B.
Whether it is optimal to train the word-based classifier on
the transcriptions (WordsHH) or the output of the recog-
nizer (WordsHM) seems to depend on the particular data
set.
When the operational setting of utterance classifica-
tion demands very high confidence, and a high degree
of rejection is acceptable (e.g. if sufficient human backup
operators are available), then the small advantage of the
word-based methods is reduced further to less than 1%.
This can be seen from the high rejection rate rows of the
accuracy tables.
Effectiveness of Unsupervised Training
Tables 5, 6, and 7, show the effect of increasing Nmax
(the final iteration number in the unsupervised phone
recognition model) for domains A, B and C, respectively.
The row with Nmax = 0 corresponds to the initial un-
weighted phone loop recognition. The classification ac-
curacies shown in this table are all at 0% rejection. Phone
recognition accuracy is the standard ASR error rate ac-
curacy in terms of the percentage of phone insertions,
deletions, and substitutions, determined by aligning the
ASR output against reference phone transcriptions pro-
duced by the pronounciation component of our speech
synthesizer. (Since these reference phone transcriptions
are not perfect, the actual phone recognition accuracy is
probably slightly higher.) Clearly, for all three domains,
unsupervised recognition model training improves both
Nmax Recog. Classif.
accuracy accuracy
0 55.4 61.1
1 59.8 61.8
2 65.3 64.3
3 68.1 66.3
4 69.1 67.4
5 69.3 68.2
Table 7: Phone recognition accuracy and phone string
classification accuracy (PhoneMM with no rejection) for
increasing values of Nmax for domain C.
recognition and classification accuracy compared with a
simple phone loop. Unsupervised training of the recogni-
tion model is particularly important for domain B where
the quality of recordings is not as high as for domains
A and C, so the system needs to depend more on the re-
estimated n-gram models to achieve the final classifica-
tion accuracy.
5 Concluding Remarks
In this paper we have presented an utterance classifica-
tion method that does not require manual transcription
of training data. The method combines unsupervised re-
estimation of phone n-ngram recognition models together
with a phone-string classifier. The utterance classifica-
tion accuracy of the method is surprisingly close to a
more traditional method involving manual transcription
of training utterances into word strings and recognition
with word trigrams. The measured absolute difference
in classification accuracy (with no rejection) between our
method and the word-based method was only 1% for one
test domain and 2% for two other test domains. The per-
formance difference is even smaller (less than 1%) if high
rejection thresholds are acceptable. This performance
level was achieved despite the large reduction in effort
required to develop new applications with the presented
utterance classification method.
References
H. Alshawi and S. Douglas. 2001. Variant transduction:
A method for rapid development of interactive spoken
interfaces. In Proceedings of the SIGDial Workshop on
Discourse and Dialogue, Aalborg, Denmark, Septem-
ber.
R. Carpenter and J. Chu-Carroll. 1998. Natural language
call routing: a robust, self-organizing approach. In
Proceedings of the International Conference on Speech
and Language Processing, Sydney, Australia.
J. Choi, D. Hindle, J. Hirschberg, F. Pereira, A. Singhal,
and S. Whittaker. 1999. Spoken content-based audio
navigation (scan). In Proceedings of ICPhS-99 (In-
ternational Congress of Phonetics Sciences, San Fran-
cisco, California, August.
J. T. Foote, S. J. Young, G. J. F Jones, and K. Sparck
Jones. 1997. Unconstrained keyword spotting using
phone lattices with application to spoken document re-
trieval. Computer Speech and Language, 11(2):207?
224.
Y. Freund and R. E. Schapire. 1997. A decision-theoretic
generalization of on-line learning and an application to
boosting. Journal of Computer and System Sciences,
55(1):119?139.
A. L. Gorin, G. Riccardi, and J. H. Wright. 1997.
How may I help you? Speech Communication, 23(1-
2):113?127.
A. L. Gorin, D. Petrovska-Delacretaz, G. Riccardi, and
J. H. Wright. 1999. Learning Spoken Language with-
out Transcription. In Proceedings of the ASRU Work-
shop, Keystone, Colorado, December.
K. Sparck Jones, G. J. F. Jones, J. T. Foote, and S. J.
Young. 1996. Experiments in spoken document
retrieval. Information Processing and Management,
32(4):399?417.
M. Levit, A. L. Gorin, and J. H. Wright. 2001. Mul-
tipass Algorithm for Acquisition of Salient Acoustic
Morphemes. In Proceedings of Eurospeech 2001, Aal-
borg, Denmark, September.
A. Ljolje, D. M. Hindle, M. D. Riley, and R. W. Sproat.
2000. The AT&T LVCSR-2000 System. In Speech
Transcription Workshop, Univ. of Maryland, May.
K. Ng and V. Zue. 1998. Phonetic recognition for spo-
ken document retrieval. In Proceedings of ICASSP 98,
Seattle, Washington, May.
D. Petrovska-Delacretaz, A. L. Gorin, J. H. Wright, and
G. Riccardi. 2000. Detecting Acoustic Morphemes in
Lattices for Spoken Language Understanding. In Pro-
ceedings of the Interanational Conference on Spoken
Language Processing, Beijing, China, October.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
G. Riccardi, R. Pieraccini, and E. Bocchieri. 1996.
Stochastic automata for language modeling. Computer
Speech and Language, 10:265?293.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A
boosting-based system for text categorization. Ma-
chine Learning, 39(2/3):135?168.
R. E. Schapire. 1999. A brief introduction to boost-
ing. In Proceedings of the Sixteenth International Joint
Conference on Artificial Intelligence.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer, New York.
J. H. Wright, A. L. Gorin, and G. Riccardi. 1997. Au-
tomatic acquisition of salient grammar fragments for
call-type classification. In Proceedings of European
Conference on Speech Communication and Technol-
ogy, pages 1419?1422, Rhodes, Greece, September.
Variant Transduction: A Method for Rapid Development of
Interactive Spoken Interfaces
Hiyan Alshawi and Shona Douglas
AT&T Labs Research
180 Park Avenue
Florham Park, NJ 07932, USA
fhiyan,shonag@research.att.com
Abstract
We describe an approach (\vari-
ant transduction") aimed at reduc-
ing the eort and skill involved
in building spoken language inter-
faces. Applications are created
by specifying a relatively small set
of example utterance-action pairs
grouped into contexts. No interme-
diate semantic representations are
involved in the specication, and
the conrmation requests used in
the dialog are constructed automat-
ically. These properties of vari-
ant transduction arise from combin-
ing techniques for paraphrase gen-
eration, classication, and example-
matching. We describe how a spo-
ken dialog system is constructed
with this approach and also provide
some experimental results on vary-
ing the number of examples used to
build a particular application.
1 Introduction
Developing non-trivial interactive spoken lan-
guage applications currently requires signi-
cant eort, often several person-months. A
major part of this eort is aimed at coping
with variation in the spoken language input
by users. One approach to handling varia-
tion is to write a large natural language gram-
mar manually and hope that its coverage is
sucient for multiple applications (Dowding
et al, 1994). Another approach is to cre-
ate a simulation of the intended system (typ-
ically with a human in the loop) and then
record users interacting with the simulation.
The recordings are then transcribed and an-
notated with semantic information relating to
the domain; the transcriptions and annota-
tions can then be used to create a statistical
understanding model (Miller et al, 1998) or
used as guidance for manual grammar devel-
opment (Aust et al, 1995).
Building mixed initiative spoken language
systems currently usually involves the design
of semantic representations specic to the ap-
plication domain. These representations are
used to pass data between the language pro-
cessing components: understanding, dialog,
conrmation generation, and response gener-
ation. However, such representations tend to
be domain-specic, and this makes it dicult
to port to new domains or to use machine
learning techniques without extensive hand-
labeling of data with the semantic represen-
tations. Furthermore, the use of intermediate
semantic representations still requires a nal
transduction step from the intermediate rep-
resentation to the action format expected by
the application back-end (e.g. SQL database
query or procedure call).
For situations when the eort and exper-
tise available to build an application is small,
the methods mentioned above are impracti-
cal, and highly directed dialog systems with
little allowance for language variability are
constructed.
In this paper, we describe an approach to
constructing interactive spoken language ap-
plications aimed at alleviating these prob-
lems. We rst outline the characteristics of
the method (section 2) and what needs to
be provided by the application builder (sec-
tion 3). In section 4 and section 5 we ex-
plain variant expansion and the operation of
the system at runtime, and in section 6 we
describe how conrmation requests are pro-
duced by the system. In section 7 we give
some initial experimental results on varying
the number of examples used to construct a
call-routing application.
2 Characteristics of our approach
The goal of the approach discussed in this pa-
per (which we refer to as \variant transduc-
tion") is to avoid the eort and specialized
expertise used to build current research pro-
totypes, while allowing more natural spoken
input than is handled by spoken dialog sys-
tems built using current commercial practice.
This led us to adopt the following constraints:
 Applications are constructed using a rel-
atively small number of example inputs
(no grammar development or extensive
data collection).
 No intermediate semantic representa-
tions are needed. Instead, manipulations
are performed on word strings and on ac-
tion strings that are nal (back-end) ap-
plication calls.
 Conrmation queries posed by the sys-
tem to the user are constructed automat-
ically from the examples, without the use
of a separate generation component.
 Dialog control should be simple to spec-
ify for simple applications, while allowing
the exibility of delegating this control
to another module (e.g. an \intelligent"
back-end agent) for more complex appli-
cations.
We have constructed two telephone-based
applications using this method, an applica-
tion to access email and a call-routing appli-
cation. These two applications were chosen
to gain experience with the method because
they have dierent usage characteristics and
back-end complexity. For the e-mail access
system, usage is typically habitual, and the
system's mapping of user utterances to back-
end actions needs to take into account dy-
namic aspects of the current email session.
For the call-routing application, the back-end
calls executed by the system are relatively
simple, but users may only encounter the sys-
tem once, and the system's initial prompt is
not intended to constrain the rst input spo-
ken by the user.
3 Constructing an application with
example-action contexts
An interactive spoken language application
constructed with the variant transduction
method consists of a set of contexts. Each
context provides the mapping between user
inputs and application actions that are mean-
ingful in a particular stage of interaction be-
tween the user and system. For example the
e-mail reader application includes contexts for
logging in and for navigating a mail folder.
The actual contexts that are used at run-
time are created through a four step process:
1. The application developer species (a
small number of) triples he; a; ci where
e is a natural language string (a typical
user input), a is an application action
(back-end application API call). For in-
stance, the string read the message from
John might be paired with the API call
mailAgent.getWithSender("jsmith@att.com").
The third element of a triple, c, is an
expression identifying another (or the
same) context, specically, the context
the system will transition to if e is the
closest match to the user's input.
2. The set of triples for each context is ex-
panded by the system into a larger set
of triples. The additional triples are of
the form hv; a
0
; ci where v is a \variant"
of example e (as explained in section 4
below), and a
0
is an \adapted" version of
the action a.
3. During an actual user session, the set of
triples for a context may optionally be
expanded further to take into account
the dynamic aspects of a particular ses-
sion. For example, in the mail access ap-
plication, the set of names available for
recognition is increased to include those
present as senders in the user's current
mail folder.
4. A speech recognition language model is
compiled from the expanded set of ex-
amples. We currently use a language
model that accepts any sequence of sub-
strings of the examples, optionally sepa-
rated by ller words, as well as sequences
of digits. (For a small number of exam-
ples, a statistical N-gram model is inef-
fective because of low N-gram counts.) A
detailed account of the recognition lan-
guage model techniques used in the sys-
tem is beyond the scope of this paper.
In the current implementation, actions are
sequences of statements in the Java language.
Constructors can be called to create new ob-
jects (e.g. a mail session object) which can be
assigned to variables and referenced in other
actions. The context interpreter loads the re-
quired classes and evaluates methods dynam-
ically as needed. It is thus possible for an
application developer to build a spoken inter-
face to their target API without introducing
any new Java classes. The system could eas-
ily be adapted to use action strings from other
interpreted languages.
A key property of the process described
above is that the application developer needs
to know only the back-end API and English
(or some other natural language).
4 Variant compilation
Dierent expansion methods can be used in
the second step to produce variants v of an
example e. In the simplest case, v may be
a paraphrase of e. Such paraphrase vari-
ants are used in the experiments in section 7,
where domain-independent \carrier" phrases
are used to create variants. For example, the
phrase I'd like to (among others) is used as a
possible alternative for the phrase I want to.
The context compiler includes an English-to-
English paraphrase generator, so the applica-
tion developer is not involved in the expan-
sion process, relieving her of the burden of
handling this type of language variation. We
are also experimenting with other forms of
variation, including those arising from lexical-
semantic relations, user-specic customiza-
tion, and those variants uttered by users dur-
ing eld trials of a system.
When v is a paraphrase of e, the adapted
action a
0
is the same string as a. In the more
general case, the meaning of variant v is dif-
ferent from that of e, and the system attempts
(not always correctly) to construct a
0
so that
it reects this dierence in meaning. For ex-
ample, including the variant show the message
from Bill Wilson of an example read the mes-
sage from John, involves modifying the ac-
tion mailAgent.getWithSender("jsmith@att.com")
to mailAgent.getWithSender("wwilson@att.com").
We currently adopt a simple approach to
the process of mapping language string vari-
ants to their corresponding target action
string variants. The process requires the
availability of a \token mapping" t between
these two string domains, or data or heuristics
fromwhich such a mapping can be learned au-
tomatically. Examples of the token mapping
are names to email addresses as illustrated in
the example above, name to identier pairs in
a database system, \soundex" phonetic string
spelling in directory applications, and a bilin-
gual dictionary in a translation application.
The process proceeds as follows:
1. Compute a set of lexical mappings be-
tween the variant v and example e. This
is currently performed by aligning the
two string in such a way as that the align-
ment minimizes the (weighted) edit dis-
tance between them (Wagner and Fis-
cher, 1974).
2. The token mapping t is used to map
substitution pairs identied by the align-
ment (hread; showi and hJohn, Bill Wil-
soni in the example above) to corre-
sponding substitution pairs in the action
string. In general this will result in a
smaller set of substitution strings since
not all word strings will be present in
the domain of t. (In the example, this re-
sults in the single pair hjsmith@att.com,
wwilson@att.comi.)
3. The action substitution pairs are applied
to a to produce a
0
.
4. The resulting action a
0
is checked for
(syntactic) well-formedness in the action
string domain; the variant v is rejected if
a
0
is ill-formed.
5 Input interpretation
When an example-action context is active
during an interaction with a user, two com-
ponents (in addition to the speech recognition
language model) are compiled from the con-
text in order to map the user inputs into the
appropriate (possibly adapted) action:
Classier A classier is built with training
pairs hv; ai where v is a variant of an
example e for which the example action
pair he; ai is a member of the unexpanded
pairs in the context. Note that the clas-
sier is not trained on pairs with adapted
examples a
0
since the set of adapted
actions may be too large for accurate
classication (with standard classica-
tion techniques). The classiers typically
use text features such as N-grams ap-
pearing in the training data. In our ex-
periments, we have used dierent classi-
ers, including BoosTexter (Schapire and
Singer, 2000), and a classier based on
Phi-correlation statistics for the text fea-
tures (see Alshawi and Douglas (2000)
for our earlier application of Phi statis-
tics in learning machine translation mod-
els from examples). Other classiers
such as decision trees (Quinlan, 1993) or
support vector machines (Vapnik, 1995)
could be used instead.
Matcher The matcher can compute a dis-
tortion mapping and associated distance
between the output s of the speech rec-
ognizer and a variant v. Various match-
ers can be used such as those suggested
in example-based approaches to machine
translation (Sumita and Iida, 1995). So
far we have used a weighted string edit
distance matcher and experimented with
dierent substitution weights including
ones based on measures of statistical sim-
ilarity between words such as the one
described by Pereira et al (1993). The
output of the matcher is a real number
(the distance) and a distortion mapping
represented as a sequence of edit opera-
tions (Wagner and Fischer, 1974).
Using these two components, the method
for mapping the user's utterance to an exe-
cutable action is as follows:
1. The language model derived from con-
text c is activated in the speech recog-
nizer.
2. The speech recognizer produces a string
s from the user's utterance.
3. The classier for c is applied to s to pro-
duce an unadapted action a.
4. The matcher is applied pairwise to com-
pare s with each variant v
a
derived from
a triple he; a; c
0
i in the unexpanded ver-
sion of c.
5. The triple hv; a
0
; c
0
i for which v pro-
duces the smallest distance is selected
and passed along with e to the dialog con-
troller.
The relationship between the input s, vari-
ant v, example e, and actions a and a
0
is
depicted in Figure 1. In the gure, f is
the mapping between examples and actions
in the unexpanded context; r is the relation
between examples and variants; and g is the
search mapping implemented by the classier-
matcher. The role of e
0
is related to conrma-
tions as explained in the following section.
6 Conrmation and dialog control
Dialog control is straightforward as the reader
might expect, except for two aspects de-
scribed in this section: (i) evaluation of next-
context expressions, and (ii) generation of
p (prompt): say a mailreader command
s (words spoken): now show me messages from Bill
v (variant): show the message from Bill Wilson
e (example): read the message from John
a (associated action): mailAgent.getWithSender("jsmith@att.com")
a
0
(adapted action): mailAgent.getWithSender("wwilson@att.com")
e
0
(adapted example): read the message from Bill Wilson
Figure 2: Example
Figure 1: Variant Transduction mappings
conrmation requests based on the examples
in the context and the user's input.
As noted in section 3 the third element c
of each triple he; a; ci in a context is an ex-
pression that evaluates to the name of the
next context (dialog state) that the system
will transition to if the triple is selected. For
simple applications, c can simply always be
an identier for a context, i.e. the dialog state
transition network is specied explicitly in ad-
vance in the triples by the application devel-
oper.
For more complex applications, next con-
text expressions c may be calls that evalu-
ate to context identiers. In our implemen-
tation, these calls can be Java methods ex-
ecuted on objects known to the action in-
terpreter. They may thus be calls on the
back-end application system, which is appro-
priate for cases when the back-end has state
information relevant to what should happen
next (e.g. if it is an \intelligent agent"). It
might also be a call to component that imple-
ments a dialog strategy learning method (e.g.
Levin and Pieraccini (1997)), though we have
not yet tried such methods in conjunction
with the present system.
A conrmation request of the form do you
mean e
0
is constructed for each variant-action
pair (v; a
0
) of an example-action pair (e; a).
The string e
0
is constructed by rst comput-
ing a submapping h
0
of the mapping h rep-
resenting the distortion between e and v. h
0
is derived from h by removing those edit op-
erations which were not involved in mapping
the action a to the adapted action a
0
. (The
matcher is used to compute h except when
the process of deriving (v; a
0
) from (e; a) al-
ready includes an explicit representation of h
and t(h).)
The restricted mapping h
0
is used instead of
h to construct e
0
in order to avoid misleading
the user about the extent to which the ap-
plication action is being adapted. Thus if h
includes the substitution w ! w
0
but t(w) is
not a substring of a then this edit operation is
not included in h
0
. This way, e
0
includes w un-
changed, so that the conrmation asked of the
user does not carry the implication that the
change w ! w
0
is taken into account in the
action a
0
to be executed by the system. For
instance, in the example in Figure 2, the word
\now" in the user's input does not correspond
to any part of the adapted action, and is not
included in the conrmation string. In prac-
tice, the conrmation string e
0
is computed
at the same time that the variant-action pair
(v; a
0
) is derived from the original example
pair (e; a).
The dialog ow of control proceeds as fol-
lows:
1. The active context c is set to a distin-
guished initial context c
0
indicated by
the application developer.
2. A prompt associated with the current ac-
tive context c is played to the user using
a speech synthesiser or by playing an au-
dio le. For this purpose the application
developer provides a text string (or audio
le) for each context in the application.
3. The user's utterance is interpreted as ex-
plained in the previous section to pro-
duce the triple hv; a
0
; c
0
i.
4. A match distance d is computed as the
sum of the distance computed for the
matcher between s and v and the dis-
tance computed by the matcher between
v and e (where e is the example from
which v was derived).
5. If d is smaller than a preset threshold, it
is assumed that no conrmation is neces-
sary and the next three steps are skipped.
6. The system asks the user do you mean:
e
0
. If the user responds positively then
proceed to the next step, otherwise re-
turn to step 2.
7. The action a
0
is executed, and any string
output it produces is read to the user
with the speech synthesizer.
8. The active context is set to the result of
evaluating the expression c
0
.
9. Return to step 2.
Figure 2 gives an example showing the
strings involved in a dialog turn. Handling
the user's verbal response to the conrmation
is done with a built-in yes-no context.
The generation of conrmation requests
requires no work by the application de-
veloper. Our approach thus provides
an even more extreme version of auto-
matic conrmation generation than that used
by Chu-Carroll and Carpenter (1999) where
only a small eort is required by the devel-
oper. In both cases, the benets of care-
fully crafted conrmation requests are being
traded for rapid application development.
7 Experiments
An important question relating to our method
is the eect of the number of examples on
system interpretation accuracy. To measure
this eect, we chose the operator services call
routing task described by Gorin et al (1997).
We chose this task because a reasonably large
data set was available in the form of actual
recordings of thousands of real customers call-
ing AT&T's operators, together with tran-
scriptions and manual labeling of the de-
sired call destination. More specically, we
measure the call routing accuracy for uncon-
strained caller responses to the initial context
prompt AT&T. How may I help you?. An-
other advantage of this task was that bench-
mark call routing accuracy gures were avail-
able for systems built with the full data set
(Gorin et al, 1997; Schapire and Singer,
2000). We have not yet measured interpreta-
tion accuracy for the structurally more com-
plex e-mail access application.
In this experiment, the responses to How
may I help you? are \routed" to fteen des-
tinations, where routing means handing o
the call to another system or human operator,
or moving to another example-action context
that will interact further with the user to elicit
further information so that a subtask (such as
making a collect call) can be completed. Thus
the actions in the initial context are simply
the destinations, i.e. a = a
0
, and the matcher
is only used to compute e
0
.
The fteen destinations include a destina-
tion \other" which is treated specially in that
it is also taken to be the destination when the
system rejects the user's input, for example
because the condence in the output of the
speech recognizer is too low. Following previ-
ous work on this task, cited above, we present
the results for each experimental condition as
an ROC curve plotting the routing accuracy
(on non-rejected utterances) as a function of
the false rejection rate (the percentage of the
samples incorrectly rejected); a classication
by the system of \other" is considered equiv-
alent to rejection.
The dataset consists of 8,844 utterances of
which 1000 were held out for testing. We refer
to the remaining 7,884 utterances as the \full
training dataset".
In the experiments, we vary two conditions:
Input uncertainty The input string to the
interpretation component is either a hu-
man transcription of the spoken utter-
ance or the output of a speech recog-
nizer. The acoustic model used for au-
tomatic speech recognition was a gen-
eral telephone speech HHM model in all
cases. (For the full dataset, better re-
sults can be achieved by an application-
specic acoustic model, as presented by
Gorin et al (1997) and conrmed by our
results below.)
Size of example set We select progres-
sively larger subsets of examples from
the full training set, as well as showing
results for the full training set itself. We
wish to approximate the situation where
an application developer uses typical
examples for the initial context without
knowing the distribution of call types.
We therefore select k utterances for each
destination, with k set to 3, 5, and 10,
respectively. This selection is random,
except for the provision that utterances
appearing more than once are preferred,
to approximate the notion of a typical
utterance. The selected examples are
expanded by the addition of variants, as
described earlier. For each value of k,
the results shown are for the median of
three runs.
Figure 3 shows the routing accuracy ROC
curves for transcribed input for k = 3; 5; 10
and for the full training dataset. These re-
sults for transcribed input were obtained with
BoosTexter (Schapire and Singer, 2000) as the
classier module in our system because we
have observed that BoosTexter generally out-
performs our Phi classier (mentioned earlier)
for text input.
Figure 4 shows the corresponding four ROC
curves for recognition output, and an ad-
ditional fth graph (the top one) showing
the improvement that is obtained with a do-
main specic acoustic model coupled with a
trigram language model. These results for
recognition output were obtained with the
Phi classier module rather than BoosTex-
ter; the Phi classier performance is generally
the same as, or slightly better than, Boos-
Texter when applied to recognition output.
The language models used in the experiments
for Figure 4 are derived from the example
sets for k = 3; 5; 10 (lower three graphs) and
for the full training set (upper two graphs),
respectively. As described earlier, the lan-
guage model for restricted numbers of exam-
ples is an unweighted one that recognizes se-
quences of substrings of the examples. For the
full training set, statistical N-gram language
models are used (N=3 for the top graph and
N=2 for the second to top) since there is suf-
cient data in the full training set for such
language models to be eective.
0 10 20 30 40 50 60 70 80
0
10
20
30
40
50
60
70
80
90
100
False rejection %
%
 C
or
re
ct
 a
ct
io
ns
full training set            
10 examples/action + variants
5 examples/action + variants 
3 examples/action + variants 
Figure 3: Routing accuracy for transcribed
utterances
Comparing the two gures, it can be seen
that the performance shortfall from using
small numbers of examples compared to the
full training set is greater when speech recog-
0 10 20 30 40 50 60 70 80
0
10
20
30
40
50
60
70
80
90
100
False rejection %
%
 C
or
re
ct
 a
ct
io
ns
full training set,  trigrams, domain acoustics
full training set, bigrams                    
10 examples/action + variants, subsequences   
5 examples/action + variants, subsequences    
3 examples/action + variants, subsequences    
Figure 4: Routing accuracy for speech recog-
nition output
nition errors are included. This suggests that
it might be advantageous to use the examples
to adapt a general statistical language model.
There also seem to be diminishing returns as
k is increased from 3 to 5 to 10. A likely
explanation is that expansion of examples by
variants is progressively less eective as the
size of the unexpanded set is increased. This
is to be expected since additional real exam-
ples presumably are more faithful to the task
than articially generated variants.
8 Concluding remarks
We have described an approach to construct-
ing interactive spoken interfaces. The ap-
proach is aimed at shifting the burden of han-
dling linguistic variation for new applications
from the application developer (or data col-
lection lab) to the underlying spoken language
understanding technology itself. Applications
are specied in terms of a relatively small
number of examples, while the mapping be-
tween the inputs that users speak, variants
of the examples, and application actions, are
handled by the system. In this approach, we
avoid the use of intermediate semantic rep-
resentations, making it possible to develop
general approaches to linguistic variation and
dialog responses in terms of word-string to
word-string transformations. Conrmation
requests used in the dialog are computed au-
tomatically from variants in a way intended to
minimize misleading the user about the appli-
cation actions to be executed by the system.
The quantitative results we have pre-
sented indicate that a surprisingly small num-
ber of training examples can provide use-
ful performance in a call routing application.
These results suggest that, even at its cur-
rent early stage of development, the vari-
ant transduction approach is a viable option
for constructing spoken language applications
rapidly without specialized expertise. This
may be appropriate, for example, for boot-
strapping data collection, as well as for situa-
tions (e.g. small businesses) for which devel-
opment of a full-blown system would be too
costly. When a full dataset is available, the
method can provide similar performance to
current techniques while reducing the level of
skill necessary to build new applications.
References
H. Alshawi and S. Douglas. 2000. Learning
dependency transduction models from unan-
notated examples. Philosophical Transactions
of the Royal Society (Series A: Mathematical,
Physical and Engineering Sciences), 358:1357{
1372, April.
H. Aust, M. Oerder, F. Seide, and V. Steinbiss.
1995. The Philips automatic train timetable
information system. Speech Communication,
17:249{262.
Jennifer Chu-Carroll and Bob Carpenter. 1999.
Vector-based natural language call routing.
Computational Linguistic, 25(3):361{388.
J. Dowding, J. M. Gawron, D. Appelt, J. Bear,
L. Cherny, R. Moore, and D. Moran. 1994.
Gemini: A Natural Language System For
Spoken-Language Understanding. In Proc.
ARPA Human Language Technology Workshop
'93, pages 43{48, Princeton, NJ.
A.L. Gorin, G. Riccardi, and J.H. Wright. 1997.
How may I help you? Speech Communication,
23(1-2):113{127.
E. Levin and R. Pieraccini. 1997. A stochas-
tic model of computer-human interaction for
learning dialogue strategies. In Proceedings of
EUROSPEECH97, pages 1883{1886, Rhodes,
Greece.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone,
Ralph Weischedel, and the Annotation Group.
1998. Algorithms that learn to extract informa-
tion { BBN: description of the SIFT system as
used for MUC-7. In Proceedings of the Seventh
Message Understanding Conference (MUC-7),
Fairfax, VA. Morgan Kaufmann.
F. Pereira, N. Tishby, and L. Lee. 1993. Distribu-
tional clustering of english words. In Proceed-
ings of the 31st meeting of the Association for
Computational Linguistics, pages 183{190.
J.R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
Robert E. Schapire and Yoram Singer. 2000.
BoosTexter: A Boosting-based System for
Text Categorization. Machine Learning,
39(2/3):135{168.
Eiichiro Sumita and Hitoshi Iida. 1995. Het-
erogeneous computing for example-based trans-
lation of spoken language. In Proceedings of
the 6
th
International Conference on Theoretical
and Methodological Issues in Machine Transla-
tion, pages 273{286, Leuven, Belgium.
V.N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York.
Robert A. Wagner and Michael J. Fischer.
1974. The String-to-String Correction Prob-
lem. Journal of the Association for Computing
Machinery, 21(1):168{173, January.
Speech Translation Performance of Statistical Dependency Transduction
and Semantic Similarity Transduction
Hiyan Alshawi and Shona Douglas
AT&T Labs - Research
Florham Park, NJ 07932, USA
 
hiyan,shona  @research.att.com
Abstract
In this paper we compare the performance
of two methods for speech translation.
One is a statistical dependency transduc-
tion model using head transducers, the
other a case-based transduction model in-
volving a lexical similarity measure. Ex-
amples of translated utterance transcrip-
tions are used in training both models,
though the case-based model also uses se-
mantic labels classifying the source utter-
ances. The main conclusion is that while
the two methods provide similar transla-
tion accuracy under the experimental con-
ditions and accuracy metric used, the sta-
tistical dependency transduction method
is significantly faster at computing trans-
lations.
1 Introduction
Machine translation, natural language processing,
and more generally other computational problems
that are not amenable to closed form solutions,
have typically been tackled by one of three broad
approaches: rule-based systems, statistical mod-
els (including generative models), and case-based
systems. Hybrid solutions combining these ap-
proaches have also been used in language pro-
cessing generally (Klavans and Resnik, 1996) and
more specifically in machine translation (for exam-
ple Frederking et al (1994)).
In this paper we compare the performance of two
methods for speech translation. One is the statistical
dependency transduction model (Alshawi and Dou-
glas, 2000; Alshawi et al, 2000b), a trainable gener-
ative statistical translation model using head trans-
ducers (Alshawi, 1996). The other is a case-based
transduction model which makes use of a semantic
similarity measure between words. Both models are
trained automatically using examples of translated
utterances (the transcription of a spoken utterance
and a translation of that transcription). The case-
based model makes use of additional information in
the form of labels associated with source language
utterances, typically one or two labels per utterance.
This additional information, which was originally
provided for a separate monolingual task, is used to
construct the lexical similarity measure.
In training these translation methods, as well as
their runtime application, no pre-existing bilingual
lexicon is needed. Instead, in both cases, the initial
phase of training from the translation data is a sta-
tistical hierarchical alignment search applied to the
set of bilingual examples. This training phase pro-
duces a bilingual lexicon, used by both methods, as
well as synchronized hierarchical alignments used to
build the dependency transduction model.
In the experiments comparing the performance
of the models we look at accuracy as well as the
time taken to translate sentences from English to
Japanese. The source language inputs used in these
experiments are naturally spoken utterances from
large numbers of real customers calling telephone
operator services.
In section 2 we describe the hierarchical align-
ment algorithm followed by descriptions of the
translation methods in sections 3 and 4. We present
the experiments in section 5 and provide concluding
remarks in section 6.
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 31-38.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
Figure 1: Alignment mapping  , source head-map

, and target head-map 
2 Hierarchical alignments
Both the translation systems described in this pa-
per make use of automatically created hierarchical
alignments of the source and target strings of the
training corpus bitexts. As will be described in sec-
tion 3, we estimate the parameters of a dependency
transduction model from such alignments. In the
case-based method described in section 4, the align-
ments are the basis for the translation lexicon used
to compute substitutions and word-for-word transla-
tions.
A hierarchical alignment consists of four func-
tions. The first two functions are an alignment
mapping  from source words  to target words

	 (which may be the empty word  ), and an in-
verse alignment mapping from target words  to
source words 	 . (The inverse mapping is needed
to handle mapping of target words to  ; it coincides
with  for pairs without  .) The other two functions
are a source head-map  mapping source dependent
words  to their heads  	 in the source string,
and a target head-map  mapping target dependent
words  to their head words 	 in the target string.
An example hierarchical alignment is shown in Fig-
ure 1.
A hierarchical alignment is synchronized (i.e.
corresponds to synchronized dependency trees) if,
roughly speaking,  induces an isomorphism be-
tween the dependency functions  and  (see
Alshawi and Douglas (2000) for a more formal def-
inition). The hierarchical alignment in Figure 1 is
synchronized.
In some previous work (Alshawi et al, 1998; Al-
shawi et al, 2000a; Alshawi et al, 2000b) the train-
ing method constructs synchronized alignments in
which each head word has at most two dependent
phrases. Here we use the technique described by
Alshawi and Douglas (2000) where the models have
greater freedom to vary the granularity of phrase lo-
cality.
Constructing synchronized hierarchical align-
ments for a corpus has two stages: (a) computing
co-occurrence statistics from the training data; (b)
searching for an optimal synchronized hierarchical
alignment for each bitext.
2.1 Word correlation statistics
For each source word in the dataset, a translation
pairing cost 	 is assigned for all possible
translations in the context of a bitext  . Here  and 
are usually words, but may also be the empty word 
or compounds formed from contiguous words; here
we restrict compounds to a maximum length of two
words.
The assignment of these lexical translation pair-
ing costs may be done using various statistical mea-
sures. The main component of  is the so-called
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705?713,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Uptraining for Accurate Deterministic Question Parsing
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, Hiyan Alshawi
Google Research
{slav,pichuan,ringgaard,hiyan}@google.com
Abstract
It is well known that parsing accuracies drop
significantly on out-of-domain data. What is
less known is that some parsers suffer more
from domain shifts than others. We show
that dependency parsers have more difficulty
parsing questions than constituency parsers.
In particular, deterministic shift-reduce depen-
dency parsers, which are of highest interest
for practical applications because of their lin-
ear running time, drop to 60% labeled accu-
racy on a question test set. We propose an
uptraining procedure in which a deterministic
parser is trained on the output of a more ac-
curate, but slower, latent variable constituency
parser (converted to dependencies). Uptrain-
ing with 100K unlabeled questions achieves
results comparable to having 2K labeled ques-
tions for training. With 100K unlabeled and
2K labeled questions, uptraining is able to
improve parsing accuracy to 84%, closing
the gap between in-domain and out-of-domain
performance.
1 Introduction
Parsing accuracies on the popular Section 23 of the
Wall Street Journal (WSJ) portion of the Penn Tree-
bank have been steadily improving over the past
decade. At this point, we have many different pars-
ing models that reach and even surpass 90% depen-
dency or constituency accuracy on this test set (Mc-
Donald et al, 2006; Nivre et al, 2007; Charniak and
Johnson, 2005; Petrov et al, 2006; Carreras et al,
2008; Koo and Collins, 2010). Quite impressively,
models based on deterministic shift-reduce parsing
algorithms are able to rival the other computation-
ally more expensive models (see Nivre (2008) and
references therein for more details). Their linear
running time makes them ideal candidates for large
scale text processing, and our model of choice for
this paper.
Unfortunately, the parsing accuracies of all mod-
els have been reported to drop significantly on out-
of-domain test sets, due to shifts in vocabulary and
grammar usage (Gildea, 2001; McClosky et al,
2006b; Foster, 2010). In this paper, we focus our
attention on the task of parsing questions. Questions
pose interesting challenges for WSJ-trained parsers
because they are heavily underrepresented in the
training data (there are only 334 questions among
the 39,832 training sentences). At the same time,
questions are of particular interest for user facing
applications like question answering or web search,
which necessitate parsers that can process questions
in a fast and accurate manner.
We start our investigation in Section 3 by train-
ing several state-of-the-art (dependency and con-
stituency) parsers on the standard WSJ training set.
When evaluated on a question corpus, we observe
dramatic accuracy drops exceeding 20% for the de-
terministic shift-reduce parsers. In general, depen-
dency parsers (McDonald et al, 2006; Nivre et al,
2007), seem to suffer more from this domain change
than constituency parsers (Charniak and Johnson,
2005; Petrov et al, 2006). Overall, the latent vari-
able approach of Petrov et al (2006) appears to gen-
eralize best to this new domain, losing only about
5%. Unfortunately, the parsers that generalize better
to this new domain have time complexities that are
cubic in the sentence length (or even higher), render-
ing them impractical for web-scale text processing.
705
SBARQ
WHNP
WP
What
SQ
VBZ
does
NP
DT
the
NNP
Peugeot
NN
company
VP
VB
manufacture
.
?
(a)
What does the Peugeot company manufacture ?ROOT
dobj aux det nn p   nsubjroot
(b)
Figure 1: Example constituency tree from the QuestionBank (a) converted to labeled Stanford dependencies (b).
We therefore propose an uptraining method, in
which a deterministic shift-reduce parser is trained
on the output of a more accurate, but slower parser
(Section 4). This type of domain adaptation is rem-
iniscent of self-training (McClosky et al, 2006a;
Huang and Harper, 2009) and co-training (Blum and
Mitchell, 1998; Sagae and Lavie, 2006), except that
the goal here is not to further improve the perfor-
mance of the very best model. Instead, our aim is
to train a computationally cheaper model (a linear
time dependency parser) to match the performance
of the best model (a cubic time constituency parser),
resulting in a computationally efficient, yet highly
accurate model.
In practice, we parse a large amount of unlabeled
data from the target domain with the constituency
parser of Petrov et al (2006) and then train a deter-
ministic dependency parser on this noisy, automat-
ically parsed data. The accuracy of the linear time
parser on a question test set goes up from 60.06%
(LAS) to 76.94% after uptraining, which is compa-
rable to adding 2,000 labeled questions to the train-
ing data. Combining uptraining with 2,000 labeled
questions further improves the accuracy to 84.14%,
fully recovering the drop between in-domain and
out-of-domain accuracy.
We also present a detailed error analysis in Sec-
tion 5, showing that the errors of the WSJ-trained
model are primarily caused by sharp changes in syn-
tactic configurations and only secondarily due to
lexical shifts. Uptraining leads to large improve-
ments across all error metrics and especially on im-
portant dependencies like subjects (nsubj).
2 Experimental Setup
We used the following experimental protocol
throughout the paper.
2.1 Data
Our main training set consists of Sections 02-21 of
the Wall Street Journal portion of the Penn Treebank
(Marcus et al, 1993), with Section 22 serving as de-
velopment set for source domain comparisons. For
our target domain experiments, we evaluate on the
QuestionBank (Judge et al, 2006), which includes
a set of manually annotated questions from a TREC
question answering task. The questions in the Ques-
tionBank are very different from our training data in
terms of grammatical constructions and vocabulary
usage, making this a rather extreme case of domain-
adaptation. We split the 4,000 questions contained
in this corpus in three parts: the first 2,000 ques-
tions are reserved as a small target-domain training
set; the remaining 2,000 questions are split in two
equal parts, the first serving as development set and
the second as our final test set. We report accuracies
on the developments sets throughout this paper, and
test only at the very end on the final test set.
We convert the trees in both treebanks from con-
stituencies to labeled dependencies (see Figure 1)
using the Stanford converter, which produces 46
types of labeled dependencies1 (de Marneffe et al,
2006). We evaluate on both unlabeled (UAS) and
labeled dependency accuracy (LAS).2
Additionally, we use a set of 2 million ques-
tions collected from Internet search queries as unla-
beled target domain data. All user information was
anonymized and only the search query string was re-
tained. The question sample is selected at random
after passing two filters that select queries that are
1We use the Stanford Lexicalized Parser v1.6.2.
2Because the QuestionBank does not contain function tags,
we decided to strip off the function tags from the WSJ be-
fore conversion. The Stanford conversion only uses the -ADV
and -TMP tags, and removing all function tags from the WSJ
changed less than 0.2% of the labels (primarily tmod labels).
706
Training on Evaluating on WSJ Section 22 Evaluating on QuestionBank
WSJ Sections 02-21 F1 UAS LAS POS F1 UAS LAS POS
Nivre et al (2007) ? 88.42 84.89 95.00 ? 74.14 62.81 88.48
McDonald et al (2006) ? 89.47 86.43 95.00 ? 80.01 67.00 88.48
Charniak (2000) 90.27 92.33 89.86 96.71 83.01 85.61 73.59 90.49
Charniak and Johnson (2005) 91.92 93.56 91.24 96.69 84.47 87.13 75.94 90.59
Petrov et al (2006) 90.70 92.91 90.48 96.27 85.52 88.17 79.10 90.57
Petrov (2010) 92.10 93.85 91.60 96.44 86.62 88.77 79.92 91.08
Our shift-reduce parser ? 88.24 84.69 95.00 ? 72.23 60.06 88.48
Our shift-reduce parser (gold POS) ? 90.51 88.53 100.00 ? 78.30 68.92 100.00
Table 1: Parsing accuracies for parsers trained on newswire data and evaluated on newswire and question test sets.
similar in style to the questions in the QuestionBank:
(i) the queries must start with an English function
word that can be used to start a question (what, who
when, how, why, can, does, etc.), and (ii) the queries
have a maximum length of 160 characters.
2.2 Parsers
We use multiple publicly available parsers, as well
as our own implementation of a deterministic shift-
reduce parser in our experiments. The depen-
dency parsers that we compare are the determinis-
tic shift-reduce MaltParser (Nivre et al, 2007) and
the second-order minimum spanning tree algorithm
based MstParser (McDonald et al, 2006). Our shift-
reduce parser is a re-implementation of the Malt-
Parser, using a standard set of features and a lin-
ear kernel SVM for classification. We also train and
evaluate the generative lexicalized parser of Char-
niak (2000) on its own, as well as in combination
with the discriminative reranker of Charniak and
Johnson (2005). Finally, we run the latent variable
parser (a.k.a. BerkeleyParser) of Petrov et al (2006),
as well as the recent product of latent variable gram-
mars version (Petrov, 2010). To facilitate compar-
isons between constituency and dependency parsers,
we convert the output of the constituency parsers to
labeled dependencies using the same procedure that
is applied to the treebanks. We also report their F1
scores for completeness.
While the constituency parsers used in our experi-
ments view part-of-speech (POS) tagging as an inte-
gral part of parsing, the dependency parsers require
the input to be tagged with a separate POS tagger.
We use the TnT tagger (Brants, 2000) in our experi-
ments, because of its efficiency and ease of use. Tag-
ger and parser are always trained on the same data.
3 Parsing Questions
We consider two domain adaptation scenarios in this
paper. In the first scenario (sometimes abbreviated
as WSJ), we assume that we do not have any labeled
training data from the target domain. In practice, this
will always be the case when the target domain is
unknown or very diverse. The second scenario (ab-
breviated as WSJ+QB) assumes a small amount of
labeled training data from the target domain. While
this might be expensive to obtain, it is certainly fea-
sible for narrow domains (e.g. questions), or when a
high parsing accuracy is really important.
3.1 No Labeled Target Domain Data
We first trained all parsers on the WSJ training set
and evaluated their performance on the two domain
specific evaluation sets (newswire and questions).
As can be seen in the left columns of Table 1, all
parsers perform very well on the WSJ development
set. While there are differences in the accuracies,
all scores fall within a close range. The table also
confirms the commonly known fact (Yamada and
Matsumoto, 2003; McDonald et al, 2005) that con-
stituency parsers are more accurate at producing de-
pendencies than dependency parsers (at least when
the dependencies were produced by a deterministic
transformation of a constituency treebank, as is the
case here).
This picture changes drastically when the per-
formance is measured on the QuestionBank devel-
opment set (right columns in Table 1). As one
707
Evaluating on Training on WSJ + QB Training on QuestionBank
QuestionBank F1 UAS LAS POS F1 UAS LAS POS
Nivre et al (2007) ? 83.54 78.85 91.32 ? 79.72 73.44 88.80
McDonald et al (2006) ? 84.95 80.17 91.32 ? 82.52 77.20 88.80
Charniak (2000) 89.40 90.30 85.01 94.17 79.70 76.69 69.69 87.84
Petrov et al (2006) 90.96 90.98 86.90 94.01 86.62 84.09 78.92 87.56
Petrov (2010) 92.81 92.23 88.84 94.48 87.72 85.07 80.08 87.79
Our shift-reduce parser ? 83.70 78.27 91.32 ? 80.44 74.29 88.80
Our shift-reduce parser (gold POS) ? 89.39 86.60 100.00 ? 87.31 84.15 100.00
Table 2: Parsing accuracies for parsers trained on newswire and question data and evaluated on a question test set.
might have expected, the accuracies are significantly
lower, however, the drop for some of the parsers
is shocking. Most notably, the deterministic shift-
reduce parsers lose almost 25% (absolute) on la-
beled accuracies, while the latent variable parsers
lose around 12%.3 Note also that even with gold
POS tags, LAS is below 70% for our determinis-
tic shift-reduce parser, suggesting that the drop in
accuracy is primarily due to a syntactic shift rather
than a lexical shift. These low accuracies are espe-
cially disturbing when one considers that the aver-
age question in the evaluation set is only nine words
long and therefore potentially much less ambiguous
than WSJ sentences. We will examine the main error
types more carefully in Section 5.
Overall, the dependency parsers seem to suf-
fer more from the domain change than the con-
stituency parsers. One possible explanation is that
they lack the global constraints that are enforced by
the (context-free) grammars. Even though the Mst-
Parser finds the globally best spanning tree, all con-
straints are local. This means for example, that it
is not possible to require the final parse to contain
a verb (something that can be easily expressed by
a top-level production of the form S ? NP VP in a
context free grammar). This is not a limitation of de-
pendency parsers in general. For example, it would
be easy to enforce such constraints in the Eisner
(1996) algorithm or using Integer Linear Program-
ming approaches (Riedel and Clarke, 2006; Martins
et al, 2009). However, such richer modeling capac-
ity comes with a much higher computational cost.
Looking at the constituency parsers, we observe
3The difference between our shift-reduce parser and the
MaltParser are due to small differences in the feature sets.
that the lexicalized (reranking) parser of Charniak
and Johnson (2005) loses more than the latent vari-
able approach of Petrov et al (2006). This differ-
ence doesn?t seem to be a difference of generative
vs. discriminative estimation. We suspect that the
latent variable approach is better able to utilize the
little evidence in the training data. Intuitively speak-
ing, some of the latent variables seem to get alo-
cated for modeling the few questions present in the
training data, while the lexicalization contexts are
not able to distinguish between declarative sentences
and questions.
To verify this hypothesis, we conducted two addi-
tional experiments. In the first experiment, we col-
lapsed the question specific phrasal categories SQ
and SBARQ to their declarative sentence equivalents
S and SBAR. When the training and test data are
processed this way, the lexicalized parser loses 1.5%
F1, while the latent variable parser loses only 0.7%.
It is difficult to examine the grammars, but one can
speculate that some of the latent variables were used
to model the question specific constructions and the
model was able to re-learn the distinctions that we
purposefully collapsed. In the second experiment,
we removed all questions from the WSJ training set
and retrained both parsers. This did not make a
significant difference when evaluating on the WSJ
development set, but of course resulted in a large
performance drop when evaluating on the Question-
Bank. The lexicalized parser came out ahead in this
experiment,4 confirming our hypothesis that the la-
tent variable model is better able to pick up the small
amount of relevant evidence that is present in the
WSJ training data (rather than being systematically
4The F1 scores were 52.40% vs. 56.39% respectively.
708
better suited for modeling questions).
3.2 Some Labeled Target Domain Data
In the above experiments, we considered a situation
where we have no labeled training data from the tar-
get domain, as will typically be the case. We now
consider a situation where a small amount of labeled
data (2,000 manually parsed sentences) from the do-
main of interest is available for training.
We experimented with two different ways of uti-
lizing this additional training data. In a first experi-
ment, we trained models on the concatenation of the
WSJ and QuestionBank training sets (we did not at-
tempt to weight the different corpora). As Table 2
shows (left columns), even a modest amount of la-
beled data from the target domain can significantly
boost parsing performance, giving double-digit im-
provements in some cases. While not shown in the
table, the parsing accuracies on the WSJ develop-
ment set where largely unaffected by the additional
training data.
Alternatively, one can also train models exclu-
sively on the QuestionBank data, resulting in ques-
tion specific models. The parsing accuracies of
these domain-specific models are shown in the right
columns of Table 2, and are significantly lower than
those of models trained on the concatenated training
sets. They are often times even lower than the results
of parsers trained exclusively on the WSJ, indicating
that 2,000 sentences are not sufficient to train accu-
rate parsers, even for quite narrow domains.
4 Uptraining for Domain-Adaptation
The results in the previous section suggest that
parsers without global constraints have difficul-
ties dealing with the syntactic differences between
declarative sentences and questions. A possible ex-
planation is that similar word configurations can ap-
pear in both types of sentences, but with very differ-
ent syntactic interpretation. Local models without
global constraints are therefore mislead into dead-
end interpretations from which they cannot recover
(McDonald and Nivre, 2007). Our approach will
therefore be to use a large amount of unlabeled data
to bias the model towards the appropriate distribu-
tion for the target domain. Rather than looking
for feature correspondences between the domains
 70
 75
 80
 85
 90
1M100K10K1K100100
U
A
S
WSJ+QB
WSJ
 60
 65
 70
 75
 80
 85
1M100K10K1K100100
LA
S
Number of unlabeled questions
WSJ+QB
WSJ
Figure 2: Uptraining with large amounts of unlabeled
data gives significant improvements over two different
supervised baselines.
(Blitzer et al, 2006), we propose to use automati-
cally labeled target domain data to learn the target
domain distribution directly.
4.1 Uptraining vs. Self-training
The idea of training parsers on their own output has
been around for as long as there have been statis-
tical parsers, but typically does not work well at
all (Charniak, 1997). Steedman et al (2003) and
Clark et al (2003) present co-training procedures
for parsers and taggers respectively, which are ef-
fective when only very little labeled data is avail-
able. McClosky et al (2006a) were the first to im-
prove a state-of-the-art constituency parsing system
by utilizing unlabeled data for self-training. In sub-
sequent work, they show that the same idea can be
used for domain adaptation if the unlabeled data is
chosen accordingly (McClosky et al, 2006b). Sagae
and Tsujii (2007) co-train two dependency parsers
by adding automatically parsed sentences for which
the parsers agree to the training data. Finally, Suzuki
et al (2009) present a very effective semi-supervised
approach in which features from multiple generative
models estimated on unlabeled data are combined in
a discriminative system for structured prediction.
All of these approaches have in common that their
ultimate goal is to improve the final performance.
Our work differs in that instead of improving the
709
Uptraining with Using only WSJ data Using WSJ + QB data
different base parsers UAS LAS POS UAS LAS POS
Baseline 72.23 60.06 88.48 83.70 78.27 91.32
Self-training 73.62 61.63 89.60 84.26 79.15 92.09
Uptraining on Petrov et al (2006) 86.02 76.94 90.75 88.38 84.02 93.63
Uptraining on Petrov (2010) 85.21 76.19 90.74 88.63 84.14 93.53
Table 3: Uptraining substantially improves parsing accuracies, while self-training gives only minor improvements.
performance of the best parser, we want to build
a more efficient parser that comes close to the ac-
curacy of the best parser. To do this, we parse
the unlabeled data with our most accurate parser
and generate noisy, but fairly accurate labels (parse
trees) for the unlabeled data. We refer to the parser
used for producing the automatic labels as the base
parser (unless otherwise noted, we used the latent
variable parser of Petrov et al (2006) as our base
parser). Because the most accurate base parsers are
constituency parsers, we need to convert the parse
trees to dependencies using the Stanford converter
(see Section 2). The automatically parsed sentences
are appended to the labeled training data, and the
shift-reduce parser (and the part-of-speech tagger)
are trained on this new training set. We did not
increase the weight of the WSJ training data, but
weighted the QuestionBank training data by a fac-
tor of ten in the WSJ+QB experiments.
4.2 Varying amounts of unlabeled data
Figure 2 shows the efficacy of uptraining as a func-
tion of the size of the unlabeled data. Both la-
beled (LAS) and unlabeled accuracies (UAS) im-
prove sharply when automatically parsed sentences
from the target domain are added to the training data,
and level off after 100,000 sentences. Comparing
the end-points of the dashed lines (models having
access only to labeled data from the WSJ) and the
starting points of the solid lines (models that have
access to both WSJ and QuestionBank), one can see
that roughly the same improvements (from 72% to
86% UAS and from 60% to 77% LAS) can be ob-
tained by having access to 2,000 labeled sentences
from the target domain or uptraining with a large
amount of unlabeled data from the target domain.
The benefits seem to be complementary and can be
combined to give the best results. The final accu-
racy of 88.63 / 84.14 (UAS / LAS) on the question
evaluation set is comparable to the in-domain per-
formance on newswire data (88.24 / 84.69).
4.3 Varying the base parser
Table 3 then compares uptraining on the output of
different base parsers to pure self-training. In these
experiments, the same set of 500,000 questions was
parsed by different base parsers. The automatic
parses were then added to the labeled training data
and the parser was retrained. As the results show,
self-training provides only modest improvements of
less than 2%, while uptraining gives double-digit
improvements in some cases. Interestingly, there
seems to be no substantial difference between up-
training on the output of a single latent variable
parser (Petrov et al, 2006) and a product of latent
variable grammars (Petrov, 2010). It appears that
the roughly 1% accuracy difference between the two
base parsers is not important for uptraining.
4.4 POS-less parsing
Our uptraining procedure improves parse quality on
out-of-domain data to the level of in-domain ac-
curacy. However, looking closer at Table 3, one
can see that the POS accuracy is still relatively low
(93.53%), potentially limiting the final accuracy.
To remove this limitation (and also the depen-
dence on a separate POS tagger), we experimented
with word cluster features. As shown in Koo et al
(2008), word cluster features can be used in con-
junction with POS tags to improve parsing accuracy.
Here, we use them instead of POS tags in order to
further reduce the domain-dependence of our model.
Similar to Koo et al (2008), we use the Brown clus-
tering algorithm (Brown et al, 1992) to produce a
deterministic hierarchical clustering of our input vo-
cabulary. We then extract features based on vary-
710
UAS LAS POS
Part-of-Speech Tags 88.35 84.05 93.53
Word Cluster Features 87.92 83.73 ?
Table 4: Parsing accuracies of uptrained parsers with and
without part-of-speech tags and word cluster features.
ing cluster granularities (6 and 10 bits in our experi-
ments). Table 4 shows that roughly the same level of
accuracy can be achieved with cluster based features
instead of POS tag features. This change makes our
parser completely deterministic and enables us to
process sentences in a single left-to-right pass.
5 Error Analysis
To provide a better understanding of the challenges
involved in parsing questions, we analyzed the er-
rors made by our WSJ-trained shift-reduce parser
and also compared them to the errors that are left
after uptraining.
5.1 POS errors
Many parsing errors can be traced back to POS tag-
ging errors, which are much more frequent on out-
of-domain data than on in-domain data (88.8% on
the question data compared to above 95.0% on WSJ
data). Part of the reason for the lower POS tagging
accuracy is the higher unknown word ratio (7.3% on
the question evaluation set, compared to 3.4% on the
WSJ evaluation set). Another reason is a change in
the lexical distribution.
For example, wh-determiners (WDT) are quite
rare in the WSJ training data (relative frequency
0.45%), but five times more common in the Ques-
tionBank training data (2.49%). In addition to this
frequency difference, 52.43% of the WDTs in the
WSJ are the word ?which? and 46.97% are?that?. In
the QuestionBank on the other hand, ?what? is by
far the most common WDT word (81.40%), while
?which? and ?that? account only for 13.65% and
4.94% respectively. Not surprisingly the most com-
mon POS error involves wh-determiners (typically
the word ?what?) being incorrectly labeled as Wh-
pronouns (WP), resulting in head and label errors
like the one shown in Figure 3(a).
To separate out POS tagging errors from parsing
errors, we also ran experiments with correct (gold)
Dep. Label Frequency WSJ Uptrained
nsubj 934 41.02 88.64
amod 556 78.21 86.00
dobj 555 70.10 83.12
attr 471 8.64 93.49
aux 467 77.31 82.56
Table 5: F1 scores for the most frequent labels in the
QuestionBank development set. Uptraining leads to huge
improvements compared to training only on the WSJ.
POS tags. The parsing accuracies of our shift-reduce
parser using gold POS tags are listed in the last rows
of Tables 1 and 2. Even with gold POS tags, the de-
terministic shift-reduce parser falls short of the ac-
curacies of the constituency parsers (with automatic
tags), presumably because the shift-reduce model is
making only local decisions and is lacking the global
constraints provided by the context-free grammar.
5.2 Dependency errors
To find the main error types, we looked at the most
frequent labels in the QuestionBank development
set, and analyzed the ones that benefited the most
from uptraining. Table 5 has the frequency and F-
scores of the dependency types that we are going to
discuss in the following. We also provide examples
which are illustrated in Figure 3.
nsubj: The WSJ-trained model is often producing
parses that are missing a subject (nsubj). Questions
like ?What is the oldest profession?? and ?When
was Ozzy Osbourne born?? should have ?profes-
sion? and ?Osbourne? as nsubjs, but in both cases
the WSJ-trained parser did not label any subj (see
Figures 3(b) and 3(c)). Another common error is to
mislabel nsubj. For example, the nsubj of ?What are
liver enzymes?? should be enzymes, but the WSJ-
trained parser labels ?What? as the nsubj, which
makes sense in a statement but not in a question.
amod: The model is overpredicting ?amod?, re-
sulting in low precision figures for this label. An
example is ?How many points make up a perfect
fivepin bowling score??. The Stanford dependency
uses ?How? as the head of ?many? in noun phrases
like ?How many points?, and the relation is a generic
?dep?. But in the WSJ model prediction, ?many?s?
head is ?points,? and the relation mislabeled as
amod. Since it?s an adjective preceding the noun,
711
What is the oldest profession ?
ROOT
det     amod proot attr nsubj
WP VBZ DT JJS NN .ROOT
det     amod proot   attrdep
WP VBZ DT JJS NN .
What is the oldest profession ?
When was Ozzy Osbourne born ?
ROOT WRB VBZ  NNP NNP VBN .
root padvmod aux nn nsubj
When was Ozzy Osbourne   born ?
ROOT WRB VBZ  NNP NNP   NNP .
   root    nnnn pcompl nsubj
What films featured the character ?
ROOT WDT NNS  VBD DT NN  NNP NNP .
Popeye Doyle
nsubj dep det    nn nn dobj
What films featured the character ?
ROOT WP NNS  VBD DT NN  NNP NNP .
Popeye Doyle
   nsubj    compl det    nn nn   root    ccomp(a)
(b)
(c)
(d)
How many people did Randy ?
ROOT WRB JJ  NNS VBD NNP  NNP VB .
Craft kill
  dobj dep aux    nn nsubj p
?
.
dep
How many people did Randy
ROOT WRB JJ  NNS VBD NNP  NNP VB
Craft kill
compl amod ccomp   nn nsubj pnsubjroot
Figure 3: Example questions from the QuestionBank development set and their correct parses (left), as well as the
predictions of a model trained on the WSJ (right).
the WSJ model often makes this mistake and there-
fore the precision is much lower when it doesn?t see
more questions in the training data.
dobj: The WSJ model doesn?t predict object ex-
traction well. For example, in ?How many people
did Randy Craft kill?? (Figure 3(d)), the direct ob-
ject of kill should be ?How many people.? In the
Stanford dependencies, the correct labels for this
noun phrase are ?dobj dep dep,? but the WSJ model
predicts ?compl amod nsubj.? This is a common
error caused by the different word order in ques-
tions. The uptrained model is much better at han-
dling these type of constructions.
attr: An attr (attributive) is a wh-noun phrase
(WHNP) complement of a copular verb. In the WSJ
training data, only 4,641 out of 950,028 dependen-
cies are attr (0.5%); in the QuestionBank training
data, 1,023 out of 17,069 (6.0%) are attr. As a con-
sequence, the WSJ model cannot predict this label
in questions very well.
aux: ?What does the abbreviation AIDS stand
for?? should have ?stand? as the main head of the
sentence, and ?does? as its aux. However, the WSJ
model labeled ?does? as the main head. Similar
patterns occur in many questions, and therefore the
WSJ has a very low recall rate.
In contrast, mostly local labels (that are not re-
lated to question/statement structure differences)
have a consistently high accuracy. For example: det
has an accuracy of 98.86% with the WSJ-trained
model, and 99.24% with the uptrained model.
6 Conclusions
We presented a method for domain adaptation of de-
terministic shift-reduce parsers. We evaluated mul-
tiple state-of-the-art parsers on a question corpus
and showed that parsing accuracies degrade substan-
tially on this out-of-domain task. Most notably, de-
terministic shift-reduce parsers have difficulty deal-
ing with the modified word order and lose more
than 20% in accuracy. We then proposed a simple,
yet very effective uptraining method for domain-
adaptation. In a nutshell, we trained a deterministic
shift-reduce parser on the output of a more accurate,
but slower parser. Uptraining with large amounts of
unlabeled data gives similar improvements as hav-
ing access to 2,000 labeled sentences from the target
domain. With 2,000 labeled questions and a large
amount of unlabeled questions, uptraining is able to
close the gap between in-domain and out-of-domain
accuracy.
712
Acknowledgements
We would like to thank Ryan McDonald for run-
ning the MstParser experiments and for many fruit-
ful discussions on this topic. We would also like to
thank Joakim Nivre for help with the MatlParser and
Marie-Catherine de Marneffe for help with the Stan-
ford Dependency Converter.
References
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
EMNLP ?06.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT ?98.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP ?00.
P. Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mer-
cer. 1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming, and the perceptron for efficient,
feature-rich parsing. In CoNLL ?08.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-
Best Parsing and MaxEnt Discriminative Reranking.
In ACL?05.
E. Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In AI ?97.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00.
S. Clark, J. Curran, and M. Osborne. 2003. Bootstrap-
ping pos-taggers using unlabelled data. In CoNLL ?03.
M.-C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC ?06.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In COLING ?96.
J. Foster. 2010. ?cba to check the spelling?: Investigat-
ing parser performance on discussion forum posts. In
NAACL ?10.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP ?01.
Z. Huang and M. Harper. 2009. Self-training PCFG
grammars with latent annotations across languages. In
EMNLP ?09.
J. Judge, A. Cahill, and J. v. Genabith. 2006. Question-
bank: creating a corpus of parse-annotated questions.
In ACL ?06.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In ACL ?10.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL ?08.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
A.F.T. Martins, N.A. Smith, and E.P. Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In ACL ?09.
D. McClosky, E. Charniak, and M. Johnson. 2006a. Ef-
fective self-training for parsing. In NAACL ?06.
D. McClosky, E. Charniak, and M. Johnson. 2006b.
Reranking and self-training for parser adaptation. In
ACL ?06.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
EMNLP ?07.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL
?05.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In CoNLL ?06.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Kbler, S. Marinov, and E. Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2).
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
S. Petrov. 2010. Products of random latent variable
grammars. In NAACL ?10.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP ?06.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In NAACL ?06.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with lr models and parser ensem-
bles. In CoNLL ?07.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In EACL ?03.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In EMNLP
?09.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In IWPT
?03.
713
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1269?1280,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Lateen EM: Unsupervised Training with Multiple Objectives,
Applied to Dependency Grammar Induction
Valentin I. Spitkovsky
Computer Science Department
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc.
Mountain View, CA, 94043, USA
hiyan@google.com
Daniel Jurafsky
Departments of Linguistics and Computer Science
Stanford University, Stanford, CA, 94305, USA
jurafsky@stanford.edu
Abstract
We present new training methods that aim to
mitigate local optima and slow convergence in
unsupervised training by using additional im-
perfect objectives. In its simplest form, lateen
EM alternates between the two objectives of
ordinary ?soft? and ?hard? expectation max-
imization (EM) algorithms. Switching objec-
tives when stuck can help escape local optima.
We find that applying a single such alternation
already yields state-of-the-art results for En-
glish dependency grammar induction. More
elaborate lateen strategies track both objec-
tives, with each validating the moves proposed
by the other. Disagreements can signal earlier
opportunities to switch or terminate, saving it-
erations. De-emphasizing fixed points in these
ways eliminates some guesswork from tuning
EM. An evaluation against a suite of unsu-
pervised dependency parsing tasks, for a vari-
ety of languages, showed that lateen strategies
significantly speed up training of both EM al-
gorithms, and improve accuracy for hard EM.
1 Introduction
Expectation maximization (EM) algorithms (Demp-
ster et al, 1977) play important roles in learning
latent linguistic structure. Unsupervised techniques
from this family excel at core natural language pro-
cessing (NLP) tasks, including segmentation, align-
ment, tagging and parsing. Typical implementations
specify a probabilistic framework, pick an initial
model instance, and iteratively improve parameters
using EM. A key guarantee is that subsequent model
instances are no worse than the previous, according
to training data likelihood in the given framework.
Another attractive feature that helped make EM
instrumental (Meng, 2007) is its initial efficiency:
Training tends to begin with large steps in a param-
eter space, sometimes bypassing many local optima
at once. After a modest number of such iterations,
however, EM lands close to an attractor. Next, its
convergence rate necessarily suffers: Disproportion-
ately many (and ever-smaller) steps are needed to
finally approach this fixed point, which is almost in-
variably a local optimum. Deciding when to termi-
nate EM often involves guesswork; and finding ways
out of local optima requires trial and error. We pro-
pose several strategies that address both limitations.
Unsupervised objectives are, at best, loosely cor-
related with extrinsic performance (Pereira and Sch-
abes, 1992; Merialdo, 1994; Liang and Klein, 2008,
inter alia). This fact justifies (occasionally) devi-
ating from a prescribed training course. For exam-
ple, since multiple equi-plausible objectives are usu-
ally available, a learner could cycle through them,
optimizing alternatives when the primary objective
function gets stuck; or, instead of trying to escape, it
could aim to avoid local optima in the first place, by
halting search early if an improvement to one objec-
tive would come at the expense of harming another.
We test these general ideas by focusing on non-
convex likelihood optimization using EM. This set-
ting is standard and has natural and well-understood
objectives: the classic, ?soft? EM; and Viterbi, or
?hard? EM (Kearns et al, 1997). The name ?la-
teen? comes from the sea ? triangular lateen sails
can take wind on either side, enabling sailing ves-
sels to tack (see Figure 1). As a captain can?t count
on favorable winds, so an unsupervised learner can?t
rely on co-operative gradients: soft EM maximizes
1269
Figure 1: A triangular sail atop a traditional Arab sail-
ing vessel, the dhow (right). Older square sails permitted
sailing only before the wind. But the efficient lateen sail
worked like a wing (with high pressure on one side and
low pressure on the other), allowing a ship to go almost
directly into a headwind. By tacking, in a zig-zag pattern,
it became possible to sail in any direction, provided there
was some wind at all (left). For centuries seafarers ex-
pertly combined both sails to traverse extensive distances,
greatly increasing the reach of medieval navigation.1
likelihoods of observed data across assignments to
hidden variables, whereas hard EM focuses on most
likely completions.2 These objectives are plausible,
yet both can be provably ?wrong? (Spitkovsky et al,
2010a, ?7.3). Thus, it is permissible for lateen EM
to maneuver between their gradients, for example by
tacking around local attractors, in a zig-zag fashion.
2 The Lateen Family of Algorithms
We propose several strategies that use a secondary
objective to improve over standard EM training. For
hard EM, the secondary objective is that of soft EM;
and vice versa if soft EM is the primary algorithm.
2.1 Algorithm #1: Simple Lateen EM
Simple lateen EM begins by running standard EM
to convergence, using a user-supplied initial model,
primary objective and definition of convergence.
Next, the algorithm alternates. A single lateen al-
ternation involves two phases: (i) retraining using
the secondary objective, starting from the previ-
ous converged solution (once again iterating until
convergence, but now of the secondary objective);
1Partially adapted from http://www.britannica.com/
EBchecked/topic/331395, http://allitera.tive.org/
archives/004922.html and http://landscapedvd.com/
desktops/images/ship1280x1024.jpg.
2See Brown et al?s (1993, ?6.2) definition of Viterbi train-
ing for a succinct justification of hard EM; in our case, the cor-
responding objective is Spitkovsky et al?s (2010a, ?7.1) ??VIT.
and (ii) retraining using the primary objective again,
starting from the latest converged solution (once
more to convergence of the primary objective). The
algorithm stops upon failing to sufficiently improve
the primary objective across alternations (applying
the standard convergence criterion end-to-end) and
returns the best of all models re-estimated during
training (as judged by the primary objective).
2.2 Algorithm #2: Shallow Lateen EM
Same as algorithm #1, but switches back to optimiz-
ing the primary objective after a single step with the
secondary, during phase (i) of all lateen alternations.
Thus, the algorithm alternates between optimizing
a primary objective to convergence, then stepping
away, using one iteration of the secondary optimizer.
2.3 Algorithm #3: Early-Stopping Lateen EM
This variant runs standard EM but quits early if
the secondary objective suffers. We redefine con-
vergence by ?or?-ing the user-supplied termination
criterion (i.e., a ?small-enough? change in the pri-
mary objective) with any adverse change of the sec-
ondary (i.e., an increase in its cross-entropy). Early-
stopping lateen EM does not alternate objectives.
2.4 Algorithm #4: Early-Switching Lateen EM
Same as algorithm #1, but with the new definition
of convergence, as in algorithm #3. Early-switching
lateen EM halts primary optimizers as soon as they
hurt the secondary objective and stops secondary op-
timizers once they harm the primary objective. This
algorithm terminates when it fails to sufficiently im-
prove the primary objective across a full alternation.
2.5 Algorithm #5: Partly-Switching Lateen EM
Same as algorithm #4, but again iterating primary
objectives to convergence, as in algorithm #1; sec-
ondary optimizers still continue to terminate early.
3 The Task and Study #1
We chose to test the impact of these five lateen al-
gorithms on unsupervised dependency parsing ? a
task in which EM plays an important role (Paskin,
2001; Klein and Manning, 2004; Gillenwater et al,
2010, inter alia). This entailed two sets of exper-
iments: In study #1, we tested whether single al-
ternations of simple lateen EM (as defined in ?2.1,
1270
System DDA (%)
(Blunsom and Cohn, 2010) 55.7
(Gillenwater et al, 2010) 53.3
(Spitkovsky et al, 2010b) 50.4
+ soft EM + hard EM 52.8 (+2.4)
lexicalized, using hard EM 54.3 (+1.5)
+ soft EM + hard EM 55.6 (+1.3)
Table 1: Directed dependency accuracies (DDA) on Sec-
tion 23 of WSJ (all sentences) for recent state-of-the-art
systems and our two experiments (one unlexicalized and
one lexicalized) with a single alternation of lateen EM.
Algorithm #1) improve our recent publicly-available
system for English dependency grammar induction.
In study #2, we introduced a more sophisticated
methodology that uses factorial designs and regres-
sions to evaluate lateen strategies with unsupervised
dependency parsing in many languages, after also
controlling for other important sources of variation.
For study #1, our base system (Spitkovsky et al,
2010b) is an instance of the popular (unlexicalized)
Dependency Model with Valence (Klein and Man-
ning, 2004). This model was trained using hard EM
on WSJ45 (WSJ sentences up to length 45) until suc-
cessive changes in per-token cross-entropy fell be-
low 2?20 bits (Spitkovsky et al, 2010b; 2010a, ?4).3
We confirmed that the base model had indeed con-
verged, by running 10 steps of hard EM on WSJ45
and verifying that its objective did not change much.
Next, we applied a single alternation of simple la-
teen EM: first running soft EM (this took 101 steps,
using the same termination criterion), followed by
hard EM (again to convergence ? another 23 it-
erations). The result was a decrease in hard EM?s
cross-entropy, from 3.69 to 3.59 bits per token (bpt),
accompanied by a 2.4% jump in accuracy, from 50.4
to 52.8%, on Section 23 of WSJ (see Table 1).4
Our first experiment showed that lateen EM holds
promise for simple models. Next, we tested it in
a more realistic setting, by re-estimating lexicalized
models,5 starting from the unlexicalized model?s
3http://nlp.stanford.edu/pubs/
markup-data.tar.bz2: dp.model.dmv
4It is standard practice to convert gold labeled constituents
from Penn English Treebank?s Wall Street Journal (WSJ) por-
tion (Marcus et al, 1993) into unlabeled reference dependency
parses using deterministic ?head-percolation? rules (Collins,
1999); sentence root symbols (but not punctuation) arcs count
towards accuracies (Paskin, 2001; Klein and Manning, 2004).
5We used Headden et al?s (2009) method (also the approach
parses; this took 24 steps with hard EM. We then
applied another single lateen alternation: This time,
soft EM ran for 37 steps, hard EM took another 14,
and the new model again improved, by 1.3%, from
54.3 to 55.6% (see Table 1); the corresponding drop
in (lexicalized) cross-entropy was from 6.10 to 6.09
bpt. This last model is competitive with the state-of-
the-art; moreover, gains from single applications of
simple lateen alternations (2.4 and 1.3%) are on par
with the increase due to lexicalization alone (1.5%).
4 Methodology for Study #2
Study #1 suggests that lateen EM can improve gram-
mar induction in English. To establish statistical sig-
nificance, however, it is important to test a hypothe-
sis in many settings (Ioannidis, 2005). We therefore
use a factorial experimental design and regression
analyses with a variety of lateen strategies. Two re-
gressions ? one predicting accuracy, the other, the
number of iterations ? capture the effects that la-
teen algorithms have on performance and efficiency,
relative to standard EM training. We controlled for
important dimensions of variation, such as the un-
derlying language: to make sure that our results are
not English-specific, we induced grammars in 19
languages. We also explored the impact from the
quality of an initial model (using both uniform and
ad hoc initializers), the choice of a primary objective
(i.e., soft or hard EM), and the quantity and com-
plexity of training data (shorter versus both short and
long sentences). Appendix A gives the full details.
4.1 Data Sets
We use all 23 train/test splits from the 2006/7
CoNLL shared tasks (Buchholz and Marsi, 2006;
Nivre et al, 2007),6 which cover 19 different lan-
guages.7 We splice out all punctuation labeled in the
data, as is standard practice (Paskin, 2001; Klein and
Manning, 2004), introducing new arcs from grand-
mothers to grand-daughters where necessary, both in
train- and test-sets. Evaluation is always against the
taken by the two stronger state-of-the-art systems): for words
seen at least 100 times in the training corpus, gold part-of-
speech tags are augmented with lexical items.
6These disjoint splits require smoothing; in the WSJ setting,
training and test sets overlapped (Klein and Manning, 2004).
7We down-weigh languages appearing in both years ? Ara-
bic, Chinese, Czech and Turkish ? by 50% in all our analyses.
1271
entire resulting test sets (i.e., all sentence lengths).8
4.2 Grammar Models
In all remaining experiments we model grammars
via the original DMV, which ignores punctuation; all
models are unlexicalized, with gold part-of-speech
tags for word classes (Klein and Manning, 2004).
4.3 Smoothing Mechanism
All unsmoothed models are smoothed immediately
prior to evaluation; some of the baseline models are
also smoothed during training. In both cases, we use
the ?add-one? (a.k.a. Laplace) smoothing algorithm.
4.4 Standard Convergence
We always halt an optimizer once a change in its ob-
jective?s consecutive cross-entropy values falls be-
low 2?20 bpt (at which point we consider it ?stuck?).
4.5 Scoring Function
We report directed accuracies ? fractions of cor-
rectly guessed (unlabeled) dependency arcs, includ-
ing arcs from sentence root symbols, as is standard
practice (Paskin, 2001; Klein and Manning, 2004).
Punctuation does not affect scoring, as it had been
removed from all parse trees in our data (see ?4.1).
5 Experiments
We now summarize our baseline models and briefly
review the proposed lateen algorithms. For details of
the default systems (standard soft and hard EM), all
control variables and both regressions (against final
accuracies and iteration counts) see Appendix A.
5.1 Baseline Models
We tested a total of six baseline models, experiment-
ing with two types of alternatives: (i) strategies that
perturb stuck models directly, by smoothing, ignor-
ing secondary objectives; and (ii) shallow applica-
tions of a single EM step, ignoring convergence.
Baseline B1 alternates running standard EM to
convergence and smoothing. A second baseline, B2,
smooths after every step of EM instead. Another
shallow baseline, B3, alternates single steps of soft
8With the exception of Arabic ?07, from which we discarded
a single sentence containing 145 non-punctuation tokens.
and hard EM.9 Three such baselines begin with hard
EM (marked with the subscript h); and three more
start with soft EM (marked with the subscript s).
5.2 Lateen Models
Ten models, A{1, 2, 3, 4, 5}{h,s}, correspond to our la-
teen algorithms #1?5 (?2), starting with either hard
or soft EM?s objective, to be used as the primary.
6 Results
Soft EM Hard EM
Model ?a ?i ?a ?i
Baselines B3 -2.7 ?0.2 -2.0 ?0.3
B2 +0.6 ?0.7 +0.6 ?1.2
B1 0.0 ?2.0 +0.8 ?3.7
Algorithms A1 0.0 ?1.3 +5.5 ?6.5
A2 -0.0 ?1.3 +1.5 ?3.6
A3 0.0 ?0.7 -0.1 ?0.7
A4 0.0 ?0.8 +3.0 ?2.1
A5 0.0 ?1.2 +2.9 ?3.8
Table 2: Estimated additive changes in directed depen-
dency accuracy (?a) and multiplicative changes in the
number of iterations before terminating (?i) for all base-
line models and lateen algorithms, relative to standard
training: soft EM (left) and hard EM (right). Bold en-
tries are statistically different (p < 0.01) from zero, for
?a, and one, for ?i (details in Table 4 and Appendix A).
Not one baseline attained a statistically significant
performance improvement. Shallow models B3{h,s},
in fact, significantly lowered accuracy: by 2.0%, on
average (p ? 7.8 ? 10?4), for B3h, which began with
hard EM; and down 2.7% on average (p ? 6.4?10?7),
for B3s, started with soft EM. They were, however,
3?5x faster than standard training, on average (see
Table 4 for all estimates and associated p-values;
above, Table 2 shows a preview of the full results).
6.1 A1{h,s} ? Simple Lateen EM
A1h runs 6.5x slower, but scores 5.5% higher, on av-
erage, compared to standard Viterbi training; A1s is
only 30% slower than standard soft EM, but does not
impact its accuracy at all, on average.
Figure 2 depicts a sample training run: Italian ?07
with A1h. Viterbi EM converges after 47 iterations,
9It approximates a mixture (the average of soft and hard
objectives) ? a natural comparison, computable via gradients
and standard optimization algorithms, such as L-BFGS (Liu and
Nocedal, 1989). We did not explore exact interpolations, how-
ever, because replacing EM is itself a significant confounder,
even with unchanged objectives (Berg-Kirkpatrick et al, 2010).
1272
50 100 150 200 250 300
3.0
3.5
4.0
4.5
3.39
3.26
(3.42)
(3.19)
3.33
3.23
(3.39)
(3.18)
3.29
3.21
(3.39)
(3.18)
3.29
3.22
bpt
iteration
cross-entropies (in bits per token)
Figure 2: Cross-entropies for Italian ?07, initialized uni-
formly and trained on sentences up to length 45. The two
curves are primary and secondary objectives (soft EM?s
lies below, as sentence yields are at least as likely as parse
trees): shaded regions indicate iterations of hard EM (pri-
mary); and annotated values are measurements upon each
optimizer?s convergence (soft EM?s are parenthesized).
reducing the primary objective to 3.39 bpt (the sec-
ondary is then at 3.26); accuracy on the held-out set
is 41.8%. Three alternations of lateen EM (totaling
265 iterations) further decrease the primary objec-
tive to 3.29 bpt (the secondary also declines, to 3.22)
and accuracy increases to 56.2% (14.4% higher).
6.2 A2{h,s} ? Shallow Lateen EM
A2h runs 3.6x slower, but scores only 1.5% higher,
on average, compared to standard Viterbi training;
A2s is again 30% slower than standard soft EM and
also has no measurable impact on parsing accuracy.
6.3 A3{h,s} ? Early-Stopping Lateen EM
Both A3h and A3s run 30% faster, on average, than
standard training with hard or soft EM; and neither
heuristic causes a statistical change to accuracy.
Table 3 shows accuracies and iteration counts for
10 (of 23) train/test splits that terminate early with
A3s (in one particular, example setting). These runs
are nearly twice as fast, and only two score (slightly)
lower, compared to standard training using soft EM.
6.4 A4{h,s} ? Early-Switching Lateen EM
A4h runs only 2.1x slower, but scores only 3.0%
higher, on average, compared to standard Viterbi
training; A4s is, in fact, 20% faster than standard soft
EM, but still has no measurable impact on accuracy.
6.5 A5{h,s} ? Partly-Switching Lateen EM
A5h runs 3.8x slower, scoring 2.9% higher, on av-
erage, compared to standard Viterbi training; A5s is
20% slower than soft EM, but, again, no more accu-
rate. Indeed, A4 strictly dominates both A5 variants.
CoNLL Year Soft EM A3s
& Language DDA iters DDA iters
Arabic 2006 28.4 180 28.4 118
Bulgarian ?06 39.1 253 39.6 131
Chinese ?06 49.4 268 49.4 204
Dutch ?06 21.3 246 27.8 35
Hungarian ?07 17.1 366 17.4 213
Italian ?07 39.6 194 39.6 164
Japanese ?06 56.6 113 56.6 93
Portuguese ?06 37.9 180 37.5 102
Slovenian ?06 30.8 234 31.1 118
Spanish ?06 33.3 125 33.1 73
Average: 35.4 216 36.1 125
Table 3: Directed dependency accuracies (DDA) and iter-
ation counts for the 10 (of 23) train/test splits affected by
early termination (setting: soft EM?s primary objective,
trained using shorter sentences and ad-hoc initialization).
7 Discussion
Lateen strategies improve dependency grammar in-
duction in several ways. Early stopping offers a
clear benefit: 30% higher efficiency yet same perfor-
mance as standard training. This technique could be
used to (more) fairly compare learners with radically
different objectives (e.g., lexicalized and unlexical-
ized), requiring quite different numbers of steps (or
magnitude changes in cross-entropy) to converge.
The second benefit is improved performance, but
only starting with hard EM. Initial local optima dis-
covered by soft EM are such that the impact on ac-
curacy of all subsequent heuristics is indistinguish-
able from noise (it?s not even negative). But for hard
EM, lateen strategies consistently improve accuracy
? by 1.5, 3.0 or 5.5% ? as an algorithm follows the
secondary objective longer (a single step, until the
primary objective gets worse, or to convergence).
Our results suggest that soft EM should use early
termination to improve efficiency. Hard EM, by con-
trast, could use any lateen strategy to improve either
efficiency or performance, or to strike a balance.
8 Related Work
8.1 Avoiding and/or Escaping Local Attractors
Simple lateen EM is similar to Dhillon et al?s (2002)
refinement algorithm for text clustering with spher-
ical k-means. Their ?ping-pong? strategy alternates
batch and incremental EM, exploits the strong points
of each, and improves a shared objective at every
1273
step. Unlike generalized (GEM) variants (Neal and
Hinton, 1999), lateen EM uses multiple objectives:
it sacrifices the primary in the short run, to escape
local optima; in the long run, it also does no harm,
by construction (as it returns the best model seen).
Of the meta-heuristics that use more than a stan-
dard, scalar objective, deterministic annealing (DA)
(Rose, 1998) is closest to lateen EM. DA perturbs
objective functions, instead of manipulating solu-
tions directly. As other continuation methods (All-
gower and Georg, 1990), it optimizes an easy (e.g.,
convex) function first, then ?rides? that optimum by
gradually morphing functions towards the difficult
objective; each step reoptimizes from the previous
approximate solution. Smith and Eisner (2004) em-
ployed DA to improve part-of-speech disambigua-
tion, but found that objectives had to be further
?skewed,? using domain knowledge, before it helped
(constituent) grammar induction. (For this reason,
we did not experiment with DA, despite its strong
similarities to lateen EM.) Smith and Eisner (2004)
used a ?temperature? ? to anneal a flat uniform dis-
tribution (? = 0) into soft EM?s non-convex objec-
tive (? = 1). In their framework, hard EM corre-
sponds to ? ?? ?, so the algorithms differ only in
their ?-schedule: DA?s is continuous, from 0 to 1; la-
teen EM?s is a discrete alternation, of 1 and +?.10
8.2 Terminating Early, Before Convergence
EM is rarely run to (even numerical) convergence.
Fixing a modest number of iterations a priori (Klein,
2005, ?5.3.4), running until successive likelihood ra-
tios become small (Spitkovsky et al, 2009, ?4.1) or
using a combination of the two (Ravi and Knight,
2009, ?4, Footnote 5) is standard practice in NLP.
Elworthy?s (1994, ?5, Figure 1) analysis of part-of-
speech tagging showed that, in most cases, a small
number of iterations is actually preferable to conver-
gence, in terms of final accuracies: ?regularization
by early termination? had been suggested for image
deblurring algorithms in statistical astronomy (Lucy,
1974, ?2); and validation against held-out data ? a
strategy proposed much earlier, in psychology (Lar-
son, 1931), has also been used as a halting crite-
rion in NLP (Yessenalina et al, 2010, ?4.2, 5.2).
10One can think of this as a kind of ?beam search? (Lowerre,
1976), with soft EM expanding and hard EM pruning a frontier.
Early-stopping lateen EM tethers termination to a
sign change in the direction of a secondary objective,
similarly to (cross-)validation (Stone, 1974; Geisser,
1975; Arlot and Celisse, 2010), but without splitting
data ? it trains using all examples, at all times.11,12
8.3 Training with Multiple Views
Lateen strategies may seem conceptually related to
co-training (Blum and Mitchell, 1998). However,
bootstrapping methods generally begin with some
labeled data and gradually label the rest (discrimina-
tively) as they grow more confident, but do not opti-
mize an explicit objective function; EM, on the other
hand, can be fully unsupervised, relabels all exam-
ples on each iteration (generatively), and guarantees
not to hurt a well-defined objective, at every step.13
Co-training classically relies on two views of the
data ? redundant feature sets that allow different al-
gorithms to label examples for each other, yielding
?probably approximately correct? (PAC)-style guar-
antees under certain (strong) assumptions. In con-
trast, lateen EM uses the same data, features, model
and essentially the same algorithms, changing only
their objective functions: it makes no assumptions,
but guarantees not to harm the primary objective.
Some of these distinctions have become blurred
with time: Collins and Singer (1999) introduced
an objective function (also based on agreement)
into co-training; Goldman and Zhou (2000), Ng
and Cardie (2003) and Chan et al (2004) made do
without redundant views; Balcan et al (2004) re-
laxed other strong assumptions; and Zhou and Gold-
man (2004) generalized co-training to accommodate
three and more algorithms. Several such methods
have been applied to dependency parsing (S?gaard
and Rish?j, 2010), constituent parsing (Sarkar,
11We see in it a milder contrastive estimation (Smith and Eis-
ner, 2005a; 2005b), agnostic to implicit negative evidence, but
caring whence learners push probability mass towards training
examples: when most likely parse trees begin to benefit at the
expense of their sentence yields (or vice versa), optimizers halt.
12For a recently proposed instance of EM that uses cross-
validation (CV) to optimize smoothed data likelihoods (in learn-
ing synchronous PCFGs, for phrase-based machine translation),
see Mylonakis and Sima?an?s (2010, ?3.1) CV-EM algorithm.
13Some authors (Nigam and Ghani, 2000; Ng and Cardie,
2003; Smith and Eisner, 2005a, ?5.2, 7; ?2; ?6) draw a hard line
between bootstrapping algorithms, such as self- and co-training,
and probabilistic modeling using EM; others (Dasgupta et al,
2001; Chang et al, 2007, ?1; ?5) tend to lump them together.
1274
2001) and parser reranking (Crim, 2002). Funda-
mentally, co-training exploits redundancies in unla-
beled data and/or learning algorithms. Lateen strate-
gies also exploit redundancies: in noisy objectives.
Both approaches use a second vantage point to im-
prove their perception of difficult training terrains.
9 Conclusions and Future Work
Lateen strategies can improve performance and effi-
ciency for dependency grammar induction with the
DMV. Early-stopping lateen EM is 30% faster than
standard training, without affecting accuracy ? it
reduces guesswork in terminating EM. At the other
extreme, simple lateen EM is slower, but signifi-
cantly improves accuracy ? by 5.5%, on average
? for hard EM, escaping some of its local optima.
It would be interesting to apply lateen algorithms
to advanced parsing models (Blunsom and Cohn,
2010; Headden et al, 2009, inter alia) and learn-
ing algorithms (Gillenwater et al, 2010; Cohen and
Smith, 2009, inter alia). Future work could explore
other NLP tasks ? such as clustering, sequence la-
beling, segmentation and alignment ? that often
employ EM. Our meta-heuristics are multi-faceted,
featuring aspects of iterated local search, determin-
istic annealing, cross-validation, contrastive estima-
tion and co-training. They may be generally useful
in machine learning and non-convex optimization.
Appendix A. Experimental Design
Statistical techniques are vital to many aspects of
computational linguistics (Johnson, 2009; Charniak,
1997; Abney, 1996, inter alia). We used factorial
designs,14 which are standard throughout the natu-
ral and social sciences, to assist with experimental
design and statistical analyses. Combined with or-
dinary regressions, these methods provide succinct
and interpretable summaries that explain which set-
tings meaningfully contribute to changes in depen-
dent variables, such as running time and accuracy.
14We used full factorial designs for clarity of exposition. But
many fewer experiments would suffice, especially in regression
models without interaction terms: for the more efficient frac-
tional factorial designs, as well as for randomized block designs
and full factorial designs, see Montgomery (2005, Ch. 4?9).
9.1 Dependent Variables
We constructed two regressions, for two types of de-
pendent variables: to summarize performance, we
predict accuracies; and to summarize efficiency, we
predict (logarithms of) iterations before termination.
In the performance regression, we used four dif-
ferent scores for the dependent variable. These in-
clude both directed accuracies and undirected accu-
racies, each computed in two ways: (i) using a best
parse tree; and (ii) using all parse trees. These four
types of scores provide different kinds of informa-
tion. Undirected scores ignore polarity of parent-
child relations (Paskin, 2001; Klein and Manning,
2004; Schwartz et al, 2011), partially correcting for
some effects of alternate analyses (e.g., systematic
choices between modals and main verbs for heads
of sentences, determiners for noun phrases, etc.).
And integrated scoring, using the inside-outside al-
gorithm (Baker, 1979) to compute expected accu-
racy across all ? not just best ? parse trees, has the
advantage of incorporating probabilities assigned to
individual arcs: This metric is more sensitive to the
margins that separate best from next-best parse trees,
and is not affected by tie-breaking. We tag scores
using two binary predictors in a simple (first order,
multi-linear) regression, where having multiple rel-
evant quality assessments improves goodness-of-fit.
In the efficiency regression, dependent variables
are logarithms of the numbers of iterations. Wrap-
ping EM in an inner loop of a heuristic has a mul-
tiplicative effect on the total number of models re-
estimated prior to termination. Consequently, loga-
rithms of the final counts better fit the observed data.
9.2 Independent Predictors
All of our predictors are binary indicators (a.k.a.
?dummy? variables). The undirected and integrated
factors only affect the regression for accuracies (see
Table 4, left); remaining factors participate also in
the running times regression (see Table 4, right). In a
default run, all factors are zero, corresponding to the
intercept estimated by a regression; other estimates
reflect changes in the dependent variable associated
with having that factor ?on? instead of ?off.?
? adhoc ? This setting controls initialization.
By default, we use the uninformed uniform ini-
tializer (Spitkovsky et al, 2010a); when it is
1275
Regression for Accuracies Regression for ln(Iterations)
Goodness-of-Fit: (R2adj ? 76.2%) (R2adj ? 82.4%)
Indicator Factors coeff. ?? adj. p-value
undirected 18.1 < 2.0 ? 10?16
integrated -0.9 ? 7.0 ? 10?7 coeff. ?? mult. e?? adj. p-value
(intercept) 30.9 < 2.0 ? 10?16 5.5 255.8 < 2.0 ? 10?16
adhoc 1.2 ? 3.1 ? 10?13 -0.0 1.0 ? 1.0
Model sweet 1.0 ? 3.1 ? 10?9 -0.2 0.8 < 2.0 ? 10?16
B3s shallow (soft-first) -2.7 ? 6.4 ? 10?7 -1.5 0.2 < 2.0 ? 10?16
B3h shallow (hard-first) -2.0 ? 7.8 ? 10?4 -1.2 0.3 < 2.0 ? 10?16
B2s shallow smooth 0.6 ? 1.0 -0.4 0.7 ? 1.4 ? 10?12
B1s smooth 0.0 ? 1.0 0.7 2.0 < 2.0 ? 10?16
A1s simple lateen 0.0 ? 1.0 0.2 1.3 ? 4.1 ? 10?4
A2s shallow lateen -0.0 ? 1.0 0.2 1.3 ? 5.8 ? 10?4
A3s early-stopping lateen 0.0 ? 1.0 -0.3 0.7 ? 2.6 ? 10?7
A4s early-switching lateen 0.0 ? 1.0 -0.3 0.8 ? 2.6 ? 10?7
A5s partly-switching lateen 0.0 ? 1.0 0.2 1.2 ? 4.2 ? 10?3
viterbi -4.0 ? 5.7 ? 10?16 -1.7 0.2 < 2.0 ? 10?16
B2h shallow smooth 0.6 ? 1.0 0.2 1.2 ? 5.6 ? 10?2
B1h smooth 0.8 ? 1.0 1.3 3.7 < 2.0 ? 10?16
A1h simple lateen 5.5 < 2.0 ? 10?16 1.9 6.5 < 2.0 ? 10?16
A2h shallow lateen 1.5 ? 5.0 ? 10?2 1.3 3.6 < 2.0 ? 10?16
A3h early-stopping lateen -0.1 ? 1.0 -0.4 0.7 ? 1.7 ? 10?11
A4h early-switching lateen 3.0 ? 1.0 ? 10?8 0.7 2.1 < 2.0 ? 10?16
A5h partly-switching lateen 2.9 ? 7.6 ? 10?8 1.3 3.8 < 2.0 ? 10?16
Table 4: Regressions for accuracies and natural-log-iterations, using 86 binary predictors (all p-values jointly adjusted
for simultaneous hypothesis testing; {langyear} indicators not shown). Accuracies? estimated coefficients ?? that are
statistically different from 0 ? and iteration counts? multipliers e?? significantly different from 1 ? are shown in bold.
on, we use Klein and Manning?s (2004) ?ad-
hoc? harmonic heuristic, bootstrapped using
sentences up to length 10, from the training set.
? sweet ? This setting controls the length cut-
off. By default, we train with all sentences con-
taining up to 45 tokens; when it is on, we use
Spitkovsky et al?s (2009) ?sweet spot? cutoff
of 15 tokens (recommended for English, WSJ).
? viterbi ? This setting controls the primary ob-
jective of the learning algorithm. By default,
we run soft EM; when it is on, we use hard EM.
? {langyeari}22i=1 ? This is a set of 22 mutually-
exclusive selectors for the language/year of a
train/test split; default (all zeros) is English ?07.
Due to space limitations, we exclude langyear pre-
dictors from Table 4. Further, we do not explore
(even two-way) interactions between predictors.15
15This approach may miss some interesting facts, e.g., that
the adhoc initializer is exceptionally good for English, with soft
9.3 Statistical Significance
Our statistical analyses relied on the R package (R
Development Core Team, 2011), which does not,
by default, adjust statistical significance (p-values)
for multiple hypotheses testing.16 We corrected
this using the Holm-Bonferroni method (Holm,
1979), which is uniformly more powerful than the
older (Dunn-)Bonferroni procedure; since we tested
many fewer hypotheses (44 + 42 ? one per inter-
cept/coefficient ??) than settings combinations, its ad-
justments to the p-values are small (see Table 4).17
EM. Instead it yields coarse summaries of regularities supported
by overwhelming evidence across data and training regimes.
16Since we would expect p% of randomly chosen hypotheses
to appear significant at the p% level simply by chance, we must
take precautions against these and other ?data-snooping? biases.
17We adjusted the p-values for all 86 hypotheses jointly, us-
ing http://rss.acs.unt.edu/Rdoc/library/multtest/
html/mt.rawp2adjp.html.
1276
CoNLL Year A3s Soft EM A3h Hard EM A1h
& Language DDA iters DDA iters DDA iters DDA iters DDA iters
Arabic 2006 28.4 118 28.4 162 21.6 19 21.6 21 32.1 200
?7 ? ? 26.9 171 24.7 17 24.8 24 22.0 239
Basque ?7 ? ? 39.9 180 32.0 16 32.2 20 43.6 128
Bulgarian ?6 39.6 131 39.1 253 41.6 22 41.5 25 44.3 140
Catalan ?7 ? ? 58.5 135 50.1 48 50.1 54 63.8 279
Chinese ?6 49.4 204 49.4 268 31.3 24 31.6 55 37.9 378
?7 ? ? 46.0 262 30.0 25 30.2 64 34.5 307
Czech ?6 ? ? 50.5 294 27.8 27 27.7 33 35.2 445
?7 ? ? 49.8 263 29.0 37 29.0 41 31.4 307
Danish ?6 ? ? 43.5 116 43.8 31 43.9 45 44.0 289
Dutch ?6 27.8 35 21.3 246 24.9 44 24.9 49 32.5 241
English ?7 ? ? 38.1 180 34.0 32 33.9 42 34.9 186
German ?6 ? ? 33.3 136 25.4 20 25.4 39 33.5 155
Greek ?7 ? ? 17.5 230 18.3 18 18.3 21 21.4 117
Hungarian ?7 17.4 213 17.1 366 12.3 26 12.4 36 23.0 246
Italian ?7 39.6 164 39.6 194 32.6 25 32.6 27 37.6 273
Japanese ?6 56.6 93 56.6 113 49.6 20 49.7 23 53.5 91
Portuguese ?6 37.5 102 37.9 180 28.6 27 28.9 41 34.4 134
Slovenian ?6 31.1 118 30.8 234 ? ? 23.4 22 33.6 255
Spanish ?6 33.1 73 33.3 125 18.2 29 18.4 36 33.3 235
Swedish ?6 ? ? 41.8 242 36.0 24 36.1 29 42.5 296
Turkish ?6 ? ? 29.8 303 17.8 19 22.2 38 31.9 134
?7 ? ? 28.3 227 14.0 9 10.7 31 33.4 242
Average: 37.4 162 37.0 206 30.0 26 30.0 35 37.1 221
Table 5: Performance (directed dependency accuracies measured against all sentences in the evaluation sets) and
efficiency (numbers of iterations) for standard training (soft and hard EM), early-stopping lateen EM (A3) and simple
lateen EM with hard EM?s primary objective (A1h), for all 23 train/test splits, with adhoc and sweet settings on.
9.4 Interpretation
Table 4 shows the estimated coefficients and their
(adjusted) p-values for both intercepts and most pre-
dictors (excluding the language/year of the data sets)
for all 1,840 experiments. The default (English) sys-
tem uses soft EM, trains with both short and long
sentences, and starts from an uninformed uniform
initializer. It is estimated to score 30.9%, converging
after approximately 256 iterations (both intercepts
are statistically different from zero: p < 2.0 ? 10?16).
As had to be the case, we detect a gain from undi-
rected scoring; integrated scoring is slightly (but
significantly: p ? 7.0 ? 10?7) negative, which is re-
assuring: best parses are scoring higher than the rest
and may be standing out by large margins. The ad-
hoc initializer boosts accuracy by 1.2%, overall (also
significant: p ? 3.1 ? 10?13), without a measurable
impact on running time (p ? 1.0). Training with
fewer, shorter sentences, at the sweet spot gradation,
adds 1.0% and shaves 20% off the total number of it-
erations, on average (both estimates are significant).
We find the viterbi objective harmful ? by 4.0%,
on average (p ? 5.7 ? 10?16) ? for the CoNLL sets.
Spitkovsky et al (2010a) reported that it helps on
WSJ, at least with long sentences and uniform ini-
tializers. Half of our experiments are with shorter
sentences, and half use ad hoc initializers (i.e., three
quarters of settings are not ideal for Viterbi EM),
which may have contributed to this negative result;
still, our estimates do confirm that hard EM is sig-
nificantly (80%, p < 2.0? 10?16) faster than soft EM.
9.5 More on Viterbi Training
The overall negative impact of Viterbi objectives is
a cause for concern: On average, A1h?s estimated
gain of 5.5% should more than offset the expected
4.0% loss from starting with hard EM. But it is, nev-
ertheless, important to make sure that simple lateen
EM with hard EM?s primary objective is in fact an
improvement over both standard EM algorithms.
Table 5 shows performance and efficiency num-
bers for A1h, A3{h,s}, as well as standard soft and
hard EM, using settings that are least favorable for
1277
CoNLL Year A3s Soft EM A3h Hard EM A1h
& Language DDA iters DDA iters DDA iters DDA iters DDA iters
Arabic 2006 ? ? 33.4 317 20.8 8 20.2 32 16.6 269
?7 18.6 60 8.7 252 26.5 9 26.4 14 49.5 171
Basque ?7 ? ? 18.3 245 23.2 16 23.0 23 24.0 162
Bulgarian ?6 27.0 242 27.1 293 40.6 33 40.5 34 43.9 276
Catalan ?7 15.0 74 13.8 159 53.2 30 53.1 31 59.8 176
Chinese ?6 63.5 131 63.6 261 36.8 45 36.8 47 44.5 213
?7 58.5 130 58.5 258 35.2 20 35.0 48 43.2 372
Czech ?6 29.5 125 29.7 224 23.6 18 23.8 41 27.7 179
?7 ? ? 25.9 215 27.1 37 27.2 64 28.4 767
Danish ?6 ? ? 16.6 155 28.7 30 28.7 30 38.3 241
Dutch ?6 20.4 51 21.2 174 25.5 30 25.6 38 27.8 243
English ?7 ? ? 18.0 162 ? ? 38.7 35 45.2 366
German ?6 ? ? 24.4 148 30.1 39 30.1 44 30.4 185
Greek ?7 25.5 133 25.3 156 ? ? 13.2 27 13.2 252
Hungarian ?7 ? ? 18.9 310 28.9 34 28.9 44 34.7 414
Italian ?7 25.4 127 25.3 165 ? ? 52.3 36 52.3 81
Japanese ?6 ? ? 39.3 143 42.2 38 42.4 48 50.2 199
Portuguese ?6 35.2 48 35.6 224 ? ? 34.5 21 36.7 143
Slovenian ?6 24.8 182 25.3 397 28.8 17 28.8 20 32.2 121
Spanish ?6 ? ? 27.7 252 ? ? 28.3 31 50.6 130
Swedish ?6 27.9 49 32.6 287 45.2 22 45.6 52 50.0 314
Turkish ?6 ? ? 30.5 239 30.2 16 30.6 24 29.0 138
?7 ? ? 48.8 254 34.3 24 33.1 34 35.9 269
Average: 27.3 161 27.3 225 33.2 28 33.2 35 38.2 236
Table 6: Performance (directed dependency accuracies measured against all sentences in the evaluation sets) and
efficiency (numbers of iterations) for standard training (soft and hard EM), early-stopping lateen EM (A3) and simple
lateen EM with hard EM?s primary objective (A1h), for all 23 train/test splits, with setting adhoc off and sweet on.
Viterbi training: adhoc and sweet on. Although A1h
scores 7.1% higher than hard EM, on average, it is
only slightly better than soft EM ? up 0.1% (and
worse than A1s). Without adhoc (i.e., using uniform
initializers ? see Table 6), however, hard EM still
improves, by 3.2%, on average, whereas soft EM
drops nearly 10%; here, A1h further improves over
hard EM, scoring 38.2% (up 5.0), higher than soft
EM?s accuracies from both settings (27.3 and 37.0).
This suggests that A1h is indeed better than both
standard EM algorithms. We suspect that our exper-
imental set-up may be disadvantageous for Viterbi
training, since half the settings use ad hoc initializ-
ers, and because CoNLL sets are small. (Viterbi EM
works best with more data and longer sentences.)
Acknowledgments
Partially funded by the Air Force Research Laboratory (AFRL),
under prime contract no. FA8750-09-C-0181, and by NSF, via
award #IIS-0811974. We thank Angel X. Chang, Spence Green,
David McClosky, Fernando Pereira, Slav Petrov and the anony-
mous reviewers, for many helpful comments on draft versions
of this paper, and Andrew Y. Ng, for a stimulating discussion.
First author is grateful to Lynda K. Dunnigan for first introduc-
ing him to lateen sails, among other connections, in Humanities.
1278
References
S. Abney. 1996. Statistical methods and linguistics. In
J. L. Klavans and P. Resnik, editors, The Balancing
Act: Combining Symbolic and Statistical Approaches
to Language. MIT Press.
E. L. Allgower and K. Georg. 1990. Numerical Contin-
uation Methods: An Introduction. Springer-Verlag.
S. Arlot and A. Celisse. 2010. A survey of cross-
validation procedures for model selection. Statistics
Surveys, 4.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. In Speech Communication Papers for the 97th
Meeting of the Acoustical Society of America.
M.-F. Balcan, A. Blum, and K. Yang. 2004. Co-training
and expansion: Towards bridging theory and practice.
In NIPS.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In NAACL-HLT.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In EMNLP.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
J. Chan, I. Koprinska, and J. Poon. 2004. Co-training
with a single natural feature set applied to email clas-
sification. In WI.
M.-W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
ACL.
E. Charniak. 1997. Statistical techniques for natural lan-
guage parsing. AI Magazine, 18.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In NAACL-HLT.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In EMNLP.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Crim. 2002. Co-training re-rankers for improved
parser accuracy.
S. Dasgupta, M. L. Littman, and D. McAllester. 2001.
PAC generalization bounds for co-training. In NIPS.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B, 39.
I. S. Dhillon, Y. Guan, and J. Kogan. 2002. Iterative
clustering of high dimensional text data augmented by
local search. In ICDM.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In ANLP.
S. Geisser. 1975. The predictive sample reuse method
with applications. Journal of the American Statistical
Association, 70.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Technical report, University of
Pennsylvania.
S. Goldman and Y. Zhou. 2000. Enhancing supervised
learning with unlabeled data. In ICML.
W. P. Headden, III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In NAACL-HLT.
S. Holm. 1979. A simple sequentially rejective multiple
test procedure. Scandinavian Journal of Statistics, 6.
J. P. A. Ioannidis. 2005. Why most published research
findings are false. PLoS Medicine, 2.
M. Johnson. 2009. How the statistical revolution
changes (computational) linguistics. In EACL: In-
teraction between Linguistics and Computational Lin-
guistics: Virtuous, Vicious or Vacuous?
M. Kearns, Y. Mansour, and A. Y. Ng. 1997. An
information-theoretic analysis of hard and soft assign-
ment methods for clustering. In UAI.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
D. Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
S. C. Larson. 1931. The shrinkage of the coefficient of
multiple correlation. Journal of Educational Psychol-
ogy, 22.
P. Liang and D. Klein. 2008. Analyzing the errors of
unsupervised learning. In HLT-ACL.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical Programming. Series B, 45.
B. T. Lowerre. 1976. The HARPY Speech Recognition
System. Ph.D. thesis, CMU.
L. B. Lucy. 1974. An iterative technique for the recti-
fication of observed distributions. The Astronomical
Journal, 79.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
1279
X.-L. Meng. 2007. EM and MCMC: Workhorses for sci-
entific computing (thirty years of EM and much more).
Statistica Sinica, 17.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20.
D. C. Montgomery. 2005. Design and Analysis of Exper-
iments. John Wiley & Sons, 6th edition.
M. Mylonakis and K. Sima?an. 2010. Learning prob-
abilistic synchronous CFGs for phrase-based transla-
tion. In CoNLL.
R. M. Neal and G. E. Hinton. 1999. A view of the EM
algorithm that justifies incremental, sparse, and other
variants. In M. I. Jordan, editor, Learning in Graphical
Models. MIT Press.
V. Ng and C. Cardie. 2003. Weakly supervised natural
language learning without redundant views. In HLT-
NAACL.
K. Nigam and R. Ghani. 2000. Analyzing the effective-
ness and applicability of co-training. In CIKM.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In EMNLP-
CoNLL.
M. A. Paskin. 2001. Grammatical bigrams. In NIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In ACL.
R Development Core Team, 2011. R: A Language and
Environment for Statistical Computing. R Foundation
for Statistical Computing.
S. Ravi and K. Knight. 2009. Minimized models for un-
supervised part-of-speech tagging. In ACL-IJCNLP.
K. Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression and related opt-
mization problems. Proceedings of the IEEE, 86.
A. Sarkar. 2001. Applying co-training methods to statis-
tical parsing. In NAACL.
R. Schwartz, O. Abend, R. Reichart, and A. Rappoport.
2011. Neutralizing linguistically problematic annota-
tions in unsupervised dependency parsing evaluation.
In ACL.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In ACL.
N. A. Smith and J. Eisner. 2005a. Contrastive estimation:
Training log-linear models on unlabeled data. In ACL.
N. A. Smith and J. Eisner. 2005b. Guiding unsupervised
grammar induction using contrastive estimation. In IJ-
CAI: Grammatical Inference Applications.
A. S?gaard and C. Rish?j. 2010. Semi-supervised de-
pendency parsing using generalized tri-training. In
COLING.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009.
Baby Steps: How ?Less is More? in unsupervised de-
pendency parsing. In NIPS: Grammar Induction, Rep-
resentation of Language and Language Learning.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010a. Viterbi training improves unsupervised
dependency parsing. In CoNLL.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010b.
Profiting from mark-up: Hyper-text annotations for
guided parsing. In ACL.
M. Stone. 1974. Cross-validatory choice and assessment
of statistical predictions. Journal of the Royal Statisti-
cal Society. Series B, 36.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level sentiment
classification. In EMNLP.
Y. Zhou and S. Goldman. 2004. Democratic co-learning.
In ICTAI.
1280
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1281?1290,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Dependency Parsing without Gold Part-of-Speech Tags
Valentin I. Spitkovsky??
valentin@cs.stanford.edu
Hiyan Alshawi?
hiyan@google.com
Angel X. Chang??
angelx@cs.stanford.edu
Daniel Jurafsky??
jurafsky@stanford.edu
?Computer Science Department
Stanford University
Stanford, CA, 94305
?Google Research
Google Inc.
Mountain View, CA, 94043
?Department of Linguistics
Stanford University
Stanford, CA, 94305
Abstract
We show that categories induced by unsuper-
vised word clustering can surpass the perfor-
mance of gold part-of-speech tags in depen-
dency grammar induction. Unlike classic clus-
tering algorithms, our method allows a word
to have different tags in different contexts.
In an ablative analysis, we first demonstrate
that this context-dependence is crucial to the
superior performance of gold tags ? requir-
ing a word to always have the same part-of-
speech significantly degrades the performance
of manual tags in grammar induction, elim-
inating the advantage that human annotation
has over unsupervised tags. We then introduce
a sequence modeling technique that combines
the output of a word clustering algorithm with
context-colored noise, to allow words to be
tagged differently in different contexts. With
these new induced tags as input, our state-of-
the-art dependency grammar inducer achieves
59.1% directed accuracy on Section 23 (all
sentences) of the Wall Street Journal (WSJ)
corpus ? 0.7% higher than using gold tags.
1 Introduction
Unsupervised learning ? machine learning without
manually-labeled training examples ? is an active
area of scientific research. In natural language pro-
cessing, unsupervised techniques have been success-
fully applied to tasks such as word alignment for ma-
chine translation. And since the advent of the web,
algorithms that induce structure from unlabeled data
have continued to steadily gain importance. In this
paper we focus on unsupervised part-of-speech tag-
ging and dependency parsing ? two related prob-
lems of syntax discovery. Our methods are applica-
ble to vast quantities of unlabeled monolingual text.
Not all research on these problems has been fully
unsupervised. For example, to the best of our knowl-
edge, every new state-of-the-art dependency gram-
mar inducer since Klein and Manning (2004) relied
on gold part-of-speech tags. For some time, multi-
point performance degradations caused by switching
to automatically induced word categories have been
interpreted as indications that ?good enough? parts-
of-speech induction methods exist, justifying the fo-
cus on grammar induction with supervised part-of-
speech tags (Bod, 2006), pace (Cramer, 2007). One
of several drawbacks of this practice is that it weak-
ens any conclusions that could be drawn about how
computers (and possibly humans) learn in the ab-
sence of explicit feedback (McDonald et al, 2011).
In turn, not all unsupervised taggers actually in-
duce word categories: Many systems ? known as
part-of-speech disambiguators (Merialdo, 1994) ?
rely on external dictionaries of possible tags. Our
work builds on two older part-of-speech inducers
? word clustering algorithms of Clark (2000) and
Brown et al (1992) ? that were recently shown to
be more robust than other well-known fully unsuper-
vised techniques (Christodoulopoulos et al, 2010).
We investigate which properties of gold part-of-
speech tags are useful in grammar induction and
parsing, and how these properties could be intro-
duced into induced tags. We also explore the number
of word classes that is good for grammar induction:
in particular, whether categorization is needed at all.
By removing the ?unrealistic simplification? of us-
ing gold tags (Petrov et al, 2011, ?3.2, Footnote 4),
we will go on to demonstrate why grammar induc-
tion from plain text is no longer ?still too difficult.?
1281
NNS VBD IN NN ?
Payrolls fell in September .
P = (1 ?
0? ?? ?
PSTOP(, L, T)) ? PATTACH(, L, VBD)
? (1 ? PSTOP(VBD, L, T)) ? PATTACH(VBD, L, NNS)
? (1 ? PSTOP(VBD, R, T)) ? PATTACH(VBD, R, IN)
? (1 ? PSTOP(IN, R, T)) ? PATTACH(IN, R, NN)
? PSTOP(VBD, L, F) ? PSTOP(VBD, R, F)
? PSTOP(NNS, L, T) ? PSTOP(NNS, R, T)
? PSTOP(IN, L, T) ? PSTOP(IN, R, F)
? PSTOP(NN, L, T) ? PSTOP(NN, R, T)
? PSTOP(, L, F)? ?? ?
1
? PSTOP(, R, T)? ?? ?
1
.
Figure 1: A dependency structure for a short WSJ sen-
tence and its probability, factored by the DMV, using gold
tags, after summing out PORDER (Spitkovsky et al, 2009).
2 Methodology
In all experiments, we model the English grammar
via Klein and Manning?s (2004) Dependency Model
with Valence (DMV), induced from subsets of not-
too-long sentences of the Wall Street Journal (WSJ).
2.1 The Model
The original DMV is a single-state head automata
model (Alshawi, 1996) over lexical word classes
{cw} ? gold part-of-speech tags. Its generative story
for a sub-tree rooted at a head (of class ch) rests on
three types of independent decisions: (i) initial di-
rection dir ? {L, R} in which to attach children, via
probability PORDER(ch); (ii) whether to seal dir, stop-
ping with probability PSTOP(ch, dir, adj), conditioned
on adj ? {T, F} (true iff considering dir?s first, i.e., ad-
jacent, child); and (iii) attachments (of class ca), ac-
cording to PATTACH(ch, dir, ca). This recursive process
produces only projective trees. A root token ? gen-
erates the head of the sentence as its left (and only)
child (see Figure 1 for a simple, concrete example).
2.2 Learning Algorithms
The DMV lends itself to unsupervised learning via
inside-outside re-estimation (Baker, 1979). Klein
and Manning (2004) initialized their system using an
?ad-hoc harmonic? completion, followed by training
using 40 steps of EM (Klein, 2005). We reproduce
this set-up, iterating without actually verifying con-
vergence, in most of our experiments (#1?4, ?3?4).
Experiments #5?6 (?5) employ our new state-of-
the-art grammar inducer (Spitkovsky et al, 2011),
which uses constrained Viterbi EM (details in ?5).
2.3 Training Data
The DMV is usually trained on a customized sub-
set of Penn English Treebank?s Wall Street Jour-
nal portion (Marcus et al, 1993). Following Klein
and Manning (2004), we begin with reference con-
stituent parses, prune out all empty sub-trees and
remove punctuation and terminals (tagged # and $)
that are not pronounced where they appear. We then
train only on the remaining sentence yields consist-
ing of no more than fifteen tokens (WSJ15), in most
of our experiments (#1?4, ?3?4); by contrast, Klein
and Manning?s (2004) original system was trained
using less data: sentences up to length ten (WSJ10).1
Our final experiments (#5?6, ?5) employ a simple
scaffolding strategy (Spitkovsky et al, 2010a) that
follows up initial training at WSJ15 (?less is more?)
with an additional training run (?leapfrog?) that in-
corporates most sentences of the data set, at WSJ45.
2.4 Evaluation Methods
Evaluation is against the training set, as is standard
practice in unsupervised learning, in part because
Klein and Manning (2004, ?3) did not smooth the
DMV (Klein, 2005, ?6.2). For most of our experi-
ments (#1?4, ?3?4), this entails starting with the ref-
erence trees from WSJ15 (as modified in ?2.3), au-
tomatically converting their labeled constituents into
unlabeled dependencies using deterministic ?head-
percolation? rules (Collins, 1999), and then com-
puting (directed) dependency accuracy scores of the
corresponding induced trees. We report overall per-
centages of correctly guessed arcs, including the
arcs from sentence root symbols, as is standard prac-
tice (Paskin, 2001; Klein and Manning, 2004).
For a meaningful comparison with previous work,
we also test some of the models from our earlier ex-
periments (#1,3) ? and both models from final ex-
periments (#5,6) ? against Section 23 of WSJ?, af-
ter applying Laplace (a.k.a. ?add one?) smoothing.
1WSJ15 contains 15,922 sentences up to length fifteen (a to-
tal of 163,715 tokens, not counting punctuation) ? versus 7,422
sentences of at most ten words (only 52,248 tokens) comprising
WSJ10 ? and is a better trade-off between the quantity and
complexity of training data in WSJ (Spitkovsky et al, 2009).
1282
Accuracy Viable
1. manual tags Unsupervised Sky Groups
gold 50.7 78.0 36
mfc 47.2 74.5 34
mfp 40.4 76.4 160
ua 44.3 78.4 328
2. tagless lexicalized models
full 25.8 97.3 49,180
partial 29.3 60.5 176
none 30.7 24.5 1
3. tags from a flat (Clark, 2000) clustering
47.8 83.8 197
4. prefixes of a hierarchical (Brown et al, 1992) clustering
first 7 bits 46.4 73.9 96
8 bits 48.0 77.8 165
9 bits 46.8 82.3 262
Table 1: Directed accuracies for the ?less is more? DMV,
trained on WSJ15 (after 40 steps of EM) and evaluated
also against WSJ15, using various lexical categories in
place of gold part-of-speech tags. For each tag-set, we
include its effective number of (non-empty) categories in
WSJ15 and the oracle skylines (supervised performance).
3 Motivation and Ablative Analyses
The concepts of polysemy and synonymy are of fun-
damental importance in linguistics. For words that
can take on multiple parts of speech, knowing the
gold tag can reduce ambiguity, improving parsing by
limiting the search space. Furthermore, pooling the
statistics of words that play similar syntactic roles,
as signaled by shared gold part-of-speech tags, can
simplify the learning task, improving generalization
by reducing sparsity. We begin with two sets of ex-
periments that explore the impact that each of these
factors has on grammar induction with the DMV.
3.1 Experiment #1: Human-Annotated Tags
Our first set of experiments attempts to isolate the
effect that replacing gold part-of-speech tags with
deterministic one class per word mappings has on
performance, quantifying the cost of switching to a
monosemous clustering (see Table 1: manual; and
Table 4). Grammar induction with gold tags scores
50.7%, while the oracle skyline (an ideal, supervised
instance of the DMV) could attain 78.0% accuracy.
It may be worth noting that only 6,620 (13.5%) of
49,180 unique tokens in WSJ appear with multiple
part-of-speech tags. Most words, like it, are always
tagged the same way (5,768 times PRP). Some words,
token mfc mfp ua
it {PRP} {PRP} {PRP}
gains {NNS} {VBZ, NNS} {VBZ, NNS}
the {DT} {JJ, DT} {VBP, NNP, NN, JJ, DT, CD}
Table 2: Example most frequent class, most frequent pair
and union all reassignments for tokens it, the and gains.
like gains, usually serve as one part of speech (227
times NNS, as in the gains) but are occasionally used
differently (5 times VBZ, as in he gains). Only 1,322
tokens (2.7%) appear with three or more different
gold tags. However, this minority includes the most
frequent word ? the (50,959 times DT, 7 times JJ,
6 times NNP and once as each of CD, NN and VBP).2
We experimented with three natural reassign-
ments of part-of-speech categories (see Table 2).
The first, most frequent class (mfc), simply maps
each token to its most common gold tag in the entire
WSJ (with ties resolved lexicographically). This ap-
proach discards two gold tags (types PDT and RBR are
not most common for any of the tokens in WSJ15)
and costs about three-and-a-half points of accuracy,
in both supervised and unsupervised regimes.
Another reassignment, union all (ua), maps each
token to the set of all of its observed gold tags, again
in the entire WSJ. This inflates the number of group-
ings by nearly a factor of ten (effectively lexicaliz-
ing the most ambiguous words),3 yet improves the
oracle skyline by half-a-point over actual gold tags;
however, learning is harder with this tag-set, losing
more than six points in unsupervised training.
Our last reassignment, most frequent pair (mfp),
allows up to two of the most common tags into
a token?s label set (with ties, once again, resolved
lexicographically). This intermediate approach per-
forms strictly worse than union all, in both regimes.
3.2 Experiment #2: Lexicalization Baselines
Our next set of experiments assesses the benefits of
categorization, turning to lexicalized baselines that
avoid grouping words altogether. All three models
discussed below estimated the DMV without using
the gold tags in any way (see Table 1: lexicalized).
2Some of these are annotation errors in the treebank (Banko
and Moore, 2004, Figure 2): such (mis)taggings can severely
degrade the accuracy of part-of-speech disambiguators, without
additional supervision (Banko and Moore, 2004, ?5, Table 1).
3Kupiec (1992) found that the 50,000-word vocabulary of
the Brown corpus similarly reduces to ?400 ambiguity classes.
1283
First, not surprisingly, a fully-lexicalized model
over nearly 50,000 unique words is able to essen-
tially memorize the training set, supervised. (With-
out smoothing, it is possible to deterministically at-
tach most rare words in a dependency tree correctly,
etc.) Of course, local search is unlikely to find good
instantiations for so many parameters, causing unsu-
pervised accuracy for this model to drop in half.
For our next experiment, we tried an intermediate,
partially-lexicalized approach. We mapped frequent
words ? those seen at least 100 times in the training
corpus (Headden et al, 2009) ? to their own indi-
vidual categories, lumping the rest into a single ?un-
known? cluster, for a total of under 200 groups. This
model is significantly worse for supervised learn-
ing, compared even with the monosemous clusters
derived from gold tags; yet it is only slightly more
learnable than the broken fully-lexicalized variant.
Finally, for completeness, we trained a model that
maps every token to the same one ?unknown? cat-
egory. As expected, such a trivial ?clustering? is
ineffective in supervised training; however, it out-
performs both lexicalized variants unsupervised,4
strongly suggesting that lexicalization alone may be
insufficient for the DMV and hinting that some de-
gree of categorization is essential to its learnability.
Cluster #173 Cluster #188
1. open 1. get
2. free 2. make
3. further 3. take
4. higher 4. find
5. lower 5. give
6. similar 6. keep
7. leading 7. pay
8. present 8. buy
9. growing 9. win
10. increased 10. sell
.
.
.
.
.
.
37. cool 42. improve
.
.
.
.
.
.
1,688. up-wind 2,105. zero-out
Table 3: Representative members for two of the flat word
groupings: cluster #173 (left) contains adjectives, espe-
cially ones that take comparative (or other) complements;
cluster #188 comprises bare-stem verbs (infinitive stems).
(Of course, many of the words have other syntactic uses.)
4Note that it also beats supervised training. That isn?t a bug:
Spitkovsky et al (2010b, ?7.2) explain this paradox in the DMV.
4 Grammars over Induced Word Clusters
We have demonstrated the need for grouping simi-
lar words, estimated a bound on performance losses
due to monosemous clusterings and are now ready
to experiment with induced part-of-speech tags. We
use two sets of established, publicly-available hard
clustering assignments, each computed from a much
larger data set than WSJ (approximately a million
words). The first is a flat mapping (200 clusters)
constructed by training Clark?s (2000) distributional
similarity model over several hundred million words
from the British National and the English Gigaword
corpora.5 The second is a hierarchical clustering ?
binary strings up to eighteen bits long ? constructed
by running Brown et al?s (1992) algorithm over 43
million words from the BLLIP corpus, minus WSJ.6
4.1 Experiment #3: A Flat Word Clustering
Our main purely unsupervised results are with a flat
clustering (Clark, 2000) that groups words having
similar context distributions, according to Kullback-
Leibler divergence. (A word?s context is an ordered
pair: its left- and right-adjacent neighboring words.)
To avoid overfitting, we employed an implemen-
tation from previous literature (Finkel and Manning,
2009). The number of clusters (200) and the suf-
ficient amount of training data (several hundred-
million words) were tuned to a task (NER) that is
not directly related to dependency parsing. (Table 3
shows representative entries for two of the clusters.)
We added one more category (#0) for unknown
words. Now every token in WSJ could again be re-
placed by a coarse identifier (one of at most 201,
instead of just 36), in both supervised and unsuper-
vised training. (Our training code did not change.)
The resulting supervised model, though not as
good as the fully-lexicalized DMV, was more than
five points more accurate than with gold part-of-
speech tags (see Table 1: flat). Unsupervised accu-
racy was lower than with gold tags (see also Table 4)
but higher than with all three derived hard assign-
ments. This suggests that polysemy (i.e., ability to
5http://nlp.stanford.edu/software/
stanford-postagger-2008-09-28.tar.gz:
models/egw.bnc.200
6http://people.csail.mit.edu/maestro/papers/
bllip-clusters.gz
1284
1 4 16 64 256 1,024 (# of clusters) 49,180
20
40
60
80
%
gold
mfc mfp ua
full
partial
none
flat
gold
mfc
mfp
ua
full
partial
none
flat
k = 1 2 3 4 5 6 7 8 9 10 11 12 ? 18 bits
Figure 2: Parsing performance (accuracy on WSJ15) as a ?function? of the number of syntactic categories, for all prefix
lengths ? k ? {1, . . . , 18} ? of a hierarchical (Brown et al, 1992) clustering, connected by solid lines (dependency
grammar induction in blue; supervised oracle skylines in red, above). Tagless lexicalized models (full, partial and
none) connected by dashed lines. Models based on gold part-of-speech tags, and derived monosemous clusters (mfc,
mfp and ua), shown as vertices of gold polygons. Models based on a flat (Clark, 2000) clustering indicated by squares.
tag a word differently in context) may be the primary
advantage of manually constructed categorizations.
4.2 Experiment #4: A Hierarchical Clustering
The purpose of this batch of experiments is to show
that Clark?s (2000) algorithm isn?t unique in its suit-
ability for grammar induction. We found that Brown
et al?s (1992) older information-theoretic approach,
which does not explicitly address the problems of
rare and ambiguous words (Clark, 2000) and was de-
signed to induce large numbers of plausible syntac-
tic and semantic clusters, can perform just as well.
Once again, the sufficient amount of data (43 mil-
lion words) was tuned in earlier work (Koo, 2010).
His task of interest was, in fact, dependency parsing.
But since this algorithm is hierarchical (i.e., there
isn?t a parameter for the number of categories), we
doubt that there was a strong enough risk of overfit-
ting to question the clustering?s unsupervised nature.
As there isn?t a set number of categories, we used
binary prefixes of length k from each word?s address
in the computed hierarchy as cluster labels. Results
for 7 ? k ? 9 bits (approximately 100?250 non-
empty clusters, close to the 200 we used before) are
similar to those of flat clusters (see Table 1: hierar-
chical). Outside of this range, however, performance
can be substantially worse (see Figure 2), consistent
with earlier findings: Headden et al (2008) demon-
strated that (constituent) grammar induction, using
the singular-value decomposition (SVD-based) tag-
ger of Schu?tze (1995), also works best with 100?200
clusters. Important future research directions may
include learning to automatically select a good num-
ber of word categories (in the case of flat clusterings)
and ways of using multiple clustering assignments,
perhaps of different granularities/resolutions, in tan-
dem (e.g., in the case of a hierarchical clustering).
4.3 Further Evaluation
It is important to enable easy comparison with pre-
vious and future work. Since WSJ15 is not a stan-
dard test set, we evaluated two key experiments ?
?less is more? with gold part-of-speech tags (#1, Ta-
ble 1: gold) and with Clark?s (2000) clusters (#3, Ta-
ble 1: flat) ? on all sentences (not just length fifteen
and shorter), in Section 23 of WSJ (see Table 4).
This required smoothing both final models (?2.4).
We showed that two classic unsupervised word
1285
System Description Accuracy
#1 (?3.1) ?less is more? (Spitkovsky et al, 2009) 44.0
#3 (?4.1) ?less is more? with monosemous induced tags 41.4 (-2.6)
Table 4: Directed accuracies on Section 23 of WSJ (all sentences) for two experiments with the base system.
clusterings ? one flat and one hierarchical ? can
be better for dependency grammar induction than
monosemous syntactic categories derived from gold
part-of-speech tags. And we confirmed that the un-
supervised tags are worse than the actual gold tags,
in a simple dependency grammar induction system.
5 State-of-the-Art without Gold Tags
Until now, we have deliberately kept our experimen-
tal methods simple and nearly identical to Klein and
Manning?s (2004), for clarity. Next, we will explore
how our main findings generalize beyond this toy
setting. A preliminary test will simply quantify the
effect of replacing gold part-of-speech tags with the
monosemous flat clustering (as in experiment #3,
?4.1) on a modern grammar inducer. And our last
experiment will gauge the impact of using a polyse-
mous (but still unsupervised) clustering instead, ob-
tained by executing standard sequence labeling tech-
niques to introduce context-sensitivity into the origi-
nal (independent) assignment or words to categories.
These final experiments are with our latest state-
of-the-art system (Spitkovsky et al, 2011) ? a par-
tially lexicalized extension of the DMV that uses
constrained Viterbi EM to train on nearly all of the
data available in WSJ, at WSJ45 (48,418 sentences;
986,830 non-punctuation tokens). The key contribu-
tion that differentiates this model from its predeces-
sors is that it incorporates punctuation into grammar
induction (by turning it into parsing constraints, in-
stead of ignoring punctuation marks altogether). In
training, the model makes a simplifying assumption
? that sentences can be split at punctuation and that
the resulting fragments of text could be parsed inde-
pendently of one another (these parsed fragments are
then reassembled into full sentence trees, by pars-
ing the sequence of their own head words). Fur-
thermore, the model continues to take punctuation
marks into account in inference (using weaker, more
accurate constraints, than in training). This system
scores 58.4% on Section 23 of WSJ? (see Table 5).
5.1 Experiment #5: A Monosemous Clustering
As in experiment #3 (?4.1), we modified the base
system in exactly one way: we swapped out gold
part-of-speech tags and replaced them with a flat dis-
tributional similarity clustering. In contrast to sim-
pler models, which suffer multi-point drops in ac-
curacy from switching to unsupervised tags (e.g.,
2.6%), our new system?s performance degrades only
slightly, by 0.2% (see Tables 4 and 5). This result
improves over substantial performance degradations
previously observed for unsupervised dependency
parsing with induced word categories (Klein and
Manning, 2004; Headden et al, 2008, inter alia).7
One risk that arises from using gold tags is that
newer systems could be finding cleverer ways to ex-
ploit manual labels (i.e., developing an over-reliance
on gold tags) instead of actually learning to acquire
language. Part-of-speech tags are known to contain
significant amounts of information for unlabeled de-
pendency parsing (McDonald et al, 2011, ?3.1), so
we find it reassuring that our latest grammar inducer
is less dependent on gold tags than its predecessors.
5.2 Experiment #6: A Polysemous Clustering
Results of experiments #1 and 3 (?3.1, 4.1) suggest
that grammar induction stands to gain from relaxing
the one class per word assumption. We next test this
conjecture by inducing a polysemous unsupervised
word clustering, then using it to induce a grammar.
Previous work (Headden et al, 2008, ?4) found
that simple bitag hidden Markov models, classically
trained using the Baum-Welch (Baum, 1972) variant
of EM (HMM-EM), perform quite well,8 on aver-
age, across different grammar induction tasks. Such
sequence models incorporate a sensitivity to context
via state transition probabilities PTRAN(ti | ti?1), cap-
turing the likelihood that a tag ti immediately fol-
lows the tag ti?1; emission probabilities PEMIT(wi | ti)
capture the likelihood that a word of type ti is wi.
7We also briefly comment on this result in the ?punctuation?
paper (Spitkovsky et al, 2011, ?7), published concurrently.
8They are also competitive with Bayesian estimators, on
larger data sets, with cross-validation (Gao and Johnson, 2008).
1286
System Description Accuracy
(?5) ?punctuation? (Spitkovsky et al, 2011) 58.4
#5 (?5.1) ?punctuation? with monosemous induced tags 58.2 (-0.2)
#6 (?5.2) ?punctuation? with context-sensitive induced tags 59.1 (+0.7)
Table 5: Directed accuracies on Section 23 of WSJ (all sentences) for experiments with the state-of-the-art system.
We need a context-sensitive tagger, and HMM
models are good ? relative to other tag-inducers.
However, they are not better than gold tags, at least
when trained using a modest amount of data.9 For
this reason, we decided to relax the monosemous
flat clustering, plugging it in as an initializer for the
HMM. The main problem with this approach is that,
at least without smoothing, every monosemous la-
beling is trivially at a local optimum, since P(ti | wi)
is deterministic. To escape the initial assignment,
we used a ?noise injection? technique (Selman et
al., 1994), inspired by the contexts of Clark (2000).
First, we collected the MLE statistics for PR(ti+1 | ti)
and PL(ti | ti+1) in WSJ, using the flat monosemous
tags. Next, we replicated the text of WSJ 100-fold.
Finally, we retagged this larger data set, as follows:
with probability 80%, a word kept its monosemous
tag; with probability 10%, we sampled a new tag
from the left context (PL) associated with the origi-
nal (monosemous) tag of its rightmost neighbor; and
with probability 10%, we drew a tag from the right
context (PR) of its leftmost neighbor.10 Given that
our initializer ? and later the input to the grammar
inducer ? are hard assignments of tags to words, we
opted for (the faster and simpler) Viterbi training.
In the spirit of reproducibility, we again used an
off-the-shelf component for tagging-related work.11
Viterbi training converged after just 17 steps, re-
placing the original monosemous tags for 22,280 (of
1,028,348 non-punctuation) tokens in WSJ. For ex-
9All of Headden et al?s (2008) grammar induction experi-
ments with induced parts-of-speech were worse than their best
results using gold part-of-speech tags, most likely because they
used a very small corpus (half of WSJ10) to cluster words.
10We chose the sampling split (80:10:10) and replication pa-
rameter (100) somewhat arbitrarily, so better results could likely
be obtained with tuning. However, we suspect that the real gains
would come from using soft clustering techniques (Hinton and
Roweis, 2003; Pereira et al, 1993, inter alia) and propagating
(joint) estimates of tag distributions into a parser. Our ad-hoc
approach is intended to serve solely as a proof of concept.
11David Elworthy?s C+ tagger, with options -i t -G -l,
available from http://friendly-moose.appspot.com/
code/NewCpTag.zip.
ample, the first changed sentence is #3 (of 49,208):
Some ?circuit breakers? installed after
the October 1987 crash failed their first
test, traders say, unable to cool the selling
panic in both stocks and futures.
Above, the word cool gets relabeled as #188 (from
#173 ? see Table 3), since its context is more
suggestive of an infinitive verb than of its usual
grouping with adjectives. (A proper analysis of all
changes, however, is beyond the scope of this work.)
Using this new context-sensitive hard assignment
of tokens to unsupervised categories our gram-
mar inducer attained a directed accuracy of 59.1%,
nearly a full point better than with the monosemous
hard assignment (see Table 5). To the best of our
knowledge it is also the first state-of-the-art unsuper-
vised dependency parser to perform better with in-
duced categories than with gold part-of-speech tags.
6 Related Work
Early work in dependency grammar induction al-
ready relied on gold part-of-speech tags (Carroll and
Charniak, 1992). Some later models (Yuret, 1998;
Paskin, 2001, inter alia) attempted full lexicaliza-
tion. However, Klein and Manning (2004) demon-
strated that effort to be worse at recovering depen-
dency arcs than choosing parse structures at random,
leading them to incorporate gold tags into the DMV.
Klein and Manning (2004, ?5, Figure 6) had also
tested their own models with induced word classes,
constructed using a distributional similarity cluster-
ing method (Schu?tze, 1995). Without gold part-of-
speech tags, their combined DMV+CCM model was
about five points worse, both in (directed) unlabeled
dependency accuracy (42.3% vs. 47.5%)12 and unla-
beled bracketing F1 (72.9% vs. 77.6%), on WSJ10.
In constituent parsing, earlier Seginer (2007a, ?6,
Table 1) built a fully-lexicalized grammar inducer
12On the same evaluation set (WSJ10), our context-sensitive
system without gold tags (Experiment #6, ?5.2) scores 66.8%.
1287
that was competitive with DMV+CCM despite not
using gold tags. His CCL parser has since been
improved via a ?zoomed learning? technique (Re-
ichart and Rappoport, 2010). Moreover, Abend et
al. (2010) reused CCL?s internal distributional rep-
resentation of words in a cognitively-motivated part-
of-speech inducer. Unfortunately their tagger did
not make it into Christodoulopoulos et al?s (2010)
excellent and otherwise comprehensive evaluation.
Outside monolingual grammar induction, fully-
lexicalized statistical dependency transduction mod-
els have been trained from unannotated parallel bi-
texts for machine translation (Alshawi et al, 2000).
More recently, McDonald et al (2011) demonstrated
an impressive alternative to grammar induction by
projecting reference parse trees from languages that
have annotations to ones that are resource-poor.13 It
uses graph-based label propagation over a bilingual
similarity graph for a sentence-aligned parallel cor-
pus (Das and Petrov, 2011), inducing part-of-speech
tags from a universal tag-set (Petrov et al, 2011).
Even in supervised parsing we are starting to see
a shift away from using gold tags. For example,
Alshawi et al (2011) demonstrated good results for
mapping text to underspecified semantics via depen-
dencies without resorting to gold tags. And Petrov et
al. (2010, ?4.4, Table 4) observed only a small per-
formance loss ?going POS-less? in question parsing.
We are not aware of any systems that induce both
syntactic trees and their part-of-speech categories.
However, aside from the many systems that induce
trees from gold tags, there are also unsupervised
methods for inducing syntactic categories from gold
trees (Finkel et al, 2007; Pereira et al, 1993), as
well as for inducing dependencies from gold con-
stituent annotations (Sangati and Zuidema, 2009;
Chiang and Bikel, 2002). Considering that Headden
et al?s (2008) study of part-of-speech taggers found
no correlation between standard tagging metrics and
the quality of induced grammars, it may be time for
a unified treatment of these very related syntax tasks.
13When the target language is English, however, their best ac-
curacy (projected from Greek) is low: 45.7% (McDonald et al,
2011, ?4, Table 2); tested on the same CoNLL 2007 evaluation
set (Nivre et al, 2007), our ?punctuation? system with context-
sensitive induced tags (trained on WSJ45, without gold tags)
performs substantially better, scoring 51.6%. Note that this is
also an improvement over our system trained on the CoNLL set
using gold tags: 50.3% (Spitkovsky et al, 2011, ?8, Table 6).
7 Discussion and Conclusions
Unsupervised word clustering techniques of Brown
et al (1992) and Clark (2000) are well-suited to de-
pendency parsing with the DMV. Both methods out-
perform gold parts-of-speech in supervised modes.
And both can do better than monosemous clusters
derived from gold tags in unsupervised training. We
showed how Clark?s (2000) flat tags can be relaxed,
using context, with the resulting polysemous cluster-
ing outperforming gold part-of-speech tags for the
English dependency grammar induction task.
Monolingual evaluation is a significant flaw in our
methodology, however. One (of many) take-home
points made in Christodoulopoulos et al?s (2010)
study is that results on one language do not neces-
sarily correlate with other languages.14 Assuming
that our results do generalize, it will still remain to
remove the present reliance on gold tokenization and
sentence boundary labels. Nevertheless, we feel that
eliminating gold tags is an important step towards
the goal of fully-unsupervised dependency parsing.
We have cast the utility of a categorization scheme
as a combination of two effects on parsing accuracy:
a synonymy effect and a polysemy effect. Results
of our experiments with both full and partial lexi-
calization suggest that grouping similar words (i.e.,
synonymy) is vital to grammar induction with the
DMV. This is consistent with an established view-
point, that simple tabulation of frequencies of words
participating in certain configurations cannot be reli-
ably used for comparing their likelihoods (Pereira et
al., 1993, ?4.2): ?The statistics of natural languages
is inherently ill defined. Because of Zipf?s law, there
is never enough data for a reasonable estimation of
joint object distributions.? Seginer?s (2007b, ?1.4.4)
argument, however, is that the Zipfian distribution
? a property of words, not parts-of-speech ?
should allow frequent words to successfully guide
14Furthermore, it would be interesting to know how sensitive
different head-percolation schemes (Yamada and Matsumoto,
2003; Johansson and Nugues, 2007) would be to gold versus
unsupervised tags, since the Magerman-Collins rules (Mager-
man, 1995; Collins, 1999) agree with gold dependency annota-
tions only 85% of the time, even for WSJ (Sangati and Zuidema,
2009). Proper intrinsic evaluation of dependency grammar in-
ducers is not yet a solved problem (Schwartz et al, 2011).
1288
parsing and learning: ?A relatively small number of
frequent words appears almost everywhere and most
words are never too far from such a frequent word
(this is also the principle behind successful part-of-
speech induction).? We believe that it is important to
thoroughly understand how to reconcile these only
seemingly conflicting insights, balancing them both
in theory and in practice. A useful starting point may
be to incorporate frequency information in the pars-
ing models directly ? in particular, capturing the
relationships between words of various frequencies.
The polysemy effect appears smaller but is less
controversial: Our experiments suggest that the pri-
mary drawback of the classic clustering schemes
stems from their one class per word nature ? and
not a lack of supervision, as may be widely believed.
Monosemous groupings, even if they are themselves
derived from human-annotated syntactic categories,
simply cannot disambiguate words the way gold tags
can. By relaxing Clark?s (2000) flat clustering, us-
ing contextual cues, we improved dependency gram-
mar induction: directed accuracy on Section 23 (all
sentences) of the WSJ benchmark increased from
58.2% to 59.1% ? from slightly worse to better than
with gold tags (58.4%, previous state-of-the-art).
Since Clark?s (2000) word clustering algorithm is
already context-sensitive in training, we suspect that
one could do better simply by preserving the polyse-
mous nature of its internal representation. Importing
the relevant distributions into a sequence tagger di-
rectly would make more sense than going through an
intermediate monosemous summary. And exploring
other uses of soft clustering algorithms ? perhaps as
inputs to part-of-speech disambiguators ? may be
another fruitful research direction. We believe that
a joint treatment of grammar and parts-of-speech in-
duction could fuel major advances in both tasks.
Acknowledgments
Partially funded by the Air Force Research Laboratory (AFRL),
under prime contract no. FA8750-09-C-0181, and by NSF, via
award #IIS-0811974. We thank Omri Abend, Spence Green,
David McClosky and the anonymous reviewers for many help-
ful comments on draft versions of this paper.
References
O. Abend, R. Reichart, and A. Rappoport. 2010. Im-
proved unsupervised POS induction through prototype
discovery. In ACL.
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learn-
ing dependency translation models as collections of
finite-state head transducers. Computational Linguis-
tics, 26.
H. Alshawi, P.-C. Chang, and M. Ringgaard. 2011. De-
terministic statistical mapping of sentences to under-
specied semantics. In IWCS.
H. Alshawi. 1996. Head automata for speech translation.
In ICSLP.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. In Speech Communication Papers for the 97th
Meeting of the Acoustical Society of America.
M. Banko and R. C. Moore. 2004. Part of speech tagging
in context. In COLING.
L. E. Baum. 1972. An inequality and associated maxi-
mization technique in statistical estimation for proba-
bilistic functions of Markov processes. In Inequalities.
R. Bod. 2006. An all-subtrees approach to unsupervised
parsing. In COLING-ACL.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-based n-gram models
of natural language. Computational Linguistics, 18.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. Technical report, Brown University.
D. Chiang and D. M. Bikel. 2002. Recovering latent
information in treebanks. In COLING.
C. Christodoulopoulos, S. Goldwater, and M. Steedman.
2010. Two decades of unsupervised POS induction:
How far have we come? In EMNLP.
A. Clark. 2000. Inducing syntactic categories by context
distribution clustering. In CoNLL-LLL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
B. Cramer. 2007. Limitations of current grammar induc-
tion algorithms. In ACL: Student Research.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In ACL.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL-HLT.
J. R. Finkel, T. Grenager, and C. D. Manning. 2007. The
infinite tree. In ACL.
J. Gao and M. Johnson. 2008. A comparison of Bayesian
estimators for unsupervised Hidden Markov Model
POS taggers. In EMNLP.
1289
W. P. Headden, III, D. McClosky, and E. Charniak.
2008. Evaluating unsupervised part-of-speech tagging
for grammar induction. In COLING.
W. P. Headden, III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In NAACL-HLT.
G. Hinton and S. Roweis. 2003. Stochastic neighbor
embedding. In NIPS.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
D. Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
T. Koo. 2010. Advances in Discriminative Dependency
Parsing. Ph.D. thesis, MIT.
J. Kupiec. 1992. Robust part-of-speech tagging using
a hidden Markov model. Computer Speech and Lan-
guage, 6.
D. M. Magerman. 1995. Statistical decision-tree models
for parsing. In ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-
source transfer of delexicalized dependency parsers.
In EMNLP.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In EMNLP-
CoNLL.
M. A. Paskin. 2001. Grammatical bigrams. In NIPS.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In ACL.
S. Petrov, P.-C. Chang, M. Ringgaard, and H. Alshawi.
2010. Uptraining for accurate deterministic question
parsing. In EMNLP.
S. Petrov, D. Das, and R. McDonald. 2011. A universal
part-of-speech tagset. In ArXiv.
R. Reichart and A. Rappoport. 2010. Improved fully un-
supervised parsing with zoomed learning. In EMNLP.
F. Sangati and W. Zuidema. 2009. Unsupervised meth-
ods for head assignments. In EACL.
H. Schu?tze. 1995. Distributional part-of-speech tagging.
In EACL.
R. Schwartz, O. Abend, R. Reichart, and A. Rappoport.
2011. Neutralizing linguistically problematic annota-
tions in unsupervised dependency parsing evaluation.
In ACL.
Y. Seginer. 2007a. Fast unsupervised incremental pars-
ing. In ACL.
Y. Seginer. 2007b. Learning Syntactic Structure. Ph.D.
thesis, University of Amsterdam.
B. Selman, H. A. Kautz, and B. Cohen. 1994. Noise
strategies for improving local search. In AAAI.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009.
Baby Steps: How ?Less is More? in unsupervised de-
pendency parsing. In NIPS: Grammar Induction, Rep-
resentation of Language and Language Learning.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a.
From Baby Steps to Leapfrog: How ?Less is More? in
unsupervised dependency parsing. In NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010b. Viterbi training improves unsupervised
dependency parsing. In CoNLL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011.
Punctuation: Making a point in unsupervised depen-
dency parsing. In CoNLL.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT.
D. Yuret. 1998. Discovery of Linguistic Relations Using
Lexical Attraction. Ph.D. thesis, MIT.
1290
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 688?698, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Three Dependency-and-Boundary Models for Grammar Induction
Valentin I. Spitkovsky
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc., Mountain View, CA, 94043
hiyan@google.com
Daniel Jurafsky
Stanford University, Stanford, CA, 94305
jurafsky@stanford.edu
Abstract
We present a new family of models for unsu-
pervised parsing, Dependency and Boundary
models, that use cues at constituent bound-
aries to inform head-outward dependency tree
generation. We build on three intuitions that
are explicit in phrase-structure grammars but
only implicit in standard dependency formu-
lations: (i) Distributions of words that oc-
cur at sentence boundaries ? such as English
determiners ? resemble constituent edges.
(ii) Punctuation at sentence boundaries fur-
ther helps distinguish full sentences from
fragments like headlines and titles, allow-
ing us to model grammatical differences be-
tween complete and incomplete sentences.
(iii) Sentence-internal punctuation boundaries
help with longer-distance dependencies, since
punctuation correlates with constituent edges.
Our models induce state-of-the-art depen-
dency grammars for many languages without
special knowledge of optimal input sentence
lengths or biased, manually-tuned initializers.
1 Introduction
Natural language is ripe with all manner of bound-
aries at the surface level that align with hierarchical
syntactic structure. From the significance of func-
tion words (Berant et al2006) and punctuation
marks (Seginer, 2007; Ponvert et al2010) as sepa-
rators between constituents in longer sentences ? to
the importance of isolated words in children?s early
vocabulary acquisition (Brent and Siskind, 2001)
? word boundaries play a crucial role in language
learning. We will show that boundary information
can also be useful in dependency grammar induc-
tion models, which traditionally focus on head rather
than fringe words (Carroll and Charniak, 1992).
DT NN VBZ IN DT NN
[The check] is in [the mail].
? ?? ?
Subject
? ?? ?
Object
Figure 1: A partial analysis of our running example.
Consider the example in Figure 1. Because the
determiner (DT) appears at the left edge of the sen-
tence, it should be possible to learn that determiners
may generally be present at left edges of phrases.
This information could then be used to correctly
parse the sentence-internal determiner in the mail.
Similarly, the fact that the noun head (NN) of the ob-
ject the mail appears at the right edge of the sentence
could help identify the noun check as the right edge
of the subject NP. As with jigsaw puzzles, working
inwards from boundaries helps determine sentence-
internal structures of both noun phrases, neither of
which would be quite so clear if viewed separately.
Furthermore, properties of noun-phrase edges are
partially shared with prepositional- and verb-phrase
units that contain these nouns. Because typical head-
driven grammars model valence separately for each
class of head, however, they cannot see that the left
fringe boundary, The check, of the verb-phrase is
shared with its daughter?s, check. Neither of these
insights is available to traditional dependency for-
mulations, which could learn from the boundaries
of this sentence only that determiners might have no
left- and that nouns might have no right-dependents.
We propose a family of dependency parsing mod-
els that are capable of inducing longer-range im-
plications from sentence edges than just fertilities
of their fringe words. Our ideas conveniently lend
themselves to implementations that can reuse much
of the standard grammar induction machinery, in-
cluding efficient dynamic programming routines for
the relevant expectation-maximization algorithms.
688
2 The Dependency and Boundary Models
Our models follow a standard generative story for
head-outward automata (Alshawi, 1996a), restricted
to the split-head case (see below),1 over lexical word
classes {cw}: first, a sentence root cr is chosen, with
probability PATTACH(cr | ?; L); ? is a special start
symbol that, by convention (Klein and Manning,
2004; Eisner, 1996), produces exactly one child, to
its left. Next, the process recurses. Each (head)
word ch generates a left-dependent with probability
1 ? PSTOP( ? | L; ? ? ? ), where dots represent additional
parameterization on which it may be conditioned. If
the child is indeed generated, its identity cd is cho-
sen with probability PATTACH(cd | ch; ? ? ? ), influenced
by the identity of the parent ch and possibly other pa-
rameters (again represented by dots). The child then
generates its own subtree recursively and the whole
process continues, moving away from the head, un-
til ch fails to generate a left-dependent. At that point,
an analogous procedure is repeated to ch?s right, this
time using stopping factors PSTOP( ? | R; ? ? ? ). All parse
trees derived in this way are guaranteed to be projec-
tive and can be described by split-head grammars.
Instances of these split-head automata have been
heavily used in grammar induction (Paskin, 2001b;
Klein and Manning, 2004; Headden et al2009,
inter alia), in part because they allow for efficient
implementations (Eisner and Satta, 1999, ?8) of
the inside-outside re-estimation algorithm (Baker,
1979). The basic tenet of split-head grammars is
that every head word generates its left-dependents
independently of its right-dependents. This as-
sumption implies, for instance, that words? left-
and right-valences ? their numbers of children
to each side ? are also independent. But it does
not imply that descendants that are closer to the
head cannot influence the generation of farther
dependents on the same side. Nevertheless, many
popular grammars for unsupervised parsing behave
as if a word had to generate all of its children
(to one side) ? or at least their count ? before
allowing any of these children themselves to recurse.
For example, Klein and Manning?s (2004) depen-
dency model with valence (DMV) could be imple-
1Unrestricted head-outward automata are strictly more pow-
erful (e.g., they recognize the language anbn in finite state) than
the split-head variants, which process one side before the other.
mented as both head-outward and head-inward au-
tomata. (In fact, arbitrary permutations of siblings
to a given side of their parent would not affect the
likelihood of the modified tree, with the DMV.) We
propose to make fuller use of split-head automata?s
head-outward nature by drawing on information in
partially-generated parses, which contain useful pre-
dictors that, until now, had not been exploited even
in featurized systems for grammar induction (Cohen
and Smith, 2009; Berg-Kirkpatrick et al2010).
Some of these predictors, including the identity
? or even number (McClosky, 2008) ? of already-
generated siblings, can be prohibitively expensive in
sentences above a short length k. For example, they
break certain modularity constraints imposed by the
charts used in O(k3)-optimized algorithms (Paskin,
2001a; Eisner, 2000). However, in bottom-up pars-
ing and training from text, everything about the yield
? i.e., the ordered sequence of all already-generated
descendants, on the side of the head that is in the
process of spawning off an additional child ? is not
only known but also readily accessible. Taking ad-
vantage of this availability, we designed three new
models for dependency grammar induction.
2.1 Dependency and Boundary Model One
DBM-1 conditions all stopping decisions on adja-
cency and the identity of the fringe word ce ? the
currently-farthest descendant (edge) derived by head
ch in the given head-outward direction (dir ? {L, R}):
PSTOP( ? | dir; adj, ce).
In the adjacent case (adj = T), ch is deciding whether
to have any children on a given side: a first child?s
subtree would be right next to the head, so the head
and the fringe words coincide (ch = ce). In the non-
adjacent case (adj = F), these will be different words
and their classes will, in general, not be the same.2
Thus, non-adjacent stopping decisions will be made
independently of a head word?s identity. Therefore,
all word classes will be equally likely to continue to
grow or not, for a specific proposed fringe boundary.
For example, production of The check is involves
two non-adjacent stopping decisions on the left: one
by the noun check and one by the verb is, both of
which stop after generating a first child. In DBM-1,
2Fringe words differ also from other standard dependency
features (Eisner, 1996, ?2.3): parse siblings and adjacent words.
689
DT NN VBZ IN DT NN ?
The check is in the mail .
P = (1?
0
? ?? ?
PSTOP(? | L; T)) ? PATTACH(VBZ | ?; L)
? (1? PSTOP( ? | L; T, VBZ)) ? PATTACH(NN | VBZ; L)
? (1? PSTOP( ? | R; T, VBZ)) ? PATTACH(IN | VBZ; R)
? PSTOP( ? | L; F, DT) // VBZ ? PSTOP( ? | R; F, NN) // VBZ
? (1? PSTOP( ? | L; T, NN))2 ? P2ATTACH(DT | NN; L)
? (1? PSTOP( ? | R; T, IN)) ? PATTACH(NN | IN; R)
? P2STOP( ? | R; T, NN) ? P2STOP( ? | L; F, DT) // NN
? PSTOP( ? | L; T, IN) ? PSTOP( ? | R; F, NN) // IN
? P2STOP( ? | L; T, DT) ? P2STOP( ? | R; T, DT)
? PSTOP(? | L; F)
? ?? ?
1
? PSTOP(? | R; T)
? ?? ?
1
.
Figure 2: Our running example ? a simple sentence and
its unlabeled dependency parse structure?s probability, as
factored by DBM-1; highlighted comments specify heads
associated to non-adjacent stopping probability factors.
this outcome is captured by squaring a shared pa-
rameter belonging to the left-fringe determiner The:
PSTOP( ? | L; F, DT)2 ? instead of by a product of two
factors, such as PSTOP( ? | L; F, NN) ? PSTOP( ? | L; F, VBZ).
In these grammars, dependents? attachment prob-
abilities, given heads, are additionally conditioned
only on their relative positions ? as in traditional
models (Klein and Manning, 2004; Paskin, 2001b):
PATTACH(cd | ch; dir).
Figure 2 shows a completely factored example.
2.2 Dependency and Boundary Model Two
DBM-2 allows different but related grammars to co-
exist in a single model. Specifically, we presuppose
that all sentences are assigned to one of two classes:
complete and incomplete (comp ? {T, F}, for now
taken as exogenous). This model assumes that word-
word (i.e., head-dependent) interactions in the two
domains are the same. However, sentence lengths
? for which stopping probabilities are responsible
? and distributions of root words may be different.
Consequently, an additional comp parameter is
added to the context of two relevant types of factors:
PSTOP( ? | dir; adj, ce, comp);
and PATTACH(cr | ?; L, comp).
For example, the new stopping factors could capture
the fact that incomplete fragments ? such as the
noun-phrases George Morton, headlines Energy and
Odds and Ends, a line item c - Domestic car, dollar
quantity Revenue: $3.57 billion, the time 1:11am,
and the like ? tend to be much shorter than com-
plete sentences. The new root-attachment factors
could further track that incomplete sentences gener-
ally lack verbs, in contrast to other short sentences,
e.g., Excerpts follow:, Are you kidding?, Yes, he
did., It?s huge., Indeed it is., I said, ?NOW??, ?Ab-
solutely,? he said., I am waiting., Mrs. Yeargin de-
clined., McGraw-Hill was outraged., ?It happens.?,
I?m OK, Jack., Who cares?, Never mind. and so on.
All other attachment probabilities PATTACH(cd | ch; dir)
remain unchanged, as in DBM-1. In practice, comp
can indicate presence of sentence-final punctuation.
2.3 Dependency and Boundary Model Three
DBM-3 adds further conditioning on punctuation
context. We introduce another boolean parameter,
cross, which indicates the presence of intervening
punctuation between a proposed head word ch and
its dependent cd. Using this information, longer-
distance punctuation-crossing arcs can be modeled
separately from other, lower-level dependencies, via
PATTACH(cd | ch; dir, cross).
For instance, in Continentals believe that the
strongest growth area will be southern Europe., four
words appear between that and will. Conditioning
on (the absence of) intervening punctuation could
help tell true long-distance relations from impostors.
All other probabilities, PSTOP( ? | dir; adj, ce, comp) and
PATTACH(cr | ?; L, comp), remain the same as in DBM-2.
2.4 Summary of DBMs and Related Models
Head-outward automata (Alshawi, 1996a; Alshawi,
1996b; Alshawi et al2000) played a central part as
generative models for probabilistic grammars, start-
ing with their early adoption in supervised split-head
constituent parsers (Collins, 1997; Collins, 2003).
Table 1 lists some parameterizations that have since
been used by unsupervised dependency grammar in-
ducers sharing their backbone split-head process.
3 Experimental Set-Up and Methodology
We first motivate each model by analyzing the Wall
Street Journal (WSJ) portion of the Penn English
Treebank (Marcus et al1993),3 before delving into
3We converted labeled constituents into unlabeled depen-
dencies using deterministic ?head-percolation? rules (Collins,
690
Split-Head Dependency Grammar PATTACH (head-root) PATTACH (dependent-head) PSTOP (adjacent and not)
GB (Paskin, 2001b) 1 / |{w}| d | h; dir 1 / 2
DMV (Klein and Manning, 2004) cr | ?; L cd | ch; dir ? | dir; adj, ch
EVG (Headden et al2009) cr | ?; L cd | ch; dir, adj ? | dir; adj, ch
DBM-1 (?2.1) cr | ?; L cd | ch; dir ? | dir; adj, ce
DBM-2 (?2.2) cr | ?; L, comp cd | ch; dir ? | dir; adj, ce, comp
DBM-3 (?2.3) cr | ?; L, comp cd | ch; dir, cross ? | dir; adj, ce, comp
Table 1: Parameterizations of the split-head-outward generative process used by DBMs and in previous models.
grammar induction experiments. Although motivat-
ing solely from this treebank biases our discussion
towards a very specific genre of just one language,
it has the advantage of allowing us to make concrete
claims that are backed up by significant statistics.
In the grammar induction experiments that follow,
we will test each model?s incremental contribution
to accuracies empirically, across many disparate lan-
guages. We worked with all 23 (disjoint) train/test
splits from the 2006/7 CoNLL shared tasks (Buch-
holz and Marsi, 2006; Nivre et al2007), span-
ning 19 languages.4 For each data set, we induced
a baseline grammar using the DMV. We excluded
all training sentences with more than 15 tokens to
create a conservative bias, because in this set-up the
baseline is known to excel (Spitkovsky et al2009).
Grammar inducers were initialized using (the same)
uniformly-at-random chosen parse trees of training
sentences (Cohen and Smith, 2010); thereafter, we
applied ?add one? smoothing at every training step.
To fairly compare the models under considera-
tion ? which could have quite different starting
perplexities and ensuing consecutive relative like-
lihoods ? we experimented with two termination
strategies. In one case, we blindly ran each learner
through 40 steps of inside-outside re-estimation, ig-
noring any convergence criteria; in the other case,
we ran until numerical convergence of soft EM?s ob-
jective function or until the likelihood of resulting
Viterbi parse trees suffered ? an ?early-stopping la-
teen EM? strategy (Spitkovsky et al2011a, ?2.3).
We evaluated against all sentences of the blind test
sets (except one 145-token item in Arabic ?07 data).
Table 2 shows experimental results, averaged over
1999), discarding any empty nodes, etc., as is standard practice.
4We did not test on WSJ data because such evaluation would
not be blind, as parse trees from the PTB are our motivating ex-
amples; instead, performance on WSJ serves as a strong base-
line in a separate study (Spitkovsky et al2012a): bootstrapping
of DBMs from mostly incomplete inter-punctuation fragments.
all 19 languages, for the DMV baselines and DBM-1
and 2. We did not test DBM-3 in this set-up because
most sentence-internal punctuation occurs in longer
sentences; instead, DBM-3 will be tested later (see
?7), using most sentences,5 in the final training step
of a curriculum strategy (Bengio et al2009) that we
will propose for DBMs. For the three models tested
on shorter inputs (up to 15 tokens) both terminating
criteria exhibited the same trend; lateen EM consis-
tently scored slightly higher than 40 EM iterations.
Termination Criterion DMV DBM-1 DBM-2
40 steps of EM 33.5 38.8 40.7
early-stopping lateen EM 34.0 39.0 40.9
Table 2: Directed dependency accuracies, averaged over
all 2006/7 CoNLL evaluation sets (all sentences), for the
DMV and two new dependency-and-boundary grammar
inducers (DBM-1,2) ? using two termination strategies.6
4 Dependency and Boundary Model One
The primary difference between DBM-1 and tradi-
tional models, such as the DMV, is that DBM-1 con-
ditions non-adjacent stopping decisions on the iden-
tities of fringe words in partial yields (see ?2.1).
4.1 Analytical Motivation
Treebank data suggests that the class of the fringe
word ? its part-of-speech, ce ? is a better predic-
tor of (non-adjacent) stopping decisions, in a given
direction dir, than the head?s own class ch. A statis-
tical analysis of logistic regressions fitted to the data
shows that the (ch, dir) predictor explains only about
7% of the total variation (see Table 3). This seems
low, although it is much better compared to direction
alone (which explains less than 2%) and slightly bet-
ter than using the (current) number of the head?s de-
5Results for DBM-3 ? given only standard input sentences,
up to length fifteen ? would be nearly identical to DBM-2?s.
6We down-weighed the four languages appearing in both
CoNLL years (see Table 8) by 50% in all reported averages.
691
Non-Adjacent Stop Predictor R2adj AICc
(dir) 0.0149 1,120,200
(n, dir) 0.0726 1,049,175
(ch, dir) 0.0728 1,047,157
(ce, dir) 0.2361 904,102.4
(ch, ce, dir) 0.3320 789,594.3
Table 3: Coefficients of determination (R2) and Akaike
information criteria (AIC), both adjusted for the number
of parameters, for several single-predictor logistic models
of non-adjacent stops, given direction dir; ch is the class
of the head, n is its number of descendants (so far) to that
side, and ce represents the farthest descendant (the edge).
scendants on that side, n, instead of the head?s class.
In contrast, using ce in place of ch boosts explanatory
power to 24%, keeping the number of parameters the
same. If one were willing to roughly square the size
of the model, explanatory power could be improved
further, to 33% (see Table 3), using both ce and ch.
Fringe boundaries thus appear to be informative
even in the supervised case, which is not surprising,
since using just one probability factor (and its com-
plement) to generate very short (geometric coin-flip)
sequences is a recipe for high entropy. But as sug-
gested earlier, fringes should be extra attractive in
unsupervised settings because yields are observable,
whereas heads almost always remain hidden. More-
over, every sentence exposes two true edges (Ha?nig,
2010): integrated over many sample sentence begin-
nings and ends, cumulative knowledge about such
markers can guide a grammar inducer inside long in-
puts, where structure is murky. Table 4 shows distri-
butions of all part-of-speech (POS) tags in the tree-
bank versus in sentence-initial, sentence-final and
sentence-root positions. WSJ often leads with deter-
miners, proper nouns, prepositions and pronouns ?
all good candidates for starting English phrases; and
its sentences usually end with various noun types,
again consistent with our running example.
4.2 Experimental Results
Table 2 shows DBM-1 to be substantially more ac-
curate than the DMV, on average: 38.8 versus 33.5%
after 40 steps of EM.7 Lateen termination improved
both models? accuracies slightly, to 39.0 and 34.0%,
respectively, with DBM-1 scoring five points higher.
7DBM-1?s 39% average accuracy with uniform-at-random
initialization is two points above DMV?s scores with the ?ad-
hoc harmonic? strategy, 37% (Spitkovsky et al2011a, Table 5).
% of All First Last Sent. Frag.
POS Tokens Tokens Tokens Roots Roots
NN 15.94 4.31 36.67 0.10 23.40
IN 11.85 13.54 0.57 0.24 4.33
NNP 11.09 20.49 12.85 0.02 32.02
DT 9.84 23.34 0.34 0.00 0.04
JJ 7.32 4.33 3.74 0.01 1.15
NNS 7.19 4.49 20.64 0.15 17.12
CD 4.37 1.29 6.92 0.00 3.27
RB 3.71 5.96 3.88 0.00 1.50
VBD 3.65 0.09 3.52 46.65 0.93
VB 3.17 0.44 1.67 0.48 6.81
CC 2.86 5.93 0.00 0.00 0.00
TO 2.67 0.37 0.05 0.02 0.44
VBZ 2.57 0.17 1.65 28.31 0.93
VBN 2.42 0.61 2.57 0.65 1.28
PRP 2.08 9.04 1.34 0.00 0.00
VBG 1.77 1.26 0.64 0.10 0.97
VBP 1.50 0.05 0.61 14.33 0.71
MD 1.17 0.07 0.05 8.88 0.57
POS 1.05 0.00 0.11 0.01 0.04
PRP$ 1.00 0.90 0.00 0.00 0.00
WDT 0.52 0.08 0.00 0.01 0.13
JJR 0.39 0.18 0.43 0.00 0.09
RP 0.32 0.00 0.42 0.00 0.00
NNPS 0.30 0.20 0.56 0.00 2.96
WP 0.28 0.42 0.01 0.01 0.04
WRB 0.26 0.78 0.02 0.01 0.31
JJS 0.23 0.27 0.06 0.00 0.00
RBR 0.21 0.20 0.54 0.00 0.04
EX 0.10 0.75 0.00 0.00 0.00
RBS 0.05 0.06 0.01 0.00 0.00
PDT 0.04 0.08 0.00 0.00 0.00
FW 0.03 0.01 0.05 0.00 0.09
WP$ 0.02 0.00 0.00 0.00 0.00
UH 0.01 0.08 0.05 0.00 0.62
SYM 0.01 0.11 0.01 0.00 0.18
LS 0.01 0.09 0.00 0.00 0.00
Table 4: Empirical distributions for non-punctuation part-
of-speech tags in WSJ, ordered by overall frequency, as
well as distributions for sentence boundaries and for the
roots of complete and incomplete sentences. (A uniform
distribution would have 1/36 = 2.7% for all POS-tags.)
?
1?
?
x
?pxqx All First Last Sent. Frag.
Uniform 0.48 0.58 0.64 0.79 0.65
All 0.35 0.40 0.79 0.42
First 0.59 0.94 0.57
Last 0.83 0.29
Sent. 0.86
Table 5: A distance matrix for all pairs of probability dis-
tributions over POS-tags shown in Table 4 and the uni-
form distribution; the BC- (or Hellinger) distance (Bhat-
tacharyya, 1943; Nikulin, 2002) between discrete distri-
butions p and q (over x ? X ) ranges from zero (iff p = q)
to one (iff p ? q = 0, i.e., when they do not overlap at all).
692
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75
250
500
750
1,000
1,250
1,500
1,750
2,000
(Box-and-whiskers quartile diagrams.)
1
3
7
17
76
1
14
20
27
171
l
Distributions of Sentence Lengths (l) in WSJ
Figure 3: Histograms of lengths (in tokens) for 2,261 non-clausal fragments (red) and other sentences (blue) in WSJ.
5 Dependency and Boundary Model Two
DBM-2 adapts DBM-1 grammars to two classes
of inputs (complete sentences and incomplete frag-
ments) by forking off new, separate multinomials for
stopping decisions and root-distributions (see ?2.2).
5.1 Analytical Motivation
Unrepresentative short sentences ? such as head-
lines and titles ? are common in news-style data
and pose a known nuisance to grammar inducers.
Previous research sometimes took radical measures
to combat the problem: for example, Gillenwater
et al2009) excluded all sentences with three or
fewer tokens from their experiments; and Marec?ek
and Zabokrtsky? (2011) enforced an ?anti-noun-root?
policy to steer their Gibbs sampler away from the
undercurrents caused by the many short noun-phrase
fragments (among sentences up to length 15, in
Czech data). We refer to such snippets of text as
?incomplete sentences? and focus our study of WSJ
on non-clausal data (as signaled by top-level con-
stituent annotations whose first character is not S).8
Table 4 shows that roots of incomplete sentences,
which are dominated by nouns, barely resemble the
other roots, drawn from more traditional verb and
modal types. In fact, these two empirical root dis-
tributions are more distant from one another than ei-
ther is from the uniform distribution, in the space of
discrete probability distributions over POS-tags (see
Table 5). Of the distributions we considered, only
sentence boundaries are as or more different from
8I.e., separating top-level types {S, SINV, SBARQ, SQ, SBAR}
from the rest (ordered by frequency): {NP, FRAG, X, PP, . . .}.
(complete) roots, suggesting that heads of fragments
too may warrant their own multinomial in the model.
Further, incomplete sentences are uncharacteris-
tically short (see Figure 3). It is this property that
makes them particularly treacherous to grammar in-
ducers, since by offering few options of root posi-
tions they increase the chances that a learner will
incorrectly induce nouns to be heads. Given that ex-
pected lengths are directly related to stopping deci-
sions, it could make sense to also model the stopping
probabilities of incomplete sentences separately.
5.2 Experimental Results
Since it is not possible to consult parse trees during
grammar induction (to check whether an input sen-
tence is clausal), we opted for a proxy: presence of
sentence-final punctuation. Using punctuation to di-
vide input sentences into two groups, DBM-2 scored
higher: 40.9, up from 39.0% accuracy (see Table 2).
After evaluating these multi-lingual experiments,
we checked how well our proxy corresponds to ac-
tual clausal sentences in WSJ. Table 6 shows the bi-
nary confusion matrix having a fairly low (but posi-
tive) Pearson correlation coefficient. False positives
r? ? 0.31 Clausal non-Clausal Total
Punctuation 46,829 1,936 48,765
no Punctuation 118 325 443
Total 46,947 2,261 49,208
Table 6: A contingency table for clausal sentences and
trailing punctuation in WSJ; the mean square contingency
coefficient r? signifies a low degree of correlation. (For
two binary variables, r? is equivalent to Karl Pearson?s
better-known product-moment correlation coefficient, ?.)
693
include parenthesized expressions that are marked
as noun-phrases, such as (See related story: ?Fed
Ready to Inject Big Funds?: WSJ Oct. 16, 1989);
false negatives can be headlines having a main verb,
e.g., Population Drain Ends For Midwestern States.
Thus, our proxy is not perfect but seems to be toler-
able in practice. We suspect that identities of punc-
tuation marks (Collins, 2003, Footnote 13) ? both
sentence-final and sentence-initial ? could be of ex-
tra assistance in grammar induction, specifically for
grouping imperatives, questions, and so forth.
6 Dependency and Boundary Model Three
DBM-3 exploits sentence-internal punctuation con-
texts by modeling punctuation-crossing dependency
arcs separately from other attachments (see ?2.3).
6.1 Analytical Motivation
Many common syntactic relations, such as between
a determiner and a noun, are unlikely to hold over
long distances. (In fact, 45% of all head-percolated
dependencies in WSJ are between adjacent words.)
However, some common constructions are more re-
mote: e.g., subordinating conjunctions are, on av-
erage, 4.8 tokens away from their dependent modal
verbs. Sometimes longer-distance dependencies can
be vetted using sentence-internal punctuation marks.
It happens that the presence of punctuation be-
tween such conjunction (IN) and verb (MD) types
serves as a clue that they are not connected (see Ta-
ble 7a); by contrast, a simpler cue ? whether these
words are adjacent ? is, in this case, hardly of any
use (see Table 7b). Conditioning on crossing punc-
tuation could be of help then, playing a role simi-
lar to that of comma-counting (Collins, 1997, ?2.1)
? and ?verb intervening? (Bikel, 2004, ?5.1) ? in
early head-outward models for supervised parsing.
a) r? ? ?0.40 Attached not Attached Total
Punctuation 337 7,645 7,982
no Punctuation 2,144 4,040 6,184
Total 2,481 11,685 14,166
non-Adjacent 2,478 11,673 14,151
Adjacent 3 12 15
b) r? ? +0.00 Attached not Attached Total
Table 7: Contingency tables for IN right-attaching MD,
among closest ordered pairs of these tokens in WSJ sen-
tences with punctuation, versus: (a) presence of interven-
ing punctuation; and (b) presence of intermediate words.
6.2 Experimental Results Postponed
As we mentioned earlier (see ?3), there is little point
in testing DBM-3 with shorter sentences, since most
sentence-internal punctuation occurs in longer in-
puts. Instead, we will test this model in a final step of
a staged training strategy, with more data (see ?7.3).
7 A Curriculum Strategy for DBMs
We propose to train up to DBM-3 iteratively ?
by beginning with DBM-1 and gradually increasing
model complexity through DBM-2, drawing on the
intuitions of IBM translation models 1?4 (Brown et
al., 1993). Instead of using sentences of up to 15 to-
kens, as in all previous experiments (?4?5), we will
now make use of nearly all available training data:
up to length 45 (out of concern for efficiency), dur-
ing later stages. In the first stage, however, we will
use only a subset of the data with DBM-1, in a pro-
cess sometimes called curriculum learning (Bengio
et al2009; Krueger and Dayan, 2009, inter alia).
Our grammar inducers will thus be ?starting small?
in both senses suggested by Elman (1993): simulta-
neously scaffolding on model- and data-complexity.
7.1 Scaffolding Stage #1: DBM-1
We begin by training DBM-1 on sentences with-
out sentence-internal punctuation but with at least
one trailing punctuation mark. Our goal is to avoid,
when possible, overly specific arbitrary parameters
like the ?15 tokens or less? threshold used to select
training sentences. Unlike DBM-2 and 3, DBM-1
does not model punctuation or sentence fragments,
so we instead explicitly restrict its attention to this
cleaner subset of the training data, which takes ad-
vantage of the fact that punctuation may generally
correlate with sentence complexity (Frank, 2000).9
Aside from input sentence selection, our exper-
imental set-up here remained identical to previous
training of DBMs (?4?5). Using this new input data,
DBM-1 averaged 40.7% accuracy (see Table 8).
This is slightly higher than the 39.0% when using
sentences up to length 15, suggesting that our heuris-
tic for clean, simple sentences may be a useful one.
9More incremental training strategies are the subject of an
unpublished companion manuscript (Spitkovsky et al2012a).
694
Directed Dependency Accuracies for: Best of State-of-the-Art Systems
CoNLL Year this Work (@10) Monolingual; POS- Cross-Lingual
& Language DMV DBM-1 DBM-2 DBM-3 +inference (i) Agnostic (ii) Identified (iii) Transfer
Arabic 2006 12.9 10.6 11.0 11.1 10.9 (34.5) 33.4 SCAJ6 ? 50.2 Sbg
?7 36.6 43.9 44.0 44.4 44.9 (48.8) 55.6 RF 54.6 RFH1 ?
Basque ?7 32.7 34.1 33.0 32.7 33.3 (36.5) 43.6 SCAJ5 34.7 MZNR ?
Bulgarian ?7 24.7 59.4 63.6 64.6 65.2 (70.4) 44.3 SCAJ5 53.9 RFH1&2 70.3 Spt
Catalan ?7 41.1 61.3 61.1 61.1 62.1 (78.1) 63.8 SCAJ5 56.3 MZNR ?
Chinese ?6 50.4 63.1 63.0 63.2 63.2 (65.7) 63.6 SCAJ6 ? ?
?7 55.3 56.8 57.0 57.1 57.0 (59.8) 58.5 SCAJ6 34.6 MZNR ?
Czech ?6 31.5 51.3 52.8 53.0 55.1 (61.8) 50.5 SCAJ5 ? ?
?7 34.5 50.5 51.2 53.3 54.2 (67.3) 49.8 SCAJ5 42.4 RFH1&2 ?
Danish ?6 22.4 21.3 19.9 21.8 22.2 (27.4) 46.0 RF 53.1 RFH1&2 56.5 Sar
Dutch ?6 44.9 45.9 46.5 46.0 46.6 (48.6) 32.5 SCAJ5 48.8 RFH1&2 65.7 MPHm:p
English ?7 32.3 29.2 28.6 29.0 29.6 (51.4) 50.3 SAJ 23.8 MZNR 45.7 MPHel
German ?6 27.7 36.3 37.9 38.4 39.1 (52.1) 33.5 SCAJ5 21.8 MZNR 56.7 MPHm:d
Greek ?6 36.3 28.1 26.1 26.1 26.9 (36.8) 39.0 MZ 33.4 MZNR 65.1 MPHm:p
Hungarian ?7 23.6 43.2 52.1 57.4 58.2 (68.4) 48.0 MZ 48.1 MZNR ?
Italian ?7 25.5 41.7 39.8 39.9 40.7 (41.8) 57.5 MZ 60.6 MZNR 69.1 MPHpt
Japanese ?6 42.2 22.8 22.7 22.7 22.7 (32.5) 56.6 SCAJ5 53.5 MZNR ?
Portuguese ?6 37.1 68.9 72.3 71.1 72.4 (80.6) 43.2 MZ 55.8 RFH1&2 76.9 Sbg
Slovenian ?6 33.4 30.4 33.0 34.1 35.2 (36.8) 33.6 SCAJ5 34.6 MZNR ?
Spanish ?6 22.0 25.0 26.7 27.1 28.2 (51.8) 53.0 MZ 54.6 MZNR 68.4 MPHit
Swedish ?6 30.7 48.6 50.3 50.0 50.7 (63.2) 50.0 SCAJ6 34.3 RFH1&2 68.0 MPHm:p
Turkish ?6 43.4 32.9 33.7 33.4 34.4 (38.1) 40.9 SAJ 61.3 RFH1 ?
?7 58.5 44.6 44.2 43.7 44.8 (44.4) 48.8 SCAJ6 ? ?
Average: 33.6 40.7 41.7 42.2 42.9 (51.9) 38.2 SCAJ6 (best average, not an average of bests)
Table 8: Average accuracies over CoNLL evaluation sets (all sentences), for the DMV baseline and DBM1?3 trained
with a curriculum strategy, and state-of-the-art results for systems that: (i) are also POS-agnostic and monolingual,
including SCAJ (Spitkovsky et al2011a, Tables 5?6) and SAJ (Spitkovsky et al2011b); (ii) rely on gold POS-tag
identities to discourage noun roots (Marec?ek and Zabokrtsky?, 2011, MZ) or to encourage verbs (Rasooli and Faili,
2012, RF); and (iii) transfer delexicalized parsers (S?gaard, 2011a, S) from resource-rich languages with transla-
tions (McDonald et al2011, MPH). DMV and DBM-1 trained on simple sentences, from uniform; DBM-2 and 3
trained on most sentences, from DBM-1 and 2, respectively; +inference is DBM-3 with punctuation constraints.
7.2 Scaffolding Stage #2: DBM-2? DBM-1
Next, we trained on all sentences up to length 45.
Since these inputs are punctuation-rich, in both re-
maining stages we used the constrained Viterbi EM
set-up suggested by Spitkovsky et al2011b) in-
stead of plain soft EM; we employ an early termina-
tion strategy, quitting hard EM as soon as soft EM?s
objective suffers (Spitkovsky et al2011a). Punc-
tuation was converted into Viterbi-decoding con-
straints during training using the so-called loose
method, which stipulates that all words in an inter-
punctuation fragment must be dominated by a single
(head) word, also from that fragment ? with only
these head words allowed to attach the head words
of other fragments, across punctuation boundaries.
To adapt to full data, we initialized DBM-2 using
Viterbi parses from the previous stage (?7.1), plus
uniformly-at-random chosen dependency trees for
the new complex and incomplete sentences, subject
to punctuation-induced constraints. This approach
improved parsing accuracies to 41.7% (see Table 8).
7.3 Scaffolding Stage #3: DBM-3? DBM-2
Next, we repeated the training process of the pre-
vious stage (?7.2) using DBM-3. To initialize this
model, we combined the final instance of DBM-2
with uniform multinomials for punctuation-crossing
attachment probabilities (see ?2.3). As a result, av-
erage performance improved to 42.2% (see Table 8).
Lastly, we applied punctuation constraints also in
inference. Here we used the sprawl method ? a
more relaxed approach than in training, allowing ar-
bitrary words to attach inter-punctuation fragments
(provided that each entire fragment still be derived
695
by one of its words) ? as suggested by Spitkovsky
et al2011b). This technique increased DBM-3?s
average accuracy to 42.9% (see Table 8). Our fi-
nal result substantially improves over the baseline?s
33.6% and compares favorably to previous work.10
8 Discussion and the State-of-the-Art
DBMs come from a long line of head-outward mod-
els for dependency grammar induction yet their gen-
erative processes feature important novelties. One
is conditioning on more observable state ? specifi-
cally, the left and right end words of a phrase being
constructed ? than in previous work. Another is al-
lowing multiple grammars ? e.g., of complete and
incomplete sentences ? to coexist in a single model.
These improvements could make DBMs quick-and-
easy to bootstrap directly from any available partial
bracketings (Pereira and Schabes, 1992), for exam-
ple capitalized phrases (Spitkovsky et al2012b).
The second part of our work ? the use of a cur-
riculum strategy to train DBM-1 through 3 ? elim-
inates having to know tuned cut-offs, such as sen-
tences with up to a predetermined number of tokens.
Although this approach adds some complexity, we
chose conservatively, to avoid overfitting settings
of sentence length, convergence criteria, etc.: stage
one?s data is dictated by DBM-1 (which ignores
punctuation); subsequent stages initialize additional
pieces uniformly: uniform-at-random parses for new
data and uniform multinomials for new parameters.
Even without curriculum learning ? trained with
vanilla EM ? DBM-2 and 1 are already strong.
Further boosts to accuracy could come from em-
ploying more sophisticated optimization algorithms,
e.g., better EM (Samdani et al2012), constrained
Gibbs sampling (Marec?ek and Zabokrtsky?, 2011) or
locally-normalized features (Berg-Kirkpatrick et al
2010). Other orthogonal dependency grammar in-
duction techniques ? including ones based on uni-
versal rules (Naseem et al2010) ? may also ben-
efit in combination with DBMs. Direct comparisons
to previous work require some care, however, as
there are several classes of systems that make dif-
ferent assumptions about training data (see Table 8).
10Note that DBM-1?s 39% average accuracy with standard
training (see Table 2) was already nearly a full point higher than
that of any single previous best system (SCAJ6 ? see Table 8).
8.1 Monolingual POS-Agnostic Inducers
The first type of grammar inducers, including our
own approach, uses standard training and test data
sets for each language, with gold part-of-speech tags
as anonymized word classes. For the purposes of
this discussion, we also include in this group trans-
ductive learners that may train on data from the test
sets. Our DBM-3 (decoded with punctuation con-
straints) does well among such systems ? for which
accuracies on all sentence lengths of the evaluation
sets are reported ? attaining highest scores for 8 of
19 languages; the DMV baseline is still state-of-the-
art for one language; and the remaining 10 bests are
split among five other recent systems (see Table 8).11
Half of the five came from various lateen EM strate-
gies (Spitkovsky et al2011a) for escaping and/or
avoiding local optima. These heuristics are compat-
ible with how we trained our DBMs and could po-
tentially provide further improvement to accuracies.
Overall, the final scores of DBM-3 were better, on
average, than those of any other single system: 42.9
versus 38.2% (Spitkovsky et al2011a, Table 6).
The progression of scores for DBM-1 through 3
without using punctuation constraints in inference
? 40.7, 41.7 and 42.2% ? fell entirely above this
previous state-of-the-art result as well; the DMV
baseline ? also trained on sentences without inter-
nal but with final punctuation ? averaged 33.6%.
8.2 Monolingual POS-Identified Inducers
The second class of techniques assumes knowledge
about identities of part-of-speech tags (Naseem et
al., 2010), i.e., which word tokens are verbs, which
ones are nouns, etc. Such grammar inducers gener-
ally do better than the first kind ? e.g., by encour-
aging verbocentricity (Gimpel and Smith, 2011) ?
though even here our results appear to be compet-
itive. In fact, to our surprise, only in 5 of 19 lan-
guages a ?POS-identified? system performed better
than all of the ?POS-agnostic? ones (see Table 8).
8.3 Multi-Lingual Semi-Supervised Parsers
The final broad class of related algorithms we con-
sidered extends beyond monolingual data and uses
11For Turkish ?06, the ?right-attach? baseline outperforms
even the DMV, at 65.4% (Rasooli and Faili, 2012, Table 1); an
important difference between 2006 and 2007 CoNLL data sets
has to do with segmentation of morphologically-rich languages.
696
both identities of POS-tags and/or parallel bitexts
to transfer (supervised) delexicalized parsers across
languages. Parser projection is by far the most suc-
cessful approach to date and we hope that it too
may stand to gain from our modeling improvements.
Of the 10 languages for which we found results
in the literature, transferred parsers underperformed
the grammar inducers in only one case: on En-
glish (see Table 8). The unsupervised system that
performed better used a special ?weighted? initial-
izer (Spitkovsky et al2011b, ?3.1) that worked well
for English (but less so for many other languages).
DBMs may be able to improve initialization. For
example, modeling of incomplete sentences could
help in incremental initialization strategies like baby
steps (Spitkovsky et al2009), which are likely sen-
sitive to the proverbial ?bum steer? from unrepresen-
tative short fragments, pace Tu and Honavar (2011).
8.4 Miscellaneous Systems on Short Sentences
Several recent systems (Cohen et al2011; S?gaard,
2011b; Naseem et al2010; Gillenwater et al2010;
Berg-Kirkpatrick and Klein, 2010, inter alia) are ab-
sent from Table 8 because they do not report perfor-
mance for all sentence lengths. To facilitate com-
parison with this body of important previous work,
we also tabulated final accuracies for the ?up-to-ten
words? task under heading @10: 51.9%, on average.
9 Conclusion
Although a dependency parse for a sentence can be
mapped to a constituency parse (Xia and Palmer,
2001), the probabilistic models generating them use
different conditioning: dependency grammars focus
on the relationship between arguments and heads,
constituency grammars on the coherence of chunks
covered by non-terminals. Since redundant views of
data can make learning easier (Blum and Mitchell,
1998), integrating aspects of both constituency and
dependency ought to be able to help grammar in-
duction. We have shown that this insight is correct:
dependency grammar inducers can gain from mod-
eling boundary information that is fundamental to
constituency (i.e., phrase-structure) formalisms.
DBMs are a step in the direction towards mod-
eling constituent boundaries jointly with head de-
pendencies. Further steps must involve more tightly
coupling the two frameworks, as well as showing
ways to incorporate both kinds of information in
other state-of-the art grammar induction paradigms.
Acknowledgments
We thank Roi Reichart and Marta Recasens, for many helpful
comments on draft versions of this paper, and Marie-Catherine
de Marneffe, Roy Schwartz, Mengqiu Wang and the anonymous
reviewers, for their apt recommendations. Funded, in part, by
Defense Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program, under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusion or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the view of the DARPA, AFRL, or the US government.
First author is grateful to Cindy Chan for her friendship and
support over many long months leading up to this publication.
References
H. Alshawi, S. Bangalore, and S. Douglas. 2000. Learning
dependency translation models as collections of finite-state
head transducers. Computational Linguistics, 26.
H. Alshawi. 1996a. Head automata for speech translation. In
ICSLP.
H. Alshawi. 1996b. Method and apparatus for an improved
language recognition system. US Patent 1999/5870706.
J. K. Baker. 1979. Trainable grammars for speech recognition.
In Speech Communication Papers for the 97th Meeting of the
Acoustical Society of America.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009.
Curriculum learning. In ICML.
J. Berant, Y. Gross, M. Mussel, B. Sandbank, E. Ruppin, and
S. Edelman. 2006. Boosting unsupervised grammar induc-
tion by splitting complex sentences on function words. In
BUCLD.
T. Berg-Kirkpatrick and D. Klein. 2010. Phylogenetic gram-
mar induction. In ACL.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with fea-
tures. In NAACL-HLT.
A. Bhattacharyya. 1943. On a measure of divergence between
two statistical populations defined by their probability distri-
butions. BCMS, 35.
D. M. Bikel. 2004. Intricacies of Collins? parsing model. Com-
putational Linguistics, 30.
A. Blum and T. Mitchell. 1998. Combining labeled and unla-
beled data with co-training. In COLT.
M. R. Brent and J. M. Siskind. 2001. The role of exposure to
isolated words in early vocabulary development. Cognition,
81.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguistics, 19.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In CoNLL.
697
G. Carroll and E. Charniak. 1992. Two experiments on learning
probabilistic dependency grammars from corpora. Technical
report, Brown University.
S. B. Cohen and N. A. Smith. 2009. Shared logistic normal dis-
tributions for soft parameter tying in unsupervised grammar
induction. In NAACL-HLT.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs:
Hardness results and competitiveness of uniform initializa-
tion. In ACL.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsupervised
structure prediction with non-parallel multilingual guidance.
In EMNLP.
M. Collins. 1997. Three generative, lexicalised models for sta-
tistical parsing. In ACL.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
M. Collins. 2003. Head-driven statistical models for natural
language parsing. Computational Linguistics, 29.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head-automaton grammars. In
ACL.
J. M. Eisner. 1996. An empirical comparison of probability
models for dependency grammar. Technical report, IRCS.
J. Eisner. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In H. C. Bunt and A. Nijholt, editors,
Advances in Probabilistic and Other Parsing Technologies.
Kluwer Academic Publishers.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
R. Frank. 2000. From regular to context-free to mildly context-
sensitive tree rewriting systems: The path of child language
acquisition. In A. Abeille? and O. Rambow, editors, Tree
Adjoining Grammars: Formalisms, Linguistic Analysis and
Processing. CSLI Publications.
J. Gillenwater, K. Ganchev, J. Grac?a, B. Taskar, and F. Pereira.
2009. Sparsity in grammar induction. In NIPS: Gram-
mar Induction, Representation of Language and Language
Learning.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and B. Taskar.
2010. Posterior sparsity in unsupervised dependency pars-
ing. Technical report, University of Pennsylvania.
K. Gimpel and N. A. Smith. 2011. Concavity and initialization
for unsupervised dependency grammar induction. Technical
report, CMU.
C. Ha?nig. 2010. Improvements in unsupervised co-occurrence
based parsing. In CoNLL.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with richer con-
texts and smoothing. In NAACL-HLT.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In ACL.
K. A. Krueger and P. Dayan. 2009. Flexible shaping: How
learning in small steps helps. Cognition, 110.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. Marec?ek and Z. Zabokrtsky?. 2011. Gibbs sampling with
treeness constraint in unsupervised dependency parsing. In
ROBUS.
D. McClosky. 2008. Modeling valence effects in unsupervised
grammar induction. Technical report, Brown University.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-source trans-
fer of delexicalized dependency parsers. In EMNLP.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010. Using
universal linguistic knowledge to guide grammar induction.
In EMNLP.
M. S. Nikulin. 2002. Hellinger distance. In M. Hazewinkel,
editor, Encyclopaedia of Mathematics. Kluwer Academic
Publishers.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In EMNLP-CoNLL.
M. A. Paskin. 2001a. Cubic-time parsing and learning algo-
rithms for grammatical bigram models. Technical report,
UCB.
M. A. Paskin. 2001b. Grammatical bigrams. In NIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
E. Ponvert, J. Baldridge, and K. Erk. 2010. Simple unsuper-
vised identification of low-level constituents. In ICSC.
M. S. Rasooli and H. Faili. 2012. Fast unsupervised depen-
dency parsing with arc-standard transitions. In ROBUS-
UNSUP.
R. Samdani, M.-W. Chang, and D. Roth. 2012. Unified expec-
tation maximization. In NAACL-HLT.
Y. Seginer. 2007. Learning Syntactic Structure. Ph.D. thesis,
University of Amsterdam.
A. S?gaard. 2011a. Data point selection for cross-language
adaptation of dependency parsers. In ACL.
A. S?gaard. 2011b. From ranked words to dependency trees:
two-stage unsupervised non-projective dependency parsing.
In TextGraphs.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby
Steps: How ?Less is More? in unsupervised dependency
parsing. In NIPS: Grammar Induction, Representation of
Language and Language Learning.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen
EM: Unsupervised training with multiple objectives, applied
to dependency grammar induction. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctu-
ation: Making a point in unsupervised dependency parsing.
In CoNLL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012a. Boot-
strapping dependency grammar inducers from incomplete
sentence fragments via austere models ? the ?wabi-sabi? of
unsupervised parsing. In submission.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012b. Capi-
talization cues improve dependency grammar induction. In
WILS.
K. Tu and V. Honavar. 2011. On the utility of curricula in
unsupervised learning of probabilistic grammars. In IJCAI.
F. Xia and M. Palmer. 2001. Converting dependency structures
to phrase structures. In HLT.
698
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1983?1995,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Breaking Out of Local Optima with Count Transforms
and Model Recombination: A Study in Grammar Induction
Valentin I. Spitkovsky
valentin@cs.stanford.edu
Hiyan Alshawi
hiyan@google.com
Daniel Jurafsky
jurafsky@stanford.edu
Abstract
Many statistical learning problems in NLP call
for local model search methods. But accu-
racy tends to suffer with current techniques,
which often explore either too narrowly or too
broadly: hill-climbers can get stuck in local
optima, whereas samplers may be inefficient.
We propose to arrange individual local opti-
mizers into organized networks. Our building
blocks are operators of two types: (i) trans-
form, which suggests new places to search, via
non-random restarts from already-found local
optima; and (ii) join, which merges candidate
solutions to find better optima. Experiments
on grammar induction show that pursuing dif-
ferent transforms (e.g., discarding parts of a
learned model or ignoring portions of train-
ing data) results in improvements. Groups of
locally-optimal solutions can be further per-
turbed jointly, by constructing mixtures. Us-
ing these tools, we designed several modu-
lar dependency grammar induction networks
of increasing complexity. Our complete sys-
tem achieves 48.6% accuracy (directed depen-
dency macro-average over all 19 languages in
the 2006/7 CoNLL data) ? more than 5%
higher than the previous state-of-the-art.
1 Introduction
Statistical methods for grammar induction often boil
down to solving non-convex optimization problems.
Early work attempted to locally maximize the likeli-
hood of a corpus, using EM to estimate probabilities
of dependency arcs between word bigrams (Paskin
2001a; 2001b). That parsing model has since been
extended to make unsupervised learning more feasi-
ble (Klein and Manning, 2004; Headden et al, 2009;
Spitkovsky et al, 2012b). But even the latest tech-
niques can be quite error-prone and sensitive to ini-
tialization, because of approximate, local search.
In theory, global optima can be found by enumer-
ating all parse forests that derive a corpus, though
this is usually prohibitively expensive in practice. A
preferable brute force approach is sampling, as in
Markov-chain Monte Carlo (MCMC) and random
restarts (Hu et al, 1994), which hit exact solutions
eventually. Restarts can be giant steps in a parameter
space that undo all previous work. At the other ex-
treme, MCMC may cling to a neighborhood, reject-
ing most proposed moves that would escape a local
attractor. Sampling methods thus take unbounded
time to solve a problem (and can?t certify optimal-
ity) but are useful for finding approximate solutions
to grammar induction (Cohn et al, 2011; Marec?ek
and ?Zabokrtsky?, 2011; Naseem and Barzilay, 2011).
We propose an alternative (deterministic) search
heuristic that combines local optimization via EM
with non-random restarts. Its new starting places are
informed by previously found solutions, unlike con-
ventional restarts, but may not resemble their prede-
cessors, unlike typical MCMC moves. We show that
one good way to construct such steps in a parame-
ter space is by forgetting some aspects of a learned
model. Another is by merging promising solutions,
since even simple interpolation (Jelinek and Mercer,
1980) of local optima may be superior to all of the
originals. Informed restarts can make it possible to
explore a combinatorial search space more rapidly
and thoroughly than with traditional methods alone.
2 Abstract Operators
Let C be a collection of counts ? the sufficient
statistics from which a candidate solution to an
optimization problem could be computed, e.g., by
smoothing and normalizing to yield probabilities.
The counts may be fractional and solutions could
take the form of multinomial distributions. A local
optimizer L will convert C into C? = LD(C) ? an
updated collection of counts, resulting in a proba-
bilistic model that is no less (and hopefully more)
consistent with a data set D than the original C:
(1)
LDC C?
1983
Unless C? is a global optimum, we should be able
to make further improvements. But if L is idempo-
tent (and ran to convergence) then L(L(C)) = L(C).
Given only C and LD, the single-node optimization
network above would be the minimal search pattern
worth considering. However, if we had another opti-
mizer L? ? or a fresh starting point C? ? then more
complicated networks could become useful.
2.1 Transforms (Unary)
New starts could be chosen by perturbing an existing
solution, as in MCMC, or independently of previous
results, as in random restarts. We focus on interme-
diate changes to C, without injecting randomness.
All of our transforms involve selective forgetting
or filtering. For example, if the probabilistic model
that is being estimated decomposes into independent
constituents (e.g., several multinomials) then a sub-
set of them can be reset to uniform distributions, by
discarding associated counts from C. In text classifi-
cation, this could correspond to eliminating frequent
or rare tokens from bags-of-words. We use circular
shapes to represent such model ablation operators:
(2)C
An orthogonal approach might separate out vari-
ous counts in C by their provenance. For instance,
if D consisted of several heterogeneous data sources,
then the counts from some of them could be ignored:
a classifier might be estimated from just news text.
We will use squares to represent data-set filtering:
(3)C
Finally, if C represents a mixture of possible inter-
pretations over D ? e.g., because it captures the out-
put of a ?soft? EM algorithm ? contributions from
less likely, noisier completions could also be sup-
pressed (and their weights redistributed to the more
likely ones), as in ?hard? EM. Diamonds will repre-
sent plain (single) steps of Viterbi training:
(4)C
2.2 Joins (Binary)
Starting from different initializers, say C1 and C2,
it may be possible for L to arrive at distinct local
optima, C?1 6= C?2 . The better of the two solutions,
according to likelihood LD of D, could then be se-
lected ? as is standard practice when sampling.
Our joining technique could do better than either
C?1 or C
?
2 , by entertaining also a third possibility,
which combines the two candidates. We construct
a mixture model by adding together all counts from
C?1 and C?2 into C+ = C?1 + C?2 . Original initializers
C1, C2 will, this way, have equal pull on the merged
model,1 regardless of nominal size (because C?1 , C?2
will have converged using a shared training set, D).
We return the best of C?1 , C?2 and C?+ = L(C+). This
approach may uncover more (and never returns less)
likely solutions than choosing among C?1 , C?2 alone:
(5)
LD
LD
LD
+
arg
M
A
X
L
D
C1
C?1 = L(C1)
C2
C?2 = L(C2)
C?1 + C?2 = C+
We will use a short-hand notation to represent the
combiner network diagrammed above, less clutter:
(6)
LDC2
C1
3 The Task and Methodology
We apply transform and join paradigms to grammar
induction, an important problem of computational
linguistics that involves notoriously difficult objec-
tives (Pereira and Schabes, 1992; de Marcken, 1995;
Gimpel and Smith, 2012, inter alia). The goal is to
induce grammars capable of parsing unseen text. In-
put, in both training and testing, is a sequence of to-
kens labeled as: (i) a lexical item and its category,
(w, cw); (ii) a punctuation mark; or (iii) a sentence
boundary. Output is unlabeled dependency trees.
3.1 Models and Data
We constrain all parse structures to be projective, via
dependency-and-boundary grammars (Spitkovsky et
al., 2012a; 2012b): DBMs 0?3 are head-outward
generative parsing models (Alshawi, 1996) that dis-
tinguish complete sentences from incomplete frag-
ments in a corpus D: Dcomp comprises inputs ending
with punctuation; Dfrag = D ? Dcomp is everything
1If desired, a scaling factor could be used to bias C+ towards
either C?1 or C?2 , for example based on their likelihood ratio.
1984
else. The ?complete? subset is further partitioned
into simple sentences, Dsimp ? Dcomp, with no inter-
nal punctuation, and others, which may be complex.
As an example, consider the beginning of an arti-
cle from (simple) Wikipedia: (i) Linguistics (ii) Lin-
guistics (sometimes called philology) is the science
that studies language. (iii) Scientists who study lan-
guage are called linguists. Since the title does not
end with punctuation, it would be relegated to Dfrag.
But two complete sentences would be in Dcomp, with
the last also filed under Dsimp, as it has only a trail-
ing punctuation mark. Spitkovsky et al suggested
two curriculum learning strategies: (i) one in which
induction begins with clean, simple data, Dsimp, and
a basic model, DBM-1 (2012b); and (ii) an alterna-
tive bootstrapping approach: starting with still more,
simpler data ? namely, short inter-punctuation frag-
ments up to length l = 15, Dlsplit ? Dlsimp ? and a
bare-bones model, DBM-0 (2012a). In our example,
Dsplit would hold five text snippets: (i) Linguistics;
(ii) Linguistics; (iii) sometimes called philology;
(iv) is the science that studies language; and (v) Sci-
entists who study language are called linguists.
Only the last piece of text would still be considered
complete, isolating its contribution to sentence root
and boundary word distributions from those of in-
complete fragments. The sparse model, DBM-0, as-
sumes a uniform distribution for roots of incomplete
inputs and reduces conditioning contexts of stopping
probabilities, which works well with split data. We
will exploit both DBM-0 and the full DBM,2 draw-
ing also on split, simple and raw views of input text.
All experiments prior to final multi-lingual eval-
uation will use the Penn English Treebank?s Wall
Street Journal (WSJ) portion (Marcus et al, 1993) as
the underlying tokenized and sentence-broken cor-
pus D. Instead of gold parts-of-speech, we plugged
in 200 context-sensitive unsupervised tags, from
Spitkovsky et al (2011c),3 for the word categories.
3.2 Smoothing and Lexicalization
All unlexicalized instances of DBMs will be esti-
mated with ?add one? (a.k.a. Laplace) smoothing,
2We use the short-hand DBM to refer to DBM-3, which is
equivalent to DBM-2 if D has no internally-punctuated sen-
tences (D=Dsplit), and DBM-1 if all inputs also have trailing
punctuation (D=Dsimp); DBM0 is our short-hand for DBM-0.
3http://nlp.stanford.edu/pubs/goldtags-data.tar.bz2
using only the word category cw to represent a token.
Fully-lexicalized grammars (L-DBM) are left un-
smoothed, and represent each token as both a word
and its category, i.e., the whole pair (w, cw). To eval-
uate a lexicalized parsing model, we will always ob-
tain a delexicalized-and-smoothed instance first.
3.3 Optimization and Viterbi Decoding
We use ?early-switching lateen? EM (Spitkovsky et
al., 2011a, ?2.4) to train unlexicalized models, alter-
nating between the objectives of ordinary (soft) and
hard EM algorithms, until neither can improve its
own objective without harming the other?s. This ap-
proach does not require tuning termination thresh-
olds, allowing optimizers to run to numerical con-
vergence if necessary, and handles only our shorter
inputs (l ? 15), starting with soft EM (L = SL, for
?soft lateen?). Lexicalized models will cover full
data (l ? 45) and employ ?early-stopping lateen?
EM (2011a, ?2.3), re-estimating via hard EM until
soft EM?s objective suffers. Alternating EMs would
be expensive here, since updates take (at least) O(l3)
time, and hard EM?s objective (L = H) is the one
better suited to long inputs (Spitkovsky et al, 2010).
Our decoders always force an inter-punctuation
fragment to derive itself (Spitkovsky et al, 2011b,
?2.2).4 In evaluation, such (loose) constraints may
help attach sometimes and philology to called (and
the science... to is). In training, stronger (strict)
constraints also disallow attachment of fragments?
heads by non-heads, to connect Linguistics, called
and is (assuming each piece got parsed correctly).
3.4 Final Evaluation and Metrics
Evaluation is against held-out CoNLL shared task
data (Buchholz and Marsi, 2006; Nivre et al, 2007),
spanning 19 languages. We compute performance
as directed dependency accuracies (DDA), fractions
of correct unlabeled arcs in parsed output (an extrin-
sic metric).5 For most WSJ experiments we include
also sentence and parse tree cross-entropies (soft and
hard EMs? intrinsic metrics), in bits per token (bpt).
4But these constraints do not impact training with shorter
inputs, since there is no internal punctuation in Dsplit or Dsimp.
5We converted gold labeled constituents in WSJ to unlabeled
reference dependencies using deterministic ?head-percolation?
rules (Collins, 1999); sentence root symbols, though not punc-
tuation arcs, contribute to scores, as is standard (Paskin, 2001b).
1985
4 Concrete Operators
We will now instantiate the operators sketched out
in ?2 specifically for the grammar induction task.
Throughout, we repeatedly employ single steps of
Viterbi training to transfer information between sub-
networks in a model-independent way: when a mod-
ule?s output is a set of (Viterbi) parse trees, it neces-
sarily contains sufficient information required to es-
timate an arbitrarily-factored model down-stream.6
4.1 Transform #1: A Simple Filter
Given a model that was estimated from (and there-
fore parses) a data set D, the simple filter (F ) at-
tempts to extract a cleaner model, based on the sim-
pler complete sentences of Dsimp. It is implemented
as a single (unlexicalized) step of Viterbi training:
(7)C F
The idea here is to focus on sentences that are not
too complicated yet grammatical. This punctuation-
sensitive heuristic may steer a learner towards easy
but representative training text and, we showed, aids
grammar induction (Spitkovsky et al, 2012b, ?7.1).
4.2 Transform #2: A Symmetrizer
The symmetrizer (S) reduces input models to sets of
word association scores. It blurs all details of in-
duced parses in a data set D, except the number of
times each (ordered) word pair participates in a de-
pendency relation. We implemented symmetrization
also as a single unlexicalized Viterbi training step,
but now with proposed parse trees? scores, for a sen-
tence in D, proportional to a product over non-root
dependency arcs of one plus how often the left and
right tokens (are expected to) appear connected:
(8)C S
The idea behind the symmetrizer is to glean infor-
mation from skeleton parses. Grammar inducers can
sometimes make good progress in resolving undi-
rected parse structures despite being wrong about
the polarities of most arcs (Spitkovsky et al, 2009,
Figure 3: Uninformed). Symmetrization offers an
extra chance to make heads or tails of syntactic rela-
tions, after learning which words tend to go together.
6A related approach ? initializing EM training with an
M-step ? was advocated by Klein and Manning (2004, ?3).
At each instance where a word a? attaches z? on
(say) the right, our implementation attributes half its
weight to the intended construction, ya? z?, reserving
the other half for the symmetric structure, z? attach-
ing a? to its left: xa? z?. For the desired effect, these
aggregated counts are left unnormalized, while all
other counts (of word fertilities and sentence roots)
get discarded. To see why we don?t turn word attach-
ment scores into probabilities, consider sentences
a? z? and c? z?. The fact that z? co-occurs with a?
introduces an asymmetry into z??s relation with c?:
P( z? | c?) = 1 differs from P( c? | z?) = 1/2. Normal-
izing might force the interpretation yc? z? (and also
y
a? z?), not because there is evidence in the data, but
as a side-effect of a model?s head-driven nature (i.e.,
factored with dependents conditioned on heads). Al-
ways branching right would be a mistake, however,
for example if z? is a noun, since either of a? or c?
could be a determiner, with the other a verb.
4.3 Join: A Combiner
The combiner must admit arbitrary inputs, includ-
ing models not estimated from D, unlike the trans-
forms. Consequently, as a preliminary step, we con-
vert each input Ci into parse trees of D, with counts
C?i, via Viterbi-decoding with a smoothed, unlexical-
ized version of the corresponding incoming model.
Actual combination is then performed in a more pre-
cise (unsmoothed) fashion: C?i are the (lexicalized)
solutions starting from C?i; and C?+ is initialized with
their sum,
?
iC
?
i . Counts of the lexicalized model
with lowest cross-entropy on D become the output:7
(9)
LDC2
C1
5 Basic Networks
We are ready to propose a non-trivial subnetwork for
grammar induction, based on the transform and join
operators, which we will reuse in larger networks.
5.1 Fork/Join (FJ)
Given a model that parses a base data set D0, the
fork/join subnetwork will output an adaptation of
that model for D. It could facilitate a grammar in-
duction process, e.g., by advancing it from smaller
7In our diagrams, lexicalized modules are shaded black.
1986
to larger ? or possibly more complex ? data sets.
We first fork off two variations of the incoming
model based on D0: (i) a filtered view, which fo-
cuses on cleaner, simpler data (transform #1); and
(ii) a symmetrized view that backs off to word asso-
ciations (transform #2). Next is grammar induction
over D. We optimize a full DBM instance starting
from the first fork, and bootstrap a reduced DBM0
from the second. Finally, the two new induced sets
of parse trees, for D, are merged (lexicalized join):
(10)
HL?DBMD
SLDBMD
SLDBM0D
C
F
S
D0
C1
C2
C?1
C?2
The idea here is to prepare for two scenarios: an
incoming grammar that is either good or bad for D.
If the model is good, DBM should be able to hang
on to it and make improvements. But if it is bad,
DBM could get stuck fitting noise, whereas DBM0
might be more likely to ramp up to a good alterna-
tive. Since we can?t know ahead of time which is the
true case, we pursue both optimization paths simul-
taneously and let a combiner later decide for us.
Note that the forks start (and end) optimizing with
soft EM. This is because soft EM integrates previ-
ously unseen tokens into new grammars better than
hard EM, as evidenced by our failed attempt to re-
produce the ?baby steps? strategy with Viterbi train-
ing (Spitkovsky et al, 2010, Figure 4). A combiner
then executes hard EM, and since outputs of trans-
forms are trees, the end-to-end process is a chain of
lateen alternations that starts and ends with hard EM.
We will use a ?grammar inductor? to represent
subnetworks that transition from Dlsplit to Dl+1split, by
taking transformed parse trees of inter-punctuation
fragments up to length l (base data set, D0) to ini-
tialize training over fragments up to length l + 1:
(11)C l+1
The FJ network instantiates a grammar inductor
with l = 14, thus training on inter-punctuation frag-
ments up to length 15, as in previous work, starting
from an empty set of counts, C = ?. Smoothing
causes initial parse trees to be chosen uniformly at
random, as suggested by Cohen and Smith (2010):
(12)? 15
5.2 Iterated Fork/Join (IFJ)
Our second network daisy-chains grammar induc-
tors, starting from the single-word inter-punctuation
fragments in D1split, then retraining on D2split, and so
forth, until finally stopping at D15split, as before:
(13)1 2 14 15
We diagrammed this system as not taking an input,
since the first inductor?s output is fully determined
by unique parse trees of single-token strings. This
iterative approach to optimization is akin to deter-
ministic annealing (Rose, 1998), and is patterned af-
ter ?baby steps? (Spitkovsky et al, 2009, ?4.2).
Unlike the basic FJ, where symmetrization was a
no-op (since there were no counts in C = ?), IFJ
makes use of symmetrizers ? e.g., in the third in-
ductor, whose input is based on strings with up to
two tokens. Although it should be easy to learn
words that go together from very short fragments,
extracting correct polarities of their relations could
be a challenge: to a large extent, outputs of early in-
ductors may be artifacts of how our generative mod-
els factor (see ?4.2) or how ties are broken in opti-
mization (Spitkovsky et al, 2012a, Appendix B). We
therefore expect symmetrization to be crucial in ear-
lier stages but to weaken any high quality grammars,
nearer the end; it will be up to combiners to handle
such phase transitions correctly (or gracefully).
5.3 Grounded Iterated Fork/Join (GIFJ)
So far, our networks have been either purely itera-
tive (IFJ) or static (FJ). These two approaches can
also be combined, by injecting FJ?s solutions into
IFJ?s more dynamic stream. Our new transition sub-
network will join outputs of grammar inductors that
either (i) continue a previous solution (as in IFJ); or
(ii) start over from scratch (?grounding? to an FJ):
(14)
HL?DBMDl+1split?
Cl Cl+1l+1
l+1
The full GIFJ network can then be obtained by un-
rolling the above template from l = 14 back to one.
1987
WSJ15split WSJ
15
simp
Instance Label Model hsents htrees DDA hsents htrees DDA TA Description
DBM 6.54 6.75 83.7 6.05 6.21 85.1 42.7 Supervised (MLE of WSJ45)
? = C ? 8.76 10.46 21.4 8.58 10.52 20.7 3.9 Random Projective Parses
SL(S(C)) = C2 DBM0 6.18 6.39 57.0 5.90 6.11 57.5 10.4 B
A
}
Unlexicalized
BaselinesSL(F (C)) = C1 DBM 5.89 5.99 62.2 5.79 5.90 60.9 12.0
H(C?2) = C?2 L-DBM 7.28 7.30 59.2 6.87 6.88 58.6 10.4
Fork/Join
?
?
?
?
?
Baseline
Combination
H(C?1) = C?1 L-DBM 7.07 7.08 62.3 6.72 6.73 60.8 12.0
C?1 + C?2 = C+ L-DBM 7.20 7.27 64.0 6.82 6.88 62.5 12.3
H(C+) = C?+ L-DBM 7.02 7.04 64.2 6.64 6.65 62.7 12.8
L-DBM 6.95 6.96 70.5 6.55 6.56 68.2 14.9 Iterated Fork/Join (IFJ)
L-DBM 6.91 6.92 71.4 6.52 6.52 69.2 15.6 Grounded Iterated Fork/Join
L-DBM 6.83 6.83 72.3 6.41 6.41 70.2 17.9 Grammar Transformer (GT)
L-DBM 6.92 6.93 71.9 6.53 6.53 69.8 16.7 IFJ
GT
}
w/Iterated
CombinersL-DBM 6.83 6.83 72.9 6.41 6.41 70.6 18.0
Table 1: Sentence string and parse tree cross-entropies (in bpt), and accuracies (DDA), on inter-punctuation fragments
up to length 15 (WSJ15split) and its subset of simple, complete sentences (WSJ15simp, with exact tree accuracies ? TA).
6 Performance of Basic Networks
We compared our three networks? performance on
their final training sets, WSJ15split (see Table 1, which
also tabulates results for a cleaner subset, WSJ15simp).
The first network starts from C = ?, helping us es-
tablish several straw-man baselines. Its empty ini-
tializer corresponds to guessing (projective) parse
trees uniformly at random, which has 21.4% accu-
racy and sentence string cross-entropy of 8.76bpt.
6.1 Fork/Join (FJ)
FJ?s symmetrizer yields random parses of WSJ14split,
which initialize training of DBM0. This baseline (B)
lowers cross-entropy to 6.18bpt and scores 57.0%.
FJ?s filter starts from parse trees of WSJ14simp only, and
trains up a full DBM. This choice makes a stronger
baseline (A), with 5.89bpt cross-entropy, at 62.2%.
The join operator uses counts from A and B, C1
and C2, to obtain parse trees whose own counts C?1
and C?2 initialize lexicalized training. From each C?i,
an optimizer arrives at C?i . Grammars corresponding
to these counts have higher cross-entropies, because
of vastly larger vocabularies, but also better accura-
cies: 59.2 and 62.3%. Their mixture C+ is a simple
sum of counts in C?1 and C?2 : it is not expected to be
an improvement but happens to be a good move, re-
sulting in a grammar with higher accuracy (64.0%),
though not better Viterbi cross-entropy (7.27 falls
between 7.08 and 7.30bpt) than both sources. The
combiner?s third alternative, a locally optimal C?+, is
then obtained by re-optimizing from C+. This so-
lution performs slightly better (64.2%) and will be
the local optimum returned by FJ?s join operator, be-
cause it attains the lowest cross-entropy (7.04bpt).
6.2 Iterated Fork/Join (IFJ)
IFJ?s iterative approach results in an improvement:
70.5% accuracy and 6.96bpt cross-entropy. To test
how much of this performance could be obtained by
a simpler iterated network, we experimented with
ablated systems that don?t fork or join, i.e., our clas-
sic ?baby steps? schema (chaining together 15 op-
timizers), using both DBM and DBM0, with and
without a transform in-between. However, all such
?linear? networks scored well below 50%. We con-
clude from these results that an ability to branch out
into different promising regions of a solution space,
and to merge solutions of varying quality into better
models, are important properties of FJ subnetworks.
6.3 Grounded Iterated Fork/Join (GIFJ)
Grounding improves GIFJ?s performance further, to
71.4% accuracy and 6.92bpt cross-entropy. This re-
sult shows that fresh perspectives from optimizers
that start over can make search efforts more fruitful.
7 Enhanced Subnetworks
Modularity and abstraction allow for compact repre-
sentations of complex systems. Another key benefit
is that individual components can be understood and
improved in isolation, as we will demonstrate next.
1988
7.1 An Iterative Combiner (IC)
Our basic combiner introduced a third option, C?+,
into a pool of candidate solutions, {C?1 , C?2}. This
new entry may not be a simple mixture of the orig-
inals, because of non-linear effects from applying L
to C?1 + C?2 , but could most likely still be improved.
Rather than stop at C?+, when it is better than both
originals, we could recombine it with a next best so-
lution, continuing until no further improvement is
made. Iterating can?t harm a given combiner?s cross-
entropy (e.g., it lowers FJ?s from 7.04 to 7.00bpt),
and its advantages can be realized more fully in the
larger networks (albeit without any end-to-end guar-
antees): upgrading all 15 combiners in IFJ would
improve performance (slightly) more than ground-
ing (71.5 vs. 71.4%), and lower cross-entropy (from
6.96 to 6.93bpt). But this approach is still a bit timid.
A more greedy way is to proceed so long as C?+
is not worse than both predecessors. We shall now
state our most general iterative combiner (IC) algo-
rithm: Start with a solution pool p = {C?i }ni=1. Next,
construct p? by adding C?+ = L(
?n
i=1 C
?
i ) to p and re-
moving the worst of n+ 1 candidates in the new set.
Finally, if p = p?, return the best of the solutions in p;
otherwise, repeat from p := p?. At n = 2, one could
think of taking L(C?1 + C?2 ) as performing a kind of
bisection search in some (strange) space. With these
new and improved combiners, the IFJ network per-
forms better: 71.9% (up from 70.5 ? see Table 1),
lowering cross-entropy (down from 6.96 to 6.93bpt).
We propose a distinguished notation for the ICs:
(15)
*C2
C1
7.2 A Grammar Transformer (GT)
The levels of our systems? performance at grammar
induction thus far suggest that the space of possible
networks (say, with up to k components) may itself
be worth exploring more thoroughly. We leave this
exercise to future work, ending with two relatively
straight-forward extensions for grounded systems.
Our static bootstrapping mechanism (?ground? of
GIFJ) can be improved by pretraining with simple
sentences first ? as in the curriculum for learning
DBM-1 (Spitkovsky et al, 2012b, ?7.1), but now
with a variable length cut-off l (much lower than the
original 45) ? instead of starting from ? directly:
(16)
SDBMDlsimp?
l+1
?
?
?
l
The output of this subnetwork can then be refined,
by reconciling it with a previous dynamic solution.
We perform a mini-join of a new ground?s counts
with Cl, using the filter transform (single steps of
lexicalized Viterbi training on clean, simple data),
ahead of the main join (over more training data):
(17)
HL?DBMDl+1splitCl Cl+1
l+1
F
l
This template can be unrolled, as before, to obtain
our last network (GT), which achieves 72.9% accu-
racy and 6.83bpt cross-entropy (slightly less accu-
rate with basic combiners, at 72.3% ? see Table 1).
8 Full Training and System Combination
All systems that we described so far stop training at
D15split. We will use a two-stage adaptor network to
transition their grammars to a full data set, D45:
(18)
HL?DBMD45split H
L?DBM
D45C
The first stage exposes grammar inducers to longer
inputs (inter-punctuation fragments with up to 45
tokens); the second stage, at last, reassembles text
snippets into actual sentences (also up to l = 45).8
After full training, our IFJ and GT systems parse
Section 23 of WSJ at 62.7 and 63.4% accuracy, bet-
ter than the previous state-of-the-art (61.2% ? see
Table 2). To test the generalized IC algorithm, we
merged our implementations of these three strong
grammar induction pipelines into a combined sys-
tem (CS). It scored highest: 64.4%.
(19)
HL?DBMD45(GT) #1
(IFJ) #2
#3
CS
The quality of bracketings corresponding to (non-
trivial) spans derived by heads of our dependency
structures is competitive with the state-of-the-art in
unsupervised constituent parsing. On the WSJ sen-
tences up to length 40 in Section 23, CS attains sim-
ilar F1-measure (54.2 vs. 54.6, with higher recall) to
8Note that smoothing in the final (unlexicalized) Viterbi step
masks the fact that model parts that could not be properly es-
timated in the first stage (e.g., probabilities of punctuation-
crossing arcs) are being initialized to uniform multinomials.
1989
System DDA (@10)
(Gimpel and Smith, 2012) 53.1 (64.3)
(Gillenwater et al, 2010) 53.3 (64.3)
(Bisk and Hockenmaier, 2012) 53.3 (71.5)
(Blunsom and Cohn, 2010) 55.7 (67.7)
(Tu and Honavar, 2012) 57.0 (71.4)
(Spitkovsky et al, 2011b) 58.4 (71.4)
(Spitkovsky et al, 2011c) 59.1 (71.4)
#3 (Spitkovsky et al, 2012a) 61.2 (71.4)
#2
w/Full Training
{
IFJ
GT
62.7 (70.3)
#1 63.4 (70.3)
#1 + #2 + #3 System Combination CS 64.4 (72.0)
Supervised DBM (also with loose decoding) 76.3 (85.4)
Table 2: Directed dependency accuracies (DDA) on Sec-
tion 23 of WSJ (all sentences and up to length ten) for
recent systems, our full networks (IFJ and GT), and three-
way combination (CS) with the previous state-of-the-art.
PRLG (Ponvert et al, 2011), which is the strongest
system of which we are aware (see Table 3).9
9 Multi-Lingual Evaluation
Last, we checked how our algorithms generalize out-
side English WSJ, by testing in 23 more set-ups: all
2006/7 CoNLL test sets (Buchholz and Marsi, 2006;
Nivre et al, 2007), spanning 19 languages. Most re-
cent work evaluates against this multi-lingual data,
with the unrealistic assumption of part-of-speech
tags. But since inducing high quality word clusters
for many languages would be beyond the scope of
our paper, here we too plugged in gold tags for word
categories (instead of unsupervised tags, as in ?3?8).
We compared to the two strongest systems we
knew:10 MZ (Marec?ek and ?Zabokrtsky?, 2012) and
SAJ (Spitkovsky et al, 2012b), which report average
accuracies of 40.0 and 42.9% for CoNLL data (see
Table 4). Our fully-trained IFJ and GT systems score
40.0 and 47.6%. As before, combining these net-
works with our own implementation of the best pre-
vious state-of-the-art system (SAJ) yields a further
improvement, increasing final accuracy to 48.6%.
9These numbers differ from Ponvert et al?s (2011, Table 6)
for the full Section 23 because we restricted their eval-ps.py
script to a maximum length of 40 words, in our evaluation, to
match other previous work: Golland et al?s (2012, Figure 1) for
CCM and LLCCM; Huang et al?s (2012, Table 2) for the rest.
10During review, another strong system (Marec?ek and Straka,
2013, scoring 48.7%) of possible interest to the reader came out,
exploiting prior knowledge of stopping probabilities (estimated
from large POS-tagged corpora, via reducibility principles).
System F1
Binary-Branching Upper Bound 85.7
Left-Branching Baseline 12.0
CCM (Klein and Manning, 2002) 33.7
Right-Branching Baseline 40.7
F-CCM (Huang et al, 2012) 45.1
HMM (Ponvert et al, 2011) 46.3
LLCCM (Golland et al, 2012) 47.6 P R
CCL (Seginer, 2007) 52.8 54.6 51.1
PRLG (Ponvert et al, 2011) 54.6 60.4 49.8
CS System Combination 54.2 55.6 52.8
Supervised DBM Skyline 59.3 65.7 54.1
Dependency-Based Upper Bound 87.2 100 77.3
Table 3: Harmonic mean (F1) of precision (P) and re-
call (R) for unlabeled constituent bracketings on Section
23 of WSJ (sentences up to length 40) for our combined
system (CS), recent state-of-the-art and the baselines.
10 Discussion
CoNLL training sets were intended for comparing
supervised systems, and aren?t all suitable for unsu-
pervised learning: 12 languages have under 10,000
sentences (with Arabic, Basque, Danish, Greek, Ital-
ian, Slovenian, Spanish and Turkish particularly
small), compared to WSJ?s nearly 50,000. In some
treebanks sentences are very short (e.g., Chinese and
Japanese, which appear to have been split on punc-
tuation), and in others extremely long (e.g., Arabic).
Even gold tags aren?t always helpful, as their num-
ber is rarely ideal for grammar induction (e.g., 42 vs.
200 for English). These factors contribute to high
variances of our (and previous) results (see Table 4).
Nevertheless, if we look at the more stable aver-
age accuracies, we see a positive trend as we move
from a simpler fully-trained system (IFJ, 40.0%),
to a more complex system (GT, 47.6%), to system
combination (CS, 48.6%). Grounding seems to be
more important for the CoNLL sets, possibly be-
cause of data sparsity or availability of gold tags.
11 Related Work
The surest way to avoid local optima is to craft
an objective that doesn?t have them. For example,
Wang et al (2008) demonstrated a convex train-
ing method for semi-supervised dependency pars-
ing; Lashkari and Golland (2008) introduced a con-
vex reformulation of likelihood functions for clus-
tering tasks; and Corlett and Penn (2010) designed
1990
Directed Dependency Accuracies (DDA) (@10)
CoNLL Data MZ SAJ IFJ GT CS
Arabic 2006 26.5 10.9 33.3 8.3 9.3 (30.2)
?7 27.9 44.9 26.1 25.6 26.8 (45.6)
Basque ?7 26.8 33.3 23.5 24.2 24.4 (32.8)
Bulgarian ?7 46.0 65.2 35.8 64.2 63.4 (69.1)
Catalan ?7 47.0 62.1 65.0 68.4 68.0 (79.2)
Chinese ?6 ? 63.2 56.0 55.8 58.4 (60.8)
?7 ? 57.0 49.0 48.6 52.5 (56.0)
Czech ?6 49.5 55.1 44.5 43.9 44.0 (52.3)
?7 48.0 54.2 42.9 24.5 34.3 (51.1)
Danish ?6 38.6 22.2 37.8 17.1 21.4 (29.8)
Dutch ?6 44.2 46.6 40.8 51.3 48.0 (48.7)
English ?7 49.2 29.6 39.3 57.6 58.2 (75.0)
German ?6 44.8 39.1 34.1 54.5 56.2 (71.2)
Greek ?6 20.2 26.9 23.7 45.0 45.4 (52.2)
Hungarian ?7 51.8 58.2 24.8 52.9 58.3 (67.6)
Italian ?7 43.3 40.7 56.8 31.1 34.9 (44.9)
Japanese ?6 50.8 22.7 32.6 63.7 63.0 (68.9)
Portuguese ?6 50.6 72.4 38.0 72.7 74.5 (81.1)
Slovenian ?6 18.1 35.2 42.1 50.8 50.9 (57.3)
Spanish ?6 51.9 28.2 57.0 61.7 61.4 (73.2)
Swedish ?6 48.2 50.7 46.6 48.6 49.7 (62.1)
Turkish ?6 ? 34.4 28.0 32.9 29.2 (33.2)
?7 15.7 44.8 42.1 41.7 37.9 (42.4)
Average: 40.0 42.9 40.0 47.6 48.6 (57.8)
Table 4: Blind evaluation on 2006/7 CoNLL test sets (all
sentences) for our full networks (IFJ and GT), previous
state-of-the-art systems of Spitkovsky et al (2012b) and
Marec?ek and ?Zabokrtsky? (2012), and three-way combi-
nation with SAJ (CS, including results up to length ten).
a search algorithm for encoding decipherment prob-
lems that guarantees to quickly converge on optimal
solutions. Convexity can be ideal for comparative
analyses, by eliminating dependence on initial con-
ditions. But for many NLP tasks, including grammar
induction, the most relevant known objective func-
tions are still riddled with local optima. Renewed ef-
forts to find exact solutions (Eisner, 2012; Gormley
and Eisner, 2013) may be a good fit for the smaller
and simpler, earlier stages of our iterative networks.
Multi-start methods (Solis and Wets, 1981) can
recover certain global extrema almost surely (i.e.,
with probability approaching one). Moreover, ran-
dom restarts via uniform probability measures can
be optimal, in a worst-case-analysis sense, with par-
allel processing sometimes leading to exponential
speed-ups (Hu et al, 1994). This approach is rarely
emphasized in NLP literature. For instance, Moore
and Quirk (2008) demonstrated consistent, substan-
tial gains from random restarts in statistical machine
translation (but also suggested better and faster re-
placements ? see below); Ravi and Knight (2009,
?5, Figure 8) found random restarts for EM to be
crucial in parts-of-speech disambiguation. However,
other reviews are few and generally negative (Kim
and Mooney, 2010; Martin-Brualla et al, 2010).
Iterated local search methods (Hoos and Stu?tzle,
2004; Johnson et al, 1988, inter alia) escape lo-
cal basins of attraction by perturbing candidate so-
lutions, without undoing all previous work. ?Large-
step? moves can come from jittering (Hinton and
Roweis, 2003), dithering (Price et al, 2005, Ch. 2)
or smoothing (Bhargava and Kondrak, 2009). Non-
improving ?sideways? moves offer substantial help
with hard satisfiability problems (Selman et al,
1992); and injecting non-random noise (Selman et
al., 1994), by introducing ?uphill? moves via mix-
tures of random walks and greedy search strate-
gies, does better than random noise alone or simu-
lated annealing (Kirkpatrick et al, 1983). In NLP,
Moore and Quirk?s (2008) random walks from pre-
vious local optima were faster than uniform sam-
pling and also increased BLEU scores; Elsner and
Schudy (2009) showed that local search can outper-
form greedy solutions for document clustering and
chat disentanglement tasks; and Mei et al (2001)
incorporated tabu search (Glover, 1989; Glover and
Laguna, 1993, Ch. 3) into HMM training for ASR.
Genetic algorithms are a fusion of what?s best in
local search and multi-start methods (Houck et al,
1996), exploiting a problem?s structure to combine
valid parts of any partial solutions (Holland, 1975;
Goldberg, 1989). Evolutionary heuristics proved
useful in the induction of phonotactics (Belz, 1998),
text planning (Mellish et al, 1998), factored mod-
eling of morphologically-rich languages (Duh and
Kirchhoff, 2004) and plot induction for story gener-
ation (McIntyre and Lapata, 2010). Multi-objective
genetic algorithms (Fonseca and Fleming, 1993) can
handle problems with equally important but con-
flicting criteria (Stadler, 1988), using Pareto-optimal
ensembles. They are especially well-suited to lan-
guage, which evolves under pressures from compet-
ing (e.g., speaker, listener and learner) constraints,
and have been used to model configurations of vow-
els and tone systems (Ke et al, 2003). Our transform
and join mechanisms also exhibit some features of
genetic search, and make use of competing objec-
1991
tives: good sets of parse trees must make sense both
lexicalized and with word categories, to rich and im-
poverished models of grammar, and for both long,
complex sentences and short, simple text fragments.
This selection of text filters is a specialized case
of more general ?data perturbation? techniques ?
even cycling over randomly chosen mini-batches
that partition a data set helps avoid some local op-
tima (Liang and Klein, 2009). Elidan et al (2002)
suggested how example-reweighing could cause ?in-
formed? changes, rather than arbitrary damage, to
a hypothesis. Their (adversarial) training scheme
guided learning toward improved generalizations,
robust against input fluctuations. Language learn-
ing has a rich history of reweighing data via (co-
operative) ?starting small? strategies (Elman, 1993),
beginning from simpler or more certain cases. This
family of techniques has met with success in semi-
supervised named entity classification (Collins and
Singer, 1999; Yarowsky, 1995),11 parts-of-speech
induction (Clark, 2000; 2003), and language model-
ing (Krueger and Dayan, 2009; Bengio et al, 2009),
in addition to unsupervised parsing (Spitkovsky et
al., 2009; Tu and Honavar, 2011; Cohn et al, 2011).
12 Conclusion
We proposed several simple algorithms for combin-
ing grammars and showed their usefulness in merg-
ing the outputs of iterative and static grammar in-
duction systems. Unlike conventional system com-
bination methods, e.g., in machine translation (Xiao
et al, 2010), ours do not require incoming mod-
els to be of similar quality to make improvements.
We exploited these properties of the combiners to
reconcile grammars induced by different views of
data (Blum and Mitchell, 1998). One such view re-
tains just the simple sentences, making it easier to
recognize root words. Another splits text into many
inter-punctuation fragments, helping learn word as-
sociations. The induced dependency trees can them-
selves also be viewed not only as directed structures
but also as skeleton parses, facilitating the recovery
of correct polarities for unlabeled dependency arcs.
By reusing templates, as in dynamic Bayesian
network (DBN) frameworks (Koller and Friedman,
11The so-called Yarowsky-cautious modification of the orig-
inal algorithm for unsupervised word-sense disambiguation.
2009, ?6.2.2), we managed to specify relatively
?deep? learning architectures without sacrificing
(too much) clarity or simplicity. On a still more
speculative note, we see two (admittedly, tenuous)
connections to human cognition. First, the benefits
of not normalizing probabilities, when symmetriz-
ing, might be related to human language process-
ing through the base-rate fallacy (Bar-Hillel, 1980;
Kahneman and Tversky, 1982) and the availability
heuristic (Chapman, 1967; Tversky and Kahneman,
1973), since people are notoriously bad at probabil-
ity (Attneave, 1953; Kahneman and Tversky, 1972;
Kahneman and Tversky, 1973). And second, inter-
mittent ?unlearning? ? though perhaps not of the
kind that takes place inside of our transforms ?
is an adaptation that can be essential to cognitive
development in general, as evidenced by neuronal
pruning in mammals (Craik and Bialystok, 2006;
Low and Cheng, 2006). ?Forgetful EM? strategies
that reset subsets of parameters may thus, possibly,
be no less relevant to unsupervised learning than is
?partial EM,? which only suppresses updates, other
EM variants (Neal and Hinton, 1999), or ?dropout
training? (Hinton et al, 2012; Wang and Manning,
2013), which is important in supervised settings.
Future parsing models, in grammar induction,
may benefit by modeling head-dependent relations
separately from direction. As frequently employed
in tasks like semantic role labeling (Carreras and
Ma`rquez, 2005) and relation extraction (Sun et al,
2011), it may be easier to first establish existence,
before trying to understand its nature. Other key
next steps may include exploring more intelligent
ways of combining systems (Surdeanu and Man-
ning, 2010; Petrov, 2010) and automating the op-
erator discovery process. Furthermore, we are opti-
mistic that both count transforms and model recom-
bination could be usefully incorporated into sam-
pling methods: although symmetrized models may
have higher cross-entropies, hence prone to rejection
in vanilla MCMC, they could work well as seeds
in multi-chain designs; existing algorithms, such as
MCMCMC (Geyer, 1991), which switch contents
of adjacent chains running at different temperatures,
may also benefit from introducing the option to com-
bine solutions, in addition to just swapping them.
1992
Acknowledgments
We thank Yun-Hsuan Sung, for early-stage discussions
on ways of extending ?baby steps,? Elias Ponvert, for
sharing all of the relevant experimental results and eval-
uation scripts from his work with Jason Baldridge and
Katrin Erk, and the anonymous reviewers, for their
helpful comments on the draft version of this paper.
Funded, in part, by Defense Advanced Research Projects
Agency (DARPA) Deep Exploration and Filtering of
Text (DEFT) Program, under Air Force Research Lab-
oratory (AFRL) prime contract no. FA8750-13-2-0040.
Any opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
DARPA, AFRL, or the US government. Once again, the
first author thanks Moofus.
References
H. Alshawi. 1996. Head automata for speech translation. In
ICSLP.
F. Attneave. 1953. Psychological probability as a function of
experienced frequency. Experimental Psychology, 46.
M. Bar-Hillel. 1980. The base-rate fallacy in probability judg-
ments. Acta Psychologica, 44.
A. Belz. 1998. Discovering phonotactic finite-state automata
by genetic search. In COLING-ACL.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009.
Curriculum learning. In ICML.
A. Bhargava and G. Kondrak. 2009. Multiple word alignment
with profile hidden Markov models. In NAACL-HLT: Stu-
dent Research and Doctoral Consortium.
Y. Bisk and J. Hockenmaier. 2012. Simple robust grammar
induction with combinatory categorial grammars. In AAAI.
A. Blum and T. Mitchell. 1998. Combining labeled and unla-
beled data with co-training. In COLT.
P. Blunsom and T. Cohn. 2010. Unsupervised induction of tree
substitution grammars for dependency parsing. In EMNLP.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In CoNLL.
X. Carreras and L. Ma`rquez. 2005. Introduction to the CoNLL-
2005 shared task: Semantic role labeling. In CoNLL.
L. J. Chapman. 1967. Illusory correlation in observational re-
port. Verbal Learning and Verbal Behavior, 6.
A. Clark. 2000. Inducing syntactic categories by context distri-
bution clustering. In CoNLL-LLL.
A. Clark. 2003. Combining distributional and morphological
information for part of speech induction. In EACL.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs:
Hardness results and competitiveness of uniform initializa-
tion. In ACL.
T. Cohn, P. Blunsom, and S. Goldwater. 2011. Inducing tree-
substitution grammars. JMLR.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In EMNLP.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
E. Corlett and G. Penn. 2010. An exact A? method for deci-
phering letter-substitution ciphers. In ACL.
F. I. M. Craik and E. Bialystok. 2006. Cognition through the
lifespan: mechanisms of change. TRENDS in Cognitive Sci-
ences, 10.
C. de Marcken. 1995. Lexical heads, phrase structure and the
induction of grammar. In WVLC.
K. Duh and K. Kirchhoff. 2004. Automatic learning of lan-
guage model structure. In COLING.
J. Eisner. 2012. Grammar induction: Beyond local search. In
ICGI.
G. Elidan, M. Ninio, N. Friedman, and D. Schuurmans. 2002.
Data perturbation for escaping local maxima in learning. In
AAAI.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
M. Elsner and W. Schudy. 2009. Bounding and comparing
methods for correlation clustering beyond ILP. In NAACL-
HLT: Integer Linear Programming for NLP.
C. M. Fonseca and P. J. Fleming. 1993. Genetic algorithms for
multiobjective optimization: Formulation, discussion and
generalization. In ICGA.
C. J. Geyer. 1991. Markov chain Monte Carlo maximum like-
lihood. In Interface Symposium.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and B. Taskar.
2010. Posterior sparsity in unsupervised dependency pars-
ing. Technical report, University of Pennsylvania.
K. Gimpel and N. A. Smith. 2012. Concavity and initialization
for unsupervised dependency parsing. In NAACL-HLT.
F. Glover and M. Laguna. 1993. Tabu search. In C. R.
Reeves, editor, Modern Heuristic Techniques for Combina-
torial Problems. Blackwell Scientific Publications.
F. Glover. 1989. Tabu search ? Part I. ORSA Journal on
Computing, 1.
D. E. Goldberg. 1989. Genetic Algorithms in Search, Opti-
mization & Machine Learning. Addison-Wesley.
D. Golland, J. DeNero, and J. Uszkoreit. 2012. A feature-
rich constituent context model for grammar induction. In
EMNLP-CoNLL.
M. R. Gormley and J. Eisner. 2013. Nonconvex global opti-
mization for latent-variable models. In ACL.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with richer con-
texts and smoothing. In NAACL-HLT.
G. Hinton and S. Roweis. 2003. Stochastic neighbor embed-
ding. In NIPS.
G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and
R. R. Salakhutdinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors. In ArXiv.
J. H. Holland. 1975. Adaptation in Natural and Artificial Sys-
tems: An Introductory Analysis with Applications to Biology,
Control, and Artificial Intelligence. University of Michigan
Press.
H. H. Hoos and T. Stu?tzle. 2004. Stochastic Local Search:
Foundations and Applications. Morgan Kaufmann.
1993
C. R. Houck, J. A. Joines, and M. G. Kay. 1996. Comparison
of genetic algorithms, random restart, and two-opt switching
for solving large location-allocation problems. Computers
& Operations Research, 23.
X. Hu, R. Shonkwiler, and M. C. Spruill. 1994. Random
restarts in global optimization. Technical report, GT.
Y. Huang, M. Zhang, and C. L. Tan. 2012. Improved con-
stituent context model with features. In PACLIC.
F. Jelinek and R. L. Mercer. 1980. Interpolated estimation
of Markov source parameters from sparse data. In Pattern
Recognition in Practice.
D. S. Johnson, C. H. Papadimitriou, and M. Yannakakis. 1988.
How easy is local search? Journal of Computer and System
Sciences, 37.
D. Kahneman and A. Tversky. 1972. Subjective probability: A
judgment of representativeness. Cognitive Psychology, 3.
D. Kahneman and A. Tversky. 1973. On the psychology of
prediction. Psychological Review, 80.
D. Kahneman and A. Tversky. 1982. Evidential impact of base
rates. In D. Kahneman, P. Slovic, and A. Tversky, editors,
Judgment under uncertainty: Heuristics and biases. Cam-
bridge University Press.
J. Ke, M. Ogura, and W. S.-Y. Wang. 2003. Optimization mod-
els of sound systems using genetic algorithms. Computa-
tional Linguistics, 29.
J. Kim and R. J. Mooney. 2010. Generative alignment and
semantic parsing for learning from ambiguous supervision.
In COLING.
S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi. 1983. Opti-
mization by simulated annealing. Science, 220.
D. Klein and C. D. Manning. 2002. A generative constituent-
context model for improved grammar induction. In ACL.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In ACL.
D. Koller and N. Friedman. 2009. Probabilistic Graphical
Models: Principles and Techniques. MIT Press.
K. A. Krueger and P. Dayan. 2009. Flexible shaping: How
learning in small steps helps. Cognition, 110.
D. Lashkari and P. Golland. 2008. Convex clustering with
exemplar-based models. In NIPS.
P. Liang and D. Klein. 2009. Online EM for unsupervised
models. In NAACL-HLT.
L. K. Low and H.-J. Cheng. 2006. Axon pruning: an essen-
tial step underlying the developmental plasticity of neuronal
connections. Royal Society of London Philosophical Trans-
actions Series B, 361.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. Marec?ek and M. Straka. 2013. Stop-probability estimates
computed on a large corpus improve unsupervised depen-
dency parsing. In ACL.
D. Marec?ek and Z. ?Zabokrtsky?. 2011. Gibbs sampling with
treeness constraint in unsupervised dependency parsing. In
ROBUS.
D. Marec?ek and Z. ?Zabokrtsky?. 2012. Exploiting reducibility
in unsupervised dependency parsing. In EMNLP-CoNLL.
R. Martin-Brualla, E. Alfonseca, M. Pasca, K. Hall, E. Robledo-
Arnuncio, and M. Ciaramita. 2010. Instance sense induction
from attribute sets. In COLING.
N. McIntyre and M. Lapata. 2010. Plot induction and evolu-
tionary search for story generation. In ACL.
X.-d. Mei, S.-h. Sun, J.-s. Pan, and T.-Y. Chen. 2001. Op-
timization of HMM by the tabu search algorithm. In RO-
CLING.
C. Mellish, A. Knott, J. Oberlander, and M. O?Donnell. 1998.
Experiments using stochastic search for text planning. In
INLG.
R. C. Moore and C. Quirk. 2008. Random restarts in min-
imum error rate training for statistical machine translation.
In COLING.
T. Naseem and R. Barzilay. 2011. Using semantic cues to learn
syntax. In AAAI.
R. M. Neal and G. E. Hinton. 1999. A view of the EM al-
gorithm that justifies incremental, sparse, and other variants.
In M. I. Jordan, editor, Learning in Graphical Models. MIT
Press.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In EMNLP-CoNLL.
M. A. Paskin. 2001a. Cubic-time parsing and learning algo-
rithms for grammatical bigram models. Technical report,
UCB.
M. A. Paskin. 2001b. Grammatical bigrams. In NIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
S. Petrov. 2010. Products of random latent variable grammars.
In NAACL-HLT.
E. Ponvert, J. Baldridge, and K. Erk. 2011. Simple unsuper-
vised grammar induction from raw text with cascaded finite
state models. In ACL-HLT.
K. V. Price, R. M. Storn, and J. A. Lampinen. 2005. Differ-
ential Evolution: A Practical Approach to Global Optimiza-
tion. Springer.
S. Ravi and K. Knight. 2009. Minimized models for unsuper-
vised part-of-speech tagging. In ACL-IJCNLP.
K. Rose. 1998. Deterministic annealing for clustering, com-
pression, classification, regression and related optmization
problems. Proceedings of the IEEE, 86.
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
ACL.
B. Selman, H. Levesque, and D. Mitchell. 1992. A new method
for solving hard satisfiability problems. In AAAI.
B. Selman, H. A. Kautz, and B. Cohen. 1994. Noise strategies
for improving local search. In AAAI.
F. J. Solis and R. J.-B. Wets. 1981. Minimization by random
search techniques. Mathematics of Operations Research, 6.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby
Steps: How ?Less is More? in unsupervised dependency
parsing. In GRLL.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Manning.
2010. Viterbi training improves unsupervised dependency
parsing. In CoNLL.
1994
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen
EM: Unsupervised training with multiple objectives, applied
to dependency grammar induction. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctu-
ation: Making a point in unsupervised dependency parsing.
In CoNLL.
V. I. Spitkovsky, A. X. Chang, H. Alshawi, and D. Jurafsky.
2011c. Unsupervised dependency parsing without gold part-
of-speech tags. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012a. Boot-
strapping dependency grammar inducers from incomplete
sentence fragments via austere models. In ICGI.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012b. Three
dependency-and-boundary models for grammar induction.
In EMNLP-CoNLL.
W. Stadler, editor. 1988. Multicriteria Optimization in Engi-
neering and in the Sciences. Plenum Press.
A. Sun, R. Grishman, and S. Sekine. 2011. Semi-supervised
relation extraction with large-scale word clustering. In ACL.
M. Surdeanu and C. D. Manning. 2010. Ensemble models for
dependency parsing: Cheap and good? In NAACL-HLT.
K. Tu and V. Honavar. 2011. On the utility of curricula in
unsupervised learning of probabilistic grammars. In IJCAI.
K. Tu and V. Honavar. 2012. Unambiguity regularization
for unsupervised learning of probabilistic grammars. In
EMNLP-CoNLL.
A. Tversky and D. Kahneman. 1973. Availability: A heuristic
for judging frequency and probability. Cognitive Psychol-
ogy, 5.
S. I. Wang and C. D. Manning. 2013. Fast dropout training. In
ICML.
Q. I. Wang, D. Schuurmans, and D. Lin. 2008. Semi-
supervised convex training for dependency parsing. In HLT-
ACL.
T. Xiao, J. Zhu, M. Zhu, and H. Wang. 2010. Boosting-based
system combination for machine translation. In ACL.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In ACL.
1995
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 751?759,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
From Baby Steps to Leapfrog: How ?Less is More?
in Unsupervised Dependency Parsing?
Valentin I. Spitkovsky
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc., Mountain View, CA, 94043
hiyan@google.com
Daniel Jurafsky
Stanford University, Stanford, CA, 94305
jurafsky@stanford.edu
Abstract
We present three approaches for unsupervised
grammar induction that are sensitive to data
complexity and apply them to Klein and Man-
ning?s Dependency Model with Valence. The
first, Baby Steps, bootstraps itself via iterated
learning of increasingly longer sentences and
requires no initialization. This method sub-
stantially exceeds Klein and Manning?s pub-
lished scores and achieves 39.4% accuracy on
Section 23 (all sentences) of the Wall Street
Journal corpus. The second, Less is More,
uses a low-complexity subset of the avail-
able data: sentences up to length 15. Focus-
ing on fewer but simpler examples trades off
quantity against ambiguity; it attains 44.1%
accuracy, using the standard linguistically-
informed prior and batch training, beating
state-of-the-art. Leapfrog, our third heuristic,
combines Less is More with Baby Steps by
mixing their models of shorter sentences, then
rapidly ramping up exposure to the full train-
ing set, driving up accuracy to 45.0%. These
trends generalize to the Brown corpus; aware-
ness of data complexity may improve other
parsing models and unsupervised algorithms.
1 Introduction
Unsupervised learning of hierarchical syntactic
structure from free-form natural language text is a
hard problem whose eventual solution promises to
benefit applications ranging from question answer-
ing to speech recognition and machine translation.
A restricted version that targets dependencies and
?Partially funded by NSF award IIS-0811974; first author
supported by the Fannie & John Hertz Foundation Fellowship.
assumes partial annotation, e.g., sentence bound-
aries, tokenization and typically even part-of-speech
(POS) tagging, has received much attention, elicit-
ing a diverse array of techniques (Smith and Eis-
ner, 2005; Seginer, 2007; Cohen et al, 2008). Klein
and Manning?s (2004) Dependency Model with Va-
lence (DMV) was the first to beat a simple parsing
heuristic ? the right-branching baseline. Today?s
state-of-the-art systems (Headden et al, 2009; Co-
hen and Smith, 2009) are still rooted in the DMV.
Despite recent advances, unsupervised parsers lag
far behind their supervised counterparts. Although
large amounts of unlabeled data are known to im-
prove semi-supervised parsing (Suzuki et al, 2009),
the best unsupervised systems use less data than is
available for supervised training, relying on complex
models instead: Headden et al?s (2009) Extended
Valence Grammar (EVG) combats data sparsity with
smoothing alone, training on the same small subset
of the tree-bank as the classic implementation of the
DMV; Cohen and Smith (2009) use more compli-
cated algorithms (variational EM and MBR decod-
ing) and stronger linguistic hints (tying related parts
of speech and syntactically similar bilingual data).
We explore what can be achieved through judi-
cious use of data and simple, scalable techniques.
Our first approach iterates over a series of training
sets that gradually increase in size and complex-
ity, forming an initialization-independent scaffold-
ing for learning a grammar. It works with Klein and
Manning?s simple model (the original DMV) and
training algorithm (classic EM) but eliminates their
crucial dependence on manually-tuned priors. The
second technique is consistent with the intuition that
learning is most successful within a band of the size-
complexity spectrum. Both could be applied to more
751
intricate models and advanced learning algorithms.
We combine them in a third, efficient hybrid method.
2 Intuition
Focusing on simple examples helps guide unsuper-
vised learning,1 as blindly added confusing data can
easily mislead training. We suggest that unless it is
increased gradually, unbridled, complexity can over-
whelm a system. How to grade an example?s diffi-
culty? The cardinality of its solution space presents
a natural proxy. In the case of parsing, the num-
ber of possible syntactic trees grows exponentially
with sentence length. For longer sentences, the un-
supervised optimization problem becomes severely
under-constrained, whereas for shorter sentences,
learning is tightly reined in by data. In the extreme
case of a single-word sentence, there is no choice
but to parse it correctly. At two words, a raw 50%
chance of telling the head from its dependent is still
high, but as length increases, the accuracy of even
educated guessing rapidly plummets. In model re-
estimation, long sentences amplify ambiguity and
pollute fractional counts with noise. At times, batch
systems are better off using less data.
Baby Steps: Global non-convex optimization is
hard. We propose a meta-heuristic that takes the
guesswork out of initializing local search. Begin-
ning with an easy (convex) case, it slowly extends it
to the fully complex target task by taking tiny steps
in the problem space, trying not to stray far from
the relevant neighborhoods of the solution space. A
series of nested subsets of increasingly longer sen-
tences that culminates in the complete data set offers
a natural progression. Its base case ? sentences of
length one ? has a trivial solution that requires nei-
ther initialization nor search yet reveals something
of sentence heads. The next step ? sentences of
length one and two ? refines initial impressions
of heads, introduces dependents, and exposes their
identities and relative positions. Although not rep-
resentative of the full grammar, short sentences cap-
ture enough information to paint most of the picture
needed by slightly longer sentences. They set up an
easier, incremental subsequent learning task. Step
k + 1 augments training input to include lengths
1It mirrors the effect that boosting hard examples has for
supervised training (Freund and Schapire, 1997).
1, 2, . . . , k, k + 1 of the full data set and executes
local search starting from the (smoothed) model es-
timated by step k. This truly is grammar induction.
Less is More: For standard batch training, just us-
ing simple, short sentences is not enough. They are
rare and do not reveal the full grammar. We find a
?sweet spot? ? sentence lengths that are neither too
long (excluding the truly daunting examples) nor too
few (supplying enough accessible information), us-
ing Baby Steps? learning curve as a guide. We train
where it flattens out, since remaining sentences con-
tribute little (incremental) educational value.2
Leapfrog: As an alternative to discarding data, a
better use of resources is to combine the results of
batch and iterative training up to the sweet spot data
gradation, then iterate with a large step size.
3 Related Work
Two types of scaffolding for guiding language learn-
ing debuted in Elman?s (1993) experiments with
?starting small?: data complexity (restricting input)
and model complexity (restricting memory). In both
cases, gradually increasing complexity allowed ar-
tificial neural networks to master a pseudo-natural
grammar they otherwise failed to learn. Initially-
limited capacity resembled maturational changes in
working memory and attention span that occur over
time in children (Kail, 1984), in line with the ?less
is more? proposal (Newport, 1988; 1990). Although
Rohde and Plaut (1999) failed to replicate this3 re-
sult with simple recurrent networks, other machine
learning techniques reliably benefit from scaffolded
model complexity on a variety of language tasks.
In word-alignment, Brown et al (1993) used IBM
Models 1-4 as ?stepping stones? to training Model 5.
Other prominent examples include ?coarse-to-fine?
2This is akin to McClosky et al?s (2006) ?Goldilocks effect.?
3Worse, they found that limiting input hindered language
acquisition. And making the grammar more English-like (by
introducing and strengthening semantic constraints), increased
the already significant advantage for ?starting large!? With it-
erative training invoking the optimizer multiple times, creating
extra opportunities to converge, Rohde and Plaut (1999) sus-
pected that Elman?s (1993) simulations simply did not allow
networks exposed exclusively to complex inputs sufficient train-
ing time. Our extremely generous, low termination threshold
for EM (see ?5.1) addresses this concern. However, given the
DMV?s purely syntactic POS tag-based approach (see ?5), it
would be prudent to re-test Baby Steps with a lexicalized model.
752
approaches to parsing, translation and speech recog-
nition (Charniak and Johnson, 2005; Charniak et al,
2006; Petrov et al, 2008; Petrov, 2009), and re-
cently unsupervised POS tagging (Ravi and Knight,
2009). Initial models tend to be particularly simple,4
and each refinement towards a full model introduces
only limited complexity, supporting incrementality.
Filtering complex data, the focus of our work,
is unconventional in natural language processing.
Such scaffolding qualifies as shaping ? a method
of instruction (routinely exploited in animal train-
ing) in which the teacher decomposes a complete
task into sub-components, providing an easier path
to learning. When Skinner (1938) coined the term,
he described it as a ?method of successive approx-
imations.? Ideas that gradually make a task more
difficult have been explored in robotics (typically,
for navigation), with reinforcement learning (Singh,
1992; Sanger, 1994; Saksida et al, 1997; Dorigo
and Colombetti, 1998; Savage, 1998; Savage, 2001).
Recently, Krueger and Dayan (2009) showed that
shaping speeds up language acquisition and leads
to better generalization in abstract neural networks.
Bengio et al (2009) confirmed this for deep de-
terministic and stochastic networks, using simple
multi-stage curriculum strategies. They conjectured
that a well-chosen sequence of training criteria ?
different sets of weights on the examples ? could
act as a continuation method (Allgower and Georg,
1990), helping find better local optima for non-
convex objectives. Elman?s learners constrained the
peaky solution space by focusing on just the right
data (simple sentences that introduced basic repre-
sentational categories) at just the right time (early
on, when their plasticity was greatest). Self-shaping,
they simplified tasks through deliberate omission (or
misunderstanding). Analogously, Baby Steps in-
duces an early structural locality bias (Smith and
Eisner, 2006), then relaxes it, as if annealing (Smith
and Eisner, 2004). Its curriculum of binary weights
initially discards complex examples responsible for
?high-frequency noise,? with earlier, ?smoothed?
objectives revealing more of the global picture.
There are important differences between our re-
sults and prior work. In contrast to Elman, we use a
4Brown et al?s (1993) Model 1 (and, similarly, the first baby
step) has a global optimum that can be computed exactly, so that
no initial or subsequent parameters depend on initialization.
large data set (WSJ) of real English. Unlike Bengio
et al and Krueger and Dayan, we shape a parser, not
a language model. Baby Steps is similar, in spirit, to
Smith and Eisner?s methods. Deterministic anneal-
ing (DA) shares nice properties with Baby Steps,
but performs worse than EM for (constituent) pars-
ing; Baby Steps handedly defeats standard training.
Structural annealing works well, but requires a hand-
tuned annealing schedule and direct manipulation of
the objective function; Baby Steps works ?out of the
box,? its locality biases a natural consequence of a
complexity/data-guided tour of optimization prob-
lems. Skewed DA incorporates a good initializer
by interpolating between two probability distribu-
tions, whereas our hybrid, Leapfrog, admits multi-
ple initializers by mixing structures instead. ?Less
is More? is novel and confirms the tacit consensus
implicit in training on small data sets (e.g., WSJ10).
4 Data Sets and Metrics
Klein and Manning (2004) both trained and tested
the DMV on the same customized subset (WSJ10)
of Penn English Treebank?s Wall Street Journal por-
tion (Marcus et al, 1993). Its 49,208 annotated
parse trees were pruned5 down to 7,422 sentences
of at most 10 terminals, spanning 35 unique POS
tags. Following standard practice, automatic ?head-
percolation? rules (Collins, 1999) were used to con-
vert the remaining trees into dependencies. Forced
to produce a single ?best? parse, their algorithm
was judged on accuracy: its directed score was the
fraction of correct dependencies; a more flattering6
undirected score was also used. We employ the
same metrics, emphasizing directed scores, and gen-
eralize WSJk to be the subset of pre-processed sen-
tences with at most k terminals. Our experiments fo-
cus on k ? {1, . . . , 45}, but we also test on WSJ100
and Section 23 of WSJ? (the entire WSJ), as well as
the held-out Brown100 (similarly derived from the
Brown corpus (Francis and Kucera, 1979)). See Fig-
ure 1 for these corpora?s sentence and token counts.
5Stripped of all empty sub-trees, punctuation, and terminals
(tagged # and $) not pronounced where they appear, those sen-
tences still containing more than ten tokens were thrown out.
6Ignoring polarity of parent-child relations partially ob-
scured effects of alternate analyses (systematic choices between
modals and main verbs for heads of sentences, determiners for
noun phrases, etc.) and facilitated comparison with prior work.
753
Corpus Sentences POS Tokens Corpus Sentences POS Tokens
WSJ1 159 159 WSJ13 12,270 110,760
WSJ2 499 839 WSJ14 14,095 136,310
WSJ3 876 1,970 WSJ15 15,922 163,715
WSJ4 1,394 4,042 WSJ20 25,523 336,555
WSJ5 2,008 7,112 WSJ25 34,431 540,895
WSJ6 2,745 11,534 WSJ30 41,227 730,099
WSJ7 3,623 17,680 WSJ35 45,191 860,053
WSJ8 4,730 26,536 WSJ40 47,385 942,801
WSJ9 5,938 37,408 WSJ45 48,418 986,830
WSJ10 7,422 52,248 WSJ100 49,206 1,028,054
WSJ11 8,856 68,022 Section 23 2,353 48,201
WSJ12 10,500 87,750 Brown100 24,208 391,796 5 10 15 20 25 30 35 40 45
5
10
15
20
25
30
35
40
45
Thousands
of Sentences
Thousands
of Tokens 100
200
300
400
500
600
700
800
900
WSJk
Figure 1: Sizes of WSJ{1, . . . , 45, 100}, Section 23 of WSJ? and Brown100.
NNS VBD IN NN ?
Payrolls fell in September .
P = (1 ?
0
z }| {
PSTOP(?, L, T)) ? PATTACH(?, L, VBD)
? (1 ? PSTOP(VBD, L, T)) ? PATTACH(VBD, L, NNS)
? (1 ? PSTOP(VBD, R, T)) ? PATTACH(VBD, R, IN)
? (1 ? PSTOP(IN, R, T)) ? PATTACH(IN, R, NN)
? PSTOP(VBD, L, F) ? PSTOP(VBD, R, F)
? PSTOP(NNS, L, T) ? PSTOP(NNS, R, T)
? PSTOP(IN, L, T) ? PSTOP(IN, R, F)
? PSTOP(NN, L, T) ? PSTOP(NN, R, T)
? PSTOP(?, L, F)
| {z }
1
? PSTOP(?, R, T)
| {z }
1
.
Figure 2: A simple dependency structure for a short sen-
tence and its probability, as factored by the DMV.
5 New Algorithms for the Classic Model
The DMV (Klein and Manning, 2004) is a single-
state head automata model (Alshawi, 1996) over lex-
ical word classes {cw} ? POS tags. Its generative
story for a sub-tree rooted at a head (of class ch) rests
on three types of independent decisions: (i) initial
direction dir ? {L, R} in which to attach children, via
probability PORDER(ch); (ii) whether to seal dir, stop-
ping with probability PSTOP(ch, dir, adj), conditioned
on adj ? {T, F} (true iff considering dir?s first, i.e.,
adjacent, child); and (iii) attachments (of class ca),
according to PATTACH(ch, dir, ca). This produces only
projective trees.7 A root token ? generates the head
of a sentence as its left (and only) child. Figure 2
displays an example that ignores (sums out) PORDER.
The DMV lends itself to unsupervised learn-
7Unlike spanning tree algorithms (McDonald et al, 2005),
DMV?s chart-based method disallows crossing dependencies.
ing via inside-outside re-estimation (Baker, 1979).
Klein and Manning did not use smoothing and
started with an ?ad-hoc harmonic? completion: aim-
ing for balanced trees, non-root heads attached de-
pendents in inverse proportion to (a constant plus)
their distance; ? generated heads uniformly at ran-
dom. This non-distributional heuristic created favor-
able initial conditions that nudged EM towards typi-
cal linguistic dependency structures.
5.1 Algorithm #0: Ad-Hoc?
? A Variation on Original Ad-Hoc Initialization
Since some of the important implementation details
are not available in the literature (Klein and Man-
ning, 2004; Klein, 2005), we had to improvise ini-
tialization and terminating conditions. We suspect
that our choices throughout this section do not match
Klein and Manning?s actual training of the DMV.
We use the following ad-hoc harmonic scores (for
all tokens other than ?): P?ORDER ? 1/2;
P?STOP ? (ds + ?s)?1 = (ds + 3)?1, ds ? 0;
P?ATTACH ? (da + ?a)?1 = (da + 2)?1, da ? 1.
Integers d{s,a} are distances from heads to stopping
boundaries and dependents.8 We initialize train-
ing by producing best-scoring parses of all input
sentences and converting them into proper proba-
bility distributions PSTOP and PATTACH via maximum-
likelihood estimation (a single step of Viterbi train-
ing (Brown et al, 1993)). Since left and right chil-
dren are independent, we drop PORDER altogether, mak-
8Constants ?{s,a} come from personal communication.
Note that ?s is one higher than is strictly necessary to avoid both
division by zero and determinism; ?a could have been safely ze-
roed out, since we never compute 1 ? PATTACH (see Figure 2).
754
ing ?headedness? deterministic. Our parser care-
fully randomizes tie-breaking, so that all parse trees
having the same score get an equal shot at being
selected (both during initialization and evaluation).
We terminate EM when a successive change in over-
all per-token cross-entropy drops below 2?20 bits.
5.2 Algorithm #1: Baby Steps
? An Initialization-Independent Scaffolding
We eliminate the need for initialization by first train-
ing on a trivial subset of the data ? WSJ1; this
works, since there is only one (the correct) way to
parse a single-token sentence. We plug the resulting
model into training on WSJ2 (sentences of up to two
tokens), and so forth, building up to WSJ45.9 This
algorithm is otherwise identical to Ad-Hoc?, with
the exception that it re-estimates each model using
Laplace smoothing, so that earlier solutions could
be passed to next levels, which sometimes contain
previously unseen dependent and head POS tags.
5.3 Algorithm #2: Less is More
? Ad-Hoc? where Baby Steps Flatlines
We jettison long, complex sentences and deploy Ad-
Hoc??s initializer and batch training at WSJk?? ? an
estimate of the sweet spot data gradation. To find
it, we track Baby Steps? successive models? cross-
entropies on the complete data set, WSJ45. An ini-
tial segment of rapid improvement is separated from
the final region of convergence by a knee ? points
of maximum curvature (see Figure 3). We use an
improved10 L method (Salvador and Chan, 2004) to
automatically locate this area of diminishing returns.
Specifically, we determine its end-points [k0, k?] by
minimizing squared error, estimating k?0 = 7 and
k?? = 15. Training at WSJ15 just misses the plateau.
5.4 Algorithm #3: Leapfrog
? A Practical and Efficient Hybrid Mixture
Cherry-picking the best features of ?Less is More?
and Baby Steps, we begin by combining their mod-
9Its 48,418 sentences (see Figure 1) cover 94.4% of all sen-
tences in WSJ; the longest of the missing 790 has length 171.
10Instead of iteratively fitting a two-segment form and adap-
tively discarding its tail, we use three line segments, applying
ordinary least squares to the first two, but requiring the third to
be horizontal and tangent to a minimum. The result is a batch
optimization routine that returns an interval for the knee, rather
than a point estimate (see Figure 3 for details).
5 10 15 20 25 30 35 40 45
3.0
3.5
4.0
4.5
5.0
WSJk
bpt
Cross-entropy h (in bits per token) on WSJ45
Knee
[7, 15] Tight, Flat, Asymptotic Bound
min
b0,m0,b1,m1
2<k0<k?<45
8
>>
>>
>
>
<
>
>
>
>>
>:
k0?1X
k=1
(hk ? b0 ? m0k)2 +
k?X
k=k0
(hk ? b1 ? m1k)2 +
45X
k=k?+1
?
hk ?
45
min
j=k?+1
hj
?2
Figure 3: Cross-entropy on WSJ45 after each baby step, a
piece-wise linear fit, and an estimated region for the knee.
els at WSJk??. Using one best parse from each,
for every sentence in WSJk??, the base case re-
estimates a new model from a mixture of twice the
normal number of trees; inductive steps leap over k??
lengths, conveniently ending at WSJ45, and estimate
their initial models by applying a previous solution
to a new input set. Both follow up the single step of
Viterbi training with at most five iterations of EM.
Our hybrid makes use of two good (condition-
ally) independent initialization strategies and exe-
cutes many iterations of EM where that is cheap ?
at shorter sentences (WSJ15 and below). It then in-
creases the step size, training just three more times
(at WSJ{15, 30, 45}) and allowing only a few (more
expensive) iterations of EM. Early termination im-
proves efficiency and regularizes these final models.
5.5 Reference Algorithms
? Baselines, a Skyline and Published Art
We carve out the problem space using two extreme
initialization strategies: (i) the uninformed uniform
prior, which serves as a fair ?zero-knowledge? base-
line for comparing uninitialized models; and (ii) the
maximum-likelihood ?oracle? prior, computed from
reference parses, which yields a skyline (a reverse
baseline) ? how well any algorithm that stumbled
on the true solution would fare at EM?s convergence.
In addition to citing Klein and Manning?s (2004)
results, we compare our accuracies on Section 23
of WSJ? to two state-of-the-art systems and past
baselines (see Table 2). Headden et al?s (2009)
lexicalized EVG is the best on short sentences, but
755
5 10 15 20 25 30 35 40
20
30
40
50
60
70
80
90
Oracle
Baby StepsAd-Hoc
Uninformed
WSJk
(a) Directed Accuracy (%) on WSJk
5 10 15 20 25 30 35 40 45
(b) Undirected Accuracy (%) on WSJk
Oracle
Baby Steps
Ad-Hoc
Uninformed
Figure 4: Directed and undirected accuracy scores attained by the DMV, when trained and tested on the same gradation
of WSJ, for several different initialization strategies. Green circles mark Klein and Manning?s (2004) published scores;
red, violet and blue curves represent the supervised (maximum-likelihood oracle) initialization, Baby Steps, and the
uninformed uniform prior. Dotted curves reflect starting performance, solid curves register performance at EM?s
convergence, and the arrows connecting them emphasize the impact of learning.
5 10 15 20 25 30 35 40 45
20
30
40
50
60
WSJk
Oracle
Leapfrog
Baby Steps
Ad-Hoc?
Uninformed
Ad-Hoc
Directed Accuracy (%) on WSJk
Figure 5: Directed accuracies for Ad-Hoc? (shown in
green) and Leapfrog (in gold); all else as in Figure 4(a).
its performance is unreported for longer sentences,
for which Cohen and Smith?s (2009) seem to be
the highest published scores; we include their in-
termediate results that preceded parameter-tying ?
Bayesian models with Dirichlet and log-normal pri-
ors, coupled with both Viterbi and minimum Bayes-
risk (MBR) decoding (Cohen et al, 2008).
6 Experimental Results
We packed thousands of empirical outcomes into the
space of several graphs (Figures 4, 5 and 6). The col-
ors (also in Tables 1 and 2) correspond to different
initialization strategies ? to a first approximation,
the learning algorithm was held constant (see ?5).
Figures 4 and 5 tell one part of our story. As data
sets increase in size, training algorithms gain access
to more information; however, since in this unsu-
pervised setting training and test sets are the same,
additional longer sentences make for substantially
more challenging evaluation. To control for these
dynamics, we applied Laplace smoothing to all (oth-
erwise unsmoothed) models and re-plotted their per-
formance, holding several test sets fixed, in Figure 6.
We report undirected accuracies parenthetically.
6.1 Result #1: Baby Steps
Figure 4 traces out performance on the training set.
Klein and Manning?s (2004) published scores ap-
pear as dots (Ad-Hoc) at WSJ10: 43.2% (63.7%).
Baby Steps achieves 53.0% (65.7%) by WSJ10;
trained and tested on WSJ45, it gets 39.7% (54.3%).
Uninformed, classic EM learns little about directed
dependencies: it improves only slightly, e.g., from
17.3% (34.2%) to 19.1% (46.5%) on WSJ45 (learn-
ing some of the structure, as evidenced by its undi-
rected scores), but degrades with shorter sentences,
where its initial guessing rate is high. In the case
of oracle training, we expected EM to walk away
from supervised solutions (Elworthy, 1994; Meri-
756
5 10 15 20 25 30 35 40
20
30
40
50
60
70
80
(a) Directed Accuracy (%) on WSJ10
WSJk
Oracle
Leapfrog
Baby Steps
Less is More
| {z }
Ad-Hoc?
Ad-Hoc
Uninformed
5 10 15 20 25 30 35 40 45
(b) Directed Accuracy (%) on WSJ40
Oracle
Leapfrog
Baby Steps
Less is More
| {z }
Ad-Hoc?
Uninformed
Figure 6: Directed accuracies attained by the DMV, when trained at various gradations of WSJ, smoothed, then tested
against fixed evaluation sets ? WSJ{10, 40}; graphs for WSJ{20, 30}, not shown, are qualitatively similar to WSJ40.
aldo, 1994; Liang and Klein, 2008), but the ex-
tent of its drops is alarming, e.g., from the super-
vised 69.8% (72.2%) to the skyline?s 50.6% (59.5%)
on WSJ45. In contrast, Baby Steps? scores usu-
ally do not change much from one step to the
next, and where its impact of learning is big (at
WSJ{4, 5, 14}), it is invariably positive.
6.2 Result #2: Less is More
Ad-Hoc??s curve (see Figure 5) suggests how Klein
and Manning?s Ad-Hoc initializer may have scaled
with different gradations of WSJ. Strangely, our im-
plementation performs significantly above their re-
ported numbers at WSJ10: 54.5% (68.3%) is even
slightly higher than Baby Steps; nevertheless, given
enough data (from WSJ22 onwards), Baby Steps
overtakes Ad-Hoc?, whose ability to learn takes a se-
rious dive once the inputs become sufficiently com-
plex (at WSJ23), and never recovers. Note that Ad-
Hoc??s biased prior peaks early (at WSJ6), eventu-
ally falls below the guessing rate (by WSJ24), yet
still remains well-positioned to climb, outperform-
ing uninformed learning.
Figure 6 shows that Baby Steps scales better with
more (complex) data ? its curves do not trend
downwards. However, a good initializer induces a
sweet spot at WSJ15, where the DMV is learned
best using Ad-Hoc?. This mode is ?Less is More,?
scoring 44.1% (58.9%) on WSJ45. Curiously, even
oracle training exhibits a bump at WSJ15: once sen-
tences get long enough (at WSJ36), its performance
degrades below that of oracle training with virtually
no supervision (at the hardly representative WSJ3).
6.3 Result #3: Leapfrog
Mixing Ad-Hoc? with Baby Steps at WSJ15 yields
a model whose performance initially falls between
its two parents but surpasses both with a little train-
ing (see Figure 5). Leaping to WSJ45, via WSJ30,
results in our strongest model: its 45.0% (58.4%) ac-
curacy bridges half of the gap between Baby Steps
and the skyline, and at a tiny fraction of the cost.
6.4 Result #4: Generalization
Our models carry over to the larger WSJ100, Section
23 of WSJ?, and the independent Brown100 (see
Table 1). Baby Steps improves out of domain, con-
firming that shaping generalizes well (Krueger and
Dayan, 2009; Bengio et al, 2009). Leapfrog does
best across the board but dips on Brown100, despite
its safe-guards against over-fitting.
Section 23 (see Table 2) reveals, unexpectedly,
that Baby Steps would have been state-of-the-art in
2008, whereas ?Less is More? outperforms all prior
work on longer sentences. Baby Steps is competi-
tive with log-normal families (Cohen et al, 2008),
scoring slightly better on longer sentences against
Viterbi decoding, though worse against MBR. ?Less
is More? beats state-of-the-art on longer sentences
by close to 2%; Leapfrog gains another 1%.
757
Ad-Hoc? Baby Steps Leapfrog Ad-Hoc? Baby Steps Leapfrog
Section 23 44.1 (58.8) 39.2 (53.8) 43.3 (55.7) 31.5 (51.6) 39.4 (54.0) 45.0 (58.4)
WSJ100 43.8 (58.6) 39.2 (53.8) 43.3 (55.6) @15 31.3 (51.5) 39.4 (54.1) 44.7 (58.1) @45
Brown100 43.3 (59.2) 42.3 (55.1) 42.8 (56.5) 32.0 (52.4) 42.5 (55.5) 43.6 (59.1)
Table 1: Directed and undirected accuracies on Section 23 of WSJ?, WSJ100 and Brown100 for Ad-Hoc?, Baby
Steps and Leapfrog, trained at WSJ15 and WSJ45.
Decoding WSJ10 WSJ20 WSJ?
Attach-Right (Klein and Manning, 2004) ? 38.4 33.4 31.7
DMV Ad-Hoc (Klein and Manning, 2004) Viterbi 45.8 39.1 34.2
Dirichlet (Cohen et al, 2008) Viterbi 45.9 39.4 34.9
Ad-Hoc (Cohen et al, 2008) MBR 46.1 39.9 35.9
Dirichlet (Cohen et al, 2008) MBR 46.1 40.6 36.9
Log-Normal Families (Cohen et al, 2008) Viterbi 59.3 45.1 39.0
Baby Steps (@15) Viterbi 55.5 44.3 39.2
Baby Steps (@45) Viterbi 55.1 44.4 39.4
Log-Normal Families (Cohen et al, 2008) MBR 59.4 45.9 40.5
Shared Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 61.3 47.4 41.4
Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 62.0 48.0 42.2
Less is More (Ad-Hoc? @15) Viterbi 56.2 48.2 44.1
Leapfrog (Hybrid @45) Viterbi 57.1 48.7 45.0
EVG Smoothed (skip-val) (Headden et al, 2009) Viterbi 62.1
Smoothed (skip-head) (Headden et al, 2009) Viterbi 65.0
Smoothed (skip-head), Lexicalized (Headden et al, 2009) Viterbi 68.8
Table 2: Directed accuracies on Section 23 of WSJ{10, 20,? } for several baselines and recent state-of-the-art systems.
7 Conclusion
We explored three simple ideas for unsupervised de-
pendency parsing. Pace Halevy et al (2009), we
find ?Less is More? ? the paradoxical result that
better performance can be attained by training with
less data, even when removing samples from the true
(test) distribution. Our small tweaks to Klein and
Manning?s approach of 2004 break through the 2009
state-of-the-art on longer sentences, when trained at
WSJ15 (the auto-detected sweet spot gradation).
The second, Baby Steps, is an elegant meta-
heuristic for optimizing non-convex training crite-
ria. It eliminates the need for linguistically-biased
manually-tuned initializers, particularly if the loca-
tion of the sweet spot is not known. This tech-
nique scales gracefully with more (complex) data
and should easily carry over to more powerful pars-
ing models and learning algorithms.
Finally, Leapfrog forgoes the elegance and metic-
ulousness of Baby Steps in favor of pragmatism.
Employing both good initialization strategies at
its disposal, and spending CPU cycles wisely, it
achieves better performance than both ?Less is
More? and Baby Steps.
Future work could explore unifying these tech-
niques with other state-of-the-art approaches. It may
be useful to scaffold on both data and model com-
plexity, e.g., by increasing head automata?s number
of states (Alshawi and Douglas, 2000). We see many
opportunities for improvement, considering the poor
performance of oracle training relative to the super-
vised state-of-the-art, and in turn the poor perfor-
mance of unsupervised state-of-the-art relative to the
oracle models.11 To this end, it would be instructive
to understand both the linguistic and statistical na-
ture of the sweet spot, and to test its universality.
Acknowledgments
We thank Angel X. Chang, Pi-Chuan Chang, David L.W. Hall,
Christopher D. Manning, David McClosky, Daniel Ramage and
the anonymous reviewers for many helpful comments on draft
versions of this paper.
References
E. L. Allgower and K. Georg. 1990. Numerical Continuation
Methods: An Introduction. Springer-Verlag.
11To facilitate future work, all of our models are publicly
available at http://cs.stanford.edu/?valentin/.
758
H. Alshawi and S. Douglas. 2000. Learning dependency trans-
duction models from unannotated examples. In Royal Soci-
ety of London Philosophical Transactions Series A, volume
358.
H. Alshawi. 1996. Head automata for speech translation. In
Proc. of ICSLP.
J. K. Baker. 1979. Trainable grammars for speech recognition.
In Speech Communication Papers for the 97th Meeting of the
Acoustical Society of America.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009.
Curriculum learning. In ICML.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguistics, 19.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In Proc. of ACL.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil, D. Ellis,
I. Haxton, C. Hill, R. Shrivaths, J. Moore, M. Pozar, and
T. Vu. 2006. Multilevel coarse-to-fine PCFG parsing. In
HLT-NAACL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic normal dis-
tributions for soft parameter tying in unsupervised grammar
induction. In Proc. of NAACL-HLT.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic nor-
mal priors for unsupervised probabilistic grammar induction.
In NIPS.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
M. Dorigo and M. Colombetti. 1998. Robot Shaping: An
Experiment in Behavior Engineering. MIT Press/Bradford
Books.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
D. Elworthy. 1994. Does Baum-Welch re-estimation help tag-
gers? In Proc. of ANLP.
W. N. Francis and H. Kucera, 1979. Manual of Information to
Accompany a Standard Corpus of Present-Day Edited Amer-
ican English, for use with Digital Computers. Department of
Linguistic, Brown University.
Y. Freund and R. E. Schapire. 1997. A decision-theoretic gen-
eralization of on-line learning and an application to boosting.
Journal of Computer and System Sciences, 55(1).
A. Halevy, P. Norvig, and F. Pereira. 2009. The unreasonable
effectiveness of data. IEEE Intelligent Systems, 24(2).
W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with richer con-
texts and smoothing. In Proc. of NAACL-HLT.
R. Kail. 1984. The development of memory in children. W. H.
Freeman and Company, 2nd edition.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In Proc. of ACL.
D. Klein. 2005. The Unsupervised Learning of Natural Lan-
guage Structure. Ph.D. thesis, Stanford University.
K. A. Krueger and P. Dayan. 2009. Flexible shaping: How
learning in small steps helps. Cognition, 110.
P. Liang and D. Klein. 2008. Analyzing the errors of unsuper-
vised learning. In Proc. of HLT-ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2).
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In Proc. of NAACL-HLT.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning tree al-
gorithms. In Proc. of HLT-EMNLP.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2):155?172.
E. L. Newport. 1988. Constraints on learning and their role in
language acquisition: Studies of the acquisition of American
Sign Language. Language Sciences, 10(1).
E. L. Newport. 1990. Maturational constraints on language
learning. Cognitive Science, 14(1).
S. Petrov, A. Haghighi, and D. Klein. 2008. Coarse-to-fine
syntactic machine translation using language projections. In
Proc. of EMNLP.
S. O. Petrov. 2009. Coarse-to-Fine Natural Language Process-
ing. Ph.D. thesis, University of California, Berkeley.
S. Ravi and K. Knight. 2009. Minimized models for unsuper-
vised part-of-speech tagging. In Proc. of ACL-IJCNLP.
D. L. T. Rohde and D. C. Plaut. 1999. Language acquisition in
the absence of explicit negative evidence: How important is
starting small? Cognition, 72(1).
L. M. Saksida, S. M. Raymond, and D. S. Touretzky. 1997.
Shaping robot behavior using principles from instrumental
conditioning. Robotics and Autonomous Systems, 22(3).
S. Salvador and P. Chan. 2004. Determining the number of
clusters/segments in hierarchical clustering/segmentation al-
gorithms. In Proc. of ICTAI.
T. D. Sanger. 1994. Neural network learning control of
robot manipulators using gradually increasing task difficulty.
IEEE Trans. on Robotics and Automation, 10.
T. Savage. 1998. Shaping: The link between rats and robots.
Connection Science, 10(3).
T. Savage. 2001. Shaping: A multiple contingencies analysis
and its relevance to behaviour-based robotics. Connection
Science, 13(3).
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
Proc. of ACL.
S. P. Singh. 1992. Transfer of learning by composing solutions
of elemental squential tasks. Machine Learning, 8.
B. F. Skinner. 1938. The behavior of organisms: An experi-
mental analysis. Appleton-Century-Crofts.
N. A. Smith and J. Eisner. 2004. Annealing techniques for
unsupervised statistical language learning. In Proc. of ACL.
N. A. Smith and J. Eisner. 2005. Guiding unsupervised gram-
mar induction using contrastive estimation. In Proc. of the
IJCAI Workshop on Grammatical Inference Applications.
N. A. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In Proc. of
COLING-ACL.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009. An
empirical study of semi-supervised structured conditional
models for dependency parsing. In Proc. of EMNLP.
759
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1278?1287,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing
Valentin I. Spitkovsky
Computer Science Department
Stanford University and Google Inc.
valentin@google.com
Daniel Jurafsky
Departments of Linguistics and
Computer Science, Stanford University
jurafsky@stanford.edu
Hiyan Alshawi
Google Inc.
hiyan@google.com
Abstract
We show how web mark-up can be used
to improve unsupervised dependency pars-
ing. Starting from raw bracketings of four
common HTML tags (anchors, bold, ital-
ics and underlines), we refine approximate
partial phrase boundaries to yield accurate
parsing constraints. Conversion proce-
dures fall out of our linguistic analysis of
a newly available million-word hyper-text
corpus. We demonstrate that derived con-
straints aid grammar induction by training
Klein and Manning?s Dependency Model
with Valence (DMV) on this data set: pars-
ing accuracy on Section 23 (all sentences)
of the Wall Street Journal corpus jumps
to 50.4%, beating previous state-of-the-
art by more than 5%. Web-scale exper-
iments show that the DMV, perhaps be-
cause it is unlexicalized, does not benefit
from orders of magnitude more annotated
but noisier data. Our model, trained on a
single blog, generalizes to 53.3% accuracy
out-of-domain, against the Brown corpus
? nearly 10% higher than the previous
published best. The fact that web mark-up
strongly correlates with syntactic structure
may have broad applicability in NLP.
1 Introduction
Unsupervised learning of hierarchical syntactic
structure from free-form natural language text is
a hard problem whose eventual solution promises
to benefit applications ranging from question an-
swering to speech recognition and machine trans-
lation. A restricted version of this problem that tar-
gets dependencies and assumes partial annotation
? sentence boundaries and part-of-speech (POS)
tagging ? has received much attention. Klein
and Manning (2004) were the first to beat a sim-
ple parsing heuristic, the right-branching baseline;
today?s state-of-the-art systems (Headden et al,
2009; Cohen and Smith, 2009; Spitkovsky et al,
2010a) are rooted in their Dependency Model with
Valence (DMV), still trained using variants of EM.
Pereira and Schabes (1992) outlined three ma-
jor problems with classic EM, applied to a related
problem, constituent parsing. They extended clas-
sic inside-outside re-estimation (Baker, 1979) to
respect any bracketing constraints included with
a training corpus. This conditioning on partial
parses addressed all three problems, leading to:
(i) linguistically reasonable constituent boundaries
and induced grammars more likely to agree with
qualitative judgments of sentence structure, which
is underdetermined by unannotated text; (ii) fewer
iterations needed to reach a good grammar, coun-
tering convergence properties that sharply deterio-
rate with the number of non-terminal symbols, due
to a proliferation of local maxima; and (iii) better
(in the best case, linear) time complexity per it-
eration, versus running time that is ordinarily cu-
bic in both sentence length and the total num-
ber of non-terminals, rendering sufficiently large
grammars computationally impractical. Their al-
gorithm sometimes found good solutions from
bracketed corpora but not from raw text, sup-
porting the view that purely unsupervised, self-
organizing inference methods can miss the trees
for the forest of distributional regularities. This
was a promising break-through, but the problem
of whence to get partial bracketings was left open.
We suggest mining partial bracketings from a
cheap and abundant natural language resource: the
hyper-text mark-up that annotates web-pages. For
example, consider that anchor text can match lin-
guistic constituents, such as verb phrases, exactly:
..., whereas McCain is secure on the topic, Obama
<a>[VP worries about winning the pro-Israel vote]</a>.
To validate this idea, we created a new data set,
novel in combining a real blog?s raw HTML with
tree-bank-like constituent structure parses, gener-
1278
ated automatically. Our linguistic analysis of the
most prevalent tags (anchors, bold, italics and un-
derlines) over its 1M+ words reveals a strong con-
nection between syntax and mark-up (all of our
examples draw from this corpus), inspiring several
simple techniques for automatically deriving pars-
ing constraints. Experiments with both hard and
more flexible constraints, as well as with different
styles and quantities of annotated training data ?
the blog, web news and the web itself, confirm that
mark-up-induced constraints consistently improve
(otherwise unsupervised) dependency parsing.
2 Intuition and Motivating Examples
It is natural to expect hidden structure to seep
through when a person annotates a sentence. As it
happens, a non-trivial fraction of the world?s pop-
ulation routinely annotates text diligently, if only
partially and informally.1 They inject hyper-links,
vary font sizes, and toggle colors and styles, using
mark-up technologies such as HTML and XML.
As noted, web annotations can be indicative of
phrase boundaries, e.g., in a complicated sentence:
In 1998, however, as I <a>[VP established in
<i>[NP The New Republic]</i>]</a> and Bill
Clinton just <a>[VP confirmed in his memoirs]</a>,
Netanyahu changed his mind and ...
In doing so, mark-up sometimes offers useful cues
even for low-level tokenization decisions:
[NP [NP Libyan ruler]
<a>[NP Mu?ammar al-Qaddafi]</a>] referred to ...
(NP (ADJP (NP (JJ Libyan) (NN ruler))
(JJ Mu))
(?? ?) (NN ammar) (NNS al-Qaddafi))
Above, a backward quote in an Arabic name con-
fuses the Stanford parser.2 Yet mark-up lines up
with the broken noun phrase, signals cohesion, and
moreover sheds light on the internal structure of
a compound. As Vadas and Curran (2007) point
out, such details are frequently omitted even from
manually compiled tree-banks that err on the side
of flat annotations of base-NPs.
Admittedly, not all boundaries between HTML
tags and syntactic constituents match up nicely:
..., but [S [NP the <a><i>Toronto
Star</i>][VP reports [NP this][PP in the
softest possible way]</a>,[S stating only that ...]]]
Combining parsing with mark-up may not be
straight-forward, but there is hope: even above,
1Even when (American) grammar schools lived up to their
name, they only taught dependencies. This was back in the
days before constituent grammars were invented.
2http://nlp.stanford.edu:8080/parser/
one of each nested tag?s boundaries aligns; and
Toronto Star?s neglected determiner could be for-
given, certainly within a dependency formulation.
3 A High-Level Outline of Our Approach
Our idea is to implement the DMV (Klein and
Manning, 2004) ? a standard unsupervised gram-
mar inducer. But instead of learning the unan-
notated test set, we train with text that contains
web mark-up, using various ways of converting
HTML into parsing constraints. We still test on
WSJ (Marcus et al, 1993), in the standard way,
and also check generalization against a hidden
data set ? the Brown corpus (Francis and Kucera,
1979). Our parsing constraints come from a blog
? a new corpus we created, the web and news (see
Table 1 for corpora?s sentence and token counts).
To facilitate future work, we make the final
models and our manually-constructed blog data
publicly available.3 Although we are unable
to share larger-scale resources, our main results
should be reproducible, as both linguistic analysis
and our best model rely exclusively on the blog.
Corpus Sentences POS Tokens
WSJ? 49,208 1,028,347
Section 23 2,353 48,201
WSJ45 48,418 986,830
WSJ15 15,922 163,715
Brown100 24,208 391,796
BLOGp 57,809 1,136,659
BLOGt45 56,191 1,048,404
BLOGt15 23,214 212,872
NEWS45 2,263,563,078 32,119,123,561
NEWS15 1,433,779,438 11,786,164,503
WEB45 8,903,458,234 87,269,385,640
WEB15 7,488,669,239 55,014,582,024
Table 1: Sizes of corpora derived from WSJ and
Brown, as well as those we collected from the web.
4 Data Sets for Evaluation and Training
The appeal of unsupervised parsing lies in its abil-
ity to learn from surface text alone; but (intrinsic)
evaluation still requires parsed sentences. Follow-
ing Klein and Manning (2004), we begin with ref-
erence constituent parses and compare against de-
terministically derived dependencies: after prun-
ing out all empty subtrees, punctuation and ter-
minals (tagged # and $) not pronounced where
they appear, we drop all sentences with more
than a prescribed number of tokens remaining and
use automatic ?head-percolation? rules (Collins,
1999) to convert the rest, as is standard practice.
3http://cs.stanford.edu/?valentin/
1279
Length Marked POS Bracketings Length Marked POS Bracketings
Cutoff Sentences Tokens All Multi-Token Cutoff Sentences Tokens All Multi-Token
0 6,047 1,136,659 7,731 6,015 8 485 14,528 710 684
1 of 57,809 149,483 7,731 6,015 9 333 10,484 499 479
2 4,934 124,527 6,482 6,015 10 245 7,887 365 352
3 3,295 85,423 4,476 4,212 15 42 1,519 65 63
4 2,103 56,390 2,952 2,789 20 13 466 20 20
5 1,402 38,265 1,988 1,874 25 6 235 10 10
6 960 27,285 1,365 1,302 30 3 136 6 6
7 692 19,894 992 952 40 0 0 0 0
Table 2: Counts of sentences, tokens and (unique) bracketings for BLOGp, restricted to only those
sentences having at least one bracketing no shorter than the length cutoff (but shorter than the sentence).
Our primary reference sets are derived from the
Penn English Treebank?s Wall Street Journal por-
tion (Marcus et al, 1993): WSJ45 (sentences with
fewer than 46 tokens) and Section 23 of WSJ? (all
sentence lengths). We also evaluate on Brown100,
similarly derived from the parsed portion of the
Brown corpus (Francis and Kucera, 1979). While
we use WSJ45 and WSJ15 to train baseline mod-
els, the bulk of our experiments is with web data.
4.1 A News-Style Blog: Daniel Pipes
Since there was no corpus overlaying syntactic
structure with mark-up, we began constructing a
new one by downloading articles4 from a news-
style blog. Although limited to a single genre ?
political opinion, danielpipes.org is clean, consis-
tently formatted, carefully edited and larger than
WSJ (see Table 1). Spanning decades, Pipes?
editorials are mostly in-domain for POS taggers
and tree-bank-trained parsers; his recent (internet-
era) entries are thoroughly cross-referenced, con-
veniently providing just the mark-up we hoped to
study via uncluttered (printer-friendly) HTML.5
After extracting moderately clean text and
mark-up locations, we used MxTerminator (Rey-
nar and Ratnaparkhi, 1997) to detect sentence
boundaries. This initial automated pass begot mul-
tiple rounds of various semi-automated clean-ups
that involved fixing sentence breaking, modifying
parser-unfriendly tokens, converting HTML enti-
ties and non-ASCII text, correcting typos, and so
on. After throwing away annotations of fractional
words (e.g., <i>basmachi</i>s) and tokens (e.g.,
<i>Sesame Street</i>-like), we broke up all mark-
up that crossed sentence boundaries (i.e., loosely
speaking, replaced constructs like <u>...][S...</u>
with <u>...</u> ][S <u>...</u>) and discarded any
4http://danielpipes.org/art/year/all
5http://danielpipes.org/article print.php?
id=. . .
tags left covering entire sentences.
We finalized two versions of the data: BLOGt,
tagged with the Stanford tagger (Toutanova and
Manning, 2000; Toutanova et al, 2003),6 and
BLOGp, parsed with Charniak?s parser (Charniak,
2001; Charniak and Johnson, 2005).7 The rea-
son for this dichotomy was to use state-of-the-art
parses to analyze the relationship between syntax
and mark-up, yet to prevent jointly tagged (and
non-standard AUX[G]) POS sequences from interfer-
ing with our (otherwise unsupervised) training.8
4.2 Scaled up Quantity: The (English) Web
We built a large (see Table 1) but messy data set,
WEB ? English-looking web-pages, pre-crawled
by a search engine. To avoid machine-generated
spam, we excluded low quality sites flagged by the
indexing system. We kept only sentence-like runs
of words (satisfying punctuation and capitalization
constraints), POS-tagged with TnT (Brants, 2000).
4.3 Scaled up Quality: (English) Web News
In an effort to trade quantity for quality, we con-
structed a smaller, potentially cleaner data set,
NEWS. We reckoned editorialized content would
lead to fewer extracted non-sentences. Perhaps
surprisingly, NEWS is less than an order of magni-
tude smaller than WEB (see Table 1); in part, this
is due to less aggressive filtering ? we trust sites
approved by the human editors at Google News.9
In all other respects, our pre-processing of NEWS
pages was identical to our handling of WEB data.
6http://nlp.stanford.edu/software/
stanford-postagger-2008-09-28.tar.gz
7ftp://ftp.cs.brown.edu/pub/nlparser/
parser05Aug16.tar.gz
8However, since many taggers are themselves trained on
manually parsed corpora, such as WSJ, no parser that relies
on external POS tags could be considered truly unsupervised;
for a fully unsupervised example, see Seginer?s (2007) CCL
parser, available at http://www.seggu.net/ccl/
9http://news.google.com/
1280
5 Linguistic Analysis of Mark-Up
Is there a connection between mark-up and syn-
tactic structure? Previous work (Barr et al, 2008)
has only examined search engine queries, show-
ing that they consist predominantly of short noun
phrases. If web mark-up shared a similar char-
acteristic, it might not provide sufficiently dis-
ambiguating cues to syntactic structure: HTML
tags could be too short (e.g., singletons like
?click <a>here</a>?) or otherwise unhelpful in re-
solving truly difficult ambiguities (such as PP-
attachment). We began simply by counting vari-
ous basic events in BLOGp.
Count POS Sequence Frac Sum
1 1,242 NNP NNP 16.1%
2 643 NNP 8.3 24.4
3 419 NNP NNP NNP 5.4 29.8
4 414 NN 5.4 35.2
5 201 JJ NN 2.6 37.8
6 138 DT NNP NNP 1.8 39.5
7 138 NNS 1.8 41.3
8 112 JJ 1.5 42.8
9 102 VBD 1.3 44.1
10 92 DT NNP NNP NNP 1.2 45.3
11 85 JJ NNS 1.1 46.4
12 79 NNP NN 1.0 47.4
13 76 NN NN 1.0 48.4
14 61 VBN 0.8 49.2
15 60 NNP NNP NNP NNP 0.8 50.0
BLOGp +3,869 more with Count ? 49 50.0%
Table 3: Top 50% of marked POS tag sequences.
Count Non-Terminal Frac Sum
1 5,759 NP 74.5%
2 997 VP 12.9 87.4
3 524 S 6.8 94.2
4 120 PP 1.6 95.7
5 72 ADJP 0.9 96.7
6 61 FRAG 0.8 97.4
7 41 ADVP 0.5 98.0
8 39 SBAR 0.5 98.5
9 19 PRN 0.2 98.7
10 18 NX 0.2 99.0
BLOGp +81 more with Count ? 16 1.0%
Table 4: Top 99% of dominating non-terminals.
5.1 Surface Text Statistics
Out of 57,809 sentences, 6,047 (10.5%) are anno-
tated (see Table 2); and 4,934 (8.5%) have multi-
token bracketings. We do not distinguish HTML
tags and track only unique bracketing end-points
within a sentence. Of these, 6,015 are multi-token
? an average per-sentence yield of 10.4%.10
10A non-trivial fraction of our corpus is older (pre-internet)
unannotated articles, so this estimate may be conservative.
As expected, many of the annotated words are
nouns, but there are adjectives, verbs and other
parts of speech too (see Table 3). Mark-up is short,
typically under five words, yet (by far) the most
frequently marked sequence of POS tags is a pair.
5.2 Common Syntactic Subtrees
For three-quarters of all mark-up, the lowest domi-
nating non-terminal is a noun phrase (see Table 4);
there are also non-trace quantities of verb phrases
(12.9%) and other phrases, clauses and fragments.
Of the top fifteen ? 35.2% of all ? annotated
productions, only one is not a noun phrase (see Ta-
ble 5, left). Four of the fifteen lowest dominating
non-terminals do not match the entire bracketing
? all four miss the leading determiner, as we saw
earlier. In such cases, we recursively split internal
nodes until the bracketing aligned, as follows:
[S [NP the <a>Toronto Star][VP reports [NP this]
[PP in the softest possible way]</a>,[S stating ...]]]
S? NP VP? DT NNP NNP VBZ NP PP S
We can summarize productions more compactly
by using a dependency framework and clipping
off any dependents whose subtrees do not cross a
bracketing boundary, relative to the parent. Thus,
DT NNP NNP VBZ DT IN DT JJS JJ NN
becomes DT NNP VBZ, ?the <a>Star reports</a>.?
Viewed this way, the top fifteen (now collapsed)
productions cover 59.4% of all cases and include
four verb heads, in addition to a preposition and
an adjective (see Table 5, right). This exposes five
cases of inexact matches, three of which involve
neglected determiners or adjectives to the left of
the head. In fact, the only case that cannot be ex-
plained by dropped dependents is #8, where the
daughters are marked but the parent is left out.
Most instances contributing to this pattern are flat
NPs that end with a noun, incorrectly assumed to
be the head of all other words in the phrase, e.g.,
... [NP a 1994 <i>New Yorker</i> article] ...
As this example shows, disagreements (as well
as agreements) between mark-up and machine-
generated parse trees with automatically perco-
lated heads should be taken with a grain of salt.11
11In a relatively recent study, Ravi et al (2008) report
that Charniak?s re-ranking parser (Charniak and Johnson,
2005) ? reranking-parserAug06.tar.gz, also available
from ftp://ftp.cs.brown.edu/pub/nlparser/ ? at-
tains 86.3% accuracy when trained on WSJ and tested against
Brown; its nearly 5% performance loss out-of-domain is con-
sistent with the numbers originally reported by Gildea (2001).
1281
Count Constituent Production Frac Sum
1 746 NP? NNP NNP 9.6%
2 357 NP? NNP 4.6 14.3
3 266 NP? NP PP 3.4 17.7
4 183 NP? NNP NNP NNP 2.4 20.1
5 165 NP? DT NNP NNP 2.1 22.2
6 140 NP? NN 1.8 24.0
7 131 NP? DT NNP NNP NNP 1.7 25.7
8 130 NP? DT NN 1.7 27.4
9 127 NP? DT NNP NNP 1.6 29.0
10 109 S ? NP VP 1.4 30.4
11 91 NP? DT NNP NNP NNP 1.2 31.6
12 82 NP? DT JJ NN 1.1 32.7
13 79 NP? NNS 1.0 33.7
14 65 NP? JJ NN 0.8 34.5
15 60 NP? NP NP 0.8 35.3
BLOGp +5,000 more with Count ? 60 64.7%
Count Head-Outward Spawn Frac Sum
1 1,889 NNP 24.4%
2 623 NN 8.1 32.5
3 470 DT NNP 6.1 38.6
4 458 DT NN 5.9 44.5
5 345 NNS 4.5 49.0
6 109 NNPS 1.4 50.4
7 98 VBG 1.3 51.6
8 96 NNP NNP NN 1.2 52.9
9 80 VBD 1.0 53.9
10 77 IN 1.0 54.9
11 74 VBN 1.0 55.9
12 73 DT JJ NN 0.9 56.8
13 71 VBZ 0.9 57.7
14 69 POS NNP 0.9 58.6
15 63 JJ 0.8 59.4
BLOGp +3,136 more with Count ? 62 40.6%
Table 5: Top 15 marked productions, viewed as constituents (left) and as dependencies (right), after
recursively expanding any internal nodes that did not align with the bracketing (underlined). Tabulated
dependencies were collapsed, dropping any dependents that fell entirely in the same region as their parent
(i.e., both inside the bracketing, both to its left or both to its right), keeping only crossing attachments.
5.3 Proposed Parsing Constraints
The straight-forward approach ? forcing mark-up
to correspond to constituents ? agrees with Char-
niak?s parse trees only 48.0% of the time, e.g.,
... in [NP<a>[NP an analysis]</a>[PP of perhaps the
most astonishing PC item I have yet stumbled upon]].
This number should be higher, as the vast major-
ity of disagreements are due to tree-bank idiosyn-
crasies (e.g., bare NPs). Earlier examples of in-
complete constituents (e.g., legitimately missing
determiners) would also be fine in many linguistic
theories (e.g., as N-bars). A dependency formula-
tion is less sensitive to such stylistic differences.
We begin with the hardest possible constraint on
dependencies, then slowly relax it. Every example
used to demonstrate a softer constraint doubles
as a counter-example against all previous versions.
? strict ? seals mark-up into attachments, i.e.,
inside a bracketing, enforces exactly one external
arc ? into the overall head. This agrees with
head-percolated trees just 35.6% of the time, e.g.,
As author of <i>The Satanic Verses</i>, I ...
? loose ? same as strict, but allows the bracket-
ing?s head word to have external dependents. This
relaxation already agrees with head-percolated de-
pendencies 87.5% of the time, catching many
(though far from all) dropped dependents, e.g.,
. . . the <i>Toronto Star</i> reports . . .
? sprawl ? same as loose, but now allows all
words inside a bracketing to attach external de-
pendents.12 This boosts agreement with head-
percolated trees to 95.1%, handling new cases,
e.g., where ?Toronto Star? is embedded in longer
mark-up that includes its own parent ? a verb:
. . . the <a>Toronto Star reports . . .</a> . . .
? tear ? allows mark-up to fracture after all,
requiring only that the external heads attaching the
pieces lie to the same side of the bracketing. This
propels agreement with percolated dependencies
to 98.9%, fixing previously broken PP-attachment
ambiguities, e.g., a fused phrase like ?Fox News in
Canada? that detached a preposition from its verb:
... concession ... has raised eyebrows among those
waiting [PP for <a>Fox News][PP in Canada]</a>.
Most of the remaining 1.1% of disagreements are
due to parser errors. Nevertheless, it is possible for
mark-up to be torn apart by external heads from
both sides. We leave this section with a (very rare)
true negative example. Below, ?CSA? modifies
?authority? (to its left), appositively, while ?Al-
Manar? modifies ?television? (to its right):13
The French broadcasting authority, <a>CSA, banned
... Al-Manar</a> satellite television from ...
12This view evokes the trapezoids of the O(n3) recognizer
for split head automaton grammars (Eisner and Satta, 1999).
13But this is a stretch, since the comma after ?CSA? ren-
ders the marked phrase ungrammatical even out of context.
1282
6 Experimental Methods and Metrics
We implemented the DMV (Klein and Manning,
2004), consulting the details of (Spitkovsky et al,
2010a). Crucially, we swapped out inside-outside
re-estimation in favor of Viterbi training. Not only
is it better-suited to the general problem (see ?7.1),
but it also admits a trivial implementation of (most
of) the dependency constraints we proposed.14
5 10 15 20 25 30 35 40 45
4.5
5.0
5.5
WSJk
bpt
lowest cross-entropy (4.32bpt) attained at WSJ8
x-Entropy h (in bits per token) on WSJ15
Figure 1: Sentence-level cross-entropy on WSJ15
for Ad-Hoc? initializers of WSJ{1, . . . , 45}.
Six settings parameterized each run:
? INIT: 0? default, uniform initialization; or
1 ? a high quality initializer, pre-trained using
Ad-Hoc? (Spitkovsky et al, 2010a): we chose the
Laplace-smoothed model trained at WSJ15 (the
?sweet spot? data gradation) but initialized off
WSJ8, since that ad-hoc harmonic initializer has
the best cross-entropy on WSJ15 (see Figure 1).
? GENRE: 0? default, baseline training on WSJ;
else, uses 1? BLOGt; 2? NEWS; or 3? WEB.
? SCOPE: 0 ? default, uses all sentences up to
length 45; if 1, trains using sentences up to length
15; if 2, re-trains on sentences up to length 45,
starting from the solution to sentences up to length
15, as recommended by Spitkovsky et al (2010a).
? CONSTR: if 4, strict; if 3, loose; and if 2,
sprawl. We did not implement level 1, tear. Over-
constrained sentences are re-attempted at succes-
sively lower levels until they become possible to
parse, if necessary at the lowest (default) level 0.15
? TRIM: if 1, discards any sentence without a sin-
gle multi-token mark-up (shorter than its length).
? ADAPT: if 1, upon convergence, initializes re-
training on WSJ45 using the solution to <GENRE>,
attempting domain adaptation (Lee et al, 1991).
These make for 294 meaningful combinations. We
judged each one by its accuracy on WSJ45, using
standard directed scoring ? the fraction of correct
dependencies over randomized ?best? parse trees.
14We analyze the benefits of Viterbi training in a compan-
ion paper (Spitkovsky et al, 2010b), which dedicates more
space to implementation and to the WSJ baselines used here.
15At level 4, <b> X<u> Y</b> Z</u> is over-constrained.
7 Discussion of Experimental Results
Evaluation on Section 23 of WSJ and Brown re-
veals that blog-training beats all published state-
of-the-art numbers in every traditionally-reported
length cutoff category, with news-training not far
behind. Here is a mini-preview of these results, for
Section 23 of WSJ10 and WSJ? (from Table 8):
WSJ10 WSJ?
(Cohen and Smith, 2009) 62.0 42.2
(Spitkovsky et al, 2010a) 57.1 45.0
NEWS-best 67.3 50.1
BLOGt-best 69.3 50.4
(Headden et al, 2009) 68.8
Table 6: Directed accuracies on Section 23 of
WSJ{10,? } for three recent state-of-the-art sys-
tems and our best runs (as judged against WSJ45)
for NEWS and BLOGt (more details in Table 8).
Since our experimental setup involved testing
nearly three hundred models simultaneously, we
must take extreme care in analyzing and interpret-
ing these results, to avoid falling prey to any loom-
ing ?data-snooping? biases.16 In a sufficiently
large pool of models, where each is trained using
a randomized and/or chaotic procedure (such as
ours), the best may look good due to pure chance.
We appealed to three separate diagnostics to con-
vince ourselves that our best results are not noise.
The most radical approach would be to write off
WSJ as a development set and to focus only on the
results from the held-out Brown corpus. It was ini-
tially intended as a test of out-of-domain general-
ization, but since Brown was in no way involved
in selecting the best models, it also qualifies as
a blind evaluation set. We observe that our best
models perform even better (and gain more ? see
Table 8) on Brown than on WSJ ? a strong indi-
cation that our selection process has not overfitted.
Our second diagnostic is a closer look at WSJ.
Since we cannot graph the full (six-dimensional)
set of results, we begin with a simple linear re-
gression, using accuracy on WSJ45 as the depen-
dent variable. We prefer this full factorial design
to the more traditional ablation studies because it
allows us to account for and to incorporate every
single experimental data point incurred along the
16In the standard statistical hypothesis testing setting, it
is reasonable to expect that p% of randomly chosen hy-
potheses will appear significant at the p% level simply by
chance. Consequently, multiple hypothesis testing requires
re-evaluating significance levels ? adjusting raw p-values,
e.g., using the Holm-Bonferroni method (Holm, 1979).
1283
Corpus Marked Sentences All Sentences POS Tokens All Bracketings Multi-Token Bracketings
BLOGt45 5,641 56,191 1,048,404 7,021 5,346
BLOG?t45 4,516 4,516 104,267 5,771 5,346
BLOGt15 1,562 23,214 212,872 1,714 1,240
BLOG?t15 1,171 1,171 11,954 1,288 1,240
NEWS45 304,129,910 2,263,563,078 32,119,123,561 611,644,606 477,362,150
NEWS?45 205,671,761 205,671,761 2,740,258,972 453,781,081 392,600,070
NEWS15 211,659,549 1,433,779,438 11,786,164,503 365,145,549 274,791,675
NEWS?15 147,848,358 147,848,358 1,397,562,474 272,223,918 231,029,921
WEB45 1,577,208,680 8,903,458,234 87,269,385,640 3,309,897,461 2,459,337,571
WEB?45 933,115,032 933,115,032 11,552,983,379 2,084,359,555 1,793,238,913
WEB15 1,181,696,194 7,488,669,239 55,014,582,024 2,071,743,595 1,494,675,520
WEB?15 681,087,020 681,087,020 5,813,555,341 1,200,980,738 1,072,910,682
Table 7: Counts of sentences, tokens and (unique) bracketings for web-based data sets; trimmed versions,
restricted to only those sentences having at least one multi-token bracketing, are indicated by a prime (?).
way. Its output is a coarse, high-level summary of
our runs, showing which factors significantly con-
tribute to changes in error rate on WSJ45:
Parameter (Indicator) Setting ?? p-value
INIT 1 ad-hoc @WSJ8,15 11.8 ***
GENRE 1 BLOGt -3.7 0.06
2 NEWS -5.3 **
3 WEB -7.7 ***
SCOPE 1 @15 -0.5 0.40
2 @15?45 -0.4 0.53
CONSTR 2 sprawl 0.9 0.23
3 loose 1.0 0.15
4 strict 1.8 *
TRIM 1 drop unmarked -7.4 ***
ADAPT 1 WSJ re-training 1.5 **
Intercept (R2Adjusted = 73.6%) 39.9 ***
We use a standard convention: *** for p < 0.001;
** for p < 0.01 (very signif.); and * for p < 0.05 (signif.).
The default training mode (all parameters zero) is
estimated to score 39.9%. A good initializer gives
the biggest (double-digit) gain; both domain adap-
tation and constraints also make a positive impact.
Throwing away unannotated data hurts, as does
training out-of-domain (the blog is least bad; the
web is worst). Of course, this overview should not
be taken too seriously. Overly simplistic, a first
order model ignores interactions between parame-
ters. Furthermore, a least squares fit aims to cap-
ture central tendencies, whereas we are more in-
terested in outliers ? the best-performing runs.
A major imperfection of the simple regression
model is that helpful factors that require an in-
teraction to ?kick in? may not, on their own, ap-
pear statistically significant. Our third diagnostic
is to examine parameter settings that give rise to
the best-performing models, looking out for com-
binations that consistently deliver superior results.
7.1 WSJ Baselines
Just two parameters apply to learning from WSJ.
Five of their six combinations are state-of-the-art,
demonstrating the power of Viterbi training; only
the default run scores worse than 45.0%, attained
by Leapfrog (Spitkovsky et al, 2010a), on WSJ45:
Settings SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 41.3 45.0 45.2
1 46.6 47.5 47.6
@45 @15 @15?45
7.2 Blog
Simply training on BLOGt instead of WSJ hurts:
GENRE=1 SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 39.6 36.9 36.9
1 46.5 46.3 46.4
@45 @15 @15?45
The best runs use a good initializer, discard unan-
notated sentences, enforce the loose constraint on
the rest, follow up with domain adaptation and
benefit from re-training ? GENRE=TRIM=ADAPT=1:
INIT=1 SCOPE=0 SCOPE=1 SCOPE=2
CONSTR=0 45.8 48.3 49.6
(sprawl) 2 46.3 49.2 49.2
(loose) 3 41.3 50.2 50.4
(strict) 4 40.7 49.9 48.7
@45 @15 @15?45
The contrast between unconstrained learning and
annotation-guided parsing is higher for the default
initializer, still using trimmed data sets (just over a
thousand sentences for BLOG?t15 ? see Table 7):
INIT=0 SCOPE=0 SCOPE=1 SCOPE=2
CONSTR=0 25.6 19.4 19.3
(sprawl) 2 25.2 22.7 22.5
(loose) 3 32.4 26.3 27.3
(strict) 4 36.2 38.7 40.1
@45 @15 @15?45
Above, we see a clearer benefit to our constraints.
1284
7.3 News
Training on WSJ is also better than using NEWS:
GENRE=2 SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 40.2 38.8 38.7
1 43.4 44.0 43.8
@45 @15 @15?45
As with the blog, the best runs use the good initial-
izer, discard unannotated sentences, enforce the
loose constraint and follow up with domain adap-
tation ? GENRE=2; INIT=TRIM=ADAPT=1:
Settings SCOPE=0 SCOPE=1 SCOPE=2
CONSTR=0 46.6 45.4 45.2
(sprawl) 2 46.1 44.9 44.9
(loose) 3 49.5 48.1 48.3
(strict) 4 37.7 36.8 37.6
@45 @15 @15?45
With all the extra training data, the best new score
is just 49.5%. On the one hand, we are disap-
pointed by the lack of dividends to orders of mag-
nitude more data. On the other, we are comforted
that the system arrives within 1% of its best result
? 50.4%, obtained with a manually cleaned up
corpus ? now using an auto-generated data set.
7.4 Web
The WEB-side story is more discouraging:
GENRE=3 SCOPE=0 SCOPE=1 SCOPE=2
INIT=0 38.3 35.1 35.2
1 42.8 43.6 43.4
@45 @15 @15?45
Our best run again uses a good initializer, keeps
all sentences, still enforces the loose constraint
and follows up with domain adaptation, but per-
forms worse than all well-initialized WSJ base-
lines, scoring only 45.9% (trained at WEB15).
We suspect that the web is just too messy for
us. On top of the challenges of language iden-
tification and sentence-breaking, there is a lot of
boiler-plate; furthermore, web text can be difficult
for news-trained POS taggers. For example, note
that the verb ?sign? is twice mistagged as a noun
and that ?YouTube? is classified as a verb, in the
top four POS sequences of web sentences:17
POS Sequence WEB Count
Sample web sentence, chosen uniformly at random.
1 DT NNS VBN 82,858,487
All rights reserved.
2 NNP NNP NNP 65,889,181
Yuasa et al
3 NN IN TO VB RB 31,007,783
Sign in to YouTube now!
4 NN IN IN PRP$ JJ NN 31,007,471
Sign in with your Google Account!
17Further evidence: TnT tags the ubiquitous but ambigu-
ous fragments ?click here? and ?print post? as noun phrases.
7.5 The State of the Art
Our best model gains more than 5% over previ-
ous state-of-the-art accuracy across all sentences
of WSJ?s Section 23, more than 8% on WSJ20 and
rivals the oracle skyline (Spitkovsky et al, 2010a)
on WSJ10; these gains generalize to Brown100,
where it improves by nearly 10% (see Table 8).
We take solace in the fact that our best mod-
els agree in using loose constraints. Of these,
the models trained with less data perform better,
with the best two using trimmed data sets, echo-
ing that ?less is more? (Spitkovsky et al, 2010a),
pace Halevy et al (2009). We note that orders of
magnitude more data did not improve parsing per-
formance further and suspect a different outcome
from lexicalized models: The primary benefit of
additional lower-quality data is in improved cover-
age. But with only 35 unique POS tags, data spar-
sity is hardly an issue. Extra examples of lexical
items help little and hurt when they are mistagged.
8 Related Work
The wealth of new annotations produced in many
languages every day already fuels a number of
NLP applications. Following their early and
wide-spread use by search engines, in service of
spam-fighting and retrieval, anchor text and link
data enhanced a variety of traditional NLP tech-
niques: cross-lingual information retrieval (Nie
and Chen, 2002), translation (Lu et al, 2004), both
named-entity recognition (Mihalcea and Csomai,
2007) and categorization (Watanabe et al, 2007),
query segmentation (Tan and Peng, 2008), plus
semantic relatedness and word-sense disambigua-
tion (Gabrilovich and Markovitch, 2007; Yeh et
al., 2009). Yet several, seemingly natural, can-
didate core NLP tasks ? tokenization, CJK seg-
mentation, noun-phrase chunking, and (until now)
parsing ? remained conspicuously uninvolved.
Approaches related to ours arise in applications
that combine parsing with named-entity recogni-
tion (NER). For example, constraining a parser to
respect the boundaries of known entities is stan-
dard practice not only in joint modeling of (con-
stituent) parsing and NER (Finkel and Manning,
2009), but also in higher-level NLP tasks, such as
relation extraction (Mintz et al, 2009), that couple
chunking with (dependency) parsing. Although
restricted to proper noun phrases, dates, times and
quantities, we suspect that constituents identified
by trained (supervised) NER systems would also
1285
Model Incarnation WSJ10 WSJ20 WSJ?
DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100
Leapfrog (Spitkovsky et al, 2010a) 57.1 48.7 45.0 43.6
default INIT=0,GENRE=0,SCOPE=0,CONSTR=0,TRIM=0,ADAPT=0 55.9 45.8 41.6 40.5
WSJ-best INIT=1,GENRE=0,SCOPE=2,CONSTR=0,TRIM=0,ADAPT=0 65.3 53.8 47.9 50.8
BLOGt-best INIT=1,GENRE=1,SCOPE=2,CONSTR=3,TRIM=1,ADAPT=1 69.3 56.8 50.4 53.3
NEWS-best INIT=1,GENRE=2,SCOPE=0,CONSTR=3,TRIM=1,ADAPT=1 67.3 56.2 50.1 51.6
WEB-best INIT=1,GENRE=3,SCOPE=1,CONSTR=3,TRIM=0,ADAPT=1 64.1 52.7 46.3 46.9
EVG Smoothed (skip-head), Lexicalized (Headden et al, 2009) 68.8
Table 8: Accuracies on Section 23 of WSJ{10, 20,? } and Brown100 for three recent state-of-the-art
systems, our default run, and our best runs (judged by accuracy on WSJ45) for each of four training sets.
be helpful in constraining grammar induction.
Following Pereira and Schabes? (1992) success
with partial annotations in training a model of
(English) constituents generatively, their idea has
been extended to discriminative estimation (Rie-
zler et al, 2002) and also proved useful in mod-
eling (Japanese) dependencies (Sassano, 2005).
There was demand for partially bracketed corpora.
Chen and Lee (1995) constructed one such corpus
by learning to partition (English) POS sequences
into chunks (Abney, 1991); Inui and Kotani (2001)
used n-gram statistics to split (Japanese) clauses.
We combine the two intuitions, using the web
to build a partially parsed corpus. Our approach
could be called lightly-supervised, since it does
not require manual annotation of a single complete
parse tree. In contrast, traditional semi-supervised
methods rely on fully-annotated seed corpora.18
9 Conclusion
We explored novel ways of training dependency
parsing models, the best of which attains 50.4%
accuracy on Section 23 (all sentences) of WSJ,
beating all previous unsupervised state-of-the-art
by more than 5%. Extra gains stem from guid-
ing Viterbi training with web mark-up, the loose
constraint consistently delivering best results. Our
linguistic analysis of a blog reveals that web an-
notations can be converted into accurate parsing
constraints (loose: 88%; sprawl: 95%; tear: 99%)
that could be helpful to supervised methods, e.g.,
by boosting an initial parser via self-training (Mc-
Closky et al, 2006) on sentences with mark-up.
Similar techniques may apply to standard word-
processing annotations, such as font changes, and
to certain (balanced) punctuation (Briscoe, 1994).
We make our blog data set, overlaying mark-up
and syntax, publicly available. Its annotations are
18A significant effort expended in building a tree-bank
comes with the first batch of sentences (Druck et al, 2009).
75% noun phrases, 13% verb phrases, 7% simple
declarative clauses and 2% prepositional phrases,
with traces of other phrases, clauses and frag-
ments. The type of mark-up, combined with POS
tags, could make for valuable features in discrimi-
native models of parsing (Ratnaparkhi, 1999).
A logical next step would be to explore the con-
nection between syntax and mark-up for genres
other than a news-style blog and for languages
other than English. We are excited by the possi-
bilities, as unsupervised parsers are on the cusp
of becoming useful in their own right ? re-
cently, Davidov et al (2009) successfully applied
Seginer?s (2007) fully unsupervised grammar in-
ducer to the problems of pattern-acquisition and
extraction of semantic data. If the strength of the
connection between web mark-up and syntactic
structure is universal across languages and genres,
this fact could have broad implications for NLP,
with applications extending well beyond parsing.
Acknowledgments
Partially funded by NSF award IIS-0811974 and by the Air
Force Research Laboratory (AFRL), under prime contract
no. FA8750-09-C-0181; first author supported by the Fannie
& John Hertz Foundation Fellowship. We thank Angel X.
Chang, Spence Green, Christopher D. Manning, Richard
Socher, Mihai Surdeanu and the anonymous reviewers for
many helpful suggestions, and we are especially grateful to
Andy Golding, for pointing us to his sample Map-Reduce
over the Google News crawl, and to Daniel Pipes, for allow-
ing us to distribute the data set derived from his blog entries.
References
S. Abney. 1991. Parsing by chunks. Principle-Based Pars-
ing: Computation and Psycholinguistics.
J. K. Baker. 1979. Trainable grammars for speech recogni-
tion. In Speech Communication Papers for the 97th Meet-
ing of the Acoustical Society of America.
C. Barr, R. Jones, and M. Regelson. 2008. The linguistic
structure of English web-search queries. In EMNLP.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP.
1286
T. Briscoe. 1994. Parsing (with) punctuation, etc. Technical
report, Xerox European Research Laboratory.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In ACL.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL.
H.-H. Chen and Y.-S. Lee. 1995. Development of a partially
bracketed corpus with part-of-speech information only. In
WVLC.
S. B. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsupervised
grammar induction. In NAACL-HLT.
M. Collins. 1999. Head-Driven Statistical Models for Nat-
ural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
D. Davidov, R. Reichart, and A. Rappoport. 2009. Supe-
rior and efficient fully unsupervised pattern-based concept
acquisition using an unsupervised parser. In CoNLL.
G. Druck, G. Mann, and A. McCallum. 2009. Semi-
supervised learning of dependency parsers using general-
ized expectation criteria. In ACL-IJCNLP.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head-automaton grammars. In
ACL.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL-HLT.
W. N. Francis and H. Kucera, 1979. Manual of Information
to Accompany a Standard Corpus of Present-Day Edited
American English, for use with Digital Computers. De-
partment of Linguistic, Brown University.
E. Gabrilovich and S. Markovitch. 2007. Computing seman-
tic relatedness using Wikipedia-based Explicit Semantic
Analysis. In IJCAI.
D. Gildea. 2001. Corpus variation and parser performance.
In EMNLP.
A. Halevy, P. Norvig, and F. Pereira. 2009. The unreasonable
effectiveness of data. IEEE Intelligent Systems, 24.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with richer
contexts and smoothing. In NAACL-HLT.
S. Holm. 1979. A simple sequentially rejective multiple test
procedure. Scandinavian Journal of Statistics, 6.
N. Inui and Y. Kotani. 2001. Robust N -gram based syntactic
analysis using segmentation words. In PACLIC.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In ACL.
C.-H. Lee, C.-H. Lin, and B.-H. Juang. 1991. A study on
speaker adaptation of the parameters of continuous den-
sity Hidden Markov Models. IEEE Trans. on Signal Pro-
cessing, 39.
W.-H. Lu, L.-F. Chien, and H.-J. Lee. 2004. Anchor text
mining for translation of Web queries: A transitive trans-
lation approach. ACM Trans. on Information Systems, 22.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In NAACL-HLT.
R. Mihalcea and A. Csomai. 2007. Wikify!: Linking docu-
ments to encyclopedic knowledge. In CIKM.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Distant
supervision for relation extraction without labeled data. In
ACL-IJCNLP.
J.-Y. Nie and J. Chen. 2002. Exploiting the Web as paral-
lel corpora for cross-language information retrieval. Web
Intelligence.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
A. Ratnaparkhi. 1999. Learning to parse natural language
with maximum entropy models. Machine Learning, 34.
S. Ravi, K. Knight, and R. Soricut. 2008. Automatic predic-
tion of parser accuracy. In EMNLP.
J. C. Reynar and A. Ratnaparkhi. 1997. A maximum entropy
approach to identifying sentence boundaries. In ANLP.
S. Riezler, T. H. King, R. M. Kaplan, R. Crouch, J. T.
Maxwell, III, and M. Johnson. 2002. Parsing the Wall
Street Journal using a lexical-functional grammar and dis-
criminative estimation techniques. In ACL.
M. Sassano. 2005. Using a partially annotated corpus to
build a dependency parser for Japanese. In IJCNLP.
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a. From
Baby Steps to Leapfrog: How ?Less is More? in unsuper-
vised dependency parsing. In NAACL-HLT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010b. Viterbi training improves unsupervised de-
pendency parsing. In CoNLL.
B. Tan and F. Peng. 2008. Unsupervised query segmenta-
tion using generative language models and Wikipedia. In
WWW.
K. Toutanova and C. D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-of-
speech tagger. In EMNLP-VLC.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic depen-
dency network. In HLT-NAACL.
D. Vadas and J. R. Curran. 2007. Adding noun phrase struc-
ture to the Penn Treebank. In ACL.
Y. Watanabe, M. Asahara, and Y. Matsumoto. 2007. A
graph-based approach to named entity categorization in
Wikipedia using conditional random fields. In EMNLP-
CoNLL.
E. Yeh, D. Ramage, C. D. Manning, E. Agirre, and A. Soroa.
2009. WikiWalk: Random walks on Wikipedia for se-
mantic relatedness. In TextGraphs.
1287
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 9?17,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Viterbi Training Improves Unsupervised Dependency Parsing
Valentin I. Spitkovsky
Computer Science Department
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc.
Mountain View, CA, 94043, USA
hiyan@google.com
Daniel Jurafsky and Christopher D. Manning
Departments of Linguistics and Computer Science
Stanford University, Stanford, CA, 94305, USA
jurafsky@stanford.edu and manning@cs.stanford.edu
Abstract
We show that Viterbi (or ?hard?) EM is
well-suited to unsupervised grammar in-
duction. It is more accurate than standard
inside-outside re-estimation (classic EM),
significantly faster, and simpler. Our ex-
periments with Klein and Manning?s De-
pendency Model with Valence (DMV) at-
tain state-of-the-art performance ? 44.8%
accuracy on Section 23 (all sentences) of
the Wall Street Journal corpus ? without
clever initialization; with a good initial-
izer, Viterbi training improves to 47.9%.
This generalizes to the Brown corpus,
our held-out set, where accuracy reaches
50.8% ? a 7.5% gain over previous best
results. We find that classic EM learns bet-
ter from short sentences but cannot cope
with longer ones, where Viterbi thrives.
However, we explain that both algorithms
optimize the wrong objectives and prove
that there are fundamental disconnects be-
tween the likelihoods of sentences, best
parses, and true parses, beyond the well-
established discrepancies between likeli-
hood, accuracy and extrinsic performance.
1 Introduction
Unsupervised learning is hard, often involving dif-
ficult objective functions. A typical approach is
to attempt maximizing the likelihood of unlabeled
data, in accordance with a probabilistic model.
Sadly, such functions are riddled with local op-
tima (Charniak, 1993, Ch. 7, inter alia), since their
number of peaks grows exponentially with in-
stances of hidden variables. Furthermore, a higher
likelihood does not always translate into superior
task-specific accuracy (Elworthy, 1994; Merialdo,
1994). Both complications are real, but we will
discuss perhaps more significant shortcomings.
We prove that learning can be error-prone even
in cases when likelihood is an appropriate mea-
sure of extrinsic performance and where global
optimization is feasible. This is because a key
challenge in unsupervised learning is that the de-
sired likelihood is unknown. Its absence renders
tasks like structure discovery inherently under-
constrained. Search-based algorithms adopt sur-
rogate metrics, gambling on convergence to the
?right? regularities in data. Their wrong objec-
tives create cases in which both efficiency and per-
formance improve when expensive exact learning
techniques are replaced by cheap approximations.
We propose using Viterbi training (Brown
et al, 1993), instead of inside-outside re-
estimation (Baker, 1979), to induce hierarchical
syntactic structure from natural language text. Our
experiments with Klein and Manning?s (2004) De-
pendency Model with Valence (DMV), a popular
state-of-the-art model (Headden et al, 2009; Co-
hen and Smith, 2009; Spitkovsky et al, 2009),
beat previous benchmark accuracies by 3.8% (on
Section 23 of WSJ) and 7.5% (on parsed Brown).
Since objective functions used in unsupervised
grammar induction are provably wrong, advan-
tages of exact inference may not apply. It makes
sense to try the Viterbi approximation ? it is also
wrong, only simpler and cheaper than classic EM.
As it turns out, Viterbi EM is not only faster but
also more accurate, consistent with hypotheses of
de Marcken (1995) and Spitkovsky et al (2009).
We begin by reviewing the model, standard data
sets and metrics, and our experimental results. Af-
ter relating our contributions to prior work, we
delve into proofs by construction, using the DMV.
9
Corpus Sentences POS Tokens Corpus Sentences POS Tokens
WSJ1 159 159 WSJ13 12,270 110,760
WSJ2 499 839 WSJ14 14,095 136,310
WSJ3 876 1,970 WSJ15 15,922 163,715
WSJ4 1,394 4,042 WSJ20 25,523 336,555
WSJ5 2,008 7,112 WSJ25 34,431 540,895
WSJ6 2,745 11,534 WSJ30 41,227 730,099
WSJ7 3,623 17,680 WSJ35 45,191 860,053
WSJ8 4,730 26,536 WSJ40 47,385 942,801
WSJ9 5,938 37,408 WSJ45 48,418 986,830
WSJ10 7,422 52,248 WSJ100 49,206 1,028,054
WSJ11 8,856 68,022 Section 23 2,353 48,201
WSJ12 10,500 87,750 Brown100 24,208 391,796 5 10 15 20 25 30 35 40 45
5
10
15
20
25
30
35
40
45
Thousands
of Sentences
Thousands
of Tokens 100
200
300
400
500
600
700
800
900
WSJk
Figure 1: Sizes of WSJ{1, . . . , 45, 100}, Section 23 of WSJ? and Brown100 (Spitkovsky et al, 2009).
NNS VBD IN NN ?
Payrolls fell in September .
P = (1?
0
z }| {
PSTOP(?, L, T)) ? PATTACH(?, L, VBD)
? (1? PSTOP(VBD, L, T)) ? PATTACH(VBD, L, NNS)
? (1? PSTOP(VBD, R, T)) ? PATTACH(VBD, R, IN)
? (1? PSTOP(IN, R, T)) ? PATTACH(IN, R, NN)
? PSTOP(VBD, L, F) ? PSTOP(VBD, R, F)
? PSTOP(NNS, L, T) ? PSTOP(NNS, R, T)
? PSTOP(IN, L, T) ? PSTOP(IN, R, F)
? PSTOP(NN, L, T) ? PSTOP(NN, R, T)
? PSTOP(?, L, F)
| {z }
1
? PSTOP(?, R, T)
| {z }
1
.
Figure 2: A dependency structure for a short sen-
tence and its probability, as factored by the DMV,
after summing out PORDER (Spitkovsky et al, 2009).
2 Dependency Model with Valence
The DMV (Klein and Manning, 2004) is a single-
state head automata model (Alshawi, 1996) over
lexical word classes {cw} ? POS tags. Its gener-
ative story for a sub-tree rooted at a head (of class
ch) rests on three types of independent decisions:
(i) initial direction dir ? {L, R} in which to attach
children, via probability PORDER(ch); (ii) whether to
seal dir, stopping with probability PSTOP(ch, dir, adj),
conditioned on adj ? {T, F} (true iff considering
dir?s first, i.e., adjacent, child); and (iii) attach-
ments (of class ca), according to PATTACH(ch, dir, ca).
This produces only projective trees. A root token
? generates the head of a sentence as its left (and
only) child. Figure 2 displays a simple example.
The DMV lends itself to unsupervised learn-
ing via inside-outside re-estimation (Baker, 1979).
Viterbi training (Brown et al, 1993) re-estimates
each next model as if supervised by the previous
best parse trees. And supervised learning from
reference parse trees is straight-forward, since
maximum-likelihood estimation reduces to count-
ing: P?ATTACH(ch, dir, ca) is the fraction of children ?
those of class ca ? attached on the dir side of a
head of class ch; P?STOP(ch, dir, adj = T), the frac-
tion of words of class ch with no children on the
dir side; and P?STOP(ch, dir, adj = F), the ratio1 of the
number of words of class ch having a child on the
dir side to their total number of such children.
3 Standard Data Sets and Evaluation
The DMV is traditionally trained and tested on
customized subsets of Penn English Treebank?s
Wall Street Journal portion (Marcus et al, 1993).
Following Klein and Manning (2004), we be-
gin with reference constituent parses and com-
pare against deterministically derived dependen-
cies: after pruning out all empty sub-trees, punc-
tuation and terminals (tagged # and $) not pro-
nounced where they appear, we drop all sentences
with more than a prescribed number of tokens
remaining and use automatic ?head-percolation?
rules (Collins, 1999) to convert the rest, as is stan-
dard practice. We experiment with WSJk (sen-
tences with at most k tokens), for 1 ? k ? 45, and
Section 23 of WSJ? (all sentence lengths). We
also evaluate on Brown100, similarly derived from
the parsed portion of the Brown corpus (Francis
and Kucera, 1979), as our held-out set. Figure 1
shows these corpora?s sentence and token counts.
Proposed parse trees are judged on accuracy: a
directed score is simply the overall fraction of cor-
rectly guessed dependencies. Let S be a set of
sentences, with |s| the number of terminals (to-
1The expected number of trials needed to get one
Bernoulli(p) success is n ? Geometric(p), with n ? Z+,
P(n) = (1 ? p)n?1p and E(n) = p?1; MoM and MLE
agree, p? = (# of successes)/(# of trials).
10
5 10 15 20 25 30 35 40
10
20
30
40
50
60
70
OracleAd-Hoc?
Uninformed
WSJk
D
irected
D
ep
end
en
cy
A
ccu
racy
o
n
W
SJ40
(a) %-Accuracy for Inside-Outside (Soft EM)
5 10 15 20 25 30 35 40
10
20
30
40
50
60
70
Oracle
Ad-Hoc? Uninformed
WSJk
(training on all WSJ sentences up to k tokens in length)
D
irected
D
ep
end
en
cy
A
ccu
racy
o
n
W
SJ40
(b) %-Accuracy for Viterbi (Hard EM)
5 10 15 20 25 30 35 40
50
100
150
200
350
400
Oracle
Ad-Hoc?
Uninformed
WSJk
Iteratio
n
s
to
C
o
nv
erg
en
ce
(c) Iterations for Inside-Outside (Soft EM)
5 10 15 20 25 30 35 40
50
100
150
200
Oracle
Ad-Hoc?
Uninformed
WSJk
Iteratio
n
s
to
C
o
nv
erg
en
ce
(d) Iterations for Viterbi (Hard EM)
Figure 3: Directed dependency accuracies attained by the DMV, when trained on WSJk, smoothed, then
tested against a fixed evaluation set, WSJ40, for three different initialization strategies (Spitkovsky et al,
2009). Red, green and blue graphs represent the supervised (maximum-likelihood oracle) initialization,
a linguistically-biased initializer (Ad-Hoc?) and the uninformed (uniform) prior. Panel (b) shows results
obtained with Viterbi training instead of classic EM ? Panel (a), but is otherwise identical (in both, each
of the 45 vertical slices captures five new experimental results and arrows connect starting performance
with final accuracy, emphasizing the impact of learning). Panels (c) and (d) show the corresponding
numbers of iterations until EM?s convergence.
kens) for each s ? S. Denote by T (s) the set
of all dependency parse trees of s, and let ti(s)
stand for the parent of token i, 1 ? i ? |s|, in
t(s) ? T (s). Call the gold reference t?(s) ? T (s).
For a given model of grammar, parameterized by
?, let t??(s) ? T (s) be a (not necessarily unique)
likeliest (also known as Viterbi) parse of s:
t??(s) ?
{
arg max
t?T (s)
P?(t)
}
;
then ??s directed accuracy on a reference set R is
100% ?
?
s?R
?|s|
i=1 1{t??i (s)=t?i (s)}?
s?R |s|
.
4 Experimental Setup and Results
Following Spitkovsky et al (2009), we trained the
DMV on data sets WSJ{1, . . . , 45} using three ini-
tialization strategies: (i) the uninformed uniform
prior; (ii) a linguistically-biased initializer, Ad-
Hoc?;2 and (iii) an oracle ? the supervised MLE
solution. Standard training is without smoothing,
iterating each run until successive changes in over-
all per-token cross-entropy drop below 2?20 bits.
We re-trained all models using Viterbi EM
instead of inside-outside re-estimation, explored
Laplace (add-one) smoothing during training, and
experimented with hybrid initialization strategies.
2Ad-Hoc? is Spitkovsky et al?s (2009) variation on Klein
and Manning?s (2004) ?ad-hoc harmonic? completion.
11
5 10 15 20 25 30 35 40
10
20
30
40
50
60
70
OracleAd-Hoc?
Uninformed
Baby Steps
WSJk
D
irected
D
ep
end
en
cy
A
ccu
racy
o
n
W
SJ40
(a) %-Accuracy for Inside-Outside (Soft EM)
5 10 15 20 25 30 35 40
10
20
30
40
50
60
70
Oracle
Ad-Hoc? Uninformed
Baby Steps
WSJk
D
irected
D
ep
end
en
cy
A
ccu
racy
o
n
W
SJ40
(b) %-Accuracy for Viterbi (Hard EM)
Figure 4: Superimposes directed accuracies attained by DMV models trained with Laplace smoothing
(brightly-colored curves) over Figure 3(a,b); violet curves represent Baby Steps (Spitkovsky et al, 2009).
4.1 Result #1: Viterbi-Trained Models
The results of Spitkovsky et al (2009), tested
against WSJ40, are re-printed in Figure 3(a); our
corresponding Viterbi runs appear in Figure 3(b).
We observe crucial differences between the two
training modes for each of the three initialization
strategies. Both algorithms walk away from the
supervised maximum-likelihood solution; how-
ever, Viterbi EM loses at most a few points of
accuracy (3.7% at WSJ40), whereas classic EM
drops nearly twenty points (19.1% at WSJ45). In
both cases, the single best unsupervised result is
with good initialization, although Viterbi peaks
earlier (45.9% at WSJ8) and in a narrower range
(WSJ8-9) than classic EM (44.3% at WSJ15;
WSJ13-20). The uniform prior never quite gets off
the ground with classic EM but manages quite well
under Viterbi training,3 given sufficient data ? it
even beats the ?clever? initializer everywhere past
WSJ10. The ?sweet spot? at WSJ15 ? a neigh-
borhood where both Ad-Hoc? and the oracle ex-
cel under classic EM ? disappears with Viterbi.
Furthermore, Viterbi does not degrade with more
(complex) data, except with a biased initializer.
More than a simple efficiency hack, Viterbi EM
actually improves performance. And its benefits to
running times are also non-trivial: it not only skips
computing the outside charts in every iteration but
also converges (sometimes an order of magnitude)
3In a concurrently published related work, Cohen and
Smith (2010) prove that the uniform-at-random initializer is a
competitive starting M-step for Viterbi EM; our uninformed
prior consists of uniform multinomials, seeding the E-step.
faster than classic EM (see Figure 3(c,d)).4
4.2 Result #2: Smoothed Models
Smoothing rarely helps classic EM and hurts in
the case of oracle training (see Figure 4(a)). With
Viterbi, supervised initialization suffers much less,
the biased initializer is a wash, and the uninformed
uniform prior generally gains a few points of ac-
curacy, e.g., up 2.9% (from 42.4% to 45.2%, eval-
uated against WSJ40) at WSJ15 (see Figure 4(b)).
Baby Steps (Spitkovsky et al, 2009) ? iterative
re-training with increasingly more complex data
sets, WSJ1, . . . ,WSJ45 ? using smoothed Viterbi
training fails miserably (see Figure 4(b)), due to
Viterbi?s poor initial performance at short sen-
tences (possibly because of data sparsity and sen-
sitivity to non-sentences ? see examples in ?7.3).
4.3 Result #3: State-of-the-Art Models
Simply training up smoothed Viterbi at WSJ15,
using the uninformed uniform prior, yields 44.8%
accuracy on Section 23 of WSJ?, already beating
previous state-of-the-art by 0.7% (see Table 1(A)).
Since both classic EM and Ad-Hoc? initializers
work well with short sentences (see Figure 3(a)),
it makes sense to use their pre-trained models to
initialize Viterbi training, mixing the two strate-
gies. We judged all Ad-Hoc? initializers against
WSJ15 and found that the one for WSJ8 mini-
mizes sentence-level cross-entropy (see Figure 5).
This approach does not involve reference parse
4For classic EM, the number of iterations to convergence
appears sometimes inversely related to performance, giving
credence to the notion of early termination as a regularizer.
12
Model Incarnation WSJ10 WSJ20 WSJ?
DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100
Less is More (Ad-Hoc? @15) (Spitkovsky et al, 2009) 56.2 48.2 44.1 43.3
A. Smoothed Viterbi Training (@15), Initialized with the Uniform Prior 59.9 50.0 44.8 48.1
B. A Good Initializer (Ad-Hoc??s @8), Classically Pre-Trained (@15) 63.8 52.3 46.2 49.3
C. Smoothed Viterbi Training (@15), Initialized with B 64.4 53.5 47.8 50.5
D. Smoothed Viterbi Training (@45), Initialized with C 65.3 53.8 47.9 50.8
EVG Smoothed (skip-head), Lexicalized (Headden et al, 2009) 68.8
Table 1: Accuracies on Section 23 of WSJ{10, 20,? } and Brown100 for three recent state-of-the-art
systems, our initializer, and smoothed Viterbi-trained runs that employ different initialization strategies.
5 10 15 20 25 30 35 40 45
4.5
5.0
5.5
WSJk
bpt
lowest cross-entropy (4.32bpt) attained at WSJ8
x-Entropy h (in bits per token) on WSJ15
Figure 5: Sentence-level cross-entropy on WSJ15
for Ad-Hoc? initializers of WSJ{1, . . . , 45}.
trees and is therefore still unsupervised. Using the
Ad-Hoc? initializer based on WSJ8 to seed classic
training at WSJ15 yields a further 1.4% gain in ac-
curacy, scoring 46.2% on WSJ? (see Table 1(B)).
This good initializer boosts accuracy attained
by smoothed Viterbi at WSJ15 to 47.8% (see Ta-
ble 1(C)). Using its solution to re-initialize train-
ing at WSJ45 gives a tiny further improvement
(0.1%) on Section 23 of WSJ? but bigger gains
on WSJ10 (0.9%) and WSJ20 (see Table 1(D)).
Our results generalize. Gains due to smoothed
Viterbi training and favorable initialization carry
over to Brown100 ? accuracy improves by 7.5%
over previous published numbers (see Table 1).5
5 Discussion of Experimental Results
The DMV has no parameters to capture syntactic
relationships beyond local trees, e.g., agreement.
Spitkovsky et al (2009) suggest that classic EM
breaks down as sentences get longer precisely be-
cause the model makes unwarranted independence
assumptions. They hypothesize that the DMV re-
serves too much probability mass for what should
be unlikely productions. Since EM faithfully al-
locates such re-distributions across the possible
parse trees, once sentences grow sufficiently long,
this process begins to deplete what began as like-
lier structures. But medium lengths avoid a flood
of exponentially-confusing longer sentences (and
5In a sister paper, Spitkovsky et al (2010) improve perfor-
mance by incorporating parsing constraints harvested from
the web into Viterbi training; nevertheless, results presented
in this paper remain the best of models trained purely on WSJ.
the sparseness of unrepresentative shorter ones).6
Our experiments corroborate this hypothesis.
First of all, Viterbi manages to hang on to su-
pervised solutions much better than classic EM.
Second, Viterbi does not universally degrade with
more (complex) training sets, except with a biased
initializer. And third, Viterbi learns poorly from
small data sets of short sentences (WSJk, k < 5).
Viterbi may be better suited to unsupervised
grammar induction compared with classic EM, but
neither is sufficient, by itself. Both algorithms
abandon good solutions and make no guarantees
with respect to extrinsic performance. Unfortu-
nately, these two approaches share a deep flaw.
6 Related Work on Improper Objectives
It is well-known that maximizing likelihood may,
in fact, degrade accuracy (Pereira and Schabes,
1992; Elworthy, 1994; Merialdo, 1994). de Mar-
cken (1995) showed that classic EM suffers from
a fatal attraction towards deterministic grammars
and suggested a Viterbi training scheme as a rem-
edy. Liang and Klein?s (2008) analysis of errors
in unsupervised learning began with the inappro-
priateness of the likelihood objective (approxima-
tion), explored problems of data sparsity (estima-
tion) and focused on EM-specific issues related to
non-convexity (identifiability and optimization).
Previous literature primarily relied on experi-
mental evidence. de Marcken?s analytical result is
an exception but pertains only to EM-specific lo-
cal attractors. Our analysis confirms his intuitions
and moreover shows that there can be global pref-
erences for deterministic grammars ? problems
that would persist with tractable optimization. We
prove that there is a fundamental disconnect be-
tween objective functions even when likelihood is
a reasonable metric and training data are infinite.
6Klein and Manning (2004) originally trained the DMV
on WSJ10 and Gillenwater et al (2009) found it useful to dis-
card data from WSJ3, which is mostly incomplete sentences.
13
7 Proofs (by Construction)
There is a subtle distinction between three differ-
ent probability distributions that arise in parsing,
each of which can be legitimately termed ?likeli-
hood? ? the mass that a particular model assigns
to (i) highest-scoring (Viterbi) parse trees; (ii) the
correct (gold) reference trees; and (iii) the sen-
tence strings (sums over all derivations). A classic
unsupervised parser trains to optimize the third,
makes actual parsing decisions according to the
first, and is evaluated against the second. There
are several potential disconnects here. First of all,
the true generative model ?? may not yield the
largest margin separations for discriminating be-
tween gold parse trees and next best alternatives;
and second, ?? may assign sub-optimal mass to
string probabilities. There is no reason why an op-
timal estimate ?? should make the best parser or
coincide with a peak of an unsupervised objective.
7.1 The Three Likelihood Objectives
A supervised parser finds the ?best? parameters
?? by maximizing the likelihood of all reference
structures t?(s) ? the product, over all sentences,
of the probabilities that it assigns to each such tree:
??SUP = arg max
?
L(?) = arg max
?
?
s
P?(t?(s)).
For the DMV, this objective function is convex ?
its unique peak is easy to find and should match
the true distribution ?? given enough data, barring
practical problems caused by numerical instability
and inappropriate independence assumptions. It is
often easier to work in log-probability space:
??SUP = arg max? logL(?)
= arg max?
?
s log P?(t?(s)).
Cross-entropy, measured in bits per token (bpt),
offers an interpretable proxy for a model?s quality:
h(?) = ?
?
s lg P?(t?(s))?
s |s|
.
Clearly, arg max? L(?) = ??SUP = arg min? h(?).
Unsupervised parsers cannot rely on references
and attempt to jointly maximize the probability of
each sentence instead, summing over the probabil-
ities of all possible trees, according to a model ?:
??UNS = arg max
?
?
s
log
?
t?T (s)
P?(t)
? ?? ?
P?(s)
.
This objective function is not convex and in gen-
eral does not have a unique peak, so in practice one
usually settles for ??UNS ? a fixed point. There is no
reason why ??SUP should agree with ??UNS, which is
in turn (often badly) approximated by ??UNS, in our
case using EM. A logical alternative to maximiz-
ing the probability of sentences is to maximize the
probability of the most likely parse trees instead:7
??VIT = arg max
?
?
s
log P?(t??(s)).
This 1-best approximation similarly arrives at ??VIT,
with no claims of optimality. Each next model is
re-estimated as if supervised by reference parses.
7.2 A Warm-Up Case: Accuracy vs. ??SUP 6= ??
A simple way to derail accuracy is to maximize
the likelihood of an incorrect model, e.g., one that
makes false independence assumptions. Consider
fitting the DMV to a contrived distribution ? two
equiprobable structures over identical three-token
sentences from a unary vocabulary { a?}:
(i) x xa? a? a?; (ii) y ya? a? a?.
There are six tokens and only two have children
on any given side, so adjacent stopping MLEs are:
P?STOP( a?, L, T) = P?STOP( a?, R, T) = 1 ?
2
6 =
2
3 .
The rest of the estimated model is deterministic:
P?ATTACH(?, L, a?) = P?ATTACH( a?, ?, a?) = 1
and P?STOP( a?, ?, F) = 1,
since all dependents are a? and every one is an
only child. But the DMV generates left- and right-
attachments independently, allowing a third parse:
(iii) x ya? a? a?.
It also cannot capture the fact that all structures are
local (or that all dependency arcs point in the same
direction), admitting two additional parse trees:
(iv) a? xa? a?; (v) ya? a? a?.
Each possible structure must make four (out of six)
adjacent stops, incurring identical probabilities:
P?STOP( a?, ?, T)4 ? (1 ? P?STOP( a?, ?, T))2 =
24
36 .
7It is also possible to use k-best Viterbi, with k > 1.
14
Thus, the MLE model does not break symmetry
and rates each of the five parse trees as equally
likely. Therefore, its expected per-token accuracy
is 40%. Average overlaps between structures (i-v)
and answers (i,ii) are (i) 100% or 0; (ii) 0 or 100%;
and (iii,iv,v) 33.3%: (3+3)/(5?3) = 2/5 = 0.4.
A decoy model without left- or right-branching,
i.e., P?STOP( a?, L, T) = 1 or P?STOP( a?, R, T) = 1,
would assign zero probability to some of the train-
ing data. It would be forced to parse every instance
of a? a? a? either as (i) or as (ii), deterministically.
Nevertheless, it would attain a higher per-token ac-
curacy of 50%. (Judged on exact matches, at the
granularity of whole trees, the decoy?s guaranteed
50% accuracy clobbers the MLE?s expected 20%.)
Our toy data set could be replicated n-fold with-
out changing the analysis. This confirms that, even
in the absence of estimation errors or data sparsity,
there can be a fundamental disconnect between
likelihood and accuracy, if the model is wrong.8
7.3 A Subtler Case: ?? = ??SUP vs. ??UNS vs. ??VIT
We now prove that, even with the right model,
mismatches between the different objective like-
lihoods can also handicap the truth. Our calcula-
tions are again exact, so there are no issues with
numerical stability. We work with a set of param-
eters ?? already factored by the DMV, so that its
problems could not be blamed on invalid indepen-
dence assumptions. Yet we are able to find another
impostor distribution ?? that outshines ??SUP = ?? on
both unsupervised metrics, which proves that the
true models ??SUP and ?? are not globally optimal,
as judged by the two surrogate objective functions.
This next example is organic. We began with
WSJ10 and confirmed that classic EM abandons
the supervised solution. We then iteratively dis-
carded large portions of the data set, so long as
the remainder maintained the (un)desired effect ?
EM walking away from its ??SUP. This procedure
isolated such behavior, arriving at a minimal set:
NP : NNP NNP ?
? Marvin Alisky.
S : NNP VBD ?
(Braniff declined).
NP-LOC : NNP NNP ?
Victoria, Texas
8And as George Box quipped, ?Essentially, all models are
wrong, but some are useful? (Box and Draper, 1987, p. 424).
This kernel is tiny, but, as before, our analysis is
invariant to n-fold replication: the problem cannot
be explained away by a small training size ? it
persists even in infinitely large data sets. And so,
we consider three reference parse trees for two-
token sentences over a binary vocabulary { a?, z?}:
(i) xa? a?; (ii) xa? z?; (iii) ya? a?.
One third of the time, z? is the head; only a? can
be a child; and only a? has right-dependents. Trees
(i)-(iii) are the only two-terminal parses generated
by the model and are equiprobable. Thus, these
sentences are representative of a length-two re-
striction of everything generated by the true ??:
PATTACH(?, L, a?) =
2
3 and PSTOP( a?, ?, T) =
4
5 ,
since a? is the head two out of three times, and
since only one out of five a??s attaches a child on
either side. Elsewhere, the model is deterministic:
PSTOP( z?, L, T) = 0;
PSTOP(?, ?, F) = PSTOP( z?, R, T) = 1;
PATTACH( a?, ?, a?) = PATTACH( z?, L, a?) = 1.
Contrast the optimal estimate ??SUP = ?? with the
decoy fixed point9 ?? that is identical to ??, except
P?STOP( a?, L, T) =
3
5 and P?STOP( a?, R, T) = 1.
The probability of stopping is now 3/5 on the left
and 1 on the right, instead of 4/5 on both sides ?
?? disallows a??s right-dependents but preserves its
overall fertility. The probabilities of leaves a? (no
children), under the models ??SUP and ??, are:
P?( a?) = P?STOP( a?, L, T)?P?STOP( a?, R, T) =
(4
5
)2
and P?( a?) = P?STOP( a?, L, T)?P?STOP( a?, R, T) =
3
5 .
And the probabilities of, e.g., structure
x
a? z?, are:
P?ATTACH(?, L, z?) ? P?STOP( z?, R, T)
? (1 ? P?STOP( z?, L, T)) ? P?STOP( z?, L, F)
? P?ATTACH( z?, L, a?) ? P?( a?)
9The model estimated from the parse trees induced by ??
over the three sentences is again ??, for both soft and hard EM.
15
= P?ATTACH(?, L, z?) ? P?( a?) =
1
3 ?
16
25
and P?ATTACH(?, L, z?) ? P?( a?) =
1
3 ?
3
5 .
Similarly, the probabilities of all four possible
parse trees for the two distinct sentences, a? a? and
a? z?, under the two models, ??SUP = ?? and ??, are:
??SUP = ?? ??
x
a? z? 13
` 16
25
?
= 13
` 3
5
?
=
16
75 = 0.213 15 = 0.2y
a? z? 0 0
x
a? a? 23
` 4
5
? `
1? 45
? ` 16
25
?
= 23
`
1 ? 35
? ` 3
5
?
=
128
1875 = 0.06826 425 = 0.16y
a? a? 0.06826 0
To the three true parses, ??SUP assigns probability
(16
75
) ( 128
1875
)2 ? 0.0009942 ? about 1.66bpt; ??
leaves zero mass for (iii), corresponding to a larger
(infinite) cross-entropy, consistent with theory.
So far so good, but if asked for best (Viterbi)
parses, ??SUP could still produce the actual trees,
whereas ?? would happily parse sentences of (iii)
and (i) the same, perceiving a joint probability of
(0.2)(0.16)2 = 0.00512 ? just 1.27bpt, appear-
ing to outperform ??SUP = ??! Asked for sentence
probabilities, ?? would remain unchanged (it parses
each sentence unambiguously), but ??SUP would ag-
gregate to
(16
75
) (
2 ? 1281875
)2 ? 0.003977, improv-
ing to 1.33bpt, but still noticeably ?worse? than ??.
Despite leaving zero probability to the truth, ??
beats ?? on both surrogate metrics, globally. This
seems like an egregious error. Judged by (extrin-
sic) accuracy, ?? still holds its own: it gets four
directed edges from predicting parse trees (i) and
(ii) completely right, but none of (iii) ? a solid
66.7%. Subject to tie-breaking, ?? is equally likely
to get (i) and/or (iii) entirely right or totally wrong
(they are indistinguishable): it could earn a perfect
100%, tie ??, or score a low 33.3%, at 1:2:1 odds,
respectively ? same as ???s deterministic 66.7%
accuracy, in expectation, but with higher variance.
8 Discussion of Theoretical Results
Daume? et al (2009) questioned the benefits of us-
ing exact models in approximate inference. In our
case, the model already makes strong simplifying
assumptions and the objective is also incorrect. It
makes sense that Viterbi EM sometimes works,
since an approximate wrong ?solution? could, by
chance, be better than one that is exactly wrong.
One reason why Viterbi EM may work well is
that its score is used in selecting actual output
parse trees. Wainwright (2006) provided strong
theoretical and empirical arguments for using the
same approximate inference method in training
as in performing predictions for a learned model.
He showed that if inference involves an approxi-
mation, then using the same approximate method
to train the model gives even better performance
guarantees than exact training methods. If our task
were not parsing but language modeling, where
the relevant score is the sum of the probabilities
over individual derivations, perhaps classic EM
would not be doing as badly, compared to Viterbi.
Viterbi training is not only faster and more accu-
rate but also free of inside-outside?s recursion con-
straints. It therefore invites more flexible model-
ing techniques, including discriminative, feature-
rich approaches that target conditional likelihoods,
essentially via (unsupervised) self-training (Clark
et al, 2003; Ng and Cardie, 2003; McClosky et
al., 2006a; McClosky et al, 2006b, inter alia).
Such ?learning by doing? approaches may be
relevant to understanding human language ac-
quisition, as children frequently find themselves
forced to interpret a sentence in order to inter-
act with the world. Since most models of human
probabilistic parsing are massively pruned (Juraf-
sky, 1996; Chater et al, 1998; Lewis and Vasishth,
2005, inter alia), the serial nature of Viterbi EM
? or the very limited parallelism of k-best Viterbi
? may be more appropriate in modeling this task
than the fully-integrated inside-outside solution.
9 Conclusion
Without a known objective, as in unsupervised
learning, correct exact optimization becomes im-
possible. In such cases, approximations, although
liable to pass over a true optimum, may achieve
faster convergence and still improve performance.
We showed that this is the case with Viterbi
training, a cheap alternative to inside-outside re-
estimation, for unsupervised dependency parsing.
We explained why Viterbi EM may be partic-
ularly well-suited to learning from longer sen-
tences, in addition to any general benefits to syn-
chronizing approximation methods across learn-
ing and inference. Our best algorithm is sim-
pler and an order of magnitude faster than clas-
sic EM. It achieves state-of-the-art performance:
3.8% higher accuracy than previous published best
16
results on Section 23 (all sentences) of the Wall
Street Journal corpus. This improvement general-
izes to the Brown corpus, our held-out evaluation
set, where the same model registers a 7.5% gain.
Unfortunately, approximations alone do not
bridge the real gap between objective functions.
This deeper issue could be addressed by drawing
parsing constraints (Pereira and Schabes, 1992)
from specific applications. One example of such
an approach, tied to machine translation, is syn-
chronous grammars (Alshawi and Douglas, 2000).
An alternative ? observing constraints induced by
hyper-text mark-up, harvested from the web ? is
explored in a sister paper (Spitkovsky et al, 2010),
published concurrently.
Acknowledgments
Partially funded by NSF award IIS-0811974 and by the Air
Force Research Laboratory (AFRL), under prime contract
no. FA8750-09-C-0181; first author supported by the Fan-
nie & John Hertz Foundation Fellowship. We thank An-
gel X. Chang, Mengqiu Wang and the anonymous reviewers
for many helpful comments on draft versions of this paper.
References
H. Alshawi and S. Douglas. 2000. Learning dependency
transduction models from unannotated examples. In
Royal Society of London Philosophical Transactions Se-
ries A, volume 358.
H. Alshawi. 1996. Head automata for speech translation. In
Proc. of ICSLP.
J. K. Baker. 1979. Trainable grammars for speech recogni-
tion. In Speech Communication Papers for the 97th Meet-
ing of the Acoustical Society of America.
G. E. P. Box and N. R. Draper. 1987. Empirical Model-
Building and Response Surfaces. John Wiley.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19.
E. Charniak. 1993. Statistical Language Learning. MIT
Press.
N. Chater, M. J. Crocker, and M. J. Pickering. 1998. The
rational analysis of inquiry: The case of parsing. In
M. Oaksford and N. Chater, editors, Rational Models of
Cognition. Oxford University Press.
S. Clark, J. Curran, and M. Osborne. 2003. Bootstrapping
POS-taggers using unlabelled data. In Proc. of CoNLL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsupervised
grammar induction. In Proc. of NAACL-HLT.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for
PCFGs: Hardness results and competitiveness of uniform
initialization. In Proc. of ACL.
M. Collins. 1999. Head-Driven Statistical Models for Nat-
ural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
H. Daume?, III, J. Langford, and D. Marcu. 2009. Search-
based structured prediction. Machine Learning, 75(3).
C. de Marcken. 1995. Lexical heads, phrase structure and
the induction of grammar. In WVLC.
D. Elworthy. 1994. Does Baum-Welch re-estimation help
taggers? In Proc. of ANLP.
W. N. Francis and H. Kucera, 1979. Manual of Information
to Accompany a Standard Corpus of Present-Day Edited
American English, for use with Digital Computers. De-
partment of Linguistic, Brown University.
J. Gillenwater, K. Ganchev, J. Grac?a, B. Taskar, and
F. Pereira. 2009. Sparsity in grammar induction. In
NIPS: Grammar Induction, Representation of Language
and Language Learning.
W. P. Headden, III, M. Johnson, and D. McClosky. 2009.
Improving unsupervised dependency parsing with richer
contexts and smoothing. In Proc. of NAACL-HLT.
D. Jurafsky. 1996. A probabilistic model of lexical and syn-
tactic access and disambiguation. Cognitive Science, 20.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In Proc. of ACL.
R. L. Lewis and S. Vasishth. 2005. An activation-based
model of sentence processing as skilled memory retrieval.
Cognitive Science, 29.
P. Liang and D. Klein. 2008. Analyzing the errors of unsu-
pervised learning. In Proc. of HLT-ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2).
D. McClosky, E. Charniak, and M. Johnson. 2006a. Effec-
tive self-training for parsing. In Proc. of NAACL-HLT.
D. McClosky, E. Charniak, and M. Johnson. 2006b. Rerank-
ing and self-training for parser adaptation. In Proc. of
COLING-ACL.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2).
V. Ng and C. Cardie. 2003. Weakly supervised natural lan-
guage learning without redundant views. In Proc. of HLT-
NAACL.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In Proc. of ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2009. Baby
Steps: How ?Less is More? in unsupervised dependency
parsing. In NIPS: Grammar Induction, Representation of
Language and Language Learning.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010. Profit-
ing from mark-up: Hyper-text annotations for guided pars-
ing. In Proc. of ACL.
M. J. Wainwright. 2006. Estimating the ?wrong? graphical
model: Benefits in the computation-limited setting. Jour-
nal of Machine Learning Research, 7.
17
Deterministic Statistical Mapping of
Sentences to Underspecified Semantics
Hiyan Alshawi
Google, Inc.
(hiyan@google.com)
Pi-Chuan Chang
Google, Inc.
(pichuan@google.com)
Michael Ringgaard
Google, Inc.
(ringgaard@google.com)
Abstract
We present a method for training a statistical model for mapping natural language sentences to
semantic expressions. The semantics are expressions of an underspecified logical form that has prop-
erties making it particularly suitable for statistical mapping from text. An encoding of the semantic
expressions into dependency trees with automatically generated labels allows application of exist-
ing methods for statistical dependency parsing to the mapping task (without the need for separate
traditional dependency labels or parts of speech). The encoding also results in a natural per-word
semantic-mapping accuracy measure. We report on the results of training and testing statistical mod-
els for mapping sentences of the Penn Treebank into the semantic expressions, for which per-word
semantic mapping accuracy ranges between 79% and 86% depending on the experimental condi-
tions. The particular choice of algorithms used also means that our trained mapping is deterministic
(in the sense of deterministic parsing), paving the way for large-scale text-to-semantic mapping.
1 Introduction
Producing semantic representations of text is motivated not only by theoretical considerations but also
by the hypothesis that semantics can be used to improve automatic systems for tasks that are intrinsically
semantic in nature such as question answering, textual entailment, machine translation, and more gen-
erally any natural language task that might benefit from inference in order to more closely approximate
human performance. Since formal logics have formal denotational semantics, and are good candidates
for supporting inference, they have often been taken to be the targets for mapping text to semantic
representations, with frameworks emphasizing (more) tractable inference choosing first order predicate
logic (Stickel, 1985) while those emphasizing representational power favoring one of the many available
higher order logics (van Benthem, 1995).
It was later recognized that in order to support some tasks, fully specifying certain aspects of a logic
representation, such as quantifier scope, or reference resolution, is often not necessary. For example, for
semantic translation, most ambiguities of quantifier scope can be carried over from the source language
to the target language without being resolved. This led to the development of underspecified semantic
representations (e.g. QLF, Alshawi and Crouch (1992) and MRS, Copestake et al(2005)) which are
easier to produce from text without contextual inference but which can be further specified as necessary
for the task being performed.
While traditionally mapping text to formal representations was predominantly rule-based, for both
the syntactic and semantic components (Montague (1973), Pereira and Shieber (1987), Alshawi (1992)),
good progress in statistical syntactic parsing (e.g. Collins (1999), Charniak (2000)) led to systems that
applied rules for semantic interpretation to the output of a statistical syntactic parser (e.g. Bos et al
(2004)). More recently researchers have looked at statistical methods to provide robust and trainable
methods for mapping text to formal representations of meaning (Zettlemoyer and Collins, 2005).
In this paper we further develop the two strands of work mentioned above, i.e. mapping text to
underspecified semantic representations and using statistical parsing methods to perform the analysis.
15
Here we take a more direct route, starting from scratch by designing an underspecified semantic repre-
sentation (Natural Logical Form, or NLF) that is purpose-built for statistical text-to-semantics mapping.
An underspecified logic whose constructs are motivated by natural language and that is amenable to
trainable direct semantic mapping from text without an intervening layer of syntactic representation. In
contrast, the approach taken by (Zettlemoyer and Collins, 2005), for example, maps into traditional logic
via lambda expressions, and the approach taken by (Poon and Domingos, 2009) depends on an initial
step of syntactic parsing.
In this paper, we describe a supervised training method for mapping text to NLF, that is, producing
a statistical model for this mapping starting from training pairs consisting of sentences and their corre-
sponding NLF expressions. This method makes use of an encoding of NLF expressions into dependency
trees in which the set of labels is automatically generated from the encoding process (rather than being
pre-supplied by a linguistically motivated dependency grammar). This encoding allows us to perform the
text-to-NLF mapping using any existing statistical methods for labeled dependency parsing (e.g. Eisner
(1996), Yamada and Matsumoto (2003), McDonald, Crammer, Pereira (2005)). A side benefit of the
encoding is that it leads to a natural per-word measure for semantic mapping accuracy which we use for
evaluation purposes. By combing our method with deterministic statistical dependency models together
with deterministic (hard) clusters instead of parts of speech, we obtain a deterministic statistical text-to-
semantics mapper, opening the way to feasible mapping of text-to-semantics at a large scale, for example
the entire web.
This paper concentrates on the text-to-semantics mapping which depends, in part, on some properties
of NLF. We will not attempt to defend the semantic representation choices for specific constructions il-
lustrated here. NLF is akin to a variable-free variant of QLF or an MRS in which some handle constraints
are determined during parsing. For the purposes of this paper it is sufficient to note that NLF has roughly
the same granularity of semantic representation as these earlier underspecified representations.
We outline the steps of our text-to-semantics mapping method in Section 2, introduce NLF in Sec-
tion 3, explain the encoding of NLF expressions as formal dependency trees in Section 4, and report on
experiments for training and testing statistical models for mapping text to NLF expressions in Section 5.
2 Direct Semantic Mapping
Our method for mapping text to natural semantics expressions proceeds as follows:
1. Create a corpus of pairs consisting of text sentences and their corresponding NLF semantic ex-
pressions.
2. For each of the sentence-semantics pairs in the corpus, align the words of the sentence to the tokens
of the NLF expressions.
3. ?Encode? each alignment pair as an ordered dependency tree in which the labels are generated by
the encoding process.
4. Train a statistical dependency parsing model with the set of dependency trees.
5. For a new input sentence S, apply the statistical parsing model to S, producing a labeled depen-
dency tree DS .
6. ?Decode? DS into a semantic expression for S.
For step 1, the experiments in this paper (Section 5) obtain the corpus by converting an existing
constituency treebank into semantic expressions. However, direct annotation of a corpus with semantic
expressions is a viable alternative, and indeed we are separately exploring that possibility for a different,
open domain, text corpus.
16
For steps 4 and 5, any method for training and applying a dependency model from a corpus of labeled
dependency trees may be used. As described in Section 5, for the experiments reported here we use an
algorithm similar to that of Nivre (2003).
For steps 2, 3 and 6, the encoding of NLF semantic expressions as dependency trees with automati-
cally constructed labels is described in Section 4.
3 Semantic Expressions
NLF expressions are by design amenable to facilitating training of text-to-semantics mappings. For this
purpose, NLF has a number of desirable properties:
1. Apart from a few built-in logical connectives, all the symbols appearing in NLF expressions are
natural language words.
2. For an NLF semantic expression corresponding to a sentence, the word tokens of the sentence
appear exactly once in the NLF expression.
3. The NLF notation is variable-free.
Technically, NLF expressions are expression of an underspecified logic, i.e. a semantic representation
that leaves open the interpretation of certain constructs (for example the scope of quantifiers and some
operators and the referents of terms such as anaphora, and certain implicit relations such as those for
compound nominals). NLF is similar in some ways to Quasi Logical Form, or QLF (Alshawi, 1992), but
the properties listed above keep NLF closer to natural language than QLF, hence natural logical form. 1
There is no explicit formal connection between NLF and Natural Logic (van Benthem, 1986), though it
may turn out that NLF is a convenient starting point for some Natural Logic inferences.
In contrast to statements of a fully specified logic in which denotations are typically taken to be
functions from possible worlds to truth values (Montague, 1973), denotations of a statement in an under-
specified logic are typically taken to be relations between possible worlds and truth values (Alshawi and
Crouch (1992), Alshawi (1996)). Formal denotations for NLF expressions are beyond the scope of this
paper and will be described elsewhere.
3.1 Connectives and Examples
A NLF expression for the sentence
In 2002, Chirpy Systems stealthily acquired two profitable companies producing pet acces-
sories.
is shown in Figure 1.
The NLF constructs and connectives are explained in Table 1. For variable-free abstraction, an NLF
expression [p, ?, a] corresponds to ?x.p(x, a). Note that some common logical operators are not
built-in since they will appear directly as words such as not.2 We currently use the unknown/unspecified
operator, %, mainly for linguistic constructions that are beyond the coverage of a particular semantic
mapping model. A simple example that includes % in our converted WSJ corpus is Other analysts are
nearly as pessimistic for which the NLF expression is
[are, analysts.other, pessimistic%nearly%as]
In Section 5 we give some statistics on the number of semantic expressions containing % in the data used
for our experiments and explain how it affects our accruracy results.
1The term QLF is now sometimes used informally (e.g. Liakata and Pulman (2002), Poon and Domingos (2009)) for any
logic-like semantic representation without explicit quantifier scope.
2NLF does include Horn clauses, which implictly encode negation, but since Horn clauses are not part of the experiments
reported in this paper, we will not discuss them further here.
17
[acquired
/stealthily
:[in, ?, 2002],
Chirpy+Systems,
companies.two
:profitable
:[producing,
?,
pet+accessories]]
Figure 1: Example of an NLF semantic expression.
Operator Example Denotation Language Constructs
[...] [sold, Chirpy, Growler] predication tuple clauses, prepositions, ...
: company:profitable intersection adjectives, relative clauses, ...
. companies.two (unscoped) quantification determiners, measure terms
? [in, ?, 2005] variable-free abstract prepositions, relatives, ...
_ [eating, _, apples] unspecified argument missing verb arguments, ...
{...} and{Chirpy, Growler} collection noun phrase coordination, ...
/ acquired/stealthily type-preserving operator adverbs, modals, ...
+ Chirpy+Systems implicit relation compound nominals, ...
@ meeting@yesterday temporal restriction bare temporal modifiers, ...
& [...] & [...] conjunction sentences, ...
|...| |Dublin, Paris, Bonn| sequence paragraphs, fragments, lists, ...
% met%as uncovered op constructs not covered
Table 1: NLF constructs and connectives.
4 Encoding Semantics as Dependencies
We encode NLF semantic expressions as labeled dependency trees in which the label set is generated
automatically by the encoding process. This is in contrast to conventional dependency trees for which
the label sets are presupplied (e.g. by a linguistic theory of dependency grammar). The purpose of
the encoding is to enable training of a statistical dependency parser and converting the output of that
parser for a new sentence into a semantic expression. The encoding involves three aspects: Alignment,
headedness, and label construction.
4.1 Alignment
Since, by design, each word token corresponds to a symbol token (the same word type) in the NLF ex-
pression, the only substantive issue in determining the alignment is the occurrence of multiple tokens
of the same word type in the sentence. Depending on the source of the sentence-NLF pairs used for
training, a particular word in the sentence may or may not already be associated with its corresponding
word position in the sentence. For example, in some of the experiments reported in this paper, this corre-
spondence is provided by the semantic expressions obtained by converting a constituency treebank (the
well-known Penn WSJ treebank). For situations in which the pairs are provided without this informa-
tion, as is the case for direct annotation of sentences with NLF expressions, we currently use a heuristic
greedy algorithm for deciding the alignment. This algorithm tries to ensure that dependents are near their
heads, with a preference for projective dependency trees. To guage the importance of including correct
alignments in the input pairs (as opposed to training with inferred alignments), we will present accuracy
results for semantic mapping for both correct and automatically infererred alignments.
18
4.2 Headedness
The encoding requires a definition of headedness for words in an NLF expression, i.e., a head-function
h from dependent words to head words. We define h in terms of a head-function g from an NLF
(sub)expression e to a word w appearing in that (sub)expression, so that, recursively:
g(w) = w
g([e1, ..., en]) = g(e1)
g(e1 : e2) = g(e1)
g(e1.e2) = g(e1)
g(e1/e2) = g(e1)
g(e1@e2) = g(e1)
g(e1&e2) = g(e1)
g(|e1, ..., en|) = g(e1)
g(e1{e2, ..., en}) = g(e1)
g(e1 + ...+ en) = g(en)
g(e1%e2) = g(e1)
Then a head word h(w) for a dependent w is defined in terms of the smallest (sub)expression e
containing w for which
h(w) = g(e) 6= w
For example, for the NLF expression in Figure 1, this yields the heads shown in Table 3. (The labels
shown in that table will be explained in the following section.)
This definition of headedness is not the only possible one, and other variations could be argued for.
The specific definition for NLF heads turns out to be fairly close to the notion of head in traditional
dependency grammars. This is perhaps not surprising since traditional dependency grammars are often
partly motivated by semantic considerations, if only informally.
4.3 Label Construction
As mentioned, the labels used during the encoding of a semantic expression into a dependency tree are
derived so as to enable reconstruction of the expression from a labeled dependency tree. In a general
sense, the labels may be regarded as a kind of formal semantic label, though more specifically, a label is
interpretable as a sequence of instructions for constructing the part of a semantic expression that links a
dependent to its head, given that part of the semantic expression, including that derived from the head,
has already been constructed. The string for a label thus consists of a sequence of atomic instructions,
where the decoder keeps track of a current expression and the parent of that expression in the expression
tree being constructed. When a new expression is created it becomes the current expression whose parent
is the old current expression. The atomic instructions (each expressed by a single character) are shown
in Table 2.
A sequence of instructions in a label can typically (but not always) be paraphrased informally as
?starting from head word wh, move to a suitable node (at or above wh) in the expression tree, add speci-
fied NLF constructs (connectives, tuples, abstracted arguments) and then add wd as a tuple or connective
argument.?
Continuing with our running example, the labels for each of the words are shown in Table 3.
Algorithmically, we find it convenient to transform semantic expressions into dependency trees and
vice versa via a derivation tree for the semantic expression in which the atomic instruction symbols listed
above are associated with individual nodes in the derivation tree.
The output of the statistical parser may contain inconsistent trees with formal labels, in particular
trees in which two different arguments are predicated to fill the same position in a semantic expression
tuple. For such cases, the decoder that produces the semantic expression applies the simple heuristic
19
Instruction Decoding action
[, {, | Set the current expression to a
newly created tuple, collection,
or sequence.
:, /, ., +, &, @, % Attach the current subexpression
to its parent with the specified
connective.
* Set the current expression to a
newly created symbol from the
dependent word.
0, 1, ... Add the current expression at the
specified parent tuple position.
?, _ Set the current subexpression to
a newly created abstracted-over or
unspecfied argument.
- Set the current subexpression to be
the parent of the current expression.
Table 2: Atomic instructions in formal label sequences.
Dependent Head Label
In acquired [:?1-*0
2002 in -*2
Chirpy Systems *+
Systems acquired -*1
stealthily acquired */
acquired [*0
two companies *.
profitable companies *:
companies acquired -*2
producing companies [:?1-*0
pet accessories *+
accessories producing -*2
Table 3: Formal labels for an example sentence.
20
Dataset Null Labels? Auto Align? WSJ sections Sentences
Train+Null-AAlign yes no 2-21 39213
Train-Null-AAlign no no 2-21 24110
Train+Null+AAlign yes yes 2-21 35778
Train-Null+AAlign no yes 2-21 22611
Test+Null-AAlign yes no 23 2416
Test-Null-AAlign no no 23 1479
Table 4: Datasets used in experiments.
of using the next available tuple position when such a conflicting configuration is predicated. In our
experiments, we are measuring per-word semantic head-and-label accuracy, so this heuristic does not
play a part in that evaluation measure.
5 Experiments
5.1 Data Preparation
In the experiments reported here, we derive our sentence-semantics pairs for training and testing from
the Penn WSJ Treebank. This choice reflects the lack, to our knowledge, of a set of such pairs for a
reasonably sized publicly available corpus, at least for NLF expressions. Our first step in preparing the
data was to convert the WSJ phrase structure trees into semantic expressions. This conversion is done
by programming the Stanford treebank toolkit to produce NLF trees bottom-up from the phrase structure
trees. This conversion process is not particularly noteworthy in itself (being a traditional rule-based
syntax-to-semantics translation process) except perhaps to the extent that the closeness of NLF to natural
language perhaps makes the conversion somewhat easier than, say, conversion to a fully resolved logical
form.
Since our main goal is to investigate trainable mappings from text strings to semantic expressions,
we only use the WSJ phrase structure trees in data preparation: the phrase structure trees are not used as
inputs when training a semantic mapping model, or when applying such a model. For the same reason,
in these experiments, we do not use the part-of-speech information associated with the phrase structure
trees in training or applying a semantic mapping model. Instead of parts-of-speech we use word cluster
features from a hierarchical clustering produced with the unsupervised Brown clustering method (Brown
et al 1992); specifically we use the publicly available clusters reported in Koo et al (2008).
Constructions in the WSJ that are beyond the explicit coverage of the conversion rules used for data
preparation result in expressions that include the unknown/unspecified (or ?Null?) operator %. We report
on different experimental settings in which we vary how we treat training or testing expressions with
%. This gives rise to the data sets in Table 4 which have +Null (i.e., including %), and -Null (i.e., not
including %) in the data set names.
Another attribute we vary in the experiments is whether to align the words in the semantic expressions
to the words in the sentence automatically, or whether to use the correct alignment (in this case preserved
from the conversion process, but could equally be provided as part of a manual semantic annotation
scheme, for example). In our current experiments, we discard non-projective dependency trees from
training sets. Automatic alignment results in additional non-projective trees, giving rise to different
effective training sets when auto-alignment is used: these sets are marked with +AAlign, otherwise -
AAlign. The training set numbers shown in Table 4 are the resulting sets after removal of non-projective
trees.
21
Training Test Accuracy(%)
+Null-AAlign +Null-AAlign 81.2
-Null-AAlign +Null-AAlign 78.9
-Null-AAlign -Null-AAlign 86.1
+Null-AAlign -Null-AAlign 86.5
Table 5: Per-word semantic accuracy when training with the correct alignment.
Training Test Accuracy(%)
+Null+AAlign +Null-AAlign 80.4
-Null+AAlign +Null-AAlign 78.0
-Null+AAlign -Null-AAlign 85.5
+Null+AAlign -Null-AAlign 85.8
Table 6: Per-word semantic accuracy when training with an auto-alignment.
5.2 Parser
As mentioned earlier, our method can make use of any trainable statistical dependency parsing algorithm.
The parser is trained on a set of dependency trees with formal labels as explained in Sections 2 and 4.
The specific parsing algorithm we use in these experiments is a deterministic shift reduce algorithm
(Nivre, 2003), and the specific implementation of the algorithm uses a linear SVM classifier for predict-
ing parsing actions (Chang et al, 2010). As noted above, hierarchical cluster features are used instead
of parts-of-speech; some of the features use coarse (6-bit) or finer (12-bit) clusters from the hierarchy.
More specifically, the full set of features is:
? The words for the current and next input tokens, for the top of the stack, and for the head of the
top of the stack.
? The formal labels for the top-of-stack token and its leftmost and rightmost children, and for the
leftmost child of the current token.
? The cluster for the current and next three input tokens and for the top of the stack and the token
below the top of the stack.
? Pairs of features combining 6-bit clusters for these tokens together with 12-bit clusters for the top
of stack and next input token.
5.3 Results
Tables 5 and 6 show the per-word semantic accuracy for different training and test sets. This measure is
simply the percentage of words in the test set for which both the predicted formal label and the head word
are correct. In syntactic dependency evaluation terminology, this corresponds to the labeled attachment
score.
All tests are with respect to the correct alignment; we vary whether the correct alignment (Table 5)
or auto-alignment (Table 6) is used for training to give an idea of how much our heuristic alignment
is hurting the semantic mapping model. As shown by comparing the two tables, the loss in accuracy
due to using the automatic alignment is only about 1%, so while the automatic alignment algorithm can
probably be improved, the resulting increase in accuracy would be relatively small.
As shown in the Tables 5 and 6, two versions of the test set are used: one that includes the ?Null?
operator %, and a smaller test set with which we are testing only the subset of sentences for which the
semantic expressions do not include this label. The highest accuracies (mid 80?s) shown are for the
22
# Labels # Train Sents Accuracy(%)
151 (all) 22611 85.5
100 22499 85.5
50 21945 85.5
25 17669 83.8
12 7008 73.4
Table 7: Per-word semantic accuracy after pruning label sets in Train-Null+AAlign (and testing with
Test-Null-AAlign).
(easier) test set which excludes examples in which the test semantic expressions contain Null operators.
The strictest settings, in which semantic expressions with Null are not included in training but included
in the test set effectively treat prediction of Null operators as errors. The lower accuracy (high 70?s) for
such stricter settings thus incorporates a penalty for our incomplete coverage of semantics for the WSJ
sentences. The less strict Test+Null settings in which % is treated as a valid output may be relevant to
applications that can tolerate some unknown operators between subexpressions in the output semantics.
Next we look at the effect of limiting the size of the automatically generated formal label set prior
to training. For this we take the configuration using the TrainWSJ-Null+AAlign training set and the
TestWSJ-Null-AAlign test set (the third row in Table refPerWordSemanticAccuracyAAlign for which
auto-alignment is used and only labels without the NULL operator % are included). For this training
set there are 151 formal labels. We then limit the training set to instances that only include the most
frequent k labels, for k = 100, 50, 25, 12, while keeping the test set the same. As can be seen in Table 7,
the accuracy is unaffected when the training set is limited to the 100 most frequent or 50 most frequent
labels. There is a slight loss when training is limited to 25 labels and a large loss if it is limited to 12
labels. This appears to show that, for this corpus, the core label set needed to construct the majority
of semantic expressions has a size somewhere between 25 and 50. It is perhaps interesting that this is
roughly the size of hand-produced traditional dependency label sets. On the other hand, it needs to be
emphasized that since Table 7 ignores beyond-coverage constructions that presently include Null labels,
it is likely that a larger label set would be needed for more complete semantic coverage.
6 Conclusion and Further Work
We?ve shown that by designing an underspecified logical form that is motivated by, and closely related to,
natural language constructions, it is possible to train a direct statistical mapping from pairs of sentences
and their corresponding semantic expressions, with per-word accuracies ranging from 79% to 86% de-
pending on the strictness of the experimental setup. The input to training does not require any traditional
syntactic categories or parts of speech. We also showed, more specifically, that we can train a model that
can be applied deterministically at runtime (using a deterministic shift reduce algorithm combined with
deterministic clusters), making large-scale text-to-semantics mapping feasible.
In traditional formal semantic mapping methods (Montague (1973), Bos et al (2004)), and even
some recent statistical mapping methods (Zettlemoyer and Collins, 2005), the semantic representation is
overloaded to performs two functions: (i) representing the final meaning, and (ii) composing meanings
from the meanings of subconstituents (e.g. through application of higher order lambda functions). In our
view, this leads to what are perhaps overly complex semantic representations of some basic linguistic
constructions. In contrast, in the method we presented, these two concerns (meaning representation and
semantic construction) are separated, enabling us to keep the semantics of constituents simple, while
turning the construction of semantic expressions into a separate structured learning problem (with its
own internal prediction and decoding mechanisms).
Although, in the experiments we reported here we do prepare the training data from a traditional
treebank, we are encouraged by the results and believe that annotation of a corpus with only semantic
23
expressions is sufficient for building an efficient and reasonably accurate text-to-semantics mapper. In-
deed, we have started building such a corpus for a question answering application, and hope to report
results for that corpus in the future. Other further work includes a formal denotational semantics of the
underspecified logical form and elaboration of practical inference operations with the semantic expres-
sions. This work may also be seen as a step towards viewing semantic interpretation of language as the
interaction between a pattern recognition process (described here) and an inference process.
References
Hiyan Alshawi and Richard Crouch. 1992. Monotonic Semantic Interpretation. Proceedings of the 30th Annual
Meeting of the Association for Computational Linguistics. Newark, Delaware, 32?39.
Hiyan Alshawi, ed. 1992. The Core Language Engine. MIT Press, Cambridge, Massachusetts.
Hiyan Alshawi. 1996. Underspecified First Order Logics. In Semantic Ambiguity and Underspecification, edited
by Kees van Deemter and Stanley Peters, CSLI Publications, Stanford, California.
Johan van Benthem. 1986. Essays in Logical Semantics. Reidel, Dordrecht.
Johan van Benthem. 1995. Language in Action: Categories, Lambdas, and Dynamic Logic. MIT Press, Cam-
bridge, Massachusetts.
Bos, Johan, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. Proceedings of the 20th International Conference on Computa-
tional Linguistics. Geneva, Switzerland, 1240?1246.
P. Brown, V. Pietra, P. Souza, J. Lai, and R. Mercer. 1992. Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Eugene Charniak. 2000. A maximum entropy inspired parser. Proceedings of the 1st Conference of the North
American Chapter of the Association for Computational Linguistics, Seattle, Washington.
Michael Collins. 1999. Head Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
A. Copestake, D. Flickinger, I. Sag, C. Pollard. 2005. Minimal Recursion Semantics, An Introduction. Research
on Language and Computation, 3(23):281-332.
D. Davidson. 1967. The Logical Form of Action Sentences. In The Logic of Decision and Action, edited by
N. Rescher, University of Pittsburgh Press, Pittsburgh, Pennsylvania.
Jason Eisner. 1996. Three New Probabilistic Models for Dependency Parsing: An Exploration. Proceedings of
the 16th International Conference on Computational Linguistics (COLING-96, 340?345.
T. Koo, X. Carreras, and M. Collins. 2008. Simple Semisupervised Dependency Parsing. Proceedings of the
Annual Meeting of the Association for Computational Linguistics.
Maria Liakata and Stephen Pulman. 2002. From trees to predicate-argument structures. Proceedings of the 19th
International Conference on Computational Linguistics. Taipei, Taiwan, 563?569.
Chang, Y.-W., C.-J. Hsieh, K.-W. Chang, M. Ringgaard, and C.-J. Lin. 2010. Training and Testing Low-degree
Polynomial Data Mappings via Linear SVM. Journal of Machine Learning Research, 11, 1471?1490.
Ryan McDonald, Koby Crammer and Fernando Pereira 2005. Online Large-Margin Training of Dependency
Parsers. Proceedomgs of the 43rd Annual Meeting of the Association for Computational Linguistics..
R. Montague. 1973. The Proper Treatment of Quantification in Ordinary English. In Formal Philosophy, edited
by R. Thomason, Yale University Press, New Haven.
Fernando Pereira and Stuart Shieber. 1987. Prolog and Natural Language Analysis. Center for the Study of
Language and Information, Stanford, California.
Joakim Nivre 2003 An Efficient Algorithm for Projective Dependency Parsing. Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies, 149?160.
H. Poon and P. Domingos 2009. Unsupervised semantic parsing. Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, Singapore, 2009.
Mark Stickel. 1985. Automated deduction by theory resolution. Journal of Automated Reasoning, 1, 4.
Hiroyasu Yamada and Yuji Matsumoto 2003. Statistical dependency analysis with support vector machines.
Proceedings of the 8th International Workshop on Parsing Technologies, 195?206.
Zettlemoyer, Luke S. and Michael Collins. 2005. Learning to map sentences to logical form: Structured classifi-
cation with probabilistic categorial grammars. Proceedings of the 21st Conference on Uncertainty in Artificial
Intelligence. Edinburgh, Scotland, 658?666.
24
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 19?28,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Punctuation: Making a Point in Unsupervised Dependency Parsing
Valentin I. Spitkovsky
Computer Science Department
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc.
Mountain View, CA, 94043, USA
hiyan@google.com
Daniel Jurafsky
Departments of Linguistics and Computer Science
Stanford University, Stanford, CA, 94305, USA
jurafsky@stanford.edu
Abstract
We show how punctuation can be used to im-
prove unsupervised dependency parsing. Our
linguistic analysis confirms the strong connec-
tion between English punctuation and phrase
boundaries in the Penn Treebank. However,
approaches that naively include punctuation
marks in the grammar (as if they were words)
do not perform well with Klein and Manning?s
Dependency Model with Valence (DMV). In-
stead, we split a sentence at punctuation and
impose parsing restrictions over its fragments.
Our grammar inducer is trained on the Wall
Street Journal (WSJ) and achieves 59.5% ac-
curacy out-of-domain (Brown sentences with
100 or fewer words), more than 6% higher
than the previous best results. Further evalu-
ation, using the 2006/7 CoNLL sets, reveals
that punctuation aids grammar induction in
17 of 18 languages, for an overall average
net gain of 1.3%. Some of this improvement
is from training, but more than half is from
parsing with induced constraints, in inference.
Punctuation-aware decoding works with exist-
ing (even already-trained) parsing models and
always increased accuracy in our experiments.
1 Introduction
Unsupervised dependency parsing is a type of gram-
mar induction ? a central problem in computational
linguistics. It aims to uncover hidden relations be-
tween head words and their dependents in free-form
text. Despite decades of significant research efforts,
the task still poses a challenge, as sentence structure
is underdetermined by only raw, unannotated words.
Structure can be clearer in formatted text, which
typically includes proper capitalization and punctua-
tion (Gravano et al, 2009). Raw word streams, such
as utterances transcribed by speech recognizers, are
often difficult even for humans (Kim and Woodland,
2002). Therefore, one would expect grammar induc-
ers to exploit any available linguistic meta-data. And
yet in unsupervised dependency parsing, sentence-
internal punctuation has long been ignored (Carroll
and Charniak, 1992; Paskin, 2001; Klein and Man-
ning, 2004; Blunsom and Cohn, 2010, inter alia).
HTML is another kind of meta-data that is ordi-
narily stripped out in pre-processing. However, re-
cently Spitkovsky et al (2010b) demonstrated that
web markup can successfully guide hierarchical
syntactic structure discovery, observing, for exam-
ple, that anchors often match linguistic constituents:
..., whereas McCain is secure on the topic, Obama
<a>[VP worries about winning the pro-Israel vote]</a>.
We propose exploring punctuation?s potential to
aid grammar induction. Consider a motivating ex-
ample (all of our examples are from WSJ), in which
all (six) marks align with constituent boundaries:
[SBAR Although it probably has reduced the level of
expenditures for some purchasers], [NP utilization man-
agement] ? [PP like most other cost containment strate-
gies] ? [VP doesn?t appear to have altered the long-term
rate of increase in health-care costs], [NP the Institute of
Medicine], [NP an affiliate of the National Academy of
Sciences], [VP concluded after a two-year study].
This link between punctuation and constituent
boundaries suggests that we could approximate
parsing by treating inter-punctuation fragments in-
dependently. In training, our algorithm first parses
each fragment separately, then parses the sequence
of the resulting head words. In inference, we use a
better approximation that allows heads of fragments
to be attached by arbitrary external words, e.g.:
The Soviets complicated the issue by offering to
[VP include light tanks], [SBAR which are as light as ... ].
19
Count POS Sequence Frac Cum
1 3,492 NNP 2.8%
2 2,716 CD CD 2.2 5.0
3 2,519 NNP NNP 2.0 7.1
4 2,512 RB 2.0 9.1
5 1,495 CD 1.2 10.3
6 1,025 NN 0.8 11.1
7 1,023 NNP NNP NNP 0.8 11.9
8 916 IN NN 0.7 12.7
9 795 VBZ NNP NNP 0.6 13.3
10 748 CC 0.6 13.9
11 730 CD DT NN 0.6 14.5
12 705 PRP VBD 0.6 15.1
13 652 JJ NN 0.5 15.6
14 648 DT NN 0.5 16.1
15 627 IN DT NN 0.5 16.6
WSJ +103,148 more with Count ? 621 83.4%
Table 1: Top 15 fragments of POS tag sequences in WSJ.
Count Non-Terminal Frac Cum
1 40,223 S 32.5%
2 33,607 NP 27.2 59.7
3 16,413 VP 13.3 72.9
4 12,441 PP 10.1 83.0
5 8,350 SBAR 6.7 89.7
6 4,085 ADVP 3.3 93.0
7 3,080 QP 2.5 95.5
8 2,480 SINV 2.0 97.5
9 1,257 ADJP 1.0 98.5
10 369 PRN 0.3 98.8
WSJ +1,446 more with Count ? 356 1.2%
Table 2: Top 99% of the lowest dominating non-terminals
deriving complete inter-punctuation fragments in WSJ.
2 Definitions, Analyses and Constraints
Punctuation and syntax are related (Nunberg, 1990;
Briscoe, 1994; Jones, 1994; Doran, 1998, inter alia).
But are there simple enough connections between
the two to aid in grammar induction? This section
explores the regularities. Our study of punctuation
in WSJ (Marcus et al, 1993) parallels Spitkovsky
et al?s (2010b, ?5) analysis of markup from a web-
log, since their proposed constraints turn out to be
useful. Throughout, we define an inter-punctuation
fragment as a maximal (non-empty) consecutive se-
quence of words that does not cross punctuation
boundaries and is shorter than its source sentence.
2.1 A Linguistic Analysis
Out of 51,558 sentences, most ? 37,076 (71.9%) ?
contain sentence-internal punctuation. These punc-
tuated sentences contain 123,751 fragments, nearly
all ? 111,774 (90.3%) ? of them multi-token.
Common part-of-speech (POS) sequences compris-
ing fragments are diverse (note also their flat distri-
bution ? see Table 1). The plurality of fragments
are dominated by a clause, but most are dominated
by one of several kinds of phrases (see Table 2).
As expected, punctuation does not occur at all con-
stituent boundaries: Of the top 15 productions that
yield fragments, five do not match the exact brack-
eting of their lowest dominating non-terminal (see
ranks 6, 11, 12, 14 and 15 in Table 3, left). Four of
them miss a left-adjacent clause, e.g., S? S NP VP:
[S [S It?s an overwhelming job], [NP she] [VP says.]]
This production is flagged because the fragment
NP VP is not a constituent ? it is two; still, 49.4%
of all fragments do align with whole constituents.
Inter-punctuation fragments correspond more
strongly to dependencies (see Table 3, right). Only
one production (rank 14) shows a daughter outside
her mother?s fragment. Some number of such pro-
ductions is inevitable and expected, since fragments
must coalesce (i.e., the root of at least one fragment
? in every sentence with sentence-internal punc-
tuation ? must be attached by some word from a
different, external fragment). We find it noteworthy
that in 14 of the 15 most common cases, a word in
an inter-punctuation fragment derives precisely the
rest of that fragment, attaching none of the other,
external words. This is true for 39.2% of all frag-
ments, and if we include fragments whose heads at-
tach other fragments? heads, agreement increases to
74.0% (see strict and loose constraints in ?2.2, next).
2.2 Five Parsing Constraints
Spitkovsky et al (2010b, ?5.3) showed how to ex-
press similar correspondences with markup as pars-
ing constraints. They proposed four constraints but
employed only the strictest three, omitting imple-
mentation details. We revisit their constraints, speci-
fying precise logical formulations that we use in our
code, and introduce a fifth (most relaxed) constraint.
Let [x, y] be a fragment (or markup) spanning po-
sitions x through y (inclusive, with 1 ? x < y ? l), in
a sentence of length l. And let [i, j]h be a sealed span
headed by h (1 ? i ? h ? j ? l), i.e., the word at po-
sition h dominates precisely i . . . j (but none other):
i h j
20
Count Constituent Production Frac Cum
1 7,115 PP? IN NP 5.7%
2 5,950 S? NP VP 4.8 10.6
3 3,450 NP? NP PP 2.8 13.3
4 2,799 SBAR? WHNP S 2.3 15.6
5 2,695 NP? NNP 2.2 17.8
6 2,615 S? S NP VP 2.1 19.9
7 2,480 SBAR? IN S 2.0 21.9
8 2,392 NP? NNP NNP 1.9 23.8
9 2,354 ADVP? RB 1.9 25.7
10 2,334 QP? CD CD 1.9 27.6
11 2,213 S? PP NP VP 1.8 29.4
12 1,441 S? S CC S 1.2 30.6
13 1,317 NP? NP NP 1.1 31.6
14 1,314 S? SBAR NP VP 1.1 32.7
15 1,172 SINV? S VP NP NP 0.9 33.6
WSJ +82,110 more with Count ? 976 66.4%
Count Head-Outward Spawn Frac Cum
1 11,928 IN 9.6%
2 8,852 NN 7.2 16.8
3 7,802 NNP 6.3 23.1
4 4,750 CD 3.8 26.9
5 3,914 VBD 3.2 30.1
6 3,672 VBZ 3.0 33.1
7 3,436 RB 2.8 35.8
8 2,691 VBG 2.2 38.0
9 2,304 VBP 1.9 39.9
10 2,251 NNS 1.8 41.7
11 1,955 WDT 1.6 43.3
12 1,409 MD 1.1 44.4
13 1,377 VBN 1.1 45.5
14 1,204 IN VBD 1.0 46.5
15 927 JJ 0.7 47.3
WSJ +65,279 more with Count ? 846 52.8%
Table 3: Top 15 productions yielding punctuation-induced fragments in WSJ, viewed as constituents (left) and as de-
pendencies (right). For constituents, we recursively expanded any internal nodes that did not align with the associated
fragmentation (underlined). For dependencies we dropped all daughters that fell entirely in the same region as their
mother (i.e., both inside a fragment, both to its left or both to its right), keeping only crossing attachments (just one).
Define inside(h, x, y) as true iff x ? h ? y; and let
cross(i, j, x, y) be true iff (i < x ? j ? x ? j < y) ?
(i > x ? i ? y ? j > y). The three tightest constraints
impose conditions which, when satisfied, disallow
sealing [i, j]h in the presence of an annotation [x, y]:
strict ? requires [x, y] itself to be sealed in the
parse tree, voiding all seals that straddle exactly one
of {x, y} or protrude beyond [x, y] if their head is in-
side. This constraint holds for 39.2% of fragments.
By contrast, only 35.6% of HTML annotations, such
as anchor texts and italics, agree with it (Spitkovsky
et al, 2010b). This necessarily fails in every sen-
tence with internal punctuation (since there, some
fragment must take charge and attach another), when
cross(i, j, x, y) ? (inside(h, x, y) ? (i < x ? j > y)).
... the British daily newspaper, The Financial Times .
x = i h = j = y
loose ? if h ? [x, y], requires that everything in
x . . . y fall under h, with only h allowed external at-
tachments. This holds for 74.0% of fragments ?
87.5% of markup, failing when cross(i, j, x, y).
... arrests followed a ? Snake Day ? at Utrecht ...
i x h = j = y
sprawl ? still requires that h derive x . . . y but
lifts restrictions on external attachments. Holding
for 92.9% of fragments (95.1% of markup), it fails
when cross(i, j, x, y) ? ?inside(h, x, y).
Maryland Club also distributes tea , which ...
x = i h y j
These three strictest constraints lend themselves to a
straight-forward implementation as an O(l5) chart-
based decoder. Ordinarily, the probability of [i, j]h
is computed by multiplying the probability of the as-
sociated unsealed span by two stopping probabilities
? that of the word at h on the left (adjacent if i = h;
non-adjacent if i < h) and on the right (adjacent if
h = j; non-adjacent if h < j). To impose a con-
straint, we ran through all of the annotations [x, y]
associated with a sentence and zeroed out this prob-
ability if any of them satisfied disallowed conditions.
There are faster ? e.g., O(l4), and even O(l3) ?
recognizers for split head automaton grammars (Eis-
ner and Satta, 1999). Perhaps a more practical, but
still clear, approach would be to generate n-best lists
using a more efficient unconstrained algorithm, then
apply the constraints as a post-filtering step.
Relaxed constraints disallow joining adjacent
subtrees, e.g., preventing the seal [i, j]h from merg-
ing below the unsealed span [j +1, J ]H , on the left:
i h j j + 1 H J
21
tear ? prevents x . . . y from being torn apart by
external heads from opposite sides. It holds for
94.7% of fragments (97.9% of markup), and is vi-
olated when (x ? j ? y > j ? h < x), in this case.
... they ?were not consulted about the [Ridley decision]
in advance and were surprised at the action taken .
thread ? requires only that no path from the root
to a leaf enter [x, y] twice. This holds for 95.0% of
all fragments (98.5% of markup); it is violated when
(x ? j ? y > j ? h < x) ? (H ? y), again, in this
case. Example that satisfies thread but violates tear:
The ... changes ?all make a lot of sense to me,? he added.
The case when [i, j]h is to the right is entirely sym-
metric, and these constraints could be incorporated
in a more sophisticated decoder (since i and J do
not appear in the formulae, above). We implemented
them by zeroing out the probability of the word at H
attaching that at h (to its left), in case of a violation.
Note that all five constraints are nested. In partic-
ular, this means that it does not make sense to com-
bine them, for a given annotation [x, y], since the re-
sult would just match the strictest one. Our markup
number for tear is lower (97.9 versus 98.9%) than
Spitkovsky et al?s (2010b), because theirs allowed
cases where markup was neither torn nor threaded.
Common structures that violate thread (and, con-
sequently, all five of the constraints) include, e.g.,
?seamless? quotations and even ordinary lists:
Her recent report classifies the stock as a ?hold.?
The company said its directors, management and
subsidiaries will remain long-term investors and ...
2.3 Comparison with Markup
Most punctuation-induced constraints are less ac-
curate than the corresponding markup-induced con-
straints (e.g., sprawl: 92.9 vs. 95.1%; loose: 74.0
vs. 87.5%; but not strict: 39.2 vs. 35.6%). However,
markup is rare: Spitkovsky et al (2010b, ?5.1) ob-
served that only 10% of the sentences in their blog
were annotated; in contrast, over 70% of the sen-
tences in WSJ are fragmented by punctuation.
Fragments are more than 40% likely to be dom-
inated by a clause; for markup, this number is be-
low 10% ? nearly 75% of it covered by noun
phrases. Further, inter-punctuation fragments are
spread more evenly under noun, verb, prepositional,
adverbial and adjectival phrases (approximately
27:13:10:3:1 versus 75:13:2:1:1) than markup.1
3 The Model, Methods and Metrics
We model grammar via Klein and Manning?s (2004)
Dependency Model with Valence (DMV), which
ordinarily strips out punctuation. Since this step
already requires identification of marks, our tech-
niques are just as ?unsupervised.? We would have
preferred to test punctuation in their original set-up,
but this approach wasn?t optimal, for several rea-
sons. First, Klein and Manning (2004) trained with
short sentences (up to only ten words, on WSJ10),
whereas most punctuation appears in longer sen-
tences. And second, although we could augment
the training data (say, to WSJ45), Spitkovsky et
al. (2010a) showed that classic EM struggles with
longer sentences. For this reason, we use Viterbi
EM and the scaffolding suggested by Spitkovsky et
al. (2010a) ? also the setting in which Spitkovsky et
al. (2010b) tested their markup-induced constraints.
3.1 A Basic System
Our system is based on Laplace-smoothed Viterbi
EM, following Spitkovsky et al?s (2010a) two-stage
scaffolding: the first stage trains with just the sen-
tences up to length 15; the second stage then retrains
on nearly all sentences ? those with up to 45 words.
Initialization
Klein and Manning?s (2004) ?ad-hoc harmonic? ini-
tializer does not work very well for longer sentences,
particularly with Viterbi training (Spitkovsky et al,
2010a, Figure 3). Instead, we use an improved ini-
tializer that approximates the attachment probability
between two words as an average, over all sentences,
of their normalized aggregate weighted distances.
Our weighting function is w(d) = 1+1/ lg(1+d).2
Termination
Spitkovsky et al (2010a) iterated until successive
changes in overall (best parse) per-token cross-
entropy dropped below 2?20 bits. Since smoothing
can (and does, at times) increase the objective, we
found it more efficient to terminate early, after ten
1Markup and fragments are as likely to be in verb phrases.
2Integer d ? 1 is a distance between two tokens; lg is log2.
22
steps of suboptimal models. We used the lowest-
perplexity (not necessarily the last) model found, as
measured by the cross-entropy of the training data.
Constrained Training
Training with punctuation replaces ordinary Viterbi
parse trees, at every iteration of EM, with the out-
put of a constrained decoder. In all experiments
other than #2 (?5) we train with the loose constraint.
Spitkovsky et al (2010b) found this setting to be
best for markup-induced constraints. We apply it to
constraints induced by inter-punctuation fragments.
Constrained Inference
Spitkovsky et al (2010b) recommended using the
sprawl constraint in inference. Once again, we fol-
low their advice in all experiments except #2 (?5).
3.2 Data Sets and Scoring
We trained on the Penn English Treebank?s Wall
Street Journal portion (Marcus et al, 1993). To eval-
uate, we automatically converted its labeled con-
stituents into unlabeled dependencies, using deter-
ministic ?head-percolation? rules (Collins, 1999),
discarding punctuation, any empty nodes, etc., as is
standard practice (Paskin, 2001; Klein and Manning,
2004). We also evaluated against the parsed portion
of the Brown corpus (Francis and Kuc?era, 1979),
used as a blind, out-of-domain evaluation set,3 sim-
ilarly derived from labeled constituent parse trees.
We report directed accuracies ? fractions of cor-
rectly guessed arcs, including the root, in unlabeled
reference dependency parse trees, as is also standard
practice (Paskin, 2001; Klein and Manning, 2004).
One of our baseline systems (?3.3) produces depen-
dency trees containing punctuation. In this case we
do not score the heads assigned to punctuation and
use forgiving scoring for regular words: crediting
correct heads separated from their children by punc-
tuation alone (from the point of view of the child,
looking up to the nearest non-punctuation ancestor).
3.3 Baseline Systems
Our primary baseline is the basic system without
constraints (standard training). It ignores punctu-
ation, as is standard, scoring 52.0% against WSJ45.
A secondary (punctuation as words) baseline in-
3Note that WSJ{15, 45} overlap with Section 23 ? training
on the test set is standard practice in unsupervised learning.
corporates punctuation into the grammar as if it were
words, as in supervised dependency parsing (Nivre
et al, 2007b; Lin, 1998; Sleator and Temperley,
1993, inter alia). It is worse, scoring only 41.0%.4,5
4 Experiment #1: Default Constraints
Our first experiment compares ?punctuation as con-
straints? to the baseline systems. We use default set-
tings, as recommended by Spitkovsky et al (2010b):
loose in training; and sprawl in inference. Evalua-
tion is on Section 23 of WSJ (all sentence lengths).
To facilitate comparison with prior work, we also re-
port accuracies against shorter sentences, with up to
ten non-punctuation tokens (WSJ10 ? see Table 4).
We find that both constrained regimes improve
performance. Constrained decoding alone increases
the accuracy of a standardly-trained system from
52.0% to 54.0%. And constrained training yields
55.6% ? 57.4% in combination with inference.
4We were careful to use exactly the same data sets in both
cases, not counting punctuation towards sentence lengths. And
we used forgiving scoring (?3.2) when evaluating these trees.
5To get this particular number we forced punctuation to be
tacked on, as a layer below the tree of words, to fairly compare
systems (using the same initializer). Since improved initializa-
tion strategies ? both ours and Klein and Manning?s (2004)
?ad-hoc harmonic? initializer ? rely on distances between to-
kens, they could be unfairly biased towards one approach or the
other, if punctuation counted towards length. We also trained
similar baselines without restrictions, allowing punctuation to
appear anywhere in the tree (still with forgiving scoring ? see
?3.2), using the uninformed uniform initializer (Spitkovsky et
al., 2010a). Disallowing punctuation as a parent of a real word
made things worse, suggesting that not all marks belong near
the leaves (sentence stops, semicolons, colons, etc. make more
sense as roots and heads). We tried the weighted initializer also
without restrictions and repeated all experiments without scaf-
folding, on WSJ15 and WSJ45 alone, but treating punctuation
as words never came within even 5% of (comparable) standard
training. Punctuation, as words, reliably disrupted learning.
WSJ? WSJ10
Supervised DMV 69.8 83.6
w/Constrained Inference 73.0 84.3
Punctuation as Words 41.7 54.8
Standard Training 52.0 63.2
w/Constrained Inference 54.0 63.6
Constrained Training 55.6 67.0
w/Constrained Inference 57.4 67.5
Table 4: Directed accuracies on Section 23 of WSJ? and
WSJ10 for the supervised DMV, our baseline systems and
the punctuation runs (all using the weighted initializer).
23
These are multi-point increases, but they could dis-
appear in a more accurate state-of-the-art system.
To test this hypothesis, we applied constrained de-
coding to a supervised system. We found that this
(ideal) instantiation of the DMV benefits as much or
more than the unsupervised systems: accuracy in-
creases from 69.8% to 73.0%. Punctuation seems
to capture the kinds of, perhaps long-distance, regu-
larities that are not accessible to the model, possibly
because of its unrealistic independence assumptions.
5 Experiment #2: Optimal Settings
Spitkovsky et al (2010b) recommended training
with loose and decoding with sprawl based on their
experiments with markup. But are these the right
settings for punctuation? Inter-punctuation frag-
ments are quite different from markup ? they are
more prevalent but less accurate. Furthermore, we
introduced a new constraint, thread, that Spitkovsky
et al (2010b) had not considered (along with tear).
We next re-examined the choices of constraints.
Our full factorial analysis was similar, but signifi-
cantly smaller, than Spitkovsky et al?s (2010b): we
excluded their larger-scale news and web data sets
that are not publicly available. Nevertheless, we
still tried every meaningful combination of settings,
testing both thread and tear (instead of strict, since
it can?t work with sentences containing sentence-
internal punctuation), in both training and inference.
We did not find better settings than loose for train-
ing, and sprawl for decoding, among our options.
A full analysis is omitted due to space constraints.
Our first observation is that constrained inference,
using punctuation, is helpful and robust. It boosted
accuracy (on WSJ45) by approximately 1.5%, on
average, with all settings. Indeed, sprawl was con-
sistently (but only slightly, at 1.6%, on average) bet-
ter than the rest. Second, constrained training hurt
more often than it helped. It degraded accuracy in all
but one case, loose, where it gained approximately
0.4%, on average. Both improvements are statisti-
cally significant: p ? 0.036 for training with loose;
and p ? 5.6? 10?12 for decoding with sprawl.
6 More Advanced Methods
So far, punctuation has improved grammar induction
in a toy setting. But would it help a modern system?
Our next two experiments employ a slightly more
complicated set-up, compared with the one used up
until now (?3.1). The key difference is that this sys-
tem is lexicalized, as is standard among the more ac-
curate grammar inducers (Blunsom and Cohn, 2010;
Gillenwater et al, 2010; Headden et al, 2009).
Lexicalization
We lexicalize only in the second (full data) stage, us-
ing the method of Headden et al (2009). For words
seen at least 100 times in the training corpus, we
augment their gold POS tag with the lexical item.
The first (data poor) stage remains entirely unlexi-
calized, with gold POS tags for word classes, as in
the earlier systems (Klein and Manning, 2004).
Smoothing
We do not use smoothing in the second stage except
at the end, for the final lexicalized model. Stage one
still applies ?add-one? smoothing at every iteration.
7 Experiment #3: State-of-the-Art
The purpose of these experiments is to compare the
punctuation-enhanced DMV with other, recent state-
of-the-art systems. We find that, lexicalized (?6), our
approach performs better, by a wide margin; without
lexicalization (?3.1), it was already better for longer,
but not for shorter, sentences (see Tables 5 and 4).
We trained a variant of our system without gold
part-of-speech tags, using the unsupervised word
clusters (Clark, 2000) computed by Finkel and Man-
ning (2009).6 Accuracy decreased slightly, to 58.2%
on Section 23 of WSJ (down only 0.2%). This result
improves over substantial performance degradations
previously observed for unsupervised dependency
parsing with induced word categories (Klein and
Manning, 2004; Headden et al, 2008, inter alia).
6Available from http://nlp.stanford.edu/software/
stanford-postagger-2008-09-28.tar.gz:
models/egw.bnc.200
Brown WSJ? WSJ10
(Headden et al, 2009) ? ? 68.8
(Spitkovsky et al, 2010b) 53.3 50.4 69.3
(Gillenwater et al, 2010) ? 53.3 64.3
(Blunsom and Cohn, 2010) ? 55.7 67.7
Constrained Training 58.4 58.0 69.3
w/Constrained Inference 59.5 58.4 69.5
Table 5: Accuracies on the out-of-domain Brown100 set
and Section 23 of WSJ? and WSJ10, for the lexicalized
punctuation run and other recent state-of-the-art systems.
24
Unlexicalized, Unpunctuated Lexicalized ... and Punctuated
CoNLL Year Initialization @15 Training @15 Retraining @45 Retraining @45 Net
& Language 1. w/Inference 2. w/Inference 3. w/Inference 3?. w/Inference Gain
Arabic 2006 23.3 23.6 (+0.3) 32.8 33.1 (+0.4) 31.5 31.6 (+0.1) 32.1 32.6 (+0.5) +1.1
?7 25.6 26.4 (+0.8) 33.7 34.2 (+0.5) 32.7 33.6 (+0.9) 34.9 35.3 (+0.4) +2.6
Basque ?7 19.3 20.8 (+1.5) 29.9 30.9 (+1.0) 29.3 30.1 (+0.8) 29.3 29.9 (+0.6) +0.6
Bulgarian ?6 23.7 24.7 (+1.0) 39.3 40.7 (+1.4) 38.8 39.9 (+1.1) 39.9 40.5 (+0.6) +1.6
Catalan ?7 33.2 34.1 (+0.8) 54.8 55.5 (+0.7) 54.3 55.1 (+0.8) 54.3 55.2 (+0.9) +0.9
Czech ?6 18.6 19.6 (+1.0) 34.6 35.8 (+1.2) 34.8 35.7 (+0.9) 37.0 37.8 (+0.8) +3.0
?7 17.6 18.4 (+0.8) 33.5 35.4 (+1.9) 33.4 34.4 (+1.0) 35.2 36.2 (+1.0) +2.7
Danish ?6 22.9 24.0 (+1.1) 35.6 36.7 (+1.2) 36.9 37.8 (+0.9) 36.5 37.1 (+0.6) +0.2
Dutch ?6 15.8 16.5 (+0.7) 11.2 12.5 (+1.3) 11.0 11.9 (+1.0) 13.7 14.0 (+0.3) +3.0
English ?7 25.0 25.4 (+0.5) 47.2 49.5 (+2.3) 47.5 48.8 (+1.3) 49.3 50.3 (+0.9) +2.8
German ?6 19.2 19.6 (+0.4) 27.4 28.0 (+0.7) 27.0 27.8 (+0.8) 28.2 28.6 (+0.4) +1.6
Greek ?7 18.5 18.8 (+0.3) 20.7 21.4 (+0.7) 20.5 21.0 (+0.5) 20.9 21.2 (+0.3) +0.7
Hungarian ?7 17.4 17.7 (+0.3) 6.7 7.2 (+0.5) 6.6 7.0 (+0.4) 7.8 8.0 (+0.2) +1.4
Italian ?7 25.0 26.3 (+1.2) 29.6 29.9 (+0.3) 29.7 29.7 (+0.1) 28.3 28.8 (+0.5) -0.8
Japanese ?6 30.0 30.0 (+0.0) 27.3 27.3 (+0.0) 27.4 27.4 (+0.0) 27.5 27.5 (+0.0) +0.1
Portuguese ?6 27.3 27.5 (+0.2) 32.8 33.7 (+0.9) 32.7 33.4 (+0.7) 33.3 33.5 (+0.3) +0.8
Slovenian ?6 21.8 21.9 (+0.2) 28.3 30.4 (+2.1) 28.4 30.4 (+2.0) 29.8 31.2 (+1.4) +2.8
Spanish ?6 25.3 26.2 (+0.9) 31.7 32.4 (+0.7) 31.6 32.3 (+0.8) 31.9 32.3 (+0.5) +0.8
Swedish ?6 31.0 31.5 (+0.6) 44.1 45.2 (+1.1) 45.6 46.1 (+0.5) 46.1 46.4 (+0.3) +0.8
Turkish ?6 22.3 22.9 (+0.6) 39.1 39.5 (+0.4) 39.9 39.9 (+0.1) 40.6 40.9 (+0.3) +1.0
?7 22.7 23.3 (+0.6) 41.7 42.3 (+0.6) 41.9 42.1 (+0.2) 41.6 42.0 (+0.4) +0.1
Average: 23.4 24.0 (+0.7) 31.9 32.9 (+1.0) 31.9 32.6 (+0.7) 32.6 33.2 (+0.5) +1.3
Table 6: Multi-lingual evaluation for CoNLL sets, measured at all three stages of training, with and without constraints.
8 Experiment #4: Multi-Lingual Testing
This final batch of experiments probes the general-
ization of our approach (?6) across languages. The
data are from 2006/7 CoNLL shared tasks (Buch-
holz and Marsi, 2006; Nivre et al, 2007a), where
punctuation was identified by the organizers, who
also furnished disjoint train/test splits. We tested
against all sentences in their evaluation sets.7,8
The gains are not English-specific (see Table 6).
Every language improves with constrained decod-
ing (more so without constrained training); and all
but Italian benefit in combination. Averaged across
all eighteen languages, the net change in accuracy is
1.3%. After standard training, constrained decoding
alone delivers a 0.7% gain, on average, never caus-
ing harm in any of our experiments. These gains are
statistically significant: p ? 1.59 ? 10?5 for con-
strained training; and p ? 4.27?10?7 for inference.
7With the exception of Arabic ?07, from which we discarded
one sentence with 145 tokens. We down-weighed languages
appearing in both years by 50% in our analyses, and excluded
Chinese entirely, since it had already been cut up at punctuation.
8Note that punctuation was treated differently in the two
years: in ?06, it was always at the leaves of the dependency
trees; in ?07, it matched original annotations of the source tree-
banks. For both, we used punctuation-insensitive scoring (?3.2).
We did not detect synergy between the two im-
provements. However, note that without constrained
training, ?full? data sets do not help, on average, de-
spite having more data and lexicalization. Further-
more, after constrained training, we detected no ev-
idence of benefits to additional retraining: not with
the relaxed sprawl constraint, nor unconstrained.
9 Related Work
Punctuation has been used to improve parsing since
rule-based systems (Jones, 1994). Statistical parsers
reap dramatic gains from punctuation (Engel et al,
2002; Roark, 2001; Charniak, 2000; Johnson, 1998;
Collins, 1997, inter alia). And it is even known to
help in unsupervised constituent parsing (Seginer,
2007). But for dependency grammar induction, until
now, punctuation remained unexploited.
Parsing Techniques Most-Similar to Constraints
A ?divide-and-rule? strategy that relies on punctua-
tion has been used in supervised constituent parsing
of long Chinese sentences (Li et al, 2005). For En-
glish, there has been interest in balanced punctua-
tion (Briscoe, 1994), more recently using rule-based
filters (White and Rajkumar, 2008) in a combinatory
categorial grammar (CCG). Our focus is specifically
25
on unsupervised learning of dependency grammars
and is similar, in spirit, to Eisner and Smith?s (2005)
?vine grammar? formalism. An important difference
is that instead of imposing static limits on allowed
dependency lengths, our restrictions are dynamic ?
they disallow some long (and some short) arcs that
would have otherwise crossed nearby punctuation.
Incorporating partial bracketings into grammar
induction is an idea tracing back to Pereira and Sch-
abes (1992). It inspired Spitkovsky et al (2010b) to
mine parsing constraints from the web. In that same
vein, we prospected a more abundant and natural
language-resource ? punctuation, using constraint-
based techniques they developed for web markup.
Modern Unsupervised Dependency Parsing
State-of-the-art in unsupervised dependency pars-
ing (Blunsom and Cohn, 2010) uses tree substitu-
tion grammars. These are powerful models, capa-
ble of learning large dependency fragments. To help
prevent overfitting, a non-parametric Bayesian prior,
defined by a hierarchical Pitman-Yor process (Pit-
man and Yor, 1997), is trusted to nudge training to-
wards fewer and smaller grammatical productions.
We pursued a complementary strategy: using
Klein and Manning?s (2004) much simpler Depen-
dency Model with Valence (DMV), but persistently
steering training away from certain constructions, as
guided by punctuation, to help prevent underfitting.
Various Other Uses of Punctuation in NLP
Punctuation is hard to predict,9 partly because it
can signal long-range dependences (Lu and Ng,
2010). It often provides valuable cues to NLP tasks
such as part-of-speech tagging and named-entity
recognition (Hillard et al, 2006), information ex-
traction (Favre et al, 2008) and machine transla-
tion (Lee et al, 2006; Matusov et al, 2006). Other
applications have included Japanese sentence anal-
ysis (Ohyama et al, 1986), genre detection (Sta-
matatos et al, 2000), bilingual sentence align-
ment (Yeh, 2003), semantic role labeling (Pradhan et
al., 2005), Chinese creation-title recognition (Chen
and Chen, 2005) and word segmentation (Li and
Sun, 2009), plus, recently, automatic vandalism de-
9Punctuation has high semantic entropy (Melamed, 1997);
for an analysis of the many roles played in the WSJ by the
comma ? the most frequent and unpredictable punctuation
mark in that data set ? see Beeferman et al (1998, Table 2).
tection in Wikipedia (Wang and McKeown, 2010).
10 Conclusions and Future Work
Punctuation improves dependency grammar induc-
tion. Many unsupervised (and supervised) parsers
could be easily modified to use sprawl-constrained
decoding in inference. It applies to pre-trained mod-
els and, so far, helped every data set and language.
Tightly interwoven into the fabric of writing sys-
tems, punctuation frames most unannotated plain-
text. We showed that rules for converting markup
into accurate parsing constraints are still optimal for
inter-punctuation fragments. Punctuation marks are
more ubiquitous and natural than web markup: what
little punctuation-induced constraints lack in preci-
sion, they more than make up in recall ? perhaps
both types of constraints would work better yet in
tandem. For language acquisition, a natural ques-
tion is whether prosody could similarly aid grammar
induction from speech (Kahn et al, 2005).
Our results underscore the power of simple mod-
els and algorithms, combined with common-sense
constraints. They reinforce insights from joint mod-
eling in supervised learning, where simplified, in-
dependent models, Viterbi decoding and expressive
constraints excel at sequence labeling tasks (Roth
and Yih, 2005). Such evidence is particularly wel-
come in unsupervised settings (Punyakanok et al,
2005), where it is crucial that systems scale grace-
fully to volumes of data, on top of the usual desider-
ata ? ease of implementation, extension, under-
standing and debugging. Future work could explore
softening constraints (Hayes and Mouradian, 1980;
Chang et al, 2007), perhaps using features (Eisner
and Smith, 2005; Berg-Kirkpatrick et al, 2010) or
by learning to associate different settings with var-
ious marks: Simply adding a hidden tag for ?ordi-
nary? versus ?divide? types of punctuation (Li et al,
2005) may already usefully extend our model.
Acknowledgments
Partially funded by the Air Force Research Laboratory (AFRL),
under prime contract no. FA8750-09-C-0181, and by NSF, via
award #IIS-0811974. We thank Omri Abend, Slav Petrov and
anonymous reviewers for many helpful suggestions, and we are
especially grateful to Jenny R. Finkel for shaming us into using
punctuation, to Christopher D. Manning for reminding us to ex-
plore ?punctuation as words? baselines, and to Noah A. Smith
for encouraging us to test against languages other than English.
26
References
D. Beeferman, A. Berger, and J. Lafferty. 1998.
CYBERPUNC: A lightweight punctuation annotation
system for speech. In ICASSP.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In NAACL-HLT.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
In EMNLP.
E. J. Briscoe. 1994. Parsing (with) punctuation, etc.
Technical report, Xerox European Research Labora-
tory.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. Technical report, Brown University.
M.-W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
ACL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL.
C. Chen and H.-H. Chen. 2005. Integrating punctuation
rules and na??ve Bayesian model for Chinese creation
title recognition. In IJCNLP.
A. Clark. 2000. Inducing syntactic categories by context
distribution clustering. In CoNLL-LLL.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
C. D. Doran. 1998. Incorporating Punctuation into
the Sentence Grammar: A Lexicalized Tree Adjoin-
ing Grammar Perspective. Ph.D. thesis, University of
Pennsylvania.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexi-
cal context-free grammars and head-automaton gram-
mars. In ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft and
hard constraints on dependency length. In IWPT.
D. Engel, E. Charniak, and M. Johnson. 2002. Parsing
and disfluency placement. In EMNLP.
B. Favre, R. Grishman, D. Hillard, H. Ji, D. Hakkani-
Tu?r, and M. Ostendorf. 2008. Punctuating speech for
information extraction. In ICASSP.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In NAACL-HLT.
W. N. Francis and H. Kuc?era, 1979. Manual of Informa-
tion to Accompany a Standard Corpus of Present-Day
Edited American English, for use with Digital Com-
puters. Department of Linguistics, Brown University.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Technical report, University of
Pennsylvania.
A. Gravano, M. Jansche, and M. Bacchiani. 2009.
Restoring punctuation and capitalization in transcribed
speech. In ICASSP.
P. J. Hayes and G. V. Mouradian. 1980. Flexible parsing.
In ACL.
W. P. Headden, III, D. McClosky, and E. Charniak.
2008. Evaluating unsupervised part-of-speech tagging
for grammar induction. In COLING.
W. P. Headden, III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In NAACL-HLT.
D. Hillard, Z. Huang, H. Ji, R. Grishman, D. Hakkani-
Tu?r, M. Harper, M. Ostendorf, and W. Wang. 2006.
Impact of automatic comma prediction on POS/name
tagging of speech. In IEEE/ACL: SLT.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24.
B. E. M. Jones. 1994. Exploring the role of punctuation
in parsing natural text. COLING.
J. G. Kahn, M. Lease, E. Charniak, M. Johnson, and
M. Ostendorf. 2005. Effective use of prosody in pars-
ing conversational speech. In HLT-EMNLP.
J.-H. Kim and P. C. Woodland. 2002. Implementation of
automatic capitalisation generation systems for speech
input. In ICASSP.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
Y.-S. Lee, S. Roukos, Y. Al-Onaizan, and K. Papineni.
2006. IBM spoken language translation system. In
TC-STAR: Speech-to-Speech Translation.
Z. Li and M. Sun. 2009. Punctuation as implicit annota-
tions for Chinese word segmentation. Computational
Linguistics, 35.
X. Li, C. Zong, and R. Hu. 2005. A hierarchical parsing
approach with punctuation processing for long Chi-
nese sentences. In IJCNLP.
D. Lin. 1998. Dependency-based evaluation of MINI-
PAR. In Evaluation of Parsing Systems.
W. Lu and H. T. Ng. 2010. Better punctuation prediction
with dynamic conditional random fields. In EMNLP.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
E. Matusov, A. Mauser, and H. Ney. 2006. Automatic
sentence segmentation and punctuation prediction for
spoken language translation. In IWSLT.
I. D. Melamed. 1997. Measuring semantic entropy. In
ACL-SIGLEX: Tagging Text with Lexical Semantics.
27
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007a. The CoNLL
2007 shared task on dependency parsing. In EMNLP-
CoNLL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryig?it,
S. Ku?bler, S. Marinov, and E. Marsi. 2007b. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13.
G. Nunberg. 1990. The Linguistics of Punctuation.
CSLI Publications.
Y. Ohyama, T. Fukushima, T. Shutoh, and M. Shutoh.
1986. A sentence analysis method for a Japanese book
reading machine for the blind. In ACL.
M. A. Paskin. 2001. Grammatical bigrams. In NIPS.
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In ACL.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25.
S. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and
D. Jurafsky. 2005. Semantic role chunking combin-
ing complementary syntactic views. In CoNLL.
V. Punyakanok, D. Roth, W.-t. Yih, and D. Zimak. 2005.
Learning and inference over constrained output. In IJ-
CAI.
B. E. Roark. 2001. Robust Probabilistic Predictive Syn-
tactic Processing: Motivations, Models, and Applica-
tions. Ph.D. thesis, Brown University.
D. Roth and W.-t. Yih. 2005. Integer linear programming
inference for conditional random fields. In ICML.
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In ACL.
D. D. Sleator and D. Temperley. 1993. Parsing English
with a link grammar. In IWPT.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010a. Viterbi training improves unsupervised
dependency parsing. In CoNLL.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010b.
Profiting from mark-up: Hyper-text annotations for
guided parsing. In ACL.
E. Stamatatos, N. Fakotakis, and G. Kokkinakis. 2000.
Text genre detection using common word frequencies.
In COLING.
W. Y. Wang and K. R. McKeown. 2010. ?Got you!?: Au-
tomatic vandalism detection in Wikipedia with web-
based shallow syntactic-semantic modeling. In COL-
ING.
M. White and R. Rajkumar. 2008. A more precise analy-
sis of punctuation for broad-coverage surface realiza-
tion with CCG. In GEAF.
K. C. Yeh. 2003. Bilingual sentence alignment based on
punctuation marks. In ROCLING: Student.
28
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 16?22,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Capitalization Cues Improve Dependency Grammar Induction
Valentin I. Spitkovsky
Stanford University and Google Inc.
valentin@cs.stanford.edu
Hiyan Alshawi
Google Inc., Mountain View, CA, 94043
hiyan@google.com
Daniel Jurafsky
Stanford University, Stanford, CA, 94305
jurafsky@stanford.edu
Abstract
We show that orthographic cues can be helpful
for unsupervised parsing. In the Penn Tree-
bank, transitions between upper- and lower-
case tokens tend to align with the boundaries
of base (English) noun phrases. Such signals
can be used as partial bracketing constraints to
train a grammar inducer: in our experiments,
directed dependency accuracy increased by
2.2% (average over 14 languages having case
information). Combining capitalization with
punctuation-induced constraints in inference
further improved parsing performance, attain-
ing state-of-the-art levels for many languages.
1 Introduction
Dependency grammar induction and related prob-
lems of unsupervised syntactic structure discovery
are attracting increasing attention (Rasooli and Faili,
2012; Marec?ek and Zabokrtsky?, 2011, inter alia).
Since sentence structure is underdetermined by raw
text, there have been efforts to simplify the task, via
(i) pooling features of syntax across languages (Co-
hen et al, 2011; McDonald et al, 2011; Cohen
and Smith, 2009); as well as (ii) identifying uni-
versal rules (Naseem et al, 2010) ? such as verbo-
centricity (Gimpel and Smith, 2011) ? that need not
be learned at all. Unfortunately most of these tech-
niques do not apply to plain text, because they re-
quire knowing, for example, which words are verbs.
As standard practice shifts away from relying on
gold part-of-speech (POS) tags (Seginer, 2007; Pon-
vert et al, 2010; S?gaard, 2011b; Spitkovsky et al,
2011c, inter alia), lighter cues to inducing linguistic
structure become more important. Examples of use-
ful POS-agnostic clues include punctuation bound-
aries (Ponvert et al, 2011; Spitkovsky et al, 2011b;
Briscoe, 1994) and various kinds of bracketing con-
straints (Naseem and Barzilay, 2011; Spitkovsky et
al., 2010b; Pereira and Schabes, 1992). We propose
adding capitalization to this growing list of sources
of partial bracketings. Our intuition stems from En-
glish, where (maximal) spans of capitalized words
? such as Apple II, World War I, Mayor William H.
Hudnut III, International Business Machines Corp. and
Alexandria, Va ? tend to demarcate proper nouns.
Consider a motivating example (all of our exam-
ples are from WSJ) without punctuation, in which all
(eight) capitalized word clumps and uncased numer-
als match base noun phrase constituent boundaries:
[NP Jay Stevens] of [NP Dean Witter] actually cut his
per-share earnings estimate to [NP $9] from [NP $9.50]
for [NP 1989] and to [NP $9.50] from [NP $10.35]
in [NP 1990] because he decided sales would be even
weaker than he had expected.
and another (whose first word happens to be a leaf),
where capitalization complements punctuation cues:
[NP Jurors] in [NP U.S. District Court] in [NP Miami]
cleared [NP Harold Hershhenson], a former executive
vice president; [NP John Pagones], a former vice presi-
dent; and [NP Stephen Vadas] and [NP Dean Ciporkin],
who had been engineers with [NP Cordis].
Could such chunks help bootstrap grammar induc-
tion and/or improve the accuracy of already-trained
unsupervised parsers? In answering these questions,
we will focus predominantly on sentence-internal
capitalization. But we will also show that first words
? those capitalized by convention ? and uncased
segments ? whose characters are not even drawn
from an alphabet ? could play a useful role as well.
2 English Capitalization from a Treebank
We began our study by consulting the 51,558 parsed
sentences of the WSJ corpus (Marcus et al, 1993):
30,691 (59.5%) of them contain non-trivially capi-
talized fragments ? maximal (non-empty and not
16
Count POS Sequence Frac Cum
1 27,524 NNP 44.6%
2 17,222 NNP NNP 27.9 72.5
3 4,598 NNP NNP NNP 7.5 79.9
4 2,973 JJ 4.8 84.8
5 1,716 NNP NNP NNP NNP 2.8 87.5
6 1,037 NN 1.7 89.2
7 932 PRP 1.5 90.7
8 846 NNPS 1.4 92.1
9 604 NNP NNPS 1.0 93.1
10 526 NNP NNP NNP NNP NNP 0.9 93.9
WSJ +3,753 more with Count ? 498 6.1%
Table 1: Top 10 fragments of POS tag sequences in WSJ.
sentence-initial) consecutive sequences of words
that each differs from its own lower-cased form.
Nearly all ? 59,388 (96.2%) ? of the 61,731 frag-
ments are dominated by noun phrases; slightly less
than half ? 27,005 (43.8%) ? perfectly align with
constituent boundaries in the treebank; and about as
many ? 27,230 (44.1%) are multi-token. Table 1
shows the top POS sequences comprising fragments.
3 Analytical Experiments with Gold Trees
We gauged the suitability of capitalization-induced
fragments for guiding dependency grammar induc-
tion by assessing accuracy, in WSJ,1 of parsing con-
straints derived from their end-points. Following the
suite of increasingly-restrictive constraints on how
dependencies may interact with fragments, intro-
duced by Spitkovsky et al (2011b, ?2.2), we tested
several such heuristics. The most lenient constraint,
thread, only asks that no dependency path from the
root to a leaf enter the fragment twice; tear requires
any incoming arcs to come from the same side of
the fragment; sprawl demands that there be exactly
one incoming arc; loose further constrains any out-
going arcs to be from the fragment?s head; and strict
? the most stringent constraint ? bans external
dependents. Since only strict is binding for single
words, we experimented also with strict?: applying
strict solely to multi-token fragments (ignoring sin-
gletons). In sum, we explored six ways in which
dependency parse trees can be constrained by frag-
ments whose end-points could be defined by capital-
ization (or in other various ways, e.g., semantic an-
1We converted labeled constituents into unlabeled depen-
dencies using deterministic ?head-percolation? rules (Collins,
1999), discarding any empty nodes, etc., as is standard practice.
markup punct. capital initial uncased
thread 98.5 95.0 99.5 98.4 99.2
tear 97.9 94.7 98.6 98.4 98.5
sprawl 95.1 92.9 98.2 97.9 96.4
loose 87.5 74.0 97.9 96.9 96.4
strict? 32.7 35.6 38.7 40.3 55.6
strict 35.6 39.2 59.3 66.9 61.1
Table 2: Several sources of fragments? end-points and
%-correctness of their derived constraints (for English).
notations, punctuation or HTML tags in web pages).
For example, in the sentence about Cordis, the
strict hypothesis would be wrong about five of the
eight fragments: Jurors attaches in; Court takes the
second in; Hershhenson and Pagones derive their ti-
tles, president; and (at least in our reference) Vadas
attaches and, Ciporkin and who. Based on this, we
would consider strict to be 37.5%-accurate. But
loose ? and the rest of the more relaxed constraints
? would get perfect scores. (And strict? would re-
tract the mistake about Jurors but also the correct
guesses about Miami and Cordis, scoring only 20%.)
Table 2 (capital) shows scores averaged over the
entire treebank. Columns markup (Spitkovsky et al,
2010b) and punct (Spitkovsky et al, 2011b) indicate
that capitalization yields across-the-board more ac-
curate constraints (for English) compared with frag-
ments derived from punctuation or markup (i.e., an-
chor text, bold, italics and underline tags in HTML),
for which such constraints were originally intended.
4 Pilot Experiments on Supervised Parsing
To further test the potential of capitalization-induced
constraints, we applied them in the Viterbi-decoding
phase of a simple (unlexicalized) supervised depen-
dency parser ? an instance of DBM-1 (Spitkovsky
et al, 2012, ?2.1), trained on WSJ sentences with up
punct.: thread tear sprawl loose
none: 71.8 74.3 74.4 74.5 73.3
capital:thread 72.3 74.6 74.7 74.9 73.6
tear 72.4 74.7 74.7 74.9 73.6
sprawl 72.4 74.7 74.7 74.9 73.4
loose 72.4 74.8 74.7 74.9 73.3
strict? 71.4 73.7 73.7 73.9 72.7
strict 71.0 73.1 73.1 73.2 72.1
Table 3: Supervised (directed) accuracy on Section 23
of WSJ using capitalization-induced constraints (vertical)
jointly with punctuation (horizontal) in Viterbi-decoding.
17
CoNLL Year Filtered Training Directed Accuracies with Initial Constraints Fragments
& Language Tokens / Sentences none thread tear sprawl loose strict? strict Multi Single
German 2006 139,333 12,296 36.3 36.3 36.3 39.1 36.2 36.3 30.1 3,287 30,435
Czech ?6 187,505 20,378 51.3 51.3 51.3 51.3 52.5 52.5 51.4 1,831 6,722
English ?7 74,023 5,087 29.2 28.5 28.3 29.0 29.3 28.3 27.7 1,135 2,218
Bulgarian ?6 46,599 5,241 59.4 59.3 59.3 59.4 59.1 59.3 59.5 184 1,506
Danish ?6 14,150 1,599 21.3 17.7 22.7 21.5 21.4 31.4 27.9 113 317
Greek ?7 11,943 842 28.1 46.1 46.3 46.3 46.4 31.1 31.0 113 456
Dutch ?6 72,043 7,107 45.9 45.8 45.9 45.8 45.8 45.7 29.6 89 4,335
Italian ?7 9,142 921 41.7 52.6 52.7 52.6 44.2 52.6 45.8 41 296
Catalan ?7 62,811 4,082 61.3 61.3 61.3 61.3 61.3 61.3 36.5 28 2,828
Turkish ?6 17,610 2,835 32.9 32.9 32.2 33.0 33.0 33.6 33.9 27 590
Portuguese ?6 24,494 2,042 68.9 67.1 69.1 69.2 68.9 68.9 38.5 9 953
Hungarian ?7 10,343 1,258 43.2 43.2 43.1 43.2 43.2 43.7 25.5 7 277
Swedish ?6 41,918 4,105 48.6 48.6 48.6 48.5 48.5 48.5 48.8 3 296
Slovenian ?6 3,627 477 30.4 30.5 30.5 30.4 30.5 30.5 30.8 1 63
Median: 42.5 46.0 46.1 46.0 45.0 44.7 32.5
Mean: 42.8 44.4 44.8 45.0 44.3 44.6 36.9
Table 4: Parsing performance for grammar inducers trained with capitalization-based initial constraints, tested against
14 held-out sets from 2006/7 CoNLL shared tasks, and ordered by number of multi-token fragments in training data.
to 45 words (excluding Section 23). Table 3 shows
evaluation results on held-out data (all sentences),
using ?add-one? smoothing. All constraints other
than strict improve accuracy by about a half-a-point,
from 71.8 to 72.4%, suggesting that capitalization
is informative of certain regularities not captured by
DBM grammars; moreover, it still continues to be
useful when punctuation-based constraints are also
enforced, boosting accuracy from 74.5 to 74.9%.
5 Multi-Lingual Grammar Induction
So far, we showed only that capitalization informa-
tion can be helpful in parsing a very specific genre
of English. Next, we tested its ability to generally
aid dependency grammar induction, focusing on sit-
uations when other bracketing cues are unavailable.
We experimented with 14 languages from 2006/7
CoNLL shared tasks (Buchholz and Marsi, 2006;
Nivre et al, 2007), excluding Arabic, Chinese and
Japanese (which lack case), as well as Basque and
Spanish (which are pre-processed in a way that loses
relevant capitalization information). For all remain-
ing languages we trained only on simple sentences
? those lacking sentence-internal punctuation ?
from the relevant training sets (for blind evaluation).
Restricting our attention to a subset of the avail-
able training data serves a dual purpose. First, it al-
lows us to estimate capitalization?s impact where no
other (known or obvious) cues could also be used.
Otherwise, unconstrained baselines would not yield
the strongest possible alternative, and hence not the
most interesting comparison. Second, to the extent
that presence of punctuation may correlate with sen-
tence complexity (Frank, 2000), there are benefits to
?starting small? (Elman, 1993): e.g., relegating full
data to later stages helps training (Spitkovsky et al,
2010a; Cohn et al, 2011; Tu and Honavar, 2011).
Our base systems induced DBM-1, starting from
uniformly-at-random chosen parse trees (Cohen and
Smith, 2010) of each sentence, followed by inside-
outside re-estimation (Baker, 1979) with ?add-one?
smoothing.2 Capitalization-constrained systems dif-
fered from controls in exactly one way: each learner
got a slight nudge towards more promising struc-
tures by choosing initial seed trees satisfying an ap-
propriate constraint (but otherwise still uniformly).
Table 4 contains the stats for all 14 training sets,
ordered by number of multi-token fragments. Fi-
nal accuracies on respective (disjoint, full) evalua-
tion sets are improved by all constraints other than
strict, with the highest average performance result-
ing from sprawl: 45.0% directed dependency accu-
racy,3 on average. This increase of about two points
over the base system?s 42.8% is driven primarily by
improvements in two languages (Greek and Italian).
2We used ?early-stopping lateen EM? (Spitkovsky et al,
2011a, ?2.3) instead of thresholding or waiting for convergence.
3Starting from five parse trees for each sentence (using con-
straints thread through strict?) was no better, at 44.8% accuracy.
18
6 Capitalizing on Punctuation in Inference
Until now we avoided using punctuation in grammar
induction, except to filter data. Yet our pilot exper-
iments indicated that both kinds of information are
helpful in the decoding stage of a supervised system.
We took trained models obtained using the sprawl
nudge (from ?5) and proceeded to again apply con-
straints in inference (as in ?4). Capitalization alone
increased parsing accuracy only slightly, from 45.0
to 45.1%, on average. Using punctuation constraints
instead led to more improved performance: 46.5%.
Combining both types of constraints again resulted
in slightly higher accuracies: 46.7%. Table 5 breaks
down our last average performance number by lan-
guage and shows the combined approach to be com-
petitive with state-of-the-art. We suspect that further
improvements could be attained by also incorporat-
ing both constraints in training and with full data.
7 Discussion and A Few Post-Hoc Analyses
Our discussion, thus far, has been English-centric.
Nevertheless, languages differ in how they use capi-
talization (and even the rules governing a given lan-
guage tend to change over time ? generally towards
having fewer capitalized terms). For instance, adjec-
tives derived from proper nouns are not capitalized
in French, German, Polish, Spanish or Swedish, un-
like in English (see Table 1: JJ). And while English
forces capitalization of the first-person pronoun in
the nominative case, I (see Table 1: PRP), in Danish
it is the plural second-person pronoun (also I) that
is capitalized; further, formal pronouns (and their
case-forms) are capitalized in German (Sie and Ihre,
Ihres...), Italian, Slovenian, Russian and Bulgarian.
In contrast to pronouns, single-word proper nouns
? including personal names ? are capitalized in
nearly all European languages. Such shortest brack-
etings are not particularly useful for constraining
sets of possible parse trees in grammar induction,
however, compared to multi-word expressions; from
this perspective, German appears less helpful than
most cased languages, because of noun compound-
ing, despite prescribing capitalization of all nouns.
Another problem with longer word-strings in many
languages is that, e.g., in French (as in English)
lower-case prepositions may be mixed in with con-
tiguous groups of proper nouns: even in surnames,
CoNLL Year this State-of-the-Art Systems: POS-
& Language Work (i) Agnostic (ii) Identified
Bulgarian 2006 64.5 44.3 SCAJ5 70.3 Spt
Catalan ?7 61.5 63.8 SCAJ5 56.3 MZNR
Czech ?6 53.5 50.5 SCAJ5 33.3? MZNR
Danish ?6 20.6 46.0 RF 56.5 Sar
Dutch ?6 46.7 32.5 SCAJ5 62.1 MPHel
English ?7 29.2 50.3 SAJ 45.7 MPHel
German ?6 42.6 33.5 SCAJ5 55.8 MPHnl
Greek ?7 49.3 39.0 MZ 63.9 MPHen
Hungarian ?7 53.7 48.0 MZ 48.1 MZNR
Italian ?7 50.5 57.5 MZ 69.1 MPHpt
Portuguese ?6 72.4 43.2 MZ 76.9 Sbg
Slovenian ?6 34.8 33.6 SCAJ5 34.6 MZNR
Swedish ?6 50.5 50.0 SCAJ6 66.8 MPHpt
Turkish ?6 34.4 40.9 SAJ 61.3 RFH1
Median: 48.5 45.2 58.9
Mean: 46.7 45.2 57.2?
Table 5: Unsupervised parsing with both capitalization-
and punctuation-induced constraints in inference, tested
against the 14 held-out sets from 2006/7 CoNLL shared
tasks, and state-of-the-art results (all sentence lengths) for
systems that: (i) are also POS-agnostic and monolingual,
including SCAJ (Spitkovsky et al, 2011a, Tables 5?6)
and SAJ (Spitkovsky et al, 2011b); and (ii) rely on gold
POS-tag identities to (a) discourage noun roots (Marec?ek
and Zabokrtsky?, 2011, MZ), (b) encourage verbs (Ra-
sooli and Faili, 2012, RF), or (c) transfer delexicalized
parsers (S?gaard, 2011a, S) from resource-rich languages
with parallel translations (McDonald et al, 2011, MPH).
the German particle von is not capitalized, although
the Dutch van is, unless preceded by a given name or
initial ? hence Van Gogh, yet Vincent van Gogh.
7.1 Constraint Accuracies Across Languages
Since even related languages (e.g., Flemish, Dutch,
German and English) can have quite different con-
ventions regarding capitalization, one would not ex-
pect the same simple strategy to be uniformly useful
? or useful in the same way ? across disparate lan-
guages. To get a better sense of how universal our
constraints may be, we tabulated their accuracies for
the full training sets of the CoNLL data, after all
grammar induction experiments had been executed.
Table 6 shows that the less-strict capitalization-
induced constraints all fall within narrow (yet high)
bands of accuracies of just a few percentage points:
99?100% in the case of thread, 98?100% for tear,
95?99% for sprawl and 94?99% for loose. By con-
trast, the ranges for punctuation-induced constraints
are all at least 10%. We do not see anything partic-
19
CoNLL Year Total Training Capitalization-Induced Constraints Punctuation-Induced Constraints
& Language Tokens / Sentences thr-d tear spr-l loose str.? strict thr-d tear spr-l loose str.? strict
Arabic 2006 52,752 1,460 ? ? ? ? ? ? 89.6 89.5 81.9 61.2 29.7 33.4
?7 102,375 2,912 ? ? ? ? ? ? 90.9 90.6 83.1 61.2 29.5 35.2
Basque ?7 41,013 3,190 ? ? ? ? ? ? 96.2 95.7 92.3 81.9 42.8 50.6
Bulgarian ?6 162,985 12,823 99.8 99.5 96.6 96.4 51.8 81.0 97.6 97.2 96.1 74.7 36.7 41.2
Catalan ?7 380,525 14,958 100 99.5 95.0 94.6 15.8 57.9 96.1 95.5 94.6 73.7 36.0 42.6
Chinese ?6 337,162 56,957 ? ? ? ? ? ? ? ? ? ? ? ?
?7 337,175 56,957 ? ? ? ? ? ? ? ? ? ? ? ?
Czech ?6 1,063,413 72,703 99.7 98.3 96.2 95.4 42.4 68.0 89.4 89.2 87.7 68.9 37.2 41.7
?7 368,624 25,364 99.7 98.3 96.1 95.4 42.6 67.6 89.5 89.3 87.8 69.3 37.4 41.9
Danish ?6 80,743 5,190 99.9 99.4 98.3 97.0 59.0 69.7 96.9 96.9 95.2 68.3 39.6 40.9
Dutch ?6 172,958 13,349 99.9 99.1 98.4 96.6 16.6 46.3 89.6 89.5 86.4 69.6 42.5 46.2
English ?7 395,139 18,577 99.3 98.7 98.0 96.0 17.5 24.8 91.5 91.4 90.6 76.5 39.6 42.3
German ?6 605,337 39,216 99.6 98.0 96.7 96.4 41.7 57.1 94.5 93.9 90.7 71.1 37.2 40.7
Greek ?7 58,766 2,705 99.9 99.3 98.5 96.6 13.6 50.1 91.3 91.0 89.8 75.7 43.7 47.0
Hungarian ?7 111,464 6,034 99.9 98.1 95.7 94.4 46.6 62.0 96.1 94.0 89.0 77.1 28.9 32.6
Italian ?7 60,653 3,110 99.9 99.6 99.0 98.8 12.8 68.2 97.1 96.8 96.0 77.8 44.7 47.9
Japanese ?6 133,927 17,044 ? ? ? ? ? ? 100 100 95.4 89.0 48.9 63.5
Portuguese ?6 177,581 9,071 100 99.0 97.6 97.0 14.4 37.7 96.0 95.8 94.9 74.5 40.3 45.0
Slovenian ?6 23,779 1,534 100 99.8 98.9 98.9 52.0 84.7 93.3 93.3 92.6 72.7 42.7 45.8
Spanish ?6 78,068 3,306 ? ? ? ? ? ? 96.5 96.0 95.2 75.4 33.4 40.9
Swedish ?6 163,301 11,042 99.8 99.6 99.0 97.0 24.7 58.4 90.8 90.4 87.4 66.8 31.1 33.9
Turkish ?6 48,373 4,997 100 99.8 96.2 94.0 22.8 42.8 99.8 99.7 95.1 76.9 37.7 42.0
?7 54,761 5,635 100 99.9 96.1 94.2 21.6 42.9 99.8 99.7 94.6 76.7 38.2 42.8
Max: 100 99.9 99.0 98.9 59.0 84.7 100 100 96.1 89.0 48.9 63.5
Mean: 99.8 99.1 97.4 96.4 30.8 57.7 94.6 94.2 91.7 74.0 38.5 43.3
Min: 99.3 98.0 95.0 94.0 12.8 24.8 89.4 89.2 81.9 61.2 28.9 32.6
Table 6: Accuracies for capitalization- and punctuation-induced constraints on all (full) 2006/7 CoNLL training sets.
ularly special about Greek or Italian in these sum-
maries that could explain their substantial improve-
ments (18 and 11%, respectively ? see Table 4),
though Italian does appear to mesh best with the
sprawl constraint (not by much, closely followed by
Swedish). And English ? the language from which
we drew our inspiration ? barely improved with
capitalization-induced constraints (see Table 4) and
caused the lowest accuracies of thread and strict.
These outcomes are not entirely surprising: some
best- and worst-performing results are due to noise,
since learning via non-convex optimization can be
chaotic: e.g., in the case of Greek, applying 113 con-
straints to initial parse trees could have a significant
impact on the first grammar estimated in training ?
and consequently also on a learner?s final, converged
model instance. We expect the averages (i.e., means
and medians) ? computed over many data sets ?
to be more stable and meaningful than the outliers.
7.2 Immediate Impact from Capitalization
Next, we considered two settings that are less af-
fected by training noise: grammar inducers immedi-
ately after an initial step of constrained Viterbi EM
and supervised DBM parsers (trained on sentences
with up to 45 words), for various languages in the
CoNLL sets. Table 7 shows effects of capitalization
to be exceedingly mild, both if applied alone and in
tandem with punctuation. Exploring better ways of
incorporating this informative resource ? perhaps
as soft features, rather than as hard constraints ?
and in combination with punctuation- and markup-
induced bracketings could be a fruitful direction.
7.3 Odds and Ends
Our earlier analysis excluded sentence-initial words
because their capitalization is, in a way, trivial. But
for completeness, we also tested constraints derived
from this source, separately (see Table 2: initials).
As expected, the new constraints scored worse (de-
spite many automatically-correct single-word frag-
ments) except for strict, whose binding constraints
over singletons drove up accuracy. It turns out, most
first words in WSJ are leaves ? possibly due to a
dearth of imperatives (or just English?s determiners).
We broadened our investigation of the ?first leaf?
20
CoNLL Year Evaluation Bracketings Unsupervised Training Supervised Parsing
& Language Tokens / Sents capital. punct. init. 1-step constrained none capital. punct. both
Arabic 2006 5,215 146 ? 101 18.4 20.6 ? ? 59.8 ? ? ?
?7 4,537 130 ? 311 19.0 23.5 ? ? 63.5 ? ? ?
Basque ?7 4,511 334 ? 547 17.4 22.4 ? ? 58.4 ? ? ?
Bulgarian ?6 5,032 398 44 552 19.4 28.9 28.4 -0.5 76.7 76.8 78.1 78.2
Catalan ?7 4,478 167 24 398 18.0 25.1 25.4 +0.3 78.1 78.3 78.6 78.9
Chinese ?6 5,012 867 ? ? 23.5 27.2 ? ? 83.7 ? ? ?
?7 5,161 690 ? ? 19.4 25.0 ? ? 81.0 ? ? ?
Czech ?6 5,000 365 48 549 18.6 19.7 19.8 +0.1 64.9 64.8 67.0 66.9
?7 4,029 286 57 466 18.0 21.7 ? ? 62.8 ? ? ?
Danish ?6 4,978 322 85 590 19.5 27.4 26.0 -1.3 71.9 72.0 74.2 74.3
Dutch ?6 4,989 386 28 318 18.7 17.9 17.7 -0.1 60.9 60.9 62.7 62.8
English ?7 4,386 214 151 423 17.6 24.0 21.9 -2.1 65.2 65.6 68.5 68.4
German ?6 4,886 357 135 523 16.4 23.0 23.7 +0.7 70.7 70.7 71.5 71.4
Greek ?7 4,307 197 47 372 17.1 17.1 16.6 -0.5 71.3 71.6 73.5 73.7
Hungarian ?7 6,090 390 28 893 17.1 18.5 18.6 +0.1 67.3 67.2 69.8 69.6
Italian ?7 4,360 249 71 505 18.6 32.5 34.2 +1.7 66.0 65.9 67.0 66.8
Japanese ?6 5,005 709 ? 0 26.5 36.8 ? ? 85.1 ? ? ?
Portuguese ?6 5,009 288 29 559 19.3 24.2 24.0 -0.1 80.5 80.5 81.6 81.6
Slovenian ?6 5,004 402 7 785 18.3 22.5 22.4 -0.1 67.5 67.4 70.9 70.9
Spanish ?6 4,991 206 ? 453 18.0 19.3 ? ? 69.5 ? ? ?
Swedish ?6 4,873 389 14 417 20.2 31.4 31.4 +0.0 74.9 74.9 74.7 74.6
Turkish ?6 6,288 623 18 683 20.4 26.4 26.7 +0.3 66.1 66.0 66.9 66.7
?7 3,983 300 4 305 20.3 24.8 ? ? 67.3 ? ? ?
Max: 20.4 32.5 34.2 +1.7 80.5 80.5 81.6 81.6
(aggregated as in Tables 4 and 5) Mean: 18.5 24.2 24.1 -0.1 70.1 70.2 71.8 71.8
Min: 16.4 17.1 16.6 -2.1 60.9 60.9 62.7 62.8
Table 7: Unsupervised accuracies for uniform-at-random projective parse trees (init), also after a step of Viterbi EM,
and supervised performance with induced constraints, on 2006/7 CoNLL evaluation sets (sentences under 145 tokens).
phenomenon and found that in 16 of the 19 CoNLL
languages first words are more likely to be leaves
than other words without dependents on the left;4
last words, by contrast, are more likely to take de-
pendents than expected. These propensities may be
related to the functional tendency of languages to
place old information before new (Ward and Birner,
2001) and could also help bias grammar induction.
Lastly, capitalization points to yet another class of
words: those with identical upper- and lower-case
forms. Their constraints too tend to be accurate (see
Table 2: uncased), but the underlying text is not par-
ticularly interesting. In WSJ, caseless multi-token
fragments are almost exclusively percentages (e.g.,
the two tokens of 10%), fractions (e.g., 1 1/4) or both.
Such boundaries could be useful in dealing with fi-
nancial data, as well as for breaking up text in lan-
guages without capitalization (e.g., Arabic, Chinese
4Arabic, Basque, Bulgarian, Catalan, Chinese, Danish,
Dutch, English, German, Greek, Hungarian, Italian, Japanese,
Portuguese, Spanish, Swedish vs. Czech, Slovenian, Turkish.
and Japanese). More generally, transitions between
different fonts and scripts should be informative too.
8 Conclusion
Orthography provides valuable syntactic cues. We
showed that bounding boxes signaled by capitaliza-
tion changes can help guide grammar induction and
boost unsupervised parsing performance. As with
punctuation-delimited segments and tags from web
markup, it is profitable to assume only that a single
word derives the rest, in such text fragments, without
further restricting relations to external words ? pos-
sibly a useful feature for supervised parsing models.
Our results should be regarded with some cau-
tion, however, since improvements due to capitaliza-
tion in grammar induction experiments came mainly
from two languages, Greek and Italian. Further re-
search is clearly needed to understand the ways that
capitalization can continue to improve parsing.
21
Acknowledgments
Funded, in part, by Defense Advanced Research Projects Age-
ncy (DARPA) Machine Reading Program under Air Force Re-
search Laboratory (AFRL) prime contract FA8750-09-C-0181.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reflect the views of DARPA, AFRL, or the US gov-
ernment. We also thank Ryan McDonald and the anonymous
reviewers for helpful comments on draft versions of this paper.
References
J. K. Baker. 1979. Trainable grammars for speech recognition.
In Speech Communication Papers for the 97th Meeting of the
Acoustical Society of America.
E. J. Briscoe. 1994. Parsing (with) punctuation, etc. Technical
report, Xerox European Research Laboratory.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In CoNLL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic normal dis-
tributions for soft parameter tying in unsupervised grammar
induction. In NAACL-HLT.
S. B. Cohen and N. A. Smith. 2010. Viterbi training for PCFGs:
Hardness results and competitiveness of uniform initializa-
tion. In ACL.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsupervised
structure prediction with non-parallel multilingual guidance.
In EMNLP.
T. Cohn, P. Blunsom, and S. Goldwater. 2011. Inducing tree-
substitution grammars. Journal of Machine Learning Re-
search.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
R. Frank. 2000. From regular to context-free to mildly context-
sensitive tree rewriting systems: The path of child language
acquisition. In A. Abeille? and O. Rambow, editors, Tree
Adjoining Grammars: Formalisms, Linguistic Analysis and
Processing. CSLI Publications.
K. Gimpel and N. A. Smith. 2011. Concavity and initialization
for unsupervised dependency grammar induction. Technical
report, CMU.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19.
D. Marec?ek and Z. Zabokrtsky?. 2011. Gibbs sampling with
treeness constraint in unsupervised dependency parsing. In
ROBUS.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-source trans-
fer of delexicalized dependency parsers. In EMNLP.
T. Naseem and R. Barzilay. 2011. Using semantic cues to learn
syntax. In AAAI.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010. Using
universal linguistic knowledge to guide grammar induction.
In EMNLP.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In EMNLP-CoNLL.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL.
E. Ponvert, J. Baldridge, and K. Erk. 2010. Simple unsuper-
vised identification of low-level constituents. In ICSC.
E. Ponvert, J. Baldridge, and K. Erk. 2011. Simple unsuper-
vised grammar induction from raw text with cascaded finite
state models. In ACL-HLT.
M. S. Rasooli and H. Faili. 2012. Fast unsupervised depen-
dency parsing with arc-standard transitions. In ROBUS-
UNSUP.
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
ACL.
A. S?gaard. 2011a. Data point selection for cross-language
adaptation of dependency parsers. In ACL-HLT.
A. S?gaard. 2011b. From ranked words to dependency trees:
two-stage unsupervised non-projective dependency parsing.
In TextGraphs.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010a. From
Baby Steps to Leapfrog: How ?Less is More? in unsuper-
vised dependency parsing. In NAACL-HLT.
V. I. Spitkovsky, D. Jurafsky, and H. Alshawi. 2010b. Profiting
from mark-up: Hyper-text annotations for guided parsing. In
ACL.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011a. Lateen
EM: Unsupervised training with multiple objectives, applied
to dependency grammar induction. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2011b. Punctu-
ation: Making a point in unsupervised dependency parsing.
In CoNLL.
V. I. Spitkovsky, A. X. Chang, H. Alshawi, and D. Jurafsky.
2011c. Unsupervised dependency parsing without gold part-
of-speech tags. In EMNLP.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2012. Three
dependency-and-boundary models for grammar induction.
In submission to EMNLP.
K. Tu and V. Honavar. 2011. On the utility of curricula in
unsupervised learning of probabilistic grammars. In IJCAI.
G. Ward and B. J. Birner. 2001. Discourse and information
structure. In D. Schiffrin, D. Tannen, and H. Hamilton, edi-
tors, Handbook of Discourse Analysis. Oxford: Basil Black-
well.
22
