A Graph Model for Unsupervised Lexical Acquisition
Dominic Widdows and Beate Dorow
Center for the Study of Language and Information
210 Panama Street
Stanford University
Stanford CA 94305-4115
{dwiddows,beate}@csli.stanford.edu
Abstract
This paper presents an unsupervised method for
assembling semantic knowledge from a part-of-
speech tagged corpus using graph algorithms.
The graph model is built by linking pairs of
words which participate in particular syntactic
relationships. We focus on the symmetric rela-
tionship between pairs of nouns which occur to-
gether in lists. An incremental cluster-building
algorithm using this part of the graph achieves
82% accuracy at a lexical acquisition task, eval-
uated against WordNet classes. The model nat-
urally realises domain and corpus specific am-
biguities as distinct components in the graph
surrounding an ambiguous word.
1 Introduction
Semantic knowledge for particular domains is
increasingly important in NLP. Many applica-
tions such as Word-Sense Disambiguation, In-
formation Extraction and Speech Recognition
all require lexicons. The coverage of hand-
built lexical resources such as WordNet (Fell-
baum, 1998) has increased dramatically in re-
cent years, but leaves several problems and
challenges. Coverage is poor in many criti-
cal, rapidly changing domains such as current
affairs, medicine and technology, where much
time is still spent by human experts employed
to recognise and classify new terms. Most
languages remain poorly covered in compari-
son with English. Hand-built lexical resources
which cannot be automatically updated can of-
ten be simply misleading. For example, using
WordNet to recognise that the word apple refers
to a fruit or a tree is a grave error in the many
situations where this word refers to a computer
manufacturer, a sense which WordNet does not
cover. For NLP to reach a wider class of appli-
cations in practice, the ability to assemble and
update appropriate semantic knowledge auto-
matically will be vital.
This paper describes a method for arranging
semantic information into a graph (Bolloba?s,
1998), where the nodes are words and the edges
(also called links) represent relationships be-
tween words. The paper is arranged as follows.
Section 2 reviews previous work on semantic
similarity and lexical acquisition. Section 3 de-
scribes how the graph model was built from the
PoS-tagged British National Corpus. Section 4
describes a new incremental algorithm used to
build categories of words step by step from the
graph model. Section 5 demonstrates this algo-
rithm in action and evaluates the results against
WordNet classes, obtaining state-of-the-art re-
sults. Section 6 describes how the graph model
can be used to recognise when words are poly-
semous and to obtain groups of words represen-
tative of the different senses.
2 Previous Work
Most work on automatic lexical acquisition has
been based at some point on the notion of
semantic similarity. The underlying claim is
that words which are semantically similar occur
with similar distributions and in similar con-
texts (Miller and Charles, 1991).
The main results to date in the field of au-
tomatic lexical acquisition are concerned with
extracting lists of words reckoned to belong to-
gether in a particular category, such as vehicles
or weapons (Riloff and Shepherd, 1997) (Roark
and Charniak, 1998). Roark and Charniak de-
scribe a ?generic algorithm? for extracting such
lists of similar words using the notion of seman-
tic similarity, as follows (Roark and Charniak,
1998, ?1).
1. For a given category, choose a small
set of exemplars (or ?seed words?)
2. Count co-occurrence of words and
seed words within a corpus
3. Use a figure of merit based upon
these counts to select new seed words
4. Return to step 2 and iterate n times
5. Use a figure of merit to rank words
for category membership and output a
ranked list
Algorithms of this type were used by Riloff
and Shepherd (1997) and Roark and Charniak
(1998), reporting accuracies of 17% and 35%
respectively. Like the algorithm we present in
Section 5, the similarity measure (or ?figure of
merit?) used in these cases was based on co-
occurrence in lists.
Both of these works evaluated their results
by asking humans to judge whether items gen-
erated were appropriate members of the cate-
gories sought. Riloff and Shepherd (1997) also
give some credit for ?related words? (for example
crash might be regarded as being related to the
category vehicles).
One problem with these techniques is the
danger of ?infections? ? once any incorrect or
out-of-category word has been admitted, the
neighbours of this word are also likely to be ad-
mitted. In Section 4 we present an algorithm
which goes some way towards reducing such in-
fections.
The early results have been improved upon by
Riloff and Jones (1999), where a ?mutual boot-
strapping? approach is used to extract words in
particular semantic categories and expression
patterns for recognising relationships between
these words for the purposes of information ex-
traction. The accuracy achieved in this experi-
ment is sometimes as high as 78% and is there-
fore comparable to the results reported in this
paper.
Another way to obtain word-senses directly
from corpora is to use clustering algorithms
on feature-vectors (Lin, 1998; Schu?tze, 1998).
Clustering techniques can also be used to dis-
criminate between different senses of an ambigu-
ous word. A general problem for such cluster-
ing techniques lies in the question of how many
clusters one should have, i.e. how many senses
are appropriate for a particular word in a given
domain (Manning and Schu?tze, 1999, Ch 14).
Lin?s approach to this problem (Lin, 1998) is
to build a ?similarity tree? (using what is in ef-
fect a hierarchical clustering method) of words
related to a target word (in this case the word
duty). Different senses of duty can be discerned
as different sub-trees of this similarity tree. We
present a new method for word-sense discrimi-
nation in Section 6.
3 Building a Graph from a
PoS-tagged Corpus
In this section we describe how a graph ? a
collection of nodes and links ? was built to
represent the relationships between nouns. The
model was built using the British National Cor-
pus which is automatically tagged for parts of
speech.
Initially, grammatical relations between pairs
of words were extracted. The relationships ex-
tracted were the following:
? Noun (assumed to be subject) Verb
? Verb Noun (assumed to be object)
? Adjective Noun
? Noun Noun (often the first noun is modify-
ing the second)
? Noun and/or Noun
The last of these relationships often occurs
when the pair of nouns is part of a list. Since
lists are usually comprised of objects which are
similar in some way, these relationships have
been used to extract lists of nouns with similar
properties (Riloff and Shepherd, 1997) (Roark
and Charniak, 1998). In this paper we too fo-
cus on nouns co-occurring in lists. This is be-
cause the noun and/or noun relationship is the
only symmetric relationship in our model, and
symmetric relationships are much easier to ma-
nipulate than asymmetric ones. Our full graph
contains many directed links between words of
different parts of speech. Initial experiments
with this model show considerable promise but
are at too early a stage to be reported upon yet.
Thus the graph used in most of this paper repre-
sents only nouns. Each node represents a noun
and two nodes have a link between them if they
co-occur separated by the conjunctions and or
or, and each link is weighted according to the
number of times the co-occurrence is observed.
Various cutoff functions were used to deter-
mine how many times a relationship must be
observed to be counted as a link in the graph.
A well-behaved option was to take the top n
neighbours of each word, where n could be de-
termined by the user. In this way the link-
weighting scheme was reduced to a link-ranking
scheme. One consequence of this decision was
that links to more common words were preferred
over links to rarer words. This decision may
have effectively boosted precision at the expense
of recall, because the preferred links are to fairly
common and (probably) more stable words. Re-
search is need to reveal theoretically motivated
or experimentally optimal techniques for select-
ing the importance to assign to each link ? the
choices made in this area so far are often of an
ad hoc nature.
The graph used in the experiments described
has 99,454 nodes (nouns) and 587,475 links.
There were roughly 400,000 different types
tagged as nouns in the corpus, so the graph
model represents about one quarter of these
nouns, including most of the more common
ones.
4 An Incremental Algorithm for
Extracting Categories of Similar
Words
In this section we describe a new algorithm for
adding the ?most similar node? to an existing
collection of nodes in a way which incremen-
tally builds a stable cluster. We rely entirely
upon the graph to deduce the relative impor-
tance of relationships. In particular, our algo-
rithm is designed to reduce so-called ?infections?
(Roark and Charniak, 1998, ?3) where the inclu-
sion of an out-of-category word which happens
to co-occur with one of the category words can
significantly distort the final list.
Here is the process we use to select and add
the ?most similar node? to a set of nodes:
Definition 1 Let A be a set of nodes and
let N(A), the neighbours of A, be the nodes
which are linked to any a ? A. (So N(A) =
?
a?AN(a).)
The best new node is taken to be the node
b ? N(A)\A with the highest proportion of links
to N(A). More precisely, for each u ? N(A)\A,
let the affinity between u and A be given by the
ratio
|N(u) ?N(A)|
|N(u)| .
The best new node b ? N(A) \ A is the node
which maximises this affinity score.
This algorithm has been built into an on-line
demonstration where the user inputs a given
seed word and can then see the cluster of re-
lated words being gradually assembled.
The algorithm is particularly effective at
avoiding infections arising from spurious co-
occurrences and from ambiguity. Consider, for
example, the graph built around the word ap-
ple in Figure 6. Suppose that we start with the
seed-list apple, orange, banana. However many
times the string ?Apple and Novell? occurs in
the corpus, the novell node will not be added
to this list because it doesn?t have a link to or-
ange, banana or any of their neighbours except
for apple. One way to summarise the effect of
this decision is that the algorithm adds words
to clusters depending on type frequency rather
than token frequency. This avoids spurious links
due to (for example) particular idioms rather
than geniune semantic similarity.
5 Examples and Evaluation
In this section we give examples of lexical cat-
egories extracted by our method and evaluate
them against the corresponding classes in Word-
Net.
5.1 Methodology
Our methodology is as follows. Consider an
intuitive category of objects such as musical
instruments. Define the ?WordNet class? or
?WordNet category? of musical instruments to
be the collection of synsets subsumed in Word-
Net by the musical instruments synset. Take a
?protypical example? of a musical instrument,
such as piano. The algorithm defined in (1)
gives a way of finding the n nodes deemed to be
most closely related to the piano node. These
can then be checked to see if they are mem-
bers of the WordNet class of musical instru-
ments. This method is easier to implement and
less open to variation than human judgements.
While WordNet or any other lexical resource is
not a perfect arbiter, it is hoped that this exper-
iment procedure is both reliable and repeatable.
The ten classes of words chosen were crimes,
places, tools, vehicles, musical instruments,
clothes, diseases, body parts, academic subjects
and foodstuffs. The classes were chosen before
the experiment was carried out so that the re-
sults could not be massaged to only use those
classes which gave good results. (The first 4 cat-
egories are also used by (Riloff and Shepherd,
1997) and (Roark and Charniak, 1998) and so
were included for comparison.) Having chosen
these classes, 20 words were retrieved using a
single seed-word chosen from the class in ques-
tion.
This list of words clearly depends on the seed
word chosen. While we have tried to optimise
this choice, it depends on the corpus and the
the model. The influence of semantic Proto-
type Theory (Rosch, 1988) is apparent in this
process, a link we would like to investigate in
more detail. It is possible to choose an optimal
seed word for a particular category: it should be
possible to compare these optimal seed words
with the ?prototypes? suggested by psychologi-
cal experiments (Mervis and Rosch, 1981).
5.2 Results
The results for a list of ten classes and proto-
typical words are given in Table 1. Words which
are correct members of the classes sought are
in Roman type: incorrect results are in ital-
ics. The decision between correctness and in-
correctness was made on a strict basis for the
sake of objectivity and to enable the repeata-
bility of the experiment: words which are in
WordNet were counted as correct results only if
they are actual members of the WordNet class
in question. Thus brigandage is not regarded
as a crime even though it is clearly an act of
wrongdoing, orchestra is not regarded as a mu-
sical instrument because it is a collection of in-
struments rather than a single instrument, etc.
The only exceptions we have made are the terms
wynd and planetology (marked in bold), which
are not in WordNet but are correct nonethe-
less. These conditions are at least as stringent
as those of previous experiments, particularly
those of Riloff and Shepherd (1997) who also
give credit for words associated with but not
belonging to a particular category. (It has been
pointed out that many polysemous words may
occur in several classes, making the task easier
because for many words there are several classes
which our algorithm would give credit for.)
With these conditions, our algorithm re-
trieves only 36 incorrect terms out of a total
of 200, giving an accuracy of 82%.
5.3 Analysis
Our results are an order of magnitude better
than those reported by Riloff and Shepherd
(1997) and Roark and Charniak (1998), who
report average accuracies of 17% and 35% re-
spectively. (Our results are also slightly better
than those reported by Riloff and Jones (1999)).
Since the algorithms used are in many ways
very similar, this improvement demands expla-
nation.
Some of the difference in accuracy can be at-
tributed to the corpora used. The experiments
in (Riloff and Shepherd, 1997) were performed
on the 500,000 word MUC-4 corpus, and those
of (Roark and Charniak, 1998) were performed
using MUC-4 and the Wall Street Journal cor-
pus (some 30 million words). Our model was
built using the British National Corpus (100
million words). On the other hand, our model
was built using only a part-of-speech tagged cor-
pus. The high accuracy achieved thus questions
the conclusion drawn by Roark and Charniak
(1998) that ?parsing is invaluable?. Our results
clearly indicate that a large PoS-tagged corpus
may be much better for automatic lexical ac-
quisition than a small fully-parsed corpus. This
claim could of course be tested by comparing
techniques on the same corpus.
To evaluate the advantage of using PoS infor-
mation, we compared the graph model with a
similarity thesaurus generated using Latent Se-
mantic Indexing (Manning and Schu?tze, 1999,
Ch 15), a ?bag-of-words? approach, on the same
corpus. The same number of nouns was re-
trieved for each class using the graph model
and LSI. The LSI similarity thesaurus obtained
an accuracy of 31%, much less than the graph
model?s 82%. This is because LSI retrieves
words which are related by context but are not
in the same class: for example, the neighbours
of piano found using LSI cosine-similarity on the
BNC corpus include words such as composer,
music, Bach, concerto and dance, which are re-
lated but certainly not in the same semantic
class.
The incremental clustering algorithm of Def-
inition (1) works well at preventing ?infections?
Class Seed Word Neighbours Produced by Graph Model
crimes murder crime theft arson importuning incest fraud larceny parricide
burglary vandalism indecency violence offences abuse brig-
andage manslaughter pillage rape robbery assault lewdness
places park path village lane viewfield church square road avenue garden
castle wynd garage house chapel drive crescent home place
cathedral street
tools screwdriver chisel naville nail shoulder knife drill matchstick morgenthau
gizmo hand knee elbow mallet penknife gallie leg arm sickle
bolster hammer
vehicle
conveyance
train tram car driver passengers coach lorry truck aeroplane coons
plane trailer boat taxi pedestrians vans vehicles jeep bus buses
helicopter
musical
instruments
piano fortepiano orchestra marimba clarsach violin cizek viola oboe
flute horn bassoon culbone mandolin clarinet equiluz contra-
bass saxophone guitar cello
clothes shirt chapeaubras cardigan trousers breeches skirt jeans boots pair
shoes blouse dress hat waistcoat jumper sweater coat cravat
tie leggings
diseases typhoid malaria aids polio cancer disease atelectasis illnesses cholera
hiv deaths diphtheria infections hepatitis tuberculosis cirrho-
sis diptheria bronchitis pneumonia measles dysentery
body parts stomach head hips thighs neck shoulders chest back eyes toes breasts
knees feet face belly buttocks haws ankles waist legs
academic
subjects
physics astrophysics philosophy humanities art religion science pol-
itics astronomy sociology chemistry history theology eco-
nomics literature maths anthropology culture mathematics
geography planetology
foodstuffs cake macaroons confectioneries cream rolls sandwiches croissant
buns scones cheese biscuit drinks pastries tea danish butter
lemonade bread chocolate coffee milk
Table 1: Classes of similar words given by the graph model.
and keeping clusters within one particular class.
The notable exception is the tools class, where
the word hand appears to introduce infection.
In conclusion, it is clear that the graph model
combined with the incremental clustering algo-
rithm of Definition 1 performs better than most
previous methods at the task of automatic lex-
ical acquisition.
6 Recognising Polysemy
So far we have presented a graph model built
upon noun co-occurrence which performs much
better than previously reported methods at the
task of automatic lexical acquisition. This is
an important task, because assembling and tun-
ing lexicons for specific NLP systems is increas-
ingly necessary. We now take a step further
and present a simple method for not only as-
sembling words with similar meanings, but for
empirically recognising when a word has several
meanings.
Recognising and resolving ambiguity is
an important task in semantic processing.
The traditional Word Sense Disambiguation
(WSD) problem addresses only the ambiguity-
resolution part of the problem: compiling a suit-
able list of polysemous words and their possible
senses is a task for which humans are tradition-
ally needed (Kilgarriff and Rosenzweig, 2000).
This makes traditional WSD an intensively su-
pervised and costly process. Breadth of cover-
age does not in itself solve this problem: general
lexical resources such as WordNet can provide
too many senses many of which are rarely used
in particular domains or corpora (Gale et al,
1992).
The graph model presented in this paper sug-
gests a new method for recognising relevant pol-
ysemy. We will need a small amount of termi-
nology from graph theory (Bolloba?s, 1998).
Definition 2 (Bolloba?s, 1998, Ch 1 ?1)
Let G = (V,E) be a graph, where V is the set
of vertices (nodes) of G and E ? V ? V is the
set of edges of G.
? Two nodes v1, vn are said to be connected
if there exists a path {v1, v2, . . . , vn?1, vn}
such that (vj , vj+1) ? E for 1 ? j < n.
? Connectedness is an equivalence relation.
? The equivalence classes of the graph G un-
der this relation are called the components
of G.
We are now in a position to define the senses
of a word as represented by a particular graph.
Definition 3 Let G be a graph of words closely
related to a seed-word w, and let G \ w be the
subgraph which results from the removal of the
seed-node w.
The connected components of the subgraph
G \ w are the senses of the word w with respect
to the graph G.
As an illustrative example, consider the local
graph generated for the word apple (6). The re-
moval of the apple node results in three separate
components which represent the different senses
of apple: fruit, trees, and computers. Definition
3 gives an extremely good model of the senses
of apple found in the BNC. (In this case better
than WordNet which does not contain the very
common corporate meaning.)
The intuitive notion of ambiguity being pre-
sented is as follows. An ambiguous word often
connects otherwise unrelated areas of meaning.
Definition 3 recognises the ambiguity of apple
because this word is linked to both banana and
novell, words which otherwise have nothing to
do with one another.
It is well-known that any graph can be
thought of as a collection of feature-vectors, for
example by taking the row-vectors in the adja-
cency matrix (Bolloba?s, 1998, Ch 2 ?3). There
might therefore be fundamental similarities be-
tween our approach and methods which rely on
similarities between feature-vectors.
Extra motivation for this technique is pro-
vided by Word-Sense Disambiguation. The
standard method for this task is to use hand-
labelled data to train a learning algorithm,
which will often pick out particular words as
Bayesian classifiers which indicate one sense or
the other. (So if microsoft occurs in the same
sentence as apple we might take this as evidence
that apple is being used in the corporate sense.)
Clearly, the words in the different components
in Diagram 6 can potentially be used as classi-
fiers for just this purpose, obviating the need for
time-consuming human annotation. This tech-
nique will be assessed and evaluated in future
experiments.
Demonstration
An online version of the graph model and the in-
cremental clustering algorithm described in this
paper are publicly available 1 for demonstration
purposes and to allow users to observe the gen-
erality of our techniques. A sample output is
included in Figure 6.
Acknowledgements
The authors would like to thank the anonymous
reviewers whose comments were a great help in
making this paper more focussed: any short-
comings remain entirely our own responsibility.
This research was supported in part by the
Research Collaboration between the NTT Com-
munication Science Laboratories, Nippon Tele-
graph and Telephone Corporation and CSLI,
Stanford University, and by EC/NSF grant IST-
1999-11438 for the MUCHMORE project. 2
1http://infomap.stanford.edu/graphs
2http://muchmore.dfki.de
Figure 1: Automatically generated graph show-
ing the word apple and semantically related
nouns
References
Be?la Bolloba?s. 1998. Modern Graph Theory.
Number 184 in Graduate texts in Mathemat-
ics. Springer-Verlag.
Christiane Fellbaum. 1998. WordNet: An elec-
tronic lexical database. MIT press, Cam-
bridge MA.
W. Gale, K. Church, and D. Yarowsky. 1992.
One sense per discourse. In DARPA speech
and Natural Language Workshop, Harriman,
NY.
Adam Kilgarriff and Joseph Rosenzweig. 2000.
English senseval: report and results. In
LREC, Athens.
Dekang Lin. 1998. Automatic retrieval and
clustering of similar words. In COLING-
ACL, Montreal, August.
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cam-
bridge, Massachusetts.
C. Mervis and E. Rosch. 1981. Categorization
of natural objects. Annual Review of Psychol-
ogy, 32:89?115.
George A. Miller and William G. Charles. 1991.
Contextual correlates of semantic similarity.
Language and Cognitive Processes, 6(1):1?28.
Ellen Riloff and Rosie Jones. 1999. Learn-
ing dictionaries for infomation extraction by
multi-level bootstrapping. In Proceedings of
the Sixteenth National Conference on Artifi-
cial Intelligence, pages 472?479. AAAI.
Ellen Riloff and Jessica Shepherd. 1997. A
corpus-based approach for building seman-
tic lexicons. In Claire Cardie and Ralph
Weischedel, editors, Proceedings of the Second
Conference on Empirical Methods in Natural
Language Processing, pages 117?124. Associ-
ation for Computational Linguistics, Somer-
set, New Jersey.
Brian Roark and Eugene Charniak. 1998.
Noun-phrase co-occurence statistics for semi-
automatic semantic lexicon construction. In
COLING-ACL, pages 1110?1116.
E. Rosch. 1988. Principles of categorization. In
A. Collins and E. E. Smith, editors, Read-
ings in Cognitive Science: A Perspective from
Psychology and Artificial Intelligence, pages
312?322. Kaufmann, San Mateo, CA.
Hinrich Schu?tze. 1998. Automatic word sense
discrimination. Computational Linguistics,
24(1):97?124.
79
80
81
82
Unsupervised methods for developing taxonomies by combining syntactic
and statistical information
Dominic Widdows
Center for the Study of Language and Information, Stanford University
dwiddows@csli.stanford.edu
Abstract
This paper describes an unsupervised algo-
rithm for placing unknown words into a taxon-
omy and evaluates its accuracy on a large and
varied sample of words. The algorithm works
by first using a large corpus to find semantic
neighbors of the unknown word, which we ac-
complish by combining latent semantic analy-
sis with part-of-speech information. We then
place the unknown word in the part of the tax-
onomy where these neighbors are most concen-
trated, using a class-labelling algorithm devel-
oped especially for this task. This method is
used to reconstruct parts of the existing Word-
Net database, obtaining results for common
nouns, proper nouns and verbs. We evaluate
the contribution made by part-of-speech tag-
ging and show that automatic filtering using the
class-labelling algorithm gives a fourfold im-
provement in accuracy.
1 Introduction
The importance of automatic methods for enriching lex-
icons, taxonomies and knowledge bases from free text is
well-recognized. For rapidly changing domains such as
current affairs, static knowledge bases are inadequate for
responding to new developments, and the cost of building
and maintaining resources by hand is prohibitive.
This paper describes experiments which develop auto-
matic methods for taking an original taxonomy as a skele-
ton and fleshing it out with new terms which are discov-
ered in free text. The method is completely automatic and
it is completely unsupervised apart from using the origi-
nal taxonomic skeleton to suggest possible classifications
for new terms. We evaluate how accurately our meth-
ods can reconstruct the WordNet taxonomy (Fellbaum,
1998).
The problem of enriching the lexical information in
a taxonomy can be posed in two complementary ways.
Firstly, given a particular taxonomic class (such as fruit)
one could seek members of this class (such as apple, ba-
nana). This problem is addressed by Riloff and Shepherd
(1997), Roark and Charniak (1998) and more recently by
Widdows and Dorow (2002). Secondly, given a partic-
ular word (such as apple), one could seek suitable tax-
onomic classes for describing this object (such as fruit,
foodstuff). The work in this paper addresses the second
of these questions.
The goal of automatically placing new words into a
taxonomy has been attempted in various ways for at least
ten years (Hearst and Schu?tze, 1993). The process for
placing a word w in a taxonomy T using a corpus C often
contains some version of the following stages:
? For a word w, find words from the corpus C whose
occurrences are similar to those of w. Consider
these the ?corpus-derived neighbors? N(w) of w.
? Assuming that at least some of these neighbors are
already in the taxonomy T , map w to the place in
the taxonomy where these neighbors are most con-
centrated.
Hearst and Schu?tze (1993) added 27 words to Word-
Net using a version of this process, with a 63% ac-
curacy at assigning new words to one of a number of
disjoint WordNet ?classes? produced by a previous al-
gorithm. (Direct comparison with this result is prob-
lematic since the number of classes used is not stated.)
A more recent example is the top-down algorithm of
Alfonseca and Manandhar (2001), which seeks the node
in T which shares the most collocational properties with
the word w, adding 42 concepts taken from The Lord of
the Rings with an accuracy of 28%.
The algorithm as presented above leaves many degrees
of freedom and open questions. What methods should
be used to obtain the corpus-derived neighbors N(w)?
This question is addressed in Section 2. Given a col-
lection of neighbors, how should we define a ?place in
the taxonomy where these neighbors are most concen-
trated?? This question is addressed in Section 3, which
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 197-204
                                                         Proceedings of HLT-NAACL 2003
defines a robust class-labelling algorithm for mapping a
list of words into a taxonomy. In Section 4 we describe
experiments, determining the accuracy with which these
methods can be used to reconstruct the WordNet taxon-
omy. To our knowledge, this is the first such evaluation
for a large sample of words. Section 5 discusses related
work and other problems to which these techniques can
be adapted.
2 Finding semantic neighbors: Combining
latent semantic analysis with
part-of-speech information.
There are many empirical techniques for recognizing
when words are similar in meaning, rooted in the idea that
?you shall know a word by the company it keeps? (Firth,
1957). It is certainly the case that words which repeat-
edly occur with similar companions often have related
meanings, and common features used for determining
this similarity include shared collocations (Lin, 1999),
co-occurrence in lists of objects (Widdows and Dorow,
2002) and latent semantic analysis (Landauer and Du-
mais, 1997; Hearst and Schu?tze, 1993).
The method used to obtain semantic neighbors in our
experiments was a version of latent semantic analysis,
descended from that used by Hearst and Schu?tze (1993,
?4). First, 1000 frequent words were chosen as col-
umn labels (after removing stopwords (Baeza-Yates and
Ribiero-Neto, 1999, p. 167)). Other words were assigned
co-ordinates determined by the number of times they oc-
cured within the same context-window (15 words) as one
of the 1000 column-label words in a large corpus. This
gave a matrix where every word is represented by a row-
vector determined by its co-occurence with frequently oc-
curing, meaningful words. Since this matrix was very
sparse, singular value decomposition (known in this con-
text as latent semantic analysis (Landauer and Dumais,
1997)) was used to reduce the number of dimensions
from 1000 to 100. This reduced vector space is called
WordSpace (Hearst and Schu?tze, 1993, ?4). Similarity
between words was then computed using the cosine sim-
ilarity measure (Baeza-Yates and Ribiero-Neto, 1999, p.
28). Such techniques for measuring similarity between
words have been shown to capture semantic properties:
for example, they have been used successfully for recog-
nizing synonymy (Landauer and Dumais, 1997) and for
finding correct translations of individual terms (Widdows
et al, 2002).
The corpus used for these experiments was the British
National Corpus, which is tagged for parts-of-speech.
This enabled us to build syntactic distinctions into
WordSpace ? instead of just giving a vector for the string
test we were able to build separate vectors for the nouns,
verbs and adjectives test. An example of the contribu-
tion of part-of-speech information to extracting seman-
tic neighbors of the word fire is shown in Table 2. As
can be seen, the noun fire (as in the substance/element)
and the verb fire (mainly used to mean firing some sort
of weapon) are related to quite different areas of mean-
ing. Building a single vector for the string fire confuses
this distinction ? the neighbors of fire treated just as a
string include words related to both the meaning of fire as
a noun (more frequent in the BNC) and as a verb.
Part of the goal of our experiments was to investi-
gate the contribution that this part-of-speech information
made for mapping words into taxonomies. As far as we
are aware, these experiments are the first to investigate
the combination of latent semantic indexing with part-of-
speech information.
3 Finding class-labels: Mapping
collections of words into a taxonomy
Given a collection of words or multiword expressions
which are semantically related, it is often important to
know what these words have in common. All adults with
normal language competence and world knowledge are
adept at this task ? we know that plant, animal and fun-
gus are all living things, and that plant, factory and works
are all kinds of buildings. This ability to classify objects,
and to work out which of the possible classifications of a
given object is appropriate in a particular context, is es-
sential for understanding and reasoning about linguistic
meaning. We will refer to this process as class-labelling.
The approach demonstrated here uses a hand-built tax-
onomy to assign class-labels to a collection of similar
nouns. As with much work of this nature, the taxonomy
used is WordNet (version 1.6), a freely-available broad-
coverage lexical database for English (Fellbaum, 1998).
Our algorithm finds the hypernyms which subsume as
many as possible of the original nouns, as closely as pos-
sible 1. The concept v is said to be a hypernym of w if
w is a kind of v. For this reason this sort of a taxonomy
is sometimes referred to as an ?IS A hierarchy?. For ex-
ample, the possible hypernyms given for the word oak in
WordNet 1.6 are
oak ? wood ? plant material ? material,
stuff ? substance, matter ? object, physical
object ? entity, something
1Another method which could be used for class-
labelling is given by the conceptual density algorithm of
Agirre and Rigau (1996), which those authors applied to word-
sense disambiguation. A different but related idea is presented
by Li and Abe (1998), who use a principle from information
theory to model selectional preferences for verbs using differ-
ent classes from a taxonomy. Their algorithm and goals are
different from ours: we are looking for a single class-label for
semantically related words, whereas for modelling selectional
preferences several classes may be appropriate.
fire (string only) fire nn1 fire vvi
fire 1.000000 fire nn1 1.000000 fire vvi 1.000000
flames 0.709939 flames nn2 0.700575 guns nn2 0.663820
smoke 0.680601 smoke nn1 0.696028 firing vvg 0.537778
blaze 0.668504 brigade nn1 0.589625 cannon nn0 0.523442
firemen 0.627065 fires nn2 0.584643 gun nn1 0.484106
fires 0.617494 firemen nn2 0.567170 fired vvd 0.478572
explosion 0.572138 explosion nn1 0.551594 detectors nn2 0.477025
burning 0.559897 destroyed vvn 0.547631 artillery nn1 0.469173
destroyed 0.558699 burning aj0 0.533586 attack vvb 0.468767
brigade 0.532248 blaze nn1 0.529126 firing nn1 0.459000
arson 0.528909 arson nn1 0.522844 volley nn1 0.458717
accidental 0.519310 alarms nn2 0.512332 trained vvn 0.447797
chimney 0.489577 destroyed vvd 0.512130 enemy nn1 0.445523
blast 0.488617 burning vvg 0.502052 alert aj0 0.443610
guns 0.487226 burnt vvn 0.500864 shoot vvi 0.443308
damaged 0.484897 blast nn1 0.498635 defenders nn2 0.438886
Table 1: Semantic neighbors of fire with different parts-of-speech. The scores are cosine similarities
oak, oak tree ? tree ? woody plant, ligneous
plant ? vascular plant, tracheophyte ? plant,
flora, plant life ? life form, organism, being,
living thing ? entity, something
Let S be a set of nouns or verbs. If the word w ? S is
recognized by WordNet, the WordNet taxonomy assigns
to w an ordered set of hypernyms H(w).
Consider the union
H =
?
w?S
H(w).
This is the set of all hypernyms of any member of S. Our
intuition is that the most appropriate class-label for the
set S is the hypernym h ? H which subsumes as many
as possible of the members of S as closely as possible
in the hierarchy. There is a trade-off here between sub-
suming ?as many as possible? of the members of S, and
subsuming them ?as closely as possible?. This line of rea-
soning can be used to define a whole collection of ?class-
labelling algorithms?.
For each w ? S and for each h ? H, define the affinity
score function ?(w, h) between w and h to be
?(w, h) =
{
f(dist(w, h)) if h ? H(w)
?g(w, h) if h /? H(w), (1)
where dist(w, h) is a measure of the distance between w
and h, f is some positive, monotonically decreasing func-
tion, and g is some positive (possibly constant) function.
The function f accords ?positive points? to h if h sub-
sumes w, and the condition that f be monotonically de-
creasing ensures that h gets more positive points the
closer it is to w. The function g subtracts ?penalty points?
if h does not subsume w. This function could depend in
many ways on w and h ? for example, there could be a
smaller penalty if h is a very specific concept than if h is
a very general concept.
The distance measure dist(w, h) could take many
forms, and there are already a number of distance mea-
sures available to use with WordNet (Budanitsky and
Hirst, 2001). The easiest method for assigning a distance
between words and their hypernyms is to count the num-
ber of intervening levels in the taxonomy. This assumes
that the distance in specificity between ontological levels
is constant, which is of course not the case, a problem
addressed by Resnik (1999).
Given an appropriate affinity score, it is a simple matter
to define the best class-label for a collection of objects.
Definition 1 Let S be a set of nouns, let H =
?
w?S H(w) be the set of hypernyms of S and let ?(w, h)
be an affinity score function as defined in equation (1).
The best class-label hmax(S) for S is the node hmax ? H
with the highest total affinity score summed over all the
members of S, so hmax is the node which gives the max-
imum score
max
h?H
?
w?S
?(w, h).
Since H is determined by S, hmax is solely determined
by the set S and the affinity score ?.
In the event that hmax is not unique, it is customary to
take the most specific class-label available.
Example
A particularly simple example of this kind of algorithm
is used by Hearst and Schu?tze (1993). First they parti-
tion the WordNet taxonomy into a number of disjoint sets
which are used as class-labels. Thus each concept has
a single ?hypernym?, and the ?affinity-score? between a
word w and a class h is simply the set membership func-
tion, ?(w, h) = 1 if w ? h and 0 otherwise. A collection
of words is assigned a class-label by majority voting.
3.1 Ambiguity
In theory, rather than a class-label for related strings, we
would like one for related meanings ? the concepts to
which the strings refer. To implement this for a set of
words, we alter our affinity score function ? as follows.
Let C(w) be the set of concepts to which the word w
could refer. (So each c ? C is a possible sense of w.)
Then
?(w, h) = max
c?C(w)
{
f(dist(c, h)) if h ? H(c)
?g(w, c) if h /? H(c), (2)
This implies that the ?preferred-sense? of w with respect
to the possible subsumer h is the sense closest to h. In
practice, our class-labelling algorithm implements this
preference by computing the affinity score ?(c, h) for all
c ? C(w) and only using the best match. This selec-
tive approach is much less noisy than simply averaging
the probability mass of the word over each possible sense
(the technique used in (Li and Abe, 1998), for example).
3.2 Choice of scoring functions for the
class-labelling algorithm
The precise choice of class-labelling algorithm depends
on the functions f and g in the affinity score function
? of equation (2). There is some tension here between
being correct and being informative: ?correct? but unin-
formative class-labels (such as entity, something) can be
obtained easily by preferring nodes high up in the hier-
archy, but since our goal in this work was to classify un-
known words in an informative and accurate fashion, the
functions f and g had to be chosen to give an appropriate
balance. After a variety of heuristic tests, the function f
was chosen to be
f = 1dist(w, h)2 ,
where for the distance function dist(w, h) we chose the
computationally simple method of counting the number
of taxonomic levels between w and h (inclusively to
avoid dividing by zero). For the penalty function g we
chose the constant g = 0.25.
The net effect of choosing the reciprocal-distance-
squared and a small constant penalty function was that
hypernyms close to the concept in question received mag-
nified credit, but possible class-labels were not penalized
too harshly for missing out a node. This made the algo-
rithm simple and robust to noise but with a strong prefer-
ence for detailed information-bearing class-labels. This
configuration of the class-labelling algorithm was used in
all the experiments described below.
4 Experiments and Evaluation
To test the success of our approach to placing unknown
words into the WordNet taxonomy on a large and signif-
icant sample, we designed the following experiment. If
the algorithm is successful at placing unknown words in
the correct new place in a taxonomy, we would expect it
to place already known words in their current position.
The experiment to test this worked as follows.
? For a word w, find the neighbors N(w) of w in
WordSpace. Remove w itself from this set.
? Find the best class-label hmax(N(w)) for this set
(using Definition 1).
? Test to see if, according to WordNet, hmax is a hy-
pernym of the original word w, and if so check how
closely hmax subsumes w in the taxonomy.
Since our class-labelling algorithm gives a ranked list
of possible hypernyms, credit was given for correct clas-
sifications in the top 4 places. This algorithm was tested
on singular common nouns (PoS-tag nn1), proper nouns
(PoS-tag np0) and finite present-tense verbs (PoS-tag
vvb). For each of these classes, a random sample of words
was selected with corpus frequencies ranging from 1000
to 250. For the noun categories, 600 words were sam-
pled, and for the finite verbs, 420. For each word w, we
found semantic neighbors with and without using part-of-
speech information. The same experiments were carried
out using 3, 6 and 12 neighbors: we will focus on the re-
sults for 3 and 12 neighbors since those for 6 neighbors
turned out to be reliably ?somewhere in between? these
two.
Results for Common Nouns
The best results for reproducing WordNet classifica-
tions were obtained for common nouns, and are sum-
marized in Table 2, which shows the percentage of test
words w which were given a class-label h which was a
correct hypernym according to WordNet (so for which
h ? H(w)). For these words for which a correct clas-
sification was found, the ?Height? columns refer to the
number of levels in the hierarchy between the target word
w and the class-label h. If the algorithm failed to find a
class-label h which is a hypernym of w, the result was
counted as ?Wrong?. The ?Missing? column records the
number of words in the sample which are not in WordNet
at all.
The following trends are apparent. For finding any
correct class-label, the best results were obtained by
taking 12 neighbors and using part-of-speech informa-
tion, which found a correct classification for 485/591 =
82% of the common nouns that were included in Word-
Net. This compares favorably with previous experiments,
though as stated earlier it is difficult to be sure we are
comparing like with like. Finding the hypernym which
immediately subsumes w (with no intervening nodes)
exactly reproduces a classification given by WordNet,
and as such was taken to be a complete success. Tak-
ing fewer neighbors and using PoS-information both im-
proved this success rate, the best accuracy obtained be-
ing 86/591 = 15%. However, this configuration actually
gave the worst results at obtaining a correct classification
overall.
Height 1 2 3 4 5 6 7 8 9 10 Wrong Missing
Common Nouns (sample size 600)
3 neighbors
With PoS 14.3 26.1 33.1 37.8 39.8 40.6 41.5 42.0 42.0 42.0 56.5 1.5
Strings only 11.8 23.3 31.3 36.6 39.6 41.1 42.1 42.3 42.3 42.3 56.1 1.5
12 neighbors
With PoS 10.0 21.8 36.5 48.5 59.3 70.0 76.6 78.8 79.8 80.8 17.6 1.5
without PoS 8.5 21.5 33.6 46.8 57.1 66.5 72.8 74.6 75.3 75.8 22.6 1.5
Proper Nouns (sample size 600)
3 neighbors
With PoS 10.6 13.8 15.5 16.5 108 18.6 18.8 18.8 19.1 19.3 25.0 55.6
Strings only 9.8 14.3 16.1 18.6 19.5 20.1 20.8 21.1 21.5 21.6 22.1 55.6
12 neighbors
With PoS 10.5 14.5 16.3 18.1 22.0 23.8 25.5 28.0 28.5 29.3 15.0 55.6
Strings only 9.5 13.8 17.5 20.8 22.3 24.6 26.6 30.7 32.5 34.3 10.0 55.6
Verbs (sample size 420)
3 neighbors
With PoS 17.6 30.2 36.1 40.4 42.6 43.0 44.0 44.0 44.0 44.0 52.6 3.3
Strings only 24.7 39.7 43.3 45.4 47.1 48.0 48.3 48.8 49.0 49.0 47.6 3.3
12 neighbors
With PoS 19.0 36.4 43.5 48.8 52.8 54.2 55.2 55.4 55.7 55.9 40.7 3.3
Strings only 28.0 48.3 55.9 60.2 63.3 64.2 64.5 65.0 65.0 65.0 31.7 3.3
Table 2: Percentage of words which were automatically assigned class-labels which subsume them in the WordNet
taxonomy, showing the number of taxonomic levels between the target word and the class-label
Height 1 2 3 4 5 6 Wrong
Common Nouns 0.799 0.905 0.785 0.858 0.671 0.671 0.569
Proper Nouns 1.625 0.688 0.350 0.581 0.683 0.430 0.529
Verbs 1.062 1.248 1.095 1.103 1.143 0.750 0.669
Table 3: Average affinity score of class-labels for successful and unsuccessful classifications
In conclusion, taking more neighbors makes the
chances of obtaining some correct classification for a
word w greater, but taking fewer neighbors increases the
chances of ?hitting the nail on the head?. The use of part-
of-speech information reliably increases the chances of
correctly obtaining both exact and broadly correct classi-
fications, though careful tuning is still necessary to obtain
optimal results for either.
Results for Proper Nouns and Verbs
The results for proper nouns and verbs (also in Table
2) demonstrate some interesting problems. On the whole,
the mapping is less reliable than for common nouns, at
least when it comes to reconstructing WordNet as it cur-
rently stands.
Proper nouns are rightly recognized as one of the cat-
egories where automatic methods for lexical acquisition
are most important (Hearst and Schu?tze, 1993, ?4). It
is impossible for a single knowledge base to keep up-to-
date with all possible meanings of proper names, and this
would be undesirable without considerable filtering abil-
ities because proper names are often domain-specific.
Ih our experiments, the best results for proper nouns
were those obtained using 12 neighbors, where a cor-
rect classification was found for 206/266 = 77% of the
proper nouns that were included in WordNet, using no
part-of-speech information. Part-of-speech information
still helps for mapping proper nouns into exactly the right
place, but in general degrades performance.
Several of the proper names tested are geographical,
and in the BNC they often refer to regions of the British
Isles which are not in WordNet. For example, hampshire
is labelled as a territorial division, which as an English
county it certainly is, but in WordNet hampshire is in-
stead a hyponym of domestic sheep. For many of the
proper names which our evaluation labelled as ?wrongly
classified?, the classification was in fact correct but a dif-
ferent meaning from those given in WordNet. The chal-
lenge for these situations is how to recognize when cor-
pus methods give a correct meaning which is different
from the meaning already listed in a knowledge base.
Many of these meanings will be systematically related
(such as the way a region is used to name an item or
product from that region, as with the hampshire example
above) by generative processes which are becoming well
understood by theoretical linguists (Pustejovsky, 1995),
and linguistic theory may help our statistical algorithms
considerably by predicting what sort of new meanings we
might expect a known word to assume through metonymy
and systematic polysemy.
Typical first names of people such as lisa and ralph al-
most always have neighbors which are also first names
(usually of the same gender), but these words are not rep-
resented in WordNet. This lexical category is ripe for
automatic discovery: preliminary experiments using the
two names above as ?seed-words? (Roark and Charniak,
1998; Widdows and Dorow, 2002) show that by taking
a few known examples, finding neighbors and removing
words which are already in WordNet, we can collect first
names of the same gender with at least 90% accuracy.
Verbs pose special problems for knowledge bases. The
usefulness of an IS A hierarchy for pinpointing informa-
tion and enabling inference is much less clear-cut than
for nouns. For example, sleeping does entail breathing
and arriving does imply moving, but the aspectual prop-
erties, argument structure and case roles may all be dif-
ferent. The more restrictive definition of troponymy is
used in WordNet to describe those properties of verbs
that are inherited through the taxonomy (Fellbaum, 1998,
Ch 3). In practice, the taxonomy of verbs in WordNet
tends to have fewer levels and many more branches than
the noun taxonomy. This led to problems for our class-
labelling algorithm ? class-labels obtained for the verb
play included exhaust, deploy, move and behave, all of
which are ?correct? hypernyms according to WordNet,
while possible class-labels obtained for the verb appeal
included keep, defend, reassert and examine, all of which
were marked ?wrong?. For our methods, the WordNet
taxonomy as it stands appears to give much less reli-
able evaluation criteria for verbs than for common nouns.
It is also plausible that similarity measures based upon
simple co-occurence are better for modelling similarity
between nominals than between verbs, an observation
which is compatible with psychological experiments on
word-association (Fellbaum, 1998, p. 90).
In our experiments, the best results for verbs were
clearly those obtained using 12 neighbors and no part-
of-speech information, for which some correct classifi-
cation was found for 273/406 = 59% of the verbs that
were included in WordNet, and which achieved better re-
sults than those using part-of-speech information even for
finding exact classifications. The shallowness of the tax-
onomy for verbs means that most classifications which
were successful at all were quite close to the word in
question, which should be taken into account when in-
terpreting the results in Table 2.
As we have seen, part-of-speech information degraded
performance overall for proper nouns and verbs. This
may be because combining all uses of a particular word-
form into a single vector is less prone to problems of data
sparseness, especially if these word-forms are semanti-
cally related in spite of part-of-speech differences 2. It is
also plausible that discarding part-of-speech information
2This issue is reminiscent of the question of whether stem-
ming improves or harms information retrieval (Baeza-Yates and
Ribiero-Neto, 1999) ? the received wisdom is that stemming
(at best) improves recall at the expense of precision and our
findings for proper nouns are consistent with this.
should improve the classification of verbs for the follow-
ing reason. Classification using corpus-derived neighbors
is markedly better for common nouns than for verbs, and
most of the verbs in our sample (57%) also occur as com-
mon nouns in WordSpace. (In contrast, only 13% of our
common nouns also occur as verbs, a reliable asymmetry
for English.) Most of these noun senses are semantically
related in some way to the corresponding verbs. Since
using neighboring words for classification is demonstra-
bly more reliable for nouns than for verbs, putting these
parts-of-speech together in a single vector in WordSpace
might be expected to improve performance for verbs but
degrade it for nouns.
Filtering using Affinity scores
One of the benefits of the class-labelling algorithm
(Definition 1) presented in this paper is that it returns not
just class-labels but an affinity score measuring how well
each class-label describes the class of objects in question.
The affinity score turns out to be signficantly correlated
with the likelihood of obtaining a successful classifica-
tion. This can be seen very clearly in Table 3, which
shows the average affinity score for correct class-labels of
different heights above the target word, and for incorrect
class-labels ? as a rule, correct and informative class-
labels have significantly higher affinity scores than incor-
rect class-labels. It follows that the affinity score can be
used as an indicator of success, and so filtering out class-
labels with poor scores can be used as a technique for
improving accuracy.
To test this, we repeated our experiments using 3
neighbors and this time only using class-labels with an
affinity score greater than 0.75, the rest being marked
?unknown?. Without filtering, there were 1143 success-
ful and 1380 unsuccessful outcomes: with filtering, these
numbers changed to 660 and 184 respectively. Filtering
discarded some 87% of the incorrect labels and kept more
than half of the correct ones, which amounts to at least a
fourfold improvement in accuracy. The improvement was
particularly dramatic for proper nouns, where filtering re-
moved 270 out of 283 incorrect results and still retained
half of the correct ones.
Conclusions
For common nouns, where WordNet is most reliable,
our mapping algorithm performs comparatively well, ac-
curately classifying several words and finding some cor-
rect information about most others. The optimum num-
ber of neighbors is smaller if we want to try for an exact
classification and larger if we want information that is
broadly reliable. Part-of-speech information noticeably
improves the process of both broad and narrow classifi-
cation. For proper names, many classifications are cor-
rect, and many which are absent or incorrect according
to WordNet are in fact correct meanings which should
be added to the knowledge base for (at least) the domain
in question. Results for verbs are more difficult to inter-
pret: reasons for this might include the shallowness and
breadth of the WordNet verb hierarchy, the suitability of
our WordSpace similarity measure, and many theoretical
issues which should be taken into account for a successful
approach to the classification of verbs.
Filtering using the affinity score from the class-
labelling algorithm can be used to dramatically increase
performance.
5 Related work and future directions
The experiments in this paper describe one combination
of algorithms for lexical acquisition: both the finding
of semantic neighbors and the process of class-labelling
could take many alternative forms, and an exhaustive
evaluation of such combinations is far beyond the scope
of this paper. Various mathematical models and distance
measures are available for modelling semantic proxim-
ity, and more detailed linguistic preprocessing (such as
chunking, parsing and morphology) could be used in a
variety of ways. As an initial step, the way the granularity
of part-of-speech classification affects our results for lex-
ical acquistion will be investigated. The class-labelling
algorithm could be adapted to use more sensitive mea-
sures of distance (Budanitsky and Hirst, 2001), and corre-
lations between taxonomic distance and WordSpace sim-
ilarity used as a filter.
The coverage and accuracy of the initial taxonomy we
are hoping to enrich has a great influence on success rates
for our methods as they stand. Since these are precisely
the aspects of the taxonomy we are hoping to improve,
this raises the question of whether we can use automati-
cally obtained hypernyms as well as the hand-built ones
to help classification. This could be tested by randomly
removing many nodes from WordNet before we begin,
and measuring the effect of using automatically derived
classifications for some of these words (possibly those
with high confidence scores) to help with the subsequent
classification of others.
The use of semantic neighbors and class-labelling for
computing with meaning go far beyond the experimen-
tal set up for lexical acquisition described in this pa-
per ? for example, Resnik (1999) used the idea of a
most informative subsuming node (which can be re-
garded as a kind of class-label) for disambiguation, as
did Agirre and Rigau (1996) with the conceptual density
algorithm. Taking a whole domain as a ?context?, this
approach to disambiguation can be used for lexical tun-
ing. For example, using the Ohsumed corpus of medical
abstracts, the top few neighbors of operation are amputa-
tion, disease, therapy and resection. Our algorithm gives
medical care, medical aid and therapy as possible class-
labels for this set, which successfully picks out the sense
of operation which is most important for the medical do-
main.
The level of detail which is appropriate for defining
and grouping terms depends very much on the domain in
question. For example, the immediate hypernyms offered
by WordNet for the word trout include
fish, foodstuff, salmonid, malacopterygian,
teleost fish, food fish, saltwater fish
Many of these classifications are inappropriately fine-
grained for many circumstances. To find a degree of
abstraction which is suitable for the way trout is used
in the BNC, we found its semantic neighbors which in-
clude herring swordfish turbot salmon tuna. The highest-
scoring class-labels for this set are
2.911 saltwater fish
2.600 food fish
1.580 fish
1.400 scombroid, scombroid
0.972 teleost fish
The preferred labels are the ones most humans would an-
swer if asked what a trout is. This process can be used
to select the concepts from an ontology which are ap-
propriate to a particular domain in a completely unsuper-
vised fashion, using only the documents from that do-
main whose meanings we wish to describe.
Demonstration
Interactive demonstrations of the class-labelling al-
gorithm and WordSpace are available on the web at
http://infomap.stanford.edu/classes and
http://infomap.stanford.edu/webdemo. An
interface to WordSpace incorporating the part-of-speech
information is currently under consideration.
Acknowledgements
This research was supported in part by the Research
Collaboration between the NTT Communication Science
Laboratories, Nippon Telegraph and Telephone Corpora-
tion and CSLI, Stanford University, and by EC/NSF grant
IST-1999-11438 for the MUCHMORE project.
References
E. Agirre and G. Rigau. 1996. Word sense disambigua-
tion using conceptual density. In Proceedings of COL-
ING?96, pages 16?22, Copenhagen, Denmark.
Enrique Alfonseca and Suresh Manandhar. 2001. Im-
proving an ontology refinement method with hy-
ponymy patterns. In Third International Conference
on Language Resources and Evaluation, pages 235?
239, Las Palmas, Spain.
Ricardo Baeza-Yates and Berthier Ribiero-Neto. 1999.
Modern Information Retrieval. Addison Wesley /
ACM press.
A. Budanitsky and G. Hirst. 2001. Semantic distance in
wordnet: An experimental, application-oriented evalu-
ation of five measures. In Workshop on WordNet and
Other Lexical Resources, Pittsburgh, PA. NAACL.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT press, Cambridge MA.
J. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis, Philological So-
ciety, Oxford, reprinted in Palmer, F. (ed. 1968) Se-
lected Papers of J. R. Firth, Longman, Harlow.
Marti Hearst and Hinrich Schu?tze. 1993. Customizing
a lexicon to better suit a computational task. In ACL
SIGLEX Workshop, Columbus, Ohio.
T. Landauer and S. Dumais. 1997. A solution to plato?s
problem: The latent semantic analysis theory of acqui-
sition. Psychological Review, 104(2):211?240.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the mdl principle. Computa-
tional Linguistics, 24(2):217?244.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In ACL:1999, pages 317?324.
James Pustejovsky. 1995. The Generative Lexicon. MIT
press, Cambridge, MA.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
artificial intelligence research, 11:93?130.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Claire
Cardie and Ralph Weischedel, editors, Proceedings of
the Second Conference on Empirical Methods in Natu-
ral Language Processing, pages 117?124. Association
for Computational Linguistics, Somerset, New Jersey.
Brian Roark and Eugene Charniak. 1998. Noun-phrase
co-occurence statistics for semi-automatic semantic
lexicon construction. In COLING-ACL, pages 1110?
1116.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In 19th In-
ternational Conference on Computational Linguistics,
pages 1093?1099, Taipei, Taiwan, August.
Dominic Widdows, Beate Dorow, and Chiu-Ki Chan.
2002. Using parallel corpora to enrich multilingual
lexical resources. In Third International Conference
on Language Resources and Evaluation, pages 240?
245, Las Palmas, Spain, May.
Monolingual and Bilingual Concept Visualization from Corpora
Dominic Widdows Scott Cederberg
Center for the Study of Language and Information, Stanford University
{dwiddows,cederber}@csli.stanford.edu
As well as identifying relevant information, a suc-
cessful information management system must be able to
present its findings in terms which are familiar to the user,
which is especially challenging when the incoming in-
formation is in a foreign language (Levow et al, 2001).
We demonstrate techniques which attempt to address this
challenge by placing terms in an abstract ?information
space? based on their occurrences in text corpora, and
then allowing a user to visualize local regions of this in-
formation space. Words are plotted in a 2-dimensional
picture so that related words are close together and whole
classes of similar words occur in recognizable clusters
which sometimes clearly signify a particular meaning. As
well as giving a clear view of which concepts are related
in a particular document collection, this technique also
helps a user to interpret unknown words.
The main technique we will demonstrate is planar pro-
jection of word-vectors from a vector space built using
Latent Semantic Analysis (LSA) (Landauer and Dumais,
1997; Schu?tze, 1998), a method which can be applied
multilingually if translated corpora are available for train-
ing. Following the method of Schu?tze (1998), we assign
each word 1000 coordinates based on the number of times
that word occurs in a 15 word window with one of 1000
?content-bearing words?, chosen by frequency, and the
number of coordinates is reduced to 100 ?latent dimen-
sions? using LSA.
This is still far too many words and too many dimen-
sions to be visualized at once. To produce a meaningful
diagram of results related to a particular word or query,
we perform two extra steps. Firstly, we restrict atten-
tion to a given number of closely related words (deter-
mined by cosine similarity of word vectors), selecting a
local group of up to 100 words and their word vectors
for deeper analysis. A second round of Latent Seman-
tic Analysis is then performed on this restricted set, giv-
ing the most significant directions to describe this local
information. The 2 most significant axes determine the
plane which best represents the data. (This process can
be regarded as a higher-dimensional analogue of finding
the line of best-fit for a normal 2-dimensional graph.) The
resulting diagrams give an summary of the areas of mean-
ing in which a word is actually used in a particular docu-
ment collection.
This is particularly effective for visualizing words in
more than one language. This can be achieved by build-
ing a single latent semantic vector space incorporat-
ing words from two languages using a parallel corpus
(Littman et al, 1998; Widdows et al, 2002b). We will
demonstrate a system which does this for English and
German terms in the medical domain. The system is
trained on a corpus of 10,000 abstracts from German
medical documents available with their English transla-
tions 1. In the demonstration, users submit a query state-
ment consisting of any combination of words in English
or German, and are then able to visualize the words most
closely related to this query in a 2-dimensional plot of the
latent semantic space.
An example output for the English query word drug is
shown in Figure below. 2. Such words are of special
interest because the English word drug has two mean-
ings which are represented by different words in German
(medikament = prescription drug and drogen = narcotic).
The 2-dimensional plot clearly distinguishes these two
areas of meaning, with the English word drug being in
between. Such techniques can enable users to recognize
and understand translational ambiguities.
As well as the Springer abstracts corpus, the system
has been trained to work with the parallel English/French
Canadian Hansard corpus and several large monolingual
corpora. Other functionalities of this system include au-
tomatic thesaurus generation, clustering of terms to deter-
mine different context areas, query refinement and docu-
ment retrieval.
As well as LSA, which only uses broad ?bag of words?
1Available from the Springer Link website,
http://link.springer.de/
2In the actual demonstration, English results appear in red
and German results in blue: for the description here we have
used different fonts instead.
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 31-32
                                                         Proceedings of HLT-NAACL 2003
DRUG
DRUGS
FATALITIES
FORENSIC
COCAINE
ABUSE
METHADONE
OPIATES
ANTIEPILEPTIC
THC
URINE
ANTICONVULSANT
NEUROTRANSMISSIONCANNABINOIDS
DOSAGE
DEPENDENCEPIGMENTATION
HAIR
ANTIARRHYTHMIC
SEROTONERGIC
HEROIN
drogentodesfa?lle
kokain
drogen
substanzen
antiepileptika
medikamento?se
antiarrhythmika
opiate
drogenabha?ngigen medikamente
medikamenten
pharmaka
methadon
gc
heroin
pigmentierung
arzneimittel
substanz
wirksame
wirksamer
beta?ubungsmittel
Figure 1: ENGLISH and German terms related to the English word drug in the Springer medical abstracts.
coocurrence to define similarities, mathematical models
can be built using local coordination of terms based on
syntactic properties. For example, list of nouns such as
?apples, pears and oranges? can be used as information
that these words are all linked, and these links can be
recorded in a database which can also be analyzed using
visualization techniques (Widdows et al, 2002a) and will
be included in the demonstration.
Demonstration website
Versions of these demonstrations are publicly avail-
able through the CSLI Infomap project website,
(http://infomap.stanford.edu/).
Acknowledgments
This research was supported in part by the Research
Collaboration between the NTT Communication Science
Laboratories, Nippon Telegraph and Telephone Corpora-
tion and CSLI, Stanford University, and by EC/NSF grant
IST-1999-11438 for the MUCHMORE project.
References
T. Landauer and S. Dumais. 1997. A solution to plato?s
problem: The latent semantic analysis theory of acqui-
sition. Psychological Review, 104(2):211?240.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2001. Rapidly retargetable interactive translingual re-
trieval. In Human Language Technology Conference
(HLT 2001), San Diego, CA.
Michael L. Littman, Susan T. Dumais, and Thomas K.
Landauer. 1998. Automatic cross-language informa-
tion retrieval using latent semantic indexing. In Gre-
gory Grefenstette, editor, Cross-language information
retrieval, chapter 4. Kluwer, Boston.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?124.
Dominic Widdows, Scott Cederberg, and Beate Dorow.
2002a. Visualisation techniques for analysing mean-
ing. In Fifth International Conference on Text, Speech
and Dialogue, Lecture Notes in Artificial Intelligence
2448, pages 107?115, Brno, Czech Republic, Septem-
ber. Springer.
Dominic Widdows, Beate Dorow, and Chiu-Ki Chan.
2002b. Using parallel corpora to enrich multilingual
lexical resources. In Third International Conference
on Language Resources and Evaluation, pages 240?
245, Las Palmas, Spain, May.
Orthogonal Negation in Vector Spaces for Modelling
Word-Meanings and Document Retrieval
Dominic Widdows ?
Stanford University
dwiddows@csli.stanford.edu
Abstract
Standard IR systems can process queries
such as ?web NOT internet?, enabling users
who are interested in arachnids to avoid
documents about computing. The docu-
ments retrieved for such a query should be
irrelevant to the negated query term. Most
systems implement this by reprocessing re-
sults after retrieval to remove documents
containing the unwanted string of letters.
This paper describes and evaluates a the-
oretically motivated method for removing
unwanted meanings directly from the orig-
inal query in vector models, with the same
vector negation operator as used in quan-
tum logic. Irrelevance in vector spaces is
modelled using orthogonality, so query vec-
tors are made orthogonal to the negated
term or terms.
As well as removing unwanted terms, this
form of vector negation reduces the occur-
rence of synonyms and neighbours of the
negated terms by as much as 76% compared
with standard Boolean methods. By alter-
ing the query vector itself, vector negation
removes not only unwanted strings but un-
wanted meanings.
1 Introduction
Vector spaces enjoy widespread use in information
retrieval (Salton and McGill, 1983; Baeza-Yates and
?This research was supported in part by the Research
Collaboration between the NTT Communication Science
Laboratories, Nippon Telegraph and Telephone Corpo-
ration and CSLI, Stanford University, and by EC/NSF
grant IST-1999-11438 for the MUCHMORE project.
Ribiero-Neto, 1999), and from this original appli-
cation vector models have been applied to seman-
tic tasks such as word-sense acquisition (Landauer
and Dumais, 1997; Widdows, 2003) and disambigua-
tion (Schu?tze, 1998). One benefit of these models is
that the similarity between pairs of terms or between
queries and documents is a continuous function, au-
tomatically ranking results rather than giving just a
YES/NO judgment. In addition, vector models can
be freely built from unlabelled text and so are both
entirely unsupervised, and an accurate reflection of
the way words are used in practice.
In vector models, terms are usually combined
to form more complicated query statements by
(weighted) vector addition. Because vector addition
is commutative, terms are combined in a ?bag of
words? fashion. While this has proved to be effective,
it certainly leaves room for improvement: any gen-
uine natural language understanding of query state-
ments cannot rely solely on commutative addition for
building more complicated expressions out of primi-
tives.
Other algebraic systems such as Boolean logic and
set theory have well-known operations for building
composite expressions out of more basic ones. Set-
theoretic models for the logical connectives ?AND?,
?NOT? and ?OR? are completely understood by most
researchers, and used by Boolean IR systems for as-
sembling the results to complicated queries. It is
clearly desirable to develop a calculus which com-
bines the flexible ranking of results in a vector model
with the crisp efficiency of Boolean logic, a goal
which has long been recognised (Salton et al, 1983)
and attempted mainly for conjunction and disjunc-
tion. This paper proposes such a scheme for nega-
tion, based upon well-known linear algebra, and
which also implies a vector form of disjunction. It
turns out that these vector connectives are precisely
those used in quantum logic (Birkhoff and von Neu-
mann, 1936), a development which is discussed in
much more detail in (Widdows and Peters, 2003).
Because of its simplicity, our model is easy to under-
stand and to implement.
Vector negation is based on the intuition that un-
related meanings should be orthogonal to one an-
other, which is to say that they should have no fea-
tures in common at all. Thus vector negation gener-
ates a ?meaning vector? which is completely orthog-
onal to the negated term. Document retrieval ex-
periments demonstrate that vector negation is not
only effective at removing unwanted terms: it is
also more effective than other methods at removing
their synonyms and related terms. This justifies the
claim that, by producing a single query vector for
?a NOT b?, we remove not only unwanted strings
but also unwanted meanings.
We describe the underlying motivation behind this
model and define the vector negation and disjunc-
tion operations in Section 2. In Section 3 we re-
view other ways negation is implemented in Infor-
mation Retrieval, comparing and contrasting with
vector negation. In Section 4 we describe experi-
ments demonstrating the benefits and drawbacks of
vector negation compared with two other methods
for negation.
2 Negation and Disjunction in
Vector Spaces
In this section we use well-known linear algebra to
define vector negation in terms of orthogonality and
disjunction as the linear sum of subspaces. The
mathematical apparatus is covered in greater detail
in (Widdows and Peters, 2003). If A is a set (in
some universe of discourse U), then ?NOT A? corre-
sponds to the complement A? of the set A in U (by
definition). By a simple analogy, let A be a vector
subspace of a vector space V (equipped with a scalar
product). Then the concept ?NOT A? should corre-
spond to the orthogonal complement A? of A under
the scalar product (Birkhoff and von Neumann, 1936,
?6). If we think of a basis for V as a set of features,
this says that ?NOT A? refers to the subspace of V
which has no features in common with A.
We make the following definitions. Let V be a
(real) vector space equipped with a scalar product.
We will use the notation A ? V to mean ?A is a
vector subspace of V .? For A ? V , define the or-
thogonal subspace A? to be the subspace
A? ? {v ? V : ?a ? A, a ? v = 0}.
For the purposes of modelling word-meanings, we
might think of ?orthogonal? as a model for ?com-
pletely unrelated? (having similarity score zero).
This makes perfect sense for information retrieval,
where we assume (for example) that if two words
never occur in the same document then they have no
features in common.
Definition 1 Let a, b ? V and A,B ? V . By
NOT A we mean A? and by NOT a, we mean
?a??, where ?a? = {?a : ? ? R} is the 1-dimensional
subspace subspace generated by a. By a NOT B we
mean the projection of a onto B? and by a NOT b
we mean the projection of a onto ?b??.
We now show how to use these notions to perform
calculations with individual term or query vectors in
a form which is simple to program and efficient to
run.
Theorem 1 Let a, b ? V . Then a NOT b is repre-
sented by the vector
a NOT b ? a ? a ? b|b|2 b.
where |b|2 = b ? b is the modulus of b.
Proof. A simple proof is given in (Widdows and Pe-
ters, 2003).
For normalised vectors, Theorem 1 takes the par-
ticularly simple form
a NOT b = a? (a ? b)b, (1)
which in practice is then renormalised for consis-
tency. One computational benefit is that Theorem 1
gives a single vector for a NOT b, so finding the sim-
ilarity between any other vector and a NOT b is just
a single scalar product computation.
Disjunction is also simple to envisage, the expres-
sion b1 OR . . . OR bn being modelled by the sub-
space
B = {?1b1 + . . .+ ?nbn : ?i ? R}.
Theoretical motivation for this formulation can be
found in (Birkhoff and von Neumann, 1936, ?1,?6)
and (Widdows and Peters, 2003): for example, B
is the smallest subspace of V which contains the set
{bj}.
Computing the similarity between a vector a and
this subspace B is computationally more expensive
than for the negation of Theorem 1, because the
scalar product of a with (up to) n vectors in an or-
thogonal basis for B must be computed. Thus the
gain we get by comparing each document with the
query a NOT b using only one scalar product oper-
ation is absent for disjunction.
However, this benefit is regained in the case of
negated disjunction. Suppose we negate not only one
argument but several. If a user specifies that they
want documents related to a but not b1, b2, . . . , bn,
then (unless otherwise stated) it is clear that they
only want documents related to none of the un-
wanted terms bi (rather than, say, the average of
these terms).
This motivates a process which can be thought of
as a vector formulation of the classical de Morgan
equivalence ? a? ? b ?? (a ? b), by which the
expression
a AND NOT b1 AND NOT b2 . . . AND NOT bn
is translated to
a NOT (b1 OR . . . OR bn). (2)
Using Definition 1, this expression can be modelled
with a unique vector which is orthogonal to all of
the unwanted arguments {b1}. However, unless the
vectors b1, . . . , bn are orthogonal (or identical), we
need to obtain an orthogonal basis for the subspace
b1 OR . . . OR bn before we can implement a higher-
dimensional version of Theorem 1. This is because
the projection operators involved are in general non-
commutative, one of the hallmark differences be-
tween Boolean and quantum logic.
In this way vector negation generates a meaning-
vector which takes into account the similarities and
differences between the negative terms. A query for
chip NOT computer, silicon
is treated differently from a query for
chip NOT computer, potato.
Vector negation is capable of realising that for the
first query, the two negative terms are referring to
the same general topic area, but in the second case
the task is to remove radically different meanings
from the query. This technique has been used to
remove several meanings from a query iteratively, al-
lowing a user to ?home in on? the desired meaning by
systematically pruning away unwanted features.
2.1 Initial experiments modelling
word-senses
Our first experiments with vector negation were to
determine whether the negation operator could find
different senses of ambiguous words by negating a
word closely related to one of the meanings. A vector
space model was built using Latent Semantic Analy-
sis, similar to the systems of (Landauer and Dumais,
1997; Schu?tze, 1998). The effect of LSA is to in-
crease linear dependency between terms, and for this
reason it is likely that LSA is a crucial step in our
approach. Terms were indexed depending on their
co-occurrence with 1000 frequent ?content-bearing
words? in a 15 word context-window, giving each
term 1000 coordinates. This was reduced to 100 di-
mensions using singular value decomposition. Later
on, document vectors were assigned in the usual
manner by summation of term vectors using tf-idf
weighting (Salton and McGill, 1983, p. 121). Vectors
were normalised, so that the standard (Euclidean)
scalar product and cosine similarity coincided. This
scalar product was used as a measure of term-term
and term-document similarity throughout our exper-
iments. This method was used because it has been
found to be effective at producing good term-term
similarities for word-sense disambiguation (Schu?tze,
1998) and automatic lexical acquisition (Widdows,
2003), and these similarities were used to generate in-
teresting queries and to judge the effectiveness of dif-
ferent forms of negation. More details on the build-
ing of this vector space model can be found in (Wid-
dows, 2003; Widdows and Peters, 2003).
suit suit NOT lawsuit
suit 1.000000 pants 0.810573
lawsuit 0.868791 shirt 0.807780
suits 0.807798 jacket 0.795674
plaintiff 0.717156 silk 0.781623
sued 0.706158 dress 0.778841
plaintiffs 0.697506 trousers 0.771312
suing 0.674661 sweater 0.765677
lawsuits 0.664649 wearing 0.764283
damages 0.660513 satin 0.761530
filed 0.655072 plaid 0.755880
behalf 0.650374 lace 0.755510
appeal 0.608732 worn 0.755260
Terms related to ?suit NOT lawsuit? (NYT data)
play play NOT game
play 1.000000 play 0.779183
playing 0.773676 playing 0.658680
plays 0.699858 role 0.594148
played 0.684860 plays 0.581623
game 0.626796 versatility 0.485053
offensively 0.597609 played 0.479669
defensively 0.546795 roles 0.470640
preseason 0.544166 solos 0.448625
midfield 0.540720 lalas 0.442326
role 0.535318 onstage 0.438302
tempo 0.504522 piano 0.438175
score 0.475698 tyrone 0.437917
Terms related to ?play NOT game? (NYT data)
Table 1: First experiments with negation and word-
senses
Two early results using negation to find senses of
ambiguous words are given in Table 1, showing that
vector negation is very effective for removing the ?le-
gal? meaning from the word suit and the ?sporting?
meaning from the word play, leaving respectively the
?clothing? and ?performance? meanings. Note that re-
moving a particular word also removes concepts re-
lated to the negated word. This gives credence to
the claim that our mathematical model is removing
the meaning of a word, rather than just a string of
characters. This encouraged us to set up a larger
scale experiment to test this hypothesis, which is de-
scribed in Section 4.
3 Other forms of Negation in IR
There have been rigourous studies of Boolean op-
erators for information retrieval, including the p-
norms of Salton et al (1983) and the matrix forms of
Turtle and Croft (1989), which have focussed partic-
ularly on mathematical expressions for conjunction
and disjunction. However, typical forms of negation
(such as NOT p = 1?p) have not taken into account
the relationship between the negated argument and
the rest of the query.
Negation has been used in two main forms in IR
systems: for the removal of unwanted documents af-
ter retrieval and for negative relevance feedback. We
describe these methods and compare them with vec-
tor negation.
3.1 Negation by filtering results after
retrieval
A traditional Boolean search for documents related
to the query a NOT b would return simply those doc-
uments which contain the term a and do not contain
the term b. More formally, let D be the document
collection and let Di ? D be the subset of docu-
ments containing the term i. Then the results to the
Boolean query for a NOT b would be the set Da?D?b,
where D?b is the complement of Db in D. Variants of
this are used within a vector model, by using vector
retrieval to retrieve a (ranked) set of relevant docu-
ments and then ?throwing away? documents contain-
ing the unwanted terms (Salton and McGill, 1983, p.
26). This paper will refer to such methods under the
general heading of ?post-retrieval filtering?.
There are at least three reasons for preferring vec-
tor negation to post-retrieval filtering. Firstly, post-
retrieval filtering is not very principled and is subject
to error: for example, it would remove a long docu-
ment containing only one instance of the unwanted
term.
One might argue here that if a document contain-
ing unwanted terms is given a ?negative-score? rather
than just disqualified, this problem is avoided. This
would leaves us considering a combined score,
sim(d, a NOT b) = d ? a ? ?d ? b
for some parameter ?. However, since this is the
same as d ? (a ? ?b), it is computationally more ef-
ficient to treat a ? ?b as a single vector. This is
exactly what vector negation accomplishes, and also
determines a suitable value of ? from a and b. Thus
a second benefit for vector negation is that it pro-
duces a combined vector for a NOT b which enables
the relevance score of each document to be computed
using just one scalar product operation.
The third gain is that vector retrieval proves to be
better at removing not only an unwanted term but
also its synonyms and related words (see Section 4),
which is clearly desirable if we wish to remove not
only a string of characters but the meaning repre-
sented by this string.
3.2 Negative relevance feedback
Relevance feedback has been shown to improve re-
trieval (Salton and Buckley, 1990). In this process,
documents judged to be relevant have (some multiple
of) their document vector added to the query: docu-
ments judged to be non-relevant have (some multiple
of) their document vector subtracted from the query,
producing a new query according to the formula
Qi+1 = ?Qi + ?
?
rel
Di
|Di|
? ?
?
nonrel
Di
|Di|
,
where Qi is the ith query vector, Di is the set of doc-
uments returned by Qi which has been partitioned
into relevant and non-relevant subsets, and ?, ?, ? ?
R are constants. Salton and Buckley (1990) report
best results using ? = 0.75 and ? = 0.25.
The positive feedback part of this process has
become standard in many search engines with op-
tions such as ?More documents like this? or ?Similar
pages?. The subtraction option (called ?negative rel-
evance feedback?) is much rarer. A widely held opin-
ion is that that negative feedback is liable to harm
retrieval, because it may move the query away from
relevant as well as non-relevant documents (Kowal-
ski, 1997, p. 160).
The concepts behind negative relevance feedback
are discussed instructively by Dunlop (1997). Neg-
ative relevance feedback introduces the idea of sub-
tracting an unwanted vector from a query, but gives
no general method for deciding ?how much to sub-
tract?. We shall refer to such methods as ?Constant
Subtraction?. Dunlop (1997, p. 139) gives an anal-
ysis which leads to a very intuitive reason for pre-
ferring vector negation over constant subtraction. If
a user removes an unwanted term which the model
deems to be closely related to the desired term, this
should have a strong effect, because there is a sig-
nificant ?difference of opinion? between the user and
the model. (From an even more informal point of
view, why would anyone take the trouble to remove
a meaning that isn?t there anyway?). With any kind
of constant subtraction, however, the removal of dis-
tant points has a greater effect on the final query-
statement than the removal of nearby points.
Vector negation corrects this intuitive mismatch.
Recall from Equation 1 that (using normalised vec-
tors for simplicity) the vector a NOT b is given by
a ? (a ? b)b. The similarity of a with a NOT b is
therefore
a ? (a ? (a ? b)b) = 1? (a ? b)2.
The closer a and b are, the greater the (a ? b)2 factor
becomes, so the similarity of a with a NOT b be-
comes smaller the closer a is to b. This coincides ex-
actly with Dunlop?s intuitive view: removing a con-
cept which in the model is very close to the original
query has a large effect on the outcome. Negative
relevance feedback introduces the idea of subtract-
ing an unwanted vector from a query, but gives no
general method for deciding ?how much to subtract?.
We shall refer to such methods as ?Constant Subtrac-
tion?.
4 Evaluation and Results
This section describes experiments which compare
the three methods of negation described above (post-
retrieval filtering, constant subtraction and vector
negation) with the baseline alternative of no nega-
tion at all. The experiments were carried out using
the vector space model described in Section 2.1.
To judge the effectiveness of different methods at
removing unwanted meanings, with a large number
of queries, we made the following assumptions. A
document which is relevant to the meaning of ?term
a NOT term b? should contain as many references to
term a and as few references to term b as possible.
Close neighbours and synonyms of term b are unde-
sirable as well, since if they occur the document in
question is likely to be related to the negated term
even if the negated term itself does not appear.
4.1 Queries and results for negating single
and multiple terms
1200 queries of the form ?term a NOT term b? were
generated for 3 different document collections. The
terms chosen were the 100 most frequently occurring
(non-stop) words in the collection, 100 mid-frequency
words (the 1001st to 1100th most frequent), and 100
low-frequency words (the 5001st to 5100th most fre-
quent). The nearest neighbour (word with highest
cosine similarity) to each positive term was taken
to be the negated term. (This assumes that a user
is most likely to want to remove a meaning closely
related to the positive term: there is no point in re-
moving unrelated information which would not be
retrieved anyway.) In addition, for the 100 most fre-
quent words, an extra retrieval task was performed
with the roles of the positive term and the negated
term reversed, so that in this case the system was be-
ing asked to remove the very most common words in
the collection from a query generated by their near-
est neighbour. We anticipated that this would be
an especially difficult task, and a particularly real-
istic one, simulating a user who is swamped with
information about a ?popular topic? in which they
are not interested.1 The document collections used
were from the British National Corpus (published by
Oxford University, the textual data consisting of ca
90M words, 85K documents), the New York Times
News Syndicate (1994-96, from the North American
News Text Corpus published by the Linguistic Data
Consortium, ca 143M words, 370K documents) and
the Ohsumed corpus of medical documents (Hersh et
al., 1994) (ca 40M words, 230K documents).
The 20 documents most relevant to each query
were obtained using each of the following four tech-
niques.
? No negation. The query was just the positive
term and the negated term was ignored.
? Post-retrieval filtering. After vector retrieval us-
ing only the positive term as the query term,
documents containing the negated term were
eliminated.
? Constant subtraction. Experiments were per-
formed with a variety of subtraction constants.
The query a NOT b was thus given the vector
a??b for some ? ? [0, 1]. The results recorded in
this paper were obtained using ? = 0.75, which
gives a direct comparison with vector negation.
? Vector negation, as described in this paper.
For each set of retrieved documents, the following
results were counted.
? The relative frequency of the positive term.
? The relative frequency of the negated term.
? The relative frequency of the ten nearest neigh-
bours of the negative term. One slight subtlety
here is that the positive term was itself a close
1For reasons of space we do not show the retrieval per-
formance on query terms of different frequencies in this
paper, though more detailed results are available from
the author on request.
neighbour of the negated term: to avoid incon-
sistency, we took as ?negative neighbours? only
those which were closer to the negated term than
to the positive term.
? The relative frequency of the synonyms of the
negated term, as given by the WordNet database
(Fellbaum, 1998). As above, words which were
also synonyms of the positive term were dis-
counted. On the whole fewer such synonyms
were found in the Ohsumed and NYT docu-
ments, which have many medical terms and
proper names which are not in WordNet.
Additional experiments were carried out to com-
pare the effectiveness of different forms of negation
at removing several unwanted terms. The same 1200
queries were used as above, and the next nearest
neighbour was added as a further negative argument.
For two negated terms, the post-retrieval filtering
process worked by discarding documents containing
either of the negative terms. Constant subtraction
worked by subtracting a constant multiple of each
of the negated terms from the query. Vector nega-
tion worked by making the query vector orthogonal
to the plane generated by the two negated terms, as
in Equation 2.
Results were collected in much the same way as the
results for single-argument negation. Occurrences
of each of the negated terms were added together,
as were occurrences of the neighbours and WordNet
synonyms of either of the negated words.
The results of our experiments are collected in
Table 2 and summarised in Figure 1. The results
for a single negated term demonstrate the following
points.
? All forms of negation proved extremely good
at removing the unwanted words. This is triv-
ially true for post-retrieval filtering, which works
by discarding any documents that contain the
negated term. It is more interesting that con-
stant subtraction and vector negation performed
so well, cutting occurrences of the negated word
by 82% and 85% respectively compared with the
baseline of no negation.
? On average, using no negation at all retrieved
the most positive terms, though not in every
case. While this upholds the claim that any form
of negation is likely to remove relevant as well
as irrelevant results, the damage done was only
around 3% for post-retrieval filtering and 25%
for constant and vector negation.
? These observations alone would suggest that
post-retrieval filtering is the best method for
the simple goal of maximising occurrences of
the positive term while minimising the occur-
rences of the negated term. However, vec-
tor negation and constant subtraction dramati-
cally outperformed post-retrieval filtering at re-
moving neighbours of the negated terms, and
were reliably better at removing WordNet syn-
onyms as well. We believe this to be good
evidence that, while post-search filtering is by
definition better at removing unwanted strings,
the vector methods (either orthogonal or con-
stant subtraction) are much better at removing
unwanted meanings. Preliminary observations
suggest that in the cases where vector negation
retrieves fewer occurrences of the positive term
than other methods, the other methods are of-
ten retrieving documents that are still related in
meaning to the negated term.
? Constant subtraction can give similar results to
vector negation on these queries (though the
vector negation results are slightly better). This
is with queries where the negated term is the
closest neighbour of the positive term, and the
assumption that the similarity between these
pairs is around 0.75 is a reasonable approxima-
tion. However, further experiments with a va-
riety of negated arguments chosen at random
from a list of neighbours demonstrated that in
this more general setting, the flexibility provided
by vector negation produced conclusively better
results than constant subtraction for any single
fixed constant.
In addition, the results for removing multiple
negated terms demonstrate the following points.
? Removing another negated term further reduces
the retrieval of the positive term for all forms of
negation. Constant subtraction is the worst af-
fected, performing noticeably worse than vector
negation.
? All three forms of negation still remove many
occurrences of the negated term. Vector nega-
tion and (trivially) post-search filtering perform
as well as they do with a single negated term.
However, constant subtraction performs much
worse, retrieving more than twice as many un-
wanted terms as vector negation.
? Post-retrieval filtering was even less effective at
removing neighbours of the negated term than
with a single negated term. Constant subtrac-
tion also performed much less well. Vector nega-
tion was by far the best method for remov-
ing negative neighbours. The same observation
1 negated term 2 negated terms
BNC NYT Ohsumed BNC NYT Ohsumed
No Negation Positive term 0.53 1.18 2.57 0.53 1.18 2.57
Negated term 0.37 0.66 1.26 0.45 0.82 1.51
Negative neighbours 0.49 0.74 0.45 0.69 1.10 0.71
Negative synonyms 0.24 0.22 0.10 0.42 0.42 0.20
Post-retrieval Positive term 0.61 1.03 2.51 0.58 0.91 2.35
filtering Negated term 0 0 0 0 0 0
Negative neighbours 0.31 0.46 0.39 0.55 0.80 0.67
Negative synonyms 0.19 0.22 0.10 0.37 0.39 0.37
Constant Positive term 0.52 0.82 1.88 0.42 0.70 1.38
Subtraction Negated term 0.09 0.13 0.20 0.18 0.21 0.35
Negative neighbours 0.08 0.11 0.14 0.30 0.33 0.18
Negative synonyms 0.19 0.16 0.07 0.33 0.29 0.12
Vector Positive term 0.50 0.83 1.85 0.45 0.69 1.51
Negation Negated term 0.08 0.12 0.16 0.08 0.11 0.15
Negative neighbours 0.10 0.10 0.10 0.17 0.16 0.16
Negative synonyms 0.18 0.16 0.07 0.31 0.27 0.12
Table 2: Table of results showing the percentage frequency of different terms in retrieved documents
Average results across corpora for one negated term
0
1
No negation Post-retrieval filtering Constant Subtraction Vector negation
% frequency
Average results across corpora for two negated terms
0
1
No negation Post-retrieval filtering Constant Subtraction Vector negation
% frequency
Positive Term Negated Term
Vector Neighbours of Negated Word WordNet Synonyms of Negated Word
Figure 1: Barcharts summarising results of Table 2
holds for WordNet synonyms, though the results
are less pronounced.
This shows that vector negation is capable of re-
moving unwanted terms and their related words from
retrieval results, while retaining more occurrences of
the original query term than constant subtraction.
Vector negation does much better than other meth-
ods at removing neighbours and synonyms, and we
therefore expect that it is better at removing doc-
uments referring to unwanted meanings of ambigu-
ous words. Experiments with sense-tagged data are
planned to test this hypothesis.
The goal of these experiments was to evaluate the
extent to which the different methods could remove
unwanted meanings, which we measured by count-
ing the frequency of unwanted terms and concepts
in retrieved documents. This leaves the problems of
determining the optimal scope for the negation quan-
tifier for an IR system, and of developing a natural
user interface for this process for complex queries.
These important challenges are beyond the scope of
this paper, but would need to be addressed to in-
corporate vector negation into a state-of-the-art IR
system.
5 Conclusions
Traditional branches of science have exploited the
structure inherent in vector spaces and developed
rigourous techniques which could contribute to nat-
ural language processing. As an example of this po-
tential fertility, we have adapted the negation and
disjunction connectives used in quantum logic to the
tasks of word-sense discrimination and information
retrieval.
Experiments focussing on the use of vector nega-
tion to remove individual and multiple terms from
queries have shown that this is a powerful and ef-
ficient tool for removing both unwanted terms and
their related meanings from retrieved documents.
Because it associates a unique vector to each query
statement involving negation, the similarity between
each document and the query can be calculated using
just one scalar product computation, a considerable
gain in efficiency over methods which involve some
form of post-retrieval filtering.
We hope that these preliminary aspects will be
initial gains in developing a concrete and effective
system for learning, representing and composing as-
pects of lexical meaning.
Demonstration
An interactive demonstration of negation for word
similarity and document retrieval is publicly avail-
able at http://infomap.stanford.edu/webdemo.
References
Ricardo Baeza-Yates and Berthier Ribiero-Neto.
1999. Modern Information Retrieval. Addison
Wesley / ACM Press.
Garrett Birkhoff and John von Neumann. 1936. The
logic of quantum mechanics. Annals of Mathemat-
ics, 37:823?843.
Mark Dunlop. 1997. The effect of accessing non-
matching documents on relevance feedback. ACM
Transactions on Information Systems, 15(2):137?
153, April.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press, Cam-
bridge MA.
William Hersh, Chris Buckley, T. J. Leone, and
David Hickam. 1994. Ohsumed: An interactive
retrieval evaluation and new large test collection
for research. In Proceedings of the 17th Annual
ACM SIGIR Conference, pages 192?201.
Gerald Kowalski. 1997. Information retrieval sys-
tems: theory and implementation. Kluwer aca-
demic publishers, Norwell, MA.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to plato?s problem: The latent semantic anal-
ysis theory of acquisition. Psychological Review,
104(2):211?240.
Gerard Salton and Chris Buckley. 1990. Improv-
ing retrieval performance by relevance feedback.
Journal of the American society for information
science, 41(4):288?297.
Gerard Salton and Michael McGill. 1983. Introduc-
tion to modern information retrieval. McGraw-
Hill, New York, NY.
Gerard Salton, Edward A. Fox, and Harry Wu. 1983.
Extended boolean information retrieval. Commu-
nications of the ACM, 26(11):1022?1036, Novem-
ber.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Howard Turtle and W. Bruce Croft. 1989. Inference
networks for document retrieval. In Proceedings of
the 13th Annual ACM SIGIR Conference, pages
1?24.
Dominic Widdows and Stanley Peters. 2003. Word
vectors and quantum logic. In Mathematics of
Language 8, Bloomington, Indiana.
Dominic Widdows. 2003. Unsupervised methods for
developing taxonomies by combining syntactic and
statistical information. HLT-NAACL, Edmonton,
Canada.
Using LSA and Noun Coordination Information to Improve the Precision
and Recall of Automatic Hyponymy Extraction
Scott Cederberg Dominic Widdows
Center for the Study of Language and Information
210 Panama Street
Stanford University
Stanford CA 94305
{cederber,dwiddows}@csli.stanford.edu
Abstract
In this paper we demonstrate methods of im-
proving both the recall and the precision of au-
tomatic methods for extraction of hyponymy
(IS A) relations from free text. By applying la-
tent semantic analysis (LSA) to filter extracted
hyponymy relations we reduce the rate of er-
ror of our initial pattern-based hyponymy ex-
traction by 30%, achieving precision of 58%.
Applying a graph-based model of noun-noun
similarity learned automatically from coordi-
nation patterns to previously extracted correct
hyponymy relations, we achieve roughly a five-
fold increase in the number of correct hy-
ponymy relations extracted.
1 Introduction
This paper demonstrates that mathematical models for
measuring semantic similarity between concepts can be
used to improve the learning of hyponymy relationships
between concepts from free text. In particular, we show
that latent semantic analysis can be used to filter results,
giving an increase in precision, and that neighbors in a
graph built from coordination information can be used to
improve recall.
The goal of extracting semantic information from text
is well-established, and has encouraged work on lexical
acquisition (Roark and Charniak, 1998), information ex-
traction (Cardie, 1997), and ontology engineering (Hahn
and Schnattinger, 1998). The purpose of this kind of
work is to collect information about the meanings of lexi-
cal items or phrases, and the relationships between them,
so that the process of building semantic resources (such
as ontologies and dictionaries) by hand can be automated
or at least helped.
One of the standard ways of arranging concepts is in a
concept hierarchy or taxonomy such as the WordNet noun
taxonomy (Fellbaum, 1998). The fundamental relation-
ship between objects in a taxonomy is called hyponymy,
where y is a hyponym of x if every y is also an x. For
example, every trout is also a fish, so we say that trout
is a hyponym (?below name?) of fish and conversely, fish
is a hypernym (?above name?) of trout. Other names ex-
ist for variants of the hyponymy relationship, such as an
IS A relationship, a parent-node / child-node relationship,
and a broader term / narrower term relationship. It is also
noted that the genus of an object, in traditional lexico-
graphic terms, is often a hypernym of that object (Guthrie
et al, 1996). Throughout this paper we will write y < x
for the relationship ?y is a hyponym of x?. In this paper,
we use the hyponymy relationship to describe subset re-
lationships, so we regard y < x to be true if the set of y?s
can reasonably be said to be a subset of the set of x?s.1
Because hyponymy relationships are so central to
knowledge engineering, there have been numerous at-
tempts to learn them from text, beginning with those
of Hearst (1992). We review this work in Section 2,
where we reproduce similar experiments as a baseline
from which to expand. The rest of the paper demon-
strates ways in which other mathematical models built
from text corpora can be used to improve hyponymy ex-
traction. In Section 3, we show how latent semantic anal-
ysis can be used to filter potential relationships accord-
ing to their ?semantic plausibility?. In Section 4, we
show how correctly extracted relationships can be used
as ?seed-cases? to extract several more relationships, thus
improving recall; this work shares some similarities with
that of Caraballo (1999). In Section 5 we show that com-
bining the techniques of Section 3 and Section 4 improves
both precision and recall. Section 6 demonstrates that
1Another possible view is that ?hyponymy? should only re-
fer to core relationships, not contingent ones (so pheasant <
bird might be accepted but pheasant < food might not be, be-
cause it depends on context and culture). We use the broader
?subset? definition because contingent relationships are an im-
portant part of world-knowledge (and are therefore worth learn-
ing), and because in practice we found the distinction difficult to
enforce. Another definition is given by Caraballo (1999): ?. . . a
word A is said to be a hypernym of a word B if native speakers
of English accept the sentence ?B is a (kind of) A.? ?
linguistic tools such as lemmatization can be used to re-
liably put the extracted relationships into a normalized or
?canonical? form for addition to a semantic resource.
2 Pattern-Based Hyponymy Extraction
The first major attempt to extract hyponyms from text
was that of Hearst (1992), described in more detail in
(Hearst, 1998), who extracted relationships from the text
of Grolier?s Encyclopedia. The method is illustrated by
the following example. The sentence excerpt
Even then, we would trail behind other Euro-
pean Community members, such as Germany,
France and Italy. . . (BNC)2
indicates that Germany, France, and Italy are all Euro-
pean Community members. More generally, phrases of
the form
x such as y1 (y2, . . . , and/or yn)
frequently indicate that the yi are all hyponyms of
the hypernym x. Hearst identifies several other con-
structions that have a tendency to indicate hyponymy,
calling these constructions lexicosyntactic patterns, and
analyses the results. She reports that 52% of the re-
lations extracted by the ?or other? pattern (see Ta-
ble 1) were judged to be ?pretty good relations?. A
more recent variant of this technique was implemented
by Alfonseca and Manandhar (2001), who compare the
collocational patterns of words from The Lord of the
Rings with those of words in the WordNet taxonomy,
adding new nouns to WordNet with an accuracy of
28%. Using a much more knowledge-intensive approach,
Hahn and Schnattinger (1998) improve ?learning accu-
racy? from around 50% to over 80% by forming a number
of hypotheses and accepting only those which are most
consistent with their current ontology. Their methods are
like ours in that the ?concept learning? combines infor-
mation from several occurrences, but differ in that they
rely on a detailed existing ontology into which to fit the
new relationships between concepts.
Our initial experiment was to construct a hyponymy
extraction system based on the six lexicosyntactic pat-
terns identified in (Hearst, 1998), which are listed in Ta-
ble 1. We first used a chunker to mark noun groups, and
then recognized and extracted noun groups occurring as
part of one of the extraction patterns.3
We applied these extraction patterns to an approxi-
mately 430,000-word extract from the beginning of the
2This excerpt and others in this paper are from the British
National Corpus.
3The chunker used was LT CHUNK, from
the University of Edinburgh?s Language Tech-
nology Group. It can be downloaded from
http://www.ltg.ed.ac.uk/software/chunk/.
x such as y1 (, y2, . . . , and/or yn)
such x as y1 (, y2, . . . , and/or yn)
y1 (, y2, . . . , yn,) or other x
y1 (, y2, . . . , yn,) and other x
x, including y1 (, y2, . . . , and/or yn)
x, especially y1 (, y2, . . . , and/or yn)
Table 1: The lexicosyntactic patterns described by
Hearst (1998), which we used in the work described in
this paper. Each of these patterns is taken to indicate the
hyponymy relation(s) yi < x.
British National Corpus (BNC). The patterns extracted
513 relations. We selected 100 of the extracted relations
at random and each author evaluated them by hand, scor-
ing each relation on a scale from 4 (correct) to 0 (incor-
rect), defined as follows:
4. Extracted hypernym and hyponym exactly correct as
extracted.
3. Extracted hypernym and hyponym are correct after
a slight modification, such as depluralization or the
removal of an article (e.g. a, the) or other preceding
word.
2. Extracted hypernym and hyponym have something
correct, e.g. a correct noun without a necessary
prepositional phrase, a correct noun with a superflu-
ous prepositional phrase, or a noun + prepositional
phrase where the object of the preposition is correct
but the preposition itself and the noun to which it
attaches are superfluous. Thus these hyponymy re-
lations are potentially correct but will require poten-
tially difficult processing to extract an exactly cor-
rect relation. Some of the errors which would need
to be corrected were in preprocessing (e.g. on the
part of the noun-group chunker) and others were er-
rors caused by our hyponymy extractor (e.g. tacking
on too many or too few prepositional phrases).
1. The relation extracted is correct in some sense, but
is too general or too context specific to be useful.
This category includes relations that could be made
useful by anaphora resolution (e.g. replacing ?this?
with its referent).
0. The relation extracted is incorrect. This results when
the constructions we recognize are used for a pur-
pose other than indicating the hyponymy relation.
The results of each of the authors? evaluations of the
100-relation random sample are show in Table 2.4 For
4Table 2 suggests that although there is significant disagree-
ment about how to assign scores of 1 and 0, inter-annotator
score Author 1 Author 2
4 4 2
3 34 35
2 14 13
1 35 22
0 13 28
Table 2: Number of the 100 randomly selected hyponymy
relations (of 513 extracted) to which each of the authors
assigned the five available scores.
purposes of calculating precision, we consider those rela-
tions with a score of 4 or 3 to be correct and those with a
lower score to be incorrect. After discussion between the
authors on disputed annotations to create ?gold standard?
annotations, we found that 40 of the 100 relations in our
random sample were correct according to this criterion.
In other words, 40% of the relations extracted were ex-
actly correct or would be correct with the use of minor
post-processing consisting of lemmatization and removal
of common types of qualifying words. (We describe our
application of such post-processing in Section 6.)
Thus our initial implementation of Hearst-style hy-
ponymy extraction achieved 40% precision. This is less
than the 52% precision reported in (Hearst, 1998). We
believe this discrepancy to be mainly due to the dif-
ference between working with the BNC and Grolier?s
encyclopedia?as noted by Hearst, the encyclopedia is
designed to be especially rich in conceptual relationships
presented in an accessible format.
Various problems with the pattern-based extraction
method explain the 60% of extracted relations that were
incorrect and/or useless. One problem is that the con-
structions that we assume to indicate hyponymy are often
used for other purposes. For instance, the pattern
x including y1, y2, . . . , and yn
which indicates hyponymy in sentences such as
Illnesses, including chronic muscle debility,
herpes, tremors and eye infections, have come
and gone. (BNC)
and is a quite productive source of hyponymy relations,
can be used instead to indicate group membership:
agreement regarding the assignment of scores of 4, 3, and 2 is
quite high. Indeed, considering the rougher distinction we use
for reporting precision, in which scores of 4 and 3 are deemed
correct and scores of 2, 1, and 0 are deemed incorrect, we found
that inter-annotator agreement across all relations annotated (in-
cluding those from this random sample and those from the sam-
ple described in Section 3) was 86%. We discussed each of
the relations in the 14% of cases where we disagreed until we
reached agreement; this produced the ?gold standard? annota-
tions to which we refer.
Often entire families including young children
need practical home care . . . (BNC)
While all children are members of families, the hy-
ponymy relationship child < family does not hold, since
it is not true that all children are families.
Another source of errors in lexicosyntactic hyponymy
extraction is illustrated by the sentence
A kit such as Edme Best Bitter, Tom Caxton
Best Bitter, or John Bull Best Bitter will be a
good starting kit. (BNC)
which indicates the (potentially useful) relations Edme
Best Bitter < beer-brewing kit, Tom Caxton Best Bitter
< beer-brewing kit, and John Bull Best Bitter < beer-
brewing kit, but only when we use the context to infer
that the type of ?kit? referred to is a beer-brewing kit, a
process that is difficult by automatic means. Without this
inference, the extracted relations Edme Best Bitter < kit,
etc., while correct in a certain sense, are not helpful. One
frequent source of such problems is anaphora that require
resolution.
There are also problems related to prepositional phrase
attachment.
3 Improving Precision Using Latent
Semantic Analysis
Solving all of the problems with pattern-based hyponymy
extraction that we describe above would require near-
human-level language understanding, but we have ap-
plied a far simpler technique for filtering out many of the
incorrect and spurious extracted relations with good re-
sults, using a variant of latent semantic analysis (LSA)
(Deerwester et al, 1990; Baeza-Yates and Ribiero-Neto,
1999, p. 44). LSA is a method for representing words
as points in a vector space, whereby words which are re-
lated in meaning should be represented by points which
are near to one another. The LSA model we built is sim-
ilar to that described in (Schu?tze, 1998). First 1000 fre-
quent content words (i.e. not on the stoplist)5 were chosen
as ?content-bearing words?. Using these content-bearing
words as column labels, the other words in the corpus
were assigned row vectors by counting the number of
times they occured within a 15-word context window of
a content-bearing word. Singular-value decomposition
(Deerwester et al, 1990) was then used to reduce the
number of dimensions from 1000 to 100. Similarity be-
tween two vectors (points) was measured using the cosine
of the angle between them, in the same way as the simi-
larity between a query and a document is often measured
5A ?stoplist? is a list of frequent words which have little
semantic content in themselves, such as prepositions and pro-
nouns (Baeza-Yates and Ribiero-Neto, 1999, p. 167).
score Author 1 Author 2
4 4 5
3 57 52
2 18 14
1 12 19
0 9 10
Table 3: Number of the 100 top-ranked hyponymy re-
lations (of 513 extracted) to which each of the authors
assigned the five available scores.
in information retrieval (Baeza-Yates and Ribiero-Neto,
1999, p. 28). Effectively, we could use LSA to measure
the extent to which two words x and y usually occur in
similar contexts. This LSA similarity score will be called
sim(x, y).
Since we expect a hyponym and its hypernym to be
semantically similar, we can use the LSA similarity be-
tween two terms as a test of the plausibility of a putative
hyponymy relation between those terms. If their similar-
ity is low, it is likely that they do not have a true and use-
ful hyponymy relationship; the relation was probably ex-
tracted erroneously for one or more of the reasons listed
above. If the similarity between two terms is high, we
have increased confidence that a hyponymy relationship
exists between them, because we know that they are at
least in similar ?semantic regions?.
We ranked the 513 putative hyponym/hypernym pairs
that we extracted from our trial excerpt of the BNC ac-
cording to the similarity between the putative hypernym
and the putative hyponym in each pair; i.e. for each pair
x and y where the relationship y < x had been suggested,
we calculated the cosine similarity sim(x, y), then we
ranked the extracted relations from highest to lowest sim-
ilarity. We then manually evaluated the accuracy of the
top 100 extracted relations according to this ranking us-
ing the 5-point scale described in Section 2. We found
that 58 of these 100 top-ranked relations received scores
of 4 or 3 according to our ?gold standard? annotations.
Comparing this 58% precision with the 40% precision
obtained on a random sample in Section 2, we determine
that LSA achieved a 30% reduction in error (see Table 3
for a breakdown of annotation results by author).6
Thus LSA proved quite an effective filter. LSA pro-
vides broad-based semantic information learned statis-
tically over many occurences of words; lexicosyntactic
hyponymy extraction learns semantic information from
specific phrases within a corpus. Thus we have bene-
fitted from combining local patterns with statistical in-
6It should be noted that 24 of the top 100 hyponymy rela-
tions evaluated in this section were also in the randomly-chosen
sample of 100 relations described in Section 2. Thus there were
a total of 176 distinct hyponymy relations across both test sets.
formation. Considered in analogy with the process by
which humans learn from reading, we might think of
the semantic information learned by LSA as background
knowledge that is applied by the reader when determining
what can accurately be gleaned from a particular sentence
when it is read.
4 Improving Recall Using Coordination
Information
One of the main challenges facing hyponymy extraction
is that comparatively few of the correct relations that
might be found in text are expressed overtly by the simple
lexicosyntactic patterns used in Section 2, as was appar-
ent in the results presented in that section.
This problem has been addressed by Caraballo (1999),
who describes a system that first builds an unlabelled hi-
erarchy of noun clusters using agglomerative bottom-up
clustering of vectors of noun coordination information.
The leaves of this hierarchy (corresponding to nouns)
are assigned hypernyms using Hearst-style lexicosyntac-
tic patterns. Internal nodes in the hierarchy are then la-
belled with hypernyms of the leaves they subsume ac-
cording to a vote of these subsumed leaves.
We proceed along similar lines, using noun coordi-
nation information and an alternative graph-based clus-
tering method. We do not build a complete hierarchy,
but our method nonetheless obtains additional hypernym-
hyponym pairs not extracted by lexicosyntactic patterns.
Our method is based on the following sort of inference.
Consider the sentence
This is not the case with sugar, honey, grape
must, cloves and other spices which increase
its merit. (BNC)
which provides evidence that clove is a kind of spice.
Given this, the sentence
Ships laden with nutmeg or cinnamon, cloves
or coriander once battled the Seven Seas to
bring home their precious cargo. (BNC)
might suggest that nutmeg, cinnamon, and coriander are
also spices, because they appear to be similar to cloves.
Thus we can learn the hyponymy relations nutmeg <
spice, cinnamon < spice, and coriander < spice that
are not directly attested by lexicosyntactic patterns in our
training corpus.
This kind of information from coordination patterns
has been used for work in automatic lexical acquisition
(Riloff and Shepherd, 1997; Roark and Charniak, 1998;
Widdows and Dorow, 2002). The basic rationale behind
these methods is that words that occur together in lists
are usually semantically similar in some way: for exam-
ple, the phrase
y1, y2, and y3
suggests that there is some link between y1 and y2, etc.
Performing this analysis on a whole corpus results in a
data structure which holds a collection of nouns and ob-
served noun-noun relationships. If we think of the nouns
as nodes and the noun-noun relationships as edges, this
data structure is a graph (Bolloba?s, 1998), and combina-
toric methods can be used to analyze its structure.
Work using such techniques for lexical acquisition has
proceeded by building classes of related words from a
single ?seed-word? with some desired property (such as
being a representative of a paticular semantic class). For
example, in order to extract a class of words referring to
kinds of disease from a corpus, you start with a single
seed-word such as typhoid, and then find other nouns that
occur in lists with typhoid. Using the graph model de-
scribed above, Widdows and Dorow (2002) developed a
combinatoric algorithm for growing clusters from a sin-
gle seed-word, and used these methods to find correct
new members for chosen categories with an accuracy of
over 80%.
The idea that certain patterns can be identified using
finite-state techniques and used as evidence for seman-
tic relationships is the same as Hearst?s (1992), but ap-
pears to be more effective for finding just similar words
rather than hypernyms because there are many more in-
stances of simple coordination patterns than of hyper-
nymy patterns?in the lists we used to extract these re-
lationships, we see much more cooccurence of words on
the same ontological level than between words from dif-
ferent ontological levels. For example, in the BNC there
are 211 instances of the phrase ?fruit and vegetables? and
9 instances of ?carrots and potatoes?, but no instances of
?fruit and potatoes?, only 1 instance of ?apples and veg-
etables?, and so on.
This sort of approach should be ideal for improving
the recall of automatic hyponymy extraction, by using the
hyponym from each of the correct hypernym/hyponym
pairs as a seed-word for the category represented by the
hypernym?for example, from the relationship clove <
spice, the word clove could be taken as a seed-word, with
the assumption that words which frequently occur in co-
ordination with clove are also names of spices.
We used the algorithm of (Widdows and Dorow, 2002)
on the British National Corpus to see if many more hy-
ponymy relations would be extracted in this way. For
each correct pair y < x where y was a single-word hy-
ponym of x discovered by the lexicosyntactic patterns of
Section 2, we collected the 10 words most similar to y ac-
cording to this algorithm and tested to see if these neigh-
bors were also hyponyms of x.
Of the 176 extracted hyponyms that we evaluated by
hand in the overlapping test sets described in Section 2
and Section 3, 95 were rated 4 or 3 on our 5-point scor-
ing system (Section 2) by at least one of the authors. Con-
sidering these correct or nearly-correct relations in their
hand-corrected form, we found that 45 of these 95 rela-
tions involved single-word hyponyms. (We restricted our
attention to these 45 relations because the graph model
was built using only single words as nodes in the graph.)
This set of 45 correct hypernym ?seed-pairs? was ex-
tended by another potential 459 pairs (slightly more than
10 for each seed-pair because if there was a tie for 10th
place both neighbors were used). Of these, 211 (46%)
were judged to be correct hypernym pairs and 248 (54%)
were not.7 This accuracy compares favorably with the ac-
curacy of 40% obtained for the raw hyponymy extraction
experiments in Section 2, suggesting that inferring new
relations by using corpus-based similarities to previously
known relations is more reliable than trying to learn com-
pletely new relations even if they are directly attested in
the corpus. However, our accuracy falls way short of the
figure of 82% reported by Widdows and Dorow (2002).
We believe this is because the classes in (Widdows and
Dorow, 2002) are built from carefully selected seed-
examples: ours are built from an uncontrolled sample
of seed-examples extracted automatically from a corpus.
We outline three cases where this causes a critical differ-
ence.
The ambiguity of ?mass?
One of the correct hyponymy relations extracted in our
experiments in Section 2 was mass < religious service.
Using mass as a seed suggested the following candidates
as potential hyponyms of religious service:
Seed Semantically Similar Words
mass length weight angle shape depth
height range charge size momentum
All these neighbors are related to the ?measurement of
physical property? sense of the word mass rather than the
?religious service? sense. The inferred hyponymy rela-
tions are all incorrect because of this mismatch.
The specific properties of ?nitrogen?
Another true relation we extracted was nitrogen < nu-
trient. Using the same process as above gave the follow-
ing neighbors of nitrogen:
Seed Semantically Similar Words
nitrogen methane dioxide carbon hydrogen methanol
vapour ammonia oxide oxygen monoxide water
These neighboring terms are not in general nutrients,
and the attempt to infer new hyponymy relations is a fail-
7As before, we consider scores of 4 and 3 on our 5-point
scale to be correct and lower scores to be incorrect. The pre-
cision of graph-model results (reported in this section and in
Section 5), unlike those reported elsewhere, are based on the
annotations of a single author.
ure in this case. While the relationship nitrogen < nu-
trient is one of the many facts which go to make up the
vast store of world-knowledge that an educated adult uses
for reasoning, it is not a necessary property of nitrogen
itself, and one could arguably ?know? the meaning of
nitrogen without being aware of this fact. In traditional
lexicographic terms, the fact that nitrogen is a nutrient
might be regarded as part of the differentiae rather than
the genus of nitrogen. Had our seed-pair instead been
nitrogen < gas or nitrogen < chemical element, many
correct hyponymy relations would have been inferred by
our method, and both of these classifications are central
to the meaning of nitrogen.
Accurate levels of abstraction for ?dill?
Finally, even when the hyponymy relationship y < x
used as a seed-case was central to the meaning of y and
all of the neighbors of y were related to this meaning,
they were still not always hyponyms of x but sometimes
members of a more general category. For example, using
the correct seed-pair dill < herb we retrieved the follow-
ing suggested hyponyms for herb:
Seed Semantically Similar Words
dill rind fennel seasoning juice sauce
pepper parsley vinegar oil pur
All of these items are related to dill, but only some of
them are herbs. The other items should also be placed
in the same general area of a taxonomy as dill, but as
cooking ingredients rather than specifically herbs.
In spite of these problems, the algorithm for improv-
ing recall by adding neighbors of the correct hyponyms
worked reasonably well, obtaining 211 correct relation-
ships from 45 seeds, an almost fivefold increase in recall,
with an accuracy of 46%, which is better than that of our
baseline pattern-matching hyponymy extractor.
It is possible that using coordination (such as co-
occurence in lists) as a measure of noun-noun similarity
is well-adapted for this sort of work, because it mainly
extracts ?horizontal? relationships between items of sim-
ilar specificity or similar generality. Continuing the ge-
ometric analogy, these mainly ?horizontal? relationships
might be expected to combine particularly well with seed
examples of ?vertical? relationships, i.e. hyponymy rela-
tionships.
5 Combining LSA and Coordination to
Improve Precision and Recall
Having used two separate techniques to improve preci-
sion and recall in isolation, it made sense to combine
our methods to improve performance overall. This was
accomplished by applying LSA filtering as described in
Section 3 to the results obtained by extending our initial
hypernym pairs with coordination patterns in Section 4.
LSA filtering of extended results: phase I
The first application of filtering to the additional hy-
ponymy relations obtained using noun-cooccurrence was
straightforward. We took the 459 potential hyponymy
relationships obtained in Section 4. For each of the
prospective hyponyms y of a given hypernym x, we com-
puted the LSA similarity sim(x, y). We then considered
only those potential hyponyms whose LSA similarity to
the hypernym surpassed a certain threshhold. Using this
technique with an experimentally determined threshhold
of 0.15, we obtained a set of 260 hyponymy relations of
which 166 were correct (64%, as opposed to the 46%
correct in the unfiltered results). The LSA filtering had
removed 154 incorrect relationships and only 45 correct
ones, reducing the overall error rate by 33%.
In particular, this technique removed all but one of
the spurious religious service hyponyms which were ob-
tained through inappropriate similarities with mass in the
example in Section 4, though it was much less effective
in filtering the neighbors of nitrogen and dill, as might be
expected.
LSA filtering of extended results: phase II
For some of the hyponymy relations to which we ap-
plied our extension technique, the hypernym had multiple
words.8 In some of these cases, it was clear that one of
the words in the hypernym had a meaning more closely
related to the original (correct) hyponym. For instance, in
the mass < religious service relation, the word religious
tells us more about the appropriate meaning of mass than
does the word service. It thus seemed that, at least in cer-
tain cases, we might be able to get more traction in LSA
filtering of potential additional hyponyms by first select-
ing a particular word from the hypernym as the ?most
important? and using that word rather than the entire hy-
pernym for filtering.9
We thus applied a simple two-step algorithm to refine
the filtering technique presented above:
1. The LSA similarity between the original (correct)
hyponym and each word in the hypernym is com-
puted. The words of the hypernym are ranked ac-
cording to these similarities.
2. The word in the hypernym that has the highest LSA
similarity to the original (correct) hyponym is used
instead of the entire hypernym for phase-I-style fil-
tering.
8The graph model used to obtain new candidate hyponyms
was built using single words, which is why our extended results
include some multiword expressions among the hypernyms but
only single word hyponyms.
9When using an entire multiword hypernym for filtering, a
term-vector was produced for the multiword hypernym by aver-
aging the LSA vectors for the constituent words.
This filtering technique, with an LSA-similarity thresh-
hold of 0.15, resulted in the extraction of 35 correct and
25 incorrect relationships. In contrast, using LSA simi-
larity with the whole expression rather than the most im-
portant word resulted in the extraction of 32 correct and
30 incorrect relationships for those hypernyms with mul-
tiple words. On the face of it, selecting only the most
important part of the hypernym for comparison enabled
us to obtain more correct and fewer incorrect relations,
but it is also clear that by this stage in our experiments
our sample of seed-relationships had become too small
for these results to be statistically significant.
However, the examples we considered did demonstrate
another point?that LSA could help to determine which
parts of a multiword expression were semantically rel-
evant. For example, one of the seed-relationships was
France < European Community member. Finding that
sim(france, european) > sim(france, community),
we could infer that the adjective European was central to
the meaning of the hyponym, whereas for the example
wallflowers < hardy biennials the opposite conclusion,
that hardy is an adjectival modifier which isn?t central to
the relationship, could be drawn. However, these conclu-
sions could also be drawn by using established colloca-
tion extraction techniques (Manning and Schu?tze, 1999,
Ch. 5) to find semantically significant multiword expres-
sions.
6 Obtaining Canonical Forms for
Relations
An important part of extracting semantic relations like
those discussed in this paper is converting the terms in
the extracted relations to a canonical form. In the case
of our extracted hyponymy relations, such normalization
consists of two steps:
1. Removing extraneous articles and qualifiers. Our
extracted hyponyms and hypernyms were often in
the form ?another x?, ?some x?, and so forth, where
x is the hypernym or hyponym that we actually want
to consider.
2. Converting nouns to their singular form. This is el-
ementary morphological analysis, or a limited form
of lemmatization.
We performed the second of these steps using the
morph morphological analysis software (Minnen et al,
2001).10 To perform the first step of removing modifiers,
we implemented a Perl script to do the following:
10This software is freely available from
http://www.cogs.susx.ac.uk/lab/nlp/carroll/morph.html.
? Remove leading determiners from the beginning of
the hypernym and from the beginning of the hy-
ponym.
? Remove leading prepositions from the beginning of
the hypernym. Doing this after removing leading
determiners eliminates the common ?those of? con-
struction.
? Remove cardinal numbers from the hypernym and
the hyponym.
? Remove possessive prefixes from the hypernym and
the hyponym.
? Remove ?set of? and ?number of? from the hy-
pernym and the hyponym. This ad hoc but rea-
sonable procedure eliminates common troublesome
constructions not covered by the above rules.
? Remove leading adjectives from hypernyms, but not
from hyponyms. In addition to removing ?other?,
this amounts to playing it safe. By removing leading
adjectives we make potential hypernyms more gen-
eral, and thus more likely to be a superset of their
potential hyponym. While this removal sometimes
makes the learned relationship less useful, it sel-
dom makes it incorrect. We leave adjectives on hy-
ponyms to make them more specific, and thus more
likely to be a subset of their purported hypernym.
Using these simple rules, we were able to convert 73
of the 78 relations orginally scored as 3 (see Section 2)
to relations receiving a score of 4. This demonstrates as
a ?proof of concept? that comparatively simple language
processing techniques can be used to map relationships
from the surface forms in which they were observed in
text to a canonical form which could be included in a se-
mantic resource.
7 Conclusion and Further Work
The results presented in this paper demonstrate that the
application of linguistic information from automatically-
learned mathematical models can significantly enhance
both the precision and the recall of pattern-based hy-
ponymy extraction techniques. Using a graph model of
noun similarity we were able to obtain an almost five-
fold improvement in recall, though the precision of this
technique is clearly affected by the correctness of the
?seed-relationships? used. Using LSA filtering we elimi-
nated spurious relations extracted by the original pattern
method, reducing errors by 30%. Such filtering also elim-
inated spurious relations learned using the graph model
that were the result of lexical ambiguity and of seed hy-
ponymy relations inappropriate for the technique, reduc-
ing errors by 33%.
This paper suggests many possibilities for future work.
First of all, it would be interesting to apply LSA to a sys-
tem for building an entire hypernym-labelled ontology in
roughly the way described in (Caraballo, 1999), perhaps
by using an LSA-weighted voting method to determine
which hypernym would be used to label each node. We
are considering how to extend our techniques to such a
task.
Also, systematic comparison of the lexicosyntactic
patterns used for extraction to determine the relative pro-
ductiveness and accuracy of each pattern might prove
illuminating, as would comparison across different cor-
pora to determine the impact of the topic area and
medium/format of documents on the effectiveness of hy-
ponymy extraction. Ultimately, the ability to predict a
priori how well a knowledge-extraction system will work
on a previously unseen corpus will be crucial to its use-
fulness.
Applying the techniques of this paper to a system that
used mutual bootstrapping (Riloff and Jones, 1999) to
find additional extraction patterns would also be interest-
ing (such an approach is suggested in (Hearst, 1998)).
And of course, further refinement of the mathematical
models we use and our methods of learning them, includ-
ing more sophisticated use of available tools for linguistic
pre-processing, such as the identification and indexing of
multiword expressions, could further improve the preci-
sion and recall of hyponymy extraction techniques.
Acknowledgements
This research was supported in part by the Research
Collaboration between the NTT Communication Science
Laboratories, Nippon Telegraph and Telephone Corpora-
tion and CSLI, Stanford University, and by EC/NSF grant
IST-1999-11438 for the MUCHMORE project. Thanks
also to Stanley Peters for his helpful comments on an ear-
lier draft.
References
Enrique Alfonseca and Suresh Manandhar. 2001. Im-
proving an ontology refinement method with hy-
ponymy patterns. In Third International Conference
on Language Resources and Evaluation, pages 235?
239, Las Palmas, Spain.
Ricardo Baeza-Yates and Berthier Ribiero-Neto. 1999.
Modern Information Retrieval. Addison Wesley /
ACM Press.
Be?la Bolloba?s. 1998. Modern Graph Theory. Num-
ber 184 in Graduate Texts in Mathematics. Springer-
Verlag.
Sharon Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In 37th
Annual Meeting of the Association for Computational
Linguistics: Proceedings of the Conference, pages
120?126.
Claire Cardie. 1997. Empirical methods in information
extraction. AI Magazine, 18:65?79.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990. In-
dexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge MA.
L Guthrie, J Pustejovsky, Y Wilks, and B Slator. 1996.
The role of lexicons in natural language processing.
Communications of the ACM, 39(1):63?72.
Udo Hahn and Klemens Schnattinger. 1998. Towards
text knowledge engineering. In AAAI/IAAI, pages
524?531.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING, Nantes,
France.
Marti A. Hearst, 1998. WordNet: An Electronic Lexical
Database, chapter 5, Automated discovery of WordNet
relations, pages 131?152. MIT Press, Cambridge MA.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, Massachusetts.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of english. Natural
Language Engineering, 7(3):207?223.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for infomation extraction by multi-level bootstrap-
ping. In Proceedings of the Sixteenth National Confer-
ence on Artificial Intelligence, pages 472?479. AAAI.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Claire
Cardie and Ralph Weischedel, editors, Proceedings of
the Second Conference on Empirical Methods in Natu-
ral Language Processing, pages 117?124. Association
for Computational Linguistics, Somerset, New Jersey.
Brian Roark and Eugene Charniak. 1998. Noun-phrase
co-occurence statistics for semi-automatic semantic
lexicon construction. In COLING-ACL, pages 1110?
1116.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?124.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In 19th In-
ternational Conference on Computational Linguistics,
pages 1093?1099, Taipei, Taiwan, August.
Unsupervised Monolingual and Bilingual Word-Sense
Disambiguation of Medical Documents using UMLS
Dominic Widdows, Stanley Peters, Scott Cederberg, Chiu-Ki Chan
Stanford University, California
{dwiddows,peters,cederber,ckchan}@csli.stanford.edu
Diana Steffen
Consultants for Language Technology,
Saarbru?cken, Germany
steffen@clt-st.de
Paul Buitelaar
DFKI, Saarbru?cken, Germany
paulb@dfki.de
Abstract
This paper describes techniques for unsu-
pervised word sense disambiguation of En-
glish and German medical documents us-
ing UMLS. We present both monolingual
techniques which rely only on the structure
of UMLS, and bilingual techniques which
also rely on the availability of parallel cor-
pora. The best results are obtained using
relations between terms given by UMLS,
a method which achieves 74% precision,
66% coverage for English and 79% preci-
sion, 73% coverage for German on evalua-
tion corpora and over 83% coverage over the
whole corpus. The success of this technique
for German shows that a lexical resource
giving relations between concepts used to
index an English document collection can
be used for high quality disambiguation in
another language.
1 Introduction
This paper reports on experiments in monolingual
and multilingual word sense disambiguation (WSD)
in the medical domain using the Unified Medical
Language System (UMLS). The work described was
carried out as part of the MUCHMORE project 1 for
multilingual organisation and retrieval of medical in-
formation, for which WSD is particularly important.
The importance of WSD to multilingual applica-
tions stems from the simple fact that meanings repre-
sented by a single word in one language may be rep-
resented by multiple words in other languages. The
English word drug when referring to medically ther-
apeutic drugs would be translated as medikamente,
1http://muchmore.dfki.de
while it would be rendered as drogen when referring
to a recreationally taken narcotic substance of the
kind that many governments prohibit by law.
The ability to disambiguate is therefore essential
to the task of machine translation ? when translat-
ing from English to Spanish or from English to Ger-
man we would need to make the distinctions men-
tioned above and other similar ones. Even short of
the task of full translation, WSD is crucial to ap-
plications such as cross-lingual information retrieval
(CLIR), since search terms entered in the language
used for querying must be appropriately rendered in
the language used for retrieval. WSD has become a
well-established subfield of natural language process-
ing with its own evaluation standards and SENSE-
VAL competitions (Kilgarriff and Rosenzweig, 2000).
Methods for WSD can effectively be divided into
those that require manually annotated training data
(supervised methods) and those that do not (unsu-
pervised methods) (Ide and Ve?ronis, 1998). In gen-
eral, supervised methods are less scalable than unsu-
pervised methods because they rely on training data
which may be costly and unrealistic to produce, and
even then might be available for only a few ambigu-
ous terms. The goal of our work on disambiguation
in the MUCHMORE project is to enable the correct
semantic annotation of entire document collections
with all terms which are potentially relevant for or-
ganisation, retrieval and summarisation of informa-
tion. Therefore a decision was taken early on in the
project that we should focus on unsupervised meth-
ods, which have the potential to be scaled up enough
to meet our needs.
This paper is arranged as follows. In Section 2 we
describe the lexical resource (UMLS) and the cor-
pora we used for our experiments. We then describe
and evaluate three different methods for disambigua-
tion. The bilingual method (Section 3) takes ad-
vantage of our having a translated corpus, because
knowing the translation of an ambiguous word can
be enough to determine its sense. The collocational
method (Section 4) uses the occurence of a term in a
recognised fixed expression to determine its meaning.
UMLS relation based methods (Section 5) use rela-
tions between terms in UMLS to determine which
sense is being used in a particular instance. Other
techniques used in the MUCHMORE project in-
clude domain-specific sense selection (Buitelaar and
Sacaleanu, 2001), used to select senses appropri-
ate to the medical domain from a general lexical
resource, and instance-based learning, a machine-
learning technique that has been adapted for word-
sense disambiguation (Widdows et al, 2003).
2 Language resources used in these
experiments
2.1 Lexical Resource ? UMLS
The Unified Medical Language System (UMLS) is
a resource that contains linguistic, terminological
and semantic information in the medical domain.2
It is organised in three parts: Specialist Lexi-
con, MetaThesaurus and Semantic Network. The
MetaThesaurus contains concepts from more than
60 standardised medical thesauri, of which for our
purposes we only use the concepts from MeSH (the
Medical Subject Headings thesaurus). This decision
is based on the fact that MeSH is also available in
German. The semantic information that we use in
annotation is the so-called Concept Unique Identifier
(CUI), a code that represents a concept in the UMLS
MetaThesaurus. We consider the possible ?senses? of
a term to be the set of CUI?s which list this term
as a possible realisation. For example, UMLS con-
tains the term trauma as a possible realisation of the
following two concepts:
C0043251 Injuries and Wounds: Wounds
and Injuries: trauma: traumatic disorders:
Traumatic injury:
C0021501 Physical Trauma: Trauma
(Physical): trauma:
Each of these CUI?s is a possible sense of the term
trauma. The term trauma is therefore noted as am-
biguous, since it can be used to express more than
one UMLS concept. The purpose of disambiguation
is to find out which of these possible senses is ac-
tually being used in each particular context where
there term trauma is used.
2UMLS is freely available under license from
the United States National Library of Medicine,
http://www.nlm.nih.gov/research/umls/
CUI?s in UMLS are also interlinked to each other
by a number of relations. These include:
? ?Broader term? which is similar to the hyper-
nymy relation in WordNet (Fellbaum, 1998). In
general, x is a ?broader term? for y if every y is
also a (kind of) x.
? More generally, ?related terms? are listed, where
possible relationships include ?is like?, ?is clini-
cally associated with?.
? Cooccurring concepts, which are pairs of con-
cepts which are linked in some information
source. In particular, two concepts are regarded
as cooccurring if they have both been used to
manually index the same document in MED-
LINE. We will refer to such pairs of concepts
as coindexing concepts.
? Collocations and multiword expressions. For ex-
ample, the term liver transplant is included sep-
arately in UMLS, as well as both the terms liver
and transplant. This information can sometimes
be used for disambiguation.
2.2 The Springer Corpus of Medical
Abstracts
The experiments and implementations of WSD de-
scribed in this paper were all carried out on a par-
allel corpus of English-German medical scientific ab-
stracts obtained from the Springer Link web site.3
The corpus consists approximately of 1 million to-
kens for each language. Abstracts are from 41 medi-
cal journals, each of which constitutes a relatively ho-
mogeneous medical sub-domain (e.g. Neurology, Ra-
diology, etc.). The corpus was automatically marked
up with morphosyntactic and semantic information,
as described by S?pela Vintar et al (2002). In brief,
whenever a token is encountered in the corpus that is
listed as a term in UMLS, the document is annotated
with the CUI under which that term is listed. Ambi-
guity is introduced by this markup process because
the lexical resources often list a particular term as a
possible realisation of more than one concept or CUI,
as with the trauma example above, in which case
the document is annotated with all of these possible
CUI?s.
The number of tokens of UMLS terms included by
this annotation process is given in Table 1. The table
shows how many tokens were found by the annota-
tion process, listed according to how many possible
senses each of these tokens was assigned in UMLS (so
that the number of ambiguous tokens is the number
3http://link.springer.de/
Number of Senses 1 2 3 4
Before Disambiguation
English 223441 31940 3079 56
German 124369 7996 0 0
After Disambiguation
English 252668 5299 568 5
German 131302 1065 0 0
Table 1: The number of tokens of terms that have 1,
2, 3 and 4 possible senses in the Springer corpus
of tokens with more than one possible sense). The
greater number of concepts found in the English cor-
pus reflects the fact that UMLS has greater cover-
age for English than for German, and secondly that
there are many small terms in English which are ex-
pressed by single words which would be expressed
by larger compound terms in German (for exam-
ple knee + joint = kniegelenk). Table 1 also shows
how many tokens of UMLS concepts were in the an-
notated corpus after we applied the disambiguation
process described in Section 5, which proved to be
our most successful method. As can be seen, our
disambiguation methods resolved some 83% of the
ambiguities in the English corpus and 87% of the
ambiguities in the German corpus (we refer to this
proportion as the ?Coverage? of the method). How-
ever, this only measures the number of disambigua-
tion decisions that were made: in order to determine
how many of these decisions were correct, evaluation
corpora were needed.
2.3 Evaluation Corpora
An important aspect of word sense disambiguation is
the evaluation of different methods and parameters.
Unfortunately, there is a lack of test sets for evalu-
ation, specifically for languages other than English
and even more so for specific domains like medicine.
Given that our work focuses on German as well as
English text in the medical domain, we had to de-
velop our own evaluation corpora in order to test our
disambiguation methods.
Because in the MUCHMORE project we devel-
oped an extensive format for linguistic and semantic
annotation (S?pela Vintar et al, 2002) that includes
annotation with UMLS concepts, we could automat-
ically generate lists of all ambiguous UMLS types
(English and German) along with their token fre-
quencies in the corpus. Using these lists we selected a
set of 70 frequent types for English (token frequencies
at least 28, 41 types having token frequencies over
100). For German, we only selected 24 ambiguous
types (token frequencies at least 11, 7 types having
token frequencies over 100) because there are fewer
ambiguous terms in the German annotation (see Ta-
ble 1). We automatically selected instances to be
annotated using a random selection of occurrences if
the token frequency was higher than 100, and using
all occurrences if the token frequency was lower than
100. The level of ambiguity for these UMLS terms is
mostly limited to only 2 senses; only 7 English terms
have 3 senses.
Correct senses of the English tokens in context
were chosen by three medical experts, two native
speakers of German and one of English. The Ger-
man evaluation corpus was annotated by the two
German speakers. Interannotator agreement for in-
dividual terms ranged from very low to very high,
with an average of 65% for German and 51% for En-
glish (where all three annotators agreed). The rea-
sons for this low score are still under investigation.
In some cases, the UMLS definitions were insufficient
to give a clear distinction between concepts, espe-
cially when the concepts came from different origi-
nal thesauri. This allowed the decision of whether
a particular definition gave a meaningful ?sense? to
be more or less subjective. Approximately half of
the disagreements between annotators occured with
terms where interannotator agreement was less than
10%, which is evidence that a significant amount of
the disagreement between annotators was on the type
level rather than the token level. In other cases, it
is possible that there was insufficient contextual in-
formation provided for annotators to agree. If one of
the annotators was unable to choose any of the senses
and declared an instance to be ?unspecified?, this also
counted against interannotator agreement. What-
ever is responsible, our interannotator agreement fell
far short of the 88%-100% achieved in SENSEVAL
(Kilgarriff and Rosenzweig, 2000, ?7), and until this
problem is solved or better datasets are found, this
poor agreement casts doubt on the generality of the
results obtained in this paper.
A ?gold standard? was produced for the German
UMLS evaluation corpus and used to evaluate the
disambiguation of German UMLS concepts. The En-
glish experiments were evaluated on those tokens for
which the annotators agreed. More details and dis-
cussion of the annotation process is available in the
project report (Widdows et al, 2003).
In the rest of this paper we describe the techniques
that used these resources to build systems for word
sense disambiguation, and evaluate their level of suc-
cess.
3 Bilingual Disambiguation
The mapping between word-forms and senses differs
across languages, and for this reason the importance
of word-sense disambiguation has long been recog-
nised for machine translation. By the same token,
pairs of translated documents naturally contain in-
formation for disambiguation. For example, if in a
particular context the English word drugs is trans-
lated into French as drogues rather than medica-
ments, then the English word drug is being used
to mean narcotics rather than medicines. This ob-
servation has been used for some years on varying
scales. Brown et al (1991) pioneered the use of sta-
tistical WSD for translation, building a translation
model from one million sentences in English and
French. Using this model to help with translation
decisions (such as whether prendre should be trans-
lated as take or make), the number of acceptable
translations produced by their system increased by
8%. Gale et al (1992) use parallel translations to
obtain training and testing data for word-sense dis-
ambiguation. Ide (1999) investigates the information
made available by a translation of George Orwell?s
Nineteen Eighty-four into six languages, using this
to analyse the related senses of nine ambiguous En-
glish words into hierarchical clusters.
These applications have all been case studies of a
handful of particularly interesting words. The large
scale of the semantic annotation carried out by the
MUCHMORE project has made it possible to extend
the bilingual disambiguation technique to entire dic-
tionaries and corpora.
To disambiguate an instance of an ambiguous
term, we consulted the translation of the abstract
in which it appeared. We regarded the translated
abstract as disambiguating the ambiguous term if it
met the following two criteria:
? Only one of the CUI?s was assigned to any term
in the translated abstract.
? At least one of the terms to which this CUI
was assigned in the translated abstract was un-
ambiguous (i.e. was not also assigned another
CUI).
3.1 Results for Bilingual Disambiguation
We attempted both to disambiguate terms in the
German abstracts using the corresponding English
abstracts, and to disambiguate terms in the English
abstracts using the corresponding German ones. In
this collection of documents, we were able to disam-
biguate 1802 occurrences of 63 English terms and
1500 occurrences of 43 German terms. Comparing
this with the evaluation corpora gave the results in
Table 2.4
4In all of the results presented in this paper, Precision
is the proportion of decisions made which were correct
Precision Recall Coverage
English 81% 18% 22%
German 66% 22% 33%
Table 2: Results for bilingual disambiguation
As can be seen, the recall and coverage of this
method is not especially good but the precision (at
least for English) is very high. The German results
contain roughly the same proportion of correct deci-
sions as the English, but many more incorrect ones
as well.
Our disambiguation results break down into three
cases:
1. Terms ambiguous in one language that translate
as multiple unambiguous terms in the other lan-
guage; one of the meanings is medical and the
other is not.
2. Terms ambiguous in one language that trans-
late as multiple unambiguous terms in the other
language; both of the terms are medical.
3. Terms that are ambiguous between two mean-
ings that are difficult to distinguish.
One striking aspect of the results was that rel-
atively few terms were disambiguated to different
senses in different occurrences. This phenomenon
was particularly extreme in disambiguating the Ger-
man terms; of the 43 German terms disambiguated,
42 were assigned the same sense every time we were
able to disambiguate them. Only one term, Metas-
tase, was assigned difference senses; 88 times it was
assigned CUI C0027627 (?The spread of cancer from
one part of the body to another ...?, associated with
the English term Metastasis and 6 times it was as-
signed CUI C0036525 ?Used with neoplasms to in-
dicate the secondary location to which the neoplas-
tic process has metastasized?, corresponding to the
English terms metastastic and secondary). Metas-
tase therefore falls into category 2 from above, al-
though the distinction between the two meanings is
relatively subtle.
The first and third categories above account for
the vast majority of cases, in which only one mean-
ing is ever selected. It is easy to see why this would
according to the evaluation corpora, Recall is the pro-
portion of instances in the evaluation corpora for which
a correct decision was made, and Coverage is the propor-
tion of instances in the evaluation corpora for which any
decision was made. It follows that
Recall = Precision ? Coverage.
happen in the first category, and it is what we want
to happen. For instance, the German term Krebse
can refer either to crabs (Crustaceans) or to cancer-
ous growths; it is not surprising that only the latter
meaning turns up in the corpus under consideration
and that we can determine this from the unambigu-
ous English translation cancers.
In English somewhat more terms were disam-
biguated multiple ways: eight terms were assigned
two different senses across their occurrences. All
three types of ambiguity were apparent. For in-
stance, the second type (medical/medical ambiguity)
appeared for the term Aging, which can refer either
to aging people (Alte Menschen) or to the process of
aging itself (Altern); both meanings appeared in our
corpus.
In general, the bilingual method correctly find the
meanings of approximately one fifth of the ambigu-
ous terms, and makes only a few mistakes for English
but many more for German.
4 Collocational disambiguation
By a ?collocation? we mean a fixed expression formed
by a group of words occuring together, such as
blood vessel or New York. (For the purposes of
this paper we only consider contiguous multiword
expressions which are listed in UMLS.) There is a
strong and well-known tendency for words to ex-
press only one sense in a given collocation. This
property of words was first described and quantified
by Yarowsky (1993), and has become known gen-
erally as the ?One Sense Per Collocation? property.
Yarowsky (1995) used the one sense per collocation
property as an essential ingredient for an unsuper-
vised Word-Sense Disambiguation algorithm. For ex-
ample, the collocations plant life and manufacturing
plant are used as ?seed-examples? for the living thing
and building senses of plant, and these examples can
then be used as high-precision training data to per-
form more general high-recall disambiguation.
While Yarowsky?s algorithm is unsupervised (the
algorithm does not need a large collection of anno-
tated training examples), it still needs direct human
intervention to recognise which ambiguous terms are
amenable to this technique, and to choose appropri-
ate ?seed-collocations? for each sense. Thus the algo-
rithm still requires expert human judgments, which
leads to a bottleneck when trying to scale such meth-
ods to provide Word-Sense Disambiguation for a
whole document collection.
A possible method for widening this bottleneck is
to use existing lexical resources to provide seed collo-
cations. The texts of dictionary definitions have been
used as a traditional source of information for disam-
biguation (Lesk, 1986). The richly detailed structure
of UMLS provides a special opportunity to combine
both of these approaches, because many multiword
expressions and collocations are included in UMLS
as separate concepts.
For example, the term pressure has the following
three senses in UMLS, each of which is assigned to a
different semantic type (TUI):
Sense of pressure Semantic Type
Physical pressure Quantitative Concept
(C0033095)
Pressure - action Therapeutic or
(C0460139) Preventive Procedure
Baresthesia, sensation
of pressure (C0234222)
Organ or Tissue Func-
tion
Many other collocations and compounds which in-
clude the word pressure are also of these semantic
types, as summarised in the following table:
Quantitative
Concept
mean pressure, bar pressure,
population pressure
Therapeutic
Procedure
orthostatic pressure, acupres-
sure
Organ or Tissue
Function
arterial pressure, lung pres-
sure, intraocular pressure
This leads to the hypothesis that the term pres-
sure, when used in any of these collocations, is used
with the meaning corresponding to the same seman-
tic type. This allows deductions of the following
form:
Collocation bar pressure, mean pressure
Semantic type Quantitative Concept
Sense of pressure C0033095, physical pressure
Since nearly all English and German multiword
technical medical terms are head-final, it follows that
the a multiword term is usually of the same seman-
tic type as its head, the final word. (So for example,
lung cancer is a kind of cancer, not a kind of lung.)
For English, UMLS 2001 contains over 800,000 multi-
word expressions the last word in which is also a term
in UMLS. Over 350,000 of these expressions have a
last word which on its own, with no other context,
would be regarded as ambiguous (has more that one
CUI in UMLS). Over 50,000 of these multiword ex-
pressions are unambiguous, with a unique semantic
type which is shared by only one of the meanings of
the potentially ambiguous final word. The ambigu-
ity of the final word in such multiword expressions
is thus resolved, providing over 50,000 ?seed colloca-
tions? for use in semantically annotating documents
with disambiguated word senses.
4.1 Results for collocational disambiguation
Unfortunately, results for collocational disambigua-
tion (Table 3) were disappointing compared with the
promising number of seed collocations we expected
to find. Precision was high, but comparatively few
of the collocations suggested by UMLS were found
in the Springer corpus.
Precision Recall Coverage
English 79% 3% 4%
German 82% 1% 1.2%
Table 3: Results for collocational disambiguation
In retrospect, this may not be surprising given that
many of the ?collocations? in UMLS are rather col-
lections of words such as
C0374270 intracoronary percutaneous
placement s single stent transcatheter vessel
which would almost never occur in natural text.
Thus very few of the potential collocations we ex-
tracted from UMLS actually occurred in the Springer
corpus. This scarcity was especially pronounced for
German, because so many terms which are several
words in English are compounded into a single word
in German. For example, the term
C0035330 retinal vessel
does occur in the (English) Springer corpus and con-
tains the ambiguous word vessel, whose ambiguity is
successfully resolved using the collocational method.
However, in German this concept is represented by
the single word
C0035330 Retinagefaesse
and so this ambiguity never arises in the first place.
It should still be remarked that the few decisions
that were made by the collocational method were
very accurate, demonstrating that we can get some
high precision results using this method. It is pos-
sible that recall could be improved by relaxing the
conditions which a multiword expression in UMLS
must satisfy to be used as a seed-collocation.
5 Disambiguation using related
UMLS terms found in the same
context
While the collocational method turned out to give
disappointing recall, it showed that accurate infor-
mation could be extracted directly from the existing
UMLS and used for disambiguation, without extra
human intervention or supervision. What we needed
was advice on how to get more of this high-quality
information out of UMLS, which we still believed to
be a very rich source of information which we were
not yet exploiting fully. Fortunately, no less than 3
additional sources of information for disambiguation
using related terms from UMLS were suggested by a
medical expert.5 The suggestion was that we should
consider terms that were linked by conceptual rela-
tions (as given by the MRREL and MRCXT files
in the UMLS source) and which were noted as coin-
dexing concepts in the same MEDLINE abstract (as
given by the MRCOC file in the UMLS source). For
each separate sense of an ambiguous word, this would
give a set of related concepts, and if examples of any
of these related concepts were found in the corpus
near to one of the ambiguous words, it might indi-
cate that the correct sense of the ambiguous word
was the one related to this particular concept.
This method is effectively one of the many variants
of Lesk?s (1986) original dictionary-based method for
disambiguation, where the words appearing in the
definitions of different senses of ambiguous words are
used to indicate that those senses are being used if
they are observed near the ambiguous word. How-
ever, we gain over purely dictionary-based methods
because the words that occur in dictionary defini-
tions rarely correspond well with those that occur
in text. The information we collected from UMLS
did not suffer from this drawback: the pairs of coin-
dexing concepts from MRCOC were derived precisely
from human judgements that these two concepts
both occured in the same text in MEDLINE.
The disambiguation method proceeds as follows.
For each ambiguous word w, we find its possible
senses {sj(w)}. For each sense sj , find all CUI?s
in MRREL, MRCXT or MRCOC files that are re-
lated to this sense, and call this set {crel(sj)}. Then
for each occurrence of the ambiguous word w in the
corpus we examine the local context to see if a term
t occurs whose sense6 (CUI) is one of the concepts
in {crel(sj)}, and if so take this as positive evidence
that the sense sj is the appropriate one for this con-
text, by increasing the score of sj by 1. In this way,
each sense sj in context gets assigned a score which
measures the number of terms in this context which
are related to this sense. Finally, choose the sense
5Personal communication from Stuart Nelson (instru-
mental in the design of UMLS), at the MUCHMORE
workshop in Croatia, September 2002.
6This fails to take into account that the term t might
itself be ambiguous ? it is possible that results could be
improved still further by allowing for mutual disambigua-
tion of more than one term at once.
with the highest score.
One open question for this algorithm is what re-
gion of text to use as a context-window. We experi-
mented with using sentences, documents and whole
subdomains, where a ?subdomain? was considered to
be all of the abstracts appearing in one of the jour-
nals in the Springer corpus, such as Arthroskopie
or Der Chirurg. Thus our results (for each lan-
guage) vary according to which knowledge sources
were used (Conceptually Related Terms from MR-
REL and MRCXT or coindexing terms from MR-
COC, or a combination), and according to whether
the context-window for recording cooccurence was a
sentence, a document or a subdomain.
5.1 Results for disambiguation based on
related UMLS concepts
The results obtained using this method (Tables 5.1
and 5.1) were excellent, preserving (and in some
cases improving) the high precision of the bilingual
and collocational methods while greatly extending
coverage and recall. The results obtained by using
the coindexing terms for disambiguation were partic-
ularly impressive, which coincides with a long-held
view in the field that terms which are topically re-
lated to a target word can be much richer clues for
disambiguation that terms which are (say) hierarchi-
cally related. We are very fortunate to have such
a wealth of information about the cooccurence of
pairs of concepts through UMLS, which appears to
have provided the benefits of cooccurence data from
a manually annotated training sample without hav-
ing to perform the costly manual annotation.
In particular, for English (Table 5.1), results were
actually better using only coindexing terms rather
than combining this information with hierarchically
related terms: both precision and recall are best
when using only the MRCOC knowledge source. As
we had expected, recall and coverage increased but
precision decreased slightly when using larger con-
texts.
The German results (Table 5.1) were slightly dif-
ferent, and even more successful, with nearly 60% of
the evaluation corpus being correctly disambiguated,
nearly 80% of the decisions being correct. Here, there
was some small gain when combining the knowledge
sources, though the results using only coindexing
terms were almost as good. For the German experi-
ments, using larger contexts resulted in greater recall
and greater precision. This was unexpected ? one
hypothesis is that the sparser coverage of the German
UMLS contributed to less predictable results on the
sentence level.
These results are comparable with some of the bet-
ter SENSEVAL results (Kilgarriff and Rosenzweig,
2000) which used fully supervised methods, though
the comparison may not be accurate because we are
choosing between fewer senses than on avarage in
SENSEVAL, and because of the doubts over our in-
terannotator agreement.
Comparing these results with the number of words
disambiguated in the whole corpus (Table 1), it is
apparent that the average coverage of this method is
actually higher for the whole corpus (over 80%) than
for the words in the evaluation corpus. It is possible
that this reflects the fact the the evaluation corpus
was specifically chosen to include words with ?inter-
esting? ambiguities, which might include words which
are more difficult than average to disambiguate. It is
possible that over the whole corpus, the method ac-
tually works even better than on just the evaluation
corpus.
This technique is quite groundbreaking, because it
shows that a lexical resource derived almost entirely
from English data (MEDLINE indexing terms) could
successfully be used for automatic disambiguation in
a German corpus. (The alignment of documents and
their translations was not even considered for these
experiments so the results do not depend at all on
our having access to a parallel corpus.) This is be-
cause the UMLS relations are defined between con-
cepts rather than between words. Thus if we know
that there is a relationship between two concepts, we
can use that relationship for disambiguation, even if
the original evidence for this relationship was derived
from information in a different language from the
language of the document we are seeking to disam-
biguate. We are assigning the correct senses based
not upon how terms are related in language, but how
medical concepts are related to one another.
It follows that this technique for disambiguation
should be applicable to any language which UMLS
covers, and applicable at very little cost. This pro-
posal should stimulate further research, and not too
far behind, successful practical implementation.
6 Summary and Conclusion
We have described three implementations of unsu-
pervised word-sense disambiguation techniques for
medical documents. The bilingual method relies on
the availability of a translated parallel corpus: the
collocational and relational methods rely solely on
the structure of UMLS, and could therefore be ap-
plied to new collections of medical documents with-
out requiring any new resources. The method of
disambiguation using relations between terms given
by UMLS was by far the most successful method,
achieving 74% precision, 66% coverage for English
ENGLISH Related terms Related terms Coindexing terms Combined
RESULTS (MRREL) (MRCXT) (MRCOC) (majority voting)
Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov.
Sentence 50 14 28 60 9 15 78 32 41 74 32 43
Document 48 24 50 63 22 35 74 46 62 72 45 63
Subdomain 51 33 65 64 38 59 74 49 66 71 49 69
Table 4: Results for disambiguation based on UMLS relations (English)
GERMAN Related terms Related terms Coindexing terms Combined
RESULTS (MRREL) (MRCXT) (MRCOC) (majority voting)
Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov.
Sentence 64 24 38 75 11 15 76 29 38 77 31 40
Document 68 43 63 75 27 36 79 52 66 79 53 67
Subdomain 70 51 73 74 52 70 79 58 73 79 58 73
Table 5: Results for disambiguation based on UMLS relations (German)
and 79% precision, 73% coverage for German on the
evaluation corpora, and achieving over 80% coverage
overall. This result for German is particularly en-
couraging, because is shows that a lexical resource
giving relations between concepts in one language
can be used for high quality disambiguation in an-
other language.
Acknowledgments
This research was supported in part by the Re-
search Collaboration between the NTT Communi-
cation Science Laboratories, Nippon Telegraph and
Telephone Corporation and CSLI, Stanford Univer-
sity, and by EC/NSF grant IST-1999-11438 for the
MUCHMORE project.
We would like to thank the National Library of
Medicine for providing the UMLS, and in particular
Stuart Nelson for his advice and guidance.
References
P. Brown, S. de la Pietra, V. de la Pietra, and R Mer-
cer. 1991. Word sense disambiguation using sta-
tistical methods. In ACL 29, pages 264?270.
Paul Buitelaar and Bogdan Sacaleanu. 2001. Rank-
ing and selecting synsets by domain relevance. In
Proceedings of WordNet and Other Lexical Re-
sources, NAACL 2001 Workshop, Pittsburgh, PA,
June.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press, Cam-
bridge MA.
W. Gale, K. Church, and D. Yarowsky. 1992. A
method for disambiguating word senses in a large
corpus. Computers and the Humanities, 26:415?
439.
Nancy Ide and Jean Ve?ronis. 1998. Introduction
to the special issue on word sense disambiguation:
The state of the art. Computational Linguistics,
24(1):1?40, March.
Nancy Ide. 1999. Parallel translations and
sense discriminators. In Proceedings of the ACL
SIGLEX workshop on Standardizing Lexical Re-
sources, pages 52?61.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for english senseval. Computers and
the Humanities, 34(1-2):15?48, April.
M. E. Lesk. 1986. Automated sense disambiguation
using machine-readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the SIGDOC conference. ACM.
S?pela Vintar, Paul Buitelaar, Ba?rbel Ripplinger,
Bogdan Sacaleanu, Diana Raileanu, and Detlef
Prescher. 2002. An efficient and flexible format
for linguistic and semantic annotation. In Third
International Language Resources and Evaluation
Conference, Las Palmas, Spain.
Dominic Widdows, Diana Steffen, Scott Ceder-
berg, Chiu-Ki Chan, Paul Buitelaar, and Bog-
dan Sacaleanu. 2003. Methods for word-sense
disambiguation. Technical report, MUCHMORE
project report.
David Yarowsky. 1993. One sense per collocation.
In ARPA Human Language Technology Workshop,
pages 266?271, Princeton, NJ.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics, pages
189?196.
An Empirical Model of Multiword Expression Decomposability
Timothy Baldwin?, Colin Bannard?, Takaaki Tanaka? and Dominic Widdows?
? CSLI
Stanford University
Stanford CA 94305, USA
{tbaldwin,dwiddows}@csli.stanford.edu
? School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
c.j.bannard@ed.ac.uk
? Communication Science
Labs
NTT Corporation
Kyoto, Japan
takaaki@cslab.kecl.ntt.co.jp
Abstract
This paper presents a construction-
inspecific model of multiword expression
decomposability based on latent semantic
analysis. We use latent semantic analysis
to determine the similarity between a
multiword expression and its constituent
words, and claim that higher similarities
indicate greater decomposability. We
test the model over English noun-noun
compounds and verb-particles, and eval-
uate its correlation with similarities and
hyponymy values in WordNet. Based on
mean hyponymy over partitions of data
ranked on similarity, we furnish evidence
for the calculated similarities being corre-
lated with the semantic relational content
of WordNet.
1 Introduction
This paper is concerned with an empirical model of
multiword expression decomposability. Multiword
expressions (MWEs) are defined to be cohesive lex-
emes that cross word boundaries (Sag et al, 2002;
Copestake et al, 2002; Calzolari et al, 2002). They
occur in a wide variety of syntactic configurations
in different languages (e.g. in the case of English,
compound nouns: post office, verbal idioms: pull
strings, verb-particle constructions: push on, etc.).
Decomposability is a description of the degree to
which the semantics of an MWE can be ascribed
to those of its parts (Riehemann, 2001; Sag et al,
2002). Analysis of the semantic correlation between
the constituent parts and whole of an MWE is per-
haps more commonly discussed under the banner of
compositionality (Nunberg et al, 1994; Lin, 1999).
Our claim here is that the semantics of the MWE are
deconstructed and the parts coerced into often id-
iosyncratic interpretations to attain semantic align-
ment, rather than the other way around. One id-
iom which illustrates this process is spill the beans,
where the semantics of reveal?(secret?) are de-
composed such that spill is coerced into the idiosyn-
cratic interpretation of reveal? and beans into the
idiosyncratic interpretation of secret?. Given that
these senses for spill and beans are not readily avail-
able at the simplex level other than in the context
of this particular MWE, it seems fallacious to talk
about them composing together to form the seman-
tics of the idiom.
Ideally, we would like to be able to differ-
entiate between three classes of MWEs: non-
decomposable, idiosyncratically decomposable and
simple decomposable (derived from Nunberg et al?s
sub-classification of idioms (1994)). With non-
decomposable MWEs (e.g. kick the bucket, shoot
the breeze, hot dog), no decompositional anal-
ysis is possible, and the MWE is semantically
impenetrable. The only syntactic variation that
non-decomposable MWEs undergo is verbal in-
flection (e.g. kicked the bucket, kicks the bucket)
and pronominal reflexivisation (e.g. wet oneself ,
wet themselves). Idiosyncratically decomposable
MWEs (e.g. spill the beans, let the cat out of the
bag, radar footprint) are decomposable but co-
erce their parts into taking semantics unavailable
outside the MWE. They undergo a certain degree
of syntactic variation (e.g. the cat was let out of
the bag). Finally, simple decomposable MWEs
(also known as ?institutionalised? MWEs, e.g. kin-
dle excitement, traffic light) decompose into simplex
senses and generally display high syntactic variabil-
ity. What makes simple decomposable expressions
true MWEs rather than productive word combina-
tions is that they tend to block compositional al-
ternates with the expected semantics (termed anti-
collocations by Pearce (2001b)). For example, mo-
tor car cannot be rephrased as *engine car or *mo-
tor automobile. Note that the existence of anti-
collocations is also a test for non-decomposable and
idiosyncratically decomposable MWEs (e.g. hot dog
vs. #warm dog or #hot canine).
Our particular interest in decomposability stems
from ongoing work on grammatical means for cap-
turing MWEs. Nunberg et al (1994) observed that
idiosyncratically decomposable MWEs (in particu-
lar idioms) undergo much greater syntactic variation
than non-decomposable MWEs, and that the vari-
ability can be partially predicted from the decompo-
sitional analysis. We thus aim to capture the decom-
posability of MWEs in the grammar and use this to
constrain the syntax of MWEs in parsing and gen-
eration. Note that it is arguable whether simple de-
composable MWEs belong in the grammar proper,
or should be described instead as lexical affinities
between particular word combinations.
As the first step down the path toward an empir-
ical model of decomposability, we focus on demar-
cating simple decomposable MWEs from idiosyn-
cratically decomposable and non-decomposable
MWEs. This is largely equivalent to classifying
MWEs as being endocentric (i.e., a hyponym of
their head) or exocentric (i.e., not a hyponym of
their head: Haspelmath (2002)).
We attempt to achieve this by looking at the se-
mantic similarity between an MWE and its con-
stituent words, and hypothesising that where the
similarity between the constituents of an MWE and
the whole is sufficiently high, the MWE must be of
simple decomposable type.
The particular similarity method we adopt is la-
tent semantic analysis, or LSA (Deerwester et al,
1990). LSA allows us to calculate the similarity
between an arbitrary word pair, offering the advan-
tage of being able to measure the similarity between
the MWE and each of its constituent words. For
MWEs such as house boat, therefore, we can expect
to capture the fact that the MWE is highly similar in
meaning to both constituent words (i.e. the modifier
house and head noun boat). More importantly, LSA
makes no assumptions about the lexical or syntac-
tic composition of the inputs, and thus constitutes a
fully construction- and language-inspecific method
of modelling decomposability. This has clear advan-
tages over a more conventional supervised classifier-
style approach, where training data would have to be
customised to a particular language and construction
type.
Evaluation is inevitably a difficulty when it comes
to the analysis of MWEs, due to the lack of con-
cise consistency checks on what MWEs should and
should not be incorporated into dictionaries. While
recognising the dangers associated with dictionary-
based evaluation, we commit ourselves to this
paradigm and focus on searching for appropriate
means of demonstrating the correlation between
dictionary- and corpus-based similarities.
The remainder of this paper is structured as fol-
lows. Section 2 describes past research on MWE
compositionality of relevance to this effort. Sec-
tion 3 provides a basic outline of the resources used
in this research, LSA, the MWE extraction methods,
and measures used to evaluate our method. Section 4
then provides evaluation of the proposed method,
and the paper is concluded with a brief discussion
in Section 5.
2 Past research
Although there has been some useful work on com-
positionality in statistical machine translation (e.g.
Melamed (1997)), there has been little work on de-
tecting ?non-compositional? (i.e. non-decomposable
and idiosyncratically decomposable) items of vari-
able syntactic type in monolingual corpora. One in-
teresting exception is Lin (1999), whose approach is
explained as follows:
The intuitive idea behind the method is
that the metaphorical usage of a non-
compositional expression causes it to
have a different distributional characteris-
tic than expressions that are similar to its
literal meaning.
The expressions he uses are taken from a colloca-
tion database (Lin, 1998b). These ?expressions that
are similar to [their] literal meaning? are found by
substituting each of the words in the expression with
the 10 most similar words according to a corpus de-
rived thesaurus (Lin, 1998a). Lin models the dis-
tributional difference as a significant difference in
mutual information. Significance here is defined as
the absence of overlap between the 95% confidence
interval of the mutual information scores. Lin pro-
vides some examples that suggest he has identified
a successful measure of ?compositionality?. He of-
fers an evaluation where an item is said to be non-
compositional if it occurs in a dictionary of idioms.
This produces the unconvincing scores of 15.7% for
precision and 13.7% for recall.
We claim that substitution-based tests are use-
ful in demarcating MWEs from productive word
combinations (as attested by Pearce (2001a) in a
MWE detection task), but not in distinguishing the
different classes of decomposability. As observed
above, simple decomposable MWEs such as mo-
tor car fail the substitution test not because of non-
decomposability, but because the expression is in-
stitutionalised to the point of blocking alternates.
Thus, we expect Lin?s method to return a wide ar-
ray of both decomposable and non-decomposable
MWEs.
Bannard (2002) focused on distributional tech-
niques for describing the meaning of verb-particle
constructions at the level of logical form. The
semantic similarity between a multiword expres-
sion and its head was used as an indicator of
decomposability. The assumption was that if a
verb-particle was sufficiently similar to its head
verb, then the verb contributed its simplex mean-
ing. It gave empirical backing to this assump-
tion by showing that annotator judgements for verb-
particle decomposability correlate significantly with
non-expert human judgements on the similarity be-
tween a verb-particle construction and its head verb.
Bannard et al (2003) extended this research in look-
ing explicitly at the task of classifying verb-particles
as being compositional or not. They successfully
combined statistical and distributional techniques
(including LSA) with a substitution test in analysing
compositionality. McCarthy et al (2003) also tar-
geted verb-particles for a study on compositionality,
and judged compositionality according to the degree
of overlap in the N most similar words to the verb-
particle and head verb, e.g., to determine composi-
tionality.
We are not the first to consider applying LSA to
MWEs. Schone and Jurafsky (2001) applied LSA to
the analysis of MWEs in the task of MWE discov-
ery, by way of rescoring MWEs extracted from a
corpus. The major point of divergence from this re-
search is that Schone and Jurafsky focused specifi-
cally on MWE extraction, whereas we are interested
in the downstream task of semantically classifying
attested MWEs.
3 Resources and Techniques
In this section, we outline the resources used in eval-
uation, give an informal introduction to the LSA
model, sketch how we extracted the MWEs from
corpus data, and describe a number of methods
for modelling decomposability within a hierarchical
lexicon.
3.1 Resources and target MWEs
The particular reference lexicon we use to eval-
uate our technique is WordNet 1.7 (Miller et
al., 1990), due to its public availability, hier-
archical structure and wide coverage. Indeed,
Schone and Jurafsky (2001) provide evidence that
suggests that WordNet is as effective an evaluation
resource as the web for MWE detection methods,
despite its inherent size limitations and static nature.
Two MWE types that are particularly well repre-
sented in WordNet are compound nouns (47,000 en-
tries) and multiword verbs (2,600 entries). Of these,
we chose to specifically target two types of MWE:
noun-noun (NN) compounds (e.g. computer net-
work, work force) and verb-particles (e.g. look on,
eat up) due to their frequent occurrence in both de-
composable and non-decomposable configurations,
and also their disparate syntactic behaviours.
We extracted the NN compounds from the 1996
Wall Street Journal data (WSJ, 31m words), and
the verb-particles from the British National Corpus
(BNC, 90m words: Burnard (2000)). The WSJ data
is more tightly domain-constrained, and thus a more
suitable source for NN compounds if we are to ex-
pect sentential context to reliably predict the seman-
tics of the compound. The BNC data, on the other
hand, contains more colloquial and prosaic texts and
is thus a richer source of verb-particles.
3.2 Description of the LSA model
Our goal was to compare the distribution of differ-
ent compound terms with their constituent words, to
see if this indicated similarity of meaning. For this
purpose, we used latent semantic analysis (LSA) to
build a vector space model in which term-term sim-
ilarities could be measured.
LSA is a method for representing words as points
in a vector space, whereby words which are related
in meaning should be represented by points which
are near to one another, first developed as a method
for improving the vector model for information re-
trieval (Deerwester et al, 1990). As a technique for
measuring similarity between words, LSA has been
shown to capture semantic properties, and has been
used successfully for recognising synonymy (Lan-
dauer and Dumais, 1997), word-sense disambigua-
tion (Schu?tze, 1998) and for finding correct transla-
tions of individual terms (Widdows et al, 2002).
The LSA model we built is similar to that de-
scribed in (Schu?tze, 1998). First, 1000 frequent con-
tent words (i.e. not on the stoplist)1 were chosen
as ?content-bearing words?. Using these content-
bearing words as column labels, the 50,000 most
frequent terms in the corpus were assigned row
vectors by counting the number of times they oc-
1A ?stoplist? is a list of frequent words which have little
independent semantic content, such as prepositions and deter-
miners (Baeza-Yates and Ribiero-Neto, 1999, p167).
curred within the same sentence as a content-bearing
word. Singular-value decomposition (Deerwester et
al., 1990) was then used to reduce the number of
dimensions from 1000 to 100. Similarity between
two vectors (points) was measured using the cosine
of the angle between them, in the same way as the
similarity between a query and a document is often
measured in information retrieval (Baeza-Yates and
Ribiero-Neto, 1999, p28). Effectively, we could use
LSA to measure the extent to which two words or
MWEs x and y usually occur in similar contexts.
Since the corpora had been tagged with parts-of-
speech, we could build syntactic distinctions into the
LSA models ? instead of just giving a vector for
the string test we were able to build separate vec-
tors for the nouns, verbs and adjectives test. This
combination of technologies was also used to good
effect by Widdows (2003): an example of the con-
tribution of part-of-speech information to extracting
semantic neighbours of the word fire is shown in
Table 1. As can be seen, the noun fire (as in the
substance/element) and the verb fire (mainly used
to mean firing some sort of weapon) are related to
quite different areas of meaning. Building a single
vector for the string fire confuses this distinction ?
the neighbours of fire treated just as a string include
words related to both the meaning of fire as a noun
(more frequent in the BNC) and as a verb. The ap-
propriate granularity of syntactic classifications is an
open question for this kind of research: treating all
the possible verbs categories as different (e.g. dis-
tinguishing infinitive from finite from gerund forms)
led to data sparseness, and instead we considered
?verb? as a single part-of-speech type.
3.3 MWE extraction methods
NN compounds were extracted from the WSJ by
first tagging the data with fnTBL 1.0 (Ngai and Flo-
rian, 2001) and then simply taking noun bigrams
(adjoined on both sides by non-nouns to assure the
bigram is not part of a larger compound nominal).
Out of these, we selected those compounds that are
listed in WordNet, resulting in 5,405 NN compound
types (208,000 tokens).
Extraction of the verb-particles was consider-
ably more involved, and drew on the method of
Baldwin and Villavicencio (2002). Essentially, we
used a POS tagger and chunker (both built using
fnTBL 1.0 (Ngai and Florian, 2001)) to first (re)tag
the BNC. This allowed us to extract verb-particle to-
kens through use of the particle POS and chunk tags
returned by the two systems. This produces high-
precision, but relatively low-recall results, so we
performed the additional step of running a chunk-
based grammar over the chunker output to detect
candidate mistagged particles. In the case that a
noun phrase followed the particle candidate, we per-
formed attachment disambiguation to determine the
transitivity of the particle candidate. These three
methods produced three distinct sets of verb-particle
tokens, which we carried out weighted voting over
to determine the final set of verb-particle tokens. A
total of 461 verb-particles attested in WordNet were
extracted (160,765 tokens).
For both the NN compound and verb-particle
data, we replaced each token occurrence with a
single-word POS-tagged token to feed into the LSA
model.
3.4 Techniques for evaluating correlation with
WordNet
In order to evaluate our approach, we employed the
lexical relations as defined in the WordNet lexical
hierarchy (Miller et al, 1990). WordNet groups
words into sets with similar meaning (known as
?synsets?), e.g. {car, auto, automobile, machine,
motorcar } . These are organised into a hierarchy
employing multiple inheritance. The hierarchy is
structured according to different principles for each
of nouns, verbs, adjectives and adverbs. The nouns
are arranged according to hyponymy or ISA rela-
tions, e.g. a car is a kind of automobile. The verbs
are arranged according to troponym or ?manner-of?
relations, where murder is a manner of killing, so
kill immediately dominates murder in the hierarchy.
We used WordNet for evaluation by way of look-
ing at: (a) hyponymy, and (b) semantic distance.
Hyponymy provides the most immediate way of
evaluating decomposability. With simple decompos-
able MWEs, we can expect the constituents (and
particularly the head) to be hypernyms (ancestor
nodes) or synonyms of the MWE. That is, simple
decomposable MWEs are generally endocentric, al-
though there are some exceptions to this generali-
sation such as vice president arguably not being a
hyponym of president. No hyponymy relation holds
with non-decomposable or idiosyncratically decom-
posable MWEs (i.e., they are exocentric), as even if
the semantics of the head noun can be determined
through decomposition, by definition this will not
correspond to a simplex sense of the word.
We deal with polysemy of the constituent words
and/or MWE by simply looking for the exis-
tence of a sense of the constituent words which
fire (string only) fire nn1 fire vvi
fire 1.000000 fire nn1 1.000000 fire vvi 1.000000
flames 0.709939 flames nn2 0.700575 guns nn2 0.663820
smoke 0.680601 smoke nn1 0.696028 firing vvg 0.537778
blaze 0.668504 brigade nn1 0.589625 cannon nn0 0.523442
firemen 0.627065 fires nn2 0.584643 gun nn1 0.484106
fires 0.617494 firemen nn2 0.567170 fired vvd 0.478572
explosion 0.572138 explosion nn1 0.551594 detectors nn2 0.477025
burning 0.559897 destroyed vvn 0.547631 artillery nn1 0.469173
destroyed 0.558699 burning aj0 0.533586 attack vvb 0.468767
brigade 0.532248 blaze nn1 0.529126 firing nn1 0.459000
arson 0.528909 arson nn1 0.522844 volley nn1 0.458717
accidental 0.519310 alarms nn2 0.512332 trained vvn 0.447797
chimney 0.489577 destroyed vvd 0.512130 enemy nn1 0.445523
blast 0.488617 burning vvg 0.502052 alert aj0 0.443610
guns 0.487226 burnt vvn 0.500864 shoot vvi 0.443308
damaged 0.484897 blast nn1 0.498635 defenders nn2 0.438886
Table 1: Semantic neighbours of fire with different parts-of-speech. The scores are cosine similarities
subsumes a sense of the MWE. The function
hyponym(word i,mwe) thus returns a value of 1 if
some sense of word i subsumes a sense of mwe , and
a value of 0 otherwise.
A more proactive means of utilising the WordNet
hierarchy is to derive a semantic distance based on
analysis of the relative location of senses in Word-
Net. Budanitsky and Hirst (2001) evaluated the per-
formance of five different methods that measure
the semantic distance between words in the Word-
Net Hierarchy, which Patwardhan et al (2003) have
then implemented and made available for general
use as the Perl package distance-0.11.2 We fo-
cused in particular on the following three measures,
the first two of which are based on information the-
oretic principles, and the third on sense topology:
? Resnik (1995) combined WordNet with corpus
statistics. He defines the similarity between
two words as the information content of the
lowest superordinate in the hierarchy, defining
the information content of a concept c (where
a concept is the WordNet class containing the
word) to be the negative of its log likelihood.
This is calculated over a corpus of text.
? Lin (1998c) also employs the idea of corpus-
derived information content, and defines the
similarity between two concepts in the follow-
ing way:
sim(C1, C2) =
2 log P (C0)
log P (C1) + log P (C2)
(1)
where C0 is the lowest class in the hierarchy
that subsumes both classes.
2http://www.d.umn.edu/?tpederse/
distance.html
? Hirst and St-Onge (1998) use a system of ?re-
lations? of different strength to determine the
similarity of word senses, conditioned on the
type, direction and relative distance of edges
separating them.
The Patwardhan et al (2003) implementation that
we used calculates the information values from
SemCor, a semantically tagged subset of the Brown
corpus. Note that the first two similarity measures
operate over nouns only, while the last can be ap-
plied to any word class.
The similarity measures described above calcu-
late the similarity between a pair of senses. In the
case that a given constituent word and/or MWE oc-
cur with more than one sense, we calculate a similar-
ity for sense pairing between them, and average over
them to produce a consolidated similarity value.
4 Evaluation
LSA was used to build models in which MWEs
could be compared with their constituent words.
Two models were built, one from the WSJ corpus
(indexing NN compounds) and one from the BNC
(indexing verb-particles). After removing stop-
words, the 50,000 most frequent terms were indexed
in each model. From the WSJ, these 50,000 terms
included 1,710 NN compounds (with corpus fre-
quency of at least 13) and from the BNC, 461 verb-
particles (with corpus frequency of at least 49).
We used these models to compare different words,
and to find their neighbours. For example, the neigh-
bours of the simplex verb cut and the verb-particles
cut out and cut off (from the BNC model) are shown
in Table 2. As can be seen, several of the neighbours
of cut out are from similar semantic areas as those
of cut, whereas those of cut off are quite different.
cut (verb) cut out (verb) cut off (verb)
cut verb 1.000000 cut out verb 1.000000 cut off verb 1.000000
trim verb 0.529886 fondant nn 0.516956 knot nn 0.448871
slash verb 0.522370 fondant jj 0.501266 choke verb 0.440587
cut nns 0.520345 strip nns 0.475293 vigorously rb 0.438071
cut nn 0.502100 piece nns 0.449555 suck verb 0.413003
reduce verb 0.465364 roll nnp 0.440769 crush verb 0.412301
cut out verb 0.433465 stick jj 0.434082 ministry nn 0.408702
pull verb 0.431929 cut verb 0.433465 glycerol nn 0.395148
fall verb 0.426111 icing nn 0.432307 tap verb 0.383932
hook verb 0.419564 piece nn 0.418780 shake verb 0.381581
recycle verb 0.413206 paste nn 0.416581 jerk verb 0.381284
project verb 0.401246 tip nn 0.413603 put down verb 0.380368
recycled jj 0.396315 hole nns 0.412813 circumference nn 0.378097
prune verb 0.395656 straw nn 0.411617 jn nnp 0.375634
pare verb 0.394991 hook nn 0.402947 pump verb 0.373984
tie verb 0.392964 strip nn 0.399974 nell nnp 0.373768
Table 2: Semantic neighbours of the verbs cut, cut out, and cut off .
Construction Method Pearson R2
Resnik .108 .012
NN compound Lin .101 .010
HSO .072 .005
verb-particle HSO .255 .065
Table 3: Correlation between LSA and WordNet
similarities
This reflects the fact that in most of its instances the
verb cut off is used to mean ?forcibly isolate?.
In order to measure this effect quantitatively, we
can simply take the cosine similarities between these
verbs, finding that sim(cut, cut out) = 0.433 and
sim(cut, cut off) = 0.183 from which we infer di-
rectly that, relative to the sense of cut, cut out is a
clearer case of a simple decomposable MWE than
cut off .
4.1 Statistical analysis
In order to get an initial feel for how well
the LSA-based similarities for MWEs and their
head words correlate with the WordNet-based
similarities over those same word pairs, we
did a linear regression and Pearson?s correla-
tion analysis of the paired data (i.e. the pair-
ing ?simLSA(word i,mwe), simWN(word i,mwe)?
for each WordNet similarity measure simWN). For
both tests, values closer to 0 indicate random distri-
bution of the data, whereas values closer to 1 indi-
cate a strong correlation. The correlation results for
NN compounds and verb-particles are presented in
Table 3, where R2 refers to the output of the linear
regression test and HSO refers to Hirst and St-Onge
similarity measure. In the case of NN compounds,
the correlation with LSA is very low for all tests,
that is LSA is unable to reproduce the relative sim-
ilarity values derived from WordNet with any reli-
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3
M
ea
n
 H
yp
on
ym
y
Partition No.
VPC(head)          
VPC(head)       ALL
HIGH
NN(mod)       
NN(head)       ALL
ALL
NN(head)         LOW
NN(head)          HIGH
VPC(head)         LOW
Figure 1: Hyponymy correlation
ability. With verb-particles, correlation is notably
higher than for NN compounds,3 but still at a low
level.
Based on these results, LSA would appear to
correlate poorly with WordNet-based similarities.
However, our main interest is not in similarity per
se, but how reflective LSA similarities are of the de-
composability of the MWE in question. While tak-
ing note of the low correlation with WordNet simi-
larities, therefore, we move straight on to look at the
hyponymy test.
4.2 Hyponymy-based analysis
We next turn to analysis of correlation between LSA
similarities and hyponymy values. Our expectation
is that for constituent word?MWE pairs with higher
LSA similarities, there is a greater likelihood of the
MWE being a hyponym of the constituent word. We
test this hypothesis by ranking the constituent word?
MWE pairs in decreasing order of LSA similarity,
3Recall that HSO is the only similarity measure which oper-
ates over verbs.
and partitioning the ranking up into m partitions of
equal size. We then calculate the average number of
hyponyms per partition. If our hypothesis is correct,
the earlier partitions (with higher LSA similarities)
will have higher occurrences of hyponyms than the
latter partitions.
Figure 1 presents the mean hyponymy values
across partitions of the NN compound data and verb-
particle data, with m set to 3 in each case. For the
NN compounds, we derive two separate rankings,
based on the similarity between the head noun and
NN compound (NN(head)) and the modifier noun
and the NN compound (NN(mod)). In the case of
the verb-particle data, WordNet has no classification
of prepositions or particles, so we can only calcu-
late the similarity between the head verb and verb-
particle (VPC(head)). Looking to the curves for
these three rankings, we see that they are all fairly
flat, nondescript curves. If we partition the data up
into low- and high-frequency MWEs, as defined by a
threshold of 100 corpus occurrences, we find that the
graphs for the low-frequency data (NN(head)LOW
and VPC(head)LOW) are both monotonically de-
creasing, whereas those for high-frequency data
(NN(head)HIGH and VPC(head)HIGH) are more hap-
hazard in nature. Our hypothesis of lesser instances
of hyponymy for lower similarities is thus supported
for low-frequency items but not for high-frequency
items, suggesting that LSA similarities are more
brittle over high-frequency items for this particu-
lar task. The results for the low-frequency items
are particularly encouraging given that the LSA-
based similarities were found to correlate poorly
with WordNet-derived similarities. The results for
NN(mod) are more erratic for both low- and high-
frequency terms, that is the modifier noun is not as
strong a predictor of decomposability as the head
noun. This is partially supported by the statistics on
the relative occurrence of NN compounds in Word-
Net subsumed by their head noun (71.4%) as com-
pared to NN compounds subsumed by their modifier
(13.7%).
In an ideal world, we would hope that the val-
ues for mean hyponymy were nearly 1 for the first
partition and nearly 0 for the last. Naturally, this
presumes perfect correlation of the LSA similarities
with decomposability, but classificational inconsis-
tencies in WordNet alo work against us. For ex-
ample, vice chairman is an immediate hyponym of
both chairman and president, but vice president is
not a hyponym of president. According to LSA,
however, sim(chairman, vice chairman) = .508 and
sim(president, vice president) = .551.
It remains to be determined why LSA should per-
form better over low-frequency items, although the
higher polysemy of high-frequency items is one po-
tential cause. We intend to further investigate this
matter in future research.
5 Discussion
While evaluation pointed to a moderate correlation
between LSA similarities and occurrences of hy-
ponymy, we have yet to answer the question of
exactly where the cutoffs between simple decom-
posable, idiosyncratically decomposable and non-
decomposable MWEs lie. While it would be pos-
sible to set arbitrary thresholds to artificially parti-
tion up the space of MWEs based on LSA similarity
(or alternatively use statistical tests to derive confi-
dence intervals for similarity values), we feel that
more work needs to be done in establishing exactly
what different LSA similarities for different MWE?
constituent word combinations mean.
One area in which we plan to extend this research
is the analysis of MWEs in languages other than
English. Because of LSA?s independence from lin-
guistic constraints, it is equally applicable to all lan-
guages, assuming there is some way of segmenting
inputs into constituent words.
To summarise, we have proposed a construction-
inspecific empirical model of MWE decomposabil-
ity, based on latent semantic analysis. We evaluated
the method over English NN compounds and verb-
particles, and showed it to correlate moderately with
WordNet-based hyponymy values.
Acknowledgements
This material is partly based upon work supported by the Na-
tional Science Foundation under Grant No. BCS-0094638 and
also the Research Collaboration between NTT Communication
Science Laboratories, Nippon Telegraph and Telephone Corpo-
ration and CSLI, Stanford University. We would like to thank
the anonymous reviewers for their valuable input on this re-
search.
References
Ricardo Baeza-Yates and Berthier Ribiero-Neto. 1999. Modern
Information Retrieval. Addison Wesley / ACM press.
Timothy Baldwin and Aline Villavicencio. 2002. Extracting
the unextractable: A case study on verb-particles. In Proc. of
the 6th Conference on Natural Language Learning (CoNLL-
2002), Taipei, Taiwan.
Colin Bannard, Timothy Baldwin, and Alex Lascarides. 2003.
A statistical approach to the semantics of verb-particles. In
Proc. of the ACL-2003 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment. (this volume).
Colin Bannard. 2002. Statistical techniques for automati-
cally inferring the semantics of verb-particle constructions.
LinGO Working Paper No. 2002-06.
Alexander Budanitsky and Graeme Hirst. 2001. Semantic dis-
tance in WordNet: An experimental, application-oriented
evaluation of five measures. In Workshop on Wordnet and
Other Lexical Resources, Second meeting of the NAACL,
Pittsburgh, USA.
Lou Burnard. 2000. User Reference Guide for the British Na-
tional Corpus. Technical report, Oxford University Comput-
ing Services.
Nicoletta Calzolari, Charles Fillmore, Ralph Grishman, Nancy
Ide, Alessandro Lenci, Catherine MacLeod, and Antonio
Zampolli. 2002. Towards best practice for multiword ex-
pressions in computational lexicons. In Proceedings of the
Third International Conference on Language Resources and
Evaluation (LREC 2002), pages 1934?40, Las Palmas, Ca-
nary Islands.
Ann Copestake, Fabre Lambeau, Aline Villavicencio, Francis
Bond, Timothy Baldwin, Ivan A. Sag, and Dan Flickinger.
2002. Multiword expressions: Linguistic precision and
reusability. In Proc. of the 3rd International Conference
on Language Resources and Evaluation (LREC 2002), pages
1941?7, Las Palmas, Canary Islands.
Scott Deerwester, Susan Dumais, George Furnas, Thomas Lan-
dauer, and Richard Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society for In-
formation Science, 41(6):391?407.
Martin Haspelmath. 2002. Understanding Morphology.
Arnold Publishers.
Graeme Hirst and David St-Onge. 1998. Lexical chains as
representations of context for the detection and correction
of malapropism. In Christiane Fellbaum, editor, WordNet:
An Electronic Lexical Database, pages 305?32. MIT Press,
Cambridge, USA.
Thomas Landauer and Susan Dumais. 1997. A solution to
Plato?s problem: The latent semantic analysis theory of ac-
quisition. Psychological Review, 104(2):211?240.
Dekang Lin. 1998a. Automatic retrieval and clustering of simi-
lar words. In Proceedings of the 36th Annual Meeting of the
ACL and 17th International Conference on Computational
Linguistics (COLING/ACL-98).
Dekang Lin. 1998b. Extracting collocations from text corpora.
In First Workshop on Computational Terminology.
Dekang Lin. 1998c. An information-theoretic definition of
similarity. In Proceedings of the 15th International Confer-
ence on Machine Learning.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of the 37th Annual Meeting
of the ACL, pages 317?24, College Park, USA.
Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting
a continuum of compositionality in phrasal verbs. In Proc. of
the ACL-2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment. (this volume).
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proc. of the
2nd Conference on Empirical Methods in Natural Language
Processing (EMNLP-97), Providence, USA.
George A. Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine J. Miller. 1990. Introduction
to WordNet: an on-line lexical database. International Jour-
nal of Lexicography, 3(4):235?44.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting
of the North American Chapter of Association for Compu-
tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,
USA.
Geoffrey Nunberg, Ivan A. Sag, and Tom Wasow. 1994. Id-
ioms. Language, 70:491?538.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen.
2003. Using measures of semantic relatedness for word
sense disambiguation. In Proc. of the 4th International Con-
ference on Intelligent Text Processing and Computational
Linguistics (CICLing-2003), Mexico City, Mexico.
Darren Pearce. 2001a. Synonymy in collocation extraction. In
Proc. of the NAACL 2001 Workshop on WordNet and Other
Lexical Resources: Applications, Extensions and Customiza-
tions, Pittsburgh, USA.
Darren Pearce. 2001b. Using conceptual similarity for collo-
cation extraction. In Proc. of the 4th UK Special Interest
Group for Computational Linguistics (CLUK4).
Philip Resnik. 1995. Using information content to evaluate
semantic similarity. In Proceedings of the 14th International
Joint Conference on Artificial Intelligence.
Susanne Riehemann. 2001. A Constructional Approach to Id-
ioms and Word Formation. Ph.D. thesis, Stanford.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake,
and Dan Flickinger. 2002. Multiword expressions: A pain in
the neck for NLP. In Proc. of the 3rd International Confer-
ence on Intelligent Text Processing and Computational Lin-
guistics (CICLing-2002), pages 1?15, Mexico City, Mexico.
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-free
induction of multiword unit dictionary headwords a solved
problem? In Proc. of the 6th Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2001), pages
100?108.
Hinrich Sch u?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
Dominic Widdows, Beate Dorow, and Chiu-Ki Chan. 2002.
Using parallel corpora to enrich multilingual lexical re-
sources. In Third International Conference on Language Re-
sources and Evaluation, pages 240?245, Las Palmas, Spain,
May.
Dominic Widdows. 2003. Unsupervised methods for develop-
ing taxonomies by combining syntactic and statistical infor-
mation. In Proc. of the 3rd International Conference on Hu-
man Language Technology Research and 4th Annual Meet-
ing of the NAACL (HLT-NAACL 2003). (to appear).
Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 48?56,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automatic Extraction of Idioms using Graph Analysis and Asymmetric
Lexicosyntactic Patterns
Dominic Widdows
MAYA Design, Inc.
Pittsburgh, Pennsylvania
widdows@maya.com
Beate Dorow
Institute for Natural Language Processing
University of Stuttgart
dorowbe@IMS.Uni-Stuttgart.DE
Abstract
This paper describes a technique for ex-
tracting idioms from text. The tech-
nique works by finding patterns such as
?thrills and spills?, whose reversals (such
as ?spills and thrills?) are never encoun-
tered.
This method collects not only idioms, but
also many phrases that exhibit a strong
tendency to occur in one particular order,
due apparently to underlying semantic is-
sues. These include hierarchical relation-
ships, gender differences, temporal order-
ing, and prototype-variant effects.
1 Introduction
Natural language is full of idiomatic and metaphor-
ical uses. However, language resources such as dic-
tionaries and lexical knowledge bases give at best
poor coverage of such phenomena. In many cases,
knowledge bases will mistakenly ?recognize? a word
and this can lead to more harm than good: for exam-
ple, a typical mistake of blunt logic would be to as-
sume that ?somebody let the cat out of the bag? im-
plied that ?somebody let some mammal out of some
container.?
Idiomatic generation of natural language is, if
anything, an even greater challenge than idiomatic
language understanding. As pointed out decades ago
by Fillmore (1967), a complete knowledge of En-
glish requires not only an understanding of the se-
mantics of the word good, but also an awareness
that this special adjective (alone) can occur with the
word any to construct phrases like ?Is this paper
any good at all??, and traditional lexical resources
were not designed to provide this information. There
are many more general examples occur: for exam-
ple, ?the big bad wolf? sounds right and the ?the bad
big wolf? sounds wrong, even though both versions
are syntactically and semantically plausible. Such
examples are perhaps ?idiomatic?, though we would
perhaps not call them ?idioms?, since they are com-
positional and can sometimes be predicted by gen-
eral pattern of word-ordering.
In general, the goal of manually creating a com-
plete lexicon of idioms and idiomatic usage patterns
in any language is unattainable, and automatic ex-
traction and modelling techniques have been devel-
oped to fill this ever-evolving need. Firstly, auto-
matically identifying potential idioms and bringing
them to the attention of a lexicographer can be used
to improve coverage and reduce the time a lexicog-
rapher must spend in searching for such examples.
Secondly and more ambitiously, the goal of such
work is to enable computers to recognize idioms in-
dependently so that the inevitable lack of coverage
in language resources does not impede their ability
to respond intelligently to natural language input.
In attempting a first-pass at this task, the exper-
iments described in this paper proceed as follows.
We focus on a particular class of idioms that can
be extracted using lexicosyntactic patterns (Hearst,
1992), which are fixed patterns in text that suggest
that the words occurring in them have some inter-
esting relationship. The patterns we focus on are
occurrences of the form ?A and/or B?, where A and
48
B are both nouns. Examples include ?football and
cricket? and ?hue and cry.? From this list, we extract
those examples for which there is a strong prefer-
ence on the ordering of the participants. For exam-
ple, we do see the pattern ?cricket and football,? but
rarely if ever encounter the pattern ?cry and hue.?
Using this technique, 4173 potential idioms were ex-
tracted. This included a number of both true idioms,
and words that have regular semantic relationships
but do appear to have interesting orderings on these
relationships (such as earlier before later, strong be-
fore weak, prototype before variant).
The rest of this paper is organized as follows. Sec-
tion 2 elaborates on some of the previous works
that motivate the techniques we have used. Sec-
tion 3 describes the precise method used to extract
idioms through their asymmetric appearance in a
large corpus. Section 4 presents and analyses several
classes of results. Section 5 describes the methods
attempted to filter these results into pairs of words
that are more and less contextually related to one an-
other. These include a statistical method that analy-
ses the original corpus for evidence of semantic re-
latedness, and a combinatoric method that relies on
link-analysis on the resulting graph structure.
2 Previous and Related Work
This section describes previous work in extracting
information from text, and inferring semantic or id-
iomatic properties of words from the information so
derived.
The main technique used in this paper to ex-
tract groups of words that are semantically or id-
iomatically related is a form of lexicosyntactic pat-
tern recognition. Lexicosyntactic patterns were pio-
neered by Marti Hearst (Hearst, 1992; Hearst and
Schu?tze, 1993) in the early 1990?s, to enable the
addition of new information to lexical resources
such as WordNet (Fellbaum, 1998). The main in-
sight of this sort of work is that certain regular pat-
terns in word-usage can reflect underlying seman-
tic relationships. For example, the phrase ?France,
Germany, Italy, and other European countries? sug-
gests that France, Germany and Italy are part of
the class of European countries. Such hierarchi-
cal examples are quite sparse, and greater coverage
was later attained by Riloff and Shepherd (1997)
and Roark and Charniak (1998) in extracting rela-
tions not of hierarchy but of similarity, by find-
ing conjunctions or co-ordinations such as ?cloves,
cinammon, and nutmeg? and ?cars and trucks.? This
work was extended by Caraballo (1999), who built
classes of related words in this fashion and then rea-
soned that if a hierarchical relationship could be ex-
tracted for any member of this class, it could be ap-
plied to all members of the class. This technique
can often mistakenly reason across an ambiguous
middle-term, a situation that was improved upon
by Cederberg and Widdows (2003), by combining
pattern-based extraction with contextual filtering us-
ing latent semantic analysis.
Prior work in discovering non-compositional
phrases has been carried out by Lin (1999)
and Baldwin et al (2003), who also used LSA
to distinguish between compositional and non-
compositional verb-particle constructions and noun-
noun compounds.
At the same time, work in analyzing idioms and
asymmetry within linguistics has become more so-
phisticated, as discussed by Benor and Levy (2004),
and many of the semantic factors underlying our re-
sults can be understood from a sophisticated theoret-
ical perspective.
Other motivating and related themes of work for
this paper include collocation extraction and ex-
ample based machine translation. In the work of
Smadja (1993) on extracting collocations, prefer-
ence was given to constructions whose constituents
appear in a fixed order, a similar (and more generally
implemented) version of our assumption here that
asymmetric constructions are more idiomatic than
symmetric ones. Recent advances in example-based
machine translation (EBMT) have emphasized the
fact that examining patterns of language use can
significantly improve idiomatic language generation
(Carl and Way, 2003).
3 The Symmetric Graph Model as used for
Lexical Acquisition and Idiom
Extraction
This section of the paper describes the techniques
used to extract potentially idiomatic patterns from
text, as deduced from previously successful experi-
ments in lexical acquisition.
49
The main extraction technique is to use lexicosyn-
tactic patterns of the form ?A, B and/or C? to find
nouns that are linked in some way. For example,
consider the following sentence from the British Na-
tional Corpus (BNC).
Ships laden with nutmeg, cinnamon,
cloves or coriander once battled the
Seven Seas to bring home their precious
cargo.
Since the BNC is tagged for parts-of-speech, we
know that the words highlighted in bold are nouns.
Since the phrase ?nutmeg, cinnamon, cloves or co-
riander? fits the pattern ?A, B, C or D?, we create
nodes for each of these nouns and create links be-
tween them all. When applied to the whole of the
BNC, these links can be aggregated to form a graph
with 99,454 nodes (nouns) and 587,475 links, as de-
scribed by Widdows and Dorow (2002). This graph
was originally used for lexical acquisition, since
clusters of words in the graph often map to recog-
nized semantic classes with great accuracy (> 80%,
(Widdows and Dorow, 2002)).
However, for the sake of smoothing over sparse
data, these results made the assumption that the links
between nodes were symmetric, rather than directed.
In other words, when the pattern ?A and/or B? was
encountered, a link from A to B and a link from B
to A was introduced. The nature of symmetric and
antisymmetric relationships is examined in detail by
Widdows (2004). For the purposes of this paper, it
suffices to say that the assumption of symmetry (like
the assumption of transitivity) is a powerful tool for
improving recall in lexical acquisition, but also leads
to serious lapses in precision if the directed nature of
links is overlooked, especially if symmetrized links
are used to infer semantic similarity.
This problem was brought strikingly to our atten-
tion by the examples in Figure 1. In spite of appear-
ing to be a circle of related concepts, many of the
nouns in this group are not similar at all, and many
of the links in this graph are derived from very very
different contexts. In Figure 1, cat and mouse are
linked (they are re both animals and the phrase ?cat
and mouse? is used quite often): but then mouse
and keyboard are also linked because they are both
objects used in computing. A keyboard, as well
as being a typewriter or computer keyboard, is also
fiddle
cat
barrowbow
cello
flute
mouse
dog
game
kitten
violin
piano
bass
fortepiano
orchestra
keyboard
screen
monitor
memory
guitar
rat
human
Figure 1: A cluster involving several idiomatic links
used to mean (part of) a musical instrument such as
an organ or piano, and keyboard is linked to vio-
lin. A violin and a fiddle are the same instrument (as
often happens with synonyms, they don?t appear to-
gether often but have many neighbours in common).
The unlikely circle is completed (it turns out) be-
cause of the phrase from the nursery rhyme
Hey diddle diddle,
The cat and the fiddle,
The cow jumped over the moon;
It became clear from examples such as these that
idiomatic links, like ambiguous words, were a seri-
ous problem when using the graph model for lexical
acquisition. However, with ambiguous words, this
obstacle has been gradually turned into an opportu-
nity, since we have also developed ways to used the
apparent flaws in the model to detect which words
are ambiguous in the first place (Widdows, 2004, Ch
4). It is now proposed that we can take the same op-
portunity for certain idioms: that is, to use the prop-
erties of the graph model to work out which links
arise from idiomatic usage rather than semantic sim-
ilarity.
3.1 Idiom Extraction by Recognizing
Asymmetric Patterns
The link between the cat and fiddle nodes in Fig-
ure 1 arises from the phrase ?the cat and the fiddle.?
50
Table 1: Sample of asymmetric pairs extracted from
the BNC.
First word Second word
highway byway
cod haddock
composer conductor
wood charcoal
element compound
assault battery
north south
rock roll
god goddess
porgy bess
middle class
war aftermath
god hero
metal alloy
salt pepper
mustard cress
stocking suspender
bits bobs
stimulus response
committee subcommittee
continent ocean
However, no corpus examples were ever found of the
converse phrase, ?the fiddle and the cat.? In cases
like these, it may be concluded that placing a sym-
metric link between these two nodes is a mistake.
Instead, a directed link may be more appropriate.
We therefore formed the hypothesis that if the
phrase ?A and/or B? occurs frequently in a corpus,
but the phrase ?B and/or A? is absent, then the link
between A and B should be attributed to idiomatic
usage rather than semantic similarity.
The next step was to rebuild, finding those rela-
tionships that have a strong preference for occurring
in a fixed order. Sure enough, several British English
idioms were extracted in this way. However, several
other kinds of relationships were extracted as well,
as shown in the sample in Table 1.1
After extracting these pairs, groups of them were
gathered together into directed subgraphs.2 Some of
these directed subgraphs are reporduced in the anal-
ysis in the following section.
1The sample chosen here was selected by the authors to be
representative of some of the main types of results. The com-
plete list can be found at http://infomap.stanford.
edu/graphs/idioms.html.
2These can be viewed at http://infomap.
stanford.edu/graphs/directed_graphs.html
4 Analysis of Results
The experimental results include representatives of
several types of asymmetric relationships, including
the following broad categories.
?True? Idioms
There are many results that display genuinely id-
iomatic constructions. By this, we mean phrases that
have an explicitly lexicalized nature that a native
speaker may be expected to recognize as having a
special reference or significance. Examples include
the following:
thrills and spills
bread and circuses
Punch and Judy
Porgy and Bess
lies and statistics
cat and fiddle
bow and arrow
skull and crossbones
This category is quite loosely defined. It includes
1. historic quotations such as ?lies, damned lies
and statistics?3 and ?bread and circuses.?4
2. titles of well-known works.
3. colloquialisms.
4. groups of objects that have become fixed nom-
inals in their own right.
All of these types share the common property that
any NLP system that encounters such groups, in or-
der to behave correctly, should recognize, generate,
or translate them as phrases rather than words.
Hierarchical Relationships
Many of the asymmetric relationships follow
some pattern that may be described as roughly hi-
erarchical. A cluster of examples from two domains
is shown in Figure 2. In chess, a rook outranks a
bishop, and the phrase ?rook and bishop? is encoun-
tered much more often than the phrase ?bishop and
3Attributed to Benjamin Disraeli, certainly popularized by
Mark Twain.
4A translation of ?panem et circenses,? from the Roman
satirist Juvenal, 1st century AD.
51
Figure 2: Asymmetric relationships in the chess and
church hierarchies
Figure 3: Different beverages, showing their di-
rected relationships
rook.? In the church, a cardinal outranks a bishop,
a bishop outranks most of the rest of the clergy, and
the clergy (in some senses) outrank the laity.
Sometimes these relationships coincide with fig-
ure / ground and agent / patient distinctions. Ex-
amples of this kind, as well as ?clergy and laity?,
include ?landlord and tenant?, ?employer and em-
ployee?, ?teacher and pupil?, and ?driver and pas-
sengers?. An interesting exception is ?passengers
and crew?, for which we have no semantic explana-
tion.
Pedigree and potency appear to be two other di-
mensions that can be used to establish the directed-
ness of an idiomatic construction. For example, Fig-
ure 3 shows that alcoholic drinks normally appear
before their cocktail mixers, but that wine outranks
some stronger drinks.
Figure 4: Hierarchical relationships between aristo-
crats, some of which appear to be gender based
Gender Asymmetry
The relationship between corresponding concepts
of different genders also appear to be heavily biased
towards appearing in one direction. Many of these
relationships are shown in Figure 4. This shows
that, in cases where one class outranks another, the
higher class appears first, but if the classes are iden-
tical, then the male version tends to appear before
the female. This pattern is repeated in many pairs
of words such as ?host and hostess?, ?god and god-
dess?, etc. One exception appears to be in parent-
ing relationships, where female precedes male, as in
?mother and father?, ?mum and dad?, ?grandma and
grandpa?.
Temporal Ordering
If one word refers to an event that precedes an-
other temporally or logically, it almost always ap-
pears first. The examples in Table 2 were extracted
by our experiment. It has been pointed out that for
cyclical events, it is perfectly possible that the order
of these pairs may be reversed (e.g., ?late night and
early morning?), though the data we extracted from
the BNC showed strong tendencies in the directions
given.
A directed subgraph showing many events in hu-
man lives in shown in Figure 5.
Prototype precedes Variant
In cases where one participant is regarded as a
?pure? substance and the other is a variant or mix-
ture, the pure substance tends to come first. These
occur particularly in scientific writing, examples
including ?element and compound?, ?atoms and
52
Table 2: Pairs of events that have a strong tendency
to occur in asymmetric patterns.
Before After
spring autumn
morning afternoon
morning evening
evening night
morning night
beginning end
question answer
shampoo conditioner
marriage divorce
arrival departure
eggs larvae
molecules?, ?metals and alloys?. Also, we see ?ap-
ples and pears?, ?apple and plums?, and ?apples and
oranges?, suggesting that an apple is a prototypical
fruit (in agreement with some of the results of pro-
totype theory; see Rosch (1975)).
Another possible version of this tendency is that
core precedes periphery, which may also account for
asymmetric ordering of food items such as ?fish and
chips?, ?bangers and mash?, ?tea and coffee? (in the
British National Corpus, at least!) In some cases
such as ?meat and vegetables?, a hierarchical or fig-
ure / ground distinction may also be argued.
Mistaken extractions
Our preliminary inspection has shown that the ex-
traction technique finds comparatively few genuine
mistakes, and the reader is encouraged to follow the
links provided to check this claim. However, there
are some genuine errors, most of which could be
avoided with more sophisticated preprocessing.
To improve recall in our initial lexical acquisition
experiments, we chose to strip off modifiers and to
stem plural forms to singular forms, so that ?apples
and green pears? would give a link between apple
and pear.
However, in many cases this is a mistake, be-
cause the bracketing should not be of the form ?A
and (B C),? but of the form ?(A and B) C.? Us-
ing part-of-speech tags alone, we cannot recover
this information. One example is the phrase ?hard-
ware and software vendors,? from which we ob-
tain a link between hardware and vendors, in-
stead of a link between hardware and software.
A fuller degree of syntactic analysis would improve
this situation. For extracting semantic relationships,
Figure 5: Directed graph showing that life-events
are usually ordered temporally when they occur to-
gether
Cederberg and Widdows (2003) demonstrated that
nounphrase chunking does this work very satisfacto-
rily, while being much more tractable than full pars-
ing.
The mistaken pair middle and class shown in
Table 1 is another of these mistakes, arising from
phrases such as ?middle and upper class? and ?mid-
dle and working class.? These examples could be
avoided simply by more accurate part-of-speech tag-
ging (since the word ?middle? should have been
tagged as an adjective in these examples).
This concludes our preliminary analysis of re-
sults.
5 Filtering using Latent Semantic Analysis
and Combinatoric Analysis
From the results in the previous section, the follow-
ing points are clear.
1. It is possible to extract many accurate exam-
ples of asymmetric constructions, that would be
necessary knowledge for generation of natural-
sounding language.
2. Some of the pairs extracted are examples of
general semantic patterns, others are examples
of genuinely idiomatic phrases.
Even for semantically predictable phrases, the
fact that the words occur in fixed patterns can be
very useful for the purposes of disambiguation, as
demonstrated by (Yarowsky, 1995). However, it
53
would be useful to be able to tell which of the asym-
metric patterns extracted by our experiments corre-
spond to semantically regular phrases which hap-
pen to have a conventional ordering preference, and
which phrases correspond to genuine idioms. This
final section demonstrates two techniques for per-
forming this filtering task, which show promising re-
sults for improving our classification, though should
not yet be considered as reliable.
5.1 Filtering using Latent Semantic Analysis
Latent semantic analysis or LSA (Landauer and Du-
mais, 1997) is by now a tried and tested technique
for determining semantic similarity between words
by analyzing large corpus (Widdows, 2004, Ch 6).
Because of this, LSA can be used to determine
whether a pair of words is likely to participate in a
regular semantic relationship, even though LSA may
not contribute specific information regarding the na-
ture of the relationship. However, once a relation-
ship is expected, LSA can be used to predict whether
this relationship is used in contexts that are typical
uses of the words in question, or whether these uses
appear to be anomalies such as rare senses or idioms.
This technique was used successfully by (Cederberg
and Widdows, 2003) to improve the accuracy of hy-
ponymy extraction. It follows that it should be use-
ful to tell the difference between regularly related
words and idiomatically related words.
To test this hypothesis, we used an LSA model
built from the BNC using the Infomap NLP soft-
ware.5 This was used to measure the LSA similar-
ity between the words in each of the pairs extracted
by the techniques in Section 4. In cases where a
word was too infrequent to appear in the LSA model,
we used ?folding in,? which assigns a word-vector
?on the fly? by adding together the vectors of any
surrounding words of a target word that are in the
model.
The results are shown in Table 3. The hypothesis
is that words whose occurrence is purely idiomatic
would have a low LSA similarity score, because
they are otherwise not closely related. However, this
hypothesis does not seem to have been confirmed,
partly due to the effects of overall frequency. For
example, the word Porgy only occurs in the phrase
5Freely available from http://infomap-nlp.
sourceforge.net/
Table 3: Ordering of results from semantically sim-
ilar to semantically dissimilar using LSA
Word pair LSA similarity
north south 0.931
middle class 0.834
porgy bess 0.766
war aftermath 0.676
salt pepper 0.672
bits bobs 0.671
mustard cress 0.603
composer conductor 0.588
cod haddock 0.565
metal alloy 0.509
highway byway 0.480
committee subcommittee 0.479
god goddess 0.456
rock roll 0.398
continent ocean 0.300
wood charcoal 0.273
stimulus response 0.261
stocking suspender 0.177
god hero 0.115
element compound 0.044
assault battery -0.068
granite
cheese
bread
chalk
limestone   flint
marble  coal sand
 sandstone butter meat wine
sugar  margarine milk  clay
Figure 6: Nodes in the original symmetric graph in
the vicinity of chalk and cheese
?Porgy and Bess,? and the word bobs almost always
occurs in the phrase ?bits and bobs.? A more effec-
tive filtering technique would need to normalize to
account for these effects. However, there are some
good results: for example, the low score between
assault and battery reflects the fact that this usage,
though compositional, is a rare meaning of the word
battery, and the same argument can be made for el-
ement and compound. Thus LSA might be a better
guide for recognizing rarity in meaning of individual
words than it is for idiomaticity of phrases.
5.2 Link analysis
Another technique for determining whether a link is
idiomatic or not is to check whether it connects two
54
areas of meaning that are otherwise unconnected. A
hallmark example of this phenomenon is the ?chalk
and cheese? example shown in Figure 6. 6 Note that
none of the other members of the rock-types clus-
ters is linked to any of the other foodstuffs. We may
be tempted to conclude that the single link between
these clusters is an idiomatic phenomenon. This
technique shows promise, but has yet to be explored
in detail.
6 Conclusions and Further Work
It is possible to extract asymmetric constructions
from text, some of which correspond to idioms
which are indecomposable (in the sense that their
meaning cannot be decomposed into a combination
of the meanings of their constituent words).
Many other phrases were extracted which exhibit
a typical directionality that follows from underlying
semantic principles. While these are sometimes not
defined as ?idioms? (because they are still compos-
able), knowledge of their asymmetric behaviour is
necessary for a system to generate natural language
utterances that would sound ?idiomatic? to native
speakers.
While all of this information is useful for cor-
rectly interpreting and generating natural language,
further work is necessary to distinguish accurately
between these different categories. The first step in
this process will be to manually classify the results,
and evaluate the performance of different classifica-
tion techniques to see if they can reliably identify
different types of idiom, and also distinguish these
cases from false positives that were mistakenly ex-
tracted. Once some of these techniques have been
evaluated, we will be in a better position to broaden
our techniques by turning to larger corpora such as
the Web.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL-2003 Workshop on Multiword Expres-
6
?Chalk and cheese? is a widespread idiom in British En-
glish, used to contrast two very different objects, e.g. ?They
are as different as chalk and cheese.? A roughly corresponding
(though more predictable) phrase in American English might be
?They are as different as night and day.?
sions: Analysis, Acquisition and Treatment, Sapporo,
Japan.
Sarah Bunin Benor and Roger Levy. 2004. The chicken
or the egg? a probabilistic analysis of english bi-
nomials. http://www.stanford.edu/?rog/
papers/binomials.pdf.
Sharon Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In 37th
Annual Meeting of the Association for Computational
Linguistics: Proceedings of the Conference, pages
120?126.
M Carl and A Way, editors. 2003. Recent Advances in
Example-Based Machine Translation. Kluwer.
Scott Cederberg and Dominic Widdows. 2003. Using
LSA and noun coordination information to improve
the precision and recall of automatic hyponymy ex-
traction. In Conference on Natural Language Learn-
ing (CoNNL), Edmonton, Canada.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge MA.
Charles J. Fillmore. 1967. The grammar of hitting and
breaking. In R. Jacobs, editor, In Readings in English:
Transformational Grammar, pages 120?133.
Marti Hearst and Hinrich Schu?tze. 1993. Customizing
a lexicon to better suit a computational task. In ACL
SIGLEX Workshop, Columbus, Ohio.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING, Nantes,
France.
Thomas Landauer and Susan Dumais. 1997. A solution
to plato?s problem: The latent semantic analysis the-
ory of acquisition. Psychological Review, 104(2):211?
240.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In ACL:1999, pages 317?324.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Claire
Cardie and Ralph Weischedel, editors, Proceedings of
the Second Conference on Empirical Methods in Natu-
ral Language Processing, pages 117?124. Association
for Computational Linguistics, Somerset, New Jersey.
Brian Roark and Eugene Charniak. 1998. Noun-phrase
co-occurence statistics for semi-automatic semantic
lexicon construction. In COLING-ACL, pages 1110?
1116.
Eleanor Rosch. 1975. Cognitive representations of se-
mantic categories. Journal of Experimental Psychol-
ogy: General, 104:192?233.
55
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?177.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In 19th In-
ternational Conference on Computational Linguistics,
pages 1093?1099, Taipei, Taiwan, August.
Dominic Widdows. 2004. Geometry and Meaning.
CSLI publications, Stanford, California.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics, pages 189?196.
56
