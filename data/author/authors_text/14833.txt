Improved Lexical Alignment by Combining Multiple Reified
Alignments
Dan Tufi?
 Institute for Artificial 
Intelligence
 13, ?13 Septembrie?, 
 050711, Bucharest 5, 
Romania 
tufis@racai.ro
Radu Ion 
Institute for Artificial
Intelligence
13, ?13 Septembrie?,
050711, Bucharest 5,
Romania 
radu@racai.ro
Alexandru Ceau?u
Institute for Artificial 
Intelligence
13, ?13 Septembrie?, 
050711, Bucharest 5, 
Romania 
alceausu@racai.ro
Dan ?tef?nescu
Institute for Artificial 
Intelligence
13, ?13 Septembrie?, 
050711, Bucharest 5,
 Romania 
danstef@racai.ro
Abstract
We describe a word alignment platform 
which ensures text pre-processing (to-
kenization, POS-tagging, lemmatization, 
chunking, sentence alignment) as re-
quired by an accurate word alignment. 
The platform combines two different 
methods, producing distinct alignments. 
The basic word aligners are described in 
some details and are individually evalu-
ated. The union of the individual align-
ments is subject to a filtering post-
processing phase. Two different filtering 
methods are also presented. The evalua-
tion shows that the combined word 
alignment contains 10.75% less errors 
than the best individual aligner. 
1 Introduction 
It is almost a truism that more decision makers, 
working together, are likely to find a better solu-
tion than when working alone. Dieterich (1998) 
discusses conditions under which different deci-
sions (in his case classifications) may be com-
bined for obtaining a better result. Essentially, a 
successful automatic combination method would 
require comparable performance for the decision 
makers and, additionally, that they should not 
make similar errors. This idea has been exploited 
by various NLP researchers in language model-
ling, statistical POS tagging, parsing, etc.
We developed two quite different word align-
ers, driven by two distinct objectives: the first 
one was motivated by a project aiming at the de-
velopment of an interlingually aligned set of 
wordnets while the other one was developed 
within an SMT ongoing project. The first one 
was used for validating, against a multilingual 
corpus, the interlingual synset equivalences and 
also for WSD experiments. Although, initially, it 
was concerned only with open class words re-
corded in a wordnet, turning it into an ?all 
words? aligner was not a difficult task. This 
word aligner, called YAWA is described in sec-
tion 3.
A quite different approach from the one used 
by YAWA, is implemented in our second word 
aligner, called MEBA, described in section 4. It 
is a multiple parameter and multiple step algo-
rithm using relevance thresholds specific to each 
parameter, but different from each step to the 
other. The implementation of MEBA was 
strongly influenced by the notorious five IBM 
models described in (Brown et al 1993). We 
used GIZA++ (Och and Ney 2000; Och and Ney, 
2003) to estimate different parameters of the 
MEBA aligner. 
The alignments produced by MEBA were 
compared to the ones produced by YAWA and 
evaluated against the Gold Standard (GS)1 anno-
tations used in the Word Alignment Shared 
Tasks (Romanian-English track) organized at 
HLT-NAACL2003 (Mihalcea and Pedersen 
2003).
Given that the two aligners are based on quite 
different models and that their F-measures are 
comparable, it was quite a natural idea to com-
bine their results and hope for an improved align-
ment. Moreover, by analyzing the alignment er-
rors done by each word aligner, we found that 
the number of common mistakes was small, so 
1 We noticed in the GS Alignment various errors (both sen-
tence and word alignment errors) that were corrected. The 
tokenization of the bitexts used in the GS Alignment was 
also modified, with the appropriate modification of the ref-
erence alignment. These reference data are available at 
http://www.racai.ro/res/WA-GS
153
the premises for a successful combination were 
very good (Dieterich, 1998). The Combined 
Word Aligner, COWAL-described in section 5, 
is a wrapper of the two aligners (YAWA and 
MEBA) merging the individual alignments and 
filtering the result. At the Shared Task on Word 
Alignment organized by the ACL2005 Work-
shop on ?Building and Using Parallel Corpora: 
Data-driven Machine Translation and Beyond? 
(Martin, et al 2005), we participated (on the 
Romanian-English track) with the two aligners 
and the combined one (COWAL). Out of 37 
competing systems, COWAL was rated the first, 
MEBA the 20th and TREQ-AL (Tufi? et al 
2003), the former version of YAWA, was rated 
the 21st. The usefulness of the aligner combina-
tion was convincingly demonstrated. 
Meanwhile, both the individual aligners and 
their combination were significantly improved. 
COWAL is now embedded into a larger platform 
that incorporates several tools for bitexts pre-
processing (briefly reviewed in section 2), a 
graphical interface that allows for comparing and 
editing different alignments, as well as a word 
sense disambiguation module.  
2 The bitext processing  
The two base aligners and their combination use 
the same format for the input data and provide 
the alignments in the same format. The input 
format is obtained from two raw texts that repre-
sent reciprocal translations. If not already sen-
tence aligned, the two texts are aligned by our 
sentence aligner that builds on Moore?s aligner 
(Moore, 2002) but which unlike it, is able to re-
cover the non-one-to-one sentence alignments. 
The texts in each language are then tokenized, 
tagged and lemmatized by the TTL module (Ion, 
2006). More often than not, the translation 
equivalents have the same part-of speech, but 
relying on such a restriction would seriously af-
fect the alignment recall. However, when the 
translation equivalents have different parts of 
speech, this difference is not arbitrary.  During 
the training phase, we estimated POS affinities: 
{p(POSmRO|POSnEN)} and p(POSnEN|POSmRO)}
and used them to filter out improbable translation 
equivalents candidates.
The next pre-processing step is represented by 
sentence chunking in both languages. The 
chunks are recognized by a set of regular expres-
sions defined over the tagsets and they corre-
spond to (non-recursive) noun phrases, adjectival 
phrases, prepositional phrases and verb com-
plexes (analytical realization of tense, aspect 
mood and diathesis and phrasal verbs). Finally, 
the bitext is assembled as an XML document 
(Tufi? and Ion, 2005), which is the standard input 
for most of our tools, including COWAL align-
ment platform. 
3 YAWA 
YAWA is a three stage lexical aligner that uses 
bilingual translation lexicons and phrase bounda-
ries detection to align words of a given bitext. 
The translation lexicons are generated by a dif-
ferent module, TREQ (Tufi?, 2002), which gen-
erates translation equivalence hypotheses for the 
pairs of words (one for each language in the par-
allel corpus) which have been observed occur-
ring in aligned sentences more than expected by 
chance. The hypotheses are filtered by a log-
likelihood score threshold. Several heuristics 
(string similarity-cognates, POS affinities and 
alignments locality2) are used in a competitive 
linking manner (Melamed, 2001) to extract the 
most likely translation equivalents. 
YAWA generates a bitext alignment by in-
crementally adding new links to those created at 
the end of the previous stage. The existing links 
act as contextual restrictors for the new added 
links. From one phase to the other new links are 
added without deleting anything. This monotonic 
process requires a very high precision (at the 
price of a modest recall) for the first step. The 
next two steps are responsible for significantly 
improving the recall and ensuring an increased 
F-measure.  
In the rest of this section we present the three 
stages of YAWA and evaluate the contribution 
of each of them to the final result. 
3.1 Phase 1: Content Words Alignment 
YAWA begins by taking into account only very 
probable links that represent the skeleton align-
ment used by the second phase. This alignment is 
done using outside resources such as translation 
lexicons and involves only the alignment of con-
tent words (nouns, verbs, adjective and adverbs). 
The translation equivalence pairs are ranked 
according to an association score (i.e. log-
likelihood, DICE, point-wise mutual informa-
2 The alignments locality heuristics exploits the observation 
made by several researchers that adjacent words of a text in 
the source language tend to align to adjacent words in the 
target language. A more strict alignment locality constraint 
requires that all alignment links starting from a chunk in the 
one language end in a chunk in the other language.
154
tion, etc.). We found that the best filtering of the 
translation equivalents was the one based on the
log-likelihood (LL) score with a threshold of 9.
Each translation unit (pair of aligned sen-
tences) of the target bitext is scanned for estab-
lishing the most likely links based on a competi-
tive linking strategy that takes into account the
LL association scores given by the TREQ trans-
lation lexicon. If a candidate pair of words is not 
found in the translation lexicon, we compute
their orthographic similarity (cognate score 
(Tufi?, 2002)). If this score is above a predeter-
mined threshold (for Romanian-English task we 
used the empirically found value of 0.43), the 
two words are treated as if they existed in the
translation lexicon with a high association score 
(in practice we have multiplied the cognate score
by 100 to yield association scores in the range 0
.. 100). The Figure 1 exemplifies the links cre-
ated between two tokens of a parallel sentence by
the end of the first phase.
Figure 1: Alignment after the first step 
3.2 Phase 2: Chunks Alignment 
The second phase requires that each part of the
bitext is chunked. In our Romanian-English ex-
periments, this requirement was fulfilled by us-
ing a set of regular expressions defined over the
tagsets used in the target bitext. These simple
chunkers recognize noun phrases, prepositional
phrases, verbal and adjectival or adverbial group-
ings of both languages.
In this second phase YAWA produces first
chunk-to-chunk matching and then aligns the 
words within the aligned chunks. Chunk align-
ment is done on the basis of the skeleton align-
ment produced in the first phase. The algorithm
is simple: align two chunks c(i) in source lan-
guage and c(j) in the target language if c(i) and 
c(j) have the same type (noun phrase, preposi-
tional phrase, verb phrase, adjectival/adverbial 
phrase) and if there exist a link ?w(s), w(t)? so 
that w(s) ? c(i) then w(t) ? c(j).
After alignment of the chunks, a language pair 
dependent module takes over to align the un-
aligned words belonging to the chunks. Our 
module for the Romanian-English pair of lan-
guages contains some very simple empirical
rules such as: if b is aligned to c and b is pre-
ceded by a, link a to c, unless there exist d in the 
same chunk with c and the POS category of d has 
a significant affinity with the category of a. The 
simplicity of these rules derives from the shallow
structures of the chunks. In the above example b
and c are content words while a is very likely a 
determiner or a modifier for b. The result of the 
second alignment phase, considering the same
sentence in Figure 1, is shown in Figure 2. The
new links are represented by the double lines. 
 Figure 2: Alignment after the second step 
3.3 Phase 3: Dealing with sequences of un-
aligned words
This phase identifies contiguous sequences of
words (blocks) in each part of the bitext which
remain unaligned and attempts to heuristically
match them. The main criteria used to this end
are the POS-affinities of the remaining unaligned 
words and their relative positions. Let us illus-
trate, using the same example and the result 
shown in Figure 2, how new links are added in
this last phase of the alignment. At the end of 
phase 2 the blocks of consecutive words that re-
main to be aligned are: English {en0 = (you), en1
= (that), en2 = (is, not), en3 = (and), en4 = (.)} and 
155
Romanian {ro0 = (), ro1 = (c?), ro2 = (nu, e), ro3 = 
(?i), ro4 = (.)}. The mapping of source and target
unaligned blocks depends on two conditions: that 
surrounding chunks are already aligned and that
pairs in candidate unaligned blocks have signifi-
cant POS-affinity. For instance in the figure
above, blocks en1 = (that) and ro1 = (c?) satisfy
the above conditions because they appear among
already aligned chunks (<?ll notice> ? <ve?i
observa> and <D?ncu ?s generosity> ? <gene- 
rozitatea lui D?ncu>) and they contain words 
with the same POS.
After block alignment3, given a pair of aligned
blocks, the algorithm links words with the same
POS and then the phase 2 is called again with 
these new links as the skeleton alignment. In 
Figure 3 is shown the result of phase 3 alignment
of the sentence we used as an example through-
out this section. The new links are shown (as
before) by double lines. 
Figure 3: Alignment after the third step 
The third phase is responsible for significant
improvement of the alignment recall, but it also 
generates several wrong links. The detection of
some of them is quite straightforward, and we
added an additional correction phase 3.f. By ana-
lysing the bilingual training data we noticed the trans-
lators? tendency to preserve the order of the 
phrasal groups. We used this finding (which 
might not be valid for any language pair) as a 
removal heuristics for the links that cross two or
more aligned phrase groups.  One should notice 
that the first word in the English side of the ex-
ample in Figure 3 (?you?) remained unaligned
(interpreted as not translated in the Romanian
side). According to the Gold Standard used for 
3 Only 1:1 links are generated between blocks. 
evaluation in the ACL2005 shared task, this in-
terpretation was correct, and therefore, for the
example in Figure 3, the F-measure for the 
YAWA alignment was 100%.
However, Romanian is a pro-drop language 
and although the translation of the English pro-
noun is not lexicalized in Romanian, one could 
argue that the auxiliary ?ve?i? should be aligned 
also to the pronoun ?you? as it incorporates the
grammatical information carried by the pronoun. 
Actually, MEBA (as exemplified in Figure 4)
produced this multiple token alignment (and was 
penalized for it!). 
3.4 Performance analysis
The table that follows presents the results of the
YAWA aligner at the end of each alignment
phase. Although the Precision decreases from
one phase to the next one, the Recall gains are 
significantly higher, so the F-measure is mono-
tonically increasing.
Precision Recall F-Measure
Phase 1 94.08% 34.99% 51.00%
Phase 1+2 89.90% 53.90% 67.40%
Phase 1+2+3 88.82% 73.44% 80.40%
Phase 1+2+3+3.f 88.80% 74.83% 81.22%
Table 1: YAWA evaluation 
4 MEBA 
MEBA uses an iterative algorithm that takes ad-
vantage of all pre-processing phases mentioned
in section 2. Similar to YAWA aligner, MEBA 
generates the links step by step, beginning with 
the most probable (anchor links). The links to be 
added at any later step are supported or restricted 
by the links created in the previous iterations. 
The aligner has different weights and different
significance thresholds on each feature and itera-
tion. Each of the iterations can be configured to
align different categories of tokens (named enti-
ties, dates and numbers, content words, func-
tional words, punctuation) in decreasing order of 
statistical evidence. 
The first iteration builds anchor links with a 
high level of certainty (that is cognates, numbers,
dates, pairs with high translation probability).
The next iteration tries to align content words
(open class categories) in the immediate vicinity
of the anchor links. In all steps, the candidates
are considered if and only if they meet the mini-
mal threshold restrictions.
A link between two tokens is characterized by
a set of features (with values in the [0,1] inter-
val). We differentiate between context independ-
156
ent features that refer only to the tokens of the
current link (translation equivalency, part-of-
speech affinity, cognates, etc.) and context de-
pendent features that refer to the properties of the 
current link with respect to the rest of links in a 
bi-text (locality, number of traversed links, to-
kens indexes displacement, collocation). Also, 
we distinguish between bi-directional features
(translation equivalence, part-of-speech affinity)
and non-directional features (cognates, locality,
number of traversed links, collocation, indexes 
displacement).
Precision Recall F-measure
?Anchor? links 98.50% 26.82% 42.16%
Words around 
?anchors? 96.78% 42.41% 58.97%
Funct. words 
and punctuation 94.74% 59.48% 73.08%
Probable links 92.05% 71.00% 80.17%
Table 2: MEBA evaluation 
The score of a candidate link (LS) between a 
source token i and a target token j is computed
by a linear function of several features scores
(Tiedemann, 2003).
?
 
 
n
i
ii ScoreFeatjiLS
1
*),( O ; 1
1
 ?
 
n
i
iO
Each feature has defined a specific signifi-
cance threshold, and if the feature?s value is be-
low this threshold, the contribution to the LS of 
the current link of the feature in case is nil. 
The thresholds of the features and lambdas are 
different from one iteration to the others and they
are set by the user during the training and system
fine-tuning phases. There is also a general 
threshold for the link scores and only the links 
that have the LS above this threshold are retained
in the bitext alignment. Given that this condition 
is not imposing unique source or target indexes,
the resulting alignment is inherently many-to-
many.
In the following subsections we briefly discuss
the main features we use in characterising a link.
4.1 Translation equivalence
This feature may be used for two types of pre-
processed data: lemmatized or non-lemmatized
input. Depending on the input format, MEBA
invokes GIZA++ to build translation probability
lists for either lemmas or the occurrence forms of 
the bitext4. Irrespective of the lemmatisation op-
tion, the considered token for the translation 
model build by GIZA++ is the respective lexical 
item (lemma or wordform) trailed by its POS tag 
(eg. plane_N, plane_V, plane_A). In this way we 
avoid data sparseness and filter noisy data. For 
instance, in case of highly inflectional languages 
(as Romanian is) the use of lemmas significantly
reduces the data sparseness. For languages with
weak inflectional character (as English is) the 
POS trailing contributes especially to the filter-
ing the search space. A further way of removing
the noise created by GIZA++ is to filter out all 
the translation pairs below a LL-threshold. We 
made various experiments and, based on the es-
timated ratio between the number of false nega-
tives and false positive, empirically set the value
of this threshold to 6. All the probability losses 
by this filtering were redistributed proportionally
to their initial probabilities to the surviving trans-
lation equivalence candidates. 
4.2 Translation equivalence entropy score 
The translation equivalence relation is a se-
mantic one and it directly addresses the notion of 
word sense. One of the Zipffian laws prescribes a 
skewed distribution of the senses of a word oc-
curring several times in a coherent text. We used
this conjecture as a highly informative informa-
tion source for the validity of a candidate link.
The translation equivalence entropy score is a 
favouring parameter for the words that have few 
high probability translations. Since this feature is
definitely sensitive to the order of the lexical 
items, we compute an average value for the link: 
DES(A)+EES(B). Currently we use D=E=0.5, but 
it might be interesting to see, depending on dif-
ferent language pairs, how the performance of 
the aligner would be affected by a different set-
tings of these parameters.
N
TRWpTRWp
N
i
ii
WES log
),(log*),(
11)(
?
 

 
4.3 Part-of-speech affinity
In faithful translations the translated words tend
to be translated by words of the same part-of-
speech. When this is not the case, the different 
POSes, are not arbitrary. The part of speech af-
finity, P(cat(A)|cat(B), can be easily computed
from a gold standard alignment. Obviously, this
4 Actually, this is a user-set parameter of the MEBA aligner;
if the input bitext contain lemmatization information, both 
translation probability tables may be requested. 
157
is a directional feature, so an averaging operation 
is necessary in order to ascribe this feature to a 
link: PA=DP(cat(A)|cat(B)) + EP(cat(B)|cat(A)).
Again, we used D=E=0.5 but different values of 
these weights might be worthwhile investigating. 
4.4 Cognates 
The similarity measure, COGN(TS, TT), is im-
plemented as a Levenstein metric. Using the
COGN test as a filtering device is a heuristic 
based on the cognate conjecture, which says that 
when the two tokens of a translation pair are 
orthographically similar, they are very likely to
have similar meanings (i.e. they are cognates). 
The threshold for the COGN(TS, TT) test was 
empirically set to 0.42. This value depends on 
the pair of languages in the bitext. The actual 
implementation of the COGN test includes a lan-
guage-dependent normalisation step, which strips 
some suffixes, discards the diacritics, reduces 
some consonant doubling, etc. This normalisa-
tion step was hand written, but, based on avail-
able lists of cognates, it could be automatically
induced.
4.5 Obliqueness 
Each token in both sides of a bi-text is character-
ized by a position index, computed as the ratio 
between the relative position in the sentence and 
the length of the sentence. The absolute value of 
the difference between tokens? position indexes,
subtracted from 15, gives the link?s ?oblique-
ness?.
)()(
1),(
TS
ji Sentlength
j
Sentlength
iTWSWOBL  
This feature is ?context free? as opposed to the 
locality feature described below.
4.6 Locality 
Locality is a feature that estimates the degree to 
which the links are sticking together. 
MEBA has three features to account for local-
ity: (i) weak locality, (ii) chunk-based locality
and (iii) dependency-based locality.
The value of the weak locality feature is de-
rived from the already existing alignments in a 
window of N tokens centred on the focused to-
ken. The window size is variable, proportional to 
the sentence length. If in the window there exist
k linked tokens and the relative positions of the 
5 This is to ensure that values close to 1 are ?good? ones and 
those near 0 are ?bad?. This definition takes into account the
relatively similar word order in English and Romanian.
tokens in these links are <i1 j1>, ?<ik jk> then 
the locality feature of the new link <ik+1, jk+1> is 
defined by the equation below: 
)
||
||1,1min(
1 1
1?
 


 
k
m mk
mk
jj
ii
k
LOC
If the new link starts from or ends in a token 
already linked, the index difference that would
be null in the formula above is set to 1. This way,
such candidate links would be given support by
the LOC feature (and avoid overflow error). In 
the case of chunk-based locality the window 
span is given by the indexes of the first and last 
tokens of the chunk. 
Dependency-based locality uses the set of the 
dependency links of the tokens in a candidate
link for the computation of the feature value. In
this case, the LOC feature of a candidate link
<ik+1, jk+1> is set to 1 or 0 according to the fol-
lowing rule: 
if between ik+1 and iD there is a (source lan-
guage) dependency and if between jk+1 and jE
there is also a (target language) dependency then 
LOC is 1 if iD and jE are aligned, and 0 otherwise. 
Please note that in case jk+1{ jE a trivial depend-
ency (identity) is considered and the LOC attrib-
ute of the link <ik+1, jk+1> is set to always to 1.
Figure 4: Chunk and dependency-based locality
4.7 Collocation 
Monolingual collocation is an important clue for 
word alignment. If a source collocation is trans-
lated by a multiword sequence, very often the
lexical cohesion of source words can also be
found in the corresponding translated words. In 
this case the aligner has strong evidence for 
158
many to many linking. When a source colloca-
tion is translated as a single word, this feature is
a strong indication for a many to 1 linking.
Bi-gram lists (only content words) were built 
from each monolingual part of the training cor-
pus, using the log-likelihood score (threshold of 
10) and minimal occurrence frequency (3) for
candidates filtering.
We used the bi-grams list to annotate the 
chains of lexical dependencies among the con-
tents words. Then, the value of the collocation 
feature is computed similar to the dependency-
based locality feature. The algorithm searches for
the links of the lexical dependencies around the 
candidate link. 
5 Combining the reified alignments 
From a given alignment one can compute a se-
ries of properties for each of its links (such as the 
parameters used by the MEBA aligner). A link
becomes this way a structured object that can be 
manipulated in various ways, independent of the
bitext (or even of the lexical tokens of the link)
from which it was extracted. We call this proce-
dure alignment reification. The properties of the 
links of two or more alignments are used for our 
methods of combining the alignments.
One simple, but very effective method of
alignment combination is a heuristic procedure, 
which merges the alignments produced by two or
more word aligners and filters out the links that 
are likely to be wrong. For the purpose of filter-
ing, a link is characterized by its type defined by
the pair of indexes (i,j) and the POS of the tokens
of the respective link. The likelihood of a link is 
proportional to the POS affinities of the tokens of
the link and inverse proportional to the bounded
relative positions (BRP) of the respective tokens:
  where avg is the average
displacement in a Gold Standard of the aligned 
tokens with the same POSes as the tokens of the 
current link. From the same gold standard we 
estimated a threshold below which a link is re-
moved from the final alignment.
||||1 avgjiBRP  
A more elaborated alignment combination
(with better results than the previous one) is 
modelled as a binary statistical classification 
problem (good / bad) and, as in the case of the 
previous method, the net result is the removal of 
the links which are likely to be wrong. We used
an ?off-the-shelf? solution for SVM training and 
classification - LIBSVM6 (Fan et al, 2005) with 
6 http://www.csie.ntu.edu.tw/~cjlin/libsvm/
the default parameters (C-SVC classification and
radial basis kernel function). Both context inde-
pendent and context dependent features charac-
terizing the links were used for training. The
classifier was trained with both positive and 
negative examples of links. A set of links ex-
tracted from the Gold Standard alignment was
used as positive examples set. The same number
of negative examples was extracted from the
alignments produced by COWAL and MEBA 
where they differ from the Gold Standard.
It is interesting to notice that for the example
discussed in Figures 1-4, the first combiner
didn?t eliminate the link <you ve?i> producing 
the result shown in Figure 4. This is because the 
relative positions of the two words are the same
and the POS-affinity of the English personal
pronouns and the Romanian auxiliaries is signifi-
cant. On the other hand, the SVM-based com-
biner deleted this link, producing the result
shown in Figure 3. The explanation is that, ac-
cording to the Gold Standard we used, the links 
between English pronouns and Romanian auxil-
iaries or main verbs in pro-drop constructions
were systematically dismissed (although we 
claim that they shouldn?t and that the alignment
in Figure 4 is better than the one in Figure 3).
The evaluation (according to the Gold Standard)
of the SVM-based combination (COWAL),
compared with the individual aligners, is shown
in Table 3. 
Aligner Precision Recall F-measure
YAWA 88.80% 74.83% 81.22%
MEBA 92.05% 71.00% 80.17%
COWAL 86.99% 79.91% 83.30%
Table 3: Combined alignment
6 Conclusions and further work
Neither YAWA nor MEBA needs an a priori bi-
lingual dictionary, as this will be automatically
extracted by TREQ or GIZA++. We made
evaluation of the individual alignments in both
experimental settings: without a start-up bilin-
gual lexicon and with an initial mid-sized bilin-
gual lexicon. Surprisingly enough, we found that
while the performance of YAWA increases a
little bit (approx. 1% increase of the F-measure)
MEBA is doing better without an additional lexi-
con. Therefore, in the evaluation presented in the 
previous section MEBA uses only the training
data vocabulary.
YAWA is very sensitive to the quality of the
bilingual lexicons it uses. We used automatically
translation lexicons (with or without a seed lexi-
159
con), and the noise inherently present might have 
had a bad influence on YAWA?s precision. Re-
placing the TREQ-generated bilingual lexicons 
with validated (reference bilingual lexicons) 
would further improve the overall performance 
of this aligner.  Yet, this might be a harder to 
meet condition for some pairs of languages than 
using parallel corpora. 
MEBA is more versatile as it does not require 
a-priori bilingual lexicons but, on the other hand, 
it is very sensitive to the values of the parameters 
that control its behaviour. Currently they are set 
according to the developers? intuition and after 
the analysis of the results from several trials. 
Since this activity is pretty time consuming (hu-
man analysis plus re-training might take a couple 
of hours) we plan to extend MEBA with a super-
vised learning module, which would automati-
cally determine the ?optimal? parameters 
(thresholds and weights) values. 
It is worth noticing that with the current ver-
sions of our basic aligners, significantly im-
proved since the ACL shared word alignment 
task in June 2005, YAWA is now doing better 
than MEBA, and the COWAL F-measure in-
creased with 9.4%. However, as mentioned be-
fore, these performances were measured on a 
different tokenization of the evaluation texts and 
on the partially corrected gold standard align-
ment (see footnote 1).  
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert J. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter 
estimation. Computational Linguistics, 19(2): 263?
311.
Thomas G. Dietterich. 1998. Approximate Statistical 
Tests for Comparing Supervised Classification 
Learning Algorithms. Neural Computation, 10 (7) 
1895-1924.
Rong-en Fan, Pai-Hsuen Chen, Chij-Jen Lin. 2005. 
Working set selection using the second order 
information for training SVM. Technical report, 
Department of Computer Science, National Taiwan 
University (www.csie.ntu.edu.tw/~cjlin/papers/
quadworkset.pdf).
William A. Gale, Kenneth W. Church. 1991. Identify-
ing word correspondences in parallel texts. In Pro-
ceedings of the Fourth DARPA Workshop on 
Speech and Natural Language. Asilomar, CA:152?
157.
Radu Ion. 2006. TTL: A portable framework for to-
kenization, tagging and lemmatization of large cor-
pora. PhD thesis progress report. Research Institute 
for Artificial Intelligence, Romanian Academy, 
Bucharest (in Romanian), 22p. 
Dan Melamed. 2001. Empirical Methods for Exploit-
ing Parallel Texts. Cambridge, MA, MIT Press. 
Rada Mihalcea, Ted Pedersen. 2003. An Evaluation 
Exercise for Word Alignment. Proceedings of the 
HLT-NAACL 2003 Workshop: Building and Using 
Parallel Texts Data Driven Machine Translation 
and Beyond. Edmonton, Canada: 1?10. 
Joel Martin, Rada Mihalcea, Ted Pedersen. 2005. 
Word Alignment for Languages with Scarce Re-
sources. In Proceeding of the ACL2005 Workshop 
on ?Building and Using Parallel Corpora: Data-
driven Machine Translation and Beyond?. June,
2005, Ann Arbor, Michigan, June, Association for 
Computational Linguistics, 65?74 
Robert Moore. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora in Machine Trans-
lation: From Research to Real Users. In Proceed-
ings of the 5th Conference of the Association for 
Machine Translation in the Americas, Tiburon, 
California), Springer-Verlag, Heidelberg, Ger-
many: 135-244. 
Franz J. Och, Herman Ney. 2003. A Systematic Com-
parison of Various Statistical Alignment Models, 
Computational Linguistics, 29(1):19-51. 
Franz J. Och, Herman Ney. 2000. Improved Statistical 
Alignment Models. In Proceedings of the 38th Con-
ference of ACL, Hong Kong: 440-447. 
Joerg Tiedemann. 2003. Combining clues for word 
alignment. In Proceedings of the 10th EACL, Bu-
dapest, Hungary: 339?346. 
Dan Tufi?. 2002. A cheap and fast way to build useful 
translation lexicons. In Proceedings of COL-
ING2002, Taipei, China: 1030-1036. 
Dan Tufi?, Ana-Maria Barbu, Radu Ion. 2003. TREQ-
AL: A word-alignment system with limited lan-
guage resources. In Proceedings of the NAACL 
2003 Workshop on Building and Using Parallel 
Texts; Romanian-English Shared Task, Edmonton, 
Canada: 36-39. 
Dan Tufi?, Radu Ion, Alexandru Ceau?u, Dan Ste-
f?nescu. 2005. Combined Aligners. In Proceeding
of the ACL2005 Workshop on ?Building and Using 
Parallel Corpora: Data-driven Machine Transla-
tion and Beyond?. June, 2005, Ann Arbor, Michi-
gan, June, Association for Computational Linguis-
tics, pp. 107-110. 
Dan Tufi?, Radu Ion. 2005. Multiple Sense Invento-
ries and Test-Bed Corpora. In C. Burileanu (ed.) 
Trends in Speech Technology, Publishing House of 
the Romanian Academy, Bucharest: 49-58.
160
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 107?110,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
 
Combined word alignments  
 
Dan Tufi?, Radu Ion, Alexandru Ceau?u, Dan ?tef?nescu 
Romanian Academy Institute for Artificial Intelligence 
13, ?13 Septembrie?, 74311, Bucharest 5, Romania 
{tufis, radu, alceusu, danstef}@racai.ro 
 
 
Abstract
 
We briefly describe a word alignment system 
that combines two different methods in bitext 
correspondences identification. The first one is 
a hypotheses testing approach (Gale and 
Church, 1991; Melamed, 2001; Tufi? 2002) 
while the second one is closer to a model 
estimating approach (Brown et al, 1993; Och 
and Ney, 2000). We show that combining the 
two aligners the results are significantly 
improved as compared to each individual 
aligner. 
 
Introduction 
In (Tufi?, 2002) we described a translation equivalence 
extraction program called TREQ the development of 
which was twofold motivated: to help enriching the 
synsets of the Romanian wordnet (Tufi? et al 2004a) 
with new literals based on bilingual corpora evidence 
and to check the interlingual alignment of our wordnet 
against the Princeton Wordnet. The translation 
equivalence extractor has been also incorporated into a 
WSD system (Tufi? et al, 2004b) part of a semantic 
web annotation platform. It also constituted the 
backbone of our TREQ-AL word aligner which 
successfully participated in the previous HLT-NAACL 
2003 Shared Task1 on word alignment for Romanian-
English parallel texts. A detailed description of 
TREQ&TREQ-AL is given in (Tufi? et al 2003b) and it 
will be very shortly overviewed. 
A quite different approach from our hypotheses 
testing implemented in the TREQ-AL aligner is taken 
by the model-estimating aligners, most of them relying 
on the IBM models (1 to 5) described in the (Brown et 
al. 1993) seminal paper. The first wide-spread and 
publicly available implementation of the IBM models 
was the GIZA program, which itself was part of the 
SMT toolkit EGYPT (Al-Onaizan et al, 1999). GIZA 
has been superseded by its recent extension GIZA++ 
(Och and Ney, 2000, 2003) publicly available2. We used 
the translation probabilities generated by GIZA++ for 
implementing a second aligner, MEBA, described in a 
                                                 
1 http://www.cs.unt.edu/~rada/wpt/index.html#shared  
2 http://www.fjoch.com/GIZA++.2003-09-30.tar.gz 
little more details in a subsequent section. The 
alignments produced by MEBA were compared to the 
ones produced by TREQ-AL. We used for comparison 
the Gold Standard3 annotation from the HLT-NAACL 
2003 Shared Task. In order to combine the two aligners 
we had to check whether their accuracy was comparable 
and that when they are wrong the set of mistakes made 
by one aligner is not a proper set of the errors made by 
the second one. The first check was performed by using 
McNamer?s test  (Dieterich, 1998) and for the second 
we used Brill &Wu test (Brill, Wu, 1998). Both tests 
confirmed that the conditions for combining were 
ensured so, we built the combiner.  
The Combined Word Aligner, COWAL, is a 
wrapper of the two aligners (TREQ-AL and MEBA) 
ensuring the pre- and post-processing. It is 
complemented by a graphical user interface that allows 
for the visualisation of the alignments (intermediary and 
the final ones) as well as for their editing. We should 
note that the corrections made by the user are stored by 
COWAL as positive and negative examples for word 
dependencies (in the monolingual context) and 
translation equivalencies (in the bilingual context). In 
the current version the editorial logs are used by the 
human developers but we plan to further extend 
COWAL for automatic learning from this extremely 
valuable kind of data.    
 
The bitext processing  
The two base aligners and their combination use the 
same format for the input data and provide the 
alignments in the same format. The input format is 
obtained from two raw texts which represent reciprocal 
translations. If not already sentence aligned, the two 
texts are aligned. In the shared task this step was not 
necessary since both the training data and evaluation 
data were provided in the sentence aligned format.  
The texts in each language are then tokenized with 
the MULTEXT multilingual tokenizer4. The tokenizer is 
a finite state automaton using language specific 
                                                 
3 We noticed in the Gold Standard two sentences where 
alignments were wrongly shifted by one position (due to an 
unprintable character) and we corrected them.  
4 http://aune.lpl.univ-aix.fr:16080/projects/multext/MtSeg/  
107
 resources. It recognizes several compounds (phrasal 
verbs, idioms, dates) and split contrasted or cliticized 
constructions. This tokenization considerably differs 
from the one prescribed by the Shared Task where a 
token is any character string delimited by a blank or a 
punctuation sign (which itself is considered a token).   
Since our processing tools (especially the tokeniser) 
were built with a different segmentation strategy in 
mind, we generated the alignments based on our own 
tokenization and, at the end, we ?re-tokenised? the text 
according to original evaluation data (and consequently 
re-index) all the linking pairs. After tokenization, both 
texts are tagged and lemmatized.  We used in-house 
language models and lemmatizers and the Brants?s TnT 
tagger5. For both English and Romanian we used 
MULTEXT-EAST6 compliant tagsets. With different 
tags, a tagset mapping table becomes an obligatory 
external resource. Although, more often than not, the 
translation equivalents have the same part-of speech, 
relying on such a restriction would seriously affect the 
alignment recall. However, when the translation 
equivalents have different parts of speech, this 
difference is not arbitrary.  During the training phase we 
estimated bilingual POS affinities:{p(POSmRO| POSnEN)} 
and {p(POSnEN|POSmRO)}. POS affinities were used as 
one of the information sources in dealing with 
competitive alignments.  
The next preprocessing step is represented by a 
rather primitive form of sentence chunking in both 
languages. They roughly correspond to (non-recursive) 
noun phrases, adjectival phrases, prepositional phrases 
and verb complexes (analytical realization of tense, 
aspect mood and diathesis and phrasal verbs).  The 
?chunks? are recognized by a set of regular expressions 
defined over the tagsets. Finally, the bitext is assembled 
as an XML document (XCES-Align-ana format), as 
used in the MULTEXT-EAST corpus, which is the 
standard input for most of our tools, including COWAL 
alignment platform. 
 
The three aligners  
TREQ-AL generates translation equivalence hypotheses 
for the pairs of words (one for each language in the 
parallel corpus) which have been observed occurring in 
aligned sentences more than expected by chance. The 
hypotheses are filtered by a loglikelihood score 
threshold. Several heuristics (string similarity-cognates, 
POS affinities and alignments locality7) are used in a 
                                                 
5 http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf  
6 http://nl.ijs.si/ME/V2/  
7 The alignments locality heuristics exploits the observation 
made by several researchers that adjacent words of a text in 
the source language tend to align to adjacent words in the 
target language. A more strict alignment locality constraint 
competitive linking manner (Melamed, 2001) to make 
the final decision on the most likely translation 
equivalents. Given that, initially, this program was 
designed for extracting translation equivalents for the 
alignment of the Romanian wordnet to the Princeton 
wordnet, it deals only with one to one mappings. To 
cope with the many to many mappings (especially for 
functional words alignment), the earlier version of the 
translation equivalence extractor encoded some general 
rules assumed to be valid over a large set of natural 
languages such as: auxiliaries and verbal particles 
(infinitive, subjunctive, aspectual and temporal) are 
related to the closest main verb, determiners (articles, 
pronominal adjectives, quantifiers) are related to the 
closest nominal category (noun or pronoun). Currently 
this part of the TREQ-AL code became redundant 
because the chunking module mentioned before does 
the same job in a more general and flexible way.  
MEBA is an iterative algorithm which uses the 
translation probabilities, distorsions and POS-affinities 
generated by GIZA++ and takes advantage of all 
preprocessing phases mentioned in the previous section. 
In each step are aligned different categories of tokens 
(content words, named entities, functional words) in 
decreasing order of statistical evidence. The score of a 
link is computed by a linear function of 7 parameters? 
scores: translation probability, POS affinity, string 
similarity, alignments locality (both strict and weaker 
versions) distortions and the entropy of the translation 
equivalents. For all these parameters, in each processing 
step, we empirically set minimal thresholds and various 
weights. The tokens considered for the computing 
translation probabilities are the lemmas trailed by the 
grammatical categories (eg. plane_N, plane_V 
plane_A). This way we aimed at avoiding data 
sparseness and filtering noisy data. For highly 
inflectional languages (as Romanian is) the use of 
lemmas instead of word occurrences contributes 
significantly to the data sparseness reduction. For 
languages with weak inflectional character (as English 
is) the POS trailing contributes especially to the filtering 
the search space. Each processing step is controlled by 
above mentioned parameters, the weights and thresholds 
of which vary from step to step (even the order of the 
processing steps is one of the possible parameters). 
The first alignment step builds only links with a 
high level of certainty (that is cognates, pairs of high 
translation probability and high POS affinity). The 
grammatical categories which are considered in this step 
are user controlled (usually nouns, adjectives or non-
auxiliary verbs and which have the fewest competitive 
translations). The next processing steps try to align 
                                                                             
requires that all alignment links starting from a chunk, in the 
one language end in a chunk in the other language. This 
restricted form of locality is relevant for related languages.  
108
 content words (open class categories) as confidently as 
possible, following the alignments in previous steps as 
anchor points. In all steps the candidates are considered 
if and only if they meet the minimal threshold 
restrictions. If the input bitext is chunked, the strict 
alignment locality heuristics is very effective to 
determine the correct alignment even for unseen pairs of 
words (or for which the translation equivalence 
probability is below the considered threshold). When 
the pre-chunking of the parallel texts is not available, 
MEBA uses the weaker form of the locality heuristics 
by analyzing the alignments already existing in a 
window of N tokens centered on the focused token. The 
window size is variable, proportional to the sentence 
length. For all alignments in the window, an average 
displacement is computed and, among the competing 
alignments, preference will be given to the links with 
displacement values closer to the average one.  
The functional words and punctuation are processed 
in the last step and their alignments are guided by the 
POS-affinities and alignment locality heuristics. If none 
of the alignment clues or their combination (Tiedemann, 
2003) is strong enough, the functional words are 
automatically aligned with the word(s) their governor is 
aligned to. The governor is chunk-based defined: it is 
the content word of a chunk (if there are more content 
words in a chunk, then the governor is the grammatical 
head). If the chunking is not available, the closest 
content word is selected as the governor. Proximity is 
checked to the left or to the right according to the 
frequencies of the POS-ngram containing the current 
functional word.  
We should mention that the probabilities computed 
during the training phase are not re-estimated for each 
run-time processing step. At run-time only the weights 
and thresholds change from step to step.  
COWAL, the combined aligner takes advantage of the 
alignments independently provided by TREQ-AL and 
MEBA. The simplest combination method consists in 
computing either the union (high recall, low precision), 
or the intersection (lower recall, higher precision) of the 
independent alignments. We evaluated both these 
simple methods of combination and found that the best 
F-measure was provided by the union-based 
combination. Although for the shared task we submitted 
the union-based combined alignment (Baseline 
COWAL, see Table 1), there are various ways to 
improve it. We discuss three cases where improvement 
is possible (C1, C2 and C3, see below) and which were 
evaluated after the submission deadline. The results of 
this (unofficial) evaluation are summarized in Table 1 
by the f-COWAL line. These cases refer to competing 
links that appeared after the union of the independent 
alignments. The conflicts resolution is based on the 
(weak) locality and distortion heuristics discussed 
before. The currently identified competing links are 
only those for which the following conditions apply: 
C1) if one aligner found for a word W a non-null 
alignment and the other aligner generated for the 
same word W a null link, then the baseline alignment 
contains an impossible situation: the token W is 
recorded both as translated and not-translated in the 
other language. The translation probabilities, POS 
affinity and the relative displacement of the tokens in 
the non-null candidates were the strongest decision 
criteria. We found that in about 60% of the cases the 
null alignments were mistaken. So, for the time being, 
we simply eliminated the null competing alignments 
(this should be addressed in a more principled way by 
the future version of the combiner).  
C2) long distant competing links; this case appears 
when one aligner found for the word Ws the link to 
the target word Wtm, the other aligner found for Ws 
the target Wtn, and the distance between Wtm and 
Wtn, is more than 3 words (in a future version this 
maximum distance will be a dynamic parameter, 
depending on the sentence length and the POS of 
Ws). 
C3) competing links to the same target(s) of a word 
occurring several times in the same sentence; 
consider, for example, the Romanian fragment:  
     ??la1 Neptun, la2 Orastie si la3 Afumati, ?   
     which in English is translated by the next segment: 
     ??in Neptun, Orastie and Afumati? 
In spite of the gold standard considering that all three 
occurrences of the preposition ?la? in Romanian (la1, 
la2 ,la3) are aligned to the same word in English (?in?), 
the filtering, in this case, licensed only the alignment 
?la1 <-> in?. We consider that this filtered alignment 
is correct, since omitting ?la2? and ?la3? does not alter 
the syntactic correctness of the Romanian text, and 
also because the insertion in the English fragment of 
the preposition ?in? before ?Orastie? and before 
?Afumati? wouldn?t alter the grammaticality of the 
English fragment. Since both repetitions and 
omissions are optional, we consider that only the first 
occurrence of the preposition (?la1?) is translated in 
English, while the others are omitted. 
Another possible improvement (not implemented yet) 
was revealed by observing that the final result contained 
several incomplete n-m (phrasal) alignments. It is likely 
that even an elementary n-gram analysis (both sides of 
the bitext) would bring valuable evidence for improving 
the phrasal alignments.  
 
Post-processing  
As said in the second section, our tokenization was 
different from the tokenization in the training and test 
data. To comply with the evaluation protocol, we had to 
re-tokenize the aligned text and re-compute the indexes 
109
 of the links. Some multi-word expressions recognized 
by the tokenizer as one token, such as dates (25 
ianuarie, 2001), compound prepositions (de la, p?n? 
la), conjunctions (pentru ca, de c?nd, p?n? c?nd) or 
adverbs (de jur ?mprejur, ?n fa?a) as well as the hyphen 
separated nominal compounds (mass-media, prim-
ministru) were split, their positions were re-indexed and 
the initial one link of a split compound was replaced 
with the set obtained by adding one link for each 
constituent of the compound to the target English word. 
The same hold for the other way around. Therefore if 
two multiword expressions were initially found to be 
translation equivalents (one alignment link) after the 
post-processing number of  generated links became 
N*M, where N represented the number of words in the 
first language compound and M the number of words in 
the second language compound.   
Evaluation and conclusions 
Neither TREQ-AL nor MEBA needs an a priori 
bilingual dictionary, as this will be automatically 
extracted by the TREQ or GIZA++. We made 
evaluation of the individual alignments in both 
experimental settings: without a startup bilingual 
lexicon and with an initial mid-sized bilingual lexicon. 
Surprisingly enough, we found that while the 
performance of TREQ-AL increases a little bit (approx. 
1% increase of the F-measure) MEBA is doing better 
without an additional lexicon. So, in the evaluation 
below MEBA uses only the training data vocabulary.  
 
Aligner Precision Recall F-
meas. 
AER 
TREQ-AL 81.71 60.57 69.57 30.43 
MEBA 82.85 60.41 69.87 30.13 
Baseline 
(union)COWAL 
70.84 76.67 73.64 26.36 
f-COWAL 
(H1+H2+H3) 
87.17 70.25 77.80 22.20 
         Table 1. Evaluation results against the official GS 
After the release of the official Gold Standard we 
noticed and corrected some obvious errors and also 
removed the controversial links of the type c) discussed 
in the previous section. The evaluations against this new 
?Gold Standard? showed, on average, 3.5% better 
figures (precision, recall, F-measure and AER) for the 
individual aligners, while for the combined classifiers, 
the performance scores were about 4% better. 
MEBA is very sensitive to the values of the 
parameters which control its behavior. Currently they 
are set according to the developers? intuition and after 
the analysis of the results from several trials. Since this 
activity is pretty time consuming (human analysis plus 
re-training might take a couple of hours) we plan to 
extend MEBA with a supervised learning module, 
which would automatically determine the ?optimal? 
parameters (thresholds and weights) values. 
 
References 
Al-Onaizan, Y., Curin, J., Jahr, M., Knight K., Lafferty, J., 
Melamed, D., Och, F. J., Purdy, D., Smith, N.A., 
Yarowsky, D. (1999) : Statistical Machine 
Translation, Final Report, JHU Workshop, 42 pages 
Brill, E., and Wu, J. (1998). ?Classifier Combination for 
Improved Lexical Disambiguation? In Proceedings of 
COLING-ACL?98  Montreal, Canada, 191-195 
Brown, P. F., Della Pietra, S.A.,  Della Pietra, V. J., 
Mercer, R. L.(1993) ?The mathematics of statistical 
machine translation: Parameter estimation?. 
Computational Linguistics, 19(2) pp. 263?311. 
Dietterich, T. G., (1998). ?Approximate Statistical Tests 
for Comparing Supervised Classification Learning 
Algorithms?. Neural Computation, 10 (7) 1895-1924. 
Gale, W.A. and Church, K.W. (1991). ?Identifying word 
correspondences in parallel texts?. Proceedings of the 
Fourth DARPA Workshop on Speech and Natural 
Language. Asilomar, CA, pp. 152?157. 
Melamed, D. (2001). Empirical Methods for Exploiting 
Parallel Texts. Cambridge, MA: MIT Press. 
Och, F.J., Ney, H. (2003) "A Systematic Comparison of 
Various Statistical Alignment Models", Computa-
tional Linguistics, 29(1), pp. 19-51 
Och, F.J., Ney, H.(2000) "Improved Statistical Alignment 
Models". Proceedings of the 38th ACL, Hongkong,  
pp. 440-447 
Tiedemann, J. (2003) ?Combining clues for word 
alignment?. In Proceedings of the 10th EACL, 
Budapest, pp. 339?346 
Tufi?, D.(2002) ?A cheap and fast way to build useful 
translation lexicons?. Proceedings of COLING2002, 
Taipei, pp. 1030-1036. 
Tufi?, D., Barbu, A.M., Ion R (2003).: ?TREQ-AL: A 
word-alignment system with limited language 
resources?, Proceedings of the NAACL 2003 
Workshop on Building and Using Parallel Texts; 
Romanian-English Shared Task, Edmonton, pp. 36-39 
Tufi?, D., Ion, R., Ide, N.(2004a): Fine-Grained Word 
Sense Disambiguation Based on Parallel Corpora, 
Word Alignment, Word Clustering and Aligned 
Wordnets. Proceedings of COLING2004, Geneva, pp. 
1312-1318 
Tufis, D., Barbu, E., Mititelu, V., Ion, R., Bozianu, 
L.(2004b): ?The Romanian Wordnet?.  In Romanian 
Journal on Information Science and Technology, Dan 
Tufi? (ed.) Special Issue on BalkaNet, Romanian 
Academy, 7(2-3), pp. 105-122.  
110
An Expectation Maximization Algorithm for Textual Unit Alignment 
Radu Ion 
Research Institute for AI 
Calea 13 Septembrie nr. 13 
Bucharest 050711, Romania 
radu@racai.ro 
Alexandru Ceau?u 
Dublin City University 
Glasnevin, Dublin 9, Ireland 
[address3] 
aceausu@computing.dcu.ie 
Elena Irimia 
Research Institute for AI 
Calea 13 Septembrie nr. 13 
Bucharest 050711, Romania 
elena@racai.ro 
 
 
Abstract 
The paper presents an Expectation Maximiza-
tion (EM) algorithm for automatic generation 
of parallel and quasi-parallel data from any 
degree of comparable corpora ranging from 
parallel to weakly comparable. Specifically, 
we address the problem of extracting related 
textual units (documents, paragraphs or sen-
tences) relying on the hypothesis that, in a 
given corpus, certain pairs of translation 
equivalents are better indicators of a correct 
textual unit correspondence than other pairs of 
translation equivalents. We evaluate our 
method on mixed types of bilingual compara-
ble corpora in six language pairs, obtaining 
state of the art accuracy figures. 
1 Introduction 
Statistical Machine Translation (SMT) is in a con-
stant need of good quality training data both for 
translation models and for the language models. 
Regarding the latter, monolingual corpora is evi-
dently easier to collect than parallel corpora and 
the truth of this statement is even more obvious 
when it comes to pairs of languages other than 
those both widely spoken and computationally 
well-treated around the world such as English, 
Spanish, French or German. 
Comparable corpora came as a possible solu-
tion to the problem of scarcity of parallel corpora 
with the promise that it may serve as a seed for 
parallel data extraction. A general definition of 
comparability that we find operational is given by 
Munteanu and Marcu (2005). They say that a (bi-
lingual) comparable corpus is a set of paired doc-
uments that, while not parallel in the strict sense, 
are related and convey overlapping information.  
Current practices of automatically collecting 
domain-dependent bilingual comparable corpora 
from the Web usually begin with collecting a list 
of t terms as seed data in both the source and the 
target languages. Each term (in each language) is 
then queried on the most popular search engine and 
the first N document hits are retained. The final 
corpus will contain t ? N documents in each lan-
guage and in subsequent usage the document 
boundaries are often disregarded. 
At this point, it is important to stress out the 
importance of the pairing of documents in a com-
parable corpus. Suppose that we want to word-
align a bilingual comparable corpus consisting of 
M documents per language, each with k words, 
using the IBM-1 word alignment algorithm (Brown 
et al, 1993). This algorithm searches for each 
source word, the target words that have a maxi-
mum translation probability with the source word. 
Aligning all the words in our corpus with no regard 
to document boundaries, would yield a time com-
plexity of      operations. The alternative would 
be in finding a 1:p (with p a small positive integer, 
usually 1, 2 or 3) document assignment (a set of 
aligned document pairs) that would enforce the ?no 
search outside the document boundary? condition 
when doing word alignment with the advantage of 
reducing the time complexity to      operations. 
When M is large, the reduction may actually be 
vital to getting a result in a reasonable amount of 
time. The downside of this simplification is the 
loss of information: two documents may not be 
correctly aligned thus depriving the word-
alignment algorithm of the part of the search space 
that would have contained the right alignments. 
128
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 128?135,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Word alignment forms the basis of the phrase 
alignment procedure which, in turn, is the basis of 
any statistical translation model. A comparable 
corpus differs essentially from a parallel corpus by 
the fact that textual units do not follow a transla-
tion order that otherwise greatly reduces the word 
alignment search space in a parallel corpus. Given 
this limitation of a comparable corpus in general 
and the sizes of the comparable corpora that we 
will have to deal with in particular,  we have de-
vised one variant of an Expectation Maximization 
(EM) algorithm (Dempster et al, 1977) that gener-
ates a 1:1 (p = 1) document assignment from a par-
allel and/or comparable corpus using only pre-
existing translation lexicons. Its generality would 
permit it to perform the same task on other textual 
units such as paragraphs or sentences. 
In what follows, we will briefly review the lit-
erature discussing document/paragraph alignment 
and then we will present the derivation of the EM 
algorithm that generates 1:1 document alignments. 
We will end the article with a thorough evaluation 
of the performances of this algorithm and the con-
clusions that arise from these evaluations. 
2 Related Work 
Document alignment and other types of textual 
unit alignment have been attempted in various sit-
uations involving extracting parallel data from 
comparable corpora. The first case study is offered 
by Munteanu and Marcu (2002). They align sen-
tences in an English-French comparable corpus of 
1.3M of words per language by comparing suffix 
trees of the sentences. Each sentence from each 
part of the corpus is encoded as a suffix tree which 
is a tree that stores each possible suffix of a string 
from the last character to the full string. Using this 
method, Munteanu and Marcu are able to detect 
correct sentence alignments with a precision of 
95% (out of 100 human-judged and randomly se-
lected sentences from the generated output). The 
running time of their algorithm is approximately 
100 hours for 50000 sentences in each of the lan-
guages. 
A popular method of aligning sentences in a 
comparable corpus is by classifying pairs of sen-
tences as parallel or not parallel. Munteanu and 
Marcu (2005) use a Maximum Entropy classifier 
for the job trained with the following features: sen-
tence lengths and their differences and ratios, per-
centage of the words in a source sentence that have 
translations in a target sentence (translations are 
taken from pre-existing translation lexicons), the 
top three largest fertilities, length of the longest 
sequence of words that have translations, etc. The 
training data consisted of a small parallel corpus of 
5000 sentences per language. Since the number of 
negative instances (50002 ? 5000) is far more large 
than the number of positive ones (5000), the nega-
tive training instances were selected randomly out 
of instances that passed a certain word overlap fil-
ter (see the paper for details). The classifier preci-
sion is around 97% with a recall of 40% at the 
Chinese-English task and around 95% with a recall 
of 41% for the Arabic-English task. 
Another case study of sentence alignment that 
we will present here is that of Chen (1993). He 
employs an EM algorithm that will find a sentence 
alignment in a parallel corpus which maximizes 
the translation probability for each sentence bead 
in the alignment. The translation probability to be 
maximized by the EM procedure considering each 
possible alignment  is given by 
 
 (     )   ( )? ([  
    
 ])
 
   
 
 
The following notations were used:   is the 
English corpus (a sequence of English sentences), 
  is the French corpus, [  
    
 ] is a sentence bead 
(a pairing of m sentences in English with n 
sentences in French),  ([  
    
 ]   [  
    
 ]) 
is the sentence alignment (a sequence of sentence 
beads) and p(L) is the probability that an alignment 
contains L beads. The obtained accuracy is around 
96% and was computed indirectly by checking 
disagreement with the Brown sentence aligner 
(Brown et al, 1991) on randomly selected 500 
disagreement cases. 
The last case study of document and sentence 
alignment from ?very-non-parallel corpora? is the 
work from Fung and Cheung (2004). Their contri-
bution to the problem of textual unit alignment 
resides in devising a bootstrapping mechanism in 
which, after an initial document pairing and conse-
quent sentence alignment using a lexical overlap-
ping similarity measure, IBM-4 model (Brown et 
al., 1993) is employed to enrich the bilingual dic-
tionary that is used by the similarity measure. The 
129
process is repeated until the set of identified 
aligned sentences does not grow anymore. The 
precision of this method on English-Chinese sen-
tence alignment is 65.7% (out of the top 2500 iden-
tified pairs). 
3 EMACC 
We propose a specific instantiation of the well-
known general EM algorithm for aligning different 
types of textual units: documents, paragraphs, and 
sentences which we will name EMACC (an acro-
nym for ?Expectation Maximization Alignment for 
Comparable Corpora?). We draw our inspiration 
from the famous IBM models (specifically from 
the IBM-1 model) for word alignment (Brown et 
al., 1993) where the translation probability (eq. (5)) 
is modeled through an EM algorithm where the 
hidden variable a models the assignment (1:1 word 
alignments) from the French sequence of words (? 
indexes) to the English one. 
By analogy, we imagined that between two sets 
of documents (from now on, we will refer to doc-
uments as our textual units but what we present 
here is equally applicable ? but with different per-
formance penalties ? to paragraphs and/or sentenc-
es) ? let?s call them   and  , there is an assignment 
(a sequence of 1:1 document correspondences1), 
the distribution of which can be modeled by a hid-
den variable   taking values in the set {true, false}. 
This assignment will be largely determined by the 
existence of word translations between a pair of 
documents, translations that can differentiate be-
tween one another in their ability to indicate a cor-
rect document alignment versus an incorrect one. 
In other words, we hypothesize that there are cer-
tain pairs of translation equivalents that are better 
indicators of a correct document correspondence 
than other translation equivalents pairs. 
We take the general formulation and derivation 
of the EM optimization problem from (Borman, 
2009). The general goal is to optimize  (   ), that 
is to find the parameter(s)   for which  (   ) is 
maximum. In a sequence of derivations that we are 
not going to repeat here, the general EM equation 
is given by: 
                                                          
1 Or ?alignments? or ?pairs?. These terms will be used with 
the same meaning throughout the presentation. 
    
       
 
? (      )    (     )
 
 (1) 
where  ?  (      )   . At step n+1, we try to 
obtain a new parameter      that is going to max-
imize (the maximization step) the sum over z (the 
expectation step) that in its turn depends on the 
best parameter    obtained at step n. Thus, in 
principle, the algorithm should iterate over the set 
of all possible   parameters, compute the expecta-
tion expression for each of these parameters and 
choose the parameter(s) for which the expression 
has the largest value. But as we will see, in prac-
tice, the set of all possible parameters has a dimen-
sion that is exponential in terms of the number of 
parameters. This renders the problem intractable 
and one should back off to heuristic searches in 
order to find a near-optimal solution. 
We now introduce a few notations that we will 
operate with from this point forward. We suggest 
to the reader to frequently refer to this section in 
order to properly understand the next equations: 
?   is the set of source documents,     is the 
cardinal of this set; 
?   is the set of target documents with     its 
cardinal; 
?     is a pair of documents,      and 
    ; 
?    is a pair of translation equivalents 
?     ? such that    is a lexical item that 
belongs to    and    is a lexical item that 
belongs to   ; 
?   is the set of all existing translation 
equivalents pairs ?     ?.   is the transla-
tion probability score (as the one given for 
instance by GIZA++ (Gao and Vogel, 
2008)). We assume that GIZA++ transla-
tion lexicons already exist for the pair of 
languages of interest. 
In order to tie equation 1 to our problem, we de-
fine its variables as follows: 
?   is the sequence of 1:1 document align-
ments of the form              ,     
{              }. We call   an assign-
ment which is basically a sequence of 1:1 
document alignments. If there are     1:1 
document alignments in   and if        , 
then the set of all possible assignments has 
130
the cardinal equal to     (
   
   
) where n! is 
the factorial function of the integer n and 
.
 
 
/ is the binomial coefficient. It is clear 
now that with this kind of dimension of the 
set of all possible assignments (or   pa-
rameters), we cannot simply iterate over it 
in order to choose the assignment that 
maximizes the expectation; 
?   *          + is the hidden variable that 
signals if a pair of documents     repre-
sents a correct alignment (true) or not 
(false); 
?   is the sequence of translation equivalents 
pairs    from T in the order they appear 
in each document pair from  . 
Having defined the variables in equation 1 this 
way, we aim at maximizing the translation equiva-
lents probability over a given assignment,  (   ). 
In doing so, through the use of the hidden variable 
z, we are also able to find the 1:1 document align-
ments that attest for this maximization. 
We proceed by reducing equation 1 to a form 
that is readily amenable to software coding. That 
is, we aim at obtaining some distinct probability 
tables that are going to be (re-)estimated by the 
EM procedure. Due to the lack of space, we omit 
the full derivation and directly give the general 
form of the derived EM equation 
           
 
,   (   )     (      )- (2) 
Equation 2 suggests a method of updating the as-
signment probability  (      )  with the lexical 
alignment probability  (   ) in an effort to pro-
vide the alignment clues that will ?guide? the as-
signment probability towards the correct 
assignment. All it remains to do now is to define 
the two probabilities. 
The lexical document alignment probability 
 (   ) is defined as follows: 
 (   )  ?
?  (   |   )     
      
     
 (3) 
where  (       )  is the simplified lexical docu-
ment alignment probability which is initially equal 
to  (   ) from the set  . This probability is to be 
read as ?the contribution    makes to the correct-
ness of the     alignment?. We want that the 
alignment contribution of one translation equiva-
lents pair    to distribute over the set of all possi-
ble document pairs thus enforcing that 
?  (   |   )   
    {              }
 (4) 
The summation over   in equation 3 is actually 
over all translation equivalents pairs that are to be 
found only in the current     document pair and 
the presence of the product        ensures that we 
still have a probability value. 
The assignment probability  (      ) is also 
defined in the following way: 
 
 (      )  ?  (        )
     
 (5) 
for which we enforce the condition: 
?  (        )   
    {             }
 
(6) 
Using equations 2, 3 and 5 we deduce the final, 
computation-ready EM equation 
     
       
 
[  ?
?  (       )     
      
     
   ?  (        )
     
]
       
 
? [  
?  (       )     
      
     
    (        )] 
(7) 
As it is, equation 7 suggests an exhaustive search 
in the set of all possible   parameters, in order to 
find the parameter(s) for which the expression that 
is the argument of ?argmax? is maximum. But, as 
we know from section 3, the size of this this set is 
prohibitive to the attempt of enumerating each   
assignment and computing the expectation expres-
sion. Our quick solution to this problem was to 
directly construct the ?best?   assignment2 using a 
                                                          
2 We did not attempt to find the mathematical maximum of the 
expression from equation 7 and we realize that the conse-
131
greedy algorithm: simply iterate over all possible 
1:1 document pairs and for each document pair 
    {              }  compute the align-
ment count (it?s not a probability so we call it a 
?count? following IBM-1 model?s terminology) 
 
  
?  (   |   )     
      
    (        ) 
 
Then, construct the best 1:1 assignment      by 
choosing those pairs     for which we have counts 
with the maximum values. Before this cycle 
(which is the basic EM cycle) is resumed, we per-
form the following updates: 
 (        )   (        )
 
?  (   |   )     
      
 
(7a) 
 
 (   |   )  ?  (   |   )
        
 (7b) 
and normalize the two probability tables with 
equations 6 and 4. The first update is to be inter-
preted as the contribution the lexical document 
alignment probability makes to the alignment 
probability. The second update equation aims at 
boosting the probability of a translation equivalent 
if and only if it is found in a pair of documents be-
longing to the best assignment so far. In this way, 
we hope that the updated translation equivalent 
will make a better contribution to the discovery of 
a correct document alignment that has not yet been 
discovered at step n + 1. 
Before we start the EM iterations, we need to 
initialize the probability tables  (        ) and 
 (   |   ) . For the second table we used the 
GIZA++ scores that we have for the     translation 
equivalents pairs and normalized the table with 
equation 4. For the first probability table we have 
(and tried) two choices: 
? (D1) a uniform distribution: 
 
      
; 
? (D2) a lexical document alignment meas-
ure  (   ) (values between 0 and 1) that is 
computed directly from a pair of docu-
                                                                                           
quence of this choice and of the greedy search procedure is not 
finding the true optimum. 
ments     using the    translation equiva-
lents pairs from the dictionary  : 
 (   )  
 
?    (  )        ?    (  )        
        
 
(8) 
where      is the number of words in document    
and    (  ) is the frequency of word    in docu-
ment    (please note that, according to section 3, 
    is not a random pair of words, but a pair of 
translation equivalents). If every word in the 
source document has at least one translation (of a 
given threshold probability score) in the target 
document, then this measure is 1. We normalize 
the table initialized using this measure with equa-
tion 6. 
EMACC finds only 1:1 textual units alignments 
in its present form but a document pair     can be 
easily extended to a document bead following the 
example from (Chen, 1993). The main difference 
between the algorithm described by Chen and ours 
is that the search procedure reported there is inva-
lid for comparable corpora in which no pruning is 
available due to the nature of the corpus. A second 
very important difference is that Chen only relies 
on lexical alignment information, on the parallel 
nature of the corpus and on sentence lengths corre-
lations while we add the probability of the whole 
assignment which, when initially set to the D2 dis-
tribution, produces a significant boost of the preci-
sion of the alignment. 
4 Experiments and Evaluations 
The test data for document alignment was com-
piled from the corpora that was previously collect-
ed in the ACCURAT project3 and that is known to 
the project members as the ?Initial Comparable 
Corpora? or ICC for short. It is important to know 
the fact that ICC contains all types of comparable 
corpora from parallel to weakly comparable docu-
ments but we classified document pairs in three 
classes: parallel (class name: p), strongly compa-
rable (cs) and weakly comparable (cw). We have 
considered the following pairs of languages: Eng-
lish-Romanian (en-ro), English-Latvian (en-lv), 
English-Lithuanian (en-lt), English-Estonian (en-
et), English-Slovene (en-sl) and English-Greek 
                                                          
3 http://www.accurat-project.eu/ 
132
(en-el). For each pair of languages, ICC also con-
tains a Gold Standard list of document alignments 
that were compiled by hand for testing purposes. 
We trained GIZA++ translation lexicons for 
every language pair using the DGT-TM4 corpus. 
The input texts were converted from their Unicode 
encoding to UTF-8 and were tokenized using a 
tokenizer web service described by Ceau?u (2009). 
Then, we applied a parallel version of GIZA++ 
(Gao and Vogel, 2008) that gave us the translation 
dictionaries of content words only (nouns, verbs, 
adjective and adverbs) at wordform level. For Ro-
manian, Lithuanian, Latvian, Greek and English, 
we had lists of inflectional suffixes which we used 
to stem entries in respective dictionaries and pro-
cessed documents. Slovene remained the only lan-
guage which involved wordform level processing. 
The accuracy of EMACC is influenced by three 
parameters whose values have been experimentally 
set: 
? the threshold over which we use transla-
tion equivalents from the dictionary   for 
textual unit alignment; values for this 
threshold (let?s name it ThrGiza) are 
from the ordered set *             +; 
? the threshold over which we decide to up-
date the probabilities of translation equiva-
lents with equation 7b; values for this 
threshold (named ThrUpdate) are from 
the same ordered set *             +; 
? the top ThrOut% alignments from the 
best assignment found by EMACC. This 
parameter will introduce precision and re-
call with the ?perfect? value for recall 
equal to ThrOut%. Values for this pa-
rameter are from the set *         +. 
We ran EMACC (10 EM steps) on every possible 
combination of these parameters for the pairs of 
languages in question on both initial distributions 
D1 and D2. For comparison, we also performed a 
baseline document alignment using the greedy al-
gorithm of EMACC with the equation 8 supplying 
the document similarity measure. The following 4 
tables report a synthesis of the results we have ob-
tained which, because of the lack of space, we 
cannot give in full. We omit the results of EMACC 
with D1 initial distribution because the accuracy 
                                                          
4 http://langtech.jrc.it/DGT-TM.html 
figures (both precision and recall) are always lower 
(10-20%) than those of EMACC with D2. 
cs P/R Prms. P/R Prms. # 
en-
ro 
1/ 
0.69047 
0.4 
0.4 
0.7 
0.85714/ 
0.85714 
0.4 
0.4 
1 
42 
en-
sl 
0.96666/ 
0.28807 
0.4 
0.4 
0.3 
0.83112/ 
0.83112 
0.4 
0.4 
1 
302 
en-
el 
0.97540/ 
0.29238 
0.001 
0.8 
0.3 
0.80098/ 
0.80098 
0.001 
0.4 
1 
407 
en-
lt 
0.97368/ 
0.29191 
0.4 
0.8 
0.3 
0.72978/ 
0.72978 
0.4 
0.4 
1 
507 
en-
lv 
0.95757/ 
0.28675 
0.4 
0.4 
0.3 
0.79854/ 
0.79854 
0.001 
0.8 
1 
560 
en-
et 
0.88135/ 
0.26442 
0.4 
0.8 
0.3 
0.55182/ 
0.55182 
0.4 
0.4 
1 
987 
Table 1: EMACC with D2 initial distribution on strong-
ly comparable corpora 
 
cs P/R Prms. P/R Prms. # 
en-
ro 
1/ 
0.69047 
0.4 
0.7 
0.85714/ 
0.85714 
0.4 
1 
42 
en-
sl 
0.97777/ 
0.29139 
0.001 
0.3 
0.81456/ 
0.81456 
0.4 
0.1 
302 
en-
el 
0.94124/ 
0.28148 
0.001 
0.3 
0.71851/ 
0.71851 
0.001 
1 
407 
en-
lt 
0.95364/ 
0.28514 
0.001 
0.3 
0.72673/ 
0.72673 
0.001 
1 
507 
en-
lv 
0.91463/ 
0.27322 
0.001 
0.3 
0.80692/ 
0.80692 
0.001 
1 
560 
en-
et 
0.87030/ 
0.26100 
0.4 
0.3 
0.57727/ 
0.57727 
0.4 
1 
987 
Table 2: D2 baseline algorithm on strongly comparable 
corpora 
 
cw P/R Prms. P/R Prms. # 
en-
ro 
1/ 
0.29411 
0.4 
0.001 
0.3 
0.66176/ 
0.66176 
0.4 
0.001 
1 
68 
en-
sl 
0.73958/ 
0.22164 
0.4 
0.4 
0.3 
0.42767/ 
0.42767 
0.4 
0.4 
1 
961 
en-
el 
0.15238/ 
0.04545 
0.001 
0.8 
0.3 
0.07670/ 
0.07670 
0.001 
0.8 
1 
352 
en-
lt 
0.55670/ 
0.16615 
0.4 
0.8 
0.3 
0.28307/ 
0.28307 
0.4 
0.8 
1 
325 
en-
lv 
0.23529/ 
0.07045 
0.4 
0.4 
0.3 
0.10176/ 
0.10176 
0.4 
0.4 
1 
511 
en-
et 
0.59027/ 
0.17634 
0.4 
0.8 
0.3 
0.27800/ 
0.27800 
0.4 
0.8 
1 
483 
Table 3: EMACC with D2 initial distribution on weakly 
comparable corpora 
133
cw P/R Prms. P/R Prms. # 
en-
ro 
0.85/ 
0.25 
0.4 
0.3 
0.61764/ 
0.61764 
0.4 
1 
68 
en-
sl 
0.65505/ 
0.19624 
0.4 
0.3 
0.39874/ 
0.39874 
0.4 
1 
961 
en-
el 
0.11428/ 
0.03428 
0.4 
0.3 
0.06285/ 
0.06285 
0.4 
1 
352 
en-
lt 
0.60416/ 
0.18012 
0.4 
0.3 
0.24844/ 
0.24844 
0.4 
1 
325 
en-
lv 
0.13071/ 
0.03921 
0.4 
0.3 
0.09803/ 
0.09803 
0.4 
1 
511 
en-
et 
0.48611/ 
0.14522 
0.001 
0.3 
0.25678/ 
0.25678 
0.4 
1 
483 
Table 4: D2 baseline algorithm on weakly comparable 
corpora 
 
In every table above, the P/R column gives the 
maximum precision and the associated recall 
EMACC was able to obtain for the corresponding 
pair of languages using the parameters (Prms.) 
from the next column. The P/R column gives the 
maximum recall with the associated precision that 
we obtained for that pair of languages.  
The Prms. columns contain parameter settings 
for EMACC (see Tables 1 and 3) and for the D2 
baseline algorithm (Tables 2 and 4): in Tables 1 
and 3 values for ThrGiza, ThrUpdate and 
ThrOut are given from the top (of the cell) to the 
bottom and in Tables 2 and 4 values of ThrGiza 
and ThrOut are also given from top to bottom 
(the ThrUpdate parameter is missing because the 
D2 baseline algorithm does not do re-estimation). 
The # column contains the size of the test set: the 
number of documents in each language that have to 
be paired. The search space is # * # and the gold 
standard contains # pairs of human aligned docu-
ment pairs.  
To ease comparison between EMACC and the 
D2 baseline for each type of corpora (strongly and 
weakly comparable), we grayed maximal values 
between the two: either the precision in the P/R 
column or the recall in the P/R column. 
In the case of strongly comparable corpora (Ta-
bles 1 and 2), we see that the benefits of re-
estimating the probabilities of the translation 
equivalents (based on which we judge document 
alignments) begin to emerge with precisions for all 
pairs of languages (except en-sl) being better than 
those obtained with the D2 baseline. But the real 
benefit of re-estimating the probabilities of transla-
tion equivalents along the EM procedure is visible 
from the comparison between Tables 3 and 4. Thus, 
in the case of weakly comparable corpora, in 
which EMACC with the D2 distribution is clearly 
better than the baseline (with the only exception of 
en-lt precision), due to the significant decrease in 
the lexical overlap, the EM procedure is able to 
produce important alignment clues in the form of 
re-estimated (bigger) probabilities of translation 
equivalents that, otherwise, would have been ig-
nored. 
It is important to mention the fact that the re-
sults we obtained varied a lot with values of the 
parameters ThrGiza and ThrUpdate. We ob-
served, for the majority of studied language pairs, 
that lowering the value for ThrGiza and/or 
ThrUpdate (0.1, 0.01, 0.001?), would negative-
ly impact the performance of EMACC due to the 
fact of introducing noise in the initial computation 
of the D2 distribution and also on re-estimating 
(increasing) probabilities for irrelevant translation 
equivalents. At the other end, increasing the 
threshold for these parameters (0.8, 0.85, 0.9?) 
would also result in performance decreasing due to 
the fact that too few translation equivalents (be 
they all correct) are not enough to pinpoint correct 
document alignments since there are great chances 
for them to actually appear in all document pairs. 
So, we have experimentally found that there is a 
certain balance between the degree of correctness 
of translation equivalents and their ability to pin-
point correct document alignments. In other words, 
the paradox resides in the fact that if a certain pair 
of translation equivalents is not correct but the re-
spective words appear only in documents which 
correctly align to one another, that pair is very im-
portant to the alignment process. Conversely, if a 
pair of translation equivalents has a very high 
probability score (thus being correct) but appears 
in almost every possible pair of documents, that 
pair is not informative to the alignment process and 
must be excluded. We see now that the EMACC 
aims at finding the set of translation equivalents 
that is maximally informative with respect to the 
set of document alignments. 
We have introduced the ThrOut parameter in 
order to have better precision. This parameter actu-
ally instructs EMACC to output only the top (ac-
cording to the alignment score probability 
 (        )) ThrOut% of the document align-
ments it has found. This means that, if all are cor-
rect, the maximum recall can only be ThrOut%. 
134
But another important function of ThrOut is to 
restrict the translation equivalents re-estimation 
(equation 7b) for only the top ThrOut% align-
ments. In other words, only the probabilities of 
translation equivalents that are to be found in top 
ThrOut% best alignments in the current EM step 
are re-estimated. We introduced this restriction in 
order to confine translation equivalents probability 
re-estimation to correct document alignments 
found so far. 
Regarding the running time of EMACC, we can 
report that on a cluster with a total of 32 CPU 
cores (4 nodes) with 6-8 GB of RAM per node, the 
total running time is between 12h and 48h per lan-
guage pair (about 2000 documents per language) 
depending on the setting of the various parameters. 
5 Conclusions 
The whole point in developing textual unit align-
ment algorithms for comparable corpora is to be 
able to provide good quality quasi-aligned data to 
programs that are specialized in extracting parallel 
data from these alignments. In the context of this 
paper, the most important result to note is that 
translation probability re-estimation is a good tool 
in discovering new correct textual unit alignments 
in the case of weakly related documents. We also 
tested EMACC at the alignment of 200 parallel 
paragraphs (small texts of no more than 50 words) 
for all pairs of languages that we have considered 
here. We can briefly report that the results are bet-
ter than the strongly comparable document align-
ments from Tables 1 and 2 which is a promising 
result because one would think that a significant 
reduction in textual unit size would negatively im-
pact the alignment accuracy. 
Acknowledgements 
This work has been supported by the ACCURAT 
project (http://www.accurat-project.eu/) funded by 
the European Community?s Seventh Framework 
Program (FP7/2007-2013) under the Grant Agree-
ment n? 248347. It has also been partially support-
ed by the Romanian Ministry of Education and 
Research through the STAR project (no. 
742/19.01.2009). 
 
 
References 
 
Borman, S. 2009. The Expectation Maximization Algo-
rithm. A short tutorial. Online at: 
http://www.isi.edu/natural-
language/teaching/cs562/2009/readings/B06.pdf 
Brown, P. F., Lai, J. C., and Mercer, R. L. 1991. Align-
ing sentences in parallel corpora. In Proceedings of 
the 29th Annual Meeting of the Association for 
Computational Linguistics, pp. 169?176, June 8-21, 
1991, University of California, Berkeley, California, 
USA. 
Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., and Mer-
cer, R. L. 1993. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation. 
Computational Linguistics, 19(2): 263?311. 
Ceau?u, A. 2009. Statistical Machine Translation for 
Romanian. PhD Thesis, Romanian Academy (in Ro-
manian). 
Chen, S. F. 1993. Aligning Sentences in Bilingual Cor-
pora Using Lexical Information. In Proceedings of 
the 31st Annual Meeting on Association for Compu-
tational Linguistics, pp. 9?16, Columbus, Ohio, 
USA. 
Dempster, A. P., Laird, N. M., and Rubin, D. B. 1977. 
Maximum likelihood from incomplete data via the 
EM algorithm. Journal of the Royal Statistical Socie-
ty, 39(B):1?38. 
Fung, P., and Cheung, P. 2004. Mining Very-Non-
Parallel Corpora: Parallel Sentence and Lexicon Ex-
traction via Bootstrapping and EM. In Proceedings of 
EMNLP 2004, Barcelona, Spain: July 2004. 
Gao, Q., and Vogel, S. 2008.  Parallel implementations 
of word alignment tool.  ACL-08 HLT: Software En-
gineering, Testing, and Quality Assurance for Natu-
ral Language Processing, pp. 49?57, June 20, 2008, 
The Ohio State University, Columbus, Ohio, USA.  
Munteanu, D. S., and Marcu, D. 2002. Processing com-
parable corpora with bilingual suffix trees. In Pro-
ceedings of the 2002 Conference on Empirical 
Methods in Natural Language Processing (EMNLP 
2002), pp. 289?295, July 6-7, 2002, University of 
Pennsylvania, Philadelphia, USA. 
Munteanu, D. S., and Marcu, D. 2005. Improving ma-
chine translation performance by exploiting non-
parallel corpora. Computational Linguistics, 
31(4):477?504. 
135
