Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 224?233,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Online Large-Margin Training of
Syntactic and Structural Translation Features
David Chiang
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292, USA
chiang@isi.edu
Yuval Marton and Philip Resnik
Department of Linguistics and the UMIACS
Laboratory for Computational Linguistics
and Information Processing
University of Maryland
College Park, MD 20742, USA
{ymarton,resnik}@umiacs.umd.edu
Abstract
Minimum-error-rate training (MERT) is a bot-
tleneck for current development in statistical
machine translation because it is limited in
the number of weights it can reliably opti-
mize. Building on the work of Watanabe et
al., we explore the use of the MIRA algorithm
of Crammer et al as an alternative to MERT.
We first show that by parallel processing and
exploiting more of the parse forest, we can
obtain results using MIRA that match or sur-
pass MERT in terms of both translation qual-
ity and computational cost. We then test the
method on two classes of features that address
deficiencies in the Hiero hierarchical phrase-
based model: first, we simultaneously train a
large number of Marton and Resnik?s soft syn-
tactic constraints, and, second, we introduce
a novel structural distortion model. In both
cases we obtain significant improvements in
translation performance. Optimizing them in
combination, for a total of 56 feature weights,
we improve performance by 2.6 B??? on a
subset of the NIST 2006 Arabic-English eval-
uation data.
1 Introduction
Since its introduction by Och (2003), minimum er-
ror rate training (MERT) has been widely adopted
for training statistical machine translation (MT) sys-
tems. However, MERT is limited in the number of
feature weights that it can optimize reliably, with
folk estimates of the limit ranging from 15 to 30 fea-
tures.
One recent example of this limitation is a series
of experiments by Marton and Resnik (2008), in
which they added syntactic features to Hiero (Chi-
ang, 2005; Chiang, 2007), which ordinarily uses no
linguistically motivated syntactic information. Each
of their new features rewards or punishes a deriva-
tion depending on how similar or dissimilar it is
to a syntactic parse of the input sentence. They
found that in order to obtain the greatest improve-
ment, these features had to be specialized for par-
ticular syntactic categories and weighted indepen-
dently. Not being able to optimize them all at once
using MERT, they resorted to running MERT many
times in order to test different combinations of fea-
tures. But it would have been preferable to use a
training method that can optimize the features all at
once.
There has been much work on improving MERT?s
performance (Duh and Kirchoff, 2008; Smith and
Eisner, 2006; Cer et al, 2008), or on replacing
MERT wholesale (Turian et al, 2007; Blunsom et
al., 2008). This paper continues a line of research on
online discriminative training (Tillmann and Zhang,
2006; Liang et al, 2006; Arun and Koehn, 2007),
extending that of Watanabe et al (2007), who use
the Margin Infused Relaxed Algorithm (MIRA) due
to Crammer et al (2003; 2006). Our guiding princi-
ple is practicality: like Watanabe et al, we train on
a small tuning set comparable in size to that used
by MERT, but by parallel processing and exploit-
ing more of the parse forest, we obtain results us-
ing MIRA that match or surpass MERT in terms of
both translation quality and computational cost on a
large-scale translation task.
Taking this further, we test MIRA on two classes
of features that make use of syntactic information
and hierarchical structure. First, we generalize Mar-
ton and Resnik?s (2008) soft syntactic constraints by
224
training all of them simultaneously; and, second, we
introduce a novel structural distortion model. We ob-
tain significant improvements in both cases, and fur-
ther large improvements when the two feature sets
are combined.
The paper proceeds as follows. We describe our
training algorithm in section 2; our generalization
of Marton and Resnik?s soft syntactic constraints in
section 3; our novel structural distortion features in
section 4; and experimental results in section 5.
2 Learning algorithm
The translation model is a standard linear model
(Och and Ney, 2002), which we train using MIRA
(Crammer and Singer, 2003; Crammer et al, 2006),
following Watanabe et al (2007). We describe the
basic algorithm first and then progressively refine it.
2.1 Basic algorithm
Let e, by abuse of notation, stand for both output
strings and their derivations. We represent the fea-
ture vector for derivation e as h(e). Initialize the fea-
ture weights w. Then, repeatedly:
? Select a batch of input sentences f1, . . . , fm.
? Decode each fi to obtain a set of hypothesis
translations ei1, . . . , ein.
? For each i, select one of the ei j to be the oracle
translation e?i , by a criterion described below.
Let ?hi j = h(e?i ) ? h(ei j).
? For each ei j, compute the loss `i j, which is
some measure of how bad it would be to guess
ei j instead of e?i .
? Update w to the value of w? that minimizes:
1
2
?w? ? w?2 + C
m?
i=1
max
1? j?n
(`i j ? ?hi j ? w?) (1)
where we set C = 0.01. The first term means
that we want w? to be close to w, and second
term (the generalized hinge loss) means that we
want w? to score e?i higher than each ei j by a
margin at least as wide as the loss `i j.
When training is finished, the weight vectors from
all iterations are averaged together. (If multiple
passes through the training data are made, we only
average the weight vectors from the last pass.) The
technique of averaging was introduced in the con-
text of perceptrons as an approximation to taking a
vote among all the models traversed during training,
and has been shown to work well in practice (Fre-
und and Schapire, 1999; Collins, 2002). We follow
McDonald et al (2005) in applying this technique to
MIRA.
Note that the objective (1) is not the same as that
used by Watanabe et al; rather, it is the same as
that used by Crammer and Singer (2003) and related
to that of Taskar et al (2005). We solve this opti-
mization problem using a variant of sequential min-
imal optimization (Platt, 1998): for each i, initialize
?i j = C for a single value of j such that ei j = e?i ,
and initialize ?i j = 0 for all other values of j. Then,
repeatedly choose a sentence i and a pair of hypothe-
ses j, j?, and let
w? ? w? + ?(?hi j ? ?hi j?) (2)
?i j ? ?i j + ? (3)
?i j? ? ?i j? ? ? (4)
where
? = clip
[??i j,?i j? ]
(`i j ? `i j?) ? (?hi j ? ?hi j?) ? w?
??hi j ? ?hi j??2
(5)
where the function clip[x,y](z) gives the closest num-
ber to z in the interval [x, y].
2.2 Loss function
Assuming B??? as the evaluation criterion, the loss
`i j of ei j relative to e?i should be related somehow
to the difference between their B??? scores. How-
ever, B??? was not designed to be used on individ-
ual sentences; in general, the highest-B??? transla-
tion of a sentence depends on what the other sen-
tences in the test set are. Sentence-level approxi-
mations to B??? exist (Lin and Och, 2004; Liang
et al, 2006), but we found it most effective to per-
form B??? computations in the context of a set O of
previously-translated sentences, following Watan-
abe et al (2007). However, we don?t try to accu-
mulate translations for the entire dataset, but simply
maintain an exponentially-weighted moving average
of previous translations.
225
More precisely: For an input sentence f, let e be
some hypothesis translation and let {rk} be the set of
reference translations for f. Let c(e; {rk}), or simply
c(e) for short, be the vector of the following counts:
|e|, the effective reference length mink |rk|, and, for
1 ? n ? 4, the number of n-grams in e, and the num-
ber of n-gram matches between e and {rk}. These
counts are sufficient to calculate a B??? score, which
we write as B???(c(e)). The pseudo-document O is
an exponentially-weighted moving average of these
vectors. That is, for each training sentence, let e? be
the 1-best translation; after processing the sentence,
we update O, and its input length O f :
O ? 0.9(O + c(e?)) (6)
O f ? 0.9(O f + |f|) (7)
We can then calculate the B??? score of hypothe-
ses e in the context of O. But the larger O is, the
smaller the impact the current sentence will have on
the B??? score. To correct for this, and to bring the
loss function roughly into the same range as typical
margins, we scale the B??? score by the size of the
input:
B(e; f, {rk}) = (O f + |f|) ? B???(O + c(e; {rk})) (8)
which we also simply write as B(e). Finally, the loss
function is defined to be:
`i j = B(e?i ) ? B(ei j) (9)
2.3 Oracle translations
We now describe the selection of e?. We know of
three approaches in previous work. The first is to
force the decoder to output the reference sentence
exactly, and select the derivation with the highest
model score, which Liang et al (2006) call bold up-
dating. The second uses the decoder to search for
the highest-B??? translation (Tillmann and Zhang,
2006), which Arun and Koehn (2007) call max-B???
updating. Liang et al and Arun and Koehn experi-
ment with these methods and both opt for a third
method, which Liang et al call local updating: gen-
erate an n-best list of translations and select the
highest-B??? translation from it. The intuition is that
due to noise in the training data or reference transla-
tions, a high-B??? translation may actually use pe-
culiar rules which it would be undesirable to en-
courage the model to use. Hence, in local updating,
Model score
B
??
?
sc
or
e
0.4
0.5
0.6
0.7
0.8
0.9
1
-90 -85 -80 -75 -70 -65 -60
? = 0
? = 0.5
? = 1
? = ?
Figure 1: Scatter plot of 10-best unique translations of a
single sentence obtained by forest rescoring using various
values of ? in equation (11).
the search for the highest-B??? translation is limited
to the n translations with the highest model score,
where n must be determined experimentally.
Here, we introduce a new oracle-translation selec-
tion method, formulating the intuition behind local
updating as an optimization problem:
e? = arg max
e
(B(e) + h(e) ? w) (10)
Instead of choosing the highest-B??? translation
from an n-best list, we choose the translation that
maximizes a combination of (approximate) B???
and the model.
We can also interpret (10) in the following way:
we want e? to be the max-B??? translation, but we
also want to minimize (1). So we balance these two
criteria against each other:
e? = arg max
e
(B(e) ? ?(B(e) ? h(e) ? w)) (11)
where (B(e) ? h(e) ? w) is that part of (1) that de-
pends on e?, and ? is a parameter that controls how
much we are willing to allow some translations to
have higher B??? than e? if we can better minimize
(1). Setting ? = 0 would reduce to max-B??? up-
dating; setting ? = ? would never update w at all.
Setting ? = 0.5 reduces to equation (10).
Figure 1 shows the 10-best unique translations for
a single input sentence according to equation (11)
under various settings of ?. The points at far right are
the translations that are scored highest according to
226
the model. The ? = 0 points in the upper-left corner
are typical of oracle translations that would be se-
lected under the max-B??? policy: they indeed have
a very high B??? score, but are far removed from the
translations preferred by the model; thus they would
cause violent updates to w. Local updating would
select the topmost point labeled ? = 1. Our scheme
would select one of the ? = 0.5 points, which have
B??? scores almost as high as the max-B??? transla-
tions, yet are not very far from the translations pre-
ferred by the model.
2.4 Selecting hypothesis translations
What is the set {ei j} of translation hypotheses? Ide-
ally we would let it be the set of all possible transla-
tions, and let the objective function (1) take all of
them into account. This is the approach taken by
Taskar et al (2004), but their approach assumes that
the loss function can be decomposed into local loss
functions. Since our loss function cannot be so de-
composed, we select:
? the 10-best translations according to the model;
we then rescore the forest to obtain
? the 10-best translations according to equation
(11) with ? = 0.5, the first of which is the oracle
translation, and
? the 10-best translations with ? = ?, to serve as
negative examples.
The last case is what Crammer et al (2006) call
max-loss updating (where ?loss? refers to the gener-
alized hinge loss) and Taskar et al (2005) call loss-
augmented inference. The rationale here is that since
the objective (1) tries to minimize max j(`i j ? ?hi j ?
w?), we should include the translations that have the
highest (`i j ? ?hi j ? w) in order to approximate the
effect of using the whole forest.
See Figure 1 again for an illustration of the hy-
potheses selected for a single sentence. The max-
B??? points in the upper left are not included (and
would have no effect even if they were included).
The ? = ? points in the lower-right are the negative
examples: they are poor translations that are scored
too high by the model, and the learning algorithm
attempts to shift them to the left.
To perform the forest rescoring, we need to use
several approximations, since an exact search for
B???-optimal translations is NP-hard (Leusch et al,
2008). For every derivation e in the forest, we calcu-
late a vector c(e) of counts as in Section 2.2 except
using unclipped counts of n-gram matches (Dreyer
et al, 2007), that is, the number of matches for an n-
gram can be greater than the number of occurrences
of the n-gram in any reference translation. This can
be done efficiently by calculating c for every hyper-
edge (rule application) in the forest:
? the number of output words generated by the
rule
? the effective reference length scaled by the frac-
tion of the input sentence consumed by the rule
? the number of n-grams formed by the applica-
tion of the rule (1 ? n ? 4)
? the (unclipped) number of n-gram matches
formed by the application of the rule (1 ? n ?
4)
We keep track of n-grams using the same scheme
used to incorporate an n-gram language model into
the decoder (Wu, 1996; Chiang, 2007).
To find the best derivation in the forest, we tra-
verse it bottom-up as usual, and for every set of al-
ternative subtranslations, we select the one with the
highest score. But here a rough approximation lurks,
because we need to calculate B on the nodes of the
forest, but B does not have the optimal substructure
property, i.e., the optimal score of a parent node can-
not necessarily be calculated from the optimal scores
of its children. Nevertheless, we find that this rescor-
ing method is good enough for generating high-B???
oracle translations and low-B??? negative examples.
2.5 Parallelization
One convenient property of MERT is that it is em-
barrassingly parallel: we decode the entire tuning set
sending different sentences to different processors,
and during optimization of feature weights, differ-
ent random restarts can be sent to different proces-
sors. In order to make MIRA comparable in effi-
ciency to MERT, we must parallelize it. But with
an online learning algorithm, parallelization requires
a little more coordination. We run MIRA on each
227
processor simultaneously, with each maintaining its
own weight vector. A master process distributes dif-
ferent sentences from the tuning set to each of the
processors; when each processor finishes decoding
a sentence, it transmits the resulting hypotheses,
with their losses, to all the other processors and re-
ceives any hypotheses waiting from other proces-
sors. Those hypotheses were generated from differ-
ent weight vectors, but can still provide useful in-
formation. The sets of hypotheses thus collected are
then processed as one batch. When the whole train-
ing process is finished, we simply average all the
weight vectors from all the processors.
Having described our training algorithm, which
includes several practical improvements to Watan-
abe et al?s usage of MIRA, we proceed in the re-
mainder of the paper to demonstrate the utility of the
our training algorithm on models with large numbers
of structurally sensitive features.
3 Soft syntactic constraints
The first features we explore are based on a line
of research introduced by Chiang (2005) and im-
proved on by Marton and Resnik (2008). A hi-
erarchical phrase-based translation model is based
on synchronous context-free grammar, but does not
normally use any syntactic information derived from
linguistic knowledge or treebank data: it uses trans-
lation rules that span any string of words in the input
sentence, without regard for parser-defined syntac-
tic constituency boundaries. Chiang (2005) exper-
imented with a constituency feature that rewarded
rules whose source language side exactly spans a
syntactic constituent according to the output of an
external source-language parser. This feature can
be viewed as a soft syntactic constraint: it biases
the model toward translations that respect syntactic
structure, but does not force it to use them. However,
this more syntactically aware model, when tested in
Chinese-English translation, did not improve trans-
lation performance.
Recently, Marton and Resnik (2008) revisited
the idea of constituency features, and succeeded in
showing that finer-grained soft syntactic constraints
yield substantial improvements in B??? score for
both Chinese-English and Arabic-English transla-
tion. In addition to adding separate features for dif-
ferent syntactic nonterminals, they introduced a new
type of constraint that penalizes rules when the
source language side crosses the boundaries of a
source syntactic constituent, as opposed to simply
rewarding rules when they are consistent with the
source-language parse tree.
Marton and Resnik optimized their features?
weights using MERT. But since MERT does not
scale well to large numbers of feature weights, they
were forced to test individual features and manu-
ally selected feature combinations each in a sepa-
rate model. Although they showed gains in trans-
lation performance for several such models, many
larger, potentially better feature combinations re-
mained unexplored. Moreover, the best-performing
feature subset was different for the two language
pairs, suggesting that this labor-intensive feature se-
lection process would have to be repeated for each
new language pair.
Here, we use MIRA to optimize Marton and
Resnik?s finer-grained single-category features all at
once. We define below two sets of features, a coarse-
grained class that combines several constituency cat-
egories, and a fine-grained class that puts different
categories into different features. Both kinds of fea-
tures were used by Marton and Resnik, but only a
few at a time. Crucially, our training algorithm pro-
vides the ability to train all the fine-grained features,
a total of 34 feature weights, simultaneously.
Coarse-grained features As the basis for coarse-
grained syntactic features, we selected the following
nonterminal labels based on their frequency in the
tuning data, whether they frequently cover a span
of more than one word, and whether they repre-
sent linguistically relevant constituents: NP, PP, S,
VP, SBAR, ADJP, ADVP, and QP. We define two
new features, one which fires when a rule?s source
side span in the input sentence matches any of the
above-mentioned labels in the input parse, and an-
other which fires when a rule?s source side span
crosses a boundary of one of these labels (e.g., its
source side span only partially covers the words in
a VP subtree, and it also covers some or all or the
words outside the VP subtree). These two features
are equivalent to Marton and Resnik?s XP= and XP+
feature combinations, respectively.
228
Fine-grained features We selected the following
nonterminal labels that appear more than 100 times
in the tuning data: NP, PP, S, VP, SBAR, ADJP,
WHNP, PRT, ADVP, PRN, and QP. The labels that
were excluded were parts of speech, nonconstituent
labels like FRAG, or labels that occurred only two
or three times. For each of these labels X, we added
a separate feature that fires when a rule?s source side
span in the input sentence matches X, and a second
feature that fires when a span crosses a boundary of
X. These features are similar to Marton and Resnik?s
X= and X+, except that our set includes features for
WHNP, PRT, and PRN.
4 Structural distortion features
In addition to parser-based syntactic constraints,
which were introduced in prior work, we introduce
a completely new set of features aimed at improv-
ing the modeling of reordering within Hiero. Again,
the feature definition gives rise to a larger number of
features than one would expect to train successfully
using MERT.
In a phrase-based model, reordering is per-
formed both within phrase pairs and by the phrase-
reordering model. Both mechanisms are able to
learn that longer-distance reorderings are more
costly than shorter-distance reorderings: phrase
pairs, because phrases that involve more extreme re-
orderings will (presumably) have a lower count in
the data, and phrase reordering, because models are
usually explicitly dependent on distance.
By contrast, in a hierarchical model, all reordering
is performed by a single mechanism, the rules of the
grammar. In some cases, the model will be able to
learn a preference for shorter-distance reorderings,
as in a phrase-based system, but in the case of a word
being reordered across a nonterminal, or two non-
terminals being reordered, there is no dependence in
the model on the size of the nonterminal or nonter-
minals involved in reordering.
So, for example, if we have rules
X? (il dit X1, he said X1) (12)
X? (il dit X1,X1 he said) (13)
we might expect that rule (12) is more common in
general, but that rule (13) becomes more and more
?
?
?
?
?
?
?
?
Figure 2: Classifying nonterminal occurrences for the
structural distortion model.
rare as X1 gets larger. The default Hiero features
have no way to learn this.
To address this defect, we can classify every
nonterminal pair occurring on the right-hand side
of each grammar rule as ?reordered? or ?not re-
ordered?, that is, whether it intersects any other word
alignment link or nonterminal pair (see Figure 2).
We then define coarse- and fine-grained versions of
the structural distortion model.
Coarse-grained features Let R be a binary-
valued random variable that indicates whether a non-
terminal occurrence is reordered, and let S be an
integer-valued random variable that indicates how
many source words are spanned by the nonterminal
occurrence. We can estimate P(R | S ) via relative-
frequency estimation from the rules as they are ex-
tracted from the parallel text, and incorporate this
probability as a new feature of the model.
Fine-grained features A difficulty with the
coarse-grained reordering features is that the gram-
mar extraction process finds overlapping rules in the
training data and might not give a sensible proba-
bility estimate; moreover, reordering statistics from
the training data might not carry over perfectly into
the translation task (in particular, the training data
may have some very freely-reordering translations
that one might want to avoid replicating in transla-
tion). As an alternative, we introduce a fine-grained
version of our distortion model that can be trained
directly in the translation task as follows: define
229
a separate binary feature for each value of (R, S ),
where R is as above and S ? {?, 1, . . . , 9,?10} and ?
means any size. For example, if a nonterminal with
span 11 has its contents reordered, then the features
(true,?10) and (true, ?) would both fire. Grouping
all sizes of 10 or more into a single feature is de-
signed to avoid overfitting.
Again, using MIRA makes it practical to train
with the full fine-grained feature set?coincidentally
also a total of 34 features.
5 Experiment and results
We now describe our experiments to test MIRA and
our features, the soft-syntactic constraints and the
structural distortion features, on an Arabic-English
translation task. It is worth noting that this exper-
imentation is on a larger scale than Watanabe et
al.?s (2007), and considerably larger than Marton
and Resnik?s (2008).
5.1 Experimental setup
The baseline model was Hiero with the following
baseline features (Chiang, 2005; Chiang, 2007):
? two language models
? phrase translation probabilities p( f | e) and
p(e | f )
? lexical weighting in both directions (Koehn et
al., 2003)
? word penalty
? penalties for:
? automatically extracted rules
? identity rules (translating a word into it-
self)
? two classes of number/name translation
rules
? glue rules
The probability features are base-100 log-
probabilities.
The rules were extracted from all the allow-
able parallel text from the NIST 2008 evalua-
tion (152+175 million words of Arabic+English),
aligned by IBM Model 4 using GIZA++ (union of
both directions). Hierarchical rules were extracted
from the most in-domain corpora (4.2+5.4 million
words) and phrases were extracted from the remain-
der. We trained the coarse-grained distortion model
on 10,000 sentences of the training data.
Two language models were trained, one on data
similar to the English side of the parallel text and
one on 2 billion words of English. Both were 5-
gram models with modified Kneser-Ney smoothing,
lossily compressed using a perfect-hashing scheme
similar to that of Talbot and Brants (2008) but using
minimal perfect hashing (Botelho et al, 2005).
We partitioned the documents of the NIST 2004
(newswire) and 2005 Arabic-English evaluation data
into a tuning set (1178 sentences) and a develop-
ment set (1298 sentences). The test data was the
NIST 2006 Arabic-English evaluation data (NIST
part, newswire and newsgroups, 1529 sentences).
To obtain syntactic parses for this data, we tok-
enized it according to the Arabic Treebank standard
using AMIRA (Diab et al, 2004), parsed it with
the Stanford parser (Klein and Manning, 2003), and
then forced the trees back into the MT system?s tok-
enization.1
We ran both MERT and MIRA on the tuning
set using 20 parallel processors. We stopped MERT
when the score on the tuning set stopped increas-
ing, as is common practice, and for MIRA, we used
the development set to decide when to stop train-
ing.2 In our runs, MERT took an average of 9 passes
through the tuning set and MIRA took an average of
8 passes. (For comparison, Watanabe et al report de-
coding their tuning data of 663 sentences 80 times.)
5.2 Results
Table 1 shows the results of our experiments with
the training methods and features described above.
All significance testing was performed against the
first line (MERT baseline) using paired bootstrap re-
sampling (Koehn, 2004).
First of all, we find that MIRA is competitive with
MERT when both use the baseline feature set. In-
1The only notable consequence this had for our experimen-
tation is that proclitic Arabic prepositions were fused onto the
first word of their NP object, so that the PP and NP brackets
were coextensive.
2We chose this policy for MIRA to avoid overfitting. How-
ever, we could have used the tuning set for this purpose, just as
with MERT: in none of our runs would this change have made
more than a 0.2 B??? difference on the development set.
230
Dev NIST 06 (NIST part)
Train Features # nw nw ng nw+ng
MERT baseline 12 52.0 50.5 32.4 44.6
syntax (coarse) 14 52.2 50.9 33.0+ 45.0+
syntax (fine) 34 52.1 50.4 33.5++ 44.8
distortion (coarse) 13 52.3 51.3+ 34.3++ 45.8++
distortion (fine) 34 52.0 50.9 34.5++ 45.5++
MIRA baseline 12 52.0 49.8? 34.2++ 45.3++
syntax (fine) 34 53.1++ 51.3+ 34.5++ 46.4++
distortion (fine) 34 53.3++ 51.5++ 34.7++ 46.7++
distortion+syntax (fine) 56 53.6++ 52.0++ 35.0++ 47.2++
Table 1: Comparison of MERT and MIRA on various feature sets. Key: # = number of features; nw = newswire, ng =
newsgroups; + or ++ = significantly better than MERT baseline (p < 0.05 or p < 0.01, respectively), ? = significantly
worse than MERT baseline (p < 0.05).
deed, the MIRA system scores significantly higher
on the test set; but if we break the test set down by
genre, we see that the MIRA system does slightly
worse on newswire and better on newsgroups. (This
is largely attributable to the fact that the MIRA trans-
lations tend to be longer than the MERT transla-
tions, and the newsgroup references are also rela-
tively longer than the newswire references.)
When we add more features to the model, the two
training methods diverge more sharply. When train-
ing with MERT, the coarse-grained pair of syntax
features yields a small improvement, but the fine-
grained syntax features do not yield any further im-
provement. By contrast, when the fine-grained fea-
tures are trained using MIRA, they yield substan-
tial improvements. We observe similar behavior for
the structural distortion features: MERT is not able
to take advantage of the finer-grained features, but
MIRA is. Finally, using MIRA to combine both
classes of features, 56 in all, produces the largest im-
provement, 2.6 B??? points over the MERT baseline
on the full test set.
We also tested some of the differences between
our training method and Watanabe et al?s (2007); the
results are shown in Table 2. Compared with local
updating (line 2), our method of selecting the ora-
cle translation and negative examples does better by
0.5 B??? points on the development data. Using loss-
augmented inference to add negative examples to lo-
cal updating (line 3) does not appear to help. Never-
theless, the negative examples are important: for if
Setting Dev
full 53.6
local updating, no LAI 53.1?
local updating, LAI 53.0??
? = 0.5 oracle, no LAI failed
no sharing of updates 53.1??
Table 2: Effect of removing various improvements in
learning method. Key: ? or ?? = significantly worse than
full system (p < 0.05 or p < 0.01, respectively); LAI =
loss-augmented inference for additional negative exam-
ples.
we use our method for selecting the oracle transla-
tion without the additional negative examples (line
4), the algorithm fails, generating very long transla-
tions and unable to find a weight setting to shorten
them. It appears, then, that the additional negative
examples enable the algorithm to reliably learn from
the enhanced oracle translations.
Finally, we compared our parallelization method
against a simpler method in which all processors
learn independently and their weight vectors are all
averaged together (line 5). We see that sharing in-
formation among the processors makes a significant
difference.
6 Conclusions
In this paper, we have brought together two existing
lines of work: the training method of Watanabe et al
(2007), and the models of Chiang (2005) and Marton
231
and Resnik (2008). Watanabe et al?s work showed
that large-margin training with MIRA can be made
feasible for state-of-the-art MT systems by using a
manageable tuning set; we have demonstrated that
parallel processing and exploiting more of the parse
forest improves MIRA?s performance and that, even
using the same set of features, MIRA?s performance
compares favorably to MERT in terms of both trans-
lation quality and computational cost.
Marton and Resnik (2008) showed that it is pos-
sible to improve translation in a data-driven frame-
work by incorporating source-side syntactic analy-
sis in the form of soft syntactic constraints. This
work joins a growing body of work demonstrating
the utility of syntactic information in statistical MT.
In the area of source-side syntax, recent research
has continued to improve tree-to-string translation
models, soften the constraints of the input tree in
various ways (Mi et al, 2008; Zhang et al, 2008),
and extend phrase-based translation with source-
side soft syntactic constraints (Cherry, 2008). All
this work shows strong promise, but Marton and
Resnik?s soft syntactic constraint approach is par-
ticularly appealing because it can be used unobtru-
sively with any hierarchically-structured translation
model. Here, we have shown that using MIRA to
weight all the constraints at once removes the cru-
cial drawback of the approach, the problem of fea-
ture selection.
Finally, we have introduced novel structural dis-
tortion features to fill a notable gap in the hierar-
chical phrase-based approach. By capturing how re-
ordering depends on constituent length, these fea-
tures improve translation quality significantly. In
sum, we have shown that removing the bottleneck
of MERT opens the door to many possibilities for
better translation.
Acknowledgments
Thanks to Michael Bloodgood for performing ini-
tial simulations of parallelized perceptron training.
Thanks also to John DeNero, Kevin Knight, Daniel
Marcu, and Fei Sha for valuable discussions and
suggestions. This research was supported in part by
DARPA contract HR0011-06-C-0022 under subcon-
tract to BBN Technologies and HR0011-06-02-001
under subcontract to IBM.
References
Abhishek Arun and Philipp Koehn. 2007. Online
learning methods for discriminative training of phrase
based statistical machine translation. In Proc. MT
Summit XI.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A
discriminative latent variable model for statistical ma-
chine translation. In Proc. ACL-08: HLT.
Fabiano C. Botelho, Yoshiharu Kohayakawa, and Nivio
Ziviani. 2005. A practical minimal perfect hashing
method. In 4th International Workshop on Efficient
and Experimental Algorithms (WEA05).
Daniel Cer, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Regularization and search for minimum
error rate training. In Proc. Third Workshop on Statis-
tical Machine Translation.
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proc. ACL-08: HLT.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL 2005.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models: Theory and experiments
with perceptron algorithms. In Proc. EMNLP 2002.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of Arabic text: From raw text to
base phrase chunks. In Proc. HLT/NAACL 2004.
Companion volume.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing reordering constraints for SMT us-
ing efficient B??? oracle computation. In Proc. 2007
Workshop on Syntax and Structure in Statistical Trans-
lation.
Kevin Duh and Katrin Kirchoff. 2008. Beyond log-linear
models: Boosted minimum error rate training for n-
best re-ranking. In Proc. ACL-08: HLT, Short Papers.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37:277?296.
Dan Klein and Chris D. Manning. 2003. Fast exact infer-
ence with a factored model for natural language pars-
ing. In Advances in Neural Information Processing
Systems 15 (NIPS 2002).
232
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proc. HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2008. Complexity of finding the BLEU-optimal hy-
pothesis in a confusion network. In Proc. EMNLP
2008. This volume.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. COLING-ACL
2006.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation metrics
for machine translation. In Proc. COLING 2004.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proc. ACL-08: HLT.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proc. ACL 2005.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. ACL-08: HLT.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. ACL 2002.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
John C. Platt. 1998. Fast training of support vector
machines using sequential minimal optimization. In
Bernhard Scho?lkopf, Christopher J. C. Burges, and
Alexander J. Smola, editors, Advances in Kernel Meth-
ods: Support Vector Learning, pages 195?208. MIT
Press.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In
Proc. COLING/ACL 2006, Poster Sessions.
David Talbot and Thorsten Brants. 2008. Random-
ized language models via perfect hash functions. In
Proc. ACL-08: HLT.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher Manning. 2004. Max-margin pars-
ing. In Proc. EMNLP 2004, pages 1?8.
Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: A large margin approach. In Proc. ICML
2005.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
Proc. COLING-ACL 2006.
Joseph Turian, Benjamin Wellington, and I. Dan
Melamed. 2007. Scalable discriminative learning for
natural language parsing and translation. In Advances
in Neural Information Processing Systems 19 (NIPS
2006).
Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. EMNLP 2007.
Dekai Wu. 1996. A polynomial-time algorithm for
statistical machine translation. In Proc. 34th Annual
Meeting of the Association for Computational Linguis-
tics, pages 152?158.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL-08: HLT.
233
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 381?390,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Improved Statistical Machine Translation
Using Monolingually-Derived Paraphrases
Yuval Marton,
?
Chris Callison-Burch,
?
and Philip Resnik
?
?
Department of Linguistics and the CLIP Lab
at the Institute for Advanced Computer Studies (UMIACS)
University of Maryland College Park, MD 20742-7505, USA
{ymarton,resnik}@umiacs.umd.edu
?
Computer Science Department, Johns Hopkins University
3400 N. Charles Street (CSEB 226-B) Baltimore, MD 21218
ccb@cs.jhu.edu
Abstract
Untranslated words still constitute a ma-
jor problem for Statistical Machine Trans-
lation (SMT), and current SMT systems
are limited by the quantity of parallel
training texts. Augmenting the training
data with paraphrases generated by pivot-
ing through other languages alleviates this
problem, especially for the so-called ?low
density? languages. But pivoting requires
additional parallel texts. We address this
problem by deriving paraphrases monolin-
gually, using distributional semantic simi-
larity measures, thus providing access to
larger training resources, such as compa-
rable and unrelated monolingual corpora.
We present what is to our knowledge the
first successful integration of a colloca-
tional approach to untranslated words with
an end-to-end, state of the art SMT sys-
tem demonstrating significant translation
improvements in a low-resource setting.
1 Introduction
Phrase-based systems, flat and hierarchical alike
(Koehn et al, 2003; Koehn, 2004b; Koehn et al,
2007; Chiang, 2005; Chiang, 2007), have achieved
a much better translation coverage than word-
based ones (Brown et al, 1993), but untranslated
words remain a major problem in SMT. For ex-
ample, according to Callison-Burch et al (2006),
a SMT system with a training corpus of 10,000
words learned only 10% of the vocabulary; the
same system learned about 30% with a training
corpus of 100,000 words; and even with a large
training corpus of nearly 10,000,000 words it only
reached about 90% coverage of the source vocab-
ulary. Coverage of higher order n-gram levels is
even harder. This problem plays a major part in re-
ducing machine translation quality, as reflected by
both automatic measures such as BLEU (Papineni
et al, 2002) and human judgment tests. Improving
translation coverage accurately is therefore impor-
tant for SMT systems.
The first solution that might come to mind is
to use larger parallel training corpora. However,
current state-of-the-art SMT systems cannot learn
from non-aligned corpora, while sentence-aligned
parallel corpora (bitexts) are a limited resource
(See Section 2 for discussion of automatically-
compiled bitexts). Another direction might be
to make use of non-parallel corpora for training.
However, this requires developing techniques to
extract alignments or translations from them, and
in a sufficiently fast, memory-efficient, and scal-
able manner. One approach that can, in princi-
ple, better exploit both alignments from bitexts
and make use of non-parallel corpora is the dis-
tributional collocational approach, e.g., as used by
Fung and Yee (1998) and Rapp (1999). However,
the systems described there are not easily scalable,
and require pre-computation of a very large col-
location counts matrix. Related attempts propose
generating bitexts from comparable and ?quasi-
comparable? bilingual texts by iteratively boot-
strapping documents, sentences, and words (Fung
and Cheung, 2004), or by using a maximum
entropy classifier (Munteanu and Marcu, 2005).
Alignment accuracy remains a challenge for them.
Recent work has proposed augmenting the
training data with paraphrases generated by pivot-
ing through other languages (Callison-Burch et al,
2006; Madnani et al, 2007). This indeed allevi-
ates the vocabulary coverage problem, especially
for the so-called ?low density? languages. How-
ever, these approaches still require bitexts where
381
one side contains the original source language.
The paradigm described in this paper involves
constructing monolingual distributional profiles
(DPs; a.k.a. word association profiles, or co-
occurrence vectors) of out-of-vocabulary words
and phrases in the source language; then, gener-
ating paraphrase candidates from phrases that co-
occur in similar contexts, and assigning them sim-
ilarity scores. The highest ranking paraphrases
are used to augment the translation phrase table.
The table augmentation idea is similar to Callison-
Burch et al?s (Callison-Burch et al, 2006), but
our proposed paradigm does not require using a
limited resource such as parallel texts in order
to generate paraphrases. Moreover, our proposed
paradigm can, in principle, achieve large-scale ac-
quisition of paraphrases with high semantic simi-
larity. However, using parallel training texts in
pivoting techniques offers the potential advantage
of implicit translational knowledge, in the form
of sentence alignments, while our approach is un-
guided in this respect. Therefore, we conducted
experiments to find out how these relative advan-
tages play out. We present here, to our knowledge
for the first time, positive results of integrating dis-
tributional monolingually-derived paraphrases in
an end-to-end state-of-the-art SMT system.
In the rest of this paper we discuss related work
in Section 2, describe the distributional hypothesis
and distributional profiles in Section 3, and present
the monolingually-derived paraphrase generation
system in Section 4. We report our experiments
and results in Section 5, and conclude by dis-
cussing the implications and future research direc-
tions in Section 6.
2 Related Work
This is not the first to attempt to ameliorate the
out-of-vocabulary (OOV) words problem in sta-
tistical machine translation, and other natural lan-
guage processing tasks. This work is most closely
related to that of Callison-Burch et al (2006),
who also translate source-side paraphrases of the
OOV phrases. There, paraphrases are generated
from bitexts of various language pairs, by ?pivot-
ing?: translating the OOV phrases to an additional
language (or languages) and back to the source
language. The quality of these paraphrases is es-
timated by marginalizing translation probabilities
to and from the additional language side(s) e, as
follows: p(f
2
|f
1
) =
?
e
p(e|f
1
)p(f
2
|e). A ma-
jor disadvantage of their approach is that it relies
on the availability of parallel corpora in other lan-
guages. While this works for English and many
European languages, it is far less likely to help
when translating from other source languages, for
which bitexts are scarce or non-existent. Also,
the pivoting approach is inherently noisy (in both
the paraphrase candidates? correct sense, and their
translational likelihood), and it is likely to fare
poorly with out-of-domain translation. One ad-
vantage of the bitext-dependent pivoting approach
is the use of the additional human knowledge that
is encapsulated in the parallel sentence alignment.
However, we argue that the ability to use much
larger resources for paraphrasing should trump the
human knowledge advantage.
More recently, Callison-Burch (2008) has im-
proved performance of this pivoting technique by
imposing syntactic constraints on the paraphrases.
The limitation of such an approach is the reliance
on a good parser (in addition to reliance on bi-
texts), but a good parser is not available in all
languages, especially not in resource-poor lan-
guages. Another approach using a pivoting tech-
nique augments the human reference translation
with paraphrases, creating additional translation
?references? (Madnani et al, 2007). Both ap-
proaches have shown gains in BLEU score.
Barzilay and McKeown (2001) extract para-
phrases from a monolingual parallel corpus, con-
taining multiple translations of the same source.
In addition to the parallel corpus usage limitations
described above, this technique is further limited
by the small size of such materials, which are even
scarcer than the resources in the pivoting case.
Dolan et al (2004) explore generating para-
phrases by edit-distance and headlines of time-
and topic-clustered news articles; they do not ad-
dress the OOV problem directly, as their focus
is sentence-level paraphrases; although they use
a standard SMT measure, alignment error rate
(AER), they only report results of the alignment
quality, and not of an end-to-end SMT system.
Much of the previous research largely focused on
morphological analysis in order to reduce type
sparseness; Callison-Burch et al (2006) list some
of the influential work in that direction.
Work that relies on the distributional hypoth-
esis using bilingual comparable corpora (with-
out the need for bitexts), typically uses a seed
lexicon for ?bridging? source language phrases
382
with their target languages paraphrases (Fung and
Yee, 1998; Rapp, 1999; Diab and Finch, 2000).
This approach is sometimes viewed as, or com-
bined with, an information retrieval (IR) approach,
and normalizes strength-of-association measures
(see Section 3) with IR-related measures such as
TF/IDF (Fung and Yee, 1998). To date, reported
implementations suffer from scalability issues, as
they pre-compute and hold in memory a huge col-
location matrix; we know of no report of using this
approach in an end-to-end SMT system.
Another approach aiming to reduce OOV rate
concentrates on increasing parallel training set
size without using more dedicated human transla-
tion (Resnik and Smith, 2003; Oard et al, 2003).
3 Collocational Profiles
The distributional hypothesis and distribu-
tional profiles. Natural language processing
(NLP) applications that assume the distributional
hypothesis (Harris, 1940; Firth, 1957) typically
keep track of word co-occurrences in distribu-
tional profiles (a.k.a. collocation vectors, or con-
text vectors). Each distributional profile DP
u
(for some word u) keeps counts of co-occurrence
of u with all words within a usually fixed dis-
tance from each of its occurrences (a sliding win-
dow) in some training corpus. More advanced pro-
files keep ?strength of association? (SoA) infor-
mation between u and each of the co-occurring
words, which is calculated from the counts of u,
the counts of the other word, their co-occurrence
count, and the count of all words in the corpus
(corpus size). The information on the other words
with respect to u is typically kept in a vector whose
dimensions correspond to all words in the training
corpus. This is described in Equation (1), where
V is the training corpus vocabulary:
DP
u
= {< w
i
, SoA(u,w
i
) > |u,w
i
? V }
for all i s.t. 1 ? i ? |V |
(1)
Semantic similarity between words u and v can
be estimated by calculating the similarity (vector
distance) between their profiles. Slightly more for-
mally, the distributional hypothesis assumes that
if we had access to the hypothetical true (psycho-
linguistic) semantic similarity function over word
pairs, semsim(u, v), then
?u, v, w ? V,
[semsim(u, v) > semsim(u,w)] =?
[psim(DP
u
, DP
v
) > psim(DP
u
, DP
w
)],
(2)
where V is the language vocabulary, DP
word
is
the distributional profile of word, and psim() is
a 2-place vector similarity function (all further
described below). Paraphrasing and other NLP
applications that are based on the distributional
hypothesis assume entailment in the reverse di-
rection: the right-hand-side of Formula (2) (pro-
file/vector similarity) entails the left-hand-side
(semantic similarity).
The sliding window and word association (SoA)
measures. Some researchers count positional
collocations in a sliding window, i.e., the co-
counts and SoA measures are calculated per rel-
ative position (e.g., for some word/token u, po-
sition 1 is the token immediately after u; posi-
tion -2 is the token preceding the token that pre-
cedes u) (Rapp, 1999); other researchers use non-
positional (which we dub here flat) collocations,
meaning, they count all token occurrences within
the sliding window, regardless of their positions
in it relative to u (McDonald, 2000; Mohammad
and Hirst, 2006). We use here flat collocations
in a 6-token sliding window. Beside simple co-
occurrence counts within sliding windows, other
SoA measures include functions based on TF/IDF
(Fung and Yee, 1998), mutual information (PMI)
(Lin, 1998), conditional probabilities (Schuetze
and Pedersen, 1997), chi-square test, and the log-
likelihood ratio (Dunning, 1993).
Profile similarity measures. A profile similar-
ity function psim(DP
u
, DP
v
) is typically defined
as a two-place function, taking vectors as argu-
ments, each vector representing a distributional
profile of some word u and v, respectively, and
whose cells contain the SoA of u (or v) with each
word (?collocate?) in the known vocabulary. Sim-
ilarity can be (and have been) estimated in several
ways, e.g., the cosine coefficient, the Jaccard co-
efficient, the Dice coefficient, and the City-Block
measure. The formula for the cosine function for
similarity measure is given in Eq. (3):
383
psim(DP
u
, DP
v
)
= cos(DP
u
, DP
v
)
=
?
w
i
?V
SoA(u,w
i
)SoA(v, w
i
)
?
?
w
i
?V
SoA(u,w
i
)
2
?
?
w
i
?V
SoA(v, w
i
)
2
(3)
In principle, any SoA can be used with any
profile similarity measure. However, in practice,
only some SoA/similarity measure combinations
do well, and finding the best combination is still
more art than science. Some successful combina-
tions are cos
CP
(Schuetze and Pedersen, 1997),
Lin
PMI
(Lin, 1998), City
LL
(Rapp, 1999), and
Jensen?Shannon divergence of conditional prob-
abilities (JSD
CP
). We use here cosine of log-
likelihood vectors (McDonald, 2000).
Phrasal distributional profiles. Word DPs can
be generalized to phrasal DPs, simply by count-
ing words that co-occur within a sliding window
around the target phrase?s occurrences (i.e., count-
ing occurrences of words up to 6 words before
or after the target phrase). For example, when
building a DP for the target phrase counting words
in the previous sentence, then simply is in rela-
tive position -2, and sliding is in relative posi-
tion 5. Searching for similar phrasal DPs poses
an additional challenge over the word DP case
(see Section 4), but there is no additional diffi-
culty in building the phrasal profile itself as de-
scribed above. In preliminary experiments we
found no gain in using phrasal collocates (i.e.,
count how many times a phrase of more than one
word co-occurs in a sliding window around the tar-
get word/phrase).
4 Searching and Scoring Phrasal
Paraphrases
The system design is as follows: upon receiv-
ing OOV phrase phr, build distributional profile
DP
phr
. Next, gather contexts: for each occur-
rence of phr, keep surrounding (left and right)
context L__R. For each such context, gather para-
phrase candidates X which occur between L and
R in other locations in the training corpus, i.e.,
all X such that LXR occur in the corpus. Fi-
nally, rank all candidates X , by building distribu-
tional profile DP
X
and measuring profile similar-
ity between DP
X
and DP
phr
, for each X . Output
k-best candidates above a certain similarity score
threshold. The rest of this section describes this
system in more detail.
Build phrasal profile DP
phr
. Build a profile of
all word collocates, as described in Section 3. Use
sliding window of size MaxPos = 6. If phr
is very frequent (above some threshold of t oc-
currences), uniformly sample only t occurrences,
multiplying the gathered co-counts by factor of
count(phr)/t. We set t = 10000.
Gather context. The challenge in choosing the
relevant context is this: if it is very short and/or
very frequent (e.g., ?the __ is?), then it might not
be very informative, in the sense that many words
can appear in that context (in this example, practi-
cally any noun); however, if it is too long (too spe-
cific), then it might not occur enough times else-
where (or not at all) in the training corpus. There-
fore, to balance between these two extremes, we
use the following heuristics. Start small: Start
with setting the left part of the context L to be a
single word/token to the left of phrase phr. If it
is stoplisted, append the next word to the left (now
having a bigram left context instead of a unigram),
and repeat until the left context is not in the sto-
plist. Repeat similarly for R, the context to the
right of phr. Add the resulting L__R context to
a context list. We stoplist ?promiscuous? words,
i.e., those that have more than StoplistThreshold
collocates in the training corpus, using the above
MaxPos parameter value. We also stoplist bi-
grams which occur more than t times and com-
prise solely from stoplisted unigrams.
Gather candidates. For each gathered context
in the context list, gather all paraphrase candidate
phrases X that connect left hand side context L
with right hand side context R, i.e., gather all X
such that the sequence LXR occurs in the corpus.
In practice, to keep search complexity low, limit
X to be up to length MaxPhraseLen. Also, to
further speed up runtime, we uniformly sample the
context occurrences.
Rank candidates. For each candidate X ,
build distributional profile DP
X
, and evaluate
psim(DP
phr
, DP
X
).
Output k-best candidates. Output k-best para-
phrase candidates for phrase phr, in descending
order of similarity. We set k = 20. Filter out para-
phrases with score less than minScore.
384
5 Experiment
We examined the application of the system?s para-
phrases to handling unknown phrases when trans-
lating from English into Chinese (E2C) and from
Spanish into English (S2E). For all baselines we
used the phrase-based statistical machine transla-
tion system Moses (Koehn et al, 2007), with the
default model features, weighted in a log-linear
framework (Och and Ney, 2002). Feature weights
were set with minimum error rate training (Och,
2003) on a development set using BLEU (Papineni
et al, 2002) as the objective function. Test re-
sults were evaluated using BLEU and TER (Snover
et al, 2005). The phrase translation probabili-
ties were determined using maximum likelihood
estimation over phrases induced from word-level
alignments produced by performing Giza++ train-
ing (Och and Ney, 2000) on both source and tar-
get sides of the parallel training sets. When the
baseline system encountered unknown words in
the test set, its behavior was simply to reproduce
the foreign word in the translated output.
The paraphrase-augmented systems were iden-
tical to the corresponding baseline system, with
the exception of additional (paraphrase-based)
translation rules, and additional feature(s). Simi-
larly to Callison-Burch et al (2006), we added the
following feature:
h(e, f) =
8
>
>
>
<
>
>
>
:
psim(DP
f
?
, DP
f
) If phrase table entry (e, f)
is generated from (e, f
?
)
using monolingually-
derived paraphrases.
1 Otherwise,
(4)
Note that it is possible to construct a new trans-
lation rule from f to e via more than one pair of
source-side phrase and its paraphrase; e.g., if f
1
is a paraphrase of f , and so is f
2
, and both f
1
, f
2
translate to the same e, then both lead to the con-
struction of the new rule translating f to e, but
with potentially different feature scores.
In order to eliminate this duplicity and lever-
age over these alternate paths which can be used
to increase our confidence level in the new rule,
we did the following: For each paraphrase f
of some source-side phrases f
i
, with respec-
tive similarity scores sim(f
i
, f), we calculated
an aggregate score asim with a ?quasi-online-
updating? method as follows: asim
i
= (1 ?
asim
i?1
)sim(f
i
, f), where asim
0
= 0. The ag-
gregate score asim is updated in an ?online? fash-
ion with each pair f
i
, f as they are processed, but
only the final asim
k
score is used, after all k pairs
have been processed. Simple arithmetics can show
that this method is insensitive to the order in which
the paraphrases are processed. We only augment
the phrase table with a single rule from f to e,
and in it are the feature values of the phrase f
i
for
which the score sim(f
i
, f) was the highest.
5.1 English-to-Chinese Translation
For the English-Chinese (E2C) baseline system,
we trained on the LCD Sinorama and FBIS
tests (LCD2005T10 and LCD2003E14), and seg-
mented the Chinese side with the Stanford Seg-
menter (Tseng et al, 2005). After tokenization
and filtering, this bitext contained 231,586 lines
(6.4M + 5.1M tokens). We trained a trigram lan-
guage model on the Chinese side. We then split the
bitext to 32 even slices, and constructed a reduced
set of about 29,000 lines (sentences) by using only
every eighth slice. The purpose of creating this
subset model was to simulate a resource-poor lan-
guage. See Table 1.
Set # Tokens Source+Target
E2C 29K 0.8 + 0.6
E2C Full 6.4 + 5.1
bnc+apw 187
S2E 10K 0.3 + 0.3
S2E 20K 0.6 + 0.6
S2E 80K 2.3 + 2.3
wmt09 84
wmt09+acquis 139
wmt09+acquis+afp 402
Table 1: Training set sizes (million tokens).
For development, we used the Chinese-English
NIST MT 2005 evaluation set, taking one of the
English references as source, and the Chinese
source as a single reference translation. We tested
the system using the English-Chinese NIST MT
evaluation 2008 test set with its four reference
translations.
We augmented the E2C baseline models with
paraphrases generated as described above, train-
ing on the British National Corpus (BNC)
v3 (Burnard, 2000) and the first 3 million lines
of the English Gigaword v2 APW, totaling 187M
terms after tokenization, and number and punc-
tuation removal. We generated paraphrases for
phrases up to six tokens in length, and used an ar-
385
bitrary similarity threshold of minScore = 0.3.
We experimented with three variants: adding a
single additional feature for all paraphrases (1-
6grams); using only paraphrases of unigrams
(1grams); and adding two features, one only sen-
sitive to unigrams, and the other only to the rest
(1 + 2-6grams). All features had the same de-
sign as described in Section 5, each had an asso-
ciated weight (as all other features), and all fea-
ture weights in each system, including the base-
line, were tuned using a separate minimum error
rate training for each system.
Results are shown in Table 2. For the E2C sys-
tems, for which we had four reference translations
for the test set, we used shortest reference length,
and used the NIST-provided script to split the out-
put words to Chinese characters before evaluation.
Statistical significance for the BLEU results were
calculated using Koehn?s (Koehn, 2004) pair-wise
bootstrapping test with 95% confidence interval.
On the E2C 29,000-line subset, the augmented
system had a significant 1.7 BLEU points gain over
its baseline. On the full size model, results were
negative. Note that our E2C full size baseline
is reasonably strong: Its character-based BLEU
score is slightly higher than the JHU-UMD sys-
tem that participated in the NIST 2008 MT evalua-
tion (constrained training track), although we used
a subset of that system?s training materials, and
a smaller language model. Results there ranged
from 15.69 to 30.38 BLEU (ignoring a seeming
outlier of 3.93).
5.2 Spanish-to-English Translation
In order to to permit a more direct comparison
with the pivoting technique, we also experimented
with Spanish to English (S2E) translation, fol-
lowing Callison-Burch et al (2006). For base-
line we used the Spanish and English sides of
the Europarl multilingual parallel corpus (Koehn,
2005), with the standard training, development,
and test sets. We created training subset models
of 10,000, 20,000, and 80,000 aligned sentences,
as described in Callison-Burch et al (2006). For
better comparison with their pivoting system, we
used the same 5-gram language model, develop-
ment and test sets: For development, we used the
Europarl dev2006 Spanish and English sides, and
for testing we used the Europarl 2006 test set.
We trained the Spanish paraphrase generation
system on the Spanish corpora available from
dataset E2C model BLEU TER
29k baseline 15.21 90.354
29k 1grams 16.87*** 90.370
29k 1-6grams 16.54*** 90.376
29k 1 + 2-6grams 16.88*** 90.349
Full baseline 22.17 90.398
Full 1grams 21.64*** 90.459
Full 1-6grams 21.75 90.421
Full 1 + 2-6grams 21.39*** 90.433
Table 2: E2C Results: character-based BLEU and
TER scores. All models have one additional fea-
ture over baseline, except for the "1 + 2-6" mod-
els that have one feature for unigrams and an-
other feature for bigrams to 6-grams. Paraphrases
with score < .3 were filtered out. *** = sig-
nificance test over baseline with p < 0.0001,
using Koehn?s (2004) pair-wise bootstrap resam-
pling test for BLEU with 95% confidence interval.
Paraphrase Score
Source: deal
agreement 0.56
accord 0.53
talks 0.45
contract 0.42
peace deal 0.33
merger 0.32
agreement is 0.30
Source: fall
rise 0.87
slip 0.82
tumbled today 0.68
fell today 0.67
tumble 0.65
fall tokyo ap stock prices fell 0.56
are mixed 0.54
Source: to provide any other
to give any 0.74
to give further 0.70
to provide any 0.68
to give any other 0.62
to provide further 0.61
to provide other 0.53
to reveal any 0.52
to provide any further 0.48
to disclose any 0.47
to publicly discuss the 0.43
Source: we have a situation that
uncontroversial question about our 0.66
obviously with the developments this morning 0.65
community staffing of community centres 0.64
perhaps we are getting rather impatient 0.63
er around the inner edge 0.60
interested in going to the topics 0.60
and that is the day that 0.60
as a as a final point 0.59
left which it may still have 0.56
Table 3: English paraphrases from E2C 29K-
bitext systems.
386
the EACL 2009 Fourth Workshop on Statistical
Machine Translation:
1
the Spanish side of the
Europarl-v4, news training 2008, and news com-
mentary 2009. We also re-trained adding the JRC-
Acquis-v3 corpus
2
to the paraphrase training set,
and then adding also the LDC Spanish Gigaword
(LDC2006T12) and truncating the resulting cor-
pus after the first 150M lines. We lowercased
these training sets, tokenized and removed punc-
tuation marks and numbers, and this resulted in
training set sizes as detailed in Table 1. We gen-
erated paraphrases for phrases up to four tokens
in length, and used two arbitrary similarity thresh-
olds of minScore = 0.3 (as in the E2C experi-
ments), and 0.6, for enforcing only higher preci-
sion paraphrasing.
We experimented with these variants: a single
feature for all paraphrase (1-4grams); using only
paraphrases of unigrams (1grams); and using two
features: one only sensitive to unigrams and bi-
grams, and the other to the rest (1-2 + 3-4grams).
Results are shown in Table 4. We used BLEU
over lowercased outputs to evaluate all S2E sys-
tems, and Koehn?s significance test as above.
On the S2E 10,000-line subset, both the 1grams
and 1-4grams models achieved significant gains of
.4 BLEU points over the baseline. We concluded
from a manual evaluation of the 10,000-line mod-
els that the two major weaknesses of the baseline
system were (not surprisingly) number of untrans-
lated (OOV) words / phrases, followed by number
of superfluous words / phrases.
On the larger subset models, no system sig-
nificantly outperformed the baseline. Note that
our S2E baselines? scores are higher than those
of Callison-Burch et al (2006), since we evaluate
lowercased outputs, instead of recased ones.
6 Discussion and Future Work
We have shown that monolingually-derived para-
phrases, based on distributional semantic similar-
ity measures over a source-language corpus, can
improve the performance of statistical machine
translation (SMT) systems. Our proposed method
has the advantage of not relying on bitexts in order
to generate the paraphrases, and therefore gives
access to large amounts of monolingual training
data, for which creating bitexts of equivalent size
is generally unfeasible. We haven?t trained our
1
http://www.statmt.org/wmt09
2
http://wt.jrc.it/lt/Acquis
system on nearly as large a corpus as it can han-
dle, and indeed we see this as a natural next step.
Results support the assumption that a larger
monolingual paraphrase training set yields bet-
ter paraphrases: our S2E 1-4grams model per-
formed significantly better than baseline when us-
ing wmt09+aquis for paraphrasing, but when only
using wmt09, the model had a smaller advantage
that did not reach significance. However, for the
S2E 1grams model, there was a slight decrease in
performance when switching paraphrasing corpus
from wmt09+aquis to wmt09+aquis+afp. This ef-
fect might be due to the genre or unbalanced con-
tent of the additional corpus, or perhaps it is the
case that in this corpus size, paraphrases of higher-
level ngrams benefitted from the additional text
much more than paraphrases of unigrams did. The
two rightmost columns in Table 5 show that al-
though Spanish monolingual paraphrases for the
unigram baile improve when using the larger cor-
pus, (e.g., danza and un balie become the third and
fourth top candidates, pushing much worse candi-
dates far down the list), the two top paraphrase
candidates remained unchanged. However, for
the 4gram a favor del informe, antonymous can-
didates, which are bad and misleading for trans-
lation, are pushed down from the top first and
third spots by synonymous, better candidates. Ta-
ble 3 contains additional examples of good and
bad top paraphrase candidates, also in English.
Paraphrases of phrases seem to be of lower qual-
ity than those of unigrams, as can be seen at the
bottom of the table.
These results also show that our method is es-
pecially useful in settings involving low-density
languages or special domains: The smaller sub-
set models, emulating a resource-poor language
situation, show higher gains than larger models
(which are supersets of the smaller subset models),
when augmented with paraphrases derived from
the same paraphrase training set. This was vali-
dated in two very different language pairs: English
to Chinese, and Spanish to English. We believe
that larger monolingual training sets for paraphras-
ing can help languages with richer resources, and
we intend to explore this too.
Although the gains in the Spanish-English sub-
sets are somewhat smaller than the pivoting tech-
nique reported in Callison-Burch et al (2006),
e.g., .7 BLEU for the 10k subset, we take these
results as a proof of concept that can yield better
387
bitext mono.corp. features minScore BLEU TER
10k (baseline) ? ? 23.78 62.382
10k wmt09 1-4grams .6 23.81
10k wmt09 1-2+3-4gr .6 23.92 62.202
10k wmt09+aquis 1-4grams .6 24.13*** 61.739
10k wmt09+aquis 1grams .6 24.11 61.979
20k (baseline) ? ? 24.68 62.333
20k wmt09+aquis 1-4grams .6 24.75 61.528
80k (baseline) ? ? 27.89 57.977
80k wmt09+aquis 1-4grams .6 27.82 57.906
10k wmt09+aquis 1grams .3 24.11 61.979
10k wmt09+aquis+afp 1grams .3 23.97 61.974
20k wmt09+aquis+afp 1grams .3 24.77 61.276
80k wmt09+aquis+afp 1grams .3 27.84*** 57.781
Table 4: S2E Results: Lowercase BLEU and TER. Paraphrases with score < minScore were filtered out.
*** = significance test over baseline with p < 0.0001, using Koehn?s (2004) pair-wise bootstrap test for
BLEU with 95% confidence interval.
pivot wmt09+acquis wmt09+acquis+afp
Source: baile
danza el baile el baile
bailar baile y baile y
a de david palomar y la danza
dans viejo como quien se acomoda una un baile
empresa por juli?n estrada el tercero de teatro
coro al baile a la baloncesto el cine
Source: a favor del informe
a favor de este informe en contra del informe favor del informe
favor del informe a favor de este informe en contra del informe
el informe en contra de este informe a favor de este informe
a favor a favor de la resoluci?n en contra de este informe
por el informe a favor de esta resoluci?n en contra de la resoluci?n
al informe a favor del informe del se?or a favor del informe del sr.
su a favor del informe del sr. en contra del informe del sr.
del informe en contra de la propuesta a favor del excelente informe
de este informe contra el informe a favor del informe deprez
Table 5: Comparison of Spanish paraphrases: by pivoting, and by two monolingual corpora. Ordered
from best to worst score.
system example
source cuando escucho las distintas intervenciones , creo que quienes afirman que deber?amos analizar
nuestras prioridades y limitar el n?mero de objetivos que queremos conseguir , est?n en lo cierto .
reference when i listen to the various comments made , i find myself agreeing with those who recommend
that we take a look at our priorities and then limit the number of aims we want to achieve
baseline escucho when the various speeches, i believe that those who afirman that we should our
environmental limitar priorities and the number of objectives we want to achieve, are in this way.
pivoting (MW) when i can hear the various speeches , i believe that those people that we should look at our
priorities and to limit the number of objectives we want to achieve , are in fact .
wmt09+acquis escucho when the various speeches, i believe that those who claiming that we should environmental
.1-4grams limitar our priorities and the number of objectives we want to achieve, are on the way.
wmt09+acquis escucho when the various speeches, i believe that those who considered that we should our
.1grams environmental priorities and reducing the number of objectives we want to achieve, are on the way.
wmt09+acquis+afp escucho when the various speeches, i believe that those who say that we should our environmental
.1grams priorities and reduce the number of objectives we want to achieve, are on the way.
Table 6: S2E translation examples on 10k-bitext systems. Some translation differences are in bold.
388
gains with larger monolingual training sets. Pivot-
ing techniques (translating back and forth) rely on
limited resources (bitexts), and are subject to shifts
in meaning due to their inherent double transla-
tion step. In contrast, large monolingual resources
are relatively easy to collect, and our system in-
volves only a single translation/paraphrasing step
per target phrase. Table 5 also shows an exemplar
comparison with the pivoting paraphrases used in
Callison-Burch et al (2006). It seems that the piv-
oting paraphrases might suffer more from having
frequent function words as top candidates, which
might be a by-product of their alignment ?promis-
cuity?. However, the top antonymous candidate
problem seems to mainly plague the monolin-
gual distributional paraphrases (but improves with
larger corpora). See also Table 6.
The paraphrase quality remains an issue with
this method (as with all other paraphrasing meth-
ods). Some possible ways of improving it, be-
sides using larger corpora, are: using syntactic in-
formation (Callison-Burch, 2008), using semantic
knowledge such as thesaurus or WordNet to per-
form word sense disambiguation (WSD) (Resnik,
1999; Mohammad and Hirst, 2006), improving
the similarity measure, and refining the similarity
threshold. We would like to explore ways of incor-
porating syntactic knowledge that do not sacrifice
coverage as much as in Callison-Burch (2008); in-
corporating semantic knowledge to disambiguate
phrasal senses; using context to help sense disam-
biguation (Erk and Pad?, 2008); and optimizing
the similarity threshold for use in SMT, for exam-
ple on a held-out dataset: too high a threshold re-
duces coverage, while too low a threshold results
in bad paraphrases and translation.
The method presented here is quite general, and
therefore different similarity measures, including
other corpus-based ones, can be plugged in to gen-
erate paraphrases. We are looking into using DPs
with word-sense disambiguation: Since it has been
shown that similarity is often judged by the se-
mantic distance of the closest senses of the two
target words (Mohammad and Hirst, 2006), and
that paraphrases generated this way are likely to
be of higher quality (Marton et al, 2009), hence
it is also likely that the overall performance of an
SMT system using them will also improve further.
One potential advantage of using bitexts for
paraphrase generation is the usage of implicit hu-
man knowledge, i.e., sentence alignments. The
concern that not using this knowledge would turn
out detrimental to the performance of SMT sys-
tems augmented by paraphrases as described here
was largely put to rest, as our method improved
the tested subset SMT systems? quality.
Acknowledgments
Many thanks to Chris Dyer for his help with
the E2C set, and to Adam Lopez for his imple-
mentation of pattern matching with Suffix Ar-
ray. This research was partially supported by
the GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-2-001 and NSF award 0838801, by the Euro-
MatrixPlus project funded by the European Com-
mission, and by the US National Science Foun-
dation under grant IIS-0713448. The views and
findings are the authors? alone.
References
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of ACL-2001.
P.F. Brown, S.A.D. Pietra, V.J.D. Pietra, and R.L. Mer-
cer. 1993. The mathematics of statistical machine
translation. Computational Linguistics, 19(2):263?
313.
Lou Burnard. 2000. Reference Guide for the British
National Corpus. Oxford University Computing
Services, Oxford, England, world edition edition.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings NAACL-
2006.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP 2008, Waikiki, Hawai?i.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL-05, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Mona Diab and Steve Finch. 2000. A statistical word-
level translation model for comparable corpora. In
Proceedings of the Conference on Content-Based
Multimedia Information Access (RIAO).
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics of the Association for
Computational Linguistics, Geneva, Switzerland.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
389
Katrin Erk and Sebastian Pad?. 2008. A struc-
tured vector space model for word meaning in con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2086), pages 897?906, Honolulu, HI.
John R. Firth. 1957. A synopsis of linguistic theory
1930
?
U55. Studies in Linguistic Analysis, (special
volume of the Philological Society):1?32. Distribu-
tional Hypothesis.
Pascale Fung and Percy Cheung. 2004. Multi-
level bootstrapping for extracting parallel sentences
from a quasi-comparable corpus. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1051, Geneva, Switzerland. Asso-
ciation for Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, com-
parable texts. In Proceedings of COLING-ACL98,
pages 414?420, Montreal, Canada.
Zellig S. Harris. 1940. Review of louis h. gray, foun-
dations of language (new york: Macmillan, 1939).
Language, 16(3):216
?
U231.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), demonstration
session, Prague, Czech Republic.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Philipp Koehn. 2004b. Pharaoh: A beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of AMTA.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th In-
ternational Conference on Machine Learning, pages
296?304, San Francisco, CA.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parame-
ter tuning in statistical machine translation. In Pro-
ceedings of the ACL Workshop on Statistical Ma-
chine Translation.
Yuval Marton, Saif Mohammad, and Philip Resnik.
2009. Estimating semantic distance using soft se-
mantic constraints in knowledge-source / corpus hy-
brid models. In Procedings of EMNLP, Singapore.
S. McDonald. 2000. Environmental determinants of
lexical processing effort. Ph.D. thesis, University of
Edinburgh.
Saif Mohammad and Graeme Hirst. 2006. Distribu-
tional measures of concept-distance: A task-oriented
evaluation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2006), Sydney, Australia.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477?504.
Doug Oard, David Doermann, Bonnie Dorr, Daqing
He, Phillip Resnik, William Byrne, Sanjeeve Khu-
danpur, David Yarowsky, Anton Leuski, Philipp
Koehn, and Kevin Knight. 2003. Desperately seek-
ing cebuano. In Proceedings of HLT-NAACL.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the ACL, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, John
Henderson, and Florence Reeder. 2002. Corpus-
based comprehensive and diagnostic MT evaluation:
Initial Arabic, Chinese, French, and Spanish results.
In Proceedings of the ACL Human Language Tech-
nology Conference, pages 124?127, San Diego, CA.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th Annual Confer-
ence of the Association for Computational Linguis-
tics., pages 519?525.
Philip Resnik and Noah Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Philip Resnik. 1999. Semantic similarity in a taxon-
omy: An information-based measure and its appli-
cation to problems of ambiguity in natural language.
Journal of Artificial Intelligence Research (JAIR),
11:95?130.
Hinrich Schuetze and Jan O. Pedersen. 1997. A
cooccurrence-based thesaurus and two applications
to information retreival. Information Processing
and Management, 33(3):307
?
U318.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
John Makhoul, Linnea Micciulla, and Ralph
Weischedel. 2005. A study of translation error rate
with targeted human annotation. Technical Report
LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-
58, University of Maryland, July, 2005.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Process-
ing.
390
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 775?783,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Estimating Semantic Distance Using Soft Semantic Constraints
in Knowledge-Source?Corpus Hybrid Models
Yuval Marton
??
, Saif Mohammad
?
, and Philip Resnik
??
?
Department of Linguistics and
?
Laboratory for Computational Linguistics and Information Processing,
Institute for Advanced Computer Studies.
University of Maryland, College Park, MD 20742-7505, USA.
{ymarton,saif,resnik}@umiacs.umd.edu
Abstract
Strictly corpus-based measures of seman-
tic distance conflate co-occurrence infor-
mation pertaining to the many possible
senses of target words. We propose a
corpus?thesaurus hybrid method that uses
soft constraints to generate word-sense-
aware distributional profiles (DPs) from
coarser ?concept DPs? (derived from a
Roget-like thesaurus) and sense-unaware
traditional word DPs (derived from raw
text). Although it uses a knowledge
source, the method is not vocabulary-
limited: if the target word is not in the
thesaurus, the method falls back grace-
fully on the word?s co-occurrence infor-
mation. This allows the method to access
valuable information encoded in a lexical
resource, such as a thesaurus, while still
being able to effectively handle domain-
specific terms and named entities. Exper-
iments on word-pair ranking by semantic
distance show the new hybrid method to
be superior to others.
1 Introduction
Semantic distance is a measure of the closeness
in meaning of two concepts. People are consis-
tent judges of semantic distance. For example, we
can easily tell that the concepts of ?exercise? and
?jog? are closer in meaning than ?exercise? and
?theater?. Studies asking native speakers of a lan-
guage to rank word pairs in order of semantic dis-
tance confirm this?average inter-annotator corre-
lation on ranking word pairs in order of semantic
distance has been repeatedly shown to be around
0.9 (Rubenstein and Goodenough, 1965; Resnik,
1999).
A number of natural language tasks such as ma-
chine translation (Lopez, 2008) and word sense
disambiguation (Banerjee and Pedersen, 2003;
McCarthy, 2006), can be framed as semantic
distance problems. Thus, developing automatic
measures that are in-line with human notions of
semantic distance has received much attention.
These automatic approaches to semantic distance
rely on manually created lexical resources such as
WordNet, large amounts of text corpora, or both.
WordNet-based information content measures
have been successful (Hirst and Budanitsky,
2005), but there are significant limitations on their
applicability. They can be applied only if a Word-
Net exists for the language of interest (which is
not the case for the ?low-density? languages); and
even if there is a WordNet, a number of domain-
specific terms may not be encoded in it. On the
other hand, corpus-based distributional measures
of semantic distance, such as cosine and ?-skew
divergence (Dagan et al, 1999), rely on raw text
alone (Weeds et al, 2004; Mohammad, 2008).
However, when used to rank word pairs in order
of semantic distance or correct real-word spelling
errors, they have been shown to perform poorly
(Weeds et al, 2004; Mohammad and Hirst, 2006).
Mohammad and Hirst (2006) and Patwardhan
and Pedersen (2006) argued that word sense ambi-
guity is a key reason for the poor performance of
traditional distributional measures, and they pro-
posed hybrid approaches that are distributional in
nature, but also make use of information in lexical
resources such as published thesauri and WordNet.
However, both these approaches can be applied to
estimate the semantic distance between two terms
only if both terms exist in the lexical resource they
rely on. We know lexical resources tend to have
limited vocabulary and a large number of domain-
775
specific terms are usually not included.
It should also be noted that similarity values
from different distance measures are not compa-
rable (even after normalization to the same scale),
that is, a similarity score of .75 as per one distance
measure does not correspond to the same seman-
tic distance as a similarity score of .75 from an-
other distance measure.
1
Thus if one uses two
independent distance measures, in this case: one
resource-reliant and one only corpus-dependent,
then these two measures are not comparable (and
hence cannot be used in tandem), even if both
rely?partially or entirely?on distributional cor-
pus statistics.
We propose a hybrid semantic distance method
that inherently combines the elements of a
resource-reliant measure and a strictly corpus-
dependent measure by imposing resource-reliant
soft constraints on the corpus-dependent model.
We choose the Mohammad and Hirst (2006)
method as the resource-reliant method and not
one of the WordNet-based measures because, un-
like the WordNet-based measures, the Moham-
mad and Hirst method is distributional in nature
and so lends itself immediately for combination
with traditional distributional similarity measures.
Our new hybrid method combines concept?word
co-occurrence information (the Mohammad and
Hirst distributional profiles of thesaurus concepts
(DPC)) with word?word co-occurrence informa-
tion, to generate word-sense-biased distributional
profiles. The ?pure? corpus-based distributional
profile (a.k.a. co-occurrence vector, or word asso-
ciation vector), for some target word u, is biased
with soft constraints towards each of the concepts
c that list u in the thesaurus, to create a distribu-
tional profile that is specific to u in the sense that
is most related to the other words listed under c.
Thus, this method can make more fine-
grained distinctions than the Mohammad and Hirst
method, and yet uses word sense information.
2
Our proposed method falls back gracefully to rely
only on word-word co-occurrence information if
any of the target terms is not listed in the lexical re-
source. Experiments on the word-pair ranking task
1
All we can infer is that if w
1
and w
2
have a similarity
score of .75 and w
3
and w
4
have a score of .5 by the same
distance measure, then w
1
?w
2
are closer in meaning than
w
3
?w
4
.
2
Even though Mohammad and Hirst (2006) use thesaurus
categories as coarse concepts, their algorithm can be applied
using more finer-grained thesaurus word groupings (para-
graphs and semicolon units), as well.
on three different datasets show that the our pro-
posed hybrid measure outperforms all other com-
parable distance measures.
Mohammad and Hirst (2007) show that their
method can be used to compute semantic dis-
tance in a resource poor language L
1
by com-
bining its text with a thesaurus in a resource-rich
language L
2
using an L
1
?L
2
bilingual lexicon to
create cross-lingual distributional profiles of con-
cepts, that is, L
2
word co-occurrence profiles of
L
1
thesaurus concepts. Since our method makes
use of the Mohammad and Hirst DPCs, it can just
as well make use of their cross-lingual DPCs, to
compute semantic distance in a resource-poor lan-
guage, just as they did. We leave that for future
work.
2 Background and Related Work
Strictly speaking, semantic distance/closeness is
a property of lexical units?a combination of the
surface form and word sense.
3
Two terms are con-
sidered to be semantically close if there is a lex-
ical semantic relation between them. Such a re-
lation may be a classical relation such as hyper-
nymy, troponymy, meronymy, and antonymy, or
it may be what have been called an ad-hoc non-
classical relation, such as cause-and-effect (Mor-
ris and Hirst, 2004). If the closeness in meaning
is due to certain specific classical relations such as
hypernymy and troponymy, then the terms are said
to be semantically similar. Semantic relatedness
is the term used to describe the more general form
of semantic closeness caused by any semantic re-
lation (Hirst and Budanitsky, 2005). So the nouns
liquid and water are both semantically similar and
semantically related, whereas the nouns boat and
rudder are semantically related, but not similar.
The next three sub-sections describe three kinds
of automatic distance measures: (1) lexical-
resource-based measures that rely on a manually
created resource such as WordNet; (2) corpus-
based measures that rely only on co-occurrence
statistics from large corpora; and (3) hybrid mea-
sures that are distributional in nature, and that also
exploit the information in a lexical resource.
2.1 Lexical-resource-based measures
WordNet is a manually-created hierarchical net-
work of nodes (taxonomy), where each node in
3
The notion of semantic distance can be generalized, of
course, to larger units such as phrases, sentences, passages,
and so on (Landauer et al, 1998).
776
the network represents a fine-grained concept or
word sense. An edge between two nodes rep-
resents a lexical semantic relation such as hy-
pernymy and troponymy. WordNet-based mea-
sures consider two terms to be close if they occur
close to each other in the network (connected by
only a few arcs), if their definitions share many
terms (Banerjee and Pedersen, 2003; Patwardhan
and Pedersen, 2006), or if they share a lot of infor-
mation (Lin, 1998; Resnik, 1999). The length of
each arc/link (distance between nodes) can be as-
sumed a unit length, or can be computed from cor-
pus statistics. Within WordNet, the is-a hierarchy
is much more well-developed than that of other
lexical semantic relations. So, not surprisingly,
the best WordNet-based measures are those that
rely only on the is-a hierarchy. Therefore, they
are good at measuring semantic similarity (e.g.,
doctor?physician), but not semantic relatedness
(e.g., doctor?scalpel). Further, the measures can
only be used in languages that have a (sufficiently
developed) WordNet. WordNet sense information
has been criticized to be too fine grained (Agirre
and Lopez de Lacalle Lekuona, 2003; Navigli,
2006). See Hirst and Budanitsky (2005) for a com-
prehensive survey of WordNet-based measures.
2.2 Corpus-based measures
Strictly corpus-based measures of distributional
similarity rely on the hypothesis that words that
occur in similar context tend to be semantically
close (Firth, 1957; Harris, 1940). The set of
contexts of each target word u is represented by
its distributional profile (DP)?the set of words
that tend to co-occur with u within a certain dis-
tance, along with numeric scores signifying this
co-occurrence tendency with u. Then measures
such as cosine or ?-skew divergence are used to
determine how close the DPs of the two target
words are. See Section 3 for more details and re-
lated work. These measures are very appealing
because they rely simply on raw text, but, as de-
scribed earlier, when used to rank word pairs in
order of semantic distance, or to correct real-word
spelling errors, they perform poorly, compared
to the WordNet-based measures. See Weeds et
al. (2004), Mohammad (2008), and Curran (2004)
for detailed surveys of distributional measures.
As Mohammad and Hirst (2006) point out, the
DP of a word u conflates information about the
potentially many senses of u. For example, con-
sider the following. The noun bank has two senses
?river bank? and ?financial institution?. Assume
that bank, when used in the ?financial institu-
tion? sense, co-occurred with the noun money 100
times in a corpus. Similarly, assume that bank,
when used in the ?river bank? sense, co-occurred
with the noun boat 80 times. So the DP of bank
will have co-occurrence information with money
as well as boat:
DPW(bank):
money,100; boat,80; bond,70; fish,77; . . .
Assume that the DP of the word ATM is:
DPW(ATM):
money,120; boat,0; bond,90; fish,0; . . .
Thus the distributional distance of bank with ATM
will be some sort of an average of the seman-
tic distance between the ?financial institution? and
?ATM? senses and the semantic distance between
the ?river bank? and ?ATM? senses. However, in
various natural language tasks, we need the se-
mantic distance between the intended senses of
bank and ATM, which often also tends to be the
semantic distance between their closest senses.
2.3 Hybrid measures
Both Mohammad and Hirst (2006) and Patward-
han and Pedersen (2006) proposed measures that
are not only distributional in nature but also rely
on a lexical resource to exploit the manually en-
coded information therein as well as to overcome
the sense-conflation problem (described in sec-
tion 2.2). Since we essentially combine the Mo-
hammad and Hirst method with a ?pure? word-
based distributional measure to create our hybrid
approach, we briefly describe their method here.
Mohammad and Hirst (2006) generate separate
distributional profiles for the different senses of
a word, without using any sense-annotated data.
They use the categories in a Roget-style thesaurus
(Macquaries (Bernard, 1986)) as coarse senses or
concepts. There are about 1000 categories in a
thesaurus, and each category has on average 120
closely related words. A word may be found in
more than one category if it has multiple meaning.
They use a simple unsupervised algorithm to de-
termine the vector of words that tend to co-occur
with each concept and the corresponding strength
of association (a measure of how strong the ten-
dency to co-occur is). The target word u will be
assigned one DPC for each of the concepts that
777
list u. Below are example DPCs of the two con-
cepts pertaining to bank:
4
DPC(?fin. inst.?):
money,1000; boat,32; bond,705; fish,0; . . .
DPC(?river bank?):
money,5; boat,863; bond,0; fish,948; . . .
The distance between two words u, v is deter-
mined by calculating the closeness of each of the
DPCs of u to each of DPCs of v, and the closest
DPC-pair distance is chosen.
Mohammad and Hirst (2006) show that their ap-
proach performs better than other strictly corpus-
based approaches that they experimented with.
However, all those experiments were on word-
pairs that were listed in the thesaurus. Their ap-
proach is not applicable otherwise. In Sections 3
and 4 we show how cosine?log-likelihood-ratio
(or any comparable distributional measure) can be
combined with the Mohammad and Hirst DPCs to
form a hybrid approach that is not limited to the
vocabulary of a lexical resource.
Erk and Pad?o (2008) proposed a way of rep-
resenting a word sense in context by biasing the
target word?s DP according to the context sur-
rounding a target (specific) occurrence of the tar-
get word. They use dependency relations and se-
lectional preferences of the target word and com-
bine multiple DPs of words appearing in the con-
text of the target occurrence, in a manner so as
to give more weight to words co-occurring with
both the target word and the target occurrence?s
context words. The advantage of this approach
is that it does not rely on a thesaurus or Word-
Net. Its disadvantage is that it relies on depen-
dency relations and selectional preferences infor-
mation, and that the context information it uses in
order to determine the word sense is quite limited
(only the words surrounding a single occurrence
of the and hence the representation of that sense
might not be sufficiently accurate. Their approach
effectively assumes that each occurrence of a word
has a unique sense.
3 Distributional Measures with Soft
Semantic Constraints
Traditional distributional profiles of words (DPW)
give word?word co-occurrence frequencies. For
example, DPW(u) gives the number of times
4
The relatively large co-occurrence frequency values for
DPCs as compared to DPWs is because a concept can be ref-
ered to by many words (on average 100).
the target word u co-occurs with with all other
words:
5
DPW(u):
w
1
,f(u,w
1
); w
2
,f(u,w
2
); w
3
,f(u,w
3
); . . .
where f stands for co-occurrence frequency (and
can be generalized to stand for any strength
of association (SoA) measure such as the log-
likelihood ratio). Mohammad and Hirst create
concept?word co-occurrence vectors, ?distribu-
tional profiles of concepts? (DPCs), from non-
annotated corpus. For example, DPC(c) gives the
number of times the concept (thesaurus category)
c co-occurs with all the words in a corpus.
DPC(c):
w
1
,f(c,w
1
); w
2
,f(c,w
2
); w
3
,f(c,w
3
); . . .
A target word u that appears under thesaurus con-
cepts c
1
, ..., c
n
would be assigned to DPC(c
1
), ...,
DPC(c
n
). Therefore, if a target word v also ap-
pears under some same concept c, the DPCs of u
and v would be indistinguishable.
We propose the creation of distributional pro-
files of word senses (DPWS(u
c
)) that approximate
the SoA of the target word u, when used in sense
c, with each of the words in the corpus:
DPWS(u
c
):
w
1
,f(u
c
,w
1
); w
2
,f(u
c
,w
2
); w
3
,f(u
c
,w
3
); . . .
In order to get exact counts, one needs sense-
annotated data. However, such data is expensive
to create, and is scarce. Therefore, we propose
estimating these counts from the DPW and DPC
counts:
f(u
c
, w
i
) = p(c|w
i
)? f(u,w
i
) (1)
where the conditional probability p(c|w
i
) is calcu-
lated from the co-occurrence frequencies in DPCs;
and the co-occurrence count f(u,w
i
) is calcu-
lated from DPWs. If the target word is not in
the thesaurus?s vocabulary, then we assume uni-
form distribution over all concepts, and in prac-
tice use a single sense, and take the conditional
probability to be 1. Since the method takes sense-
proportional co-occurrence counts, we will refer
to this method as the hybrid-sense-proportional-
counts method (or, hybrid-prop for short).
5
The dimensions of the DP co-occurrence vector can be
defined arbitrarily, and do not have to correspond to the words
in the vocabulary. The most notable alternative representation
is the Latent Semantic Analysis and its variants (Landauer et
al., 1998; Finkelstein et al, 2002; Budiu et al, 2006).
778
For example, below is the DPWS of bank in
the ?financial institution? sense, calculated from
its DPW and DPCs:
DPW(bank):
money,100; boat,80; bond,70; fish,77; . . .
DPC(?fin. inst.?):
money,1000; boat,32; bond,705; fish,0; . . .
DPC(?river bank?):
money,5; boat,863; bond,0; fish,948; . . .
DPWS(bank
?fin.inst.?
):
money,(
1000
1000+5
? 100); boat,(
32
32+863
? 80);
bond,(
705
705+0
? 70); fish,(
0
0+948
? 77); . . .
Once the DPWS are calculated, any counts-
based SoA and distance measures can be ap-
plied. For example, in this work we use log-
likelihood ratio (Dunning, 1993) to determine
the SoA between a word sense and co-occurring
words, and cosine to determine the distance be-
tween two DPWS?s log likelihood vectors (Mc-
Donald, 2000). We also contrast this measure with
cosine of conditional probabilities vectors. Given
two target words, we determine the distance be-
tween each of their DPWS pairings and the closest
DPWS-pair distance is chosen.
3.1 The hybrid-sense-filtered-counts method
Since the DPCs are created in an unsupervised
manner, they are expected to be somewhat noisy.
Therefore, we also experimented with a variant of
the method proposed above, that simply makes use
of whether the conditional probability p(c|w
i
) is
greater than 0 or not:
f(u
c
, w
i
) =
{
f(u,w
i
) If p(c|w
i
) > 0
0 Otherwise
(2)
Since this method essentially filters out collocates
that are likely not relevant to the target sense c of
the target word u, we will refer to this method
as the hybrid-sense-filtered-counts method (or,
just hybrid-filt for short). Below is an example
hybrid-filtered DPWS of bank in the ?financial in-
stitution? sense:
DPWS(bank
?fin.inst.?
:
money,100); boat,80; bond,70; . . .
Note that the collocate fish is now filtered out,
whereas bank?s co-occurrence counts with money,
boat, and bond are left as is (and not sense-
proportionally attenuated).
4 Evaluation
We evaluated various methods on the task of
ranking word pairs in order of semantic dis-
tance. These methods included our sense-biased
methods as well as several baselines: the Mo-
hammad and Hirst (2006) DPC-based methods,
the traditional word-based distributional similar-
ity methods, and several Latent Semantic Analysis
(LSA)-based methods. We used three testsets and
their corresponding human judgment gold stan-
dards: (1) the Rubenstein and Goodenough (1965)
set of 65 noun pairs?denoted RG-65; (2) the
WordSimilarity-353 (Finkelstein et al, 2002) set
of 353 noun pairs (which include the RG-65
pairs) of which we discarded of one repeating
pair?denoted WS-353; and (3) the Resnik and
Diab (2000) set of 27 verb pairs?denoted RD-00.
4.1 Corpora and Pre-processing
We generated distributional profiles (DPWs
and DPCs) from the British National Corpus
(BNC) (Burnard, 2000), which is a balanced cor-
pus. We lowercased the characters, and stripped
numbers, punctuation marks, and any SGML-like
syntactic tags, but kept sentence boundary mark-
ers. The BNC contained 102,100,114 tokens of
546,299 types (vocabulary size) after tokenization.
For the verb set, we also lemmatized this corpus.
We considered two words as co-occurring if
they occurred in a window of?5 words from each
other. We stoplisted words that co-occurred with
more than 2000 word types.
4.2 Results
The Spearman rank correlations of the automatic
rankings of the RG-65, WS353, and RD-00 test-
sets with the corresponding gold-standard human
rankings is listed in Table 1.
6
The higher the
Spearman rank correlation, the more accurate is
the distance measure.
4.2.1 Results on the RG-65 testset
Baselines. We replicated the traditional word-
based distributional distance measure using co-
sine of vectors (DPs) containing conditional prob-
abilities (word-cos-cp). Its rank correlation of
.53 is close to the correlation of .54 reported in
Mohammad and Hirst (2006), hereafter MH06.
We replicated the MH06 concept-based approach
6
Certain experiments were not pursued as they were re-
dundant in supporting our claims.
779
Method RG-65 WS-353 RD-00
Baselines (replicated):
Traditional distributional measures
word-cos-cp .53 .31 .46
word-cos-ll .70 .54 .51
word-cos-pmi .62 .43 .57
Mohammad and Hirst methods and variants
concept-cos-cp .62 .38 .41
concept*-cos-cp .65 .33 .43
concept-cos-ll .60 .37 .43
concept*-cos-ll .64 .25 .27
concept*-cos-pmi .40 .19 .28
Other (LSA and variants)
LSA .56 .47 .55
GLSA-cos-pmi .18 n.p. n.p.
GLSA-cos-ll .47 n.p. .29
New methods:
hybrid-prop-cos-ll .72 .49 .53
hybrid-prop*-cos-ll .69 .46 .45
hybrid-filt-cos-ll .73 .54 .38
hybrid-filt*-cos-ll .77 .54 .39
hybrid-prop*-cos-pmi .58 .43 .71
hybrid-filt*-cos-pmi .61 .42 .64
Table 1: Spearman rank correlation on RG-65,
WS-353, and RD-00 testsets, trained on BNC.
?*? indicates the use of a smaller bootstrapped
concept?word co-occurrence matrix. ?n.p.? indi-
cates that the experiment was not pursued.
(concept-cos-cp), and its bootstrapped variant that
uses a smaller concept?word co-occurrence matrix
(concept*-cos-cp). The latter yielded a correla-
tion score .65, close to the .69 reported in MH06.
We also experimented with cosine of PMI vec-
tors (word-cos-pmi) which obtained a correlation
of .62. Log likelihood ratios (word-cos-ll) gave
best results among the baseline methods (.70), and
so we it more often in the implementations of our
hybrid method.
We conducted experiments with LSA and its
GLSA variants (Budiu et al, 2006) as additional
baselines. A limited vocabulary of the 33,000
most frequent words in the BNC and all test words
was used in these experiments. (A larger vocab-
ulary was computationally expensive and 33,000
is also the vocabulary size used by Budiu et
al. (2006) in their LSA experiments.)
New Methods: The hybrid method variants
proposed in this paper (hybrid-prop-cos-ll and
hybrid-filt-cos-ll) were the best performers on the
RG-65 test set. Particularly, they performed better
than both the traditional word-distance measures
(word-cos-ll), and our concept-based methods?
variants of the MH06 method that are used with
likelihood ratios (concept-cos-ll, concept*-cos-
ll). The -pmi methods were all poorer performers
than their -ll counterparts. The -pmi hybrid vari-
ants obtained higher scores than the concept-based
ones, but almost the same scores as the word-
based ones.
4.2.2 Results on WS-353 and RD-00 testsets
On WS-353, all our hybrid methods out-
performed their concept counterparts, and were
on par with their word-based counterparts. On
RD-00, word-cos-pmi out-performed all other
word-based methods, and the hybrid -pmi meth-
ods were best performers with scores of .64 and
.71. Our word-cos-ll, hybrid-prop-cos-ll, and
the two hybrid pmi results on RD-00 are better
than any non-WordNet results reported by Resnik
and Diab (2000), including their syntax-informed
methods?the variants of Lin (?distrib?, .43) and
Dorr (?LCS?, .39). In fact, our hybrid*-prop-cos-
pmi and hybrid*-filt-cos-pmi results reach corre-
lation levels of the WordNet-based methods re-
ported there (.66?.68). Also, on WS-353, our
hybrid sense-filtered variants and word-cos-ll ob-
tained a correlation score higher than published re-
sults using WordNet-based measures (Jarmasz and
Szpakowicz, 2003) (.33 to .35) and Wikipedia-
based methods (Ponzetto and Strube, 2006) (.19
to .48); and very close to the results obtained by
thesaurus-based (Jarmasz and Szpakowicz, 2003)
(.55) and LSA-based methods (Finkelstein et al,
2002) (.56).
The lower correlation scores of all measures on
the WS-353 test set are possibly due to it hav-
ing politically biased word pairs (examples in-
clude: Arafat?peace, Arafat?terror, Jerusalem?
Palestinian) for which BNC texts are likely to in-
duce low correlation with the human raters of WS-
353. This testset alo has disproportionately many
terms from the news domain.
The concept methods performed poorly on WS-
353 partly because many of the target words do
not exist in the thesaurus. For instance, there
were 17 such word types that occurred in 20 WS-
353 testset word pairs. When excluding these
pairs, concept-cos-cp goes up from .38 to .45, and
concept*-cos-pmi from .19 to .24. Interestingly,
results of the hybrid methods show that they were
largely unaffected by the out-of-vocabulary prob-
lem on the WS-353 dataset.
On the verbs dataset RD-00, while hybrid-prop-
cos-ll fared slightly better than word-cos-ll, using
the smaller matrix seemed to hurt performance of
780
hybrid*-prop-cos-ll compared to word-cos-ll. But
results suggest that the -pmi methods might serve
as a better measure than -ll for verbs, although this
claim should be tested more rigorously.
Human judgments of semantic distance are less
consistent on verb-pairs than on noun-pairs, as re-
flected in inter-rater agreement measures in Resnik
and Diab (2000) and others). Thus, not surpris-
ingly, the scores of almost all measures are lower
for the verb data than the RG-65 noun data.
5 Discussion
The hybrid methods proposed in this paper ob-
tained higher accuracies than all other methods on
the RG-65 testset (all of whose words were in the
published thesaurus), and on the RD-00 testset,
and their performance was at least respectable on
the WS-353 testset (many of whose words were
not in the published thesaurus). This is in con-
trast to the concept-distance methods which suf-
fer greatly when the target words are not in the
lexical resource (here, the thesaurus) they rely on,
even though these methods can make use of co-
occurrence information of words not in the the-
saurus with concepts from the thesaurus.
Amongst the two hybrid methods proposed, the
sense-filtered-counts method performed better
using the smaller bootstrapped concept?word co-
occurrence matrix whereas the sense-proportional
method performed better using the larger concept?
word co-occurrence matrix. We believe this is be-
cause the bootstrapping method proposed in Mo-
hammad and Hirst (2006) has the effect of reset-
ting to 0 the small co-occurrence counts. The
noise from these small co-occurrence counts af-
fects the sense-filtered-counts method more ad-
versely (since any non-zero value will cause the
inclusion of the corresponding collocate?s full co-
occurrence count) and so the bootstrapped matrix
is more suitable for this method.
The results also show that the cosine of log-
likelihood ratios method mostly performs better
than cosine of conditional probabilities and the
pmi methods on the noun sets. This further
supports the claim by Dunning (1993) that log-
likelihood ratio is much less sensitive than pmi
to low counts. Interestingly, on the verb set, the
pmi methods, and especially hybrid*-prop-cos-
pmi, did extremely well. Further investigation is
needed in order to determine if pmi is indeed more
suitable for verb semantic similarity, and why.
6 Conclusion
Traditional distributional similarity conflates co-
occurrence information pertaining to the many
senses of the target words. Mohammad and
Hirst (2006) show how distributional measures
can be used to compute distance between very
coarse word senses or concepts (thesaurus cat-
egories), and even obtain better results than
traditional distributional similarity. However,
their method requires that the target words be
listed in the thesaurus, which is often not the
case for domain-specific terms and named enti-
ties. In this paper, we proposed hybrid meth-
ods (hybrid-sense-filtered-counts and hybrid-
sense-proportional-counts) that combine word?
word co-occurrence information (traditional dis-
tributional similarity) with word?concept co-
occurrence information (Mohammad and Hirst,
2006), with soft constraints in such a manner
that the method makes use of information en-
coded in the thesaurus when available, and de-
grades gracefully if the target word is not listed
in the thesaurus. Our method generates word-
sense-biased distributional profiles (DPs) from
non-annotated corpus-based word-based DPs and
coarser-grained aggregated thesaurus-based ?con-
cept DPs? (DPCs). We showed that the hybrid
method correlates with human judgments of se-
mantic distance in most cases better than any of
the other methods we replicated.
We are now interested in improving seman-
tic distance measures for verb?verb, adjective?
adjective, and cross-part-of-speech pairs, by ex-
ploiting specific information pertaining to these
parts of speech in lexical resources in addition to
purely co-occurrence information.
Acknowledgments
We thank Mona Diab for her help with her verb
test set, Raluca Budiu for her help and clarifica-
tions regarding the GLSA method and its imple-
mentation details, and the anonymous reviewers
for their valuable feedback. This work was sup-
ported, in part, by the National Science Founda-
tion under Grant No. IIS-0705832, and in part, by
the Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the sponsor.
781
References
Eneko Agirre and Oier Lopez de Lacalle Lekuona.
2003. Clustering WordNet word senses. In Pro-
ceedings of the 1st International Conference on
Recent Advances in Natural Language Processing
(RANLP-2003), Borovets, Bulgaria.
Satanjeev Banerjee and Ted Pedersen. 2003. Ex-
tended gloss overlaps as a measure of semantic re-
latedness. In Proceedings of the Eighteenth Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-03), pages 805?810, Acapulco, Mexico.
John R. L. Bernard, editor. 1986. The Macquarie The-
saurus. Macquarie Library, Sydney, Australia.
Raluca Budiu, Christiaan Royer, and Peter Pirolli.
2006. Modeling information scent: A compari-
son of LSA, PMI and GLSA similarity measures
on common tests and corpora. In Proceedings of
RIAO?07, Pittsburgh, PA.
Lou Burnard. 2000. Reference Guide for the British
National Corpus. Oxford University Computing
Services, Oxford, England, world edition edition.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, School of Informatics,
University of Edinburgh, Edinburgh, UK.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of cooccurrence probabili-
ties. Machine Learning, 34(1?3):43?69.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Katrin Erk and Sebastian Pad?o. 2008. A struc-
tured vector space model for word meaning in con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2086), pages 897?906, Honolulu, HI.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
John R. Firth. 1957. A synopsis of linguistic theory
193055. Studies in Linguistic Analysis, (special vol-
ume of the Philological Society):132. Distributional
Hypothesis.
Zellig S. Harris. 1940. Review of Louis H. Gray, foun-
dations of language (New York: Macmillan, 1939).
Language, 16(3):216?231.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexical
cohesion. NLE, 11(1):87?111.
Mario Jarmasz and Stan Szpakowicz. 2003. Ro-
get?s Thesaurus and semantic similarity. In Pro-
ceedings of the International Conference on Recent
Advances in Natural Language Processing (RANLP-
2003), pages 212?219, Borovets, Bulgaria.
Thomas Landauer, Peter Foltz, and Darrell Laham.
1998. Introduction to latent semantic analysis. Dis-
course Processes, 25:259 ? 284.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th In-
ternational Conference on Machine Learning, page
296304, San Francisco, CA.
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):149.
Diana McCarthy. 2006. Relating WordNet senses for
word sense disambiguation. In Proceedings of the
European Chapter of the Association for Computa-
tional Linguistics Workshop Making Sense of Sense -
Bringing Computational Linguistics and Psycholin-
guistics Together, pages 17?24, Trento, Italy.
S. McDonald. 2000. Environmental determinants of
lexical processing effort. Ph.D. thesis, University of
Edinburgh, Edinburgh, UK.
Saif Mohammad and Graeme Hirst. 2006. Distribu-
tional measures of concept-distance: A task-oriented
evaluation. In Proceedings of EMNLP.
Saif Mohammad, Iryna Gurevych, Graeme Hirst, and
Torsten Zesch. 2007. Cross-lingual distribu-
tional profiles of concepts for measuring seman-
tic distance. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP/CoNLL-2007), pages 571?580,
Prague, Czech Republic.
Saif Mohammad. 2008. Measuring Semantic Distance
using Distributional Profiles of Concepts. Ph.D. the-
sis, Department of Computer Science, University of
Toronto, Toronto, Canada.
Jane Morris and Graeme Hirst. 2004. Non-classical
lexical semantic relations. In Proceedings of the
Workshop on Computational Lexical Semantics, Hu-
man Language Technology Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 46?51, Boston, Mas-
sachusetts.
Roberto Navigli. 2006. Meaningful clustering of
senses helps boost word sense disambiguation per-
formance. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association, pages 105?
112, Sydney, Australia.
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing WordNet based context vectors to estimate the
semantic relatedness of concepts. In Proceedings of
Making Sense of Sense EACL Workshop, pages 1?8.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL-2006),
pages 192?199, New York, NY.
Philip Resnik and Mona Diab. 2000. Measuring verb
similarity. In 22nd Annual Meeting of the Cognitive
Science Society (COGSCI2000), Philadelphia, PA.
Philip Resnik. 1999. Semantic similarity in a taxon-
omy: An information-based measure and its appli-
cation to problems of ambiguity in natural language.
JAIR, 11:95?130.
782
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics
(COLING-04), pages 1015?1021, Geneva, Switzer-
land.
783
Improved Cross-Language Retrieval
using Backoff Translation
Philip Resnik,1;2 Douglas Oard,2;3 and Gina Levow2
Department of Linguistics,1
Institute for Advanced Computer Studies,2
College of Information Studies,3
University of Maryland
College Park, MD 20742
fresnik,ginag@umiacs.umd.edu, oard@glue.umd.edu
ABSTRACT
The limited coverage of available translation lexicons can pose a se-
rious challenge in some cross-language information retrieval appli-
cations. We present two techniques for combining evidence from
dictionary-based and corpus-based translation lexicons, and show
that backoff translation outperforms a technique based on merging
lexicons.
1. INTRODUCTION
The effectiveness of a broad class of cross-language information
retrieval (CLIR) techniques that are based on term-by-term transla-
tion depends on the coverage and accuracy of the available trans-
lation lexicon(s). Two types of translation lexicons are commonly
used, one based on translation knowledge extracted from bilingual
dictionaries [1] and the other based on translation knowledge ex-
tracted from bilingual corpora [8]. Dictionaries provide reliable ev-
idence, but often lack translation preference information. Corpora,
by contrast, are often a better source for translations of slang or newly
coined terms, but the statistical analysis through which the trans-
lations are extracted sometimes produces erroneous results. In this
paper we explore the question of how best to combine evidencefrom
these two sources.
2. TRANSLATION LEXICONS
Our term-by-term translation technique (described below) requires
a translation lexicon (henceforth tralex) in which each word f is as-
sociated with a ranked set fe
1
; e
2
; : : : e
n
g of translations. We used
two translation lexicons in our experiments.
2.1 WebDict Tralex
We downloadeda freely available, manually constructedEnglish-
French term list from the Web1 and inverted it to French-English
1http://www.freedict.com
.
format. Since the WebDict translations appear in no particular or-
der, we ranked the e
i
based on target language unigram statistics
calculated over a large comparable corpus, the English portion of
the Cross-LanguageEvaluation Forum (CLEF) collection, smoothed
with statistics from the Brown corpus, a balanced corpus covering
many genres of English. All single-word translations are ordered by
decreasing unigram frequency, followed by all multi-word transla-
tions, and finally by any single-word entries not found in either cor-
pus. This ordering has the effect of minimizing the effect of infre-
quent words in non-standard usages or of misspellings that some-
times appear in bilingual term lists.
2.2 STRAND Tralex
Our second lexical resource is a translation lexicon obtained fully
automatically via analysisof parallel French-Englishdocuments from
the Web. A collection of 3,378 document pairs was obtained using
STRAND, our technique for mining the Web for bilingual text [7].
These document pairs were aligned internally, using their HTML
markup, to produce 63,094 aligned text ?chunks? ranging in length
from 2 to 30 words, 8 words on average per chunk, for a total of
500K words per side. Viterbi word-alignments for these paired
chunks were obtained using the GIZA implementation of the IBM
statistical translation models.2 An ordered set of translation pairs
was obtained by treating each alignment link between words as a
co-occurrence and scoring each word pair according to the likeli-
hood ratio [2]. We then rank the translation alternatives in order of
decreasing likelihood ratio score.
3. CLIR EXPERIMENTS
Ranked tralexes are particularly well suited to a simple ranked
term-by-term translation approach. In our experiments, we use top-
2 balanced document translation, in which we produce exactly two
English terms for each French term. For terms with no known trans-
lation, the untranslated French term is generated twice (often appro-
priate for proper names). For French terms with one translation, that
translation is generated twice. For French terms with two or more
translations, we generate the first two translations in the tralex. Thus
balanced translation has the effect of introducing a uniform weight-
ing over the top n translations for each term (here n = 2).
Benefits of the approachinclude simplicity and modularity ? no-
tice that a lexicon containing ranked translations is the only require-
ment, and in particular that there is no need for access to the in-
ternals of the IR system or to the document collection in order to
2http://www.clsp.jhu.edu/ws99/projects/mt/
perform computations on term frequencies or weights. In addition,
the approach is an effective one: in previous experiments we have
found that this balancedtranslation strategy significantly outperforms
the usual (unbalanced) technique of including all known translations [3].
We have also investigated the relationship between balanced trans-
lation and Pirkola?s structured query formulation method [6].
For our experiments we used the CLEF-2000 French document
collection (approximately 21 million words from articles in Le Monde).
Differences in use of diacritics, case, and punctuation can inhibit
matching between tralex entries and document terms, so we normal-
ize the tralex and the documents by converting characters to low-
ercase and removing all diacritic marks and punctuation. We then
translate the documents using the process described above, index
the translated documentswith the Inquery information retrieval sys-
tem, and perform retrieval using ?long? queries formulated by group-
ing all terms in the title, narrative, and description fields of each
English topic description using Inquery?s #sum operator. We report
mean average precision on the 34 topics for which relevant French
documentsexist, basedon the relevancejudgments provided by CLEF.
We evaluated several strategies for using the WebDict and STRAND
tralexes.
3.1 WebDict Tralex
Since a tralex may contain an eclectic mix of root forms and mor-
phological variants, we use a four-stage backoff strategy to maxi-
mize coverage while limiting spurious translations:
1. Match the surface form of a document term to surface forms
of French terms in the tralex.
2. Match the stem of a document term to surface forms of French
terms in the tralex.
3. Match the surface form of a document term to stems of French
terms in the tralex.
4. Match the stem of a document term to stems of French terms in
the tralex.
We used unsupervisedinduction of stemming rules basedon the French
collection to build the stemmer [5]. The process terminates as soon
as a match is found at any stage, and the known translations for that
match are generated. The process may produce an inappropriate
morphological variant for a correct English translation, so we used
Inquery?s English kstem stemmer at indexing time to minimize the
effect of that factor on retrieval effectiveness.
3.2 STRAND Tralex
One limitation of a statistically derived tralex is that any term has
some probability of aligning with any other term. Merely sorting
translation alternatives in order of decreasing likelihood ratio will
thus find some translation alternatives for every French term that ap-
peared at least once in the set of parallel Web pages. In order to limit
the introduction of spurious translations, we included only transla-
tion pairs with at least N co-occurrences in the set used to build the
tralex. We performed runs with N = 1; 2; 3, using the four-stage
backoff strategy described above.
3.3 WebDict Merging using STRAND
When two sources of evidence with different characteristics are
available, a combination-of-evidence strategy can sometimes out-
perform either source alone. Our initial experiments indicated that
the WebDict tralex was the better of the two (see below), so we adopted
a reranking strategy in which the WebDict tralex was refined ac-
cording a voting strategy to which both the original WebDict and
STRAND tralex rankings contributed.
Condition MAP
STRAND (N = 1) 0.2320
STRAND (N = 2) 0.2440
STRAND (N = 3) 0.2499
Merging 0.2892
WebDict 0.2919
Backoff 0.3282
Table 1: Mean Average Precision (MAP), averaged over 34 top-
ics
For each French term that appeared in both tralexes, we gave the
top-ranked translation in each tralex a score of 100, the next a score
of 99, and so on. We then summed the WebDict and STRAND scores
for each translation, reranked the WebDict translations based on that
sum, and then appendedany STRAND-only translations for that French
term. Thus, although both sourcesof evidence were weighted equally
in the voting, STRAND-only evidence received lower precedence
in the merged ranking. For French terms that appeared in only one
tralex, we included those entries unchangedin the merged tralex. In
this experiment run we used a threshold of N = 1, and applied the
four-stage backoff strategy described above to the merged resource.
3.4 WebDict Backoff to STRAND
A possibleweaknessof our merging strategy is that inflected forms
are more common in our STRAND tralex, while root forms are more
common in our WebDict tralex. STRAND tralex entries that were
copied unchangedinto the merged tralex thus often matched in step
1 of the four-stage backoff strategy, preventing WebDict contribu-
tions from being used. With the WebDict tralex outperforming the
STRAND tralex, this factor could hurt our results. As an alterna-
tive to merging, therefore, we also tried a simple backoff strategy in
which we used the original WebDict tralex with the four-stage back-
off strategy described above, to which we added a fifth stage in the
event that fewer than two WebDict tralex matches were found:
5. Match the surface form of a document term to surface forms
of French terms in the STRAND tralex.
We used a threshold of N = 2 for this experiment run.
4. RESULTS
Table 1 summarizes our results. Increasing thresholds seem to
be helpful with the STRAND tralex, although the differences were
not found to be statistically significant by a paired two-tailed t-test
with p < 0:05. Merging the tralexes provided no improvement
over using the WebDict tralex alone, but our backoff strategy pro-
duced a statistically significant 12% improvement in mean average
precision (at p < 0:01) over the next best tralex (WebDict alone).
As Figure 1 shows, the improvement is remarkably consistent, with
only four of the 34 topics adverselyaffected and only one topic show-
ing a substantial negative impact.
Breaking down the backoff results by stage (Table 2), we find
that the majority of query-to-document hits are obtained in the first
stage, i.e. matches of the term?s surface form in the document to a
translation of the surface form in the dictionary. However, the back-
off process improves by-token coverage of terms in documents by
8%, and gives a 3% relative improvement in retrieval results; it also
contributed additional translations to the top-2 set in approximately
30% of the cases, leading to the statistically significant 12% relative
improvement in mean averageprecision as compared to the baseline
using WebDict alone with 4-stage backoff.
Figure 1: WebDict-to-tralex backoff vs. WebDict alone, by
query
Stage (forms) Lexicon matches
1 (surface-surface) 70.38%
2 (stem-surface) 3.18%
3 (surface-stem) 0.46%
4 (stem-stem) 0.98%
5 (STRAND) 8.34%
No match found 16.66%
Table 2: Term matches in 5-stage backoff
5. CONCLUSIONS
There are many ways of combining evidence from multiple trans-
lation lexicons. We use tralexes similar to those usedby Nie et al [4],
but our work differs in our use of balanced translation and a back-
off translation strategy (which produces a stronger baseline for our
WebDict tralex), and in our comparisonof merging and backoff trans-
lation strategies for combining resources. In future work we plan to
explore other combinations of merging and backoff and other merg-
ing strategies, including post-retrieval merging of the ranked lists.
In addition, parallel corpora can be exploited for more than just
the extraction of a non-contextualized translation lexicon. We are
currently engagedin work on lexical selection methods that take ad-
vantage of contextual information, in the context of our research on
machine translation, and we expect that CLIR results will be im-
proved by contextually-informed scoring of term translations.
6. ACKNOWLEDGMENTS
This research was supported in part by Department of Defense
contract MDA90496C1250 and TIDES DARPA/ITO Cooperative
Agreement N660010028910,
7. REFERENCES
[1] L. Ballesteros and W. B. Croft. Resolving ambiguity for
cross-language retrieval. In W. B. Croft, A. Moffat, and C. V.
Rijsbergen, editors, Proceedings of the 21st Annual
International ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 64?71. ACM
Press, Aug. 1998.
[2] T. Dunning. Accurate methods for the statistics of surprise and
coincidence. Computational Linguistics, 19(1):61?74, March
1993.
[3] G.-A. Levow and D. W. Oard. Translingual topic tracking
with PRISE. In Working Notes of the Third Topic Detection
and Tracking Workshop, Feb. 2000.
[4] J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand.
Cross-language information retrieval based on parallel texts
and automatic mining of parallel texts from the web. In
M. Hearst, F. Gey, and R. Tong, editors, Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages
74?81, Aug. 1999.
[5] D. W. Oard, G.-A. Levow, and C. I. Cabezas. CLEF
experiments at Maryland: Statistical stemming and backoff
translation. In C. Peters, editor, Proceedings of the First
Cross-Language Evaluation Forum. 2001. To appear.
http://www.glue.umd.edu/oard/research.html.
[6] D. W. Oard and J. Wang. NTCIR-2 ECIR experiments at
Maryland: Comparing structured queries and balanced
translation. In Second National Institute of Informatics (NII)
Test Collection Information Retrieval (NTCIR) workshop.
forthcoming.
[7] P. Resnik. Mining the Web for bilingual text. In 37th Annual
Meeting of the Association for Computational Linguistics
(ACL?99), College Park, Maryland, June 1999.
[8] P. Sheridan and J. P. Ballerini. Experiments in multilingual
information retrieval using the SPIDER system. In
Proceedings of the 19th Annual International ACM SIGIR
Conference on Research and Development in Information
Retrieval, Aug. 1996.
Rapidly Retargetable Interactive Translingual Retrieval
Gina-Anne Levow
Institute for Advanced
Computer Studies
University of Maryland,
College Park, MD 20742
gina@umiacs.umd.edu
Douglas W. Oard
College of Information Studies
Institute for Advanced
Computer Studies
University of Maryland,
College Park, MD 20742
oard@glue.umd.edu
Philip Resnik
Department of Linguistics
Institute for Advanced
Computer Studies
University of Maryland,
College Park, MD 20742
resnik@umiacs.umd.edu
ABSTRACT
This paper describes a system for rapidly retargetable interactive
translingual retrieval. Basic functionality can be achieved for a new
document language in a single day, and further improvements re-
quire only a relatively modest additional investment. We applied
the techniquesfirst to searchChinese collections using English queries,
and have successfully added French, German, and Italian document
collections. We achievethis capability through separation of language-
dependent and language-independent components and through the
application of asymmetric techniques that leverage an extensiveEn-
glish retrieval infrastructure.
Keywords
Cross-language information retrieval
1. INTRODUCTION
Our goal is to producesystems that allow interactive users to present
English queries and retrieve documents in languages that they can-
not read. In this paper we focus on what we call ?rapid retargetabil-
ity?: extending interactive translingual retrieval functionality for a
new document languagerapidly with few language-specificresources.
Our current system can be retargeted to a new language in one day
with only one language-dependent resource: a bilingual term list.1
Our language-independent architecture consists of two main com-
ponents:
1. Document translation and indexing
2. Interactive retrieval
We describe each of these components, demonstrate their effective-
ness for information retrieval tasks, and then concludeby describing
our experience with adding French, German and Italian document
collections to a system that was originally developed for Chinese.
1For Asian languages we also use a language-specificsegmentation
system.
.
2. DOCUMENT TRANSLATION AND IN-
DEXING
We have adopted a document translation architecture for two rea-
sons. First, we support a single query language (English) but multi-
ple document languages, so indexing English terms simplifies query
processing (where interactive response time can be a concern). Sec-
ond, a document translation architecture simplifies the display of
translated documents by decoupling the translation and display pro-
cesses. Gigabyte collections require machine translation that is or-
ders of magnitude faster than present commercial systems. We ac-
complish this using term-by-term translation, in which the basic data
structure is a simple hash table lookup. Any translation requires
some source of translation knowledge?we use a bilingual term list
containing English translation(s) for eachforeign language term. We
typically construct these term lists by harvesting Internet-available
translation resources, so the foreign language terms for which trans-
lations are known are typically an eclectic mix of root and inflected
forms. We accommodate this limitation using a four-stage backoff
statistical stemming approach to enhance translation coverage.
2.1 Preprocessing.
Differences in use of diacritic-s, case, and punctuation can inhibit
matching between term list entries and document terms, so normal-
ization is important. In order to maximize the probability of match-
ing document words with term list entries, we normalize the bilin-
gual term list and the documents by:
 converting characters in Western languages to lowercase,
 removing all accents and diacritics, and
 segmentation, which for Western languages merely involves
separating punctuation from other text by the addition of white
space.
Our preprocessingalso includes conversion of the bilingual term list
and the document collection into standard formats. The preprocess-
ing typically requires about half a day of programmer time.
2.2 Four-Stage Backoff Translation.
Bilingual term lists found on the Web often contain an eclectic
mix of root forms and morphological variants. We thus developed
a four-stage backoff strategy to maximize coverage while limiting
spurious translations:
1. Match the surface form of a document term to surface forms
of source language terms in the bilingual term list.
2. Match the stem of a document term to surfaceforms of source
language terms in the bilingual term list.
3. Match the surface form of a document term to stems of source
language terms in the bilingual term list.
4. Match the stem of a document term to stems of source lan-
guage terms in the bilingual term list.
The process terminates as soon as a match is found at any stage, and
the known translations for that match are generated. Although this
may produce an inappropriate morphological variant for a correct
English translation, use of English stemming at indexing time mini-
mizes the effect of that factor on retrieval effectiveness. Becausewe
are ultimately interested in processing documents in any language,
we may not have a hand-crafted stemmer available for the document
language. We have thus explored the application of rule induction
to learn stemming rules in an unsupervised fashion from the collec-
tion that is being indexed [2].
2.3 Balanced Top-2 Translation.
We produce exactly two English terms for each foreign-language
term. For terms with no known translation, the untranslated term is
generated twice (often appropriate for proper names in the Latin-
1 character set). For terms with one translation, that translation is
generated twice. For terms with two or more known translations,
we generate the ?best? two translations. In prior experiments we
have found that this balanced translation strategy significantly out-
performs the usual (unbalanced) technique of including all known
translations [1]. We establish the ?best? translations by sorting the
bilingual term list in advanceusing only English resources. All single-
word translations are ordered by decreasing unigram frequency in
the Brown corpus, followed by all multi-word translations, and fi-
nally by any single word entries not found in the Brown corpus.
This ordering has the effect of minimizing the effect of infrequent
words in non-standard usages or of misspellings that sometimes ap-
pear in bilingual term lists. This translation strategy allows balanc-
ing of translations in a modular fashion, even when one does not
have access to the internal parameters of the information retrieval
system. We translate  100 MB per hour using Perl on a SPARC
Ultra 5.
2.4 Post-translation Document Expansion.
We implement post-translation document expansion for the for-
eign language stories after translation into English in order to en-
rich the indexing vocabulary beyond that which was available af-
ter term-by-term translation. This is analogous to the process that
Singhal et al applied to monolingual speech retrieval [4].
Term-by-term translation producesa set of English terms that serve
as a noisy representation of the original source language document.
These terms are then treated as a query to a comparable English col-
lection, typically contemporaneous newswire text, from which we
retrieve the five highest ranked documents. From those five docu-
ments, we extract the most selective terms and use them to enrich
the original translations of the documents. For this expansion pro-
cess we select one instance of every term with an IDF value above
an ad hoc threshold that was tuned to yield approximately 50 new
terms. This optional step is the slowest processing stage, with a
throughput of about 20 MB per hour.
2.5 Indexing
The resulting collection is then indexed using Inquery (version
3.1p1), with the kstem stemmer and default English stopword list.
Indexing is the fastest stage in the process, with throughput exceed-
ing one gigabyte per hour.
3. INTERACTIVE RETRIEVAL
Interactive searches are performed using a Web interface. Sum-
mary information for the top-ranked documents is displayedin groups
of ten per page. Document summaries consist of the date and a gloss
translation of the document title. Users can inspect a gloss transla-
tion of the full text of any document if the title is not sufficiently
informative. For both title and full text, the gloss translations are
generated in advance using the same process as translation for in-
dexing, with the following differences in detail:
 Terms added as a result of document expansion are not dis-
played.
 The number of retained translations is separately selectable
for the title and for full text indexing.
 Translations are not duplicated when fewer than the maximum
allowable number of translations are known.
Our goal is to support the process of finding documents, with the
realization that the process of using documents may need to be sup-
ported in some other way (e.g., by forwarding relevant documents
to someone who is able to read that language). We have therefore
designedour interface to highlight the query terms in translated doc-
uments and to facilitate skimming by emphasizing the most com-
mon translation when multiple translations are displayed. We have
found that such displays can support a classification task, even when
the translation is not easy to read [3]. Documents must be classified
by the user as relevant or not relevant, so our classification results
suggest that this can be an effective user interface design.
4. RESULTS
We present results both for component-level performance of our
language-independentretargeting modules and an assessmentof the
overall retargeting process.
4.1 Component-level Evaluation
We applied our retargeting approach and retrieval enhancement
techniquesdescribedabove in the context of the first Cross-Language
Evaluation Forum?s (CLEF) multilingual task. We used the English
language forms of the queries to retrieve English, French, German,
and Italian documents. Below we present comparative performance
measuresfor two of the main processingcomponentsdescribed above
- statistical stemming backoff translation - applied to the English-
French cross-languagesegment of the CLEF task. The post-translation
document expansion component was applied to the smaller Topic
Detection and Tracking (TDT-3) collection to improve retrieval of
Mandarin documents using English.
4.1.1 Baseline CLEF System Configuration
Our baseline run was conducted as follows. We translated the
 44; 000 documents from the 1994 issues of Le Monde. We used
the English-French bilingual term list downloaded from the Web at
http://www.freedict.com. We then inverted the term list
to form a 35,000 term French-English translation resource. We per-
formed the necessary document and term list normalization; in this
case, removing accentsfrom document surface forms to enable match-
ing with the un-accentedterm list entries, converting case, and split-
ting clitic contractions, such as l?horlage, on punctuation. We trained
the statistical stemming rules on a sample of the bilingual term list
and document collection and applied these rules in stemming back-
off. Our default condition was run with top-2 balanced translation
using the Brown corpus as a source of target language unigram fre-
quency information. Translated documents were then indexed with
Stage 1 Stage 2 Stage 3 Stage 4
Match 70% 3% 0.5% 1%
Table 1: Percentage of document terms translated at each stage
of 4-stage backoff translation with statistical stemming.
the InQuery (version 3.1p1) system, using the kstem stemmer for
English stemming and InQuery?s default English stopword list. Long
queries were formed by concatenatingthe title, description, and nar-
rative fields of the original query specification. The resulting word
sequence was enclosed in an InQuery #sum operator, indicating
unweighted sum.
Our figure of merit for the evaluations below is mean (uninter-
polated) average precision computed using trec eval 2 across the 34
topics in the CLEF evaluation for which relevant French documents
are known.
4.1.2 Backoff Translation with Statistical Stemming
We first contrast the above baseline system with the effectiveness
of an otherwise identical run without the stemming backoff compo-
nent. Terms in the documents are thus only translated if there is an
exact match between the surface form in the document and a surface
form in the bilingual term list. We find that mean average preci-
sion for unstemmed translation is 0.19 as compared with 0.2919 for
our baseline system including stemming backoff based on trained
rules. This difference is significant at p < 0:05, by paired t-test,
two-tailed. The per-query effectiveness is illustrated in Figure 1.
Backoff translation improves translation coverage while retaining
relatively high precision of matching in contrast to unstemmed ef-
fectiveness.
Backoff translation improves cross-languageinformation retrieval
effectiveness by improving translation coverage of the terms in the
document collection. Using the statistical stemmer, by-token cover-
age of document terms increased by 7coverage. The different stages
of the four-stage backoff process contributed as illustrated in 1. The
majority of terms match in the Stage 1 exact match, accounting for
70% of the term instances in the documents. The remaining stages
each accountfor between 0.5% and 3% of the document terms, while
20% of document term instances remain untranslatable. However,
this relatively small increase in coverage results in the highly sig-
nificant improvement in retrieval effectiveness above.
4.1.3 Top-2 Balanced Translation
Here we contrast top-2 balanced translation with top-1 transla-
tion. We retain statistical stemming backoff for the top-1 transla-
tion. We replace each French document term with the highest ranked
English translation by target languageunigram frequencyin the Brown
Corpus as detailed above, retaining the original French term when
no translation is found in the bilingual term list. We achieve a mean
average precision of 0.2532 in contrast with the baseline condition.
This difference is significant at p < 0:01 by paired t-test, two-tailed.
We can effectively incorporate additional translations using top-2
balanced translation without degrading performance by introducing
significant additional noise. A query-by-query contrast is presented
in Figure 2.
4.1.4 Document Expansion
We evaluatedpost-translation documentexpansionusing the Topic
Detection and Tracking (TDT-3) collection. For this evaluation, we
used the TDT-1999 topic detection task evaluation framework, but
2Available at ftp://ftp.cs.cornell.edu/pub/smart/.
because out focus in this paper is on ranked retrieval effectiveness
we report mean uninterpolated averageprecision rather than the topic-
weighted detection cost measure typically reported in TDT. In the
topic detection task, the system is presented with one or more exem-
plar stories from the training epoch?a form of query-by-example?
and must determine whether each story in the evaluation epoch ad-
dresses either the same seminal event or activity or some directly
related event or activity. This is generally thought to be a some-
what narrower formulation than the more widely used notion of top-
ical relevance, but it seems to be well suited to query-by-example
evaluations. The TDT-1999 tracking task was multilingual, search-
ing stories in both English and Mandarin Chinese, and multi-modal,
involving both newswire text and broadcast news audio. We fo-
cus on the cross-language spoken document retrieval component of
the tracking task, using English exemplars to identify on-topic sto-
ries in Mandarin Chinese broadcast news audio. We compare top-1
translation of the Mandarin Chinese stories with and without post-
translation document expansion.3 We used the earlier TDT-2 En-
glish newswire text collection as our side collection for expansion.
We perform topic tracking on 60 topics with 4 exemplarseach. Here,
we report the mean average precision on the 55 topics for which
there are on-topic Mandarin audio stories. The mean uninterpolated
averageprecision for retrieval of unexpandeddocuments is 0.36 while
post-translation document expansion raises this figure to 0.41. This
difference is significant at p < 0:01 by paired t-test, two-tailed. The
contrast is illustrated in Figure 3. Interestingly, when we tried this
with French, we noted that expansion tended to select terms from
the few foreign-language documents that happened to be present in
our expansion collection. We have not yet explored that effect in de-
tail, but this observation suggests that the document expansion may
be sensitive to the characteristics of the expansioncollection that are
not immediately apparent.
4.2 The Learning Curve
We havefound that retargeting can be accomplishedquite quickly
(a day without document expansion, three days for TREC-sized col-
lections with document expansion), but only if the required infras-
tructure is in place. Adapting a system that was developed initially
for Chinese to handle French documents required several weeks,
with most of that effort invested in development of four-stage back-
off translation and statistical stemming. Further adapting the system
to handle German documents revealed the importance of compound
splitting, a problem that we will ultimately need to address by incor-
porating a more general segmentationstrategy than we used initially
for Chinese. In extending the system to Italian we have found that
although our statistical stemmer presently performs poorly in that
language, we can achieve quite credible results even with a fairly
small (17,313 term) bilingual term list using a freely available Mus-
cat stemmer (which exist for ten languages). So although it is pos-
sible in concept to retarget to a new language in just a few days, ex-
tending the system typically takes us between one and three weeks
because we are still climbing the learning curve.
5. CONCLUSION
By building on the lessons learned using the TREC, CLEF, NT-
CIR, and TDT collections, we have sought to build an infrastructure
that can be applied to a broad array of languages. Arabic and Ko-
rean collections are expected to become available in the next year,
and we are now evolving our interface to support user studies. Our
approach is distinguished by support for interactive retrieval even
3Since Mandarin Chinese has little surface morphology, we omit
backoff translation in this case.
Figure 1: Comparison of effectiveness of backoff versus unstemmed translation of French documents: Bars above x-axis indicate
backoff transltion outperforms unstemmed translation.
Figure 2: Comparison of effectiveness of top-2 balanced versus top-1 translation of French documents: Bars above x-axis indicate
?Top-2? outperforms ?Top-1?
Figure 3: Comparison of effectiveness of top-1 post-translation document expansion versus bare top-1 translation of Chinese docu-
ments: Bars above x-axis indicate document expansion outperforms bare translation
in languagesfor which machine translation is presently unavailable,
and our ultimate goal is to characterize how closely we can approx-
imate the retrieval effectiveness users would obtain if they had the
best available machine translations for the retrieved documents.
Acknowledgements
This work was supported in part by DARPA contract N6600197C8540
and DARPA cooperative agreement N660010028910.
6. ADDITIONAL AUTHORS
Clara I. Cabezas ( Department of Linguistics, top University of
Maryland, College Park, email: clarac@umiacs.umd.edu)
7. REFERENCES
[1] G.-A. Levow and D. W. Oard. Translingual topic tracking
with PRISE. In Working Notes of the Third Topic Detection
and Tracking Worksho p, Feb. 2000.
http://www.glue.umd.edu/oard/research.html.
[2] D. W. Oard, G.-A. Levow, and C. I. Cabezas. CLEF
experiments at Maryland: Statistical stemming and backof f
translation. In C. Peters, editor, Proceedings of the First
Cross-Language Evaluation Forum. 2001. To appear.
http://www.glue.umd.edu/oard/research .html.
[3] D. W. Oard and P. Resnik. Support for interactive document
selection in cross-language information retrieval. Information
Processing and Management, 35(3):363?379, July 1999.
[4] A. Singhal, J. Choi, D. Hindle, J. Hirschberg, F. Pereira, and
S. Whittaker. AT&T at TREC-7 SDR Track. In Proceedings of
the DARPA Broadcast News Workshop, 1999.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 779?786, Vancouver, October 2005. c?2005 Association for Computational Linguistics
The Hiero Machine Translation System:
Extensions, Evaluation, and Analysis
David Chiang, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, Michael Subotin
Institute for Advanced Computer Studies (UMIACS)
University of Maryland, College Park, MD 20742, USA
{dchiang,alopez,nmadnani,christof,resnik,msubotin}@umiacs.umd.edu
Abstract
Hierarchical organization is a well known prop-
erty of language, and yet the notion of hierarchi-
cal structure has been largely absent from the best
performing machine translation systems in recent
community-wide evaluations. In this paper, we dis-
cuss a new hierarchical phrase-based statistical ma-
chine translation system (Chiang, 2005), present-
ing recent extensions to the original proposal, new
evaluation results in a community-wide evaluation,
and a novel technique for fine-grained comparative
analysis of MT systems.
1 Introduction
Hierarchical organization is a well known prop-
erty of language, and yet the notion of hierarchi-
cal structure has, for the last several years, been
absent from the best performing machine transla-
tion systems in community-wide evaluations. Statis-
tical phrase-based models (e.g. (Och and Ney, 2004;
Koehn et al, 2003; Marcu andWong, 2002)) charac-
terize a source sentence f as a flat partition of non-
overlapping subsequences, or ?phrases?, f?1 ? ? ? f?J ,
and the process of translation involves selecting tar-
get phrases e?i corresponding to the f? j and modify-
ing their sequential order. The need for some way
to model aspects of syntactic behavior, such as the
tendency of constituents to move together as a unit,
is widely recognized?the role of syntactic units is
well attested in recent systematic studies of trans-
lation (Fox, 2002; Hwa et al, 2002; Koehn and
Knight, 2003), and their absence in phrase-based
models is quite evident when looking at MT system
output. Nonetheless, attempts to incorporate richer
linguistic features have generally met with little suc-
cess (Och et al, 2004a).
Chiang (2005) introduces Hiero, a hierarchical
phrase-based model for statistical machine transla-
tion. Hiero extends the standard, non-hierarchical
notion of ?phrases? to include nonterminal sym-
bols, which permits it to capture both word-level and
phrase-level reorderings within the same framework.
The model has the formal structure of a synchronous
CFG, but it does not make any commitment to a
linguistically relevant analysis, and it does not re-
quire syntactically annotated training data. Chiang
(2005) reported significant performance improve-
ments in Chinese-English translation as compared
with Pharaoh, a state-of-the-art phrase-based system
(Koehn, 2004).
In Section 2, we review the essential elements
of Hiero. In Section 3 we describe extensions to
this system, including new features involving named
entities and numbers and support for a fourfold
scale-up in training set size. Section 4 presents new
evaluation results for Chinese-English as well as
Arabic-English translation, obtained in the context
of the 2005 NISTMT Eval exercise. In Section 5, we
introduce a novel technique for fine-grained com-
parative analysis of MT systems, which we em-
ploy in analyzing differences between Hiero?s and
Pharaoh?s translations.
2 Hiero
Hiero is a stochastic synchronous CFG, whose pro-
ductions are extracted automatically from unanno-
tated parallel texts, and whose rule probabilities
form a log-linear model learned by minimum-error-
rate training; together with a modified CKY beam-
search decoder (similar to that of Wu (1996)). We
describe these components in brief below.
779
S ? ?S 1 X 2 ,S 1 X 2 ?
S ? ?X 1 ,X 1 ?
X ? ?yu X 1 you X 2 , have X 2 with X 1 ?
X ? ?X 1 de X 2 , the X 2 that X 1 ?
X ? ?X 1 zhiyi, one of X 1 ?
X ? ?Aozhou,Australia?
X ? ?shi, is?
X ? ?shaoshu guojia, few countries?
X ? ?bangjiao, diplomatic relations?
X ? ?Bei Han,North Korea?
Figure 1: Example synchronous CFG
2.1 Grammar
A synchronous CFG or syntax-directed transduction
grammar (Lewis and Stearns, 1968) consists of pairs
of CFG rules with aligned nonterminal symbols. We
denote this alignment by coindexation with boxed
numbers (Figure 1). A derivation starts with a pair
of aligned start symbols, and proceeds by rewrit-
ing pairs of aligned nonterminal symbols using the
paired rules (Figure 2).
Training begins with phrase pairs, obtained as by
Och, Koehn, and others: GIZA++ (Och and Ney,
2000) is used to obtain one-to-many word align-
ments in both directions, which are combined into a
single set of refined alignments using the ?final-and?
method of Koehn et al (2003); then those pairs of
substrings that are exclusively aligned to each other
are extracted as phrase pairs.
Then, synchronous CFG rules are constructed
out of the initial phrase pairs by subtraction: ev-
ery phrase pair ? f? , e?? becomes a rule X ? ? f? , e??,
and a phrase pair ? f? , e?? can be subtracted from a
rule X ? ??1 f??2, ?1e??2? to form a new rule X ?
??1X i ?2, ?1X i ?2?, where i is an index not already
used. Various filters are also applied to reduce the
number of extracted rules. Since one of these filters
restricts the number of nonterminal symbols to two,
our extracted grammar is equivalent to an inversion
transduction grammar (Wu, 1997).
2.2 Model
The model is a log-linear model (Och and Ney,
2002) over synchronous CFG derivations. The
weight of a derivation is PLM(e)?LM , the weighted
language model probability, multiplied by the prod-
uct of the weights of the rules used in the derivation.
The weight of each rule is, in turn:
(1) w(X ? ??, ??) =
?
i
?i(X ? ??, ??)?i
where the ?i are features defined on rules. The ba-
sic model uses the following features, analogous to
Pharaoh?s default feature set:
? P(? | ?) and P(? | ?)
? the lexical weights Pw(? | ?) and Pw(? | ?)
(Koehn et al, 2003);1
? a phrase penalty exp(1);
? a word penalty exp(l), where l is the number of
terminals in ?.
The exceptions to the above are the two ?glue?
rules, which are the rules with left-hand side S in
Figure 1. The second has weight one, and the first
has weight w(S ? ?S 1 X 2 ,S 1 X 2 ?) = exp(??g),
the idea being that parameter ?g controls the model?s
preference for hierarchical phrases over serial com-
bination of phrases.
Phrase translation probabilities are estimated by
relative-frequency estimation. Since the extraction
process does not generate a unique derivation for
each training sentence pair, a distribution over pos-
sible derivations is hypothesized, which gives uni-
form weight to all initial phrases extracted from a
sentence pair and uniform weight to all rules formed
out of an initial phrase. This distribution is then used
to estimate the phrase translation probabilities.
The lexical-weighting features are estimated us-
ing a method similar to that of Koehn et al (2003).
The language model is a trigram model with mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998), trained using the SRI-LM toolkit (Stolcke,
2002).
1This feature uses word alignment information, which is dis-
carded in the final grammar. If a rule occurs in training with
more than one possible word alignment, Koehn et al take the
maximum lexical weight; Hiero uses a weighted average.
780
?S 1 ,S 1 ? ? ?S 2 X 3 ,S 2 X 3 ?
? ?S 4 X 5 X 3 ,S 4 X 5 X 3 ?
? ?X 6 X 5 X 3 ,X 6 X 5 X 3 ?
? ?Aozhou X 5 X 3 ,Australia X 5 X 3 ?
? ?Aozhou shi X 3 ,Australia is X 3 ?
? ?Aozhou shi X 7 zhiyi,Australia is one of X 7 ?
? ?Aozhou shi X 8 de X 9 zhiyi,Australia is one of the X 9 that X 8 ?
? ?Aozhou shi yu X 1 you X 2 de X 9 zhiyi,Australia is one of the X 9 that have X 2 with X 1 ?
Figure 2: Example partial derivation of a synchronous CFG.
The feature weights are learned by maximizing
the BLEU score (Papineni et al, 2002) on held-out
data, using minimum-error-rate training (Och, 2003)
as implemented by Koehn. The implementation was
slightly modified to ensure that the BLEU scoring
matches NIST?s definition and that hypotheses in
the n-best lists are merged when they have the same
translation and the same feature vector.
3 Extensions
In this section we describe our extensions to the base
Hiero system that improve its performance signif-
icantly. First, we describe the addition of two new
features to the Chinese model, in a manner similar
to that of Och et al (2004b); then we describe how
we scaled the system up to a much larger training
set.
3.1 New features
The LDC Chinese-English named entity lists (900k
entries) are a potentially valuable resource, but
previous experiments have suggested that simply
adding them to the training data does not help
(Vogel et al, 2003). Instead, we placed them in
a supplementary phrase-translation table, giving
greater weight to phrases that occurred less fre-
quently in the primary training data. For each en-
try ? f , {e1, . . . , en}?, we counted the number of times
c( f ) that f appeared in the primary training data,
and assigned the entry the weight 1c( f )+1 , which
was then distributed evenly among the supplemen-
tary phrase pairs {? f , ei?}. We then created a new
model feature for named entities. When one of these
supplementary phrase pairs was used in transla-
tion, its feature value for the named-entity feature
was the weight defined above, and its value in the
other phrase-translation and lexical-weighting fea-
tures was zero. Since these scores belonged to a sep-
arate feature from the primary translation probabili-
ties, they could be reweighted independently during
minimum-error-rate training.
Similarly, to process Chinese numbers and dates,
we wrote a rule-based Chinese number/date transla-
tor, and created a new model feature for it. Again,
the weight given to this module was optimized
during minimum-error-rate training. In some cases
we wrote the rules to provide multiple uniformly-
weighted English translations for a Chinese phrase
(for example,k? (bari) could become ?the 8th? or
?on the 8th?), allowing the language model to decide
between the options.
3.2 Scaling up training
Chiang (2005) reports on experiments in Chinese-
English translation using a model trained on
7.2M+9.2M words of parallel data.2 For the NIST
MT Eval 2005 large training condition, consider-
ably more data than this is allowable. We chose
to use only newswire data, plus data from Sino-
rama, a Taiwanese news magazine.3 This amounts
to almost 30M+30M words. Scaling to this set re-
quired reducing the initial limit on phrase lengths,
previously fixed at 10, to avoid explosive growth of
2Here and below, the notation ?X + Y words? denotes X
words of foreign text and Y words of English text.
3From Sinorama, only data from 1991 and later were used,
as articles prior to that were translated quite loosely.
781
the extracted grammar. However, since longer initial
phrases can be beneficial for translation accuracy,
we adopted a variable length limit: 10 for the FBIS
corpus and other mainland newswire sources, and 7
for the HK News corpus and Sinorama. (During de-
coding, limits of up to 15 were sometimes used; in
principle these limits should all be the same, but in
practice it is preferable to tune them separately.)
For Arabic-English translation, we used the ba-
sic Hiero model, without special features for named
entities or numbers/dates. We again used only the
newswire portions of the allowable training data; we
also excluded the Ummah data, as the translations
were found to be quite loose. Since this amounted
to only about 1.5M+1.5M words, we used a higher
initial phrase limit of 15 during both training and de-
coding.
4 Evaluation
Figure 1 shows the performance of several systems
on NIST MT Eval 2003 Chinese test data: Pharaoh
(2004 version), trained only on the FBIS data; Hi-
ero, with various combinations of the new features
and the larger training data.4 This table also shows
Hiero?s performance on the NIST 2005 MT evalua-
tion task.5 The metric here is case-sensitive BLEU.6
Figure 2 shows the performance of two systems
on Arabic in the NIST 2005 MT Evaluation task:
DC, a phrase-based decoder for a model trained by
Pharaoh, and Hiero.
5 Analysis
Over the last few years, several automatic metrics
for machine translation evaluation have been intro-
duced, largely to reduce the human cost of itera-
tive system evaluation during the development cy-
cle (Lin and Och, 2004; Melamed et al, 2003; Pap-
ineni et al, 2002). All are predicated on the concept
4The third line, corresponding to the model without new fea-
tures trained on the larger data, may be slightly depressed be-
cause the feature weights from the fourth line were used instead
of doing minimum-error-rate training specially for this model.
5Full results are available at http://www.nist.gov/
speech/tests/summaries/2005/mt05.htm. For this test, a
phrase length limit of 15 was used during decoding.
6For this task, the translation output was uppercased using
the SRI-LM toolkit: essentially, it was decoded again using
an HMM whose states and transitions are a trigram language
model of cased English, and whose emission probabilities are
reversed, i.e., probability of cased word given lowercased word.
System Features Train MT03 MT05
Pharaoh standard FBIS 0.268
Hiero standard FBIS 0.288
Hiero standard full 0.329
Hiero +nums, names full 0.339 0.300
Table 1: Chinese results. (BLEU-4; MT03 case-
insensitive, MT05 case-sensitive)
System Train MT05
DC full 0.399
Hiero full 0.450
Table 2: Arabic results. (BLEU-4; MT03 case-
insensitive, MT05 scores case-sensitive.
of n-gram matching between the sentence hypothe-
sized by the translation system and one or more ref-
erence translations?that is, human translations for
the test sentence. Although the motivations and for-
mulae underlying these metrics are all different, ul-
timately they all produce a single number represent-
ing the ?goodness? of the MT system output over a
set of reference documents. This facility is valuable
in determining whether a given system modification
has a positive impact on overall translation perfor-
mance. However, the metrics are all holistic. They
provide no insight into the specific competencies or
weaknesses of one system relative to another.
Ideally, we would like to use automatic methods
to provide immediate diagnostic information about
the translation output?what the system does well,
and what it does poorly. At the most general level,
we want to know how our system performs on the
two most basic problems in translation?word trans-
lation and reordering. Unigram precision and recall
statistics tell us something about the performance of
an MT system?s internal translation dictionaries, but
nothing about reordering. It is thought that higher or-
der n-grams correlate with the reordering accuracy
of MT systems, but this is again a holistic metric.
What we would really like to know is howwell the
system is able to capture systematic reordering pat-
terns in the input, which ones it is successful with,
and which ones it has difficulty with. Word n-grams
are little help here: they are too many, too sparse, and
it is difficult to discern general patterns from them.
782
5.1 A New Analysis Method
In developing a new analysis method, we are moti-
vated in part by recent studies suggesting that word
reorderings follow general patterns with respect to
syntax, although there remains a high degree of flex-
ibility (Fox, 2002; Hwa et al, 2002). This suggests
that in a comparative analysis of two MT systems, it
may be useful to look for syntactic patterns that one
system captures well in the target language and the
other does not, using a syntax based metric.
We propose to summarize reordering patterns us-
ing part-of-speech sequences. Unfortunately, recent
work has shown that applying statistical parsers to
ungrammatical MT output is unreliable at best, with
the parser often assigning unreasonable probabili-
ties and incongruent structure (Yamada and Knight,
2002; Och et al, 2004a). Anticipating that this
would be equally problematic for part-of-speech
tagging, we make the conservative choice to apply
annotation only to the reference corpus. Word n-
gram correspondences with a reference translation
are used to infer the part-of-speech tags for words in
the system output.
First, we tagged the reference corpus with parts
of speech. We used MXPOST (Ratnaparkhi, 1996),
and in order to discover more general patterns, we
map the tag set down after tagging, e.g. NN, NNP,
NNPS and NNS all map to NN. Second, we com-
puted the frequency freq(ti . . . t j) of every possible
tag sequence ti . . . t j in the reference corpus. Third,
we computed the correspondence between each hy-
pothesis sentence and each of its corresponding ref-
erence sentences using an approximation to max-
imum matching (Melamed et al, 2003). This al-
gorithm provides a list of runs or contiguous se-
quences of words ei . . . e j in the reference that are
also present in the hypothesis. (Note that runs are
order-sensitive.) Fourth, for each recalled n-gram
ei . . . e j, we looked up the associated tag sequence
ti . . . t j and incremented a counter recalled(ti . . . t j).
Finally, we computed the recall of tag patterns,
R(ti . . . t j) = recalled(ti . . . t j)/freq(ti . . . t j), for all
patterns in the corpus.
By examining examples of these tag sequences in
the reference corpus and their hypothesized trans-
lations, we expect to gain some insight into the
comparative strengths and weaknesses of the MT
systems? reordering models. (An interactive plat-
form for this analysis is demonstrated by Lopez and
Resnik (2005).)
5.2 Chinese
We performed tag sequence analysis on the Hiero
and Pharaoh systems trained on the FBIS data only.
Table 3 shows those n-grams for which Hiero and
Pharaoh?s recall differed significantly (p < 0.01).
The numbers shown are the ratio of Hiero?s recall
to Pharaoh?s. Note that the n-grams on which Hi-
ero had better recall are dominated by fragments of
prepositional phrases (in the Penn Treebank tagset,
prepositions are tagged IN or TO).
Our hypothesis is that Hiero produces English PPs
better because many of them are translated from
Chinese phrases which have an NP modifying an NP
to its right, often connected with the particle? (de).
These are often translated into English as PPs, which
modify the NP to the left. A correct translation, then,
would have to reorder the two NPs. Notice in the ta-
ble that Hiero recalls proportionally more n-grams
as n increases, corroborating the intuition that Hiero
should be better at longer-distance reorderings.
Investigating this hypothesis qualitatively, we in-
spected the first five occurrences of the n-grams of
the first type on the list (JJ NN IN DT NN). Of
these, we omit one example because both systems
recalled the n-gram correctly, and one because they
differed only in lexical choice (Hiero matched the
5-gram with one reference sentence, Pharaoh with
zero). The other three examples are shown below (H
= Hiero, P = Pharaoh):
(2) T?
UN
?h
security
?Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 867?874, Vancouver, October 2005. c?2005 Association for Computational Linguistics
OCR Post-Processing for Low Density Languages
Okan Kolak
Computer Science and UMIACS
University of Maryland
College Park, MD 20742
okan@umiacs.umd.edu
Philip Resnik
Linguistics and UMIACS
University of Maryland
College Park, MD 20742
resnik@umiacs.umd.edu
Abstract
We present a lexicon-free post-processing
method for optical character recognition
(OCR), implemented using weighted fi-
nite state machines. We evaluate the
technique in a number of scenarios rele-
vant for natural language processing, in-
cluding creation of new OCR capabilities
for low density languages, improvement
of OCR performance for a native com-
mercial system, acquisition of knowledge
from a foreign-language dictionary, cre-
ation of a parallel text, and machine trans-
lation from OCR output.
1 Introduction
The importance of rapidly retargeting existing natu-
ral language processing (NLP) technologies to new
languages is widely accepted (Oard, 2003). Statisti-
cal NLP models have a distinct advantage over rule
based approaches to achieve this goal, as they re-
quire far less manual labor; however, training statis-
tical NLP methods requires on-line text, which can
be hard to find for so-called ?low density? languages
? that is, languages where few on-line resources ex-
ist. In addition, for many languages of interest input
data are available mostly in printed form, and must
be converted to electronic form prior to processing.
Optical character recognition (OCR) is often the
only feasible method to perform this conversion,
owing to its speed and cost-effectiveness. Unfor-
tunately, the performance of OCR systems is far
from perfect and recognition errors significantly de-
grade the performance of NLP applications. This is
true both in resource acquisition, such as automated
bilingual lexicon generation (Kolak et al, 2003),
and for end-user applications such as rapid machine
translation (MT) in the battlefield for document fil-
tering (Voss and Ess-Dykema, 2000). Moreover, for
low density languages, there simply may not be an
OCR system available.
In this paper, we demonstrate that via statistical
post-processing of existing systems, it is possible
to achieve reasonable recognition accuracy for low
density languages altogether lacking an OCR sys-
tem, to significantly improve on the performance of
a trainable commercial OCR system, and even to
improve significantly on a native commercial OCR
system.1 By taking a post-processing approach, we
require minimal assumptions about the OCR system
used as a starting point.
The proper role of our post-processing approach
depends on the language. For languages with little
commercial potential for OCR, it may well provide
the most practical path for language-specific OCR
development, given the expensive and time consum-
ing nature of OCR development for new languages
and the ?black box? nature of virtually all state-of-
the-art OCR systems. For languages where native
OCR development may take place, it is a fast, prac-
tical method that allows entry into a new language
until native OCR development catches up. For these,
and also for languages where native systems exist,
1Currently we assume the availability of an OCR system that
supports the script of the language-of-interest, or which is script
independent (Natarajan et al, 2001).
867
we show that post-processing can yield improve-
ments in performance.
Sections 2 and 3 describe the method and its im-
plementation. In Section 4 we cover a variety of rel-
evant NLP scenarios: Creating OCR capabilities for
Igbo, performing OCR on a dictionary for Cebuano,
using OCR to acquire the Arabic side of a common
parallel text, and evaluating the value of OCR post-
processing for machine translation of Arabic and
Spanish. In Sections 5 and 6 we discuss related work
and summarize our findings.
2 Post-Processing System
We use the noisy channel framework to formulate
the correction problem, revising our previous model
(Kolak et al, 2003). That model takes the form
P (O, b, a, C,W ) =
P (O, b|a,C,W )P (a|C,W )P (C|W )P (W )
whose components are a word-level source model
P (W ), a word-to-character model P (C|W ), a seg-
mentation model P (a|C,W ), and a model for char-
acter sequence transformation, P (O, b|a,C,W ). W
is the correct word sequence and C is the corre-
sponding character sequence, which is recognized
as O by the OCR system. a and b are segmentation
vectors for C and O.
The original model requires a lexicon that covers
all words in the processed text ? a strong assump-
tion, especially for low density languages. We con-
verted the model into a character-based one, remov-
ing the need for a lexicon. Generation of W is re-
placed by generation of C, which renders P (C|W )
irrelevant, and the model becomes
P (O, b, a, C) = P (O, b|a,C)P (a|C)P (C)
Although word-based models generally perform bet-
ter, moving from words to characters is a necessary
compromise because word-based models are useless
in the absence of a lexicon, which is the case for
many low-density languages.
In addition to eliminating the need for a lexicon,
we developed a novel method for handling word
merge/split errors.2 Rather than modeling these er-
2A merge error occurs when two or more adjacent items are
recognized as one, and a split error occurs when an item is rec-
ognized as two or more items. These errors can happen both at
word level and character level.
rors explicitly using a segmentation model, we sim-
ply treat them as character deletion/insertion errors
involving the space character, allowing us to handle
them within the error model. The segmentation step
is absorbed into the character transformation step,
so a and b are no longer necessary, hence the final
equation becomes
P (O,C) = P (O|C)P (C)
which is a direct application of the noisy chan-
nel model. We can describe the new generative
process as follows: First, a sequence of charac-
ters C are generated, with probability P (C), and
the OCR system converts it into O with probability
P (O|C). For example, if the actual input was a car
and it was recognized as ajar, P (ajar, a car) =
P (ajar|a car)P (a car). Using the channel model
to address word merge/split errors without actually
using a word level model is, to our knowledge, a
novel contribution of our approach.
3 Implementation
We implemented our post-processing system using
the framework of weighted finite state machines
(WFSM), which provides a strong theoretical foun-
dation and reduces implementation time, thanks to
freely available toolkits, such as the AT&T FSM
Toolkit (Mohri et al, 1998). It also allows easy
integration of our post-processor with numerous
NLP applications that are implemented using FSMs
(e.g. (Knight and Graehl, 1997; Kumar and Byrne,
2003)).
3.1 Source Model
The source model assigns probability P (C) to orig-
inal character sequences, C. We use character level
n-gram language models as the source model, since
n-gram models are simple, easy to train, and usually
achieve good performance. More complicated mod-
els that make use of constraints imposed by a par-
ticular language, such as vowel harmony, can be uti-
lized if desired. We used the CMU-Cambridge Lan-
guage Modeling Toolkit v2 (Clarkson and Rosen-
feld, 1997) for training, using Witten-Bell smooth-
ing and vocabulary type 1; all other parameters were
left at their default values.
868
3.2 Channel Model
The channel model assigns a probability to O given
that it was generated from C. We experimented with
two probabilistic string edit distance models for im-
plementing the channel model. The first, following
our earlier model (2003), permits single-character
substitutions, insertions, and deletions, with associ-
ated probabilities. For example, P (ajar|a car) ?
P (a7?a)P ( 7??)P (c7?j)P (a7?a)P (r 7?r). Note
that we are only considering the most likely edit se-
quence here, as opposed to summing over all pos-
sible ways to convert a car to ajar. The second
is a slightly modified version of the spelling correc-
tion model of Brill and Moore (2000).3 This model
allows many-to-many edit operations, which makes
P (liter|litre) ? P (l 7?l)P (i7?i)P (tre7?ter) pos-
sible. We will refer to the these as the single-
character (SC) and multi-character (MC) error mod-
els, respectively.
We train both error models over a set of
corresponding ground truth and OCR sequences,
?C,O?. Training is performed using expectation-
maximization: We first find the most likely edit
sequence for each training pair to update the edit
counts, and then use the updated counts to re-
estimate edit probabilities. For MC, after finding the
most likely edit sequence, extended versions of each
non-copy operation that include neighboring charac-
ters are also considered, which allows learning any
common multi-character mappings. Following Brill
and Moore, MC training performs only one iteration
of expectation-maximization.
In order to reduce the time and space require-
ments of the search at correction time, we impose a
limit on number of errors per token. Note that this is
not a parameter of the model, but a limit required by
its computational complexity. A lower limit will al-
most always result in lower correction performance,
so the highest possible limit allowed by time and
memory constraints should be used. It is possible to
correct more errors per token by iterating the correc-
tion process. However, iterative correction cannot
guarantee that the result is optimal under the model.
3We ignore the location of the error within the word, since it
is not as important for OCR as it is for spelling.
3.3 Chunking
Since we do not require a lexicon, we work on
lines of text rather than words. Unfortunately the
search space for correcting a complete line is pro-
hibitively large and we need a way to break it down
to smaller, independent chunks. The chunking step
is not part of the model, but rather a pre-processing
step: chunks are identified, each chunk is corrected
independently using the model, and the corrected
chunks are put back together to generate the output.
Spaces provide a natural break point for chunks.
However, split errors complicate the process: if parts
of a split word are placed in different chunks, the er-
ror cannot be corrected. For example, in Figure 1,
chunking (b) allows the model to produce the de-
sired output, but chunking (a) simply does not allow
combining ?sam? and ?ple? into ?sample?, as each
chunk is corrected independently.
Figure 1: Example of a bad and a good chunking
We address this by using the probabilities as-
signed to spaces by the source model for chunking.
We break the line into two chunks using the space
with the highest probability and repeat the process
recursively until all chunks are reduced to a rea-
sonable size, as defined by time and memory lim-
itations. Crucially, spurious spaces that cause split
errors are expected to have a low probability, and
therefore breaking the line using high probability
spaces reduces the likelihood of placing parts of a
split word in different chunks.
If a lexicon does happen to be available, we can
use it to achieve more reliable chunking, as follows.
The tokens of the input line that are present in the
lexicon are assumed to be correct. We identify runs
of out-of-lexicon tokens and attempt to correct them
together, allowing us to handle split errors. Note
that in this case the lexicon is used only to improve
chunking, not for correction. Consequently, cover-
age of the lexicon is far less important.
Our lexicon-free chunking algorithm placed an
erroneous boundary at 11.3% of word split points
for Arabic test data (Section 4.3). However, cor-
rection performance was identical to that of error-
869
Figure 2: A small excerpt from Aku
.
ko
.
free chunking.4 Incorrect decisions did not hurt be-
cause the correction method was not able to fix those
particular split errors, regardless. The errors of the
chunking and correction models coincided as they
both rely on the same language model. Therefore,
chunking errors are unlikely to reduce the correction
performance.
3.4 Correction
Correction is performed by estimating the most
probable source character sequence C? for a given
observed character sequence O, using the formula:
C? = argmax
C
{P (O|C)P (C)}
We first encode O as an FSA and compose it with
the inverted error model FST.5 The resulting FST is
then composed with the language model FSA. The
final result is a lattice that encodes all feasible se-
quences C, along with their probabilities, that could
have generated O. We take the sequence associated
with the most likely path through the lattice as C?.
4 Evaluation
We evaluate our work on OCR post-processing in
a number of scenarios relevant for NLP, including
creation of new OCR capabilities for low density
languages, improvement of OCR performance for
a native commercial system, acquisition of knowl-
edge from a foreign-language dictionary, creation of
a parallel text, and machine translation from OCR
output. The languages studied include Igbo, Ce-
buano, Arabic, and Spanish.
For intrinsic evaluation, we use the conventional
Word Error Rate (WER) metric, which is defined as
WER(C,O) =
WordEditDistance(C,O)
WordCount(C)
4Ignoring errors that result in valid words, lexicon-based
chunking is always error-free.
5Inversion reverses the direction of the error model, map-
ping observed sequences to possible ground truth sequences.
We do not use the Character Error Rate (CER) met-
ric, since for almost all NLP applications the unit of
information is the words. For extrinsic evaluation of
machine translation, we use the BLEU metric (Pap-
ineni et al, 2002).
4.1 Igbo: Creating an OCR System
Igbo is an African language spoken mainly in Nige-
ria by an estimated 10 to 18 million people, written
in Latin script. Although some Igbo texts use dia-
critics to mark tones, they are not part of the official
orthography and they are absent in most printed ma-
terials. Other than grammar books, texts for Igbo,
even hardcopy, are extremely difficult to obtain. To
our knowledge, the work reported here creates the
first OCR system for this language.
For the Igbo experiments, we used two sources.
The first is a small excerpt containing 6727 words
from the novel ?Juo Obinna? (Ubesie, 1993). The
second is a small collection of short stories named
?Aku
.
ko
.
Ife Nke Ndi
.
Igbo? (Green and Onwua-
maegbu, 1970) containing 3544 words. We will re-
fer to the former as ?Juo? and the latter as ?Aku
.
ko
.
?
hereafter. We generated the OCR data using a com-
mercial English OCR system.6 Juo image files were
generated by scanning 600dpi laser printer output at
300dpi resolution. Aku
.
ko
.
image files were gener-
ated by scanning photocopies from the bound hard-
copy at 300dpi. Figure 2 provides a small excerpt
from the actual Aku
.
ko
.
page images used for recog-
nition. For both texts, we used the first two thirds for
training and the remaining third for testing.
We trained error and language models (EMs and
LMs) using the training sets for Juo and Aku
.
ko
.
sep-
arately, and performed corrections of English OCR
output using different combinations of these mod-
els on both test sets. Table 1 shows the results for
the Juo test set while Table 2 presents the results
for Aku
.
ko
.
. The relative error reduction ranges from
30% to almost 80%. The SC error model performs
better than the MC error model under all conditions.
6Abby Fine Reader Professional Edition Version 7.0
870
Conditions Results
LM Data EM Data EM type WER (%) Red. (%)
Juo Juo MC 8.66 74.18
Juo Aku
.
ko
.
MC 15.23 54.59
Aku
.
ko
.
Juo MC 13.25 60.49
Aku
.
ko
.
Aku
.
ko
.
MC 19.08 43.11
Juo Juo SC 7.11 78.80
Juo Aku
.
ko
.
SC 11.49 65.74
Aku
.
ko
.
Juo SC 13.42 59.99
Aku
.
ko
.
Aku
.
ko
.
SC 18.92 43.59
Original OCR Output 35.44 -
Table 1: Post-correction WER for English OCR on Juo
Conditions Results
LM Data EM Data EM type WER (%) Red. (%)
Juo Juo MC 21.42 36.33
Juo Aku
.
ko
.
MC 18.08 46.25
Aku
.
ko
.
Juo MC 21.51 36.06
Aku
.
ko
.
Aku
.
ko
.
MC 18.16 46.02
Juo Juo SC 19.92 40.78
Juo Aku
.
ko
.
SC 16.49 50.98
Aku
.
ko
.
Juo SC 19.92 40.78
Aku
.
ko
.
Aku
.
ko
.
SC 16.40 51.25
Original OCR Output 33.64 -
Table 2: Post-correction WER for English OCR on Aku
.
ko
.
This is due to the fact that MC requires more train-
ing data than SC. Furthermore, most of the errors in
the data did not require many-to-many operations.
Results in Tables 1 and 2 are for 6-gram language
model and error limit of 5; corresponding 3-gram
error rates were 1% to 2% (absolute) higher.
The best correction performance is achieved when
both the EM and LM training data come from the
same source as the test data, almost doubling the
performance achieved when they were from a dif-
ferent source.7 Note that the amount of training data
is small, four to eight pages, so optimizing perfor-
mance via manual entry of document-specific train-
ing text is not unrealistic for scenarios involving
long documents such as books.
4.1.1 Using a Trainable OCR System
In an additional experiment with Igbo, we found that
post-processing can improve performance substan-
tially even when an OCR system trained on Igbo
characters is the starting point. In particular, the
commercial OCR system used for Igbo experiments
supports user-trained character shape models. Us-
7There was no overlap between training and test data under
any circumstance.
Conditions Results
LM Data EM Data WER (%) Red. (%)
Juo Juo 3.69 50.34
Juo Aku
.
ko
.
5.24 29.48
Aku
.
ko
.
Juo 5.08 31.63
Aku
.
ko
.
Aku
.
ko
.
7.38 0.67
Original OCR Output 7.43 -
Table 3: Post-correction WER for trained OCR system on Juo
ing Juo as the source, we trained the commercial
OCR system manually on Igbo characters, result-
ing in a 7.43% WER on Juo without postprocess-
ing.8 Note that this is slightly higher than the 7.11%
WER achieved using an English OCR system to-
gether with our post-processing model. We used a
6-gram LM, and a SC EM with error limit of 5. Ta-
ble 3 shows that by post-processing the Igbo-trained
OCR system, we reduce the word error rate by 50%.
4.2 Cebuano: Acquiring a Dictionary
Cebuano is a language spoken by about 15 million
people in the Philippines, written in Latin script.
The scenario for this experiment is converting a
Cebuano hardcopy dictionary into electronic form,
as in DARPA?s Surprise Language Dry Run (Oard,
2003). The dictionary that we used had diacritics,
probably to aid in pronunciation. The starting-point
OCR data was generated using a commercial OCR
system.9 The fact that the tokens to be corrected
come from a dictionary means (1) there is little con-
text available and (2) word usage frequencies are not
reflected. Character-based models may be affected
by these considerations, but probably not to the ex-
tent that word-based models would be.
Table 4 shows WER for Cebuano after post-
processing. The size column represents the number
of dictionary entries used for training, where each
entry consists of one or more Cebuano words. As
can be seen from the table, our model reduces WER
substantially for all cases, ranging from 20% to 50%
relative reduction. As expected, the correction per-
formance increases with the amount of training data;
note, however, that we achieve reasonable correction
performance even using only 500 dictionary entries
for training.
8The system trains by attempting OCR on a document and
asking for the correct character whenever it is not confident.
9ScanSoft Developer?s Kit 2000, which has no built-in sup-
port for Cebuano.
871
Conditions Results
Size LM EM WER (%) Red. (%)
500 3-gram SC 5.37 33.04
500 3-gram MC 5.05 37.03
500 6-gram SC 6.41 20.07
500 6-gram MC 5.33 33.54
1000 3-gram SC 5.33 33.54
1000 3-gram MC 4.63 42.27
1000 6-gram SC 5.58 30.42
1000 6-gram MC 4.67 41.77
27363 3-gram SC 4.34 45.89
27363 3-gram MC 4.14 48.38
27363 6-gram SC 4.55 43.27
27363 6-gram MC 3.97 50.50
Original OCR Output 8.02 -
Table 4: Post-correction WER for Cebuano
Contrary to the Igbo results, the MC error model
performs better than the SC error model. And, inter-
estingly, the 3-gram language model performs better
than the 6-gram model, except for the largest train-
ing data and MC error model combination. Both dif-
ferences are most likely caused by the implications
of using a dictionary as discussed above.
4.3 Arabic: Acquiring Parallel Text
We used Arabic to illustrate conversion from hard-
copy to electronic text for a widely available paral-
lel text, the Bible (Resnik et al, 1999; Kanungo et
al., 2005; Oard, 2003). We divided the Bible into
ten equal size segments, using the first segment for
training the error model, the first nine segments for
the language model, and the first 500 verses from
the last segment for testing. Since diacritics are only
used in religious text, we removed all diacritics. The
OCR data was generated using a commercial Ara-
bic OCR system.10 Note that this evaluation differs
from Igbo and Cebuano, as the experiments were
performed using an existing native OCR system. It
also allowed us to evaluate chunking, as Arabic data
has far more word merge/split errors compared to
Igbo and Cebuano.
Table 5 shows the correction performance for
Arabic under various conditions. The Limit col-
umn lists the maximum number of errors per to-
ken allowed and the M/S column indicates whether
correction of word merge/split errors was allowed.
We achieve significant reductions in WER for Ara-
bic. The first two rows show that the 6-gram lan-
10Sakhr Automatic Reader Version 6.0
Conditions Results
M/S LM Limit WER (%) Red. (%)
no 3-gram 2 22.14 10.33
no 6-gram 2 17.99 27.14
yes 3-gram 2 18.26 26.04
yes 3-gram 4 17.74 28.15
yes 5-gram 2 20.74 16.00
Original OCR Output 24.69 -
Table 5: Post-correction WER for Arabic
guage model performs much better than the 3-gram
model. Interestingly, higher order n-grams perform
worse when we allow word merge/split errors. Note
that for handling word merge/split errors we need to
learn the character distributions within lines, rather
than within words as we normally do. Consequently,
more training data is required for reliable parameter
estimation. Handling word merge/split errors im-
prove the performance, which is expected. Allow-
ing fewer errors per token reduces the performance,
since it is not possible to correct words that have
more character errors than the limit. Unfortunately,
increasing the error limit increases the search space
exponentially, making it impossible to use high lim-
its. As mentioned in Section 3.2, iterative correction
is a way to address this problem.
4.4 Extrinsic Evaluation: MT
While our post-processing methods reduce WER,
our main interest is their impact on NLP applica-
tions. We have performed machine translation ex-
periments to measure the effects of OCR errors and
the post-processing approach on NLP application
performance.
For Arabic, we trained a statistical MT system us-
ing the first nine sections of the Bible data. The lan-
guage model is trained using the CMU-Cambridge
toolkit and the translation model using the GIZA++
toolkit (Och and Ney, 2000). We used the ReWrite
decoder (Germann, 2003) for translation.
BLEU scores for OCR, corrected, and clean text
were 0.0116, 0.0141, and 0.0154, respectively. This
establishes that OCR errors degrade the performance
of the MT system, and we are able to bring the per-
formance much closer to the level of performance
on clean text by using post-processing. Clearly the
BLEU scores are quite low; we are planning to per-
form experiments on Arabic using a more advanced
translation system, such as Hiero (Chiang, 2005).
872
MT System Input Text BLEU Score
Systran OCR 0.2000
Systran Corrected 0.2606
Systran Clean 0.3188
ReWrite OCR 0.1792
ReWrite Corrected 0.2234
ReWrite Clean 0.2590
Table 6: Spanish-English translation results
In order to test in a scenario with better trans-
lation performance, we performed MT evaluations
using Spanish. We used a commercial translation
system, Systran, in addition to statistical translation.
More resources being available for this language,
corrected text for Spanish experiments was obtained
using our original model that takes advantage of a
lexicon (2003). Table 6 shows that scores are much
higher compared to Arabic, but the pattern of im-
provements using post-processing is the same.
5 Related Work
There has been considerable research on automatic
error correction in text. Kukich (1992) provides a
general survey of the research in the area. Unfor-
tunately, there is no standard evaluation benchmark
for OCR correction, and implementations are usu-
ally not publicly available, making a direct compar-
ison difficult.
Most correction methods are not suitable for
low density languages as they rely on lexicons.
Goshtasby and Ehrich (1988) present a lexicon-free
method based on probabilistic relaxation labeling.
However, they use the probabilities assigned to in-
dividual characters by the OCR system, which is
not always available. Perez-Cortes et al (2000) de-
scribe a method which does not have this limitation.
They use a stochastic FSM that accepts the smallest
k-testable language consistent with a representative
sample. While the method can handle words not in
its lexicon in theory, it was evaluated using a large
k to restrict corrections to the lexicon. They report
reducing error rate from 33% to below 2% on OCR
output of hand-written Spanish names.
In addition to providing alternatives, the litera-
ture provides complementary methods. Guyon and
Pereira (1995) present a linguistic post-processor
based on variable memory length Markov models
that is designed to be used as the language model
component of character recognizers. Their model
can be used as the source model for our method.
Since it is a variable length model, it can allow us
to handle higher order n-grams.
A script-independent OCR system is presented by
Natarajan et al (2001). The system is evaluated
on Arabic, Chinese, and English, achieving 0.5% to
5% CER under various conditions. Since our post-
processing method can be used to reduce the error
rate of a trained OCR system, the two methods can
be combined to better adapt to new languages.
Voss and Ess-Dykema (2000) evaluated the ef-
fects of OCR errors on MT in the context of the
FALCon project, which combines off-the-shelf OCR
and MT components to provide crude translations
for filtering. They report significant degradation in
translation performance as a result of OCR errors.
For instance, for the Spanish system, OCR process
reduced the number of words that can be recognized
by the translation module by more than 60%.
6 Conclusions
We have presented a statistical post-processing
method for OCR error correction that requires mini-
mal resources, aimed particularly at low density lan-
guages and NLP scenarios. The technique gains
leverage from existing OCR systems, enabling both
minimal-labor adaptation of systems to new low
density languages and improvements in native OCR
performance.
We rigorously evaluated our approach using real
OCR data, and have shown that we can achieve
recognition accuracy lower than that achieved by a
trainable OCR system for a new language. For Igbo,
a very low density language, adapting English OCR
achieved relative error reductions as high as 78%, re-
sulting in 7.11% WER. We also showed that the er-
ror rate of a trainable OCR system after training can
be further reduced up to 50% using post-processing,
achieving a WER as low as 3.7%. Post-processing
experiments using Cebuano validate our approach in
a dictionary-acquisition scenario, with a 50.5% rel-
ative reduction in error rate from 8.02% to 3.97%.
Evaluation on Arabic demonstrated that the error
rate for a native commercial OCR system can be re-
duced by nearly 30%. In addition, we measured the
impact of post-processing on machine translation,
873
quantifying OCR degradation of MT performance
and showing that our technique moves the perfor-
mance of MT on OCR data significantly closer to
performance on clean input. See Kolak (forthcom-
ing) for more details and discussion.
One limitation of our approach is its reliance on
an existing OCR system that supports the script of
the language of interest. Trainable OCR systems
are the only option if there is no OCR system that
supports the script of interest; however, training an
OCR system from scratch is usually a tedious and
time consuming task. Post-processing can be used
to reduce the training time and improve recognition
accuracy by aiding generation of more training data
once basic recognition capability is in place.
Acknowledgments
This research was supported in part by Department
of Defense contract RD-02-5700 and DARPA/ITO
Cooperative Agreement N660010028910. We are
grateful to Mohri et al for the AT&T FSM
Toolkit and Clarkson and Rosenfeld for the CMU-
Cambridge Toolkit. We thank David Doermann
and his students for the Cebuano text; Chinedu
Uchechukwu for the Igbo text and useful informa-
tion on the language. We also thank Mustafa Murat
T?k?r for his help in generating Igbo ground truth for
Aku
.
ko
.
, and for useful discussion.
References
Eric Brill and Robert C. Moore. 2000. An improved model
for noisy channel spelling correction. In Proceedings of the
ACL-00, Hong Kong, China, October.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the ACL-
05, pages 263?270, Ann Arbor, Michigan, USA, June.
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical lan-
guage modeling using the CMU-Cambridge toolkit. In Pro-
ceedings of the ESCA Eurospeech, Rhodes, Greece.
Ulrich Germann. 2003. Greedy decoding for statistical ma-
chine translation in almost linear time. In Proceedings of the
HLT-NAACL-03, Edmonton, Alberta, Canada, May.
Ardeshir Goshtasby and Roger W. Ehrich. 1988. Contex-
tual word recognition using probabilistic relaxation labeling.
Pattern Recognition, 21(5):455?462.
M. M. Green and M. O. Onwuamaegbu, editors. 1970. Aku
.
ko
.
Ife Nke Ndi
.
Igbo. Oxford University Press, Ibadan, Nigeria.
Isabelle Guyon and Fernando Pereira. 1995. Design of a lin-
guistic postprocessor using variable memory length Markov
models. In Proceedings of the ICDAR-95, volume 1, Mon-
treal, Quebec, Canada, August.
Tapas Kanungo, Philip Resnik, Song Mao, Doe wan Kim, and
Qigong Zheng. 2005. The Bible and multilingual opti-
cal character recognition. Communications of the ACM,
48(6):124?130.
Kevin Knight and Jonathan Graehl. 1997. Machine translitera-
tion. In Proceedings of the ACL-97, Madrid, Spain, July.
Okan Kolak, William Byrne, and Philip Resnik. 2003. A
generative probabilistic OCR model for NLP applications.
In Proceedings of the HLT-NAACL-03, Edmonton, Alberta,
Canada, May.
Okan Kolak. forthcoming. Cross-Lingual Utilization of NLP
Resources for New Languages. Ph.D. thesis, University of
Maryland, College Park, Maryland, USA.
Karen Kukich. 1992. Techniques for automatically correct-
ing words in text. ACM Computing Surveys, 24(4):377?439,
December.
Shankar Kumar and William Byrne. 2003. A weighted finite
state transducer implementation of the alignment template
model for statistical machine translation. In Proceedings of
the HLT-NAACL-03, Edmonton, Alberta, Canada, May.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley.
1998. A rational design for a weighted finite-state transducer
library. Lecture Notes in Computer Science, 1436.
Premkumar Natarajan, Zhidong Lu, Richard Schwartz, Issam
Bazzi, and John Makhoul. 2001. Multilingual machine
printed ocr. International Journal of Pattern Recognition
and Artificial Intelligence, 15(1):43?63.
Douglas W. Oard. 2003. The surprise language exercises.
ACM Transactions on Asian Language Information Process-
ing (TALIP), 2(2):79?84, June.
Franz. J. Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proceedings of the ACL-00, pages
440?447, Hongkong, China, October.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the ACL-02, Philedel-
phia, Pennsylvania, USA, July.
Juan Carlos Perez-Cortes, Juan-Carlos Amengual, Joaquim Ar-
landis, and Rafael Llobet. 2000. Stochastic error-correcting
parsing for OCR post-processing. In Proceedings of the
ICPR-00, Barcelona, Spain, September.
Philip Resnik, Mari Broman Olsen, and Mona Diab. 1999. The
Bible as a parallel corpus: Annotating the ?Book of 2000
Tongues?. Computers and the Humanities, 33(1-2):129?
153.
Tony Uchenna Ubesie. 1993. Juo Obinna. University Press
PLC, Ibadan, Nigeria. ISBN: 19575395X.
Clare R. Voss and Carol Van Ess-Dykema. 2000. When is an
embedded MT system ?good enough? for filtering? In Pro-
ceedings of the Workshop on Embedded MT Systems, ANLP-
NAACL-00, Seattle, Washington, USA, May.
874
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 12?13,
Vancouver, October 2005.
Pattern Visualization for Machine Translation Output
Adam Lopez
Institute for Advanced Computer Studies
Department of Computer Science
University of Maryland
College Park, MD 20742
alopez@cs.umd.edu
Philip Resnik
Institute for Advanced Computer Studies
Department of Linguistics
University of Maryland
College Park, MD 20742
resnik@umd.edu
Abstract
We describe a method for identifying system-
atic patterns in translation data using part-of-
speech tag sequences. We incorporate this
analysis into a diagnostic tool intended for de-
velopers of machine translation systems, and
demonstrate how our application can be used
by developers to explore patterns in machine
translation output.
1 Introduction
Over the last few years, several automatic metrics for ma-
chine translation (MT) evaluation have been introduced,
largely to reduce the human cost of iterative system evalu-
ation during the development cycle (Papineni et al, 2002;
Melamed et al, 2003). All are predicated on the con-
cept of n-gram matching between the sentence hypoth-
esized by the translation system and one or more ref-
erence translations?that is, human translations for the
test sentence. Although the formulae underlying these
metrics vary, each produces a single number represent-
ing the ?goodness? of the MT system output over a set
of reference documents. We can compare the numbers of
competing systems to get a coarse estimate of their rela-
tive performance. However, this comparison is holistic.
It provides no insight into the specific competencies or
weaknesses of either system.
Ideally, we would like to use automatic methods to pro-
vide immediate diagnostic information about the transla-
tion output?what the system does well, and what it does
poorly. At the most general level, we want to know how
our system performs on the two most basic problems in
translation ? word translation and reordering. Holistic
metrics are at odds with day-to-day hypothesis testing on
these two problems. For instance, during the develop-
ment of a new MT system we may may wish to compare
competing reordering models. We can incorporate each
model into the system in turn, and rank the results on a
test corpus using BLEU (Papineni et al, 2002). We might
then conclude that the model used in the highest-scoring
system is best. However, this is merely an implicit test
of the hypothesis; it does not tell us anything about the
specific strengths and weaknesses of each method, which
may be different from our expectations. Furthermore, if
we understand the relative strengths of each method, we
may be able to devise good ways to combine them, rather
than simply using the best one, or combining strictly by
trial and error. In order to fine-tune MT systems, we need
fine-grained error analysis.
What we would really like to know is how well the
system is able to capture systematic reordering patterns
in the input, which ones it is successful with, and which
ones it has difficulty with. Word n-grams are little help
here: they are too many, too sparse, and it is difficult to
discern general patterns from them.
2 Part-of-Speech Sequence Recall
In developing a new analysis method, we are motivated
in part by recent studies suggesting that word reorder-
ings follow general patterns with respect to syntax, al-
though there remains a high degree of flexibility (Fox,
2002; Hwa et al, 2002). This suggests that in a com-
parative analysis of two MT systems (or two versions of
the same system), it may be useful to look for syntactic
patterns that one system (or version) captures well in the
target language and the other does not, using a syntax-
based, recall-oriented metric.
As an initial step, we would like to summarize reorder-
ing patterns using part-of-speech sequences. Unfortu-
nately, recent work has confirmed the intuition that ap-
plying statistical analyzers trained on well-formed text to
the noisy output of MT systems produces unuseable re-
sults (e.g. (Och et al, 2004)). Therefore, we make the
conservative choice to apply annotation only to the refer-
ence corpus. Word n-gram correspondences with a refer-
ence translation are used to infer the part-of-speech tags
for words in the system output.
The method:
1. Part-of-speech tag the reference corpus. We used
12
Figure 1: Comparing two systems that differ significantly in their recall for POS n-gram JJ NN IN DT NN. The
interface uses color to make examples easy to find.
MXPOST (Ratnaparkhi, 1996), and in order to dis-
cover more general patterns, we map the tag set
down after tagging, e.g. NN, NNP, NNPS and NNS
all map to NN.
2. Compute the frequency freq(ti . . . t j) of every possi-
ble tag sequence ti . . . t j in the reference corpus.
3. Compute the correspondence between each hypoth-
esis sentence and each of its corresponding refer-
ence sentences using an approximation to maximum
matching (Melamed et al, 2003). This algorithm
provides a list of runs or contiguous sequences of
words ei . . .e j in the reference that are also present in
the hypothesis. (Note that runs are order-sensitive.)
4. For each recalled n-gram ei . . .e j, look up the asso-
ciated tag sequence ti . . . t j and increment a counter
recalled(ti . . . t j)
Using this method, we compute the recall of tag pat-
terns, R(ti . . . t j) = recalled(ti . . . t j)/freq(ti . . . t j), for all
patterns in the corpus.
To compare two systems (which could include two ver-
sions of the same system), we identify POS n-grams that
are recalled significantly more frequently by one system
than the other, using a difference-of-proportions test to
assess statistical significance. We have used this method
to analyze the output of two different statistical machine
translation models (Chiang et al, 2005).
3 Visualization
Our demonstration system uses an HTML interface to
summarize the observed pattern recall. Based on frequent
or significantly-different recall, the user can select and
visually inspect color-coded examples of each pattern of
interest in context with both source and reference sen-
tences. An example visualization is shown in Figure 1.
4 Acknowledgements
The authors would like to thank David Chiang, Christof
Monz, and Michael Subotin for helpful commentary on
this work. This research was supported in part by ONR
MURI Contract FCPO.810548265 and Department of
Defense contract RD-02-5700.
References
David Chiang, Adam Lopez, Nitin Madnani, Christof Monz,
Philip Resnik, and Michael Subotin. 2005. The hiero ma-
chine translation system: Extensions, evaluation, and analy-
sis. In Proceedings of HLT/EMNLP 2005, Oct.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of the 2002 Conference
on EMNLP, pages 304?311, Jul.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan Kolak.
2002. Evaluating translational correspondence using annota-
tion projection. In Proceedings of the 40th Annual Meeting
of the ACL, pages 392?399, Jul.
I. Dan Melamed, Ryan Green, and Joseph P. Turian. 2003.
Precision and recall of machine translation. In HLT-NAACL
2003 Companion Volume, pages 61?63, May.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin
Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and
Dragomir Radev. 2004. A smorgasbord of features for sta-
tistical machine translation. In Proceedings of HLT-NAACL
2004, pages 161?168, May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual Meeting
of the ACL, pages 311?318, Jul.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Conference on
EMNLP, pages 133?142, May.
13
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 35?42,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Cross-Language Parser Adaptation between Related Languages 
Daniel Zeman 
Univerzita Karlova 
?stav form?ln? a aplikovan? lingvistiky 
Malostransk? n?m?st? 25 
CZ-11800 Praha 
zeman@ufal.mff.cuni.cz 
Philip Resnik 
University of Maryland 
Department of Linguistics and  
Institute for Advanced Computer Studies 
College Park, MD 20742, USA 
resnik@umd.edu 
 
Abstract 
The present paper describes an approach to 
adapting a parser to a new language. 
Presumably the target language is much 
poorer in linguistic resources than the source 
language. The technique has been tested on 
two European languages due to test data 
availability; however, it is easily applicable 
to any pair of sufficiently related languages, 
including some of the Indic language group. 
Our adaptation technique using existing 
annotations in the source language achieves 
performance equivalent to that obtained by 
training on 1546 trees in the target language. 
1 Introduction 
Natural language parsing is one of the key areas of 
natural language processing, and its output is used 
in numerous end-user applications, e.g. machine 
translation or question answering. Unfortunately, it 
is not easy to build a parser for a resource-poor 
language. Either a reasonably-sized syntactically 
annotated corpus (treebank) or a human-designed 
formal grammar is typically needed. These types of 
resources are costly to build, both in terms of time 
and of the expenses on qualified manpower. Both 
also require, in addition to the actual annotation 
process, a substantial effort on treebank/grammar 
design, format specifications, tailoring of annota-
tion guidelines etc; the latter costs are rather con-
stant no matter how small the resulting corpus is. 
In this context, there is the intriguing question 
whether we can actually build a parser without a 
treebank (or a broad-coverage formal grammar) of 
the particular language. There is some related 
work that addresses the issue by a variety of means. 
Klein and Manning (2004) use a hybrid unsuper-
vised approach, which combines a constituency 
and a dependency model, and achieve an unlabeled 
F-score of 77.6% on Penn Treebank Wall Street 
Journal data (English), 63.9% on Negra Corpus 
(German), and 46.7% on the Penn Chinese Tree-
bank.1 Bod (2006) uses unsupervised data-oriented 
parsing; the input of his parser contains manually 
assigned gold-standard tags. He reports 64.2% 
unlabeled F-score on WSJ sentences up to 40 
words long.2 
Hwa et al (2004) explore a different approach to 
attacking a new language. They train Collins?s 
(1997) Model 2 parser on the Penn Treebank WSJ 
data and use it to parse the English side of a paral-
lel corpus. The resulting parses are converted to 
dependencies, the dependencies are projected to a 
second language using automatically obtained 
word alignments as a bridge, and the resulting de-
pendency trees cleaned up using a limited set of 
language-specific post-projection transformation 
rules. Finally a dependency parser for the target 
language is trained on this projected dependency 
treebank, and the accuracy of the parser is meas-
ured against a gold standard. Hwa et al report de-
pendency accuracy of 72.1 for Spanish, compara-
ble to a rule-based commercial parser; accuracy on 
Chinese is 53.9%, the equivalent of a parser trained 
on roughly 2000 sentences of the Penn Chinese 
Treebank (sentences ?40 words, average length 
20.6). 
                                                 
1 Note that in all these experiments they restrict themselves to 
sentences of 10 words or less. 
2 On sentences of ?10 words, Bod achieves 78.5% for English 
(WSJ), 65.4% for German (Negra) and 46.7% for Chinese 
(CTB). 
35
Our own approach is motivated by McClosky et 
al.?s (2006) reranking-and-self-training algorithm, 
used successfully in adapting a parser to a new 
domain. One can easily imagine viewing two dia-
lects of a language or even two related languages 
as two domains of one ?super-language? while the 
vocabulary will certainly differ (due to independ-
ently designed orthographies for the two lan-
guages), many morphological and syntactic proper-
ties may be shared. We trained Charniak and John-
son?s (2005) reranking parser on one language and 
applied it to another closely related language. In 
addition, we investigated the utility of large but 
unlabeled data in the target language, and of a 
large parallel corpus of the two languages.3 
2 Corpora and Other Resources 
The selection of our source and target languages 
was driven by the need for two closely related lan-
guages with associated treebanks. (In a real-world 
application we would not assume the existence of a 
target-language treebank, but one is needed here 
for evaluation.) Danish served as the source lan-
guage and Swedish as target, since these languages 
are closely related and there are freely available 
treebanks for both.4 
The Danish Dependency Treebank (Kromann et 
al. 2004) contains 5,190 sentences (94,386 tokens). 
The texts come from the Danish Parole Corpus 
(1998?2002, mixed domain). We split the data into 
4,900 training and 290 test sentences, keeping the 
276 not exceeding 40 words. 
The Swedish treebank Talbanken05 (Nivre et al 
2006) contains 11,042 sentences (191,467 tokens). 
It was converted at V?xj? from the much older 
Talbanken76 treebank, created at the Lund Univer-
sity. Again, the texts belong to mixed domains. We 
split the data to 10,700 training and 342 test sen-
tences, out of which 317 do not exceed 40 words. 
Both treebanks are dependency treebanks, while 
the Charniak-Johnson reranking parser works with 
phrase structures. For our experiments, we con-
                                                 
3 There are other approaches to domain adaptation as 
well. For instance, Steedman et al (2003) address do-
main adaptation using a weakly supervised method 
called co-training. Two parsers, each applying a differ-
ent strategy, mutually prepare new training examples for 
each other. We have not tested co-training for cross-
language adaptation. 
4 We used the CoNLL 2006 versions of these treebanks. 
verted the treebanks from dependencies to phrases, 
using the ?flattest-possible? algorithm (Collins et 
al. 1999; algorithm 2 of Xia and Palmer 2001). The 
morphological annotation of the treebanks helped 
us to label the non-terminals. Although the 
Charniak?s parser can be taught a new inventory of 
labels, we found it easier to map head morpho-tags 
directly to Penn-Treebank-style non-terminals. 
Hence the parser can think it?s processing Penn 
Treebank data. The morphological annotation of 
the treebanks is further discussed in Section 4. 
We also experimented with a large body of un-
annotated Swedish texts. Such data could theoreti-
cally be acquired by crawling the Web; here, how-
ever, we used the freely available JRC-Acquis cor-
pus of EU legislation (Steinberger et al 2006).5 
The Acquis corpus is segmented at the paragraph 
level. We ran a simple procedure to split the para-
graphs into sentences and pruned sentences with 
suspicious length, contents (sequence of dashes, 
for instance) or both. We ended up with 430,808 
Swedish sentences and 6,154,663 tokens. 
Since the Acquis texts are available in 21 lan-
guages, we can also exploit the Danish Acquis and 
its alignment with the Swedish one. We use it to 
study the similarity of the two languages, and for 
the ?gloss? experiment in Section 5.1. Paragraph-
level alignment is provided as part of Acquis and 
contains 283,509 aligned segments. Word-level 
alignment, needed for our experiment, was ob-
tained using GIZA++ (Och and Ney 2000). 
The treebanks are manually tagged with parts of 
speech and morphological information. For some 
of our experiments, we needed to automatically re-
tag the target (Swedish) treebank, and to tag the 
Swedish Acquis. For that purpose we used the 
Swedish tagger of Jan Haji?, a variant of Haji??s 
Czech tagger (Haji? 2004) retrained on Swedish 
data. 
3 Treebank Normalization 
The two treebanks were developed by different 
teams, using different annotation styles and guide-
lines. They would be systematically different even 
if their texts were in the same language, but it is 
                                                 
5 Legislative texts are a specialized domain that cannot 
be expected to match the domain of our treebanks, how-
ever vaguely defined it is. But presumably the domain 
matching would be even less trustworthy if we acquired 
the unlabeled data from the web. 
36
the impact of the language difference, not annota-
tion style differences, that we want to measure; 
therefore we normalize the treebanks so that they 
are as similar as possible. 
While this may sound suspicious at first glance 
(?wow, are they refining their test data?!?), it is 
important to understand why it does not 
unacceptably bias the results. If our method were 
applied to a new language, where no treebank 
exists, trees conforming to the annotation scenario 
of a treebank of related language would be 
perfectly satisfying. In addition, note that we apply 
only systematic changes, mostly reversible. 
Moreover, the transformations can be done on the 
training data side, instead of test data. 
Following are examples of the style differences 
that underwent normalization: 
DET-ADJ-NOUN. Da: de norske piger. Sv:6 en 
gammal institution (?an old institution?) In DDT, 
the determiner governs the adjective and the noun. 
The approach of Talbanken (and of a number of 
other dependency treebanks) is that both deter-
miner and adjective depend on the noun. 
NUM-NOUN. Da: 100 procent (?100 percent?) 
Sv: tv? eventuellt tre ?r (?two, possibly three 
years?) In DDT, the number governs the noun. In 
Talbanken, the number depends on the noun. 
GENITIVE-NOMINATIVE. Da: Ruslands vej 
(?Russia?s way?) Sv: ?rs inkomster (?year?s 
income?). In DDT, the nominative noun (the 
owned) governs the noun in genitive (the owner). 
Talbanken goes the opposite way. 
COORDINATION. Da: F?r?erne og 
Gr?nland (?Faroe Islands and Greenland?) Sv: 
socialgrupper, nationer och raser (?social groups, 
nations and races?) In DDT, the last coordination 
member depends on the conjunction, the 
conjunction and everything else (punctuation, inner 
members) depend on the first member, which is the 
head of the coordination. In Talbanken, every 
member depends on the previous member, commas 
and conjunctions depend on the member following 
them. 
4 Mapping Tag Sets 
The nodes (words) of the Danish Dependency 
Treebank are tagged with the Parole morphological 
                                                 
6 These are separate examples from the two treebanks. 
They are not translations of each other! 
tags. Talbanken is tagged using the much coarser 
Mamba tag set (part of speech, no morphology). 
The tag inventory of Haji??s tagger is quite similar 
to the Danish Parole tags, but not identical. We 
need to be able to map tags from one set to the 
other. In addition, we also convert pre-terminal 
tags to the Penn Treebank tag set when converting 
dependencies to constituents. 
Mapping tag sets to each other is obviously an 
information-lossy process, unless both tag sets 
cover identical feature-value spaces. Apart from 
that, there are numerous considerations that make 
any such conversion difficult, especially when the 
target tags have been designed for a different 
language. 
We take an Interlingua-like (or Inter-tag-set) 
approach. Every tag set has a driver that 
implements decoding of the tags into a nearly 
universal feature space that we have defined, and 
encoding of the feature values by the tags. The 
encoding is (or aims at being) independent of 
where the feature values come from, and the 
decoding does not make any assumptions about the 
subsequent encoding. Hence the effort put in 
implementing the drivers is reusable for other 
tagset pairs. 
The key function, responsible for the 
universality of the method, is encode(). 
Consider the following example. There are two 
features set, POS = ?noun? and GENDER = 
?masc?. The target set is not capable of encoding 
masculine nouns. However, it allows for ?noun? + 
?com? | ?neut?, or ?pronoun? + ?masc? | ?fem? | 
?com? | ?neut?. An internal rule of encode() 
indicates that the POS feature has higher priority 
than the GENDER feature. Therefore the algorithm 
will narrow the tag selection to noun tags. Then the 
gender will be forced to common (i.e. ?com?). 
Even the precise feature mapping does not 
guarantee that the distribution of the tags in two 
corpora will be reasonably close. All converted 
source tags will now fit in the target tag set. 
However, some tags of the target tag set may not 
be used, although they are quite frequent in the 
corpus where the target tags are native. Some 
examples:  
? Unlike in Talbanken, there are no deter-
miners in DDT. That does not mean there 
are no determiners in Danish ? but DDT 
tags them as pronouns. 
37
? Swedish tags encode a special feature of 
personal pronouns, ?subject? vs. ?object? 
form (the distinction between English he 
and him). DDT calls the same paradigm 
?nominative? vs. ?unmarked? case. 
? Most noun phrases in both languages 
distinguish just the common and neuter 
genders. However, some pronouns could be 
classified as masculine or feminine. 
Swedish tags use the masculine gender, 
Danish do not. 
? DDT does not use special part of speech for 
numbers ? they are tagged as adjectives. 
All of the above discrepancies are caused by 
differing designs, not by differences in language. 
The only linguistically grounded difference we 
were able to identify is the supine verb form in 
Swedish, missing from Danish. 
When not just the tag inventories, but also the 
tag distributions have to be made compatible 
(which is the case of our delexicalization 
experiments later in this paper), we can create a 
new hybrid tag set, omitting any information 
specific for one or the other side. Tags of both 
languages can then be converted to this new set, 
using the universal approach described above. 
5 Using Related Languages 
The Figure 1 gives an example of matching Danish 
and Swedish sentences. This is a real example 
from the Acquis corpus. Even a non-speaker of 
these languages can detect the evident correspon-
dence of at least 13 words, out of the total of 16 
(ignoring final punctuation). However, due to dif-
ferent spelling rules, only 5 word pairs are string-
wise identical. From a parser?s perspective, the rest 
is unknown words, as it cannot be matched against 
the vocabulary learned from training data. 
We explore two techniques of making unknown 
words known. We call them glosses and delexicali-
zation, respectively. 
5.1 Glosses 
This approach needs a Danish-Swedish (da-sv) 
bitext. As shown by Resnik and Smith (2003), 
parallel texts can be acquired from the Web, which 
makes this type of resource more easily available 
than a treebank. We benefited from the Acquis da-
sv alignments. 
Similarly to phrase-based translation systems, 
we used GIZA++ (Och and Ney 2000) to obtain 
one-to-many word alignments in both directions, 
then combined them into a single set of refined 
alignments using the ?final-and? method of Koehn 
et al (2003). The refined alignments provided us 
with two-way tables of a source word and all its 
possible translations, with weights. Using these 
tables, we glossed each Swedish word by its 
Danish, using the translation with the highest 
weight. 
The glosses are used to replace Swedish words 
in test data by Danish, making it more likely that 
the parser knows them. After a parse has been 
obtained, the trees are ?restuffed? with the original 
Swedish words, and evaluated. 
5.2 Delexicalization 
A second approach relies on the hypothesis that the 
interaction between morphology and syntax in the 
two languages will be very similar. The basic idea 
is as follows: Replace Danish words in training 
data with their morphological (POS) tags. Simi-
larly, replace the Swedish words in test data with 
tags. This replacement is called delexicalization. 
Note that there are now two levels of tags in the 
trees: the Danish/Swedish tags in terminal nodes, 
and the Penn-style tags as pre-terminals. The ter-
minal tags are more descriptive because both Nor-
Bestemmelserne i denne aftale kan ?ndres og revideres helt eller delvis efter f?lles 
Best?mmelserna i detta avtal f?r ?ndras eller revideras helt eller delvis efter gemensam 
overenskomst mellem parterne. 
?verenskommelse mellan parterna. 
Figure 1. Comparison of matching Danish (upper) and Swedish (lower) sentences from Acquis. De-
spite the one-to-one word mapping, only the 5 bold words have identical spelling. 
38
dic languages have a slightly richer morphology 
than English, and the conversion to the Penn tag 
set loses information. 
The crucial point is that both Danish and 
Swedish use the same tag set, which helps to deal 
with the discrepancy between the training and the 
test terminals. 
Otherwise, the algorithm is similar to that of 
glosses: train the parser on delexicalized Danish, 
run it over delexicalized Swedish, restuff the 
resulting trees with the original Swedish words 
(?re-lexicalize?) and evaluate them. 
6 Experiments: Part One 
We ran most experiments twice: once with 
Charniak?s parser alone (?C?) and once with the 
reranking parser of Charniak and Johnson, which 
we label simply Brown parser (?B?). 
We use the standard evalb program by Sekine 
and Collins to evaluate the parse trees. Keeping 
with tradition, we report the F-score of the labeled 
precision and recall on the sentences of up to 40 
words.7 
 
Language Parser P R F 
C 77.84 78.48 78.16 da 
B 78.28 78.20 78.24 
C 79.50 79.73 79.62 da-hybrid 
B 80.60 79.80 80.20 
C 77.61 78.00 77.81 sv 
B 79.16 78.33 78.74 
C 77.54 78.93 78.23 sv-mamba 
B 79.67 79.26 79.46 
C 76.10 76.04 76.07 sv-hybrid 
B 78.12 75.93 77.01 
Table 1. Monolingual parsing accuracy. 
 
To put the experiments in the right context, we 
first ran two monolingual tracks and evaluated 
Danish-trained parsers on Danish, and Swedish-
trained parsers on Swedish test data. Both 
treebanks have also been parsed after 
delexicalization into various tag sets: Danish gold 
standard converted to the hybrid sv/da tag set, 
Swedish Mamba gold standard, and Swedish 
automatically tagged with hybrid tags. 
The reranker did not prove useful for lexicalized 
Swedish, although it helped with Danish. (We cur-
                                                 
7 F = 2?P?R / (P+R) 
rently have no explanation of this.) On the other 
hand, delexicalized reranking parsers outperformed 
lexicalized parsers for both languages. This holds 
for delexicalization using the gold standard tags 
(even though the Mamba tag set encodes much less 
information than the hybrid tags). Automatically 
assigned tags perform significantly worse. 
Our baseline condition is simply to train the 
parsers on Danish treebank and run them over 
Swedish test data. Then we evaluate the two 
algorithms described in the previous section: 
glosses and delexicalization (hybrid tags). 
 
Approach Parser P R F 
C 44.59 42.04 43.28 baseline 
B 42.94 40.80 41.84 
C 61.85 65.03 63.40 glosses 
B 60.22 62.85 61.50 
C 63.47 67.67 65.50 delex 
B 64.74 68.15 66.40 
 
Table 2. Cross-language parsing accuracy. 
 
7 Self-Training 
Finally, we explored the self-training based 
domain-adaptation technique of McClosky et al 
(2006) in this setting. McClosky et al trained the 
Brown parser on one domain of English (WSJ), 
parsed a large corpus of a second domain 
(NANTC), trained a new Charniak (non-reranking) 
parser on WSJ plus the parsed NANTC, and tested 
the new parser on data from a third domain (Brown 
Corpus). They observed improvement over 
baseline in spite of the fact that the large corpus 
was not in the third domain. 
 
Our setting is similar. We train the Brown parser 
on Danish treebank and apply it to Swedish Acquis. 
Then we train new Charniak parser on Danish 
treebank and the parsed Swedish Acquis, and test 
the parser on the Swedish test data. The hope is 
that the parser will get lexical context for the 
structures from the parsed Swedish Acquis. 
 
We did not retrain the reranker on the parsed 
Acquis, as we found it prohibitively expensive in 
both time and space. Instead, we created a new 
Brown parser by combining the new Charniak 
parser, and the old reranker trained only on Danish. 
39
A different scenario is used with the gloss and 
delex techniques. In this case, we only use delexi-
calization/glosses to parse the Acquis corpus. The 
new Charniak model is always trained directly on 
lexicalized Swedish, i.e. the parsed Acquis is re-
stuffed before being handed over to the trainer. 
Table-3 shows the corresponding application chart. 
8 Experiments: Part Two 
The following table shows the results of the self-
training experiments. All F-scores outperform the 
corresponding results obtained without self-
training. 
 
Approach Parser P R F 
C 45.14 43.96 44.54 Plain 
B 43.12 42.23 42.67 
C 62.87 66.17 64.48 Glosses 
B 61.94 64.77 63.32 
C 55.87 63.86 59.60 Delex 
B 53.87 61.45 57.41 
Table 3. Self-training adaptation results. 
 
Not surprisingly, the Danish-trained reranker 
does not help here. However, even the first-stage 
parser failed to outperform the Part One results. 
Therefore the 66.40% labeled F-score of the del-
exicalized Brown parser is our best result. It im-
proves the baseline by 23% absolute, or 41% error 
reduction. 
9 Discussion 
As one way of assessing the usefulness of the 
result, we compared it to the learning curve on the 
Swedish treebank. This corresponds to the question 
?How big a treebank would we have to build, so 
that the parser trained on the treebank achieves the 
same F-score?? We measured the F-scores for 
Swedish-trained parsers on gradually increasing 
amounts of training data (50, 100, 250, 500, 1000, 
2500, 5000 and 10700 sentences). 
The learning curve is shown in Figure 3. Using 
interpolation, we see that more than 1500 Swedish 
parse trees would be required for training, in order 
to achieve the performance we obtained by adapt-
ing an existing Danish treebank. This result is 
similar in spirit to the results Hwa et al (2004) re-
port when training a Chinese parser using depend-
ency trees projected from English. As they observe, 
creating a treebank of even a few thousand trees is 
a daunting undertaking ? consistent annotation 
typically requires careful design of guidelines for 
the annotators, testing of the guidelines on data, 
refinement of those guidelines, ramp-up of annota-
tors, double-annotation for quality control, and so 
forth. As a case in point, the Prague Dependency 
Treebank (B?hmov? et al 2003) project began in 
Danish treebank
PARSER 0 RERANKER
 
Swedish 
Acquis 1 
PARSER 1
Swedish test
DELEX 
GLOSSES 
 
Swedish 
Acquis RESTUFF
 
Parsed Swedish
Acquis 
Figure 2. Scheme of the self-training system. 
40
1996, and required almost a year for its first 1000 
sentences to appear (although things sped up 
quickly, and over 20000 sentences were available 
by fall 1998). In contrast, if the source and target 
language are sufficiently related ? consider Danish 
and Swedish, as we have done, or Hindi and 
Urdu ? our approach should in principle permit a 
parser to be constructed in a matter of days.). 
9.1 Ways to Improve: Future Work 
The 77.01% F-score of a parser trained on 
delexicalized automatically assigned hybrid 
Swedish tags is an upper bound. Some obvious 
ways of getting closer to it include better treebank 
and tag-set mapping and better tagging. In addition, 
we are interested in seeing to what extent 
performance can be further improved by better 
iterative self-training. 
We also want to explore classifier combination 
techniques on glosses, delexicalization, and the N-
best outputs of the Charniak parser. One could also 
go further, and explore a combination of tech-
niques, e.g. taking advantage of the ideas proposed 
here in tandem with unsupervised parsing (as in 
Bod 2006) or projection of annotations across a 
parallel corpus (as in Hwa et al 2004). 
Acknowledgements 
The authors thank Eugene Charniak and Mark 
Johnson for making their reranking parser 
available, as well as the creators of the corpora 
used in this research. We also thank the 
anonymous reviewers for useful remarks on where 
to focus our workshop presentation. 
The research reported on in this paper has been 
supported by the Fulbright-Masaryk Fellowship 
(first author), and by Grant No. N00014-01-1-0685 
ONR. Ongoing research (first author) is supported 
by the Ministry of Education of the Czech 
Republic, project MSM0021620838, and Czech 
Academy of Sciences, project No. 1ET101470416. 
References 
Rens Bod. 2006a. Unsupervised Parsing with U-DOP. 
In: Proceedings of the Conference on Natural 
Language Learning (CoNLL-2006). New York, New 
York, USA. 
Rens Bod. 2006b. An All-Subtrees Approach to Unsu-
pervised Parsing. In: Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and 
the 44th Annual Meeting of the ACL (COLING-
ACL-2006). Sydney, Australia. 
0
10
20
30
40
50
60
70
80
50 100 250 500 1000 2500 5000 10700
Training sentences
F
66.40 
(delex) 
~ 1546 
sentences 
Figure 3. The learning curve on the Swedish training data. 
41
Alena B?hmov?, Jan Haji?, Eva Haji?ov?, Barbora 
Hladk?. 2003. The Prague Dependency Treebank: A 
Three-Level Annotation Scenario. In: Anne Abeill? 
(ed.): Treebanks: Building and Using Syntactically 
Annotated Corpora. Kluwer Academic Publishers, 
Dordrecht, The Netherlands. 
Eugene Charniak, Mark Johnson. 2005. Coarse-to-Fine 
N-Best Parsing and MaxEnt Discriminative 
Reranking. In: Proceedings of the 43rd Annual 
Meeting of the ACL (ACL-2005), pp. 173?180. Ann 
Arbor, Michigan, USA. 
Michael Collins. 1997. Three Generative, Lexicalized 
Models for Statistical Parsing. In: Proceedings of the 
35th Annual Meeting of the ACL, pp. 16?23. Madrid, 
Spain. 
Michael Collins, Jan Haji?, Lance Ramshaw, Christoph 
Tillmann. 1999. A Statistical Parser for Czech. In: 
Proceedings of the 37th Annual Meeting of the ACL 
(ACL-1999), pp. 505?512. College Park, Maryland, 
USA. 
Jan Haji?. 2004. Disambiguation of Rich Inflection 
(Computational Morphology of Czech). Karolinum, 
Charles University Press, Praha, Czechia. 
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara 
Cabezas, Okan Kolak. 2004. Bootstrapping Parsers 
via Syntactic Projection across Parallel Texts. In: 
Natural Language Engineering 1 (1): 1?15. 
Cambridge University Press, Cambridge, England. 
Dan Klein, Christopher D. Manning. 2004. Corpus-
Based Induction of Syntactic Structure: Models of 
Dependency and Constituency. In: Proceedings of the 
42nd Annual Meeting of the ACL (ACL-2004). 
Barcelona, Spain. 
Philipp Koehn, Franz Josef Och, Daniel Marcu. 2003. 
Statistical Phrase-Based Translation. In: Proceedings 
of HLT-NAACL 2003, pp. 127?133. Edmonton, 
Canada. 
Matthias T. Kromann, Line Mikkelsen, Stine Kern 
Lynge. 2004. Danish Dependency Treebank. At: 
http://www.id.cbs.dk/~mtk/treebank/. K?benhavn, 
Denmark. 
Mitchell P. Marcus, Beatrice Santorini, Mary Ann Mar-
cinkiewicz. 1993. Building a Large Annotated Cor-
pus of English: the Penn Treebank. In: Computa-
tional Linguistics, vol. 19, pp. 313?330. 
David McClosky, Eugene Charniak, Mark Johnson. 
2006. Reranking and Self-Training for Parser Adap-
tation. In: Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th 
Annual Meeting of the ACL (COLING-ACL-2006). 
Sydney, Australia. 
Joakim Nivre, Jens Nilsson, Johan Hall. 2006. 
Talbanken05: A Swedish Treebank with Phrase 
Structure and Dependency Annotation. In: 
Proceedings of the 5th International Conference on 
Language Resources and Evaluation (LREC-2006). 
May 24-26. Genova, Italy. 
Franz Josef Och, Hermann Ney. 2000. Improved 
Statistical Alignment Models. In: Proceedings of the 
38th Annual Meeting of the ACL (ACL-2000), pp. 
440?447. Hong Kong, China. 
Philip Resnik, Noah A. Smith. 2003. The Web as a 
Parallel Corpus. In: Computational Linguistics, 
29(3), pp. 349?380. 
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen 
Clark, Rebecca Hwa, Julia Hockenmaier, Paul 
Ruhlen, Steven Baker, Jeremiah Crim. 2003. 
Bootstrapping Statistical Parsers from Small 
Datasets. In: Proceedings of the 11th Conference of 
the European Chapter of the ACL (EACL-2003). 
Budapest, Hungary. 
Ralf Steinberger, Bruno Pouliquen, Anna Widiger, 
Camelia Ignat, Toma? Erjavec, Dan Tufi?, D?niel 
Varga. 2006. The JRC-Acquis: A Multilingual 
Aligned Parallel Corpus with 20+ Languages. In: 
Proceedings of the 5th International Conference on 
Language Resources and Evaluation (LREC-2006). 
May 24-26. Genova, Italy. 
Fei Xia, Martha Palmer. 2001. Converting Dependency 
Structures to Phrase Structures. In: Proceedings of 
the 1st Human Language Technology Conference 
(HLT-2001). San Diego, California, USA. 
42
c? 2003 Association for Computational Linguistics
The Web as a Parallel Corpus
Philip Resnik? Noah A. Smith?
University of Maryland Johns Hopkins University
Parallel corpora have become an essential resource for work in multilingual natural language
processing. In this article, we report on our work using the STRAND system for mining parallel
text on the World Wide Web, first reviewing the original algorithm and results and then presenting
a set of significant enhancements. These enhancements include the use of supervised learning
based on structural features of documents to improve classification performance, a new content-
based measure of translational equivalence, and adaptation of the system to take advantage of the
Internet Archive for mining parallel text from the Web on a large scale. Finally, the value of these
techniques is demonstrated in the construction of a significant parallel corpus for a low-density
language pair.
1. Introduction
Parallel corpora?bodies of text in parallel translation, also known as bitexts?have
taken on an important role in machine translation and multilingual natural language
processing. They represent resources for automatic lexical acquisition (e.g., Gale and
Church 1991; Melamed 1997), they provide indispensable training data for statistical
translation models (e.g., Brown et al 1990; Melamed 2000; Och and Ney 2002), and
they can provide the connection between vocabularies in cross-language information
retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard
1997). More recently, researchers at Johns Hopkins University and the University of
Maryland have been exploring new ways to exploit parallel corpora in order to de-
velop monolingual resources and tools, using a process of annotation, projection, and
training: Given a parallel corpus in English and a less resource-rich language, we
project English annotations across the parallel corpus to the second language, using
word-level alignments as the bridge, and then use robust statistical techniques in learn-
ing from the resulting noisy annotations (Cabezas, Dorr, and Resnik 2001; Diab and
Resnik 2002; Hwa et al 2002; Lopez et al 2002; Yarowsky, Ngai, and Wicentowski 2001;
Yarowsky and Ngai 2001; Riloff, Schafer, and Yarowsky 2002).
For these reasons, parallel corpora can be thought of as a critical resource. Unfor-
tunately, they are not readily available in the necessary quantities. Until very recently,
for example, statistical work in machine translation focused heavily on French-English
translation because the Canadian parliamentary proceedings (Hansards) in English
and French were the only large bitext available. Things have improved somewhat, but
it is still fair to say that for all but a relatively few language pairs, parallel corpora
tend to be accessible only in specialized forms such as United Nations proceedings
(e.g., via the Linguistic Data Consortium, ?http://www.ldc.upenn.edu?), religious texts
(Resnik, Olsen, and Diab 1999), localized versions of software manuals (Resnik and
? Department of Linguistics and Institute for Advanced Computer Studies, University of Maryland,
College Park, MD 20742. E-mail: resnik@umd.edu
? Department of Computer Science and Center for Language and Speech Processing, Johns Hopkins
University, Baltimore, MD 21218. E-mail: nasmith@cs.jhu.edu
350
Computational Linguistics Volume 29, Number 3
Melamed 1997; Menezes and Richardson 2001), and the like. Even for the top handful
of majority languages, the available parallel corpora tend to be unbalanced, represent-
ing primarily governmental or newswire-style texts. In addition, like other language
resources, parallel corpora are often encumbered by fees or licensing restrictions. For
all these reasons, it is difficult to follow the ?more data are better data? advice of
Church and Mercer (1993), abandoning balance in favor of volume, with respect to
parallel text.
Then there is the World Wide Web. People tend to see the Web as a reflection of
their own way of viewing the world?as a huge semantic network, or an enormous
historical archive, or a grand social experiment. We are no different: As computational
linguists working on multilingual issues, we view the Web as a great big body of text
waiting to be mined, a huge fabric of linguistic data often interwoven with parallel
threads.
This article describes our techniques for mining the Web in order to extract the
parallel text it contains. It presents, in revised and considerably extended form, our
early work on mining the Web for bilingual text (STRAND) (Resnik 1998, 1999), incor-
porating new work on content-based detection of translations (Smith 2001, 2002), and
efficient exploitation of the Internet Archive. In Section 2 we lay out the STRAND ar-
chitecture, which is based on the insight that translated Web pages tend quite strongly
to exhibit parallel structure, permitting them to be identified even without looking at
content; we also show how we have improved STRAND?s performance by training
a supervised classifier using structural parameters rather than relying on manually
tuned thresholds. In Section 3 we present an approach to detecting translations that
relies entirely on content rather than structure, demonstrating performance comparable
to STRAND?s using this orthogonal source of information. In Section 4 we describe
how we have adapted the STRAND approach to the Internet Archive, dramatically
improving our ability to identify parallel Web pages on a large scale. Section 5 puts
all the pieces together, using structural and combined content-structure matching of
pages on the Internet Archive in order to obtain a sizable corpus of English-Arabic
Web document pairs. Finally we present our thoughts on future work and conclusions.
2. The STRAND Web-Mining Architecture
STRAND (Resnik 1998, 1999) is an architecture for structural translation recognition,
acquiring natural data. Its goal is to identify pairs of Web pages that are mutual
translations. In order to do this, it exploits an observation about the way that Web
page authors disseminate information in multiple languages: When presenting the
same content in two different languages, authors exhibit a very strong tendency to
use the same document structure (e.g., Figure 1). STRAND therefore locates pages that
might be translations of each other, via a number of different strategies, and filters out
page pairs whose page structures diverge by too much.
In this section we describe how STRAND works, and we also discuss several
related Web-mining methods, focusing on the overall architecture these systems have
in common and the important system-specific variations. We then show how tuning
STRAND?s structural parameters using supervised training can significantly increase
its performance.
2.1 STRAND
Finding parallel text on the Web consists of three main steps:
? Location of pages that might have parallel translations
351
Resnik and Smith The Web as a Parallel Corpus
Figure 1
Example of a candidate pair.
? Generation of candidate pairs that might be translations
? Structural filtering out of nontranslation candidate pairs
We consider each of these steps in turn.
2.1.1 Locating Pages. The original STRAND architecture accomplished the first step by
using the AltaVista search engine?s ?http://www.av.com? advanced search to search
for two types of Web pages: parents and siblings.
A parent page is one that contains hypertext links to different-language versions of
a document; for example, if we were looking for English and French bitexts, the page
at the left in Figure 2 would lead us to one such candidate pair. To perform this search
for the English-French language pair, we ask AltaVista for pages in any language
that satisfy this Boolean expression: (anchor:"english" OR anchor:"anglais") AND
(anchor:"french" OR anchor:"franc?ais"). A 10-line distance filter is used to re-
strict attention to pages on which the English and French pointers occur reason-
ably close to one another?specifically, those for which the regular expression (in
Perl) /(english|anglais)/ is satisfied within 10 lines of the Perl regular expres-
sion /(french|fran\w+ais)/ in the HTML source. This helps filter out a page that
contained, for example, a link to ?English literature courses? and also contained an
unrelated link to ?French version? at the top.
A sibling page is a page in one language that itself contains a link to a version
of the same page in another language; for example, the page at the right of Figure 2
contains a link on the left that says ?This page in english.? To perform this search for
English pages matching a given French page, we request pages in French that match
the Boolean expression anchor:"english" OR anchor:"anglais".
More recent versions of STRAND (unpublished) have added a ?spider? compo-
nent for locating pages that might have translations. Given a list of Web sites thought
to contain bilingual text for a given language pair (e.g., sites identified using the
AltaVista-based search), it is possible to download all the pages on each site, any
352
Computational Linguistics Volume 29, Number 3
Figure 2
Excerpts from a parent page (left) and a sibling page (right). The parent page is in Italian and
contains links marked ?Italiano/Italian,? ?Francese/French,? and ?Inglese/English.? The
sibling page is in Dutch and contains a link marked ?This page in english? in the leftmost
column.
of which might have a translation on that site. Although simple to implement, this
method of locating pages shifts the burden of narrowing down the possibilities to
the process of generating candidate document pairs. The results reported here do not
make use of the spider.
2.1.2 Generating Candidate Pairs. Pairing up potentially translated pages is simple
when a search engine has been used to generate parent or sibling pages: One simply
pairs the two child pages to which the parent links, or the sibling page together with
the page to which it links.
When all the pages on a site are under consideration, the process is rather differ-
ent. The simplest possibility is to separate the pages on a site into the two languages of
interest using automatic language identification (Ingle 1976; Beesley 1988; Cavnar and
Trenkle 1994; Dunning 1994), throwing away any pages that are not in either language,
and then generate the cross product. This potentially leads to a very large number of
candidate page pairs, and there is no particular reason to believe that most of them
are parallel translations, other than the fact that they appear on the same Web site. The
spider component of STRAND adds a URL-matching stage, exploiting the fact that the
directory structure on many Web sites reflects parallel organization when pages are
translations of each other. Matching is performed by manually creating a list of substi-
tution rules (e.g., english ? big5),1 and for each English URL, applying all possible
rules to generate URLs that might appear on the list of pages for the other language. If
such a URL is found, the pair with similar URLs is added to the list of candidate doc-
ument pairs. For example, suppose an English-Chinese site contains a page with URL
?http://mysite.com/english/home en.html?, on which one combination of substitu-
tions might produce the URL ?http://mysite.com/big5/home ch.html?. The original
page and the produced URL are probably worth considering as a likely candidate pair.
1 Big5 is the name of a commonly used character encoding for Chinese.
353
Resnik and Smith The Web as a Parallel Corpus
Owing to the combinatorics (an exponential number of possible substitutions), only a
fixed number of substitution combinations can be tried per English URL; however, in
Section 4.3 we describe a more scalable URL-matching algorithm.
Another possible criterion for matching is the use of document lengths. Texts
that are translations of one another tend to be similar in length, and it is reasonable to
assume that for text E in language 1 and text F in language 2, length(E) ? C ? length(F),
where C is a constant tuned for the language pair. The use of a document length filter
is described in Smith (2001), in which such a filter is shown, at the sentence level,
to reduce the size of the search space exponentially in the confidence p in a (1 ? p)
confidence interval for a linear regression model with only linear loss of good pairs.
2.1.3 Structural Filtering. The heart of STRAND is a structural filtering process that
relies on analysis of the pages? underlying HTML to determine a set of pair-specific
structural values, and then uses those values to decide whether the pages are transla-
tions of one another. The first step in this process is to linearize the HTML structure
and ignore the actual linguistic content of the documents. We do not attempt to exploit
nonlinear structure (e.g., embedded chunks), for two reasons. First, we suspect that
many HTML authors use tags for formatting text rather than for indicating document
structure; therefore any ?tree? structure is likely to be inconsistent or poorly matched.
Second, we required the matching algorithm to be fast, and algorithms for aligning
tree structures are more demanding than those for linear structures.
Both documents in the candidate pair are run through a markup analyzer that
acts as a transducer, producing a linear sequence containing three kinds of token:
[START:element_label] e.g., [START:A], [START:LI]
[END:element_label] e.g., [END:A]
[Chunk:length] e.g., [Chunk:174]
The chunk length is measured in nonwhitespace bytes, and the HTML tags are nor-
malized for case. Attribute-value pairs within the tags are treated as nonmarkup text
(e.g., <FONT COLOR="BLUE"> produces [START:FONT] followed by [Chunk:12]).
The second step is to align the linearized sequences using a standard dynamic pro-
gramming technique (Hunt and McIlroy 1975). For example, consider two documents
that begin as follows:
<HTML> <HTML>
<TITLE>Emergency Exit</TITLE> <TITLE>Sortie de Secours</TITLE>
<BODY> <BODY>
<H1>Emergency Exit</H1> Si vous e?tes assis a`
If seated at an exit and co?te? d?une . . .
...
...
The aligned linearized sequence would be as follows:
[START:HTML] [START:HTML]
[START:TITLE] [START:TITLE]
[Chunk:13] [Chunk:15]
[END:TITLE] [END:TITLE]
[START:BODY] [START:BODY]
[START:H1]
[Chunk:13]
[END:H1]
[Chunk:112] [Chunk:122]
354
Computational Linguistics Volume 29, Number 3
Using this alignment, we compute four scalar values that characterize the quality
of the alignment:
dp The difference percentage, indicating nonshared material (i.e., alignment
tokens that are in one linearized file but not the other).
n The number of aligned nonmarkup text chunks of unequal length.
r The correlation of lengths of the aligned nonmarkup chunks.
p The significance level of the correlation r.
The difference percentage (dp) quantifies the extent to which there are mismatches
in the alignment: sequence tokens on one side that have no corresponding token on
the other side. In the example above, one document contains an H1 header that is
missing from the second document. Large numbers of such mismatches can indicate
that the two documents do not present the same material to a great enough extent to
be considered translations. This can happen, for example, when two documents are
translations up to a point (e.g., an introduction), but one document goes on to include a
great deal more content than another. Even more frequently, the difference percentage
is high when two documents are prima facie bad candidates for a translation pair.
The number of aligned nonmarkup text chunks (n) helps characterize the quality of
the alignment. The dynamic programming algorithm tries to optimize the correspon-
dence of identical tokens, which represent markup.2 As a side effect, the nonmarkup
text chunks are placed in correspondence with one another (e.g., the ?Emergency Exit?
and ?Sortie de Secours? chunks in the above example). The more such pairings are
found, the more likely the candidate documents are to represent a valid translation
pair.
The remaining two parameters (r and p) quantify the extent to which the cor-
responding nonmarkup chunks are correlated in length. When two documents are
aligned with one another and are valid translations, there is a reliably linear relation-
ship in the length of translated chunks of text: short pieces correspond with short
pieces, medium with medium, and long with long. The Pearson correlation coefficient
r for the lengths will be closer to one when the alignment has succeeded in lining
up translated pieces of text, and the p value quantifies the reliability of the correla-
tion; for example, the standard threshold of p < .05 indicates 95% confidence that the
correlation was not obtained by chance.
In our original work, we used fixed thresholds, determined manually by inspection
of development (nontest) data for English-Spanish, to decide whether a candidate pair
should be kept or filtered out. Thresholds of dp < 20% and p < 0.05 were used.
2.2 STRAND Results
As with most search tasks, performance at finding parallel Web pages can be evaluated
using standard measures of precision and recall and by combining those figures using
the F-measure. It is not possible for us to measure recall relative to the entire set of
document pairs that should have been found; this would require exhaustive evaluation
using the entire Web, or pooling results from a large number of different systems, as
is done in the TREC information retrieval evaluations. Therefore, recall in this setting
is measured relative to the set of candidate pairs that was generated.
2 ?Nonmarkup? tokens with exactly the same length almost always turn out to be pieces of
markup-related text (e.g., key=value pairs within HTML tags).
355
Resnik and Smith The Web as a Parallel Corpus
Since the ?truth? in this task is a matter for human judgment, we rely on bilingual
speakers to judge independently whether page pairs are actually translations of each
other for any given test set. In our experience no bilingual speaker is completely
comfortable saying that another person?s translation is a good translation, so in creating
the gold standard, we instead ask, ?Was this pair of pages intended to provide the
same content in the two different languages?? Asking the question in this way leads
to high rates of interjudge agreement, as measured using Cohen?s ? measure.
2.2.1 Using Manually Set Parameters. Using the manually set thresholds for dp and n,
we have obtained 100% precision and 68.6% recall in an experiment using STRAND
to find English-French Web pages (Resnik 1999). In that experiment, 326 candidate
pairs, randomly selected from a larger set of 16,763 candidates, were judged by two
human annotators. The humans agreed (i.e., both marked a page ?good? or both
marked a page ?bad?) on 261 page pairs (86 ?good? and 175 ?bad?), and it is rela-
tive to those 261 that we compute recall. A modified version of STRAND was used
to obtain English-Chinese pairs (see related work, below), and in a similar formal
evaluation, we found that the resulting set had 98% precision and 61% recall for
Chinese ?http://umiacs.umd.edu/?resnik/strand/?. Both these results are consistent
with our preliminary findings for English-Spanish using a less rigorous evaluation
(using the judgments of the first author rather than independent bilingual evaluators)
and a very small test set; precision in this preliminary experiment was near ceiling
and recall was in the vicinity of 60% (Resnik 1998).
2.2.2 Assessing the STRAND Data. Although our focus here is finding parallel text,
not using it, a natural question is whether parallel text from the Web is in fact of value.
Two sources of evidence suggest that it is.
First, Web-based parallel corpora have already demonstrated their utility in cross-
language information retrieval experiments. Resnik, Oard, and Levow (2001) showed
that a translation lexicon automatically extracted from the French-English STRAND
data could be combined productively with a bilingual French-English dictionary in or-
der to improve retrieval results using a standard cross-language IR test collection (En-
glish queries against the CLEF-2000 French collection, which contains approximately
21 million words from articles in Le Monde). During document translation, backing off
from the dictionary to the STRAND translation lexicon accounted for over 8% of the
lexicon matches (by token), reducing the number of untranslatable terms by a third
and producing a statistically significant 12% relative improvement in mean average
precision as compared to using the dictionary alone. Similarly, Nie and Cai (2001) have
demonstrated improved cross-language IR results for English and Chinese using data
gathered by the PTMiner system (Chen and Nie 2000), a related approach that we
discuss in Section 2.3.
Second, since bag-of-words IR experiments are not very illuminating with respect
to fluency and translation quality, we conducted a ratings-based assessment of English-
Chinese data, asking two native Chinese speakers (who are fluent in English) to assign
ratings to a set of English-Chinese items. The set contained:
? 30 human-translated sentence pairs from the FBIS (Release 1)
English-Chinese parallel corpus, sampled at random.
? 30 Chinese sentences from the FBIS corpus, sampled at random, paired
with their English machine translation output from AltaVista?s Babelfish
?http://babelfish.altavista.com?.
356
Computational Linguistics Volume 29, Number 3
? 30 paired items from Chinese-English Web data, sampled at random
from ?sentence-like? aligned chunks as identified using the HTML-based
chunk alignment process of Section 2.1.3.
The human-translated and machine-translated pairs were included in order to provide
upper-bound and lower-bound comparisons. The items were presented to one judge
in a random order, and to the other judge in the reverse order.
Chinese-English Web data were those collected by Jinxi Xu using a modified ver-
sion of STRAND (see Section 2.3), excluding those that did not pass STRAND?s struc-
tural filter with the manually set thresholds. Sentence-like chunk pairs were defined
as those in which the English side was 5?50 whitespace-delimited tokens long and
that began with an uppercase alphabetic character and contained at least one token
from an English stop list.3 This fairly strict filter provided a set of approximately 7,000
pairs from which the 30 test items were sampled.
Participants were asked to provide each pair of items with three ratings, assess-
ing English fluency, Chinese fluency, and adequacy of the translation. The choice and
wording of the ratings criteria were derived from the human evaluation measures pro-
posed by Dabbadie et al (2002), with the wording of the translation assessment crite-
rion modified to eliminate references to the direction of translation. (See Appendix B.)
For all three measures, the two judges? ratings were significantly correlated (p <
0.0001). Figure 3 shows additional quantitative results of the assessment, comparing
judgments among human-translated, Web-generated, and machine-translated data.
The ratings indicate that pairs from the Web contain on average somewhere be-
tween ?mostly the same meaning? and ?entirely the same meaning? (median 3.25).4
In comparison, current commercial-quality machine translation output achieves per-
formance only between ?much of the same meaning? and ?mostly the same meaning?
(median 2.5). Moreover, it is very likely that the Web translation quality is an under-
estimate: Some of the low-scoring outliers within the Web data could be eliminated
by using state-of-the-art sentence alignment techniques and automatic detection and
0
5
10
15
20
25
30
0 1 2 3 4
N
u
m
be
r 
of
 p
ai
rs
Rating
Human
WWW
MT
0
5
10
15
20
25
30
0 1 2 3 4
N
u
m
be
r 
of
 p
ai
rs
Rating
Human
WWW
MT
Figure 3
Translation adequacy ratings: Distribution over scores for human-translated (Human),
Web-generated (WWW), and machine-translated (MT) data. The left plot provides results for
judge 1, the right plot for judge 2.
3 We also excluded pairs either side of which contained a curly bracket, since these were almost
invariably fragments of Javascript code.
4 The medians noted refer to the median of R1+R22 within the set, where Rn is the rating given by judge
n. We use the median rather than the mean because it is less sensitive to outliers.
357
Resnik and Smith The Web as a Parallel Corpus
elimination of noisy pairs at the sentence level (cf. Nie and Cai [2001]). We observe
that the distribution of scores for Web data peaks at the highest rating and that the
data are in both cases modestly bimodally distributed. Machine-translated pairs, on
the other hand, have generally lower quality. This suggests that high-quality parallel
translations are present in this corpus and that poor-quality parallel translations are
very poor (whether because of misalignment or simply because of poor translation
quality at the document level) and might therefore be easily distinguishable from the
better material. We plan to address this in future work.
Qualitatively, the results are a source of optimism about parallel data from the
Web. Looking monolingually, fluency of the English side is statistically comparable to
that of English sentences from the FBIS parallel corpus (essentially at ceiling), and on
average the Chinese Web data are judged somewhere between ?fairly fluent? and ?very
fluent,? with the median at ?very fluent? (only the second judge found the fluency
of the Chinese Web sentences to be significantly worse than the human-generated
Chinese sentences, Mann-Whitney test, p < 0.02).
2.2.3 Optimizing Parameters Using Machine Learning. Based on experiments with
several language pairs, it appears that STRAND?s structure-based filter consistently
throws out around one-third of the candidate document pairs it has found in order
to maintain its precision in the 98?100% range. It does so by respecting parameter
thresholds that were determined manually using English-Spanish development data;
the same parameters seem to have worked reasonably well not only for English-
Spanish, but also for English-French and English-Chinese pairs. It is possible, however,
that classification can be tuned for better performance. In order to investigate this
possibility, we took a machine-learning approach: We used the four structural values
(dp, n, r, and p) as features characterizing each document pair and treated the problem
as a binary decision task, using supervised learning to make an attempt at better
predicting human judgments.
Using the English-French data, we constructed a ninefold cross-validation experi-
ment using decision tree induction to predict the class assigned by the human judges.
The decision tree software was the widely used C5.0 ?http://www.rulequest.com/
demoeula.html?. We used a decision tree learner because it is transparent (it is easy to
see which features are being used to classify page pairs). In addition, a decision tree
produced by C5.0 can be translated into a fast C program that is a rapid classifier of
document pairs.
Each fold had 87 test items and 174 training items; the fraction of good and bad
pairs in each fold?s test and training sets was roughly equal to the overall division
(33% to 67%, respectively). Precision and recall results are reported in Table 1, together
with baseline results from STRAND?s untuned classifier as reported above.
Looking at the decision trees learned, we see that they are very similar to one
another. In every case a tree that looked like the following was learned:
if dp > 37 then BAD
else
if n > 11 then GOOD
else ...
where the remaining branch involved various additional partitionings of the candi-
date pairs that had few aligned text chunks (small n) and relatively low (but perhaps
unreliable) difference percentage (dp). That branch handled around 10% of the docu-
ment set and was prone to overtraining. (The documents handled by this branch were
mostly marked ?bad? by the judges but appear to have been difficult to classify based
358
Computational Linguistics Volume 29, Number 3
Table 1
Effects of parameter tuning.
Precision Recall
Untuned 1.000 0.686
Fold 1 0.875 1.000
Fold 2 0.857 0.667
Fold 3 1.000 1.000
Fold 4 1.000 0.923
Fold 5 1.000 0.875
Fold 6 1.000 1.000
Fold 7 0.889 0.667
Fold 8 1.000 0.889
Fold 9 1.000 0.545
Average 0.958 0.841
on the structural features.) Note that the learned classifiers were substantially different
from the heuristic threshold used earlier.
Without tuning, the manually set parameters result in good document pairs? be-
ing discarded 31% of the time. Our cross-validation results indicate that tuning the
parameters cuts that figure in half: Only 16% of the good pairs will be discarded, at a
cost of admitting 4 false positives from every 100 candidate pairs.
This approach is quite general and uses only a minimum of language-dependent
knowledge. The features we are using are the same for any language pair. The tuning
process needs to be done only once per language pair and requires only a few hours
of annotation from untrained speakers of both languages to obtain the small labeled
sample.
2.3 Related Work
Several other systems for discovering parallel text, developed independently, can be
described as operating within the same three-stage framework as STRAND.
Parallel Text Miner (PTMiner) (Chen and Nie 2000) exploits already-existing Web
search engines to locate pages by querying for pages in a given language that contain
links to pages that are likely to be in the other language of interest. Once bilingual sites
are located, they are crawled exhaustively. In order to generate candidate pairs, PT-
Miner uses a URL-matching process similar to the one described above; for example,
the French translation of a URL like ?http://www.foo.ca/english-index.html? might
be ?http://www.foo.ca/french-index.html?. PTMiner?s matching process uses a map-
ping of language-specific prefixes and suffixes and does not handle cases in which URL
matching requires multiple substitutions. PTMiner also applies a length filter and auto-
matic language identification to verify that the pages are in the appropriate languages.
Chen and Nie report a 95% precise English-French corpus of 118MB/135MB of text and
a 90% precise English-Chinese corpus of 137MB/117MB of text, based on inspection.
Later versions of PTMiner include a final filtering stage to clean the extracted
corpus; Nie and Cai (2001) independently used features similar to those described
here to eliminate noisy pairs. Specifically, they used a file length ratio filter, the pro-
portion of sentences that are aligned to empty (after a sentence alignment algorithm
is applied), and a criterion that rewards sentence pairs that contain elements from a
bilingual dictionary. Nie and Cai showed that hand-tuned combination of these crite-
359
Resnik and Smith The Web as a Parallel Corpus
ria improved the quality of their parallel English-Chinese corpus by 8% (F-measure)
at the text level.
Bilingual Internet Text Search (BITS) (Ma and Liberman 1999) starts with a given
list of domains to search for parallel text. It operates by sampling pages from each
domain and identifying their languages; if a domain is deemed to be multilingual,
all pages on the site are crawled exhaustively. BITS appears to consider all possible
combinations of Web page pairs in the two languages (i.e., the full cross product within
each site) and filters out bad pairs by using a large bilingual dictionary to compute a
content-based similarity score and comparing that score to a threshold. For each page
pair, the similarity score is
similarity(A, B) =
number of translation token pairs
number of tokens in A
(1)
Translation token pairs are considered within a fixed window (i.e., a distance-based
measure of co-occurrence is used).5 In addition to cross-lingual lexical matching, BITS
filters out candidate pairs that do not match well in terms of file size, anchors (num-
bers, acronyms, and some named entities), or paragraph counts. Using an English-
German bilingual lexicon of 117,793 entries, Ma and Liberman report 99.1% precision
and 97.1% recall on a hand-picked set of 600 documents (half in each language) con-
taining 240 translation pairs (as judged by humans). This technique yielded a 63MB
parallel corpus of English-German.
Other work on Web mining has been done by Jinxi Xu of BBN (personal com-
munication), who began with our STRAND implementation and added a module for
automatically learning string substitution patterns for URLs and also implemented
a different dynamic programming algorithm for assessing structural alignment. Xu
used the modified STRAND to obtain 3,376 Chinese-English document pairs, which
we evaluated formally (see above), determining that the set has 98% precision and
61% recall.
In addition, STRAND has been reimplemented by David Martinez and colleagues
at Informatika Fakultatea in the Basque Country (personal communication), in order
to perform exploratory experiments for discovering English-Basque document pairs.
It is worth noting that STRAND, PTMiner, and BITS are all largely independent of
linguistic knowledge about the particular languages, and therefore very easily ported
to new language pairs. With the exception of the use of a bilingual dictionary (in BITS
and later versions of PTMiner), these systems require, at most, a set of URL substring
patterns for the URL pattern-matching stage (e.g., big5 ? english in the example above;
see further discussion in Section 4.3), and a modest amount of monolingual data for
training n-gram-based language identifiers (typically 50,000 to 100,000 characters of
text per language).
Word-level translations are worth exploiting when they are available. In Section 3
we describe a bitext-matching process using a content-based similarity score grounded
in information theory, and in Section 5 we show how structural and content-based
criteria can be combined in order to obtain performance superior to that obtained
using either method alone.
5 Many details of this technique are left unspecified in Ma and Liberman (1999), such as the threshold
for the similarity score, the distance threshold, and matching of non-one-word to one-word entries in
the dictionary.
360
Computational Linguistics Volume 29, Number 3
Maria does n?t like fruit
Maria n? aime pas de fruits
X:
Y:
NULLNULL
NULL
Figure 4
An example of two texts with links shown. There are seven link tokens, five of which are
lexical (non-NULL) in X (the English side), six in Y (French).
3. Content-Based Matching
The approach discussed thus far relies heavily on document structure. However, as Ma
and Liberman (1999) point out, not all translators create translated pages that look like
the original page. Moreover, structure-based matching is applicable only in corpora
that include markup, and there are certainly multilingual collections on the Web and
elsewhere that contain parallel text without structural tags. Finally, other applications
for translation detection exist, such as subdocument text alignment and cross-lingual
duplicate detection (i.e., location of already-existing translations in a multilingual cor-
pus). All these considerations motivate an approach to matching translations that pays
attention to similarity of content, whether or not similarities of structure exist.
We present here a generic score of translational similarity that is based upon
any word-to-word translation lexicon (hand-crafted or automatically generated, or a
combination, and possibly highly noisy). The technique is shown to perform competi-
tively to the structure-based approach of STRAND on the task of identifying English-
French document translations.
3.1 Quantifying Translational Similarity
We define a cross-language similarity score, tsim , for two texts by starting with a
generative, symmetric word-to-word model of parallel texts (Melamed?s [2000]
Method A). Let a link be a pair (x, y) in which x is a word in language L1 and y
is a word in L2. The model consists of a bilingual dictionary that gives a probability
distribution p over all possible link types. Within a particular link, one of the words
may be NULL, but not both. In the generative process, a sequence of independent link
tokens is sampled from the distribution. The model does not account for word order.
An example of two texts with links is illustrated in Figure 4.
Next, we desire to compute the probability of the most probable link sequence
that could have accounted for the two texts.6 The probability of a link sequence is
simply the product of the probabilities p of the links it contains. As noted by Melamed
(2000), the problem of finding the best set of links is the maximum-weighted bipartite
matching (MWBM) problem: Given a weighted bipartite graph G = (V1 ? V2, E) with
edge weights ci,j(i ? V1, j ? V2), find a matching M ? E such that each vertex has at
most one edge in M and
?
e?M ci,j is maximized. The fastest known MWBM algorithm
runs in O(ve+v2 log v) time (Ahuja, Magnati, and Orlin 1993). Applied to this problem,
that is O(max(|X|, |Y|)3), where X and Y are the text lengths in words.
To use MWBM to find the most probable link sequence, let the L1 words be V1 and
the L2 words be V2. If two words x, y have p(x, y) > 0, an edge exists between them
with weight log p(x, y). If a word x (or y) may link to NULL with nonzero probability,
then that potential link is added as an additional edge in the graph between x (or y)
6 Of course, all permutations of a given link sequence will have the same probability (since the links are
sampled independently from the same distribution), so the order of the sequence is not important.
361
Resnik and Smith The Web as a Parallel Corpus
and a NULL vertex added to V2 (or V1). Each such x (or y) gets its own NULL vertex,
so that multiple words may ultimately link to NULL. A sum of weights of links in a
matching will be the log-probability of the (unordered) link sequence, and maximizing
that sum maximizes the probability.
The similarity score should be high when many of the link tokens in the best
sequence do not involve NULL tokens. Further, it should normalize for text length.
Specifically, the score is
tsim =
log Pr (two-word links in best matching)
log Pr (all links in best matching)
(2)
This score is an application of Lin?s (1998) information-theoretic definition of similarity.
Starting with a set of axioms relating intuitions about similarity to the mathematical
notion of mutual information (Shannon 1948), Lin derives the measure
sim(X, Y) =
log Pr (common(X, Y))
log Pr (description(X, Y))
(3)
where X and Y are any objects generated by a probabilistic model.
This technique of using a translation model to define translational similarity is
generic to different sources of lexical translation information. An important feature
is that it can be used with any symmetric translation model in which events can be
divided into those that both sides of a bitext have in common and those that affect
only one side.
The measure is simplified by assuming that all links in a given translation lexicon
are equiprobable. The assumption reduces the formula for tsim to
tsim =
number of two-word links in best matching
number of links in best matching
(4)
The key reason to compute tsim under the equiprobability assumption is that we
need not compute the MWBM, but may find just the maximum cardinality bipartite
matching (MCBM), since all potential links have the same weight. An O(e
?
v) (or
O(|X| ? |Y| ?
?
|X|+ |Y|) for this purpose) algorithm exists for MCBM (Ahuja, Magnati,
and Orlin 1993). For example, if the matching shown in Figure 4 is the MCBM (for
some translation lexicon), then tsim(X, Y) = 47 under the simplifying assumption.
In earlier work (Smith 2002), we sought to show how multiple linguistic resources
could be exploited in combination to recognize translation, and how the equiproba-
bility assumption allowed straightforward combination of resources (i.e., set union of
translation lexicon entries). In Section 3.2.1 we provide a clean solution to the problem
of using unweighted translation lexicons along with probabilistic ones that improves
performance over the earlier result.
This would appear to make the equiprobability assumption unnecessary (apart
from concerns about computational expense). However, we found that, if p(x, y) is set
to the empirically estimated joint probability of the lexical link type (x, y), then per-
formance turns out to be dismal. This is understandable: Using parameter estimation
techniques like the one we used, a great deal of probability mass in the distribution
p tends to go to frequent words, which are relatively uninformative with regard to
whether texts are mutual translations. The equiprobability assumption helps to coun-
teract this; in fact one could apply scoring techniques from information retrieval and
cross-lingual information retrieval in weighting the lexicon. We leave this area of ex-
ploration to future work.
362
Computational Linguistics Volume 29, Number 3
Melamed (2000) used a greedy approximation to MWBM called competitive link-
ing. Competitive linking iteratively selects the edge with the highest weight, links
the two vertices of the edge, then removes them from the graph. (Ties are broken at
random.) A heap-based implementation of competitive linking runs in O(max(|X|, |Y|)
log max(|X|, |Y|)). Under the equiprobability assumption, all the weights are the same,
so that competitive linking proceeds simply by randomly making legal links (those
allowed by the translation lexicon) until no more can be made.
If definition (4) is applied to pairs of documents in the same language, with a
??translation lexicon? defined by the identity relation, then tsim is a variant of resem-
blance (r), as defined by Broder et al (1997) for the problem of monolingual duplicate
detection, except that tsim has the advantage of being token-based rather than type-
based, incorporating word frequency.
We have demonstrated that the tsim score can be used to extract translationally
equivalent English-Chinese sentence pairs from even a noisy space with high precision
(Smith 2002). It was also shown that combining multiple sources of word-level trans-
lation information (dictionaries, word-to-word translation models, cognates) had pos-
itive effects on performance on the sentence-matching task. These information sources
were presumed to be extremely noisy (they are so presumed here, as well), though
no independent evaluation was carried out on them. If the ad hoc translation lexicon
induction techniques used here give good performance, then better techniques might
lead to further improvement. In addition, the competitive linking approximation was
shown to perform nearly as well as MCBM.
3.2 Experiment
We now apply our content-based similarity measure to the candidate pair classification
task presented by STRAND. Recall that both the original STRAND classifier and those
learned using decision tree methods, described in Section 2.2.3, employ only structural
features of the documents to determine whether they are translations. Here we apply
the tsim score to the same task and compare the results with those of the original
STRAND classifier.
3.2.1 Translation Lexicon. The word-level translation lexicon is derived from several
sources. We begin with an English-French dictionary (a total of 34,808 entries, 4,021 of
which are not one-to-one).7 Next, a word-to-word translation model (Melamed 2000)
was trained on the dictionary. Note that the parameter estimation task here is very
simple; in most cases the pairs are one-word to one-word, making the hidden link
structure unambiguous (modulo NULLs). The training primarily served the purpose
of breaking down multiword entries, informed by the rest of the entries, so as to obtain
a fully one-word-to-one-word dictionary. The training procedure was an expectation-
maximization (EM) procedure like that used by Melamed (2000), except that maximum
weighted bipartite matching was used instead of competitive linking. Any entry con-
taining a NULL was then removed.
We add to this dictionary a list of English-French cognate pairs, identified using
the method of Tiedemann (1999). Tiedemann?s approach involved learning language-
specific character weights for the computation of weighted edit distance to measure
cognateness. He used a list of known cognates to train the weights. We instead used
7 This dictionary was generated by Gina Levow, who kindly made it available to us. It was derived from
data available at ?http://www.freedict.com? and contains morphological variants but not character
accents.
363
Resnik and Smith The Web as a Parallel Corpus
Table 2
Fifteen randomly chosen cognate pairs. The noise is apparent.
English French English French
closest choses TOXLINE LIABLE
extensions extension RELATIONS ENNEMIS
answer passer TARNOS TARNOS
APPLICATION PROTECTION Generation information
missions maisons Commerce Community
proportion prestation private pre?tant
Fischer Fischer traditions attributions
Anglais dAlcatel
the weighted translation pairs in a translation model lexicon built from the Bible.8
The result is 35,513 word pairs from the corpus of Web pages under consideration.
An additional set of 11,264 exact string matches were added (also from the corpus of
Web pages). Qualitatively, these entries were highly noisy; a random selection of the
cognate pairs is shown in Table 2. All of these word pairs were added to the dictionary,
each with a count of one.
We took the enhanced dictionary with counts to define a Dirichlet prior, which is
the conjugate prior to a multinomial distribution over discrete events (like the distri-
bution p over link types we seek to estimate) (MacKay and Peto 1995). Such a prior is
characterized by counts of all such events; when it is used in an EM procedure, these
prior counts are added to those produced by the E step on every iteration. Intuitively,
if a word pair (x, y) is expected to be a likely lexical word pair in the dictionary and
cognate set, then models that make (x, y) probable are more likely (according to the
prior). Therefore the expected count of (x, y) is increased at each iteration of training
to the extent that the prior favors it.
Using the enhanced, weighted lexicon as a Dirichlet prior (containing 77,699 entries
and a total count of 85,332), a word-to-word translation model (Melamed 2000) was
trained on a verse-aligned Bible (15,548 verses, averaging 25.5 English words, 23.4
French words after tokenization). As before, we used the maximum weighted bipar-
tite matching algorithm. The final lexicon consists of all word pairs with nonzero
probability and contains 132,155 entries. Note that all word pairs in the enhanced
dictionary are included; we have merely added to that dictionary by bootstrapping
additional entries from the Bible.
3.2.2 Results. In order to compare tsim with structural similarity scoring, we applied
it to 325 English-French Web document pairs for which human evaluations were car-
ried out in Section 2. As there is only one feature under consideration (tsim), the
classifier must be a threshold on that value. At different thresholds, Cohen?s ? score
of agreement (with each of Resnik?s (1999) two judges and their intersection) may be
computed for comparison with STRAND, along with recall and precision against a
gold standard (for which we use the intersection of the judges: the set of examples
8 There is some circularity here; the cognates were derived using weighted word pairs from the Bible,
then used again in the prior distribution. We note that the resources required to extract cognates in this
way are no different from those required for the translation model.
364
Computational Linguistics Volume 29, Number 3
Table 3
Comparison with STRAND. The test set contains 293 of the 326 pairs in Resnik?s (1999) test
set. The 32 development pairs were used to select manually the 0.44 threshold. N is the
number of examples for which judgment comparison was possible in each case (human judges
were sometimes undecided; those cases are ignored in computing ?).
Comparison N Pr(Agree) ? prec rec F
J1, J2 245 0.98 0.96
J1, STRAND 250 0.88 0.70
J2, STRAND 284 0.88 0.69
J1 ? J2, STRAND 241 0.90 0.75 1.000 0.684 0.812
J1, tsim(? = 0.44) 249 0.96 0.92
J2, tsim(? = 0.44) 283 0.95 0.88
J1 ? J2, tsim(? = 0.44) 240 0.97 0.92 0.833 0.921 0.875
for which the judges agreed). The gold standard contained 86 page pairs marked as
?good? by both judges and 174 page pairs marked as ?bad? by both judges.9
Computing tsim (MCBM on the words in the document pair) is not tractable for
very large documents and translation lexicons. However, in preliminary comparisons,
we found that representing tsim for long documents by as few as their first 500 words
results in excellent performance on the ? measure.10 This allows O(1) estimation of
tsim for two documents. Further, the competitive linking algorithm appears to be as
reliable as MCBM, and it runs significantly faster in practice. The results reported here
approximated tsim in this way.
Of the 325 pairs, 32 were randomly selected as a development set, which we used
to select manually a threshold ? = 0.44. This value maximized the ? score against gold-
standard human judgments on the development set.11 ? scores against each judge and
their intersection were then computed at that threshold on the test set (the remaining
293 pairs). These are compared to ? scores of the STRAND system (with original,
untuned parameters), on the same test set, in Table 3. In every case, the tsim classifier
agreed more strongly with the human evaluations, and its F score is higher than that
of STRAND. Figure 5 shows ? and the F measure plotted against ? .
In this application, the content-based classifier (at its approximate best perfor-
mance, thresholding at 0.44) complements the structural classifier?s high precision.
Given two high-performing methods that use orthogonal information for identifying
good candidate pairs (one using only structure, the other using only content), the nat-
ural question is whether the techniques can be combined for even better performance.
We repeated the experiment presented in Section 2.2.3, adding the tsim score as a
feature. The same cross-validation setup was used, with the same division into folds.
Precision and recall results are reported in Table 4.
9 One additional pair was thrown out because it contained compressed data; it is assumed that this pair
would not pass a language identification filter.
10 This does, of course, run the risk of failing to identify that a page pair is similar only in the beginning,
with diverging content in later text. When the content-based classifier is combined with a structural
classifier, this problem is expected to be eliminated, since the structural method detects such cases. See
the last experiment described in this section.
11 One could select such a threshold to maximize any objective function over the development set. We
note that this threshold differs from that reported in Smith (2002); it was chosen by the same procedure,
though the translation lexicon is different, moving the distribution of tsim scores into a higher range.
365
Resnik and Smith The Web as a Parallel Corpus
STRAND?s 
?
?
STRAND?s F
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Threshold
F
Cohen?s    
Figure 5
Performance measures as the threshold varies (all measures are on the test set): the ?
agreement score with the two judges? intersection and F measure. Scores obtained by
STRAND are shown as well.
Table 4
Effects of parameter tuning with the additional tsim feature.
Precision Recall
Untuned STRAND 1.000 0.686
Tuned STRAND (average) 0.958 0.841
tsim(? = .44) 0.833 0.921
Fold 1 0.875 1.000
Fold 2 1.000 1.000
Fold 3 1.000 1.000
Fold 4 1.000 1.000
Fold 5 1.000 1.000
Fold 6 0.889 1.000
Fold 7 1.000 1.000
Fold 8 1.000 1.000
Fold 9 1.000 0.818
Average 0.974 0.980
The decision trees learned were once again all quite similar, with eight of the nine
rooted as follows (a few of the trees differed slightly in the numerical values used as
thresholds):
if tsim > 0.432 then GOOD
else
if dp > 22.9 then BAD
else ...
The remainder of each tree varied, and there was some evidence of overtraining.
366
Computational Linguistics Volume 29, Number 3
These results clearly demonstrate the benefit of combining structural and content-
based approaches. We next describe how we have adapted the STRAND architecture
to the Internet Archive, in order to generate the candidate pairs on a scale that has
previously been unattainable.
4. Exploiting the Internet Archive
One of the difficulties with doing research on Web mining is that large-scale crawling of
the Web is a significant enterprise. Chen and Nie?s (2000) PTMiner represents one solu-
tion, a carefully thought-out architecture for mining on a large scale. Here we present a
different solution, taking advantage of an existing large-scale repository of Web pages
maintained on an ongoing basis by an organization known as the Internet Archive.
4.1 The Internet Archive
The Internet Archive ?http://www.archive.org/web/researcher/? is a nonprofit or-
ganization attempting to archive the entire publicly available Web, preserving the
content and providing free access to researchers, historians, scholars, and the general
public. Data come from crawls done by Alexa Internet, and hence they represent an
industry-level resource of the sort not easily constructed within academia. At present,
the Archive contains 120TB (terabytes) of data, by a conservative estimate, and it is
growing at approximately 8TB per month. Text on the archive comprises over 10 bil-
lion Web pages, and the estimated duplicate rate is a factor of two (i.e., two copies of
everything).12
The Internet Archive provides public access to the data via the Wayback Machine
Web interface. As of this writing, a search for the ACL home page brings up links
to 72 snapshots of that page dating back to June 7, 1997.13 The reader can get to that
page directly on the Wayback Machine using a URL that points to the Internet Archive
and provides both the desired page and the time stamp indicating which snapshot to
retrieve.14
The Archive also provides researchers with free, direct access to its data via ac-
counts on their cluster. The data are stored on the disk drives of approximately 300 ma-
chines, each running some variety of UNIX, creating what is in essence one huge file
system. This provides a researcher with the remarkable sensation of having the entire
Web on his or her hard drive.
4.2 Properties of the Archive
Mining terabytes on the Archive presents a number of challenges:
? The Archive is a temporal database, but it is not stored in temporal
order. Hence a document and its translation may be in files on different
machines; a global merge of data is required for any hope of complete
extraction.
? Extracting a document for inspection is an expensive operation involving
text decompression.
12 We are grateful to Paula Keezer of Alexa for these figures.
13 http://www.cs.columbia.edu/?acl/home.html
14 http://web.archive.org/web/19970607032410/http://www.cs.columbia.edu/?acl/home.html (this is a
single, long URL.)
367
Resnik and Smith The Web as a Parallel Corpus
? The Archive?s size makes it essential to keep computational complexity
low.
On the other hand, as it turns out, aspects of the Archive?s architecture make rapid
development remarkably feasible:
? Almost all data are stored in compressed plain-text files, rather than in
databases.
? The data relevant for our purposes are organized into archive files
(arcfiles), which contain the stored pages, and index files, which contain
plain text tuples ?URL, time stamp, arcfile, offset, . . .?.
? A suite of tools exists for processing the archive (e.g., for extracting
individual pages from archive files).
? The Archive?s infrastructure for cluster computing makes it easy to write
UNIX scripts or programs and run them in parallel across machines.
The last of these, the cluster computing tools,15 is turned out to reduce drastically the
time needed to port STRAND to the Archive, as well as the size of the STRAND code
base. The centerpiece in Archive cluster computing is a parallelization tool called p2,
which offers a UNIX command-line interface that allows one to specify (1) a paral-
lelizable task, (2) a way to split it up, (3) a way to combine the results, and (4) a set of
processors among which to divide the task. The p2 tool divides up tasks intelligently,
invoking each parallel computation on the local machine where the data reside.
4.3 STRAND on the Archive
In adapting STRAND?s three-stage process to the Internet Archive, the primary chal-
lenge was in the first two steps, locating possible translations and matching them
up to produce candidate document pairs. Structural filtering remained essentially un-
changed.
Generating candidate pairs on the Archive involves the following steps:
1. Extracting URLs from index files using simple pattern matching
2. Combining the results from step 1 into a single huge list
3. Grouping URLs into buckets by handles
4. Generating candidate pairs from buckets
Steps 1 and 2 are performed via a parallel search operation plus combination of results;
for example, extracting all URLs in the Hong Kong, Taiwan, or China domains (and
their associated bookkeeping data) using a pattern like /(.hk|.tw|.cn)/.16
Step 3 is potentially tricky owing to computational complexity issues. As noted
in Section 2.1.2, examining the cross product of a site?s page sets in two different
languages is potentially very expensive, and matching documents by similarity of
URLs can represent a combinatoric process in the general case.
15 The Archive intends to release these cluster tools under the GNU Public License.
16 We also take advantage of the Archive?s list of .com domains paired with the nation in which each is
registered, making it possible to include commercial sites in the search without an explosion of
irrelevant possibilities.
368
Computational Linguistics Volume 29, Number 3
URLs:
saudifrenchbank.com.sa/English/English.htm
? saudifrenchbank.com.sa/English/English.htm
patterns removed:
a e a a english english
saudifrenchbank.com.sa/Arabic/arabic.htm
? saudifrenchbank.com.sa/Arabic/arabic.htm
patterns removed:
a e a a arabic arabic
Same handle for both URLs:
sudifrchbnk.com.s//.htm
Figure 6
Example of LSS subtraction.
We arrived at an algorithmically simple solution that avoids this problem but is
still based on the idea of language-specific substrings (LSSs). The idea is to identify a
set of language-specific URL substrings that pertain to the two languages of interest,
(e.g., based on language names, countries, character codeset labels, abbreviations, etc.).
For example, a set of LSSs for English-Arabic might be as follows (those containing
numerals correspond to character code sets):
1256, 437, 864, 8859-1, 8859-6, a, ar, ara, arab, arabic, cp1256, cp437,
cp864, e, en, eng, english, gb, iso, iso-8859-1, iso-8859-6, latin, latin-1,
latin1, uk, us, usa
For each URL, we form a ?handle? by subtracting any substrings that match (insensi-
tive to case) any item on the LSS pattern list. The subtraction process is implemented
reasonably efficiently: If there are p patterns with maximum length l, and the URL?s
length in characters is u, then the current implementation will do at most p ? u string
matches of length no more than l.17 (Currently we use C strcmp for string matching.)
In practice, this is extremely fast: We can generate handles for nearly 5,000 URLs per
second on a six-year-old Sun Ultra 1 workstation.
Figure 6 illustrates handle generation on two real URLs. As one would hope, these
two URLs produce the same handle, and as a result, they wind up in the same bucket
in step 3.18
In step 4, the URLs in each bucket are used to generate candidate pairs by taking
the cross product and keeping those URL pairs for which the URL bookkeeping data
indicate pages that are in the correct languages. For example, given the bucket con-
taining the two URLs in Figure 6, this step would generate a single pair consisting of
17 We are grateful to Bill Pugh of the University of Maryland for suggesting this algorithm.
18 Conceptually, they hash to the same bucket in a hash table; in practice on the Archive, it turns out to
be more efficient to create buckets by doing a parallel sort of the entire URL set using the handle as the
key, and then creating buckets based on identical handles? being on adjacent lines.
369
Resnik and Smith The Web as a Parallel Corpus
the URL for the English page and the URL for the Arabic page, assuming the language
ID information associated with each URL confirmed it was in the proper language.19
At this point, the candidate generation process is complete. The final step is to
apply STRAND?s filtering step to each candidate pair, an operation that can itself be
parallelized, since each candidate pair can be processed independently. The filtering
pass will eliminate those page pairs (roughly 10% in our experience) whose URLs
show similarity to each other but whose content and/or structure do not.
It is interesting to note that by taking advantage of the Archive?s p2 cluster
computing tool, together with its simple flat-text representations, adapting STRAND?s
candidate generation process resulted in a dramatic reduction in the size of the pro-
gram, cutting it literally in half, as measured in lines of code.
5. Building an English-Arabic Corpus
In the previous sections, we have described methods and results for structural match-
ing, for content-based matching, and for dramatically scaling up the number of candi-
date pairs that can be generated for any given language pair by using the industrial-
strength Web crawls stored on the Internet Archive. In this section we put all these
pieces together, describing an experiment in mining the Internet Archive to find
English-Arabic parallel text. The language pair English-Arabic is of particular global
importance, but resources for it, particularly bilingual text, have generally not been
easy to obtain. Moreover, Arabic is far behind on the Web?s exponential growth curve:
Arabic text (as opposed to images) did not really start emerging on the Web until
the release of Microsoft Windows 98, which provided Arabic support in its version of
Internet Explorer.20
5.1 Finding English-Arabic Candidate Pairs on the Internet Archive
The input resources for our search for English-Arabic candidate pairs were a list of
Internet domains likely to contain Arabic text.21 The list included 24 top-level national
domains for countries where Arabic is spoken by a significant portion of the popula-
tion: Egypt (.eg), Saudi Arabia (.sa), Kuwait (.kw), etc. In addition, we used a list of
.com domains known to originate in Arabic-speaking countries. This list provided an
additional 21 specific domains (e.g., ?emiratesbank.com?, ?checkpoint.com?); note that
the list is by no means exhaustive.
In the experiments we report here, we mined two crawls from 2001, compris-
ing 8TB and 12TB (i.e., less than one-sixth of the Archive as it existed at the time
of the mining effort in early 2002) spread over 27 machines. Our list of URLs with
relevant domains, obtained through pattern matching in Archive index files, numbers
19,917,923 pages.22 The language-specific substrings given earlier were subtracted from
these URLs to generate handles, resulting in 786,880 buckets with an average of 25
pages per bucket. When all possible English-Arabic page pairs were generated from all
19 The Internet Archive tags its data for language using standard n-gram language identification
techniques.
20 While this article was under review, a large Arabic-English parallel corpus of United Nations
proceedings was released by the Linguistic Data Consortium ?http://www.ldc.upenn.edu?. Although
this corpus is certainly of great import, its availability does not detract from the main point of this
study. As noted in Section 1, UN parallel text from LDC is a specialized genre and is encumbered by
fees and licensing restrictions. The experiment reported here provides, at minimum, a supplementary
resource for English-Arabic, and it provides evidence for the viability of the approach.
21 We are grateful to Nizar Habash for constructing this list.
22 Pages with the same URL but different time stamps are counted separately; there were 10,701,622
unique URLs.
370
Computational Linguistics Volume 29, Number 3
Table 5
English-Arabic structural classification results.
Precision Recall
Baseline (on the full set) 0.8993 1.0000
Without tuning Fold 1 1.0000 0.0227
Fold 2 1.0000 0.1364
Fold 3 0.6667 0.0435
Average 0.8889 0.0675
With tuning Fold 1 0.9111 0.9318
Fold 2 0.9302 0.9090
Fold 3 0.9565 0.9565
Average 0.9326 0.9324
buckets, the result was 8,294 candidate pairs. This number is lower than what might
be expected, given the huge number of buckets, because many buckets were mono-
lingual; note that only pairs of one English and one Arabic document are deemed to
be candidates.
A random sample of two hundred candidate pairs was given to two human eval-
uators, bilingual in English and Arabic, who were asked (independently) to answer,
for each pair, the question ?Is this pair of pages intended to show the same material
to two different users, one a reader of English and the other a reader of Arabic?? The
judges? answers showed a Cohen?s ? agreement of 0.6955, which is generally consid-
ered fair to good reliability. (Qualitatively, one judge was rather more strict than the
other; when the stricter judge identified a page pair as valid translations, the less strict
judge virtually always agreed.)
5.2 Evaluating Structure-Based Matching
Taking the set of 149 labeled pairs on which the two judges agreed (134 were marked
?good,? 15 ?bad?), we carried out an evaluation of the full candidate set similar to the
one for English-French discussed in Section 2.2.3. This was a threefold cross-validation
experiment in which decision tree classifiers were tuned on the features extracted for
each candidate pair by structure-based classification.23 In addition to the four structural
scores, we included two language identification confidence scores (one for the English
page, one for the Arabic page); these were available as part of the Internet Archive?s
bookkeeping information for each URL and required no additional computation on
our part. Table 5 shows precision and recall of each fold?s classifier applied to the
corresponding test set of page pairs. The value of the parameter-tuning process is
dramatically confirmed by comparing the learned parameters with STRAND?s default
parameters (manually determined by Resnik [1999]).
Note, however, that the candidate generation system is highly precise to begin
with; only around 10% of the pairs in the random sample of candidates were con-
sidered ?bad? by both judges. A baseline system in which no filtering is done at all
achieves 89.93% precision on the full labeled set (with 100% recall). Depending on the
relative importance of precision and recall, these structure-based classifiers might be
considered worse than that baseline.
23 We did not use more than three folds in the cross-validation, since there were only 15 bad pairs and
more folds would have made random division into folds that each contained both ?good? and ?bad?
pairs difficult.
371
Resnik and Smith The Web as a Parallel Corpus
Upon inspection, we discovered that nearly 5,000 of the pairs in our candidate
set were from a single domain, ?maktoob.com?. This site supports an online market-
place, and many of the pages discovered by our search were dedicated to specific
merchandise categories within that service; a large portion of these were simply ?no
items available? and one or two similar messages. We ignored this domain completely
in order to be conservative about the yield of page pairs, though we note that many
of the pages within it are legitimate parallel text that could be extracted if a good
duplicates filter were applied.24
In order to construct a final classifier, we trained a decision tree on all 149 of the
manually judged examples on which both judges agreed. This was then applied to
the candidate pairs, producing a set of 1,741 HTML document pairs hypothesized to
be valid translations of one another. By way of simple duplicate detection, if a pair of
URLs appeared multiple times (under case-insensitive matching), it was counted only
once. (Note that when this occurs, the duplicate pair will differ by at least one time
stamp, and therefore a more sophisticated technique for eliminating duplication might
extract more text.) The remaining set contained 1,399 pairs.25 Converting from HTML
to plain text and tokenizing, the English documents in this corpus total approximately
673,108 tokens, with an average of 481 tokens per document; the Arabic side contains
845,891 tokens, averaging 605 tokens per document.26
5.3 Combining Structural and Content-Based Matching
We combined the structural and content-based approaches to detecting translations
by adding the tsim score to the set of structural features associated with each candi-
date pair, and then training a new decision tree classifier. Because Arabic is a highly
inflected language with many surface forms, we found it necessary to use morpho-
logical preprocessing in order to make effective use of a dictionary. For English, we
tokenized the text and used the WordNet lemmatizer to strip suffixes. The Arabic texts
were tokenized at punctuation, then romanized and converted to root forms using a
morphological analysis tool (Darwish 2002). This approximately halved the vocabulary
size for the Arabic texts (from 89,047 types to 48,212 types).
The translation lexicon used to compute tsim contained 52,211 entries, each contain-
ing one English lemma and one Arabic root.27 Of these, 16,944 contained two items that
were both present in the candidate set of 8,294 Web page pairs. The approximations
discussed in Section 3.2.2 were employed: Competitive linking on the first 500 words
in each document was used to compute the score.
Carrying out the same cross-validation experiment (on the same random split
of data), the combined structural and content-based classifier produced the results
in Table 6. Also shown is the performance of the tsim-only classifier, assuming an
24 One of our human evaluators confirmed that no other domains appeared to significantly dominate the
candidate pairs as did ?maktoob.com?, providing some assurance that the rest of the data are diverse.
25 Within that set, some documents (not pairs) were present multiple times; there were 1,385 unique
(apart from case) English URLs and 1,385 unique (apart from case) URLs. Since this is only a 1%
duplication rate for each language, we did not attempt to filter further.
26 We converted HTML to text using the lynx browser, performed cleanups such as removing references,
and tokenized using the tokenizers included with the Egypt statistical MT package (Al-Onaizan et
al. 1999). Those tokenizers are somewhat aggressive about separating out punctuation, so, being
aggressive in the opposite direction, we also tried counting only tokens containing at least one of
[A-Za-z] (which excludes punctuation as well as dates, percentages, etc.). Using that very conservative
counting method, the size of the English side is 493,702 words. Counting only tokens on the Arabic
side that contained at least one non-numeric, nonpunctuation character yielded 745,480 words.
27 This translation lexicon was used with the kind permission of Kareem Darwish.
372
Computational Linguistics Volume 29, Number 3
Table 6
English-Arabic combined structural/content-based classification results. The baseline and
content-only classifiers are on the full set, and the structure-only classifier is repeated for
reference.
Precision Recall
Baseline 0.8993 1.0000
Structure only (tuned, average) 0.9326 0.9324
Content only (oracle threshold, ? = 0.058) 0.7688 0.9925
Fold 1 0.9167 1.0000
Fold 2 0.9767 0.9545
Fold 3 0.9583 1.0000
Average 0.9506 0.9848
Table 7
Yield: The English-Arabic Internet Archive corpus, tokenized several ways.
Tokenization method English Tokens Arabic Tokens
Without tsim English lemmas, Arabic roots 620,826 641,654
Egypt tokenizers (Al-Onaizan et al 1999) 673,108 845,891
Egypt tokenizers, words with letters 493,702 745,480
With tsim English lemmas, Arabic roots 960,280 972,392
Egypt tokenizers (Al-Onaizan et al 1999) 1,021,839 1,326,803
Egypt tokenizers, words with letters 800,231 1,213,066
optimal threshold is chosen. Averaged over three folds, the classifier achieved 95.06%
precision and 98.48% recall (1.8% and 5.24% better than without tsim, respectively).
After building a single classifier on all 149 test pairs (the set on which both human
judges agreed), we reclassified the entire candidate set. Ignoring again pages from the
?maktoob.com? domain, 2,206 pairs were marked as translations. The same crude du-
plicate filter was applied, cutting the set back to 1,821 pairs.28 Table 7 shows word
counts for various tokenization schemes: the morphological analysis used for com-
puting tsim, the Egypt tokenizer (which is aggressive), and counting only tokens with
some alphabetic character from the Egypt tokenizer (a conservative approximation).
The analogous results, using the classifier from Section 5.2, are shown for comparison.
To summarize the results, using the content-based similarity score as a feature not
only improved precision, it increased the size of the corpus (in words) by 51?63%,
depending on the tokenization scheme.29
6. Future Work
A number of the techniques we have used to mine parallel data from the Web can be
improved, and we suggest here some directions.
28 There were 1,796 unique English URLs and 1,779 unique Arabic URLs, giving document duplication
rates of 1.4% and 2.4%, respectively.
29 A list of Wayback Machine URLs is available at ?http://umiacs.umd.edu/?resnik/strand/?; a sample
of the document pairs is included in Appendix A.
373
Resnik and Smith The Web as a Parallel Corpus
With respect to classifying document pairs as translations, the reader will notice
that our approach to content-based cross-lingual similarity essentially boils down to
a greedy matching of some of the words in a document pair using a dictionary. It
remains to be seen whether weights in the dictionary can be exploited (Smith [2001]
suggests that empirically estimated joint translation probabilities for word pairs are
not useful). We suggest that the incorporation of scores from information retrieval (e.g.,
inverse document frequency) might be useful in discerning which lexicon entries are
the strongest cues of translational equivalence. We also have not explored any filtering
on the noisy translation lexicon; doing so might improve the quality of the tsim score.
The competitive linking approximation (which, without weights, is essentially
random matching of word pairs) and the use of only the initial portion of each doc-
ument provide significant computational savings. In our experience, neither of these
has significantly hurt the performance of tsim-based classifiers (as compared to find-
ing the maximum cardinality bipartite matching and/or using the full documents),
and in some cases competitive linking seems to improve performance. It is possible
that some sample selection of words from document candidates might be profitable
(e.g., creating a sample of size proportional to the document length, as opposed to a
fixed size, or sampling only from content words, or sampling only words present in
the dictionary).
Smith (2002) suggested a bootstrapping paradigm for the construction of parallel
corpora. Beginning with a seed set of translation information (either parallel corpora
or a bilingual dictionary), high-precision initial classifiers might be constructed using
content and/or structural features (whichever are available). We might then iteratively
select additional page pairs in which the current classifier has high confidence of
translational equivalence, gradually increasing the pool of parallel data and at the
same time expanding the bilingual lexicon. This approach to minimally supervised
classifier construction has been widely studied (Yarowsky 1995), especially in cases in
which the features of interest are orthogonal in some sense (e.g., Blum and Mitchell
1998; Abney 2002).
With respect to the generation of candidate pairs, we have described a progression
from index-based searches on AltaVista to exhaustive matching of URLs on the Inter-
net Archive. The combination of these approaches may be profitable, particularly for
languages that are represented only very sparsely on the Web. For such languages,
index-based searches on words from a language of interest might be used to identify
sites potentially containing parallel text. Within such sites, it would likely be profitable
to look for parallel documents in the full cross product of documents in the two lan-
guages of interest, obtained both on the Internet Archive and via crawling all pages
on relevant sites.
Finally, we plan to utilize parallel texts mined from the Web in our work on
machine translation and acquisition of bilingual lexicons, and in the creation of re-
sources for new languages via projection of annotations from English.
7. Conclusions
Although efforts at discovering parallel text on the Web were first reported in 1998,
Web-based parallel corpora appear to have had only a limited impact on the commu-
nity. Three reasons for this suggest themselves.
Too few languages. Parallel text from the Web has been made available to the
community in only a few pairs of languages. As of this writing, the STRAND Web
site ?http://umiacs.umd.edu/?resnik/strand/?, presenting URL pairs discovered via
STRAND runs, contains collections only for English-French, English-Chinese, English-
374
Computational Linguistics Volume 29, Number 3
Basque, and now English-Arabic, and we are not aware of any other efforts to dissem-
inate Web-based parallel data publicly. Up to this point, it simply has not been easy
to search the Web for parallel text in new language pairs. The most difficult part is
finding the candidates: A year or two ago, we attempted to apply the original Web-
based STRAND to the problem of finding English-Arabic text, and we were unable to
locate enough search engine hits or sites to yield useful results.
Too little data. Very large Web-based parallel text collections are not available to
the community. The largest appear to have been obtained by Chen and Nie (2000),
who acquired collections on the order of 15,000 document pairs for English-French,
English-German, English-Dutch, English-Italian, and English-Chinese using the PT-
Miner system. However, these collections have not been made available to the gen-
eral community.30 In contrast, the STRAND collections, which are available to the
community in the form of URL pairs, are modest in size: The English-Chinese col-
lection contains fewer than 3,500 document pairs, and the English-French fewer than
2,500.
Difficulty with dissemination. Web-based collections are difficult to distribute.
Standard mechanisms of the sort used by the LDC (a CD or downloadable file) are
fraught with difficult legal issues, since, technically speaking, redistributing the ac-
tual content of Web pages could require permission from the author of every page.
For example, presumably as a risk reduction strategy, the Web track for TREC-2002
(Text Retrieval Conference) limited its attention to the .gov domain and required the
recipient of the data to sign a form that reduced the distributor?s liability.31 Simi-
larly, the Google Programming Contest data set arrived with a limited-use license,
indemnification from third-party claims, and a collection limited to the .edu do-
main, from which, presumably, authors are less likely to bring expensive lawsuits
?http://www.google.com/programming-contest/?.
A possible fourth reason may have to do with questions about the utility of the
data. For example, a Web-based parallel collection may be unpredictable in terms of its
coverage, and the community is well aware of the dangers of using training data that
are not representative of the test domain. A solution to this problem might be to extract
topically relevant subsets of the collection for particular domains or applications, but
of course this requires a ?more is better? approach in order to obtain subsets that are
large enough to be useful.
The work reported in this article addresses each of these major problems. With
respect to the number of language pairs, the Internet Archive offers us a huge sample
of pages on the Web, and our techniques make it easy to explore that collection in an
efficient way. Although it is probably impossible to crawl more than a small fraction of
the Web, the Internet Archive is storing the results of commercial-scale Web crawling
and has as its explicit mission the permanent storage of everything that can be found.
The fact that we were able to find a substantial quantity of English-Arabic text (on
the order of a million words per side, looking at less than a sixth of the Archive in
2002) offers the hope that it will be possible to find data for the less well-represented
language pairs, if and when those data actually exist. Moreover, the final implemen-
tation we described here retains the almost entirely language-independent character
of the original STRAND system, adding only the requirement of a reasonable transla-
30 These data were made available only to participants in the iCLEF evaluations for interactive
cross-language IR (C. Picci, personal communication).
31 ?The limitation on permitted use . . . is intended to reduce the risk of any action being brought by
copyright owners, but if this happens the Organisation [recipient] agrees to bear all associated liability?
?http://www.ted.cmis.csiro.au/TRECWeb/access to data.html?.
375
Resnik and Smith The Web as a Parallel Corpus
tion lexicon. Therefore success in mining for parallel text in other languages depends
primarily on whether the data exist in the Archive.
With regard to corpus size, we demonstrated that the recall of structural matching,
and hence its yield, can be significantly improved by simple and automatic classifier
construction, requiring only a few hours? work from a bilingual annotator to create
the training material. These results are further improved by adding content-based
similarity as a feature. Our success with English-Arabic, a language pair that is not
one of those usually considered well represented on the Web, encourages us to believe
that for other languages of interest, we will be similarly successful. We have also
done a bit of exploration to gauge the potential of the Archive for better-represented
language pairs, using English-Chinese as an example. By way of context, Chen and Nie
(2000) reported that PTMiner found around 15,000 English-Chinese document pairs by
crawling 185 sites in the .hk (Hong Kong) domain, with the run taking about a week.
We did a STRAND search of the two Internet Archive crawls used in the English-Arabic
study, seeking English-Chinese parallel text in multiple domains where Chinese is a
dominant language (e.g., .hk, .tw, .cn). Our initial candidate pair set was generated
in approximately 30 hours, and contains over 70,000 candidate page pairs. We are
optimistic that this can be improved still further by expanding the search to include
all sites that contain at least one Chinese document, regardless of the domain.
In terms of dissemination, the STRAND distribution mechanism models itself af-
ter Web search engines, distributing the URLs rather than the pages themselves. This
places the legal burden on individual users, who are presumably safe under fair use
provisions if they download pages for their individual use. Until recently the diffi-
culty with this solution has been that the collection of URLs deteriorates over time as
sites disappear, pages are reorganized, and underlying content changes: For example,
in April 2002, we attempted to download the documents in the STRAND English-
French, English-Chinese, and English-Basque collections, and we were able to access
successfully only around 67%, 43%, and 40% of the URL pairs, respectively. However,
the Internet Archive?s Wayback Machine provides a way to distribute persistent URLs.
With regard to the quality of the data, in Section 2.2.2 we discussed two studies
that demonstrate the utility of parallel Web data in acquiring translation lexicons for
cross-language information retrieval. We also reported on the results of a human rat-
ings study, which provided evidence that English-Chinese data mined from the Web
contain reasonably fluent, reasonably translated sentence pairs. It is worth pointing
out that, because STRAND expects pages to be very similar in structural terms, the
resulting document collections are particularly amenable to sentence- or segment-level
alignment. Indeed, just using dynamic programming to align the markup, ignoring
the text, produces reasonable first-pass alignments of the intervening text as a side
effect. We are currently adapting statistical text-based sentence alignment techniques
to take advantage of the markup available in Web-based document pairs.
Ultimately, the utility of parallel data from the Web is a question that will need
to be addressed in practice. The potential, of course, is as rich and diverse as the Web
itself, and what we as researchers can do with it is an exciting question that remains
to be answered.
Appendix A: Examples
The following page pairs (Figures 7?8) are representative of English-Arabic paral-
lel corpus extracted from the Internet Archive. The text from the pages is shown
in full. Note that the full corpus is available as a list of Wayback Machine URLs at
?http://umiacs.umd.edu/?resnik/strand?. These pages show the generally high qual-
376
Computational Linguistics Volume 29, Number 3
Figure 7
Representative English-Arabic page pair: ?petranews.gov.jo:80/2001/86/tocen.htm? (8 April
2001) and ?petranews.gov.jo:80/2001/86/tocar.htm? (6 April 2001).
Figure 8
Representative English-Arabic page pair: ?www.ole.com.kw:80/pictures.htm? (9 April 2001)
and ?www.ole.com.kw:80/picturesa.htm? (17 April 2001).
377
Resnik and Smith The Web as a Parallel Corpus
ity of the corpus and also illustrate some of the potential difficulties with parallel Web
data. For example, the Arabic page in the first pair includes an additional caption
not present in the English side. These kinds of problems are expected to be overcome
during sentence alignment processing.
Appendix B: Translation Ratings Criteria
For each item, participants were instructed to provide three ratings.
Quality of the English:
3. Very fluent: All of the English is comprehensible.
2. Fairly fluent: The major part of the English passes, but there are
noticeable errors.
1. Barely fluent: Only part of the English meaning is understandable.
0. Unintelligible: Nothing or almost nothing of the English is
comprehensible.
Quality of the Chinese:
3. Very fluent: All of the Chinese is comprehensible.
2. Fairly fluent: The major part of the Chinese passes, but there are
noticeable errors.
1. Barely fluent: Only part of the Chinese meaning is understandable.
0. Unintelligible: Nothing or almost nothing of the Chinese is
comprehensible.
Adequacy of translation:
4. The English and Chinese contain entirely the same meaning.
3. The English and Chinese contain mostly the same meaning.
2. The English and Chinese contain much of the same meaning.
1. The English and Chinese contain little of the same meaning.
0. The English and Chinese contain none of the same meaning.
Acknowledgments
This work has been supported in part by
Department of Defense Contract
RD-02-5700, DARPA/ITO Cooperative
Agreement N660010028910, ONR MURI
Contract FCPO.810548265, and National
Science Foundation Grant EIA0130422. The
second author is supported by a Fannie and
John Hertz Foundation Fellowship. We
would like to thank Nizar Habash, Kareem
Darwish, Mona Diab, Usama Soltan, and
David Smith for their assistance with
Arabic, and Jason Eisner, Rebecca Hwa,
Mark Liberman, Dan Melamed, Michael
Nossal, and Doug Oard for helpful
conversations. We are grateful to Yevgeniy
Lyudovyk for allowing us to use his
language identification software. We would
also like to thank Sang Hong, Greg Marton,
and Michael Subotin for assistance with
implementation, and Brewster Kahle, Andy
Jewell, Jad DeFanti, Brad Tofel, and Paula
378
Computational Linguistics Volume 29, Number 3
Keezer for permitting and facilitating our
use of the Internet Archive. Finally, we are
indebted to several Computational Linguistics
reviewers, whose comments helped us to
greatly improve this article.
References
Abney, Steven. 2002. Bootstrapping. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics
(ACL), Philadelphia, pages 360?367.
Ahuja, Ravindra K., Thomas L. Magnati,
and James B. Orlin. 1993. Network Flows:
Theory, Algorithms, and Applications.
Prentice Hall, Englewood Cliffs, NJ.
Al-Onaizan, Yaser, Jan Curin, Michael Jahr,
Kevin Knight, John Lafferty, I. Dan
Melamed, Franz-Josef Och, David Purdy,
Noah A. Smith, and David Yarowsky.
1999. Statistical machine translation.
Technical report, Johns Hopkins
University. Available at ?citeseer.
nj.nec.com/al-onaizan99statistical.html?.
Beesley, Kenneth R. 1988. Language
identifier: A computer program for
automatic natural-language identification
of on-line text. In D. L. Hammond, editor,
Language at the Crossroads: Proceedings of the
29th Annual Conference of the American
Translators Association, Medford, NJ, pages
47?54.
Blum, Avrim and Tom Mitchell. 1998.
Combining labeled and unlabeled data
with co-training. In Proceedings of the
Eleventh Annual Conference on
Computational Learning Theory, pages
92?100, Madison, WI, July.
Broder, Andrei Z., Steven C. Glassman,
Mark S. Manasse, and Geoffrey Zweig.
1997. Syntactic clustering of the Web. In
Proceedings of the Sixth International
World-Wide Web Conference, pages 391?404,
Santa Clara, CA, April.
Brown, Peter F., John Cocke, Stephen A.
Della Pietra, Vincent J. Della Pietra,
Frederick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine
translation. Computational Linguistics,
16(2):79?85.
Cabezas, Clara, Bonnie Dorr, and Philip
Resnik. 2001. Spanish language
processing at University of Maryland:
Building infrastructure for multilingual
applications. In Proceedings of the Second
International Workshop on Spanish Language
Processing and Language Technologies
(SLPLT-2). Available at ?ftp://ftp.
umiacs.umd.edu/pub/bonnie/slplt-
01.htm?.
Cavnar, William B. and John M. Trenkle.
1994. N-gram-based text categorization.
In Proceedings of Third Annual Symposium
on Document Analysis and Information
Retrieval (SDAIR-94), pages 161?175, Las
Vegas, NV.
Chen, Jiang and Jian-Yun Nie. 2000. Parallel
Web text mining for cross-language
information retrieval. In Recherche
d?Informations Assiste?e par Ordinateur
(RIAO), pages 62?77, Paris, April.
Church, Kenneth W. and Robert Mercer.
1993. Introduction to the special issue on
computational linguistics using large
corpora. Computational Linguistics,
19(1):1?24.
Dabbadie, Marianne, Anthony Hartley,
Margaret King, Keith J. Miller, Widad
Mustafa El Hadi, Andrei Popescu-Bellis,
Florence Reeder, and Michelle Vanni.
2002. A hands-on study of the reliability
and coherence of evaluation metrics. In
Workshop on Machine Translation Evaluation:
Human Evaluators Meet Automated Metrics
at the Third International Conference on
Language Resources and Evaluation
(LREC-2000), pages 8?16, Las Palmas,
Canary Islands, Spain, May.
Darwish, Kareem. 2002. Building a shallow
Arabic morphological analyser in one
day. In Proceedings of the Workshop on
Computational Approaches to Semitic
Languages, pages 22?29, Philadelphia, July.
Davis, Mark and Ted Dunning. 1995. A
TREC evaluation of query translation
methods for multi-lingual text retrieval. In
Fourth Text Retrieval Conference (TREC-4),
pages 483?498. NIST, Gaithersburg, MD.
Diab, Mona and Philip Resnik. 2002. An
unsupervised method for word sense
tagging using parallel corpora. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics
(ACL), Philadelphia, July.
Dunning, Ted. 1994. Statistical identification
of language. Computing Research
Laboratory Technical Memo MCCS
94-273, New Mexico State University, Las
Cruces, NM. Available at ?http://
citeseer.nj.nec.com/dunning94statistical.
html?.
Gale, William A. and Kenneth W. Church.
1991. Identifying word correspondences
in parallel texts. In Fourth DARPA
Workshop on Speech and Natural Language,
pages 152?157, Asilomar, CA, February.
Hunt, James W. and M. Douglas McIlroy.
1975. An algorithm for differential file
comparison. Technical Memorandum
75-1271-11, Bell Laboratories, October.
379
Resnik and Smith The Web as a Parallel Corpus
Hwa, Rebecca, Philip Resnik, Amy
Weinberg, and Okan Kolak. 2002.
Evaluating translational correspondence
using annotation projection. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages
392?399, Philadelphia, July.
Ingle, Norman C. 1976. A language
identification table. The Incorporated
Linguist, 15(4):98?101.
Landauer, Thomas K. and Michael L.
Littman. 1990. Fully automatic
cross-language document retrieval using
latent semantic indexing. In Proceedings of
the Sixth Annual Conference of the UW Centre
for the New Oxford English Dictionary and
Text Research, pages 31?38, Waterloo,
Ontario, October.
Lin, Dekang. 1998. An information-theoretic
definition of similarity. In Proceedings of the
Fifteenth International Conference on Machine
Learning (ICML-98), pages 296?304,
Madison, WI.
Lopez, Adam, Michael Nossal, Rebecca
Hwa, and Philip Resnik. 2002. Word-level
alignment for multilingual resource
acquisition. In Workshop on Linguistic
Knowledge Acquisition and Representation:
Bootstrapping Annotated Language Data, at
the Third International Conference on
Language Resources and Evaluation
(LREC-2000), Las Palmas, Canary Islands,
Spain, June. Available at ?http://
www.umiacs.umd.edu/?hwa/lnhr02.ps?.
Ma, Xiaoyi and Mark Liberman. 1999. Bits:
A method for bilingual text search over
the Web. In Machine Translation Summit
VII, September. Available at ?http://
www.ldc.upenn.edu/Papers/
MTSVII1999/BITS.ps?.
MacKay, David and Linda Peto. 1995. A
hierarchical Dirichlet language model.
Journal of Natural Language Engineering,
1(3):1?19.
Melamed, I. Dan. 1997. Automatic discovery
of non-compositional compounds in
parallel data. In Proceedings of the 2nd
Conference on Empirical Methods in Natural
Language Processing (EMNLP-97), pages
97?108, Providence, RI, August.
Melamed, I. Dan. 2000. Models of
translational equivalence among words.
Computational Linguistics, 26(2):221?249.
Menezes, Arul and Stephen D. Richardson.
2001. A best-first alignment algorithm for
automatic extraction of transfer mappings
from bilingual corpora. In Proceedings of
the ACL 2001 Workshop on Data-Driven
Methods in Machine Translation, pages
39?46, Toulouse, France.
Nie, Juan-Yun and Jian Cai. 2001. Filtering
noisy parallel corpora of Web pages. In
IEEE Symposium on Natural Language
Processing and Knowledge Engineering,
pages 453?458, Tucson, AZ, October.
Oard, Douglas W. 1997. Cross-language text
retrieval research in the USA. In Third
DELOS Workshop on Cross-Language
Information Retrieval, Zurich, March.
European Research Consortium for
Informatics and Mathematics, Sophia
Antipolis, France. Available at
?http://www.ercim.org/publication/ws-
proceedings/DELOS3/Oard.ps.gz?.
Och, Franz-Josef and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics (ACL), pages
295?302, Philadelphia, July.
Resnik, Philip. 1998. Parallel strands: A
preliminary investigation into mining the
Web for bilingual text. In Proceedings of the
Third Conference of the Association for
Machine Translation in the Americas
(AMTA-98). Langhorne, PA, October
28-31. Lecture Notes in Artificial
Intelligence 1529. Available at
?http://umiacs.umd.edu/?resnik/pubs/
amta98.ps.gz?.
Resnik, Philip. 1999. Mining the Web for
bilingual text. In Proceedings of the 37th
Annual Meeting of the ACL, pages 527?534,
College Park, MD, June.
Resnik, Philip and I. Dan Melamed. 1997.
Semi-automatic acquisition of
domain-specific translation lexicons. In
Fifth Conference on Applied Natural Language
Processing, Washington, DC.
Resnik, Philip, Douglas Oard, and Gina
Levow. 2001. Improved cross-language
retrieval using backoff translation. In
Proceedings of the First International
Conference on Human Language Technology
Research (HLT-2001), pages 153?155, San
Diego, CA, March.
Resnik, Philip, Mari Broman Olsen, and
Mona Diab. 1999. The Bible as a parallel
corpus: Annotating the ?Book of 2000
Tongues.? Computers and the Humanities,
33:129?153.
Riloff, Ellen, Charles Schafer, and David
Yarowsky. 2002. Inducing information
extraction systems for new languages via
cross-language projection. In Nineteenth
International Conference on Computational
Linguistics (COLING-2002), pages 828?834,
Taipei, Taiwan, August.
Shannon, Claude E. 1948. A mathematical
theory of communication. Bell Systems
Technical Journal, 27:379?423, 623?656.
380
Computational Linguistics Volume 29, Number 3
Smith, Noah A. 2001. Detection of
translational equivalence. Undergraduate
honors thesis, University of Maryland
College Park. Available at
?http://nlp.cs.jhu.edu/?nasmith/cmsc-
thesis.ps?.
Smith, Noah A. 2002. From words to
corpora: Recognizing translation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 95?102, Philadelphia,
July.
Tiedemann, Jo?rg. 1999. Automatic
construction of weighted string similarity
measures. In Joint SIGDAT Conference on
Empirical Methods in Natural Language
Processing and Very Large Corpora, pages
213?219, College Park, MD, June.
Yarowsky, David. 1995. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 189?196,
Cambridge, MA.
Yarowsky, David and Grace Ngai. 2001.
Inducing multilingual POS taggers and
NP bracketers via robust projection across
aligned corpora. In Proceedings of the
Second Meeting of the North American
Association for Computational Linguistics
(NAACL-2001), pages 200?207, Pittsburgh,
PA, June.
Yarowsky, David, Grace Ngai, and Richard
Wicentowski. 2001. Inducing multilingual
text analysis tools via robust projection
across aligned corpora. In Proceedings of
the First International Conference on Human
Language Technology Research (HLT-2001),
pages 161?168, San Diego, CA, March.
A Generative Probabilistic OCR Model for NLP Applications
Okan Kolak
Computer Science and UMIACS
University of Maryland
College Park, MD 20742, USA
okan@umiacs.umd.edu
William Byrne
CLSP
The Johns Hopkins University
Baltimore, MD 21218, USA
byrne@jhu.edu
Philip Resnik
Linguistics and UMIACS
University of Maryland
College Park, MD 20742, USA
resnik@umiacs.umd.edu
Abstract
In this paper, we introduce a generative prob-
abilistic optical character recognition (OCR)
model that describes an end-to-end process in
the noisy channel framework, progressing from
generation of true text through its transforma-
tion into the noisy output of an OCR system.
The model is designed for use in error correc-
tion, with a focus on post-processing the output
of black-box OCR systems in order to make
it more useful for NLP tasks. We present an
implementation of the model based on finite-
state models, demonstrate the model?s ability
to significantly reduce character and word er-
ror rate, and provide evaluation results involv-
ing automatic extraction of translation lexicons
from printed text.
1 Introduction
Although a great deal of text is now available in elec-
tronic form, vast quantities of information still exist pri-
marily (or only) in print. Critical applications of NLP
technology, such as rapid, rough document translation in
the field (Holland and Schlesiger, 1998) or information
retrieval from scanned documents (Croft et al, 1994), can
depend heavily on the quality of optical character recog-
nition (OCR) output. Doermann (1998) comments, ?Al-
though the concept of a raw document image database is
attractive, comprehensive solutions which do not require
complete and accurate conversion to a machine-readable
form continue to be elusive for practical systems.?
Unfortunately, the output of commercial OCR systems
is far from perfect, especially when the language in ques-
tion is resource-poor (Kanungo et al, in revision). And
efforts to acquire new language resources from hardcopy
using OCR (Doermann et al, 2002) face something of a
chicken-and-egg problem. The problem is compounded
by the fact that most OCR system are black boxes that do
not allow user tuning or re-training ? Baird (1999, re-
ported in (Frederking, 1999)) comments that the lack of
ability to rapidly retarget OCR/NLP applications to new
languages is ?largely due to the monolithic structure of
current OCR technology, where language-specific con-
straints are deeply enmeshed with all the other code.?
In this paper, we describe a complete probabilistic,
generative model for OCR, motivated specifically by (a)
the need to deal with monolithic OCR systems, (b) the fo-
cus on OCR as a component in NLP applications, and (c)
the ultimate goal of using OCR to help acquire resources
for new languages from printed text. After presenting
the model itself, we discuss the model?s implementation,
training, and its use for post-OCR error correction. We
then present two evaluations: one for standalone OCR
correction, and one in which OCR is used to acquire a
translation lexicon from printed text. We conclude with
a discussion of related research and directions for future
work.
2 The Model
Generative ?noisy channel? models relate an observable
string
 
to an underlying sequence, in this case recog-
nized character strings and underlying word sequences

. This relationship is modeled by  
 
, decom-
posed by Bayes?s Rule into steps modeled by  
 (the
source model) and   	   (comprising sub-steps gen-
erating
 
from
 ). Each step and sub-step is completely
modular, so one can flexibly make use of existing sub-
models or devise new ones as necessary.1
We begin with preliminary definitions and notation,
illustrated in Figure 1. A true word sequence
 



corresponds to a true character sequence
1Note that the process of ?generating?  from  is a math-
ematical abstraction, not necessarily related to the operation of
any particular OCR system.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 55-62
                                                         Proceedings of HLT-NAACL 2003
Figure 1: Word and character segmentation
 



 
 
 

, and the OCR system?s output char-
acter sequence is given by
 



 

 

.
A segmentation of the true character sequence into
 subsequences is represented as

 


 

. Seg-
ment boundaries are only allowed between characters.
Subsequences are denoted using segmentation positions





 

	
 
, where 



,



, and




. The 
 define character subsequences
 





 Desparately Seeking Cebuano
Douglas W. Oard, David Doermann, Bonnie Dorr, Daqing He, Philip Resnik, and Amy Weinberg
UMIACS, University of Maryland, College Park, MD, 20742
(oard,doermann,bonnie,resnik,weinberg)@umiacs.umd.edu
William Byrne, Sanjeev Khudanpur and David Yarowsky
CLSP, Johns Hopkins University, 3400 North Charles Street, Barton Hall, Baltimore, MD 21218
(byrne,khudanpur,yarowsky)@jhu.edu
Anton Leuski, Philipp Koehn and Kevin Knight
USC Information Sciences Institute, 4676 Admiralty Way, Marina Del Rey, CA 90292
(leuski,koehn,knight)@isi.edu
Abstract
This paper describes an effort to rapidly de-
velop language resources and component tech-
nology to support searching Cebuano news sto-
ries using English queries. Results from the
first 60 hours of the exercise are presented.
1 Introduction
The Los Angeles Times reported that at about 5:20 P.M.
on Tuesday March 4, 2003, a bomb concealed in a back-
pack exploded at the airport in Davao City, the second
largest city in the Philippines. At least 23 people were
reported dead, with more than 140 injured, and Pres-
ident Arroyo of the Philippines characterized the blast
as a terrorist act. With the 13 hour time difference, it
was then 4:20 A.M on the same date in Washington, DC.
Twenty-four hours later, at 4:13 A.M. on March 5, partic-
ipants in the Translingual Information Detection, Extrac-
tion and Summarization (TIDES) program were notified
that Cebuano had been chosen as the language of interest
for a ?surprise language? practice exercise that had been
planned quite independently to begin on that date. The
notification observed that Cebuano is spoken by 24% of
the population of the Philippines, and that it is the lingua
franca in the south Philippines, where the event occurred.
One goal of the TIDES program is to develop the abil-
ity to rapidly deploy a broad array of language technolo-
gies for previously unforeseen languages in response to
unexpected events. That capability will be formally ex-
ercised for the first time during June 2003, in a month-
long ?Surprise Language Experiment.? To prepare for
that event, the Linguistic Data Consortium (LDC) orga-
nized a ?dry run? for March 5-14 in order to refine pro-
cedures for rapidly developing language resources of the
type that the TIDES community will need during the July
evaluation.
Development of interactive Cross-Language Informa-
tion Retrieval (CLIR) systems that can be rapidly adapted
to accommodate new languages has been the focus of
extensive collaboration between the University of Mary-
land and The Johns Hopkins University, and more re-
cently with the University of Southern California. The
capability for rapid development of necessary language
resources is an essential part of that process, so we had
been planning to participate in the surprise language dry
run to refine our procedures for sharing those resources
with other members of the TIDES community. Naturally,
we chose CLIR as a driving application to focus our ef-
fort. Our goal, therefore, was to build an interactive sys-
tem that would allow a searcher posing English queries
to find relevant Cebuano news articles from the period
immediately following the bombing.
2 Obtaining Language Resources
Our basic approach to development of an agile system for
interactive CLIR relies on three strategies: (1) create an
infrastructure in advance for English as a query language
that makes only minimal assumptions about the docu-
ment language; (2) leverage the asymmetry inherent in
the problem by assembling strong resources for English
in advance; and (3) develop a robust suite of capabilities
to exploit any language resources that can be found for
the ?surprise language.? We defer the first two topics to
the next section, and focus here on the third. We know of
five possible sources of translation expertise:
People. People who know the language are an excellent
source of insight, and universities are an excellent
place to find such people. We were able to locate
a speaker of Cebuano within 50 feet of one of our
offices, and to schedule an interview with a second
Cebuano speaker within 36 hours of the announce-
ment of the language.
Scholarly literature. Major research universities are
also an excellent place to find written materials de-
scribing a broad array of languages. Within 12 hours
of the announcement, reference librarians at the Uni-
versity of Maryland had identified a textbook on
?Beginning Cebuano,? and we had located a copy
at the University of Southern California. Together
with the excellent electronic resources located by the
LDC, this allowed us to develop a rudimentary stem-
mer within 36 hours.
Translation lexicons. Simple bilingual term lists are
available for many language pairs. Using links pro-
vided by the LDC and our own Web searches, we
were able to construct an English-Cebuano term list
with over 14,000 translation pairs within 12 hours of
the announcement. This largely duplicated a simul-
taneous effort at the LDC, and we later merged our
term list with theirs.
Parallel text. Translation-equivalent documents, when
aligned at the word level, provide an excellent
source of information about not just possible trans-
lations, but their relative predominance. Within 24
hours of the announcement, we had aligned Ce-
buano and English versions of the Holy Bible at
the word level using Giza++. An evaluation by a
native Cebuano speaker of a stratified random sam-
ple of 88 translation pairs showed remarkably high
precision. On a 4-point scale with 1=correct and
4=incorrect the most frequent 100 words averaged
1.3, the next 400 most frequent terms averaged 1.6,
and the 500 next most frequent terms after that aver-
aged 1.7. The Bible?s vocabulary covers only about
half of the words found in typical English news text
(counted by-token), so it is useful to have additional
sources of parallel text. For this reason, we have ex-
tended our previously developed STRAND system
to locate likely translations in the Internet Archive.
Those runs were not yet complete when this paper
was submitted.
Printed Dictionaries. People learning a new language
make extensive use of bilingual dictionaries, so we
have developed a system that mimics that process
to some extent. Within 12 hours of the announce-
ment we had zoned page images from a Cebuano-
English dictionary that was available commercially
in Adobe Page Description Format (PDF) to iden-
tify each dictionary entry, performed optical charac-
ter recognition, and parsed the entries to construct a
bilingual term list. We were aided in this process by
the fact that Cebuano is written in a Roman script.
Again, we achieved good precision, with a sampled
word error rate for OCR of 6.9% and a precision for
a random sample of translation pairs of 87%. Part of
speech tags were also extracted, although they are
not used in our process.
As this description illustrates, these five sources pro-
vide complementary information. Since there is some
uncertainty at the outset about how long it will be before
each delivers useful results, we chose a strategy based
on concurrency, balancing our investment over each the
five sources. This allowed us to use whatever resources
became available first to get an initial system running,
with refinements subsequently being made as additional
resources became available. Because Cebuano and En-
glish are written in the same script, we did not need char-
acter set conversion or phonetic cognate matching in this
case. The CLIR system described in the next section
was therefore constructed using only English resources
that were (or could have been) pre-assembled, plus a
Cebuano-English bilingual term list, a rule-based stem-
mer, and the Cebuano Bible.
3 Building a Cross-Language Retrieval
System
Ideally, we would like to build a system that would find
whatever documents the searcher would wish to read in a
fully automatic mode. In practice, fully automatic search
systems are imperfect even in monolingual applications.
We therefore have developed an interactive approach that
functions something like a typical Web search engine: (1)
the searcher poses their query in English, (2) the sys-
tem ranks the Cebuano documents in decreasing order
of likely relevance to the query, (3) the searcher exam-
ines a list of document titles in something approximat-
ing English, and (4) the searcher may optionally exam-
ine the full text of any document in something approx-
imating English. The intent is to support an iterative
process in which searchers learn to better express their
query through experience. We are only able to provide
very rough translations, so we expect that such a sys-
tem would be used in an environment where searchers
could send documents that appear promising off for pro-
fessional translation when necessary.
At the core of our system is the capability to au-
tomatically rank Cebuano documents based on an En-
glish query. We chose a query translation architecture
using backoff translation and Pirkola?s structured query
method, implemented using Inquery version 3.1p1. The
key idea in backoff translation is to first try to find con-
secutive sequences of query words on the English side
of the bilingual term list, where that fails to try to find
the surface form of each remaining English term, to fall
back to stem matching when necessary, and ultimately to
fall back to retaining the English term unchanged in the
hope that it might be a proper name or some other form
of cognate with Cebuano. Accents are stripped from the
documents and all language resources to facilitate match-
ing at that final step.
Although we have chosen techniques that are relatively
robust and therefore require relatively little domain-
specific tuning, stemmer design is an area of uncertainty
that could adversely affect retrieval effectiveness. We
therefore needed a test collection on which we could try
out variants of the Cebuano stemmer. We built this test
collection using 34,000 Cebuano Bible verses and 50 En-
glish questions that we found on the Web for which ap-
propriate Bible verses were known. Each question was
posed as a query using the batch mode of Inquery, and
the rank of the known relevant verse was taken as a mea-
sure of effectiveness. We took the mean reciprocal rank
(the inverse of the harmonic mean) as a figure of merit
for each configuration, and used a paired two-tailed   -
test (with p  0.05) to assess the statistical significance of
observed differences. Our initial configuration, without
stemming, obtained a mean inverse rank of 0.14, which
is a statistically significant improvement over no transla-
tion at all (mean inverse rank 0.02 from felicitous cognate
and loan word matches). The addition of Cebuano stem-
ming resulted in a reduction in mean inverse rank to 0.09.
Although the reduction is not statistically significant in
that case, the result suggests that our initial stemmer is
not yet useful for information retrieval tasks.
The other key capability that is needed is title and doc-
ument translation. We can accomplish this in one of two
ways. The simplest approach is to reverse the bilingual
term list, and to reverse the role of Cebuano and En-
glish in the process described above for query transla-
tion. Our user interface is capable of displaying multi-
ple translations for a single term (arranged horizontally
for compact depiction or vertically for clearer depiction),
but searchers can choose to display only the single most
likely translation. When reliable translation probability
statistics (from parallel text) are not available, we use the
relative word unigram frequency of each translation of a
Cebuano term in a representative English collection as a
substitute for that probability. A more sophisticated way
is to build a statistical machine translation system using
parallel text. We built our first statistical machine trans-
lation system within 40 hours of the announcement, and
one sentence of the resulting translation using each tech-
nique is shown below:
Cebuano: ?ang rebeldeng milf, kinsa
lakip sa nangamatay, nagdala og
backpack nga dunay explosives nga
niguba sa waiting lounge sa airport,
matod sa mga defense official.?
Term-by-term translation:
?(carelessness, circumference,
conveyence) rebeldeng milf, who lakip
(at in of) nangamatay, nagdala og
backpack nga valid explosives nga
niguba (at, in of) waiting lounge
(at, in, of) airport, matod (at, in,
of) mga defense official?
Statistical translation: ?who was
accused of rank, ehud og niguba
waiting lounge defense of those dumah
milf rebeldeng explosives backpack
airport matod official.?
At this point, term-by-term translation is clearly the bet-
ter choice. But as more parallel text becomes available,
we expect the situation to reverse. The LDC is prepar-
ing a set of human reference translations that will allow
us to detect that changeover point automatically using the
NIST variant of the BLEU measure for machine transla-
tion effectiveness.
4 Conclusion
The results reported in this paper were accomplished by
a team of 20 people with expertise in various facets of te
task that invested about 250 person-hours over two and
a half days. As additional Cebuano-specific evaluation
resources are developed, we expect to gain additional in-
sight into the quality of these early resources. Moreover,
once we see what works best for Cebuano by the end of
the process, we plan to revisit our process design with
an eye towards better optimizing our initial time invest-
ments. We expect to be able to address both of those
points in detail by the time of the conference.
This exercise was originally envisioned as a dry run to
work out the kinks in our process, and indeed we have
already learned a lot on that score. First, we learned that
our basic approach seems sound; we built the key com-
ponents of an interactive CLIR system in about 40 hours,
and by the 60-hour point we had some basis for believing
that each of those components could at least minimally
fulfill their role in a fully integrated system. Some of our
time was, however, spent on things that could have been
done in advance. Perhaps the most important of these
was the development of an information retrieval test col-
lection using the Bible. That job, and numerous smaller
ones, are now done, so we expect that we will be able to
obtain similar results with about half the effort next time
around.
Acknowledgments
Thanks to Clara Cabezas, Tim Hackman, Margie Hi-
nonangan, Burcu Karagol-Ayan, Okan Kolak, Huanfeng
Ma, Grazia Russo-Lassner, Michael Subotin, Jianqiang
Wang and the LDC! This work has been supported in part
by DARPA contract N660010028910.
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 503?511,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
More than Words:
Syntactic Packaging and Implicit Sentiment
Stephan Greene?
ATG, Inc.
1111 19th St, NW Suite 600
Washington, DC 20036
sgreene@atg.com
Philip Resnik
Linguistics / UMIACS CLIP Laboratory
University of Maryland
College Park, MD 20742
resnik@umiacs.umd.edu
Abstract
Work on sentiment analysis often focuses on
the words and phrases that people use in
overtly opinionated text. In this paper, we in-
troduce a new approach to the problem that
focuses not on lexical indicators, but on the
syntactic ?packaging? of ideas, which is well
suited to investigating the identification of im-
plicit sentiment, or perspective. We establish a
strong predictive connection between linguis-
tically well motivated features and implicit
sentiment, and then show how computational
approximations of these features can be used
to improve on existing state-of-the-art senti-
ment classification results.
1 Introduction
As Pang and Lee (2008) observe, the last several
years have seen a ?land rush? in research on senti-
ment analysis and opinion mining, with a frequent
emphasis on the identification of opinions in evalua-
tive text such as movie or product reviews. How-
ever, sentiment also may be carried implicitly by
statements that are not only non-evaluative, but not
even visibly subjective. Consider, for example, the
following two descriptions of the same (invented)
event:
1(a) On November 25, a soldier veered his jeep into
a crowded market and killed three civilians.
(b) On November 25, a soldier?s jeep veered into a
crowded market, causing three civilian deaths.
?This work was done while the first author was a student in
the Department of Linguistics, University of Maryland.
Both descriptions appear on the surface to be objec-
tive statements, and they use nearly the same words.
Lexically, the sentences? first clauses differ only in
the difference between ?s and his to express the rela-
tionship between the soldier and the jeep, and in the
second clauses both kill and death are terms with
negative connotations, at least according to the Gen-
eral Inquirer lexicon (Stone, 1966). Yet the descrip-
tions clearly differ in the feelings they evoke: if the
soldier were being tried for his role in what hap-
pened on November 25, surely the prosecutor would
be more likely to say (1a) to the jury, and the defense
attorney (1b), rather than the reverse.1
Why, then, should a description like (1a) be per-
ceived as less sympathetic to the soldier than (1b)?
If the difference is not in the words, it must be in
the way they are put together; that is, the structure
of the sentence. In Section 2, we offer a specific hy-
pothesis about the connection between structure and
implicit sentiment: we suggest that the relationship
is mediated by a set of ?grammatically relevant? se-
mantic properties well known to be important cross-
linguistically in characterizing the interface between
syntax and lexical semantics. In Section 3, we val-
idate this hypothesis by means of a human ratings
study, showing that these properties are highly pre-
dictive of human sentiment ratings. In Section 4, we
introduce observable proxies for underlying seman-
tics (OPUS), a practical way to approximate the rele-
vant semantic properties automatically as features in
a supervised learning setting. In Section 5, we show
that these features improve on the existing state of
the art in automatic sentiment classification. Sec-
1We refer readers not sharing this intuition to Section 3.
503
tions 6 and 7 discuss related work and summarize.
2 Linguistic Motivation
Verbal descriptions of an event often carry along
with them an underlying attitude toward what is be-
ing described. By framing the same event in differ-
ent ways, speakers or authors ?select some aspects
of a perceived reality and make them more salient
in a communicating text, in such a way as to pro-
mote a particular problem definition, causal inter-
pretation, moral evaluation, and/or treatment recom-
mendation? (Entman, 1993, p. 52). Clearly lexi-
cal choices can accomplish this kind of selection,
e.g. choosing to describe a person as a terrorist
rather than a freedom fighter, or referencing killer
whales rather than orcas.2 Syntactic choices can
also have framing effects. For example, Ronald Rea-
gan?s famous use of the passive construction, ?Mis-
takes were made? (in the context of the Iran-Contra
scandal), is a classic example of framing or spin:
used without a by-phrase, the passive avoids iden-
tifying a causal agent and therefore sidesteps the is-
sue of responsibility (Broder, 2007). A toddler who
says ?My toy broke? instead of ?I broke my toy? is
employing the same linguistic strategy.
Linguists have long studied syntactic variation
in descriptions of the same event, often under the
general heading of syntactic diathesis alternations
(Levin, 1993; Levin and Hovav, 2005). This line
of research has established a set of semantic prop-
erties that are widely viewed as ?grammatically rel-
evant? in the sense that they enable generalizations
about syntactic ?packaging? of meaning within (and
across) the world?s languages. For example, the
verb break in English participates in the causative-
inchoative alternation (causative event X broke Y
can also be expressed without overt causation as Y
broke), but the verb climb does not (X also causes
the event in X climbed Y, but that event cannot be
expressed as Y climbed). These facts about partic-
ipation in the alternation turn out to be connected
with the fact that a breaking event entails a change of
state in Y but a climbing event does not. Grammati-
cally relevant semantic properties of events and their
2Supporters of an endangered species listing in Puget Sound
generally referred to the animals as orcas, while opponents gen-
erally said killer whales (Harden, 2006).
participants ? causation, change of state, and others
? are central not only in theoretical work on lex-
ical semantics, but in computational approaches to
the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr,
1993; Wu and Palmer, 1994; Dang et al, 1998)).
The approach we propose draws on two influ-
ential discussions about grammatically relevant se-
mantic properties in theoretical work on lexical se-
mantics. First, Dowty (1991) characterizes gram-
matically relevant properties of a verb?s arguments
(e.g. subject and object) via inferences that follow
from the meaning of the verb. For example, expres-
sions like X murders Y or X interrogates Y entail
that subject X caused the event.3 Second, Hopper
and Thompson (1980) characterize ?semantic transi-
tivity? using similar properties, connecting semantic
features to morphosyntactic behavior across a wide
variety of languages.
Bringing together Dowty with Hopper and
Thompson, we find 13 semantic properties or-
ganized into three groups, corresponding to the
three components of a canonical transitive clause,
expressed as X verb Y in English.4 Proper-
ties associated with X involve volitional involve-
ment in the event or state, causation of the event,
sentience/awareness and/or perception, causing a
change of state in Y , kinesis or movement, and ex-
istence independent of the event. Properties asso-
ciated with the event or state conveyed by the verb
include aspectual features of telicity (a defined end-
point) and punctuality (the latter of which may be
inversely related to a property known as incremen-
tal theme). Properties associated with Y include
affectedness, change of state, (lack of) kinesis or
movement, and (lack of) existence independent of
the event.
Now, observe that this set of semantic proper-
ties involves many of the questions that would nat-
urally help to shape one?s opinion about the event
described by veer in (1). Was anyone or anything
affected by what took place, and to what degree?
Did the event just happen or was it caused? Did the
event reach a defined endpoint? Did participation in
3Kako (2006) has verified that people make these inferences
based on X?s syntactic position even when a semantically empty
nonsense verb is used.
4We are deliberately sidestepping the choice of terminology
for X and Y, e.g. proto-Patient, theme, etc.
504
the event involve conscious thought or intent? Our
hypothesis is that the syntactic aspects of ?framing?,
as characterized by Entman, involve manipulation of
these semantic properties, even when overt opinions
are not being expressed. That is, we propose a con-
nection between syntactic choices and implicit senti-
ment mediated by the very same semantic properties
that linguists have already identified as central when
connecting surface expression to underlying mean-
ing more generally.
3 Empirical Validation
We validated the hypothesized connection between
implicit sentiment and grammatically relevant se-
mantic properties using psycholinguistic methods,
by varying the syntactic form of event descriptions,
and showing that the semantic properties of descrip-
tions do indeed predict perceived sentiment.5
3.1 Semantic property ratings
Materials. Stimuli were constructed using 11
verbs of killing, which are widely viewed as proto-
typical for the semantic properties of interest here
(Lemmens, 1998): X killed Y normally involves
conscious, intentional causation by X of a kinetic
event that causes a (rather decisive and clearly ter-
minated!) change of state in Y . The verbs comprise
two classes: the ?transitive? class, involving ex-
ternally caused change-of-state verbs (kill, slaugh-
ter, assassinate, shoot, poison), and the ?ergative?
class (strangle, smother, choke, drown, suffocate,
starve), within which verbs are internally caused
(McKoon and MacFarland, 2000) or otherwise em-
phasize properties of the object. Variation of syntac-
tic description involved two forms: a transitive syn-
tactic frame with a human agent as subject (?transi-
tive form?, 2a), and a nominalization of the verb as
subject and the verb kill as the predicate (?nominal-
ized form?, 2b).
2(a) The gunmen shot the opposition leader
(b) The shooting killed the opposition leader
Participants and procedure. A set of 18 vol-
unteer participants, all native speakers of English,
were presented with event descriptions and asked to
answer questions probing both Dowty?s proto-role
5Full details and materials in Greene (2007).
properties as well as Hopper and Thompson?s se-
mantic transitivity components, responding via rat-
ings on a 1-to-7 scale. For example, the questions
probing volition were: ?In this event, how likely
is it that ?subject? chose to be involved??, where
?subject? was the gunmen and the shooting, for 2(a-
b), respectively.6
3.2 Sentiment ratings
Materials. We used the materials above to con-
struct short, newspaper-like paragraphs, each one
accompanied by a ?headline? version of the same
syntactic descriptions used above. For example,
given this paragraph:
A man has been charged for the suffocation of a
woman early Tuesday morning. City police say
the man suffocated the 24-year-old woman using
a plastic garbage bag. The woman, who police say
had a previous relationship with her attacker, was
on her way to work when the incident happened.
Based on information provided by neighbors, po-
lice were able to identify the suspect, who was ar-
rested at gunpoint later the same day.
the three alternative headlines would be:
3(a) Man suffocates 24-year old woman
(b) Suffocation kills 24-year-old woman
(c) 24-year-old woman is suffocated
Some paragraphs were based on actual news sto-
ries.7 In all paragraphs, there is an obvious nomi-
nal referent for both the perpetrator and the victim,
it is clear that the victim dies, and the perpetrator
in the scenario is responsible for the resulting death
directly rather than indirectly (e.g. through negli-
6Standard experimental design methods were followed with
respect to counterbalancing, block design, and distractor stim-
uli; for example, no participant saw more than one of 2(a) or
2(b), and all participants saw equal numbers of transitive and
nominalized descriptions. The phrase In this event was repeated
in each question and emphasized visually in order to encourage
participants to focus on the particular event described in the sen-
tence, rather than on the entities or events denoted in general.
7In those cases no proper names were used, to avoid any
inadvertent emotional reactions or legal issues, although the de-
scriptions retained emotional impact because we wanted readers
to have some emotional basis with which to judge the headlines.
505
gence).8 The stem of the nominalization always ap-
peared in the event description in either verbal or
nominal form.
Participants and procedure. A set of 31 volun-
teers, all native speakers of English, were presented
with the paragraph-length descriptions and accom-
panying headlines. As a measure of sentiment, par-
ticipants were asked to rate headlines on a 1-to-7
scale with respect to how sympathetic they perceive
the headline to be toward the perpetrator. For exam-
ple, given the paragraph and one of the associated
headlines in (3), a participant would be asked to rate
?How sympathetic or unsympathetic is this headline
to the man??9
3.3 Analysis and discussion
Unsurprisingly, but reassuringly, an analysis of the
sentiment ratings yields a significant effect of syn-
tactic form on sympathy toward the perpetrator
(F (2, 369) = 33.902, p < .001), using a mixed
model ANOVA run with the headline form as fixed
effect. The transitive form of the headline yielded
significantly lower sympathy ratings than the nom-
inalized or passive forms in pairwise comparisons
(both p < .001). We have thus confirmed empir-
ically that Reagan?s ?Mistakes were made? was a
wise choice of phrasing on his part.
More important, we are now in a position to ex-
amine the relationship between syntactic forms and
perceived sentiment in more detail. We performed
regression analyses treating the 13 semantic prop-
erty ratings plus the identity of the verb as indepen-
dent variables to predict sympathy rating as a de-
pendent variable, using the 24 stimulus sentences
that bridged both collections of ratings.10 Consid-
8An alert reader may observe that headlines with nominal-
ized subjects using the verb kill require some other nominaliza-
tion, so they don?t say ?Killing kills victim?. For these cases
in the data, an appropriate nominalization drawn from the event
description was used (e.g., explosion).
9Again, standard experimental design methods were used
with respect to block design, distractor stimuli, etc. The phrase
this headline was emphasized to stress that it is the headline
being rated, not the story. A second question rating sympathy
toward the victim was also asked in each case, as an additional
distractor.
10These involved only the transitive and nominalized forms,
because many of the questions were inapplicable to the passive
form. Since the two ratings studies involved different subject
ering semantic properties individually, we find that
volition has the strongest correlation with sympathy
(a negative correlation, with r = ?.776), followed
by sentience (r = ?.764) and kinesis/movement
(r = ?.751). Although performing a multiple re-
gression with all variables for this size dataset is im-
possible, owing to overfitting (as a rule of thumb,
5 to 10 observed items are necessary per each in-
dependent variable), a multiple regression involving
verb, volition, and telicity as independent variables
yields R = .88, R2 = .78 (p < .001). The value for
adjusted R2, which explicitly takes into account the
small number of observations, is 74.1.
In summary, then, this ratings study confirms the
influence of syntactic choices on perceptions of im-
plicit sentiment. Furthermore, it provides support
for the idea that this influence is mediated by ?gram-
matically relevant? semantic properties, demonstrat-
ing that these accounted for approximately 75% of
the variance in implicit sentiment expressed by al-
ternative headlines describing the same event.
4 Observable Approximation
Thus far, we have established a predictive connec-
tion between syntactic choices and underlying or im-
plicit sentiment, mediated by grammatically relevant
semantic properties. In an ideal world, we could har-
ness the predictive power of those properties by us-
ing volition, causation, telicity, etc. as features for
regression or classification in sentiment prediction
tasks. Unfortunately, the properties are not directly
observable, and neither automatic annotators nor la-
beled training data currently exist.
We therefore pursue a different strategy, which we
refer to as observable proxies for underlying seman-
tics (OPUS). It can be viewed as a middle ground
between relying on construction-level syntactic dis-
tinctions (such as the 3-way transitive, nominalized
subject, passive distinction in Section 3) and an-
notation of fine-grained semantic properties. The
key idea is to use observable grammatical relations,
drawn from the usages of terms determined to be
relevant to a domain, as proxies for the underlying
semantic properties that gave rise to their syntactic
realization using those relations. Automatically cre-
pools, regression models were run over the mean values of each
observation in the experimental data.
506
ated features based on those observable proxies are
then used in classification as described in Section 5.
In order to identify the set T of terms relevant
to a particular document collection, we adopt the
relative frequency ratio (Damerau, 1993), R(t) =
Rtdomain/Rtreference, where Rtc = f
t
c
Nc is the ratio of
term t?s frequency in corpus c to the size Nc of that
corpus. R(t) is a simple but effective comparison
of a term?s prevalence in a particular collection as
compared to a general reference corpus. We used
the British National Corpus as the reference because
it is both very large and representative of text from a
wide variety of domains and genres. The threshold
of R(t) permitting membership in T is an experi-
mental parameter.
OPUS features are defined in terms of syntactic
dependency relations involving terms in T . Given a
set D of syntactic dependency relations, features are
of the form t : d or d : t, with d ? D, t ? T . That
is, they are term-dependency pairs extracted from
term-dependency-term dependency tuples, preserv-
ing whether the term is the head or the dependent
in the dependency relation. In addition, we add two
construction-specific features: TRANS:v, which rep-
resents verb v in a canonical, syntactically transitive
usage, and NOOBJ:v, present when verb v is used
without a direct object.11
Example 4 shows source text (bolded clause in
4a), an illustrative subset of parser dependencies
(4b), and corresponding OPUS features (4c):
4(a) Life Without Parole does not eliminate the risk
that the prisoner will murder a guard, a visi-
tor, or another inmate.
(b) nsubj(murder, prisoner); aux(murder, will);
dobj(murder, guard)
(c) TRANS:murder, murder:nsubj, nsubj:prisoner,
murder:aux, aux:will, murder:dobj, dobj:guard
Intuitively the presence of TRANS:murder suggests
the entire complex of semantic properties discussed
in Section 2, bringing together the impliciation of
volition, causation, etc. on the part of prisoner
(as does nsubj:prisoner), affectedness and change of
state on the part of guard (as does dobj:guard), and
so forth.
11We parsed English text using the Stanford parser.
The NOOBJ features can capture a habitual read-
ing, or in some cases a detransitivizing effect as-
sociated with omission of the direct object (Olsen
and Resnik, 1997). The bold text in (5) yields
NOOBJ:kill as a feature.
5(a) At the same time, we should never ignore the
risks of allowing the inmate to kill again.
In this case, omitting the direct object decreases the
extent to which the killing event is interpreted as
telic, and it eliminates the possibility of attributing
change-of-state to a specific affected object (much
like ?Mistakes were made? avoids attributing cause
to a specified subject), placing the phrasing at a
less ?semantically transitive? point on the transi-
tivity continuum (Hopper and Thompson, 1980).
Some informants find a perceptible increase in neg-
ative sentiment toward inmate when the sentence is
phrased as in 5(b):
5(b) At the same time, we should never ignore the
risks of allowing the inmate to kill someone
again.
5 Computational Application
Having discussed linguistic motivation, empirical
validation, and practical approximation of seman-
tically relevant features, we now present two stud-
ies demonstrating their value in sentiment classifica-
tion. For the first study, we have constructed a new
data set particularly well suited for testing our ap-
proach, based on writing about the death penalty. In
our second study, we make a direct comparison with
prior state-of-the-art classification using the Bitter
Lemons corpus of Lin et al (2006).
5.1 Predicting Opinions of the Death Penalty
Corpus. We constructed a new corpus for exper-
imentation on implicit sentiment by downloading
the contents of pro- and anti-death-penalty Web
sites and manually checking, for a large subset,
that the viewpoints expressed in documents were as
expected. The collection, which we will refer to
as the DP corpus, comprises documents from five
pro-death-penalty sites and three anti-death-penalty
sites, and the corpus was engineered to have an even
balance, 596 documents per side.12
12Details in Greene (2007).
507
Frequent bigram baseline. We adopted a super-
vised classification approach based on word n-gram
features, using SVM classification in the WEKA
machine learning package. In initial exploration us-
ing both unigrams and bigrams, and using both word
forms and stems, we found that performance did not
differ significantly, and chose stemmed bigrams for
our baseline comparisons. In order to control for the
difference in the number of features available to the
classifier in our comparisons, we use the N most fre-
quent stemmed bigrams as the baseline feature set
where N is matched to number of OPUS features
used in the comparison condition.
OPUS-kill verbs: OPUS features for manually
selected verbs. We created OPUS features for 14
verbs ? those used in Section 3, plus murder, exe-
cute, and stab and their nominalizations (including
both event and -er nominals, e.g. both killing and
killer) ? generating N = 1016 distinct features.
OPUS-domain: OPUS features for domain-
relevant verbs. We created OPUS features for the
117 verbs for which the relative frequency ratio
was greater than 1. This list includes many of the
kill verbs we used in Section 3, and introduces,
among others, many transitive verbs describing acts
of physical force (e.g. rape, rob, steal, beat, strike,
force, fight) as well as domain-relevant verbs such
as testify, convict, and sentence. Included verbs near
the borderline included, for example, hold, watch,
allow, and try. Extracting OPUS features for these
verbs yielded N = 7552 features.
Evaluation. Cross-validation at the document
level does not test what we are interested in, since
a classifier might well learn to bucket documents ac-
cording to Web site, not according to pro- or anti-
death-penalty sentiment. To avoid this difficulty, we
performed site-wise cross-validation. We restricted
our attention to the two sites from each perspec-
tive with the most documents, which we refer to as
pro1, pro2, anti1, and anti2, yielding 4-fold cross-
validation. Each fold ftrain,test is defined as con-
taining all documents from one pro and one anti site
for training, using all documents from the remain-
ing pro and anti sites for testing. So, for exam-
ple, fold f11,22 uses all documents from pro1 and
anti1 in training, and all documents from pro2 and
Condition N features SVM accuracy
Baseline 1016 68.37
OPUS-kill verbs 1016 82.09
Baseline 7552 71.96
OPUS-domain 7552 88.10
Table 1: Results for 4-fold site-wise cross-validation us-
ing the DP corpus
Condition N features SVM accuracy
Baseline 1518 55.95
OPUS-frequent verbs 1518 55.95
OPUS-kill verbs 1062 66.67
Table 2: DP corpus comparison for OPUS features based
on frequent vs. domain-relevant verbs
anti2 for testing.13 As Table 1 shows, OPUS fea-
tures provide substantial and statistically significant
gains (p < .001).
As a reality check to verify that it is domain-
relevant verb usages and the encoding of events they
embody that truly drives improved classification, we
extracted OPUS features for the 14 most frequent
verbs found in the DP Corpus that were not in our
manually created list of kill verbs, along with their
nominalizations. Table 2 shows the results of a clas-
sification experiment using a single train-test split,
training on 1062 documents from pro1, pro2, anti1,
anti2 and testing on 84 test documents from the sig-
nificantly smaller remaining sites. Using OPUS
features for the most frequent non-kill verbs fails
to beat the baseline, establishing that it is not sim-
ply term frequency, the presence of particular gram-
matical relations, or a larger feature set that the kill-
verb OPUS model was able to exploit, but rather the
properties of event encodings involving the kill verbs
themselves.
5.2 Predicting Points of View in the
Israeli-Palestinian Conflict
In order to make a direct comparison here with prior
state-of-the-art work on sentiment analysis, we re-
port on sentiment classification using OPUS features
in experiments using a publicly available corpus in-
volving opposing perspectives, the Bitter Lemons
13Site (# of documents): pro1= clarkprosecutor.org (437),
pro2= prodeathpenalty.com (117), anti1= deathpenaltyinfo.org
(319), anti2= nodeathpenalty.org (212)
508
(hence BL) corpus introduced by Lin et al (2006).
Corpus. The Bitter Lemons corpus comprises es-
says posted at www.bitterlemons.org, which,
in the words of the site, ?present Israeli and Pales-
tinian viewpoints on prominent issues of concern?.
As a corpus, it has a number of interesting proper-
ties. First, its topic area is one of significant interest
and considerable controversy, yet the general tenor
of the web site is one that eschews an overly shrill
or extreme style of writing. Second, the site is orga-
nized in terms of issue-focused weekly editions that
include essays with contrasting viewpoints from the
site?s two editors, plus two essays, also contrasting,
from guest editors. This creates a natural balance be-
tween the two sides and across the subtopics being
discussed. The BL corpus as prepared by Lin et al
contains 297 documents from each of the Israeli and
Palestinian viewpoints, averaging 700-800 words in
length.
Lin et al classifiers. Lin et al report results on
distinguishing Israeli vs. Palestinian perspectives
using an SVM classifier, a naive Bayes classifier
NB-M using maximum a posteriori estimation, and a
naive Bayes classifier NB-B using full Bayesian in-
ference. (Document perspectives are labeled clearly
on the site.) We continue to use the WEKA SVM
classifier, but compare our results to both their SVM
and NB-B, since the latter achieved their best results.
OPUS features. As in Section 5.1, we experi-
mented with OPUS features driven by automati-
cally extracted lists of domain-relevant verbs. For
these experiments, we included domain-relevant
nouns, and we varied a threshold ? for the rela-
tive frequency ratio, including only terms for which
log(R(t)) > ?. In addition, we introduced a gen-
eral filter on OPUS features, eliminating syntactic
dependency types that do not usefully reflect seman-
tically relevant properties: det, predet, preconj, prt,
aux, auxpas, cc, punct, complm, mark, rel, ref, expl.
Evaluation. Lin et al describe two test scenar-
ios. In the first, referred to as Test Scenario 1, they
trained on documents written by the site?s guests,
and tested on documents from the site?s editors. Test
Scenario 2 represents the reverse, training on docu-
ments from the site editors and testing on documents
Classification Accuracy, BL Corpus
Test Scenario 1 (GeneralFilter)
0
2
4
6
8
10
12
Individual Experiment (? values and accuracy)
Te
rm
 
Th
re
sh
o
ld
 
(?)
84
86
88
90
92
94
96
98
Pe
rc
en
t C
o
rr
ec
t ? (Verb)
? (Noun)
OPUS
Lin 2006 NB-B
Lin 2006 SVM
 
Classification Accuracy, BL Corpus
Test Scenario 2 (GeneralFilter)
0
2
4
6
8
10
12
Individual Experiment (? values and accuracy)
Te
rm
 
Th
re
sh
o
ld
 
(?)
65
70
75
80
85
Pe
rc
en
t C
o
rr
ec
t ? (Verb)
? (Noun)
OPUS
Lin 2006 NB-B
Lin 2006 SVM
 Figure 1: Results on the Bitter Lemons corpus
from guest authors. As in our site-wise cross vali-
dation for the DP corpus, this strategy ensures that
what is being tested is classification according to the
viewpoint, not author or topic.
Figure 1 (top) summarizes a large set of experi-
ments for Test Scenario 1, in which we varied the
values of ? for verbs and nouns. Each experiment,
using a particular ??(verbs), ?(nouns)?, corresponds
to a vertical strip on the x-axis. The points on that
strip include the ? values for verbs and nouns, mea-
sured by the scale on the y-axis at the left of the
figure; the accuracy of Lin et al?s SVM (88.22% ac-
curacy, constant across all our variations); the accu-
racy of Lin et al?s NB-B classifier (93.46% accu-
racy, constant across all our variations), and the ac-
curacy of our SVM classifier using OPUS features,
which varies depending on the ? values. Across 423
experiments, our average accuracy is 95.41%, with
the best accuracy achieved being 97.64%. Our clas-
sifier underperformed NB-B slightly, with accura-
cies from 92.93% to 93.27%, in just 8 of the 423
experiments.
Figure 1 (bottom) provides a similar summary for
509
experiments in Test Scenario 2. The first thing to no-
tice is that accuracy for all methods is lower than for
Test Scenario 1. This is not terribly surprising: it is
likely that training a classifier on the more uniform
authorship of the editor documents builds a model
that generalizes less well to the more diverse au-
thorship of the guest documents (though accuracy
is still quite high). In addition, the editor-authored
documents comprise a smaller training set, consist-
ing of 7,899 sentences, while the guest documents
have a total of 11,033 sentences, a 28% difference.
In scenario 2, we obtain average accuracy across ex-
periments of 83.12%, with a maximum of 85.86%,
in this case outperforming the 81.48% obtained by
Lin?s SVM fairly consistently, and in some cases ap-
proaching or matching NB-B at 85.85%.
6 Related Work
Pang and Lee?s (2008) excellent monograph pro-
vides a thorough, well organized, and relatively re-
cent description of computational work on senti-
ment, opinion, and subjectivity analysis.
The problem of classifying underlying sentiment
in statements that are not overtly subjective is less
studied within the NLP literature, but it has received
some attention in other fields. These include, for ex-
ample, research on content analysis in journalism,
media studies, and political economy (Gentzkow
and Shapiro, 2006a; Gentzkow and Shapiro, 2006b;
Groseclose and Milyo, 2005; Fader et al, 2007); au-
tomatic identification of customer attitudes for busi-
ness e-mail routing (Durbin et al, 2003). And, of
course, the study of perceptions in politics and me-
dia bears a strong family resemblance to real-world
marketing problems involving reputation manage-
ment and business intelligence (Glance et al, 2005).
Within computational linguistics, what we call
implicit sentiment was introduced as a topic of study
by Lin et al (2006) under the rubric of identifying
perspective, though similar work had begun earlier
in the realm of political science (e.g. (Laver et al,
2003)). Other recent work focusing on the notion of
perspective or ideology has been reported by Martin
and Vanberg (2008) and Mullen and Malouf (2008).
Among prior authors, Gamon?s (2004) research is
perhaps closest to the work described here, in that
he uses some features based on a sentence?s logical
form, generated using a proprietary system. How-
ever, his features are templatic in nature in that they
do not couple specific lexical entries with their logi-
cal form. Hearst (1992) and Mulder et al (2004) de-
scribe systems that make use of argument structure
features coupled with lexical information, though
neither provides implementation details or experi-
mental results.
In terms of computational experimentation, work
by Thomas et al (2006), predicting yes and no
votes in corpus of United States Congressional floor
debate speeches, is quite relevant. They combined
SVM classification with a min-cut model on graphs
in order to exploit both direct textual evidence and
constraints suggested by the structure of Congres-
sional debates, e.g. the fact that the same individ-
ual rarely gives one speech in favor of a bill and an-
other opposing it. We have extend their method to
use OPUS features in the SVM and obtained signifi-
cant improvements over their classification accuracy
(Greene, 2007; Greene and Resnik, in preparation).
7 Conclusions
In this paper we have introduced an approach to
implicit sentiment motivated by theoretical work in
lexical semantics, presenting evidence for the role of
semantic properties in human sentiment judgments.
This research is, to our knowledge, the first to draw
an explicit and empirically supported connection be-
tween theoretically motivated work in lexical se-
mantics and readers? perception of sentiment. In ad-
dition, we have reported positive sentiment classifi-
cation results within a standard supervised learning
setting, employing a practical first approximation to
those semantic properties, including positive results
in a direct comparison with the previous state of the
art.
Because we computed OPUS features for opin-
ionated as well as non-evaluative language in our
corpora, obtaining overall positive results, we be-
lieve these features may also improve conventional
opinion labeling for subjective text. This will be in-
vestigated in future work.
Acknowledgments
The authors gratefully acknowledge useful discus-
sions with Don Hindle and Chip Denman.
510
References
John Broder. 2007. Familiar fallback for officials: ?mis-
takes were made?. New York Times. March 14.
F. J. Damerau. 1993. Generating and evaluating domain-
oriented multi-word terms from texts. Information
Processing and Management, 29:433?447.
Hoa Trang Dang, Karin Kipper, Martha Palmer, and
Joseph Rosenzweig. 1998. Investigating Regu-
lar Sense Extensions Based on Intersective Levin
Classes. In ACL/COLING 98, pages 293?299, Mon-
treal, Canada, August 10?14.
Bonnie J. Dorr. 1993. Machine Translation: A View from
the Lexicon. The MIT Press, Cambridge, MA.
David Dowty. 1991. Thematic Proto-Roles and Argu-
ment Selection. Language, 67:547?619.
S. D. Durbin, J. N. Richter, and D. Warner. 2003. A sys-
tem for affective rating of texts. In Proc. 3rd Workshop
on Operational Text Classification, KDD-2003.
Robert M. Entman. 1993. Framing: Toward clarification
of a fractured paradigm. Journal of Communication,
43(4):51?58.
Anthony Fader, Dragomir R. Radev, Michael H. Crespin,
Burt L. Monroe, Kevin M. Quinn, and Michael Co-
laresi. 2007. MavenRank: Identifying influential
members of the US Senate using lexical centrality. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vectors,
and the role of linguistic analysis. In Proc. COLING.
M. Gentzkow and J. Shapiro. 2006a. Media bias and
reputation. Journal of Political Economy, 114:280?
316.
M. Gentzkow and J. Shapiro. 2006b. What drives
media slant? Evidence from U.S. newspapers.
http://ssrn.com/abstract=947640.
Natalie Glance, Matthew Hurst, Kamal Nigam, Matthew
Siegler, Robert Stockton, and Takashi Tomokiyo.
2005. Deriving marketing intelligence from online
discussion. In Proc. KDD?05, pages 419?428, New
York, NY, USA. ACM.
Stephan Greene. 2007. Spin: Lexical Semantics, Tran-
sitivity, and the Identification of Implicit Sentiment.
Ph.D. thesis, University of Maryland.
T. Groseclose and J. Milyo. 2005. A measure of media
bias. The Quarterly Journal of Economics, 120:1191?
1237.
B. Harden. 2006. On Puget Sound, It?s Orca vs. Inc. The
Washington Post. July 26, page A3.
Marti Hearst. 1992. Direction-based text interpretation
as an information access refinement. In Paul Jacobs,
editor, Text-Based Intelligent Systems, pages 257?274.
Lawrence Erlbaum Associates.
Paul Hopper and Sandra Thompson. 1980. Transitivity
in Grammar and Discourse. Language, 56:251?295.
E. Kako. 2006. Thematic role properties of subjects and
objects. Cognition, 101(1):1?42, August.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
97(2):311?331.
M. Lemmens. 1998. Lexical perspectives on transitivity
and ergativity. John Benjamins.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment Realization. Research Surveys in Linguistics.
Cambridge University Press, New York.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago, IL.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you on?
identifying perspectives at the document and sentence
levels. In Proceedings of the Conference on Natural
Language Learning (CoNLL).
Lanny W. Martin and Georg Vanberg. 2008. A ro-
bust transformation procedure for interpreting political
text. Political Analysis, 16(1):93?100.
G. McKoon and T. MacFarland. 2000. Externally and
internally caused change of state verbs. Language,
pages 833?858.
M. Mulder, A. Nijholt, M. den Uyl, and P. Terpstra.
2004. A lexical grammatical implementation of affect.
In Proc. TSD-04, Lecture notes in computer science
3206, pages 171?178). Springer-Verlag.
Tony Mullen and Robert Malouf. 2008. Taking sides:
User classification for informal online political dis-
course. Internet Research, 18:177?190.
Mari Broman Olsen and Philip Resnik. 1997. Implicit
Object Constructions and the (In)transitivity Contin-
uum. In 33rd Proceedings of the Chicago Linguistic
Society, pages 327?336.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
James Pustejovsky. 1991. The Generative Lexicon.
Computational Linguistics, 17(4):409?441.
Philip J. Stone. 1966. The General Inquirer: A Com-
puter Approach to Content Analysis. The MIT Press.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition
from Congressional floor-debate transcripts. In Proc.
EMNLP, pages 327?335.
Zhibao Wu and Martha Palmer. 1994. Verb Semantics
and Lexical Selection. In Proc. ACL, pages 133?138,
Las Cruces, New Mexico.
511
Mapping Lexical Entries in a Verbs Database
to WordNet Senses
Rebecca Green   and Lisa Pearl   and Bonnie J. Dorr   and Philip Resnik  

Institute for Advanced Computer Studies
 
Department of Computer Science
University of Maryland
College Park, MD 20742 USA

rgreen,llsp,bonnie,resnik  @umiacs.umd.edu
Abstract
This paper describes automatic tech-
niques for mapping 9611 entries in a
database of English verbs to Word-
Net senses. The verbs were initially
grouped into 491 classes based on
syntactic features. Mapping these
verbs into WordNet senses provides a
resource that supports disambiguation
in multilingual applications such as
machine translation and cross-language
information retrieval. Our techniques
make use of (1) a training set of 1791
disambiguated entries, representing
1442 verb entries from 167 classes;
(2) word sense probabilities, from
frequency counts in a tagged corpus;
(3) semantic similarity of WordNet
senses for verbs within the same class;
(4) probabilistic correlations between
WordNet data and attributes of the
verb classes. The best results achieved
72% precision and 58% recall, versus a
lower bound of 62% precision and 38%
recall for assigning the most frequently
occurring WordNet sense, and an upper
bound of 87% precision and 75% recall
for human judgment.
1 Introduction
Our goal is to map entries in a lexical database
of 4076 English verbs automatically to Word-
Net senses (Miller and Fellbaum, 1991), (Fell-
baum, 1998) to support such applications as ma-
chine translation and cross-language information
retrieval. For example, the verb drop is multi-
ply ambiguous, with many potential translations
in Spanish: bajar, caerse, dejar caer, derribar,
disminuir,echar, hundir, soltar, etc. The database
specifies a set of interpretations for drop, depend-
ing on its context in the source-language (SL). In-
clusion of WordNet senses in the database enables
the selection of an appropriate verb in the target
language (TL). Final selection is based on a fre-
quency count of WordNet senses across all classes
to which the verb belongs?e.g., disminuir is se-
lected when the WordNet sense corresponds to the
meaning of drop in Prices dropped.
Our task differs from standard word sense dis-
ambiguation (WSD) in several ways. First, the
words to be disambiguated are entries in a lexical
database, not tokens in a text corpus. Second, we
take an ?all-words? rather than a ?lexical-sample?
approach (Kilgarriff and Rosenzweig, 2000): All
words in the lexical database ?text? are disam-
biguated, not just a small number for which de-
tailed knowledge is available. Third, we replace
the contextual data typically used for WSD with
information about verb senses encoded in terms
of thematic grids and lexical-semantic representa-
tions from (Olsen et al, 1997). Fourth, whereas a
single word sense for each token in a text corpus
is often assumed, the absence of sentential context
leads to a situation where several WordNet senses
may be equally appropriate for a database entry.
Indeed, as distinctions between WordNet senses
can be fine-grained (Palmer, 2000), it may be un-
clear, even in context, which sense is meant.
The verb database contains mostly syntactic in-
formation about its entries, much of which ap-
plies at the class level within the database. Word-
Net, on the other hand, is a significant source for
information about semantic relationships, much
of which applies at the ?synset? level (?synsets?
are WordNet?s groupings of synonymous word
senses). Mapping entries in the database to their
corresponding WordNet senses greatly extends
the semantic potential of the database.
2 Lexical Resources
We use an existing classification of 4076 English
verbs, based initially on English Verbs Classes
and Alternations (Levin, 1993) and extended
through the splitting of some classes into sub-
classes and the addition of new classes. The re-
sulting 491 classes (e.g., ?Roll Verbs, Group I?,
which includes drift, drop, glide, roll, swing) are
referred to here as Levin+ classes. As verbs may
be assigned to multiple Levin+ classes, the actual
number of entries in the database is larger, 9611.
Following the model of (Dorr and Olsen, 1997),
each Levin+ class is associated with a thematic
grid (henceforth abbreviated  -grid), which sum-
marizes a verb?s syntactic behavior by specify-
ing its predicate argument structure. For exam-
ple, the Levin+ class ?Roll Verbs, Group I? is as-
sociated with the  -grid [th goal], in which a
theme and a goal are used (e.g., The ball dropped
to the ground).1 Each  -grid specification corre-
sponds to a Grid class. There are 48 Grid classes,
with a one-to-many relationship between Grid and
Levin+ classes.
WordNet, the lexical resource to which we are
mapping entries from the lexical database, groups
synonymous word senses into ?synsets? and struc-
tures the synsets into part-of-speech hierarchies.
Our mapping operation uses several other data el-
ements pertaining to WordNet: semantic relation-
ships between synsets, frequency data, and syn-
tactic information.
Seven semantic relationship types exist be-
tween synsets, including, for example, antonymy,
hyperonymy, and entailment. Synsets are often
related to a half dozen or more other synsets; they
1There is also a Levin+ class ?Roll Verbs, Group II?
which is associated with the  -grid [th particle(down)], in
which a theme and a particle ?down? are used (e.g., The ball
dropped down).
may be related to multiple synsets through a single
relationship or may be related to a single synset
through multiple relationship types.
Our frequency data for WordNet senses is de-
rived from SEMCOR?a semantic concordance in-
corporating tagging of the Brown corpus with
WordNet senses.2
Syntactic patterns (?frames?) are associated
with each synset, e.g., Somebody s something;
Something s; Somebody s somebody into
V-ing something. There are 35 such verb frames
in WordNet and a synset may have only one or as
many as a half dozen or so frames assigned to it.
Our mapping of verbs in Levin+ classes to
WordNet senses relies in part on the relation be-
tween thematic roles in Levin+ and verb frames in
WordNet. Both reflect how many and what kinds
of arguments a verb may take. However, con-
structing a direct mapping between  -grids and
WordNet frames is not possible, as the underly-
ing classifications differ in significant ways. The
correlations between the two sets of data are better
viewed probabilistically.
Table 1 illustrates the relation between Levin+
classes and WordNet for the verb drop. In our
multilingual applications (e.g., lexical selection in
machine translation), the Grid information pro-
vides a context-based means of associating a verb
with a Levin+ class according to its usage in the
SL sentence. The WordNet sense possibilities are
thus pared down during SL analysis, but not suffi-
ciently for the final selection of a TL verb. For ex-
ample, Levin+ class 9.4 has three possible Word-
Net senses for drop. However, the WordNet sense
8 is not associated with any of the other classes;
thus, it is considered to have a higher ?information
content? than the others. The upshot is that the
lexical-selection routine prefers dejar caer over
other translations such as derribar and bajar.3
The other classes are similarly associated with ap-
2For further information see the WordNet manuals, sec-
tion 7, SEMCOR at http://www.cogsci.princeton.edu.
3This lexical-selection approach is an adaptation of the
notion of reduction in entropy, measured by information
gain (Mitchell, 1997). Using information content to quan-
tify the ?value? of a node in the WordNet hierarchy has
also been used for measuring semantic similarity in a tax-
onomy (Resnik, 1999b). More recently, context-based mod-
els of disambiguation have been shown to represent signif-
icant improvements over the baseline (Bangalore and Ram-
bow, 2000), (Ratnaparkhi, 2000).
Levin+ Grid/Example WN Sense Spanish Verb(s)
9.4
Directional
Put
[ag th mod-loc src goal]
I dropped the stone
1. move, displace
2. descend, fall, go down
8. drop set down, put down
1. derribar, echar
2. bajar, caerse
8. dejar caer, echar, soltar
45.6
Calibratable
Change of
State
[th]
Prices dropped
1. move, displace
3. decline, go down, wane
1. derribar, echar
3. disminuir
47.7
Meander
[th src goal]
The river dropped from
the lake to the sea
2. descend, fall, go down
4. sink, drop, drop down
2. bajar, caerse
4. hundir, caer
51.3.1
Roll I
[th goal]
The ball dropped to the
ground
2. descend, fall, go down 2. bajar, caerse
51.3.1
Roll II
[th particle(down)]
The ball dropped down
2. descend, fall, go down 2. bajar, caerse
Table 1: Relation Between Levin+ and WN Senses for ?drop?
propriate TL verbs during lexical selection: dis-
minuir (class 45.6), hundir (class 47.7), and bajar
(class 51.3.1).4
3 Training Data
We began with the lexical database of (Dorr and
Jones, 1996), which contains a significant number
of WordNet-tagged verb entries. Some of the as-
signments were in doubt, since class splitting had
occurred subsequent to those assignments, with
all old WordNet senses carried over to new sub-
classes. New classes had also been added since
the manual tagging. It was determined that the
tagging for only 1791 entries?including 1442
verbs in 167 classes?could be considered stable;
for these entries, 2756 assignments of WordNet
senses had been made. Data for these entries,
taken from both WordNet and the verb lexicon,
constitute the training data for this study.
The following probabilities were generated
from the training data:

	ff 	
  	
	
Inducing Frame Semantic Verb Classes from WordNet and LDOCE
Rebecca Green,  Bonnie J. Dorr,  and Philip Resnik*??   *?   *?
Institute for Advanced Computer Studies*
Department of Computer Science?
College of Information Studies?
University of Maryland
College Park, MD 20742 USA
{rgreen, bonnie, resnik}@umiacs.umd.edu
Abstract
This paper presents SemFrame, a system
that induces frame semantic verb classes
from WordNet and LDOCE.  Semantic
frames are thought to have significant
potential in resolving the paraphrase
problem challenging many language-
based applications.
When compared to the handcrafted
FrameNet, SemFrame achieves its best
recall-precision balance with 83.2%
recall (based on SemFrame's coverage of
FrameNet frames) and 73.8% precision
(based on SemFrame verbs? semantic
relatedness to frame-evoking verbs).  The
next best performing semantic verb
classes achieve 56.9% recall and 55.0%
precision. 
1 Introduction
Semantic content can almost always be expressed
in a variety of ways.  Lexical synonymy (She
esteemed him highly vs. She respected him
greatly), syntactic variation (John paid the bill vs.
The bill was paid by John), overlapping meanings
(Anna turned at Elm vs. Anna rounded the corner
at Elm), and other phenomena interact to produce
a broad range of choices for most language
generation tasks (Hirst, 2003; Rinaldi et al, 2003;
Kozlowski et al, 2003). At the same time, natural
language understanding must recognize what
remains constant across paraphrases.
The paraphrase phenomenon affects many
computational linguistic applications, including
information retrieval, information extraction,
question-answering, and machine translation.  For
example, documents that express the same
content using different linguistic means should
typically be retrieved for the same queries.
Information sought to answer a question needs to
be recognized no matter how it is expressed.
Semantic frames (Fillmore, 1982; Fillmore
and Atkins, 1992) address the paraphrase problem
through their slot-and-filler templates, representing
frequently occurring, structured experiences.
Semantic frame types of an intermediate granularity
have the potential to fulfill an interlingua role within
a solution to the paraphrase problem.  
Until now, semantic frames have been
generated by hand (as in Fillmore and Atkins,
1992), based on native speaker intuition; the
FrameNet project (http://www.icsi.berkeley.edu/
~framenet; Johnson et al, 2002) now couples this
generation with empirical validation.  Only
recently has this project begun to achieve relative
breadth in its inventory of semantic frames. To
have a comprehensive inventory of semantic
frames, however, we need the capacity to generate
semantic frames semi-automatically (the need for
manual post-editing is assumed).
To address these challenges, we have
developed SemFrame, a system that induces
semantic frames automatically.  Overall, the
system performs two primary functions:  (1)
identification of sets of verb senses that evoke a
common semantic frame (in the sense that lexical
units call forth corresponding conceptual
structures); and (2) identification of the
conceptual structure of semantic frames.  This
paper explores the first task of identifying frame
semantic verb classes.  These classes have several
types of uses.  First, they are the basis for
identifying the internal structure of the frame
proper, as set forth in Green and Dorr, 2004.
Second, they may be used to extend FrameNet.
Third, they support applications needing access to
sets of semantically related words, for example,
text segmentation and word sense disambiguation,
as explored to a limited degree in Green, 2004.
Section 2 presents related research efforts on
developing semantic verb classes.  Section 3
summarizes the features of WordNet
(http://www.cogsci.princeton.edu/~wn) and
LDOCE (Procter, 1978) that support the
automatic induction of semantic verb classes, definitions and example sentences often mention
while Section 4 sets forth the approach taken by their participants using semantic-type-like nouns,
SemFrame to accomplish this task.  Section 5 thus mapping easily to the corresponding frame
presents a brief synopsis of SemFrame?s results, element.  Corpus data, however, are more likely
while Section 6 presents an evaluation of to include instantiated participants, which may
SemFrame?s ability to identify semantic verb not generalize to the frame element.  Second,
classes of a FrameNet-like nature.  Section 7 lexical resources provide a consistent amount of
summarizes our work and motivates directions for data for word senses, while the amount of data in
further development of SemFrame. a corpus for word senses is likely to vary widely.
2 Previous Work
The EAGLES (1998) report on semantic
encoding differentiates between two approaches
to the development of semantic verb classes:
those based on syntactic behavior and those based
on semantic criteria. 
Levin (1993) groups verbs based on an
analysis of their syntactic properties, especially
their ability to be expressed in diathesis
alternations; her approach reflects the assumption
that the syntactic behavior of a verb is determined
in large part by its meaning.  Verb classes at the
bottom of Levin?s shallow network group
together (quasi-) synonyms, hierarchically related
verbs, and antonyms, alongside verbs with looser
semantic relationships.
The verb categories based on Pantel and Lin
(2002) and Lin and Pantel (2001) are induced
automatically from a large corpus, using an
unsupervised clustering algorithm, based on
syntactic dependency features.  The resulting
clusters contain synonyms, hierarchically related
verbs, and antonyms, as well as verbs more
loosely related from the perspective of
paraphrase. 
The handcrafted WordNet (Fellbaum, 1998a)
uses the hyperonymy/hyponymy  relationship to
structure the English verb lexicon into a semantic
network. Each collection of a top-level node
supplemented by its descendants may be seen as
a semantic verb class. 
In all fairness, resolution of the paraphrase
problem is not the explicit goal of most efforts to
build semantic verb classes.  However, they can
process some paraphrases through lexical
synonymy, hierarchically related terms, and
antonymy.
3 Resources Used in SemFrame
We adopt an approach that relies heavily on
pre-existing lexical resources.  Such resources
have several advantages over corpus data in
identifying semantic frames. First, both
Third, lexical resources provide their data in a
more systematic fashion than do corpora. 
Most centrally, the syntactic arguments of the
verbs used in a definition often correspond to the
semantic arguments of the verb being defined.
For example, Table 1 gives the definitions of
several verb senses in LDOCE that evoke the
COMMERCIAL TRANSACTION frame, which
includes as its semantic arguments a Buyer, a
Seller, some Merchandise, and Money.  Words
corresponding to the Money (money, value), the
Merchandise (property, goods), and the Buyer
(buyer, buyers) are present in, and to some extent
shared across, the definitions; however, no words
corresponding to the Seller are present.
Verb LDOCE Definition
sense
buy 1 to obtain (something) by giving money
(or something else of value)
buy 2 to obtain in exchange for something,
often something of great value
buy 3 to be exchangeable for
purchase 1 to gain (something) at the cost of
effort, suffering, or loss of something
of value
sell 1 to give up (property or goods) to
another for money or other value
sell 2 to offer (goods) for sale
sell 3 to be bought; get a buyer or buyers;
gain a sale
Table 1.  LDOCE Definitions for Verbs 
Evoking the COMMERCIAL TRANSACTION Frame
Of available machine-readable dictionaries,
LDOCE appears especially useful for this
research.  It uses a restricted vocabulary of about
2000 words in its definitions and example
sentences, thus increasing the likelihood that
words with closely related meanings will use
Merge pairs, filtering out those
not meeting threshold criteria
Map WordNet synsets
to LDOCE senses
Extract verb sense
pairs from WordNet
Extract verb sense
pairs from LDOCE
Build fully-connected verb groups
Cluster related verb groups
Verb sense framesets
the same words in their definitions and support WordNet verb synsets and LDOCE verb senses
the pattern of discovery envisioned.  LDOCE?s relies on finding matches between the data
subject field codes also accomplish some of the available for the verb senses in each resource
same type of grouping as semantic frames. (e.g., other words in the synset; words in
WordNet is a machine-readable lexico- definitions and example sentences; words closely
semantic database whose primary organizational related to these words; and stems of these words).
structure is the synset?a set of synonymous word The similarity measure used is the average of the
senses.  A limited number of relationship types proportion of words on each side of the
(e.g., antonymy, hyponymy, meronymy, comparison that are matched in the other. This
troponymy, entailment) also relate synsets within mapping is used both to relate LDOCE verb senses,
a part of speech.  (Version 1.7.1 was used.) that map to the same WordNet synset (fig. 3f) and to
Fellbaum (1998b) suggests that relationships translate previously paired WordNet verb synsets
in WordNet ?reflect some of the structure of into LDOCE verb sense pairs.
frame semantics? (p. 5).  Through the relational In the third stage, the resulting verb sense
structure of WordNet, buy, purchase, sell, and pay pairs are merged into a single data set, retaining
are related together:  buy and purchase comprise one only those pairs whose cumulative support
synset; they entail paying and are opposed to sell. exceeds thresholds for either the number of
The relationship of buy, purchase, sell, and supporting data sources or strength of support,
pay to other COMMERCIAL TRANSACTION thus achieving higher precision in the merged
verbs?for example, cost, price, and the demand data set than in the input data sets.  Then, the
payment sense of charge?is not made explicit in graph formed by the verb sense pairs in the
WordNet, however.  Further, as Roger Chaffin merged data set is analyzed to find the fully
has noted, the specialized vocabulary of, for connected components.
example, tennis (e.g. racket, court, lob) is not co- Finally, these groups of verb senses become
located, but is dispersed across different branches input to a clustering operation (Voorhees, 1986).
of the noun network (Miller, 1998, p. 34). Those groups whose similarity (due to overlap in
4 SemFrame Approach
SemFrame gathers evidence about frame
semantic relatedness between verb senses by
analyzing LDOCE and WordNet data from a
variety of perspectives.  The overall approach
used is shown in Figure 1.  The first stage of
processing extracts pairs of LDOCE and
WordNet verb senses that potentially evoke the
same frame.  By exploiting many different clues
to semantic relatedness, we overgenerate these
pairs, favoring recall; subsequent stages improve
the precision of the resulting data.
Figures 2 and 3 give details of the algorithms
for extracting verb pairs based on different types
of evidence.  These include:  clustering LDOCE
verb senses/WordNet synsets on the basis of
words in their definitions and example sentences
(fig. 2); relating LDOCE verb senses defined in
terms of the same verb (fig. 3a); relating LDOCE
verb senses that share a common stem (fig. 3b);
extracting explicit sense-linking relationships in
LDOCE (fig. 3c); relating verb senses that share
general or specific subject field codes in LDOCE
(fig. 3d); and extracting (direct or extended)
semantic relationships in WordNet (fig. 3e).
In the second stage, mapping between
membership) exceed a threshold are merged
together, thus reducing the number of verb sense
groups. The verb senses within each resulting
group are hypothesized to evoke the same
semantic frame and constitute a frameset.  
 
Figure 1. Approach for Building 
Frame Semantic Verb Classes
wgt
wordf
 1frequencyf
wgt
wordf
 .01
Input. SW, a set of stop words; M, a set of
(word, stem) pairs; F, a set of (word,
frequency) pairs; DE, a set of
(verb_sense_id, def+ex) pairs, where
def+ex  = the set of words in thed
definitions and example sentences of
verb_sense_idd
Step 1. forall d  DE, append to def+ex : d
verb_sense_id  and remove fromd
def+ex  any word w  SWd
Step 2. forall d  DE
forall m  M
if word  exists in def+ex , m   d
substitute stem  for wordm  m
Step 3. forall f  F
if frequency   > 1,f
, 
else if frequency   == 1,f
Step 4. O Voorhees? average link clustering
algorithm applied to DE, with initial
weights forall t in def+ex set to wgtt
Step 5. forall o  O
return all combinations of two
members from o
Figure 2. Algorithm for Generating 
Clustering-based Verb Pairs
5 Results
We explored a range of thresholds in the final
stage of the algorithm.   In general, the lower the1
threshold, the looser the verb grouping. The
number of verb senses retained (out of 12,663
non-phrasal verb senses in LDOCE) and the verb
sense groups produced by using these thresholds
are recorded in Table 2. 
6 Evaluation
One of our goals is to produce sets of verb senses
capable of extending FrameNet's coverage while
requiring reasonably little post-editing.   This goal
has two subgoals: identifying new frames and
identifying additional lexical units that evoke
Threshold Num verb senses Num groups
0.5 6461 1338
1.0 6414 1759
1.5 5607 1421
2.0 5604 1563
Table 2.  Results of Frame Clustering Process
previously recognized frames.  We use the hand-
crafted FrameNet, which is of reliably high
precision, as a gold standard  for the initial2
evaluation of SemFrame's ability to achieve these
subgoals.  For the first, we evaluate SemFrame?s
ability to generate frames that correspond to
FrameNet?s frames, reasoning that the system
must be able to identify a large proportion of
known frames if the quality of its output is good
enough to identify new frames.  (At this stage we
do not measure the quality  of new frames.)   For
the second subgoal we can be more concrete:  For
frames identified by both systems, we measure the
degree to which the verbs identified by
SemFrame can be shown to evoke those frames,
even if FrameNet has not identified them as
frame-evoking verbs.
FrameNet includes hierarchically organized
frames of varying levels of generality: Some
semantic areas are covered by a general frame,
some by a combination of specific frames, and
some by a mix of general and specific frames.
Because of this variation we determined the
degree to which SemFrame and FrameNet overlap
by automatically finding and comparing
corresponding frames instead of fully equivalent
frames.  Frames correspond if the semantic scope
of one frame is included within the semantic
For the clustering algorithm used, the clustering FrameNet's frames are more syntactically than1
threshold range is open-ended.  The values semantically motivated (e.g., EXPERIENCER-OBJECT,
investigated in the evaluation are fairly low. EXPERIENCER-SUBJECT).  
Certain constraints imposed by FrameNet's2
development strategy restrict its use as a full-fledged
gold standard for evaluating semantic frame induction.
(1) As of summer 2003, only 382 frames had been
identified within the FrameNet project.  (2) Low recall
affects not only the set of semantic frames identified
by FrameNet, but also the sets of frame-evoking units
listed for each frame. No verbs are listed for 38.5% of
FrameNet's frames, while another 13.1% of them list
only 1 or 2 verbs. The comparison here is limited to
the 197 FrameNet frames for which at least one verb
is listed with a counterpart in LDOCE.  (3) Some of
a. Relates LDOCE verb senses that are defined in terms of the same verb
Input. D, a set of (verb_sense_id, def_verb) pairs, where def_verb  = the verb in terms of whichd
verb_sense_id  is definedd
Step 1. forall v that exist as def_verb in D, form DV   D, by extracting all (verb_sense_id, def_verb)v
pairs where v = def_verb
Step 2. remove all DV  for which | DV  | > 40v    v
Step 3. forall v that exist as def_verb in D, return all combinations of two members from DVv
b. Relates LDOCE verb senses that share a common stem
Input. D, a set of (verb_sense_id, verb_stem) pairs, where verb_stem  = the stem for the verb on whichd
verb_sense_id  is basedd
Step 1. forall m that exist as verb_stem in D, form DV   D, by extracting all (verb_sense_id,m
verb_stem) pairs where m = verb_stem
Step 2. forall m that exist as verb_stem in D, return all combinations of two members from DVv
c. Extracts explicit sense-linking relationships in LDOCE
Input. D, a set of (verb_sense_id, def) pairs, where def  = the definition for verb_sense_idd     d
Step 1. forall d  D, if def  contains compare or opposite note, extract related_verb from note; generated
(verb_sense_id , related_verb ) pair d  d
Step 2. forall d  D, if def  defines verb_sense_id  in terms of a related standalone verb (in BLOCKd  d
CAPS), extract related_verb from definition; generate (verb_sense_id , related_verb ) pair d  d
Step 3. forall (verb_sense_id , related_verb ) pairs, if there is only one sense of related_verb , choose itd  d          d
and return (verb_sense_id , related_verb_sense_id ), else apply generalized mappingd  d
algorithm to return (verb_sense_id , related_verb_sense_id ) pairs where overlap occurs ind  d
the glosses of verb_sense_id  and related_verb_sense_idd  d
d. Relates verb senses that share general or specific subject field codes in LDOCE
Input. D, a set of (verb_sense_id, subject_code) pairs, where subject_code  = any 2- or 4-characterd
subject field code assigned to verb_sense_id 
Step 1. forall c that exist as subject_code in D, form DV   D, by extracting all (verb_sense_id,c
subject_code) pairs where c = subject_code
Step 2. forall c that exist as subject_code in D, 
return all combinations of two members from DVv
e. Extracts (direct or extended) semantic relationships in WordNet
Input. WordNet data file for verb synsets
Step 1. forall synset lines in input file
return (synset, related_synset) pairs for all synsets directly related through hyponymy,
antonymy, entailment, or cause_to relationships in WordNet
(for extended relationship pairs, also return (synset, related_synset) pairs for all synsets within
hyponymy tree, i.e., no matter how many levels removed)
f. Relates LDOCE verb senses that map to the same WordNet synset
Input. mapping of LDOCE verb senses to WordNet synsets
Step 1. forall lines in input file
return all combinations of two LDOCE verb senses mapped to the same WordNet?synset
Figure 3. Algorithms for Generating Non-clustering-based Verb Pairs
scope of the other frame or if the semantic scopes SemFrame?s verb classes list specific LDOCE
of the two frames have significant overlap.  Since verb senses.  In extending FrameNet, verbs from
FrameNet lists evoking words, without SemFrame would be word-sense-disambiguated
specification of word sense, the comparison was in the same way that FrameNet verbs currently
done on the word level rather than on the word are, through the correspondence of lexeme and
sense level, as if LDOCE verb senses were not frame.
specified in SemFrame.  However, it is clearly Incompleteness in the listing of evoking verbs
specific word senses that evoke frames, and in FrameNet and SemFrame precludes a straight-
forward detection of correspondences between incrust, and ornament.  Two of the verbs?adorn
their frames. Instead, correspondence between and decorate?are shared.  In addition, the frame
FrameNet and SemFrame frames is established names are semantically related through a
using either of two somewhat indirect approaches. WordNet synset consisting of decorate, adorn
In the first approach, a SemFrame frame is (which CatVar relates to ADORNING), grace,
deemed to correspond to a FrameNet frame if the ornament (which CatVar relates to
two frames meet both a minimal-overlap ORNAMENTATION), embellish, and beautify.  The
criterion (i.e., there is some, perhaps small, two frames are therefore designated as
overlap between the FrameNet and SemFrame corresponding frames by meeting both the
framesets) and a frame-name-relatedness minimal-overlap and the frame-name relatedness
criterion.  The minimal-overlap criterion is met if criteria.
either of two conditions is met:  (1) If the In the second approach, a SemFrame frame is
FrameNet frame lists four or fewer verbs (true of deemed to correspond to a FrameNet frame if the
over one-third of the FrameNet frames that list two frames  meet either of two relatively stringent
associated verbs), minimal overlap occurs when verb overlap criteria, the majority-match criterion
any one verb associated with the FrameNet frame or the majority-related criterion, in which case
matches a verb associated with a SemFrame examination of frame names is unnecessary.  
frame.  (2) If the FrameNet frame lists five or The majority-match criterion is met if the set
more verbs, minimal overlap occurs when two or of verbs shared by FrameNet and SemFrame
more verbs in the FrameNet frame are matched by framesets account for half or more of the verbs in
verbs in the SemFrame frame. either frameset.  For example, the APPLY_HEAT
The looseness of the minimal overlap frame in FrameNet includes 22 verbs:  bake,
criterion is tightened by also requiring that the blanch, boil, braise, broil, brown, char, coddle,
names of the FrameNet and SemFrame frames be cook, fry, grill, microwave, parboil, poach, roast,
closely related.  Establishing this frame-name saute, scald, simmer, steam, steep, stew, and
relatedness involves identifying individual toast, while the BOILING frame in SemFrame
components of each frame name  and augmenting includes 7 verbs:  boil, coddle, jug, parboil,3
this set with morphological variants from CatVar poach, seethe, and simmer.  Five of these
(Habash and Dorr 2003).  The resulting set for verbs?boil, coddle, parboil, poach, and
each FrameNet and SemFrame frame name is simmer?are shared across the two frames and
then searched in both the noun and verb WordNet constitute over half of the SemFrame frameset.
networks to find all the synsets that might Therefore the two frames are deemed to
correspond to the frame name.  To these sets are correspond by meeting the majority-match
also added all synsets directly related to the criterion.
synsets corresponding to the frame names.  If the The majority-related criterion is met if half or
resulting set of synsets gathered for a FrameNet more of the verbs from the SemFrame frame are
frame name intersects with the set of synsets semantically related to verbs from the FrameNet
gathered for a SemFrame frame name, the two frame (that is, if the precision of the SemFrame
frame names are deemed to be semantically verb set is at least 0.5).  To evaluate this criterion,
related.  each FrameNet and SemFrame verb is associated
For example, the FrameNet ADORNING frame with the WordNet verb synsets it occurs in,
contains 17 verbs: adorn, blanket, cloak, coat, augmented by the synsets to which the initial sets
cover, deck, decorate, dot, encircle, envelop, of synsets are directly related.  If the sets of
festoon, fill, film, line, pave, stud, and wreathe. synsets corresponding to two verbs share one or
The SemFrame ORNAMENTATION frame contains more synsets, the two verbs are deemed to be
12 verbs:  adorn, caparison, decorate, embellish, semantically related.  This process is extended
embroider, garland, garnish, gild, grace, hang, one further level, such that a SemFrame verb
found by this process to be semantically related to
a SemFrame verb, whose semantic relationship to
a FrameNet verb has already been established,
will also be designated a frame-evoking verb.  If
half or more of the verbs listed for a SemFrame
frame are established as evoking the same frame
as the list of WordNet verbs, then the FrameNet
All SemFrame frame names are nouns.  (See3
Green and Dorr, 2004 for an explanation of their
selection.)  FrameNet frame names (e.g., ABUNDANCE,
A C T I V I T Y _ S T A R T ,  C A U S E _ T O _ B E _ W E T ,
INCHOATIVE_ATTACHING), however, exhibit
considerable variation.
and SemFrame frames are hypothesized to bound on the task, i.e., 100% recall and 100%
correspond through the majority-related criterion. precision.  The Lin & Pantel results are here a
For example, the FrameNet ABUNDANCE lower bound for automatically induced semantic
frame includes 4 verbs: crawl, swarm, teem, and verb classes and probably reflect the limitations of
throng.  The SemFrame FLOW frame likewise using only corpus data.  Among efforts to develop
includes 4 verbs:  pour, teem, stream, and semantic verb classes, SemFrame?s results
pullulate.  Only one verb?teem?is shared, so correspond more closely to semantic frames than
the majority-match criterion is not met, nor is the do others.
related-frame-name criterion met, as the frame
names are not semantically related.  The majority-
related criterion, however, is met through a
WordNet verb synset that includes pour, swarm,
stream, teem, and pullulate.
Of the 197 FrameNet frames that include at
least one LDOCE verb, 175 were found to have a
corresponding SemFrame frame. But this 88.8%
recall level should be balanced against the
precision ratio of SemFrame verb framesets.
After all, we could get 100% recall by listing all
verbs in every SemFrame frame.  
The majority-related function computes the
precision ratio of the SemFrame frame for each
pair of FrameNet and SemFrame frames being
compared.  By modifying the minimum precision
threshold, the balance between recall and
precision, as measured using F-score, can be
investigated. The best balance for the SemFrame
version is based on a clustering threshold of 2.0
and a minimum precision threshold of 0.4, which
yields a recall of 83.2% and overall precision of
73.8%. 
To interpret these results meaningfully, one
would like to know if SemFrame achieves more
FrameNet-like results than do other available verb
category data, more specifically the 258 verb
classes from Levin, the 357 semantic verb classes
of WordNet 1.7.1, or the 272 verb clusters of Lin
and Pantel, as described in Section 2.
For purposes of comparison with FrameNet,
Levin?s verb class names have been hand-edited
to isolate the word that best captures the semantic
sense of the class; the name of a WordNet-based
frame is taken from the words for the root-level
synset; and the name of each Lin and Pantel
cluster is taken to be the first verb in the cluster.4
Evaluation results for the best balance
between recall and precision (i.e., the maximum
F-score) of the four comparisons are summarized
in Table 3.  FrameNet itself constitutes the upper
Semantic verb Precision Recall Precision
classes threshold
at max F-
score
SemFrame 0.40 0.832 0.738
Levin 0.20 0.569 0.550
WordNet 0.15 0.528 0.466
Lin & Pantel 0.15 0.472 0.407
Table 3. Best Recall-Precision Balance 
When Compared with FrameNet
7 Conclusions and Future Work
We have demonstrated that sets of verbs evoking
a common semantic frame can be induced from
existing lexical tools.  In a head-to-head
comparison with frames in FrameNet, the frame
semantic verb classes developed by the
SemFrame approach achieve a recall of 83.2%
and the verbs listed for frames achieve a precision
of 73.8%; these results far outpace those of other
semantic verb classes.  On a practical level, a
large number of frame semantic verb classes have
been identified. Associated with clustering
threshold 1.5 are 1421 verb classes, averaging
14.1 WordNet verb synsets. Associated with
clustering threshold 2.0 are 1563 verb classes,
averaging 6.6  WordNet verb synsets.
Despite these promising results, we are
limited by the scope of our input data set. While
LDOCE and WordNet data are generally of high
quality, the relative sparseness of these resources
has an adverse impact on recall.  In addition, the
mapping technique used for picking out
corresponding word senses in WordNet and
LDOCE is shallow, thus constraining  the recall
and precision of SemFrame outputs.  Finally, the
multi-step process of merging smaller verb groups
into verb groups that are intended to correspond
to frames sometimes fails to achieve an
appropriate degree of correspondence (all the verb
classes discovered are not distinct). 
Lin and Pantel have taken a similar approach,4
?naming? their verb clusters by the first three verbs
listed for a cluster, i.e., the three most similar verbs.
In our future work, we will experiment with
the more recent release of WordNet (2.0).  This
version provides derivational morphology links
between nouns and verbs, which will promote far
greater precision in the linking of verb senses
based on morphology than was possible in our
initial implementation. Another significant
addition to WordNet 2.0 is the inclusion of
category domains, which co-locate words
pertaining to a subject and perform the same
function as LDOCE's subject field codes.  
Finally, data sparseness issues may be
addressed by supplementing the use of the lexical
resources used here with access to, for example,
the British National Corpus, with its broad
coverage and carefully-checked parse trees.
Acknowledgments
This research has been supported in part by a
National Science Foundation Graduate Research
Fellowship NSF ITR grant #IIS-0326553, and
NSF CISE Research Infrastructure Award
EIA0130422.
References
Boguraev, Bran and Ted Briscoe. 1989. Introduction. In
B. Boguraev and T. Briscoe (Eds.), Computational
Lexicography for Natural Language Processing, 1-
40. London: Longman.
EAGLES Lexicon Interest Group. 1998. EAGLES
Preliminary Recommendations on Semantic
Encoding: Interim Report,  <http://
www.ilc.cnr.it/EAGLES96/rep2/ rep2.html>.
Fellbaum, Christiane (Ed.). 1998a. WordNet: An
Electronic Lexical Database. Cambridge, MA:
The MIT Press.
Fellbaum, Christiane. 1998b. Introduction. In C.
Fellbaum, 1998a, 1-17. 
Fillmore, Charles J. 1982. Frame semantics. In
Linguistics in the Morning Calm, 111-137. Seoul:
Hanshin. 
Fillmore, Charles J. and B. T. S. Atkins. 1992.
Towards a frame-based lexicon: The semantics of
RISK and its neighbors. In A. Lehrer and E. F.
Kittay (Eds.), Frames, Fields, and Contrasts, 75-
102. Hillsdale, NJ: Erlbaum.
Green, Rebecca. 2004. Inducing Semantic Frames
from Lexical Resources. Ph.D. dissertation,
University of Maryland.
Green, Rebecca and Bonnie J. Dorr. 2004. Inducing A
Semantic Frame Lexicon from WordNet Data. In
Proceedings of the 2nd Workshop on Text
Meaning and Interpretation (ACL 2004).
Habash, Nizar and Bonnie Dorr.  2003. A categorial
variation database for English.  In Proceedings of
North American Association for Computational
Linguistics, 96-102.
Hirst, Graeme.  2003.  Paraphrasing paraphrased.
Keynote address for The Second International
Workshop on Paraphrasing: Paraphrase
Acquisition and Applications, ACL 2003,
<http://nlp.nagaokaut.ac.jp/IWP2003/pdf/
Hirst-slides.pdf>.
Johnson, Christopher R., Charles J. Fillmore,
Miriam R. L. Petruck, Collin F. Baker,
Michael Ellsworth, Josef Ruppenhofer, and
Esther J. Wood. 2002. FrameNet: Theory and
P r a c t i c e ,  v e r s i o n  1 . 0 ,
< h t t p : / / w w w . i c s i . b e r k e l e y . e d u /
~framenet/book/book.html>.
Kozlowski, Raymond, Kathleen F. McCoy, and K.
Vijay-Shanker. 2003. Generation of
single-sentence paraphrases from
predicate/argument structure using
lexico-grammatical resources. In The Second
International Workshop on Paraphrasing:
Paraphrase Acquisition and Applications
(IWP2003), ACL 2003, 1-8.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
Chicago: University of Chicago Press.
Lin, Dekang and Patrick Pantel. 2001. Induction of
semantic classes from natural language text. In
Proceedings of ACM SIGKDD Conference on
Knowledge Discovery and Data Mining, 317-322.
Litkowski, Ken. 2004. Senseval-3 task: Word-sense
disambiguation of WordNet glosses,
<http://www.clres.com/SensWNDisamb.html>.
Miller, George A. 1998. Nouns in WordNet. In C.
Fellbaum, 1998a, 23-67. 
Pantel, Patrick and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the
Eighth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, 613-
619.
Procter, Paul (Ed.). 1978. Longman Dictionary of
Contemporary English. Longman Group Ltd.,
Essex, UK.
Rinaldi, Fabio, James Dowdall, Kaarel Kaljurand,
Michael Hess, and Diego Moll?. 2003. Exploiting
paraphrases in a question answering system. In
The Second International Workshop on
Paraphrasing: Paraphrase Acquisition and
Applications (IWP2003), ACL 2003, 25-32.
Voorhees, Ellen. 1986. Implementing agglomerative
hierarchic clustering algorithms for use in
document retrieval. Information Processing &
Management 22/6: 465-476.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 33?36, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Linguist?s Search Engine: An Overview 
 
 
 
Philip Resnik Aaron Elkiss 
Department of Linguistics and UMIACS UMIACS 
University of Maryland University of Maryland 
College Park, MD 20742 College Park, MD 20742 
resnik@umd.edu aelkiss@umiacs.umd.edu 
 
 
 
 
Abstract 
The Linguist?s Search Engine (LSE) was 
designed to provide an intuitive, easy-to-
use interface that enables language re-
searchers to seek linguistically interesting 
examples on the Web, based on syntactic 
and lexical criteria.  We briefly describe 
its user interface and architecture, as well 
as recent developments that include LSE 
search capabilities for Chinese. 
1 Introduction 
The idea for the Linguist?s Search Engine origi-
nated in a simple frustration shared by many peo-
ple who study language: the fact that so much of 
the argumentation in linguistic theory is based on 
subjective judgments.  Who among us has not, in 
some talk or class, heard an argument based on a 
?starred? (deemed-ungrammatical) example, and 
whispered to someone nearby, Did that sound ok to 
you? because we thought it sounded fine? As Bard 
et al (1996) put it, each linguistic judgment is a 
?small and imperfect experiment'?.  Sch?tze (1996) 
and Cowart (1997) provide detailed discussion of 
instability and unreliability in such informal 
methods, which can lead to biased or even 
misleading results. 
Recent work on linguistics methodology draws 
on the perception literature in psychology to 
provide principled methods for eliciting gradient, 
rather than discrete, linguistic judgments (Sorace 
and Keller, 2005).  In addition, at least as far back 
as Rich Pito?s 1992 tgrep, distributed with the 
Penn Treebank, computationally sophisticated 
linguists have had the option of looking at 
naturally occurring data rather than relying on 
constructed sentences and introspective judgments 
(e.g., Christ, 1994; Corley et al, 2001; Blaheta, 
2002; Kehoe and Renouf 2002; K?nig and Lezius, 
2002; Fletcher 2002; Kilgarriff 2003).  
Unfortunately, many linguists are unwilling to 
invest in psycholinguistic methods, or in the 
computational skills necessary for working with 
corpus search tools.  A variety of people interested 
in language have moved in the direction of using 
Web search engines such as Google as a source of 
naturally occurring data, but conventional search 
engines do not provide the mechanisms needed to 
perform many of the simplest linguistically 
informed searches ? e.g., seeking instances of a 
particular verb used only intransitively. 
The Linguist?s Search Engine (LSE) was 
designed to provide the broadest possible range of 
users with an intuitive, linguistically sophisticated 
but user-friendly way to search the Web for 
naturally occurring data.   Section 2 lays out the 
LSE?s  basic interface concepts via several 
illustrative examples.  Section 3 discusses its 
architecture and implementation.   Section 4 
discusses the current status of the LSE and recent 
developments. 
2 LSE Interface Concepts 
The design of the LSE was guided by a simple 
basic premise: a tool can?t be a success unless 
people use it.  This led to the following principles 
in its design:  
33
 
? Minimize learning/ramp-up time. 
? Have a linguist-friendly look and feel. 
? Permit rapid interaction. 
? Permit large-scale searches. 
? Allow searches using linguistic criteria. 
 
Some of these principles conflict with each other.  
For example, sophisticated searches are difficult to 
specify in a linguist-friendly way and without 
requiring some learning by the user, and rapid 
interaction is difficult to accomplish for Web-sized 
searches. 
2.1 Query By Example 
The LSE adopts a strategy one can call ?query by 
example,? in order to provide sophisticated search 
functionality without requiring the user to learn a 
complex query language.  For example, consider 
the so-called ?comparative correlative? 
construction (Culicover and Jackendoff, 1999).  
Typing the bigger the house the richer the buyer 
automatically produces the analysis in Figure 1, 
which can be edited with a few mouse clicks to get 
the generalized structure in Figure 2, converted 
with one button push into the LSE?s query lan-
guage, and then submitted in order to find other 
examples of this construction, such as The higher 
the rating, the lower the interest rate that must be 
paid to investors; The more you bingo, the more 
chances you have in the drawing; The more we 
plan and prepare, the easier the transition. 
 
 
Figure 1. Querying by example 
 
 
 
Figure 2. Generalized query 
 
Crucially, users need not learn a query language, 
although advanced users can edit or create queries 
directly if so desired.  Nor do users need to agree 
with (or even understand) the LSE's automatic 
parse, in order to find sentences with parses similar 
to the exemplar.  Indeed, as is the case in Figure 1, 
the parse need not even be entirely reasonable; 
what is important is that the structure produced 
when analyzing the query will be the same 
structure produced via analysis of the 
corresponding sentences in the corpus. 
Other search features include the ability to 
specify immediate versus non-immediate 
dominance; the ability to negate relationships  
(e.g. a VP that does not immediately dominate an 
NP);  the ability to specify that words should 
match on all morphological forms; the ability to 
match nodes based on WordNet relationships (e.g. 
all descendants of a particular word sense); the 
ability to save and reload queries;  the ability to 
download results in keyword-in-context (KWIC) 
format; and the ability to apply a simple keyword-
based filter to avoid offensive results during live 
demonstrations.  
Results are typically returned by the LSE within 
a few seconds, in a simple search-engine style 
format.   In addition, however, the user has rapid 
access to the immediate preceding and following 
contexts of returned sentences, their annotations, 
and the Web page where the example occurred. 
 
2.2 Built-In and Custom Collections 
Linguistically annotating and indexing the entire 
Web is beyond impractical, and therefore there is a 
clear tradeoff between rapid response time and the 
ability to search the Web as a whole.  In order to 
manage this tradeoff, the LSE provides, by default, 
a built-in collection of English sentences taken 
randomly from a Web-scale crawl at the Internet 
34
Archive.1  This static collection is often useful by 
itself. 
 In order to truly search the entire Web, the LSE 
permits users to define their own custom collec-
tions, piggybacking on commercial Web search 
engines.  Consider, as an example, a search 
involving the verb titrate, which is rare enough 
that it occurs only twice in a collection of millions 
of sentences.  Using the LSE?s ?Build Custom 
Collection? functionality, the user can specify that 
the LSE should: 
 
? Query Altavista to find pages containing any 
morphological form of titrate 
? Extract only sentences containing that verb 
? Annotate and index those sentences 
? Augment the collection by iterating this 
process with different specifications 
 
Doing the Altavista query and extracting, parsing, 
and indexing the sentences can take some time, but 
the LSE permits the user to begin searching his or 
her custom collection as soon as any sentences 
have been added into it.  Typically dozens to 
hundreds of sentences are available within a few 
minutes, and a typical custom collection, 
containing thousands or tens of thousands of 
sentences, is completed within a few hours. 
Collections can be named, saved, augmented, and 
deleted. 
Currently the LSE supports custom collections 
built using searches on Altavista and Microsoft?s 
MSN Search.  It is interesting to note that the 
search engines? capabilities can be used to create 
custom collections based on extralinguistic criteria; 
for example, specifying pages originating only in 
the .uk domain in order to increase the likelihood 
of finding British usages, or specifying additional 
query terms in order to bias the collection toward 
particular topics or domains. 
3 Architecture and Implementation 
The LSE?s design can be broken into the following 
high level components: 
 
                                                          
1 The built-in LSE Web collection contains 3 million sen-
tences at the time of this writing.  We estimate that it can be 
increased by an order of magnitude without seriously degrad-
ing  response time, and we expect to do so by the time of the 
demonstration. 
? User interface 
? Search engine interface 
? NLP annotation  
? Indexing  
? Search 
 
The design is centered on a relational database that 
maintains information about users, collections, 
documents, and sentences, and the implementation 
combines custom-written code with significant use 
of off-the-shelf packages.  The interface with 
commercial  search engines is accomplished 
straightforwardly by use of the WWW::Search perl 
module (currently using a custom-written variant 
for MSN Search).  
Natural language annotation is accomplished via 
a parallel, database-centric annotation architecture 
(Elkiss, 2003). A configuration specification 
identifies dependencies between annotation tasks 
(e.g. tokenization as a prerequisite to part-of-
speech tagging). After documents are processed to 
handle markup and identify sentence boundaries, 
individual sentences are loaded into a central 
database that holds annotations, as well as 
information about which sentences remain to be 
annotated.  Crucially, sentences can be annotated 
in parallel by task processes residing on distributed 
nodes. 
Indexing and search of annotations is informed 
by the recent literature on semistructured data.  
However, linguistic databases are unlike most 
typical semistructured data sets (e.g., sets of XML 
documents) in a number of respects ? these include 
the fact that the dataset has a very large schema 
(tens of millions of distinct paths from root node to 
terminal symbols), long path lengths, a need for 
efficient handling of queries containing wildcards, 
and a requirement that all valid results be retrieved.  
On the other hand, in this application incremental 
updating is not a requirement, and neither is 100% 
precision: results can be overgenerated and then 
filtered using a less efficient comparison tools such 
as tgrep2.  Currently the indexing scheme follows 
ViST (Wang et al, 2003), an approach based on 
suffix trees that indexes structure and content 
together.   The variant implemented in the LSE 
ignores insufficiently selective query branches, and 
achieves more efficient search by modifying the 
ordering within the structural index, creating an in-
memory tree for the query, ordering processing of 
35
query branches from most to least selective, and 
memoizing query subtree matches.  
4 Status and Recent Developments 
The LSE ?went live? on January 20, 2004 and 
approximately 1000 people have registered and 
tried at least one query.   In response to a recent 
survey, several dozen LSE users reported having 
tried it more than casually, and there are a dozen or 
so reports of the LSE having proven useful in real 
work, either for research or as a tool that was 
useful in teaching.  Resnik et al (2005) describe 
two pieces of mainstream linguistics research ? 
one in psycholinguistics and one in theoretical 
syntax ? in which the LSE played a pivotal role. 
The LSE software is currently being 
documented and packaged up, for an intended 
open-source release.2  In addition to continuing 
linguistic research with the LSE, we are also 
experimenting with alternative indexing/search 
schemes.  Finally, we are engaged in a project 
adapting the LSE for use in language pedagogy ? 
specifically, as a tool assisting language teaching 
specialists in creating training and testing materials 
for learners of Chinese.  For that purpose, we are 
experimenting with a built-in collection of Chinese 
Web documents that includes links to their English 
translations (Resnik and Smith, 2003). 
Acknowledgments 
This work has received support from the National Science 
Foundation under ITR grant IIS01130641, and from the Cen-
ter for the Advanced Study of Language under TTO32.  The 
authors are grateful to Christiane Fellbaum and Mari Broman 
Olsen for collaboration and discussions; to Rafi Khan, Sau-
rabh Khandelwal, Jesse Metcalf-Burton, G. Craig Murray, 
Usama Soltan, and James Wren for their contributions to LSE 
development; and to Doug Rohde, Eugene Charniak, Adwait 
Ratnaparkhi, Dekang Lin, UPenn?s XTAG group, Princeton?s 
WordNet project, and untold others for software components 
used in this work. 
References  
Bard, E.G., Robertson, D. and A. Sorace. Magnitude 
estimation of linguistic acceptability.  Language 
72.1: 32-68, 1996. 
                                                          
2 Documentation maintained at http://lse.umiacs.umd.edu/. 
Christ, Oli. A modular and flexible architecture for an 
integrated corpus query system, COMPLEX'94, Bu-
dapest, 1994. 
Corley, Steffan, Martin Corley, Frank Keller, Matthew 
W. Crocker, and Shari Trewin. Finding Syntactic 
Structure in Unparsed Corpora: The Gsearch Corpus 
Query System, Computers and the Humanities, 35:2, 
81-94, 2001. 
Cowart, Wayne.  Experimental Syntax: Applying Objec-
tive Methods to Sentence Judgments, Sage Publica-
tions, Thousand Oaks, CA, 1997. 
Culicover, Peter and Ray Jackendoff.  The view from 
the periphery: the English comparative correlative.  
Linguistic Inquiry 30:543-71, 1999. 
Elkiss, Aaron. A Scalable Architecture for Linguistic 
Annotation. Computer Science Undergraduate Hon-
ors Thesis.  University of Maryland.  May 2003. 
Fletcher, William.  Making the Web More Useful as a 
Source for Linguistic Corpora, North American 
Symposium on Corpus Linguistics, 2002. 
Kehoe, Andrew and Antoinette Renouf, WebCorp: Ap-
plying the Web to linguistics and linguistics to the 
Web, in Proceedings of WWW2002, Honolulu, Ha-
waii, 7-11 May 2002. 
Adam Kilgarriff, Roger Evans, Rob Koeling, David 
Tugwell.  WASPBENCH: a lexicographer's work-
bench incorporating state-of-the-art word sense dis-
ambiguation.  Proceedings of EACL 2003, 211-214, 
2003. 
Koenig, Esther and Lezius, Wolfgang, A description 
language for syntactically annotated corpora.  In: 
Proceedings of the COLING Conference, pp. 1056-
1060, Saarbruecken, Germany, 2002. 
Schuetze, Carson.  The Empirical Base of Linguistics, 
University of Chicago Press, 1996. 
Sorace, Antonella and Frank Keller. Gradience in Lin-
guistic Data. To appear in Lingua, 2005. 
Philip Resnik and Noah A. Smith, The Web as a Parallel 
Corpus, Computational Linguistics 29(3), pp. 349-
380, September 2003. 
 
Philip Resnik, Aaron Elkiss, Ellen Lau, and Heather 
Taylor.  The Web in Theoretical Linguistics Re-
search: Two Case Studies Using the Linguist's Search 
Engine. 31st Meeting of the Berkeley Linguistics So-
ciety, February 2005. 
 
H Wang, S Park, W Fan, and P Yu.  ViST: a dynamic 
index method for querying XML data by tree struc-
tures.  ACM SIGMOD 2003.  pp. 110-121. 
36
Proceedings of ACL-08: HLT, pages 1003?1011,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Soft Syntactic Constraints for Hierarchical Phrased-Based Translation
Yuval Marton and Philip Resnik
Department of Linguistics
and the Laboratory for Computational Linguistics and Information Processing (CLIP)
at the Institute for Advanced Computer Studies (UMIACS)
University of Maryland, College Park, MD 20742-7505, USA
{ymarton, resnik} @t umiacs.umd.edu
Abstract
In adding syntax to statistical MT, there is
a tradeoff between taking advantage of lin-
guistic analysis, versus allowing the model
to exploit linguistically unmotivated mappings
learned from parallel training data. A num-
ber of previous efforts have tackled this trade-
off by starting with a commitment to linguisti-
cally motivated analyses and then finding ap-
propriate ways to soften that commitment. We
present an approach that explores the trade-
off from the other direction, starting with a
context-free translation model learned directly
from aligned parallel text, and then adding soft
constituent-level constraints based on parses
of the source language. We obtain substantial
improvements in performance for translation
from Chinese and Arabic to English.
1 Introduction
The statistical revolution in machine translation, be-
ginning with (Brown et al, 1993) in the early 1990s,
replaced an earlier era of detailed language analy-
sis with automatic learning of shallow source-target
mappings from large parallel corpora. Over the last
several years, however, the pendulum has begun to
swing back in the other direction, with researchers
exploring a variety of statistical models that take ad-
vantage of source- and particularly target-language
syntactic analysis (e.g. (Cowan et al, 2006; Zoll-
mann and Venugopal, 2006; Marcu et al, 2006; Gal-
ley et al, 2006) and numerous others).
Chiang (2005) distinguishes statistical MT ap-
proaches that are ?syntactic? in a formal sense, go-
ing beyond the finite-state underpinnings of phrase-
based models, from approaches that are syntactic
in a linguistic sense, i.e. taking advantage of a
priori language knowledge in the form of annota-
tions derived from human linguistic analysis or tree-
banking.1 The two forms of syntactic modeling are
doubly dissociable: current research frameworks in-
clude systems that are finite state but informed by
linguistic annotation prior to training (e.g., (Koehn
and Hoang, 2007; Birch et al, 2007; Hassan et al,
2007)), and also include systems employing context-
free models trained on parallel text without benefit
of any prior linguistic analysis (e.g. (Chiang, 2005;
Chiang, 2007; Wu, 1997)). Over time, however,
there has been increasing movement in the direction
of systems that are syntactic in both the formal and
linguistic senses.
In any such system, there is a natural tension be-
tween taking advantage of the linguistic analysis,
versus allowing the model to use linguistically un-
motivated mappings learned from parallel training
data. The tradeoff often involves starting with a sys-
tem that exploits rich linguistic representations and
relaxing some part of it. For example, DeNeefe et
al. (2007) begin with a tree-to-string model, using
treebank-based target language analysis, and find it
useful to modify it in order to accommodate useful
?phrasal? chunks that are present in parallel train-
ing data but not licensed by linguistically motivated
parses of the target language. Similarly, Cowan et al
(2006) focus on using syntactically rich representa-
tions of source and target parse trees, but they re-
sort to phrase-based translation for modifiers within
1See (Lopez, to appear) for a comprehensive survey.
1003
clauses. Finding the right way to balance linguistic
analysis with unconstrained data-driven modeling is
clearly a key challenge.
In this paper we address this challenge from a less
explored direction. Rather than starting with a sys-
tem based on linguistically motivated parse trees, we
begin with a model that is syntactic only in the for-
mal sense. We then introduce soft constraints that
take source-language parses into account to a lim-
ited extent. Introducing syntactic constraints in this
restricted way allows us to take maximal advantage
of what can be learned from parallel training data,
while effectively factoring in key aspects of linguis-
tically motivated analysis. As a result, we obtain
substantial improvements in performance for both
Chinese-English and Arabic-English translation.
In Section 2, we briefly review the Hiero statis-
tical MT framework (Chiang, 2005, 2007), upon
which this work builds, and we discuss Chiang?s ini-
tial effort to incorporate soft source-language con-
stituency constraints for Chinese-English transla-
tion. In Section 3, we suggest that an insufficiently
fine-grained view of constituency constraints was re-
sponsible for Chiang?s lack of strong results, and
introduce finer grained constraints into the model.
Section 4 demonstrates the the value of these con-
straints via substantial improvements in Chinese-
English translation performance, and extends the ap-
proach to Arabic-English. Section 5 discusses the
results, and Section 6 considers related work. Fi-
nally we conclude in Section 7 with a summary and
potential directions for future work.
2 Hierarchical Phrase-based Translation
2.1 Hiero
Hiero (Chiang, 2005; Chiang, 2007) is a hi-
erarchical phrase-based statistical MT framework
that generalizes phrase-based models by permit-
ting phrases with gaps. Formally, Hiero?s trans-
lation model is a weighted synchronous context-
free grammar. Hiero employs a generalization of
the standard non-hierarchical phrase extraction ap-
proach in order to acquire the synchronous rules
of the grammar directly from word-aligned paral-
lel text. Rules have the form X ? ?e?, f??, where
e? and f? are phrases containing terminal symbols
(words) and possibly co-indexed instances of the
nonterminal symbol X.2 Associated with each rule
is a set of translation model features, ?i(f? , e?); forexample, one intuitively natural feature of a rule is
the phrase translation (log-)probability ?(f? , e?) =
log p(e?|f?) , directly analogous to the corresponding
feature in non-hierarchical phrase-based models like
Pharaoh (Koehn et al, 2003). In addition to this
phrase translation probability feature, Hiero?s fea-
ture set includes the inverse phrase translation prob-
ability log p(f? |e?), lexical weights lexwt(f? |e?) and
lexwt(e?|f?), which are estimates of translation qual-
ity based on word-level correspondences (Koehn et
al., 2003), and a rule penalty allowing the model to
learn a preference for longer or shorter derivations;
see (Chiang, 2007) for details.
These features are combined using a log-linear
model, with each synchronous rule contributing
?
i
?i?i(f? , e?) (1)
to the total log-probability of a derived hypothesis.
Each ?i is a weight associated with feature ?i, andthese weights are typically optimized using mini-
mum error rate training (Och, 2003).
2.2 Soft Syntactic Constraints
When looking at Hiero rules, which are acquired au-
tomatically by the model from parallel text, it is easy
to find many cases that seem to respect linguistically
motivated boundaries. For example,
X ? ?jingtian X1, X1 this year?,
seems to capture the use of jingtian/this year as
a temporal modifier when building linguistic con-
stituents such as noun phrases (the election this
year) or verb phrases (voted in the primary this
year). However, it is important to observe that noth-
ing in the Hiero framework actually requires nonter-
minal symbols to cover linguistically sensible con-
stituents, and in practice they frequently do not.3
2This is slightly simplified: Chiang?s original formulation
of Hiero, which we use, has two nonterminal symbols, X and
S. The latter is used only in two special ?glue? rules that permit
complete trees to be constructed via concatenation of subtrees
when there is no better way to combine them.
3For example, this rule could just as well be applied with X1
covering the ?phrase? submitted and to produce non-constituent
substring submitted and this year in a hypothesis like The bud-
get was submitted and this year cuts are likely.
1004
Chiang (2005) conjectured that there might be
value in allowing the Hiero model to favor hy-
potheses for which the synchronous derivation re-
spects linguistically motivated source-language con-
stituency boundaries, as identified using a parser.
He tested this conjecture by adding a soft constraint
in the form of a ?constituency feature?: if a syn-
chronous rule X ? ?e?, f?? is used in a derivation,
and the span of f? is a constituent in the source-
language parse, then a term ?c is added to the modelscore in expression (1).4 Unlike a hard constraint,
which would simply prevent the application of rules
violating syntactic boundaries, using the feature to
introduce a soft constraint allows the model to boost
the ?goodness? for a rule if it is constitent with the
source language constituency analysis, and to leave
its score unchanged otherwise. The weight ?c, likeall other ?i, is set via minimum error rate train-ing, and that optimization process determines em-
pirically the extent to which the constituency feature
should be trusted.
Figure 1 illustrates the way the constituency fea-
ture worked, treating English as the source language
for the sake of readability. In this example, ?c wouldbe added to the hypothesis score for any rule used in
the hypothesis whose source side spanned the minis-
ter, a speech, yesterday, gave a speech yesterday, or
the minister gave a speech yesterday. A rule trans-
lating, say, minister gave a as a unit would receive
no such boost.
Chiang tested the constituency feature for
Chinese-English translation, and obtained no signif-
icant improvement on the test set. The idea then
seems essentially to have been abandoned; it does
not appear in later discussions (Chiang, 2007).
3 Soft Syntactic Constraints, Revisited
On the face of it, there are any number of possi-
ble reasons Chiang?s (2005) soft constraint did not
work ? including, for example, practical issues like
the quality of the Chinese parses.5 However, we fo-
cus here on two conceptual issues underlying his use
of source language syntactic constituents.
4Formally, ?c(f? , e?) is defined as a binary feature, with
value 1 if f? spans a source constituent and 0 otherwise. In the
latter case ?c?c(f? , e?) = 0 and the score in expression (1) is
unaffected.
5In fact, this turns out not to be the issue; see Section 4.
Figure 1: Illustration of Chiang?s (2005) syntactic con-
stituency feature, which does not distinguish among con-
stituent types.
First, the constituency feature treats all syntac-
tic constituent types equally, making no distinction
among them. For any given language pair, however,
there might be some source constituents that tend to
map naturally to the target language as units, and
others that do not (Fox, 2002; Eisner, 2003). More-
over, a parser may tend to be more accurate for some
constituents than for others.
Second, the Chiang (2005) constituency feature
gives a rule additional credit when the rule?s source
side overlaps exactly with a source-side syntactic
constituent. Logically, however, it might make sense
not just to give a rule X ? ?e?, f?? extra credit when
f? matches a constituent, but to incur a cost when f?
violates a constituent boundary. Using the example
in Figure 1, we might want to penalize hypotheses
containing rules where f? is the minister gave a (and
other cases, such as minister gave, minister gave a,
and so forth).6
These observations suggest a finer-grained ap-
proach to the constituency feature idea, retaining the
idea of soft constraints, but applying them using var-
ious soft-constraint constituency features. Our first
observation argues for distinguishing among con-
stituent types (NP, VP, etc.). Our second observa-
tion argues for distinguishing the benefit of match-
6This accomplishes coverage of the logically complete set of
possibilities, which include not only f? matching a constituent
exactly or crossing its boundaries, but also f? being properly
contained within the constituent span, properly containing it,
or being outside it entirely. Whenever these latter possibilities
occur, f? will exactly match or cross the boundaries of some
other constituent.
1005
ing constituents from the cost of crossing constituent
boundaries. We therefore define a space of new fea-
tures as the cross product
{CP, IP, NP, VP, . . .} ? {=,+}.
where = and + signify matching and crossing bound-
aries, respectively. For example, ?NP= would de-note a binary feature that matches whenever the span
of f? exactly covers an NP in the source-side parse
tree, resulting in ?NP= being added to the hypoth-esis score (expression (1)). Similarly, ?VP+ woulddenote a binary feature that matches whenever the
span of f? crosses a VP boundary in the parse tree,
resulting in ?VP+ being subtracted from the hypoth-esis score.7 For readability from this point forward,
we will omit ? from the notation and refer to features
such as NP= (which one could read as ?NP match?),
VP+ (which one could read as ?VP crossing?), etc.
In addition to these individual features, we define
three more variants:
? For each constituent type, e.g. NP, we define
a feature NP_ that ties the weights of NP= and
NP+. If NP= matches a rule, the model score is
incremented by ?NP_, and if NP+ matches, themodel score is decremented by the same quan-
tity.
? For each constituent type, e.g. NP, we define a
version of the model, NP2, in which NP= and
NP+ are both included as features, with sepa-
rate weights ?NP= and ?NP+.
? We define a set of ?standard? linguistic labels
containing {CP, IP, NP, VP, PP, ADJP, ADVP,
QP, LCP, DNP} and excluding other labels such
as PRN (parentheses), FRAG (fragment), etc.8
We define feature XP= as the disjunction of
{CP=, IP=, . . ., DNP=}; i.e. its value equals 1
for a rule if the span of f? exactly covers a con-
stituent having any of the standard labels. The
7Formally, ?VP+ simply contributes to the sum in expres-sion (1), as with all features in the model, but weight optimiza-
tion using minimum error rate training should, and does, auto-
matically assign this feature a negative weight.
8We map SBAR and S labels in Arabic parses to CP and IP,
respectively, consistent with the Chinese parses. We map Chi-
nese DP labels to NP. DNP and LCP appear only in Chinese. We
ran no ADJP experiment in Chinese, because this label virtually
aways spans only one token in the Chinese parses.
definitions of XP+, XP_, and XP2 are analo-
gous.
? Similarly, since Chiang?s original constituency
feature can be viewed as a disjunctive ?all-
labels=? feature, we also defined ?all-labels+?,
?all-labels2?, and ?all-labels_? analogously.
4 Experiments
We carried out MT experiments for translation
from Chinese to English and from Arabic to En-
glish, using a descendant of Chiang?s Hiero sys-
tem. Language models were built using the SRI
Language Modeling Toolkit (Stolcke, 2002) with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998). Word-level alignments were obtained
using GIZA++ (Och and Ney, 2000). The base-
line model in both languages used the feature set
described in Section 2; for the Chinese baseline we
also included a rule-based number translation fea-
ture (Chiang, 2007).
In order to compute syntactic features, we an-
alyzed source sentences using state of the art,
tree-bank trained constituency parsers ((Huang et
al., 2008) for Chinese, and the Stanford parser
v.2007-08-19 for Arabic (Klein and Manning,
2003a; Klein and Manning, 2003b)). In addition
to the baseline condition, and baseline plus Chi-
ang?s (2005) original constituency feature, experi-
mental conditions augmented the baseline with ad-
ditional features as described in Section 3.
All models were optimized and tested using the
BLEU metric (Papineni et al, 2002) with the NIST-
implemented (?shortest?) effective reference length,
on lowercased, tokenized outputs/references. Sta-
tistical significance of difference from the baseline
BLEU score was measured by using paired boot-
strap re-sampling (Koehn, 2004).9
4.1 Chinese-English
For the Chinese-English translation experiments, we
trained the translation model on the corpora in Ta-
ble 1, totalling approximately 2.1 million sentence
pairs after GIZA++ filtering for length ratio. Chi-
nese text was segmented using the Stanford seg-
menter (Tseng et al, 2005).
9Whenever we use the word ?significant?, we mean ?statis-
tically significant? (at p < .05 unless specified otherwise).
1006
LDC ID Description
LDC2002E18 Xinhua Ch/Eng Par News V1 beta
LDC2003E07 Ch/En Treebank Par Corpus
LDC2005T10 Ch/En News Mag Par Txt (Sinorama)
LDC2003E14 FBIS Multilanguage Txts
LDC2005T06 Ch News Translation Txt Pt 1
LDC2004T08 HK Par Text (only HKNews)
Table 1: Training corpora for Chinese-English translation
We trained a 5-gram language model using the
English (target) side of the training set, pruning 4-
gram and 5-gram singletons. For minimum error
rate training and development we used the NIST
MTeval MT03 set.
Table 2 presents our results. We first evaluated
translation performance using the NIST MT06 (nist-
text) set. Like Chiang (2005), we find that the orig-
inal, undifferentiated constituency feature (Chiang-
05) introduces a negligible, statistically insignificant
improvement over the baseline. However, we find
that several of the finer-grained constraints (IP=,
VP=, VP+, QP+, and NP=) achieve statistically
significant improvements over baseline (up to .74
BLEU), and the latter three also improve signifi-
cantly on the undifferentiated constituency feature.
By combining multiple finer-grained syntactic fea-
tures, we obtain significant improvements of up to
1.65 BLEU points (NP_, VP2, IP2, all-labels_, and
XP+).
We also obtained further gains using combina-
tions of features that had performed well; e.g., con-
dition IP2.VP2.NP_ augments the baseline features
with IP2 and VP2 (i.e. IP=, IP+, VP= and VP+),
and NP_ (tying weights of NP= and NP+; see Sec-
tion 3). Since component features in those combi-
nations were informed by individual-feature perfor-
mance on the test set, we tested the best perform-
ing conditions from MT06 on a new test set, NIST
MT08. NP= and VP+ yielded significant improve-
ments of up to 1.53 BLEU. Combination conditions
replicated the pattern of results from MT06, includ-
ing the same increasing order of gains, with im-
provements up to 1.11 BLEU.
4.2 Arabic-English
For Arabic-English translation, we used the train-
ing corpora in Table 3, approximately 100,000 sen-
Chinese MT06 MT08
Baseline .2624 .2064
Chiang-05 .2634 .2065
PP= .2607
DNP+ .2621
CP+ .2622
AP+ .2633
AP= .2634
DNP= .2640
IP+ .2643
PP+ .2644
LCP= .2649
LCP+ .2654
CP= .2657
NP+ .2662
QP= .2674^+ .2071
IP= .2680*+ .2061
VP= .2683* .2072
VP+ .2693**++ .2109*+
QP+ .2694**++ .2091
NP= .2698**++ .2217**++
Multiple / conflated features:
QP2 .2614
NP2 .2621
XP= .2630
XP2 .2633
all-labels+ .2633
VP_ .2637
QP_ .2641
NP.VP.IP=.QP.VP+ .2646
IP_ .2647
IP2+VP2 .2649
all-labels2 .2673*- .2070
NP_ .2690**++ .2101^+
IP2.VP2.NP_ .2699**++ .2105*+
VP2 .2722**++ .2123**++
all-labels_ .2731**++ .2125*++
IP2 .2750**++ .2132**+
XP+ .2789**++ .2175**++
Table 2: Chinese-English results. *: Significantly better
than baseline (p < .05). **: Significantly better than
baseline (p < .01). ^: Almost significantly better than
baseline (p < .075). +: Significantly better than Chiang-
05 (p < .05). ++: Significantly better than Chiang-05
(p < .01). -: Almost significantly better than Chiang-05
(p < .075).
1007
LDC ID Description
LDC2004T17 Ar News Trans Txt Pt 1
LDC2004T18 Ar/En Par News Pt 1
LDC2005E46 Ar/En Treebank En Translation
LDC2004E72 eTIRR Ar/En News Txt
Table 3: Training corpora for Arabic-English translation
tence pairs after GIZA++ length-ratio filtering. We
trained a trigram language model using the English
side of this training set, plus the English Gigaword
v2 AFP and Gigaword v1 Xinhua corpora. Devel-
opment and minimum error rate training were done
using the NIST MT02 set.
Table 4 presents our results. We first tested on
on the NIST MT03 and MT06 (nist-text) sets. On
MT03, the original, undifferentiated constituency
feature did not improve over baseline. Two individ-
ual finer-grained features (PP+ and AdvP=) yielded
statistically significant gains up to .42 BLEU points,
and feature combinations AP2, XP2 and all-labels2
yielded significant gains up to 1.03 BLEU points.
XP2 and all-labels2 also improved significantly on
the undifferentiated constituency feature, by .72 and
1.11 BLEU points, respectively.
For MT06, Chiang?s original feature improved the
baseline significantly ? this is a new result using
his feature, since he did not experiment with Ara-
bic ? as did our our IP=, PP=, and VP= condi-
tions. Adding individual features PP+ and AdvP=
yielded significant improvements up to 1.4 BLEU
points over baseline, and in fact the improvement for
individual feature AdvP= over Chiang?s undifferen-
tiated constituency feature approaches significance
(p < .075).
More important, several conditions combining
features achieved statistically significant improve-
ments over baseline of up 1.94 BLEU points: XP2,
IP2, IP, VP=.PP+.AdvP=, AP2, PP+.AdvP=, and
AdvP2. Of these, AdvP2 is also a significant im-
provement over the undifferentiated constituency
feature (Chiang-05), with p < .01. As we did
for Chinese, we tested the best-performing models
on a new test set, NIST MT08. Consistent patterns
reappeared: improvements over the baseline up to
1.69 BLEU (p < .01), with AdvP2 again in the
lead (also outperforming the undifferentiated con-
stituency feature, p < .05).
Arabic MT03 MT06 MT08
Baseline .4795 .3571 .3571
Chiang-05 .4787 .3679** .3678**
VP+ .4802 .3481
AP+ .4856 .3495
IP+ .4818 .3516
CP= .4815 .3523
NP= .4847 .3537
NP+ .4800 .3548
AP= .4797 .3569
AdvP+ .4852 .3572
CP+ .4758 .3578
IP= .4811 .3636** .3647**
PP= .4801 .3651** .3662**
VP= .4803 .3655** .3694**
PP+ .4837** .3707** .3700**
AdvP= .4823** .3711**- .3717**
Multiple / conflated features:
XP+ .4771 .3522
all-labels2 .4898**+ .3536 .3572
all-labels_ .4828 .3548
VP2 .4826 .3552
NP2 .4832 .3561
AdvP.VP.PP.IP= .4826 .3571
VP_ .4825 .3604
all-labels+ .4825 .3600
XP2 .4859**+ .3605^ .3613**
IP2 .4793 .3611* .3593
IP_ .4791 .3635* .3648**
XP= .4808 .3659** .3704**+
VP=.PP+.AdvP= .4833** .3677** .3718**
AP2 .4840** .3692** .3719**
PP+.AdvP= .4777 .3708** .3680**
AdvP2 .4803 .3765**++ .3740**+
Table 4: Arabic-English Experiments. Results are
sorted by MT06 BLEU score. *: Better than baseline
(p < .05). **: Better than baseline (p < .01). +: Bet-
ter than Chiang-05 (p < .05). ++: Better than Chiang-05
(p < .01). -: Almost significantly better than Chiang-05
(p < .075)
1008
5 Discussion
The results in Section 4 demonstrate, to our knowl-
edge for the first time, that significant and sometimes
substantial gains over baseline can be obtained by
incorporating soft syntactic constraints into Hiero?s
translation model. Within language, we also see
considerable consistency across multiple test sets, in
terms of which constraints tend to help most.
Furthermore, our results provide some insight into
why the original approach may have failed to yield a
positive outcome. For Chinese, we found that when
we defined finer-grained versions of the exact-match
features, there was value for some constituency
types in biasing the model to favor matching the
source language parse. Moreover, we found that
there was significant value in allowing the model
to be sensitive to violations (crossing boundaries)
of source parses. These results confirm that parser
quality was not the limitation in the original work
(or at least not the only limitation), since in our ex-
periments the parser was held constant.
Looking at combinations of new features, some
?double-feature? combinations (VP2, IP2) achieved
large gains, although note that more is not neces-
sarily better: combinations of more features did not
yield better scores, and some did not yield any gain
at all. No conflated feature reached significance, but
it is not the case that all conflated features are worse
than their same-constituent ?double-feature? coun-
terparts. We found no simple correlation between
finer-grained feature scores (and/or boundary con-
dition type) and combination or conflation scores.
Since some combinations seem to cancel individ-
ual contributions, we can conclude that the higher
the number of participant features (of the kinds de-
scribed here), the more likely a cancellation effect is;
therefore, a ?double-feature? combination is more
likely to yield higher gains than a combination con-
taining more features.
We also investigated whether non-canonical lin-
guistic constituency labels such as PRN, FRAG,
UCP and VSB introduce ?noise?, by means of the
XP features ? the XP= feature is, in fact, simply the
undifferentiated constituency feature, but sensitive
only to ?standard? XPs. Although performance of
XP=, XP2 and all-labels+ were similar to that of the
undifferentiated constituency feature, XP+ achieved
the highest gain. Intuitively, this seems plausible:
the feature says, at least for Chinese, that a transla-
tion hypothesis should incur a penalty if it is trans-
lating a substring as a unit when that substring is not
a canonical source constituent.
Having obtained positive results with Chinese, we
explored the extent to which the approach might
improve translation using a very different source
language. The approach on Arabic-English trans-
lation yielded large BLEU gains over baseline, as
well as significant improvements over the undiffer-
entiated constituency feature. Comparing the two
sets of experiments, we see that there are definitely
language-specific variations in the value of syntactic
constraints; for example, AdvP, the top performer in
Arabic, cannot possibly perform well for Chinese,
since in our parses the AdvP constituents rarely in-
clude more than a single word. At the same time,
some IP and VP variants seem to do generally well
in both languages. This makes sense, since ? at
least for these language pairs and perhaps more gen-
erally ? clauses and verb phrases seem to corre-
spond often on the source and target side. We found
it more surprising that no NP variant yielded much
gain in Arabic; this question will be taken up in fu-
ture work.
6 Related Work
Space limitations preclude a thorough review of
work attempting to navigate the tradeoff between us-
ing language analyzers and exploiting unconstrained
data-driven modeling, although the recent literature
is full of variety and promising approaches. We limit
ourselves here to several approaches that seem most
closely related.
Among approaches using parser-based syntactic
models, several researchers have attempted to re-
duce the strictness of syntactic constraints in or-
der to better exploit shallow correspondences in
parallel training data. Our introduction has al-
ready briefly noted Cowan et al (2006), who relax
parse-tree-based alignment to permit alignment of
non-constituent subphrases on the source side, and
translate modifiers using a separate phrase-based
model, and DeNeefe et al (2007), who modify
syntax-based extraction and binarize trees (follow-
ing (Wang et al, 2007b)) to improve phrasal cov-
1009
erage. Similarly, Marcu et al (2006) relax their
syntax-based system by rewriting target-side parse
trees on the fly in order to avoid the loss of ?non-
syntactifiable? phrase pairs. Setiawan et al (2007)
employ a ?function-word centered syntax-based ap-
proach?, with synchronous CFG and extended ITG
models for reordering phrases, and relax syntac-
tic constraints by only using a small number func-
tion words (approximated by high-frequency words)
to guide the phrase-order inversion. Zollman and
Venugopal (2006) start with a target language parser
and use it to provide constraints on the extraction of
hierarchical phrase pairs. Unlike Hiero, their trans-
lation model uses a full range of named nonterminal
symbols in the synchronous grammar. As an alter-
native way to relax strict parser-based constituency
requirements, they explore the use of phrases span-
ning generalized, categorial-style constituents in the
parse tree, e.g. type NP/NN denotes a phrase like
the great that lacks only a head noun (say, wall) in
order to comprise an NP.
In addition, various researchers have explored the
use of hard linguistic constraints on the source side,
e.g. via ?chunking? noun phrases and translating
them separately (Owczarzak et al, 2006), or by per-
forming hard reorderings of source parse trees in
order to more closely approximate target-language
word order (Wang et al, 2007a; Collins et al, 2005).
Finally, another soft-constraint approach that can
also be viewed as coming from the data-driven side,
adding syntax, is taken by Riezler and Maxwell
(2006). They use LFG dependency trees on both
source and target sides, and relax syntactic con-
straints by adding a ?fragment grammar? for un-
parsable chunks. They decode using Pharaoh, aug-
mented with their own log-linear features (such as
p(esnippet|fsnippet) and its converse), side by side to?traditional? lexical weights. Riezler and Maxwell
(2006) do not achieve higher BLEU scores, but
do score better according to human grammaticality
judgments for in-coverage cases.
7 Conclusion
When hierarchical phrase-based translation was in-
troduced by Chiang (2005), it represented a new and
successful way to incorporate syntax into statistical
MT, allowing the model to exploit non-local depen-
dencies and lexically sensitive reordering without
requiring linguistically motivated parsing of either
the source or target language. An approach to incor-
porating parser-based constituents in the model was
explored briefly, treating syntactic constituency as a
soft constraint, with negative results.
In this paper, we returned to the idea of linguis-
tically motivated soft constraints, and we demon-
strated that they can, in fact, lead to substantial
improvements in translation performance when in-
tegrated into the Hiero framework. We accom-
plished this using constraints that not only dis-
tinguish among constituent types, but which also
distinguish between the benefit of matching the
source parse bracketing, versus the cost of us-
ing phrases that cross relevant bracketing bound-
aries. We demonstrated improvements for Chinese-
English translation, and succeed in obtaining sub-
stantial gains for Arabic-English translation, as well.
Our results contribute to a growing body of work
on combining monolingually based, linguistically
motivated syntactic analysis with translation mod-
els that are closely tied to observable parallel train-
ing data. Consistent with other researchers, we find
that ?syntactic constituency? may be too coarse a no-
tion by itself; rather, there is value in taking a finer-
grained approach, and in allowing the model to de-
cide how far to trust each element of the syntactic
analysis as part of the system?s optimization process.
Acknowledgments
This work was supported in part by DARPA prime
agreement HR0011-06-2-0001. The authors would
like to thank David Chiang and Adam Lopez for
making their source code available; the Stanford
Parser team and Mary Harper for making their
parsers available; David Chiang, Amy Weinberg,
and CLIP Laboratory colleagues, particularly Chris
Dyer, Adam Lopez, and Smaranda Muresan, for dis-
cussion and invaluable assistance.
1010
References
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the ACL Workshop on
Statistical Machine Translation 2007.
P.F. Brown, S.A.D. Pietra, V.J.D. Pietra, and R.L. Mercer.
1993. The mathematics of statistical machine transla-
tion. Computational Linguistics, 19(2):263?313.
S. F. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Tech.
Report TR-10-98, Comp. Sci. Group, Harvard U.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL-05, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL-05.
Brooke Cowan, Ivona Kucerova, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proc. EMNLP.
S DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007.
What can syntax-based MT learn from phrase-based
MT? In Proceedings of EMNLP-CoNLL.
J. Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In ACL Companion Vol.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. EMNLP 2002.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL-06.
H. Hassan, K. Sima?an, and A. Way. 2007. Integrating
supertags into phrase-based statistical machine trans-
lation. In Proc. ACL-07, pages 288?295.
Zhongqiang Huang, Denis Filimonov, and Mary Harper.
2008. Accuracy enhancements for mandarin parsing.
Tech. report, University of Maryland.
Dan Klein and Christopher D. Manning. 2003a. Accu-
rate unlexicalized parsing. In Proceedings of ACL-03,
pages 423?430.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural lan-
guage parsing. Advances in Neural Information Pro-
cessing Systems, 15(NIPS 2002):3?10.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proc. EMNLP+CoNLL, pages 868?
876, Prague.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL, pages 127?133.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Adam Lopez. (to appear). Statistical machine transla-
tion. ACM Computing Surveys. Earlier version: A
Survey of Statistical Machine Translation. U. of Mary-
land, UMIACS tech. report 2006-47. Apr 2007.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine trans-
lation with syntactified target language phrases. In
Proc. EMNLP, pages 44?52.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the ACL, pages 440?447. GIZA++.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160?167.
K. Owczarzak, B. Mellebeek, D. Groves, J. Van Gen-
abith, and A. Way. 2006. Wrapper syntax for
example-based machine translation. In Proceedings
of the 7th Conference of the Association for Machine
Translation in the Americas, pages 148?155.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002. Corpusbased
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish results. In Pro-
ceedings of the Human Language Technology Confer-
ence (ACL?2002), pages 124?127, San Diego, CA.
Stefan Riezler and John Maxwell. 2006. Grammatical
machine translation. In Proc. HLT-NAACL, New York,
NY.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007.
Ordering phrases with function words. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 712?719.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901?904.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Processing.
Chao Wang, Michael Collins, and Phillip Koehn. 2007a.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007b. Bi-
narizing syntax trees to improve syntax-based machine
translation accuracy. In Proc. EMNLP+CoNLL 2007.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the SMT Workshop, HLT-NAACL.
1011
Proceedings of ACL-08: HLT, pages 1012?1020,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Generalizing Word Lattice Translation
Christopher Dyer?, Smaranda Muresan, Philip Resnik?
Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
?Department of Linguistics
University of Maryland
College Park, MD 20742, USA
redpony, smara, resnik AT umd.edu
Abstract
Word lattice decoding has proven useful in
spoken language translation; we argue that it
provides a compelling model for translation of
text genres, as well. We show that prior work
in translating lattices using finite state tech-
niques can be naturally extended to more ex-
pressive synchronous context-free grammar-
based models. Additionally, we resolve a
significant complication that non-linear word
lattice inputs introduce in reordering mod-
els. Our experiments evaluating the approach
demonstrate substantial gains for Chinese-
English and Arabic-English translation.
1 Introduction
When Brown and colleagues introduced statistical
machine translation in the early 1990s, their key in-
sight ? harkening back to Weaver in the late 1940s ?
was that translation could be viewed as an instance
of noisy channel modeling (Brown et al, 1990).
They introduced a now standard decomposition that
distinguishes modeling sentences in the target lan-
guage (language models) from modeling the rela-
tionship between source and target language (trans-
lation models). Today, virtually all statistical trans-
lation systems seek the best hypothesis e for a given
input f in the source language, according to
e? = arg max
e
Pr(e|f) (1)
An exception is the translation of speech recogni-
tion output, where the acoustic signal generally un-
derdetermines the choice of source word sequence
f . There, Bertoldi and others have recently found
that, rather than translating a single-best transcrip-
tion f , it is advantageous to allow the MT decoder to
consider all possibilities for f by encoding the alter-
natives compactly as a confusion network or lattice
(Bertoldi et al, 2007; Bertoldi and Federico, 2005;
Koehn et al, 2007).
Why, however, should this advantage be limited
to translation from spoken input? Even for text,
there are often multiple ways to derive a sequence
of words from the input string. Segmentation of
Chinese, decompounding in German, morpholog-
ical analysis for Arabic ? across a wide range
of source languages, ambiguity in the input gives
rise to multiple possibilities for the source word se-
quence. Nonetheless, state-of-the-art systems com-
monly identify a single analysis f during a prepro-
cessing step, and decode according to the decision
rule in (1).
In this paper, we go beyond speech translation
by showing that lattice decoding can also yield im-
provements for text by preserving alternative anal-
yses of the input. In addition, we generalize lattice
decoding algorithmically, extending it for the first
time to hierarchical phrase-based translation (Chi-
ang, 2005; Chiang, 2007).
Formally, the approach we take can be thought of
as a ?noisier channel?, where an observed signal o
gives rise to a set of source-language strings f ? ?
F(o) and we seek
e? = arg max
e
max
f ??F(o)
Pr(e, f ?|o) (2)
= arg max
e
max
f ??F(o)
Pr(e)Pr(f ?|e, o) (3)
= arg max
e
max
f ??F(o)
Pr(e)Pr(f ?|e)Pr(o|f ?).(4)
Following Och and Ney (2002), we use the maxi-
mum entropy framework (Berger et al, 1996) to di-
rectly model the posterior Pr(e, f ?|o) with parame-
ters tuned to minimize a loss function representing
1012
the quality only of the resulting translations. Thus,
we make use of the following general decision rule:
e? = arg max
e
max
f ??F(o)
M?
m=1
?m?m(e, f
?, o) (5)
In principle, one could decode according to (2)
simply by enumerating and decoding each f ? ?
F(o); however, for any interestingly large F(o) this
will be impractical. We assume that for many in-
teresting cases of F(o), there will be identical sub-
strings that express the same content, and therefore
a lattice representation is appropriate.
In Section 2, we discuss decoding with this model
in general, and then show how two classes of trans-
lation models can easily be adapted for lattice trans-
lation; we achieve a unified treatment of finite-state
and hierarchical phrase-based models by treating
lattices as a subcase of weighted finite state au-
tomata (FSAs). In Section 3, we identify and solve
issues that arise with reordering in non-linear FSAs,
i.e. FSAs where every path does not pass through
every node. Section 4 presents two applications of
the noisier channel paradigm, demonstrating sub-
stantial performance gains in Arabic-English and
Chinese-English translation. In Section 5 we discuss
relevant prior work, and we conclude in Section 6.
2 Decoding
Most statistical machine translation systems model
translational equivalence using either finite state
transducers or synchronous context free grammars
(Lopez, to appear 2008). In this section we discuss
the issues associated with adapting decoders from
both classes of formalism to process word lattices.
The first decoder we present is a SCFG-based de-
coder similar to the one described in Chiang (2007).
The second is a phrase-based decoder implementing
the model of Koehn et al (2003).
2.1 Word lattices
A word lattice G = ?V,E? is a directed acyclic
graph that formally is a weighted finite state automa-
ton (FSA). We further stipulate that exactly one node
has no outgoing edges and is designated the ?end
node?. Figure 1 illustrates three classes of word
lattices.
0
1x 2a
y
3bc
0 1
ax
?
2b 3dc
0 1a 2b 3c
Figure 1: Three examples of word lattices: (a) sentence,
(b) confusion network, and (c) non-linear word lattice.
A word lattice is useful for our purposes because
it permits any finite set of strings to be represented
and allows for substrings common to multiple mem-
bers of the set to be represented with a single piece
of structure. Additionally, all paths from one node to
another form an equivalence class representing, in
our model, alternative expressions of the same un-
derlying communicative intent.
For translation, we will find it useful to encode
G in a chart based on a topological ordering of the
nodes, as described by Cheppalier et al (1999). The
nodes in the lattices shown in Figure 1 are labeled
according to an appropriate numbering.
The chart-representation of the graph is a triple of
2-dimensional matrices ?F,p,R?, which can be con-
structed from the numbered graph. Fi,j is the word
label of the jth transition leaving node i. The cor-
responding transition cost is pi,j . Ri,j is the node
number of the node on the right side of the jth tran-
sition leaving node i. Note that Ri,j > i for all i, j.
Table 1 shows the word lattice from Figure 1 repre-
sented in matrix form as ?F,p,R?.
0 1 2
a 1 1 b 1 2 c 1 3
a 13 1 b 1 2 c
1
2 3
x 13 1 d
1
2 3
 13 1
x 12 1 y 1 2 b
1
2 3
a 12 2 c
1
2 3
Table 1: Topologically ordered chart encoding of the
three lattices in Figure 1. Each cell ij in this table is a
triple ?Fij ,pij ,Rij?
1013
2.2 Parsing word lattices
Chiang (2005) introduced hierarchical phrase-based
translation models, which are formally based
on synchronous context-free grammars (SCFGs).
Translation proceeds by parsing the input using the
source language side of the grammar, simultane-
ously building a tree on the target language side via
the target side of the synchronized rules. Since de-
coding is equivalent to parsing, we begin by present-
ing a parser for word lattices, which is a generaliza-
tion of a CKY parser for lattices given in Cheppalier
et al (1999).
Following Goodman (1999), we present our lat-
tice parser as a deductive proof system in Figure 2.
The parser consists of two kinds of items, the first
with the form [X ? ? ? ?, i, j] representing rules
that have yet to be completed and span node i to
node j. The other items have the form [X, i, j] and
indicate that non-terminal X spans [i, j]. As with
sentence parsing, the goal is a deduction that covers
the spans of the entire input lattice [S, 0, |V | ? 1].
The three inference rules are: 1) match a terminal
symbol and move across one edge in the lattice 2)
move across an -edge without advancing the dot in
an incomplete rule 3) advance the dot across a non-
terminal symbol given appropriate antecedents.
2.3 From parsing to MT decoding
A target language model is necessary to generate flu-
ent output. To do so, the grammar is intersected with
an n-gram LM. To mitigate the effects of the combi-
natorial explosion of non-terminals the LM intersec-
tion entails, we use cube-pruning to only consider
the most promising expansions (Chiang, 2007).
2.4 Lattice translation with FSTs
A second important class of translation models in-
cludes those based formally on FSTs. We present a
description of the decoding process for a word lattice
using a representative FST model, the phrase-based
translation model described in Koehn et al (2003).
Phrase-based models translate a foreign sentence
f into the target language e by breaking up f into
a sequence of phrases f
I
1, where each phrase f i can
contain one or more contiguous words and is trans-
lated into a target phrase ei of one or more contigu-
ous words. Each word in f must be translated ex-
actly once. To generalize this model to word lattices,
it is necessary to choose both a path through the lat-
tice and a partitioning of the sentence this induces
into a sequence of phrases f
I
1. Although the number
of source phrases in a word lattice can be exponen-
tial in the number of nodes, enumerating the possible
translations of every span in a lattice is in practice
tractable, as described by Bertoldi et al (2007).
2.5 Decoding with phrase-based models
We adapted the Moses phrase-based decoder to
translate word lattices (Koehn et al, 2007). The
unmodified decoder builds a translation hypothesis
from left to right by selecting a range of untrans-
lated words and adding translations of this phrase to
the end of the hypothesis being extended. When no
untranslated words remain, the translation process is
complete.
The word lattice decoder works similarly, only
now the decoder keeps track not of the words that
have been covered, but of the nodes, given a topo-
logical ordering of the nodes. For example, assum-
ing the third lattice in Figure 1 is our input, if the
edge with word a is translated, this will cover two
untranslated nodes [0,1] in the coverage vector, even
though it is only a single word. As with sentence-
based decoding, a translation hypothesis is complete
when all nodes in the input lattice are covered.
2.6 Non-monotonicity and unreachable nodes
The changes described thus far are straightfor-
ward adaptations of the underlying phrase-based
sentence decoder; however, dealing properly with
non-monotonic decoding of word lattices introduces
some minor complexity that is worth mentioning. In
the sentence decoder, any translation of any span of
untranslated words is an allowable extension of a
partial translation hypothesis, provided that the cov-
erage vectors of the extension and the partial hypoth-
esis do not intersect. In a non-linear word lattice,
a further constraint must be enforced ensuring that
there is always a path from the starting node of the
translation extension?s source to the node represent-
ing the nearest right edge of the already-translated
material, as well as a path from the ending node of
the translation extension?s source to future translated
spans. Figure 3 illustrates the problem. If [0,1] is
translated, the decoder must not consider translating
1014
Axioms:
[X ? ??, i, i] : w
(X
w
?? ??, ??) ? G, i ? [0, |V | ? 2]
Inference rules:
[X ? ? ? Fj,k?, i, j] : w
[X ? ?Fj,k ? ?, i,Rj,k] : w ? pj,k
[X ? ? ? ?, i, j] : w
[X ? ? ? ?, i,Rj,k] : w ? pj,k
Fj,k = 
[Z ? ? ?X?, i, k] : w1 [X ? ??, k, j] : w2
[Z ? ?X ? ?, i, j] : w1 ? w2
Goal state:
[S ? ??, 0, |V | ? 1]
Figure 2: Word lattice parser for an unrestricted context free grammar G.
0 1x
2
ay
Figure 3: The span [0, 3] has one inconsistent covering,
[0, 1] + [2, 3].
[2,3] as a possible extension of this hypothesis since
there is no path from node 1 to node 2 and therefore
the span [1,2] would never be covered. In the parser
that forms the basis of the hierarchical decoder de-
scribed in Section 2.3, no such restriction is neces-
sary since grammar rules are processed in a strictly
left-to-right fashion without any skips.
3 Distortion in a non-linear word lattice
In both hierarchical and phrase-based models, the
distance between words in the source sentence is
used to limit where in the target sequence their trans-
lations will be generated. In phrase based transla-
tion, distortion is modeled explicitly. Models that
support non-monotonic decoding generally include
a distortion cost, such as |ai ? bi?1 ? 1| where ai is
the starting position of the foreign phrase f i and bi?1
is the ending position of phrase f i?1 (Koehn et al,
2003). The intuition behind this model is that since
most translation is monotonic, the cost of skipping
ahead or back in the source should be proportional
to the number of words that are skipped. Addition-
ally, a maximum distortion limit is used to restrict
0 1x
2a y3
b cd
Figure 4: Distance-based distortion problem. What is the
distance between node 4 to node 0?
the size of the search space.
In linear word lattices, such as confusion net-
works, the distance metric used for the distortion
penalty and for distortion limits is well defined;
however, in a non-linear word lattice, it poses the
problem illustrated in Figure 4. Assuming the left-
to-right decoding strategy described in the previous
section, if c is generated by the first target word, the
distortion penalty associated with ?skipping ahead?
should be either 3 or 2, depending on what path is
chosen to translate the span [0,3]. In large lattices,
where a single arc may span many nodes, the possi-
ble distances may vary quite substantially depending
on what path is ultimately taken, and handling this
properly therefore crucial.
Although hierarchical phrase-based models do
not model distortion explicitly, Chiang (2007) sug-
gests using a span length limit to restrict the win-
dow in which reordering can take place.1 The de-
coder enforces the constraint that a synchronous rule
learned from the training data (the only mechanism
by which reordering can be introduced) can span
1This is done to reduce the size of the search space and be-
cause hierarchical phrase-based translation models are inaccu-
rate models of long-distance distortion.
1015
Distance metric MT05 MT06
Difference 0.2943 0.2786
Difference+LexRO 0.2974 0.2890
ShortestP 0.2993 0.2865
ShortestP+LexRO 0.3072 0.2992
Table 2: Effect of distance metric on phrase-based model
performance.
maximally ? words in f . Like the distortion cost
used in phrase-based systems, ? is also poorly de-
fined for non-linear lattices.
Since we want a distance metric that will restrict
as few local reorderings as possible on any path,
we use a function ?(a, b) returning the length of the
shortest path between nodes a and b. Since this func-
tion is not dependent on the exact path chosen, it can
be computed in advance of decoding using an all-
pairs shortest path algorithm (Cormen et al, 1989).
3.1 Experimental results
We tested the effect of the distance metric on trans-
lation quality using Chinese word segmentation lat-
tices (Section 4.1, below) using both a hierarchical
and phrase-based system modified to translate word
lattices. We compared the shortest-path distance
metric with a baseline which uses the difference in
node number as the distortion distance. For an ad-
ditional datapoint, we added a lexicalized reorder-
ing model that models the probability of each phrase
pair appearing in three different orientations (swap,
monotone, other) in the training corpus (Koehn et
al., 2005).
Table 2 summarizes the results of the phrase-
based systems. On both test sets, the shortest path
metric improved the BLEU scores. As expected,
the lexicalized reordering model improved transla-
tion quality over the baseline; however, the improve-
ment was more substantial in the model that used the
shortest-path distance metric (which was already a
higher baseline). Table 3 summarizes the results of
our experiment comparing the performance of two
distance metrics to determine whether a rule has ex-
ceeded the decoder?s span limit. The pattern is the
same, showing a clear increase in BLEU for the
shortest path metric over the baseline.
Distance metric MT05 MT06
Difference 0.3063 0.2957
ShortestP 0.3176 0.3043
Table 3: Effect of distance metric on hierarchical model
performance.
4 Exploiting Source Language Alternatives
Chinese word segmentation. A necessary first
step in translating Chinese using standard models
is segmenting the character stream into a sequence
of words. Word-lattice translation offers two possi-
ble improvements over the conventional approach.
First, a lattice may represent multiple alternative
segmentations of a sentence; input represented in
this way will be more robust to errors made by the
segmenter.2 Second, different segmentation granu-
larities may be more or less optimal for translating
different spans. By encoding alternatives in the in-
put in a word lattice, the decision as to which granu-
larity to use for a given span can be resolved during
decoding rather than when constructing the system.
Figure 5 illustrates a lattice based on three different
segmentations.
Arabic morphological variation. Arabic orthog-
raphy is problematic for lexical and phrase-based
MT approaches since a large class of functional el-
ements (prepositions, pronouns, tense markers, con-
junctions, definiteness markers) are attached to their
host stems. Thus, while the training data may pro-
vide good evidence for the translation of a partic-
ular stem by itself, the same stem may not be at-
tested when attached to a particular conjunction.
The general solution taken is to take the best pos-
sible morphological analysis of the text (it is of-
ten ambiguous whether a piece of a word is part
of the stem or merely a neighboring functional el-
ement), and then make a subset of the bound func-
tional elements in the language into freestanding to-
kens. Figure 6 illustrates the unsegmented Arabic
surface form as well as the morphological segmen-
tation variant we made use of. The limitation of this
approach is that as the amount and variety of train-
ing data increases, the optimal segmentation strat-
egy changes: more aggressive segmentation results
2The segmentation process is ambiguous, even for native
speakers of Chinese.
1016
01
?
2
??
4
????
?
3
?
??
?
5
?
6
??
?
7
"
8
?
9
??
?
10
?
11
??
?
12
"
Figure 5: Sample Chinese segmentation lattice using three segmentations.
in fewer OOV tokens, but automatic evaluation met-
rics indicate lower translation quality, presumably
because the smaller units are being translated less
idiomatically (Habash and Sadat, 2006). Lattices al-
low the decoder to make decisions about what gran-
ularity of segmentation to use subsententially.
4.1 Chinese Word Segmentation Experiments
In our experiments we used two state-of-the-art Chi-
nese word segmenters: one developed at Harbin
Institute of Technology (Zhao et al, 2001), and
one developed at Stanford University (Tseng et al,
2005). In addition, we used a character-based seg-
mentation. In the remaining of this paper, we use cs
for character segmentation, hs for Harbin segmenta-
tion and ss for Stanford segmentation. We built two
types of lattices: one that combines the Harbin and
Stanford segmenters (hs+ss), and one which uses
all three segmentations (hs+ss+cs).
Data and Settings. The systems used in these
experiments were trained on the NIST MT06 Eval
corpus without the UN data (approximatively 950K
sentences). The corpus was analyzed with the three
segmentation schemes. For the systems using word
lattices, the training data contained the versions of
the corpus appropriate for the segmentation schemes
used in the input. That is, for the hs+ss condition,
the training data consisted of two copies of the cor-
pus: one segmented with the Harbin segmenter and
the other with the Stanford segmenter.3 A trigram
English language model with modified Kneser-Ney
smoothing (Kneser and Ney, 1995) was trained on
the English side of our training data as well as por-
tions of the Gigaword v2 English Corpus, and was
used for all experiments. The NIST MT03 test set
was used as a development set for optimizing the in-
terpolation weights using minimum error rate train-
3The corpora were word-aligned independently and then
concatenated for rule extraction.
ing (Och, 2003). The testing was done on the NIST
2005 and 2006 evaluation sets (MT05, MT06).
Experimental results: Word-lattices improve
translation quality. We used both a phrase-based
translation model, decoded using our modified ver-
sion of Moses (Koehn et al, 2007), and a hierarchi-
cal phrase-based translation model, using our modi-
fied version of Hiero (Chiang, 2005; Chiang, 2007).
These two translation model types illustrate the ap-
plicability of the theoretical contributions presented
in Section 2 and Section 3.
We observed that the coverage of named entities
(NEs) in our baseline systems was rather poor. Since
names in Chinese can be composed of relatively
long strings of characters that cannot be translated
individually, when generating the segmentation lat-
tices that included cs arcs, we avoided segmenting
NEs of type PERSON, as identified using a Chinese
NE tagger (Florian et al, 2004).
The results are summarized in Table 4. We see
that using word lattices improves BLEU scores both
in the phrase-based model and hierarchical model as
compared to the single-best segmentation approach.
All results using our word-lattice decoding for the
hierarchical models (hs+ss and hs+ss+cs) are sig-
nificantly better than the best segmentation (ss).4
For the phrase-based model, we obtain significant
gains using our word-lattice decoder using all three
segmentations on MT05. The other results, while
better than the best segmentation (hs) by at least
0.3 BLEU points, are not statistically significant.
Even if the results are not statistically significant
for MT06, there is a high decrease in OOV items
when using word-lattices. For example, for MT06
the number of OOVs in the hs translation is 484.
4Significance testing was carried out using the bootstrap re-
sampling technique advocated by Koehn (2004). Unless other-
wise noted, all reported improvements are signficant at at least
p < 0.05.
1017
surface wxlAl ftrp AlSyf kAn mEZm AlDjyj AlAElAmy m&ydA llEmAd .
segmented w- xlAl ftrp Al- Syf kAn mEZm Al- Djyj Al- AElAmy m&ydA l- Al- EmAd .
(English) During the summer period , most media buzz was supportive of the general .
Figure 6: Example of Arabic morphological segmentation.
The number of OOVs decreased by 19% for hs+ss
and by 75% for hs+ss+cs. As mentioned in Section
3, using lexical reordering for word-lattices further
improves the translation quality.
4.2 Arabic Morphology Experiments
We created lattices from an unsegmented version of
the Arabic test data and generated alternative arcs
where clitics as well as the definiteness marker and
the future tense marker were segmented into tokens.
We used the Buckwalter morphological analyzer and
disambiguated the analysis using a simple unigram
model trained on the Penn Arabic Treebank.
Data and Settings. For these experiments we
made use of the entire NIST MT08 training data,
although for training of the system, we used a sub-
sampling method proposed by Kishore Papineni that
aims to include training sentences containing n-
grams in the test data (personal communication).
For all systems, we used a 5-gram English LM
trained on 250M words of English training data.
The NIST MT03 test set was used as development
set for optimizing the interpolation weights using
MER training (Och, 2003). Evaluation was car-
ried out on the NIST 2005 and 2006 evaluation sets
(MT05, MT06).
Experimental results: Word-lattices improve
translation quality. Results are presented in Table
5. Using word-lattices to combine the surface forms
with morphologically segmented forms significantly
improves BLEU scores both in the phrase-based and
hierarchical models.
5 Prior work
Lattice Translation. The ?noisier channel? model
of machine translation has been widely used in spo-
ken language translation as an alternative to select-
ing the single-best hypothesis from an ASR system
and translating it (Ney, 1999; Casacuberta et al,
2004; Zhang et al, 2005; Saleem et al, 2005; Ma-
tusov et al, 2005; Bertoldi et al, 2007; Mathias,
2007). Several authors (e.g. Saleem et al (2005)
and Bertoldi et al (2007)) comment directly on
the impracticality of using n-best lists to translate
speech.
Although translation is fundamentally a non-
monotonic relationship between most language
pairs, reordering has tended to be a secondary con-
cern to the researchers who have worked on lattice
translation. Matusov et al (2005) decodes monoton-
ically and then uses a finite state reordering model
on the single-best translation, along the lines of
Bangalore and Riccardi (2000). Mathias (2007)
and Saleem et al (2004) only report results of
monotonic decoding for the systems they describe.
Bertoldi et al (2007) solve the problem by requiring
that their input be in the format of a confusion net-
work, which enables the standard distortion penalty
to be used. Finally, the system described by Zhang
et al (2005) uses IBM Model 4 features to translate
lattices. For the distortion model, they use the maxi-
mum probability value over all possible paths in the
lattice for each jump considered, which is similar
to the approach we have taken. Mathias and Byrne
(2006) build a phrase-based translation system as a
cascaded series of FSTs which can accept any input
FSA; however, the only reordering that is permitted
is the swapping of two adjacent phrases.
Applications of source lattices outside of the do-
main of spoken language translation have been far
more limited. Costa-jussa` and Fonollosa (2007) take
steps in this direction by using lattices to encode
multiple reorderings of the source language. Dyer
(2007) uses confusion networks to encode mor-
phological alternatives in Czech-English translation,
and Xu et al (2005) takes an approach very similar
to ours for Chinese-English translation and encodes
multiple word segmentations in a lattice, but which
is decoded with a conventionally trained translation
model and without a sophisticated reordering model.
The Arabic-English morphological segmentation
lattices are similar in spirit to backoff translation
models (Yang and Kirchhoff, 2006), which consider
alternative morphological segmentations and simpli-
1018
MT05 MT06
(Source Type) BLEU BLEU
cs 0.2833 0.2694
hs 0.2905 0.2835
ss 0.2894 0.2801
hs+ss 0.2938 0.2870
hs+ss+cs 0.2993 0.2865
hs+ss+cs.lexRo 0.3072 0.2992
MT05 MT06
(Source Type) BLEU BLEU
cs 0.2904 0.2821
hs 0.3008 0.2907
ss 0.3071 0.2964
hs+ss 0.3132 0.3006
hs+ss+cs 0.3176 0.3043
(a) Phrase-based model (b) Hierarchical model
Table 4: Chinese Word Segmentation Results
MT05 MT06
(Source Type) BLEU BLEU
surface 0.4682 0.3512
morph 0.5087 0.3841
morph+surface 0.5225 0.4008
MT05 MT06
(Source Type) BLEU BLEU
surface 0.5253 0.3991
morph 0.5377 0.4180
morph+surface 0.5453 0.4287
(a) Phrase-based model (b) Hierarchical model
Table 5: Arabic Morphology Results
fications of a surface token when the surface token
can not be translated.
Parsing and formal language theory. There has
been considerable work on parsing word lattices,
much of it for language modeling applications in
speech recognition (Ney, 1991; Cheppalier and Raj-
man, 1998). Additionally, Grune and Jacobs (2008)
refines an algorithm originally due to Bar-Hillel for
intersecting an arbitrary FSA (of which word lattices
are a subset) with a CFG. Klein and Manning (2001)
formalize parsing as a hypergraph search problem
and derive an O(n3) parser for lattices.
6 Conclusions
We have achieved substantial gains in translation
performance by decoding compact representations
of alternative source language analyses, rather than
single-best representations. Our results generalize
previous gains for lattice translation of spoken lan-
guage input, and we have further generalized the
approach by introducing an algorithm for lattice
decoding using a hierarchical phrase-based model.
Additionally, we have shown that although word
lattices complicate modeling of word reordering, a
simple heuristic offers good performance and en-
ables many standard distortion models to be used
directly with lattice input.
Acknowledgments
This research was supported by the GALE program
of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-2-0001. The authors wish
to thank Niyu Ge for the Chinese named-entity anal-
ysis, Pi-Chuan Chang for her assistance with the
Stanford Chinese segmenter, and Tie-Jun Zhao and
Congui Zhu for making the Harbin Chinese seg-
menter available to us.
References
S. Bangalore and G. Riccardi. 2000. Finite state models
for lexical reordering in spoken language translation.
In Proc. Int. Conf. on Spoken Language Processing,
pages 422?425, Beijing, China.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural lan-
guage processing. Comput. Linguist., 22(1):39?71.
N. Bertoldi and M. Federico. 2005. A new decoder for
spoken language translation based on confusion net-
works. In Proceedings of the IEEE Automatic Speech
Recognition and Understanding Workshop.
N. Bertoldi, R. Zens, and M. Federico. 2007. Speech
translation by confusion network decoding. In Pro-
ceeding of ICASSP 2007, Honolulu, Hawaii, April.
P.F. Brown, J. Cocke, S. Della-Pietra, V.J. Della-Pietra,
F. Jelinek, J.D. Lafferty, R.L. Mercer, and P.S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16:79?85.
F. Casacuberta, H. Ney, F. J. Och, E. Vidal, J. M. Vilar,
S. Barrachina, I. Garcia-Varea, D. Llorens, C. Mar-
1019
tinez, S. Molau, F. Nevado, M. Pastor, D. Pico, A. San-
chis, and C. Tillmann. 2004. Some approaches to
statistical and finite-state speech-to-speech translation.
Computer Speech & Language, 18(1):25?47, January.
J. Cheppalier and M. Rajman. 1998. A generalized CYK
algorithm for parsing stochastic CFG. In Proceedings
of the Workshop on Tabulation in Parsing and Deduc-
tion (TAPD98), pages 133?137, Paris, France.
J. Cheppalier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice parsing for speech recognition.
In Sixth Conference sur le Traitement Automatique du
Langage Naturel (TANL?99), pages 95?104.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of the 43rd
Annual Meeting of the Association for Computational
Linguistics (ACL?05), pages 263?270.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
T.H. Cormen, C. E. Leiserson, and R. L. Rivest, 1989.
Introduction to Algorithms, pages 558?565. The MIT
Press and McGraw-Hill Book Company.
M. Costa-jussa` and J.A.R. Fonollosa. 2007. Analy-
sis of statistical and morphological classes to gener-
ate weighted reordering hypotheses on a statistical ma-
chine translation system. In Proc. of the Second Work-
shop on SMT, pages 171?176, Prague.
C. Dyer. 2007. Noisier channel translation: translation
from morphologically complex languages. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, Prague, June.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proc. of HLT-NAACL 2004, pages 1?8.
J. Goodman. 1999. Semiring parsing. Computational
Linguistics, 25:573?605.
D. Grune and C.J. H. Jacobs. 2008. Parsing as intersec-
tion. Parsing Techniques, pages 425?442.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Proc. of
NAACL, New York.
D. Klein and C. D. Manning. 2001. Parsing with hyper-
graphs. In Proceedings of IWPT 2001.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of IEEE
Internation Conference on Acoustics, Speech, and Sig-
nal Processing, pages 181?184.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL
2003, pages 48?54.
P. Koehn, A. Axelrod, A. Birch Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Edinburgh
system description for the 2005 IWSLT speech trans-
lation evaluation. In Proc. of IWSLT 2005, Pittsburgh.
P. Koehn, H. Hoang, A. Birch Mayne, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Annual Meeting
of the Association for Computation Linguistics (ACL),
Demonstration Session, pages 177?180, Jun.
P. Koehn. 2004. Statistical significance tests for machine
translation evluation. In Proc. of the 2004 Conf. on
EMNLP, pages 388?395.
A. Lopez. to appear 2008. Statistical machine transla-
tion. ACM Computing Surveys.
L. Mathias and W. Byrne. 2006. Statistical phrase-
based speech translation. In IEEE Conf. on Acoustics,
Speech and Signal Processing.
L. Mathias. 2007. Statistical Machine Translation
and Automatic Speech Recognition under Uncertainty.
Ph.D. thesis, The Johns Hopkins University.
E. Matusov, S. Kanthak, and H. Ney. 2005. On the in-
tegration of speech recognition and statistical machine
translation. In Proceedings of Interspeech 2005.
H. Ney. 1991. Dynamic programming parsing for
context-free grammars in continuous speech recogni-
tion. IEEE Transactions on Signal Processing, 39(2).
H. Ney. 1999. Speech translation: Coupling of recogni-
tion and translation. In Proc. of ICASSP, pages 517?
520, Phoenix.
F. Och and H. Ney. 2002. Discriminitive training
and maximum entropy models for statistical machine
translation. In Proceedings of the 40th Annual Meet-
ing of the ACL, pages 295?302.
S. Saleem, S.-C. Jou, S. Vogel, and T. Schulz. 2005. Us-
ing word lattice information for a tighter coupling in
speech translation systems. In Proc. of ICSLP, Jeju
Island, Korea.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005. A conditional random field word seg-
menter. In Fourth SIGHANWorkshop on Chinese Lan-
guage Processing.
J. Xu, E. Matusov, R. Zens, and H. Ney. 2005. Inte-
grated Chinese word segmentation in statistical ma-
chine translation. In Proc. of IWSLT 2005, Pittsburgh.
M. Yang and K. Kirchhoff. 2006. Phrase-based back-
off models for machine translation of highly inflected
languages. In Proceedings of the EACL 2006, pages
41?48.
R. Zhang, G. Kikui, H. Yamamoto, and W. Lo. 2005.
A decoding algorithm for word lattice translation in
speech translation. In Proceedings of the 2005 Inter-
national Workshop on Spoken Language Translation.
T. Zhao, L. Yajuan, Y. Muyun, and Y. Hao. 2001. In-
creasing accuracy of chinese segmentation with strat-
egy of multi-step processing. In J Chinese Information
Processing (Chinese Version), volume 1, pages 13?18.
1020
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 324?332,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Topological Ordering of Function Words
in Hierarchical Phrase-based Translation
Hendra Setiawan
1
and Min-Yen Kan
2
and Haizhou Li
3
and Philip Resnik
1
1
University of Maryland Institute for Advanced Computer Studies
2
School of Computing, National University of Singapore
3
Human Language Technology, Institute for Infocomm Research, Singapore
{hendra,resnik}@umiacs.umd.edu,
kanmy@comp.nus.edu.sg, hli@i2r.a-star.edu.sg
Abstract
Hierarchical phrase-based models are at-
tractive because they provide a consis-
tent framework within which to character-
ize both local and long-distance reorder-
ings, but they also make it difcult to
distinguish many implausible reorderings
from those that are linguistically plausi-
ble. Rather than appealing to annotation-
driven syntactic modeling, we address this
problem by observing the inuential role
of function words in determining syntac-
tic structure, and introducing soft con-
straints on function word relationships as
part of a standard log-linear hierarchi-
cal phrase-based model. Experimentation
on Chinese-English and Arabic-English
translation demonstrates that the approach
yields signicant gains in performance.
1 Introduction
Hierarchical phrase-based models (Chiang, 2005;
Chiang, 2007) offer a number of attractive bene-
ts in statistical machine translation (SMT), while
maintaining the strengths of phrase-based systems
(Koehn et al, 2003). The most important of these
is the ability to model long-distance reordering ef-
ciently. To model such a reordering, a hierar-
chical phrase-based system demands no additional
parameters, since long and short distance reorder-
ings are modeled identically using synchronous
context free grammar (SCFG) rules. The same
rule, depending on its topological ordering ? i.e.
its position in the hierarchical structure ? can af-
fect both short and long spans of text. Interest-
ingly, hierarchical phrase-based models provide
this benet without making any linguistic commit-
ments beyond the structure of the model.
However, the system's lack of linguistic com-
mitment is also responsible for one of its great-
est drawbacks. In the absence of linguistic knowl-
edge, the system models linguistic structure using
an SCFG that contains only one type of nontermi-
nal symbol
1
. As a result, the system is susceptible
to the overgeneration problem: the grammar may
suggest more reordering choices than appropriate,
and many of those choices lead to ungrammatical
translations.
Chiang (2005) hypothesized that incorrect re-
ordering choices would often correspond to hier-
archical phrases that violate syntactic boundaries
in the source language, and he explored the use
of a ?constituent feature? intended to reward the
application of hierarchical phrases which respect
source language syntactic categories. Although
this did not yield signicant improvements, Mar-
ton and Resnik (2008) and Chiang et al (2008)
extended this approach by introducing soft syn-
tactic constraints similar to the constituent feature,
but more ne-grained and sensitive to distinctions
among syntactic categories; these led to substan-
tial improvements in performance. Zollman et al
(2006) took a complementary approach, constrain-
ing the application of hierarchical rules to respect
syntactic boundaries in the target language syn-
tax. Whether the focus is on constraints from the
source language or the target language, the main
ingredient in both previous approaches is the idea
of constraining the spans of hierarchical phrases to
respect syntactic boundaries.
In this paper, we pursue a different approach
to improving reordering choices in a hierarchical
phrase-based model. Instead of biasing the model
toward hierarchical phrases whose spans respect
syntactic boundaries, we focus on the topologi-
cal ordering of phrases in the hierarchical struc-
ture. We conjecture that since incorrect reorder-
ing choices correspond to incorrect topological or-
derings, boosting the probability of correct topo-
1
In practice, one additional nonterminal symbol is used in
?glue rules?. This is not relevant in the present discussion.
324
logical ordering choices should improve the sys-
tem. Although related to previous proposals (cor-
rect topological orderings lead to correct spans
and vice versa), our proposal incorporates broader
context and is structurally more aware, since we
look at the topological ordering of a phrase relative
to other phrases, rather than modeling additional
properties of a phrase in isolation. In addition, our
proposal requires no monolingual parsing or lin-
guistically informed syntactic modeling for either
the source or target language.
The key to our approach is the observation that
we can approximate the topological ordering of
hierarchical phrases via the topological ordering
of function words. We introduce a statistical re-
ordering model that we call the pairwise domi-
nance model, which characterizes reorderings of
phrases around a pair of function words. In mod-
eling function words, our model can be viewed as
a successor to the function words-centric reorder-
ing model (Setiawan et al, 2007), expanding on
the previous approach by modeling pairs of func-
tion words rather than individual function words
in isolation.
The rest of the paper is organized as follows. In
Section 2, we briey review hierarchical phrase-
based models. In Section 3, we rst describe the
overgeneration problem in more detail with a con-
crete example, and then motivate our idea of us-
ing the topological ordering of function words to
address the problem. In Section 4, we develop
our idea by introducing the pairwise dominance
model, expressing function word relationships in
terms of what we call the the dominance predi-
cate. In Section 5, we describe an algorithm to es-
timate the parameters of the dominance predicate
from parallel text. In Sections 6 and 7, we describe
our experiments, and in Section 8, we analyze the
output of our system and lay out a possible future
direction. Section 9 discusses the relation of our
approach to prior work and Section 10 wraps up
with our conclusions.
2 Hierarchical Phrase-based System
Formally, a hierarchical phrase-based SMT sys-
tem is based on a weighted synchronous context
free grammar (SCFG) with one type of nonter-
minal symbol. Synchronous rules in hierarchical
phrase-based models take the following form:
X ? ??, ?,?? (1)
where X is the nonterminal symbol and ? and ?
are strings that contain the combination of lexical
items and nonterminals in the source and target
languages, respectively. The ? symbol indicates
that nonterminals in ? and ? are synchronized
through co-indexation; i.e., nonterminals with the
same index are aligned. Nonterminal correspon-
dences are strictly one-to-one, and in practice the
number of nonterminals on the right hand side is
constrained to at most two, which must be sepa-
rated by lexical items.
Each rule is associated with a score that is com-
puted via the following log linear formula:
w(X ? ??, ?,??) =
?
i
f?ii (2)
where fi is a feature describing one particular as-
pect of the rule and ?i is the corresponding weight
of that feature. Given e? and f? as the source
and target phrases associated with the rule, typi-
cal features used are rule's translation probability
Ptrans(f? |e?) and its inverse Ptrans(e?|f?), the lexi-
cal probability Plex(f? |e?) and its inverse Plex(e?|f?).
Systems generally also employ a word penalty, a
phrase penalty, and target language model feature.
(See (Chiang, 2005) for more detailed discussion.)
Our pairwise dominance model will be expressed
as an additional rule-level feature in the model.
Translation of a source sentence e using hier-
archical phrase-based models is formulated as a
search for the most probable derivationD? whose
source side is equal to e:
D? = argmax P (D),where source(D)=e.
D = Xi, i ? 1...|D| is a set of rules following a
certain topological ordering, indicated here by the
use of the superscript.
3 Overgeneration and Topological
Ordering of Function Words
The use of only one type of nonterminal allows a
exible permutation of the topological ordering of
the same set of rules, resulting in a huge number of
possible derivations from a given source sentence.
In that respect, the overgeneration problem is not
new to SMT: Bracketing Transduction Grammar
(BTG) (Wu, 1997) uses a single type of nontermi-
nal and is subject to overgeneration problems, as
well.
2
2
Note, however, that overgeneration in BTG can be
viewed as a feature, not a bug, since the formalism was origi-
325
The problem may be less severe in hierarchi-
cal phrase-based MT than in BTG, since lexical
items on the rules' right hand sides often limit the
span of nonterminals. Nonetheless overgeneration
of reorderings is still problematic, as we illustrate
using the hypothetical Chinese-to-English exam-
ple in Fig. 1.
Suppose we want to translate the Chinese sen-
tence in Fig. 1 into English using the following set
of rules:
1. Xa ? ??Z X1, computers andX1?
2. Xb ? ?X14 X2, X1 are X2?
3. Xc ? ?C? , cell phones ?
4. Xd ? ?X1{? , inventions of X1?
5. Xe ? ???- , the last century ?
Co-indexation of nonterminals on the right hand
side is indicated by subscripts, and for our ex-
amples the label of the nonterminal on the left
hand side is used as the rule's unique identier.
To correctly translate the sentence, a hierarchical
phrase-based system needs to model the subject
noun phrase, object noun phrase and copula con-
structions; these are captured by rulesXa,Xd and
Xb respectively, so this set of rules represents a
hierarchical phrase-based system that can be used
to correctly translate the Chinese sentence. Note
that the Chinese word order is correctly preserved
in the subject (Xa) as well as copula constructions
(Xb), and correctly inverted in the object construc-
tion (Xd).
However, although it can generate the correct
translation in Fig. 2, the grammar has no mech-
anism to prevent the generation of an incorrect
translation like the one illustrated in Fig. 3. If
we contrast the topological ordering of the rules
in Fig. 2 and Fig. 3, we observe that the difference
is small but quite signicant. Using precede sym-
bol (?) to indicate the rst operand immediately
dominates the second operand in the hierarchical
structure, the topological orderings in Fig. 2 and
Fig. 3 are Xa ? Xb ? Xc ? Xd ? Xe and
Xd ? Xa ? Xb ? Xc ? Xe, respectively. The
only difference is the topological ordering of Xd:
in Fig. 2, it appears below most of the other hier-
archical phrases, while in Fig. 3, it appears above
all the other hierarchical phrases.
nally introduced for bilingual analysis rather than generation
of translations.
Modeling the topological ordering of hierarchi-
cal phrases is computationally prohibitive, since
there are literally millions of hierarchical rules in
the system's automatically-learned grammar and
millions of possible ways to order their applica-
tion. To avoid this computational problem and
still model the topological ordering, we propose
to use the topological ordering of function words
as a practical approximation. This is motivated by
the fact that function words tend to carry crucial
syntactic information in sentences, serving as the
?glue? for content-bearing phrases. Moreover, the
positional relationships between function words
and content phrases tends to be xed (e.g., in En-
glish, prepositions invariably precede their object
noun phrase), at least for the languages we have
worked with thus far.
In the Chinese sentence above, there are three
function words involved: the conjunctionZ (and),
the copula 4 (are), and the noun phrase marker
{ (of).3 Using the function words as approximate
representations of the rules in which they appear,
the topological ordering of hierarchical phrases in
Fig. 2 is Z(and) ? 4(are) ? {(of), while that
in Fig. 3 is {(of) ? Z(and) ? 4(are).4 We
can distinguish the correct and incorrect reorder-
ing choices by looking at this simple information.
In the correct reordering choice,{(of) appears at
the lower level of the hierarchy while in the incor-
rect one,{(of) appears at the highest level of the
hierarchy.
4 Pairwise Dominance Model
Our example suggests that we may be able to im-
prove the translation model's sensitivity to correct
versus incorrect reordering choices by modeling
the topological ordering of function words. We do
so by introducing a predicate capturing the domi-
nance relationship in a derivation between pairs of
neighboring function words.
5
Let us dene a predicate d(Y ?, Y ??) that takes
two function words as input and outputs one of
3
We use the term ?noun phrase marker? here in a general
sense, meaning that in this example it helps tell us that the
phrase is part of an NP, not as a technical linguistic term. It
serves in other grammatical roles, as well. Disambiguating
the syntactic roles of function words might be a particularly
useful thing to do in the model we are proposing; this is a
question for future research.
4
Note that for expository purposes, we designed our sim-
ple grammar to ensure that these function words appear in
separate rules.
5
Two function words are considered neighbors iff no other
function word appears between them in the source sentence.
326
? Z C? 4 ?{??-
?
XXXXXz
?????9?? ? ?
arecomputers and cell phones inventions of the last century
Figure 1: A running example of Chinese-to-English translation.
Xa???Z Xb, computers andXb?
???Z Xc4 Xd, computers andXc are Xd?
???ZC?4 Xd, computers and cell phones areXd?
???ZC?4 Xe{? , computers and cell phones are inventions ofXe?
???ZC?4??-{? , computers and cell phones are inventions of the last century?
Figure 2: The derivation that leads to the correct translation
Xd??Xa{? , inventions of Xa?
???Z Xb{? , inventions of computers andXb?
???Z Xc4 Xe{? , inventions of computers andXc are Xe?
???ZC?4 Xe{? , inventions of computers and cell phones areXe?
???ZC?4??-{? , inventions of computers and cell phones are the last century?
Figure 3: The derivation that leads to the incorrect translation
four values: {leftFirst, rightFirst, dontCare, nei-
ther}, where Y ? appears to the left of Y ?? in the
source sentence. The value leftFirst indicates that
in the derivation's topological ordering, Y ? pre-
cedes Y ?? (i.e. Y ? dominates Y ?? in the hierarchi-
cal structure), while rightFirst indicates that Y ??
dominates Y ?. In Fig. 2, d(Y ?, Y ??) = leftFirst
for Y ? = the copula 4 (are) and Y ?? = the noun
phrase marker{ (of).
The dontCare and neither values capture two
additional relationships: dontCare indicates that
the topological ordering of the function words is
exible, and neither indicates that the topologi-
cal ordering of the function words is disjoint. The
former is useful in cases where the hierarchical
phrases suggest the same kind of reordering, and
therefore restricting their topological ordering is
not necessary. This is illustrated in Fig. 2 by the
pairZ(and) and the copula 4(are), where putting
either one above the other does not change the -
nal word order. The latter is useful in cases where
the two function words do not share a same parent.
Formally, this model requires several changes in
the design of the hierarchical phrase-based system.
1. To facilitate topological ordering of function
words, the hierarchical phrases must be sub-
categorized with function words. Taking Xb
in Fig. 2 as a case in point, subcategorization
using function words would yield:
6
Xb(4 ?{) ? Xc4 Xd({) (3)
The subcategorization (indicated by the
information in parentheses following the
nonterminal) propagates the function word
4(are) of Xb to the higher level structure to-
gether with the function word {(of) of Xd.
This propagation process generalizes to other
rules by maintaining the ordering of the func-
tion words according to their appearance in
the source sentence. Note that the subcate-
gorized nonterminals often resemble genuine
syntactic categories, for instance X({) can
frequently be interpreted as a noun phrase.
2. To facilitate the computation of the domi-
nance relationship, the coindexing in syn-
chronized rules (indicated by the ? symbol
in Eq. 1) must be expanded to include infor-
mation not only about the nonterminal corre-
spondences but also about the alignment of
the lexical items. For example, adding lexi-
cal alignment information to rule Xd would
yield:
Xd ? ?X1{2?3, inventions3 of2 X1?
(4)
6
The target language side is concealed for clarity.
327
The computation of the dominance relation-
ship using this alignment information will be
discussed in detail in the next section.
Again takingXb in Fig. 2 as a case in point, the
dominance feature takes the following form:
fdom(Xb) ? dom(d(4,{)|4, {)) (5)
dom(d(YL, YR)|YL, YR)) (6)
where the probability of4 ?{ is estimated ac-
cording to the probability of d(4,{).
In practice, both 4(are) and {(of) may ap-
pear together in one same rule. In such a case, a
dominance score is not calculated since the topo-
logical ordering of the two function words is un-
ambiguous. Hence, in our implementation, a
dominance score is only calculated at the points
where the topological ordering of the hierarchical
phrases needs to be resolved, i.e. the two function
words always come from two different hierarchi-
cal phrases.
5 Parameter Estimation
Learning the dominance model involves extract-
ing d values for every pair of neighboring func-
tion words in the training bitext. Such statistics
are not directly observable in parallel corpora, so
estimation is needed. Our estimation method is
based on two facts: (1) the topological ordering
of hierarchical phrases is tightly coupled with the
span of the hierarchical phrases, and (2) the span
of a hierarchical phrase at a higher level is al-
ways a superset of the span of all other hierarchical
phrases at the lower level of its substructure. Thus,
to establish soft estimates of dominance counts,
we utilize alignment information available in the
rule together with the consistent alignment heuris-
tic (Och and Ney, 2004) traditionally used to guess
phrase alignments.
Specically, we dene the span of a function
word as a maximal, consistent alignment in the
source language that either starts from or ends
with the function word. (Requiring that spans be
maximal ensures their uniqueness.) We will re-
fer to such spans as Maximal Consistent Align-
ments (MCA). Note that each function word has
two such Maximal Consistent Alignments: one
that ends with the function word (MCAR)and an-
other that starts from the function word (MCAL).
Y ? Y ?? left- right- dont- nei-
First First Care ther
Z (and) 4 (are) 0.11 0.16 0.68 0.05
4 (are) { (of) 0.57 0.15 0.06 0.22
Table 1: The distribution of the dominance values
of the function words involved in Fig. 1. The value
with the highest probability is in bold.
Given two function words Y ? and Y ??, with Y ?
preceding Y ??, we dene the value of d by exam-
ining the MCAs of the two function words.
d(Y ?, Y ??) =?
?????
?????
leftFirst, Y ? 6? MCAR(Y ??) ? Y ??? MCAL(Y ?)
rightFirst, Y ?? MCAR(Y ??) ? Y ?? 6? MCAL(Y ?)
dontCare, Y ?? MCAR(Y ??) ? Y ??? MCAL(Y ?)
neither, Y ? 6? MCAR(Y ??) ? Y ?? 6? MCAL(Y ?)
(6)
Fig. 4a illustrates the leftFirst dominance value
where the intersection of the MCAs contains only
the second function word ({(of)). Fig. 4b illus-
trates the dontCare value, where the intersection
contains both function words. Similarly, rightFirst
and neither are represented by an intersection that
contains only Y ?, or by an empty intersection, re-
spectively. Once all the d values are counted, the
pairwise dominance model of neighboring func-
tion words can be estimated simply from counts
using maximum likelihood. Table 1 illustrates es-
timated dominance values that correctly resolve
the topological ordering for our running example.
6 Experimental Setup
We tested the effect of introducing the pairwise
dominance model into hierarchical phrase-based
translation on Chinese-to-English and Arabic-to-
English translation tasks, thus studying its effect
in two languages where the use of function words
differs signicantly. Following Setiawan et al
(2007), we identify function words as the N most
frequent words in the corpus, rather than identify-
ing them according to linguistic criteria; this ap-
proximation removes the need for any additional
language-specic resources. We report results
for N = 32, 64, 128, 256, 512, 1024, 2048.7 For
7
We observe that even N = 2048 represents less than
1.5% and 0.8% of the words in the Chinese and Arabic vo-
cabularies, respectively. The validity of the frequency-based
strategy, relative to linguistically-dened function words, is
discussed in Section 8
328
n
a
n
b
j
j
j
z
j
z
j
the last century
of
innovations
are
cell phones
and
computers
?
Z
C
? 4
?
-
 {

?
j
z
j
z
j
j
j
the last century
of
innovations
are
cell phones
and
computers
?
 Z
C
? 4
?
-
{

?
Figure 4: Illustrations for: a) the leftFirst value,
and b) the dontCare value. Thickly bordered
boxes are MCAs of the function words while solid
circles are the alignment points of the function
words. The gray boxes are the intersections of the
two MCAs.
all experiments, we report performance using the
BLEU score (Papineni et al, 2002), and we assess
statistical signicance using the standard boot-
strapping approach introduced by (Koehn, 2004).
Chinese-to-English experiments. We trained
the system on the NIST MT06 Eval corpus ex-
cluding the UN data (approximately 900K sen-
tence pairs). For the language model, we used a 5-
gram model with modied Kneser-Ney smoothing
(Kneser and Ney, 1995) trained on the English side
of our training data as well as portions of the Giga-
word v2 English corpus. We used the NIST MT03
test set as the development set for optimizing inter-
polation weights using minimum error rate train-
ing (MERT; (Och and Ney, 2002)). We carried out
evaluation of the systems on the NIST 2006 eval-
uation test (MT06) and the NIST 2008 evaluation
test (MT08). We segmented Chinese as a prepro-
cessing step using the Harbin segmenter (Zhao et
al., 2001).
Arabic-to-English experiments. We trained
the system on a subset of 950K sentence pairs
from the NIST MT08 training data, selected by
?subsampling? from the full training data using a
method proposed by Kishore Papineni (personal
communication). The subsampling algorithm se-
lects sentence pairs from the training data in a
way that seeks reasonable representation for all n-
grams appearing in the test set. For the language
model, we used a 5-gram model trained on the En-
glish portion of the whole training data plus por-
tions of the Gigaword v2 corpus. We used the
NIST MT03 test set as the development set for
optimizing the interpolation weights using MERT.
We carried out the evaluation of the systems on the
NIST 2006 evaluation set (MT06) and the NIST
2008 evaluation set (MT08). Arabic source text
was preprocessed by separating clitics, the de-
niteness marker, and the future tense marker from
their stems.
7 Experimental Results
Chinese-to-English experiments. Table 2 sum-
marizes the results of our Chinese-to-English ex-
periments. These results conrm that the pairwise
dominance model can signicantly increase per-
formance as measured by the BLEU score, with a
consistent pattern of results across the MT06 and
MT08 test sets. Modeling N = 32 drops the per-
formance marginally below baseline, suggesting
that perhaps there are not enough words for the
pairwise dominance model to work with. Dou-
bling the number of words (N = 64) produces
a small gain, and dening the pairwise dominance
model using N = 128 most frequent words pro-
duces a statistically signicant 1-point gain over
the baseline (p < 0.01). Larger values of N
yield statistically signicant performance above
the baseline, but without further improvements
over N = 128.
Arabic-to-English experiments. Table 3 sum-
marizes the results of our Arabic-to-English ex-
periments. This set of experiments shows a pat-
tern consistent with what we observed in Chinese-
to-English translation, again generally consistent
across MT06 and MT08 test sets although mod-
eling a small number of lexical items (N = 32)
brings a marginal improvement over the baseline.
In addition, we again nd that the pairwise dom-
inance model with N = 128 produces the most
signicant gain over the baseline in the MT06,
although, interestingly, modeling a much larger
number of lexical items (N = 2048) yields the
strongest improvement for the MT08 test set.
329
MT06 MT08
baseline 30.58 24.08
+dom(N = 32) 30.43 23.91
+dom(N = 64) 30.96 24.45
+dom(N = 128) 31.59 24.91
+dom(N = 256) 31.24 24.26
+dom(N = 512) 31.33 24.39
+dom(N = 1024) 31.22 24.79
+dom(N = 2048) 30.75 23.92
Table 2: Experimental results on Chinese-to-
English translation with the pairwise dominance
model (dom) of different N . The baseline (the
rst line) is the original hierarchical phrase-based
system. Statistically signicant results (p < 0.01)
over the baseline are in bold.
MT06 MT08
baseline 41.56 40.06
+dom(N = 32) 41.66 40.26
+dom(N = 64) 42.03 40.73
+dom(N = 128) 42.66 41.08
+dom(N = 256) 42.28 40.69
+dom(N = 512) 41.97 40.95
+dom(N = 1024) 42.05 40.55
+dom(N = 2048) 42.48 41.47
Table 3: Experimental results on Arabic-to-
English translation with the pairwise dominance
model (dom) of different N . The baseline (the
rst line) is the original hierarchical phrase-based
system. Statistically signicant results over the
baseline (p < 0.01) are in bold.
8 Discussion and Future Work
The results in both sets of experiments show con-
sistently that we have achieved a signicant gains
by modeling the topological ordering of function
words. When we visually inspect and compare
the outputs of our system with those of the base-
line, we observe that improved BLEU score often
corresponds to visible improvements in the sub-
jective translation quality. For example, the trans-
lations for the Chinese sentence ?<1 
?2 :3
?4 ?5 ?6 8?7 8 9 ?10 ?11 ?12
?13?, taken from Chinese MT06 test set, are as
follows (co-indexing subscripts represent recon-
structed word alignments):
? baseline: ?military1 intelligence2 un-
der observation8 in5 u.s.6 air raids7 :3 iran4
to9 how11 long12 ?13 ?
? +dom(N=128): ? military1 survey2 :3 how11
long12 iran4 under8 air strikes7 of the u.s6
can9 hold out10 ?13 ?
In addition to some lexical translation errors
(e.g. ?6 should be translated to U.S. Army),
the baseline system also makes mistakes in re-
ordering. The most obvious, perhaps, is its fail-
ure to capture the wh-movement involving the in-
terrogative word ?11 (how); this should move
to the beginning of the translated clause, consis-
tent with English wh-fronting as opposed to Chi-
nese wh in situ. The pairwise dominance model
helps, since the dominance value between the in-
terrogative word and its previous function word,
the modal verb 9(can) in the baseline system's
output, is neither, rather than rightFirst as in the
better translation.
The fact that performance tends to be best us-
ing a frequency threshold of N = 128 strikes
us as intuitively sensible, given what we know
about word frequency rankings.
8
In English,
for example, the most frequent 128 words in-
clude virtually all common conjunctions, deter-
miners, prepositions, auxiliaries, and comple-
mentizers ? the crucial elements of ?syntactic
glue? that characterize the types of linguistic
phrases and the ordering relationships between
them ? and a very small proportion of con-
tent words. Using Adam Kilgarriff's lemma-
tized frequency list from the British National Cor-
pus, http://www.kilgarriff.co.uk/bnc-readme.html,
the most frequent 128 words in English are heav-
ily dominated by determiners, ?functional? ad-
verbs like not and when, ?particle? adverbs like
up, prepositions, pronouns, and conjunctions, with
some arguably ?functional? auxiliary and light
verbs like be, have, do, give, make, take. Con-
tent words are generally limited to a small number
of frequent verbs like think and want and a very
small handful of frequent nouns. In contrast, ranks
129-256 are heavily dominated by the traditional
content-word categories, i.e. nouns, verbs, adjec-
tives and adverbs, with a small number of left-over
function words such as less frequent conjunctions
while, when, and although.
Consistent with these observations for English,
the empirical results for Chinese suggest that our
8
In fact, we initially simply choseN = 128 for our exper-
imentation, and then did runs with alternative N to conrm
our intuitions.
330
approximation of function words using word fre-
quency is reasonable. Using a list of approxi-
mately 900 linguistically identied function words
in Chinese extracted from (Howard, 2002), we ob-
serve that that the performance drops when in-
creasing N above 128 corresponds to a large in-
crease in the number of non-function words used
in the model. For example, with N = 2048, the
proportion of non-function words is 88%, com-
pared to 60% when N = 128.9
One natural extension of this work, therefore,
would be to tighten up our characterization of
function words, whether statistically, distribution-
ally, or simply using manually created resources
that exist for many languages. As a rst step, we
did a version of the Chinese-English experiment
using the list of approximately 900 genuine func-
tion words, testing on the Chinese MT06 set. Per-
haps surprisingly, translation performance, 30.90
BLEU, was around the level we obtained when
using frequency to approximate function words at
N = 64. However, we observe that many of
the words in the linguistically motivated function
word list are quite infrequent; this suggests that
data sparseness may be an additional factor worth
investigating.
Finally, although we believe there are strong
motivations for focusing on the role of function
words in reordering, there may well be value in
extending the dominance model to include content
categories. Verbs and many nouns have subcat-
egorization properties that may inuence phrase
ordering, for example, and this may turn out to ex-
plain the increase in Arabic-English performance
for N = 2048 using the MT08 test set. More gen-
erally, the approach we are taking can be viewed
as a way of selectively lexicalizing the automati-
cally extracted grammar, and there is a large range
of potentially interesting choices in how such lex-
icalization could be done.
9 Related Work
In the introduction, we discussed Chiang's (2005)
constituency feature, related ideas explored by
Marton and Resnik (2008) and Chiang et al
(2008), and the target-side variation investigated
by Zollman et al (2006). These methods differ
from each other mainly in terms of the specic lin-
9
We plan to do corresponding experimentation and anal-
ysis for Arabic once we identify a suitable list of manually
identied function words.
guistic knowledge being used and on which side
the constraints are applied.
Shen et al (2008) proposed to use lin-
guistic knowledge expressed in terms of a de-
pendency grammar, instead of a syntactic con-
stituency grammar. Villar et al (2008) attempted
to use syntactic constituency on both the source
and target languages in the same spirit as the con-
stituency feature, along with some simple pattern-
based heuristics ? an approach also investigated by
Iglesias et al (2009). Aiming at improving the se-
lection of derivations, Zhou et al (2008) proposed
prior derivation models utilizing syntactic annota-
tion of the source language, which can be seen as
smoothing the probabilities of hierarchical phrase
features.
A key point is that the model we have intro-
duced in this paper does not require the linguistic
supervision needed in most of this prior work. We
estimate the parameters of our model from parallel
text without any linguistic annotation. That said,
we would emphasize that our approach is, in fact,
motivated in linguistic terms by the role of func-
tion words in natural language syntax.
10 Conclusion
We have presented a pairwise dominance model
to address reordering issues that are not handled
particularly well by standard hierarchical phrase-
based modeling. In particular, the minimal lin-
guistic commitment in hierarchical phrase-based
models renders them susceptible to overgenera-
tion of reordering choices. Our proposal han-
dles the overgeneration problem by identifying
hierarchical phrases with function words and by
using function word relationships to incorporate
soft constraints on topological orderings. Our
experimental results demonstrate that introducing
the pairwise dominance model into hierarchical
phrase-based modeling improves performance sig-
nicantly in large-scale Chinese-to-English and
Arabic-to-English translation tasks.
Acknowledgments
This research was supported in part by the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-2-001. Any opinions, ndings, conclusions or
recommendations expressed in this paper are those
of the authors and do not necessarily reect the
view of the sponsors.
331
References
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 224?233, Honolulu,
Hawaii, October.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL'05), pages
263?270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jiaying Howard. 2002. A Student Handbook for Chi-
nese Function Words. The Chinese University Press.
Gonzalo Iglesias, Adria de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule ltering by pattern
for efcient hierarchical translation. In Proceedings
of the 12th Conference of the European Chapter of
the Association of Computational Linguistics (to ap-
pear).
R. Kneser and H. Ney. 1995. Improved backing-
off for m-gram language modeling. In Proceed-
ings of IEEE International Conference on Acoustics,
Speech, and Signal Processing95, pages 181?184,
Detroit, MI, May.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 127?133,
Edmonton, Alberta, Canada, May. Association for
Computational Linguistics.
Philipp Koehn. 2004. Statistical signicance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395, Barcelona, Spain,
July.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of The 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1003?
1011, Columbus, Ohio, June.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 295?302, Philadelphia,
Pennsylvania, USA, July.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 712?
719, Prague, Czech Republic, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of The 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 577?585, Columbus,
Ohio, June.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing soft syntax features and heuristics for hi-
erarchical phrase based machine translation. Inter-
national Workshop on Spoken Language Translation
2008, pages 190?197, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Sep.
Tiejun Zhao, Yajuan Lv, Jianmin Yao, Hao Yu, Muyun
Yang, and Fang Liu. 2001. Increasing accuracy
of chinese segmentation with strategy of multi-step
processing. Journal of Chinese Information Pro-
cessing (Chinese Version), 1:13?18.
Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntac-
tic parsing and tree kernels. In Proceedings of
the ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
19?27, Columbus, Ohio, June.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York City,
June.
332
The University of Maryland SENSEVAL-3 System Descriptions
Clara Cabezas, Indrajit Bhattacharya, and Philip Resnik
University of Maryland, College Park, MD 20742 USA
clarac@umiacs.umd.edu, indrajit@cs.umd.edu, resnik@umd.edu
Abstract
For SENSEVAL-3, the University of Maryland
(UMD) team focused on two primary issues: the
portability of sense disambigation across languages,
and the exploitation of real-world bilingual text as a
resource for unsupervised sense tagging. We vali-
dated the portability of our supervised disambigua-
tion approach by applying it in seven tasks (En-
glish, Basque, Catalan, Chinese, Romanian, Span-
ish, and ?multilingual? lexical samples), and we ex-
perimented with a new unsupervised algorithm for
sense modeling using parallel corpora.
1 Supervised Sense Tagging for Lexical
Samples
1.1 Tagging Framework
For the English, Basque, Catalan, Chinese, Roma-
nian, Spanish, and ?multilingual? lexical samples,
we employed the UMD-SST system developed for
SENSEVAL-2 (Cabezas et al, 2001); we refer the
reader to that paper for a detailed system descrip-
tion. Briefly, UMD-SST takes a supervised learning
approach, treating each word in a task?s vocabulary
as an independent problem of classification into that
word?s sense inventory. Each training and test item
is represented as a weighted feature vector, with di-
mensions corresponding to properties of the con-
text. As in SENSEVAL-2, our system supported the
following kinds of features:
  Local context. For each  = 1, 2, and 3, and for
each word  in the vocabulary, there is a fea-
ture  representing the presence of word
 at a distance of  words to the left of the word
being disambigated; there is a corresponding
set of features 	

 for the local context to
the right of the word.
  Wide context. Each word  in the training set
vocabulary has a corresponding feature indi-
cating its presence. For SENSEVAL-3, wide
context features were taken from the entire
training or test instance. In other settings, one
might make further distinctions, e.g. between
words in the same paragraph and words in the
document.
We also experimented with the following additional
kinds of features for English:
  Grammatical context. We use a syntactic de-
pendency parser (Lin, 1998) to produce, for
each word to be disambiguated, features iden-
tifying relevant syntactic relationships in the
sentence where it occurs. For example, in the
sentence The U.S. government announced a
new visa waiver policy, the word government
would have syntactic features like DET:THE,
MOD:U.S., and SUBJ-OF:ANNOUNCED.
  Expanded context. In information retrieval,
we and other researchers have found that it
can be useful to expand the representation of a
document to include informative words from
similar documents (Levow et al, 2001). In
a similar spirit, we create a set of expanded-
context features  by (a) treating the
WSD context as a bag of words, (b) issuing it
as a query to a standard information retrieval
system that has indexed a large collection
of documents, and (c) including the non-
stopword vocabulary of the top  documents
returned. So, for example, in a context
containing the sentence The U.S. government
announced a new visa waiver policy, the query
might retrieve news articles like ?US to Ex-
tend Fingerprinting to Europeans, Japanese?
(Bloomberg.com, April 2, 2004), leading to
the addition of features like EXT:EUROPEAN,
EXT:JAPANESE, EXT:FINGERPRINTING
EXT:VISITORS, EXT:TOURISM, and so forth.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Lexical Sample Coarse (prec/rec) Fine (prec/rec)
UMD-SST 0.643/0.643 0.568/0.568
UMD-SST-gram 0.600/0.600 0.576/0.576
UMD-SST-docexp 0.541/0.542 0.516/0.491
Table 1: UMD-SST variations on the SENSEVAL-2
English lexical sample task
As described by Cabezas et al (2001), we have
adopted the framework of support vector machines
(SVMs) in order to perform supervised classifica-
tion. Because we used a version of SVM learn-
ing designed for binary classification tasks, rather
than the multi-way classification needed for disam-
biguating among   senses, we constructed a family
of SVM classifiers for each word  ? one for each
of the word?s   senses. All positive training ex-
amples for a sense   of  were treated as negative
training examples for all the other senses  , 	  .
Table 1 shows the performance of our approach
on the English lexical sample task from the previ-
ous SENSEVAL exercise (SENSEVAL-2), includ-
ing the basic system (UMD-SST), the basic sys-
tem with grammatical features added (UMD-SST-
gram), and the basic system with document expan-
sion features added (UMD-SST-docexp). (We have
not done a run with both sets of features added.) The
results showed a possible potential benefit for us-
ing grammatical features, in the fine-grained scor-
ing. However, we deemed the benefit too small to
rely upon, and submitted our official SENSEVAL-3
runs using UMD-SST without the grammatical or
document-expansion features.
1.2 SENSEVAL-3 Lexical Sample Tasks
For SENSEVAL-3, the modularity of our system
made it very easy to participate in the many lex-
ical sample tasks, including the multilingual lexi-
cal sample, where the ?sense inventory? consisted
of vocabulary items from a second language.1 In-
deed, we participated in several tasks without hav-
ing anyone on the team who could read the lan-
guage. (Whether or not this was a good idea remains
to be seen.) For Basque, Catalan, and Spanish, we
used the lemmatized word forms provided by the
task organizers; for the other languages, including
English, we used only simple tokenization.
Table 2 shows the UMD-SST system?s official
1Data format problems prevented us from participating in
the Italian lexical sample task.
Lexical Sample Precision (%) Recall (%)
Basque 65.6 58.7
Catalan 81.5 80.3
Chinese 51.3 51.2
English 66.0 66.0
Romanian 70.7 70.7
Spanish 82.5 82.5
Multilingual 58.8 58.8
Table 2: UMD-SST results (fine-grained) on
SENSEVAL-3 lexical sample tasks
Lexical Sample Coarse (prec/rec) Fine (prec/rec)
UMD-SST 0.709/0.709 0.660/0.660
UMD-SST-gram 0.703/0.703 0.655/0.655
UMD-SST-docexp 0.691/0.680 0.637/0.627
Table 3: UMD-SST variations on the SENSEVAL-3
English lexical sample task
SENSEVAL-3 performance on the lexical sample
runs in which we participated, using fine-grained
scores.
In unofficial runs, we also experimented with
the grammatical and document-expansion features.
Table 3 shows the results, which indicate that on this
task the additional features did not help and may
have hurt performance slightly. Although we have
not yet reached any firm conclusions, we conjecture
that value potentially added by these features may
have been offset by the expansion in the size of the
feature space; in future work we plan to explore fea-
ture selection and alternative learning frameworks.
2 Unsupervised Sense Tagging using
Bilingual Text
2.1 Probabilistic Sense Model
For the past several years, the University of Mary-
land group has been exploring unsupervised ap-
proaches to word sense disambiguation that take ad-
vantage of parallel corpora (Diab and Resnik, 2002;
Diab, 2003). Recently, Bhattacharya et al (2004)
(in a UMD/Montreal collaboration) have developed
a variation on this bilingual approach that is in-
spired by the central insight of Diab?s work, but re-
casts it in a probabilistic framework. A generative
model, it is a variant of the graphical model of Ben-
gio and Kermorvant (2003), which groups seman-
tically related words from the two languages into
?senses?; translations are generated by probabilis-
tically choosing a sense and then words from the
sense.
Briefly, the model of Bhattacharya et al uses
probabilistic analysis and independence assump-
tions: it assumes that senses and words have cer-
tain occurrence probabilities and that the choice of
the word can be made independently once the sense
has been decided. Here interaction between dif-
ferent words arising from the same sense comes
into play, even if the words are not related through
translations, and this interdependence of the senses
through common words plays a role in sense disam-
biguation.
The model takes as its starting point the idea of a
?translation pair? ? a pair of words  and   that are
aligned in two sentences (here ?English? and ?non-
English?) that are translations of each other. For
example, in the English-Spanish sentence pair Me
gusta la ciudad/I like the city, one would find the
translation pairs  	 , 
      ,    
 ,
and   Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 83?86,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Improved HMM Alignment Models for Languages with Scarce Resources
Adam Lopez
Institute for Advanced Computer Studies
Department of Computer Science
University of Maryland
College Park, MD 20742
alopez@cs.umd.edu
Philip Resnik
Institute for Advanced Computer Studies
Department of Linguistics
University of Maryland
College Park, MD 20742
resnik@umiacs.umd.edu
Abstract
We introduce improvements to statistical word
alignment based on the Hidden Markov
Model. One improvement incorporates syntac-
tic knowledge. Results on the workshop data
show that alignment performance exceeds that
of a state-of-the art system based on more com-
plex models, resulting in over a 5.5% absolute
reduction in error on Romanian-English.
1 Introduction
The most widely used alignment model is IBM Model 4
(Brown et al, 1993). In empirical evaluations it has out-
performed the other IBM Models and a Hidden Markov
Model (HMM) (Och and Ney, 2003). It was the basis
for a system that performed very well in a comparison
of several alignment systems (Dejean et al, 2003; Mihal-
cea and Pedersen, 2003). Implementations are also freely
available (Al-Onaizan et al, 1999; Och and Ney, 2003).
The IBM Model 4 search space cannot be efficiently
enumerated; therefore it cannot be trained directly using
Expectation Maximization (EM). In practice, a sequence
of simpler models such as IBM Model 1 and an HMM
Model are used to generate initial parameter estimates
and to enumerate a partial search space which can be ex-
panded using hill-climbing heuristics. IBM Model 4 pa-
rameters are then estimated over this partial search space
as an approximation to EM (Brown et al, 1993; Och and
Ney, 2003). This approach yields good results, but it has
been observed that the IBM Model 4 performance is only
slightly better than that of the underlying HMM Model
used in this bootstrapping process (Och and Ney, 2003).
This is illustrated in Figure 1.
Based on this observation, we hypothesize that imple-
mentations of IBM Model 4 derive most of their per-
formance benefits from the underlying HMM Model.
Furthermore, owing to the simplicity of HMM Models,
we believe that they are more conducive to study and
improvement than more complex models such as IBM
Model 4. We illustrate this point by introducing modifi-
cations to the HMM model which improve performance.
.3
.35
.4
.45
.5
.55
.6
.65
.7
Model 1 HMM Model 4
AER
Training Iterations
?
?
? ? ?
? ? ? ? ? ? ? ? ? ? ?
?
?
?
?
? ? ? ? ? ? ? ? ? ?
Figure 1: The improvement in Alignment Error Rate
(AER) is shown for both P(f|e) and P(e|f) alignments on
the Romanian-English development set over several iter-
ations of the IBM Model 1 ? HMM ? IBM Model 4
training sequence.
2 HMMs and Word Alignment
The objective of word alignment is to discover the word-
to-word translational correspondences in a bilingual cor-
pus of S sentence pairs, which we denote {(f(s),e(s)) : s ?
[1,S]}. Each sentence pair (f,e) = ( f M1 ,eN1 ) consists of
a sentence f in one language and its translation e in the
other, with lengths M and N, respectively. By convention
we refer to e as the English sentence and f as the French
sentence. Correspondences in a sentence are represented
by a set of links between words. A link ( f j ,ei) denotes a
correspondence between the ith word ei of e and the jth
word f j of f.
Many alignment models arise from the conditional dis-
tribution P(f|e). We can decompose this by introducing
the hidden alignment variable a = aM1 . Each element of
a takes on a value in the range [1,N]. The value of ai
determines a link between the ith French word fi and
the aith English word eai . This representation introduces
83
an asymmetry into the model because it constrains each
French word to correspond to exactly one English word,
while each English word is permitted to correspond to an
arbitrary number of French words. Although the result-
ing set of links may still be relatively accurate, we can
symmetrize by combining it with the set produced by ap-
plying the complementary model P(e|f) to the same data
(Och and Ney, 2000b). Making a few independence as-
sumptions we arrive at the decomposition in Equation 1. 1
P(f,a|e) =
M
?
i=1
d(ai|ai?1) ? t( fi|eai) (1)
We refer to d(ai|ai?1) as the distortion model and t( fi|eai)
as the translation model. Conveniently, Equation 1 is in
the form of an HMM, so we can apply standard algo-
rithms for HMM parameter estimation and maximization.
This approach was proposed in Vogel et al (1996) and
subsequently improved (Och and Ney, 2000a; Toutanova
et al, 2002).
2.1 The Tree Distortion Model
Equation 1 is adequate in practice, but we can improve
it. Numerous parameterizations have been proposed for
the distortion model. In our surface distortion model, it
depends only on the distance ai ? ai?1 and an automati-
cally determined word class C(eai?1) as shown in Equa-
tion 2. It is similar to (Och and Ney, 2000a). The word
class C(eai?1) is assigned using an unsupervised approach
(Och, 1999).
d(ai|ai?1) = p(ai|ai ?ai?1,C(eai?1)) (2)
The surface distortion model can capture local move-
ment but it cannot capture movement of structures or the
behavior of long-distance dependencies across transla-
tions. The intuitive appeal of capturing richer informa-
tion has inspired numerous alignment models (Wu, 1995;
Yamada and Knight, 2001; Cherry and Lin, 2003). How-
ever, we would like to retain the simplicity and good per-
formance of the HMM Model.
We introduce a distortion model which depends on the
tree distance ?(ei,ek) = (w,x,y) between each pair of En-
glish words ei and ek. Given a dependency parse of eM1 ,
w and x represent the respective number of dependency
links separating ei and ek from their closest common an-
cestor node in the parse tree. 2 The final element y = {1
1We ignore the sentence length probability p(M|N), which
is not relevant to word alignment. We also omit discussion
of HMM start and stop probabilities, and normalization of
t( fi|eai), although we find in practice that attention to these de-
tails can be beneficial.
2The tree distance could easily be adapted to work with
phrase-structure parses or tree-adjoining parses instead of de-
pendency parses.
I1 very2 much3 doubt4 that5
?(I1,very2) = (1,2,0)
?(very2, I1) = (2,1,1)
?(I1,doubt4) = (1,0,0)
?(that5, I1) = (1,1,1)
Figure 2: Example of tree distances in a sentence from
the Romanian-English development set.
if i > k; 0 otherwise} is simply a binary indicator of the
linear relationship of the words within the surface string.
Tree distance is illustrated in Figure 2.
In our tree distortion model, we condition on the tree
distance and the part of speech T (ei?1), giving us Equa-
tion 3.
d(ai|ai?1) = p(ai, |?(eai ,eai?1),T (eai?1)) (3)
Since both the surface distortion and tree distortion
models represent p(ai|ai?1), we can combine them using
linear interpolation as in Equation 4.
d(ai|ai?1) =
?C(eai?1),T (eai?1 )p(ai|?(eai ,eai?1),T (eai?1)) +
(1??C(eai?1),T (eai?1 ))p(ai|ai ?ai?1,C(eai?1))
(4)
The ?C,T parameters can be initialized from a uniform
distribution and trained with the other parameters using
EM. In principle, any number of alternative distortion
models could be combined with this framework.
2.2 Improving Initialization
Our HMM produces reasonable results if we draw our
initial parameter estimates from a uniform distribution.
However, we can do better. We estimate the initial
translation probability t( f j |ei) from the smoothed log-
likelihood ratio LLR(ei, f j)?1 computed over sentence
cooccurrences. Since this method works well, we apply
LLR(ei, f j) in a single reestimation step shown in Equa-
tion 5.
t( f |e) = LLR( f |e)
?2 +n
?e? LLR( f |e?)?2 +n ? |V |
(5)
In reestimation LLR( f |e) is computed from the expected
counts of f and e produced by the EM algorithm. This is
similar to Moore (2004); as in that work, |V | = 100,000,
and ?1, ?2, and n are estimated on development data.
We can also use an improved initial estimate for distor-
tion. Consider a simple distortion model p(ai|ai ?ai?1).
We expect this distribution to have a maximum near
P(ai|0) because we know that words tend to retain their
locality across translation. Rather than wait for this to
occur, we use an initial estimate for the distortion model
given in Equation 6.
84
corpus n ?1 ?2 ? symmetrization n?1 ??11 ??12 ??1
English-Inuktitut 1?4 1.0 1.75 -1.5 ? 5?4 1.0 1.75 -1.5
Romanian-English 5?4 1.5 1.0 -2.5 refined (Och and Ney, 2000b) 5?4 1.5 1.0 -2.5
English-Hindi 1?4 1.5 3.0 -2.5 ? 1?2 1.0 1.0 -1.0
Table 1: Training parameters for the workshop data (see Section 2.2). Parameters n, ?1, ?2, and ? were used in the
initialization of P(f|e) model, while n?1, ??11 , ??12 , and ??1 were used in the initialization of the P(e|f) model.
corpus type HMM limited (Eq. 2) HMM unlimited (Eq. 4) IBM Model 4P R AER P R AER P R AER
English-Inuktitut
P(f|e) .4962 .6894 .4513 ? ? ? .4211 .6519 .5162
P(e|f) .5789 .8635 .3856 ? ? ? .5971 .8089 .3749
? .8916 .6280 .2251 ? ? ? .8682 .5700 .2801
English-Hindi
P(f|e) .5079 .4769 .5081 .5057 .4748 .5102 .5219 .4223 .5332
P(e|f) .5566 .4429 .5067 .5566 .4429 .5067 .5652 .3939 .5358
? .4408 .5649 .5084 .4365 .5614 .5088 .4543 .5401 .5065
Romanian-English
P(f|e) .6876 .6233 .3461 .6876 .6233 .3461 .6828 .5414 .3961
P(e|f) .7168 .6217 .3341 .7155 .6205 .3354 .7520 .5496 .3649
refined .7377 .6169 .3281 .7241 .6215 .3311 .7620 .5134 .3865
Table 2: Results on the workshop data. The systems highlighted in bold are the ones that were used in the shared task.
For each corpus, the last row shown represents the results that were actually submitted. Note that for English-Hindi,
our self-reported results in the unlimited task are slightly lower than the original results submitted for the workshop,
which contained an error.
d(ai|ai?1) =
{
|ai ?ai?1|?/Z,? < 0 if ai 6= ai?1.
1/Z if ai = ai?1.
(6)
We choose Z to normalize the distribution. We must
optimize ? on a development set. This distribution has
a maximum when |ai ? ai?1| ? {?1,0,1}. Although we
could reasonably choose any of these three values as the
maximum for the initial estimate, we found in develop-
ment that the maximum of the surface distortion distribu-
tion varied with C(eai?1), although it was always in the
range [?1,2].
2.3 Does NULL Matter in Asymmetric Alignment?
Och and Ney (2000a) introduce a NULL-alignment ca-
pability to the HMM alignment model. This allows any
word f j to link to a special NULL word ? by conven-
tion denoted e0 ? instead of one of the words eN1 . A link
( f j ,e0) indicates that f j does not correspond to any word
in e. This improved alignment performance in the ab-
sence of symmetrization, presumably because it allows
the model to be conservative when evidence for an align-
ment is lacking.
We hypothesize that NULL alignment is unnecessary
for asymmetric alignment models when we symmetrize
using intersection-based methods (Och and Ney, 2000b).
The intuition is simple: if we don?t permit NULL align-
ments, then we expect to produce a high-recall, low-
precision alignment; the intersection of two such align-
ments should mainly improve precision, resulting in a
high-recall, high-precision alignment. If we allow NULL
alignments, we may be able produce a high-precision,
low-recall asymmetric alignment, but symmetrization by
intersection will not improve recall.
3 Results with the Workshop Data
In our experiments, the dependency parse and parts of
speech are produced by minipar (Lin, 1998). This parser
has been used in a much different alignment model
(Cherry and Lin, 2003). Since we only had parses for
English, we did not use tree distortion in the application
of P(e|f), needed for symmetrization.
The parameter settings that we used in aligning the
workshop data are presented in Table 1. Although our
prior work with English and French indicated that in-
tersection was the best method for symmetrization, we
found in development that this varied depending on the
characteristics of the corpus and the type of annotation
(in particular, whether the annotation set included proba-
ble alignments). The results are summarized in Table 2.
It shows results with our HMM model using both Equa-
tions 2 and 4 as our distortion model, which represent
85
the unlimited and limited resource tracks, respectively.
It also includes a comparison with IBM Model 4, for
which we use a training sequence of IBM Model 1 (5
iterations), HMM (6 iterations), and IBM Model 4 (5 it-
erations). This sequence performed well in an evaluation
of the IBM Models (Och and Ney, 2003).
For comparative purposes, we show results of apply-
ing both P(f|e) and P(e|f) prior to symmetrization, along
with results of symmetrization. Comparison of the asym-
metric and symmetric results largely supports the hypoth-
esis presented in Section 2.3, as our system generally pro-
duces much better recall than IBM Model 4, while of-
fering a competitive precision. Our symmetrized results
usually produced higher recall and precision, and lower
alignment error rate.
We found that the largest gain in performance came
from the improved initialization. The combined distor-
tion model (Equation 4), which provided a small benefit
over the surface distortion model (Equation 2) on the de-
velopment set, performed slightly worse on the test set.
We found that the dependencies on C(eai?1) and
T (eai?1) were harmful to the P(f|e) alignment for Inukti-
tut, and did not submit results for the unlimited resources
configuration. However, we found that alignment was
generally difficult for all models on this particular task,
perhaps due to the agglutinative nature of Inuktitut.
4 Conclusions
We have proposed improvements to the largely over-
looked HMM word alignment model. Our improvements
yield good results on the workshop data. We have addi-
tionally shown that syntactic information can be incorpo-
rated into such a model; although the results are not su-
perior, they are competitive with surface distortion. In fu-
ture work we expect to explore additional parameteriza-
tions of the HMM model, and to perform extrinsic evalu-
ations of the resulting alignments by using them in the pa-
rameter estimation of a phrase-based translation model.
Acknowledgements
This research was supported in part by ONR MURI Con-
tract FCPO.810548265. The authors would like to thank
Bill Byrne, David Chiang, Okan Kolak, and the anony-
mous reviewers for their helpful comments.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz Josef Och,
David Purdy, Noah A. Smith, and David Yarowsky.
1999. Statistical machine translation: Final report. In
Johns Hopkins University 1999 Summer Workshop on
Language Engineering.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311, Jun.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In ACL Proceed-
ings, Jul.
Herve Dejean, Eric Gaussier, Cyril Goutte, and Kenji
Yamada. 2003. Reducing parameter space for word
alignment. In Proceedings of the Workshop on Build-
ing and Using Parallel Texts: Data Driven Machine
Translation and Beyond, pages 23?26, May.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on the Eval-
uation of Parsing Systems, May.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Proceedings of the
Workshop on Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond, pages 1?10,
May.
Robert C. Moore. 2004. Improving IBM word-
alignment model 1. In ACL Proceedings, pages 519?
526, Jul.
Franz Josef Och and Hermann Ney. 2000a. A compari-
son of alignment models for statistical machine trans-
lation. In COLING Proceedings, pages 1086?1090,
Jul.
Franz Josef Och and Hermann Ney. 2000b. Improved
statistical alignment models. In ACL Proceedings,
pages 440?447, Oct.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison on various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In EACL Proceedings,
pages 71?76, Jun.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to hmm-based statistical
word alignment models. In EMNLP, pages 87?94, Jul.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. Hmm-based word alignment in statistical ma-
chine translation. In COLING Proceedings, pages
836?841, Aug.
Dekai Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation, bracket-
ing, and alignment of parallel corpora. In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 1328?1335, Aug.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL Proceedings.
86
Proceedings of the Second Workshop on Statistical Machine Translation, pages 120?127,
Prague, June 2007. c?2007 Association for Computational Linguistics
Using Paraphrases for Parameter Tuning in Statistical Machine Translation
Nitin Madnani, Necip Fazil Ayan, Philip Resnik & Bonnie J. Dorr
Institute for Advanced Computer Studies
University of Maryland
College Park, MD, 20742
{nmadnani,nfa,resnik,bonnie}@umiacs.umd.edu
Abstract
Most state-of-the-art statistical machine
translation systems use log-linear models,
which are defined in terms of hypothesis fea-
tures and weights for those features. It is
standard to tune the feature weights in or-
der to maximize a translation quality met-
ric, using held-out test sentences and their
corresponding reference translations. How-
ever, obtaining reference translations is ex-
pensive. In this paper, we introduce a new
full-sentence paraphrase technique, based
on English-to-English decoding with an MT
system, and we demonstrate that the result-
ing paraphrases can be used to drastically re-
duce the number of human reference transla-
tions needed for parameter tuning, without a
significant decrease in translation quality.
1 Introduction
Viewed at a very high level, statistical machine
translation involves four phases: language and trans-
lation model training, parameter tuning, decoding,
and evaluation (Lopez, 2007; Koehn et al, 2003).
Since their introduction in statistical MT by Och and
Ney (2002), log-linear models have been a standard
way to combine sub-models in MT systems. Typi-
cally such a model takes the form
?
i
?i?i(f? , e?) (1)
where ?i are features of the hypothesis e and ?i are
weights associated with those features.
Selecting appropriate weights ?i is essential
in order to obtain good translation performance.
Och (2003) introduced minimum error rate train-
ing (MERT), a technique for optimizing log-linear
model parameters relative to a measure of translation
quality. This has become much more standard than
optimizing the conditional probability of the train-
ing data given the model (i.e., a maximum likelihood
criterion), as was common previously. Och showed
that system performance is best when parameters are
optimized using the same objective function that will
be used for evaluation; BLEU (Papineni et al, 2002)
remains common for both purposes and is often re-
tained for parameter optimization even when alter-
native evaluation measures are used, e.g., (Banerjee
and Lavie, 2005; Snover et al, 2006).
Minimum error rate training?and more gener-
ally, optimization of parameters relative to a trans-
lation quality measure?relies on data sets in which
source language sentences are paired with (sets of)
reference translations. It is widely agreed that, at
least for the widely used BLEU criterion, which is
based on n-gram overlap between hypotheses and
reference translations, the criterion is most accu-
rate when computed with as many distinct reference
translations as possible. Intuitively this makes sense:
if there are alternative ways to phrase the meaning
of the source sentence in the target language, then
the translation quality criterion should take as many
of those variations into account as possible. To do
otherwise is to risk the possibility that the criterion
might judge good translations to be poor when they
fail to match the exact wording within the reference
translations that have been provided.
This reliance on multiple reference translations
creates a problem, because reference translations are
labor intensive and expensive to obtain. A com-
mon source of translated data for MT research is the
Linguistic Data Consortium (LDC), where an elab-
orate process is undertaken that involves translation
agencies, detailed translation guidelines, and qual-
ity control processes (Strassel et al, 2006). Some
120
efforts have been made to develop alternative pro-
cesses for eliciting translations, e.g., from users on
the Web (Oard, 2003) or from informants in low-
density languages (Probst et al, 2002). However,
reference translations for parameter tuning and eval-
uation remain a severe data bottleneck for such ap-
proaches.
Note, however, one crucial property of reference
translations: they are paraphrases, i.e., multiple ex-
pressions of the same meaning. Automatic tech-
niques exist for generating paraphrases. Although
one would clearly like to retain human transla-
tions as the benchmark for evaluation of translation,
might it be possible to usefully increase the number
of reference translations for tuning by using auto-
matic paraphrase techniques?
In this paper, we demonstrate that it is, in fact,
possible to do so. Section 2 briefly describes our
translation framework. Section 3 lays out a novel
technique for paraphrasing, designed with the ap-
plication to parameter tuning in mind. Section 4
presents evaluation results using a state of the art sta-
tistical MT system, demonstrating that half the hu-
man reference translations in a standard 4-reference
tuning set can be replaced with automatically gener-
ated paraphrases, with no significant decrease in MT
system performance. In Section 5 we discuss related
work, and in Section 6 we summarize the results and
discuss plans for future research.
2 Translation Framework
The work described in this paper makes use
of the Hiero statistical MT framework (Chiang,
2007). Hiero is formally based on a weighted syn-
chronous context-free grammar (CFG), containing
synchronous rules of the form
X ? ?e?, f? , ?k1(f? , e?, X)? (2)
where X is a symbol from the nonterminal alpha-
bet, and e? and f? can contain both words (terminals)
and variables (nonterminals) that serve as placehold-
ers for other phrases. In the context of statistical
MT, where phrase-based models are frequently used,
these synchronous rules can be interpreted as pairs
of hierarchical phrases. The underlying strength
of a hierarchical phrase is that it allows for effec-
tive learning of not only the lexical re-orderings, but
phrasal re-orderings, as well. Each ?(e?, f? , X) de-
notes a feature function defined on the pair of hierar-
chical phrases.1 Feature functions represent condi-
tional and joint co-occurrence probabilities over the
hierarchical paraphrase pair.
The Hiero framework includes methods to learn
grammars and feature values from unannotated par-
allel corpora, without requiring syntactic annotation
of the data. Briefly, training a Hiero model proceeds
as follows:
? GIZA++ (Och and Ney, 2000) is run on the
parallel corpus in both directions, followed by
an alignment refinement heuristic that yields a
many-to-many alignment for each parallel sen-
tence.
? Initial phrase pairs are identified following the
procedure typically employed in phrase based
systems (Koehn et al, 2003; Och and Ney,
2004).
? Grammar rules in the form of equation (2)
are induced by ?subtracting? out hierarchical
phrase pairs from these initial phrase pairs.
? Fractional counts are assigned to each pro-
duced rule:
c(X ? ?e?, f??) =
m?
j=1
1
njr
(3)
where m is the number of initial phrase pairs
that give rise to this grammar rule and njr is
the number of grammar rules produced by the
jth initial phrase pair.
? Feature functions ?k1(f? , e?, X) are calculated
for each rule using the accumulated counts.
Once training has taken place, minimum error rate
training (Och, 2003) is used to tune the parameters
?i.
Finally, decoding in Hiero takes place using a
CKY synchronous parser with beam search, aug-
mented to permit efficient incorporation of language
model scores (Chiang, 2007). Given a source lan-
guage sentence f, the decoder parses the source lan-
guage sentence using the grammar it has learned
1Currently only one nonterminal symbol is used in Hiero
productions.
121
during training, with parser search guided by the
model; a target-language hypothesis is generated
simultaneously via the synchronous rules, and the
yield of that hypothesized analysis represents the hy-
pothesized string e in the target language.
3 Generating Paraphrases
As discussed in Section 1, our goal is to make it pos-
sible to accomplish the parameter-tuning phase us-
ing fewer human reference translations. We accom-
plish this by beginning with a small set of human
reference translations for each sentence in the devel-
opment set, and expanding that set by automatically
paraphrasing each member of the set rather than by
acquiring more human translations.
Most previous work on paraphrase has focused
on high quality rather than coverage (Barzilay and
Lee, 2003; Quirk et al, 2004), but generating ar-
tificial references for MT parameter tuning in our
setting has two unique properties compared to other
paraphrase applications. First, we would like to ob-
tain 100% coverage, in order to avoid modifications
to our minimum error rate training infrastructure.2
Second, we prefer that paraphrases be as distinct as
possible from the original sentences, while retaining
as much of the original meaning as possible.
In order to satisfy these two properties, we ap-
proach sentence-level paraphrase for English as
a problem of English-to-English translation, con-
structing the model using English-F translation, for
a second language F , as a pivot. Following Ban-
nard and Callison-Burch (2005), we first identify
English-to-F correspondences, then map from En-
glish to English by following translation units from
English to F and back. Then, generalizing their ap-
proach, we use those mappings to create a well de-
fined English-to-English translation model. The pa-
rameters of this model are tuned using MERT, and
then the model is used in an the (unmodified) sta-
tistical MT system, yielding sentence-level English
paraphrases by means of decoding input English
sentences. The remainder of this section presents
this process in detail.
2Strictly speaking, this was not a requirement of the ap-
proach, but rather a concession to practical considerations.
3.1 Mapping and Backmapping
We employ the following strategy for the induction
of the required monolingual grammar. First, we train
the Hiero system in standard fashion on a bilingual
English-F training corpus. Then, for each exist-
ing production in the resulting Hiero grammar, we
create multiple new English-to-English productions
by pivoting on the foreign hierarchical phrase in the
rule. For example, assume that we have the follow-
ing toy grammar for English-F , as produced by Hi-
ero:
X ? ?e?1, f?1?
X ? ?e?3, f?1?
X ? ?e?1, f?2?
X ? ?e?2, f?2?
X ? ?e?4, f?2?
If we use the foreign phrase f?1 as a pivot and
backmap, we can extract the two English-to-English
rules: X ? ?e?1, e?3? and X ? ?e?3, e?1?. Backmap-
ping using both f?1 and f?2 produces the following
new rules (ignoring duplicates and rules that map
any English phrase to itself):
X ? ?e?1, e?2?
X ? ?e?1, e?3?
X ? ?e?1, e?4?
X ? ?e?2, e?1?
X ? ?e?2, e?4?
3.2 Feature values
Each rule production in a Hiero grammar is
weighted by several feature values defined on the
rule themselves. In order to perform accurate
backmapping, we must recompute these feature
functions for the newly created English-to-English
grammar. Rather than computing approximations
based on feature values already existing in the bilin-
gual Hiero grammar, we calculate these features
in a more principled manner, by computing max-
imum likelihood estimates directly from the frac-
tional counts that Hiero accumulates in the penul-
timate training step.
We use the following features in our induced
English-to-English grammar:3
3Hiero also uses lexical weights (Koehn et al, 2003) in both
122
? The joint probability of the two English hierar-
chical paraphrases, conditioned on the nonter-
minal symbol, as defined by this formula:
p(e?1, e?2|x) =
c(X ? ?e?1, e?2?)
?
e?1?,e?2? c(X ? ?e?1
?, e?2??)
=
c(X ? ?e?1, e?2?)
c(X)
(4)
where the numerator is the fractional count of
the rule under consideration and the denomina-
tor represents the marginal count over all the
English hierarchical phrase pairs.
? The conditionals p(e?1, x|e?2) and p(e?2, x|e?1)
defined as follows:
p(e?1, x|e?2) =
c(X ? ?e?1, e?2?)
?
e?1? c(X ? ?e?1
?, e?2?)
(5)
p(e?2, x|e?1) =
c(X ? ?e?1, e?2?)
?
e?2? c(X ? ?e?1, e?2
??)
(6)
Finally, for all induced rules, we calculate a word
penalty exp(?T (e?2)), where T (e?2) just counts the
number of terminal symbols in e?2. This feature al-
lows the model to learn whether it should produce
shorter or longer paraphrases.
In addition to the features above that are estimated
from the training data, we also use a trigram lan-
guage model. Since we are decoding to produce
English sentences, we can use the same language
model employed in a standard statistical MT setting.
Calculating the proposed features is complicated
by the fact that we don?t actually have the counts
for English-to-English rules because there is no
English-to-English parallel corpus. This is where
the counts provided by Hiero come into the picture.
We estimate the counts that we need as follows:
c(X ? ?e?1, e?2?) =
?
f?
c(X ? ?e?1, f??)c(X ? ?e?2, f??) (7)
An intuitive way to think about the formula above
is by using an example at the corpus level. As-
sume that, in the given bilingual parallel corpus,
there are m sentences in which the English phrase
directions as features but we don?t use them for our grammar.
e?1 co-occurs with the foreign phrase f? and n sen-
tences in which the same foreign phrase f? co-occurs
with the English phrase e?2. The problem can then
be thought of as defining a function g(m,n) which
computes the number of sentences in a hypotheti-
cal English-to-English parallel corpus wherein the
phrases e?1 and e?1 co-occur. For this paper, we de-
fine g(m,n) to be the upper bound mn.
Tables 1 and 2 show some examples of para-
phrases generated by our system across a range of
paraphrase quality for two different pivot languages.
3.3 Tuning Model Parameters
Although the goal of the paraphrasing approach
is to make it less data-intensive to tune log-linear
model parameters for translation, our paraphrasing
approach, since it is based on an English-to-English
log-linear model, also requires its own parameter
tuning. This, however, is straightforward: regard-
less of how the paraphrasing model will be used
in statistical MT, e.g., irrespective of source lan-
guage, it is possible to use any existing set of English
paraphrases as the tuning set for English-to-English
translation. We used the 2002 NIST MT evaluation
test set reference translations. For every item in the
set, we randomly chose one sentence as the source
sentence, and the remainder as the ?reference trans-
lations? for purposes of minimum error rate training.
4 Evaluation
Having developed a paraphrasing approach based on
English-to-English translation, we evaluated its use
in improving minimum error rate training for trans-
lation from a second language into English.
Generating paraphrases via English-to-English
translation makes use of a parallel corpus, from
which a weighted synchronous grammar is automat-
ically acquired. Although nothing about our ap-
proach requires that the paraphrase system?s training
bitext be the same one used in the translation exper-
iments (see Section 6), doing so is not precluded, ei-
ther, and it is a particularly convenient choice when
the paraphrasing is being done in support of MT.4
The training bitext comprised of Chinese-English
4The choice of the foreign language used as the pivot should
not really matter but it is worth exploring this using other lan-
guage pairs as our bitext.
123
O: we must bear in mind the community as a whole .
P: we must remember the wider community .
O: thirdly , the implications of enlargement for the union ?s regional policy cannot be overlooked .
P: finally , the impact of enlargement for eu regional policy cannot be ignored .
O: how this works in practice will become clear when the authority has to act .
P: how this operate in practice will emerge when the government has to play .
O: this is an ill-advised policy .
P: this is an unwelcome in europe .
Table 1: Example paraphrases with French as the pivot language. O = Original Sentence, P = Paraphrase.
O: alcatel added that the company?s whole year earnings would be announced on february 4 .
P: alcatel said that the company?s total annual revenues would be released on february 4 .
O: he was now preparing a speech concerning the us policy for the upcoming world economic forum .
P: he was now ready to talk with regard to the us policies for the forthcoming international economic forum .
O: tibet has entered an excellent phase of political stability, ethnic unity and people living in peace .
P: tibetans have come to cordial political stability, national unity and lived in harmony .
O: its ocean and blue-sky scenery and the mediterranean climate make it world?s famous scenic spot .
P: its harbour and blue-sky appearance and the border situation decided it world?s renowned tourist attraction .
Table 2: Example paraphrases with Chinese as the pivot language. O = Original Sentence, P = Paraphrase.
Corpus # Sentences # Words
HK News 542540 11171933
FBIS 240996 9121210
Xinhua 54022 1497562
News1 9916 314121
Treebank 3963 125848
Total 851437 22230674
Table 3: Chinese-English corpora used as training
bitext both for paraphrasing and for evaluation.
parallel corpora containing 850, 000 sentence pairs ?
approx. 22 million words (details shown in Table 3).
As the source of development data for minimum
error rate training, we used the 919 source sen-
tences and human reference translations from the
2003 NIST Chinese-English MT evaluation exer-
cise. As raw material for experimentation, we gen-
erated a paraphrase for each reference sentence via
1-best decoding using the English-to-English trans-
lation approach of Section 3.
As our test data, we used the 1082 source sen-
tences and human reference translations from the
2005 NIST Chinese-English MT evaluation.
Our core experiment involved three conditions
where the only difference was the set of references
for the development set used for tuning feature
weights. For each condition, once the weights were
tuned, they were used to decode the test set. Note
that for all the conditions, the decoded test set was
always scored against the same four high-quality hu-
man reference translations included with the set.
The three experimental conditions were designed
around the constraint that our development set con-
tains a total of four human reference translations per
sentence, and therefore a maximum of four human
references with which to compute an upper bound:
? Baseline (2H): For each item in the devel-
opment set, we randomly chose two of the
four human-constructed reference translations
as references for minimum error rate training.
? Expanded (2H + 2P): For each of the two hu-
man references in the baseline tuning set, we
automatically generated a corresponding para-
phrase using (1-best) English-to-English trans-
lation, decoding using the model developed in
Section 3. This condition represents the critical
case in which you have a limited number of hu-
124
man references (two, in this case) and augment
them with artificially generated reference trans-
lations. This yields a set of four references for
minimum error rate training (two human, two
paraphrased), which permits a direct compar-
ison against the upper bound of four human-
generated reference translations.
? Upper bound: 4H: We performed minimum
error rate training using the four human refer-
ences from the development set.
In addition to these core experimental conditions,
we added a fourth condition to assess the effect on
performance when all four human reference trans-
lations are used in expanding the reference set via
paraphrase:
? Expanded (4H + 4P): This is the same as Con-
dition 2, but using all four human references.
Note that since we have only four human references
per item, this fourth condition does not permit com-
parison with an upper bound of eight human refer-
ences.
Table 4 shows BLEU and TER scores on the test
set for all four conditions.5 If only two human ref-
erences were available (simulated by using only two
of the available four), expanding to four using para-
phrases would yield a clear improvement. Using
bootstrap resampling to compute confidence inter-
vals (Koehn, 2004), we find that the improvement in
BLEU score is statistically significant at p < .01.
Equally interesting, expanding the number of ref-
erence translations from two to four using para-
phrases yields performance that approaches the up-
per bound obtained by doing MERT using all four
human reference translations. The difference in
BLEU between conditions 2 and 3 is not significant.
Finally, our fourth condition asks whether it is
possible to improve MT performance given the
typical four human reference translations used for
MERT in most statistical MT systems, by adding a
paraphrase to each one for a total eight references
per translation. There is indeed further improve-
ment, although the difference in BLEU score does
not reach significance.
5We plan to include METEOR scores in future experiments.
Condition References used BLEU TER
1 2 H 30.43 59.82
2 2 H + 2 P 31.10 58.79
3 4 H 31.26 58.66
4 4 H + 4 P 31.68 58.24
Table 4: BLEU and TER scores showing utility of
paraphrased reference translations. H = human ref-
erences, P = paraphrased references.
We also evaluated our test set using TER (Snover
et al, 2006) and observed that the TER scores follow
the same trend as the BLEU scores. Specifically, the
TER scores demonstrate that using paraphrases to
artificially expand the reference set is better than us-
ing only 2 human reference translations and as good
as using 4 human reference translations.6
5 Related Work
The approach we have taken here arises from a typ-
ical situation in NLP systems: the lack of sufficient
data to accurately estimate a model based on super-
vised training data. In a structured prediction prob-
lem such as MT, we have an example input and a
single labeled, correct output. However, this output
is chosen from a space in which the number of pos-
sible outputs is exponential in the input size, and in
which there are many good outputs in this space (al-
though they are vastly outnumbered by the bad out-
puts). Various discriminative learning methods have
attempted to deal with the first of these issues, often
by restricting the space of examples. For instance,
some max-margin methods restrict their computa-
tions to a set of examples from a ?feasible set,?
where they are expected to be maximally discrim-
inative (Tillmann and Zhang, 2006). The present
approach deals with the second issue: in a learning
problem where the use of a single positive example
is likely to be highly biased, how can we produce a
set of positive examples that is more representative
of the space of correct outcomes? Our method ex-
ploits alternative sources of information to produce
new positive examples that are, we hope, reasonably
likely to represent a consensus of good examples.
Quite a bit of work has been done on paraphrase,
6We anticipate doing significance tests for differences in
TER in future work.
125
some clearly related to our technique, although in
general previous work has been focused on human
readability rather than high coverage, noisy para-
phrases for use downstream in an automatic process.
At the sentence level, (Barzilay and Lee, 2003)
employed an unsupervised learning approach to
cluster sentences and extract lattice pairs from
comparable monolingual corpora. Their technique
produces a paraphrase only if the input sentence
matches any of the extracted lattice pairs, leading to
a bias strongly favoring quality over coverage. They
were able to generate paraphrases for 59 sentences
(12%) out of a 484-sentence test set, generating no
paraphrases at all for the remainder.
Quirk et al (2004) also generate sentential para-
phrases using a monolingual corpus. They use
IBM Model-1 scores as the only feature, and em-
ploy a monotone decoder (i.e., one that cannot pro-
duce phrase-level reordering). This approach em-
phasizes very simple ?substitutions of words and
short phrases,? and, in fact, almost a third of their
best sentential ?paraphrases? are identical to the in-
put sentence.
A number of other approaches rely on parallel
monolingual data and, additionally, require pars-
ing of the training sentences (Ibrahim et al, 2003;
Pang et al, 2003). Lin and Pantel (2001) use a
non-parallel corpus and employ a dependency parser
and computation of distributional similarity to learn
paraphrases.
There has also been recent work on using para-
phrases to improve statistical machine translation.
Callison-Burch et al (2006) extract phrase-level
paraphrases by mapping input phrases into a phrase
table and then mapping back to the source language.
However, they do not generate paraphrases of entire
sentences, but instead employ paraphrases to add en-
tries to an existing phrase table solely for the pur-
pose of increasing source-language coverage.
Other work has incorporated paraphrases into MT
evaluation: Russo-Lassner et al (2005) use a com-
bination of paraphrase-based features to evaluate
translation output; Zhou et al (2006) propose a new
metric that extends n-gram matching to include syn-
onyms and paraphrases; and Lavie?s METEOR met-
ric (Banerjee and Lavie, 2005) can be used with ad-
ditional knowledge such as WordNet in order to sup-
port inexact lexical matches.
6 Conclusions and Future Work
We introduced an automatic paraphrasing technique
based on English-to-English translation of full sen-
tences using a statistical MT system, and demon-
strated that, using this technique, it is possible to
cut in half the usual number of reference transla-
tions used for minimum error rate training with no
significant loss in translation quality. Our method
enables the generation of paraphrases for thousands
of sentences in a very short amount of time (much
shorter than creating other low-cost human refer-
ences). This might prove beneficial for various dis-
criminative training methods (Tillmann and Zhang,
2006).
This has important implications for data acquisi-
tion strategies For example, it suggests that rather
than obtaining four reference translations per sen-
tence for development sets, it may be more worth-
while to obtain fewer translations for a wider range
of sentences, e.g., expanding into new topics and
genres. In addition, this approach can significantly
increase the utility of datasets which include only a
single reference translation.
A number of future research directions are pos-
sible. First, since we have already demonstrated
that noisy paraphrases can nonetheless add value,
it would be straightforward to explore the quan-
tity/quality tradeoff by expanding the MERT refer-
ence translations with n-best paraphrases for n > 1.
We also plan to conduct an intrinsic evaluation of
the quality of paraphrases that our technique gener-
ates. It is important to note that a different tradeoff
ratio may lead to even better results, e.g, using only
the paraphrased references when they pass some
goodness threshold, as used in Ueffing?s (2006) self-
training MT approach.
We have also observed that named entities are
usually paraphrased incorrectly if there is a genre
mismatch between the training and the test data. The
Hiero decoder allows spans of source text to be an-
notated with inline translations using XML. We plan
to identify and annotate named entities in the En-
glish source so that they are left unchanged.
Also, since the languageF for English-F pivoting
is arbitrary, we plan to investigate using English-to-
English grammars created using multiple English-F
grammars based on different languages, both indi-
126
vidually and in combination, in order to improve
paraphrase quality.
We also plan to explore a wider range of
paraphrase-creation techniques, ranging from sim-
ple word substitutions (e.g., based on WordNet) to
using the pivot technique with other translations sys-
tems.
7 Acknowledgments
We are indebted to David Chiang, Adam Lopez
and Smaranda Muresan for insights and comments.
This work has been supported under the GALE pro-
gram of the Defense Advaned Research Projects
Agency, Contract No. HR0011-06-2-001. Any opin-
ions, findings, conclusions or recommendations ex-
pressed in this paper are those of the authors and do
not necessarily reflect the view of DARPA.
References
S. Banerjee and A. Lavie. 2005. Meteor: An auto-
matic metric for mt evaluation with improved correla-
tion with human judgments. In Proceedings of Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization at ACL.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of HLT-NAACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
A. Ibrahim, B. Katz, and J. Lin. 2003. Extracting struc-
tural paraphrases from aligned monolingual corpora.
In Proceedings the Second International Workshop on
Paraphrasing (ACL 2003).
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT-NAACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
A. Lopez. 2007. A survey of statistical machine transla-
tion. Technical Report 2006-47, University of Mary-
land, College Park.
D. W. Oard. 2003. The surprise langauge exercises.
ACM Transactions on Asian Language Information
Processing, 2(3).
Franz J. Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of ACL.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL.
Franz Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Com-
putational Linguistics, 30(4).
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
K. Probst, L. Levin, E. Peterson, A. Lavie, and J. Car-
bonell. 2002. Mt for minority languages using
elicitation-based learning of syntactic transfer rules.
Machine Translation, 17(4).
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of EMNLP 2004.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
2005. A paraphrase-based approach to machine trans-
lation evaluation. Technical Report UMIACS-TR-
2005-57, University of Maryland, College Park.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of AMTA.
S. Strassel, C. Cieri, A. Cole, D. DiPersio, M. Liberman,
X. Ma, M. Maamouri, and K. Maeda. 2006. Inte-
grated linguistic resources for language exploitation
technologies. In Proceedings of LREC.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
Proceedings of ACL.
Nicola Ueffing. 2006. Using monolingual source-
language data to improve MT performance. In Pro-
ceedings of IWSLT.
L. Zhou, C.-Y. Lin, D. Muntenau, and E. Hovy. 2006.
ParaEval: Using paraphrases to evaluate summaries
automatically. In Proceedings of HLT-NAACL.
127
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 326?333,
Prague, June 2007. c?2007 Association for Computational Linguistics
TOR, TORMD: Distributional Profiles of Concepts
for Unsupervised Word Sense Disambiguation
Saif Mohammad
Dept. of Computer Science
University of Toronto
Toronto, ON M5S 3G4
Canada
smm@cs.toronto.edu
Graeme Hirst
Dept. of Computer Science
University of Toronto
Toronto, ON M5S 3G4
Canada
gh@cs.toronto.edu
Philip Resnik
Dept. of Linguistics and UMIACS
University of Maryland
College Park, MD 20742
USA
resnik@umiacs.umd.edu
Abstract
Words in the context of a target word
have long been used as features by su-
pervised word-sense classifiers. Moham-
mad and Hirst (2006a) proposed a way to
determine the strength of association be-
tween a sense or concept and co-occurring
words?the distributional profile of a con-
cept (DPC)?without the use of manually
annotated data. We implemented an unsu-
pervised na??ve Bayes word sense classifier
using these DPCs that was best or within
one percentage point of the best unsuper-
vised systems in the Multilingual Chinese?
English Lexical Sample Task (task #5) and
the English Lexical Sample Task (task #17).
We also created a simple PMI-based classi-
fier to attempt the English Lexical Substi-
tution Task (task #10); however, its perfor-
mance was poor.
1 Introduction
Determining the intended sense of a word is poten-
tially useful in many natural language tasks includ-
ing machine translation and information retrieval.
The best approaches for word sense disambiguation
are supervised and they use words that co-occur with
the target as features. These systems rely on sense-
annotated data to identify words that are indicative
of the use of the target in each of its senses.
However, only limited amounts of sense-
annotated data exist and it is expensive to create. In
our previous work (Mohammad and Hirst, 2006a),
we proposed an unsupervised approach to determine
the strength of association between a sense or con-
cept and its co-occurring words?the distributional
profile of a concept (DPC)?relying simply on raw
text and a published thesaurus. The categories in a
published thesaurus were used as coarse senses or
concepts (Yarowsky, 1992). We now show how dis-
tributional profiles of concepts can be used to cre-
ate an unsupervised na??ve Bayes word-sense classi-
fier. We also implemented a simple classifier that
relies on the pointwise mutual information (PMI)
between the senses of the target and co-occurring
words. These DPC-based classifiers participated in
three SemEval 2007 tasks: the English Lexical Sam-
ple Task (task #17), the English Lexical Substitu-
tion Task (task #10), and the Multilingual Chinese?
English Lexical Sample Task (task #5).
The English Lexical Sample Task (Pradhan et al,
2007) is a traditional word sense disambiguation
task wherein the intended (WordNet) sense of a tar-
get word is to be determined from its context. We
manually mapped the WordNet senses to the cate-
gories in a thesaurus and the DPC-based na??ve Bayes
classifier was used to identify the intended sense
(category) of the target words.
The object of the Lexical Substitution Task (Mc-
Carthy and Navigli, 2007) is to replace a target word
in a sentence with a suitable substitute that preserves
the meaning of the utterance. The list of possible
substitutes for a given target word is usually contin-
gent on its intended sense. Therefore, word sense
disambiguation is expected to be useful in lexical
substitution. We used the PMI-based classier to de-
termine the intended sense.
326
The objective of the Multilingual Chinese?
English Lexical Sample Task (Jin et al, 2007) is to
select from a given list a suitable English translation
of a Chinese target word in context. Mohammad et
al. (2007) proposed a way to create cross-lingual
distributional profiles of a concepts (CL-DPCs)?
the strengths of association between the concepts of
one language and words of another. For this task, we
mapped the list of English translations to appropri-
ate thesaurus categories and used an implementation
of a CL-DPC?based unsupervised na??ve Bayes clas-
sifier to identify the intended senses (and thereby the
English translations) of target Chinese words.
2 Distributional profiles of concepts
In order to determine the strength of association be-
tween a sense of the target word and its co-occurring
words, we need to determine their individual and
joint occurrence counts in a corpus. Mohammad and
Hirst (2006a) and Mohammad et al (2007) proposed
ways to determine these counts in a monolingual and
cross-lingual framework without the use of sense-
annotated data. We summarize the ideas in this sec-
tion; the original papers give more details.
2.1 Word?category co-occurrence matrix
We create a word?category co-occurrence matrix
(WCCM) having English word types wen as one di-
mension and English thesaurus categories cen as an-
other. We used the Macquarie Thesaurus (Bernard,
1986) both as a very coarse-grained sense inventory
and a source of words that together represent each
category (concept). The WCCM is populated with
co-occurrence counts from a large English corpus
(we used the British National Corpus (BNC)). A par-
ticular cell mi j, corresponding to word weni and con-
cept cenj , is populated with the number of times weni
co-occurs with any word that has cenj as one of its
senses (i.e., weni co-occurs with any word listed un-
der concept cenj in the thesaurus).
cen1 c
en
2 . . . c
en
j . . .
wen1 m11 m12 . . . m1 j . . .
wen2 m21 m22 . . . m2 j . . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
weni mi1 mi2 . . . mi j . . .
.
.
.
.
.
.
.
.
. . . .
.
.
.
.
.
.
A particular cell mi j, corresponding to word weni
and concept cenj , is populated with the number of
times weni co-occurs with any word that has cenj
as one of its senses (i.e., weni co-occurs with any
word listed under concept cenj in the thesaurus).
This matrix, created after a first pass of the corpus,
is the base word?category co-occurrence matrix
(base WCCM) and it captures strong associations
between a sense and co-occurring words (see dis-
cussion of the general principle in Resnik (1998)).
From the base WCCM we can determine the num-
ber of times a word w and concept c co-occur, the
number of times w co-occurs with any concept, and
the number of times c co-occurs with any word. A
statistic such as PMI can then give the strength of
association between w and c. This is similar to how
Yarowsky (1992) identifies words that are indicative
of a particular sense of the target word.
Words that occur close to a target word tend to
be good indicators of its intended sense. Therefore,
we make a second pass of the corpus, using the base
WCCM to roughly disambiguate the words in it. For
each word, the strength of association of each of
the words in its context (?5 words) with each of its
senses is summed. The sense that has the highest cu-
mulative association is chosen as the intended sense.
A new bootstrapped WCCM is created such that
each cell mi j, corresponding to word weni and con-
cept cenj , is populated with the number of times weni
co-occurs with any word used in sense cenj .
Mohammad and Hirst (2006a) used the DPCs
created from the bootstrapped WCCM to attain
near-upper-bound results in the task of determin-
ing word sense dominance. Unlike the McCarthy
et al (2004) dominance system, this approach can
be applied to much smaller target texts (a few
hundred sentences) without the need for a large
similarly-sense-distributed text1. Mohammad and
Hirst (2006b) used the DPC-based monolingual dis-
tributional measures of concept-distance to rank
word pairs by their semantic similarity and to correct
real-word spelling errors, attaining markedly better
results than monolingual distributional measures of
word-distance. In the spelling correction task, the
1The McCarthy et al (2004) system needs to first gener-
ate a distributional thesaurus from the target text (if it is large
enough?a few million words) or from another large text with a
distribution of senses similar to the target text.
327
Figure 1: The cross-lingual candidate senses of Chi-
nese words and .
distributional concept-distance measures performed
better than all WordNet-based measures as well, ex-
cept for the Jiang and Conrath (1997) measure.
2.2 Cross-lingual word?category
co-occurrence matrix
Given a Chinese word wch in context, we use a
Chinese?English bilingual lexicon to determine its
different possible English translations. Each En-
glish translation wen may have one or more possi-
ble coarse senses, as listed in an English thesaurus.
These English thesaurus concepts (cen) will be re-
ferred to as cross-lingual candidate senses of the
Chinese word wch.2 Figure 1 depicts examples.
We create a cross-lingual word?category co-
occurrence matrix (CL-WCCM) with Chinese word
types wch as one dimension and English thesaurus
concepts cen as another.
cen1 c
en
2 . . . c
en
j . . .
wch1 m11 m12 . . . m1 j . . .
wch2 m21 m22 . . . m2 j . . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
wchi mi1 mi2 . . . mi j . . .
.
.
.
.
.
.
.
.
. . . .
.
.
.
.
.
.
The matrix is populated with co-occurrence counts
from a large Chinese corpus; we used a collection of
LDC-distributed corpora3?Chinese Treebank En-
glish Parallel Corpus, FBIS data, Xinhua Chinese?
English Parallel News Text Version 1.0 beta 2, Chi-
nese English News Magazine Parallel Text, Chinese
2Some of the cross-lingual candidate senses of wch might not
really be senses of wch (e.g., ?celebrity?, ?practical lesson?, and
?state of the atmosphere? in Figure 1). However, as substanti-
ated by experiments by Mohammad et al (2007), our algorithm
is able to handle the added ambiguity.
3http://www.ldc.upenn.edu
Figure 2: Chinese words having ?celestial body? as
one of their cross-lingual candidate senses.
News Translation Text Part 1, and Hong Kong Paral-
lel Text. A particular cell mi j, corresponding to word
wchi and concept cenj , is populated with the number
of times the Chinese word wchi co-occurs with any
Chinese word having cenj as one of its cross-lingual
candidate senses. For example, the cell for
(?space?) and ?celestial body? will have the sum of
the number of times co-occurs with , ,
, , , and so on (see Figure 2). We used
the Macquarie Thesaurus (Bernard, 1986) (about
98,000 words). The possible Chinese translations
of an English word were taken from the Chinese?
English Translation Lexicon version 3.0 (Huang and
Graff, 2002) (about 54,000 entries).
This base word?category co-occurrence matrix
(base WCCM), created after a first pass of the cor-
pus, captures strong associations between a cate-
gory (concept) and co-occurring words. For ex-
ample, even though we increment counts for both
??celestial body? and ??celebrity? for a par-
ticular instance where co-occurs with ,
will co-occur with a number of words such as
, , and that each have the sense of ce-
lestial body in common (see Figure 2), whereas all
their other senses are likely different and distributed
across the set of concepts. Therefore, the co-
occurrence count of and ?celestial body? will
be relatively higher than that of and ?celebrity?.
As in the monolingual case, a second pass of
the corpus is made to disambiguate the (Chinese)
words in it. For each word, the strength of associ-
ation of each of the words in its context (?5 words)
with each of its cross-lingual candidate senses is
summed. The sense that has the highest cumula-
tive association with co-occurring words is chosen
as the intended sense. A new bootstrapped WCCM
is created by populating each cell mi j, correspond-
ing to word wchi and concept cenj , with the number of
times the Chinese word wchi co-occurs with any Chi-
328
nese word used in cross-lingual sense cenj . A statistic
such as PMI is then applied to these counts to deter-
mine the strengths of association between a target
concept and co-occurring words, giving the distri-
butional profile of the concept.
Mohammad et al (2007) combined German text
with an English thesaurus using a German?English
bilingual lexicon to create German?English DPCs.
These DPCs were used to determine semantic dis-
tance between German words, showing that state-of-
the-art accuracies for one language can be achieved
using a knowledge source (thesaurus) from another.
Given that a published thesaurus has about 1000
categories and the size of the vocabulary N is at
least 100,000, the CL-WCCM and the WCCM are
much smaller matrices (about 1000?N) than the tra-
ditional word?word co-occurrence matrix (N ?N).
Therefore the WCCMs are relatively inexpensive
both in terms of memory and computation.
3 Classification
We implemented two unsupervised classifiers. The
words in context were used as features.
3.1 Unsupervised Na??ve Bayes Classifier
The na??ve Bayes classifier has the following formula
to determine the intended sense cnb:
cnb = argmax
c j?C
P(c j) ?
wi?W
P(wi|c j) (1)
where C is the set of possible senses (as listed in
the Macquarie Thesaurus) and W is the set of words
that co-occur with the target (we used a window of
?5 words).
Traditionally, prior probabilities of the senses
(P(c j)) and the conditional probabilities in the like-
lihood (?wi?W P(wi|c j)) are determined by sim-
ple counts in sense-annotated data. We approx-
imate these probabilities using counts from the
word?category co-occurrence matrix (monolingual
or cross-lingual), thereby obviating the need for
manually-annotated data.
P(c j) =
?i mi j
?i, j mi j
(2)
P(wi|c j) =
mi j
?i mi j
(3)
For the English Lexical Task, mi j is the number of
times the English word wi co-occurs with the En-
glish category c j?as listed in the word?category
co-occurrence matrix (WCCM). For the Multilin-
gual Chinese?English Lexical Task, mi j is the num-
ber of times the Chinese word wi co-occurs with the
English category c j?as listed in the cross-lingual
word?category co-occurrence matrix (CL-WCCM).
3.2 PMI-based classifier
We calculate the pointwise mutual information be-
tween a sense of the target word and a co-occurring
word using the following formula:
PMI(wi,c j) = log
P(wi,c j)
P(wi)?P(c j)
(4)
where P(wi,c j) =
mi j
?i, j mi j
(5)
and P(wi) =
? j mi j
?i, j mi j
(6)
mi j is the count in the WCCM or CL-WCCM (as de-
scribed in the previous subsection). For each sense
of the target, the sum of the strength of association
(PMI) between it and each of the co-occurring words
(in a window of ?5 words) is calculated. The sense
with the highest sum is chosen as the intended sense.
cpmi = argmax
c j?C
?
wi?W
PMI(wi,c j) (7)
Note that this PMI-based classifier does not capital-
ize on prior probabilities of the different senses.
4 Data
4.1 English Lexical Sample Task
The English Lexical Sample Task training and test
data (Pradhan et al, 2007) have 22281 and 4851
instances respectively for 100 target words (50
nouns and 50 verbs). WordNet 2.1 is used as
the sense inventory for most of the target words,
but certain words have one or more senses from
OntoNotes (Hovy et al, 2006). Many of the fine-
grained senses are grouped into coarser senses.
Our approach relies on representing a sense with
a number of near-synonymous words, for which a
thesaurus is a natural source. Even though the ap-
proach can be ported to WordNet4, there was no easy
4The synonyms within a synset, along with its one-hop
neighbors and all its hyponyms, can represent that sense.
329
TRAINING DATA TEST DATA
WORDS BASELINE PMI-BASED NAI?VE BAYES PRIOR LIKELIHOOD NAI?VE BAYES
all 27.8 41.4 50.8 37.4 49.4 52.1
nouns only 25.6 43.4 53.6 18.1 49.6 49.7
verbs only 29.2 38.4 44.5 58.9 49.1 54.7
Table 1: English Lexical Sample Task: Results obtained using the PMI-based classifier on the training data
and the na??ve Bayes classifier on both training and test data
way of representing OntoNotes senses with near-
synonymous words. Therefore, we asked four na-
tive speakers of English to map the WordNet and
OntoNotes senses of the 100 target words to the
Macquarie Thesaurus and use it as our sense inven-
tory. We also wanted to examine the effect of using
a very coarse sense inventory such as the categories
in a published thesaurus (811 in all).
The annotators were presented with a target word,
its WordNet/OntoNotes senses, and the Macquarie
senses. WordNet senses were represented by syn-
onyms, gloss, and example usages. The OntoNotes
senses were described through syntactic patterns and
example usages (provided by the task organizers).
The Macquarie senses (categories) were described
by the category head (a representative word for
the category) and five other words in the category.
Specifically, words in the same semicolon group5 as
the target were chosen. Annotators 1 and 2 labeled
each WordNet/OntoNotes sense of the first 50 target
words with one or more appropriate Macquarie cat-
egories. Annotators 3 and 4 labeled the senses of the
other 50 words. We combined all four annotations
into a WordNet?Macquarie mapping file by taking,
for each target word, the union of categories chosen
by the two annotators.
4.2 English Lexical Substitution Task
The English Lexical Substitution Task has 1710 test
instances for 171 target words (nouns, verbs, adjec-
tives, and adverbs) (McCarthy and Navigli, 2007).
Some instances were randomly extracted from an
Internet corpus, whereas others were selected man-
ually from it. The target word might or might not be
part of a multiword expression. The task is not tied
to any particular sense inventory.
5Words within a semicolon group of a thesaurus tend to be
more closely related than words across groups.
4.3 Multilingual Chinese?English Lexical
Sample Task
The Multilingual Chinese?English Lexical Sample
Task training and test data (Jin et al, 2007) have
2686 and 935 instances respectively for 40 target
words (19 nouns and 21 verbs). The instances are
taken from a corpus of People?s Daily News. The
organizers used the Chinese Semantic Dictionary
(CSD), developed by the Institute of Computational
Linguistics, Peking University, both as a sense in-
ventory and bilingual lexicon (to extract a suitable
English translation of the target word once the in-
tended Chinese sense is determined).
In order to determine the English translations of
Chinese words in context, our system relies on Chi-
nese text and an English thesaurus. As the thesaurus
is used as our sense inventory, the first author and a
native speaker of Chinese mapped the English trans-
lations of the target to appropriate Macquarie cate-
gories. We used three examples (from the training
data) per English translation for this purpose.
5 Evaluation
5.1 English Lexical Sample Task
Both the na??ve Bayes classifier and the PMI-based
one were applied to the training data. For each in-
stance, the Macquarie category c that best captures
the intended sense of the target was determined. The
instance was labeled with all the WordNet senses
that are mapped to c in the WordNet?Macquarie
mapping file (described earlier in Section 4.1).
5.1.1 Results
Table 1 shows the performances of the two clas-
sifiers. The system attempted to label all instances
and so we report accuracy values instead of pre-
cision and recall. The na??ve Bayes classifier per-
formed markedly better in training than the PMI-
330
based one and so was applied to the test data. The
table also lists baseline results obtained when a sys-
tem randomly guesses one of the possible senses for
each target word. Note that since this is a com-
pletely unsupervised system, it is not privy to the
dominant sense of the target words. We do not rely
on the ranking of senses in WordNet as that would
be an implicit use of the sense-tagged SemCor cor-
pus. Therefore, the most-frequent-sense baseline
does not apply. Table 1 also shows results obtained
using just the prior probability and likelihood com-
ponents of the na??ve Bayes formula. Note that the
combined accuracy is higher than individual com-
ponents for nouns but not for verbs.
5.1.2 Discussion
The na??ve Bayes classifier?s accuracy is only
about one percentage point lower than that of the
best unsupervised system taking part in the task
(Pradhan et al, 2007). One reason that it does bet-
ter than the PMI-based one is that it takes into ac-
count prior probabilities of the categories. However,
using just the likelihood also outperforms the PMI
classifier. This may be because of known problems
of using PMI with low frequencies (Manning and
Schu?tze, 1999). In case of verbs, lower combined
accuracies compared to when using just prior proba-
bilities suggests that the bag-of-words type features
are not very useful. It is expected that more syntac-
tically oriented features will give better results. Us-
ing window sizes (?1,?2, and ?10) on the training
data resulted in lower accuracies than that obtained
using a window of ?5 words. A smaller window
size is probably missing useful co-occurring words,
whereas a larger window size is adding words that
are not indicative of the target?s intended sense.
The use of a sense inventory (Macquarie The-
saurus) different from that used to label the data
(WordNet) clearly will have a negative impact on
the results. The mapping from WordNet/OntoNotes
to Macquarie is likely to have some errors. Further,
for 19 WordNet/OntoNotes senses, none of the an-
notators found a thesaurus category close enough in
meaning. This meant that our system had no way
of correctly disambiguating instances with these
senses. Also impacting accuracy is the significantly
fine-grained nature of WordNet compared to the the-
saurus. For example, following are the three coarse
BEST OOT
Acc Mode Acc Acc Mode Acc
all 2.98 4.72 11.19 14.63
Further Analysis
NMWT 3.22 5.04 11.77 15.03
NMWS 3.32 4.90 12.22 15.26
RAND 3.10 5.20 9.98 13.00
MAN 2.84 4.17 12.61 16.49
Table 2: English Lexical Substitution Task: Results
obtained using the PMI-based classifier
senses for the noun president in WordNet: (1) exec-
utive officer of a firm or college, (2) the chief exec-
utive of a republic, and (3) President of the United
States. The last two senses will fall into just one cat-
egory for most, if not all, thesauri.
5.2 English Lexical Substitution Task
We used the PMI-based classifier6 for the English
Lexical Substitution Task. Once it identifies a suit-
able thesaurus category as the intended sense for a
target, ten candidate substitutes are chosen from that
category. Specifically, the category head word and
up to nine words in the same semicolon group as the
target are selected (words within a semicolon group
are closer in meaning). Of the ten candidates, the
single-word expression that is most frequent in the
BNC is chosen as the best substitute; the motivation
is that the annotators, who created the gold standard,
were instructed to give preference to single words
over multiword expressions as substitutes.
5.2.1 Results
The system was evaluated not only on the best
substitute (BEST) but also on how good the top ten
candidate substitutes are (OOT). Table 2 presents the
results.7 The system attempted all instances. The
table also lists performances of the system on in-
stances where the target is not part of a multiword
expression (NMWT), on instances where the substi-
tute is not a multiword expression (NMWS), on in-
stances randomly extracted from the corpus (RAND),
and on instances manually selected (MAN).
6Due to time constraints, we were able to upload results only
with the PMI-based classifier by the task deadline.
7The formulae for accuracy and mode accuracy are as de-
scribed by Pradhan et al (2007).
331
TRAINING DATA TEST DATA
BASELINE PMI-BASED NAI?VE BAYES PRIOR LIKELIHOOD NAI?VE BAYES
WORDS micro macro micro macro micro macro micro macro micro macro micro macro
all 33.1 38.3 33.9 40.0 38.5 44.7 35.4 41.7 38.8 44.6 37.5 43.1
nouns only 41.9 43.5 43.6 45.0 49.4 50.5 45.3 47.1 48.1 50.8 50.0 51.6
verbs only 28.0 34.1 28.0 35.6 31.9 39.6 29.1 36.8 32.9 39.0 29.6 35.5
Table 3: Multilingual Chinese?English Lexical Sample Task: Results obtained using the PMI-based classi-
fier on the training data and the na??ve Bayes classifier on both training and test data
5.2.2 Discussion
Competitive performance of our DPC-based sys-
tem on the English Lexical Sample Task and the
Chinese?English Lexical Sample Task (see next
subsection) suggests that DPCs are useful for sense
disambiguation. Poor results on the substitution task
can be ascribed to several factors. First, we used
the PMI-based classifier that we found later to be
markedly less accurate than the na??ve Bayes clas-
sifier in the other two tasks. Second, the words in
the thesaurus categories may not always be near-
synonyms; they might just be strongly related. Such
words will be poor substitutes for the target. Also,
we chose as the best substitute simply the most fre-
quent of the ten candidates. This simple technique
is probably not accurate enough. On the other hand,
because we chose the candidates without any regard
to frequency in a corpus, the system chose certain
infrequent words such as wellnigh and ecchymosed,
which were not good candidate substitutes.
5.3 Multilingual Chinese?English Lexical
Sample Task
In the Multilingual Chinese?English Lexical Sample
Task, both the na??ve Bayes classifier and the PMI-
based classifier were applied to the training data.
For each instance, the Macquarie category, say c,
that best captures the intended sense of the target
word is determined. Then the instance is labeled
with all the English translations that are mapped to c
in the English translations?Macquarie mapping file
(described earlier in Section 4.3).
5.3.1 Results
Table 3 shows accuracies of the two classifiers.
Macro average is the ratio of number of instances
correctly disambiguated to the total, whereas micro
average is the average of the accuracies achieved
on each target word. As in the English Lexical
Sample Task, both classifiers, especially the na??ve
Bayes classifier, perform well above the random
baseline. Since the na??ve Bayes classifier also per-
formed markedly better than the PMI-based one in
training, it was applied to the test data. Table 3
also shows results obtained using just the likelihood
and prior probability components of the na??ve Bayes
classifier on the test data.
5.3.2 Discussion
Our na??ve Bayes classifier scored highest of all
unsupervised systems taking part in the task (Jin et
al., 2007). As in the English Lexical Sample Task,
using just the likelihood again outperforms the PMI
classifier on the training data. The use of a sense
inventory different from that used to label the data
again will have a negative impact on the results as
the mapping may have a few errors. The anno-
tator believed none of the given Macquarie cate-
gories could be mapped to two Chinese Semantic
Dictionary senses. This meant that our system had
no way of correctly disambiguating instances with
these senses.
There were also a number of cases where more
than one CSD sense of a word was mapped to the
same Macquarie category. This occurred for two
reasons: First, the categories of the Macquarie The-
saurus act as very coarse senses. Second, for cer-
tain target words, the two CSD senses may be differ-
ent in terms of their syntactic behavior, yet semanti-
cally very close (for example, the ?be shocked? and
?shocked? senses of ). This many-to-one map-
ping meant that for a number of instances more than
one English translation was chosen. Since the task
required us to provide exactly one answer (and there
was no partial credit in case of multiple answers), a
category was chosen at random.
332
6 Conclusion
We implemented a system that uses distributional
profiles of concepts (DPCs) for unsupervised word
sense disambiguation. We used words in the con-
text as features. Specifically, we used the DPCs
to create a na??ve Bayes word-sense classifier and a
simple PMI-based classifier. Our system attempted
three SemEval-2007 tasks. On the training data
of the English Lexical Sample Task (task #17) and
the Multilingual Chinese?English Lexical Sample
Task (task #5), the na??ve Bayes classifier achieved
markedly better results than the PMI-based classi-
fier and so was applied to the respective test data.
On both test and training data of both tasks, the
system achieved accuracies well above the random
baseline. Further, our system placed best or close to
one percentage point from the best among the unsu-
pervised systems. In the English Lexical Substitu-
tion Task (task #10), for which there was no train-
ing data, we used the PMI-based classifier. The
system performed poorly, which is probably a re-
sult of using the weaker classifier and a simple brute
force method for identifying the substitute among
the words in a thesaurus category. Markedly higher-
than-baseline performance of the na??ve Bayes clas-
sifier on task #17 and task #5 suggests that the DPCs
are useful for word sense disambiguation.
Acknowledgments
We gratefully acknowledge Xiaodan Zhu, Michael Demko,
Christopher Parisien, Frank Rudicz, and Timothy Fowler for
mapping training-data labels to categories in the Macquarie
Thesaurus. We thank Michael Demko, Siddharth Patwardhan,
Xinglong Wang, Vivian Tsang, and Afra Alishahi for helpful
discussions. This research is financially supported by the Natu-
ral Sciences and Engineering Research Council of Canada, the
University of Toronto, ONR MURI Contract FCPO.810548265
and Department of Defense contract RD-02-5700.
References
J.R.L. Bernard, editor. 1986. The Macquarie Thesaurus.
Macquarie Library, Sydney, Australia.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL, pages 57?60, New York, NY.
Shudong Huang and David Graff. 2002. Chinese?
english translation lexicon version 3.0. Linguistic
Data Consortium.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In Proceedings of International Conference on
Research on Computational Linguistics, Taiwan.
Peng Jin, Yunfang Wu, and Shiwen Yu. 2007. SemEval-
2007 task 05: Multilingual Chinese-English lexical
sample task. In Proceedings of the Fourth Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, Prague, Czech Republic.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge, Massachusetts.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 task 10: English lexical substitution task. In
Proceedings of the Fourth International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text (SemEval-2007), Prague, Czech Republic.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-04), pages 280?267, Barcelona, Spain.
Saif Mohammad and Graeme Hirst. 2006a. Determining
word sense dominance using a thesaurus. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), Trento, Italy.
Saif Mohammad and Graeme Hirst. 2006b. Distribu-
tional measures of concept-distance: A task-oriented
evaluation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2006), Sydney, Australia.
Saif Mohammad, Iryna Gurevych, Graeme Hirst, and
Torsten Zesch. 2007. Cross-lingual distributional
profiles of concepts for measuring semantic dis-
tance. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP/CoNLL-2007), Prague, Czech Republic.
Sameer Pradhan, Martha Palmer, and Edward Loper.
2007. SemEval-2007 task 17: English lexical sample,
English SRL and English all-words tasks. In Proceed-
ings of the Fourth International Workshop on the Eval-
uation of Systems for the Semantic Analysis of Text
(SemEval-2007), Prague, Czech Republic.
Philip Resnik. 1998. Wordnet and class-based prob-
abilities. In Christiane Fellbaum, editor, WordNet:
An Electronic Lexical Database, pages 239?263. The
MIT Press, Cambridge, Massachusetts.
David Yarowsky. 1992. Word-sense disambiguation us-
ing statistical models of Roget?s categories trained on
large corpora. In Proceedings of the 14th International
Conference on Computational Linguistics (COLING-
92), pages 454?460, Nantes, France.
333
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 145?149,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The University of Maryland Statistical Machine Translation System for
the Fourth Workshop on Machine Translation
Chris Dyer??, Hendra Setiawan?, Yuval Marton??, and Philip Resnik??
?UMIACS Laboratory for Computational Linguistics and Information Processing
?Department of Linguistics
University of Maryland, College Park, MD 20742, USA
{redpony,hendra,ymarton,resnik} AT umd.edu
Abstract
This paper describes the techniques we
explored to improve the translation of
news text in the German-English and
Hungarian-English tracks of the WMT09
shared translation task. Beginning with a
convention hierarchical phrase-based sys-
tem, we found benefits for using word seg-
mentation lattices as input, explicit gen-
eration of beginning and end of sentence
markers, minimum Bayes risk decoding,
and incorporation of a feature scoring the
alignment of function words in the hy-
pothesized translation. We also explored
the use of monolingual paraphrases to im-
prove coverage, as well as co-training to
improve the quality of the segmentation
lattices used, but these did not lead to im-
provements.
1 Introduction
For the shared translation task of the Fourth Work-
shop on Machine Translation (WMT09), we fo-
cused on two tasks: German to English and Hun-
garian to English translation. Despite belonging to
different language families, German and Hungar-
ian have three features in common that complicate
translation into English:
1. productive compounding (especially of
nouns),
2. rich inflectional morphology,
3. widespread mid- to long-range word order
differences with respect to English.
Since these phenomena are poorly addressed with
conventional approaches to statistical machine
translation, we chose to work primarily toward
mitigating their negative effects when construct-
ing our systems. This paper is structured as fol-
lows. In Section 2 we describe the baseline model,
Section 3 describes the various strategies we em-
ployed to address the challenges just listed, and
Section 4 summarizes the final translation system.
2 Baseline system
Our translation system makes use of a hierarchical
phrase-based translation model (Chiang, 2007),
which we argue is a strong baseline for these
language pairs. First, such a system makes use
of lexical information when modeling reorder-
ing (Lopez, 2008), which has previously been
shown to be useful in German-to-English trans-
lation (Koehn et al, 2008). Additionally, since
the decoder is based on a CKY parser, it can con-
sider all licensed reorderings of the input in poly-
nomial time, and German and Hungarian may re-
quire quite substantial reordering. Although such
decoders and models have been common for sev-
eral years, there have been no published results for
these language pairs.
The baseline system translates lowercased and
tokenized source sentences into lowercased target
sentences. The features used were the rule transla-
tion relative frequency P (e?|f?), the ?lexical? trans-
lation probabilities Plex(e?|f?) and Plex(f? |e?), a rule
count, a target language word count, the target
(English) language model P (eI1), and a ?pass-
through? penalty for passing a source language
word to the target side.1 The rule feature values
were computed online during decoding using the
suffix array method described by Lopez (2007).
1The ?pass-through? penalty was necessary since the En-
glish language modeling data contained a large amount of
source-language text.
145
2.1 Training and development data
To construct the translation suffix arrays used to
compute the translation grammar, we used the par-
allel training data provided. The preprocessed
training data was filtered for length and aligned
using the GIZA++ implementation of IBM Model
4 (Och and Ney, 2003) in both directions and sym-
metrized using the grow-diag-final-and
heuristic. We trained a 5-gram language model
from the provided English monolingual training
data and the non-Europarl portions of the parallel
training data using modified Kneser-Ney smooth-
ing as implemented in the SRI language modeling
toolkit (Kneser and Ney, 1995; Stolcke, 2002). We
divided the 2008 workshop ?news test? sets into
two halves of approximately 1000 sentences each
and designated one the dev set and the other the
dev-test set.
2.2 Automatic evaluation metric
Since the official evaluation criterion for WMT09
is human sentence ranking, we chose to minimize
a linear combination of two common evaluation
metrics, BLEU and TER (Papineni et al, 2002;
Snover et al, 2006), during system development
and tuning:
TER ? BLEU
2
Although we are not aware of any work demon-
strating that this combination of metrics correlates
better than either individually in sentence ranking,
Yaser Al-Onaizan (personal communication) re-
ports that it correlates well with the human evalua-
tion metric HTER. In this paper, we report uncased
TER and BLEU individually.
2.3 Forest minimum error training
To tune the feature weights of our system, we used
a variant of the minimum error training algorithm
(Och, 2003) that computes the error statistics from
the target sentences from the translation search
space (represented by a packed forest) that are ex-
actly those that are minimally discriminable by
changing the feature weights along a single vector
in the dimensions of the feature space (Macherey
et al, 2008). The loss function we used was the
linear combination of TER and BLEU described in
the previous section.
3 Experimental variations
This section describes the experimental variants
explored.
3.1 Word segmentation lattices
Both German and Hungarian have a large number
of compound words that are created by concate-
nating several morphemes to form a single ortho-
graphic token. To deal with productive compound-
ing, we employ word segmentation lattices, which
are word lattices that encode alternative possible
segmentations of compound words. Doing so en-
ables us to use possibly inaccurate approaches to
guess the segmentation of compound words, al-
lowing the decoder to decide which to use during
translation. This is a further development of our
general source-lattice approach to decoding (Dyer
et al, 2008).
To construct the segmentation lattices, we de-
fine a log-linear model of compound word seg-
mentation inspired by Koehn and Knight (2003),
making use of features including number of mor-
phemes hypothesized, frequency of the segments
as free-standing morphemes in a training corpus,
and letters in each segment. To tune the model
parameters, we selected a set of compound words
from a subset of the German development set,
manually created a linguistically plausible seg-
mentation of these words, and used this to select
the parameters of the log-linear model using a lat-
tice minimum error training algorithm to minimize
WER (Macherey et al, 2008). We reused the same
features and weights to create the Hungarian lat-
tices. For the test data, we created a lattice of ev-
ery possible segmentation of any word 6 charac-
ters or longer and used forward-backward pruning
to prune out low-probability segmentation paths
(Sixtus and Ortmanns, 1999). We then concate-
nated the lattices in each sentence.
Source Condition BLEU TER
German
baseline 20.8 60.7
lattice 21.3 59.9
Hungarian
baseline 11.0 71.1
lattice 12.3 70.4
Table 1: Impact of compound segmentation lat-
tices.
To build the translation model for lattice sys-
tem, we segmented the training data using the one-
best split predicted by the segmentation model,
146
and word aligned this with the English side. This
variant version of the training data was then con-
catenated with the baseline system?s training data.
3.1.1 Co-training of segmentation model
To avoid the necessity of manually creating seg-
mentation examples to train the segmentation
model, we attempted to generate sets of training
examples by selecting the compound splits that
were found along the path chosen by the decoder?s
one-best translation. Unfortunately, the segmen-
tation system generated in this way performed
slightly worse than the one-best baseline and so
we continued to use the parameter settings derived
from the manual segmentation.
3.2 Modeling sentence boundaries
Incorporating an n-gram language model proba-
bility into a CKY-based decoder is challenging.
When a partial hypothesis (also called an ?item?)
has been completed, it has not yet been determined
what strings will eventually occur to the left of
its first word, meaning that the exact computation
must deferred, which makes pruning a challenge.
In typical CKY decoders, the beginning and ends
of the sentence (which often have special charac-
teristics) are not conclusively determined until the
whole sentence has been translated and the proba-
bilities for the beginning and end sentence proba-
bilities can be added. However, by this point it is
often the case that a possibly better sentence be-
ginning has been pruned away. To address this,
we explicitly generate beginning and end sentence
markers as part of the translation process, as sug-
gested by Xiong et al (2008). The results of doing
this are shown in Table 2.
Source Condition BLEU TER
German
baseline 21.3 59.9
+boundary 21.6 60.1
Hungarian
baseline 12.3 70.4
+boundary 12.8 70.4
Table 2: Impact of modeling sentence boundaries.
3.3 Source language paraphrases
In order to deal with the sparsity associated with
a rich source language morphology and limited-
size parallel corpora (bitexts), we experimented
with a novel approach to paraphrasing out-of-
vocabulary (OOV) source language phrases in
our Hungarian-English system, using monolingual
contextual similarity rather than phrase-table piv-
oting (Callison-Burch et al, 2006) or monolin-
gual bitexts (Barzilay and McKeown, 2001; Dolan
et al, 2004). Distributional profiles for source
phrases were represented as context vectors over
a sliding window of size 6, with vectors defined
using log-likelihood ratios (cf. Rapp (1999), Dun-
ning (1993)) but using cosine rather than city-
block distance to measure profile similarity.
The 20 distributionally most similar source
phrases were treated as paraphrases, considering
candidate phrases up to a width of 6 tokens and fil-
tering out paraphrase candidates with cosine simi-
larity to the original of less than 0.6. The two most
likely translations for each paraphrase were added
to the grammar in order to provide mappings to
English for OOV Hungarian phrases.
This attempt at monolingually-derived source-
side paraphrasing did not yield improvements over
baseline. Preliminary analysis suggests that the
approach does well at identifying many content
words in translating extracted paraphrases of OOV
phrases (e.g., a kommunista part vezetaje ? ,
leader of the communist party or a ra tervezett?
until the planned to), but at the cost of more fre-
quently omitting target words in the output.
3.4 Dominance feature
Although our baseline hierarchical system permits
long-range reordering, it lacks a mechanism to
identify the most appropriate reordering for a spe-
cific sentence translation. For example, when the
most appropriate reordering is a long-range one,
our baseline system often also has to consider
shorter-range reorderings as well. In the worst
case, a shorter-range reordering has a high proba-
bility, causing the wrong reordering to be chosen.
Our baseline system lacks the capacity to address
such cases because all the features it employs are
independent of the phrases being moved; these are
modeled only as an unlexicalized generic nonter-
minal symbol.
To address this challenge, we included what we
call a dominance feature in the scoring of hypothe-
sis translations. Briefly, the premise of this feature
is that the function words in the sentence hold the
key reordering information, and therefore function
words are used to model the phrases being moved.
The feature assesses the quality of a reordering by
looking at the phrase alignment between pairs of
147
function words. In our experiments, we treated
the 128 most frequent words in the corpus as func-
tion words, similar to Setiawan et al (2007). Due
to space constraints, we will discuss the details in
another publication. As Table 3 reports, the use of
this feature yields positive results.
Source Condition BLEU TER
German
baseline 21.6 60.1
+dom 22.2 59.8
Hungarian
baseline 12.8 70.4
+dom 12.6 70.0
Table 3: Impact of alignment dominance feature.
3.5 Minimum Bayes risk decoding
Although during minimum error training we as-
sume a decoder that uses the maximum derivation
decision rule, we find benefits to translating using
a minimum risk decision rule on a test set (Kumar
and Byrne, 2004). This seeks the translation E of
the input lattice F that has the least expected loss,
measured by some loss function L:
E? = arg min
E?
EP (E|F)[L(E,E
?)] (1)
= arg min
E?
?
E
P (E|F)L(E,E?) (2)
We approximate the posterior distribution
P (E|F) and the set of possible candidate transla-
tions using the unique 500-best translations of a
source lattice F . If H(E,F) is the decoder?s path
weight, this is:
P (E|F) ? exp?H(E,F)
The optimal value for the free parameter ?must
be experimentally determined and depends on the
ranges of the feature functions and weights used in
the model, as well as the amount and kind of prun-
ing using during decoding.2 For our submission,
we used ? = 1. Since our goal is to minimize
TER?BLEU
2 we used this as the loss function in (2).
Table 4 shows the results on the dev-test set for
MBR decoding.
2If the free parameter ? lies in (1,?) the distribution is
sharpened, if it lies in [0, 1), the distribution is flattened.
Source Decoder BLEU TER
German
Max-D 22.2 59.8
MBR 22.6 59.4
Hungarian
Max-D 12.6 70.0
MBR 12.8 69.8
Table 4: Performance of maximum derivation vs.
MBR decoders.
4 Conclusion
Table 5 summarizes the impact on the dev-test set
of all features included in the University of Mary-
land system submission.
Condition
German Hungarian
BLEU TER BLEU TER
baseline 20.8 60.7 11.0 71.1
+lattices 21.3 59.9 12.3 70.4
+boundary 21.6 60.1 12.8 70.4
+dom 22.2 59.8 12.6 70.0
+MBR 22.6 59.4 12.8 69.8
Table 5: Summary of all features
Acknowledgments
This research was supported in part by the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-2-001, and the Army Research Laboratory.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the view of the
sponsors. Discussions with Chris Callison-Burch
were helpful in carrying out the monolingual para-
phrase work.
References
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In In
Proceedings of ACL-2001.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings NAACL-
2006.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
148
exploiting massively parallel news sources. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics of the Association for
Computational Linguistics, Geneva, Switzerland.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
Chris Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-08: HLT. Association for Compu-
tational Linguistics, June.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
IEEE Internation Conference on Acoustics, Speech,
and Signal Processing, pages 181?184.
P. Koehn and K. Knight. 2003. Empirical methods
for compound splitting. In Proceedings of the EACL
2003.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better machine translation quality for
the German-English language pairs. In ACL Work-
shop on Statistical Machine Translation.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 976?
985.
Adam Lopez. 2008. Tera-scale translation models
via pattern matching. In Proceedings of COLING,
Manchester, UK.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proceedings of EMNLP, Honolulu, HI.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 160?167, Sapporo, Japan,
July.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the ACL, pages 311?318.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th Annual
Conference of the Association for Computational
Linguistics., pages 519?525.
Hendra Setiawan, Min-Yen Kan, and Haizhao Li.
2007. Ordering phrases with function words. In
Proceedings of ACL.
S. Sixtus and S. Ortmanns. 1999. High quality word
graphs using forward-backward pruning. In Pro-
ceedings of ICASSP, Phoenix, AZ.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted human an-
notation. In Proceedings of Association for Machine
Translation in the Americas.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Intl. Conf. on Spoken Language
Processing.
Deyi Xiong, Min Zhang, Ai Ti Aw, Haitao Mi, Qun
Liu, and Shouxun Lin. 2008. Refinements in BTG-
based statistical machine translation. In Proceed-
ings of IJCNLP 2008.
149
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 45?55,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Holistic Sentiment Analysis Across Languages:
Multilingual Supervised Latent Dirichlet Allocation
Jordan Boyd-Graber
UMD iSchool
and UMIACS
University of Maryland
College Park, MD
jbg@umiacs.umd.edu
Philip Resnik
Department of Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
In this paper, we develop multilingual super-
vised latent Dirichlet alocation (MLSLDA),
a probabilistic generative model that allows
insights gleaned from one language?s data to
inform how the model captures properties of
other languages. MLSLDA accomplishes this
by jointly modeling two aspects of text: how
multilingual concepts are clustered into themat-
ically coherent topics and how topics associ-
ated with text connect to an observed regres-
sion variable (such as ratings on a sentiment
scale). Concepts are represented in a general
hierarchical framework that is flexible enough
to express semantic ontologies, dictionaries,
clustering constraints, and, as a special, degen-
erate case, conventional topic models. Both
the topics and the regression are discovered
via posterior inference from corpora. We show
MLSLDA can build topics that are consistent
across languages, discover sensible bilingual
lexical correspondences, and leverage multilin-
gual corpora to better predict sentiment.
Sentiment analysis (Pang and Lee, 2008) offers
the promise of automatically discerning how people
feel about a product, person, organization, or issue
based on what they write online, which is potentially
of great value to businesses and other organizations.
However, the vast majority of sentiment resources
and algorithms are limited to a single language, usu-
ally English (Wilson, 2008; Baccianella and Sebas-
tiani, 2010). Since no single language captures a
majority of the content online, adopting such a lim-
ited approach in an increasingly global community
risks missing important details and trends that might
only be available when text in multiple languages is
taken into account.
Up to this point, multiple languages have been
addressed in sentiment analysis primarily by trans-
ferring knowledge from a resource-rich language to
a less rich language (Banea et al, 2008), or by ig-
noring differences in languages via translation into
English (Denecke, 2008). These approaches are lim-
ited to a view of sentiment that takes place through
an English-centric lens, and they ignore the poten-
tial to share information between languages. Ide-
ally, learning sentiment cues holistically, across lan-
guages, would result in a richer and more globally
consistent picture.
In this paper, we introduce Multilingual Super-
vised Latent Dirichlet Allocation (MLSLDA), a
model for sentiment analysis on a multilingual cor-
pus. MLSLDA discovers a consistent, unified picture
of sentiment across multiple languages by learning
?topics,? probabilistic partitions of the vocabulary
that are consistent in terms of both meaning and rel-
evance to observed sentiment. Our approach makes
few assumptions about available resources, requiring
neither parallel corpora nor machine translation.
The rest of the paper proceeds as follows. In Sec-
tion 1, we describe the probabilistic tools that we use
to create consistent topics bridging across languages
and the MLSLDA model. In Section 2, we present
the inference process. We discuss our set of seman-
tic bridges between languages in Section 3, and our
experiments in Section 4 demonstrate that this ap-
proach functions as an effective multilingual topic
model, discovers sentiment-biased topics, and uses
multilingual corpora to make better sentiment pre-
dictions across languages. Sections 5 and 6 discuss
related research and discusses future work, respec-
tively.
45
1 Predictions from Multilingual Topics
As its name suggests, MLSLDA is an extension of
Latent Dirichlet alocation (LDA) (Blei et al, 2003),
a modeling approach that takes a corpus of unan-
notated documents as input and produces two out-
puts, a set of ?topics? and assignments of documents
to topics. Both the topics and the assignments are
probabilistic: a topic is represented as a probability
distribution over words in the corpus, and each doc-
ument is assigned a probability distribution over all
the topics. Topic models built on the foundations of
LDA are appealing for sentiment analysis because
the learned topics can cluster together sentiment-
bearing words, and because topic distributions are a
parsimonious way to represent a document.1
LDA has been used to discover latent structure
in text (e.g. for discourse segmentation (Purver et
al., 2006) and authorship (Rosen-Zvi et al, 2004)).
MLSLDA extends the approach by ensuring that this
latent structure ? the underlying topics ? is consis-
tent across languages. We discuss multilingual topic
modeling in Section 1.1, and in Section 1.2 we show
how this enables supervised regression regardless of
a document?s language.
1.1 Capturing Semantic Correlations
Topic models posit a straightforward generative pro-
cess that creates an observed corpus. For each docu-
ment d, some distribution ?d over unobserved topics
is chosen. Then, for each word position in the doc-
ument, a topic z is selected. Finally, the word for
that position is generated by selecting from the topic
indexed by z. (Recall that in LDA, a ?topic? is a
distribution over words).
In monolingual topic models, the topic distribution
is usually drawn from a Dirichlet distribution. Us-
ing Dirichlet distributions makes it easy to specify
sparse priors, and it also simplifies posterior infer-
ence because Dirichlet distributions are conjugate
to multinomial distributions. However, drawing top-
ics from Dirichlet distributions will not suffice if
our vocabulary includes multiple languages. If we
are working with English, German, and Chinese at
the same time, a Dirichlet prior has no way to fa-
vor distributions z such that p(good|z), p(gut|z), and
1The latter property has also made LDA popular for infor-
mation retrieval (Wei and Croft, 2006)).
p(ha?o|z) all tend to be high at the same time, or low
at the same time. More generally, the structure of our
model must encourage topics to be consistent across
languages, and Dirichlet distributions cannot encode
correlations between elements.
One possible solution to this problem is to use the
multivariate normal distribution, which can produce
correlated multinomials (Blei and Lafferty, 2005),
in place of the Dirichlet distribution. This has been
done successfully in multilingual settings (Cohen
and Smith, 2009). However, such models complicate
inference by not being conjugate.
Instead, we appeal to tree-based extensions of the
Dirichlet distribution, which has been used to induce
correlation in semantic ontologies (Boyd-Graber et
al., 2007) and to encode clustering constraints (An-
drzejewski et al, 2009). The key idea in this ap-
proach is to assume the vocabularies of all languages
are organized according to some shared semantic
structure that can be represented as a tree. For con-
creteness in this section, we will use WordNet (Miller,
1990) as the representation of this multilingual se-
mantic bridge, since it is well known, offers conve-
nient and intuitive terminology, and demonstrates the
full flexibility of our approach. However, the model
we describe generalizes to any tree-structured rep-
resentation of multilingual knowledge; we discuss
some alternatives in Section 3.
WordNet organizes a vocabulary into a rooted, di-
rected acyclic graph of nodes called synsets, short for
?synonym sets.? A synset is a child of another synset
if it satisfies a hyponomy relationship; each child ?is
a? more specific instantiation of its parent concept
(thus, hyponomy is often called an ?isa? relationship).
For example, a ?dog? is a ?canine? is an ?animal? is
a ?living thing,? etc. As an approximation, it is not
unreasonable to assume that WordNet?s structure of
meaning is language independent, i.e. the concept
encoded by a synset can be realized using terms in
different languages that share the same meaning. In
practice, this organization has been used to create
many alignments of international WordNets to the
original English WordNet (Ordan and Wintner, 2007;
Sagot and Fis?er, 2008; Isahara et al, 2008).
Using the structure of WordNet, we can now de-
scribe a generative process that produces a distribu-
tion over a multilingual vocabulary, which encour-
ages correlations between words with similar mean-
46
ings regardless of what language each word is in.
For each synset h, we create a multilingual word
distribution for that synset as follows:
1. Draw transition probabilities ?h ? Dir (?h)
2. Draw stop probabilities ?h ? Dir (?h)
3. For each language l, draw emission probabilities for
that synset ?h,l ? Dir (pih,l).
For conciseness in the rest of the paper, we will refer
to this generative process as multilingual Dirichlet
hierarchy, or MULTDIRHIER(? ,?,pi).2 Each ob-
served token can be viewed as the end result of a
sequence of visited synsets ?. At each node in the
tree, the path can end at node i with probability ?i,1,
or it can continue to a child synset with probability
?i,0. If the path continues to another child synset, it
visits child j with probability ?i,j . If the path ends at
a synset, it generates word k with probability ?i,l,k.3
The probability of a word being emitted from a path
with visited synsets r and final synset h in language
l is therefore
p(w, ? = r, h|l,?,?,?) =
?
?
?
(i,j)?r
?i,j?i,0
?
? (1? ?h,1)?h,l,w. (1)
Note that the stop probability ?h is independent of
language, but the emission ?h,l is dependent on the
language. This is done to prevent the following sce-
nario: while synset A is highly probable in a topic
and words in language 1 attached to that synset have
high probability, words in language 2 have low prob-
ability. If this could happen for many synsets in
a topic, an entire language would be effectively si-
lenced, which would lead to inconsistent topics (e.g.
2Variables ?h, pih,l, and ?h are hyperparameters. Their mean
is fixed, but their magnitude is sampled during inference (i.e.
?h,iP
k ?h,k
is constant, but ?h,i is not). For the bushier bridges,
(e.g. dictionary and flat), their mean is uniform. For GermaNet,
we took frequencies from two balanced corpora of German and
English: the British National Corpus (University of Oxford,
2006) and the Kern Corpus of the Digitales Wo?rterbuch der
Deutschen Sprache des 20. Jahrhunderts project (Geyken, 2007).
We took these frequencies and propagated them through the
multilingual hierarchy, following LDAWN?s (Boyd-Graber et
al., 2007) formulation of information content (Resnik, 1995) as
a Bayesian prior. The variance of the priors was initialized to be
1.0, but could be sampled during inference.
3Note that the language and word are taken as given, but the
path through the semantic hierarchy is a latent random variable.
Topic 1 is about baseball in English and about travel
in German). Separating path from emission helps
ensure that topics are consistent across languages.
Having defined topic distributions in a way that can
preserve cross-language correspondences, we now
use this distribution within a larger model that can
discover cross-language patterns of use that predict
sentiment.
1.2 The MLSLDA Model
We will view sentiment analysis as a regression prob-
lem: given an input document, we want to predict
a real-valued observation y that represents the senti-
ment of a document. Specifically, we build on super-
vised latent Dirichlet alocation (SLDA, (Blei and
McAuliffe, 2007)), which makes predictions based
on the topics expressed in a document; this can be
thought of projecting the words in a document to low
dimensional space of dimension equal to the number
of topics. Blei et al showed that using this latent
topic structure can offer improved predictions over re-
gressions based on words alone, and the approach fits
well with our current goals, since word-level cues are
unlikely to be identical across languages. In addition
to text, SLDA has been successfully applied to other
domains such as social networks (Chang and Blei,
2009) and image classification (Wang et al, 2009).
The key innovation in this paper is to extend SLDA
by creating topics that are globally consistent across
languages, using the bridging approach above.
We express our model in the form of a probabilis-
tic generative latent-variable model that generates
documents in multiple languages and assigns a real-
valued score to each document. The score comes
from a normal distribution whose sum is the dot prod-
uct between a regression parameter ? that encodes
the influence of each topic on the observation and
a variance ?2. With this model in hand, we use sta-
tistical inference to determine the distribution over
latent variables that, given the model, best explains
observed data.
The generative model is as follows:
1. For each topic i = 1 . . .K, draw a topic distribution
{?i,?i,?i} from MULTDIRHIER(? ,?,pi).
2. For each document d = 1 . . .M with language ld:
(a) Choose a distribution over topics ?d ?
Dir (?).
47
(b) For each word in the document n = 1 . . . Nd,
choose a topic assignment zd,n ? Mult (?d)
and a path ?d,n ending at word wd,n according
to Equation 1 using {?zd,n ,?zd,n ,?zd,n}.
3. Choose a response variable from y ?
Norm
(
?>z?, ?2
)
, where z?d ? 1N
?N
n=1 zd,n.
Crucially, note that the topics are not indepen-
dent of the sentiment task; the regression encourages
terms with similar effects on the observation y to
be in the same topic. The consistency of topics de-
scribed above allows the same regression to be done
for the entire corpus regardless of the language of the
underlying document.
2 Inference
Finding the model parameters most likely to explain
the data is a problem of statistical inference. We em-
ploy stochastic EM (Diebolt and Ip, 1996), using a
Gibbs sampler for the E-step to assign words to paths
and topics. After randomly initializing the topics,
we alternate between sampling the topic and path
of a word (zd,n, ?d,n) and finding the regression pa-
rameters ? that maximize the likelihood. We jointly
sample the topic and path conditioning on all of the
other path and document assignments in the corpus,
selecting a path and topic with probability
p(zn = k, ?n = r|z?n,??n, wn, ?, ?,?) =
p(yd|z, ?, ?)p(?n = r|zn = k,??n, wn, ? ,?,pi)
p(zn = k|z?n, ?). (2)
Each of these three terms reflects a different influence
on the topics from the vocabulary structure, the doc-
ument?s topics, and the response variable. In the next
paragraphs, we will expand each of them to derive
the full conditional topic distribution.
As discussed in Section 1.1, the structure of the
topic distribution encourages terms with the same
meaning to be in the same topic, even across lan-
guages. During inference, we marginalize over pos-
sible multinomial distributions ?, ?, and ?, using
the observed transitions from i to j in topic k; Tk,i,j ,
stop counts in synset i in topic k, Ok,i,0; continue
counts in synsets i in topic k, Ok,i,1; and emission
counts in synset i in language l in topic k, Fk,i,l. The
H
L
M
N
?
d
z
d,n
?
d,n
?
w
d,n
?
?
y
d
K
?
i,h
?
h
?
i,h
?
h
?
i,h,l
?
h,l
Multilingual Topics Text Documents Sentiment Prediction
Figure 1: Graphical model representing MLSLDA.
Shaded nodes represent observations, plates denote repli-
cation, and lines show probabilistic dependencies.
probability of taking a path r is then
p(?n = r|zn = k,??n) =
?
(i,j)?r
(
Bk,i,j + ?i,j
?
j? Bk,i,j? + ?i,j
Ok,i,1 + ?i
?
s?0,1Ok,i,s + ?i,s
)
? ?? ?
Transition
Ok,rend,0 + ?rend?
s?0,1Ok,rend,s + ?rend,s
Fk,rend,wn + pirend,l?
w? Frend,w? + pirend,w?
? ?? ?
Emission
.
(3)
Equation 3 reflects the multilingual aspect of this
model. The conditional topic distribution for
SLDA (Blei and McAuliffe, 2007) replaces this term
with the standard Multinomial-Dirichlet. However,
we believe this is the first published SLDA-style
model using MCMC inference, as prior work has
used variational inference (Blei and McAuliffe, 2007;
Chang and Blei, 2009; Wang et al, 2009).
Because the observed response variable depends
on the topic assignments of a document, the condi-
tional topic distribution is shifted toward topics that
explain the observed response. Topics that move the
predicted response y?d toward the true yd will be fa-
vored. We drop terms that are constant across all
48
topics for the effect of the response variable,
p(yd|z, ?, ?) ?
exp
[
1
?2
(
yd ?
?
k? Nd,k??k??
k? Nd,k?
)
?zk?
k? Nd,k?
]
? ?? ?
Other words? influence
exp
[
??2zk
2?2
?
k? N
2
d,k?
]
? ?? ?
This word?s influence
. (4)
The above equation represents the supervised aspect
of the model, which is inherited from SLDA.
Finally, there is the effect of the topics already
assigned to a document; the conditional distribution
favors topics already assigned in a document,
p(zn = k|z?n, ?) =
Td,k + ?k
?
k? Td,k? + ?k?
. (5)
This term represents the document focus of this
model; it is present in all Gibbs sampling inference
schemes for LDA (Griffiths and Steyvers, 2004).
Multiplying together Equations 3, 4, and 5 allows
us to sample a topic using the conditional distribution
from Equation 2, based on the topic and path of the
other words in all languages. After sampling the
path and topic for each word in a document, we then
find new regression parameters ? that maximize the
likelihood conditioned on the current state of the
sampler. This is simply a least squares regression
using the topic assignments z?d to predict yd.
Prediction on documents for which we don?t have
an observed yd is equivalent to marginalizing over
yd and sampling topics for the document from Equa-
tions 3 and 5. The prediction for yd is then the dot
product of ? and the empirical topic distribution z?d.
We initially optimized all hyperparameters using
slice sampling. However, we found that the regres-
sion variance ?2 was not stable. Optimizing ?2 seems
to balance between modeling the language in the doc-
uments and the prediction, and thus is sensitive to
documents? length. Given this sensitivity, we did
not optimize ?2 for our prediction experiments in
Section 4, but instead kept it fixed at 0.25. We leave
optimizing this variable, either through cross valida-
tion or adapting the model, to future work.
3 Bridges Across Languages
In Section 1.1, we described connections across lan-
guages as offered by semantic networks in a general
way, using WordNet as an example. In this section,
we provide more specifics, as well as alternative ways
of building semantic connections across languages.
Flat First, we can consider a degenerate mapping
that is nearly equivalent to running SLDA indepen-
dently across multiple languages, relating topics only
based on the impact on the response variable. Con-
sider a degenerate tree with only one node, with all
words in all languages associated with that node. This
is consistent with our model, but there is really no
shared semantic space, as all emitted words must
come from this degenerate ?synset? and the model
only represents the output distribution for this single
node.
WordNet We took the alignment of GermaNet to
WordNet 1.6 (Kunze and Lemnitzer, 2002) and re-
moved all synsets that were had no mapped German
words. Any German synsets that did not have English
translations had their words mapped to the lowest
extant English hypernym (e.g. ?beinbruch,? a bro-
ken leg, was mapped to ?fracture?). We stemmed
all words to account for inflected forms not being
present (Porter and Boulton, 1970). An example
of the paths for the German word ?wunsch? (wish,
request) is shown in Figure 2(a).
Dictionaries A dictionary can be viewed as a many
to many mapping, where each entry ei maps one
or more words in one language si to one or more
words ti in another language. Entries were taken
from an English-German dictionary (Richter, 2008)
a Chinese-English dictionary (Denisowski, 1997),
and a Chinese-German dictionary (Hefti, 2005). As
with WordNet, the words in entries for English and
German were stemmed to improve coverage. An
example for German is shown in Figure 2(b).
Algorithmic Connections In addition to hand-
curated connections across languages, one could also
consider automatic means of mapping across lan-
guages, such as using edit distance or local con-
text (Haghighi et al, 2008; Rapp, 1995) or us-
ing a lexical translation table obtained from paral-
lel text (Melamed, 1998). While we experimented
49
wish.n.04
wish wunsch
entity.n.01
entitiabstraction.n.06
cognition.n.01 event.n.01
event ereignis vorgang act.n.02
deed act handlung
speech_act.n.01
request.n.02
option.n.02
ask request anfrag wunsch
altern option choic option
preference.n.03
objekt
(a) GermaNet
dict.1
room gelass
root
dict.2
room raum platz
room zimm raum
dict.3
stub
(b) Dictionary
Figure 2: Two methods for constructing multilingual distributions over words. On the left, paths to the German word
?wunsch? in GermaNet are shown. On the right, paths to the English word ?room? are shown. Both English and German
words are shown; some internal nodes in GermaNet have been omitted for space (represented by dashed lines). Note
that different senses are denoted by different internal paths, and that internal paths are distinct from the per-language
expression.
with these techniques, constructing appropriate hier-
archies from these resources required many arbitrary
decisions about cutoffs and which words to include.
Thus, we do not consider them in this paper.
4 Experiments
We evaluate MLSLDA on three criteria: how well
it can discover consistent topics across languages
for matching parallel documents, how well it can
discover sentiment-correlated word lists from non-
aligned text, and how well it can predict sentiment.
4.1 Matching on Multilingual Topics
We took the 1996 documents from the Europarl cor-
pus (Koehn, 2005) using three bridges: GermaNet,
dictionary, and the uninformative flat matching.4 The
model is unaware that the translations of documents
in one language are present in the other language.
Note that this does not use the supervised framework
4For English and German documents in all experiments,
we removed stop words (Loper and Bird, 2002), stemmed
words (Porter and Boulton, 1970), and created a vocabulary
of the most frequent 5000 words per language (this vocabulary
limit was mostly done to ensure that the dictionary-based bridge
was of manageable size). Documents shorter than fifty content
words were excluded.
(as there is no associated response variable for Eu-
roparl documents); this experiment is to demonstrate
the effectiveness of the multilingual aspect of the
model. To test whether the topics learned by the
model are consistent across languages, we represent
each document using the probability distribution ?d
over topic assignments. Each ?d is a vector of length
K and is a language-independent representation of
the document.
For each document in one language, we computed
the Hellinger distance between it and all of the docu-
ments in the other language and sorted the documents
by decreasing distance. The translation of the docu-
ment is somewhere in that set; the higher the normal-
ized rank (the percentage of documents with a rank
lower than the translation of the document), the better
the underlying topic model connects languages.
We compare three bridges against what is to our
knowledge the only other topic model for unaligned
text, Multilingual Topics for Unaligned Text (Boyd-
Graber and Blei, 2009).5
5The bipartite matching was initialized with the dictionary
weights as specified by the Multilingual Topics for Unaligned
Text algorithm. The matching size was limited to 250 and the
bipartite matching was only updated on the initial iteration then
held fixed. This yielded results comparable to when the matching
50
Average Parallel Document Rank
Br
id
ge
Flat
GermaNet
MuTo
Dictionary
Flat
GermaNet
MuTo
Dictionary
Flat
GermaNet
MuTo
Dictionary
Flat
GermaNet
MuTo
Dictionary
0.0 0.2 0.4 0.6 0.8
5
25
50
75
Figure 3: Average rank of paired translation document
recovered from the multilingual topic model. Random
guessing would yield 0.5; MLSLDA with a dictionary
based matching performed best.
Figure 3 shows the results of this experiment. The
dictionary-based bridge had the best performance on
the task, ranking a large proportion of documents
(0.95) below the translated document once enough
topics were available. Although GermaNet is richer,
its coverage is incomplete; the dictionary structure
had a much larger vocabulary and could build a more
complete multilingual topics. Using comparable in-
put information, this more flexible model performed
better on the matching task than the existing multi-
lingual topic model available for unaligned text. The
degenerate flat bridge did no better than the baseline
of random guessing, as expected.
4.2 Qualitative Sentiment-Correlated Topics
One of the key tasks in sentiment analysis has been
the collection of lists of words that convey senti-
ment (Wilson, 2008; Riloff et al, 2003). These
resources are often created using or in reference
to resources like WordNet (Whitelaw et al, 2005;
Baccianella and Sebastiani, 2010). MLSLDA pro-
vides a method for extracting topical and sentiment-
correlated word lists from multilingual corpora. If
was updated more frequently.
a WordNet-like resource is used as the bridge, the
resulting topics are distributions over synsets, not just
over words.
As our demonstration corpus, we used the Amherst
Sentiment Corpus (Constant et al, 2009), as it has
documents in multiple languages (English, Chinese,
and German) with numerical assessments of senti-
ment (number of stars assigned to the review). We
segmented the Chinese text (Tseng et al, 2005) and
used a classifier trained on character n-grams to re-
move English-language documents that were mixed
in among the Chinese and German language reviews.
Figure 4 shows extracted topics from German-
English and German-Chinese corpora. MLSLDA
is able to distinguish sentiment-bearing topics from
content bearing topics. For example; in the German-
English corpus, ?food? and ?children? topics are
not associated with a consistent sentiment signal,
while ?religion? is associated with a more negative
sentiment. In contrast, in the German-Chinese cor-
pus, the ?religion/society? topic is more neutral, and
the gender-oriented topic is viewed more negatively.
Negative sentiment-bearing topics have reasonable
words such as ?pages,? ?ko?ng pa`? (Chinese for ?I?m
afraid that . . . ?) and ?tuo? (Chienese for ?discard?),
and positive sentiment-bearing topics have reason-
able words such as ?great,? ?good,? and ?juwel? (Ger-
man for ?jewel?).
The qualitative topics also betray some of the
weaknesses of the model. For example, in one of
the negative sentiment topics, the German word ?gut?
(good) is present. Because topics are distributions
over words, they can encode the presence of nega-
tions like ?kein? (no) and ?nicht? (not), but not collo-
cations like ?nicht gut.? More elaborate topic models
that can model local syntax and collocations (John-
son, 2010) provide options for addressing such prob-
lems.
We do not report the results for sentiment predic-
tion for this corpus because the baseline of predicting
a positive review is so strong; most algorithms do ex-
tremely well by always predicting a positive review,
ours included.
4.3 Sentiment Prediction
We gathered 330 film reviews from a German film
review site (Vetter et al, 2000) and combined them
with a much larger English film review corpus of over
51
0.0
-0.4
-0.8
-1.2 0.4
0.8
1.2
himmel
gedanken
glaube
unsere
kirche
wahrheit
god
us
religion
church
human
buch
roman
leser
seiten
geschichte
handlung
book
novel
reader
pages
books
tale
essen
di?t
verlieren
befinden
k?rper
rezepte
diet
food
eat
weight
eating
healthy
fat
buch
immer
leben
art
lesen
thema
autor
book
books
one
life
person
people
kind
kinder
eltern
baby
nacht
children
baby
child
parents
sleep
film
filme
episode
star
story
gibt
movie
film
episode
movies
scenes
separate
gives
gesellschaft
genau
?berzeugt
ergebnis
mittel
verlangen
great
good
business
all
one
companies
right
(a) German / English
-0.4-0.8-1.2-1.6 0.0 0.4 0.8
?? (god)?? (lord)? (both)?? (religion)?? (science)?? (community)
(god) gott(lord) herr(religion) religion(universe) all(world) welt(science) wissenschaft(medicine) medizin(society) gesellschaft
? (good)? (set)? (treasure)? (handsome)? (both)?? (story)? (small)
(good) gut(sentence) satz(two) zwei(story) story(treasure) schatz(attractive) attraktiv(elegant) elegant(gem) juwel
(book) buch(itself) sich(that) dass(much) viel(no) kein(good) gut(when) wenn
? (book) ?? ([I'm afraid that...])?  (myself)? (both)??? (mostly)?  (book)?? ([really isn't])? (discard)
(woman) frau(point) punkt(man) mann(equal) gleich(fast) schnell(female) weiblich(soon) bald
? (quick)? (a little)?? (woman)?? (man)? (female)? (male)?? (female)? (both)
?? (harry)? (belt)? (sky)? (both)? (section)??? (vampire)?? (strong)?? (last)
(harry) harry(volume) band(sky) himmel (universe) all(vampire) vampir(last) letzt(part) teil
(b) German / Chinese
Figure 4: Topics, along with associated regression coefficient ? from a learned 25-topic model on German-English (left)
and German-Chinese (right) documents. Notice that theme-related topics have regression parameter near zero, topics
discussing the number of pages have negative regression parameters, topics with ?good,? ?great,? ?ha?o? (good) and
?u?berzeugt? (convinced) have positive regression parameters. For the German-Chinese corpus, note the presence of ?gut?
(good) in one of the negative sentiment topics, showing the difficulty of learning collocations.
Train Test GermaNet Dictionary Flat
DE DE 73.8 24.8 92.2
EN DE 7.44 2.68 18.3
EN + DE DE 1.17 1.46 1.39
Table 1: Mean squared error on a film review corpus.
All results are on the same German test data, varying the
training data. Over-fitting prevents the model learning on
the German data alone; adding English data to the mix
allows the model to make better predictions.
5000 film reviews (Pang and Lee, 2005) to create a
multilingual film review corpus.6
The results for predicting sentiment in German
documents with 25 topics are presented in Table 1.
On a small monolingual corpus, prediction is very
poor. The model over-fits, especially when it has
the entire vocabulary to select from. The slightly
better performance using GermaNet and a dictionary
as topic priors can be viewed as basic feature selec-
tion, removing proper names from the vocabulary to
6We followed Pang and Lee?s method for creating a nu-
merical score between 0 and 1 from a star rating. We
then converted that to an integer by multiplying by 100;
this was done because initial data preprocessing assumed
integer values (although downstream processing did not as-
sume integer values). The German movie review corpus
is available at http://www.umiacs.umd.edu/?jbg/
static/downloads_and_media.html
prevent over-fitting.
One would expect that prediction improves with a
larger training set. For this model, such an improve-
ment is seen even when the training set includes no
documents in the target language. Note that even the
degenerate flat bridge across languages provides use-
ful information. After introducing English data, the
model learns to prefer smaller regression parameters
(this can be seen as a form of regularization).
Performance is best when a reasonably large cor-
pus is available including some data in the target
language. For each bridge, performance improves
dramatically, showing that MLSLDA is successfully
able to incorporate information learned from both
languages to build a single, coherent picture of how
sentiment is expressed in both languages. With the
GermaNet bridge, performance is better than both
the degenerate and dictionary based bridges, showing
that the model is sharing information both through
the multilingual topics and the regression parameters.
Performance on English prediction is comparable
to previously published results on this dataset (Blei
and McAuliffe, 2007); with enough data, a monolin-
gual model is no longer helped by adding additional
multilingual data.
52
5 Relationship to Previous Research
The advantages of MLSLDA reside largely in the
assumptions that it makes and does not make: docu-
ments need not be parallel, sentiment is a normally
distributed document-level property, words are ex-
changeable, and sentiment can be predicted as a re-
gression on a K-dimensional vector.
By not assuming parallel text, this approach can
be applied to a broad class of corpora. Other mul-
tilingual topic models require parallel text, either at
the document (Ni et al, 2009; Mimno et al, 2009)
or word-level (Kim and Khudanpur, 2004; Zhao and
Xing, 2006). Similarly, other multilingual sentiment
approaches also require parallel text, often supplied
via automatic translation; after the translated text
is available, either monolingual analysis (Denecke,
2008) or co-training is applied (Wan, 2009). In con-
trast, our approach requires fewer resources for a lan-
guage: a dictionary (or similar knowledge structure
relating words to nodes in a graph) and comparable
text, instead of parallel text or a machine translation
system.
Rather than viewing one language through the
lens of another language, MLSLDA views all lan-
guages through the lens of the topics present in a
document. This is a modeling decision with pros and
cons. It allows a language agnostic decision about
sentiment to be made, but it restricts the expressive-
ness of the model in terms of sentiment in two ways.
First, it throws away information important to sen-
timent analysis like syntactic constructions (Greene
and Resnik, 2009) and document structure (McDon-
ald et al, 2007) that may impact the sentiment rating.
Second, a single real number is not always sufficient
to capture the nuances of sentiment. Less critically,
assuming that sentiment is normally distributed is not
true of all real-world corpora; review corpora often
have a skew toward positive reviews. We standardize
responses by the mean and variance of the training
data to partially address this issue, but other response
distributions are possible, such as generalized linear
models (Blei and McAuliffe, 2007) and vector ma-
chines (Zhu et al, 2009), which would allow more
traditional classification predictions.
Other probabilistic models for sentiment classifi-
cation view sentiment as a word level feature. Some
models use sentiment word lists, either given or
learned from a corpus, as a prior to seed topics so
that they attract other sentiment bearing words (Mei
et al, 2007; Lin and He, 2009). Other approaches
view sentiment or perspective as a perturbation of
a log-linear topic model (Lin et al, 2008). Such
techniques could be combined with the multilingual
approach presented here by using distributions over
words that not only bridge different languages but
also encode additional information. For example, the
vocabulary hierarchies could be structured to encour-
age topics that encourage correlation among similar
sentiment-bearing words (e.g. clustering words asso-
ciated with price, size, etc.). Future work could also
more rigorously validate that the multilingual topics
discovered by MLSLDA are sentiment-bearing via
human judgments.
In contrast, MLSLDA draws on techniques that
view sentiment as a regression problem based on the
topics used in a document, as in supervised latent
Dirichlet alocation (SLDA) (Blei and McAuliffe,
2007) or in finer-grained parts of a document (Titov
and McDonald, 2008). Extending these models to
multilingual data would be more straightforward.
6 Conclusions
MLSLDA is a ?holistic? statistical model for multi-
lingual corpora that does not require parallel text
or expensive multilingual resources. It discovers
connections across languages that can recover la-
tent structure in parallel corpora, discover sentiment-
correlated word lists in multiple languages, and make
accurate predictions across languages that improve
with more multilingual data, as demonstrated in the
context of sentiment analysis.
More generally, MLSLDA provides a formalism
that can be used to incorporate the many insights of
topic modeling-driven sentiment analysis to multi-
lingual corpora by tying together word distributions
across languages. MLSLDA can also contribute to
the development of word list-based sentiment sys-
tems: the topics discovered by MLSLDA can serve
as a first-pass means of sentiment-based word lists
for languages that might lack annotated resources.
MLSLDA also can be viewed as a sentiment-
informed multilingual word sense disambiguation
(WSD) algorithm. When the multilingual bridge is an
explicit representation of sense such as WordNet, part
53
of the generative process is an explicit assignment
of every word to sense (the path latent variable ?);
this is discovered during inference. The dictionary-
based technique may be viewed as a disambiguation
via a transfer dictionary. How sentiment prediction
impacts the implicit WSD is left to future work.
Better capturing local syntax and meaningful col-
locations would also improve the model?s ability to
predict sentiment and model multilingual topics, as
would providing a better mechanism for represent-
ing words not included in our bridges. We intend to
develop such models as future work.
7 Acknowledgments
This research was funded in part by the Army Re-
search Laboratory through ARL Cooperative Agree-
ment W911NF-09-2-0072 and by the Office of the
Director of National Intelligence (ODNI), Intelli-
gence Advanced Research Projects Activity (IARPA),
through the Army Research Laboratory. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of ARL, IARPA, the ODNI, or the U.S. Govern-
ment. The authors thank the anonymous reviewers,
Jonathan Chang, Christiane Fellbaum, and Lawrence
Watts for helpful comments. The authors especially
thank Chris Potts for providing help in obtaining and
processing reviews.
References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic mod-
eling via Dirichlet forest priors. In ICML.
Andrea Esuli Stefano Baccianella and Fabrizio Sebastiani.
2010. Sentiwordnet 3.0: An enhanced lexical resource
for sentiment analysis and opinion mining. In LREC.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer
Hassan. 2008. Multilingual subjectivity analysis using
machine translation. In EMNLP.
David M. Blei and John D. Lafferty. 2005. Correlated
topic models. In NIPS.
David M. Blei and Jon D. McAuliffe. 2007. Supervised
topic models. In NIPS. MIT Press.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. JMLR, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In UAI.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In EMNLP.
Jonathan Chang and David M. Blei. 2009. Relational
topic models for document networks. In AISTATS.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In NAACL.
Noah Constant, Christopher Davis, Christopher Potts, and
Florian Schwarz. 2009. The pragmatics of expressive
content: Evidence from large corpora. Sprache und
Datenverarbeitung, 33(1?2).
Kerstin Denecke. 2008. Using SentiWordNet for multilin-
gual sentiment analysis. In ICDEW 2008.
Paul Denisowski. 1997. CEDICT.
http://www.mdbg.net/chindict/.
Jean Diebolt and Eddie H.S. Ip, 1996. Markov Chain
Monte Carlo in Practice, chapter Stochastic EM:
method and application. Chapman and Hall, London.
Alexander Geyken. 2007. The DWDS corpus: A ref-
erence corpus for the German language of the 20th
century. In Idioms and Collocations: Corpus-based
Linguistic, Lexicographic Studies. Continuum Press.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
NAACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(Suppl 1):5228?5235.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and
Dan Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In ACL, Columbus, Ohio.
Jan Hefti. 2005. HanDeDict. http://chdw.de.
Hitoshi Isahara, Fransis Bond, Kiyotaka Uchimoto, Masao
Utiyama, and Kyoko Kanzaki. 2008. Development of
the Japanese WordNet. In LREC.
Mark Johnson. 2010. PCFGs, topic models, adaptor
grammars and learning topical collocations and the
structure of proper names. In ACL.
Woosung Kim and Sanjeev Khudanpur. 2004. Lexical
triggers and latent semantic analysis for cross-lingual
language model adaptation. TALIP, 3(2):94?112.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In MT Summit.
http://www.statmt.org/europarl/.
Claudia Kunze and Lothar Lemnitzer. 2002. Standardiz-
ing WordNets in a web-compliant format: The case of
GermaNet. In Workshop on Wordnets Structures and
Standardisation.
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic
model for sentiment analysis. In CIKM.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In ECML PKDD.
54
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Tools and methodologies for
teaching. ACL.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In ACL.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In WWW.
Ilya Dan Melamed. 1998. Empirical methods for exploit-
ing parallel texts. Ph.D. thesis, University of Pennsyl-
vania.
George A. Miller. 1990. Nouns in WordNet: A lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264.
David Mimno, Hanna Wallach, Jason Naradowsky, David
Smith, and Andrew McCallum. 2009. Polylingual
topic models. In EMNLP.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia. In
WWW.
Noam Ordan and Shuly Wintner. 2007. Hebrew Word-
Net: a test case of aligning lexical databases across lan-
guages. International Journal of Translation, 19(1):39?
58.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In ACL.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Now Publishers Inc.
Martin Porter and Richard Boulton. 1970. Snowball
stemmer. http://snowball.tartarus.org/credits.php.
Matthew Purver, Konrad Ko?rding, Thomas L. Griffiths,
and Joshua Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In ACL.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In ACL, pages 320?322.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In IJCAI, pages
448?453.
Frank Richter. 2008. Dictionary nice grep. http://www-
user.tu-chemnitz.de/ fri/ding/.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern boot-
strapping. In NAACL.
Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model for
authors and documents. In UAI.
Beno??t Sagot and Darja Fis?er. 2008. Building a Free
French WordNet from Multilingual Resources. In On-
toLex.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization. In
ACL, pages 308?316.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Ju-
rafsky, and Christopher Manning. 2005. A conditional
random field word segmenter. In SIGHAN Workshop
on Chinese Language Processing.
University of Oxford. 2006. British Na-
tional Corpus. http://www.natcorp.ox.ac.uk/.
http://www.natcorp.ox.ac.uk/.
Tobias Vetter, Manfred Sauer, and Philipp Wallutat.
2000. Filmrezension.de: Online-magazin fu?r filmkritik.
http://www.filmrezension.de.
Xiaojun Wan. 2009. Co-training for cross-lingual senti-
ment classification. In ACL.
Chong Wang, David Blei, and Li Fei-Fei. 2009. Simulta-
neous image classification and annotation. In CVPR.
Xing Wei and Bruce Croft. 2006. LDA-based document
models for ad-hoc retrieval. In SIGIR.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In CIKM.
Theresa Ann Wilson. 2008. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polarity,
and Attitudes of Private States. Ph.D. thesis, University
of Pittsburgh.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In ACL.
Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. Medlda:
maximum margin supervised topic models for regres-
sion and classification. In ICML.
55
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 127?137,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Improving Translation via Targeted Paraphrasing
Philip Resnik
Linguistics and UMIACS
University of Maryland
resnik@umd.edu
Olivia Buzek
Linguistics and Computer Science
University of Maryland
olivia.buzek@gmail.com
Chang Hu
Computer Science
University of Maryland
changhu@cs.umd.edu
Yakov Kronrod
Linguistics and UMIACS
University of Maryland
yakov@umd.edu
Alex Quinn
Computer Science
University of Maryland
aq@cs.umd.edu
Benjamin B. Bederson
Computer Science and UMIACS
University of Maryland
bederson@cs.umd.edu
Abstract
Targeted paraphrasing is a new approach to the
problem of obtaining cost-effective, reasonable
quality translation that makes use of simple and
inexpensive human computations by monolin-
gual speakers in combination with machine
translation. The key insight behind the process
is that it is possible to spot likely translation
errors with only monolingual knowledge of the
target language, and it is possible to generate al-
ternative ways to say the same thing (i.e. para-
phrases) with only monolingual knowledge
of the source language. Evaluations demon-
strate that this approach can yield substantial
improvements in translation quality.
1 Introduction
For most of the world?s languages, the availability of
translation is limited to two possibilities: high qual-
ity at high cost, via professional translators, and low
quality at low cost, via machine translation (MT). The
spectrum between these two extremes is very poorly
populated, and at any point on the spectrum the ready
availability of translation is limited to only a small
fraction of the world?s languages. There is, of course,
a long history of technological assistance to transla-
tors, improving cost effectiveness using translation
memory (Laurian, 1984; Bowker and Barlow, 2004)
or other interactive tools to assist translators (Esteban
et al, 2004; Khadivi et al, 2006). And there is a
recent and rapidly growing interest in crowdsourc-
ing with non-professional translators, which can be
remarkably effective (Munro, 2010). However, all
these alternatives face a central availability bottle-
neck: they require the participation of humans with
bilingual expertise.
In this paper, we report on a new exploration of
the middle ground, taking advantage of a virtually
unutilized resource: speakers of the source and tar-
get language who are effectively monolingual, i.e.
who each only know one of the two languages rel-
evant for the translation task. The solution we are
proposing has the potential to provide a more cost
effective approach to translation in scenarios where
machine translation would be considered acceptable
to use, if only it were generally of high enough qual-
ity. This would clearly exclude tasks like transla-
tion of medical reports, business contracts, or literary
works, where the validation of a qualified bilingual
translator is absolutely necessary. However, it does
include a great many real-world scenarios, such as
following news reports in another country, reading in-
ternational comments about a product, or generating
a decent first draft translation of a Wikipedia page
for Wikipedia editors to improve.
The use of monolingual participants in a human-
machine translation process is not entirely new.
Callison-Burch et al (2004) pioneered the explo-
ration of monolingual post-editing within the MT
community, an approach extended more recently to
provide richer information to the user by Albrecht et
al. (2009) and Koehn (2009). There have also been at
least two independently developed human-machine
translation frameworks that employ an iterative pro-
tocol involving monolinguals on both the source and
target side. Morita and Ishida (2009) describe a sys-
tem in which target and source language speakers
perform editing of MT output to improve fluency
and adequacy, respectively; they utilize source-side
paraphrasing at a course grain level, although their ap-
proach is limited to requests to paraphrase the entire
sentence when the translation cannot be understood.
127
Bederson et al (2010) describe a similar protocol in
which cross-language communication is enhanced by
metalinguistic communication in the user interface.
Shahaf and Horvitz (2010) use machine translation
as a specific instance of a general game-based frame-
work for combining a range of machine and human
capabilities.
We call the technique used here targeted para-
phrasing. In a nutshell, target-language monolin-
guals identify parts of an initial machine translation
that don?t appear to be right, and source-language
monolinguals provide the MT system with alterna-
tive phrasings that might lead to better translations;
these are then passed through MT again and the best
scoring hypothesis is selected as the final translation.
This technique can be viewed as compatible with
the richer protocol- and game-based approaches, but
it is considerably simpler; in Sections 2 through 4
we describe the method and present evaluation re-
sults on Chinese-English translation. Unlike other
approaches, the technique also offers clear opportu-
nities to replace human participation with machine
components if the latter are up to the task; we discuss
this in Section 5 before wrapping up in Section 6
with conclusions and directions for future work.
2 Targeted Paraphrasing
The starting point for our approach is an observa-
tion: the source sentence provided as input to an MT
system is just one of many ways in which the mean-
ing could have been expressed, and for any given
MT system, some forms of expression are easier to
translate than others. The same basic observation
has been applied quite fruitfully over the past several
years to deal with statistical MT challenges involv-
ing segmentation, morphological analysis, and more
recently, source language word order (Dyer, 2007;
Dyer et al, 2008; Dyer and Resnik, 2010). Here we
apply it to the surface expression of meaning.
For example, consider the following real example
of translation from English to French by an automatic
MT system:
? Source: Polls indicate Brown, a state senator,
and Coakley, Massachusetts? Attorney General,
are locked in a virtual tie to fill the late Sen. Ted
Kennedy?s Senate seat.
? System: Les sondages indiquent Brown,
un se?nateur d?e?tat, et Coakley,
Massachusetts? Procureur ge?ne?ral, sont en-
ferme?s dans une cravate virtuel a` remplir le
regrette? se?nateur Ted Kennedy?s sie`ge au Se?nat.
A French speaker can look at this automatic transla-
tion and see immediately that the underlined parts
are wrong, even without knowing the intended source
meaning. We can identify the spans in the source En-
glish sentence that are responsible for these badly
translated French spans, and change them to alterna-
tive expressions with the same meaning (e.g. chang-
ing Massachusetts? Attorney General to the Attorney
General of Massachusetts); if we do so and then use
the same MT system again, we obtain a translation
that is still imperfect (e.g. cravate means necktie),
but is more acceptable:
? System: Les sondages indiquent que Brown, un
se?nateur d?e?tat, et Coakley, le procureur ge?ne?ral
du Massachusetts, sont enferme?s dans une cra-
vate virtuel pourvoir le sie?ge au Se?nat de Sen.
Ted Kennedy, qui est de?ce?de? re?cemment.
Operationally, then, translation with targeted para-
phrasing includes the following steps.
Initial machine translation. For this paper, we
use the Google Translate Research API, which,
among other advantages, provides word-level align-
ments between the source text and its output. In
principle, however, any automatic translation system
can be used in this role, potentially at some cost
to quality, by performing post hoc target-to-source
alignment.
Identification of mistranslated spans. This step
identifies parts of the source sentence that lead to
ungrammatical, nonsensical, or apparently incorrect
translations on the target side. In the experiments
of Sections 3 and 4, this step is performed by hav-
ing monolingual target speakers identify likely error
spans on the target side, as in the French example
above, and projecting those spans back to the source
spans that generated them using word alignments
as the bridge (Hwa et al, 2005; Yarowsky et al,
2001). In Section 5, we describe a heuristic but effec-
tive method for performing this fully automatically.
Du et al (2010), in this proceedings, explore the
128
use of source paraphrases without targeting appar-
ent mistranslations, using lattice translation (Dyer
et al, 2008) to efficiently represent and decode the
resulting very large space of paraphrase alternatives.
Source paraphrase generation. This step gener-
ates alternative expressions for the source spans iden-
tified in the previous step. In this paper, it is per-
formed by monolingual source speakers who perform
the paraphrase task: the speaker is given a sentence
with a phrase span marked, and is asked to replace the
marked text with a different way of saying the same
thing, so that the resulting sentence still makes sense
and means the same thing as the original sentence.
To illustrate in English, someone seeing John and
Mary took a European vacation this summer might
supply the paraphrase Mary went on a European, ver-
ifying that the resulting John and Mary went on a
European vacation this summer preserves the origi-
nal meaning. This step can also be fully automated
(Max, 2009) by taking advantage of bilingual phrase-
table pivoting (Bannard and Callison-Burch, 2005);
see Max (2010), in these proceedings, for a related
approach in which the paraphrases of a source phrase
are used to refine the estimated probability distribu-
tion over its possible target phrases.
Generating sentential source paraphrases. For
each sentence, there may be multiple paraphrased
spans. These are multiplied out to provide full-
sentence paraphrases. For example, if two non-
overlapping source spans are each paraphrased in
three ways, we generate 9 sentential source para-
phrases, each of which represents an alternative way
of expressing the original sentence.
Machine translation of alternative sentences.
The alternative source sentences, produced via para-
phrase, are sent through the same MT system, and
a single-best translation hypothesis is selected, e.g.
on the basis of the translation system?s model score.
In principle, one could also combine the alternatives
into a lattice representation and decode to find the
best path using lattice translation (Dyer et al, 2008);
cf. Du et al (2010). One could also present trans-
lation alternatives to a target speaker for selection,
similarly to Callison-Burch et al (2004).
Notice that with the exception of the initial trans-
lation, each remaining step in this pipeline can in-
volve either human participation or fully automatic
processing. The targeted paraphrasing framework
therefore defines a rich set of intermediate points on
the spectrum between fully automatic and fully hu-
man translation, of which we explore only a few in
this paper.
3 Pilot Study
In order to assess the potential of our approach,
we conducted a small pilot study, using eleven
sentences in simplified Chinese selected from
the article on ?Water? in Chinese Wikipedia
(http://zh.wikipedia.org/zh-cn/%E6%B0%B4). This
article was chosen because its topic is well known
in both English-speaking and Chinese-speaking pop-
ulations. The first five sentences were taken from
the first paragraph of the article. The other six sen-
tences were taken from a randomly-chosen paragraph
in the article. As a preprocessing step, we removed
any parenthetical items from the input sentences, e.g.
?(H20)?. The shortest sentence in this set has 12 Chi-
nese characters, the longest has 54.1
Human participation in this task was accomplished
using Amazon Mechanical Turk, an online market-
place that enables human performance of small ?hu-
man intelligence tasks? (HITs) in return for micropay-
ments. For each sentence, after we translated it au-
tomatically (using Google Translate), three English-
speaking Mechanical Turk workers (?Turkers?) on
the target side performed identification of mistrans-
lated spans. Each span identified was projected back
to its corresponding source span, and three Chinese-
speaking Turkers were asked to provide paraphrases
of each source span. These tasks were easy to per-
form (no more than around 30 seconds to complete
on average) and inexpensive (less than $1 for the
entire pilot study).2 The Chinese source span para-
phrases were then used to construct full-sentence
paraphrases, which were retranslated, once again by
Google Translate, to produce the output of the tar-
geted paraphrasing translation process.
1Note that this page is not a translation of the corresponding
English Wikipedia page or vice versa.
2The four English-speaking Turkers were recruited through
the normal Mechanical Turk mechanism. The three Chinese-
speaking Turkers were recruited offline by the authors in order to
quickly obtain results, although they participated as full-fledged
Turkers.
129
The initial translation outputs from Google Trans-
late (GT) and the results of the targeted paraphrasing
translation process (TP) were evaluated according
to widely used critera of fluency and adequacy. Flu-
ency ratings were obtained on a 5-point scale from
three native English speakers without knowledge of
Chinese. Translation adequacy ratings were obtained
from three native Chinese speakers who are also flu-
ent in English; they assessed adequacy of English
sentences by comparing the communicated meaning
to the Chinese source sentences.
Fluency was rated on the following scale:
1. Unintelligible: nothing or almost nothing of the sen-
tence is comprehensible.
2. Barely intelligible: only a part of the sentence (less
than 50%) is understandable.
3. Fairly intelligible: the major part of the sentence
passes.
4. Intelligible: all the content of the sentence is com-
prehensible, but there are errors of style and/or of
spelling, or certain words are missing.
5. Very intelligible: all the content of the sentence is
comprehensible. There are no mistakes.
Adequacy was rated on the following scale:
1. None of the meaning expressed in the reference sen-
tence is expressed in the sentence.
2. Little of the reference sentence meaning is expressed
in the sentence.
3. Much of the reference sentence meaning is expressed
in the sentence.
4. Most of the reference sentence meaning is expressed
in the sentence.
5. All meaning expressed in the reference sentence ap-
pears in the sentence.
For each GT output, we averaged across the ratings
of the alternative TP to produce average TP fluency
and adequacy scores. The average GT output rat-
ings, measuring the pure machine translation base-
line, were 2.36 for fluency and 2.91 for adequacy.
Averaging across the TP outputs, these rose to 3.32
and 3.49, respectively.
One could argue that a more sensible evaluation
is not to average across alternative TP outputs, but
rather to simulate the behavior of a target-language
speaker who simply chooses the one translation
among the alternatives that seems most fluent. If
we select the most fluent TP output for each source
sentence according to the English-speakers? average
fluency ratings, we obtain average test set ratings of
3.58 for fluency and 3.73 for adequacy. Those are
respective gains of 0.82 and 1.21 over the baseline
initial MT output, each on a 5-point scale.
Figure 1 shows a selection of outputs: we present
the two cases where the most fluent TP alternative
shows the greatest gain in average fluency rating (best
gain +2.67); two cases near the median gain in av-
erage fluency (median +1); and the worst two cases
with respect to effect on average fluency rating (worst
-0.33). The table accurately conveys a qualitative im-
pression corresponding to the quantitative results: the
overall quality of translations appears to be improved
by our process consistently, despite the absence of
any bilingual input in the improvements.
4 Chinese-English Evaluation
As a followup to our pilot study, we conducted an
evaluation using Chinese-English test data taken from
the NIST MT?08 machine translation evaluation, in
order to obtain fully automatic translation evaluation
scores. We report on results for 49 sentences of the
1,357 in this data set. These underwent the same
targeted paraphrasing process as in the pilot study,
with the addition of a basic step to filter out cheaters:
we disregarded as invalid any responses consisting
purely of ASCII characters (signifying a non-Chinese
response) or responses that were identical to the orig-
inal source text.
Target English speakers identified 115 potential
mistranslation spans, or 2.3 spans per sentence, that
yielded at least one source paraphrase on the source
Chinese side. Chinese speakers provided 138 valid
paraphrases. The entire cost for the human tasks in
this experiment was $5.06, or a bit under $0.11 per
sentence on average.3
Table 1 reports on the results, evaluating in stan-
dard fashion using BLEU with the four English
MT?08 references for each Chinese sentence. Since
the targeted paraphrasing translation process (TP)
produces multiple hypotheses ? one automatic trans-
lation output per sentential paraphrases ? we se-
lected the single best output for each sentence by
3Invalid paraphrase responses were rejected, i.e. zero-cost.
130
Condition Fluency Adequacy Sentence
GT 1.33 2.33 Water play life evolve into important to use.
TP 4.00 4.33 Water in the evolution of life played an important role.
GT 1.33 2.67 Human civilization from the source of the majority of large rivers
in the domain.
TP 3.33 4.67 Most of the origin of human civilization in river basin.
GT 2.33 3.00 In human daily life, the water in drinking, cleaning, washing and
other side to make use of an indispensable.
TP 3.67 3.33 In human daily life, water for drinking, cleaning, washing and other
essential role.
GT 2.00 2.33 Eastern and Western ancient Pak prime material view of both the
water regarded as a kind of basic groups into the elements, water is
the Chinese ancient five rows of a; the West ancient four elements
that also have water.
TP 3.00 3.33 East and West in ancient concept of simple substances regarded wa-
ter as a basic component elements. Among them, the five elements
of water is one of ancient China; Western ancient four elements
that also have water.
GT 4.00 4.00 Early cities will generally be in the water side of the establishment,
in order to solve irrigation, drinking and sewage problems.
TP 4.67 4.33 Early cities are generally built near the water to solve the irrigation,
drinking and sewage problems.
GT 3.0 3.33 Human very early on began to produce a water awareness.
TP 2.67 3.00 Man long ago began to understand the water produced.
Figure 1: Original Google Translate output (GT) for the pilot study in Section 3, together with translations produced by
the targeted paraphrase translation process (TP), selected to show a range from strong to weak improvements in fluency.
131
Condition BLEU
GT (baseline) 28.33
GT n-best oracle 28.47
TP one-best 30.01
TP oracle 30.79
Human upper bound 49.41
Table 1: Results on a 49-sentence subset of the NIST
MT?08 Chinese-English test set
selecting the highest scoring English translation, ac-
cording to the translation score delivered with each
output by the Google Translate Research API. (The
original translation was, of course, included among
the candidates for selection.) This yielded an im-
provement of 1.68 BLEU points on the 49-sentence
test set (TP one-best).
One could argue that this result is simply a result of
having more hypotheses to choose from, not a result
of the targeted paraphrasing process itself. In order
to rule out this possibility, we generated (n+ 1)-best
Google translations, setting n for each sentence to
match the number of alternative translations gener-
ated via targeted paraphrasing. We then chose the
best translation for each sentence, among the (n+1)-
best Google hypotheses, via oracle selection, using
the TERp metric (Snover et al, 2009) to evaluate
each hypothesis against the reference translations.4
The resulting BLEU score for the full set showed
negligible improvement (GT n-best oracle).
We did a similar oracle-best calculation using
TERp for targeted paraphrasing (TP oracle). The
result shows a potential gain of 2.46 BLEU points
over the baseline, if the best scoring alternative from
the targeted paraphrasing process were always cho-
sen.
In addition to aggregate scoring using BLEU, we
also looked at oracle results on a per-sentence ba-
sis using TERp (since BLEU more appropriate to
use at the document level, not the sentence level).
Identifying the best sentential paraphrase alternative
using TERp as an oracle, we find that the TERp
score would improve for 32 of the 49 test sentences,
4An ?oracle? telling us which variant is best is not available
in the real world, of course, but in situations like this one, oracle
studies are often used to establish the magnitude of the potential
gain (Och et al, 2004).
65.3%. For those 32 sentences, the average gain is
8.36 TERp points.5 A fairer measure is the average
obtained when scoring zero gain for the 17 sentences
where no improvement was obtained; taking these
into account, i.e. assuming an oracle who chooses the
original translation if none of the paraphrase-based
alternatives are better, the average improvement over
the entire set of 49 sentences is 5.46 TERp points.
Although we have obtained results on only a small
subset of the full NIST MT?08 test set, our automatic
evaluation confirms the qualitative impressions in
Figure 1 and the subjective ratings results obtained
in our pilot study in Section 3. The TP oracle results
establish that by taking advantage of monolingual
human speakers, it is possible to obtain quite sub-
stantial gains in translation quality. The TP one-best
results demonstrate that the majority of that oracle
gain is obtained in automatic hypothesis selection,
simply by selecting the paraphrase-based alternative
translation with the highest translation score.
The last line in Table 1 shows a human upper
bound computed using the reference translations via
cross validation; that is, for each of the four reference
translations, we evaluate it as a hypothesized transla-
tion using the other three references as ground truth;
these four scores were then averaged. The value of
this upper bound is quite consistent with the bound
computed similarly by Callison-Burch (2009).
5 English-Chinese Evaluation
As we noted in Section 2, the targeted paraphrasing
translation process defines a set of human-machine
combinations that do not require bilingual expertise.
The previous section described human identification
of mistranslated spans on the target side, human gen-
eration of paraphrases for problematic sub-sentential
spans on the source side, and both automatic hypothe-
sis selection and human selection (via fluency ratings,
in Section 3).
In this section, we take a step toward more au-
tomated processing, replacing human identification
of mistranslated spans with an a fully automatic
method.6 The idea behind our automatic error iden-
tification is straightforward: if the source sentence
5?Gains? refer to a lower score: since TERp is an error
measure, lower is better.
6This section contains material we originally reported in
Buzek et al (2010).
132
GT: WTO chief negotiator on behalf of the United States to propose substantial reduction of
agricultural subsidies, Kai Fa countries substantially reduce industrial products import tariffs to Dapo
?? Doha Round of negotiations deadlock.
TP: World Trade Organization negotiator suggested the United States today, a substantial reduction
of agricultural subsidies, developing countries substantially reduce industrial products?? Import
tariffs, in order to break the deadlock in the Doha Round of trade negotiations.
REF: the main delegates at the world trade organization talks today suggested that the us make major
cuts in its agricultural subsidies and that developing countries significantly reduce import duties on
industrial products in order to break the deadlock in the doha round of trade talks .
GT: Emergency session of the Palestinian prime minister Salam Fayyad state will set a new Govern-
ment
TP: Emergency session of the Palestinian Prime Minister Salam Fayyad will set the new government
REF: state of emergency period ends ; palestinian prime minister fayyad to form new government
GT: Indian territory from south to north, one week before the start after another wet season, the
provincial residents hold long drought every rain in the mood to meet the heavy rain, but did not
expect rain came unexpectedly fierce, a rain disaster, roads become rivers, low-lying areas housing to
make Mo in the water, transport almost paralyzed, Zhi Jin statistics about You nearly 500 people due
to floods were killed.
TP: Indian territory from south to north, one week before the start have entered into the rainy season,
provincial residents hold long drought to hope rain in the mood to meet the heavy rain, but did not
feed rain came unexpectedly fierce, a rain disaster, roads change the river, low-lying areas housing
do not water, traffic almost to a standstill, since statistics are nearly 500 people due to floods killed.
REF: the whole of india , from south to north , started to progressively enter the monsoon season a
week ago . the residents of each state all greeted the heavy rains as relief at the end of a long drought
, but didn?t expect that the rain would come with unexpected violence , a real deluge . highways have
become rivers ; houses in low-lying areas have been surbmerged in the water ; the transport system is
nearly paralyzed . to date , figures show that nearly 500 people have unfortunately lost their lives to
the floods .
GT: But the Taliban said in the meantime, the other a German hostages kidnapped in very poor
health, began to fall into a coma and lost consciousness.
TP: But the Taliban said in the meantime, another German hostages kidnapped a very weak body
fell into a coma and began to lose consciousness.
REF: but at the same time the taliban said that another german hostage who had been kidnapped
was in extremely poor health , and had started to become comatose and to lose consciousness .
GT: Taliban spokesman Ahmadi told AFP in an unknown location telephone interview, said: We,
through tribal elders, representatives of direct contact with South Korea.
TP: Taliban spokesman Ahmadi told AFP in an unknown location telephone interview, said: We are
through tribal elders, directly with the South Korean leadership, business
REF: taliban spokesperson ahmadi said in a telephone interview by afp at an undisclosed location :
we have established direct contact with the south korean delegation through tribal elders .
Figure 2: Random sample of 5 items from study in Section 4: original Google translation (GT), results of targeted
paraphrasing translation process (TP), and a human reference translation.
133
is translated to the target and then back-translated, a
comparison of the result with the original is likely to
identify places where the translation process encoun-
tered difficulty.7 Briefly, we automatically translate
source F to target E, then back-translate to produce F?
in the source language. We compare F and F? using
TERp ? which, in addition to its use as an evaluation
metric, is a form of string-edit distance that identifies
various categories of differences between two sen-
tences. When at least two consecutive edits are found,
we flag their smallest containing syntactic constituent
as a potential source of translation difficulty.8
In more detail, we posit that if an area of backtrans-
lation F? has many edits relative to original sentence
F, then that area probably comes from parts of the
target translation that did not represent the desired
meaning in F very well. We only consider consec-
utive edits in certain of the TERp edit categories,
specifically, deletions (D), insertions (I), and shifts
(S); the two remaining categories, matches (M) and
paraphrases (P), indicate that the words are identical
or that the original meaning was preserved. Further-
more, we assume that while a single D, S, or I edit
might be fairly meaningless, a string of at least two of
those types of edits is likely to represent a substantive
problem in the translation.
In order to identify reasonably meaningful para-
phrase units based on potential errors, we rely on a
source language constituency parser. Using the parse,
we find the smallest constituent of the sentence con-
taining all of the tokens in a particular error string. At
times, these constituents can be quite large, even the
entire sentence. To weed out these cases, we restrict
constituent length to no more than 7 tokens.
For example, given
F The most recent probe to visit Jupiter was the
Pluto-bound New Horizons spacecraft in late Febru-
ary 2007.
E La investigacio?n ma?s reciente fue la visita de Ju?piter
a Pluto?n de la envolvente sonda New Horizons a
fines de febrero de 2007.
7Exactly the same insight is behind the ?source-side pseudo-
referencebased feature? employed by Soricut and Echihabi
(2010) in their system for predicting the trustworthiness of trans-
lations.
8It is possible that the difficulty so identified involves back-
translation only, not translation in the original direction. If that
is the case, then more paraphrasing will be done than necessary,
but the quality of the TP process?s output should not suffer.
F? The latest research visit Jupiter was the Pluto-bound
New Horizons spacecraft in late February 2007.
spans in the the bolded phrase in F would be iden-
tified, based on the TERp alignment and smallest
containing constituent as shown in Figure 3.
In order to evaluate this approach, we again use
NIST MT08 data, this time going in the English-
to-Chinese direction since we are assuming source
language resources not currently available for Chi-
nese.9 We used English reference 0 as the source
sentence, and the original Chinese sentence as the
target.10
The data set comprises 1,357 sentence pairs. Us-
ing the above described algorithm to automatically
identify possible problem areas in the translation,
with the Google Translate API providing both the
translation and back-translation, we generated 1,780
potential error spans in 1,006 of the sentences, and,
continuing the targeted paraphrasing process, we ob-
tained up to three source paraphrases per span, for
the problemantic spans in 1,000 of those sentences.
(For six sentences, no paraphrases weres suggested
for any of the problematic spans.) These yielded
full-sentence paraphrase alternatives for the 1,000
sentences, which we again evaluated via an oracle
study.
For this study we used the TER metric (Snover
et al, 2006) rather than TERp. Comparing with the
GT output, we find that TP yields a better-translated
paraphrase sentence is available in 313 of the 1000
cases, or 31.3%, and for those 313 cases, TER for the
oracle-best paraphrase alternative improves on the
TER for the original sentence by 12.16 TER points.
Also taking into account the cases where there is
no improvement over the baseline, the average TER
score improves by 3.8 points. The cost for human
tasks in this study ? just paraphrases, since identi-
fying problematic spans was done automatically ?
was $117.48, or a bit under $0.12 per sentence.
9The Stanford parser (Klein and Manning, 2002), which
we use to identify source syntactic constituents, exists for both
English and Chinese, but TERp uses English resources such as
WordNet in order to capture acceptable variants of expression
for the same meaning. Matt Snover (personal communication) is
working on extension of TERp to other languages.
10We chose reference 0 because on inspection these references
seemed most reflective of native English grammar and usage.
134
NP PP 
NP 
Figure 3: TERp alignment of a source sentence and its back-translation in order to identify a problematic source span.
6 Conclusions and Future Work
In this paper we have focused on a relatively less-
explored space on the spectrum between high quality
and low cost translation: sharing the burden of the
translation task among a fully automatic system and
monolingual human participants, without requiring
human bilingual expertise. The monolingual par-
ticipants in this framework perform straightforward
tasks: they identify parts of sentences in their lan-
guage that seem to have errors, they provide sub-
sentential paraphrases in context, and they judge the
fluency of sentences they are presented with (or, in a
variant still to be explored, they simply select which
target sentence they like the best). Unlike other pro-
posals for exploiting monolingual speakers in human-
machine collaborative translation, the human steps
here are amenable to automation, and in addition
to evaluating a mostly-human variant of our targeted
paraphrasing translation framework, we also assessed
a version in which the identification of mistranslated
spans (to be paraphrased) is done automatically.
Our experimentation yielded a consistent pattern
of results, supporting the conclusion that targeted
paraphrasing can lead to significant improvements
in translation, via several different measures. First,
a very small pilot study for Chinese-English trans-
lation in Wikipedia provided preliminary validation
that translation fluency and accuracy can be improved
quite significantly for a set of fairly chosen test sen-
tences, according to human ratings. Second, a small
experiment in Chinese-English translation using stan-
dard NIST test sentences suggested the potential for
dramatic gains using the BLEU and TERp scores,
with oracle improvements of 2.46 points and 5.46
points, respectively. In addition, a non-oracle experi-
ment, selecting the best hypothesis according to the
MT system?s model score, yielded a gain of nearly 1.7
BLEU points. And third, in a large scale evaluation
of the approach using English-Chinese translation
of 1,000 sentences, this time automating the step of
identifying potentially mistranslated parts of source
sentences, the oracle results demonstrated that a gain
of nearly 4 TER points is available.
These initial studies leave considerable room for
future work. One important step will be to better char-
acterize the relationship between cost and quality in
quantitative terms: how much does it cost to obtain
135
how much quality improvement, and how does that
compare with typical professional translation costs of
$0.25 per word? This question is closely connected
with the dynamics of crowdsourcing platforms such
as Mechanical Turk ? the cost per sentence in these
experiments works out to be around $0.12, but trans-
lation on a large scale will involve a complicated
ecosystem of workers and cheaters, tasks and motiva-
tions and incentives (Quinn and Bederson, 2009). A
related crowdsourcing issue requiring further study
is the availability of monolingual human participants
for a range of language pairs, in order to validate
the argument that drawing on monolingual human
participation will significantly reduce the severity of
the availability bottleneck. And, of course, in the
upper bound in Table 1 makes quite clear the cru-
cial value added by bilingual translators, when they
are available; we hope to explore whether the tar-
geted paraphrasing translation pipeline can improve
the productivity of post-editing by bilinguals, mak-
ing it easier to move toward the upper bound in a
cost-effective way.
Another set of issues concerns the underlying trans-
lation technology. A reviewer correctly notes that the
value of the approach taken here is likely to vary
depending upon the quality of the underlying trans-
lation system, and the approach may break down at
the extrema, when the baseline translation is either
already very good or completely awful. We chose
to use Google Translate for its wide availability and
the fact that it represents a state of the art baseline to
beat; however, in future work we plan to substitute
our own statistical MT systems, which will permit us
to experiment across a range of translation model and
language model LM training set sizes, and therefore
to vary quality while keeping other system details
constant. More directly connected to research in ma-
chine translation, this framework provides a variety
of opportunities for improving fully automatic sta-
tistical MT systems. We plan to implement a fully
automatic targeted paraphrasing translation pipeline,
using the automated methods discussed when intro-
ducing the pipeline in Section 2, including transla-
tion of targeted paraphrase lattices (cf. (Max, 2010;
Du et al, 2010)). Finally, we intend to explore the
application of our approach in scenarios involving
less-common languages, by using a more common
language as a pivot or bridge (Habash and Hu, 2009).
Acknowledgments
This work has been supported in part by the National
Science Foundation under awards BCS0941455 and
IIS0838801. The authors would like to thank three
anonymous reviewers for their helpful comments,
and Chris Callison-Burch and Chris Dyer for their
helpful comments and discussion.
References
Joshua S. Albrecht, Rebecca Hwa, and G. Elisabeta Marai.
2009. Correcting automatic translations through collab-
orations between mt and monolingual target-language
users. In EACL ?09: Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 60?68, Morristown,
NJ, USA. Association for Computational Linguistics.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 597?604,
Morristown, NJ, USA. Association for Computational
Linguistics.
Benjamin B. Bederson, Chang Hu, and Philip Resnik.
2010. Translation by iterative collaboration between
monolingual users. In Graphics Interface (GI) confer-
ence.
Lynne Bowker and Michael Barlow. 2004. Bilingual
concordancers and translation memories: a comparative
evaluation. In LRTWRT ?04: Proceedings of the Second
International Workshop on Language Resources for
Translation Work, Research and Training, pages 70?79,
Morristown, NJ, USA. Association for Computational
Linguistics.
Olivia Buzek, Philip Resnik, and Ben Bederson. 2010. Er-
ror driven paraphrase annotation using mechanical turk.
In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 217?221, Los Angeles, June.
Association for Computational Linguistics.
Chris Callison-Burch, Colin Bannard, , and Josh
Schroeder. 2004. Improving statistical translation
through editing. In Workshop of the European Associa-
tion for Machine Translation.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Mechan-
ical Turk. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 286?295, Singapore, August. Association for
Computational Linguistics.
Jinhua Du, Jie Jiang, and Andy Way. 2010. Facilitating
translation using source language paraphrase lattices.
136
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Chris Dyer and Philip Resnik. 2010. Forest translation.
In NAACL?10.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proceedings of HLT-ACL,
Columbus, OH.
C. Dyer. 2007. Noisier channel translation: translation
from morphologically complex languages. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, Prague, June.
Jose? Esteban, Jose? Lorenzo, Antonio S. Valderra?banos,
and Guy Lapalme. 2004. Transtype2 - an innovative
computer-assisted translation system. In The Compan-
ion Volume to the Proceedings of 42st Annual Meeting
of the Association for Computational Linguistics, pages
94?97, Barcelona, Spain, jul. Association for Computa-
tional Linguistics. TT2.
Nizar Habash and Jun Hu. 2009. Improving arabic-
chinese statistical machine translation using english
as pivot language. In StatMT ?09: Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 173?181, Morristown, NJ, USA. Association for
Computational Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3):311?325.
Shahram Khadivi, Richard Zens, and Hermann Ney. 2006.
Integration of speech to computer-assisted translation
using finite-state automata. In Proceedings of the COL-
ING/ACL on Main conference poster sessions, pages
467?474, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural lan-
guage parsing. In Suzanna Becker, Sebastian Thrun,
and Klaus Obermayer, editors, Advances in Neural
Information Processing Systems 15 - Neural Informa-
tion Processing Systems, NIPS 2002, pages 3?10. MIT
Press.
Philipp Koehn. 2009. A web-based interactive computer
aided translation tool. In Proceedings of the ACL-
IJCNLP 2009 Software Demonstrations, pages 17?20,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Anne-Marie Laurian. 1984. Machine translation : What
type of post-editing on what type of documents for
what type of users. In 10th International Conference on
Computational Linguistics and 22nd Annual Meeting
of the Association for Computational Linguistics.
Aure?lien Max. 2009. Sub-sentencial paraphrasing by con-
textual pivot translation. In Proceedings of the 2009
Workshop on Applied Textual Inference, pages 18?26,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Aure?lien Max. 2010. Example-based paraphrasing for
improved phrase-based statistical machine translation.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Daisuke Morita and Toru Ishida. 2009. Designing pro-
tocols for collaborative translation. In PRIMA ?09:
Proceedings of the 12th International Conference on
Principles of Practice in Multi-Agent Systems, pages
17?32, Berlin, Heidelberg. Springer-Verlag.
Robert Munro. 2010. Haiti emergency response: the
power of crowdsourcing and SMS. Relief 2.0 in Haiti,
Stanford, CA.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alexander Fraser,
Shankar Kumar, Libin Shen, David Smith, Katherine
Eng, Viren Jain, Zhen Jin, and Dragomir R. Radev.
2004. A smorgasbord of features for statistical ma-
chine translation. In HLT-NAACL, pages 161?168.
Alex Quinn and Benjamin B. Bederson. 2009. A tax-
onomy of distributed human computation. Technical
Report HCIL-2009-23, University of Maryland, Octo-
ber.
D. Shahaf and E. Horvitz. 2010. Generalized task markets
for human and machine computation. In AAAI 2010,
July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings of Association for Machine Translation in
the Americas, pages 223?231.
Matt Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. 2009. TER-Plus: Paraphrases, Semantic,
and Alignment Enhancements to Translation Edit Rate.
Machine Translation.
Radu Soricut and Abdessamad Echihabi. 2010. Trustrank:
Inducing trust in automatic translations via ranking. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 612?621,
Uppsala, Sweden, July. Association for Computational
Linguistics.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In HLT ?01:
Proceedings of the first international conference on
Human language technology research, pages 1?8, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
137
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284?292,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Modeling Perspective using Adaptor Grammars
Eric A. Hardisty
Department of Computer Science
and UMIACS
University of Maryland
College Park, MD
hardisty@cs.umd.edu
Jordan Boyd-Graber
UMD iSchool
and UMIACS
University of Maryland
College Park, MD
jbg@umiacs.umd.edu
Philip Resnik
Department of Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
Strong indications of perspective can often
come from collocations of arbitrary length; for
example, someone writing get the government
out of my X is typically expressing a conserva-
tive rather than progressive viewpoint. How-
ever, going beyond unigram or bigram features
in perspective classification gives rise to prob-
lems of data sparsity. We address this prob-
lem using nonparametric Bayesian modeling,
specifically adaptor grammars (Johnson et al,
2006). We demonstrate that an adaptive na??ve
Bayes model captures multiword lexical usages
associated with perspective, and establishes a
new state-of-the-art for perspective classifica-
tion results using the Bitter Lemons corpus, a
collection of essays about mid-east issues from
Israeli and Palestinian points of view.
1 Introduction
Most work on the computational analysis of senti-
ment and perspective relies on lexical features. This
makes sense, since an author?s choice of words is
often used to express overt opinions (e.g. describing
healthcare reform as idiotic or wonderful) or to frame
a discussion in order to convey a perspective more
implicitly (e.g. using the term death tax instead of
estate tax). Moreover, it is easy and efficient to rep-
resent texts as collections of the words they contain,
in order to apply a well known arsenal of supervised
techniques (Laver et al, 2003; Mullen and Malouf,
2006; Yu et al, 2008).
At the same time, standard lexical features have
their limitations for this kind of analysis. Such fea-
tures are usually created by selecting some small
n-gram size in advance. Indeed, it is not uncommon
to see the feature space for sentiment analysis limited
to unigrams. However, important indicators of per-
spective can also be longer (get the government out
of my). Trying to capture these using standard ma-
chine learning approaches creates a problem, since
allowing n-grams as features for larger n gives rise
to problems of data sparsity.
In this paper, we employ nonparametric Bayesian
models (Orbanz and Teh, 2010) in order to address
this limitation. In contrast to parametric models, for
which a fixed number of parameters are specified in
advance, nonparametric models can ?grow? to the
size best suited to the observed data. In text analysis,
models of this type have been employed primarily
for unsupervised discovery of latent structure ? for
example, in topic modeling, when the true number of
topics is not known (Teh et al, 2006); in grammatical
inference, when the appropriate number of nontermi-
nal symbols is not known (Liang et al, 2007); and
in coreference resolution, when the number of enti-
ties in a given document is not specified in advance
(Haghighi and Klein, 2007). Here we use them for
supervised text classification.
Specifically, we use adaptor grammars (Johnson
et al, 2006), a formalism for nonparametric Bayesian
modeling that has recently proven useful in unsuper-
vised modeling of phonemes (Johnson, 2008), gram-
mar induction (Cohen et al, 2010), and named entity
structure learning (Johnson, 2010), to make super-
vised na??ve Bayes classification nonparametric in
order to improve perspective modeling. Intuitively,
na??ve Bayes associates each class or label with a
probability distribution over a fixed vocabulary. We
introduce adaptive na??ve Bayes (ANB), for which in
principle the vocabulary can grow as needed to in-
clude collocations of arbitrary length, as determined
284
by the properties of the dataset. We show that using
adaptive na??ve Bayes improves on state of the art
classification using the Bitter Lemons corpus (Lin
et al, 2006), a document collection that has been
used by a variety of authors to evaluate perspective
classification.
In Section 2, we review adaptor grammars, show
how na??ve Bayes can be expressed within the for-
malism, and describe how ? and how easily ? an
adaptive na??ve Bayes model can be created. Section 3
validates the approach via experimentation on the Bit-
ter Lemons corpus. In Section 4, we summarize the
contributions of the paper and discuss directions for
future work.
2 Adapting Na??ve Bayes to be Less Na??ve
In this work we apply the adaptor grammar formal-
ism introduced by Johnson, Griffiths, and Goldwa-
ter (Johnson et al, 2006). Adaptor grammars are a
generalization of probabilistic context free grammars
(PCFGs) that make it particularly easy to express non-
parametric Bayesian models of language simply and
readably using context free rules. Moreover, John-
son et al provide an inference procedure based on
Markov Chain Monte Carlo techniques that makes
parameter estimation straightforward for all models
that can be expressed using adaptor grammars.1 Vari-
ational inference for adaptor grammars has also been
recently introduced (Cohen et al, 2010).
Briefly, adaptor grammars allow nonterminals to
be rewritten to entire subtrees. In contrast, a non-
terminal in a PCFG rewrites only to a collection
of grammar symbols; their subsequent productions
are independent of each other. For instance, a tradi-
tional PCFG might learn probabilities for the rewrite
rule PP 7? P NP. In contrast, an adaptor gram-
mar can learn (or ?cache?) the production PP 7?
(P up)(NP(DET a)(N tree)). It does this by posit-
ing that the distribution over children for an adapted
non-terminal comes from a Pitman-Yor distribution.
A Pitman-Yor distribution (Pitman and Yor, 1997)
is a distribution over distributions. It has three pa-
rameters: the discount, a, such that 0 ? a < 1,
the strength, b, a real number such that ?a < b,
1And, better still, they provide code that
implements the inference algorithm; see
http://www.cog.brown.edu/ mj/Software.htm.
and a probability distribution G0 known as the base
distribution. Adaptor grammars allow distributions
over subtrees to come from a Pitman-Yor distribu-
tion with the PCFG?s original distribution over trees
as the base distribution. The generative process for
obtaining draws from a distribution drawn from a
Pitman-Yor distribution can be described by the ?Chi-
nese restaurant process? (CRP). We will use the CRP
to describe how to obtain a distribution over obser-
vations composed of sequences of n-grams, the key
to our model?s ability to capture perspective-bearing
n-grams.
Suppose that we have a base distribution ? that is
some distribution over all sequences of words (the
exact structure of such a distribution is unimportant;
such a distribution will be defined later in Table 1).
Suppose further we have a distribution ? drawn from
PY (a, b,?), and we wish to draw a series of obser-
vations w from ?. The CRP gives us a generative
process for doing those draws from ?, marginaliz-
ing out ?. Following the restaurant metaphor, we
imagine the ith customer in the series entering the
restaurant to take a seat at a table. The customer sits
by making a choice that determines the value of the
n-gram wi for that customer: she can either sit at an
existing table or start a new table of her own.2
If she sits at a new table j, that table is assigned
a draw yj from the base distribution, ?; note that,
since ? is a distribution over n-grams, yj is an n-
gram. The value of wi is therefore assigned to be yj ,
and yj becomes the sequence of words assigned to
that new table. On the other hand, if she sits at an
existing table, then wi simply takes the sequence of
words already associated with that table (assigned as
above when it was first occupied).
The probability of joining an existing table j,
with cj patrons already seated at table j, is
cj?a
c?+b
,
where c? is the number of patrons seated at all tables:
c? =
?
j? cj? . The probability of starting a new table
is b+t?ac?+b , where t is the number of tables presently
occupied.
Notice that ? is a distribution over the same space
as ?, but it can drastically shift the mass of the dis-
tribution, compared with ?, as more and more pa-
2Note that we are abusing notation by allowing wi to cor-
respond to a word sequence of length ? 1 rather than a single
word.
285
trons are seated at tables. However, there is always
a chance of drawing from the base distribution, and
therefore every word sequence can also always be
drawn from ?.
In the next section we will write a na??ve Bayes-like
generative process using PCFGs. We will then use
the PCFG distribution as the base distribution for a
Pitman-Yor distribution, adapting the na??ve Bayes
process to give us a distribution over n-grams, thus
learning new language substructures that are useful
for modeling the differences in perspective.
2.1 Classification Models as PCFGs
Na??ve Bayes is a venerable and popular mechanism
for text classification (Lewis, 1998). It posits that
there are K distinct categories of text ? each with a
distinct distribution over words ? and that every doc-
ument, represented as an exchangeable bag of words,
is drawn from one (and only one) of these distribu-
tions. Learning the per-category word distributions
and global prevalence of the classes is a problem of
posterior inference which can be approached using a
variety of inference techniques (Lowd and Domingos,
2005).
More formally, na??ve Bayes models can be ex-
pressed via the following generative process:3
1. Draw a global distribution over classes ? ?
Dir (?)
2. For each class i ? {1, . . . ,K}, draw a word
distribution ?i ? Dir (?)
3. For each document d ? {1, . . . ,M}:
(a) Draw a class assignment zd ? Mult (?)
(b) For each word position n ? {1, . . . , Nd,
draw wd,n ? Mult (?zd)
A variant of the na??ve Bayes generative process can
be expressed using the adaptor grammar formalism
(Table 1). The left hand side of each rule represents
a nonterminal which can be expanded, and the right
hand side represents the rewrite rule. The rightmost
indices show replication; for instance, there are |V |
rules that allow WORDi to rewrite to each word in the
3Here ? and ? are hyperparameters used to specify priors
for the class distribution and classes? word distributions, respec-
tively; ? is a symmetric K-dimensional vector where each ele-
ment is pi. Nd is the length of document d. Resnik and Hardisty
(2010) provide a tutorial introduction to the na??ve Bayes genera-
tive process and underlying concepts.
SENT 7? DOCd d = 1, . . . ,m
DOCd0.001 7? IDd WORDSi d = 1, . . . ,m;
i ? {1,K}
WORDSi 7? WORDSi WORDi i ? {1,K}
WORDSi 7? WORDi i ? {1,K}
WORDi 7? v v ? V ; i ? {1,K}
Table 1: A na??ve Bayes-inspired model expressed as a
PCFG.
vocabulary. One can assume a symmetric Dirichlet
prior of Dir (1?) over the production choices unless
otherwise specified ? as with the DOCd production
rule above, where a sparse prior is used.
Notice that the distribution over expansions for
WORDi corresponds directly to ?i in Figure 1(a).
There are, however, some differences between the
model that we have described above and the standard
na??ve Bayes model depicted in Figure 1(a). In par-
ticular, there is no longer a single choice of class per
document; each sentence is assigned a class. If the
distribution over per-sentence labels is sparse (as it
is above for DOCd), this will closely approximate
na??ve Bayes, since it will be very unlikely for the
sentences in a document to have different labels. A
non-sparse prior leads to behavior more like models
that allow parts of a document to express sentiment
or perspective differently.
2.2 Moving Beyond the Bag of Words
The na??ve Bayes generative distribution posits that
when writing a document, the author selects a distri-
bution of categories zd for the document from ?. The
author then generates words one at a time: each word
is selected independently from a flat multinomial
distribution ?zd over the vocabulary.
However, this is a very limited picture of how text
is related to underlying perspectives. Clearly words
are often connected with each other as collocations,
and, just as clearly, extending a flat vocabulary to
include bigram collocations does not suffice, since
sometimes relevant perspective-bearing phrases are
longer than two words. Consider phrases like health
care for all or government takeover of health care,
connected with progressive and conservative posi-
tions, respectively, during the national debate on
healthcare reform. Simply applying na??ve Bayes,
or any other model, to a bag of n-grams for high n is
286
KM
?
z
d
N
d
W
d,n
?
?
?
i
(a) Na??ve Bayes
K
M
?
z
d
N
d
W
d,n
a
?
?
i
b
?
?
(b) Adaptive Na??ve Bayes
Figure 1: A plate diagram for na??ve Bayes and adaptive na??ve Bayes. Nodes represent random variables and parameters;
shaded nodes represent observations; lines represent probabilistic dependencies; and the rectangular plates denote
replication.
going to lead to unworkable levels of data sparsity;
a model should be flexible enough to support both
unigrams and longer phrases as needed.
Following Johnson (2010), however, we can use
adaptor grammars to extend na??ve Bayes flexibly to
include richer structure like collocations when they
improve the model, and not including them when
they do not. This can be accomplished by introduc-
ing adapted nonterminal rules: in a revised genera-
tive process, the author can draw from Pitman-Yor
distribution whose base distribution is over word se-
quences of arbitrary length.4 Thus in a setting where,
say, K = 2, and our two classes are PROGRESSIVE
and CONSERVATIVE, the sequence health care for all
might be generated as a single unit for the progressive
perspective, but in the conservative perspective the
same sequence might be generated as three separate
draws: health care, for, all. Such a model is pre-
sented in Figure 1(b). Note the following differences
between Figures 1(a) and 1(b):
? zd selects which Pitman-Yor distribution to draw
from for document d.
? ?i is the distribution over n-grams that comes
from the Pitman-Yor distribution.
? Wd,n represents an n-gram draw from ?i
? a, b are the Pitman-Yor strength and discount
parameters.
? ? is the Pitman-Yor base distribution with ? as
its uniform hyperparameter.
4As defined above, the base distribution is that of the PCFG
production rule WORDSi. Although it has non-zero probability
of producing any sequence of words, it is biased toward shorter
word sequences.
Returning to the CRP metaphor discussed when we
introduced the Pitman-Yor distribution, there are two
restaurants, one for the PROGRESSIVE distribution
and one for the CONSERVATIVE distribution. Health
care for all has its own table in the PROGRESSIVE
restaurant, and enough people are sitting at it to make
it popular. There is no such table in the CONSERVA-
TIVE restaurant, so in order to generate those words,
the phrase health care for all would need to come
from a new table; however, it is more easily explained
by three customers sitting at three existing, popular
tables: health care, for, and all.
We follow the convention of Johnson (2010) by
writing adapted nonterminals as underlined. The
grammar for adaptive na??ve Bayes is shown in Ta-
ble 2. The adapted COLLOCi rule means that every
time we need to generate that nonterminal, we are
actually drawing from a distribution drawn from a
Pitman-Yor distribution. The distribution over the
possible yields of the WORDSi rule serves as the
base distribution.
Given this generative process for documents, we
can now use statistical inference to uncover the pos-
terior distribution over the latent variables, thus dis-
covering the tables and seating assignments of our
metaphorical restaurants that each cater to a specific
perspective filled with tables populated by words and
n-grams.
The model presented in Table 2 is the most straight-
forward way of extending na??ve Bayes to collocations.
For completeness, we also consider the alternative
of using a shared base distribution rather than dis-
tinguishing the base distributions of the two classes.
287
SENT 7? DOCd d = 1, . . . ,m
DOCd0.001 7? IDd SPANi d = 1, . . . ,m;
i ? {1,K}
SPANi 7? SPANi COLLOCi i ? {1,K}
SPANi 7? COLLOCi i ? {1,K}
COLLOCi 7? WORDSi i ? {1,K}
WORDSi 7? WORDSi WORDi i ? {1,K}
WORDSi 7? WORDi i ? {1,K}
WORDi 7? v v ? V ; i ? {1,K}
Table 2: An adaptive na??ve Bayes grammar. The
COLLOCi nonterminal?s distribution over yields is drawn
from a Pitman-Yor distribution rather than a Dirichlet over
production rules.
SENT 7? DOCd d = 1, . . . ,m
DOCd0.001 7? IDd SPANi d = 1, . . . ,m;
i ? {1,K}
SPANi 7? SPANi COLLOCi i ? {1,K}
SPANi 7? COLLOCi i ? {1,K}
COLLOCi 7? WORDS i ? {1,K}
WORDS 7? WORDS WORD
WORDS 7? WORD
WORD 7? v v ? V
Table 3: An adaptive na??ve Bayes grammar with a com-
mon base distribution for collocations. Note that, in con-
trast to Table 2, there are no subscripts on WORDS or
WORD.
Briefly, using a shared base distribution posits that
the two classes use similar word distributions, but
generate collocations unique to each class, whereas
using separate base distributions assumes that the
distribution of words is unique to each class.
3 Experiments
3.1 Corpus Description
We conducted our classification experiments on the
Bitter Lemons (BL) corpus, which is a collection of
297 essays averaging 700-800 words in length, on
various Middle East issues, written from both the
Israeli and Palestinian perspectives. The BL corpus
was compiled by Lin et al (2006) and is derived from
a website that invites weekly discussions on a topic
and publishes essays from two sets of authors each
week.5 Two of the authors are guests, one from each
perspective, and two essays are from the site?s regular
contributors, also one from each perspective, for a
5http://www.bitterlemons.org
K
M
?
z
d
N
d
W
d,n
a
?
?
i
b
?
?
Figure 2: An alternative adaptive na??ve Bayes with a com-
mon base distribution for both classes.
Training Set
Test Set
Corpus Filter
Grammar 
Generator
Corpus Filter
Vocabulary 
Generator
AG Classifier
Figure 3: Corpus preparation and experimental setup.
total of four essays on each topic per week. We chose
this corpus to allow us to directly compare our results
with Greene and Resnik?s (2009) Observable Proxies
for Underlying Semantics (OPUS) features and Lin
et al?s Latent Sentence Perspective Model (LSPM).
The classification goal for this corpus is to label each
document with the perspective of its author, either
Israeli or Palestinian.
Consistent with prior work, we prepared the corpus
by dividing it into two groups, one group containing
all of the essays written by the regular site contrib-
utors, which we call the Editor set, and one group
comprised of all the essays written by the guest con-
tributors, which we call the Guest set. Similar to the
above mentioned prior work, we perform classifica-
tion using one group as training data and the other as
test data and perform two folds of classification. The
overall experimental setup and corpus preparation
process is presented in Figure 3.
288
3.2 Experimental Setup
The vocabulary generator determines the vocabulary
used by a given experiment by converting the training
set to lower case, stemming with the Porter stemmer,
and filtering punctuation. We remove from the vocab-
ulary any words that appeared in only one document
regardless of frequency within that document, words
with frequencies lower than a threshold, and stop
words.6 The vocabulary is then passed to a grammar
generator and a corpus filter.
The grammar generator uses the vocabulary to gen-
erate the terminating rules of the grammar from the
ANB grammar presented in Tables 2 and 3. The cor-
pus filter takes in a set of documents and replaces all
words not in the vocabulary with ?out of vocabulary?
markers. This process ensures that in all experiments
the vocabulary is composed entirely of words from
the training set. After the groups have been filtered,
the group used as the test set has its labels removed.
The test and training set are then sent, along with the
grammar, into the adaptor grammar inference engine.
Each experiment ran for 3000 iterations. For the
runs where adaptation was used we set the initial
Pitman-Yor a and b parameters to 0.01 and 10 respec-
tively, then slice sample (Johnson and Goldwater,
2009).
We use the resulting sentence parses for classifi-
cation. By design of the grammar, each sentence?s
words will belong to one and only one distribution.
We identify that distribution from each of the test
set sentence parses and use it as the sentence level
classification for that particular sentence. We then
use majority rule on the individual sentence classifi-
cations in a document to obtain the document classifi-
cation. (In most cases the sentence-level assignments
are overwhelmingly dominated by one class.)
3.3 Results and Analysis
Table 4 gives the results and compares to prior
work. The support vector machine (SVM), NB-
B and LSPM results are taken directly from Lin
et al (2006). NB-B indicates na??ve Bayes with
full Bayesian inference. LSPM is the Latent
Sentence Perspective Model, also from Lin et
al. (2006). OPUS results are taken from Greene
6In these experiments, a frequency threshold of 4 was se-
lected prior to testing.
Training Set Test Set Classifier Accuracy
Guests Editors SVM 88.22
Guests Editors NB-B 93.46
Guests Editors LSPM 94.93
Guests Editors OPUS 97.64
Guests Editors ANB* 99.32
Guests Editors ANB Com 99.93
Guests Editors ANB Sep 99.87
Editors Guests SVM 81.48
Editors Guests NB-B 85.85
Editors Guests LSPM 86.99
Editors Guests OPUS 85.86
Editors Guests ANB* 84.98
Editors Guests ANB Com 82.76
Editors Guests ANB Sep 88.28
Table 4: Classification results. ANB* indicates the same
grammar as Adapted Na??ve Bayes, but with adaptation dis-
abled. Com and Sep refer to whether the base distribution
was common to both classes or separate.
and Resnik (2009). Briefly, OPUS features are gener-
ated from observable grammatical relations that come
from dependency parses of the corpus. Use of these
features provided the best classification accuracy for
this task prior to this work. ANB* refers to the gram-
mar from Table 2, but with adaptation disabled. The
reported accuracy values for ANB*, ANB with a
common base distribution (see Table 3), and ANB
with separate base distributions (see Table 2) are
the mean values from five separate sampling chains.
Bold face indicates statistical signficance (p < 0.05)
by unpaired t-test between the reported value and
ANB*.
Consistent with all prior work on this corpus we
found that the classification accuracy for training on
editors and testing on guests was lower than the other
direction since the larger number of editors in the
guest set alows for greater generalization. The dif-
ference between ANB* and ANB with a common
base distribution is not statistically significant. Also
of note is that the classification accuracy improves
for testing on Guests when the ANB grammar is al-
lowed to adapt and a separate base distribution is used
for the two classes (88.28% versus 84.98% without
adaptation).
Table 5 presents some data on adapted rules
289
Unique Unique Percent of Group
Class Group Unigrams Cached n-grams Cached Vocabulary Cached
Israeli Editors 2,292 19,614 77.62
Palestinian Editors 2,180 17,314 86.54
Israeli Guests 2,262 19,398 79.91
Palestinian Guests 2,005 16,946 74.94
Table 5: Counts of cached unigrams and n-grams for the two classes compared to the vocabulary sizes.
Israeli Palestinian
zionist dream american jew
zionist state achieve freedom
zionist movement palestinian freedom
american leadership support palestinian
american victory palestinian suffer
abandon violence palestinian territory
freedom (of the) press palestinian statehood
palestinian violence palestinian refugee
Table 6: Charged bigrams captured by the framework.
learned once inference is complete. The column
labeled unique unigrams cached indicates the num-
ber of unique unigrams that appear on the right hand
side of the adapted rules. Similarly, unique n-grams
cached indicates the number of unique n-grams that
appear on the right hand side of the adapted rules.
The rightmost column indicates the percentage of
terms from the group vocabulary that appear on the
right hand side of adapted rules as unigrams. Values
less than 100% indicate that the remaining vocabu-
lary terms are cached in n-grams. As the table shows,
a significant number of the rules learned during infer-
ence are n-grams of various sizes.
Inspection of the captured bigrams showed that
it captured sequences that a human might associate
with one perspective over the other. Table 6 lists just
a few of the more charged bigrams that were captured
in the adapted rules.
More specific caching information on the individ-
ual groups and classes is provided in Table 7. This
data clearly demonstrates that raw n-gram frequency
alone is not indicative of how many times an n-gram
is used as a cached rule. For example, consider the
bigram people go, which is used as a cached bigram
only three times, yet appears in the corpus 407 times.
Compare that with isra palestinian, which is cached
the same number of times but appears only 18 times
in the corpus. In other words, the sequence people go
is more easily explained by two sequential unigrams,
not a bigram. The ratio of cache use counts to raw
bigrams gives a measure of strength of collocation
between the terms of the n-gram. We conjecture that
the rareness of caching for n > 2 is a function of the
small corpus size. Also of note is the improvement in
performance of ANB* over NB-B when training on
guests, which we suspect is due to our use of sampled
versus fixed hyperparameters.
4 Conclusions
In this paper, we have applied adaptor grammars in
a supervised setting to model lexical properties of
text and improve document classification according
to perspective, by allowing nonparametric discovery
of collocations that aid in perspective classification.
The adaptive na??ve Bayes model improves on state
of the art supervised classification performance in
head-to-head comparisons with previous approaches.
Although there have been many investigations on
the efficacy of using multiword collocations in text
classification (Bekkerman and Allan, 2004), usually
such approaches depend on a preprocessing step such
as computing tf-idf or other measures of frequency
based on either word bigrams (Tan et al, 2002) or
character n-grams (Raskutti et al, 2001). In con-
trast, our approach combines phrase discovery with
the probabilistic model of the text. This allows for
the collocation selection and classification to be ex-
pressed in a single model, which can then be extended
later; it also is truly generative, as compared with fea-
ture induction and selection algorithms that either
under- or over-generate data.
There are a number of interesting directions in
which to take this research. As Johnson et al (2006)
argue, and as we have confirmed here, the adaptor
290
Guest Editor
Israeli Palestinian Israeli Palestinian
palestinian OOV 11 299 palestinian isra 6 178 palestinian OOV 8 254 OOV israel 7 198
OOV palestinian 7 405 OOV palestinian 6 405 OOV palestinian 7 319 OOV palestinian 6 319
isra OOV 6 178 palestinian OOV 5 29 OOV israel 7 123 OOV work 5 254
israel OOV 6 94 one OOV 4 25 OOV us 6 115 OOV agreement 5 75
sharon OOV 4 74 side OOV 3 21 OOV part 5 56 palestinian reform 4 49
polit OOV 4 143 polit OOV 3 299 israel OOV 5 81 palestinian OOV 4 81
OOV us 4 29 peopl go 3 407 attempt OOV 5 91 OOV isra 4 15
OOV state 4 37 palestinian govern 3 94 time OOV 4 63 one OOV 4 27
israel palestinian 4 52 palestinian accept 3 220 remain OOV 4 85 isra palestinian 4 17
even OOV 4 43 OOV state 3 150 OOV time 4 70 isra OOV 4 63
arafat OOV 4 41 OOV israel 3 18 OOV area 4 49 howev OOV 4 149
appear OOV 4 53 OOV end 3 20 OOV arafat 4 28 want OOV 3 36
total OOV 3 150 OOV act 3 105 isra OOV 4 8 us OOV 3 35
palestinian would 3 65 isra palestinian 3 18 would OOV 3 28 recent OOV 3 220
palestinian isra 3 35 israel OOV 3 198 use OOV 3 198 palestinian isra 3 115
Table 7: Most frequently used cached bigrams. The first colum in each section is the number of times that bigram was
used as a cached rule. The second column indicates the raw count of that bigram in the Guests or Editors group.
grammar formalism makes it quite easy to work with
latent variable models, in order to automatically dis-
cover structures in the data that have predictive value.
For example, it is easy to imagine a model where in
addition to a word distribution for each class, there
is also an additional shared ?neutral? distribution:
for each sentence, the words in that sentence can ei-
ther come from the class-specific content distribution
or the shared neutral distribution. This turns out to
be the Latent Sentence Perspective Model of Lin et
al. (2006), which is straightforward to encode using
the adaptor grammar formalism simply by introduc-
ing two new nonterminals to represent the neutral
distribution:
SENT 7? DOCd d = 1, . . . ,m
DOCd 7? IDd WORDSi d = 1, . . . ,m;
i ? {1,K}
DOCd 7? IDd NEUTS d = 1, . . . ,m;
WORDSi 7? WORDSi WORDi i ? {1,K}
WORDSi 7? WORDi i ? {1,K}
WORDi 7? v v ? V ; i ? {1,K}
NEUT 7? NEUTSi NEUTi
NEUT 7? NEUT
NEUT 7? v v ? V
Running this grammar did not produce improvements
consistent with those reported by Lin et al We plan to
investigate this further, and a natural follow-on would
be to experiment with adaptation for this variety of
latent structure, to produce an adapted LSPM-like
model analogous to adaptive na??ve Bayes.
Viewed in a larger context, computational classi-
fication of perspective is closely connected to social
scientists? study of framing, which Entman (1993)
characterizes as follows: ?To frame is to select some
aspects of a perceived reality and make them more
salient in a communicating text, in such a way as
to promote a particular problem definition, causal
interpretation, moral evaluation, and/or treatment rec-
ommendation for the item described.? Here and in
other work (e.g. (Laver et al, 2003; Mullen and Mal-
ouf, 2006; Yu et al, 2008; Monroe et al, 2008)),
it is clear that lexical evidence is one key to under-
standing how language is used to frame discussion
from one perspective or another; Resnik and Greene
(2009) have shown that syntactic choices can pro-
vide important evidence, as well. Another promising
direction for this work is the application of adaptor
grammar models as a way to capture both lexical and
grammatical aspects of framing in a unified model.
Acknowledgments
This research was funded in part by the Army Re-
search Laboratory through ARL Cooperative Agree-
ment W911NF-09-2-0072 and by the Office of the
Director of National Intelligence (ODNI), Intelli-
gence Advanced Research Projects Activity (IARPA),
through the Army Research Laboratory. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be con-
strued as representing the official views or policies
291
of ARL, IARPA, the ODNI, or the U.S. Government.
The authors thank Mark Johnson and the anonymous
reviewers for their helpful comments and discussions.
We are particularly grateful to Mark Johnson for mak-
ing his adaptor grammar code available.
References
R. Bekkerman and J. Allan. 2004. Using bigrams in text
categorization. Technical Report IR-408, Center of
Intelligent Information Retrieval, UMass Amherst.
S. B. Cohen, D. M. Blei, and N. A. Smith. 2010. Varia-
tional inference for adaptor grammars. In Conference
of the North American Chapter of the Association for
Computational Linguistics.
R.M. Entman. 1993. Framing: Toward Clarification of a
Fractured Paradigm. The Journal of Communication,
43(4):51?58.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 503?511.
Aria Haghighi and Dan Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric bayesian model.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 848?855,
Prague, Czech Republic, June.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Conference of the North American Chapter of
the Association for Computational Linguistics, pages
317?325, Boulder, Colorado, June.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2006. Adaptor grammars: A framework for
specifying compositional nonparametric Bayesian mod-
els. In Proceedings of Advances in Neural Information
Processing Systems.
Mark Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguistic
structure. In Proceedings of ACL-08: HLT, pages 398?
406, Columbus, Ohio, June.
Mark Johnson. 2010. PCFGs, topic models, adaptor gram-
mars and learning topical collocations and the structure
of proper names. In Proceedings of the Association for
Computational Linguistics, pages 1148?1157, Uppsala,
Sweden, July.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
pages 311?331.
David D. Lewis. 1998. Naive (bayes) at forty: The inde-
pendence assumption in information retrieval. In Claire
Ne?dellec and Ce?line Rouveirol, editors, Proceedings
of ECML-98, 10th European Conference on Machine
Learning, number 1398, pages 4?15, Chemnitz, DE.
Springer Verlag, Heidelberg, DE.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of Emperical Methods in
Natural Language Processing, pages 688?697.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexan-
der Hauptmann. 2006. Which side are you on? Identi-
fying perspectives at the document and sentence levels.
In Proceedings of the Conference on Natural Language
Learning (CoNLL).
Daniel Lowd and Pedro Domingos. 2005. Naive bayes
models for probability estimation. In ICML ?05: Pro-
ceedings of the 22nd international conference on Ma-
chine learning, pages 529?536, New York, NY, USA.
ACM.
Burt L. Monroe, Michael P. Colaresi, and Kevin M. Quinn.
2008. Fightin? Words: Lexical Feature Selection and
Evaluation for Identifying the Content of Political Con-
flict. Political Analysis, Vol. 16, Issue 4, pp. 372-403,
2008.
Tony Mullen and Robert Malouf. 2006. A preliminary in-
vestigation into sentiment analysis of informal political
discourse. In AAAI Symposium on Computational Ap-
proaches to Analysing Weblogs (AAAI-CAAW), pages
159?162.
P. Orbanz and Y. W. Teh. 2010. Bayesian nonparamet-
ric models. In Encyclopedia of Machine Learning.
Springer.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordinator.
Annals of Probability, 25(2):855?900.
Bhavani Raskutti, Herman L. Ferra?, and Adam Kowal-
czyk. 2001. Second order features for maximising text
classification performance. In EMCL ?01: Proceedings
of the 12th European Conference on Machine Learning,
pages 419?430, London, UK. Springer-Verlag.
Philip Resnik and Eric Hardisty. 2010. Gibbs
sampling for the uninitiated. Technical Re-
port UMIACS-TR-2010-04, University of Maryland.
http://www.lib.umd.edu/drum/handle/1903/10058.
Chade-Meng Tan, Yuan-Fang Wang, and Chan-Do Lee.
2002. The use of bigrams to enhance text categoriza-
tion. Inf. Process. Manage., 38(4):529?546.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Association,
101(476):1566?1581.
B. Yu, S. Kaufmann, and D. Diermeier. 2008. Classify-
ing party affiliation from political speech. Journal of
Information Technology and Politics, 5(1):33?48.
292
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 534?544,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Discriminative Word Alignment with a Function Word Reordering Model
Hendra Setiawan
UMIACS
University of Maryland
hendra@umiacs.umd.edu
Chris Dyer
Language Technologies Institute
Carnegie Mellon University
cdyer@cs.cmu.edu
Philip Resnik
Linguistics and UMIACS
University of Maryland
resnik@umd.edu
Abstract
We address the modeling, parameter estima-
tion and search challenges that arise from the
introduction of reordering models that capture
non-local reordering in alignment modeling.
In particular, we introduce several reordering
models that utilize (pairs of) function words
as contexts for alignment reordering. To ad-
dress the parameter estimation challenge, we
propose to estimate these reordering models
from a relatively small amount of manually-
aligned corpora. To address the search chal-
lenge, we devise an iterative local search al-
gorithm that stochastically explores reorder-
ing possibilities. By capturing non-local re-
ordering phenomena, our proposed alignment
model bears a closer resemblance to state-
of-the-art translation model. Empirical re-
sults show significant improvements in align-
ment quality as well as in translation perfor-
mance over baselines in a large-scale Chinese-
English translation task.
1 Introduction
In many Statistical Machine Translation (SMT) sys-
tems, alignment represents an important piece of in-
formation, from which translation rules are learnt.
However, while translation models have evolved
from word-based to syntax-based modeling, the de
facto alignment model remains word-based (Brown
et al, 1993; Vogel et al, 1996). This gap be-
tween alignment modeling and translation modeling
is clearly undesirable as it often generates tensions
that would prevent the extraction of many useful
translation rules (DeNero and Klein, 2007). Recent
work, e.g. by Blunsom et al (2009) and Haghihi et
al. (2009) just to name a few, show that alignment
models that bear closer resemblance to state-of-the-
art translation model consistently yields not only a
better alignment quality but also an improved trans-
lation quality.
In this paper, we follow this recent effort to nar-
row the gap between alignment model and trans-
lation model to improve translation quality. More
concretely, we focus on the reordering component
since we observe that the treatment of reordering re-
mains significantly different when comparing align-
ment versus translation: the reordering component
in state-of-the-art translation models has focused
on long-distance reordering, but its counterpart in
alignment models has remained focused on local
reordering, typically modeling distortion based en-
tirely on positional information. This leaves most
alignment decisions to association-based scores.
Why is employing stronger reordering models
more challenging in alignment than in translation?
One answer can be attributed to the fact that align-
ment points are unobserved in parallel text, thus so
are their reorderings. As such, introducing stronger
reordering often further exacerbates the computa-
tional complexity to do inference over the model.
Some recent alignment models appeal to external
linguistic knowledge, mostly by using monolingual
syntactic parses (Cherry and Lin, 2006; Pauls et al,
2010), which at the same time, provides an approx-
imation of the bilingual syntactic divergences that
drive the reordering. To our knowledge, however,
this approach has been used mainly to constrain re-
ordering possibilities, or to add to the generalization
ability of association-based scores, not to directly
model reordering in the context of alignment.
534
In this paper, we introduce a new approach to im-
proving the modeling of reordering in alignment. In-
stead of relying on monolingual parses, we condi-
tion our reordering model on the behavior of func-
tion words and the phrases that surround them.
Function words are the ?syntactic glue? of sen-
tences, and in fact many syntacticians believe that
functional categories, as opposed to substantive cat-
egories like noun and verb, are primarily responsi-
ble for cross-language syntactic variation (Ouhalla,
1991). Our reordering model can be seen as offering
a reasonable approximation to more fully elaborated
bilingual syntactic modeling, and this approxima-
tion is also highly practical, as it demands no exter-
nal knowledge (other than a list of function words)
and avoids the practical issues associated with the
use of monolingual parses, e.g. whether the mono-
lingual parser is robust enough to produce reliable
output for every sentence in training data.
At a glance, our reordering model enumerates
the function words on both source and target sides,
modeling their reordering relative to their neighbor-
ing phrases, their neighboring function words, and
the sentence boundaries. Because the frequency of
function words is high, we find that by predicting the
reordering of function words accurately, the reorder-
ing of the remaining words improves in accuracy as
well. In total, we introduce six sub-models involving
function words, and these serve as features in a log
linear model. We train model weights discrimina-
tively using Minimum Error Rate Training (MERT)
(Och, 2003), optimizing F-measure.
The parameters of our sub-models are estimated
from manually-aligned corpora, leading the reorder-
ing model more directly toward reproducing human
alignments, rather than maximizing the likelihood
of unaligned training data. This use of manual data
for parameter estimation is a reasonable choice be-
cause these models depend on a small, fixed number
of lexical items that occur frequently in language,
hence only small training corpora are required. In
addition, the availability of manually-aligned cor-
pora has been growing steadily.
The remainder of the paper proceeds as follows.
In Section 2, we provide empirical motivation for
our approach. In Section 3, we discuss six sub-
models based on function word relationships and
how their parameters are estimated; these are com-
????
?
??
?
?
?
???
one
of
few
that
have
dipl. rels.
with
countries
Australia
is
?
??
North Korea
the
1
1 2 3 4 5 6 7 8 9 10 11
2
3
4
5
6
7
8
9
10
11
12
Figure 1: An aligned Chinese-English sentence pair.
bined with additional features in Section 4 to pro-
duce a single discriminative alignment model. Sec-
tion 5 describes a simple decoding algorithm to find
the most probable alignment under the combined
model, Section 6 describes the training of our dis-
criminative model and Section 7 presents experi-
mental results for the model using this algorithm.
We wrap up in Sections 8 and 9 with a discussion
of related work and a summary of our conclusions.
2 Empirical Motivation
Fig. 1 shows an example of a Chinese-English sen-
tence pair together with correct alignment points.
Predicting the alignment for this particular Chinese-
English sentence pair is challenging, since the sig-
nificantly different syntactic structures of these two
languages lead to non-monotone reordering. For ex-
ample, an accurate alignment model should account
for the fact that prepositional phrases in Chinese ap-
pear in a different order than in English, as illus-
trated by the movement of the phrase ????/with
North Korea? from the beginning of the Chinese
noun phrase to the end of the corresponding English.
The central question that concerns us here is how
to define and infer regularities that can be useful
to predict alignment reorderings. The approach we
take here is supported by empirical results from a
pilot study, conducted as an inquiry into the idea of
focusing on function words to model alignment re-
ordering, which we briefly describe.
We took a Chinese-English manually-aligned cor-
pus of approximately 21 thousand sentence pairs,
535
?
??
?
?
?
?
?
? ?
one
of
few
that
have
dipl. rels.
with
countries
Australia
is
?
?
North Korea
the
1
1 2 3 4 5 6 7 8 9 10 11
2
3
4
5
6
7
8
9
10
11
12
Figure 2: The all-monotone phrase pairs, indicated as
rectangular areas in bold, that can be extracted from the
Fig. 1 example.
and divided each sentence pair into all-monotone
phrase pairs. Visually, an all-monotone phrase pair
corresponds to a maximal block in the alignment
matrix for which internal alignment points appear
in monotone order from the top-left corner to the
bottom-right corner. Fig. 2 illustrates seven such
pairs that can be extracted from the example in
Fig. 1. In total, there are 154,517 such phrase pairs
in our manually-aligned corpus.
The alignment configuration internal to all-
monotone phrase pair blocks is, obviously, mono-
tonic, which is a configuration that is effectively
modeled by traditional alignments models. On the
other hand, the reordering between two adjacent
blocks is the focus of our efforts since existing mod-
els are less effective at modeling non-monotonic
alignment configurations. To measure the function
words? potential to predict non-monotone reorder-
ings, we examined the border words where two ad-
jacent blocks meet. In particular, we are interested
in how many adjacent blocks whose border words
are function words.
The results of this pilot study were quite encour-
aging. If we consider only the Chinese side of the
phrase pairs, 88.35% adjacent blocks have function
words as their boundary words. If we consider only
the English side, function words appear at the bor-
ders of 93.91% adjacent blocks. If we consider
both the Chinese and English sides, the percentage
increases to 95.53%. Notice that in Fig. 2, func-
tion words appear at the borders of all adjacent all-
monotone phrase pairs, if both Chinese and English
sides are considered. Clearly with such high cov-
erage, function words are central in predicting non-
monotone reordering in alignment.
3 Reordering with Function Words
The reordering models we describe follow our previ-
ous work using function word models for translation
(Setiawan et al, 2007; Setiawan et al, 2009). The
core hypothesis in this work is that function words
provide robust clues to the reordering patterns of the
phrases surrounding them. To make this insight use-
ful for alignment, we develop features that score the
alignment configuration of the neighboring phrases
of a function word (which functions as an anchor)
using two kinds of information: 1) the relative order-
ing of the phrases with respect to the function word
anchor; and 2) the span of the phrases. This sec-
tion provides a high level overview of our reordering
model, which attempts to leverage this information.
To facilitate subsequent discussions, we introduce
the notion of monolingual function word phrase
FWi, which consists of the tuple (Yi, Li, Ri), where
Yi is the i-th function word and Li,Ri are its left and
right neighboring phrases, respectively. Note that
this notion of ?phrase? is defined only for reorder-
ing purposes in our model, and does not necessar-
ily correspond to a linguistic phrase. We define
such phrases on both sides to cover as many non-
monotone reorderings as possible, as suggested by
the pilot study. To denote the side, we append a sub-
script: FWi,S = (Yi,S , Li,S , Ri,S) refers to a func-
tion word phrase on the source side, and FWi,T =
(Yi,T , Li,T , Ri,T ) to one on the target side. In our
subsequent discussion, we will mainly use FWi,S ,
and we will omit subscripts S or T if they are clear
from context.
The primary objective of our reordering model
is to predict the projection of monolingual func-
tion word phrases from one language to the
other, inferring bilingual function word phrase pairs
FWi,S?T = (Yi,S?T , Li,S?T , Ri,S?T ), which en-
code the two aforementioned pieces of informa-
tion.1 To infer these phrases, we take a probabilis-
1The subscript S ? T denotes the projection direction from
source to target. The subscript for the other direction is T ? S.
536
tic approach. For instance, to estimate the spans of
Li,S?T , Ri,S?T , our reordering model assumes that
any span to the left of Yi,S is a possible Li,S and
any span to the right of Yi,S is a possible Ri,S , de-
ciding which is most probable via features, rather
than committing to particular spans (e.g. as defined
by a monolingual text chunker or parser). We only
enforce one criterion on Li,S?T and Ri,S?T : they
have to be the maximal alignment blocks satisfying
the consistent heuristic (Och and Ney, 2004) that end
or start with Yi,S?T on the source S side respec-
tively.2
To infer these phrases, we decompose Li,S?T
into (o(Li,S?T ), d(FWi?1,S?T ), b(?s?)); sim-
ilarly, Ri,S?T into (o(Ri,S?T ),d(FWi+1,S?T ),
b(?/s?) )). Taking the decomposition of Li,S?T as
a case in point, here o(Li,S?T ) describes the re-
ordering of the left neighbor Li,S?T with respect
to the function word Yi,S?T , while d(FWi?1,S?T )
and b(?s?)) probe the span of Li,S?T , i.e. whether
it goes beyond the preceding function word phrase
pairs FWi?1,S?T and up to the beginning-of-
sentence marker ?s? respectively. The same defini-
tion applies to the decomposition of Ri,S?T , where
FWi+1,S?T is the succeeding function word phrase
pair and ?/s? is the end-of-sentence marker.
3.1 Six (Sub-)Models
To model o(Li,S?T ), o(Ri,S?T ), i.e. the re-
ordering of the neighboring phrases of a func-
tion word, we employ the orientation model in-
troduced by Setiawan et al (2007). Formally,
this model takes the form of probability distribution
Pori(o(Li,S?T ), o(Ri,S?T )|Yi,S?T ), which condi-
tions the reordering on the lexical identity of the
function word alignment (but independent of the lex-
ical identity of its neighboring phrases). In particu-
lar, o maps the reordering into one of the following
four orientation values (borrowed from Nagata et al
(2006)) with respect to the function word: Mono-
tone Adjacent (MA), Monotone Gap (MG), Reverse
Adjacent (RA) and Reverse Gap (RG). The Mono-
tone/Reverse distinction indicates whether the pro-
jected order follows the original order, while the
Adjacent/Gap distinction indicates whether the pro-
2This heuristic is commonly used in learning phrase pairs
from parallel text. The maximality ensures the uniqueness of L
and R.
jections of the function word and the neighboring
phrase are adjacent or separated by an intervening
phrase.
To model d(FWi?1,S?T ), d(FWi+1,S?T ), i.e.
whether Li,S?T and Ri,S?T extend beyond the
neighboring function word phrase pairs, we uti-
lize the pairwise dominance model of Setiawan
et al (2009). Taking d(FWi?1,S?T ) as
a case in point, this model takes the form
Pdom(d(FWi?1,S?T )|Yi?1,S?T , Yi,S?T ), where d
takes one of the following four dominance val-
ues: leftFirst, rightFirst, dontCare, or neither.
We will detail the exact formulation of these val-
ues in the next subsection. However, to provide
intuition, the value of either leftFirst or neither
for d(FWi?1,S?T ) would suggest that the span of
Li,S?T doesn?t extend to Yi?1,S?T ; the further dis-
tinction between leftFirst and neither concerns with
whether the span of Ri?1,S?T extends to FWi,S?T .
To model b(?s?), b(?/s?), i.e. whether the span of
Li,S?T and Ri,S?T extends up to sentence mark-
ers, we introduce the borderwise dominance model.
Formally, this model is similar to the pairwise domi-
nance model, except that we use the sentence bound-
aries as the anchors instead of the neighboring
phrase pairs. This model captures longer distance
dependencies compared to the previous two mod-
els; in the Chinese-English case, in particular, it is
useful to discourage word alignments from crossing
clause or sentence boundaries. The sentence bound-
ary issue is especially important in machine trans-
lation (MT) experimentation, since the Chinese side
of English-Chinese parallel text often includes long
sentences that are composed of several independent
clauses joined together; in such cases, words from
one clause should be discouraged from aligning to
words from other clauses. In Fig. 1, this model is
potentially useful to discourage words from cross-
ing the copula ??/is?.
We define each model for all (pairs of) function
word phrase pairs, forming features over a set of
word alignments (A) between source (S) and target
537
(T ) sentence pair, as follows:
fori =
N
?
i=1
Pori(o(Li), o(Ri)|Yi) (1)
fdom =
N
?
i=2
Pdom(d(FWi?1)|Yi?1, Yi) (2)
fbdom =
N
?
i=1
Pbdom(b (?s?)|?s?, Yi) ?
Pbdom(b (?/s?)|Yi, ?/s?) (3)
where N is the number of function words (of the
source side, in the S ? T case). As the bilingual
function word phrase pairs are uni-directional, we
employ these three models in both directions, i.e.
T ? S as well as S ? T . As a result, there are
six reordering models based on function words.
3.2 Prediction and Parameter Estimation
Given FWi?1,S?T (and all other FW?i?/i,S?T ),
our reordering model has to decompose Li,S?T into
(o(Li,S?T ), d(FWi?1,S?T ), b(?s?)); and Ri,S?T
into (o(Ri,S?T ),d(FWi+1,S?T ), b(?/s?) )) during
prediction and parameter estimation. In prediction
mode (described in Section 5), it has to make the de-
composition on the current state of alignment, while
during parameter estimation, it has to make the
same decomposition on the manually-aligned cor-
pora. Since the process is identical, we proceed with
the discussion in the context of parameter estima-
tion, where the decomposition is performed to col-
lect counts to estimate the parameters of our models.
Orientation model. Using Li,S?T as a case in
point and given (Yi,S?T =sll/tmm, Li,S?T =sl2l1/tm2m1 ,
Ri,S?T =s
l4
l3
/tm4m3)
3
, the value of o(Li,S?T ) in terms
of Monotone/Reverse is:
Monotone/Reverse =
{
M, m2 < m,
R, m < m1.
(4)
while its value in terms of Adjacent/Gap values is:
Adjacent/Gap =
{
A, |m ? m1| ? |m ? m2| = 1,
G, otherwise.
(5)
3We use subscripts to indicate the starting index, and super-
scripts the ending index.
By adjusting the indices, the computation of
o(Ri,S?T ) follows similarly to the procedure above.
Suppose we want to estimate the probability of
Li,S?T =MA for a particular Yi. Note that here, we
are interested in the lexical identity of Yi, thus the
index i is irrelevant. We first gather the counts of the
orientation value for all Li,S?T of Yi in the corpus:
c(o(Li,S?T ) ? {MA, RA, MG, RG}, Yi). Then
Pori(MA|Yi) is estimated as follows:
Pori(MA|Yi) =
c(MA, Yi)
c(Yi)
(6)
where c(Yi) is the frequency of Yi in the corpus. The
estimation of other orientation values as well as the
T ? S version of the model, follows the same pro-
cedure.
Pairwise and Borderwise dominance models.
Given Ri,S?T = sl2l1/t
m2
m1 and Li+1,S?T =
sl4l3/t
m4
m3 , i.e. the spans of the neighbors of a
pair of neighboring function word phrase pairs
(Yi = sl5l5/tm5m5 , Yi+1 = s
l6
l6
/tm6m6), the value of
d(FWi+1,S?T ) is:
=
?
?
?
?
?
?
?
?
?
?
?
leftFirst, l2 ? l6
?
l3 > l5
rightFirst, l2 < l6
?
l3 ? l5
dontCare, l2 ? l6
?
l3 ? l5
neither, l2 < l6
?
l3 > l5
(7)
Note that the neighbors of the sentence markers for
the borderwise models span the whole sentence, thus
value of neither is impossible for these models.
Suppose we want to estimate the probability of Yi
and Yi+1 having a dontCare dominance value. Note
that here we are interested in the lexical identity of
Yi and Yi+1, thus the models are insensitive to the in-
dices. We first gather the counts of the Yi and Yi+1
having the dontCare value c(dontCare, Yi, Yi+1);
then Pdom(dontCare|Yi, Yi+1) is estimated as fol-
lows:
Pdom(dontCare|Yi, Yi+1) =
c(dontCare, Yi, Yi+1)
c(Yi, Yi+1)
(8)
where c(Yi, Yi+1) is the count of Yi appears after
Yi+1 in the training corpus without any other func-
tion word comes in between.
538
4 Alignment Model
To use the function word alignment features de-
scribed in the previous section to predict alignments,
we use a linear model of the following form:
A? = arg max
A?A(S,T )
? ? f(A, S, T ) (9)
where A(S, T ) is the set of all possible alignments
of a source sentence S and target sentence T , and
f(A, S, T ) is a vector of feature functions on A, S,
and T , and ? is a parameter vector.
In addition to the six reordering models, our
model employs several association-based scores that
look at alignments in isolation. These features in-
clude:
1. Normalized log-likelihood ratio (LLR). This
feature represents an association score, derived from
statistical testing statistics. LLR (Dunning, 1993)
has been widely used especially to measure lexical
association. Since the values of LLR are unnormal-
ized, we normalize them on a per-sentence basis, so
that the normalized LLRs of, say, a particular source
word to the target words in a particular sentence sum
up to one.
2. Translation table from IBM model 4. This
feature represents another association score, derived
from a generative model, in particular the word-
based IBM model 4. The use of this feature is
widespread in recent alignment models, since it pro-
vides a relatively accurate initial prediction.
3. Translation table from manually-aligned
corpora. This feature represents a gold-standard as-
sociation score, based on human annotation. While
attractive, this feature suffers from data sparse-
ness issues since the lexical coverage of manually-
aligned corpora, especially over content words, is
very low. To overcome this issue, we design this
feature to have two levels of granularity; as such, a
fine-grained one is applied for function words and
the coarse-grained one for content words.
4. Grow-diag-final alignments bonus. This fea-
ture encourages our alignment model to reuse align-
ment points that are part of the alignments created
by the grow-diag-final heuristic, which we used as
the baseline of our machine translation experiments.
5. Fertility model from IBM model 4. This fea-
ture, which is another by-product of IBM model 4,
measures the probability of a certain word aligning
to zero, one, or two or more words.
6. Null-alignment probability. This bino-
mial feature models preference towards not aligning
words, i.e. aligning to the NULL token. The intu-
ition is to penalize NULL alignments depending on
word class, by assigning lower probability mass to
unaligned content words than to unaligned function
words. In our experiment, we assign feature value
10?3 for a function word aligning to NULL, and
10?5 for a content word aligning to NULL.
Note that with the exception of the alignment
bonus feature (4), all features are uni-directional,
and therefore we employ these features in both di-
rections just as was done for the reordering models.
5 Search
To find A? using the model in Eq. 9, it is neces-
sary to search 2|S|?|T | different alignment config-
urations, and, because of the non-local dependen-
cies in some of our features, it is not possible to use
dynamic programming to perform this search effi-
ciently. We therefore employ an approximate search
for the best alignment. We use a local search pro-
cedure which starts from some alignment (in our
case, a symmetrized Model 4 alignment) and make
local changes to it. Rather than taking a pure hill-
climbing approach which greedily moves to locally
better configurations (Brown et al, 1993), we use
a stochastic search procedure which can move into
lower-scoring states with some probability, similar
to the Monte Carlo techniques used to draw sam-
ples from analytically intractable probability distri-
butions.
5.1 Algorithm
To find A?, our search algorithm starts with an initial
alignment A(1) and iteratively draws a new set by
making a few small changes to the current set. For
each step i = [1, n], with alignment A(i), a set of
neighboring alignments N (A(i)) is induced by ap-
plying small transformations (discussed below) to
the current alignment. The next alignment A(i+1)
539
is sampled from the following distribution:
p(A(i+1)|S, T, A(i)) = exp? ? f(A
(i+1), S, T )
Z(A(i), S, T )
where Z(A(i), S, T ) =
?
A??N (A(i))
exp? ? f(A?, S, T )
In addition to the current ?active? alignment configu-
ration A(i), the algorithm keeps track of the highest
scoring alignment observed so far, Amax. After n
steps, the algorithm returns Amax as its approxima-
tion of A?. In the experiments reported below, we
initialized A(1) with the Model 4 alignments sym-
metrized by using the grow-diag-final-and heuristic
(Koehn et al, 2003).
5.2 Alignment Neighborhoods
We now turn to a discussion of how the alignment
neighborhoods used by our stochastic search algo-
rithm are generated. We define three local transfor-
mation operations that apply to single columns of
the alignment grid (which represent all of the align-
ments to the lth source word), rows, or existing align-
ment points (l, m). Our three neighborhood gener-
ating operators are ALIGN, ALIGNEXCLUSIVE, and
SWAP. The ALIGN operator applies to the lth col-
umn of A and can either add an alignment point
(l, m?) or move an existing one (including to null,
thus deleting it). ALIGNEXCLUSIVE adds an align-
ment point (l, m) and deletes all other points from
row m. Finally, the SWAP operator swaps (l, m) and
(l?, m?), resulting in new alignment points (l, m?)
and (l?, m). We increase the decoder?s mobility
by traversing the target side and applying the same
steps above for each target word. Fig. 3 illustrates
the three operators. By iterating over all columns l
and rows m, the full alignment space A(S, T ) can
be explored.4
To further reduce the search space, an alignment
point (l, m) is only admitted into a neighborhood if
it is found in the high-recall alignment set R(S, T ),
which we define to be the model 4 union alignments
(bidirectional model 4 symmetrized via union) plus
the 5 best alignments according to the log-likelihood
ratio.
4Using only the ALIGN operator, it is possible to explore
the full alignment space; however, using all three operators in-
creases mobility.
(a)
(b)
(c)
l l' l l'
m
m'
m
m'
m
m'
Figure 3: Illustrations for (a) ALIGN, (b) ALIGNEXCLU-
SIVE, and (c) SWAP operators, as applied to align the dot-
ted, smaller circle (l,m) to (l,m?). The left hand side rep-
resents A(i), while the right hand side represents a can-
didate for A(i+1). The solid circles represent the new
alignment points added to A(i+1).
6 Discriminative Training
To set the model parameters ?, we used the min-
imum error rate training (MERT) algorithm (Och,
2003) to maximize the F-measure of the 1-best
alignment of the model on a development set con-
sisting of sentence pairs with manually generated
alignments. The candidate set used by MERT to ap-
proximate the model is simply the set of alignments
{A(1), A(2), . . . , A(n)} encountered in the stochastic
search.
While MERT does not scale to large numbers of
features, the scarcity of manually aligned training
data also means that models with large numbers of
sparse features would be difficult to learn discrimi-
natively, so this limitation is somewhat inherent in
the problem space. Additionally, MERT has sev-
eral advantages that make it particularly useful for
our task. First, we can optimize F-measure of the
alignments directly, which has been shown to corre-
late with translation quality in a downstream system
(Fraser and Marcu, 2007b). Second, we are opti-
mizing the quality of the 1-best alignments under the
model. Since translation pipelines typically use only
a single word alignment, this criterion is appropri-
ate. Finally, and very importantly for us, MERT re-
quires only an approximation of the model?s hypoth-
esis space to carry out optimization. Since we are
using a stochastic search, this is crucial, since sub-
540
sequent evaluations of the same sentence pair (even
with the same weights) may result in a different can-
didate set.
Although MERT is a non-probabilistic optimizer,
we explore the alignment space stochastically. This
is necessary to make sure that the weights we use
correspond to a probability distribution that is not
overly peaked (which would result in a greedy hill-
climbing search) or flat (which would explore the
model space without information from the model).
We found that normalizing the weights by the Eu-
clidean norm resulted in a distribution that was well-
balanced between the two extremes.
7 Experiments
We evaluated our proposed alignment model intrin-
sically on an alignment task and extrinsically on a
large-scale translation task, focusing on Chinese-
English as the language pair. Our training data
consists of manually aligned corpora available from
LDC (LDC2006E93 and LDC2008E57) and un-
aligned corpora, which include FBIS, ISI, HKNews
and Xinhua. In total, the manually aligned corpora
consist of more than 21 thousand sentence pairs,
while the unaligned corpora consist of more than
710 thousand sentence pairs. The manually-aligned
corpora are primarily used for training the reorder-
ing models and for discriminative training purposes.
For translation experiments, we used cdec (Dyer
et al, 2010), a fast implementation of hierarchi-
cal phrase-based translation models (Chiang, 2005),
which represents a state-of-the-art translation sys-
tem.
We constructed the list of function words in En-
glish manually and in Chinese from (Howard, 2002).
Punctuation marks were added to the list, result-
ing in 883 and 359 tokens in the Chinese and En-
glish lists, respectively. For the alignment experi-
ments, we took the first 500 sentence pairs from the
newswire genre of the manually-aligned corpora and
used the first 250 sentences as the development set,
with the remaining 250 as the test set. To ensure
blind experimentation, we excluded these sentence
pairs from the training of the features, including the
reordering models.
7.1 Alignment Quality
We used GIZA++, the implementation of the de-
facto standard IBM alignment model, as our base-
line alignment model. In particular, we used
GIZA++ to align the concatenation of the develop-
ment set, the test set, and the unaligned corpora, with
5, 5, 3 and 3 iterations of model 1, HMM, model
3, and model 4 respectively. Since the IBM model
is asymmetric, we followed the standard practice of
running GIZA++ twice, once in each direction, and
combining the resulting outputs heuristically. We
chose to use the grow-diag-final-and heuristic as it
worked well for hierarchical phrase-based transla-
tion in our early experiments. We recorded the align-
ment quality of the test set as our baseline perfor-
mance.
For our alignment model, we used the same set of
training data. To align the test set, we first tuned
the weights of the features in our discriminative
alignment model using minimum error rate training
(MERT) (Och, 2003) with F?=0.1 as the optimiza-
tion criterion. At each iteration, our aligner outputs
k-best alignments under current set of weights, from
which MERT proceeds to compute the next set of
weights. MERT terminates once the improvement
over the previous iteration is lower than a predefined
value. Once tuned, we ran our aligner on the test set
and measured the quality of the resulting alignment
as the performance of our model.
Model P R F0.5 F0.1
gdfa 70.97 63.83 67.21 64.48
association 73.70 76.85 75.24 76.52
+ori 74.09 78.29 76.13 77.85
+dom 75.06 78.98 76.97 78.57
+bdom 75.41 80.53 77.89 79.99
Table 1: Alignment quality results (F0.1) for our discrim-
inative reordering models with various features (lines 2-
5) versus the baseline IBM word-based Model 4 sym-
metrized using the grow-diag-final-and heuristic. The
balanced F0.5 measure is reported for reference. The best
scores are bolded.
Table 1 reports the results of our experiments,
which are conducted in an incremental fashion pri-
marily to highlight the role of reordering model-
ing. The first line (gdfa) reports the baseline perfor-
541
mance. In the first experiment (association), we em-
ployed only the association-based features described
in Section 4. As shown, we obtain a significant im-
provement over baseline. This result is consistent
with recent literature (Fraser and Marcu, 2007a) that
shows that a discriminatively trained model outper-
forms baseline unsupervised models like GIZA++.
In the second set of experiments, we added the re-
ordering models into our discriminative model one
by one, starting with the orientation models, then
the pairwise dominance model and finally the bor-
derwise dominance model, reported in lines +ori,
+dom and +bdom respectively. As shown, each ad-
ditional reordering model provides a significant ad-
ditional improvement. The best result is obtained by
employing all reordering models. These results em-
pirically confirm our hypothesis that we can improve
alignment quality by employing reordering models
that capture non-local reordering phenomena.
7.2 Translation Quality
For translation experiments, we used the products
from our intrinsic experiments to learn translation
rules for the hierarchical phrase-based decoder, i.e.
the features weights of the +bdom experiment to
align the MT training data using our discriminative
model. For our translation model, we used the stan-
dard features based on the relative frequency counts,
including a 5-gram language model feature trained
on the English portion of the whole training data
plus portions of the Gigaword v2 corpus. Specif-
ically, we tuned the weights of these features via
MERT on the NIST MT06 set and we report the re-
sult on the NIST MT02, MT03, MT04 and MT05
sets.
MT02 MT03 MT04 MT05
gdfa 25.61 32.05 31.80 29.34
this work 26.56 33.79 32.61 30.47
Table 2: The translation performance (BLEU) of hierar-
chical phrase-based translation trained on training data
aligned by IBM model 4 symmetrized with the grow-
diag-final-and heuristic, versus being trained on align-
ments by our discriminative alignment model. Bolded
scores indicate that the improvement is statistically sig-
nificant.
Table 2 shows the result of our translation exper-
iments. In our alignment model, we employed the
whole set of reordering models, i.e. the one reported
in the +bdom line in Table 1. As shown, our dis-
criminative alignment model produces a consistent
and significant improvement over the baseline IBM
model 4 (p < 0.01), ranging between 0.81 and 1.71
BLEU points.
8 Related Work
The focus of our work is to strengthen the reordering
component of alignment modeling. Although the de
facto standard, the IBM models do not generalize
well in practice: the IBM approach employs a series
of reordering models based on the word?s position,
but reordering depends on syntactic context rather
than absolute position in the sentence. Over the
years, there have been many proposals to improve
these reordering models, most notably Vogel et al
(1996), which adds a first-order dependency. Never-
theless, the use of these distortion-based models re-
mains widespread (Marcu and Wong, 2002; Moore,
2004).
Alignment modeling is challenging because it
often has to consider a prohibitively large align-
ment space. Efforts to constrain the space gen-
erally comes from the use of Inversion Transduc-
tion Grammar (ITG) (Wu, 1997). Recent propos-
als that use ITG constraints include (Haghighi et
al., 2009; Blunsom et al, 2009) just to name a few.
More recent models have begun to use linguistically-
motivated constraints, often in combination with
ITG, primarily exploiting monolingual syntactic in-
formation (Burkett et al, 2010; Pauls et al, 2010).
Our reordering model is closely related to the
model proposed by Zhang and Gildea (2005; 2006;
2007a), with respect to conditioning the reordering
predictions on lexical items. These related models
treat their lexical items as latent variables to be es-
timated from training data, while our model uses
a fixed set of lexical items that correspond to the
class of function words. With respect to the focus
on function words, our reordering model is closely
related to the UALIGN system (Hermjakob, 2009).
However, UALIGN uses deep syntactic analysis and
hand-crafted heuristics in its model.
542
9 Conclusions
Languages exhibit regularities of word order that
are preserved when projected to another language.
We use the notion of function words to infer such
regularities, resulting in several reordering models
that are employed as features in a discriminative
alignment model. In particular, our models pre-
dict the reordering of function words by looking
at their dependencies with respect to their neigh-
boring phrases, their neighboring function words,
and the sentence boundaries. By capturing such
long-distance dependencies, our proposed align-
ment model contributes to the effort to unify align-
ment and translation. Our experiments demonstrate
that our alignment approach achieves both its intrin-
sic and extrinsic goals.
Acknowledgements
This research was supported in part by the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-2-001. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the view of the sponsors.
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In ACL, pages 782?790, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In HLT-NAACL, pages 127?135, Los An-
geles, California, June. Association for Computational
Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discriminative
training. In COLING/ACL, pages 105?112, Sydney,
Australia, July. Association for Computational Lin-
guistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL, pages
263?270, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL,
pages 17?24, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74, March.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In ACL, Up-
psala, Sweden.
Alexander Fraser and Daniel Marcu. 2007a. Getting
the structure right for word alignment: LEAF. In
EMNLP-CoNLL, pages 51?60, Prague, Czech Repub-
lic, June. Association for Computational Linguistics.
Alexander Fraser and Daniel Marcu. 2007b. Measuring
word alignment quality for statistical machine transla-
tion. Computational Linguistics, 33(3):293?303.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
itg models. In ACL, pages 923?931, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
Ulf Hermjakob. 2009. Improved word alignment with
statistics and linguistic heuristics. In EMNLP, pages
229?237, Singapore, August. Association for Compu-
tational Linguistics.
Jiaying Howard. 2002. A Student Handbook for Chinese
Function Words. The Chinese University Press.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HTL-NAACL,
pages 127?133, Edmonton, Alberta, Canada, May. As-
sociation for Computational Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In EMNLP, July 23.
Robert C. Moore. 2004. Improving ibm word alignment
model 1. In ACL, pages 518?525, Barcelona, Spain,
July.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A clustered global phrase
reordering model for statistical machine translation. In
ACL, pages 713?720, Sydney, Australia, July. Associ-
ation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL, pages 160?
167.
Jamal Ouhalla. 1991. Functional Categories and Para-
metric Variation. Routledge.
543
Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment with
inversion transduction grammars. In HLT-NAACL,
pages 118?126, Los Angeles, California, June. Asso-
ciation for Computational Linguistics.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007.
Ordering phrases with function words. In ACL, pages
712?719, Prague, Czech Republic, June. Association
for Computational Linguistics.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological ordering of function
words in hierarchical phrase-based translation. In
ACL, pages 324?332, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING, pages 836?841, Copenhagen.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Sep.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
ACL. The Association for Computer Linguistics.
Hao Zhang and Daniel Gildea. 2006. Inducing word
alignments with bilexical synchronous trees. In ACL.
The Association for Computer Linguistics.
544
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1348?1353,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Using Topic Modeling to Improve Prediction of Neuroticism and Depression
in College Students
Philip Resnik
University of Maryland
College Park, MD 20742
resnik@umd.edu
Anderson Garron
University of Maryland
College Park, MD 20742
agarron@cs.umd.edu
Rebecca Resnik
Mindwell Psychology Bethesda
5602 Shields Drive
Bethesda, MD 20817
drrebeccaresnik@gmail.com
Abstract
We investigate the value-add of topic model-
ing in text analysis for depression, and for neu-
roticism as a strongly associated personality
measure. Using Pennebaker?s Linguistic In-
quiry and Word Count (LIWC) lexicon to pro-
vide baseline features, we show that straight-
forward topic modeling using Latent Dirich-
let Allocation (LDA) yields interpretable, psy-
chologically relevant ?themes? that add value
in prediction of clinical assessments.
1 Introduction
In the United States, where 25 million adults per
year suffer a major depressive episode (NAMI,
2013), identifying people with mental health prob-
lems is a key challenge. For clinical psychologists,
language plays a central role in diagnosis: many
clinical instruments fundamentally rely on what is,
in effect, manual coding of patient language. Au-
tomating language assessment in this domain poten-
tially has enormous impact, for two reasons. First,
conventional clinical assessments for affective dis-
orders (e.g. The Minnesota Multiphasic Personal-
ity Inventory, MMPI) are based on norm-referenced
self-report, and therefore depend on patients? will-
ingness and ability to report symptoms. However,
some individuals are motivated to underreport symp-
toms to avoid negative consequences (e.g. active
duty soldiers, parents undergoing child custody eval-
uations), and others lack the self awareness to report
accurately.1 Second, many people ? e.g. those with-
1As evidenced by the fact that assessments such as the MMPI-2-
RF include validity scales to detect, e.g., defensiveness, atyp-
out adequate insurance or in rural areas ? cannot ac-
cess a clinician qualified to perform a psychological
evaluation (Sibelius, 2013; APA, 2013). There is
enormous value in inexpensive screening measures
that could be administered in primary care and by
social workers and other providers.
We take as a starting point the well known
lexicon-driven methods of Pennebaker and col-
leagues (LIWC, Pennebaker and King (1999)),
which relate language use to psychological vari-
ables, and improve on them straightforwardly using
topic modeling (LDA, Blei et al (2003)). First, we
show that taking automatically derived topics into
account improves prediction of neuroticism (emo-
tional instability, John and Srivastava (1999)), as
measured by correlation with widely used clinical
instruments, when compared with lexically-based
prediction alone. Neuroticism is of particular inter-
est as a personality measure because higher scores
on neuroticism scales are consistent with increased
distress and more difficulty coping; individuals with
high levels of neuroticism may also be at higher risk
of psychiatric problems categorized as Axis I in the
DSM-IV (Association, 2000), including the inter-
nalizing disorders (depression, anxiety).2 Second,
we show a similar correlation improvement result
for prediction of depression, adding improvement
ical responses, and overly positive self-portrayals (Tellegen et
al., 2003).
2The Diagnostic and Statistical Manual of Mental Disorders is a
widely used organization of mental health conditions; it served
as the standard for diagnosis from 1994 until the release of
the (quite controversial) DSM-5 in May, 2013. Axis I in the
DSM-IV includes all the major diagnostic categories, exclud-
ing mental retardation and personality disorders.
1348
on precision (with no decrease in recall), as well
as comparison with human performance by clinical
psychologists. Third, we show that LDA has iden-
tified meaningful, population-specific themes that
go beyond the pre-defined LIWC categories and are
psychologically relevant.
2 Predicting neuroticism
2.1 Experimental framework
Data. We utilize a collection of 6,459 stream-of-
consciousness essays collected from college stu-
dents by Pennebaker and King (1999) between 1997
and 2008, averaging approximately 780 words each.
Students were asked to think about their thoughts,
sensations, and feelings in the moment and ?write
your thoughts as they come to you?.
Each essay is accompanied by metadata for its
author, which includes scores for the Big-5 person-
ality traits (John and Srivastava, 1999): agreeable-
ness, conscientiousness, extraversion, neuroticism,
and openness. Because Big-5 assessment can be
done using a variety of different survey instruments
(John et al, 2008), and different instruments were
used from year to year, we treat the data from each
year as an independent dataset.
Any author missing any Big-5 attribute was ex-
cluded. Essays were tokenized using NLTK (Bird
et al, 2009) and lowercased, eliminating those that
failed tokenization because of encoding issues. This
resulted in a dataset containing 4,777 essays with as-
sociated Big-5 metadata.
LIWC features. For each document, we calcu-
late the number of observed words in each of Pen-
nebaker and King?s 64 LIWC categories. These in-
clude, among others, syntactic categories (e.g. pro-
nouns, verbs), affect categories (e.g. negative emo-
tion words, anger words), semantic categories (e.g.
causation words, cognitive words), and topical cat-
egories (e.g. health, religion). For instance, the
anger category contains 190 word patterns specify-
ing, for example, words descriptive of contexts in-
volving anger (e.g. brutal, hostile, shoot) and words
that would be used by someone when angry (e.g.
bullshit, hate, moron). We also explored includ-
ing essays? average sentence length and total word-
count, as an initial proxy for language complexity,
which often figures into psychological assessments.
However, results adding these features to LIWC did
not differ significantly from LIWC alone, and for
brevity we do not report them.3
LDA features. We use vanilla LDA as imple-
mented in the Mallet toolkit (McCallum, 2002), de-
veloping a k-topic model on just training documents,
and using the posterior topic distribution for each
training and test document as a set of k features.
Mallet?s stoplist and default parameters were used
for burn-in, lag, number of iterations, priors, etc.
Details on train/test splits and number k of topics
appear below.
LIWC+LDA features. The union of the LIWC
features (one feature per category) and LDA features
(one feature per topic).
Prediction. We utilize linear regression in the
WEKA toolkit (Hall et al, 2009), estimated on train-
ing documents, to predict the neuroticism score as-
sociated with the author of each test document.4
2.2 Results
Table 1 shows the quality of prediction via linear
regression, averaged over the eleven datasets, 1997
through 2008, using Pearson correlation (r) as the
evaluation metric. For each year, we used 10-fold
cross-validation to ensure proper separation of train-
ing and test data. We experimented with LDA us-
ing 20, 30, 40, and 50 topics.5
A first thing to observe is that the multiple re-
gressions using all LIWC categories produce much
stronger correlations with neuroticism than the indi-
vidual category correlations reported by Pennebaker
and King.6 There the strongest individual corre-
lations with neuroticism for any LIWC categories
are .16 (negative emotion words) and -.13 (positive
emotion words), though it should be noted that their
goal was to validate their categories as a meaningful
3Using richer measures of complexity, e.g. Pakhomov et al
(2011), is a topic for future work.
4In previous work we have found that multiple linear regression
is competitive with more complicated techniques such as SVM
regression, though we plan to explore the latter in future work.
5Full year-by-year data appears in supplemental materi-
als at http://umiacs.umd.edu/?resnik/papers/
emnlp2013-supplemental/.
6The comparison is not perfect, since they used Big-5 data col-
lected between 1993 and 1998, and we also eliminated some
files during preprocessing.
1349
Feature set LIWC LDA20 LIWC+LDA20 LDA30 LIWC+LDA30 LDA40 LIWC+LDA40 LDA50 LIWC+LDA50
Average r 0.413 0.384 0.430 0.407 0.442* 0.420 0.459** 0.440 0.443*
Table 1: Prediction quality for neuroticism, for alternative feature sets (Pearson?s r). *p< .03, **p < .02
way to explore personality differences, not predic-
tion.
As noted in Table 1, paired t-tests (df=10, ? =
.05) establish that, in comparing the cross-year av-
erages, augmenting LIWC with topic features im-
proves average correlation significantly over using
LIWC features alone for 30, 40, and 50 topics.
LDA features alone do not improve significantly
over LIWC.
3 Predicting Depression
3.1 Experimental framework
Data. We use essays collected by Rude et al
(2004) similarly to ?2.1; in this case, students were
asked to ?describe your deepest thoughts and feel-
ings about being in college?. Each essay is accom-
panied by the author?s Beck Depression Inventory
score (BDI). BDI (Beck et al, 1961) is a widely
used 21-question instrument that correlates strongly
with ratings of depression by psychiatrists. Follow-
ing Rude et al, we treat BDI ? 14 as the threshold
for a positive instance of depression.7 Text prepro-
cessing was done as in ?2.1, with 124 documents in
total averaging around 390 words each.
Training/test split. Because only 12 of 124 au-
thors met the BDI ? 14 threshold, we did not split
randomly, lest the test sample include too few pos-
itive instances to be useful. Instead we included a
random 6 of the 12 above-threshold cases, plus 24
more items sampled at random, to create a 30-item
test set. To form the training set from the comple-
mentary items, we added two more copies of each
positive instance to help address class imbalance
(Batista et al, 2004) and, following Rude et al, we
excluded items with with BDI of 0 or 1 as potentially
invalid.
Human comparison. We created a set of expert
human results for comparison by asking three prac-
ticing clinical psychologists to review the test doc-
uments and rate whether or not the author is suffer-
7Each question contributes a score value from 0 to 3, so BDI
scores range from 0 to 63.
ing depression.8 They were asked to ?decide how
at-risk this person might be for depression?, assign-
ing 0 (no significant concerns), 1 (mild concerns, but
does not require futher evaluation), or 2 (requires at-
tention, refer for further evaluation). Following rec-
ommended practice for cases where different labels
are not equally distinct from each other (Artstein
and Poesio, 2008), we evaluate inter-coder agree-
ment using Krippendorff?s ?; our ?, computed for
ordinal data, is 0.722.
Features. We ran 50-topic LDA on the 4,777 es-
says from ?2.1 plus the BDI training items, using the
posterior topic distributions as features as in ?2.1.
As in ?2.1, the LIWC features comprised one count
per LIWC category, and LIWC+LDA features were
the union of the two.
3.2 Results
Regression on LIWC features alone achieved r =
.288, and adding topic features improved this sub-
stantially to r = .416. Treating BDI ? 14 as the
threshold for positive instances (i.e. that an author is
depressed), Table 2 shows that adding topic features
improves precision without harming recall. Auto-
matic prediction is more conservative than human
ratings, trading recall for precision to achieve com-
parable F-measure on this test set.9
8These psychologists all have doctoral degrees, are licensed,
and spend significant time primarily in assessment and diag-
nosis of psychological disorders. None were familiar with the
specifics of this study.
9A reviewer observes, correctly, that in a scenario where a sys-
tem is providing preliminary screenings to aid psychologists,
the precision/recall tradeoff demonstrated here would poten-
tially be undesirable, since a presumed goal would be to not
miss any cases, even at the risk of some false positives. We
note, however, that the real world is unfortunately replete with
situations where there is significant cost or social/professional
stigma associated with interventions or follow-up testing; in
such situations it might be high precision that is desirable.
These are challenging questions, and the ability to trade off
precision versus recall more flexibly is a topic we are inter-
ested in investigating in future work.
1350
P R F1
LIWC .43 .50 .46
LIWC+LDA .50 .50 .50
Rater 1 .38 .83 .52
Rater 2 .33 .83 .47
Rater 3 .33 .66 .44
Table 2: Prediction quality for depression.
4 Qualitative themes
In order to explore the relevance of the themes
uncovered by LDA, the third author, a practicing
clinical psychologist, reviewed the 50 LDA cate-
gories created in ?3.1. Each category, represented
by its 20 highest-probability words, was given a
readable description. Then, for each category, she
was asked: ?If you were conducting a clinical in-
terview, would observing these themes in a patient?s
responses make you more (less) likely to feel that the
patient merited further evaluation for depression??
Table 3 shows the seven topics selected as par-
ticularly indicative of further evaluation.10 These
capture population-specific properties in ways that
LIWC cannot ? for example, although LIWC does
have a body category, it does not have a category
that corresponds to somatic complaints, which often
co-occur with depression. Similarly, some words re-
lated to energy level, e.g. tired, would be captured
in LIWC?s body, bio, and/or health category, but
the LDA theme corresponding to low energy or lack
of sleep, another potential depression cue, contains
words that make sense there only in context (e.g. to-
morrow, late). Other themes, such as the one labeled
HOMESICKNESS, are clearly relevant (potentially in-
dicative of an adjustment disorder), but even more
specific to the population and context.
5 Related Work
The application of NLP to psychological variables
has seen a recent uptick in community activity. One
recent shared task brings together research on the
Big-5 personality traits (Celli et al, 2013; Kosin-
ski et al, 2013), and another involved research on
identification of emotion in suicide notes (Pestian et
al., 2012). Other examples include NLP research on
10All 50 can be found in the supplemental materials at
http://umiacs.umd.edu/?resnik/papers/
emnlp2013-supplemental/.
autistic spectrum disorders (Van Santen et al, 2010;
Prudhommeaux et al, 2011; Lehr et al, 2013) and
dementia (Pakhomov et al, 2011; Lehr et al, 2012;
Roark et al, 2011).
With regard to depression, Neuman et al (2012)
develop a corpus-based ?depression lexicon? and
produce promising screening results, and De Choud-
hury et al (2013) predict social network behav-
ior changes related to post-partum depression. Nei-
ther, however, evaluates using formal instruments
for clinical assessment.
Related investigations involving LDA include
Zhai et al (2012), who use LIWC to provide pri-
ors for corpus-specific emotion categories; Stark et
al. (2012), who combine LIWC and LDA-based
features in classification of social relationships; and
Schwartz et al (2013), who use lexical and topic-
based features in Twitter to predict life satisfaction.
6 Conclusions
In this paper, we have aimed for a small, fo-
cused contribution, investigating the value-add of
topic modeling in text analysis for depression, and
for neuroticism as a strongly associated personal-
ity measure. Our contribution here is not techni-
cal: corpus-specific topics/themes are anticipated by
Zhai et al (2012), and Stark et al (2012) employ
topic-based features for prediction in a supervised
setting. Rather, our contribution here has been to
show that topic models can get us beyond the LIWC
categories to relevant, population-specific themes
related to neuroticism and depression, and to sup-
port that claim using evaluation against formal clin-
ical assessments. More data (e.g. Kosinski et al
(2013)) and more sophisticated models (e.g. super-
vised LDA, Blei and McAuliffe (2008), and exten-
sions such as Nguyen et al (2013)) will be the key
to further progress.
Acknowledgments
We are grateful to Jamie Pennebaker for the LIWC
lexicon and for allowing us to use data from Pen-
nebaker and King (1999) and Rude et al (2004),
to the three psychologists who kindly took the time
to provide human ratings, and to our reviewers for
helpful comments. This work has been supported in
part by NSF grant IIS-1211153.
1351
VEGETATIVE/ENERGY LEVEL sleep tired night bed morning class early tomorrow wake late asleep long hours day sleeping nap today fall stay
time
SOMATIC hurts sick eyes hurt cold head tired back nose itches hate stop starting water neck hand stomach feels kind sore
NEGATIVE/TROUBLE COPING don(?t) hate doesn care didn(?t) understand anymore feel isn(?t) stupid make won(?t) wouldn talk scared wanted
wrong mad stop shouldn(?t)
ANGER/FRUSTRATION hate damn stupid sucks hell shit crap man ass god don blah thing bad suck doesn fucking fuck freaking real
HOMESICKNESS home miss friends back school family weekend austin parents college mom lot boyfriend left houston visit weeks
wait high homesick
EMOTIONAL STRESS feel feeling thinking makes make felt feels things nervous scared lonely feelings afraid moment happy worry
comfortable stress excited guilty
ANXIETY feel happy things lot sad good makes bad make hard mind happen crazy cry day worry times talk great wanted
Table 3: LDA-induced themes related to depression.
References
[APA2013] APA. 2013. The critical
need for psychologists in rural america.
http://www.apa.org/about/gr/education/rural-
need.aspx, Downloaded September 16, 2013.
[Artstein and Poesio2008] Ron Artstein and Massimo
Poesio. 2008. Inter-coder agreement for computa-
tional linguistics. Comput. Linguist., 34(4):555?596,
December.
[Association2000] American Psychiatric Association.
2000. Diagnostic and Statistical Manual of Mental
Disorders, 4th Edition, Text Revision (DSM-IV-TR).
American Psychiatric Association, 4th edition, July.
[Batista et al2004] Gustavo E. A. P. A. Batista,
Ronaldo C. Prati, and Maria Carolina Monard.
2004. A study of the behavior of several methods for
balancing machine learning training data. SIGKDD
Explor. Newsl., 6(1):20?29, June.
[Beck et al1961] Aaron T Beck, Calvin H Ward, Mock
Mendelson, Jeremiah Mock, and J1 Erbaugh. 1961.
An inventory for measuring depression. Archives of
general psychiatry, 4(6):561.
[Bird et al2009] Steven Bird, Ewan Klein, and Edward
Loper. 2009. Natural language processing with
Python. O?Reilly.
[Blei and McAuliffe2008] David Blei and Jon McAuliffe.
2008. Supervised topic models. In J.C. Platt,
D. Koller, Y. Singer, and S. Roweis, editors, Advances
in Neural Information Processing Systems 20, pages
121?128. MIT Press, Cambridge, MA.
[Blei et al2003] David M Blei, Andrew Y Ng, and
Michael I Jordan. 2003. Latent dirichlet alocation.
The Journal of Machine Learning Research, 3:993?
1022.
[Celli et al2013] Fabio Celli, Fabio Pianesi, David Still-
well, and Michal Kosinski. 2013. Computational per-
sonality recognition (shared task) workshop. In In-
ternational Conference on Weblogs and Social Media.
AAAI.
[De Choudhury et al2013] Munmun De Choudhury,
Scott Counts, and Eric Horvitz. 2013. Predicting
postpartum changes in emotion and behavior via
social media. In Proceedings of the 2013 ACM annual
conference on Human factors in computing systems,
pages 3267?3276. ACM.
[Hall et al2009] Mark Hall, Eibe Frank, Geoffrey
Holmes, Bernhard Pfahringer, Peter Reutemann, and
Ian H Witten. 2009. The weka data mining software:
an update. ACM SIGKDD Explorations Newsletter,
11(1):10?18.
[John and Srivastava1999] Oliver P John and Sanjay Sri-
vastava. 1999. The big five trait taxonomy: History,
measurement, and theoretical perspectives. Handbook
of personality: Theory and research, 2:102?138.
[John et al2008] O. P. John, L. P. Naumann, and C. J.
Soto. 2008. Paradigm shift to the integrative big five
trait taxonomy: History, measurement, and conceptual
issues. In Handbook of personality: Theory and re-
search, pages 114?158.
[Kosinski et al2013] Michal Kosinski, David Stillwell,
and Thore Graepel. 2013. Private traits and attributes
are predictable from digital records of human behav-
ior. Proceedings of the National Academy of Sciences,
110(15):5802?5805.
[Lehr et al2012] Maider Lehr, Emily Tucker
Prud?hommeaux, Izhak Shafran, and Brian Roark.
2012. Fully automated neuropsychological assess-
ment for detecting mild cognitive impairment. In
INTERSPEECH.
[Lehr et al2013] Maider Lehr, Izhak Shafran, Emily
Prudhommeaux, and Brian Roark. 2013. Discrimi-
native joint modeling of lexical variation and acous-
tic confusion for automated narrative retelling assess-
ment. In Proceedings of NAACL-HLT, pages 211?220.
[McCallum2002] Andrew Kachites McCallum. 2002.
Mallet: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
[NAMI2013] NAMI. 2013. Ma-
jor depression fact sheet, April.
http://www.nami.org/Template.cfm?Section=depression.
[Neuman et al2012] Yair Neuman, Yohai Cohen, Dan
Assaf, and Gabbi Kedma. 2012. Proactive screen-
ing for depression through metaphorical and automatic
1352
text analysis. Artif. Intell. Med., 56(1):19?25, Septem-
ber.
[Nguyen et al2013] Viet-An Nguyen, Jordan Boyd-
Graber, and Philip Resnik. 2013. Lexical and
hierarchical topic regression. In Neural Information
Processing Systems.
[Pakhomov et al2011] Serguei Pakhomov, Dustin Cha-
con, Mark Wicklund, and Jeanette Gundel. 2011.
Computerized assessment of syntactic complexity in
alzheimers disease: a case study of iris murdochs writ-
ing. Behavior Research Methods, 43(1):136?144.
[Pennebaker and King1999] James W Pennebaker and
Laura A King. 1999. Linguistic styles: language use
as an individual difference. Journal of personality and
social psychology, 77(6):1296.
[Pestian et al2012] John P Pestian, Pawel Matykiewicz,
Michelle Linn-Gust, Brett South, Ozlem Uzuner, Jan
Wiebe, K Bretonnel Cohen, John Hurdle, Christopher
Brew, et al 2012. Sentiment analysis of suicide
notes: A shared task. Biomedical Informatics Insights,
5(Suppl. 1):3.
[Prudhommeaux et al2011] Emily T Prudhommeaux,
Brian Roark, Lois M Black, and Jan van Santen.
2011. Classification of atypical language in autism.
ACL HLT 2011, page 88.
[Roark et al2011] Brian Roark, Margaret Mitchell, J Ho-
som, Kristy Hollingshead, and Jeffrey Kaye. 2011.
Spoken language derived measures for detecting mild
cognitive impairment. Audio, Speech, and Language
Processing, IEEE Transactions on, 19(7):2081?2090.
[Rude et al2004] Stephanie Rude, Eva-Maria Gortner,
and James Pennebaker. 2004. Language use of de-
pressed and depression-vulnerable college students.
Cognition & Emotion, 18(8):1121?1133.
[Schwartz et al2013] H. Andrew Schwartz, Johannes C.
Eichstaedt, Margaret L. Kern, Lukasz Dziurzynski,
Megha Agrawal, Gregory J. Park, Shrinidhi K. Lak-
shmikanth, Sneha Jha, Martin E. P. Seligman, and
Lyle Ungar. 2013. Characterizing geographic varia-
tion in well-being using tweets. In Seventh Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM 2013).
[Sibelius2013] Kathleen Sibelius. 2013. Increas-
ing access to mental health services, April.
http://www.whitehouse.gov/blog/2013/04/10/increasing-
access-mental-health-services.
[Stark et al2012] Anthony Stark, Izhak Shafran, and Jef-
frey Kaye. 2012. Hello, who is calling?: can words
reveal the social nature of conversations? In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 112?119.
Association for Computational Linguistics.
[Tellegen et al2003] A. Tellegen, Y.S. Ben-Porath, J.L.
McNulty, P.A. Arbisi, and B. Graham, J.R.and Kaem-
mer. 2003. The MMPI-2 Restructured Clinical
Scales: Development, validation, and interpretation.
Minneapolis, MN: University of Minnesota Press.
[Van Santen et al2010] Jan PH Van Santen, Emily T
Prud?hommeaux, Lois M Black, and Margaret
Mitchell. 2010. Computational prosodic markers for
autism. Autism, 14(3):215?236.
[Zhai et al2012] Ke Zhai, Jordan Boyd-Graber, Nima
Asadi, and Mohamad L. Alkhouja. 2012. Mr. lda:
a flexible large scale topic modeling package using
variational inference in mapreduce. In Proceedings of
the 21st international conference on World Wide Web,
WWW ?12, pages 879?888, New York, NY, USA.
ACM.
1353
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1752?1757,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Sometimes Average is Best: The Importance of Averaging for Prediction
using MCMC Inference in Topic Modeling
Viet-An Nguyen
Computer Science
University of Maryland
College Park, MD
vietan@cs.umd.edu
Jordan Boyd-Graber
Computer Science
University of Colorado
Boulder, CO
jbg@boydgraber.org
Philip Resnik
Linguistics and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
Markov chain Monte Carlo (MCMC) approxi-
mates the posterior distribution of latent vari-
able models by generating many samples and
averaging over them. In practice, however, it
is often more convenient to cut corners, using
only a single sample or following a suboptimal
averaging strategy. We systematically study dif-
ferent strategies for averaging MCMC samples
and show empirically that averaging properly
leads to significant improvements in prediction.
1 Introduction
Probabilistic topic models are powerful methods to un-
cover hidden thematic structures in text by projecting
each document into a low dimensional space spanned
by a set of topics, each of which is a distribution over
words. Topic models such as latent Dirichlet alloca-
tion (Blei et al., 2003, LDA) and its extensions discover
these topics from text, which allows for effective ex-
ploration, analysis, and summarization of the otherwise
unstructured corpora (Blei, 2012; Blei, 2014).
In addition to exploratory data analysis, a typical goal
of topic models is prediction. Given a set of unanno-
tated training data, unsupervised topic models try to
learn good topics that can generalize to unseen text.
Supervised topic models jointly capture both the text
and associated metadata such as a continuous response
variable (Blei and McAuliffe, 2007; Zhu et al., 2009;
Nguyen et al., 2013), single label (Rosen-Zvi et al.,
2004; Lacoste-Julien et al., 2008; Wang et al., 2009)
or multiple labels (Ramage et al., 2009; Ramage et al.,
2011) to predict metadata from text.
Probabilistic topic modeling requires estimating the
posterior distribution. Exact computation of the poste-
rior is often intractable, which motivates approximate
inference techniques (Asuncion et al., 2009). One popu-
lar approach is Markov chain Monte Carlo (MCMC), a
class of inference algorithms to approximate the target
posterior distribution. To make prediction, MCMC al-
gorithms generate samples on training data to estimate
corpus-level latent variables, and use them to generate
samples to estimate document-level latent variables for
test data. The underlying theory requires averaging on
both training and test samples, but in practice it is often
convenient to cut corners: either skip averaging entirely
by using just the values of the last sample or use a single
training sample and average over test samples.
We systematically study non-averaging and averaging
strategies when performing predictions using MCMC in
topic modeling (Section 2). Using popular unsupervised
(LDA in Section 3) and supervised (SLDA in Section 4)
topic models via thorough experimentation, we show
empirically that cutting corners on averaging leads to
consistently poorer prediction.
2 Learning and Predicting with MCMC
While reviewing all of MCMC is beyond the scope of
this paper, we need to briefly review key concepts.
1
To
estimate a target density p(x) in a high-dimensional
space X , MCMC generates samples {x
t
}
T
t=1
while ex-
ploring X using the Markov assumption. Under this
assumption, sample x
t+1
depends on sample x
t
only,
forming a Markov chain, which allows the sampler to
spend more time in the most important regions of the
density. Two concepts control sample collection:
Burn-in B: Depending on the initial value of the
Markov chain, MCMC algorithms take time to reach
the target distribution. Thus, in practice, samples before
a burn-in period B are often discarded.
Sample-lag L: Averaging over samples to estimate
the target distribution requires i.i.d. samples. However,
future samples depend on the current samples (i.e., the
Markov assumption). To avoid autocorrelation, we dis-
card all but every L samples.
2.1 MCMC in Topic Modeling
As generative probabilistic models, topic models define
a joint distribution over latent variables and observable
evidence. In our setting, the latent variables consist of
corpus-level global variables g and document-level lo-
cal variables l; while the evidence consists of words w
and additional metadata y?the latter omitted in unsu-
pervised models.
During training, MCMC estimates the posterior
p(g, l
TR
|w
TR
,y
TR
) by generating a training Markov
chain of T
TR
samples.
2
Each training sample i pro-
vides a set of fully realized global latent variables
?
g(i),
which can generate test data. During test time, given a
1
For more details please refer to Neal (1993), Andrieu et
al. (2003), Resnik and Hardisty (2010).
2
We omit hyperparameters for clarity. We split data into
training (TR) and testing (TE) folds, and denote the training
iteration i and the testing iteration j within the corresponding
Markov chains.
1752
Training burn-in Btr Training lag LTR Training lag Ltr
Training period Ttr
T
e
s
t
b
u
r
n
-
i
n
B
te
T
e
s
t
p
e
r
i
o
d
T
te
T
e
s
t
l
a
g
L
te
1 2
3 4
2
4
2
4
3 4
3 4
4
4
4 4
1
2
3
4
Samples used in Single Final (SF)
Samples used in Single Average (SA)
Samples used in Multiple Final (MF)
Samples used in Multiple Average (MA)
Training chain
Single test chains
sample i in
training chain
(learned model)
test chain i
sample j in 
test chain i 
(prediction S(i,j))
Discarded samples during training
Discarded samples during test
Selected samples during training
Selected samples during test
Figure 1: Illustration of training and test chains in MCMC, showing samples used in four prediction strategies studied
in this paper: Single Final (SF), Single Average (SA), Multiple Final (MF), and Multiple Average (MA).
learned model from training sample i, we generate a test
Markov chain of T
TE
samples to estimate the local latent
variables p(l
TE
|w
TE
,
?
g(i)) of test data. Each sample
j of test chain i provides a fully estimated local latent
variables
?
l
TE
(i, j) to make a prediction.
Figure 1 shows an overview. To reduce the ef-
fects of unconverged and autocorrelated samples, dur-
ing training we use a burn-in period of B
TR
and a
sample-lag of L
TR
iterations. We use T
TR
= {i | i ?
(B
TR
, T
TR
] ? (i ? B
TR
) mod L
TR
= 0} to denote the
set of indices of the selected models. Similarly, B
TE
and L
TE
are the test burn-in and sample-lag. The
set of indices of selected samples in test chains is
T
TE
= {j | j ? (B
TE
, T
TE
] ? (j ?B
TE
) mod L
TE
= 0}.
2.2 Averaging Strategies
We use S(i, j) to denote the prediction obtained from
sample j of the test chain i. We now discuss different
strategies to obtain the final prediction:
? Single Final (SF) uses the last sample of last test
chain to obtain the predicted value,
S
SF
= S(T
TR
, T
TE
). (1)
? Single Average (SA) averages over multiple sam-
ples in the last test chain
S
SA
=
1
|T
TE
|
?
j?T
TE
S(T
TR
, j). (2)
This is a common averaging strategy in which we
obtain a point estimate of the global latent variables
at the end of the training chain. Then, a single test
chain is generated on the test data and multiple sam-
ples of this test chain are averaged to obtain the final
prediction (Chang, 2012; Singh et al., 2012; Jiang et
al., 2012; Zhu et al., 2014).
? Multiple Final (MF) averages over the last sam-
ples of multiple test chains from multiple models
S
MF
=
1
|T
TR
|
?
i?T
TR
S(i, T
TE
). (3)
? Multiple Average (MA) averages over all samples
of multiple test chains for distinct models,
S
MA
=
1
|T
TR
|
1
|T
TE
|
?
i?T
TR
?
j?T
TE
S(i, j), (4)
3 Unsupervised Topic Models
We evaluate the predictive performance of the unsu-
pervised topic model LDA using different averaging
strategies in Section 2.
LDA: Proposed by Blei et al. in 2003, LDA posits that
each document d is a multinomial distribution ?
d
over
K topics, each of which is a multinomial distribution
?
k
over the vocabulary. LDA?s generative process is:
1. For each topic k ? [1,K]
(a) Draw word distribution ?
k
? Dir(?)
2. For each document d ? [1, D]
(a) Draw topic distribution ?
d
? Dir(?)
(b) For each word n ? [1, N
d
]
i. Draw topic z
d,n
? Mult(?
d
)
ii. Draw word w
d,n
? Mult(?
z
d,n
)
In LDA, the global latent variables are topics {?
k
}
K
k=1
and the local latent variables for each document d are
topic proportions ?
d
.
Train: During training, we use collapsed Gibbs sam-
pling to assign each token in the training data with a
topic (Steyvers and Griffiths, 2006). The probability of
1753
assigning token n of training document d to topic k is
p(z
TR
d,n
= k | z
TR
?d,n
,w
TR
?d,n
, w
TR
d,n
= v) ?
N
?d,n
TR,d,k
+ ?
N
?d,n
TR,d,?
+K?
?
N
?d,n
TR,k,v
+ ?
N
?d,n
TR,k,?
+ V ?
, (5)
where N
TR,d,k
is the number of tokens in the training
document d assigned to topic k, and N
TR,k,v
is the num-
ber of times word type v assigned to topic k. Marginal
counts are denoted by ?, and
?d,n
denotes the count
excluding the assignment of token n in document d.
At each training iteration i, we estimate the distribu-
tion over words
?
?
k
(i) of topic k as
?
?
k,v
(i) =
N
TR,k,v
(i) + ?
N
TR,k,?
(i) + V ?
(6)
where the counts N
TR,k,v
(i) and N
TR,k,?
(i) are taken at
training iteration i.
Test: Because we lack explicit topic annotations for
these data (c.f. Nguyen et al. (2012)), we use perplexity?
a widely-used metric to measure the predictive power
of topic models on held-old documents. To compute
perplexity, we follow the estimating ? method (Wal-
lach et al., 2009, Section 5.1) and evenly split each test
document d into w
TE
1
d
and w
TE
2
d
. We first run Gibbs
sampling on w
TE
1
d
to estimate the topic proportion
?
?
TE
d
of test document d. The probability of assigning topic k
to token n inw
TE
1
d
is p(z
TE
1
d,n
= k | z
TE
1
?d,n
,w
TE
1
,
?
?(i)) ?
N
?d,n
TE
1
,d,k
+ ?
N
?d,n
TE
1
,d,?
+K?
?
?
?
k,w
TE
1
d,n
(i)
(7)
whereN
TE
1
,d,k
is the number of tokens inw
TE
1
d
assigned
to topic k. At each iteration j in test chain i, we can
estimate the topic proportion vector
?
?
TE
d
(i, j) for test
document d as
?
?
TE
d,k
(i, j) =
N
TE
1
,d,k
(i, j) + ?
N
TE
1
,d,?
(i, j) +K?
(8)
where both the counts N
TE
1
,d,k
(i, j) and N
TE
1
,d,?
(i, j)
are taken using sample j of test chain i.
Prediction: Given
?
?
TE
d
(i, j) and
?
?(i) at sample j
of test chain i, we compute the predicted likeli-
hood for each unseen token w
TE
2
d,n
as S(i, j) ?
p(w
TE
2
d,n
|
?
?
TE
d
(i, j),
?
?(i)) =
?
K
k=1
?
?
TE
d,k
(i, j) ?
?
?
k,w
TE
2
d,n
(i).
Using different strategies described in Section 2,
we obtain the final predicted likelihood for each un-
seen token p(w
TE
2
d,n
|
?
?
TE
d
,
?
?) and compute the perplex-
ity as exp
(
?(
?
d
?
n
log(p(w
TE
2
d,n
|
?
?
TE
d
,
?
?)))/N
TE
2
)
where N
TE
2
is the number of tokens in w
TE
2
.
Setup: We use three Internet review datasets in our
experiment. For all datasets, we preprocess by tokeniz-
ing, removing stopwords, stemming, adding bigrams to
l
l
l l l l l l l l
l
l
l l l l l l l l
l
l
l l l l l l l l
Restaurant Reviews
Movie Reviews
Hotel Reviews
1160
1200
1240
19502000
20502100
2150
750
775
800
600 700 800 900 1000
600 700 800 900 1000
600 700 800 900 1000Number of training iterations
Perp
lexity
lMultiple?Average Multiple?Final Single?Average Single?Final
Figure 2: Perplexity of LDA using different averaging
strategies with different number of training iterations
T
TR
. Perplexity generally decreases with additional
training iterations, but the drop is more pronounced
with multiple test chains.
the vocabulary, and we filter using TF-IDF to obtain a
vocabulary of 10,000 words.
3
The three datasets are:
? HOTEL: 240,060 reviews of hotels from TripAdvi-
sor (Wang et al., 2010).
? RESTAURANT: 25,459 reviews of restaurants from
Yelp (Jo and Oh, 2011).
? MOVIE: 5,006 reviews of movies from Rotten
Tomatoes (Pang and Lee, 2005)
We report cross-validated average performance over
five folds, and use K = 50 topics for all datasets. To
update the hyperparameters, we use slice sampling (Wal-
lach, 2008, p. 62).
4
Results: Figure 2 shows the perplexity of the four
averaging methods, computed with different number
of training iterations T
TR
. SA outperforms SF, showing
the benefits of averaging over multiple test samples
from a single test chain. However, both multiple chain
methods (MF and MA) significantly outperform these
two methods.
This result is consistent with Asuncion et al. (2009),
who run multiple training chains but a single test chain
for each training chain and average over them. This
is more costly since training chains are usually signif-
icantly longer than test chains. In addition, multiple
training chains are sensitive to their initialization.
3
To find bigrams, we begin with bigram candidates that
occur at least 10 times in the corpus and use a ?
2
test to filter
out those having a ?
2
value less than 5. We then treat selected
bigrams as single word types and add them to the vocabulary.
4
MCMC setup: T
TR
= 1, 000, B
TR
= 500, L
TR
= 50,
T
TE
= 100, B
TE
= 50 and L
TE
= 5.
1754
MSE
pR.squared
0.60
0.65
0.70
0.75
0.25
0.30
0.35
0.40
1000 2000 3000 4000 5000
1000 2000 3000 4000 5000Number of iterations
(a) Restaurant reviews MSE
pR.squared
9000
10000
11000
12000
13000
30000
31000
32000
33000
34000
1000 2000 3000 4000 5000
1000 2000 3000 4000 5000Number of iterations
(b) Movie reviews MSE
pR.squared
0.400
0.425
0.450
0.475
0.500
0.500
0.525
0.550
0.575
0.600
600 700 800 900 1000
600 700 800 900 1000Number of iterations
(c) Hotel reviews
Multiple AverageMultiple FinalSingle AverageSingle Final
Figure 3: Performance of SLDA using different averaging strategies computed at each training iteration.
4 Supervised Topic Models
We evaluate the performance of different prediction
methods using supervised latent Dirichlet allocation
(SLDA) (Blei and McAuliffe, 2007) for sentiment anal-
ysis: predicting review ratings given review text. Each
review text is the document w
d
and the metadata y
d
is
the associated rating.
SLDA: Going beyond LDA, SLDA captures the rela-
tionship between latent topics and metadata by mod-
eling each document?s continuous response variable
using a normal linear model, whose covariates are
the document?s empirical distribution of topics: y
d
?
N (?
T
?
z
d
, ?) where ? is the regression parameter vec-
tor and
?
z
d
is the empirical distribution over topics of
document d. The generative process of SLDA is:
1. For each topic k ? [1,K]
(a) Draw word distribution ?
k
? Dir(?)
(b) Draw parameter ?
k
? N (?, ?)
2. For each document d ? [1, D]
(a) Draw topic distribution ?
d
? Dir(?)
(b) For each word n ? [1, N
d
]
i. Draw topic z
d,n
? Mult(?
d
)
ii. Draw word w
d,n
? Mult(?
z
d,n
)
(c) Draw response y
d
? N (?
T
?
z
d
, ?) where
z?
d,k
=
1
N
d
?
N
d
n=1
I [z
d,n
= k]
where I [x] = 1 if x is true, and 0 otherwise.
In SLDA, in addition to the K multinomials {?
k
}
K
k=1
,
the global latent variables also contain the regression
parameter ?
k
for each topic k. The local latent variables
of SLDA resembles LDA?s: the topic proportion vector
?
d
for each document d.
Train: For posterior inference during training, follow-
ing Boyd-Graber and Resnik (2010), we use stochastic
EM, which alternates between (1) a Gibbs sampling
step to assign a topic to each token, and (2) optimizing
the regression parameters. The probability of assigning
topic k to token n in the training document d is
p(z
TR
d,n
= k | z
TR
?d,n
,w
TR
?d,n
, w
TR
d,n
= v) ?
N (y
d
;?
d,n
, ?) ?
N
?d,n
TR,d,k
+ ?
N
?d,n
TR,d,?
+K?
?
N
?d,n
TR,k,v
+ ?
N
?d,n
TR,k,?
+ V ?
(9)
where ?
d,n
= (
?
K
k
?
=1
?
k
?
N
?d,n
TR,d,k
?
+ ?
k
)/N
TR,d
is the
mean of the Gaussian generating y
d
if z
TR
d,n
= k. Here,
N
TR,d,k
is the number of times topic k is assigned to
tokens in the training document d;N
TR,k,v
is the number
of times word type v is assigned to topic k; ? represents
marginal counts and
?d,n
indicates counts excluding the
assignment of token n in document d.
We optimize the regression parameters ? using L-
BFGS (Liu and Nocedal, 1989) via the likelihood
L(?) = ?
1
2?
D?
d=1
(y
TR
d
??T ?zTR
d
)
2
?
1
2?
K?
k=1
(?
k
??)
2
(10)
At each iteration i in the training chain, the estimated
global latent variables include the a multinomial
?
?
k
(i)
and a regression parameter ??
k
(i) for each topic k.
Test: Like LDA, at test time we sample the topic as-
signments for all tokens in the test data
p(z
TE
d,n
= k | z
TE
?d,n
,w
TE
) ?
N
?d,n
TE,d,k
+ ?
N
?d,n
TE,d,?
+K?
?
?
?
k,w
TE
d,n
(11)
Prediction: The predicted value S(i, j) in this case is
the estimated value of the metadata review rating
S(i, j) ? y?
TE
d
(i, j) =
?
?(i)
T
z?
TE
d
(i, j), (12)
where the empirical topic distribution of test document d
is z?
TE
d,k
(i, j) ?
1
N
TE,d
?
N
TE,d
n=1
I
[
z
TE
d,n
(i, j) = k
]
.
1755
MSE
pR?squared
0.60
0.65
0.70
0.30
0.35
0.40
50 100 150 200
50 100 150 200Number of Topics
(a) Restaurant reviews MSE
pR?squared
0.60
0.70
0.80
0.90
0.00
0.10
0.20
0.30
0.40
40 60 80
40 60 80Number of Topics
(a) Restaurant reviews MSE
pR?squared
0.40
0.42
0.44
0.46
0.48
0.52
0.54
0.56
0.58
0.60
50 100 150 200
50 100 150 200Number of Topics
(a) Restaurant reviews
MLRSLDA?MASLDA?MFSLDA?SASLDA?SFSVR
Figure 4: Performance of SLDA using different averaging strategies computed at the final training iteration T
TR
,
compared with two baselines MLR and SVR. Methods using multiple test chains (MF and MA) perform as well as or
better than the two baselines, whereas methods using a single test chain (SF and SA) perform significantly worse.
Experimental setup: We use the same data as in Sec-
tion 3. For all datasets, the metadata are the review
rating, ranging from 1 to 5 stars, which is standard-
ized using z-normalization. We use two evaluation
metrics: mean squared error (MSE) and predictive R-
squared (Blei and McAuliffe, 2007).
For comparison, we consider two baselines: (1) multi-
ple linear regression (MLR), which models the metadata
as a linear function of the features, and (2) support vec-
tor regression (Joachims, 1999, SVR). Both baselines
use the normalized frequencies of unigrams and bigrams
as features. As in the unsupervised case, we report av-
erage performance over five cross-validated folds. For
all models, we use a development set to tune their pa-
rameter(s) and use the set of parameters that gives best
results on the development data at test.
5
Results: Figure 3 shows SLDA prediction results with
different averaging strategies, computed at different
training iterations.
6
Consistent with the unsupervised
results in Section 3, SA outperforms SF, but both are
outperformed significantly by the two methods using
multiple test chains (MF and MA).
We also compare the performance of the four pre-
diction methods obtained at the final iteration T
TR
of
the training chain with the two baselines. The results in
Figure 4 show that the two baselines (MLR and SVR) out-
perform significantly the SLDA using only a single test
5
For MLR we use a Gaussian prior N (0, 1/?) with ? =
a ? 10
b
where a ? [1, 9] and b ? [1, 4]; for SVR, we use
SVM
light
(Joachims, 1999) and vary C ? [1, 50], which
trades off between training error and margin; for SLDA, we fix
? = 10 and vary ? ? {0.1, 0.5, 1.0, 1.5, 2.0}, which trades
off between the likelihood of words and response variable.
6
MCMC setup: T
TR
= 5, 000 for RESTAURANT and
MOVIE and 1, 000 for HOTEL; for all datasets B
TR
= 500,
L
TR
= 50, T
TE
= 100, B
TE
= 20 and L
TE
= 5.
chains (SF and SA). Methods using multiple test chains
(MF and MA), on the other hand, match the baseline
7
(HOTEL) or do better (RESTAURANT and MOVIE).
5 Discussion and Conclusion
MCMC relies on averaging multiple samples to approxi-
mate target densities. When used for prediction, MCMC
needs to generate and average over both training sam-
ples to learn from training data and test samples to make
prediction. We have shown that simple averaging?not
more aggressive, ad hoc approximations like taking the
final sample (either training or test)?is not just a ques-
tion of theoretical aesthetics, but an important factor in
obtaining good prediction performance.
Compared with SVR and MLR baselines, SLDA using
multiple test chains (MF and MA) performs as well as
or better, while SLDA using a single test chain (SF and
SA) falters. This simple experimental setup choice can
determine whether a model improves over reasonable
baselines. In addition, better prediction with shorter
training is possible with multiple test chains. Thus, we
conclude that averaging using multiple chains produces
above-average results.
Acknowledgments
We thank Jonathan Chang, Ke Zhai and Mohit Iyyer for
helpful discussions, and thank the anonymous reviewers
for insightful comments. This research was supported
in part by NSF under grant #1211153 (Resnik) and
#1018625 (Boyd-Graber and Resnik). Any opinions,
findings, conclusions, or recommendations expressed
here are those of the authors and do not necessarily
reflect the view of the sponsor.
7
This gap is because SLDA has not converged after 1,000
training iterations (Figure 3).
1756
References
Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and
Michael I. Jordan. 2003. An introduction to MCMC for
machine learning. Machine Learning, 50(1-2):5?43.
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference for
topic models. In UAI.
David M. Blei and Jon D. McAuliffe. 2007. Supervised topic
models. In NIPS.
David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent
Dirichlet allocation. JMLR, 3.
David M. Blei. 2012. Probabilistic topic models. Commun.
ACM, 55(4):77?84, April.
David M. Blei. 2014. Build, compute, critique, repeat: Data
analysis with latent variable models. Annual Review of
Statistics and Its Application, 1(1):203?232.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic sen-
timent analysis across languages: Multilingual supervised
latent Dirichlet allocation. In EMNLP.
Jonathan Chang. 2012. lda: Collapsed Gibbs sampling meth-
ods for topic models. http://cran.r-project.
org/web/packages/lda/index.html. [Online;
accessed 02-June-2014].
Qixia Jiang, Jun Zhu, Maosong Sun, and Eric P. Xing. 2012.
Monte Carlo methods for maximum margin supervised
topic models. In NIPS.
Yohan Jo and Alice H. Oh. 2011. Aspect and sentiment
unification model for online review analysis. In WSDM.
Thorsten Joachims. 1999. Making large-scale SVM learning
practical. In Advances in Kernel Methods - Support Vector
Learning, chapter 11. Cambridge, MA.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. 2008.
DiscLDA: Discriminative learning for dimensionality re-
duction and classification. In NIPS.
D. Liu and J. Nocedal. 1989. On the limited memory BFGS
method for large scale optimization. Math. Prog.
Radford M. Neal. 1993. Probabilistic inference using Markov
chain Monte Carlo methods. Technical Report CRG-TR-
93-1, University of Toronto.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2012. SITS: A hierarchical nonparametric model using
speaker identity for topic segmentation in multiparty con-
versations. In ACL.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2013. Lexical and hierarchical topic regression. In Neural
Information Processing Systems.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect to
rating scales. In ACL.
Daniel Ramage, David Hall, Ramesh Nallapati, and Christo-
pher Manning. 2009. Labeled LDA: A supervised topic
model for credit attribution in multi-labeled corpora. In
EMNLP.
Daniel Ramage, Christopher D. Manning, and Susan Dumais.
2011. Partially labeled topic models for interpretable text
mining. In KDD, pages 457?465.
Philip Resnik and Eric Hardisty. 2010. Gibbs
sampling for the uninitiated. Technical Report
UMIACS-TR-2010-04, University of Maryland.
http://drum.lib.umd.edu//handle/1903/10058.
Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for authors
and documents. In UAI.
Sameer Singh, Michael Wick, and Andrew McCallum. 2012.
Monte Carlo MCMC: Efficient inference by approximate
sampling. In EMNLP, pages 1104?1113.
Mark Steyvers and Tom Griffiths. 2006. Probabilistic topic
models. In T. Landauer, D. Mcnamara, S. Dennis, and
W. Kintsch, editors, Latent Semantic Analysis: A Road to
Meaning. Laurence Erlbaum.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic models.
In Leon Bottou and Michael Littman, editors, ICML.
Hanna M Wallach. 2008. Structured Topic Models for Lan-
guage. Ph.D. thesis, University of Cambridge.
Chong Wang, David Blei, and Li Fei-Fei. 2009. Simultaneous
image classification and annotation. In CVPR.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. La-
tent aspect rating analysis on review text data: A rating
regression approach. In SIGKDD, pages 783?792.
Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. MedLDA:
maximum margin supervised topic models for regression
and classification. In ICML.
Jun Zhu, Ning Chen, Hugh Perkins, and Bo Zhang. 2014.
Gibbs max-margin topic models with data augmentation.
Journal of Machine Learning Research, 15:1073?1110.
1757
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 349?352,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Generalizing Hierarchical Phrase-based Translation
using Rules with Adjacent Nonterminals
Hendra Setiawan and Philip Resnik
UMIACS Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742, USA
hendra, resnik @umd.edu
Abstract
Hierarchical phrase-based translation (Hiero,
(Chiang, 2005)) provides an attractive frame-
work within which both short- and long-
distance reorderings can be addressed consis-
tently and efciently. However, Hiero is gen-
erally implemented with a constraint prevent-
ing the creation of rules with adjacent nonter-
minals, because such rules introduce compu-
tational and modeling challenges. We intro-
duce methods to address these challenges, and
demonstrate that rules with adjacent nontermi-
nals can improve Hiero's generalization power
and lead to signicant performance gains in
Chinese-English translation.
1 Introduction
Hierarchical phrase-based translation (Hiero, (Chi-
ang, 2005)) has proven to be a very useful com-
promise between syntactically informed and purely
corpus-driven translation. By automatically learn-
ing synchronous grammar rules from parallel text,
Hiero captures short- and long-distance reorderings
consistently and efciently. However, implementa-
tions of Hiero generally forbid adjacent nonterminal
symbols on the source side of hierarchical rules, a
practice we will refer to as the non-adjacent nonter-
minals constraint. The main argument against such
rules is that they cause the system to produce multi-
ple derivations that all lead to the same translation ?
a form of redundancy known as spurious ambiguity.
Spurious ambiguity can lead to drastic reductions in
decoding efciency, and the obvious solutions, such
as reducing beam width, erode translation quality.
In Section 2, we argue that the non-adjacent non-
terminals constraints severely limits Hiero's gener-
alization power, limiting its coverage of important
reordering phenomena. In Section 3, we discuss
the challenges that arise in relaxing this constraint.
In Section 4 we introduce new methods to address
those challenges, and Section 5 validates the ap-
proach empirically.
Improving Hiero via variations on rule prun-
ing and ltering is well explored, e.g., (Chiang,
2005; Chiang et al, 2008; Zollmann and Venugopal,
2006), to name just a few. These proposals dif-
fer from each other mainly in the specic linguis-
tic knowledge being used, and on which side the
constraints are applied. In contrast, we complement
previous work by showing that adding rules to Hiero
can provide benets if done judiciously.
2 Judicious Use of Adjacent Nonterminals
Our motivations largely followMenezes and Quirk's
(2007) discussion of reorderings and generalization.
As a specic example, we will use a Chinese to En-
glish verb phrase (VP) translation (Fig. 1), which
represents one of the most prominent phrase con-
structions in Chinese. Here the construction of the
Chinese VP involves joining a prepositional phrase
(PP) and a smaller verbal phrase (VP-A), with the
preposition at the beginning as a PP marker. In the
translation, the VP-A precedes the PP, a shift from
pre-verbal PP in Chinese to post-verbal in English.
?\??? ???
rank 10th at Eastern division
????
????
?
PPP
PPP
P
PPP
PPP
P
P NP VP-A
PP
VP
?? HH
?? HHHHHH
Figure 1: A Chinese-English verb phrase translation
349
Hiero can correctly translate the example if it
learns any of the following rules from training data:
X??? X1 ???, rank 10th at X1? (1)
X?? ??\?? X1, X1 at Eastern div.? (2)
X??X1 ?\?? X2, X2 X1 Eastern div.? (3)
However, in practice, data sparsity makes the chance
of learning these rules rather slim. For instance,
learning Rule 1 depends on training data containing
instances of the shift with identical wording for the
VP-A, which belongs to an open word class.
If Hiero fails to learn any of the above rules, it
will apply the ?glue rules? S ? ?S X1, S X1? and
S ? ?X, X?. But these glue rules clearly can-
not model the VP-A's movement. In failing to learn
Rules 1-3, Hiero has no choice but to translate VP-A
in a monotone order.
On the other hand, consider the following rules
with adjacent nonterminals on the source side (or XX
rules, for brevity):
X??? X1X2, X2 at X1? (4)
X??X1X2???, rank 10th X1X2? (5)
X??X1X2, X2X1? (6)
Note that although XX rules 4-6 can potentially in-
crease the chance of modeling the pre-verbal to post-
verbal shift, not all of them are benecial to learn.
For instance, Rule 5 models the word order shift but
introduces spurious ambiguity, since the nontermi-
nals are translated in monotone order. Rule 6, which
resembles the inverted rule of the Inversion Trans-
duction Grammar (Wu, 1997), is highly ambigu-
ous because its application has no lexical grounding.
Rule 4 avoids both problems, and is also easier to
learn, since it is lexically anchored by a preposition,
?(at), which we can expect to appear frequently in
training. These observations will motivate us to fo-
cus on rules that model non-monotone reordering of
phrases surrounding a lexical item on the target side.
3 Addressing XX Rule Challenges
The rst challenge created by introducing XX rules
is computational: relaxing the constraint signi-
cantly increases the grammar size. Motivated by
our earlier discussion, we address this by permitting
only rules that model non-monotone reordering, i.e.
those rules whose nonterminals are projected into
the target language in a different word order, leaving
monotone mappings to be handled by the glue rules
as previously. This choice helps keep the search
space more manageable, and also avoids spurious
ambiguity. In addition, we disallow rules in which
nonterminals are adjacent on both the source and tar-
get sides, by imposing the non adjacent nonterminal
constraint on the target side whenever the constraint
is relaxed on the source side. This forces any non-
monotone reorderings to always be grounded in lex-
ical evidence. We refer to the permitted subset of
XX rules as XX-nonmono rules.
The second challenge involves modeling: intro-
ducing XX rules places them in competition with
the existing glue rules. In particular, these two kinds
of rules try to model the same phenomena, namely
the translations of phrases that appear next to each
other. However, they differ in terms of the features
associated with the rules. XX rules will be asso-
ciated with the same features as any other hierar-
chical rules, since they are all learned via an iden-
tical training method. In contrast, glue rules are
introduced into the grammar in an ad hoc manner,
and the only feature associated with them is a ?glue
penalty?. These distinct feature sets makes direct
comparison of scores unreliable. As a result the de-
coder may simply prefer to always select glue rules
because they are associated with fewer features re-
sulting in adjacent phrases always being translated
in a monotone order. To address this issue, we in-
troduce a new model, which we call the target-side
function words orientation-based model, or simply
Porit , which evaluates the application of the two
kinds of rules on the same context, i.e. for our ex-
ample, it is the function word?(at).
4 Target-side Function Words
Orientation-based Model
The Porit model is motivated by the function words
reordering hypothesis (Setiawan et al, 2007), which
suggests that function words encode essential infor-
mation about the (re)ordering of their neighboring
phrases. In contrast to Setiawan et al (2007), who
looked at neighboring contexts for function words
on the source side, we focus here on modeling the
inuence of function words on neighboring phrases
350
on the target side. We argue that this focus better ts
our purpose, since the phrases that we want to model
are the function words' neighbors on the target side,
as illustrated in Fig. 1.
To develop this idea, we rst dene an orit func-
tion that takes a source function word as a refer-
ence point, along with its neighboring phrase on the
target side. The orit function outputs one of the
following orientation values (Nagata et al, 2006):
Monotone-Adjacent (MA); Reverse-Adjacent (RA);
Monotone-Gap (MG); and Reverse-Gap (RG). The
Monotone/Reverse distinction indicates whether the
source order follows the target order. The Ad-
jacent/Gap distinction indicates whether the two
phrases are adjacent or separated by an intervening
phrase on the source side. For example, in Fig. 1,
the value of orit for right neighbor Eastern division
with respect to function word? (at) is MA, since its
corresponding source phrase ?\?? is adjacent
to? (at) and their order is preserved on the English
side. The value for left neighbor rank 10th with re-
spect to? (at) is RG, since ??? is separated
from ? (at) and their order is reversed on the En-
glish side.
More formally, we dene Porit(orit(Y,X)|Y ),
where orit(Y,X) ? {MA,RA,MG,RG} is the ori-
entation of a target phrase X with a source function
word Y as the reference point.1
We estimate the orientation model us-
ing maximum likelihood, which involves
counting and normalizing events of interest:
(Y, o = orit(Y,X)). Specically, we estimate
Porit(o|Y ) = C(Y, o)/C(Y, ?). Collecting training
counts C(Y, o) involves several steps. First, we
run GIZA++ on the training bitext and apply the
?grow-diag-nal? heuristic over the training data
to produce a bi-directional word alignment. Then,
we enumerate all occurrences of Y and determine
orit(Y,X). To ensure uniqueness, we enforce
that neighbor X be the longest possible phrase
that satises the consistency constraint (Och and
Ney, 2004). Determining orit(Y,X) can then be
done in a straightforward manner by looking at the
monotonicity (monotone or reverse) and adjacency
(adjacent or gap) between Y 's and X .
1
In fact, separate models are developed for left and right
neighbors, although for clarity we suppress this distinction
throughout.
MT06 MT08
baseline 30.58 23.59
+itg 29.82 23.21
+XX 30.10 22.86
+XX-nonmono 30.96 24.07
+orit 30.19 23.69
+XX-nonmono+orit 31.49 24.73
Table 1: Experimental results where better than baseline
results are italicized, and statistically signicant better
(p < 0.01) are in bold.
5 Experiments
We evaluated the generalization of Hiero to include
XX rules on a Chinese-to-English translation task.
We treat the N = 128 most frequent words in
the corpus as function words, an approximation that
has worked well in the past and minimized depen-
dence on language-specic resources (Setiawan et
al., 2007). We report BLEU r4n4 and assess signi-
cance using the standard bootstrapping approach.
We trained on the NIST MT06 Eval corpus ex-
cluding the UN data (approximately 900K sentence
pairs), segmenting Chinese using the Harbin seg-
menter (Zhao et al, 2001). Our 5-gram language
model with modied Kneser-Ney smoothing was
trained on the English side of our training data plus
portions of the Gigaword v2 English corpus. We
optimized the feature weights using minimum er-
ror rate training, using the NIST MT03 test set as
the development set. We report the results on the
NIST 2006 evaluation test (MT06) and the NIST
2008 evaluation test (MT08).
Table 1 reports experiments in an incremental
fashion, starting from the baseline model (the orig-
inal Hiero), then adding different sets of rules, and
nally adding the orientation-based model. In our
rst experiments, we investigated the introduction
of three different sets of XX rules. First (+itg),
we simply add the ITG's inverted rule (Rule 6) to
the baseline system in an ad-hoc manner, similar to
the glue rules. This hurts performance consistently
across MT06 and MT08 sets, which we suspect is
a result of ITG rule applications often aggravating
search error. Second (+XX), we permitted general
XX rules. This results in a grammar size increase of
25-26%, ltering out rules irrelevant for the test set,
351
and leads to a signicant performance drop, again
perhaps attributable to search error. When we in-
spected the rules, we observe that the majority of
these rules involve spurious word insertions. Third
(+XX-nonmono), we introduced only XX-nonmono
rules; this produced only a 5% additional rules, and
yielded a marginal but consistent gain.
In a second experiment (+orit), we introduced
the target-side function words orientation-based
model. Note that this experiment is orthogonal to the
rst set, since we introduce no additional rules. Re-
sults are mixed, worse for MT06 but better (with sig-
nicance) for MT08. Here, we suspect the model's
potential has not been fully realized, since Hiero
only considers monotone reordering in unseen cases.
Finally, we combine both the XX-nonmono rules
and the Porit model (+XX-nonmono+orit). The
combination produces a signicant, consistent gain
across all test sets. This result suggests that the ori-
entation model contributes more strongly in unseen
cases when Hiero also considers non-monotone re-
ordering. We interpret this result as a validation
of our hypothesis that carefully relaxing the non-
adjacent constraint improves translation.
6 Discussion and Future Work
To our knowledge, the work reported here is the
rst to relax the non-adjacent nonterminals con-
straint in hierarchical phrase-based models. The re-
sults conrm that judiciously adding rules to a Hiero
grammar, adjusting the modeling accordingly, can
achieve signicant gains.
Although we found that XX-nonmono rules per-
formed better than general XX rules, we believe the
latter may nonetheless prove useful. Manually in-
specting our system's output, we nd that the output
is often shorter than the references, and the missing
words often correspond to function words that are
modeled by those rules. Using XX rules to model
legitimate word insertions is a topic for future work.
Acknowledgments
The authors gratefully acknowledge partial support
from the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-2-001. Any opinions, ndings, conclusions or
recommendations expressed in this paper are those
of the authors and do not necessarily reect the
views of the sponsors.
References
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 224?233, Honolulu, Hawaii,
October.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL'05), pages 263?270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Arul Menezes and Chris Quirk. 2007. Using dependency
order templates to improve generality in translation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 1?8, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A clustered global phrase
reordering model for statistical machine translation.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 713?720, Sydney, Australia, July. Association
for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007.
Ordering phrases with function words. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 712?719, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Sep.
Tiejun Zhao, Yajuan Lv, Jianmin Yao, Hao Yu, Muyun
Yang, and Fang Liu. 2001. Increasing accuracy of
chinese segmentation with strategy of multi-step pro-
cessing. Journal of Chinese Information Processing
(Chinese Version), 1:13?18.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138?141, New York City, June. As-
sociation for Computational Linguistics.
352
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 858?866,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Context-free reordering, finite-state translation
Chris Dyer and Philip Resnik
UMIACS Laboratory for Computational Linguistics and Information Processing
Department of Linguistics
University of Maryland, College Park, MD 20742, USA
redpony, resnik AT umd.edu
Abstract
We describe a class of translation model in
which a set of input variants encoded as a
context-free forest is translated using a finite-
state translation model. The forest structure of
the input is well-suited to representing word
order alternatives, making it straightforward to
model translation as a two step process: (1)
tree-based source reordering and (2) phrase
transduction. By treating the reordering pro-
cess as a latent variable in a probabilistic trans-
lation model, we can learn a long-range source
reordering model without example reordered
sentences, which are problematic to construct.
The resulting model has state-of-the-art trans-
lation performance, uses linguistically moti-
vated features to effectively model long range
reordering, and is significantly smaller than a
comparable hierarchical phrase-based transla-
tion model.
1 Introduction
Translation models based on synchronous context-
free grammars (SCFGs) have become widespread in
recent years (Wu, 1997; Chiang, 2007). Compared
to phrase-based models, which can be represented as
finite-state transducers (FSTs, Kumar et al (2006)),
one important benefit that SCFG models have is the
ability to process long range reordering patterns in
space and time that is polynomial in the length of
the displacement, whereas an FST must generally
explore a number of states that is exponential in
this length.1 As one would expect, for language
1Our interest here is the reordering made possible by varying
the arrangement of the translation units, not the local word order
differences captured inside memorized phrase pairs.
pairs with substantial structural differences (and thus
requiring long-range reordering during translation),
SCFG models have come to outperform the best FST
models (Zollmann et al, 2008).
In this paper, we explore a new way to take advan-
tage of the computational benefits of CFGs during
translation. Rather than using a single SCFG to both
reorder and translate a source sentence into the target
language, we break the translation process into a two
step pipeline where (1) the source language is re-
ordered into a target-like order, with alternatives en-
coded in a context-free forest, and (2) the reordered
source is transduced into the target language using
an FST that represents phrasal correspondences.
While multi-step decompositions of the transla-
tion problem have been proposed before (Kumar et
al., 2006), they are less practical with the rise of
SCFG models, since the context-free languages are
not closed under intersection (Hopcroft and Ullman,
1979). However, the CFLs are closed under intersec-
tion with regular languages. By restricting ourselves
to a finite-state phrase transducer and representing
reorderings of the source in a context-free forest, ex-
act inference over the composition of the two models
is possible.
The paper proceeds as follows. We first ex-
plore reordering forests and describe how to trans-
late them with an FST (?2). Since we would like our
reordering model to discriminate between good re-
orderings of the source and bad ones, we show how
to train our reordering component as a latent variable
in an end-to-end translation model (?3). We then
presents experimental results on language pairs re-
quiring small amounts and large amounts of reorder-
ing (?4). We conclude with a discussion of related
858
work (?6) and possible extensions (?7).
2 Reordering forests and translation
In this section, we describe source reordering
forests, a context-free representation of source lan-
guage word order alternatives.2 The basic idea is
that for the source sentence, f, that is to be trans-
lated, we want to create a (monolingual) context-free
grammarF that generates strings (f?) of words in the
source language that are permutations of the origi-
nal sentence. Specifically, this forest should contain
derivations that put the source words into an order
that approximates how they will be ordered in the
grammar of the target language.
For a concrete example, let us consider the task of
English-Japanese translation.3 Our input sentence
is John ate an apple. Japanese is a head-final lan-
guage, where the heads of phrases (such as the verb
in a verb phrase) typically come last, and English
is a head-initial language, where heads come first.
As a result, the usual order for a declarative sen-
tence in English is SVO (subject-verb-object), but
in Japanese, it is SOV, and the desired translation
is John-ga ringo-o [an apple] tabeta [ate]. In sum-
mary, when translating from English into Japanese,
it is usually necessary to move verbs from their po-
sition between the subject and object to the end of
the sentence.
This reordering can happen in two ways, which
we depict in Figure 1. In the derivation on the left,
a memorized phrase pair captures the movement of
the verb (Koehn et al, 2003). In the other deriva-
tion, the source is first reordered into target word
order and then translated, using smaller translation
units. In addition, we have assumed that the phrase
translations were learned from a parallel corpus that
is in the original ordering, so the reordering forest F
should include derivations of phrase-size units in the
source order as well as the target order.
2Note that forests are isomorphic to context-free grammars.
For example, what is referred to as the ?parse forest?, and un-
derstood to encode all derivations of a sentence s under some
grammar, can also be understood as being a context-free gram-
mar itself that exactly generates s. We therefore refer to a forest
as a grammar sometimes, or vice versa, depending on which
characterization is clearer in context.
3We use English as the source language since we expect the
parse structure of English sentences will be more familiar to
many readers.
0 1
an : ?
apple : ???? 
John : ???? 
ate : ??? 
[John-ga]
[ringo-o]
[tabeta]
23
ate : ?an : ?
apple : ????  ???  
[ringo-o   tabeta]
Figure 2: A fragment of a phrase-based English-Japanese
translation model, represented as an FST. Japanese ro-
manization is given in brackets.
A minimal reordering forest that supports the
derivations depicted needs to include both an SOV
and SVO version of the source. This could be ac-
complished trivially with the following grammar:
S ? John ate an apple
S ? John an apple ate
However, this grammar misses the opportunity to
take advantage of the regularities in the permuted
structure. A better alternative might be:
S ? John VP
VP ? ate NP
VP ? NP ate
NP ? an apple
In this grammar, the phrases John and an apple are
fixed and only the VP contains ordering ambiguity.
2.1 Reordering forests based on source parses
Many kinds of reordering forests are possible; in
general, the best one for a particular language pair
will be one that is easiest to create given the re-
sources available in the source language. It will
also be the one that most compactly expresses the
source reorderings that are most likely to be use-
ful for translation. In this paper, we consider a
particular kind of reordering forest that is inspired
by the reordering model of Yamada and Knight
(2001).4 These are generated by taking a source lan-
guage parse tree and ?expanding? each node so that it
4One important difference is that our translation model is not
restricted by the structure of the source parse tree; i.e., phrases
used in transduction need not correspond to constituents in the
source reordering forest. However, if a phrase does cross a con-
stituent boundary between constituents A and B, then transla-
tions that use that phrase will have A and B adjacent.
859
????    ????   ??? 
John-ga      ringo-o     tabeta
John an apple ate
????    ????   ??? 
John-ga      ringo-o     tabeta
John ate an apple
John ate an appleJohn ate an apple
f
f'
e
Figure 1: Two possible derivations of a Japanese translation of an English source sentence.
rewrites with different permutations of its children.5
For an illustration using our example sentence, re-
fer to Figure 3 for the forest representation and Fig-
ure 4 for its isomorphic CFG representation. It is
easy to see that this forest generates the two ?good?
order variants from Figure 1; however, the forest in-
cludes many other derivations that will probably not
lead to good translations. For this reason, it is help-
ful to associate the edges in the forest (that is, the
rules in the CFG) with weights reflecting how likely
that rule is to lead to a good translation. We discuss
how these weights can be learned automatically in
?3.
2.2 Translating reordering forests with FSTs
Having described how to construct a context-free re-
ordering forest for the source sentence, we now turn
to the problem of how to translate the source forest
into the target language using a phrase-based trans-
lation model encoded as an FST, e.g. Figure 2. The
process is quite similar to the one used when trans-
lating a source sentence with an SCFG, but with a
twist: rather than representing the translation model
as a grammar and parsing the source sentence, we
represent the source sentence as a grammar (i.e. its
reordering forest), and we use it to ?parse? the trans-
lation model (i.e. the FST representation of the
phrase-based model). The end result (either way!)
is a translation forest containing all possible target-
language translations of the source.
Parsing can be understood as a means of comput-
ing the intersection of an FSA and a CFG (Grune and
Jacobs, 2008). Since we are dealing with FSTs that
define binary relations over strings, not FSAs defin-
ing strings, this operation is more properly compo-
sition. However, since CFG/FSA intersection is less
5For computational tractability, we only consider all permu-
tations only when the number of children is less than 5, other-
wise we exclude permutations where a child moves more than
4 positions away from where it starts.
cumbersome to describe, we present the algorithm
in terms of intersection.
To compute the composition of a reordering for-
est, G, with an FSA, F , we will make use of a variant
of Earley?s algorithm (Earley, 1970). Let weighted
finite-state automaton F = ??, Q, q0, qfinal, ?, w?.
? is a finite alphabet; Q is a set of states; q0 and
qfinal ? Q are start and accept states, respectively,6 ?
is the transition function Q? ?? 2Q, and w is the
transition cost function Q ? Q ? R. We use vari-
ables that refer to states in the FSA with the letters
q, r, and s. We use x to represent a variable that is
an element of ?. Variables u and v represent costs.
X and Y are non-terminals. Lowercase Greek let-
ters are strings of terminals and non-terminals. The
function ?(q, x) returns the state(s) that are reach-
able from state q by taking a transition labeled with
x in the FSA.
Figure 5 provides the inference rules for a
top-down intersection algorithm in the form of a
weighted logic program; the three inference rules
correspond to Earley?s SCAN, PREDICT, and COM-
PLETE, respectively.
3 Reordering and translation model
As pointed out in ?2.1, our reordering forests may
contain many paths, some of which when translated
will lead to good translations and others that will be
bad. We would like a model to distinguish the two.
If we had a parallel corpus of source language
sentences paired with ?reference reorderings?, such
a model could be learned directly as a supervised
learning task. However, creating the optimal target-
language reordering f? for some f is a nontrivial
task.7 Instead of trying to solve this problem, we
opt to treat the reordered from of the source, f?, as a
6Other FSA definitions permit sets of start and final states.
We use the more restricted definition for simplicity and because
in our FSTs q0 = qfinal.
7For a discussion of methods for generating reference re-
860
Original parse:
Reordering forest:
S
V DT NN
VP
NP
subj
NP
obj
John ate
an apple
1 1
1
1
1
1
2
2
2
22
2
S
V DT NN
VP
NP
subj
NP
obj
John ate
an apple
1 1
1
2
2
2
Figure 3: Example of a reordering forest. Linearization
order of non-terminals is indicated by the index at the tail
of each edge. The isomorphic CFG is shown in Figure 4;
dashed edges correspond to reordering-specific rules.
latent variable in a probabilistic translation model.
By doing this, we only require a parallel corpus of
translations to learn the reordering model. Not only
does this make our lives easier, since ?reference re-
orderings? are not necessary, but it is also intuitively
satisfying because from a task perspective, we are
not concerned with values of f?, but only with pro-
ducing a good translation e.
3.1 A probabilistic translation model with a
latent reordering variable
The translation model we use is a two phase process.
First, source sentence f is reordered into a target-
like word order f? according to a reordering model
r(f?|f). The reordered source is then transduced into
the target language according to a translation model
t(e|f?). We require that r(f?|f) can be represented by
orderings from word aligned parallel corpora, refer to Tromble
and Eisner (2009).
Original parse grammar: S? NPsubj VP
VP? V NPobj NPobj ? DT NN
NPsubj ? John V? ate
DT? an NN? apple
Additional reordering grammar rules:
S? VP NPsubj
VP? NPobj V
NPobj ? NN DT
Figure 4: Context-free grammar representation of the for-
est in Figure 3. The reordering grammar contains the
parse grammar, plus the reordering-specific rules.
Initialization:
[S? ? ?S, q0, q0] : 1
Inference rules:
[X ? ? ? x?, q, r] : u
[X ? ?x ? ?, q, ?(r, x)] : u? w(?(r, x))
[X ? ? ? Y ?, q, r]
[Y ? ??, r, r] : u
Y
u
?? ? ? G
[X ? ? ? Y ?, q, s] : u [Y ? ??, s, r] : v
[X ? ?Y ? ?, q, r] : u? v
Goal state:
[S? ? S?, q0, qfinal]
Figure 5: Weighted logic program for computing the in-
tersection of a weighted FSA and a weighted CFG.
a recursion-free probabilistic context-free grammar,
i.e. a forest as in ?2.1, and that t(e|f?) is represented
by a (cyclic) finite-state transducer, as in Figure 2.
Since the reordering forest may define multiple
derivations a from f to a particular f?, and the trans-
ducer may define multiple derivations d from f? to
a particular translation e, we marginalize over these
nuisance variables as follows to define the probabil-
ity of a translation given the source:
p(e|f) =
?
d
?
f?
t(e,d|f?)
?
a
r(f?, a|f) (1)
Crucially, since we have restricted r(f?|f) to have
the form of a weighted CFG and t(e|f?) to be an
861
FST, the quantity (1), which sums over all reorder-
ings (and derivations), can be computed in polyno-
mial time with dynamic programming composition,
as described in ?2.2.
3.2 Conditional training
While it is straightforward to use expectation maxi-
mization to optimize the joint likelihood of the paral-
lel training data with a latent variable model, instead
we use a log-linear parameterization and maximize
conditional likelihood (Blunsom et al, 2008; Petrov
and Klein, 2008). This enables us to employ a rich
set of (possibly overlapping, non-independent) fea-
tures to discriminate among translations. The proba-
bility of a derivation from source to reordered source
to target is thus written in terms of model parameters
? = {?i} as:
p(e,d, f?, a|f; ?) =
exp
?
i ?i ?Hi(e,d, f
?, a, f)
Z(f; ?)
where Hi(e,d, f?, a, f) =
?
r?d
hi(f?, r) +
?
s?a
hi(f, s)
The derivation probability is globally normalized by
the partition Z(f; ?), which is just the sum of the
numerator for all derivations of f (corresponding to
any e). The Hi (written below without their argu-
ments) are real-valued feature functions that may
be overlapping and non-independent. For compu-
tational tractability, we assume that the feature func-
tions Hi decompose with the derivations of f? and e
in terms of local feature functions hi. We also de-
fineZ(e, f;?) to be the sum of the numerator over all
derivations that yield the sentence pair ?e, f?. Rather
than training purely to optimize conditional likeli-
hood, we also make use of a spherical Gaussian prior
on the value of ? with mean 0 and variance ?2,
which helps prevent overfitting of the model (Chen
and Rosenfeld, 1998). Our objective is thus to select
? minimizing:
L = ? log
?
?e,f?
p(e|f; ?)?
||?||2
2?2
= ?
?
?e,f?
[logZ(e, f; ?)? logZ(f; ?)]?
||?||2
2?2
The gradient of Lwith respect to the feature weights
has a parallel form; it is the difference in feature ex-
pectations under the reference distribution and the
translation distribution with a penalty term due to
the prior:
?L
??i
=
?
?e,f?
Ep(d,a|e,f;?)[hi]? Ep(e,d,a|f;?)[hi]?
?i
?2
The form of the objective and gradient are quite sim-
ilar to the traditional fully observed training scenario
for CRFs (Sha and Pereira, 2003). However, rather
than matching the feature expectations in the model
to an observable feature value, we have to sum over
the latent structure that remains after observing our
target e, which makes the form of the first summand
an expectation rather than just a feature function
value.
3.2.1 Computing the objective and gradient
The objective and gradient that were just introduced
can be computed in two steps. Given a training pair
?e, f?, we generate the forest of reorderings F from f
as described in ?2.1. We then compose this grammar
with T , the FST representing the translation model,
which yields F ?T , a translation forest that contains
all possible translations of f into the target language,
as described in ?2.2. Running the inside algorithm
on the translation forest computes Z(f; ?), the first
term in the objective, and the inside-outside algo-
rithm can be used to compute Ep(e,d,a|f)[hi]. Next,
to compute Z(e, f; ?) and the first expectation in the
gradient, we need to find the subset of the transla-
tion forest F ? T that exactly derives the reference
translation e. To do this, we again rely on the fact
that F ? T is a forest and therefore itself a context-
free grammar. So, we use this grammar to parse
the target reference string e. The resulting forest,
F ?T ?e, contains all and only derivations that yield
the pair ?e, f?. Here, the inside algorithm computes
Z(e, f; ?) and the inside-outside algorithm can be
used to compute Ep(e,d,a|f)[hi].
Once we have an objective and gradient, we can
apply any first-order numerical optimization tech-
nique.8 Although the conditional likelihood surface
of this model is non-convex (on account of the la-
tent variables), we did not find a significant initial-
ization effect. For the experiments below, we ini-
tialized ? = 0 and set ?2 = 1. Training generally
converged in fewer than 1500 function evaluations.
8For our experiments we used L-BFGS (Liu and Nocedal,
1989).
862
4 Experimental setup
We now turn to an experimental validation of the
models we have introduced. We define three con-
ditions: a small data scenario consisting of a trans-
lation task based on the BTEC Chinese-English cor-
pus (Takezawa et al, 2002), a large data Chinese-
English condition designed to be more comparable
to conditions in a NIST MT evaluation, and a large
data Arabic-English task.
For each condition, phrase tables were extracted
as described in Koehn et al (2003) with a maxi-
mum phrase size of 5. The parallel training data
was aligned using the Giza++ implementation of
IBM Model 4 (Och and Ney, 2003). The Chinese
text was segmented using a CRF-based word seg-
menter (Tseng et al, 2005). The Arabic text was
segmented using the technique described in Lee et
al. (2003). The Stanford parser was used to generate
source parses for all conditions, and these were then
used to generate the reordering forests as described
in ?2.1.
Table 1 summarizes statistics about the cor-
pora used. The reachability statistic indicates
what percentage of sentence pairs in the train-
ing data could be regenerated using our reorder-
ing/translation model.9 To train the reordering
model, we used all of the reachable sentence pairs
from BTEC, 20% of the reachable set in the
Chinese-English condition, and all reachable sen-
tence pairs under 40 words (source) in length in the
Arabic-English condition.
Error analysis indicates that a substantial portion
of unreachable sentence pairs are due to alignment
(word or sentence) or parse errors; however, in some
cases the reordering forests did not contain an ad-
equate source reordering to produce the necessary
target. For example, in Arabic, which is a VSO lan-
guage, the treebank annotation is to place the sub-
ject NP as the ?middle child? between the V and the
object constituent. This can be reordered into an En-
glish SVO order using our child-permutation rules;
however, if the source VP is modified by a modal
particle, the parser makes the particle the parent of
the VP, and it is no longer possible to move the sub-
ject to the first position in the sentence. Richer re-
ordering rules are needed to address this problem.
9Only sentences that can be generated by the model can be
used in training.
Other solutions to the reachability problem include
targeting reachable oracles instead of the reference
translation (Li and Khudanpur, 2009) or making use
of alternative training criteria, such as minimum risk
training (Li and Eisner, 2009).
4.1 Features
We briefly describe the feature functions we used
in our model. These include the typical dense fea-
tures used in translation: relative phrase translation
frequencies p(e|f) and p(f |e), ?lexically smoothed?
translation probabilities plex(e|f) and plex(f |e), and
a phrase count feature. For the reordering model, we
used a binary feature for each kind of rule used, for
example ?VP?V NP(a) would fire once for each time
the rule VP ? V NP was used in a derivation, a.
For the Arabic-English condition, we observed that
the parse trees tended to be quite flat, with many re-
peated non-terminal types in one rule, so we aug-
mented the non-terminal types with an index indi-
cating where they were located in the original parse
tree. This resulted in a total of 6.7k features for
IWSLT, 18k features for the large Chinese-English
condition, and 516k features for Arabic-English.10
A target language model was not used during the
training of the source reordering model, but it was
used during the translation experiments (see below).
4.2 Qualitative assessment of reordering model
Before looking at the translation results, we exam-
ine what the model learns during training. Figure 6
lists the 10 most highly weighted reordering features
learned by the BTEC model (above) and shows an
example reordering using this model (below), with
the most English-like reordering indicated with a
star.11 Keep in mind, we expect these features to
reflect what the best English-like order of the input
should be. All are almost surprisingly intuitive, but
this is not terribly surprising since Chinese and En-
glish have very similar large-scale structures (both
are head initial, both have adjectives and quanti-
fiers that precede nouns). However, we see two en-
tries in the list (starred) that correspond to an En-
10The large number of features in the Arabic system was due
to the relative flatness of the Arabic parse trees.
11The italicized symbols in the English gloss are functional
elements with no precise translation. Q is an interrogative parti-
cle, and DE marks a variety of attributive roles and is used here
as the head of a relative clause.
863
Table 1: Corpus statistics
Condition Sentences Source words Target words Reachability
BTEC 44k 0.33M 0.36M 81%
Chinese-English 400k 9.4M 10.9M 25%
Arabic-English 120k 3.3M 3.6M 66%
glish word order that is ungrammatical in Chinese:
PP modifiers in Chinese typically precede the VPs
they modify, and CPs (relative clauses) also typi-
cally precede the nouns they modify. In English, the
reverse is true, and we see that the model has indeed
learned to prefer this ordering. It was not necessary
that this be the case: since our model makes use
of phrases memorized from a non-reordered training
set, it could hav relied on those for all its reordering.
Yet these results provide evidence that it is learning
large-scale reordering successfully.
Feature ? note
VP? VE NP 0.995
VP? VV VP 0.939 modal + VP
VP? VV NP 0.895
VP? VP PP? 0.803 PP modifier of VP
VP? VV NP IP 0.763
PP? P NP 0.753
IP? NP VP PU 0.728 PU = punctuation
VP? VC NP 0.598
NP? DP NP 0.538
NP? NP CP? 0.537 rel. clauses follow
?   ?     ??           ?   ???  ??    ?   ?? ?  ?
 I    CAN  CATCH  [
NP
[
CP 
GO   HILTON  HOTEL  DE]   BUS]   Q   ? 
I  CAN  CATCH  [
NP
 BUS [
CP  
GO  HILTON  HOTEL  DE]]  Q  ? 
I  CAN  CATCH  [
NP
 BUS [
CP  
DE  GO  HILTON  HOTEL]]  Q  ? 
I  CAN  CATCH  [
NP
 BUS [
CP  
GO  HOTEL  HILTON  DE]]  Q  ? 
I  CAN  CATCH  [
NP
 BUS [
CP  
DE  GO  HOTEL  HILTON]]  Q  ? 
I  CATCH  [
NP
 BUS [
CP  
GO  HILTON  HOTEL  DE]]  CAN  Q  ? 
Input:
5-best reordering:
(Can I catch a bus that goes to the Hilton Hotel ?)
Figure 6: (Above) The 10 most highly-weighted features
in a Chinese-English reordering model. (Below) Exam-
ple reordering of a Chinese sentence (with English gloss,
translation, and partial syntactic information).
5 Translation experiments
We now consider how to apply this model to a trans-
lation task. The training we described in ?3.2 is
suboptimal for state-of-the-art translation systems,
since (1) it optimizes likelihood rather than an MT
metric and (2) it does not include a language model.
We describe how we addressed these problems here,
and then present our results in the three conditions
defined above.
5.1 Training for Viterbi decoding
A language model was incorporated using cube
pruning (Huang and Chiang, 2007), using a 200-
best limit at each node during LM integration. To
improve the ability of the phrase model to match
reordered phrases, we extracted the 1-best reorder-
ing of the training data under the learned reordering
model and generated the phrase translation model so
that it contained phrases from both the original order
and the 1-best reordering.
To be competitive with other state-of-the-art sys-
tems, we would like to use Och?s minimum error
training algorithm for training; however, we can-
not tune the model as described with it, since it has
far too many features. To address this, we con-
verted the coefficients on the reordering features into
a single reordering feature which then had a coef-
ficient assigned to it. This technique is similar to
what is done with logarithmic opinion pools, only
the learned model is not a probability distribution
(Smith et al, 2005). Once we collapsed the reorder-
ing weights into a single feature, we used the tech-
niques described by Kumar et al (2009) to optimize
the feature weights to maximize corpus BLEU on a
held-out development set.
5.2 Translation results
Scores on a held-out test set are reported in Table 2
using case-insensitive BLEU with 4 reference trans-
lations (16 for BTEC) using the original definition
of the brevity penalty. We report the results of our
864
model along with three baseline conditions, one with
no-reordering at all (mono), the performance of a
phrase-based translation model with distance-based
distortion, the performance of our implementation of
a hierarchical phrase-based translation model (Chi-
ang, 2007), and then our model.
Table 2: Translation results (BLEU)
Condition Mono PB Hiero Forest
BTEC 47.4 51.8 52.4 54.1
Chinese-Eng. 29.0 30.9 32.1 32.4
Arabic-Eng. 41.2 45.8 46.6 44.9
6 Related work
A variety of translation processes can be formalized
as the composition of a finite-state representation of
input (typically just a sentence, but often a more
complex structure, like a word lattice) with an SCFG
(Wu, 1997; Chiang, 2007; Zollmann and Venugopal,
2006). Like these, our work uses parsing algorithms
to perform the composition operation. But this is the
first time that the input to a finite-state transducer has
a context-free structure.12 Although not described
in terms of operations over formal languages, the
model of Yamada and Knight (2001) can be under-
stood as an instance of our class of models with a
specific input forest and phrases restricted to match
syntactic constituents.
In terms of formal similarity, Mi et al (2008) use
forests as input to a tree-to-string transducer pro-
cess, but the forests are used to recover from 1-
best parsing errors (as such, all derivations yield
the same source string). Iglesias et al (2009) use
a SCFG-based translation model, but implement it
using FSTs, although they use non-regular exten-
sions that make FSTs equivalent to recursive tran-
sition networks. Galley and Manning (2008) use
a context-free reordering model to score a phrase-
based (exponential) search space.
Syntax-based preprocessing approaches that have
relied on hand-written rules to restructure source
trees for particular translation tasks have been quite
widely used (Collins et al, 2005; Wang et al, 2007;
Xu et al, 2009; Chang et al, 2009). Discrimina-
tively trained reordering models have been exten-
sively explored. A widely used approach has been to
12Satta (submitted) discusses the theoretical possibility of
this sort of model but provides no experimental results.
use a classifier to predict the orientation of phrases
during decoding (Zens and Ney, 2006; Chang et al,
2009). These classifiers must be trained indepen-
dently from the translation model using training ex-
amples extracted from the training data. A more am-
bitious approach is described by Tromble and Eisner
(2009), who build a global reordering model that is
learned automatically from reordered training data.
The latent variable discriminative training ap-
proach we describe is similar to the one originally
proposed by Blunsom et al (2008).
7 Discussion and conclusion
We have described a new model of translation that
takes advantage of the strengths of context-free
modeling, but splits reordering and phrase transduc-
tion into two separate models. This lets the context-
free part handle what it does well, mid-to-long range
reordering, and lets the finite-state part handle lo-
cal phrasal correspondences. We have further shown
that the reordering component can be trained effec-
tively as a latent variable in a discriminative transla-
tion model using only conventional parallel training
data.
This model holds considerable promise for fu-
ture improvement. Not only does it already achieve
quite reasonable performance (performing particu-
larly well in Chinese-English, where mid-range re-
ordering is often required), but we have only begun
to scratch the surface in terms of the kinds of fea-
tures that can be included to predict reordering, as
well as the kinds of reordering forests used. Fur-
thermore, by reintroducing the concept of a cascade
of transducers into the context-free model space, it
should be possible to develop new and more effec-
tive rescoring mechanisms. Finally, unlike SCFG
and phrase-based models, our model does not im-
pose any distortion limits.
Acknowledgements
The authors gratefully acknowledge partial support from
the GALE program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001. Any
opinions, findings, conclusions or recommendations ex-
pressed in this paper are those of the authors and do not
necessarily reflect the views of the sponsors. Thanks
to Hendra Setiawan, Vlad Eidelman, Zhifei Li, Chris
Callison-Burch, Brian Dillon and the anonymous review-
ers for insightful comments.
865
References
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proceedings of ACL-HLT.
P.-C. Chang, D. Jurafsky, and C. D. Manning. 2009. Dis-
ambiguating ?DE? for Chinese-English machine trans-
lation,. In Proc. WMT.
S. F. Chen and R. Rosenfeld. 1998. A Gaussian prior
for smoothing maximum entropy models. Technical
Report TR-10-98, Computer Science Group, Harvard
University.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings of ACL 2005.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the Association for Com-
puting Machinery, 13(2):94?102.
M. Galley and C. D. Manning. 2008. A simple and ef-
fective hierarchical phrase reordering model. In Proc.
EMNLP.
D. Grune and C. J. H. Jacobs. 2008. Parsing as intersec-
tion. In D. Gries and F. B. Schneider, editors, Parsing
Techniques, pages 425?442. Springer, New York.
J. E. Hopcroft and J. D. Ullman. 1979. Introduc-
tion to Automata Theory, Languages and Computa-
tion. Addison-Wesley.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In ACL.
G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne.
2009. Hierarchical phrase-based translation with
weighted finite state transducers. In Proc. NAACL.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of NAACL, pages 48?54.
S. Kumar, Y. Deng, and W. Byrne. 2006. A weighted fi-
nite state transducer translation template model for sta-
tistical machine translation. Journal of Natural Lan-
guage Engineering, 12(1):35?75.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009.
Efficient minimum error rate training and minimum
Bayes-risk decoding for translation hypergraphs and
lattices. In Proc. ACL.
Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Has-
san. 2003. Language model based Arabic word seg-
mentation. In Proc. ACL.
Z. Li and J. Eisner. 2009. First- and second-order ex-
pectation semirings with applications to minimum-risk
training on translation forests. In Proc. EMNLP.
Z. Li and S. Khudanpur. 2009. Efficient extraction of
oracle-best translations from hypergraphs. In Proc.
NAACL.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical Programming B, 45(3):503?528.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-based transla-
tion. In Proceedings of ACL-08: HLT, pages 192?199,
Columbus, Ohio, June. Association for Computational
Linguistics.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
S. Petrov and D. Klein. 2008. Discriminative log-linear
grammars with latent variables. In Advances in Neu-
ral Information Processing Systems 20 (NIPS), pages
1153?1160.
G. Satta. submitted. Translation algorithms by means of
language intersection.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proceedings of HLT-NAACL,
pages 213?220.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In Proc.
ACL.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proceedings of LREC 2002,
pages 147?152, Las Palmas, Spain.
R. Tromble and J. Eisner. 2009. Learning linear or-
der problems for better translation. In Proceedings of
EMNLP 2009.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005. A conditional random field word seg-
menter. In Fourth SIGHAN Workshop on Chinese Lan-
guage Processing.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
Proc. EMNLP.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a
dependency parser to improve SMT for subject-object-
verb languages. In Proc. NAACL, pages 245?253.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In Proc. ACL.
R. Zens and H. Ney. 2006. Discriminative reordering
models for statistical machine translation. In Proc. of
the Workshop on SMT.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In Proc.
of the Workshop on SMT.
A. Zollmann, A. Venugopal, F. Och, and J. Ponte. 2008.
A systematic comparison of phrase-based, hierarchical
and syntax-augmented statistical MT. In Proc. Coling.
866
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 417?426,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Encouraging Consistent Translation Choices
Ferhan Ture,
1
Douglas W. Oard,
2,4
Philip Resnik
3,4
1
Department of Computer Science
2
College of Information Studies
3
Department of Linguistics
4
Institute for Advanced Computer Studies
University of Maryland, College Park, MD 20740 USA
fture@cs.umd.edu, oard@umd.edu, resnik@umd.edu
Abstract
It has long been observed that monolingual text
exhibits a tendency toward ?one sense per dis-
course,? and it has been argued that a related
?one translation per discourse? constraint is op-
erative in bilingual contexts as well. In this pa-
per, we introduce a novel method using forced
decoding to confirm the validity of this con-
straint, and we demonstrate that it can be ex-
ploited in order to improve machine translation
quality. Three ways of incorporating such a
preference into a hierarchical phrase-based MT
model are proposed, and the approach where all
three are combined yields the greatest improve-
ments  for  both  Arabic-English  and  Chinese-
English translation experiments.
1 Introduction
In statistical Machine Translation (MT), the state-of-
the-art approach is to translate phrases in the context
of a sentence and to re-order those phrases appro-
priately. Intuitively, it seems as if it should also be
possible to draw on information outside of a single
sentence to further improve translation quality. In
this paper, we challenge the conventional approach
of translating each sentence independently, and ar-
gue that it can indeed also be beneficial to consider
document-scale context when translating text. Mo-
tivated by the success of a ?one sense per discourse?
heuristic in Word Sense Disambiguation (WSD), we
explore the potential  benefit of leveraging a ?one
translation per discourse? heuristic in MT.
The paper is organized as follows. We begin with
related work in Section 2. Next, we provide new
confirmation that the hypothesized one-translation-
per-discourse  condition  does  indeed  often  hold,
based  on  a  novel  analysis  using  forced  decoding
(Section 3). We incorporate this idea into a hierarchi-
cal MT framework by adding three new document-
scale features to the translation model (Section 4).
We then present  experimental  results  demonstrat-
ing  solid  improvements  in  translation  quality  ob-
tained by leveraging these features, both for Arabic-
English (Ar-En) and Chinese-English (Zh-En) trans-
lation (Section 5). Conclusions and future work are
presented in Section 6.
2 Related work
Exploiting  discourse-level  context  has  to  date
received  only  limited  attention  in  MT re-
search (e.g., (Gime?nez  and  Ma`rquez, 2007; Liu
et al, 2010; Carpuat, 2009; Brown, 2008; Xiao et
al., 2011)). Exploratory analysis of reference trans-
lations by Carpuat  (2009)  motivates  a  hypothesis
that MT systems might benefit from the ?one sense
per discourse? heuristic, first introduced by Gale et
al. (1992), which has proven to be effective in the
context of WSD (Yarowsky, 1995). Carpuat?s ap-
proach was to do post-processing on the translation
output to impose a ?one translation per discourse?
constraint where the system would otherwise have
made a different choice. A manual evaluation on
a sample of sentences suggested promise from the
technique, which  Carpuat  suggested  in  favor  of
exploring more integrated approaches.
Xiao et al (2011) took this one step further and
implement an approach where they identified am-
biguous translations within each document, and at-
417
tempt to fix them by replacing each ambiguity with
the most frequent translation choice. Based on their
error analysis, the authors indicate two shortcomings
when trying to find the correct translation of a given
phrase. First, frequency may not provide sufficient
information to distinguish between translation can-
didates, which is why we take rareness into account
when scoring translation candidates. Another prob-
lem is, like any other heuristic, that there may be
cases where the heuristic fails and there are multi-
ple senses per discourse. Guaranteeing consistency
hurts performance in such situations, which is why
we implement the heuristic as a model feature, and
let the model score decide for each case.
We are aware of a few other analyses that have
shown promising results based on a similar motiva-
tion. For instance, Wasser and Dorr (2008)?s ap-
proach biases the MT system based on term statistics
from relevant documents in comparable corpora. Ma
et al (2011) show that a translation memory can be
used to find similar source sentences, and consecu-
tively adapt translation choices towards consistency.
Domain adaptation for MT has has also been shown
to be useful in some cases (Bertoldi and Federico,
2009; Hildebrand et al, 2005; Sanchis-Trilles and
Casacuberta, 2010; Tiedemann, 2010; Zhao et al,
2004), so to the extent we consider documents to be
micro-domains we might expect similar approaches
to be useful at document scale. Indeed, hints that
such ideas may work have been available for some
time. For example, there is clear evidence that the
behavior of human translators can provide evidence
that is often useful for automating WSD (Diab and
Resnik, 2002; Ng et al, 2003). When coupled with
the one-sense-per-discourse heuristic, this suggests
that the reverse may also be true.
3 Exploratory analysis
It is well known that writing styles vary by genre,
and in particular that the amount of vocabulary vari-
ation within a document depends to some extent on
the genre (e.g., higher in poetry than in engineering
writing). The degree to which authors tend to make
consistent word choices in any particular genre is,
therefore, an empirical question. In order to gain in-
sight into the extent to which human translators make
consistent vocabulary choices in the types of materi-
als that we wish to translate (in this work, news sto-
ries), we first explore the degree of support for our
one-translation-per-discourse hypothesis in the ref-
erence translations of a standard MT test collection.
We used the Ar-En MT08 data set, which con-
tains 74 newswire documents with a total  of 813
sentences, each of which has four reference trans-
lations. Throughout this paper we consistently use
the  document  (i.e., one  news  story)  as  a  conve-
nient discourse unit, although of course finer-scale or
broader-scale discourse units might also be explored
in future work. Moreover, throughout this paper we
use the hierarchical phrase-based translation system
(Hiero), which is based on a synchronous context-
free grammar (SCFG) model (Chiang, 2005). In a
SCFG, the rule [X] ||| ? ||| ? indicates that con-
text free expansion X ? ? in the source language
can occur synchronously with X ? ? in the target
language. In this case, we call ? the left hand side
(LHS) of the rule, and ? the right hand side (RHS)
of the rule.
To determine the extent and nature of translation
consistency choices made by human translators, we
randomly selected one of the four sets of reference
translations (first set, with id 0) and we used forced
decoding to find all possible sequences of rules that
could transform the source sentence into the target
sentence. In forced decoding, given a pair of source
and target sentences, and a grammar consisting of
learned translation rules with associated probabili-
ties, the decoder searches all possible derivations for
the one sequence of rules that is most likely (under
the learned translation model) to synchronously pro-
duce the source sentence on the LHS and the target
sentence on the RHS. For instance, consider the fol-
lowing Arabic sentence as input:
???? ??? ?????????? ??????? .
and its uncased reference translation:
there is a link between the three attacks .
The following four rules, which are part of the SCFG
learned from the the same translation pairs, allows
the decoder to find a sequence of derivations that
?translates? the source-side Arabic sentence into the
418
X16 
X7  ??? X12 
X3  ??????????  ????  ??????? .
R1 
R2 R3 
R4 
X16 
X7 between X12 
X3  the  there  attacks .  three  is  a  link 
R1 
R2 R3 
R4 
Figure 1: Illustration of forced decoding.
target-side reference translation.
1
R1. [X12] ||| ???? ||| there is a link
R2. [X16] ||| [2] ??? [1] ||| [X12, 1] between [X7, 2]
R3. [X7] ||| [1] ?????????? . ||| [X3, 1] attacks .
R4. [X3] ||| ??????? ||| the three
Figure 1 illustrates how the decoder uses these
rules  to  produce the source and target  sides  syn-
chronously.
As we repeated this  procedure  for  all  sentence
pairs, we kept track of all rules that were actually
used by the decoder to generate a reference English
translation from the corresponding Arabic sentences.
Our next step was to identify cases in which the
SCFG could reasonably have produced a substan-
tially  different  translation. Whenever  an  Arabic
phrase f occurs multiple times in a document, and f
appears on the LHS of two or more different gram-
mar rules in the SCFG, we count this as a single
?case?.
2
These cases correspond to unique (source
phrase f , document d) pairs in which a translation
process using that SCFG could have chosen to pro-
duce two or more different translations of f in d.
Since the multiple appearances of f are distributed
among sentences of d, each counted case may cor-
respond to a number of sentences ranging from 1 to
the number of sentences in that document.
Table 1 shows a small sample of the cases (i.e.,
(source phrase f , document d) pairs) identified as a
result of forced decoding. There were 321 such cases
in our dataset and there were 672 sentences in which
at least one case occurred. This is not an uncommon
phenomenon; these 672 sentences comprise 83% of
1
Since our goal was an exploratory analysis, the MT08 test
set was combined with the training set in order to ensure reach-
ability of the reference translations using the learned grammar.
Proper train/dev/test splits were, of course, used for the evalua-
tion results reported in Section 5.
2
We define a phrase as any text that constitutes the entire
LHS of a grammar rule.
the test set. However, many of these cases repre-
sent either unlikely choices or inconsequential dif-
ferences, so some post-processing is called for.
Since  grammar  rules  are  typically  more  fine-
grained than is necessary for our purposes (e.g., to
capture various punctuation and determiner differ-
ences that do not affect the ?sense? of the transla-
tion), we applied a few simple heuristics to edit the
source and target  sides and group all  such minor
variations into a single ?mega-rule? (e.g., ?how???,
how?, ?third???a third?, ?want???we want?). For
this, we removed nonterminal symbols and punc-
tuation, and  considered  two target  phrases e and
e? to  be different only  if edit distance(e, e?) >
max(length(e), length(e?))/2, where the edit dis-
tance is based on character removal and insertion.
For instance, the third example in Table 1 would
have been considered to be translated consistently
as a result of this heuristic, as opposed to the first
example. We also eliminated cases in which no rea-
sonable alternatives were available in the translation
grammar (i.e., cases where the second most probable
rule with the same LHS was assigned a probability
below 0.1 in the grammar). Cases 4 and 5 would
have been removed by this heuristic.
After this filtering and aggregation we were left
with 176 (f , d) pairs in which the translation model
could reasonably have selected between rules that
would have produced substantially different English
translations of f in d (such as cases 1?3 and 6?9).
It was these 176 cases, affecting a total of 512 sen-
tences (63% of test set) for which we then examined
what forced decoding could tell us about translation
consistency.
So now that we know what the human who pro-
duced the reference translations actually did (accord-
ing to forced decoding), and in which cases they
might reasonably have chosen to do something sub-
stantially different (according to the SCFG), we can
ask in which cases the human (effectively) made a
consistent choice of translation rules when encoun-
tering the same Arabic phrase in the same document.
In 128 of the 176 cases, that is what they did (i.e.,
when the same phrase occurred multiple times in a
single document and more than one translation was
reasonably possible, forced decoding indicated that
the human translator translated that phrase in essen-
tially the same way). These cases affected the trans-
419
Case
Translation counts
Source phrase Doc #
???? 566 that killed = 1
killing of = 1
??????? 782 hostages = 2
??????? 138 hostage = 1
hostages = 2
????? 466 korea = 2
????? 763 korea = 2
?? 30 from = 2
?? 7 of = 1
from = 1
?? ?????? 717 of the current = 2
???? 30 the = 1
which =1
Table 1: A sample of cases (i.e., (source phrase f , docu-
ment d) pairs) identified as a result of forced decoding.
lation of 455 sentences (56% of the test set), suggest-
ing that if we can replicate this human behavior in a
system, it might affect a nontrivial number of trans-
lation choices.
These statistics also suggest, however, that there
may be some risk incurred in such a process, since
in 48 of the 176 cases, the human translator opted
for a substantially different translation. When we
closely examined these 48 instances, we found that
19 (40%) involved changing a content-bearing word
(sometimes to a word with similar meaning). The re-
maining 29 (60%) involved function words or simi-
lar constructions. See Figures 2 and 3 for examples.
1a. [X] ||| ???? ||| had allowed
1b. [X] ||| ???? ||| has permitted
2a. [X] ||| [X,1] ???? ||| examining [X,1]
2b. [X] ||| [X,1] ???? ||| is considering [X,1]
3a. [X] ||| [X,1] ?????? ||| neighbors
3b. [X] ||| [X,1] ?????? ||| neighboring countries
Figure 2: Examples of differences in lexical choice for
content-bearing words within the same document.
We can make several observations based on this
analysis. First, there does indeed seem to be ev-
idence to support the one-translation-per-discourse
heuristic, and to suggest that respecting that heuris-
4a. [X] ||| ?? ||| on
4b. [X] ||| ?? ||| in
4c. [X] ||| ?? ||| ?s
5a. [X] ||| ?? ||| had
5b. [X] ||| ?? ||| was
Figure 3: Examples of differences in lexical choice for
other types of lexical units within the same document.
tic could improve translation outcomes for a substan-
tial number of sentences. Second, even when a ref-
erence translation contains different translations of
the same phrase, this may sometimes be the result of
stylistic choices rather than an intent by the transla-
tor to affect the expressed meaning. If a system were
try to ?fix? such cases by enforcing consistent trans-
lation, the resulting translation might be somewhat
more stilted, but perhaps not less accurate or less in-
telligible. Finally, sentence structure conventions or
other language-specific phenomena may sometimes
require the same phrase to be translated differently,
so some way of encouraging consistency while still
allowing the model to consider other contextual fac-
tors might be better than always imposing a hard con-
sistency constraint.
4 Approach
To incorporate document-level features into an MT
system  that  would  otherwise  operate  with  only
sentence-level  evidence, we  added  three  super-
sentential ?consistency features? to the translation
model. The decoder computes scores for these fea-
tures in two passes over each document; in each pass,
each sentence in the document is decoded. In the
first pass, the decoder keeps track of the number of
occurrences of some aspects of each grammar rule
and stores that information. The consistency fea-
tures are disabled during this pass, and do not affect
decoder scoring. In the second pass, each grammar
rule is assigned as many as three consistency feature
scores, each of which is based on some frozen counts
from the first pass. These features are designed to
introduce a bias towards translation consistency, but
to leave the final decision to the decoder, which of
course also has access to other  features from the
translation and language model. At this point we are
more interested in effectiveness than efficiency, so
420
we simply note that this approach doubles the run-
ning time of the decoder and that future work on a
more elegant implementation might be productive.
We explore three ways to compute features in this
section. The essential idea behind all of them is to
define some feature function that increases monoton-
ically with an increase in some count that we believe
to be informative, and in which the rate of increase is
damped more strongly as that count increases. Sev-
eral feature functions could satisfy those broad re-
quirements; in this section, we describe three vari-
ants, C1, C2 and C3, and discuss the potential bene-
fits and drawbacks of each.
C1: Counting rules In this variant, we count in-
stances of the same entire grammar rule, where a rule
r contains both the source phrase f and the target
phrase e. During the first pass, whenever a grammar
rule is chosen by the decoder for the one-best output,
the count for that rule is incremented. Given a gram-
mar rule r and the number of times r was counted in
the first pass (given by N{r}), the consistency fea-
ture score is computed as follows:
C1(r) =
2.2N{r}
1.2 + N{r}
(1)
Equation 1 is the term frequency component of the
well known Okapi BM25 term weighting function,
when parameters are set to the conventional values
k = 1.2, b = 0. This is an increasing and con-
cave function in which the count has a diminishing
marginal effect on the feature score. It has proven
to be useful in information retrieval applications, in
which the goal is to model ?aboutness? based on term
counts (Robertson et al, 1994). Because our goal is
to demonstrate the potential of consistency features,
it seemed reasonable to work with some simple func-
tion that has a shape like the one we desired. We
leave exploration of optimal damping functions for
future work.
A drawback of this C1 approach is that as we saw
in Section 3, grammar rules in phrase-based MT sys-
tems tend to be somewhat more fine-grained than
seems optimal for constructing a consistency fea-
ture. For instance, consider the following rules that
all translate the same Arabic term:
R1. [X] ||| [X,1] ????? ||| [X,1] the bodies
R2. [X] ||| [X,1] ????? ||| [X,1] the organs
R3. [X] ||| [X,1] ????? ||| [X,1] organs
R4. [X] ||| ????? [X,1] ||| the organs of [X,1]
R5. [X] ||| ????? [X,1] ||| [X,1] bodies
Based on these grammar rules, we as human read-
ers infer that this Arabic phrase can be translated in
two different ways: as organs or as bodies. An opti-
mal application of the one-translation-per-discourse
heuristic would thus group the rules based on the
presence of one of those words. However, in the C1
variant, each of these rules would be counted sepa-
rately because of differences that in some cases do
not directly affect the choice of content words. For
instance, on the source side, the Arabic token ap-
pears to the right of the nonterminal symbol in R1,
R2 andR3, while it is to the left of the nonterminal in
R4 andR5. On the target side, differences are due to
both nonterminal symbol position and the existence
of determiners. Motivated by many examples like
this, we came up with an alternative way of count-
ing rules.
C2: Counting target tokens To partially address
this sparseness issue, variant C2 focuses only on the
target side. We extract all target tokens whenever a
grammar rule is used by the decoder in a one-best
derivation and increment a counter for each. Since
we are mainly interested in content words (e.g. bod-
ies, organs), we use simple pattern matching to dis-
card nonterminal symbols and punctuation, and we
ignore terms that appear in more than 50% of all doc-
uments (a convenient way of discarding common to-
kens such as the, or, and). This approach separates
the rules in the example above into two groups: rules
with bodies on the target side and rules with organs
on the target side. Upon completion of the first pass,
the consistency feature score for rule r is then de-
termined by first computing a score for each unique
target-side token w using:
bm25(w) = 2.2N{w}
1.2 + N{w}
log
D + 1
DF (w) + 0.5
(2)
where in this caseN{w}maps tokens to their respec-
tive counts in the document, D is the total number
of documents in the collection, and DF (document
frequency) is the number of documents in which the
token occurs. This is a fuller version of the BM25
function in which (in the information retrieval ap-
plication) both high term frequencies and rare terms
421
are rewarded. We then set the feature score for each
rule r to the maximum score of any of its target-side
terminal tokens:
C2(r) = max
e?RHS(r)
bm25(e) (3)
Our motivation for choosing the maximum is that
when there is more than one content word that sur-
vives the pruning of common terms, we want the
score to be influenced most strongly by the most im-
portant of those terms. Since BM25 term weights
can be thought of as a measure of term importance,
taking the maximum is a simple expedient.
Although counting only target-side tokens yields
coarser granularity than counting rules, ignoring the
source side of the rule risks combining target side
statistics from translations of unrelated source lan-
guage terms. Consider the following grammar rule:
R6. [X] ||| <s> [X,1] ????? ||| <s> [X,1] life support
Since the counter for life and support will both be
incremented whenever rule R6 fires in the one-best
decoding during the first pass, problems could arise
if a rule with a different LHS that also contains sup-
port on the RHS were to fire in the same document,
for example:
R7. [X] ||| ?????? ||| support
If we don?t take the source side into account, both oc-
currences of support will be grouped together when
counting and R7 will receive extra score from the
consistency feature whenever R6 is used by the de-
coder. Of course, this problem will only arise when
the LHS of R6 and R7 are present in the same doc-
ument, and how often that happens (and thus how
large the risk from this factor is) is an empirical ques-
tion. We therefore developed a third alternative as a
middle ground between the fine-grained C1 and the
coarse-grained C2.
C3: Counting  token  translation  pairs In  this
variant, we count each terminal (source token, tar-
get token) pair that survives pruning. Specifically,
if grammar rule [X]|||f1f2...fm|||e1e2...en fires, we
increment the count of every pair ?fi, ej?, where fi
is aligned to ej . After the first pass, we compute the
feature value of each observed pair, based on this
count and the DF of the target-side of the pair. We
chose to use only the target token in the DF com-
putation (i.e., aggregating over all source tokens) to
reduce sparsity effects. Similar to C2, the feature of
a rule r is defined by the maximum of scores of all
pairs extracted from r.
C3(r) = max
f?LHS(r)
e?RHS(r)
?f,e? aligned
bm25(?f, e?) (4)
Since each variant has its benefits and drawbacks, we
can include all three in the system and let the tuning
process decide on how each should be weighted.
5 Evaluation and Discussion
We have evaluated the one-translation-per-discourse
feature using the cdecMT system (Dyer et al, 2010).
We started by building a baseline system using stan-
dard features in cdec: lexical and phrase transla-
tion probabilities in both directions, word and arity
penalty features, and a 5-gram language model. We
then added each of the three consistency feature vari-
ants, along with all two-way and the one three-way
combinations of them, thus yielding a total of eight
systems for comparison, including the baseline.
For training the Ar-En system, we used the dataset
from the DARPA GALE evaluation (Olive et  al.,
2011), which consists of NIST and LDC releases.
The corpus was filtered to remove sentence pairs
with  anomalous  length  ratios  and  subsampled  to
yield a training set containing 3.4 million parallel
sentence pairs. The Arabic text was preprocessed to
produce two different segmentations (simple punctu-
ation tokenization with orthographic normalization,
and LDC?s ATBv3 representation (Maamouri et al,
2008)), represented together using cdec?s lattice in-
put format (Dyer et al, 2008).
The Zh-En system was trained on parallel train-
ing text consisting of the non-UN portions and non-
HK Hansards portions of the NIST training corpora.
Chinese was automatically segmented by the Stan-
ford segmenter (Tseng et al, 2005), and traditional
characters were simplified. After subsampling and
filtering, we obtain a training corpus of 1.6 million
parallel sentences.
Both  training  sets  were  word-aligned  with
GIZA++ (Och and Ney, 2003), using 5 Model  1
and  5  HMM iterations. A SCFG was  then  ex-
tracted from these alignments using a suffix array
extractor (Chiang, 2007). Evaluation was done with
multi-reference BLEU (Papineni et al, 2002) on test
422
sets with four references for each language pair, and
MIRA was used for tuning (Crammer et al, 2006).
In our experiments, we run the first decoding phase
using feature weights that are guessed heuristically
based on weights from previously tuned systems.
All feature weights, including the discourse feature,
were then tuned together, based on the output  of
the  second  decoding  phase. For  Ar-En  parame-
ter  tuning, we  used  the  MT06 newswire  dataset,
which contains 104 documents and a total of 1,797
sentences. For testing, we used the MT08 dataset
described  above  (74  documents, 813  sentences).
For Zh-En experiments, the MT02 newswire dataset
(100 documents, 878 sentences) was used for tuning,
and evaluation was done on the MT06 test set (79
documents, 1,664 sentences). For  both language
pairs, DF values were computed from the tuning
set for both tuning and evaluation experiments.
When we used NIST?s official metric (BLEU-4)
to compare our results to the official NIST evalu-
ation (NIST, 2006; NIST, 2008), our baseline sys-
tem achieved 54.70 for  Ar-En and 31.69 for  Zh-
En. Based on reported NIST results, our baseline
would have ranked 4
th
in the Zh-En MT06 evalua-
tion, and would have outperformed all Ar-En MT08
systems. We used a slightly different IBM-BLEU
metric for the rest of our evaluation. In this case,
the baseline system achieved 53.07 BLEU points
for  Ar-En  and  30.43  points  for  Zh-En. Among
more recent papers, the best reported results were
56.87  for  Ar-En  MT08 (Zhao  et  al., 2011a)  and
35.87 for Zh-En MT06 (Zhao et al, 2011b), although
many papers report BLEU scores below 53 points
for Arabic (Carpuat et al, 2011) and 32 points for
Chinese (Monz, 2011). The systems that outper-
formed our baseline applied novel techniques, and
used larger language models, as well as many non-
standard features. We argue that these novelties are
complementary to our approach, and therefore do not
damage the credibility of our baseline.
Among the single-feature runs, C3 had the best
performance  in  Ar-En  experiments, with  53.84
BLEU points, whereas C2 yielded the best results
for Zh-En with a BLEU score of 30.96. In any case,
all three variants outperformed the baseline (see Ta-
ble 2). When multiple features were combined, we
generally observed an increase in BLEU, suggesting
that our features have usefully different error char-
Method BLEU
Ar-En Zh-En
Baseline 53.07 30.43
C1 53.82 30.59
C2 53.70 30.96
C3 53.84 30.54
C12 53.82 30.79
C13 53.82 30.76
C23 53.88 30.63
C123 53.98 31.42
Table 2: Evaluation results: BLEU scores with four ref-
erences for Ar-En and Zh-En experiments.
Method # documents
Ar-En Zh-En
Docs 74 79
C1 37 30
C2 37 35
C3 42 36
C123 43 41
Table 3: Doc-level analysis: Number of documents where
each variant outperforms baseline.
acteristics. The combination of all three variants,
C123, yielded the best results, nearly 1.0 BLEU point
higher  than  the  baseline  for  both  language  pairs.
Evaluation results are summarized in Table 2.
Given our focus on documents, it  is  natural  to
ask  what  fraction  of  the  documents  were  helped
or  harmed  by  consistency  features. Document-
level  BLEU scores  for  Arabic-to-English  transla-
tions show that C3 outperformed the baseline on a
larger number of documents than any other single
feature (42/74=57%), compared with 37/74 (50%)
for both C1 and C2. C123 did better by this measure
as well, with BLEU increasing for 43 of the docu-
ments. There were no documents where the BLEU
score  was  exactly  the  same, therefore  the  BLEU
score declined for the remaining documents. As Ta-
ble 3 indicates, document-level BLEU for the Zh-En
experiments shows similar results.
We can also look at our results in a more fine-
grained way, focusing on differences in how each
system translated the same source-language phrase.
For  this  analysis, we  defined  English  phrases e
and e? to  be different if edit distance(e, e?) >
423
Method Ar-En Zh-En
Cases Test set Cases Test set
C1 77 24% 401 48%
C2 127 35% 686 60%
C3 101 33% 491 53%
Any 197 68% 968 94%
C123 141 41% 651 59%
Table 4: Effect of applying variants of the consistency
feature (Any=C1 or C2 or C3).
max(length(e), length(e?))/2. By  this  way  of
counting, there are 197 unique (Arabic phrase, docu-
ment) pairs for which at least one single-feature sys-
tem produced translations differently from the base-
line system. Together, these cases affect 553 sen-
tences (68%) in 67 of the 74 documents, with as
many as 12 differences observed in a single doc-
ument. The  number  of  such  differences  is  even
higher for Chinese-to-English translation, probably
due to lower confidence from the translation model
and longer documents. Table 4 shows the number of
changes by each system, and the percentage of the
test set affected by these changes.
In order to gain greater insight into the effect of
the consistency features, we randomly sampled 60
of the 197 cases and analyzed the influence of the
change to the document BLEU score. In 25 of the
sampled cases, at least one of the three systems made
a change that improved the BLEU score, whereas the
score was adversely affected for at least one system
in 13 cases. BLEU remained unchanged in the re-
maining 22 cases, mostly due to the use of multi-
ple reference translations. When we analyze the ef-
fect of each system separately, we see that C2 was
the most aggressive, making 25 changes that influ-
enced BLEU (16 positive, 9 negative). C1 was the
most conservative, with only 13 such changes (8 pos-
itive, 5 negative). Consistent with the overall BLEU
scores, C3 evidenced the best ratio between benefit
and harm, making 20 changes that affected the score
(16 positive, 4 negative).
Looking at specific cases can yield some insight
into how the consistency features achieve improve-
ments. For example, results improved when trans-
lating the phrase ???????, (Eng. organizational,
regulatory), which appears in the context of organi-
zational groups that support terrorist ideology. The
baseline system translated this as organizational in
one case, and regulatory in another. Variants C1
and C2 changed this behavior, so that the translation
was organizational in both cases. One of the refer-
ence translations used organizational in one case and
dropped the phrase in the other, and the other three
translators  provided  consistent  translations  (using
organized and organizational). As a result, applying
the one-translation-per-discourse heuristic improved
the multi-reference BLEU score.
On the other hand, here is one of the cases where
our  feature  hurt  performance. The  phrase ??
?? (Eng. border/frontier troops/guards) appears
in two sentences of a Chinese news story about vio-
lence along the India - Nepal border. All reference
translations consistently used the word border in the
translation, as it is a better choice in this context.
The baseline system translated the phrase as fron-
tier guards and border troops in the two sentences.
All system variants replaced border with frontier to
maintain consistency, and therefore produced worse
translations, causing a decrease in BLEU score.
Examples can, however, also point up limitations
in our ability to measure improvements. In one of
the test documents, the Arabic phrase ?????? ???
(Eng. sneak, infiltrate, enter without approval) ap-
pears in the context of Turkey trying to enter the Eu-
ropean Union. This was translated by the baseline
system as sneak into in one occurrence and infiltrate
into in another. C1 didn?t change the output, but
C2 and C3 translated the phrase as infiltrate into in
both cases. Although all of the four reference trans-
lators were consistent within their choices, each of
them chose different translations, namely worm its
way, enter, sneak and sneak into. This resulted in
a decrease in BLEU score for the two systems that
chose infiltrate into. This case illustrates a limita-
tion to fine-grained use of BLEU alone as a basis
for analysis, since we might argue that infiltrate into
is no less appropriate than sneak into in this con-
text. In other words, some of the reductions we see
in BLEU may not be actual errors but rather sim-
ply changes that take us outside of the coverage of
the test set. We did not find any cases in our sample
in which improvements in BLEU seemed to reward
changes that adversely affected meaning. From this,
424
we conclude that BLEU is a somewhat conservative
measure when used in this way, and that the actual
overall improvement in translation quality over our
baseline may be somewhat more than our roughly
1.0 measured BLEU improvement would suggest.
6 Conclusions and Future Work
In this paper, we started with a new way of look-
ing at, and largely supporting, the ?one translation
per discourse? hypothesis using forced decoding of
human reference translations. We then leveraged
insights  from that  analysis  to  design  the  transla-
tion model consistency features, obtaining solid im-
provements for both Ar-En and Zh-En translation.
In future work, we plan to explore additional vari-
ants. For example, we can further address sparsity by
incorporating monolingual paraphrase detection on
the source side, the target side or both. We can and
should explore other monotonically increasing con-
cave feature functions in addition to the Okapi BM25
function that we have found to be useful in this work,
we should explore alternatives to our use of the max-
imum function in C2 and C3, and we should con-
sider optimizing to measures other than BLEU (e.g.,
METEOR) that extend the range of rewarded lexical
choices by leveraging monolingual paraphrase evi-
dence.
In designing our features we were guided by our
intuition about which kinds of consistency should be
rewarded. Data can be superior to intuition, how-
ever, and our forced decoding technique might also
be helpful in generating new insights that could help
to guide the design of even more useful features. For
example, our forced decoding clearly points to cases
in which translators have chosen different structural
variants when translating the same phrase, and closer
examination of these cases might help us to automat-
ically detect which kinds of structural variation can
most profitably be moderated using a consistency
feature. We should also note that we have only done
forced decoding to date in one language pair (Ar-
En), and there might be more to be learned about
language-specific issues from doing the same anal-
ysis for additional language pairs.
Finally, the time seems propitious to reconsider
our choice of document-scale as our discourse con-
text. Documents have much to recommend them, but
much of the content that we might wish to translate
(conversational speech, text chat, email threads, . . . )
doesn?t present the kinds of obvious and unambigu-
ous document boundaries that we find in MT test
collections that are built from news stories. More-
over, some documents (e.g., textbooks) may be too
diverse for an entire document to be the right scale
for consistency. We might also be able to produc-
tively group similar documents into clusters in which
the vocabulary choices are (or should be) mutually
reinforcing.
We therefore  end where  we began, with  many
questions to be answered. Now, however, we have
somewhat different questions ? not whether to en-
courage consistency at a super-sentential scale, but
rather when and how best to do that.
Acknowledgements
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the view of DARPA.
References
Nicola  Bertoldi  and  Marcello  Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation (StatMT
?09), pages 182?189.
Ralf D. Brown. 2008. Exploiting document-level context
for data-driven machine translation. In Proceedings of
the the Eighth Conference of the Association for Ma-
chine Translation in the Americas (AMTA ?08).
Marine Carpuat, Yuval Marton, and Nizar Habash. 2011.
Improved Arabic-to-English statistical machine trans-
lation  by  reordering  post-verbal  subjects  for  word
alignment. Machine Translation, pages 1?16.
Marine Carpuat. 2009. One translation per discourse. In
Proceedings of the Workshop on Semantic Evaluations:
Recent Achievements and Future Directions, DEW ?09,
pages 19?27.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL ?05.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
425
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL ?02.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing Word Lattice Translation. In Pro-
ceedings of ACL-HLT?08, pages 1012?1020, June.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitke-
vitch, Phil Blunsom, and Philip Resnik. 2010. cdec: a
decoder, alignment, and learning framework for finite-
state and context-free translation models. In ACLDe-
mos ?10, pages 7?12.
William A.  Gale, Kenneth W.  Church, and  David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, HLT ?91, pages 233?237.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Context-aware
discriminative phrase selection for statistical machine
translation. In Proceedings  of  StatMT ?07, pages
159?166.
AS Hildebrand, M Eck, S Vogel, and Alex Waibel. 2005.
Adaptation of the translation model for statistical ma-
chine translation based on information retrieval. In
Proceedings of The European Association for Machine
Translation (EAMT ?05).
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 2010.
Improving statistical machine translation with mono-
lingual collocation. In ACL ?10.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent translation using discrimina-
tive learning: a translation memory-inspired approach.
In Proceedings of ACL-HLT?11, pages 1239?1248.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhancing the Arabic Treebank: A Collaborative Ef-
fort toward New Annotation Guidelines. In LREC ?08.
Christof Monz. 2011. Statistical Machine Translation
with Local Language Models. In EMNLP ?11.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
an empirical study. In ACL ?03.
NIST. 2006. http://www.itl.nist.gov/iad/mig/tests/mt/2006/.
NIST. 2008. http://www.itl.nist.gov/iad/mig/tests/mt/2008/.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
Joseph Olive, Caitlin  Christianson, and John McCary.
2011. Handbook  of  Natural  Language  Processing
and Machine Translation: DARPAGlobal Autonomous
Language  Exploitation. Springer  Publishing  Com-
pany, Inc., 1st edition.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02.
Stephen E. Robertson, Steve Walker, Susan Jones, Miche-
line  Hancock-Beaulieu, and  Mike  Gatford. 1994.
Okapi at TREC-3. In TREC.
Germa?n  Sanchis-Trilles  and  Francisco  Casacuberta.
2010. Bayesian  adaptation  for  statistical  machine
translation. In Proceedings of the workshop on Struc-
tural and Syntactic Pattern Recognition (SSPR ?10),
pages 620?629.
Jo?rg Tiedemann. 2010. Context adaptation in statistical
machine translation using models with exponentially
decaying cache. In Proceedings of the workshop on
Domain Adaptation for Natural Language Processing
(DANLP ?10), pages 8?15.
Huihsin Tseng, Pi-Chuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional  random  field  word  segmenter. In Fourth
SIGHAN Workshop on Chinese Language Processing.
Michael M.  Wasser  and  Bonnie  Dorr. 2008. Ma-
chine  translation  with  cross-lingual  information  re-
trieval based document relevance scores. Unpublished.
Tong Xiao, Jingbo Zhu, Shujie  Yao, and Hao Zhang.
2011. Document-level consistency verification in ma-
chine translation. InMachine Translation Summit XIII
(MTS?11).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In ACL ?95.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Lan-
guage model adaptation for statistical machine transla-
tion with structured query models. In COLING ?04.
Bing Zhao, Young-Suk Lee, Xiaoqiang Luo, and Liu Li.
2011a. Learning to transform and select elementary
trees for improved syntax-based machine translations.
In ACL-HLT ?11, pages 846?855.
Yinggong Zhao, Yangsheng Ji, Ning Xi, Shujian Huang,
and Jiajun Chen. 2011b. Language model weight
adaptation based on cross-entropy for statistical ma-
chine translation. In Pacific Asia Conference on Lan-
guage, Information and Computation (PACLIC ?11).
426
Proceedings of NAACL-HLT 2013, pages 540?549,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Modeling Syntactic and Semantic Structures in Hierarchical Phrase-based
Translation
Junhui Li
University of Maryland
College Park, USA
lijunhui@umiacs.umd.edu
Philip Resnik
University of Maryland
College Park, USA
resnik@umd.edu
Hal Daume? III
University of Maryland
College Park, USA
hal@umiacs.umd.edu
Abstract
Incorporating semantic structure into a
linguistics-free translation model is chal-
lenging, since semantic structures are
closely tied to syntax. In this paper, we
propose a two-level approach to exploiting
predicate-argument structure reordering in a
hierarchical phrase-based translation model.
First, we introduce linguistically motivated
constraints into a hierarchical model, guiding
translation phrase choices in favor of those
that respect syntactic boundaries. Second,
based on such translation phrases, we propose
a predicate-argument structure reordering
model that predicts reordering not only
between an argument and its predicate, but
also between two arguments. Experiments on
Chinese-to-English translation demonstrate
that both advances significantly improve
translation accuracy.
1 Introduction
Hierarchical phrase-based (HPB) translation mod-
els (Chiang, 2005; Chiang, 2007) that utilize syn-
chronous context free grammars (SCFG) have been
widely adopted in statistical machine translation
(SMT). Although formally syntactic, such models
rarely respect linguistically-motivated syntax, and
have no formal notion of semantics. As a re-
sult, they tend to produce translations containing
both grammatical errors and semantic role confu-
sions. Our goal is to take advantage of syntactic
and semantic parsing to improve translation qual-
ity of HPB translation models. Rather than intro-
ducing semantic structure into the HPB model di-
rectly, we construct an improved translation model
by incorporating linguistically motivated syntactic
constraints into a standard HPB model. Once the
translation phrases are linguistically constrained, we
are able to propose a predicate-argument reorder-
ing model. This reordering model aims to solve
two problems: ensure that arguments are ordered
properly after translation, and to ensure that the
proper argument structures even exist, for instance
in the case of PRO-drop languages. Experimental
results on Chinese-to-English translation show that
both the hard syntactic constraints and the predicate-
argument reordering model obtain significant im-
provements over the syntactically and semantically
uninformed baseline.
In principle, semantic frames (or, more specifi-
cally, predicate-argument structures: PAS) seem to
be a promising avenue for translational modeling.
While languages might diverge syntactically, they
are less likely to diverge semantically. This has
previously been recognized by Fung et al (2006),
who report that approximately 84% of semantic
role mappings remained consistent across transla-
tions between English and Chinese. Subsequently,
Zhuang and Zong (2010) took advantage of this
consistency to jointly model semantic frames on
Chinese/English bitexts, yielding improved frame
recognition accuracy on both languages.
While there has been some encouraging work on
integrating syntactic knowledge into Chiang?s HPB
model, modeling semantic structure in a linguisti-
cally naive translation model is a challenge, because
the semantic structures themselves are syntactically
motivated. In previous work, Liu and Gildea (2010)
model the reordering/deletion of source-side seman-
tic roles in a tree-to-string translation model. While
it is natural to include semantic structures in a tree-
based translation model, the effect of semantic struc-
tures is presumably limited, since tree templates
themselves have already encoded semantics to some
540
extent. For example, template (VP (VBG giving)
NP#1 NP#2) entails NP#1 as receiver and NP#2 as
thing given. Xiong et al (2012) model the reorder-
ing between predicates and their arguments by as-
suming arguments are translated as a unit. However,
they only considered the reordering between argu-
ments and their predicates.
2 Syntactic Constraints for HPB
Translation Model
In this section, we briefly review the HPB model,
then present our approach to incorporating syntactic
constraints into it.
2.1 HPB Translation Model
In HPB models, synchronous rules take the form
X ? ??, ?,??, where X is the non-terminal sym-
bol, ? and ? are strings of lexical items and non-
terminals in the source and target side, respectively,
and ? indicates the one-to-one correspondence be-
tween non-terminals in ? and ?. Each such rule
is associated with a set of translation model fea-
tures {?i}, including phrase translation probabil-
ity p (? | ?) and its inverse p (? | ?), the lexical
translation probability plex (? | ?) and its inverse
plex (? | ?), and a rule penalty that affects prefer-
ence for longer or shorter derivations. Two other
widely used features are a target language model
feature and a target word penalty.
Given a derivation d, its translation probability is
estimated as:
P (d) ?
?
i
?i (d)
?i
(1)
where ?i is the corresponding weight of feature ?i.
See (Chiang, 2007) for more details.
2.2 Syntactic Constraints
Translation rules in an HPB model are extracted
from initial phrase pairs, which must include at least
one word inside one phrase aligned to a word inside
the other, such that no word inside one phrase can
be aligned to a word outside the other phrase. It
is not surprising to observe that initial phrases fre-
quently are non-intuitive and inconsistent with lin-
guistic constituents, because they are based only on
statistical word alignments. Nothing in the frame-
work actually requires linguistic knowledge.
Koehn et al (2003) conjectured that such non-
intuitive phrases do not help in translation. They
tested this conjecture by restricting phrases to syn-
tactically motivated constituents on both the source
and target side: only those initial phrase pairs are
subtrees in the derivations produced by the model.
However, their phrase-based translation experiments
(on Europarl data) showed the restriction to syn-
tactic constituents is actually harmful, because too
many phrases are eliminated. The idea of hard syn-
tactic constraints then seems essentially to have been
abandoned: it doesn?t appear in later work.
On the face of it, there are many possible rea-
sons Koehn et al (2003)?s hard constraints did not
work, including, for example, tight restrictions that
unavoidably exclude useful phrases, and practical is-
sues like the quality of parse trees. Although en-
suing work moved in the direction of soft syntactic
constraints (see Section 6), our ultimate goal of cap-
turing predicate-argument structure requires linguis-
tically valid syntactic constituents, and therefore we
revisit the idea of hard constraints, avoiding prob-
lems with their strictness by relaxing them in three
ways.
First, requiring source phrases to be subtrees in
a linguistically informed syntactic parse eliminates
many reasonable phrases. Consider the English-
Chinese phrase pair ?the red car, hongse de qiche?.1
It is easily to get a translation entry for the whole
phrase pair. By contrast, the phrase pair ?the red,
hongse de? is typically excluded because it does not
correspond to a complete subtree on the source side.
Yet translating the red is likely to be more useful
than translating the red car, since it is more general:
it can be followed by any other noun translation. To
this end, we relax the syntactic constraints by allow-
ing phrases on the source side corresponding to ei-
ther one subtree or sibling subtrees with a common
parent node in the syntactic parse. For example, the
red in Figure 1(a) is allowed since it spans two sub-
trees that have a common parent node NP.
Second, we might still exclude useful phrases be-
cause the syntactic parses of some languages, like
Chinese, prefer deep trees, resulting in a head and
its modifiers being distributed across multiple struc-
tural levels. Consider the English sentence I still
1We use English as source language for better readability.
541
like the red car very much and its syntactic structure
as shown in Figure 1(a). Phrases I still, still like,
I still like are not allowed, since they don?t map to
either a subtree or sibling subtrees. Logically, how-
ever, it might make sense not just to include phrases
mapping to (sibling) subtrees, but to include phrases
mapping to subtrees with the same head. To this end,
we flatten the syntactic parse so that a head and all its
modifiers appear at the same level. Another advan-
tage of this flattened structure is that flattened trees
are more reliable than unflattened ones, in the sense
that some bracketing errors in unflattened trees can
be eliminated during tree flattening. Figure 1(b) il-
lustrates flattening a syntactic parse by moving the
head (like) and all its modifiers (I, still, the red car,
and very much) to the same level.
Third, initial phrase pair extraction in Chiang?s
HPB generates a very large number of rules, which
makes training and decoding very slow. To avoid
this, a widely used strategy is to limit initial phrases
to a reasonable length on either side during rule ex-
traction (e.g., 10 in Chiang (2007)). A correspond-
ing constraint to speed up decoding prohibits any X
from spanning a substring longer than a fixed length,
often the same as the maximum phrase length in rule
extraction. Although the initial phrase length limita-
tion mainly keeps non-intuitive phrases out, it also
closes the door on some useful phrases. For ex-
ample, a translation rule ?I still like X, wo rengran
xihuan X? will be prohibited if the non-terminal X
covers 8 or more words. In contrast, our hard con-
straints have already filtered out dominating non-
intuitive phrases; thus there is more room to include
additional useful phrases. As a result, we can switch
off the constraints on initial phrase length in both
training and decoding.
2.3 Reorderable Glue Rules
In decoding, if no good rule (e.g., a rule whose left-
hand side is X) can be applied or the length of the
potential source span is larger than a pre-defined
length, a glue rule (either S ? ?X1, X1? or S ?
?S1X2, S1X2?) will be used to simply stitch two
consequent translated phrases together in monotonic
way. This will obviously prevent some reasonable
translation derivations because in certain cases, the
order of phrases may be inverted on the target side.
Moreover, even that the syntactic constraints dis-
a. Word alignment for an English-Chinese sentence pair 
with the parse tree for the English sentence 
I 
ADVP
like the red car very much
NP 
still 
VBP NP ADVP
S
b. Flattened parse tree for the English sentence
S
?  ??   ??  ??  ?? ???
wo rengran feichang xihua hongse de qiche 
VP
VP 
I 
ADVP 
like the red car very much
NP 
still 
VBP NP ADVP
Figure 1: Example of flattening parse tree.
cussed above make translation nodeXs are syntacti-
cally informed, stitching translated phrases from left
to right will unavoidably generate non-syntactically
informed node Ss. For example, the combination of
X (like) and X (the) does not make much sense in
linguistic perspective.
Alternatively, we replace glue rules of HPB with
reorderable ones:
? T ? ?X1, X1?
? T ? ?T1T2, T1T2?
? T ? ?T1T2, T2T1?
where the second (third) rule combines two trans-
lated phrases in a monotonic (inverted) way. Specif-
ically, we set the translation probability of the first
translation rule as 1 while estimating the probabil-
ities of the other two rules from training data. In
both training and decoding, we require the phrases
covered by T to satisfy our syntactic constraints.
Therefore, all translation nodes (both Xs and T s)
in derivations are syntactically informed, providing
room to explore PAS reordering in HPB model.
3 PAS Reordering Model
Ideally, we aim to model PAS reordering based on
the true semantic roles of both the source and tar-
get side, as to better cater not only consistence but
542
I 
AM-TMP 
like the red car very much
A0 
still 
VBP A1 AM-MNR
?  ??   ??  ??  ?? ? ??
wo rengran feichang xihua hongse de qiche
a. Word alignment for an English-Chinese sentence 
pair with semantic roles for the English sentence 
PAS-S 
AM-TMP 2A0 1  VBP3 A1 4  AM-MNR 5  
PAS-T 
X 2  X 1  X 5  X 3  X 4  
b. PAS-S and PAS-T for predicate like  
Figure 2: Example of PAS on both the source and target
side. Items are aligned by indices.
divergence between semantic frames of the source
and target language. However, considering there is
no efficient way of jointly performing MT and SRL,
accurate SRL on target side can only be done after
translation. Similar to related work (Liu and Gildea,
2010; Xiong et al, 2012), we obtain the PAS of
the source language (PAS-S) via a shallow seman-
tic parser and project the PAS of the target language
(PAS-T) using the word alignment derived from the
translation process. Specifically, we use PropBank
standard (Palmer et al, 2005; Xue, 2008) which de-
fines a set of numbered core arguments (i.e., A0-A5)
and adjunct-like arguments (e.g., AM-TMP for tem-
poral, AM-MNR for manner). Figure 2(b) shows
an example of PAS projection from source language
to target language.2 The PAS reordering model de-
scribes the probability of reordering PAS-S into PAS-
T. Given a predicate p, it takes the following form:
P (PAS-T | PAS-S, PRE=p) (2)
Note that cases for untranslated roles can be natu-
rally reflected in our PAS reordering model. For ex-
ample, if the argument IA0 is untranslated in Figure
2, its PAS-T will be X2X5X3X4.
2In PAS-S, we use parts-of-speech (POS) of predicates to
distinguish different types of verbs since the semantic structures
of Chinese adjective verbs are different from those of others.
3.1 Probability Estimation
While it is hard and unnecessary to translate a pred-
icate and all its associated arguments with one rule,
especially if the sentence is long, a practicable way,
as most decoders do, is to translate them in multi-
ple level rules. In addition, some adjunct-like argu-
ments are optional, or structurally dispensable part
of a sentence, which may result in data sparsity is-
sue. Based on these observations, we decompose
Formula 2 into two parts: predicate-argument re-
ordering and argument-argument reordering.
Predicate-Argument Reordering estimates the
reordering probability between a predicate and one
of its arguments. Taking predicate like and its argu-
ment A1 the red car in Figure 2(a) as an example,
the predicate-argument pattern on the source side
(PA-S) is VBP1 A12 while the predicate-argument
pattern on the target side (PA-T) is X1X2. The re-
ordering probability is estimated as:
PP-A (PA-T=X1 X2 | PA-S=VBP1 A12, PRE=like) =
Count (PA-T=X1 X2, PA-S=VBP1 A12, PRE=like)
?
T ??(PA-S) Count (PA-T=T , PA-S=VBP1 A12, PRE=like)
(3)
where ? (PA-S) enumerates all possible reorder-
ings on the target side. Moreover, we take the pred-
icate lexicon of predicate into account. To avoid
data sparsity, we set a threshold (e.g., 100) to re-
tain frequent predicates. For infrequent predicates,
their probabilities are smoothed by replacing predi-
cate lexicon with its POS. Finally, if source side pat-
terns are infrequent (e.g., less than 10) for frequent
predicates, their probabilities are smoothed as well
with the same way.
Argument-Argument Reordering estimates the
reordering probability between two arguments, i.e.,
argument-argument pattern on the source side (AA-
S) and its counterpart on the target side (AA-T).
However, due to that arguments are driven and piv-
oted by their predicates, we also include predicate
in patterns of AA-S and AA-T. Let?s revisit Fig-
ure 2(a). A1 the red car and AM-MNR very much
are inverted on the target side, whose probability is
estimated as:
PA-A (AA-T=X3 X1 X2 | AA-S=VBP1 A12 AM-MNR3, PRE=like)
(4)
Similarly we smooth the probabilities by distin-
guishing frequent predicates from infrequent ones,
543
as well as frequent patterns from infrequent ones.
3.2 Integrating the PAS Reordering Model into
the HPB Model
We integrate the PAS reordering model into the HPB
SMT by adding a new feature into the log-linear
translation model. Unlike the conventional phrase
and lexical translation features whose values are
phrase pair-determined and thus can be calculated
offline, the value of the PAS reordering model can
only be obtained with being aware of the predicate-
argument structures a hypothesis may cover. Before
we present the algorithm of integrating the PAS re-
ordering model, we define a few functions by assum-
ing p for a predicate, a for an argument, and H for a
hypothesis:
? A (i, j, p): returns arguments of p which are
fully located within the span from word i to j
on the source side. For example, in Figure 2,
A (4, 8, like) = {A1, AM -MRN}.3
? B (i, j, p): returns true if p is located within [i, j];
otherwise returns false.
? C (a, p): returns true if predicate-argument reorder-
ing for a and p has not calculated yet; otherwise re-
turns false.
? D (a1, a2, p): returns true if argument-argument
reordering for p?s arguments a1 and a2 has not cal-
culated yet; otherwise returns false.
? PP -A (H, a, p): according to Eq. 3, returns the
probability of predicate-argument reordering of a
and p, given a and p are covered by H . The po-
sitional relation of a and p on the target side can be
detected according to translation derivation of H .
? PA-A (H, a1, a2, p): according to Eq. 4, returns
the probability of argument-argument reordering of
p?s arguments a1 and a2, given a1, a2 and p are cov-
ered by H .
Algorithm 1 integrates the PAS reordering model
into a CKY-style decoder whenever a new hypothe-
sis is generated. Given a hypothesis H , it first looks
for predicates and their arguments which are covered
3The hard constraints make sure a valid source text span
would never fully cover some roles while partially cover other
roles. For example, phrases like the red, the read car very in
Figure 1 are invalid.
Algorithm 1: Integrating the PAS reordering
model into a CKY-style decoder
Input: Sentence f in the source language
Predicate-Argument Structures of f
Hypothesis H spanning from word i to j
Output: Log-Probability of the PAS reordering
model
1. set prob = 0.0
2. for predicate p in f , such that B (i, j, p) is true
3. ARG = A (i, j, p)
4. for a ? ARG such that C (a, p) is true
5. prob+= logPP -A (H, a, p)
6. for a1, a2 ? ARG such that a1 6= a2 and
D (a1, a2, p) is true
7. prob+= logPA-A (H, a1, a2, p)
8. return prob
by H (line 2-3). Then it respectively calculates the
probabilities of predicate-argument reordering and
argument-argument reordering(line 4-7).
4 Experiments
We have presented our two-level approach to in-
corporating syntactic and semantic structures in a
HPB system. In this section, we test the effect of
such structural information on a Chinese-to-English
translation task. The baseline system is a reproduc-
tion of Chiang?s (2007) HPB system. The bilin-
gual training data contains 1.5M sentence pairs with
39.4M Chinese words and 46.6M English words.4
We obtain the word alignments by running GIZA++
(Och and Ney, 2000) on the corpus in both direc-
tions and applying ?grow-diag-final-and? refinement
(Koehn et al, 2003). We use the SRI language mod-
eling toolkit to train a 5-gram language model on the
Xinhua portion of the Gigaword corpus and standard
MERT (Och, 2003) to tune the feature weights on
the development data.
To obtain syntactic parse trees for instantiating
syntactic constraints and predicate-argument struc-
tures for integrating the PAS reordering model, we
first parse the source sentences with the Berkeley
Parser (Petrov and Klein, 2007) trained on Chinese
TreeBank 6.0 and then ran the Chinese semantic role
4This dataset includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06
544
System MT 02 MT 04 MT 05 Ave.
max-phrase-length=10
max-char-span=10
base HPB 40.00 35.33 32.97 36.10
+ basic constraints + unflattened tree 33.90 32.00 29.83 31.91
+ our constraints + unflattened tree 38.47 34.51 32.15 35.04
+ our constraints + flattened tree 38.55 35.38 32.44 35.46
max-phrase-length=?
max-char-span=?
+ basic constraints + unflattened tree 35.38 32.89 30.42 32.90
+ our constraints + unflattened tree 39.41 36.02 33.21 36.21
+ our constraints + flattened tree 40.01 36.24 33.65 36.71
Table 1: Effects of hard constraints. Here max-phrase-length is for maximum initial phrase length in training and
max-char-span for maximum phrase length can be covered by non-terminal X in decoding.
labeler (Li et al, 2010) on all source parse trees to
annotate semantic roles for all verbal predicates.
We use the 2003 NIST MT evaluation test data
(919 sentence pairs) as the development data, and
the 2002, 2004 and 2005 NIST MT evaluation
test data (878, 1788 and 1082 sentence pairs, re-
spectively) as the test data. For evaluation, the
NIST BLEU script (version 11b) is used to calcu-
late the NIST BLEU scores, which measures case-
insensitive matching of n-grams with n up to 4. To
test whether a performance difference is statistically
significant, we conduct significance tests following
the paired bootstrapping approach (Koehn, 2004).
4.1 Effects of Syntactic Constraints
We have also tested syntactic constraints that simply
require phrases on the source side to map to a sub-
tree (called basic constraints). Similar to requiring
initial phrases on the source side to satisfy the con-
straints in training process, we only perform chart
parsing on text spans which satisfy the constraints
in decoding process. Table 1 shows the results of
applying syntactic constraints with different experi-
mental settings. From the table, we have the follow-
ing observations.
? Consistent with the conclusion in Koehn et
al. (2003), using the basic constraints is harmful to
HPB. Fortunately, our constraints consistently work
better than the basic constraints.
? Relaxing maximum phrase length in training and
maximum char span length in decoding, we obtain
an average improvement of about 1.0?1.2 BLEU
points for systems with both basic constraints and
our constraints. It is worth noting that after re-
laxing the lengths, the system with our constraints
performs on a par with the base HPB system (e.g.,
36.21 vs. 36.10).
System MT 02 MT 04 MT 05 Ave.
base HPB 40.00 35.33 32.97 36.10
+our constraints 40.01 36.24++ 33.65+ 36.71
with reorderable
glue rules
40.70+ 36.00+ 33.67+ 36.79
+PAS model 40.41+ 36.73++?? 34.24++? 37.13
Table 2: Effects of reorderable glue rules and the PAS
reordering model. +/++: significant over base HPB at
0.05/0.01; */**: significant over the system with reorder-
able glue rules at 0.05/0.01.
? Flattening parse trees further improves 0.4?0.5
BLEU points on average for systems with our syn-
tactic constraints. Our final system with constraints
outperforms the base HPB system with an average
of 0.6 BLEU points improvement (36.71 vs. 36.10).
Another advantage of applying syntactic constraints
is efficiency. By comparing the base HPB system
and the system with our syntactic constraints (i.e.,
the last row in Table 1), it is not surprising to ob-
serve that the size of rules extracted from training
data drops sharply from 193M in base HPB sys-
tem to 60M in the other. Moreover, the system
with constraints needs less decoding time than base
HPB does. Observation on 2002 NIST MT test data
(26 words per sentence on average) shows that basic
HPB system needs to fill 239 cells per sentence on
average in chart parsing while the other only needs
to fill 108 cells.
4.2 Effects of Reorderable Glue Rules
Based on the system with our syntactic constraints
and relaxed phrase lengths in training and decoding,
we replace traditional glue rules with reorderable
glue rules. Table 2 shows the results, from which
we find that the effect of reorderable glue rules is
elusive: surprisingly, it achieves 0.7 BLEU points
545
sentence length 1-10 11-20 21-30 31-40 41+ all
sentence count 337 1001 1052 768 590 3748
base HPB 32.21 37.51 36.71 34.96 35.00 35.73
+our constraints 31.70 37.57 37.10 36.20++ 35.78++ 36.39++
Table 3: Experimental results over different sentence
length on the three test sets. +/++: significant over base
HPB at 0.05/0.01.
improvement on NIST MT 2002 test set while hav-
ing negligible or even slightly negative impact on the
other two test sets. The reason of reorderable glue
rules having limited influence on translation results
over monotonic only glue rules may be due to that
the monotonic reordering overwhelms the inverted
one: estimated from training data, the probability of
the monotonic glue rule is 95.5%.
4.3 Effects of the PAS Reordering Model
Based on the system with reorderable glue rules, we
examine whether the PAS reordering model is capa-
ble of improving translation performance. The last
row in Table 2 presents the results . It shows the sys-
tem with the PAS reordering model obtains an aver-
age of 0.34 BLEU points over the system without it
(e.g., 37.13 vs. 36.79). It is interesting to note that it
achieves significant improvement on NIST MT 2004
and 2005 test sets (p < 0.05) while slightly lowering
performance on NIST MT 2002 test set (p > 0.05):
the surprising improvement of applying reorderable
glue rules on NIST MT 2002 test set leaves less
room for further improvement. Finally, it shows we
obtain an average improvement of 1.03 BLEU points
on the three test sets over the base HPB system.
5 Discussion and Future Work
The results in Table 1 demonstrate that significant
and sometimes substantial gains over baseline can
be obtained by incorporating hard syntactic con-
straints into the HPB model. Due to the capability of
translation phrases of arbitrary length, we conjecture
that the improvement of our system over the baseline
HPB system mostly comes from long sentences. To
test the conjecture, we combine all test sentences in
the three test sets and group them in terms of sen-
tence length. Table 3 presents the sentence distri-
bution and BLEU scores over different length. The
results validate our assumption that the system with
constraints outperforms the base systems on long
sentences (e.g., sentences with 20+ words).
Figure 3 displays a translation example which
shows the difference between the base HPB
system and the system with constraints. The
inappropriate translation of the base HPB
system can be mainly blamed on the rule
?
X[2,5] ? ?2??3X[4,5], X[4,5] the development of
?
,
where ?2 ??3 , a part of the subtree [0, 3] span-
ning from word 0 to 3, is translated immediately
to the right of X[2,5], making a direct impact that
subtree [0, 3] is translated discontinuously on the
target side. On the contrary, we can see that our
constraints are able to help select appropriate phrase
segments with respect to its syntactic structure.
Although our syntactic constraints apply on the
source side, they are completely ignorant of syn-
tax on the target side, which might result in ex-
cluding some useful translation rules. Let?s re-
visit the sentence in Figure 3, where we can see
that a transition rule spanning from word 0 to 5,
say
?
X[0,5] ? X[0,3]?4???5, X[0,3] depends on
?
is intuitive: the syntactic structure on the target side
satisfies the constraints, although that of the source
side doesn?t. One natural extension of this work,
therefore, would be to relax the constraints by in-
cluding translation rules whose syntactic structure
of either the source side or the target side satisfies
the constraints.
To illustrate how the PAS reordering model im-
pacts translation output, Figure 4 displays two trans-
lation examples of systems with or without it. The
predicate ??/convey in the first example has three
core arguments, i.e., A0, A2, and A1. The difference
between the two outputs is the reordering of A1 and
A2 while the PAS reordering model gives priority to
pattern VV A1 A2. In the second example, we clearly
observe two serious translation errors in the system
without PAS reordering model: ??/themA1 is un-
translated; ??/chinaA0 is moved to the immediate
right of predicate ??/allow and plays as direct ob-
ject.
Including the PAS reordering model improves the
BLEU scores. One further direction to refine the ap-
proach is to alleviate verb sparsity via verb classes.
Another direction is to include useful context in es-
timating reordering probability. For example, the
content of a temporal argument AM-TMP can be a
546
( ( ( ( ??   ?? )   ?)    ?? )   ?   ( ???   ( ( ( ??    ?? )   ? )   ??) )  ?)
    0      1      2      3     4       5        6        7     8      9      10 
X [0,1] : lot X [4,5] : depends on
X [2,5] : X [4,5]  the development of
X [6,9] : the devet. of the world sit. X [10,10] : . 
X [7,7] : sit.
X [6,7] : world X [7,7]
X [ 9,9] : devet.X [0,1] : lot 
X [0,3] : X [0,1]  development 
X [5,9] : depends on X [9,9]  of the X [6,7]  
X [0,10] : X [0,3]  X [5,9] .  
Figure 3: A translation example of the base HPB system (above) and the system with constraints (below).
 
[ ?] A0  [ ?] AM-ADVP  [ ? ?] A2  [ ??] PRE  ?? [ ?? ?? ? ??] A1   A0 1   AM-ADVP 2   A2 3   VV4   A1 5  
[south korean] [will] [deliver] hope [resume talks message] [to the dprk]       X 1   X 2   X 4   X 5   X 3  
[korean] [will] [convey] [to the] hope of [resuming talks information]         X 1   X 2   X 4   X 3   X 5  
Source 
w/o 
with 
Ref. south korean conveys its desire to resume talking with north korean                  ---- 
[ ??] A0  [ ???] AM-TMP  [ ??] PRE  [ ??] A1  [ ?? ?? ??] A2  ?       A0 1  AM-TMP 2  VV3  A1 4  A2 5  
[china] [friday] [allowed] [them] [to seoul th rough the philippines] .           X 1   X 2   X 3   X 4   X 5  
[friday] [allowed] [china] [to seoul through the philippines] .                  X 2   X 3   X 1   X 5  
Source 
w/o 
with 
Ref. in friday, china allowed them to travel to seoul through philippines .                  ---- 
Figure 4: Two translation examples of the system with/without PAS reordering model
short/simple phrase (e.g., friday) or a long/complex
one (e.g., when I was 20 years old), which has im-
pact on its reordering in translation.
6 Related Work
While there has been substantial work on linguis-
tically motivated SMT, we limit ourselves here
to several approaches that leverage syntactic con-
straints yet still allow cross-constituent transla-
tions. In terms of tree-based SMT with cross-
constituent translations, Cowan et al (2006) al-
lowed non-constituent sub phrases on the source
side and adopted phrase-based translation model for
modifiers in clauses. Marcu (2006) and Galley
et al (2006) inserted artificial constituent nodes in
parsing tree as to capture useful but non-constituent
phrases. The parse tree binarization approach
(Wang et al, 2007; Marcu, 2007) and the forest-
based approach (Mi et al, 2008) would also cover
non-constituent phrases to some extent. Shen et
al. (2010) defined well-formed dependency struc-
ture to cover uncompleted dependency structure in
translation rules. In addition to the fact that the
constraints of Shen et al (2010) and this paper
are based on different syntactic perspectives (i.e.,
dependency structure vs. constituency structure),
the major difference is that in this work we don?t
limit the length of phrases to a fixed maximum size
(e.g., 10 in Hiero). Consequently, we obtain some
translation rules that are not found in Hiero sys-
tems constrained by the length. In terms of (hi-
erarchical) phrase-based SMT with syntactic con-
straints, particular related to constituent boundaries,
Koehn et al (2003) tested constraints allowing con-
stituent matched phrases only. Chiang (2005) and
Cherry (2008) used a soft constraint to award or pe-
nalize hypotheses which respect or violate syntactic
boundaries. Marton and Resnik (2008) further ex-
plored the idea of soft constraints by distinguishing
among constituent types. Xiong et al (2009; 2010)
presented models that learn phrase boundaries from
aligned dataset.
On the other hand, semantics motivated SMT has
also seen an increase in activity recently. Wu and
547
Fung (2009) re-ordered arguments on the target side
translation output, seeking to maximize the cross-
lingual match of the semantic frames of the re-
ordered translation to that of the source sentence.
Liu and Gildea (2010) added two types of semantic
role features into a tree-to-string translation model.
Although Xiong et al (2012) and our work are both
focusing on source side PAS reordering, our model
differs from theirs in two main aspects: 1) we con-
sider reordering not only between an argument and
its predicate, but also between two arguments; and
2) our reordering model can naturally model cases
of untranslated arguments or predicates.
7 Conclusion
In this paper, we have presented an approach to
incorporating syntactic and semantic structures for
the HPB translation model. To accommodate the
close tie of semantic structures to syntax, we first
revisited the idea of hard syntactic constraints, and
we demonstrated that hard constraints can, in fact,
lead to significant improvement in translation qual-
ity when applied to Chiang?s HPB framework. Then
our PAS reordering model, thanks to the constraints
which guided translation phrases in favor of syntac-
tic boundaries, made further improvements by pre-
dicting reordering not only between an argument
and its predicate, but also between two arguments.
In the future work, we will extend the PAS reorder-
ing model to include useful context, e.g., the head
words and the syntactic categories of arguments.
Acknowledgments
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the view of DARPA. The
authors would like to thank three anonymous re-
viewers for providing helpful suggestions, and also
acknowledge Ke Wu and other CLIP labmates in
MT group for useful discussions. We also thank
creators of the valuable off-the-shelf NLP packages,
such as GIZA++ and Berkeley Parser.
References
Colin Cherry. 2008. Cohesive phrase-based decoding
for statistical machine translation. In Proceedings of
ACL-HLT 2008, pages 72?80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree transla-
tion. In Proceedings of EMNLP 2006, pages 232?241.
Pascale Fung, Zhaojun Wu, Yongsheng Yang, and Dekai
Wu. 2006. Automatic learning of Chinese-English
semantic structure mapping. In Proceedings of SLT
2006, pages 230?233.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of ACL-COLING 2006, pages 961?968.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL 2003, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of Chinese. In
Proceedings of ACL 2010, pages 1108?1117.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Proceedings of COL-
ING 2010, pages 716?724.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of EMNLP 2006, pages 44?52.
Steve DeNeefe; Kevin Knight; Wei Wang; Daniel Marcu.
2007. What can syntax-based mt learn from phrase-
based mt? In Proceedings of EMNLP-CoNLL 2007,
pages 755?763.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT 2008, pages 1003?1011.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-HLT 2008,
pages 192?199.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440?447.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
548
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL-
HLT 2007, pages 404?411.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine translation.
Computational Linguistics, 36(4):649?671.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In Proceedings of EMNLP-
CoNLL 2007, pages 746?754.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of
NAACL-HLT 2009, pages 13?16.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009.
A syntax-driven bracketing model for phrase-based
translation. In Proceedings of ACL-IJCNLP 2009,
pages 315?323.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding.
In Proceedings of NAACL-HLT 2010, pages 136?144.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Model-
ing the translation of predicate-argument structure for
smt. In Proceedings of ACL 2012, pages 902?911.
Nianwen Xue. 2008. Automatic labeling of semantic
roles. Computational Linguistics, 34(4):225?255.
Tao Zhuang and Chengqing Zong. 2010. Joint inference
for bilingual semantic role labeling. In Proceedings of
EMNLP 2010, pages 304?314.
549
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 36?39,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
Argviz: Interactive Visualization of Topic Dynamics in Multi-party
Conversations
Viet-An Nguyen
Dept. of Comp. Science
and UMIACS
University of Maryland
College Park, MD
vietan@cs.umd.edu
Yuening Hu
Dept. of Comp. Science
and UMIACS
University of Maryland
College Park, MD
ynhu@cs.umd.edu
Jordan Boyd-Graber
iSchool and
UMIACS
University of Maryland
College Park, MD
jbg@umiacs.umd.edu
Philip Resnik
Department of Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
We introduce an efficient, interactive
framework?Argviz?for experts to analyze
the dynamic topical structure of multi-party
conversations. Users inject their needs,
expertise, and insights into models via iterative
topic refinement. The refined topics feed into a
segmentation model, whose outputs are shown
to users via multiple coordinated views.
1 Introduction
Uncovering the structure of conversations often re-
quires close reading by a human expert to be effective.
Political debates are an interesting example: political
scientists carefully analyze what gets said in debates
to explore how candidates shape the debate?s agenda
and frame issues or how answers subtly (or not so
subtly) shift the conversation by dodging the question
that was asked (Rogers and Norton, 2011).
Computational methods can contribute to the
analysis of topical dynamics, for example through
topic segmentation, dividing a conversation into
smaller, topically coherent segments (Purver, 2011);
or through identifying and summarizing the topics
under discussion (Blei et al, 2003; Blei, 2012). How-
ever, the topics uncovered by such methods can be
difficult for people to interpret (Chang et al, 2009),
and previous visualization frameworks for topic
models?e.g., ParallelTopics (Dou et al, 2011), Top-
icViz (Eisenstein et al, 2012), the Topical Guide,1 or
topic model visualization (Chaney and Blei, 2012)?
are not particularly well suited for linearly structured
conversations.
This paper describes Argviz, an integrated, inter-
active system for analyzing the topical dynamics of
1http://tg.byu.edu/
multi-party conversations. We bring together previ-
ous work on Interactive Topic Modeling (ITM) (Hu
et al, 2011), which allows users to efficiently inject
their needs, expertise, and insights into model build-
ing via iterative topic refinement, with Speaker Iden-
tity for Topic Segmentation (SITS) (Nguyen et al,
2012), a state-of-the-art model for topic segmenta-
tion and discovery of topic shifts in conversations.
Argviz?s interface allows users to quickly grasp the
topical flow of the conversation, discern when the
topic changes and by whom, and interactively visual-
ize the conversation?s details on demand.
2 System Overview
Our overall system consists of three steps: (1) data
preprocessing, (2) interactive topic modeling, and (3)
conversational topic segmentation and visualization.
Data preprocessing Preprocessing creates bags of
words that can be used by models. First, stopwords
and low frequency terms are removed from tokenized
text. This is then used as the data for topic modeling.
Interactive topic modeling The topic model-
ing process then discovers?through posterior
inference?the topics that best explain the conver-
sational turns. Each of the topics is a multinomial
distribution over words, which can be displayed to
users along with the association of turns (documents)
to these topics.
The result of topic modeling may be imperfect;
we give users an opportunity to refine and curate the
topics using Interactive Topic Modeling (ITM) (Hu
et al, 2011). The feedback from users is encoded
in the form of correlations: word types that should
co-occur in a topic or which should not. As these
correlations are incorporated into the model, the top-
ics learned by the model change and are presented
36
again to the user. The process repeats over multiple
iterations until the user is satisfied.
In addition, a simple but important part of the
interactive user experience is the ability for users to
label topics, i.e., to identify a ?congress? topic that
includes ?bill?, ?vote?, ?representative?, etc.
ITM is a web-based application with a HTML and
jQuery2 front end, connected via Ajax and JSON.
Topic segmentation After the user has built inter-
pretable topics, we use SITS?a hierarchical topic
model (Nguyen et al, 2012)?to jointly discover the
set of topics discussed in a given set of conversations
and how these topics change during each conversa-
tion. We use the output of ITM to initialize SITS3
with a high quality user-specific set of topics. The
outputs of SITS consist of (1) a set of topics, (2) a
distribution over topics for each turn, and (3) a proba-
bility associated with each turn indicating how likely
the topic of that turn has been shifted.
The outputs of SITS are displayed using Argviz
(Figure 2). Argviz is a web-based application, built
using Google Web Toolkit (GWT),4 which allows
users to visualize and manipulate SITS?s outputs en-
tirely in their browser after a single server request.
3 Argviz: Coordinated Conversational
Views
Given the limited screen of a web browser, Argviz
follows the multiple coordinated views approach
(Wang Baldonado et al, 2000; North and Shneider-
man, 2000) successfully used in Spotfire (Ahlberg,
1996), Improvise (Weaver, 2004), and SocialAc-
tion (Perer and Shneiderman, 2006). Argviz supports
three main coordinated views: transcript, overview
and topic.
Transcript occupies the prime real estate for a
close reading. It has a transcript panel and a speaker
panel. The transcript panel displays the original tran-
script. Each conversational turn is numbered and
color-coded by speaker. The color associated with
each speaker can be customized using the speaker
panel, which lists all the speakers.
2 http://jquery.com/
3Through per-word topic assignments
4 https://developers.google.com/web-toolkit/
Overview shows how topics gain and lose promi-
nence during the conversation. SITS?s outputs in-
clude a topic distribution and a topic shift probability
for each turn in the conversation. In Argviz, these are
represented using a heatmap and topic shift column.
In the heatmap, each turn-specific topic distribu-
tion is displayed by a heatmap row (Sopan et al,
2013). There is a cell for each topic, and the color
intensity of each cell is proportional to the probability
of the corresponding topic of a particular turn. Thus,
users can see the topical flow of the conversation
through the vertical change in cells? color intensities
as the conversation progresses. In addition, the topic
shift column shows the topic shift probability (in-
ferred by SITS) using color-coded bar charts, helping
users discern large topic changes in the conversation.
Each row is associated with a turn in the conversation;
clicking on one shifts the transcript view.
Topic displays the set of topics learned by SITS
(primed by ITM), with font-size proportional to the
words? topic probabilities. The selected topic panel
goes into more detail, with bar charts showing the
topic-word distribution. For example, in Figure 2, the
Foreign Affairs topic in panel E has high probability
words ?iraq?, ?afghanistan?, ?war?, etc. in panel F.
4 Demo: Detecting 2008 Debate Dodges
Visitors will have the opportunity to experiment with
the process of analyzing the topical dynamics of dif-
ferent multi-party conversations. Multiple datasets
will be preprocessed and set up for users to choose
and analyze. Examples of datasets that will be avail-
able include conversation transcripts from CNN?s
Crossfire program and debates from the 2008 and
2012 U.S. presidential campaigns. For this section,
we focus on examples from the 2008 campaign.
Interactive topic refinement After selecting a
dataset and a number of topics, the first thing a user
can do is to label topics. This will be used later in
Argviz and helps users build a mental model of what
the topics are. For instance, the user may rename the
second topic ?Foreign Policy?.
After inspecting the ?Foreign Policy? topic, the
user may notice the omission of Iran from the most
probable words in the topic. A user can remedy that
by adding the words ?Iran? and ?Iranians? into the
37
Figure 1: ITM user interface for refining a topic. Users can iteratively put words into different ?bins?, label topics, and
add new words to the topic. Users can also click on the provided links to show related turns for each topic in context.
Figure 2: The Argviz user interface consists of speaker panel (A), transcript panel (B), heatmap (C), topic shift column
(D), topic cloud panel (E), selected topic panel (F).
38
important words bin (Figure 1). Other bins include
ignored words for words that should be removed (e.g.,
?thing? and ?work? from this topic) from the topic
and trash (e.g., ?don?, which is a stop word).
The user can commit these changes by pressing the
Save changes button. The back end relearns given
the user?s feedback. Once users are satisfied with
the topic quality, they can click on the Finish button
to stop updating topics and start running the SITS
model, initialized using the final set of refined topics.
Visual analytic of conversations After SITS fin-
ishes (which takes just a few moments), users see the
dataset?s conversations in the Argviz interface. Fig-
ure 2 shows Argviz displaying the 2008 vice presiden-
tial debate between Senator Joe Biden and Governor
Sarah Palin, moderated by Gwen Ifill.
Users can start exploring the interface from any
of the views described in Section 3 to gain insight
about the conversation. For example, a user may
be interested in seeing how the ?Economy? is dis-
cussed in the debates. Clicking on a topic in the topic
cloud panel highlights that column in the heatmap.
The user can now see where the ?Economy? topic
is discussed in the debate. Next to the heatmap, the
topic shift column when debate participants changed
the topic. The red bar in turn 48 shows an interac-
tion where Governor Palin dodged a question on the
?bankruptcy bill? to discuss her ?record on energy?.
Clicking on this turn shows the interaction in the
transcript view, allowing a closer reading.
Users might also want to contrast the topics that
were discussed before and after the shift. This can
be easily done with the coordination between the
heatmap and the topic cloud panel. Clicking on a
cell in the heatmap will select the corresponding
topic to display in the selected topic panel. In our
example, the topic of the conversation was shifted
from ?Economy? to ?Energy? at turn 48.
5 Conclusion
Argviz is an efficient, interactive framework that al-
lows experts to analyze the dynamic topical structure
of multi-party conversations. We are engaged in col-
laborations with domain experts in political science
exploring the application of this framework to politi-
cal debates, and collaborators in social psychology
exploring the analysis of intra- and inter-cultural ne-
gotiation dialogues.
References
[Ahlberg, 1996] Ahlberg, C. (1996). Spotfire: an informa-
tion exploration environment. SIGMOD, 25(4):25?29.
[Blei, 2012] Blei, D. M. (2012). Probabilistic topic mod-
els. Communications of the ACM, 55(4):77?84.
[Blei et al, 2003] Blei, D. M., Ng, A., and Jordan, M.
(2003). Latent Dirichlet alocation. JMLR, 3.
[Chaney and Blei, 2012] Chaney, A. J.-B. and Blei, D. M.
(2012). Visualizing topic models. In ICWSM.
[Chang et al, 2009] Chang, J., Boyd-Graber, J., Wang, C.,
Gerrish, S., and Blei, D. M. (2009). Reading tea leaves:
How humans interpret topic models. In NIPS.
[Dou et al, 2011] Dou, W., Wang, X., Chang, R., and Rib-
arsky, W. (2011). ParallelTopics: A probabilistic ap-
proach to exploring document collections. In VAST.
[Eisenstein et al, 2012] Eisenstein, J., Chau, D. H., Kittur,
A., and Xing, E. (2012). TopicViz: interactive topic
exploration in document collections. In CHI.
[Hu et al, 2011] Hu, Y., Boyd-Graber, J., and Satinoff, B.
(2011). Interactive topic modeling. In ACL.
[Nguyen et al, 2012] Nguyen, V.-A., Boyd-Graber, J., and
Resnik, P. (2012). SITS: A hierarchical nonparametric
model using speaker identity for topic segmentation in
multiparty conversations. In ACL.
[North and Shneiderman, 2000] North, C. and Shneider-
man, B. (2000). Snap-together visualization: a user
interface for coordinating visualizations via relational
schemata. In AVI, pages 128?135.
[Perer and Shneiderman, 2006] Perer, A. and Shneider-
man, B. (2006). Balancing systematic and flexible
exploration of social networks. IEEE Transactions on
Visualization and Computer Graphics, 12(5):693?700.
[Purver, 2011] Purver, M. (2011). Topic segmentation. In
Spoken Language Understanding: Systems for Extract-
ing Semantic Information from Speech.
[Rogers and Norton, 2011] Rogers, T. and Norton, M. I.
(2011). The artful dodger: Answering the wrong ques-
tion the right way. Journal of Experimental Psychology:
Applied, 17(2):139?147.
[Sopan et al, 2013] Sopan, A., Freier, M., Taieb-Maimon,
M., Plaisant, C., Golbeck, J., and Shneiderman, B.
(2013). Exploring data distributions: Visual design
and evaluation. JHCI, 29(2):77?95.
[Wang Baldonado et al, 2000] Wang Baldonado, M. Q.,
Woodruff, A., and Kuchinsky, A. (2000). Guidelines
for using multiple views in information visualization.
In AVI, pages 110?119.
[Weaver, 2004] Weaver, C. (2004). Building highly-
coordinated visualizations in Improvise. In INFOVIS.
39
Proceedings of the ACL 2010 System Demonstrations, pages 7?12,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
cdec: A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Models
Chris Dyer
University of Maryland
redpony@umd.edu
Adam Lopez
University of Edinburgh
alopez@inf.ed.ac.uk
Juri Ganitkevitch
Johns Hopkins University
juri@cs.jhu.edu
Jonathan Weese
Johns Hopkins University
jweese@cs.jhu.edu
Ferhan Ture
University of Maryland
fture@cs.umd.edu
Phil Blunsom
Oxford University
pblunsom@comlab.ox.ac.uk
Hendra Setiawan
University of Maryland
hendra@umiacs.umd.edu
Vladimir Eidelman
University of Maryland
vlad@umiacs.umd.edu
Philip Resnik
University of Maryland
resnik@umiacs.umd.edu
Abstract
We present cdec, an open source frame-
work for decoding, aligning with, and
training a number of statistical machine
translation models, including word-based
models, phrase-based models, and models
based on synchronous context-free gram-
mars. Using a single unified internal
representation for translation forests, the
decoder strictly separates model-specific
translation logic from general rescoring,
pruning, and inference algorithms. From
this unified representation, the decoder can
extract not only the 1- or k-best transla-
tions, but also alignments to a reference,
or the quantities necessary to drive dis-
criminative training using gradient-based
or gradient-free optimization techniques.
Its efficient C++ implementation means
that memory use and runtime performance
are significantly better than comparable
decoders.
1 Introduction
The dominant models used in machine transla-
tion and sequence tagging are formally based
on either weighted finite-state transducers (FSTs)
or weighted synchronous context-free grammars
(SCFGs) (Lopez, 2008). Phrase-based models
(Koehn et al, 2003), lexical translation models
(Brown et al, 1993), and finite-state conditional
random fields (Sha and Pereira, 2003) exemplify
the former, and hierarchical phrase-based models
the latter (Chiang, 2007). We introduce a soft-
ware package called cdec that manipulates both
classes in a unified way.1
Although open source decoders for both phrase-
based and hierarchical translation models have
been available for several years (Koehn et al,
2007; Li et al, 2009), their extensibility to new
models and algorithms is limited by two sig-
nificant design flaws that we have avoided with
cdec. First, their implementations tightly couple
the translation, language model integration (which
we call rescoring), and pruning algorithms. This
makes it difficult to explore alternative transla-
tion models without also re-implementing rescor-
ing and pruning logic. In cdec, model-specific
code is only required to construct a translation for-
est (?3). General rescoring (with language models
or other models), pruning, inference, and align-
ment algorithms then apply to the unified data
structure (?4). Hence all model types benefit im-
mediately from new algorithms (for rescoring, in-
ference, etc.); new models can be more easily pro-
totyped; and controlled comparison of models is
made easier.
Second, existing open source decoders were de-
signed with the traditional phrase-based parame-
terization using a very small number of dense fea-
tures (typically less than 10). cdec has been de-
signed from the ground up to support any parame-
terization, from those with a handful of dense fea-
tures up to models with millions of sparse features
(Blunsom et al, 2008; Chiang et al, 2009). Since
the inference algorithms necessary to compute a
training objective (e.g. conditional likelihood or
expected BLEU) and its gradient operate on the
unified data structure (?5), any model type can be
trained using with any of the supported training
1The software is released under the Apache License, ver-
sion 2.0, and is available from http://cdec-decoder.org/ .
7
criteria. The software package includes general
function optimization utilities that can be used for
discriminative training (?6).
These features are implemented without com-
promising on performance. We show experimen-
tally that cdec uses less memory and time than
comparable decoders on a controlled translation
task (?7).
2 Decoder workflow
The decoding pipeline consists of two phases. The
first (Figure 1) transforms input, which may be
represented as a source language sentence, lattice
(Dyer et al, 2008), or context-free forest (Dyer
and Resnik, 2010), into a translation forest that has
been rescored with all applicable models.
In cdec, the only model-specific logic is con-
fined to the first step in the process where an
input string (or lattice, etc.) is transduced into
the unified hypergraph representation. Since the
model-specific code need not worry about integra-
tion with rescoring models, it can be made quite
simple and efficient. Furthermore, prior to lan-
guage model integration (and distortion model in-
tegration, in the case of phrase based translation),
pruning is unnecessary for most kinds of mod-
els, further simplifying the model-specific code.
Once this unscored translation forest has been
generated, any non-coaccessible states (i.e., states
that are not reachable from the goal node) are re-
moved and the resulting structure is rescored with
language models using a user-specified intersec-
tion/pruning strategy (?4) resulting in a rescored
translation forest and completing phase 1.
The second phase of the decoding pipeline (de-
picted in Figure 2) computes a value from the
rescored forest: 1- or k-best derivations, feature
expectations, or intersection with a target language
reference (sentence or lattice). The last option
generates an alignment forest, from which a word
alignment or feature expectations can be extracted.
Most of these values are computed in a time com-
plexity that is linear in the number of edges and
nodes in the translation hypergraph using cdec?s
semiring framework (?5).
2.1 Alignment forests and alignment
Alignment is the process of determining if and
how a translation model generates a ?source, tar-
get? string pair. To compute an alignment under
a translation model, the phase 1 translation hyper-
graph is reinterpreted as a synchronous context-
free grammar and then used to parse the target
sentence.2 This results in an alignment forest,
which is a compact representation of all the deriva-
tions of the sentence pair under the translation
model. From this forest, the Viterbi or maximum a
posteriori word alignment can be generated. This
alignment algorithm is explored in depth by Dyer
(2010). Note that if the phase 1 forest has been
pruned in some way, or the grammar does not de-
rive the sentence pair, the target intersection parse
may fail, meaning that an alignment will not be
recoverable.
3 Translation hypergraphs
Recent research has proposed a unified repre-
sentation for the various translation and tagging
formalisms that is based on weighted logic pro-
gramming (Lopez, 2009). In this view, trans-
lation (or tagging) deductions have the structure
of a context-free forest, or directed hypergraph,
where edges have a single head and 0 or more tail
nodes (Nederhof, 2003). Once a forest has been
constructed representing the possible translations,
general inference algorithms can be applied.
In cdec?s translation hypergraph, a node rep-
resents a contiguous sequence of target language
words. For SCFG models and sequential tag-
ging models, a node also corresponds to a source
span and non-terminal type, but for word-based
and phrase-based models, the relationship to the
source string (or lattice) may be more compli-
cated. In a phrase-based translation hypergraph,
the node will correspond to a source coverage vec-
tor (Koehn et al, 2003). In word-based models, a
single node may derive multiple different source
language coverages since word based models im-
pose no requirements on covering all words in the
input. Figure 3 illustrates two example hyper-
graphs, one generated using a SCFG model and
other from a phrase-based model.
Edges are associated with exactly one syn-
chronous production in the source and target lan-
guage, and alternative translation possibilities are
expressed as alternative edges. Edges are further
annotated with feature values, and are annotated
with the source span vector the edge corresponds
to. An edge?s output label may contain mixtures
of terminal symbol yields and positions indicating
where a child node?s yield should be substituted.
2The parser is smart enough to detect the left-branching
grammars generated by lexical translation and tagging mod-
els, and use a more efficient intersection algorithm.
8
SCFG parser
FST transducer
Tagger
Lexical transducer
Phrase-based 
transducer
Source CFG
Source 
sentence
Source lattice
Unscored 
hypergraph
Input Transducers
Cube pruning
Full intersection
FST rescoring
Translation 
hypergraph
Output
Cube growing
No rescoring
Figure 1: Forest generation workflow (first half of decoding pipeline). The decoder?s configuration
specifies what path is taken from the input (one of the bold ovals) to a unified translation hypergraph.
The highlighted path is the workflow used in the test reported in ?7.
Translation 
hypergraph
Target 
reference
Viterbi extraction
k-best extraction
max-translation 
extraction
feature 
expectations
intersection by 
parsing
Alignment 
hypergraph
feature 
expectations
max posterior 
alignment
Viterbi alignment
Translation outputs Alignment outputs
Figure 2: Output generation workflow (second half of decoding pipeline). Possible output types are
designated with a double box.
In the case of SCFG grammars, the edges corre-
spond simply to rules in the synchronous gram-
mar. For non-SCFG translation models, there are
two kinds of edges. The first have zero tail nodes
(i.e., an arity of 0), and correspond to word or
phrase translation pairs (with all translation op-
tions existing on edges deriving the same head
node), or glue rules that glue phrases together.
For tagging, word-based, and phrase-based mod-
els, these are strictly arranged in a monotone, left-
branching structure.
4 Rescoring with weighted FSTs
The design of cdec separates the creation of a
translation forest from its rescoring with a lan-
guage models or similar models.3 Since the struc-
ture of the unified search space is context free (?3),
we use the logic for language model rescoring de-
scribed by Chiang (2007), although any weighted
intersection algorithm can be applied. The rescor-
3Other rescoring models that depend on sequential con-
text include distance-based reordering models or Markov fea-
tures in tagging models.
ing models need not be explicitly represented as
FSTs?the state space can be inferred.
Although intersection using the Chiang algo-
rithm runs in polynomial time and space, the re-
sulting rescored forest may still be too large to rep-
resent completely. cdec therefore supports three
pruning strategies that can be used during intersec-
tion: full unpruned intersection (useful for tagging
models to incorporate, e.g., Markov features, but
not generally practical for translation), cube prun-
ing, and cube growing (Huang and Chiang, 2007).
5 Semiring framework
Semirings are a useful mathematical abstraction
for dealing with translation forests since many
useful quantities can be computed using a single
linear-time algorithm but with different semirings.
A semiring is a 5-tuple (K,?,?, 0, 1) that indi-
cates the set from which the values will be drawn,
K, a generic addition and multiplication operation,
? and ?, and their identities 0 and 1. Multipli-
cation and addition must be associative. Multi-
plication must distribute over addition, and v ? 0
9
Goal
JJ NN
1 2
a
s
m
a
l
l
l
i
t
t
l
e
h
o
u
s
e
s
h
e
l
l
Goal
010
100 101
110
a
s
m
a
l
l
l
i
t
t
l
e
1
a
1
house
1
shell
1
little
1
small
1
house
1
shell
1
little
1
small
Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines
(small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distor-
tion limit of 1 (right).
must equal 0. Values that can be computed using
the semirings include the number of derivations,
the expected translation length, the entropy of the
translation posterior distribution, and the expected
values of feature functions (Li and Eisner, 2009).
Since semirings are such a useful abstraction,
cdec has been designed to facilitate implementa-
tion of new semirings. Table 1 shows the C++ rep-
resentation used for semirings. Note that because
of our representation, built-in types like double,
int, and bool (together with their default op-
erators) are semirings. Beyond these, the type
prob t is provided which stores the logarithm of
the value it represents, which helps avoid under-
flow and overflow problems that may otherwise
be encountered. A generic first-order expectation
semiring is also provided (Li and Eisner, 2009).
Table 1: Semiring representation. T is a C++ type
name.
Element C++ representation
K T
? T::operator+=
? T::operator*=
0 T()
1 T(1)
Three standard algorithms parameterized with
semirings are provided: INSIDE, OUTSIDE, and
INSIDEOUTSIDE, and the semiring is specified us-
ing C++ generics (templates). Additionally, each
algorithm takes a weight function that maps from
hypergraph edges to a value in K, making it possi-
ble to use many different semirings without alter-
ing the underlying hypergraph.
5.1 Viterbi and k-best extraction
Although Viterbi and k-best extraction algorithms
are often expressed as INSIDE algorithms with
the tropical semiring, cdec provides a separate
derivation extraction framework that makes use of
a < operator (Huang and Chiang, 2005). Thus,
many of the semiring types define not only the el-
ements shown in Table 1 but T::operator< as
well. The k-best extraction algorithm is also pa-
rameterized by an optional predicate that can filter
out derivations at each node, enabling extraction
of only derivations that yield different strings as in
Huang et al (2006).
6 Model training
Two training pipelines are provided with cdec.
The first, called Viterbi envelope semiring train-
ing, VEST, implements the minimum error rate
training (MERT) algorithm, a gradient-free opti-
mization technique capable of maximizing arbi-
trary loss functions (Och, 2003).
6.1 VEST
Rather than computing an error surface using k-
best approximations of the decoder search space,
cdec?s implementation performs inference over
the full hypergraph structure (Kumar et al, 2009).
In particular, by defining a semiring whose values
are sets of line segments, having an addition op-
eration equivalent to union, and a multiplication
operation equivalent to a linear transformation of
the line segments, Och?s line search can be com-
puted simply using the INSIDE algorithm. Since
the translation hypergraphs generated by cdec
may be quite large making inference expensive,
the logic for constructing error surfaces is fac-
tored according to the MapReduce programming
paradigm (Dean and Ghemawat, 2004), enabling
parallelization across a cluster of machines. Im-
plementations of the BLEU and TER loss functions
are provided (Papineni et al, 2002; Snover et al,
2006).
10
6.2 Large-scale discriminative training
In addition to the widely used MERT algo-
rithm, cdec also provides a training pipeline for
discriminatively trained probabilistic translation
models (Blunsom et al, 2008; Blunsom and Os-
borne, 2008). In these models, the translation
model is trained to maximize conditional log like-
lihood of the training data under a specified gram-
mar. Since log likelihood is differentiable with
respect to the feature weights in an exponential
model, it is possible to use gradient-based opti-
mization techniques to train the system, enabling
the parameterization of the model using millions
of sparse features. While this training approach
was originally proposed for SCFG-based transla-
tion models, it can be used to train any model
type in cdec. When used with sequential tagging
models, this pipeline is identical to traditional se-
quential CRF training (Sha and Pereira, 2003).
Both the objective (conditional log likelihood)
and its gradient have the form of a difference in
two quantities: each has one term that is com-
puted over the translation hypergraph which is
subtracted from the result of the same computa-
tion over the alignment hypergraph (refer to Fig-
ures 1 and 2). The conditional log likelihood is
the difference in the log partition of the translation
and alignment hypergraph, and is computed using
the INSIDE algorithm. The gradient with respect
to a particular feature is the difference in this fea-
ture?s expected value in the translation and align-
ment hypergraphs, and can be computed using ei-
ther INSIDEOUTSIDE or the expectation semiring
and INSIDE. Since a translation forest is generated
as an intermediate step in generating an alignment
forest (?2) this computation is straightforward.
Since gradient-based optimization techniques
may require thousands of evaluations to converge,
the batch training pipeline is split into map and
reduce components, facilitating distribution over
very large clusters. Briefly, the cdec is run as the
map function, and sentence pairs are mapped over.
The reduce function aggregates the results and per-
forms the optimization using standard algorithms,
including LBFGS (Liu et al, 1989), RPROP (Ried-
miller and Braun, 1993), and stochastic gradient
descent.
7 Experiments
Table 2 compares the performance of cdec, Hi-
ero, and Joshua 1.3 (running with 1 or 8 threads)
decoding using a hierarchical phrase-based trans-
lation grammar and identical pruning settings.4
Figure 4 shows the cdec configuration and
weights file used for this test.
The workstation used has two 2GHz quad-core
Intel Xenon processors, 32GB RAM, is running
Linux kernel version 2.6.18 and gcc version 4.1.2.
All decoders use SRI?s language model toolkit,
version 1.5.9 (Stolcke, 2002). Joshua was run on
the Sun HotSpot JVM, version 1.6.0 12. A hierar-
chical phrase-based translation grammar was ex-
tracted for the NIST MT03 Chinese-English trans-
lation using a suffix array rule extractor (Lopez,
2007). A non-terminal span limit of 15 was used,
and all decoders were configured to use cube prun-
ing with a limit of 30 candidates at each node and
no further pruning. All decoders produced a BLEU
score between 31.4 and 31.6 (small differences are
accounted for by different tie-breaking behavior
and OOV handling).
Table 2: Memory usage and average per-sentence
running time, in seconds, for decoding a Chinese-
English test set.
Decoder Lang. Time (s) Memory
cdec C++ 0.37 1.0Gb
Joshua (1?) Java 0.98 1.5Gb
Joshua (8?) Java 0.35 2.5Gb
Hiero Python 4.04 1.1Gb
formalism=scfg
grammar=grammar.mt03.scfg.gz
add pass through rules=true
scfg max span limit=15
feature function=LanguageModel \
en.3gram.pruned.lm.gz -o 3
feature function=WordPenalty
intersection strategy=cube pruning
cubepruning pop limit=30
LanguageModel 1.12
WordPenalty -4.26
PhraseModel 0 0.963
PhraseModel 1 0.654
PhraseModel 2 0.773
PassThroughRule -20
Figure 4: Configuration file (above) and feature
weights file (below) used for the decoding test de-
scribed in ?7.
4http://sourceforge.net/projects/joshua/
11
8 Future work
cdec continues to be under active development.
We are taking advantage of its modular design to
study alternative algorithms for language model
integration. Further training pipelines are un-
der development, including minimum risk train-
ing using a linearly decomposable approximation
of BLEU (Li and Eisner, 2009), and MIRA train-
ing (Chiang et al, 2009). All of these will be
made publicly available as the projects progress.
We are also improving support for parallel training
using Hadoop (an open-source implementation of
MapReduce).
Acknowledgements
This work was partially supported by the GALE
program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of the
sponsors. Further support was provided the Euro-
Matrix project funded by the European Commis-
sion (7th Framework Programme). Discussions
with Philipp Koehn, Chris Callison-Burch, Zhifei
Li, Lane Schwarz, and Jimmy Lin were likewise
crucial to the successful execution of this project.
References
P. Blunsom and M. Osborne. 2008. Probalistic inference for
machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrimina-
tive latent variable model for statistical machine transla-
tion. In Proc. of ACL-HLT.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263?311.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Proc. of
NAACL, pages 218?226.
D. Chiang. 2007. Hierarchical phrase-based translation.
Comp. Ling., 33(2):201?228.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In Proc. of the 6th Sym-
posium on Operating System Design and Implementation
(OSDI 2004), pages 137?150.
C. Dyer and P. Resnik. 2010. Context-free reordering, finite-
state translation. In Proc. of HLT-NAACL.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proc. of HLT-ACL.
C. Dyer. 2010. Two monolingual parses are better than one
(synchronous parse). In Proc. of HLT-NAACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In In
Proc. of IWPT, pages 53?64.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. ACL.
L. Huang, K. Knight, and A. Joshi. 2006. A syntax-directed
translator with extended domain of locality. In Proc. of
AMTA.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT/NAACL, pages 48?54.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007. Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of ACL, Demonstration Ses-
sion, pages 177?180, June.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient
minimum error rate training and minimum B ayes-risk de-
coding for translation hypergraphs and lattices. In Proc.
of ACL, pages 163?171.
Z. Li and J. Eisner. 2009. First- and second-order expectation
semirings with applications to minimum-risk training on
translation forests. In Proc. of EMNLP, pages 40?51.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khu-
danpur, L. Schwartz, W. N. G. Thornton, J. Weese, and
O. F. Zaidan. 2009. Joshua: an open source toolkit for
parsing-based machine translation. In Proc. of the Fourth
Workshop on Stat. Machine Translation, pages 135?139.
D. C. Liu, J. Nocedal, D. C. Liu, and J. Nocedal. 1989. On
the limited memory BFGS method for large scale opti-
mization. Mathematical Programming B, 45(3):503?528.
A. Lopez. 2007. Hierarchical phrase-based translation with
suffix arrays. In Proc. of EMNLP, pages 976?985.
A. Lopez. 2008. Statistical machine translation. ACM Com-
puting Surveys, 40(3), Aug.
A. Lopez. 2009. Translation as weighted deduction. In Proc.
of EACL, pages 532?540.
M.-J. Nederhof. 2003. Weighted deductive parsing and
Knuth?s algorithm. Comp. Ling., 29(1):135?143, Mar.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
M. Riedmiller and H. Braun. 1993. A direct adaptive method
for faster backpropagation learning: The RPROP algo-
rithm. In Proc. of the IEEE international conference on
neural networks, pages 586?591.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL, pages 134?141.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Intl. Conf. on Spoken Language Processing.
12
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 78?87,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
SITS: A Hierarchical Nonparametric Model using Speaker Identity for
Topic Segmentation in Multiparty Conversations
Viet-An Nguyen
Department of Computer Science
and UMIACS
University of Maryland
College Park, MD
vietan@cs.umd.edu
Jordan Boyd-Graber
iSchool
and UMIACS
University of Maryland
College Park, MD
jbg@umiacs.umd.edu
Philip Resnik
Department of Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
One of the key tasks for analyzing conversa-
tional data is segmenting it into coherent topic
segments. However, most models of topic
segmentation ignore the social aspect of con-
versations, focusing only on the words used.
We introduce a hierarchical Bayesian nonpara-
metric model, Speaker Identity for Topic Seg-
mentation (SITS), that discovers (1) the top-
ics used in a conversation, (2) how these top-
ics are shared across conversations, (3) when
these topics shift, and (4) a person-specific
tendency to introduce new topics. We eval-
uate against current unsupervised segmenta-
tion models to show that including person-
specific information improves segmentation
performance on meeting corpora and on po-
litical debates. Moreover, we provide evidence
that SITS captures an individual?s tendency to
introduce new topics in political contexts, via
analysis of the 2008 US presidential debates
and the television program Crossfire.
1 Topic Segmentation as a Social Process
Conversation, interactive discussion between two or
more people, is one of the most essential and com-
mon forms of communication. Whether in an in-
formal situation or in more formal settings such as
a political debate or business meeting, a conversa-
tion is often not about just one thing: topics evolve
and are replaced as the conversation unfolds. Dis-
covering this hidden structure in conversations is a
key problem for conversational assistants (Tur et al,
2010) and tools that summarize (Murray et al, 2005)
and display (Ehlen et al, 2007) conversational data.
Topic segmentation also can illuminate individuals?
agendas (Boydstun et al, 2011), patterns of agree-
ment and disagreement (Hawes et al, 2009; Abbott
et al, 2011), and relationships among conversational
participants (Ireland et al, 2011).
One of the most natural ways to capture conversa-
tional structure is topic segmentation (Reynar, 1998;
Purver, 2011). Topic segmentation approaches range
from simple heuristic methods based on lexical simi-
larity (Morris and Hirst, 1991; Hearst, 1997) to more
intricate generative models and supervised meth-
ods (Georgescul et al, 2006; Purver et al, 2006;
Gruber et al, 2007; Eisenstein and Barzilay, 2008),
which have been shown to outperform the established
heuristics.
However, previous computational work on con-
versational structure, particularly in topic discovery
and topic segmentation, focuses primarily on con-
tent, ignoring the speakers. We argue that, because
conversation is a social process, we can understand
conversational phenomena better by explicitly model-
ing behaviors of conversational participants. In Sec-
tion 2, we incorporate participant identity in a new
model we call Speaker Identity for Topic Segmen-
tation (SITS), which discovers topical structure in
conversation while jointly incorporating a participant-
level social component. Specifically, we explicitly
model an individual?s tendency to introduce a topic.
After outlining inference in Section 3 and introducing
data in Section 4, we use SITS to improve state-of-
the-art-topic segmentation and topic identification
models in Section 5. In addition, in Section 6, we
also show that the per-speaker model is able to dis-
cover individuals who shape and influence the course
of a conversation. Finally, we discuss related work
and conclude the paper in Section 7.
2 Modeling Multiparty Discussions
Data Properties We are interested in turn-taking,
multiparty discussion. This is a broad category, in-
78
cluding political debates, business meetings, and on-
line chats. More formally, such datasets contain C
conversations. A conversation c has Tc turns, each of
which is a maximal uninterrupted utterance by one
speaker.1 In each turn t ? [1, Tc], a speaker ac,t utters
N words {wc,t,n}. Each word is from a vocabulary
of size V , and there are M distinct speakers.
Modeling Approaches The key insight of topic
segmentation is that segments evince lexical cohe-
sion (Galley et al, 2003; Olney and Cai, 2005).
Words within a segment will look more like their
neighbors than other words. This insight has been
used to tune supervised methods (Hsueh et al, 2006)
and inspire unsupervised models of lexical cohesion
using bags of words (Purver et al, 2006) and lan-
guage models (Eisenstein and Barzilay, 2008).
We too take the unsupervised statistical approach.
It requires few resources and is applicable in many
domains without extensive training. Like previ-
ous approaches, we consider each turn to be a bag
of words generated from an admixture of topics.
Topics?after the topic modeling literature (Blei and
Lafferty, 2009)?are multinomial distributions over
terms. These topics are part of a generative model
posited to have produced a corpus.
However, topic models alone cannot model the dy-
namics of a conversation. Topic models typically do
not model the temporal dynamics of individual docu-
ments, and those that do (Wang et al, 2008; Gerrish
and Blei, 2010) are designed for larger documents
and are not applicable here because they assume that
most topics appear in every time slice.
Instead, we endow each turn with a binary latent
variable lc,t, called the topic shift. This latent variable
signifies whether the speaker changed the topic of the
conversation. To capture the topic-controlling behav-
ior of the speakers across different conversations, we
further associate each speaker m with a latent topic
shift tendency, pim. Informally, this variable is in-
tended to capture the propensity of a speaker to effect
a topic shift. Formally, it represents the probability
that the speakerm will change the topic (distribution)
of a conversation.
We take a Bayesian nonparametric ap-
proach (Mu?ller and Quintana, 2004). Unlike
1Note the distinction with phonetic utterances, which by
definition are bounded by silence.
parametric models, which a priori fix the number of
topics, nonparametric models use a flexible number
of topics to better represent data. Nonparametric
distributions such as the Dirichlet process (Ferguson,
1973) share statistical strength among conversations
using a hierarchical model, such as the hierarchical
Dirichlet process (HDP) (Teh et al, 2006).
2.1 Generative Process
In this section, we develop SITS, a generative model
of multiparty discourse that jointly discovers topics
and speaker-specific topic shifts from an unannotated
corpus (Figure 1a). As in the hierarchical Dirichlet
process (Teh et al, 2006), we allow an unbounded
number of topics to be shared among the turns of the
corpus. Topics are drawn from a base distribution
H over multinomial distributions over the vocabu-
lary, a finite Dirichlet with symmetric prior ?. Unlike
the HDP, where every document (here, every turn)
draws a new multinomial distribution from a Dirich-
let process, the social and temporal dynamics of a
conversation, as specified by the binary topic shift
indicator lc,t, determine when new draws happen.
The full generative process is as follows:
1. For speaker m ? [1,M ], draw speaker shift probability
pim ? Beta(?)
2. Draw global probability measure G0 ? DP(?,H)
3. For each conversation c ? [1, C]
(a) Draw conversation distribution Gc ? DP(?0, G0)
(b) For each turn t ? [1, Tc] with speaker ac,t
i. If t = 1, set the topic shift lc,t = 1. Otherwise,
draw lc,t ? Bernoulli(piac,t).
ii. If lc,t = 1, draw Gc,t ? DP (?c, Gc). Other-
wise, set Gc,t ? Gc,t?1.
iii. For each word index n ? [1, Nc,t]
? Draw ?c,t,n ? Gc,t
? Draw wc,t,n ? Multinomial(?c,t,n)
The hierarchy of Dirichlet processes allows sta-
tistical strength to be shared across contexts; within
a conversation and across conversations. The per-
speaker topic shift tendency pim allows speaker iden-
tity to influence the evolution of topics.
To make notation concrete and aligned with the
topic segmentation, we introduce notation for seg-
ments in a conversation. A segment s of conver-
sation c is a sequence of turns [?, ? ?] such that
lc,? = lc,? ?+1 = 1 and lc,t = 0, ?t ? (?, ? ?]. When
lc,t = 0, Gc,t is the same as Gc,t?1 and all topics (i.e.
multinomial distributions over words) {?c,t,n} that
generate words in turn t and the topics {?c,t?1,n}
that generate words in turn t? 1 come from the same
79
pim ?
ac,2 ac,Tc
wc,1,n wc,2,n wc,Tc,n
?c,1,n ?c,2,n ?c,Tc,n
Gc,1 Gc,2 Gc,Tc
?c lc,2 lc,TcGc?0
G0
? H
C
M
Nc,1 Nc,2 Nc,Tc
(a)
?k?
? pim ?
ac,2 ac,Tc
wc,1,n
zc,1,n
?c,1
wc,2,n
zc,2,n
?c,2
lc,2
wc,Tc,n
zc,Tc,n
?c,Tc
lc,Tc
C
K
M
Nc,1 Nc,2 Nc,Tc
(b)
Figure 1: Graphical model representations of our proposed models: (a) the nonparametric version; (b) the
parametric version. Nodes represent random variables (shaded ones are observed), lines are probabilistic
dependencies. Plates represent repetition. The innermost plates are turns, grouped in conversations.
distribution. Thus all topics used in a segment s are
drawn from a single distribution, Gc,s,
Gc,s | lc,1, lc,2, ? ? ? , lc,Tc , ?c, Gc ? DP(?c, Gc) (1)
For notational convenience, Sc denotes the num-
ber of segments in conversation c, and st denotes
the segment index of turn t. We emphasize that all
segment-related notations are derived from the poste-
rior over the topic shifts l and not part of the model
itself.
Parametric Version SITS is a generalization of a
parametric model (Figure 1b) where each turn has
a multinomial distribution over K topics. In the
parametric case, the number of topics K is fixed.
Each topic, as before, is a multinomial distribution
?1 . . . ?K . In the parametric case, each turn t in con-
versation c has an explicit multinomial distribution
over K topics ?c,t, identical for turns within a seg-
ment. A new topic distribution ? is drawn from a
Dirichlet distribution parameterized by ? when the
topic shift indicator l is 1.
The parametric version does not share strength
within or across conversations, unlike SITS. When
applied on a single conversation without speaker iden-
tity (all speakers are identical) it is equivalent to
(Purver et al, 2006). In our experiments (Section 5),
we compare against both.
3 Inference
To find the latent variables that best explain observed
data, we use Gibbs sampling, a widely used Markov
chain Monte Carlo inference technique (Neal, 2000;
Resnik and Hardisty, 2010). The state space is latent
variables for topic indices assigned to all tokens z =
{zc,t,n} and topic shifts assigned to turns l = {lc,t}.
We marginalize over all other latent variables. Here,
we only present the conditional sampling equations;
for more details, see our supplement.2
3.1 Sampling Topic Assignments
To sample zc,t,n, the index of the shared topic as-
signed to token n of turn t in conversation c, we need
to sample the path assigning each word token to a
segment-specific topic, each segment-specific topic
to a conversational topic and each conversational
topic to a shared topic. For efficiency, we make use
of the minimal path assumption (Wallach, 2008) to
generate these assignments.3 Under the minimal path
assumption, an observation is assumed to have been
generated by using a new distribution if and only if
there is no existing distribution with the same value.
2 http://www.cs.umd.edu/?vietan/topicshift/appendix.pdf
3We also investigated using the maximal assumption and
fully sampling assignments. We found the minimal path assump-
tion worked as well as explicitly sampling seating assignments
and that the maximal path assumption worked less well.
80
We use Nc,s,k to denote the number of tokens in
segment s in conversation c assigned topic k; Nc,k
denotes the total number of segment-specific top-
ics in conversation c assigned topic k and Nk de-
notes the number of conversational topics assigned
topic k. TWk,w denotes the number of times the
shared topic k is assigned to word w in the vocab-
ulary. Marginal counts are represented with ? and
? represents all hyperparameters. The conditional
distribution for zc,t,n is P (zc,t,n = k | wc,t,n =
w, z?c,t,n,w?c,t,n, l, ?) ?
N?c,t,nc,st,k + ?c
N?c,t,nc,k +?0
N?c,t,nk +
?
K
N?c,t,n? +?
N?c,t,nc,? +?0
N?c,t,nc,st,? + ?c
?
?
?
?
?
?
TW?c,t,nk,w + ?
TW?c,t,nk,? + V ?
,
1
V
k new.
(2)
Here V is the size of the vocabulary, K is the current
number of shared topics and the superscript ?c,t,n
denotes counts without considering wc,t,n. In Equa-
tion 2, the first factor is proportional to the probability
of sampling a path according to the minimal path as-
sumption; the second factor is proportional to the
likelihood of observing w given the sampled topic.
Since an uninformed prior is used, when a new topic
is sampled, all tokens are equiprobable.
3.2 Sampling Topic Shifts
Sampling the topic shift variable lc,t requires us to
consider merging or splitting segments. We use kc,t
to denote the shared topic indices of all tokens in
turn t of conversation c; Sac,t,x to denote the num-
ber of times speaker ac,t is assigned the topic shift
with value x ? {0, 1}; Jxc,s to denote the number of
topics in segment s of conversation c if lc,t = x and
Nxc,s,j to denote the number of tokens assigned to the
segment-specific topic j when lc,t = x.4 Again, the
superscript ?c,t is used to denote exclusion of turn t
of conversation c in the corresponding counts.
Recall that the topic shift is a binary variable. We
use 0 to represent the case that the topic distribution
is identical to the previous turn. We sample this
assignment P (lc,t = 0 | l?c,t,w,k,a, ?) ?
S?c,tac,t,0 + ?
S?c,tac,t,? + 2?
?
?
J0c,stc
?J0c,st
j=1 (N
0
c,st,j ? 1)!
?N0c,st,?
x=1 (x? 1 + ?c)
. (3)
4Deterministically knowing the path assignments is the pri-
mary efficiency motivation for using the minimal path assump-
tion. The alternative is to explicitly sample the path assignments,
which is more complicated (for both notation and computation).
This option is spelled in full detail in the supplementary material.
In Equation 3, the first factor is proportional to the
probability of assigning a topic shift of value 0 to
speaker ac,t and the second factor is proportional to
the joint probability of all topics in segment st of
conversation c when lc,t = 0.
The other alternative is for the topic shift to be
1, which represents the introduction of a new distri-
bution over topics inside an existing segment. We
sample this as P (lc,t = 1 | l?c,t,w,k,a, ?) ?
S?c,tac,t,1 + ?
S?c,tac,t,? + 2?
?
?
?
?
J1c,(st?1)c
?J1c,(st?1)
j=1 (N
1
c,(st?1),j ? 1)!
?N1c,(st?1),?
x=1 (x? 1 + ?c)
?
J1c,stc
?J1c,st
j=1 (N
1
c,stj ? 1)!
?N1c,st,?
x=1 (x? 1 + ?c)
?
? . (4)
As above, the first factor in Equation 4 is propor-
tional to the probability of assigning a topic shift of
value 1 to speaker ac,t; the second factor in the big
bracket is proportional to the joint distribution of the
topics in segments st ? 1 and st. In this case lc,t = 1
means splitting the current segment, which results in
two joint probabilities for two segments.
4 Datasets
This section introduces the three corpora we use. We
preprocess the data to remove stopwords and remove
turns containing fewer than five tokens.
The ICSI Meeting Corpus: The ICSI Meeting
Corpus (Janin et al, 2003) is 75 transcribed meetings.
For evaluation, we used a standard set of reference
segmentations (Galley et al, 2003) of 25 meetings.
Segmentations are binary, i.e., each point of the doc-
ument is either a segment boundary or not, and on
average each meeting has 8 segment boundaries. Af-
ter preprocessing, there are 60 unique speakers and
the vocabulary contains 3346 non-stopword tokens.
The 2008 Presidential Election Debates Our sec-
ond dataset contains three annotated presidential de-
bates (Boydstun et al, 2011) between Barack Obama
and John McCain and a vice presidential debate be-
tween Joe Biden and Sarah Palin. Each turn is one
of two types: questions (Q) from the moderator or
responses (R) from a candidate. Each clause in a
turn is coded with a Question Topic (TQ) and a Re-
sponse Topic (TR). Thus, a turn has a list of TQ?s and
TR?s both of length equal to the number of clauses in
the turn. Topics are from the Policy Agendas Topics
81
Speaker Type Turn clauses TQ TR
Brokaw Q Sen. Obama, [. . . ] Are you saying [. . . ] that the American economy is going to get much worse
before it gets better and they ought to be prepared for that?
1 N/A
Obama R
No, I am confident about the American economy. 1 1
[. . . ] But most importantly, we?re going to have to help ordinary families be able to stay in their
homes, make sure that they can pay their bills [. . . ]
1 14
Brokaw Q Sen. McCain, in all candor, do you think the economy is going to get worse before it gets better? 1 N/A
McCain R
[. . . ] I think if we act effectively, if we stabilize the housing market?which I believe we can, 1 14
if we go out and buy up these bad loans, so that people can have a new mortgage at the new value
of their home
1 14
I think if we get rid of the cronyism and special interest influence in Washington so we can act
more effectively. [. . . ]
1 20
Table 1: Example turns from the annotated 2008 election debates. The topics (TQ and TR) are from the Policy
Agendas Topics Codebook which contains the following codes of topic: Macroeconomics (1), Housing &
Community Development (14), Government Operations (20).
Codebook, a manual inventory of 19 major topics
and 225 subtopics.5 Table 1 shows an example anno-
tation.
To get reference segmentations, we assign each
turn a real value from 0 to 1 indicating how much a
turn changes the topic. For a question-typed turn, the
score is the fraction of clause topics not appearing in
the previous turn; for response-typed turns, the score
is the fraction of clause topics that do not appear in
the corresponding question. This results in a set of
non-binary reference segmentations. For evaluation
metrics that require binary segmentations, we create
a binary segmentation by setting a turn as a segment
boundary if the computed score is 1. This threshold
is chosen to include only true segment boundaries.
CNN?s Crossfire Crossfire was a weekly U.S. tele-
vision ?talking heads? program engineered to incite
heated arguments (hence the name). Each episode
features two recurring hosts, two guests, and clips
from the week?s news. Our Crossfire dataset con-
tains 1134 transcribed episodes aired between 2000
and 2004.6 There are 2567 unique speakers. Unlike
the previous two datasets, Crossfire does not have
explicit topic segmentations, so we use it to explore
speaker-specific characteristics (Section 6).
5 Topic Segmentation Experiments
In this section, we examine how well SITS can repli-
cate annotations of when new topics are introduced.
5 http://www.policyagendas.org/page/topic-codebook
6 http://www.cs.umd.edu/?vietan/topicshift/crossfire.zip
We discuss metrics for evaluating an algorithm?s seg-
mentation against a gold annotation, describe our
experimental setup, and report those results.
Evaluation Metrics To evaluate segmentations,
we use Pk (Beeferman et al, 1999) and WindowDiff
(WD) (Pevzner and Hearst, 2002). Both metrics mea-
sure the probability that two points in a document
will be incorrectly separated by a segment boundary.
Both techniques consider all spans of length k in the
document and count whether the two endpoints of
the window are (im)properly segmented against the
gold segmentation.
However, these metrics have drawbacks. First,
they require both hypothesized and reference seg-
mentations to be binary. Many algorithms (e.g., prob-
abilistic approaches) give non-binary segmentations
where candidate boundaries have real-valued scores
(e.g., probability or confidence). Thus, evaluation
requires arbitrary thresholding to binarize soft scores.
To be fair, thresholds are set so the number of seg-
ments are equal to a predefined value (Purver et al,
2006; Galley et al, 2003).
To overcome these limitations, we also use Earth
Mover?s Distance (EMD) (Rubner et al, 2000), a
metric that measures the distance between two distri-
butions. The EMD is the minimal cost to transform
one distribution into the other. Each segmentation
can be considered a multi-dimensional distribution
where each candidate boundary is a dimension. In
EMD, a distance function across features allows par-
tial credit for ?near miss? segment boundaries. In
82
addition, because EMD operates on distributions, we
can compute the distance between non-binary hy-
pothesized segmentations with binary or real-valued
reference segmentations. We use the FastEMD im-
plementation (Pele and Werman, 2009).
Experimental Methods We applied the following
methods to discover topic segmentations in a docu-
ment:
? TextTiling (Hearst, 1997) is one of the earliest general-
purpose topic segmentation algorithms, sliding a fixed-
width window to detect major changes in lexical similarity.
? P-NoSpeaker-S: parametric version without speaker iden-
tity run on each conversation (Purver et al, 2006)
? P-NoSpeaker-M: parametric version without speaker
identity run on all conversations
? P-SITS: the parametric version of SITS with speaker iden-
tity run on all conversations
? NP-HMM: the HMM-based nonparametric model which
a single topic per turn. This model can be considered a
Sticky HDP-HMM (Fox et al, 2008) with speaker identity.
? NP-SITS: the nonparametric version of SITS with speaker
identity run on all conversations.
Parameter Settings and Implementations In our
experiment, all parameters of TextTiling are the
same as in (Hearst, 1997). For statistical models,
Gibbs sampling with 10 randomly initialized chains
is used. Initial hyperparameter values are sampled
from U(0, 1) to favor sparsity; statistics are collected
after 500 burn-in iterations with a lag of 25 itera-
tions over a total of 5000 iterations; and slice sam-
pling (Neal, 2003) optimizes hyperparameters.
Results and Analysis Table 2 shows the perfor-
mance of various models on the topic segmentation
problem, using the ICSI corpus and the 2008 debates.
Consistent with previous results, probabilistic
models outperform TextTiling. In addition, among
the probabilistic models, the models that had access
to speaker information consistently segment better
than those lacking such information, supporting our
assertion that there is benefit to modeling conversa-
tion as a social process. Furthermore, NP-SITS out-
performs NP-HMM in both experiments, suggesting
that using a distribution over topics to turns is bet-
ter than using a single topic. This is consistent with
parametric results reported in (Purver et al, 2006).
The contribution of speaker identity seems more
valuable in the debate setting. Debates are character-
ized by strong rewards for setting the agenda; dodg-
ing a question or moving the debate toward an oppo-
nent?s weakness can be useful strategies (Boydstun
et al, 2011). In contrast, meetings (particularly low-
stakes ICSI meetings) are characterized by pragmatic
rather than strategic topic shifts. Second, agenda-
setting roles are clearer in formal debates; a modera-
tor is tasked with setting the agenda and ensuring the
conversation does not wander too much.
The nonparametric model does best on the smaller
debate dataset. We suspect that an evaluation that
directly accessed the topic quality, either via predic-
tion (Teh et al, 2006) or interpretability (Chang et al,
2009) would favor the nonparametric model more.
6 Evaluating Topic Shift Tendency
In this section, we focus on the ability of SITS to
capture speaker-level attributes. Recall that SITS
associates with each speaker a topic shift tendency
pi that represents the probability of asserting a new
topic in the conversation. While topic segmentation
is a well studied problem, there are no established
quantitative measurements of an individual?s ability
to control a conversation. To evaluate whether the
tendency is capturing meaningful characteristics of
speakers, we compare our inferred tendencies against
insights from political science.
2008 Elections To obtain a posterior estimate of pi
(Figure 3) we create 10 chains with hyperparameters
sampled from the uniform distribution U(0, 1) and
averaged pi over 10 chains (as described in Section 5).
In these debates, Ifill is the moderator of the debate
between Biden and Palin; Brokaw, Lehrer and Schief-
fer are the three moderators of three debates between
Obama and McCain. Here ?Question? denotes ques-
tions from audiences in ?town hall? debate. The role
of this ?speaker? can be considered equivalent to the
debate moderator.
The topic shift tendencies of moderators are
much higher than for candidates. In the three de-
bates between Obama and McCain, the moderators?
Brokaw, Lehrer and Schieffer?have significantly
higher scores than both candidates. This is a useful
reality check, since in a debate the moderators are
the ones asking questions and literally controlling the
topical focus. Interestingly, in the vice-presidential
debate, the score of moderator Ifill is only slightly
higher than those of Palin and Biden; this is consis-
tent with media commentary characterizing her as a
83
Model EMD
Pk WindowDiff
k = 5 10 15 k = 5 10 15
IC
SI
D
at
as
et
TextTiling 2.507 .289 .388 .451 .318 .477 .561
P-NoSpeaker-S 1.949 .222 .283 .342 .269 .393 .485
P-NoSpeaker-M 1.935 .207 .279 .335 .253 .371 .468
P-SITS 1.807 .211 .251 .289 .256 .363 .434
NP-HMM 2.189 .232 .257 .263 .267 .377 .444
NP-SITS 2.126 .228 .253 .259 .262 .372 .440
D
eb
at
es
D
at
as
et TextTiling 2.821 .433 .548 .633 .534 .674 .760
P-NoSpeaker-S 2.822 .426 .543 .653 .482 .650 .756
P-NoSpeaker-M 2.712 .411 .522 .589 .479 .644 .745
P-SITS 2.269 .380 .405 .402 .482 .625 .719
NP-HMM 2.132 .362 .348 .323 .486 .629 .723
NP-SITS 1.813 .332 .269 .231 .470 .600 .692
Table 2: Results on the topic segmentation task.
Lower is better. The parameter k is the window
size of the metrics Pk and WindowDiff chosen to
replicate previous results.
0 0.1 0.2 0.3 0.4
IFILLBIDENPALINOBAMAMCCAINBROKAWLEHRERSCHIEFFERQUESTION
Table 3: Topic shift tendency pi of speakers in the
2008 Presidential Election Debates (larger means
greater tendency)
weak moderator.7 Similarly, the ?Question? speaker
had a relatively high variance, consistent with an
amalgamation of many distinct speakers.
These topic shift tendencies suggest that all can-
didates manage to succeed at some points in setting
and controlling the debate topics. Our model gives
Obama a slightly higher score than McCain, consis-
tent with social science claims (Boydstun et al, 2011)
that Obama had the lead in setting the agenda over
McCain. Table 4 shows of SITS-detected topic shifts.
Crossfire Crossfire, unlike the debates, has many
speakers. This allows us to examine more closely
what we can learn about speakers? topic shift ten-
dency. We verified that SITS can segment topics,
and assuming that changing the topic is useful for a
speaker, how can we characterize who does so effec-
tively? We examine the relationship between topic
shift tendency, social roles, and political ideology.
To focus on frequent speakers, we filter out speak-
ers with fewer than 30 turns. Most speakers have
relatively small pi, with the mode around 0.3. There
are, however, speakers with very high topic shift
tendencies. Table 5 shows the speakers having the
highest values according to SITS.
We find that there are three general patterns for
who influences the course of a conversation in Cross-
fire. First, there are structural ?speakers? the show
uses to frame and propose new topics. These are
7 http://harpers.org/archive/2008/10/hbc-90003659
audience questions, news clips (e.g. many of Gore?s
and Bush?s turns from 2000), and voice overs. That
SITS is able to recover these is reassuring. Second,
the stable of regular hosts receives high topic shift
tendencies, which is reasonable given their experi-
ence with the format and ostensible moderation roles
(in practice they also stoke lively discussion).
The remaining class is more interesting. The re-
maining non-hosts with high topic shift tendency are
relative moderates on the political spectrum:
? John Kasich, one of few Republicans to support the assault
weapons ban and now governor of Ohio, a swing state
? Christine Todd Whitman, former Republican governor of
New Jersey, a very Democratic state
? John McCain, who before 2008 was known as a ?maverick?
for working with Democrats (e.g. Russ Feingold)
This suggests that, despite Crossfire?s tendency to
create highly partisan debates, those who are able to
work across the political spectrum may best be able
to influence the topic under discussion in highly po-
larized contexts. Table 4 shows detected topic shifts
from these speakers; two of these examples (McCain
and Whitman) show disagreement of Republicans
with President Bush. In the other, Kasich is defend-
ing a Republican plan (school vouchers) popular with
traditional Democratic constituencies.
7 Related and Future Work
In the realm of statistical models, a number of tech-
niques incorporate social connections and identity to
explain content in social networks (Chang and Blei,
84
Previous turn Turn detected as shifting topic
D
eb
at
es
D
at
as
et
PALIN: Your question to him was whether he sup-
ported gay marriage and my answer is the same as
his and it is that I do not.
IFILL: Wonderful. You agree. On that note, let?s move to foreign policy. You
both have sons who are in Iraq or on their way to Iraq. You, Governor Palin,
have said that you would like to see a real clear plan for an exit strategy. [. . . ]
MCCAIN: I think that Joe Biden is qualified in
many respects. . . .
SCHIEFFER: [. . . ] Let?s talk about energy and climate control. Every president
since Nixon has said what both of you [. . . ]
IFILL: So, Governor, as vice president, there?s
nothing that you have promised [. . . ] that you
wouldn?t take off the table because of this finan-
cial crisis we?re in?
BIDEN: Again, let me?let?s talk about those tax breaks. [Obama] voted for an
energy bill because, for the first time, it had real support for alternative energy.
[. . . ] on eliminating the tax breaks for the oil companies, Barack Obama voted
to eliminate them. [. . . ]
C
ro
ss
fir
e
D
at
as
et
PRESS: But what do you say, governor, to Gov-
ernor Bush and [. . . ] your party who would let
politicians and not medical scientists decide what
drugs are distributed [. . . ]
WHITMAN: Well I disagree with them on this particular issues [. . . ] that?s
important to me that George Bush stands for education of our children [. . . ] I
care about tax policy, I care about the environment. I care about all the issues
where he has a proven record in Texas [. . . ]
WEXLER: [. . . ] They need a Medicare prescrip-
tion drug plan [. . . ] Talk about schools, [. . . ] Al
Gore has got a real plan. George Bush offers us
vouchers. Talk about the environment. [. . . ] Al
Gore is right on in terms of the majority of Ameri-
cans, but George Bush [. . . ]
KASICH: [. . . ] I want to talk about choice. [. . . ] George Bush believes that, if
schools fail, parents ought to have a right to get their kids out of those schools
and give them a chance and an opportunity for success. Gore says ?no way? [. . . ]
Social Security. George Bush says [. . . ] direct it the way federal employees do
[. . . ] Al Gore says ?No way? [. . . ] That?s real choice. That?s real bottom-up,
not a bureaucratic approach, the way we run this country.
PRESS: Senator, Senator Breaux mentioned that
it?s President Bush?s aim to start on education [. . . ]
[McCain] [. . . ] said he was going to do introduce
the legislation the first day of the first week of the
new administration. [. . . ]
MCCAIN: After one of closest elections in our nation?s history, there is one
thing the American people are unanimous about They want their government
back. We can do that by ridding politics of large, unregulated contributions that
give special interests a seat at the table while average Americans are stuck in the
back of the room.
Table 4: Example of turns designated as a topic shift by SITS. Turns were chosen with speakers to give
examples of those with high topic shift tendency pi.
Rank Speaker pi Rank Speaker pi
1 Announcer .884 10 Kasich .570
2 Male .876 11 Carville? .550
3 Question .755 12 Carlson? .550
4 G. W. Bush? .751 13 Begala? .545
5 Press? .651 14 Whitman .533
6 Female .650 15 McAuliffe .529
7 Gore? .650 16 Matalin? .527
8 Narrator .642 17 McCain .524
9 Novak? .587 18 Fleischer .522
Table 5: Top speakers by topic shift tendencies. We
mark hosts (?) and ?speakers? who often (but not al-
ways) appeared in clips (?). Apart from those groups,
speakers with the highest tendency were political
moderates.
2009) and scientific corpora (Rosen-Zvi et al, 2004).
However, these models ignore the temporal evolution
of content, treating documents as static.
Models that do investigate the evolution of topics
over time typically ignore the identify of the speaker.
For example: models having sticky topics over n-
grams (Johnson, 2010), sticky HDP-HMM (Fox et al,
2008); models that are an amalgam of sequential
models and topic models (Griffiths et al, 2005; Wal-
lach, 2006; Gruber et al, 2007; Ahmed and Xing,
2008; Boyd-Graber and Blei, 2008; Du et al, 2010);
or explicit models of time or other relevant features
as a distinct latent variable (Wang and McCallum,
2006; Eisenstein et al, 2010).
In contrast, SITS jointly models topic and individ-
uals? tendency to control a conversation. Not only
does SITS outperform other models using standard
computational linguistics baselines, but it also pro-
poses intriguing hypotheses for social scientists.
Associating each speaker with a scalar that mod-
els their tendency to change the topic does improve
performance on standard tasks, but it?s inadequate to
fully describe an individual. Modeling individuals?
perspective (Paul and Girju, 2010), ?side? (Thomas
et al, 2006), or personal preferences for topics (Grim-
mer, 2009) would enrich the model and better illumi-
nate the interaction of influence and topic.
Statistical analysis of political discourse can help
discover patterns that political scientists, who often
work via a ?close reading,? might otherwise miss.
We plan to work with social scientists to validate
our implicit hypothesis that our topic shift tendency
correlates well with intuitive measures of ?influence.?
85
Acknowledgements
This research was funded in part by the Army Re-
search Laboratory through ARL Cooperative Agree-
ment W911NF-09-2-0072 and by the Office of the
Director of National Intelligence (ODNI), Intelli-
gence Advanced Research Projects Activity (IARPA),
through the Army Research Laboratory. Jordan
Boyd-Graber and Philip Resnik are also supported
by US National Science Foundation Grant NSF grant
#1018625. Any opinions, findings, conclusions, or
recommendations expressed are the authors? and do
not necessarily reflect those of the sponsors.
References
[Abbott et al, 2011] Abbott, R., Walker, M., Anand, P.,
Fox Tree, J. E., Bowmani, R., and King, J. (2011). How
can you say such things?!?: Recognizing disagreement
in informal political argument. In Proceedings of the
Workshop on Language in Social Media (LSM 2011),
pages 2?11.
[Ahmed and Xing, 2008] Ahmed, A. and Xing, E. P.
(2008). Dynamic non-parametric mixture models and
the recurrent Chinese restaurant process: with applica-
tions to evolutionary clustering. In SDM, pages 219?
230.
[Beeferman et al, 1999] Beeferman, D., Berger, A., and
Lafferty, J. (1999). Statistical models for text segmen-
tation. Mach. Learn., 34:177?210.
[Blei and Lafferty, 2009] Blei, D. M. and Lafferty, J.
(2009). Text Mining: Theory and Applications, chapter
Topic Models. Taylor and Francis, London.
[Boyd-Graber and Blei, 2008] Boyd-Graber, J. and Blei,
D. M. (2008). Syntactic topic models. In Proceedings
of Advances in Neural Information Processing Systems.
[Boydstun et al, 2011] Boydstun, A. E., Phillips, C., and
Glazier, R. A. (2011). It?s the economy again, stupid:
Agenda control in the 2008 presidential debates. Forth-
coming.
[Chang and Blei, 2009] Chang, J. and Blei, D. M. (2009).
Relational topic models for document networks. In
Proceedings of Artificial Intelligence and Statistics.
[Chang et al, 2009] Chang, J., Boyd-Graber, J., Wang, C.,
Gerrish, S., and Blei, D. M. (2009). Reading tea leaves:
How humans interpret topic models. In Neural Infor-
mation Processing Systems.
[Du et al, 2010] Du, L., Buntine, W., and Jin, H. (2010).
Sequential latent dirichlet alocation: Discover underly-
ing topic structures within a document. In Data Mining
(ICDM), 2010 IEEE 10th International Conference on,
pages 148 ?157.
[Ehlen et al, 2007] Ehlen, P., Purver, M., and Niekrasz, J.
(2007). A meeting browser that learns. In In: Pro-
ceedings of the AAAI Spring Symposium on Interaction
Challenges for Intelligent Assistants.
[Eisenstein and Barzilay, 2008] Eisenstein, J. and Barzi-
lay, R. (2008). Bayesian unsupervised topic segmenta-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, Proceedings
of Emperical Methods in Natural Language Processing.
[Eisenstein et al, 2010] Eisenstein, J., O?Connor, B.,
Smith, N. A., and Xing, E. P. (2010). A latent variable
model for geographic lexical variation. In EMNLP?10,
pages 1277?1287.
[Ferguson, 1973] Ferguson, T. S. (1973). A Bayesian anal-
ysis of some nonparametric problems. The Annals of
Statistics, 1(2):209?230.
[Fox et al, 2008] Fox, E. B., Sudderth, E. B., Jordan, M. I.,
and Willsky, A. S. (2008). An hdp-hmm for systems
with state persistence. In Proceedings of International
Conference of Machine Learning.
[Galley et al, 2003] Galley, M., McKeown, K., Fosler-
Lussier, E., and Jing, H. (2003). Discourse segmenta-
tion of multi-party conversation. In Proceedings of the
Association for Computational Linguistics.
[Georgescul et al, 2006] Georgescul, M., Clark, A., and
Armstrong, S. (2006). Word distributions for thematic
segmentation in a support vector machine approach.
In Conference on Computational Natural Language
Learning.
[Gerrish and Blei, 2010] Gerrish, S. and Blei, D. M.
(2010). A language-based approach to measuring schol-
arly impact. In Proceedings of International Confer-
ence of Machine Learning.
[Griffiths et al, 2005] Griffiths, T. L., Steyvers, M., Blei,
D. M., and Tenenbaum, J. B. (2005). Integrating topics
and syntax. In Proceedings of Advances in Neural
Information Processing Systems.
[Grimmer, 2009] Grimmer, J. (2009). A Bayesian Hier-
archical Topic Model for Political Texts: Measuring
Expressed Agendas in Senate Press Releases. Political
Analysis, 18:1?35.
[Gruber et al, 2007] Gruber, A., Rosen-Zvi, M., and
Weiss, Y. (2007). Hidden topic Markov models. In
Artificial Intelligence and Statistics.
[Hawes et al, 2009] Hawes, T., Lin, J., and Resnik, P.
(2009). Elements of a computational model for multi-
party discourse: The turn-taking behavior of Supreme
Court justices. Journal of the American Society for In-
formation Science and Technology, 60(8):1607?1615.
[Hearst, 1997] Hearst, M. A. (1997). TextTiling: Segment-
ing text into multi-paragraph subtopic passages. Com-
putational Linguistics, 23(1):33?64.
86
[Hsueh et al, 2006] Hsueh, P.-y., Moore, J. D., and Renals,
S. (2006). Automatic segmentation of multiparty dia-
logue. In Proceedings of the European Chapter of the
Association for Computational Linguistics.
[Ireland et al, 2011] Ireland, M. E., Slatcher, R. B., East-
wick, P. W., Scissors, L. E., Finkel, E. J., and Pen-
nebaker, J. W. (2011). Language style matching pre-
dicts relationship initiation and stability. Psychological
Science, 22(1):39?44.
[Janin et al, 2003] Janin, A., Baron, D., Edwards, J., El-
lis, D., Gelbart, D., Morgan, N., Peskin, B., Pfau, T.,
Shriberg, E., Stolcke, A., and Wooters, C. (2003). The
ICSI meeting corpus. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing.
[Johnson, 2010] Johnson, M. (2010). PCFGs, topic mod-
els, adaptor grammars and learning topical collocations
and the structure of proper names. In Proceedings of
the Association for Computational Linguistics.
[Morris and Hirst, 1991] Morris, J. and Hirst, G. (1991).
Lexical cohesion computed by thesaural relations as
an indicator of the structure of text. Computational
Linguistics, 17:21?48.
[Mu?ller and Quintana, 2004] Mu?ller, P. and Quintana,
F. A. (2004). Nonparametric Bayesian data analysis.
Statistical Science, 19(1):95?110.
[Murray et al, 2005] Murray, G., Renals, S., and Carletta,
J. (2005). Extractive summarization of meeting record-
ings. In European Conference on Speech Communica-
tion and Technology.
[Neal, 2000] Neal, R. M. (2000). Markov chain sampling
methods for Dirichlet process mixture models. Journal
of Computational and Graphical Statistics, 9(2):249?
265.
[Neal, 2003] Neal, R. M. (2003). Slice sampling. Annals
of Statistics, 31:705?767.
[Olney and Cai, 2005] Olney, A. and Cai, Z. (2005). An
orthonormal basis for topic segmentation in tutorial di-
alogue. In Proceedings of the Human Language Tech-
nology Conference.
[Paul and Girju, 2010] Paul, M. and Girju, R. (2010). A
two-dimensional topic-aspect model for discovering
multi-faceted topics. In Association for the Advance-
ment of Artificial Intelligence.
[Pele and Werman, 2009] Pele, O. and Werman, M.
(2009). Fast and robust earth mover?s distances. In
International Conference on Computer Vision.
[Pevzner and Hearst, 2002] Pevzner, L. and Hearst, M. A.
(2002). A critique and improvement of an evaluation
metric for text segmentation. Computational Linguis-
tics, 28.
[Purver, 2011] Purver, M. (2011). Topic segmentation. In
Tur, G. and de Mori, R., editors, Spoken Language
Understanding: Systems for Extracting Semantic Infor-
mation from Speech, pages 291?317. Wiley.
[Purver et al, 2006] Purver, M., Ko?rding, K., Griffiths,
T. L., and Tenenbaum, J. (2006). Unsupervised topic
modelling for multi-party spoken discourse. In Pro-
ceedings of the Association for Computational Linguis-
tics.
[Resnik and Hardisty, 2010] Resnik, P. and Hardisty, E.
(2010). Gibbs sampling for the uninitiated. Technical
Report UMIACS-TR-2010-04, University of Maryland.
http://www.lib.umd.edu/drum/handle/1903/10058.
[Reynar, 1998] Reynar, J. C. (1998). Topic Segmentation:
Algorithms and Applications. PhD thesis, University of
Pennsylvania.
[Rosen-Zvi et al, 2004] Rosen-Zvi, M., Griffiths, T. L.,
Steyvers, M., and Smyth, P. (2004). The author-topic
model for authors and documents. In Proceedings of
Uncertainty in Artificial Intelligence.
[Rubner et al, 2000] Rubner, Y., Tomasi, C., and Guibas,
L. J. (2000). The earth mover?s distance as a metric
for image retrieval. International Journal of Computer
Vision, 40:99?121.
[Teh et al, 2006] Teh, Y. W., Jordan, M. I., Beal, M. J.,
and Blei, D. M. (2006). Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Association,
101(476):1566?1581.
[Thomas et al, 2006] Thomas, M., Pang, B., and Lee, L.
(2006). Get out the vote: Determining support or op-
position from Congressional floor-debate transcripts.
In Proceedings of Emperical Methods in Natural Lan-
guage Processing.
[Tur et al, 2010] Tur, G., Stolcke, A., Voss, L., Peters, S.,
Hakkani-Tu?r, D., Dowding, J., Favre, B., Ferna?ndez,
R., Frampton, M., Frandsen, M., Frederickson, C., Gra-
ciarena, M., Kintzing, D., Leveque, K., Mason, S.,
Niekrasz, J., Purver, M., Riedhammer, K., Shriberg, E.,
Tien, J., Vergyri, D., and Yang, F. (2010). The CALO
meeting assistant system. Trans. Audio, Speech and
Lang. Proc., 18:1601?1611.
[Wallach, 2006] Wallach, H. M. (2006). Topic modeling:
Beyond bag-of-words. In Proceedings of International
Conference of Machine Learning.
[Wallach, 2008] Wallach, H. M. (2008). Structured Topic
Models for Language. PhD thesis, University of Cam-
bridge.
[Wang et al, 2008] Wang, C., Blei, D. M., and Heckerman,
D. (2008). Continuous time dynamic topic models. In
Proceedings of Uncertainty in Artificial Intelligence.
[Wang and McCallum, 2006] Wang, X. and McCallum, A.
(2006). Topics over time: a non-Markov continuous-
time model of topical trends. In Knowledge Discovery
and Data Mining, Knowledge Discovery and Data Min-
ing.
87
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 115?119,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Topic Models for Dynamic Translation Model Adaptation
Vladimir Eidelman
Computer Science
and UMIACS
University of Maryland
College Park, MD
vlad@umiacs.umd.edu
Jordan Boyd-Graber
iSchool
and UMIACS
University of Maryland
College Park, MD
jbg@umiacs.umd.edu
Philip Resnik
Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
We propose an approach that biases machine
translation systems toward relevant transla-
tions based on topic-specific contexts, where
topics are induced in an unsupervised way
using topic models; this can be thought of
as inducing subcorpora for adaptation with-
out any human annotation. We use these topic
distributions to compute topic-dependent lex-
ical weighting probabilities and directly in-
corporate them into our translation model as
features. Conditioning lexical probabilities
on the topic biases translations toward topic-
relevant output, resulting in significant im-
provements of up to 1 BLEU and 3 TER on
Chinese to English translation over a strong
baseline.
1 Introduction
The performance of a statistical machine translation
(SMT) system on a translation task depends largely
on the suitability of the available parallel training
data. Domains (e.g., newswire vs. blogs) may vary
widely in their lexical choices and stylistic prefer-
ences, and what may be preferable in a general set-
ting, or in one domain, is not necessarily preferable
in another domain. Indeed, sometimes the domain
can change the meaning of a phrase entirely.
In a food related context, the Chinese sentence
????? ? (?fe?nsi? he?nduo??) would mean ?They
have a lot of vermicelli?; however, in an informal In-
ternet conversation, this sentence would mean ?They
have a lot of fans?. Without the broader context, it
is impossible to determine the correct translation in
otherwise identical sentences.
This problem has led to a substantial amount of
recent work in trying to bias, or adapt, the transla-
tion model (TM) toward particular domains of inter-
est (Axelrod et al, 2011; Foster et al, 2010; Snover
et al, 2008).1 The intuition behind TM adapta-
tion is to increase the likelihood of selecting rele-
vant phrases for translation. Matsoukas et al (2009)
introduced assigning a pair of binary features to
each training sentence, indicating sentences? genre
and collection as a way to capture domains. They
then learn a mapping from these features to sen-
tence weights, use the sentence weights to bias the
model probability estimates and subsequently learn
the model weights. As sentence weights were found
to be most beneficial for lexical weighting, Chiang
et al (2011) extends the same notion of condition-
ing on provenance (i.e., the origin of the text) by re-
moving the separate mapping step, directly optimiz-
ing the weight of the genre and collection features
by computing a separate word translation table for
each feature, estimated from only those sentences
that comprise that genre or collection.
The common thread throughout prior work is the
concept of a domain. A domain is typically a hard
constraint that is externally imposed and hand la-
beled, such as genre or corpus collection. For ex-
ample, a sentence either comes from newswire, or
weblog, but not both. However, this poses sev-
eral problems. First, since a sentence contributes its
counts only to the translation table for the source it
came from, many word pairs will be unobserved for
a given table. This sparsity requires smoothing. Sec-
ond, we may not know the (sub)corpora our training
1Language model adaptation is also prevalent but is not the
focus of this work.
115
data come from; and even if we do, ?subcorpus? may
not be the most useful notion of domain for better
translations.
We take a finer-grained, flexible, unsupervised ap-
proach for lexical weighting by domain. We induce
unsupervised domains from large corpora, and we
incorporate soft, probabilistic domain membership
into a translation model. Unsupervised modeling of
the training data produces naturally occurring sub-
corpora, generalizing beyond corpus and genre. De-
pending on the model used to select subcorpora, we
can bias our translation toward any arbitrary distinc-
tion. This reduces the problem to identifying what
automatically defined subsets of the training corpus
may be beneficial for translation.
In this work, we consider the underlying latent
topics of the documents (Blei et al, 2003). Topic
modeling has received some use in SMT, for in-
stance Bilingual LSA adaptation (Tam et al, 2007),
and the BiTAM model (Zhao and Xing, 2006),
which uses a bilingual topic model for learning
alignment. In our case, by building a topic distri-
bution for the source side of the training data, we
abstract the notion of domain to include automati-
cally derived subcorpora with probabilistic member-
ship. This topic model infers the topic distribution
of a test set and biases sentence translations to ap-
propriate topics. We accomplish this by introduc-
ing topic dependent lexical probabilities directly as
features in the translation model, and interpolating
them log-linearly with our other features, thus allow-
ing us to discriminatively optimize their weights on
an arbitrary objective function. Incorporating these
features into our hierarchical phrase-based transla-
tion system significantly improved translation per-
formance, by up to 1 BLEU and 3 TER over a strong
Chinese to English baseline.
2 Model Description
Lexical Weighting Lexical weighting features es-
timate the quality of a phrase pair by combining
the lexical translation probabilities of the words in
the phrase2 (Koehn et al, 2003). Lexical condi-
tional probabilities p(e|f) are obtained with maxi-
mum likelihood estimates from relative frequencies
2For hierarchical systems, these correspond to translation
rules.
c(f, e)/
?
e c(f, e) . Phrase pair probabilities p(e|f)
are computed from these as described in Koehn et
al. (2003).
Chiang et al (2011) showed that is it benefi-
cial to condition the lexical weighting features on
provenance by assigning each sentence pair a set
of features, fs(e|f), one for each domain s, which
compute a new word translation table ps(e|f) esti-
mated from only those sentences which belong to s:
cs(f, e)/
?
e cs(f, e) , where cs(?) is the number of
occurrences of the word pair in s.
Topic Modeling for MT We extend provenance
to cover a set of automatically generated topics zn.
Given a parallel training corpus T composed of doc-
uments di, we build a source side topic model over
T , which provides a topic distribution p(zn|di) for
zn = {1, . . . ,K} over each document, using Latent
Dirichlet Allocation (LDA) (Blei et al, 2003). Then,
we assign p(zn|di) to be the topic distribution for
every sentence xj ? di, thus enforcing topic sharing
across sentence pairs in the same document instead
of treating them as unrelated. Computing the topic
distribution over a document and assigning it to the
sentences serves to tie the sentences together in the
document context.
To obtain the lexical probability conditioned on
topic distribution, we first compute the expected
count ezn(e, f) of a word pair under topic zn:
ezn(e, f) =
?
di?T
p(zn|di)
?
xj?di
cj(e, f) (1)
where cj(?) denotes the number of occurrences of
the word pair in sentence xj , and then compute:
pzn(e|f) =
ezn(e, f)?
e ezn(e, f)
(2)
Thus, we will introduce 2?K new word trans-
lation tables, one for each pzn(e|f) and pzn(f |e),
and as many new corresponding features fzn(e|f),
fzn(f |e). The actual feature values we compute will
depend on the topic distribution of the document we
are translating. For a test document V , we infer
topic assignments on V , p(zn|V ), keeping the topics
found from T fixed. The feature value then becomes
fzn(e|f) = ? log
{
pzn(e|f) ? p(zn|V )
}
, a combi-
nation of the topic dependent lexical weight and the
116
topic distribution of the sentence from which we are
extracting the phrase. To optimize the weights of
these features we combine them in our linear model
with the other features when computing the model
score for each phrase pair3:
?
p
?php(e, f)
? ?? ?
unadapted features
+
?
zn
?znfzn(e|f)
? ?? ?
adapted features
(3)
Combining the topic conditioned word translation
table pzn(e|f) computed from the training corpus
with the topic distribution p(zn|V ) of the test sen-
tence being translated provides a probability on how
relevant that translation table is to the sentence. This
allows us to bias the translation toward the topic of
the sentence. For example, if topic k is dominant in
T , pk(e|f) may be quite large, but if p(k|V ) is very
small, then we should steer away from this phrase
pair and select a competing phrase pair which may
have a lower probability in T , but which is more rel-
evant to the test sentence at hand.
In many cases, document delineations may not be
readily available for the training corpus. Further-
more, a document may be too broad, covering too
many disparate topics, to effectively bias the weights
on a phrase level. For this case, we also propose a
local LDA model (LTM), which treats each sentence
as a separate document.
While Chiang et al (2011) has to explicitly
smooth the resulting ps(e|f), since many word pairs
will be unseen for a given domain s, we are already
performing an implicit form of smoothing (when
computing the expected counts), since each docu-
ment has a distribution over all topics, and therefore
we have some probability of observing each word
pair in every topic.
Feature Representation After obtaining the topic
conditional features, there are two ways to present
them to the model. They could answer the question
F1: What is the probability under topic 1, topic 2,
etc., or F2: What is the probability under the most
probable topic, second most, etc.
A model using F1 learns whether a specific topic
is useful for translation, i.e., feature f1 would be
f1 := pz=1(e|f) ? p(z = 1|V ). With F2, we
3The unadapted lexical weight p(e|f) is included in the
model features.
are learning how useful knowledge of the topic dis-
tribution is, i.e., f1 := p(argmaxzn (p(zn|V ))(e|f) ?
p(argmaxzn(p(zn|V ))|V ).
Using F1, if we restrict our topics to have a one-
to-one mapping with genre/collection4 we see that
our method fully recovers Chiang (2011).
F1 is appropriate for cross-domain adaptation
when we have advance knowledge that the distribu-
tion of the tuning data will match the test data, as in
Chiang (2011), where they tune and test on web. In
general, we may not know what our data will be, so
this will overfit the tuning set.
F2, however, is intuitively what we want, since
we do not want to bias our system toward a spe-
cific distribution, but rather learn to utilize informa-
tion from any topic distribution if it helps us cre-
ate topic relevant translations. F2 is useful for dy-
namic adaptation, where the adapted feature weight
changes based on the source sentence.
Thus, F2 is the approach we use in our work,
which allows us to tune our system weights toward
having topic information be useful, not toward a spe-
cific distribution.
3 Experiments
Setup To evaluate our approach, we performed ex-
periments on Chinese to English MT in two set-
tings. First, we use the FBIS corpus as our training
bitext. Since FBIS has document delineations, we
compare local topic modeling (LTM) with model-
ing at the document level (GTM). The second setting
uses the non-UN and non-HK Hansards portions of
the NIST training corpora with LTM only. Table 1
summarizes the data statistics. For both settings,
the data were lowercased, tokenized and aligned us-
ing GIZA++ (Och and Ney, 2003) to obtain bidi-
rectional alignments, which were symmetrized us-
ing the grow-diag-final-and method (Koehn
et al, 2003). The Chinese data were segmented us-
ing the Stanford segmenter. We trained a trigram
LM on the English side of the corpus with an addi-
tional 150M words randomly selected from the non-
NYT and non-LAT portions of the Gigaword v4 cor-
pus using modified Kneser-Ney smoothing (Chen
and Goodman, 1996). We used cdec (Dyer et al,
4By having as many topics as genres/collections and setting
p(zn|di) to 1 for every sentence in the collection and 0 to ev-
erything else.
117
Corpus Sentences Tokens
En Zh
FBIS 269K 10.3M 7.9M
NIST 1.6M 44.4M 40.4M
Table 1: Corpus statistics
2010) as our decoder, and tuned the parameters of
the system to optimize BLEU (Papineni et al, 2002)
on the NIST MT06 tuning corpus using the Mar-
gin Infused Relaxed Algorithm (MIRA) (Crammer
et al, 2006; Eidelman, 2012). Topic modeling was
performed with Mallet (Mccallum, 2002), a stan-
dard implementation of LDA, using a Chinese sto-
plist and setting the per-document Dirichlet parame-
ter ? = 0.01. This setting of was chosen to encour-
age sparse topic assignments, which make induced
subdomains consistent within a document.
Results Results for both settings are shown in Ta-
ble 2. GTM models the latent topics at the document
level, while LTM models each sentence as a separate
document. To evaluate the effect topic granularity
would have on translation, we varied the number of
latent topics in each model to be 5, 10, and 20. On
FBIS, we can see that both models achieve moderate
but consistent gains over the baseline on both BLEU
and TER. The best model, LTM-10, achieves a gain
of about 0.5 and 0.6 BLEU and 2 TER. Although the
performance on BLEU for both the 20 topic models
LTM-20 and GTM-20 is suboptimal, the TER im-
provement is better. Interestingly, the difference in
translation quality between capturing document co-
herence in GTM and modeling purely on the sen-
tence level is not substantial.5 In fact, the opposite
is true, with the LTM models achieving better per-
formance.6
On the NIST corpus, LTM-10 again achieves the
best gain of approximately 1 BLEU and up to 3 TER.
LTM performs on par with or better than GTM, and
provides significant gains even in the NIST data set-
ting, showing that this method can be effectively ap-
plied directly on the sentence level to large training
5An avenue of future work would condition the sentence
topic distribution on a document distribution over topics (Teh
et al, 2006).
6As an empirical validation of our earlier intuition regarding
feature representation, presenting the features in the form of F1
caused the performance to remain virtually unchanged from the
baseline model.
Model MT03 MT05
?BLEU ?TER ?BLEU ?TER
BL 28.72 65.96 27.71 67.58
GTM-5 28.95ns 65.45 27.98ns 67.38ns
GTM-10 29.22 64.47 28.19 66.15
GTM-20 29.19 63.41 28.00ns 64.89
LTM-5 29.23 64.57 28.19 66.30
LTM-10 29.29 63.98 28.18 65.56
LTM-20 29.09ns 63.57 27.90ns 65.17
Model MT03 MT05
?BLEU ?TER ?BLEU ?TER
BL 34.31 61.14 30.63 65.10
MERT 34.60 60.66 30.53 64.56
LTM-5 35.21 59.48 31.47 62.34
LTM-10 35.32 59.16 31.56 62.01
LTM-20 33.90ns 60.89ns 30.12ns 63.87
Table 2: Performance using FBIS training corpus (top)
and NIST corpus (bottom). Improvements are significant
at the p <0.05 level, except where indicated (ns).
corpora which have no document markings. De-
pending on the diversity of training corpus, a vary-
ing number of underlying topics may be appropriate.
However, in both settings, 10 topics performed best.
4 Discussion and Conclusion
Applying SMT to new domains requires techniques
to inform our algorithms how best to adapt. This pa-
per extended the usual notion of domains to finer-
grained topic distributions induced in an unsuper-
vised fashion. We show that incorporating lexi-
cal weighting features conditioned on soft domain
membership directly into our model is an effective
strategy for dynamically biasing SMT towards rele-
vant translations, as evidenced by significant perfor-
mance gains. This method presents several advan-
tages over existing approaches. We can construct
a topic model once on the training data, and use
it infer topics on any test set to adapt the transla-
tion model. We can also incorporate large quanti-
ties of additional data (whether parallel or not) in
the source language to infer better topics without re-
lying on collection or genre annotations. Multilin-
gual topic models (Boyd-Graber and Resnik, 2010)
would provide a technique to use data from multiple
languages to ensure consistent topics.
118
Acknowledgments
Vladimir Eidelman is supported by a National De-
fense Science and Engineering Graduate Fellow-
ship. This work was also supported in part by
NSF grant #1018625, ARL Cooperative Agree-
ment W911NF-09-2-0072, and by the BOLT and
GALE programs of the Defense Advanced Research
Projects Agency, Contracts HR0011-12-C-0015 and
HR0011-06-2-001, respectively. Any opinions, find-
ings, conclusions, or recommendations expressed
are the authors? and do not necessarily reflect those
of the sponsors.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of Emperical Methods in Natural
Language Processing.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent Dirichlet Allocation.
Journal of Machine Learning Research, 3:2003.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
sentiment analysis across languages: Multilingual su-
pervised latent Dirichlet alocation. In Proceedings of
Emperical Methods in Natural Language Processing.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting of
the Association for Computational Linguistics, pages
310?318.
David Chiang, Steve DeNeefe, and Michael Pust. 2011.
Two easy improvements to lexical weighting. In Pro-
ceedings of the Human Language Technology Confer-
ence.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL System Demonstrations.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statistical
Machine Translation.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of Emperical Methods in Natural Language Process-
ing.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, Stroudsburg, PA, USA.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of Em-
perical Methods in Natural Language Processing.
A. K. Mccallum. 2002. MALLET: A Machine Learning
for Language Toolkit.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. In
Computational Linguistics, volume 29(21), pages 19?
51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of the As-
sociation for Computational Linguistics, pages 311?
318.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation us-
ing comparable corpora. In Proceedings of Emperical
Methods in Natural Language Processing.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, 21(4):187?207.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the Association for Computational Lin-
guistics.
119
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116?1126,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Online Relative Margin Maximization for Statistical Machine Translation
Vladimir Eidelman
Computer Science
and UMIACS
University of Maryland
College Park, MD
vlad@umiacs.umd.edu
Yuval Marton
Microsoft
City Center Plaza
Bellevue, WA
yuvalmarton@gmail.com
Philip Resnik
Linguistics
and UMIACS
University of Maryland
College Park, MD
resnik@umd.edu
Abstract
Recent advances in large-margin learning
have shown that better generalization can
be achieved by incorporating higher order
information into the optimization, such as
the spread of the data. However, these so-
lutions are impractical in complex struc-
tured prediction problems such as statis-
tical machine translation. We present an
online gradient-based algorithm for rela-
tive margin maximization, which bounds
the spread of the projected data while max-
imizing the margin. We evaluate our op-
timizer on Chinese-English and Arabic-
English translation tasks, each with small
and large feature sets, and show that our
learner is able to achieve significant im-
provements of 1.2-2 BLEU and 1.7-4.3
TER on average over state-of-the-art opti-
mizers with the large feature set.
1 Introduction
The desire to incorporate high-dimensional sparse
feature representations into statistical machine
translation (SMT) models has driven recent re-
search away from Minimum Error Rate Training
(MERT) (Och, 2003), and toward other discrim-
inative methods that can optimize more features.
Examples include minimum risk (Smith and Eis-
ner, 2006), pairwise ranking (PRO) (Hopkins and
May, 2011), RAMPION (Gimpel and Smith, 2012),
and variations of the margin-infused relaxation al-
gorithm (MIRA) (Watanabe et al, 2007; Chiang et
al., 2008; Cherry and Foster, 2012). While the ob-
jective function and optimization method vary for
each optimizer, they can all be broadly described
as learning a linear model, or parameter vector w,
which is used to score alternative translation hy-
potheses.
In every SMT system, and in machine learn-
ing in general, the goal of learning is to find a
model that generalizes well, i.e. one that will yield
good translations for previously unseen sentences.
However, as the dimension of the feature space in-
creases, generalization becomes increasingly diffi-
cult. Since only a small portion of all (sparse) fea-
tures may be observed in a relatively small fixed
set of instances during tuning, we are prone to
overfit the training data. An alternative approach
for solving this problem is estimating discrimina-
tive feature weights directly on the training bi-
text (Tillmann and Zhang, 2006; Blunsom et al,
2008; Simianer et al, 2012), which is usually sub-
stantially larger than the tuning set, but this is com-
plementary to our goal here of better generaliza-
tion given a fixed size tuning set.
In order to achieve that goal, we need to care-
fully choose what objective to optimize, and how
to perform parameter estimation of w for this ob-
jective. We focus on large-margin methods such
as SVM (Joachims, 1998) and passive-aggressive
algorithms such as MIRA. Intuitively these seek
a w such that the separating distance in geomet-
ric space of two hypotheses is at least as large as
the cost incurred by selecting the incorrect one.
This criterion performs well in practice at find-
ing a linear separator in high-dimensional feature
spaces (Tsochantaridis et al, 2004; Crammer et
al., 2006).
Now, recent advances in machine learning have
shown that the generalization ability of these
learners can be improved by utilizing second or-
der information, as in the Second Order Percep-
tron (Cesa-Bianchi et al, 2005), Gaussian Margin
Machines (Crammer et al, 2009b), confidence-
weighted learning (Dredze and Crammer, 2008),
AROW (Crammer et al, 2009a; Chiang, 2012)
and Relative Margin Machines (RMM) (Shiv-
aswamy and Jebara, 2009b). The latter, RMM,
was introduced as an effective and less computa-
tionally expensive way to incorporate the spread
of the data ? second order information about the
1116
distance between hypotheses when projected onto
the line defined by the weight vector w.
Unfortunately, not all advances in machine
learning are easy to apply to structured prediction
problems such as SMT; the latter often involve la-
tent variables and surrogate references, resulting
in loss functions that have not been well explored
in machine learning (Mcallester and Keshet, 2011;
Gimpel and Smith, 2012). Although Shivaswamy
and Jebara extended RMM to handle sequen-
tial structured prediction (Shivaswamy and Jebara,
2009a), their batch approach to quadratic opti-
mization, using existing off-the-shelf QP solvers,
does not provide a practical solution: as Taskar et
al. (2006) observe, ?off-the-shelf QP solvers tend
to scale poorly with problem and training sam-
ple size? for structured prediction problems.. This
motivates an online gradient-based optimization
approach?an approach that is particularly attrac-
tive because its simple update is well suited for ef-
ficiently processing structured objects with sparse
features (Crammer et al, 2012).
The contributions of this paper include (1) in-
troduction of a loss function for structured RMM
in the SMT setting, with surrogate reference trans-
lations and latent variables; (2) an online gradient-
based solver, RM, with a closed-form parameter
update to optimize the relative margin loss; and
(3) an efficient implementation that integrates well
with the open source cdec SMT system (Dyer et
al., 2010).1 In addition, (4) as our solution is not
dependent on any specific QP solver, it can be
easily incorporated into practically any gradient-
based learning algorithm.
After background discussion on learning in
SMT (?2), we introduce a novel online learning al-
gorithm for relative margin maximization suitable
for SMT (?3). First, we introduce RMM (?3.1) and
propose a latent structured relative margin objec-
tive which incorporates cost-augmented hypothe-
sis selection and latent variables. Then, we de-
rive a simple closed-form online update necessary
to create a large margin solution while simulta-
neously bounding the spread of the projection of
the data (?3.2). Chinese-English translation exper-
iments show that our algorithm, RM, significantly
outperforms strong state-of-the-art optimizers, in
both a basic feature setting and high-dimensional
(sparse) feature space (?4). Additional Arabic-
English experiments further validate these results,
1https://github.com/veidel/cdec
even where previously MERT was shown to be ad-
vantageous (?5). Finally, we discuss the spread
and other key issues of RM (?6), and conclude
with discussion of future work (?7).
2 Learning in SMT
Given an input sentence in the source language
x ? X , we want to produce a translation y ? Y(x)
using a linear model parameterized by a weight
vector w:
(y?, d?) = arg max
(y,d)?Y(x),D(x)
w>f(x, y, d)
where w>f(x, y, d) is the weighted feature scor-
ing function, hereafter s(x, y, d), and Y(x) is the
space of possible translations of x. While many
derivations d ? D(x) can produce a given transla-
tion, we are only able to observe y; thus we model
d as a latent variable. Although our models are
actually defined over derivations, they are always
paired with translations, so our feature function
f(x, y, d) is defined over derivation?translation
pairs.2 The learning goal is then to estimate w.
The instability of MERT in larger feature
sets (Foster and Kuhn, 2009; Hopkins and May,
2011), has motivated many alternative tuning
methods for SMT. These include strategies based
on batch log-linear models (Tillmann and Zhang,
2006; Blunsom et al, 2008), as well as the in-
troduction of online linear models (Liang et al,
2006a; Arun and Koehn, 2007).
Recent batch optimizers, PRO and RAMPION,
and Batch-MIRA (Cherry and Foster, 2012), have
been partly motivated by existing MT infrastruc-
tures, as they iterate between decoding the entire
tuning set and optimizing the parameters. PRO
considers tuning a classification problem and em-
ploys a binary classifier to rank pairs of outputs.
RAMPION aims to address the disconnect between
MT and machine learning by optimizing a struc-
tured ramp loss with a concave-convex procedure.
2.1 Large-Margin Learning
Online large-margin algorithms, such as MIRA,
have also gained prominence in SMT, thanks to
their ability to learn models in high-dimensional
feature spaces (Watanabe et al, 2007; Chiang et
al., 2009). The usual presentation of MIRA?s opti-
mization problem is given as a quadratic program:
2We may omit d in some equations for clarity.
1117
wt+1 = arg min
w
1
2 ||w ?wt||
2 + C?i
s.t. s(xi, yi, d)? s(xi, y?, d) ? ?i(y?)? ?i
(1)
where y? is the single most violated constraint, the
cost ?i(y) is computed using an external measure
of quality, such as 1-BLEU(yi, y), and a slack vari-
able ?i is introduced to allow for non-separable
instances. C acts as a regularization parameter,
trading off between margin maximization and con-
straint violations.
While solving the optimization problem relies
on computing the margin between the correct out-
put yi, and y?, in SMT our decoder is often inca-
pable of producing the reference translation, i.e.
yi /? Y(xi). We must instead resort to selecting a
surrogate reference, y+ ? Y(xi). This issue has
recently received considerable attention (Liang
et al, 2006a; Eidelman, 2012; Chiang, 2012),
with preference given to surrogate references ob-
tained through cost-diminished hypothesis selec-
tion. Thus, y+ is selected based on a combination
of model score and error metric from the k-best
list produced by our current model. A similar se-
lection is made for the cost-augmented hypothesis
y? ? Y(xi):
(y+, d+)? arg max
(y,d)?Y(xi),D(xi)
s(xi, y, d)??i(y)
(y?, d?)? arg max
(y,d)?Y(xi),D(xi)
s(xi, y, d) + ?i(y)
In this setting, the optimization problem be-
comes:
wt+1 = arg min
w
1
2 ||w ?wt||
2 + C?i
s.t. ?s(xi, y+, y?) ? ?i(y?)??i(y+)? ?i
(2)
where ?s(xi, y+, y?)=s(xi, y+, d+)-s(xi, y?, d?)
This leads to a variant of the structured ramp
loss to be optimized:
` =
? max
(y+,d+)?Y(xi),D(xi)
(s(xi, y+, d+)??i(y+)
)
+ max
(y?,d?)?Y(xi),D(xi)
(s(xi, y?, d?) + ?i(y?)
)
(3)
The passive-aggressive update (Crammer et al,
2006), which is used to solve this problem, up-
dates w on each round such that the score of the
correct hypothesis y+ is greater than the score of
the incorrect y? by a margin at least as large as the
cost incurred by predicting the incorrect hypothe-
sis, while keeping the change to w small.
 
(a)
 
(b)
Figure 1: (a) RM and large margin solution comparison and
(b) the spread of the projections given by each. RM and large
margin solutions are shown with a darker dotted line and a
darker solid line, respectively.
3 The Relative Margin Machine in SMT
3.1 Relative Margin Machine
The margin, the distance between the correct
hypothesis and incorrect one, is defined by
s(xi, y+, d+) and s(xi, y?, d?). It is maxi-
mized by minimizing the norm in SVM, or
analogously, the proximity constraint in MIRA:
arg minw 12 ||w ?wt||2. However, theoretical re-sults supporting large-margin learning, such as the
VC-dimension (Vapnik, 1995) or the Rademacher
bound (Bartlett and Mendelson, 2003) consider
measures of complexity, in addition to the empir-
ical performance, when describing future predic-
tive ability. The measures of complexity usually
take the form of some value on the radius of the
data, such as the ratio of the radius of the data to
the margin (Shivaswamy and Jebara, 2009a). As
radius is a way of measuring spread in any pro-
jection direction, here we will specifically be in-
terested in the the spread of the data as measured
after the projection defined by the learned model
w.
More formally, the spread is the dis-
tance between y+, and the worst candidate
(yw, dw)? arg min(y,d)?Y(xi),D(xi) s(xi, y, d),after projecting both onto the line defined by the
weight vector w. For each y?, this projection is
conveniently given by s(xi, y?, d), thus the spread
is calculated as ?s(xi, y+, yw).
RMM was introduced as a generalization over
SVM that incorporates both the margin constraint
1118
and information regarding the spread of the data.
The relative margin is the ratio of the absolute,
or maximum margin, to the spread of the pro-
jected data. Thus, the RMM learns a large mar-
gin solution relative to the spread of the data, or
in other words, creates a max margin while si-
multaneously bounding the spread of the projected
data. As a concrete example, consider the plot
shown in Figure 1(a), with hypotheses represented
by two-dimensional feature vectors. The point
marked with a circle in the upper right represents
f(xi, y+), while all other squares represent alter-
native incorrect hypotheses f(xi, y?). The large
margin decision boundary is shown with a darker
solid line, while the relative margin solution is
shown with a darker dotted line. The lighter lines
parallel to each define the margins, with the square
at the intersection being f(xi, y?). The bottom
portion of Figure 1(b) presents an alternative view
of each solution, showing the projections of the
hypotheses given the learned model of each. No-
tice that with a large margin solution, although the
distance between y+ and y? is greater, the points
are highly spread, extending far to the left of the
decision boundary.
In contrast, with a relative margin, although
we have a smaller absolute margin, the spread is
smaller, all points being within a smaller distance 
of the decision boundary. The higher the spread of
the projection, the higher the variance of the pro-
jected points, and the greater the likelihood that
we will mislabel a new instance, since the high
variance projections may cross the learned deci-
sion boundary. In higher dimensions, accounting
for the spread becomes even more crucial, as will
be discussed in Section 6.3
Although RMM is theoretically well-founded
and improves practical performance over large-
margin learning in the settings where it was intro-
duced, it is unsuitable for most complex structured
prediction in NLP. Nonetheless, since structured
RMM is a generalization of Structured SVM,
which shares its underlying objective with MIRA,
our intuition is that SMT should be able to benefit
as well. But to take advantage of the second-order
information RMM utilizes for increased general-
izability in SMT, we need a computationally effi-
3The motivation of confidence-weighted estima-
tion (Dredze and Crammer, 2008) and AROW (Crammer
et al, 2009a) is related in spirit. They use second-order
information in the form of a distribution over weights to
change the maximum margin solution.
cient optimization procedure that does not require
batch training or an off-the-shelf QP solver.
3.2 RM Algorithm
We address the above-mentioned limitations by in-
troducing a novel online learning algorithm for
relative margin maximization, RM. The relative
margin solution is obtained by maximizing the
same margin as Equation (2), but now with re-
spect to the distance between y+, and the worst
candidate yw. Thus, the relative margin dictates
trading-off between a large margin as before, and
a small spread of the projection, in other words,
bounding the distance between y+ and yw. The
additional computation required, namely, obtain-
ing yw, is efficient to perform, and has likely al-
ready happened while obtaining the k-best deriva-
tions necessary for the margin update. The online
latent structured soft relative margin optimization
problem is then:
wt+1 = arg min
w
1
2 ||w ?wt||
2 + C?i + D?i
s.t.: ?s(xi, y+, y?) ? ?i(y?)??i(y+)? ?i
?B ? ?i ? ?s(xi, y+, yw) ? B + ?i
(4)
where additional bounding constraints are added
to the usual margin constraints in order to contain
the spread by bounding the difference in projec-
tions. B is an additional parameter; it controls
the spread, trading off between margin maximiza-
tion and spread minimization. Notice that when
B ? ?, the bounding constraints disappear, and
we are left with the original problem in Equa-
tion (2). D, which plays an analogous role to C,
allows penalized violations of the bounding con-
straints.
The dual of Equation (4) can be derived as:
max
?,?,??
L =
?
y?Y(xi)
?y ?B
?
y?Y(xi)
?y ?B
?
y?Y(xi)
??y
?12
? ?
y?Y(xi)
?y?i(y+, y)?
?
y?Y(xi)
?y?i(y+, y)
+
?
y?Y(xi)
??y?i(y+, y),
?
y??Y(xj)
?y??j(y+, y?)?
?
y??Y(xj)
?y??j(y+, y?)
+
?
y??Y(xj)
??y??j(y+, y?)
?
(5)
where the ? Lagrange multiplier corresponds
to the standard margin constraint, while ? and
1119
?? each correspond to a bounding constraint,
and ?i(y+, y?) corresponds to the difference of
f(xi, y+, d+) and f(xi, y?, d?). The weight up-
date can then be obtained from the dual variables:
?
?y?i(y+, y)?
?
?y?i(y+, y) +
?
??y?i(y+, y)
(6)
The dual in Equation (5) can be optimized us-
ing a cutting plane algorithm, an effective method
for solving a relaxed optimization problem in
the dual, used in Structured SVM, MIRA, and
RMM (Tsochantaridis et al, 2004; Chiang, 2012;
Shivaswamy and Jebara, 2009a). The cutting
plane presented in Alg. 1 decomposes the overall
problem into subproblems which are solved inde-
pendently by creating working sets Sji , which cor-
respond to the largest violations of either the mar-
gin constraint, or bounding constraints, and itera-
tively satisfying the constraints in each set.
The cutting plane in Alg. 1 makes use of the
the closed-form gradient-based updates we de-
rived for RM presented in Alg. 2. The updates
amount to performing a subgradient descent step
to update w in accordance with the constraints.
Since the constraint matrix of the dual program is
not strictly decomposable across constraint types,
we are in effect solving an approximation of the
original problem.
Algorithm 1 RM Cutting Plane Algorithm
(adapted from (Shivaswamy and Jebara, 2009a))
Require: ith training example (xi, yi), weight w, margin
reg. C, bound B, bound reg. D, , B
1: S1i ?
{
y+
}, S2i ?
{
y+
}, S3i ?
{
y+
}
2: repeat
3: H(y) := ?i(y)??i(y+)? ?s(xi, y+, y)
4: y1 ? arg maxy?Y(xi) H(y)
5: y2 ? arg maxy?Y(xi) G(y) := ?s(xi, y+, y)6: y3 ? arg miny?Y(xi)?G(y)7: ? ? max {0,maxy?Si H(y)}8: V1 ? H(y1)? ? ? 
9: V2 ? G(y2)?B ? B
10: V3 ? ?G(y3)?B ? B
11: j ? argmaxj??{1,2,3} Vj?
12: if Vj > 0 then
13: Sji ? Sji ? {yj}
14: OPTIMIZE(w, S1i , S2i , S3i , C,B) . see Alg. 2
15: end if
16: until S1i , S2i , S3i do not change
Alternatively, we could utilize a passive-
aggressive updating strategy (Crammer et al,
2006), which would simply bypass the cutting
plane and select the most violated constraint for
Algorithm 2 RM update with ?, ?, ??
1: procedure OPTIMIZE(w, S1i , S2i , S3i , C,B)
2: whilew changes do
3: if ??S1i
?? > 1 then
4: UPDATEMARGIN(w, S1i , C)
5: end if
6: if ??S2i
?? > 1 then
7: UPDATEUPPERBOUND(w, S2i , B)
8: end if
9: if ??S3i
?? > 1 then
10: UPDATELOWERBOUND(w, S3i , B)
11: end if
12: end while
13: end procedure
14: procedure UPDATEMARGIN(w, S1i , C)
15: ?y ? 0 for all y ? S1i
16: ?y+i ? C17: for n? 1...MaxIter do
18: Select two constraints y, y? from S1i
19: ?? ? ?i(y?)??i(y)??s(xi, y, y?)||?(y,y?)||2
20: ?? ? max(??y,min(?y? , ??))
21: ?y ? ?y + ?? ; ??y ? ??y ? ??
22: w? w + ??(?(y, y?))
23: end for
24: end procedure
25: procedure UPDATEUPPERBOUND(w, S2i , B)
26: ?y ? 0 for all y ? S2i
27: for n? 1...MaxIter do
28: Select one constraint y from S2i
29: ?? ? max(0, B??s(xi ,y+ ,y)||?(y+,y)||2 )
30: ?y ? ?y + ??
31: w? w ? ??(?(y+, y))
32: end for
33: end procedure
34: procedure UPDATELOWERBOUND(w, S3i , B)
35: ??y ? 0 for all y ? S3i
36: for n? 1...MaxIter do
37: Select one constraint y from S3i
38: ??? ? max(0, ?B??s(xi ,y+ ,y)||?(y+,y)||2 )
39: ??y ? ??y + ???
40: w? w + ???(?(y+, y))
41: end for
42: end procedure
each set, if there is one, and perform the corre-
sponding parameter updates in Alg. 2. We re-
fer to the resulting passive-aggressive algorithm as
RM-PA, and the cutting plane version as RM-CP.
Preliminary experiments showed that RM-PA per-
forms on par with RM-CP, thus RM-PA is the one
used in the empirical evaluation below.
A graphical depiction of the passive-aggressive
RM update is presented in Figure 2. The upper
right circle represents y+, while all other squares
represent alternative hypotheses y?. As in the stan-
dard MIRA solution, we select the maximum mar-
gin constraint violator, y?, shown as the triangle,
and update such that the margin is greater than the
cost. Additionally, we select the maximum bound-
1120
 
Bounding Constraint 
 
dist 
 
cost > margin 
 
B
LE
U
  
S
co
re
 
Margin Constraint 
cost 
 
margin 
 
Model Score 
dist > B 
 
 B 
 
Figure 2: RM update with margin and bounding con-
straints. The diagonal dotted line depicts cost?margin equi-
librium. The vertical gray dotted line depicts the bound B.
White arrows indicate updates triggered by constraint viola-
tions. Squares are data points in the k-best list not selected
for update in this round.
task Corpus Sentences Tokens
En Zh/Ar
Zh-En
training 1.6M 44.4M 40.4M
tune (MT06) 1664 48k 39k
MT03 919 28k 24k
MT05 1082 35k 33k
Ar-En
training 1M 23.7M 22.8M
tune (MT06) 1797 55k 49k
MT05 1056 36k 33k
MT08 1360 51k 45k
4-gram LM 24M 600M ?
Table 1: Corpus statistics
ing constraint violator, yw, shown as the upside-
down triangle, and update so the distance from y+
is no greater than B.
4 Experiments
4.1 Setup
To evaluate the advantage of explicitly accounting
for the spread of the data, we conducted several
experiments on two Chinese-English translation
test sets, using two different feature sets in each.
For training we used the non-UN and non-HK
Hansards portions of the NIST training corpora,
which was segmented using the Stanford seg-
menter (Tseng et al, 2005). The data statistics are
summarized in the top half of Table 1. The English
data was lowercased, tokenized and aligned using
GIZA++ (Och and Ney, 2003) to obtain bidirec-
tional alignments, which were symmetrized using
the grow-diag-final-and method (Koehn
et al, 2003). We trained a 4-gram LM on the
English side of the corpus with additional words
from non-NYT and non-LAT, randomly selected
portions of the Gigaword v4 corpus, using modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1996). We used cdec (Dyer et al, 2010) as our
hierarchical phrase-based decoder, and tuned the
parameters of the system to optimize BLEU (Pap-
ineni et al, 2002) on the NIST MT06 corpus.
We applied several competitive optimizers as
baselines: hypergraph-based MERT (Kumar et al,
2009), k-best variants of MIRA (Crammer et al,
2006; Chiang et al, 2009), PRO (Hopkins and
May, 2011), and RAMPION (Gimpel and Smith,
2012). The size of the k-best list was set to 500
for RAMPION, MIRA and RM, and 1500 for PRO,
with both PRO and RAMPION utilizing k-best ag-
gregation across iterations. RAMPION settings
were as described in (Gimpel and Smith, 2012),
and PRO settings as described in (Hopkins and
May, 2011), with PRO requiring regularization
tuning in order to be competitive with the other op-
timizers. MIRA and RM were run with 15 paral-
lel learners using iterative parameter mixing (Mc-
Donald et al, 2010). All optimizers were imple-
mented in cdec and use the same system config-
uration, thus the only independent variable is the
optimizer itself. We set C to 0.01, and MaxIter
to 100. We selected the bound step size D, based
on performance on a held-out dev set, to be 0.01
for the basic feature set and 0.1 for the sparse fea-
ture set. The bound constraintB was set to 1.4 The
approximate sentence-level BLEU cost ?i is com-
puted in a manner similar to (Chiang et al, 2009),
namely, in the context of previous 1-best transla-
tions of the tuning set. All results are averaged
over 3 runs.
4.2 Feature Sets
We experimented with a small (basic) feature set,
and a large (sparse) feature set. For the small
feature set, we use 14 features, including a lan-
guage model, 5 translation model features, penal-
ties for unknown words, the glue rule, and rule
arity. For experiments with a larger feature set,
we introduced additional lexical and non-lexical
sparse Boolean features of the form commonly
found in the literature (Chiang et al, 2009; Watan-
4We also conducted an investigation into the setting of the
B parameter. We explored alternative values for B, as well
as scaling it by the current candidate?s cost, and found that
the optimizer is fairly insensitive to these changes, resulting
in only minor differences in BLEU.
1121
Optimizer Zh Ar
MIRA 35k 37k
PRO 95k 115k
RAMPION 22k 24k
RM 30k 32k
Active+Inactive 3.4M 4.9M
Table 2: Active sparse feature templates
abe et al, 2007; Simianer et al, 2012).
Non-lexical features include structural distor-
tion, which captures the dependence between re-
ordering and the size of a filler, and rule shape,
which bins grammar rules by their sequence of
terminals and nonterminals (Chiang et al, 2008).
Lexical features on rules include rule ID, which
fires on a specific grammar rule. We also in-
troduce context-dependent lexical features for the
300 most frequent aligned word pairs (f ,e) in the
training corpus, which fire on triples (f ,e,f+1) and
(f ,e,f?1), capturing when we see f aligned to e,
with f+1 and f?1 occurring to the right or left of f ,
respectively. All other words fall into the default
?unk? feature bin. In addition, we have insertion
and deletion features for the 150 most frequently
unaligned target and source words. These feature
templates resulted in a total of 3.4 million possible
features, of which only a fraction were active for
the respective tuning set and optimizer, as shown
in Table 2.
4.3 Results
As can be seen from the results in Table 3, our
RM method was the best performer in all Chinese-
English tests according to all measures ? up to 1.9
BLEU and 6.6 TER over MIRA ? even though we
only optimized for BLEU.5 Surprisingly, it seems
that MIRA did not benefit as much from the sparse
features as RM. The results are especially notable
for the basic feature setting ? up to 1.2 BLEU and
4.6 TER improvement over MERT ? since MERT
has been shown to be competitive with small num-
bers of features compared to high-dimensional op-
timizers such as MIRA (Chiang et al, 2008).
For the tuning set, the decoder performance was
consistently the lowest with RM, compared to the
5In the small feature set RAMPION yielded similar best
BLEU scores, but worse TER. In preliminary experiments
with a smaller trigram LM, our RM method consistently
yielded the highest scores in all Chinese-English tests ? up
to 1.6 BLEU and 6.4 TER from MIRA, the second best per-
former.
other optimizers. We believe this is due to the
RM bounding constraint being more resistant to
overfitting the training data, and thus allowing for
improved generalization. Conversely, while PRO
had the second lowest tuning scores, it seemed to
display signs of underfitting in the basic and large
feature settings.
5 Additional Experiments
In order to explore the applicability of our ap-
proach to a wider range of languages, we also eval-
uated its performance on Arabic-English transla-
tion. All experimental details were the same as
above, except those noted below.
For training, we used the non-UN portion of the
NIST training corpora, which was segmented us-
ing an HMM segmenter (Lee et al, 2003). Dataset
statistics are given in the bottom part of Table 1.
The sparse feature templates resulted here in a to-
tal of 4.9 million possible features, of which again
only a fraction were active, as shown in Table 2.
As can be seen in Table 4, in the smaller feature
set, RM and MERT were the best performers, with
the exception that on MT08, MIRA yielded some-
what better (+0.7) BLEU but a somewhat worse
(-0.9) TER score than RM.
On the large feature set, RM is again the best
performer, except, perhaps, a tied BLEU score
with MIRA on MT08, but with a clear 1.8 TER
gain. In both Arabic-English feature sets, MIRA
seems to take the second place, while RAMPION
lags behind, unlike in Chinese-English (?4).6
Interestingly, RM achieved substantially higher
BLEU precision scores in all tests for both lan-
guage pairs. However, this was also usually cou-
pled had a higher brevity penalty (BP) thanMIRA,
with the BP increasing slightly when moving to
the sparse setting.
6 Discussion
The trend of the results, summarized as RM gain
over other optimizers averaged over all test sets, is
presented in Table 5. RM shows clear advantage
in both basic and sparse feature sets, over all other
state-of-the-art optimizers. The RM gains are no-
tably higher in the large feature set, which we take
6In our preliminary experiments with the smaller trigram
LM, MERT did better on MT05 in the smaller feature set, and
MIRA had a small advantage in two cases. RAMPION per-
formed similarly to RM on the smaller feature set. RM?s loss
was only up to 0.8 BLEU (0.7 TER) from MERT or MIRA,
while its gains were up to 1.7 BLEU and 2.1 TER over MIRA.
1122
Small (basic) feature set Large (sparse) feature set
Optimizer Tune MT03 MT05 Tune MT03 MT05
?BLEU ?BLEU ?TER ?BLEU ?TER ?BLEU ?BLEU ?TER ?BLEU ?TER
MERT 35.4 35.8 60.8 32.4 63.9 - - - - -
MIRA 35.5 35.8 61.1 32.1 64.6 36.6 35.9 60.6 32.1 64.1
PRO 34.1 36.0 60.2 31.7 63.4 35.7 34.8 56.1 31.4 59.1
RAMPION 35.1 36.5 58.6 33.0 61.3 36.7 36.9 57.7 33.3 60.6
RM 31.3 36.5 56.4 33.6 59.3 33.2 37.5 54.6 34.0 57.5
Table 3: Performance on Zh-En with basic (left) and sparse (right) feature sets on MT03 and MT05.
Small (basic) feature set Large (sparse) feature set
Optimizer Tune MT05 MT08 Tune MT05 MT08
?BLEU ?BLEU ?TER ?BLEU ?TER ?BLEU ?BLEU ?TER ?BLEU ?TER
MERT 43.8 53.3 40.2 41.0 50.7 - - - - -
MIRA 43.0 52.8 40.8 41.3 50.6 44.4 53.4 40.1 41.8 50.2
PRO 41.5 51.3 41.5 39.4 51.5 46.8 53.2 40.0 41.4 49.7
RAMPION 42.4 52.0 40.8 40.0 50.8 44.6 52.9 40.4 41.0 50.4
RM 38.5 53.3 39.8 40.6 49.7 43.0 55.3 37.5 41.8 48.4
Table 4: Performance on Ar-En with basic (left) and sparse (right) feature sets on MT05 and MT08.
Small set Large set
Optimizer BLEU TER BLEU TER
MERT 0.4 2.6 - -
MIRA 0.5 3.0 1.4 4.3
PRO 1.4 2.9 2.0 1.7
RAMPION 0.6 1.6 1.2 2.8
Table 5: RM gain over other optimizers averaged
over all test sets.
as an indication for the importance of bounding
the spread.
Spread analysis: For RM, the average spread
of the projected data in the Chinese-English small
feature set was 0.9?3.6 for all tuning iterations,
and 0.7?2.9 for the iteration with the highest de-
coder performance. In comparison, the spread of
the data for MIRA was 5.9?20.5 for the best it-
eration. In the sparse setting, RM had an aver-
age spread of 0.9?2.4 for the best iteration, while
MIRA had a spread of 14.0?31.1. Similarly,
on Arabic-English, RM had a spread of 0.7?2.4
in the small setting, and 0.82?1.4 in the sparse
setting, while MIRA?s spread was 9.4?26.8 and
11.4?22.1, for the small and sparse settings, re-
spectively. Notice that the average spread for RM
stays about the same when moving to higher di-
mensions, with the variance decreasing in both
cases. For MIRA, however, the average spread
increases in both cases, with the variance being
much higher than RM. For instance, observe that
the spread of MIRA on Chinese grows from 5.9 to
14.0 in the sparse feature setting. While bounding
the spread is useful in the low-dimensional setting
(0.7-1.5 BLEU gain with RM over MIRA as shown
in Table 3), accounting for the spread is even more
crucial with sparse features, where MIRA gains
only up to 0.1 BLEU, while RM gains 1 BLEU.
These results support the claim that our imposed
bound B indeed helps decrease the spread, and
that, in turn, lower spread yields better general-
ization performance.
Error Analysis: The inconclusive advantage
of RM over MIRA (in BLEU vs. TER scores)
on Arabic-English MT08 calls for a closer look.
Therefore we conducted a coarse error analysis
on 15 randomly selected sentences from MERT,
RMM and MIRA, with basic and sparse feature
settings for the latter two. This sample yielded
450 data points for analysis: output of the 5 con-
ditions on 15 sentences scored in 6 violation cate-
gories. The categories were: function word drop,
content word drop, syntactic error (with a reason-
able meaning), semantic error (regardless of syn-
tax), word order issues, and function word mis-
translation and ?hallucination?. The purpose of
this analysis was to get a qualitative feel for the
output of each model, and a better idea as to why
we obtained performance improvements. RM no-
1123
ticeably had more word order and excess/wrong
function word issues in the basic feature setting
than any optimizer. However, RM seemed to ben-
efit the most from the sparse features, as its bad
word order rate dropped close toMIRA, and its ex-
cess/wrong function word rate dropped below that
of MIRA with sparse features (MIRA?s rate actu-
ally doubled from its basic feature set). We con-
jecture both these issues will be ameliorated with
syntactic features such as those in Chiang et al
(2008). This correlates with our observation that
RM?s overall BLEU score is negatively impacted
by the BP, as the BLEU precision scores are no-
ticeably higher.
K-best: RM is potentially more sensitive to the
size and order of the k-best list. While MIRA is
only concerned with the margin between y+ and
y?, RM also accounts for the distance between y+
and yw. It might be the case that a larger k-best, or
revisiting previous strategies for y+ and y? selec-
tion, such as bold updating, local updating (Liang
et al, 2006b), or max-BLEU updating (Tillmann
and Zhang, 2006) might have a greater impact.
Also, we only explored several settings of B, and
there remains a continuum of RM solutions that
trade off between margin and spread in different
ways.
Active features: Perhaps contrary to expecta-
tion, we did not see evidence of a correlation be-
tween the number of active features and optimizer
performance. RAMPION, with the fewest features,
is the closest performer to RM in Chinese, while
MIRA, with a greater number, is the closest on
Arabic. We also notice that while PRO had the
lowest BLEU scores in Chinese, it was competi-
tive in Arabic with the highest number of features.
7 Conclusions and Future Work
We have introduced RM, a novel online margin-
based algorithm designed for optimizing high-
dimensional feature spaces, which introduces con-
straints into a large-margin optimizer that bound
the spread of the projection of the data while max-
imizing the margin. The closed-form online up-
date for our relative margin solution accounts for
surrogate references and latent variables.
Experimentation in statistical MT yielded sig-
nificant improvements over several other state-
of-the-art optimizers, especially in a high-
dimensional feature space (up to 2 BLEU and 4.3
TER on average). Overall, RM achieves the best or
comparable performance according to two scoring
methods in two language pairs, with two test sets
each, in small and large feature settings. More-
over, across conditions, RM always yielded the
best combined TER-BLEU score.7
These improvements are achieved using stan-
dard, relatively small tuning sets, contrasted with
improvements involving sparse features obtained
using much larger tuning sets, on the order of
hundreds of thousands of sentences (Liang et al,
2006a; Tillmann and Zhang, 2006; Blunsom et al,
2008; Simianer et al, 2012). Since our approach
is complementary to scaling up the tuning data, in
future work we intend to combine these two meth-
ods. In future work we also intend to explore using
additional sparse features that are known to be use-
ful in translation, e.g. syntactic features explored
by Chiang et al (2008).
Finally, although motivated by statistical ma-
chine translation, RM is a gradient-based method
that can easily be applied to other problems. We
plan to investigate its utility elsewhere in NLP
(e.g. for parsing) as well as in other domains in-
volving high-dimensional structured prediction.
Acknowledgments
We would like to thank Pannaga Shivaswamy for
valuable discussions, and the anonymous review-
ers for their comments. Vladimir Eidelman is sup-
ported by a National Defense Science and Engi-
neering Graduate Fellowship. This work was also
supported in part by the BOLT program of the De-
fense Advanced Research Projects Agency, Con-
tract HR0011-12-C-0015.
References
Abishek Arun and Philipp Koehn. 2007. Online learn-
ing methods for discriminative training of phrase
based statistical machine translation. In MT Summit
XI.
Peter L. Bartlett and Shahar Mendelson. 2003.
Rademacher and gaussian complexities: risk bounds
and structural results. J. Mach. Learn. Res., 3:463?
482, March.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL-08:
HLT, Columbus, Ohio, June.
7We and other researchers often use 12 (TER?BLEU) as acombined SMT quality metric.
1124
Nicolo` Cesa-Bianchi, Alex Conconi, and Claudio Gen-
tile. 2005. A second-order perceptron algorithm.
SIAM J. Comput., 34(3):640?668, March.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310?318.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of NAACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), Waikiki, Honolulu, Hawaii.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, NAACL ?09, pages 218?226.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Machine
Learning Research.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line passive-aggressive algorithms. J. Mach. Learn.
Res., 7:551?585.
Koby Crammer, Alex Kulesza, and Mark Dredze.
2009a. Adaptive regularization of weight vectors.
In Advances in Neural Information Processing Sys-
tems 22, pages 414?422.
Koby Crammer, Mehryar Mohri, and Fernando Pereira.
2009b. Gaussian margin machines. Journal of
Machine Learning Research - Proceedings Track,
5:105?112.
Koby Crammer, Mark Dredze, and Fernando Pereira.
2012. Confidence-weighted linear classification
for text categorization. J. Mach. Learn. Res.,
98888:1891?1926, June.
Mark Dredze and Koby Crammer. 2008. Confidence-
weighted linear classification. In In ICML 08: Pro-
ceedings of the 25th international conference on
Machine learning, pages 264?271. ACM.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL System Demonstrations.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
George Foster and Roland Kuhn. 2009. Stabilizing
minimum error rate training. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 242?249, Athens, Greece, March. As-
sociation for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Thorsten Joachims. 1998. Text Categorization with
Support Vector Machines: Learning with Many Rel-
evant Features. In Claire Ne?dellec and Ce?line Rou-
veirol, editors, European Conference on Machine
Learning, pages 137?142, Berlin. Springer.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, Stroudsburg, PA, USA.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 163?171.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based Arabic word segmentation. In Pro-
ceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics - Volume 1, pages
399?406.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006a. An end-to-end discrimi-
native approach to machine translation. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
ACL-44, pages 761?768.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006b. An end-to-end discrimi-
native approach to machine translation. In Proceed-
ings of the 2006 International Conference on Com-
putational Linguistics (COLING) - the Association
for Computational Linguistics (ACL).
David Mcallester and Joseph Keshet. 2011. Gener-
alization bounds and consistency for latent struc-
tural probit and ramp loss. In J. Shawe-Taylor,
1125
R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q.
Weinberger, editors, Advances in Neural Informa-
tion Processing Systems 24, pages 2205?2212.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 456?464, Los Angeles, California.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics, volume 29(21), pages
19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Pannagadatta Shivaswamy and Tony Jebara. 2009a.
Structured prediction with relative margin. In In
International Conference on Machine Learning and
Applications.
Pannagadatta K Shivaswamy and Tony Jebara. 2009b.
Relative margin machines. In In Advances in Neural
Information Processing Systems 21. MIT Press.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Jeju Island, Korea, July.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ben Taskar, Simon Lacoste-Julien, and Michael I. Jor-
dan. 2006. Structured prediction, dual extragradi-
ent and bregman projections. J. Mach. Learn. Res.,
7:1627?1653, December.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical MT.
In Proceedings of the 2006 International Conference
on Computational Linguistics (COLING) - the Asso-
ciation for Computational Linguistics (ACL).
Huihsin Tseng, Pi-Chuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning. 2005. A
conditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Process-
ing.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proceedings of the twenty-first in-
ternational conference on Machine learning, ICML
?04.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), Prague, Czech Republic, June. Association
for Computational Linguistics.
1126
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 199?204,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mr. MIRA: Open-Source Large-Margin Structured Learning on
MapReduce
Vladimir Eidelman1, Ke Wu1, Ferhan Ture1, Philip Resnik2, Jimmy Lin3
1 Dept. of Computer Science 2 Dept. of Linguistics 3 The iSchool
Institute for Advanced Computer Studies
University of Maryland
{eidelman,wuke,fture,resnik,jimmylin}@umd.edu
Abstract
We present an open-source framework
for large-scale online structured learning.
Developed with the flexibility to handle
cost-augmented inference problems such
as statistical machine translation (SMT),
our large-margin learner can be used with
any decoder. Integration with MapReduce
using Hadoop streaming allows efficient
scaling with increasing size of training
data. Although designed with a focus
on SMT, the decoder-agnostic design of
our learner allows easy future extension to
other structured learning problems such as
sequence labeling and parsing.
1 Introduction
Structured learning problems such as sequence la-
beling or parsing, where the output has a rich in-
ternal structure, commonly arise in NLP. While
batch learning algorithms adapted for structured
learning such as CRFs (Lafferty et al, 2001)
and structural SVMs (Joachims, 1998) have re-
ceived much attention, online methods such as
the structured perceptron (Collins, 2002) and a
family of Passive-Aggressive algorithms (Cram-
mer et al, 2006) have recently gained promi-
nence across many tasks, including part-of-speech
tagging (Shen, 2007), parsing (McDonald et
al., 2005) and statistical machine translation
(SMT) (Chiang, 2012), due to their ability to deal
with large training sets and high-dimensional in-
put representations.
Unlike batch learners, which must consider all
examples when optimizing the objective, online
learners operate in rounds, optimizing using one
example or a handful of examples at a time. This
online nature offers several attractive properties,
facilitating scaling to large training sets while re-
maining simple and offering fast convergence.
Mr. MIRA, the open source system1 de-
scribed in this paper, implements an online large-
margin structured learning algorithm based on
MIRA (?2.1), for cost-augmented online large-
scale training in high-dimensional feature spaces.
Our contribution lies in providing the first pub-
lished decoder-agnostic parallelization of MIRA
with Hadoop for structured learning.
While the current demonstrated application fo-
cuses on large-scale discriminative training for
machine translation, the learning algorithm is gen-
eral with respect to the inference algorithm em-
ployed. We are able to decouple our learner en-
tirely from the MT decoder, allowing users to
specify their own inference procedure through a
simple text communication protocol (?2.2). The
learner only requires k-best output with feature
vectors, as well as the specification of a cost func-
tion. Standard automatic evaluation metrics for
MT, such as BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006), have already been imple-
mented. Furthermore, our system can be extended
to other structured learning problems with a min-
imal amount of effort, simply by implementing a
task-specific cost function and specifying an ap-
propriate decoder.
Through Hadoop streaming, our system can
take advantage of commodity clusters to handle
large-scale training (?3), while also being capable
of running in environments ranging from a single
machine to a PBS-managed batch cluster. Experi-
mental results (?4) show that it scales linearly and
makes fast parameter tuning on large tuning sets
for SMT practical.
2 Learning and Inference
2.1 Online Large-Margin Learning
MIRA is a popular online large-margin structured
learning method for NLP tasks (McDonald et al,
2005; Chiang et al, 2009; Chiang, 2012). The
1https://github.com/kho/mr-mira
199
main intuition is that we want our model to enforce
a margin between the correct and incorrect out-
puts of a sentence that agrees with our cost func-
tion. This is done by making the smallest update
we can to our parameters, w, on every sentence,
that will ensure that the difference in model scores
?fi(y?) = w>(f(xi, y+) ? f(xi, y?)) between the
correct output y+ and incorrect output y? is at least
as large as the cost, ?i(y?), incurred by predicting
the incorrect output:2
wt+1 = arg minw
1
2 ||w ?wt||
2 + C?i
s.t. ?y? ? Y(xi), ?fi(y?) ? ?i(y?)? ?i
where Y(xi) is the space of possible structured
outputs we are able to produce from xi, and
C is a regularization parameter that controls the
size of the update. In practice, we can de-
fine Y(xi) to be the k-best output. With a
passive-aggressive (PA) update, the ?y? constraint
above can be approximated by selecting the sin-
gle most violated constraint, which maximizes
y? ? arg maxy?Y(xi) w>f(xi, y) + ?i(y). This
optimization problem is attractive because it re-
duces to a simple analytical solution, essentially
performing a subgradient descent step with the
step size adjusted based on each example:
?? min
(
C, ?i(y
?)? ?fi(y?)
?f(xi, y+)? f(xi, y?)?2
)
w? w + ??
(
f(xi, y+)? f(xi, y?)
)
The user-defined cost function is a task-specific
external measure of quality that relays how bad se-
lecting y? truly is on the task we care about. The
cost can take any form as long as it decomposes
across the local parts of the structure, just as the
feature functions. For instance, it could be the
Hamming loss for sequence labeling, F-score for
parsing, or an approximate BLEU score for SMT.
Cost-augmented Inference For most struc-
tured prediction problems in machine learning,
yi ? Y(xi), that is, the model is able to produce,
and thus score, the correct output structure, mean-
ing y+ = yi. However, for certain NLP prob-
lems this may not be the case. For instance in
SMT, our model may not be able to produce or
reach the correct reference translation, which pro-
hibits our model from scoring it. This problem
2For a more formal description we refer the reader
to (Crammer et al, 2006; Chiang, 2012).
necessitates cost-augmented inference, where we
select y+ ? arg maxy?Y(xi) w>f(xi, y)??i(y)
from the space of structures our model can pro-
duce, to stand in for the correct output in optimiza-
tion. Our system was developed to handle both
cases, with the decoder providing the k-best list
to the learner, specifying whether to perform cost-
augmented selection.
Sparse Features While utilizing sparse features
is a primary motivation for performing large-scale
discriminative training, which features to use and
how to learn their weights can have a large im-
pact on the potential benefit. To this end, we in-
corporate `1/`2 regularization for joint feature se-
lection in order to improve efficiency and counter
overfitting effects (Simianer et al, 2012). Further-
more, the PA update has a single learning rate ?
for all features, which specifies how much the fea-
ture weights can change at each update. How-
ever, since dense features (e.g., language model)
are observed far more frequently than sparse fea-
tures (e.g., rule id), we may instead want to use
a per-feature learning rate that allows larger steps
for features that do not have much support. Thus,
we allow setting an adaptive per-feature learning
rate (Green et al, 2013; Crammer et al, 2009;
Duchi et al, 2011).
2.2 Learner/Decoder Communication
Training requires communication between the de-
coder and the learner. The decoder needs to re-
ceive weight updates and the input sentence from
the learner; and the learner needs to receive k-best
output with feature vectors from the decoder. This
is essentially all the required communication be-
tween the learner and the decoder. Below, we de-
scribe a simple line-based text protocol.
Input sentence and weight updates Follow-
ing common practice in machine translation, the
learner encodes each input sentence as a single-
line SGML entry named seg and sends it to the
decoder. The first line of Figure 1 is an exam-
ple sentence in this format. In addition to the
required sentence ID (useful in parallel process-
ing), an optional delta field is used to encode
the weight updates, as a sparse vector indexed
by feature names. First, for each name and up-
date pair, a binary record consisting of a null-
terminated string (name) and a double-precision
floating point number in native byte order (up-
date) is created. Then, all binary records are con-
200
<seg id="123" delta="TE0AexSuR+F6hD8="> das ist ein kleine haus </seg>
<seg id="124"> ein kleine haus </seg>\tein kleine ||| a small\thaus ||| house
Figure 1: Example decoder input in SGML
5
123 ||| 5 ||| this is a small house ||| TE0AAAAA... <base64> ||| 120.3
123 ||| 5 ||| this is the small house ||| <base64> ||| 118.4
123 ||| 5 ||| this was small house ||| <base64> ||| 110.5
<empty>
<empty>
Figure 2: Example k-best output
catenated and encoded in base64. In the example
above, the value of delta is the base64 encod-
ing of 0x4c 0x4d 0x00 0x7b 0x14 0xae 0x47
0xe1 0x7a 0x84 0x3f. The first 3 bytes store the
feature name (LM) and the next 8 bytes is its update
(0.01), to be added to the decoder?s current value
of the corresponding feature weight.
The learner also allows the user to pass any ad-
ditional information to the decoder, as long as it
can be encoded as a single-line text string. Such
information, if given, is appended after the seg en-
try, with a leading tab character as the delimiter.
For example, the second line of Figure 1 passes
two phrase translation rules to the decoder.
k-best output The decoder reads from standard
input and outputs the k-best output for one input
sentence before consuming the next line. For the
k-best output, the decoder first outputs to standard
output a line consisting of a single integerN . Next
the decoder outputs N lines where each line can
be either empty or an actual hypothesis. When the
line is an actual hypothesis, it consists of the fol-
lowing parts:
SID ||| LEN ||| TOK ||| FEAT [ REST ]
SID is the sentence ID of the corresponding input;
LEN is the length of source sentence;3 TOK contains
the tokens of the hypothesis sentence separated by
spaces; FEAT is the feature vector, encoded in the
same way as the weight updates, delimited by a
whitespace. Everything after FEAT until the end of
the line is discarded. See Figure 2 for an example
of k-best output. Note the scores after the last |||
are discarded by the learner.
Overall workflow The learner reads lines from
standard input in the following tab-delimited for-
mat:
3This is used in computing the smoothed cost. Usually
this is identical for all hypotheses if the input is a plain sen-
tence. But in applications such as lattice-based translation,
each hypothesis can be produced from different source sen-
tences, resulting in different lengths.
SRC<tab>REF<tab>REST
SRC is the actual input sentence as a seg entry; REF
is the gold output for the input sentence, for ex-
ample, reference translations in MT;4 REST is the
additional information that will be appended after
the seg entry and passed to the decoder.
The learner creates a sub-process for the de-
coder and connects to the sub-process? standard
input and output with pipes. Then it processes the
input lines one by one. For each line, it first sends
a composed input message to the decoder, combin-
ing the input sentence, weight updates, and user-
supplied information. Next it collects the k-best
output from the decoder, solves the QP problem to
obtain weight updates and repeats.
The learner produces two types of output. First,
the 1-best hypothesis for each input sentence, in
the following format:
SID<tab>TOK
Second, when there are no more input lines, the
learner outputs final weights and the number of
lines processed, in the following format:
-1<tab>NUM ||| WEIGHTS
The 1-best hypotheses can be scored against ref-
erences to obtain an estimate of cost. The final
weights are stored in a way convenient for averag-
ing in a parallel setting, as we shall discuss next.
3 Large-Scale Discriminative Training
3.1 MapReduce
With large amounts of data available today,
distributed computations have become essen-
tial. MapReduce (Dean and Ghemawat, 2004)
has emerged as a popular distributed process-
ing framework for commodity clusters that has
gained widespread adoption in both industry and
academia, thanks to its simplicity and the avail-
ability of the Hadoop open-source implementa-
tion. MapReduce provides a higher level of
4There can be multiple references, separated by |||.
201
abstraction for designing distributed algorithms
compared to, say, MPI or pthreads, by hiding
system-level details (e.g., deadlock, race condi-
tions, machine failures) from the developer.
A single MapReduce program begins with a
map phase, where mapper processes input key-
value pairs to produce an arbitrary number of in-
termediate key-value pairs. The mappers execute
in parallel, consuming data splits independently.
Following the map phase, all key-value pairs emit-
ted by the mappers are sorted by key and dis-
tributed to the reducers, such that all pairs shar-
ing the same key are guaranteed to arrive at the
same reducer. Finally, in the reduce phase, each
reducer processes the intermediate key-value pairs
it receives and emits final output key-value pairs.
3.2 System Architecture
Algorithm design We use Hadoop streaming to
parallelize the training process. Hadoop stream-
ing allows any arbitrary executable to serve as the
mapper or reducer, as long as it handles key-value
pairs properly.5 One iteration of training is im-
plemented as a single Hadoop streaming job. In
the map step, our learner can be directly used as
the mapper. Each mapper loads the same initial
weights, processes a single split of data and pro-
duces key-value pairs: the one-best hypothesis of
each sentence is output with the sentence ID as
the key (non-negative); the final weights with re-
spect to the split are output with a special negative
key. In the reduce step, a single reducer collects all
key-value pairs, grouped and sorted by keys. The
one-best hypotheses are output to disk in the or-
der they are received, so that the order matches the
reference translation set. The reducer also com-
putes the feature selection and weighted average
of final weights received from all of the mappers.
Assuming mapper i produces the final weights wi
after processing ni sentences, the weighted aver-
aged is defined as w? =
?
iwi?ni?
i ni
. Although aver-
aging yields different result from running a single
learner over the entire data, we have found the dif-
ference to be quite small in terms of convergence
and quality of tuned weights in practice.
After the reducer finishes, the averaged weights
are extracted and used as the initial weights for the
next iteration; the emitted hypotheses are scored
5By default, each line is treated as a key-value pair en-
coded in text, where the key and the value are separated by a
<tab>.
against the references, which allows us to track the
learning curve and the progress of convergence.
Scalability In an application such as SMT, the
decoder requires access to the translation gram-
mar and language model to produce translation hy-
potheses. For small tuning sets, which have been
typical in MT research, having these files trans-
ferred across the network to individual servers
(which then load the data into memory) is not
a problem. However, for even modest input on
the order of tens of thousands of sentences, this
creates a challenge. For example, distributing
thousands of per-sentence grammar files to all the
workers in a Hadoop cluster is time-consuming,
especially when this needs to be performed prior
to every iteration.
To benefit from MapReduce, it is essential to
avoid dependencies on ?side data? as much as
possible, due to the challenges explained above
with data transfer. To address this issue, we ap-
pend the per-sentence translation grammar as user-
supplied additional information to each input sen-
tence. This results in a large input file (e.g., 75 gi-
gabytes for 50,000 sentences), but this is not an is-
sue since the data reside on the Hadoop distributed
file system and MapReduce optimizes for data lo-
cality when scheduling mappers.
Unfortunately, it is much more difficult to ob-
tain per-sentence language models that are small
enough to handle in this same manner. Currently,
the best solution we have found is to use Hadoop?s
distributed cache to ship the single large language
model to each worker.
4 Evaluation
We evaluated online learning in Hadoop Map-
Reduce by applying it to German-English ma-
chine translation, using our hierarchical phrase-
based translation system with cdec as the de-
coder (Dyer et al, 2010). The parallel training
data consist of the Europarl and News Commen-
tary corpora from the WMT12 translation task,6
containing 2.08M sentences. A 5-gram language
model was trained on the English side of the bi-
text along with 750M words of news using SRILM
with modified Kneser-Ney smoothing (Chen and
Goodman, 1996).
We experimented with two feature sets: (1) a
small set with standard MT features, including
6http://www.statmt.org/wmt12/translation-task.html
202
Tuning set size Time/iteration # splits # features Tuning BLEU Test
(corpus) (on disk, GB) (in seconds) BLEU TER
dev 3.3 119 120 16 22.38 22.69 60.61
5k 7.8 289 120 16 32.60 22.14 59.60
10k 15.2 432 120 16 33.16 22.06 59.43
25k 37.2 942 300 16 32.48 22.21 59.54
50k 74.5 1802 600 16 32.21 22.21 59.39
dev 3.3 232 120 85k 23.08 23.00 60.19
5k 7.8 610 120 159k 33.70 22.26 59.26
10k 15.2 1136 120 200k 34.00 22.12 59.24
25k 37.2 2395 300 200k 32.96 22.35 59.29
50k 74.5 4465 600 200k 32.86 22.40 59.15
Table 1: Evaluation of our Hadoop implementation of MIRA, showing running time as well as BLEU
and TER values for tuning and testing data.
dev test 5k 10k 25k 50k
Sentences 3003 3003 5000 10000 25000 50000
Tokens en 75k 74k 132k 255k 634k 1258k
Tokens de 74k 73k 133k 256k 639k 1272k
Table 2: Corpus statistics
phrase and lexical translation probabilities in both
directions, word and arity penalties, and language
model scores; and (2) a large set containing the top
200k sparse features that might be useful to train
on large numbers of instances: rule id and shape,
target bigrams, insertions and deletions, and struc-
tural distortion features.
All experiments were conducted on a Hadoop
cluster (running Cloudera?s distribution, CDH
4.2.1) with 16 nodes, each with two quad-core 2.2
GHz Intel Nehalem Processors, 24 GB RAM, and
three 2 TB drives. In total, the cluster is configured
for a capacity of 128 parallel workers, although
we do not have direct control over the number
of simultaneous mappers, which depends on the
number of input splits. If the number of splits is
smaller than 128, then the cluster is under-utilized.
To note this, we report the number of splits for
each setting in our experimental results (Table 1).
We ran MIRA on a number of tuning sets, de-
scribed in Table 2, in order to test the effective-
ness and scalability of our system. First, we used
the standard development set from WMT12, con-
sisting of 3,003 sentences from news domain. In
order to show the scaling characteristics of our ap-
proach, we then used larger portions of the train-
ing bitext directly to tune parameters. In order to
avoid overfitting, we used a jackknifing method
to split the training data into n = 10 folds, and
built a translation system on n ? 1 folds, while
adjusting the sampling rate to sample sentences
from the other fold to obtain tuning sets ranging
from 5,000 sentences to 50,000 sentences. Table 1
shows details of experimental results for each set-
ting. The second column shows the space each
tuning set takes up on disk when we include refer-
ence translations and grammar files along with the
sentences. The reported tuning BLEU is from the
iteration with best performance, and running times
are reported from the top-scoring iteration as well.
Even though our focus in this evaluation is to
show the scalability of our implementation to large
input and feature sets, it is also worthwhile to men-
tion the effectiveness aspect. As we increase the
tuning set size by sampling sentences from the
training data, we see very little improvement in
BLEU and TER with the smaller feature set. This
is not surprising, since sparse features are more
likely to gain from additional tuning instances. In-
deed, tuning scores for all sets improve substan-
tially with sparse features, accompanied by small
increases on test.
While tuning on dev data results in better BLEU
on test data than when tuning on the larger sets, it
is important to note that although we are able to
tune more features on the larger bitext tuning sets,
they are not composed of the same genre as the
dev and test sets, resulting in a domain mismatch.
203
Therefore, we are actually comparing a smaller in-
domain tuning set with a larger out-of-domain set.
While this domain adaptation is problematic (Had-
dow and Koehn, 2012), the ability to discrimina-
tively tune on larger sets remains highly desirable.
In terms of running time, we observe that the al-
gorithm scales linearly with respect to input size,
regardless of the feature set. With more features,
running time increases due to a more complex
translation model, as well as larger intermediate
output (i.e., amount of information passed from
mappers to reducers). The scaling characteristics
point out the strength of our system: our scalable
MIRA implementation allows one to tackle learn-
ing problems where there are many parameters,
but also many training instances.
Comparing the wall clock time of paralleliza-
tion with Hadoop to the standard mode of 10?20
learner parallelization (Haddow et al, 2011; Chi-
ang et al, 2009), for the small 25k feature set-
ting, after one iteration, which takes 4625 sec-
onds using 15 learners on our PBS cluster, the tun-
ing score is 19.5 BLEU, while in approximately
the same time, we can perform five iterations
with Hadoop and obtain 30.98 BLEU. While this
is not a completely fair comparison, as the two
clusters utilize different resources and the num-
ber of learners, it suggests the practical benefits
that Hadoop can provide. Although increasing the
number of learners on our PBS cluster to the num-
ber of mappers used in Hadoop would result in
roughly equivalent performance, arbitrarily scal-
ing out learners on the PBS cluster to handle larger
training sets can be challenging since we?d have to
manually coordinate the parallel processes in an
ad-hoc manner. In contrast, Hadoop provides scal-
able parallelization in a manageable framework,
providing data distribution, synchronization, fault
tolerance, as well as other features, ?for free?.
5 Conclusion
In this paper, we presented an open-source
framework that allows seamlessly scaling struc-
tured learning to large feature-rich problems with
Hadoop, which lets us take advantage of large
amounts of data as well as sparse features. The
development of Mr. MIRA has been motivated pri-
marily by application to SMT, but we are planning
to extend our system to other structured prediction
tasks in NLP such as parsing, as well as to facili-
tate its use in other domains.
Acknowledgments
This research was supported in part by the DARPA
BOLT program, Contract No. HR0011-12-C-
0015; NSF under awards IIS-0916043 and IIS-
1144034. Vladimir Eidelman is supported by a
NDSEG Fellowship. Any opinions, findings, con-
clusions, or recommendations expressed are those
of the authors and do not necessarily reflect views
of the sponsors.
References
S. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In ACL.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new fea-
tures for statistical machine translation. In NAACL-HLT.
D. Chiang. 2012. Hope and fear for discriminative training
of statistical translation models. JMLR, 13:1159?1187.
M. Collins. 2002. Ranking algorithms for named-entity ex-
traction: boosting and the voted perceptron. In ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online passive-aggressive algorithms.
JMLR, 7:551?585.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adaptive
regularization of weight vectors. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In OSDI.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgra-
dient methods for online learning and stochastic optimiza-
tion. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, P. Blun-
som, H. Setiawan, V. Eidelman, and P. Resnik. 2010.
cdec: A decoder, alignment, and learning framework for
finite-state and context-free translation models. In ACL
System Demonstrations.
S. Green, S. Wang, D. Cer, and C. Manning. 2013. Fast and
adaptive online training of feature-rich translation models.
In ACL.
B. Haddow and P. Koehn. 2012. Analysing the effect of out-
of-domain data on smt systems. In WMT.
B. Haddow, A. Arun, and P. Koehn. 2011. SampleRank
training for phrase-based machine translation. In WMT.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features. In
ECML.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In ICML.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL.
L. Shen. 2007. Guided learning for bidirectional sequence
classification. In ACL.
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint feature
selection in distributed stochastic learning for large-scale
discriminative training in SMT. In ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In AMTA.
204
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1113?1122,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Political Ideology Detection Using Recursive Neural Networks
Mohit Iyyer
1
, Peter Enns
2
, Jordan Boyd-Graber
3,4
, Philip Resnik
2,4
1
Computer Science,
2
Linguistics,
3
iSchool, and
4
UMIACS
University of Maryland
{miyyer,peter,jbg}@umiacs.umd.edu, resnik@umd.edu
Abstract
An individual?s words often reveal their po-
litical ideology. Existing automated tech-
niques to identify ideology from text focus
on bags of words or wordlists, ignoring syn-
tax. Taking inspiration from recent work in
sentiment analysis that successfully models
the compositional aspect of language, we
apply a recursive neural network (RNN)
framework to the task of identifying the po-
litical position evinced by a sentence. To
show the importance of modeling subsen-
tential elements, we crowdsource political
annotations at a phrase and sentence level.
Our model outperforms existing models on
our newly annotated dataset and an existing
dataset.
1 Introduction
Many of the issues discussed by politicians and
the media are so nuanced that even word choice
entails choosing an ideological position. For ex-
ample, what liberals call the ?estate tax? conser-
vatives call the ?death tax?; there are no ideolog-
ically neutral alternatives (Lakoff, 2002). While
objectivity remains an important principle of jour-
nalistic professionalism, scholars and watchdog
groups claim that the media are biased (Groseclose
and Milyo, 2005; Gentzkow and Shapiro, 2010;
Niven, 2003), backing up their assertions by pub-
lishing examples of obviously biased articles on
their websites. Whether or not it reflects an under-
lying lack of objectivity, quantitative changes in the
popular framing of an issue over time?favoring
one ideologically-based position over another?can
have a substantial effect on the evolution of policy
(Dardis et al, 2008).
Manually identifying ideological bias in polit-
ical text, especially in the age of big data, is an
impractical and expensive process. Moreover, bias
They 
dubbed it 
the
death tax? ?
and created a 
big lie about
its adverse effects
on small 
businesses
Figure 1: An example of compositionality in ideo-
logical bias detection (red? conservative, blue?
liberal, gray? neutral) in which modifier phrases
and punctuation cause polarity switches at higher
levels of the parse tree.
may be localized to a small portion of a document,
undetectable by coarse-grained methods. In this pa-
per, we examine the problem of detecting ideologi-
cal bias on the sentence level. We say a sentence
contains ideological bias if its author?s political
position (here liberal or conservative, in the sense
of U.S. politics) is evident from the text.
Ideological bias is difficult to detect, even for
humans?the task relies not only on political
knowledge but also on the annotator?s ability to
pick up on subtle elements of language use. For
example, the sentence in Figure 1 includes phrases
typically associated with conservatives, such as
?small businesses? and ?death tax?. When we take
more of the structure into account, however, we
find that scare quotes and a negative propositional
attitude (a lie about X) yield an evident liberal bias.
Existing approaches toward bias detection have
not gone far beyond ?bag of words? classifiers, thus
ignoring richer linguistic context of this kind and
often operating at the level of whole documents.
In contrast, recent work in sentiment analysis has
used deep learning to discover compositional ef-
fects (Socher et al, 2011b; Socher et al, 2013b).
Building from those insights, we introduce a re-
cursive neural network (RNN) to detect ideological
bias on the sentence level. This model requires
1113
wb
 = changew
a
 = climate
w
d
 = so-called
p
c
 = climate change
p
e
 = so-called climate change
x
d
= x
c
=
x
e
=
x
a
= x
b
=
W
L
W
R
W
R
W
L
Figure 2: An example RNN for the phrase ?so-
called climate change?. Two d-dimensional word
vectors (here, d = 6) are composed to generate a
phrase vector of the same dimensionality, which
can then be recursively used to generate vectors at
higher-level nodes.
richer data than currently available, so we develop
a new political ideology dataset annotated at the
phrase level. With this new dataset we show that
RNNs not only label sentences well but also im-
prove further when given additional phrase-level
annotations. RNNs are quantitatively more effec-
tive than existing methods that use syntactic and
semantic features separately, and we also illustrate
how our model correctly identifies ideological bias
in complex syntactic constructions.
2 Recursive Neural Networks
Recursive neural networks (RNNs) are machine
learning models that capture syntactic and semantic
composition. They have achieved state-of-the-art
performance on a variety of sentence-level NLP
tasks, including sentiment analysis, paraphrase de-
tection, and parsing (Socher et al, 2011a; Hermann
and Blunsom, 2013). RNN models represent a shift
from previous research on ideological bias detec-
tion in that they do not rely on hand-made lexicons,
dictionaries, or rule sets. In this section, we de-
scribe a supervised RNN model for bias detection
and highlight differences from previous work in
training procedure and initialization.
2.1 Model Description
By taking into account the hierarchical nature of
language, RNNs can model semantic composition,
which is the principle that a phrase?s meaning is a
combination of the meaning of the words within
that phrase and the syntax that combines those
words. While semantic composition does not ap-
ply universally (e.g., sarcasm and idioms), most
language follows this principle. Since most ide-
ological bias becomes identifiable only at higher
levels of sentence trees (as verified by our annota-
tion, Figure 4), models relying primarily on word-
level distributional statistics are not desirable for
our problem.
The basic idea behind the standard RNN model
is that each word w in a sentence is associated
with a vector representation x
w
? R
d
. Based on a
parse tree, these words form phrases p (Figure 2).
Each of these phrases also has an associated vector
x
p
? R
d
of the same dimension as the word vectors.
These phrase vectors should represent the meaning
of the phrases composed of individual words. As
phrases themselves merge into complete sentences,
the underlying vector representation is trained to
retain the sentence?s whole meaning.
The challenge is to describe how vectors com-
bine to form complete representations. If two
words w
a
and w
b
merge to form phrase p, we posit
that the phrase-level vector is
x
p
= f(W
L
? x
a
+W
R
? x
b
+ b
1
), (1)
where W
L
and W
R
are d ? d left and right com-
position matrices shared across all nodes in the
tree, b
1
is a bias term, and f is a nonlinear activa-
tion function such as tanh. The word-level vectors
x
a
and x
b
come from a d ? V dimensional word
embedding matrix W
e
, where V is the size of the
vocabulary.
We are interested in learning representations that
can distinguish political polarities given labeled
data. If an element of this vector space, x
d
, repre-
sents a sentence with liberal bias, its vector should
be distinct from the vector x
r
of a conservative-
leaning sentence.
Supervised RNNs achieve this distinction by ap-
plying a regression that takes the node?s vector x
p
as input and produces a prediction y?
p
. This is a
softmax layer
y?
d
= softmax(W
cat
? x
p
+ b
2
), (2)
where the softmax function is
softmax(q) =
exp q
?
k
j=1
exp q
j
(3)
and W
cat
is a k ? d matrix for a dataset with k-
dimensional labels.
We want the predictions of the softmax layer to
match our annotated data; the discrepancy between
categorical predictions and annotations is measured
1114
through the cross-entropy loss. We optimize the
model parameters to minimize the cross-entropy
loss over all sentences in the corpus. The cross-
entropy loss of a single sentence is the sum over
the true labels y
i
in the sentence,
`(y?
s
) =
k
?
p=1
y
p
? log(y?
p
). (4)
This induces a supervised objective function
over all sentences: a regularized sum over all node
losses normalized by the number of nodes N in the
training set,
C =
1
N
N
?
i
`(pred
i
) +
?
2
???
2
. (5)
We use L-BFGS with parameter averag-
ing (Hashimoto et al, 2013) to optimize the model
parameters ? = (W
L
,W
R
,W
cat
,W
e
, b
1
, b
2
). The
gradient of the objective, shown in Eq. (6), is
computed using backpropagation through struc-
ture (Goller and Kuchler, 1996),
?C
??
=
1
N
N
?
i
?`(y?
i
)
??
+ ??. (6)
2.2 Initialization
When initializing our model, we have two choices:
we can initialize all of our parameters randomly or
provide the model some prior knowledge. As we
see in Section 4, these choices have a significant
effect on final performance.
Random The most straightforward choice is to
initialize the word embedding matrix W
e
and com-
position matrices W
L
and W
R
randomly such that
without any training, representations for words and
phrases are arbitrarily projected into the vector
space.
word2vec The other alternative is to initialize the
word embedding matrix W
e
with values that reflect
the meanings of the associated word types. This
improves the performance of RNN models over ran-
dom initializations (Collobert and Weston, 2008;
Socher et al, 2011a). We initialize our model with
300-dimensional word2vec toolkit vectors gener-
ated by a continuous skip-gram model trained on
around 100 billion words from the Google News
corpus (Mikolov et al, 2013).
The word2vec embeddings have linear relation-
ships (e.g., the closest vectors to the average of
?green? and ?energy? include phrases such as ?re-
newable energy?, ?eco-friendly?, and ?efficient
lightbulbs?). To preserve these relationships as
phrases are formed in our sentences, we initialize
our left and right composition matrices such that
parent vector p is computed by taking the average
of children a and b (W
L
= W
R
= 0.5I
d?d
). This
initialization of the composition matrices has pre-
viously been effective for parsing (Socher et al,
2013a).
3 Datasets
We performed initial experiments on a dataset of
Congressional debates that has annotations on the
author level for partisanship, not ideology. While
the two terms are highly correlated (e.g., a member
of the Republican party likely agrees with conserva-
tive stances on most issues), they are not identical.
For example, a moderate Republican might agree
with the liberal position on increased gun control
but take conservative positions on other issues. To
avoid conflating partisanship and ideology we cre-
ate a new dataset annotated for ideological bias on
the sentence and phrase level. In this section we
describe our initial dataset (Convote) and explain
the procedure we followed for creating our new
dataset (IBC).
1
3.1 Convote
The Convote dataset (Thomas et al, 2006) con-
sists of US Congressional floor debate transcripts
from 2005 in which all speakers have been labeled
with their political party (Democrat, Republican,
or independent). We propagate party labels down
from the speaker to all of their individual sentences
and map from party label to ideology label (Demo-
crat? liberal, Republican? conservative). This
is an expedient choice; in future work we plan to
make use of work in political science characteriz-
ing candidates? ideological positions empirically
based on their behavior (Carroll et al, 2009).
While the Convote dataset has seen widespread
use for document-level political classification, we
are unaware of similar efforts at the sentence level.
3.1.1 Biased Sentence Selection
The strong correlation between US political parties
and political ideologies (Democrats with liberal,
Republicans with conservative) lends confidence
that this dataset contains a rich mix of ideological
1
Available at http://cs.umd.edu/
?
miyyer/ibc
1115
statements. However, the raw Convote dataset con-
tains a low percentage of sentences with explicit
ideological bias.
2
We therefore use the features
in Yano et al (2010), which correlate with politi-
cal bias, to select sentences to annotate that have
a higher likelihood of containing bias. Their fea-
tures come from the Linguistic Inquiry and Word
Count lexicon (LIWC) (Pennebaker et al, 2001),
as well as from lists of ?sticky bigrams? (Brown
et al, 1992) strongly associated with one party or
another (e.g., ?illegal aliens? implies conservative,
?universal healthcare? implies liberal).
We first extract the subset of sentences that con-
tains any words in the LIWC categories of Negative
Emotion, Positive Emotion, Causation, Anger, and
Kill verbs.
3
After computing a list of the top 100
sticky bigrams for each category, ranked by log-
likelihood ratio, and selecting another subset from
the original data that included only sentences con-
taining at least one sticky bigram, we take the union
of the two subsets. Finally, we balance the resulting
dataset so that it contains an equal number of sen-
tences from Democrats and Republicans, leaving
us with a total of 7,816 sentences.
3.2 Ideological Books
In addition to Convote, we use the Ideologi-
cal Books Corpus (IBC) developed by Gross et
al. (2013). This is a collection of books and maga-
zine articles written between 2008 and 2012 by au-
thors with well-known political leanings. Each doc-
ument in the IBC has been manually labeled with
coarse-grained ideologies (right, left, and center) as
well as fine-grained ideologies (e.g., religious-right,
libertarian-right) by political science experts.
There are over a million sentences in the IBC,
most of which have no noticeable political bias.
Therefore we use the filtering procedure outlined
in Section 3.1.1 to obtain a subset of 55,932 sen-
tences. Compared to our final Convote dataset, an
even larger percentage of the IBC sentences exhibit
no noticeable political bias.
4
Because our goal
is to distinguish between liberal and conservative
2
Many sentences in Convote are variations on ?I think this
is a good/bad bill?, and there is also substantial parliamentary
boilerplate language.
3
While Kill verbs are not a category in LIWC, Yano et
al. (2010) adopted it from Greene and Resnik (2009) and
showed it to be a useful predictor of political bias. It includes
words such as ?slaughter? and ?starve?.
4
This difference can be mainly attributed to a historical
topics in the IBC (e.g., the Crusades, American Civil War).
In Convote, every sentence is part of a debate about 2005
political policy.
bias, instead of the more general task of classify-
ing sentences as ?neutral? or ?biased?, we filter
the dataset further using DUALIST (Settles, 2011),
an active learning tool, to reduce the proportion
of neutral sentences in our dataset. To train the
DUALIST classifier, we manually assigned class la-
bels of ?neutral? or ?biased? to 200 sentences, and
selected typical partisan unigrams to represent the
?biased? class. DUALIST labels 11,555 sentences as
politically biased, 5,434 of which come from con-
servative authors and 6,121 of which come from
liberal authors.
3.2.1 Annotating the IBC
For purposes of annotation, we define the task of
political ideology detection as identifying, if pos-
sible, the political position of a given sentence?s
author, where position is either liberal or conser-
vative.
5
We used the Crowdflower crowdsourcing
platform (crowdflower.com), which has previously
been used for subsentential sentiment annotation
(Sayeed et al, 2012), to obtain human annotations
of the filtered IBC dataset for political bias on both
the sentence and phrase level. While members of
the Crowdflower workforce are certainly not ex-
perts in political science, our simple task and the
ubiquity of political bias allows us to acquire useful
annotations.
Crowdflower Task First, we parse the filtered
IBC sentences using the Stanford constituency
parser (Socher et al, 2013a). Because of the ex-
pense of labeling every node in a sentence, we only
label one path in each sentence. The process for
selecting paths is as follows: first, if any paths
contain one of the top-ten partisan unigrams,
6
we
select the longest such path; otherwise, we select
the path with the most open class constituencies
(NP, VP, ADJP). The root node of a sentence is
always included in a path.
Our task is shown in Figure 3. Open class con-
stituencies are revealed to the worker incrementally,
starting with the NP, VP, or ADJP furthest from
the root and progressing up the tree. We choose
this design to prevent workers from changing their
lower-level phrase annotations after reading the full
sentence.
5
This is a simplification, as the ideological hierarchy in
IBC makes clear.
6
The words that the multinomial na??ve Bayes classifier
in DUALIST marked as highest probability given a polarity:
market, abortion, economy, rich, liberal, tea, economic, taxes,
gun, abortion
1116
Filtering the Workforce To ensure our anno-
tators have a basic understanding of US politics,
we restrict workers to US IP addresses and require
workers manually annotate one node from 60 dif-
ferent ?gold ? paths annotated by the authors. We
select these nodes such that the associated phrase is
either obviously biased or obviously neutral. Work-
ers must correctly annotate at least six of eight
gold paths before they are granted access to the full
task. In addition, workers must maintain 75% accu-
racy on gold paths that randomly appear alongside
normal paths. Gold paths dramatically improve
the quality of our workforce: 60% of contributors
passed the initial quiz (the 40% that failed were
barred from working on the task), while only 10%
of workers who passed the quiz were kicked out
for mislabeling subsequent gold paths.
Annotation Results Workers receive the
following instructions:
Each task on this page contains a set of
phrases from a single sentence. For each
phrase, decide whether or not the author fa-
vors a political position to the left (Liberal) or
right (Conservative) of center.
? If the phrase is indicative of a position to
the left of center, please choose Liberal.
? If the phrase is indicative of a position to
the right of center, please choose Conser-
vative.
? If you feel like the phrase indicates some
position to the left or right of the political
center, but you?re not sure which direc-
tion, please mark Not neutral, but I?m
unsure of which direction.
? If the phrase is not indicative of a posi-
tion to the left or right of center, please
mark Neutral.
We had workers annotate 7,000 randomly se-
lected paths from the filtered IBC dataset, with half
of the paths coming from conservative authors and
the other half from liberal authors, as annotated
by Gross et al (2013). Three workers annotated
each path in the dataset, and we paid $0.03 per
sentence. Since identifying political bias is a rela-
tively difficult and subjective task, we include all
sentences where at least two workers agree on a
label for the root node in our final dataset, except
when that label is ?Not neutral, but I?m unsure of
Figure 3: Example political ideology annotation
task showing incremental reveal of progressively
longer phrases.
which direction?. We only keep phrase-level an-
notations where at least two workers agree on the
label: 70.4% of all annotated nodes fit this defini-
tion of agreement. All unannotated nodes receive
the label of their closest annotated ancestor. Since
the root of each sentence is always annotated, this
strategy ensures that every node in the tree has a
label. Our final balanced IBC dataset consists of
3,412 sentences (4,062 before balancing and re-
moving neutral sentences) with a total of 13,640
annotated nodes. Of these sentences, 543 switch
polarity (liberal? conservative or vice versa) on
an annotated path.
While we initially wanted to incorporate neutral
labels into our model, we observed that lower-level
phrases are almost always neutral while full sen-
tences are much more likely to be biased (Figure 4).
Due to this discrepancy, the objective function in
Eq. (5) was minimized by making neutral predic-
tions for almost every node in the dataset.
4 Experiments
In this section we describe our experimental frame-
work. We discuss strong baselines that use lexi-
cal and syntactic information (including framing-
specific features from previous work) as well as
multiple RNN configurations. Each of these mod-
els have the same task: to predict sentence-level
ideology labels for sentences in a test set. To ac-
count for label imbalance, we subsample the data
so that there are an equal number of labels and
report accuracy over this balanced dataset.
1117
0 1 2 3 4 5 6 7 8 9 10Node Depth
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Lab
el P
rob
abil
ity
Label Probability vs. Node Depth
ConservativeLiberalNeutral / No Agreement
Figure 4: Proportion of liberal, conservative, and
neutral annotations with respect to node depth (dis-
tance from root). As we get farther from the root
of the tree, nodes are more likely to be neutral.
4.1 Baselines
? The RANDOM baseline chooses a label at ran-
dom from {liberal, conservative}.
? LR1, our most basic logistic regression base-
line, uses only bag of words (BoW) features.
? LR2 uses only BoW features. However, LR2
also includes phrase-level annotations as sep-
arate training instances.
7
? LR3 uses BoW features as well as syntac-
tic pseudo-word features from Greene &
Resnik (2009). These features from depen-
dency relations specify properties of verbs
(e.g., transitivity or nominalization).
8
? LR-(W2V) is a logistic regression model
trained on the average of the pretrained word
embeddings for each sentence (Section 2.2).
The LR-(W2V) baseline allows us to compare
against a strong lexical representation that encodes
syntactic and semantic information without the
RNN tree structure. (LR1, LR2) offer a compari-
son to simple bag of words models, while the LR3
baseline contrasts traditional syntactic features with
those learned by RNN models.
4.2 RNN Models
For RNN models, we generate a feature vector for
every node in the tree. Equation 1 allows us to
7
The Convote dataset was not annotated on the phrase
level, so we only provide a result for the IBC dataset.
8
We do not include phrase-level annotations in the LR3
feature set because the pseudo-word features can only be
computed from full sentence parses.
Model Convote IBC
RANDOM 50% 50%
LR1 64.7% 62.1%
LR2 ? 61.9%
LR3 66.9% 62.6%
LR-(W2V) 66.6% 63.7%
RNN1 69.4% 66.2%
RNN1-(W2V) 70.2% 67.1%
RNN2-(W2V) ? 69.3%
Table 1: Sentence-level bias detection accuracy.
The RNN framework, adding phrase-level data, and
initializing with word2vec all improve performance
over logistic regression baselines. The LR2 and
RNN2-(W2V) models were not trained on Convote
since it lacks phrase annotations.
percolate the representations to the root of the tree.
We generate the final instance representation by
concatenating the root vector and the average of
all other vectors (Socher et al, 2011b). We train
an L
2
-regularized logistic regression model over
these concatenated vectors to obtain final accuracy
numbers on the sentence level.
To analyze the effects of initialization and
phrase-level annotations, we report results for three
different RNN settings. All three models were im-
plemented as described in Section 2 with the non-
linearity f set to the normalized tanh function,
f(v) =
tanh(v)
?tanh(v)?
. (7)
We perform 10-fold cross-validation on the training
data to find the best RNN hyperparameters.
9
We report results for RNN models with the fol-
lowing configurations:
? RNN1 initializes all parameters randomly and
uses only sentence-level labels for training.
? RNN1-(W2V) uses the word2vec initialization
described in Section 2.2 but is also trained on
only sentence-level labels.
? RNN2-(W2V) is initialized using word2vec
embeddings and also includes annotated
phrase labels in its training. For this model,
we also introduce a hyperparameter ? that
weights the error at annotated nodes (1? ?)
higher than the error at unannotated nodes (?);
since we have more confidence in the anno-
tated labels, we want them to contribute more
towards the objective function.
9
[?
W
e
=1e-6, ?
W
=1e-4, ?
W
cat
=1e-3, ? = 0.3]
1118
For all RNN models, we set the word vector
dimension d to 300 to facilitate direct comparison
against the LR-(W2V) baseline.
10
5 Where Compositionality Helps Detect
Ideological Bias
In this section, we examine the RNN models to see
why they improve over our baselines. We also give
examples of sentences that are correctly classified
by our best RNN model but incorrectly classified by
all of the baselines. Finally, we investigate sentence
constructions that our model cannot handle and
offer possible explanations for these errors.
Experimental Results Table 1 shows the RNN
models outperforming the bag-of-words base-
lines as well as the word2vec baseline on both
datasets. The increased accuracy suggests that the
trained RNNs are capable of detecting bias polar-
ity switches at higher levels in parse trees. While
phrase-level annotations do not improve baseline
performance, the RNN model significantly bene-
fits from these annotations because the phrases are
themselves derived from nodes in the network struc-
ture. In particular, the phrase annotations allow our
best model to detect bias accurately in complex
sentences that the baseline models cannot handle.
Initializing the RNN W
e
matrix with word2vec
embeddings improves accuracy over randomly ini-
tialization by 1%. This is similar to improvements
from pretrained vectors from neural language mod-
els (Socher et al, 2011b).
We obtain better results on Convote than on IBC
with both bag-of-words and RNN models. This
result was unexpected since the Convote labels
are noisier than the annotated IBC labels; however,
there are three possible explanations for the discrep-
ancy. First, Convote has twice as many sentences
as IBC, and the extra training data might help the
model more than IBC?s better-quality labels. Sec-
ond, since the sentences in Convote were originally
spoken, they are almost half as short (21.3 words
per sentence) as those in the IBC (42.2 words per
sentence). Finally, some information is lost at ev-
ery propagation step, so RNNs are able to model
the shorter sentences in Convote more effectively
than the longer IBC sentences.
Qualitative Analysis As in previous work
(Socher et al, 2011b), we visualize the learned
10
Using smaller vector sizes (d ? {50, 100}, as in previous
work) does not significantly change accuracy.
vector space by listing the most probable n-grams
for each political affiliation in Table 2. As expected,
conservatives emphasize values such as freedom
and religion while disparaging excess government
spending and their liberal opposition. Meanwhile,
liberals inveigh against the gap between the rich
and the poor while expressing concern for minority
groups and the working class.
Our best model is able to accurately model the
compositional effects of bias in sentences with com-
plex syntactic structures. The first three sentences
in Figure 5 were correctly classified by our best
model (RNN2-(W2V)) and incorrectly classified by
all of the baselines. Figures 5A and C show tradi-
tional conservative phrases, ?free market ideology?
and ?huge amounts of taxpayer money?, that switch
polarities higher up in the tree when combined with
phrases such as ?made worse by? and ?saved by?.
Figure 5B shows an example of a bias polarity
switch in the opposite direction: the sentence neg-
atively portrays supporters of nationalized health
care, which our model picks up on.
Our model often makes errors when polarity
switches occur at nodes that are high up in the
tree. In Figure 5D, ?be used as an instrument to
achieve charitable or social ends? reflects a lib-
eral ideology, which the model predicts correctly.
However, our model is unable to detect the polarity
switch when this phrase is negated with ?should
not?. Since many different issues are discussed
in the IBC, it is likely that our dataset has too few
examples of some of these issues for the model to
adequately learn the appropriate ideological posi-
tions, and more training data would resolve many
of these errors.
6 Related Work
A growing NLP subfield detects private states such
as opinions, sentiment, and beliefs (Wilson et al,
2005; Pang and Lee, 2008) from text. In general,
work in this category tends to combine traditional
surface lexical modeling (e.g., bag-of-words) with
hand-designed syntactic features or lexicons. Here
we review the most salient literature related to the
present paper.
6.1 Automatic Ideology Detection
Most previous work on ideology detection ignores
the syntactic structure of the language in use in
favor of familiar bag-of-words representations for
1119
be used as an instrument to 
achieve charitable or social ends
should notthe law
X
X
X
nationalized health care
An entertainer once
said a sucker is born
every minute , and
surely this is the
case with
those who
support
made worse by
the implementing
Thus , the harsh
conditions for
farmers caused
by a number of
factors ,
, have created a 
continuing stream of 
people leaving the 
countryside and going 
to live in cities that do 
not have jobs for them .
of free-market
ideology
huge 
amounts of 
taxpayer 
money
saved byBut taxpayers 
do know
already that 
TARP was
designed in a 
way that
allowed
to continue to 
show the same 
arrogant traits 
that should have
destroyed their
companies .
the same 
corporations
who were
A B
C D
Figure 5: Predictions by RNN2-(W2V) on four sentences from the IBC. Node color is the true label (red
for conservative, blue for liberal), and an ?X? next to a node means the model?s prediction was wrong. In
A and C, the model accurately detects conservative-to-liberal polarity switches, while in B it correctly
predicts the liberal-to-conservative switch. In D, negation confuses our model.
the sake of simplicity. For example, Gentzkow
and Shapiro (2010) derive a ?slant index? to rate
the ideological leaning of newspapers. A newspa-
per?s slant index is governed by the frequency of
use of partisan collocations of 2-3 tokens. Simi-
larly, authors have relied on simple models of lan-
guage when leveraging inferred ideological posi-
tions. E.g., Gerrish and Blei (2011) predict the
voting patterns of Congress members based on bag-
of-words representations of bills and inferred polit-
ical leanings of those members.
Recently, Sim et al (2013) have proposed a
model to infer mixtures of ideological positions
in documents, applied to understanding the evolu-
tion of ideological rhetoric used by political can-
didates during the campaign cycle. They use an
HMM-based model, defining the states as a set
of fine-grained political ideologies, and rely on
a closed set of lexical bigram features associated
with each ideology, inferred from a manually la-
beled ideological books corpus. Although it takes
elements of discourse structure into account (cap-
turing the?burstiness? of ideological terminology
usage), their model explicitly ignores intrasenten-
tial contextual influences of the kind seen in Fig-
ure 1. Other approaches on the document level use
topic models to analyze bias in news articles, blogs,
and political speeches (Ahmed and Xing, 2010; Lin
et al, 2008; Nguyen et al, 2013).
6.2 Subjectivity Detection
Detecting subjective language, which conveys opin-
ion or speculation, is a related NLP problem. While
sentences lacking subjective language may con-
tain ideological bias (e.g., the topic of the sen-
tence), highly-opinionated sentences likely have
obvious ideological leanings. In addition, senti-
ment and subjectivity analysis offers methodolog-
ical approaches that can be applied to automatic
bias detection.
Wiebe et al (2004) show that low-frequency
words and some collocations are a good indica-
tors of subjectivity. More recently, Recasens et al
(2013) detect biased words in sentences using indi-
cator features for bias cues such as hedges and fac-
tive verbs in addition to standard bag-of-words and
part-of-speech features. They show that this type of
linguistic information dramatically improves per-
formance over several standard baselines.
Greene and Resnik (2009) also emphasize the
connection between syntactic and semantic rela-
tionships in their work on ?implicit sentiment?,
1120
n Most conservative n-grams Most liberal n-grams
1 Salt, Mexico, housework, speculated, consensus, lawyer,
pharmaceuticals, ruthless, deadly, Clinton, redistribution
rich, antipsychotic, malaria, biodiversity, richest, gene,
pesticides, desertification, Net, wealthiest, labor, fertil-
izer, nuclear, HIV
3 prize individual liberty, original liberal idiots, stock mar-
ket crash, God gives freedom, federal government inter-
ference, federal oppression nullification, respect individ-
ual liberty, Tea Party patriots, radical Sunni Islamists,
Obama stimulus programs
rich and poor,?corporate greed?, super rich pay, carrying
the rich, corporate interest groups, young women work-
ers, the very rich, for the rich, by the rich, soaking the
rich, getting rich often, great and rich, the working poor,
corporate income tax, the poor migrants
5 spending on popular government programs, bailouts and
unfunded government promises, North America from
external threats, government regulations place on busi-
nesses, strong Church of Christ convictions, radical Is-
lamism and other threats
the rich are really rich, effective forms of worker partic-
ipation, the pensions of the poor, tax cuts for the rich,
the ecological services of biodiversity, poor children and
pregnant women, vacation time for overtime pay
7 government intervention helped make the Depression
Great, by God in His image and likeness, producing
wealth instead of stunting capital creation, the tradi-
tional American values of limited government, trillions
of dollars to overseas oil producers, its troubled assets to
federal sugar daddies, Obama and his party as racialist
fanatics
African Americans and other disproportionately poor
groups; the growing gap between rich and poor; the
Bush tax cuts for the rich; public outrage at corporate
and societal greed; sexually transmitted diseases , most
notably AIDS; organize unions or fight for better condi-
tions, the biggest hope for health care reform
Table 2: Highest probability n-grams for conservative and liberal ideologies, as predicted by the RNN2-
(W2V) model.
which refers to sentiment carried by sentence struc-
ture and not word choice. They use syntactic depen-
dency relation features combined with lexical infor-
mation to achieve then state-of-the-art performance
on standard sentiment analysis datasets. However,
these syntactic features are only computed for a
thresholded list of domain-specific verbs. This
work extends their insight of modeling sentiment
as an interaction between syntax and semantics to
ideological bias.
Future Work There are a few obvious directions
in which this work can be expanded. First, we can
consider more nuanced political ideologies beyond
liberal and conservative. We show that it is pos-
sible to detect ideological bias given this binary
problem; however, a finer-grained study that also
includes neutral annotations may reveal more sub-
tle distinctions between ideologies. While acquir-
ing data with obscure political biases from the IBC
or Convote is unfeasible, we can apply a similar
analysis to social media (e.g., Twitter or Facebook
updates) to discover how many different ideologies
propagate in these networks.
Another direction is to implement more sophis-
ticated RNN models (along with more training
data) for bias detection. We attempted to apply
syntactically-untied RNNs (Socher et al, 2013a)
to our data with the idea that associating separate
matrices for phrasal categories would improve rep-
resentations at high-level nodes. While there were
too many parameters for this model to work well
here, other variations might prove successful, espe-
cially with more data. Finally, combining sentence-
level and document-level models might improve
bias detection at both levels.
7 Conclusion
In this paper we apply recursive neural networks
to political ideology detection, a problem where
previous work relies heavily on bag-of-words mod-
els and hand-designed lexica. We show that our
approach detects bias more accurately than existing
methods on two different datasets. In addition, we
describe an approach to crowdsourcing ideological
bias annotations. We use this approach to create a
new dataset from the IBC, which is labeled at both
the sentence and phrase level.
Acknowledgments
We thank the anonymous reviewers, Hal Daum?e,
Yuening Hu, Yasuhiro Takayama, and Jyothi Vinju-
mur for their insightful comments. We also want to
thank Justin Gross for providing the IBC and Asad
Sayeed for help with the Crowdflower task design,
as well as Richard Socher and Karl Moritz Her-
mann for assisting us with our model implemen-
tations. This work was supported by NSF Grant
CCF-1018625. Boyd-Graber is also supported by
NSF Grant IIS-1320538. Any opinions, findings,
conclusions, or recommendations expressed here
are those of the authors and do not necessarily re-
flect the view of the sponsor.
1121
References
Amr Ahmed and Eric P Xing. 2010. Staying informed: super-
vised and semi-supervised multi-view topical analysis of
ideological perspective. In EMNLP.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent
J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram
models of natural language. Comp. Ling., 18(4):467?479.
Royce Carroll, Jeffrey B Lewis, James Lo, Keith T Poole, and
Howard Rosenthal. 2009. Measuring bias and uncertainty
in dw-nominate ideal point estimates via the parametric
bootstrap. Political Analysis, 17(3):261?275.
Ronan Collobert and Jason Weston. 2008. A unified architec-
ture for natural language processing: Deep neural networks
with multitask learning. In ICML.
Frank E Dardis, Frank R Baumgartner, Amber E Boydstun,
Suzanna De Boef, and Fuyuan Shen. 2008. Media framing
of capital punishment and its impact on individuals? cogni-
tive responses. Mass Communication & Society, 11(2):115?
140.
Matthew Gentzkow and Jesse M Shapiro. 2010. What drives
media slant? evidence from us daily newspapers. Econo-
metrica, 78(1):35?71.
Sean Gerrish and David M Blei. 2011. Predicting legislative
roll calls from text. In ICML.
Christoph Goller and Andreas Kuchler. 1996. Learning task-
dependent distributed representations by backpropagation
through structure. In Neural Networks, 1996., IEEE Inter-
national Conference on, volume 1.
Stephan Greene and Philip Resnik. 2009. More than words:
Syntactic packaging and implicit sentiment. In NAACL.
Tim Groseclose and Jeffrey Milyo. 2005. A measure of media
bias. The Quarterly Journal of Economics, 120(4):1191?
1237.
Justin Gross, Brice Acree, Yanchuan Sim, and Noah A Smith.
2013. Testing the etch-a-sketch hypothesis: A compu-
tational analysis of mitt romney?s ideological makeover
during the 2012 primary vs. general elections. In APSA
2013 Annual Meeting Paper.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and
Takashi Chikayama. 2013. Simple customization of recur-
sive neural networks for semantic relation classification. In
EMNLP.
Karl Moritz Hermann and Phil Blunsom. 2013. The Role of
Syntax in Vector Space Models of Compositional Seman-
tics. In ACL.
George Lakoff. 2002. Moral Politics: How Liberals and Con-
servatives Think, Second Edition. University of Chicago
Press.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann. 2008.
A joint topic and perspective model for ideological dis-
course. In Machine Learning and Knowledge Discovery in
Databases, pages 17?32. Springer.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
2013. Efficient estimation of word representations in vector
space. arXiv preprint arXiv:1301.3781.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2013. Lexical and hierarchical topic regression. In NIPS,
pages 1106?1114.
David Niven. 2003. Objective evidence on media bias: News-
paper coverage of congressional party switchers. Journal-
ism & Mass Communication Quarterly, 80(2):311?326.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment
analysis. Foundations and trends in information retrieval,
2(1-2).
James W. Pennebaker, Martha E. Francis, and Roger J. Booth.
2001. Linguistic inquiry and word count [computer soft-
ware]. Mahwah, NJ: Erlbaum Publishers.
Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan
Jurafsky. 2013. Linguistic models for analyzing and de-
tecting biased language.
Asad B Sayeed, Jordan Boyd-Graber, Bryan Rusk, and Amy
Weinberg. 2012. Grammatical structures for word-level
sentiment detection. In NAACL.
Burr Settles. 2011. Closing the loop: Fast, interactive
semi-supervised annotation with queries on features and
instances. In EMNLP.
Yanchuan Sim, Brice Acree, Justin H Gross, and Noah A
Smith. 2013. Measuring ideological proportions in politi-
cal speeches. In EMNLP.
Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y.
Ng, and Christopher D. Manning. 2011a. Dynamic Pool-
ing and Unfolding Recursive Autoencoders for Paraphrase
Detection. In NIPS.
Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y.
Ng, and Christopher D. Manning. 2011b. Semi-Supervised
Recursive Autoencoders for Predicting Sentiment Distribu-
tions. In EMNLP.
Richard Socher, John Bauer, Christopher D. Manning, and
Andrew Y. Ng. 2013a. Parsing With Compositional Vector
Grammars. In ACL.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang,
Christopher D Manning, Andrew Y Ng, and Christopher
Potts. 2013b. Recursive deep models for semantic compo-
sitionality over a sentiment treebank. In EMNLP.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the
vote: Determining support or opposition from Congres-
sional floor-debate transcripts. In EMNLP.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell,
and Melanie Martin. 2004. Learning subjective language.
Comp. Ling., 30(3):277?308.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In EMNLP.
Tae Yano, Philip Resnik, and Noah A Smith. 2010. Shedding
(a thousand points of) light on biased language. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechanical
Turk, pages 152?158.
1122
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1123?1133,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Unified Model for Soft Linguistic Reordering Constraints
in Statistical Machine Translation
Junhui Li
?
Yuval Marton
?
Philip Resnik
?
Hal Daum
?
e III
?
?
UMIACS, University of Maryland, College Park, MD
{lijunhui, resnik, hal}@umiacs.umd.edu
?
Microsoft Corp., City Center Plaza, Bellevue, WA
yumarton@microsoft.com
Abstract
This paper explores a simple and effec-
tive unified framework for incorporating
soft linguistic reordering constraints into a
hierarchical phrase-based translation sys-
tem: 1) a syntactic reordering model
that explores reorderings for context free
grammar rules; and 2) a semantic re-
ordering model that focuses on the re-
ordering of predicate-argument structures.
We develop novel features based on both
models and use them as soft constraints
to guide the translation process. Ex-
periments on Chinese-English translation
show that the reordering approach can sig-
nificantly improve a state-of-the-art hier-
archical phrase-based translation system.
However, the gain achieved by the seman-
tic reordering model is limited in the pres-
ence of the syntactic reordering model,
and we therefore provide a detailed analy-
sis of the behavior differences between the
two.
1 Introduction
Reordering models in statistical machine transla-
tion (SMT) model the word order difference when
translating from one language to another. The
popular distortion or lexicalized reordering mod-
els in phrase-based SMT make good local pre-
dictions by focusing on reordering on word level,
while the synchronous context free grammars in
hierarchical phrase-based (HPB) translation mod-
els are capable of handling non-local reordering
on the translation phrase level. However, reorder-
ing, especially without any help of external knowl-
edge, remains a great challenge because an ac-
curate reordering is usually beyond these word
level or translation phrase level reordering mod-
els? ability. In addition, often these translation
models fail to respect linguistically-motivated syn-
tax and semantics. As a result, they tend to pro-
duce translations containing both syntactic and se-
mantic reordering confusions. In this paper our
goal is to take advantage of syntactic and seman-
tic parsing to improve translation quality. Rather
than introducing reordering models on either the
word level or the translation phrase level, we pro-
pose a unified approach to modeling reordering on
the linguistic unit level, e.g., syntactic constituents
and semantic roles. The reordering unit falls into
multiple granularities, from single words to more
complex constituents and semantic roles, and of-
ten crosses translation phrases. To show the ef-
fectiveness of our reordering models, we integrate
both syntactic constituent reordering models and
semantic role reordering models into a state-of-
the-art HPB system (Chiang, 2007; Dyer et al,
2010). We further contrast it with a stronger base-
line, already including fine-grained soft syntac-
tic constraint features (Marton and Resnik, 2008;
Chiang et al, 2008). The general ideas, however,
are applicable to other translation models, e.g.,
phrase-based model, as well.
Our syntactic constituent reordering model con-
siders context free grammar (CFG) rules in the
source language and predicts the reordering of
their elements on the target side, using word align-
ment information. Due to the fact that a con-
stituent, especially a long one, usually maps into
multiple discontinuous blocks in the target lan-
guage, there is more than one way to describe the
monotonicity or swapping patterns; we therefore
design two reordering models: one is based on the
leftmost aligned target word and the other based
on the rightmost target word.
While recently there has also been some encour-
aging work on incorporating semantic structure
(or, more specifically, predicate-argument struc-
ture: PAS) reordering in SMT, it is still an open
question whether semantic structure reordering
1123
strongly overlaps with syntactic structure reorder-
ing, since the semantic structure is closely tied to
syntax. To this end, we employ the same reorder-
ing framework as syntactic constituent reordering
and focus on semantic roles in a PAS. We then an-
alyze the differences between the syntactic and se-
mantic features.
The contributions of this paper include the fol-
lowing:
? We introduce novel soft reordering con-
straints, using syntactic constituents or se-
mantic roles, composed over word alignment
information in translation rules used during
decoding time;
? We introduce a unified framework to incor-
porate syntactic and semantic reordering con-
straints;
? We provide a detailed analysis providing in-
sight into why the semantic reordering model
is significantly less effective when syntactic
reordering features are also present.
The rest of the paper is organized as follows.
Section 2 provides an overview of HPB transla-
tion model. Section 3 describes the details of our
unified reordering models. Section 4 gives our ex-
perimental results and Section 5 discusses the be-
havior difference between syntactic constituent re-
ordering and semantic role reordering. Section 6
reviews related work and, finally Section 7 con-
cludes the paper.
2 HPB Translation Model: an Overview
In HPB models (Chiang, 2007), synchronous rules
take the formX ? ??, ?,??, whereX is the non-
terminal symbol, ? and ? are strings of lexical
items and non-terminals in the source and target
side, respectively, and ? indicates the one-to-one
correspondence between non-terminals in ? and ?.
Each such rule is associated with a set of transla-
tion model features {?
i
}, such as phrase transla-
tion probability p (? | ?) and its inverse p (? | ?),
the lexical translation probability p
lex
(? | ?) and
its inverse p
lex
(? | ?), and a rule penalty that af-
fects preference for longer or shorter derivations.
Two other widely used features are a target lan-
guage model feature and a target word penalty.
Given a derivation d, its translation log-
probability is estimated as:
logP (d) ?
?
i
?
i
?
i
(d)
(1)
	 ?
PAS	 ?
A0	 ?(NP)	 ? TMP	 ?(NP)	 ? Pre	 ?(VBD)	 ? A1	 ?(NP)	 ?Applicants	 ?	 ?	 ?	 ?	 ?	 ?yesterday	 ?	 ?	 ?	 ?	 ?	 ?filled	 ?	 ?	 ?	 ?	 ?	 ?the	 ?forms	 ?
Figure 1: Example of predicate-argument struc-
ture.
where ?
i
is the corresponding weight of feature ?
i
.
See (Chiang, 2007) for more details.
3 Unified Linguistic Reordering Models
As mentioned earlier, the linguistic reordering unit
is the syntactic constituent for syntactic reorder-
ing, and the semantic role for semantic reordering.
The syntactic reordering model takes a CFG rule
(e.g., VP ? VP PP PP) and models the reorder-
ing of the constituents on the left hand side by ex-
amining their translation or visit order according
to the target language. For the semantic reorder-
ing model, it takes a PAS and models its reorder-
ing on the target side. Figure 1 shows an example
of a PAS where the predicate (Pre) has two core
arguments (A0 and A1) and one adjunct (TMP).
Note that we refer all core arguments, adjuncts,
and predicates as semantic roles; thus we say the
PAS in Figure 1 has 4 roles. According to the an-
notation principles in (Chinese) PropBank (Palmer
et al, 2005; Xue and Palmer, 2009), all the roles
in a PAS map to a corresponding constituent in the
parse tree, and these constituents (e.g., NPs and
VBD in Figure 1) do not overlap with each other.
Next, we use a CFG rule to describe our syn-
tactic reordering model. Treating the two forms
of reorderings in a unified way, the semantic re-
ordering model is obtainable by regarding a PAS
as a CFG rule and considering a semantic role as a
constituent.
Because the translation of a source constituent
might result in multiple discontinuous blocks,
there can be several ways to describe or group
the reordering patterns. Therefore, we design
two general constituent reordering sub-models.
One is based on the leftmost aligned word (left-
most reordering model) and the other is based on
the rightmost aligned word (rightmost reordering
model), as follows. Figure 2 shows the model-
ing steps for the leftmost reordering model. Fig-
ure 2(a) is an example of a CFG rule in the source
1124
	 ?
XP	 ?XP1	 ? XP2	 ? XP3	 ? XP4	 ?f3	 ?	 ?f4	 ? f5	 ?	 ? f6	 ?	 ?f7	 ? f8	 ?...	 ?
...	 ?
...	 ?
...	 ?
?	 ?	 ?	 ?e2	 ?	 ?	 ?	 ?	 ?e3	 ?	 ?	 ?	 ?e4	 ?	 ?	 ?	 ?e5	 ?	 ?	 ?	 ?e6	 ?	 ?	 ?	 ?e7	 ?	 ?	 ?	 ?e8	 ?	 ?	 ?	 ?e9	 ?	 ??	 ? XP1	 ? XP2	 ? XP3	 ? XP4	 ?e2	 ? e3	 ? e5	 ?(a)	 ?a	 ?CFG	 ?rule	 ?and	 ?its	 ?alignment	 ? (b)	 ?leftmost	 ?aligned	 ?target	 ?words	 ?
XP1	 ? XP2	 ? XP3	 ? XP4	 ?1	 ? 4	 ? 2	 ? 3	 ? XP1	 ? XP2	 ? XP3	 ? XP4	 ?DM	 ? DS	 ? M	 ?(c)	 ?visit	 ?order	 ? (d)	 ?reordering	 ?types	 ?
Figure 2: Modeling process illustration for leftmost reordering model.
parse tree and its word alignment links to the target
language. Note that constituent XP
4
, which covers
word f
8
, has no alignment. Then for each XP
i
, we
find the leftmost target word which is aligned to a
source word covered by XP
i
. Figure 2(b) shows
that the leftmost target words for XP
1
, XP
2
, and
XP
3
are e
2
, e
5
, and e
3
, respectively, while XP
4
has no aligned target word. Then we get visit
order V = {v
i
} for {XP
i
} in the transformation
from Figure 2(b) to Figure 2(c), with the follow-
ing strategies for special cases:
? if the first constituent XP
1
is unaligned, we
add a NULL word at the beginning of the tar-
get side and link XP
1
to the NULL word;
? if a constituent XP
i
(i > 1) is unaligned, we
add a link to the target word which is aligned
to XP
i?1
, e.g., XP
4
will be linked to e
3
; and
? if k constituents XP
m
1
. . .XP
m
k
(m
1
<
. . . < m
k
) are linked to the same target word,
then v
m
i
= v
m
i+1
? 1, e.g., since XP
3
and
XP
4
are both linked to e
3
, then v
3
= v
4
? 1.
Finally Figure 2(d) converts the visit order V =
{v
1
, . . . v
n
} into a sequence of leftmost reordering
types LRT = {lrt
1
, . . . , lrt
n?1
}. For every two
adjacent constituents XP
i
and XP
i+1
with corre-
sponding visit order v
i
and v
i+1
, their reordering
could be one of the following:
? Monotone (M) if v
i+1
= v
i
+ 1;
? Discontinuous Monotone (DM) if v
i+1
> v
i
+ 1;
? Swap (S) if v
i+1
= v
i
? 1;
? Discontinuous Swap (DS) if v
i+1
< v
i
? 1.
Up to this point, we have generated a se-
quence of leftmost reordering types LRT =
{lrt
1
, . . . , lrt
n?1
} for a given CFG rule cfg:
XP ? XP
1
. . .XP
n
. The leftmost reordering
model takes the following form:
score
lrt
(cfg) = P
l
(lrt
1
, . . . , lrt
n?1
| ? (cfg))
(2)
where ? (cfg) indicates the surrounding context of
the CFG. By assuming that any two reordering
types in LRT = {lrt
1
, . . . , lrt
n?1
} are indepen-
dent of each other, we reformulate Eq. 2 into:
score
lrt
(cfg) =
n?1
?
i=1
P
l
(lrt
i
| ? (cfg))
(3)
Similarly, the sequence of rightmost reordering
types RRT can be decided for a CFG rule XP ?
XP
1
. . .XP
n
.
Accordingly, for a PAS pas: PAS ? R
1
. . .R
n
,
we can obtain its sequences of leftmost and right-
most reordering types by using the same way de-
scribed above.
3.1 Probability Estimation
In order to predict either the leftmost or right-
most reordering type for two adjacent constituents,
we use a maximum entropy classifier to esti-
mate the probability of the reordering type rt ?
{M,DM,S,DS} as follows:
P (rt | ? (cfg)) =
exp (
?
k
?
k
f
k
(rt, ? (cfg)))
?
rt
?
exp (
?
k
?
k
f
i
(rt
?
, ? (cfg)))
(4)
where f
k
are binary features, ?
k
are the weights of
these features. Most of our features f
k
are syntax-
based. For XP
i
and XP
i+1
in cfg, the features
1125
#Index Feature
cf1 L(XP
i
) & L(XP
i+1
) & L(XP)
cf2
for each XP
j
(j < i)
L(XP
i
) & L(XP
i+1
) & L(XP) & L(XP
j
)
cf3
for each XP
j
(j > i+ 1)
L(XP
i
) & L(XP
i+1
) & L(XP) & L(XP
j
)
cf4 L(XP
i
) & L(XP
i+1
) & P(XP
i
)
cf5 L(XP
i
) & L(XP
i+1
) &H(XP
i
)
cf6 L(XP
i
) & L(XP
i+1
) & P(XP
i+1
)
cf7 L(XP
i
) & L(XP
i+1
) &H(XP
i+1
)
cf8 L(XP
i
) & L(XP
i+1
) & S(XP
i
)
cf9 L(XP
i
) & L(XP
i+1
) & S(XP
i+1
)
cf10 L(XP
i
) & L(XP)
cf11 L(XP
i+1
) & L(XP)
Table 1: Features adopted in the syntactic leftmost
and rightmost reordering models. L (XP) returns
the syntactic category of XP, e.g., NP, VP, PP etc.;
H (XP) returns the head word of XP; P (XP) re-
turns the POS tagger of the head word; S (XP)
returns the translation status of XP on the target
language: un. if it is untranslated; cont. if it is
a continuous block; and discont. if it maps into
multiple discontinuous blocks.
are aimed to examine which of them should be
translated first. Therefore, most features share two
common components: the syntactic categories of
XP
i
and XP
i+1
. Table 1 shows the features used in
syntactic leftmost and rightmost reordering mod-
els. Note that we use the same features for both.
Although the semantic reordering model is
structured in precisely the same way, we use dif-
ferent feature sets to predict the reordering be-
tween two semantic roles. Given the two adjacent
roles R
i
and R
i+1
in a PAS pas, Table 2 shows the
features that are used in the semantic leftmost and
rightmost reordering models.
3.2 Integrating into the HPB Model
For models with syntactic reordering, we add two
new features (i.e., one for the leftmost reorder-
ing model and the other for the rightmost reorder-
ing model) into the log-linear translation model in
Eq. 1. Unlike the conventional phrase and lexi-
cal translation features, whose values are phrase
pair-determined and thus can be calculated offline,
the value of the reordering features can only be
obtained during decoding time, and requires word
alignment information as well. Before we present
the algorithm integrating the reordering models,
we define the following functions by assuming
XP
i
and XP
i+1
are the constituent pair of interest
in CFG rule cfg, H is the translation hypothesis
and a is its word alignment:
#Index Feature
rf1
R(R
i
) &R(R
i+1
) & P(pas)
R(R
i
) &R(R
i+1
)
rf2
for each R
j
(j < i)
R(R
i
) &R(R
i+1
) &R(R
j
) & P(pas)
R(R
i
) &R(R
i+1
) &R(R
j
)
rf3
for each R
j
(j > i+ 1)
R(R
i
) &R(R
i+1
) &R(R
j
) & P(pas)
R(R
i
) &R(R
i+1
) &R(R
j
)
rf4 R(R
i
) &R(R
i+1
) & P(R
i
)
rf5 R(R
i
) &R(R
i+1
) &H(R
i
)
rf6 R(R
i
) &R(R
i+1
) & L(R
i
)
rf7 R(R
i
) &R(R
i+1
) & P(R
i+1
)
rf8 R(R
i
) &R(R
i+1
) &H(R
i+1
)
rf9 R(R
i
) &R(R
i+1
) & L(R
i+1
)
rf10 R(R
i
) &R(R
i+1
) & S(R
i
)
rf11 R(R
i
) &R(R
i+1
) & S(R
i+1
)
rf12
R(R
i
) & P(pas)
R(R
i
)
rf13
R(R
i+1
) & P(pas)
R(R
i+1
)
Table 2: Features adopted in the semantic leftmost
and rightmost reordering models. P (pas) returns
the predicate content of pas;R (R) returns the role
type of R, e.g., Pred, A0, TMP, etc. For features
rf1, rf2, rf3, rf12 and rf13, we include another ver-
sion which excludes the predicate content P(pas)
for reasons of sparsity.
? F
1
(w
1
, w
2
, XP): returns true if constituent XP is
within the span from word w
1
to w
2
; otherwise returns
false.
? F
2
(H, cfg, XP
i
, XP
i+1
) returns true if the reordering
of the pair ?XP
i
, XP
i+1
? in rule cfg has not been calcu-
lated yet; otherwise returns false.
? F
3
(H, a, XP
i
, XP
i+1
) returns the leftmost and right-
most reordering types for the constituent pair ?XP
i
,
XP
i+1
?, given alignment a, according to Section 3.
? F
4
(rt, cfg, XP
i
, XP
i+1
) returns the probability of
leftmost reordering type rt for the constituent pair
?XP
i
, XP
i+1
? in rule cfg.
? F
5
(rt, cfg, XP
i
, XP
i+1
) returns the probability of
rightmost reordering type rt for the constituent pair
?XP
i
, XP
i+1
? in rule cfg.
Algorithm 1 integrates the syntactic leftmost
and rightmost reordering models into a CKY-style
decoder whenever a new hypothesis is generated.
Given a hypothesis H with its alignment a, it tra-
verses all CFG rules in the parse tree and sees if
two adjacent constituents are conditioned to trig-
ger the reordering models (lines 2-4). For each
pair of constituents, it first extracts its leftmost and
rightmost reordering types (line 6) and then gets
their respective probabilities returned by the max-
imum entropy classifiers defined in Section 3.1
1126
Algorithm 1: Integrating the syntactic reordering models
into a CKY-style decoder
Input: Sentence f in the source language
Parse tree t of f
All CFG rules {cfg} in t
Hypothesis H spanning from word w
1
to w
2
Alignment a of H
Output: Log-Probabilities of the syntactic leftmost
and rightmost reordering models
1. set l prob = r
p
rob = 0.0
2. foreach cfg in {cfg}
3. foreach pair XP
i
and XP
i+1
in cfg
4. if F
1
(w
1
, w
2
, XP
i
) = false or
F
1
(w
1
, w
2
, XP
i+1
) = false or
F
2
(H, cfg, XP
i
, XP
i+1
) = false
5. continue
6. (l type, r type) = F
3
(H, a, XP
i
, XP
i+1
)
7. l prob += logF
4
(l type, cfg,XP
i
,XP
i+1
)
8. r prob += logF
5
(r type, cfg,XP
i
,XP
i+1
)
9. return (l prob, r prob)
(lines 7-8). Then the algorithm returns two log-
probabilities of the syntactic reordering models.
Note that Function F
1
returns true if hypothesis
H fully covers, or fully contains, constituentXP
i
,
regardless of the reordering type of XP
i
. Do not
confuse any parsing tag XP
i
with the nameless
variables X
i
in Hiero or cdec rules.
For the semantic reordering models, we also
add two new features into the log-linear transla-
tion model. To get the two semantic reordering
model feature values, we simply use Algorithm 1
and its associated functions from F
1
to F
5
replac-
ing a CFG rule cfg with a PAS pas, and a con-
stituent XP
i
with a semantic role R
i
. Algorithm 1
therefore permits a unified treatment of syntactic
and PAS-based reordering, even though it is ex-
pressed in terms of syntactic reordering here for
ease of presentation.
4 Experiments
We have presented our unified approach to in-
corporating syntactic and semantic soft reorder-
ing constraints in an HPB system. In this section,
we test its effectiveness in Chinese-English trans-
lation.
4.1 Experimental Settings
For training we use 1.6M sentence pairs of the
non-UN and non-HK Hansards portions of NIST
MT training corpora, segmented with the Stan-
ford segmenter (Tseng et al, 2005). The En-
glish data is lowercased, tokenized and aligned
with GIZA++ (Och and Ney, 2000) to obtain bidi-
rectional alignments, which are symmetrized us-
ing the grow-diag-final-and method (Koehn et al,
2003). We train a 4-gram LM on the English
side of the corpus with 600M additional words
from non-NYT and non-LAT, randomly selected
portions of the Gigaword v4 corpus, using modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1996). We use the HPB decoder cdec (Dyer et
al., 2010), with Mr. Mira (Eidelman et al, 2013),
which is a k-best variant of MIRA (Chiang et al,
2008), to tune the parameters of the system.
We use NIST MT 06 dataset (1664 sentence
pairs) for tuning, and NIST MT 03, 05, and 08
datasets (919, 1082, and 1357 sentence pairs, re-
spectively) for evaluation.
1
We use BLEU (Pap-
ineni et al, 2002) for both tuning and evaluation.
To obtain syntactic parse trees and semantic
roles on the tuning and test datasets, we first
parse the source sentences with the Berkeley
Parser (Petrov and Klein, 2007), trained on the
Chinese Treebank 7.0 (Xue et al, 2005). We
then pass the parses to a Chinese semantic role
labeler (Li et al, 2010), trained on the Chinese
PropBank 3.0 (Xue and Palmer, 2009), to anno-
tate semantic roles for all verbal predicates (part-
of-speech tag VV, VE, or VC).
Our basic baseline system employs 19 basic
features: a language model feature, 7 transla-
tion model features, word penalty, unknown word
penalty, the glue rule, date, number and 6 pass-
through features. Our stronger baseline employs,
in addition, the fine-grained syntactic soft con-
straint features of Marton and Resnik (2008), here-
after MR08. The syntactic soft constraint features
include both MR08 exact-matching and cross-
boundary constraints (denoted XP= and XP+).
Since the syntactic parses of the tuning and test
data contain 29 types of constituent labels and 35
types of POS tags, we have 29 types of XP+ fea-
tures and 64 types of XP= features.
4.2 Model Training
To train the syntactic and semantic reordering
models, we use a gold alignment dataset.
2
It con-
tains 7,870 sentences with 191,364 Chinese words
and 261,399 English words. We first run syn-
1
http://www.itl.nist.gov/iad/mig//tests/mt
2
This dataset includes LDC2006E86, and newswire
parts of LDC2012T16, LDC2012T20, LDC2012T24, and
LDC2013T05. Indeed, the reordering models can also be
trained on the MT training data with its automatic alignment.
However, our preliminary experiments showed that the re-
ordering models trained on gold alignment yielded higher im-
provement.
1127
Reordering
Type
Syntactic Semantic
l-m r-m l-m r-m
M 73.5 80.6 63.8 67.9
DM 3.9 3.3 14.0 12.0
S 19.5 13.2 13.1 10.7
DS 3.2 3.0 9.1 9.5
#instance 199,234 66,757
Table 3: Reordering type distribution over the re-
ordering model?s training data. Hereafter, l-m and
r-m are for leftmost and rightmost, respectively.
tactic parsing and semantic role labeling on the
Chinese sentences, then train the models by us-
ing MaxEnt toolkit with L1 regularizer (Tsuruoka
et al, 2009).
3
Table 3 shows the reordering type
distribution over the training data. Interestingly,
about 17% of the syntactic instances and 16% of
the semantic instances differ in their leftmost and
rightmost reordering types, indicating that the left-
most/rightmost distinction is informative. We also
see that the number of semantic instances is about
1/3 of that of syntactic instances, but the entropy
of the semantic reordering classes is higher, indi-
cating the reordering of semantic roles is harder
than that of syntactic constituents.
A deeper examination of the reordering model?s
training data reveals that some constituent pairs
and semantic role pairs have a preference for a
specific reordering type (monotone or swap). In
order to understand how well the MR08 system
respects their reordering preference, we use the
gold alignment dataset LDC2006E86, in which
the source sentences are from the Chinese Tree-
bank, and thus both the gold parse trees and gold
predicate-argument structures are available. Ta-
ble 4 presents examples comparing the reordering
distribution between gold alignment and the out-
put of the MR08 system. For example, the first
row shows that based on the gold alignment, for
?PP,VP?, 16% are in monotone and 76% are in
swap reordering. However, our MR08 system out-
puts 46% of them in monotone and and 50% in
swap reordering. Hence, the reordering accuracy
for ?PP,VP? is 54%. Table 4 also shows that the
semantic reordering between core arguments and
predicates (e.g., ?Pred,A1?, ?A0,Pred?) has a less
ambiguous pattern than that between adjuncts and
other roles (e.g., ?LOC,Pred?, ?A0,TMP?), indicat-
ing the higher reordering flexibility of adjuncts.
3
http://www.logos.ic.i.u-tokyo.ac.jp/?tsuruoka/maxent/
Const. Pair
Gold MR08 output
M S M S acc.
PP VP 16 76 46 50 54
NP LC 26 74 58 42 50
DNP NP 24 72 78 19 39
CP NP 26 67 84 10 33
NP DEG 39 61 31 69 66
... ... ...
all 81 13 79 14 80
Role Pair
Gold MR08 output
M S M S acc.
Pred A1 84 6 82 9 72
A0 Pred 82 11 79 8 75
LOC Pred 17 30 36 25 49
A0 TMP 35 25 61 6 45
TMP Pred 30 22 49 19 43
... ... ...
all 63 13 73 9 64
Table 4: Examples of the reordering distribution
(%) of gold alignment and the MR08 system out-
put. For simplicity, we only focus on (M)onotone
and (S)wap based on leftmost reordering.
4.3 Translation Experiment Results
Our first group of experiments investigates
whether the syntactic reordering models are able
to improve translation quality in terms of BLEU.
To this end, we respectively add our syntactic re-
ordering models into both the baseline and MR08
systems. The effect is shown in the rows of ?+ syn-
reorder? in Table 5. From the table, we have the
following two observations.
? Although the HPB model is capable of
handling non-local phrase reordering using
synchronous context free grammars, both
our syntactic leftmost reordering model and
rightmost model are still able to achieve im-
provement over both the baseline and MR08.
This suggests that our syntactic reordering
features interact well with the MR08 syntac-
tic soft constraints: the XP+ and XP= fea-
tures focus on a single constituent each, while
our reordering features focus on a pair of con-
stituents each.
? There is no clear indication of whether the
leftmost reordering model works better than
the other. In addition, integrating both the
leftmost and rightmost reordering models has
limited improvement over a single reordering
model.
Our second group of experiments is to vali-
date the semantic reordering models. Results are
1128
System
Tuning Test
MT06 MT03 MT05 MT08 Avg.
Baseline 34.1 36.1 32.3 27.4 31.9
+
syn-
reorder
l-m 35.2 36.9? 33.6? 28.4? 33.0
r-m 35.2 37.2? 33.7? 28.6? 33.2
both 35.6 37.1? 33.6? 28.8? 33.1
+
sem-
reorder
l-m 34.4 36.7? 33.0? 27.8? 32.5
r-m 34.5 36.7? 33.1? 27.8? 32.5
both 34.5 37.0? 33.6? 27.7? 32.8
+syn+sem 35.5 37.3? 33.7? 29.0? 33.3
MR08 35.6 37.4 34.2 28.7 33.4
+
syn-
reorder
l-m 36.0 38.2? 35.0? 29.2? 34.1
r-m 36.0 38.1? 34.8? 29.2? 34.0
both 35.9 38.2? 35.3? 29.5? 34.3
+
sem-
reorder
l-m 35.8 37.6? 34.7? 28.7 33.7
r-m 35.8 37.4 34.5? 28.8 33.6
both 35.8 37.6? 34.7? 28.8 33.7
+syn+sem 36.1 38.4? 35.2? 29.5? 34.4
Table 5: System performance in BLEU scores.
?/?: significant over baseline or MR08 at 0.01
/ 0.05, respectively, as tested by bootstrap re-
sampling (Koehn, 2004)
shown in the rows of ?+ sem-reorder? in Table 5.
Here we observe:
? The semantic reordering models also achieve
significant gain of 0.8 BLEU on average over
the baseline system, demonstrating the ef-
fectiveness of PAS-based reordering. How-
ever, the gain diminishes to 0.3 BLEU on the
MR08 system.
? The syntactic reordering models outperform
the semantic reordering models on both the
baseline and MR08 systems.
Finally, we integrate both the syntactic and se-
mantic reordering models into the final system.
The two models collectively achieve a gain of up
to 1.4 BLEU over the baseline and 1.0 BLEU over
MR08 on average, which is shown in the rows of
?+syn+sem? in Table 5.
5 Discussion
The trend of the results, summarized as perfor-
mance gain over the baseline and MR08 systems
averaged over all test sets, is presented in Table 6.
The syntactic reordering models outperform the
semantic reordering models, and the gain achieved
by the semantic reordering models is limited in the
presence of the MR08 syntactic features. In this
section, we look at MR08 system and the systems
improving it to explore the behavior differences
between the two reordering models.
Coverage analysis: Our statistics show that
syntactic reordering features (either leftmost or
System Baseline MR08
+syn-reorder 1.2 0.9
+sem-reorder 0.8 0.3
+ both 1.4 1.0
Table 6: Performance gain in BLEU over baseline
and MR08 systems averaged over all test sets.
rightmost) are called 24 times per sentence on av-
erage. This is compared to only 9 times per sen-
tence for semantic reordering features. This is not
surprising since the semantic reordering features
are exclusively attached to predicates, and the span
set of the semantic roles is a strict subset of the
span set of the syntactic constituents; only 22% of
syntactic constituents are semantic roles. On aver-
age, a sentences has 4 PASs and each PAS contains
3 semantic roles. Of all the semantic role pairs,
44% are in the same CFG rules, indicating that this
part of semantic reordering has overlap with syn-
tactic reordering. Therefore, the PAS model has
fewer opportunities to influence reordering.
Reordering accuracy analysis: The reordering
type distribution on the reordering model training
data in Table 3 suggests that semantic reordering
is more difficult than syntactic reordering. To val-
idate this conjecture on our translation test data,
we compare the reordering performance among
the MR08 system, the improved systems and the
maximum entropy classifiers. For the test set, we
have four reference translations. We run GIZA++
on the data combination of our translation train-
ing data and test data to get the alignment for the
test data and each reference translation. Once we
have the (semi-)gold alignment, we compute the
gold reordering types between two adjacent syn-
tactic constituents or semantic roles. Then we
evaluate the automatic reordering outputs gener-
ated from both our translation systems and max-
imum entropy classifiers. Table 7 shows the ac-
curacy averaged over the four gold reordering sets
(the four reference translations). It shows that 1)
as expected, our classifiers do worse on the harder
semantic reordering prediction than syntactic re-
ordering prediction; 2) thanks to the high accu-
racy obtained by the maxent classifiers, integrat-
ing either the syntactic or the semantic reorder-
ing constraints results in better reordering perfor-
mance from both syntactic and semantic perspec-
tives; 3) in terms of the mutual impact, the syn-
tactic reordering models help improving seman-
tic reordering more than the semantic reordering
1129
System
Syntactic Semantic
l-m r-m l-m r-m
MR08 75.0 78.0 66.3 68.5
+syn-reorder 78.4 80.9 69.0 70.2
+sem-reorder 76.0 78.8 70.7 72.7
+both 78.6 81.7 70.6 72.1
Maxent Classifier 80.7 85.6 70.9 73.5
Table 7: Reordering accuracy on four gold sets.
System
Syntactic Semantic
l-m r-m l-m r-m
+syn-reorder 1.2 1.2 - -
+sem-reorder - - 0.7 0.9
+both 1.2 1.0 0.5 0.4
Table 8: Reordering feature weights.
models help improving syntactic reordering; and
4) the rightmost models have a learnability advan-
tage over the leftmost models, achieving higher
accuracy across the board.
Feature weight analysis: Table 8 shows the
syntactic and semantic reordering feature weights.
It shows that the semantic feature weights de-
crease in the presence of the syntactic features, in-
dicating that the decoder learns to trust semantic
features less in the presence of the more accurate
syntactic features. This is consistent with our ob-
servation that semantic reordering is harder than
syntactic reordering, as seen in Tables 3 and 7.
Potential improvement analysis: Table 7 also
shows that our current maximum entropy classi-
fiers have room for improvement, especially for
semantic reordering. In order to explore the error
propagation from the classifiers themselves and
explore the upper bound for improvement from the
reordering models, we perform an ?oracle? study,
letting the classifiers be aware of the ?gold? re-
ordering type between two syntactic constituents
or two semantic roles, and returning a higher prob-
ability for the gold reordering type and a smaller
one for the others (i.e., we set 0.9 for the gold
System MT 03 MT 05 MT 08 Avg.
Non-
Oracle
MR08 37.4 34.2 28.7 33.4
+syn-
reorder
38.2 35.3 29.5 34.3
+sem-
reorder
37.6 34.7 28.8 33.7
+ both 38.4 35.2 29.5 34.4
Oracle
+syn-
reorder
39.2 35.9 29.6 34.9
+sem-
reorder
37.9 34.8 28.9 33.9
+ both 39.1 36.0 29.8 35.0
Table 9: Performance (BLEU score) comparison
between non-oracle and oracle experiments.
reordering type, and let the other non-gold three
types share 0.1). Again, to get the gold reorder-
ing type, we run GIZA++ to get the alignment for
tuning/test source sentences and each of four ref-
erence translations. We report the averaged per-
formance by using the gold reordering type ex-
tracted from the four reference translations. Ta-
ble 9 compares the performance between the non-
oracle and oracle settings. We clearly see that us-
ing gold syntactic reordering types significantly
improves the performance (e.g., 34.9 vs. 33.4 on
average) and there is still some room for improve-
ment by building a better maximum entropy clas-
sifiers (e.g., 34.9 vs. 34.3). To our surprise, how-
ever, the improvement achieved by gold semantic
reordering types is still small (e.g., 33.9 vs. 33.4),
suggesting that the potential improvement of se-
mantic reordering models is much more limited.
And we again see that the improvement achieved
by semantic reordering models is limited in the
presence of the syntactic reordering models.
6 Related Work
Syntax-based reordering: Some previous work
pre-ordered words in the source sentences, so that
the word order of source and target sentences is
similar. The reordering rules were either manu-
ally designed (Collins et al, 2005; Wang et al,
2007; Xu et al, 2009; Lee et al, 2010) or auto-
matically learned (Xia and McCord, 2004; Gen-
zel, 2010; Visweswariah et al, 2010; Khalilov
and Sima?an, 2011; Lerner and Petrov, 2013), us-
ing syntactic parses. Li et al (2007) focused on
finding the n-best pre-ordered source sentences by
predicting the reordering of sibling constituents,
while Yang et al (2012) obtained word order by
using a reranking approach to reposition nodes in
syntactic parse trees. Both are close to our work;
however, our model generates reordering features
that are integrated into the log-linear translation
model during decoding.
Another approach in previous work added soft
constraints as weighted features in the SMT de-
coder to reward good reorderings and penalize bad
ones. Marton and Resnik (2008) employed soft
syntactic constraints with weighted binary features
and no MaxEnt model. They did not explicitly
target reordering (beyond applying constraints on
HPB rules). Although employing linguistically
motivated labels in SCFG is capable of captur-
ing constituent reorderings (Chiang, 2010; Mylon-
1130
akis and Sima?an, 2011), the rules are sparser than
SCFG with nameless non-terminals (i.e., Xs) and
soft constraints. Ge (2010) presented a syntax-
driven maximum entropy reordering model that
predicted the source word translation order. Gao
et al (2011) employed dependency trees to predict
the translation order of a word and its head word.
Huang et al (2013) predicted the translation order
of two source words.
4
Our work, which shares this
approach, differs from their work primarily in that
our syntactic reordering models are based on the
constituent level, rather than the word level.
Semantics-based reordering: Semantics-
based reordering has also seen an increase
in activity recently. In the pre-ordering ap-
proach, Wu et al (2011) automatically learned
pre-ordering rules from PAS. In the soft con-
straint or reordering model approach, Liu and
Gildea (2010) modeled the reordering/deletion
of source-side semantic roles in a tree-to-string
translation model. Xiong et al (2012) and Li et
al. (2013) predicted the translation order between
either two arguments or an argument and its
predicate. Instead of decomposing a PAS into
individual units, Zhai et al (2013) constructed
a classifier for each source side PAS. Finally in
the post-processing approach category, Wu and
Fung (2009) performed semantic role labeling
on translation output and reordered arguments to
maximize the cross-lingual match of the semantic
frames between the source sentence and the target
translation. To our knowledge, their semantic
reordering models were PAS-specific. In contrast,
our model is universal and can be easily adopted
to model the reordering of other linguistic units
(e.g., syntactic constituents). Moreover, we
have studied the effectiveness of the semantic
reordering model in different scenarios.
Non-syntax-based reorderings in HPB: Re-
cently we have also seen work on lexicalized re-
ordering models without syntactic information in
HPB (Setiawan et al, 2009; Huck et al, 2013;
Nguyen and Vogel, 2013). The non-syntax-
based reordering approach models the reorder-
ing of translation words/phrases while the syntax-
based approach models the reordering of syn-
tactic constituents. Although there are overlaps
between translation phrases and syntactic con-
stituents, it is reasonable to think that the two re-
4
Note that they obtained the translation order of source
word pairs by predicting the reordering of adjacent con-
stituents, which was quite close to our work.
ordering approaches can work together well and
even complement each other, as the linguistic pat-
terns they capture differ substantially. Setiawan
et al (2013) modeled the orientation decisions
between anchors and two neighboring multi-unit
chunks which might cross phrase or rule bound-
aries. Last, we also note that recent work on non-
syntax-based reorderings in (flat) phrase-based
models (Cherry, 2013; Feng et al, 2013) can also
be potentially adopted to hpb models.
7 Conclusion and Future Work
In this paper, we have presented a unified reorder-
ing framework to incorporate soft linguistic con-
straints (of syntactic or semantic nature) into the
HPB translation model. The syntactic reordering
models take CFG rules and model their reordering
on the target side, while the semantic reordering
models work with PAS. Experiments on Chinese-
English translation show that the reordering ap-
proach can significantly improve a state-of-the-art
hierarchical phrase-based translation system. We
have also discussed the differences between the
two linguistic reordering models.
There are many directions in which this work
can be continued. First, the syntactic reordering
model can be extended to model reordering among
constituents that cross CFG rules. Second, al-
though we do not see obvious gain from the se-
mantic reordering model when the syntactic model
is adopted, it might be beneficial to further jointly
consider the two reordering models, focusing on
where each one does well. Third, to better exam-
ine the overlap or synergy between our approach
and the non-syntax-based reordering approach, we
will conduct direct comparisons and combinations
with the latter.
Acknowledgments
This research was supported in part by the
BOLT program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0012-
12-C-0015. Any opinions, findings, conclusions
or recommendations expressed in this paper are
those of the authors and do not necessarily re-
flect the view of DARPA. The authors would like
to thank three anonymous reviewers for providing
helpful comments, and also acknowledge Ke Wu,
Vladimir Eidelman, Hua He, Doug Oard, Yuening
Hu, Jordan Boyd-Graber, and Jyothi Vinjumur for
useful discussions.
1131
References
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of ACL 1996, pages 310?
318.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of HLT-NAACL 2013, pages 22?31.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP
2008, pages 224?233.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of ACL 2010,
pages 1443?1452.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL 2005, pages
531?540.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL 2010 System Demonstra-
tions, pages 7?12.
Vladimir Eidelman, Ke Wu, Ferhan Ture, Philip
Resnik, and Jimmy Lin. 2013. Mr. mira: Open-
source large-margin structured learning on mapre-
duce. In Proceedings of ACL 2013 System Demon-
strations, pages 199?204.
Minwei Feng, Jan-Thorsten Peter, and Hermann Ney.
2013. Advancements in reordering models for sta-
tistical machine translation. In Proceedings of ACL
2013, pages 322?332.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hier-
archical phrase-based translation. In Proceedings of
EMNLP 2011, pages 857?868.
Niyu Ge. 2010. A direct syntax-driven reordering
model for phrase-based machine translation. In Pro-
ceedings of HLT-NAACL 2010, pages 849?857.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of COLING 2010, pages 376?
384.
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.
2013. Factored soft source syntactic constraints for
hierarchical machine translation. In Proceedings of
EMNLP 2013, pages 556?566.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A phrase orientation model for
hierarchical machine translation. In Proceedings of
WMT 2013, pages 452?463.
Maxim Khalilov and Khalil Sima?an. 2011. Context-
sensitive syntactic source-reordering by statistical
transduction. In Proceedings of IJCNLP 2011,
pages 38?46.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL 2003, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.
2010. Constituent reordering and syntax models for
English-to-Japanese statistical machine translation.
In Proceedings of COLING 2010, pages 626?634.
Uri Lerner and Slav Petrov. 2013. Source-side clas-
sifier preordering for machine translation. In Pro-
ceedings of EMNLP 2013, pages 513?523.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proceedings of ACL 2007,
pages 720?727.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of Chinese. In
Proceedings of ACL 2010, pages 1108?1117.
Junhui Li, Philip Resnik, and Hal Daum?e III. 2013.
Modeling syntactic and semantic structures in hier-
archical phrase-based translation. In Proceedings of
HLT-NAACL 2013, pages 540?549.
Ding Liu and Daniel Gildea. 2010. Semantic role
features for machine translation. In Proceedings of
COLING 2010, pages 716?724.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of ACL-HLT 2008, pages
1003?1011.
Markos Mylonakis and Khalil Sima?an. 2011. Learn-
ing hierarchical translation structure with linguistic
annotations. In Proceedings of ACL 2011, pages
642?652.
ThuyLinh Nguyen and Stephan Vogel. 2013. Integrat-
ing phrase-based reordering features into a chart-
based decoder for machine translation. In Proceed-
ings of ACL 2013, pages 1587?1596.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440?447.
1132
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of ACL 2002, pages 311?318.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL 2007, pages 404?411.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological ordering of function
words in hierarchical phrase-based translation. In
Proceedings of ACL-IJCNLP 2009, pages 324?332.
Hendra Setiawan, Bowen Zhou, Bing Xiang, and Libin
Shen. 2013. Two-neighbor orientation model with
cross-boundary global contexts. In Proceedings of
ACL 2013, pages 1264?1274.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
168?171.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumula-
tive penalty. In Proceedings of ACL-IJCNLP 2009,
pages 477?485.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of COLING 2010, pages
1119?1127.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of EMNLP
2007, pages 737?745.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
smt: A hybrid two-pass model. In Proceedings of
HLT-NAACL 2009: short papers, pages 13?16.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proceedings of IJCNLP 2011, pages 29?
37.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of COLING 2004, pages
508?514.
Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In Proceedings of ACL 2012, pages 902?
911.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceed-
ings of HLT-NAACL 2009, pages 245?253.
Nianwen Xue and Martha Palmer. 2009. Adding se-
mantic roles to the Chinese Treebank. Natural Lan-
guage Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A ranking-based approach to word reordering
for statistical machine translation. In Proceedings of
ACL 2012, pages 912?920.
Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing
Zong. 2013. Handling ambiguities of bilingual
predicate-argument structures for statistical machine
translation. In Proceedings of ACL 2013, pages
1127?1136.
1133
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 152?158,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Shedding (a Thousand Points of) Light on Biased Language
Tae Yano
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
taey@cs.cmu.edu
Philip Resnik
Department of Linguistics and UMIACS
University of Maryland
College Park, MD 20742, USA
resnik@umiacs.umd.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
This paper considers the linguistic indicators of bias
in political text. We used Amazon Mechanical Turk
judgments about sentences from American political
blogs, asking annotators to indicate whether a sen-
tence showed bias, and if so, in which political di-
rection and through which word tokens. We also
asked annotators questions about their own political
views. We conducted a preliminary analysis of the
data, exploring how different groups perceive bias in
different blogs, and showing some lexical indicators
strongly associated with perceived bias.
1 Introduction
Bias and framing are central topics in the study of com-
munications, media, and political discourse (Scheufele,
1999; Entman, 2007), but they have received relatively
little attention in computational linguistics. What are the
linguistic indicators of bias? Are there lexical, syntactic,
topical, or other clues that can be computationally mod-
eled and automatically detected?
Here we use Amazon Mechanical Turk (MTurk) to en-
gage in a systematic, empirical study of linguistic indi-
cators of bias in the political domain, using text drawn
from political blogs. Using the MTurk framework, we
collected judgments connected with the two dominant
schools of thought in American politics, as exhibited in
single sentences. Since no one person can claim to be an
unbiased judge of political bias in language, MTurk is an
attractive framework that lets us measure perception of
bias across a population.
2 Annotation Task
We drew sentences from a corpus of American political
blog posts from 2008. (Details in Section 2.1.) Sentences
were presented to participants one at a time, without con-
text. Participants were asked to judge the following (see
Figure 1 for interface design):
? To what extent a sentence or clause is biased (none,
somewhat, very);
? The nature of the bias (very liberal, moderately lib-
eral, moderately conservative, very conservative, bi-
ased but not sure which direction); and
? Which words in the sentence give away the author?s
bias, similar to ?rationale? annotations in Zaidan et
al. (2007).
For example, a participant might identify a moderate
liberal bias in this sentence,
Without Sestak?s challenge, we would have
Specter, comfortably ensconced as a Democrat
in name only.
adding checkmarks on the underlined words. A more
neutral paraphrase is:
Without Sestak?s challenge, Specter would
have no incentive to side more frequently with
Democrats.
It is worth noting that ?bias,? in the sense we are us-
ing it here, is distinct from ?subjectivity? as that topic
has been studied in computational linguistics. Wiebe
et al (1999) characterize subjective sentences as those
that ?are used to communicate the speaker?s evaluations,
opinions, and speculations,? as distinguished from sen-
tences whose primary intention is ?to objectively com-
municate material that is factual to the reporter.? In con-
trast, a biased sentence reflects a ?tendency or preference
towards a particular perspective, ideology or result.?1 A
subjective sentence can be unbiased (I think that movie
was terrible), and a biased sentence can purport to com-
municate factually (Nationalizing our health care system
1http://en.wikipedia.org/wiki/Bias as of 13 April,
2010.
152
is a point of no return for government interference in the
lives of its citizens2).
In addition to annotating sentences, each participant
was asked to complete a brief questionnaire about his or
her own political views. The survey asked:
1. Whether the participant is a resident of the United
States;
2. Who the participant voted for in the 2008 U.S.
presidential election (Barack Obama, John McCain,
other, decline to answer);
3. Which side of political spectrum he/she identified
with for social issues (liberal, conservative, decline
to answer); and
4. Which side of political spectrum he/she identified
with for fiscal/economic issues (liberal, conserva-
tive, decline to answer).
This information was gathered to allow us to measure
variation in bias perception as it relates to the stance of
the annotator, e.g., whether people who view themselves
as liberal perceive more bias in conservative sources, and
vice versa.
2.1 Dataset
We extracted our sentences from the collection of blog
posts in Eisenstein and Xing (2010). The corpus con-
sists of 2008 blog posts gathered from six sites focused
on American politics:
? American Thinker (conservative),3
? Digby (liberal),4
? Hot Air (conservative),5
? Michelle Malkin (conservative),6
? Think Progress (liberal),7 and
? Talking Points Memo (liberal).8
13,246 posts were gathered in total, and 261,073 sen-
tences were extracted using WebHarvest9 and OpenNLP
1.3.0.10 Conservative and liberal sites are evenly rep-
resented (130,980 sentences from conservative sites,
130,093 from liberal sites). OpenNLP was also used for
tokenization.
2Sarah Palin, http://www.facebook.com/note.php?
note_id=113851103434, August 7, 2009.
3http://www.americanthinker.com
4http://digbysblog.blogspot.com
5http://hotair.com
6http://michellemalkin.com
7http://thinkprogress.org
8http://www.talkingpointsmemo.com
9http://web-harvest.sourceforge.net
10http://opennlp.sourceforge.net
Liberal Conservative
thinkprogress org exit question
video thinkprogress hat tip
et rally ed lasky
org 2008 hot air
gi bill tony rezko
wonk room ed morrissey
dana perino track record
phil gramm confirmed dead
senator mccain american thinker
abu ghraib illegal alien
Table 1: Top ten ?sticky? partisan bigrams for each side.
2.2 Sentence Selection
To support exploratory data analysis, we sought a di-
verse sample of sentences for annotation, but we were
also guided by some factors known or likely to correlate
with bias. We extracted sentences from our corpus that
matched at least one of the categories below, filtering to
keep those of length between 8 and 40 tokens. Then, for
each category, we first sampled 100 sentences without re-
placement. We then randomly extracted sentences up to
1,100 from the remaining pool. We selected the sentences
this way so that the collection has variety, while including
enough examples for individual categories. Our goal was
to gather at least 1,000 annotated sentences; ultimately
we collected 1,041. The categories are as follows.
?Sticky? partisan bigrams. One likely indicator of
bias is the use of terms that are particular to one side or
the other in a debate (Monroe et al, 2008). In order to
identify such terms, we independently created two lists
of ?sticky? (i.e., strongly associated) bigrams in liberal
and conservative subcorpora, measuring association us-
ing the log-likelihood ratio (Dunning, 1993) and omitting
bigrams containing stopwords.11 We identified a bigram
as ?liberal? if it was among the top 1,000 bigrams from
the liberal blogs, as measured by strength of association,
and was also not among the top 1,000 bigrams on the con-
servative side. The reverse definition yielded the ?conser-
vative? bigrams. The resulting liberal list contained 495
bigrams, and the conservative list contained 539. We then
manually filtered cases that were clearly remnant HTML
tags and other markup, arriving at lists of 433 and 535,
respectively. Table 1 shows the strongest weighted bi-
grams.
As an example, consider this sentence (with a preced-
ing sentence of context), which contains gi bill. There is
no reason to think the bigram itself is inherently biased
(in contrast to, for example, death tax, which we would
11We made use of Pedersen?s N -gram Statistics Package (Banerjee
and Pedersen, 2003).
153
perceive as biased in virtually any unquoted context), but
we do perceive bias in the full sentence.
Their hard fiscal line softens in the face of
American imperialist adventures. According to
CongressDaily the Bush dogs are also whining
because one of their members, Stephanie Her-
seth Sandlin, didn?t get HERGI Bill to the floor
in favor of Jim Webb?s .
Emotional lexical categories. Emotional words might
be another indicator of bias. We extracted four categories
of words from Pennebaker?s LIWC dictionary: Nega-
tive Emotion, Positive Emotion, Causation, and Anger.12
The following is one example of a biased sentence in our
dataset that matched these lexicons, in this case the Anger
category; the match is in bold.
A bunch of ugly facts are nailing the biggest
scare story in history.
The five most frequent matches in the corpus for each
category are as follows.13
Negative Emotion: war attack* problem* numb* argu*
Positive Emotion: like well good party* secur*
Causation: how because lead* make why
Anger: war attack* argu* fight* threat*
Kill verbs. Greene and Resnik (2009) discuss the rel-
evance of syntactic structure to the perception of senti-
ment. For example, their psycholinguistic experiments
would predict that when comparing Millions of people
starved under Stalin (inchoative) with Stalin starved mil-
lions of people (transitive), the latter will be perceived as
more negative toward Stalin, because the transitive syn-
tactic frame tends to be connected with semantic prop-
erties such as intended action by the subject and change
of state in the object. ?Kill verbs? provide particularly
strong examples of such phenomena, because they ex-
hibit a large set of semantic properties canonically as-
sociated with the transitive frame (Dowty, 1991). The
study by Greene and Resnik used 11 verbs of killing and
similar action to study the effect of syntactic ?packag-
ing? on perceptions of sentiment.14 We included mem-
bership on this list (in any morphological form) as a se-
lection criterion, both because these verbs may be likely
12http://www.liwc.net. See Pennebaker et al (2007) for de-
tailed description of background theory, and how these lexicons were
constructed. Our gratitude to Jamie Pennebaker for the use of this dic-
tionary.
13Note that some LIWC lexical entries are specified as pre-
fixes/stems, e.g. ugl*, which matches ugly uglier, etc.
14The verbs are: kill, slaughter, assassinate, shoot, poison, strangle,
smother, choke, drown, suffocate, and starve.
to appear in sentences containing bias (they overlap sig-
nificantly with Pennebaker?s Negative Emotion list), and
because annotation of bias will provide further data rel-
evant to Greene and Resnik?s hypothesis about the con-
nections among semantic propeties, syntactic structures,
and positive or negative perceptions (which are strongly
connected with bias).
In our final 1,041-sentence sample, ?sticky bigrams?
occur 235 times (liberal 113, conservative 122), the lexi-
cal category features occur 1,619 times (Positive Emotion
577, Negative Emotion 466, Causation 332, and Anger
244), and ?kill? verbs appear as a feature in 94 sentences.
Note that one sentence often matches multiple selection
criteria. Of the 1,041-sentence sample, 232 (22.3%) are
from American Thinker, 169 (16.2%) from Digby, 246
(23.6%) from Hot Air, 73 (7.0%) from Michelle Malkin,
166 (15.9%) from Think Progress, and 155 (14.9%) from
Talking Points Memo.
3 Mechanical Turk Experiment
We prepared 1,100 Human Intelligence Tasks (HITs),
each containing one sentence annotation task. 1,041 sen-
tences were annotated five times each (5,205 judgements
total). One annotation task consists of three bias judge-
ment questions plus four survey questions. We priced
each HIT between $0.02 and $0.04 (moving from less
to more to encourage faster completion). The total cost
was $212.15 We restricted access to our tasks to those
who resided in United States and who had above 90% ap-
proval history, to ensure quality and awareness of Amer-
ican political issues. We also discarded HITs annotated
by workers with particularly low agreement scores. The
time allowance for each HIT was set at 5 minutes.
3.1 Annotation Results
3.1.1 Distribution of Judgments
Overall, more than half the judgments are ?not biased,?
and the ?very biased? label is used sparingly (Table 2).
There is a slight tendency among the annotators to assign
the ?very conservative? label, although moderate bias is
distributed evenly on both side (Table 3). Interestingly,
there are many ?biased, but not sure? labels, indicating
that the annotators are capable of perceiving bias (or ma-
nipulative language), without fully decoding the intent of
the author, given sentences out of context.
Bias 1 1.5 2 2.5 3
% judged 36.0 26.6 25.5 9.4 2.4
Table 2: Strength of perceived bias per sentence, averaged over
the annotators (rounded to nearest half point). Annotators rate
bias on a scale of 1 (no bias), 2 (some bias), and 3 (very biased).
15This includes the cost for the discarded annotations.
154
Figure 1: HIT: Three judgment questions. We first ask for the strength of bisa, then the direction. For the word-level annotation
question (right), workers are asked to check the box to indicate the region which ?give away? the bias.
Bias type VL ML NB MC VC B
% judged 4.0 8.5 54.8 8.2 6.7 17.9
Table 3: Direction of perceived bias, per judgment (very lib-
eral, moderately liberal, no bias, moderately conservative, very
conservative, biased but not sure which).
Economic
L M C NA
So
ci
al
L 20.1 10.1 4.9 0.7
M 0.0 21.9 4.7 0.0
C 0.1 0.4 11.7 0.0
NA 0.1 0.0 11.2 14.1
Table 4: Distribution of judgements by annotators? self-
identification on social issues (row) and fiscal issue (column);
{L, C, M, NA} denote liberal, conservative, moderate, and de-
cline to answer, respectively.
3.1.2 Annotation Quality
In this study, we are interested in where the wisdom of
the crowd will take us, or where the majority consensus
on bias may emerge. For this reason we did not contrive a
gold standard for ?correct? annotation. We are, however,
mindful of its overall quality?whether annotations have
reasonable agreement, and whether there are fraudulent
responses tainting the results.
To validate our data, we measured the pair-wise Kappa
statistic (Cohen, 1960) among the 50 most frequent work-
ers16 and took the average over all the scores.17. The
average of the agreement score for the first question is
0.55, and the second 0.50. Those are within the range of
reasonable agreement for moderately difficult task. We
also inspected per worker average scores for frequent
workers18 and found one with consistently low agreement
scores. We discarded all the HITs by this worker from our
results. We also manually inspected the first 200 HITs for
apparent frauds. The annotations appeared to be consis-
tent. Often annotators agreed (many ?no bias? cases were
unanimous), or differed in only the degree of strength
(?very biased? vs. ?biased?) or specificity (?biased but I
am not sure? vs. ?moderately liberal?). The direction of
bias, if specified, was very rarely inconsistent.
Along with the annotation tasks, we asked workers
how we could improve our HITs. Some comments were
16258 workers participated; only 50 of them completed more than 10
annotations.
17Unlike traditional subjects for a user-annotation study, our annota-
tors have not judged all the sentences considered in the study. There-
fore, to compute the agreement, we considered only the case where two
annotators share 20 or more sentences.
18We consider only those with 10 or more annotations.
155
insightful for our study (as well as for the interface de-
sign). A few pointed out that an impolite statement or
a statement of negative fact is not the same as bias, and
therefore should be marked separately from bias. Others
mentioned that some sentences are difficult to judge out
of context. These comments will be taken into account in
future research.
4 Analysis and Significance
In the following section we report some of the interesting
trends we found in our annotation results. We consider a
few questions and report the answers the data provide for
each.
4.1 Is a sentence from a liberal blog more likely be
seen as liberal?
In our sample sentence pool, conservatives and liberals
are equally represented, though each blog site has a dif-
ferent representation.19 We grouped sentences by source
site, then computed the percentage representation of each
site within each bias label; see Table 5. In the top row,
we show the percentage representation of each group in
overall judgements.
In general, a site yields more sentences that match its
known political leanings. Note that in our annotation
task, we did not disclose the sentence?s source to the
workers. The annotators formed their judgements solely
based on the content of the sentence. This result can
be taken as confirming people?s ability to perceive bias
within a sentence, or, conversely, as confirming our a pri-
ori categorizations of the blogs.
at ha mm db tp tpm
Overall 22.3 23.6 7.0 16.2 15.9 14.9
NB 23.7 22.3 6.1 15.7 17.0 15.3
VC 24.8 32.3 19.3 6.9 7.5 9.2
MC 24.4 33.6 8.0 8.2 13.6 12.2
ML 16.6 15.2 3.4 21.1 22.9 20.9
VL 16.7 9.0 4.3 31.0 22.4 16.7
B 20.1 25.4 7.2 19.5 12.3 13.7
Table 5: Percentage representation of each site within bias label
pools from question 2 (direction of perceived bias): very liberal,
moderately liberal, no bias, moderately conservative, very con-
servative, biased but not sure which. Rows sum to 100. Bold-
face indicates rates higher than the site?s overall representation
in the pool.
4.2 Does a liberal leaning annotator see more
conservative bias?
In Table 5, we see that blogs are very different from each
other in terms of the bias annotators perceive in their lan-
19Posts appear on different sites at different rates.
1
3
10
32
100
very conservative no bias very liberal
not sure
LL
MM
CC
Overall
Figure 2: Distribution of bias labels (by judgment) for social
and economic liberals (LL), social and economic moderates
(MM), and social and economic conservatives (CC), and over-
all. Note that this plot uses a logarithmic scale, to tease apart
the differences among groups.
guage. In general, conservative sites seemingly produced
much more identifiable partisan bias than liberal sites.20
This impression, however, might be an artifact of the
distribution of the annotators? own bias. As seen in Ta-
ble 4, a large portion of our annotators identified them-
selves as liberal in some way. People might call a state-
ment biased if they disagree with it, while showing le-
niency toward hyperbole more consistent with their opin-
ions.
To answer this question, we break down the judgement
labels by the annotators? self-identification, and check
the percentage of each bias type within key groups (see
Figure 2). In general, moderates perceive less bias than
partisans (another useful reality check, in the sense that
this is to be expected), but conservatives show a much
stronger tendency to label sentences as biased, in both
directions. (We caution that the underrepresentation of
self-identifying conservatives in our worker pool means
that only 608 judgments from 48 distinct workers were
used to estimate these statistics.) Liberals in this sample
are less balanced, perceiving conservative bias at double
the rate of liberal bias.
4.3 What are the lexical indicators of perceived
bias?
For a given word type w, we calculate the frequency that
it was marked as indicating bias, normalized by its total
number of occurrences. To combine the judgments of dif-
ferent annotators, we increment w?s count by k/n when-
ever k judgments out of n marked the word as showing
bias. We perform similar calculations with a restriction
to liberal and conservative judgments on the sentence as a
20Liberal sites cumulatively produced 64.9% of the moderately lib-
eral bias label and 70.1 % of very liberal, while conservative sites pro-
duced 66.0% of moderately conservative and 76.4% of very conserva-
tive, respectively.
156
Overall Liberal Conservative Not Sure Which
bad 0.60 Administration 0.28 illegal 0.40 pass 0.32
personally 0.56 Americans 0.24 Obama?s 0.38 bad 0.32
illegal 0.53 woman 0.24 corruption 0.32 sure 0.28
woman 0.52 single 0.24 rich 0.28 blame 0.28
single 0.52 personally 0.24 stop 0.26 they?re 0.24
rich 0.52 lobbyists 0.23 tax 0.25 happen 0.24
corruption 0.52 Republican 0.22 claimed 0.25 doubt 0.24
Administration 0.52 union 0.20 human 0.24 doing 0.24
Americans 0.51 torture 0.20 doesn?t 0.24 death 0.24
conservative 0.50 rich 0.20 difficult 0.24 actually 0.24
doubt 0.48 interests 0.20 Democrats 0.24 exactly 0.22
torture 0.47 doing 0.20 less 0.23 wrong 0.22
Table 6: Most strongly biased words, ranked by relative frequency of receiving a bias mark, normalized by total frequency. Only
words appearing five times or more in our annotation set are ranked.
whole. Top-ranked words for each calculation are shown
in Table 6.
Some of the patterns we see are consistent with what
we found in our automatic method for proposing biased
bigrams. For example, the bigrams tended to include
terms that refer to members or groups on the opposing
side. Here we find that Republican and Administration
(referring in 2008 to the Bush administration) tends to
show liberal bias, while Obama?s and Democrats show
conservative bias.
5 Discussion and Future Work
The study we have conducted here represents an initial
pass at empirical, corpus-driven analysis of bias using the
methods of computational linguistics. The results thus far
suggest that it is possible to automatically extract a sam-
ple that is rich in examples that annotators would con-
sider biased; that na??ve annotators can achieve reason-
able agreement with minimal instructions and no train-
ing; and that basic exploratory analysis of results yields
interpretable patterns that comport with prior expecta-
tions, as well as interesting observations that merit further
investigation.
In future work, enabled by annotations of biased and
non-biased material, we plan to delve more deeply into
the linguistic characteristics associated with biased ex-
pression. These will include, for example, an analysis
of the extent to which explicit ?lexical framing? (use of
partisan terms, e.g., Monroe et al, 2008) is used to con-
vey bias, versus use of more subtle cues such as syntactic
framing (Greene and Resnik, 2009). We will also explore
the extent to which idiomatic usages are connected with
bias, with the prediction that partisan ?memes? tend to be
more idiomatic than compositional in nature.
In our current analysis, the issue of subjectivity was not
directly addressed. Previous work has shown that opin-
ions are closely related to subjective language (Pang and
Lee, 2008). It is possible that asking annotators about
sentiment while asking about bias would provide a deeper
understanding of the latter. Interestingly, annotator feed-
back included remarks that mere negative ?facts? do not
convey an author?s opinion or bias. The nature of subjec-
tivity as a factor in bias perception is an important issue
for future investigation.
6 Conclusion
This paper considered the linguistic indicators of bias in
political text. We used Amazon Mechanical Turk judg-
ments about sentences from American political blogs,
asking annotators to indicate whether a sentence showed
bias, and if so, in which political direction and through
which word tokens; these data were augmented by a po-
litical questionnaire for each annotator. Our preliminary
analysis suggests that bias can be annotated reasonably
consistently, that bias perception varies based on personal
views, and that there are some consistent lexical cues for
bias in political blog data.
Acknowledgments
The authors acknowledge research support from HP
Labs, help with data from Jacob Eisenstein, and help-
ful comments from the reviewers, Olivia Buzek, Michael
Heilman, and Brendan O?Connor.
References
Satanjeev Banerjee and Ted Pedersen. 2003. The design, implementa-
tion and use of the ngram statistics package. In the Fourth Interna-
tional Conference on Intelligent Text Processing and Computational
Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nominal scales.
Educational and Psychological Measurement, 20(1):37?46.
David Dowty. 1991. Thematic Proto-Roles and Argument Selection.
Language, 67:547?619.
157
Ted Dunning. 1993. Accurate methods for the statistics of surprise and
coincidence. Computational Linguistics, 19(1):61?74.
Jacob Eisenstein and Eric Xing. 2010. The CMU 2008 political blog
corpus. Technical report CMU-ML-10-101.
Robert M. Entman. 2007. Framing bias: Media in the distribution of
power. Journal of Communication, 57(1):163?173.
Stephan Greene and Philip Resnik. 2009. More than words: Syntactic
packaging and implicit sentiment. In NAACL, pages 503?511, June.
Burt L. Monroe, Michael P. Colaresi, and Kevin M. Quinn. 2008.
Fightin? words: Lexical feature selection and evaluation for identi-
fying the content of political conflict. Political Analysis, 16(4):372?
403, October.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis.
Foundations and Trends in Information Retrieval, 2(1-2):1?135.
J.W. Pennebaker, C.K Chung, M. Ireland, A Gonzales, and R J. Booth,
2007. The development and psychometric properties of LIWC2007.
Dietram A. Scheufele. 1999. Framing as a theory of media effects.
Journal of Communication, 49(1):103?122.
Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P. O?Hara. 1999.
Development and use of a gold standard data set for subjectivity
classifications. In Proceedings of the Association for Computational
Linguistics (ACL), pages 246?253.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using ?anno-
tator rationales? to improve machine learning for text categorization.
In NAACL, pages 260?267, April.
158
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 188?194,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Measuring Transitivity Using Untrained Annotators
Nitin Madnania,b Jordan Boyd-Grabera Philip Resnika,c
aInstitute for Advanced Computer Studies
bDepartment of Computer Science
cDepartment of Linguistics
University of Maryland, College Park
{nmadnani,jbg,resnik}@umiacs.umd.edu
Abstract
Hopper and Thompson (1980) defined a multi-axis
theory of transitivity that goes beyond simple syn-
tactic transitivity and captures how much ?action?
takes place in a sentence. Detecting these features
requires a deep understanding of lexical semantics
and real-world pragmatics. We propose two gen-
eral approaches for creating a corpus of sentences
labeled with respect to the Hopper-Thompson transi-
tivity schema using Amazon Mechanical Turk. Both
approaches assume no existing resources and incor-
porate all necessary annotation into a single system;
this is done to allow for future generalization to other
languages. The first task attempts to use language-
neutral videos to elicit human-composed sentences
with specified transitivity attributes. The second task
uses an iterative process to first label the actors and
objects in sentences and then annotate the sentences?
transitivity. We examine the success of these tech-
niques and perform a preliminary classification of
the transitivity of held-out data.
Hopper and Thompson (1980) created a multi-axis the-
ory of Transitivity1 that describes the volition of the sub-
ject, the affectedness of the object, and the duration of the
action. In short, this theory goes beyond the simple gram-
matical notion of transitivity (whether verbs take objects
? transitive ? or not ? intransitive) and captures how
much ?action? takes place in a sentence. Such notions of
Transitivity are not apparent from surface features alone;
identical syntactic constructions can have vastly different
Transitivity. This well-established linguistic theory, how-
ever, is not useful for real-world applications without a
Transitivity-annotated corpus.
Given such a substantive corpus, conventional machine
learning techniques could help determine the Transitivity
of verbs within sentences. Transitivity has been found to
play a role in what is called ?syntactic framing,? which
expresses implicit sentiment (Greene and Resnik, 2009).
1We use capital ?T? to differentiate from conventional syntactic tran-
sitivity throughout the paper.
In these contexts, the perspective or sentiment of the
writer is reflected in the constructions used to express
ideas. For example, a less Transitive construction might
be used to deflect responsibility (e.g. ?John was killed?
vs. ?Benjamin killed John?).
In the rest of this paper, we review the Hopper-
Thompson transitivity schema and propose two relatively
language-neutral methods to collect Transitivity ratings.
The first asks humans to generate sentences with de-
sired Transitivity characteristics. The second asks hu-
mans to rate sentences on dimensions from the Hopper-
Thompson schema. We then discuss the difficulties of
collecting such linguistically deep data and analyze the
available results. We then pilot an initial classifier on the
Hopper-Thompson dimensions.
1 Transitivity
Table 1 shows the subset of the Hopper-Thompson di-
mensions of Transitivity used in this study. We excluded
noun-specific aspects as we felt that these were well cov-
ered by existing natural language processing (NLP) ap-
proaches (e.g. whether the object / subject is person, ab-
stract entity, or abstract concept is handled well by exist-
ing named entity recognition systems) and also excluded
aspects which we felt had significant overlap with the
dimensions we were investigating (e.g. affirmation and
mode).
We also distinguished the original Hopper-Thompson
?Affectedness? aspect into separate ?Benefit? and
?Harm? components, as we suspect that these data will
be useful to other applications such as sentiment analy-
sis.
We believe that these dimensions of transitivity are
simple and intuitive enough that they can be understood
and labeled by the people on Amazon Mechanical Turk,
a web service. Amazon Mechanical Turk (MTurk) allows
individuals to post jobs on MTurk with a set fee that are
then performed by workers on the Internet. MTurk con-
nects workers to people with tasks and handles the coor-
dination problems of payment and transferring data.
188
Kinesis Sentences where movement happens are perceived to be more Transitive.?Sue jumped out of an airplane? vs. ?The corporation jumped to a silly conclusion.?
Punctuality Sentences where the action happens quickly are perceived to be more Transitive.?She touched her ID to the scanner to enter? vs. ?I was touched by how much she helped me.?
Mode Sentences with no doubt about whether the action happened are perceived to be more Transitive.?Bob was too busy to fix the drain? vs. ?Bob fixed the drain.?
Affectedness Sentences where the object is more affected by the action are perceived to be more Transitive.?The St. Bernard saved the climber? vs. ?Melanie looked at the model.?
Volition Sentences where the actor chose to perform the action are perceived to be more Transitive.?Paul jumped out of the bushes and startled his poor sister? vs. ?The picture startled George.?
Aspect Sentences where the action is done to completion are perceived to be more Transitive.?Walter is eating the hamburger? vs. ?Walter ate the pudding up.?
Table 1: The Hopper-Thompson dimensions of transitivity addressed in this paper. In experiments, ?Affectedness? was divided into
?Harm? and ?Benefit.?
2 Experiments
Our goal is to create experiments for MTurk that will pro-
duce a large set of sentences with known values of Tran-
sitivity. With both experiments, we design the tasks to
be as language independent as possible, thus not depend-
ing on language-specific preprocessing tools. This allows
the data collection approach to be replicated in other lan-
guages.
2.1 Elicitation
The first task is not corpus specific, and requires no
language-specific resources. We represent verbs using
videos (Ma and Cook, 2009). This also provides a form
of language independent sense disambiguation. We dis-
play videos illustrating verbs (Figure 1) and ask users on
MTurk to identify the action and give nouns that can do
the action and ? in a separate task ? the nouns that the
action can be done to. For quality control, Turkers must
match a previous Turker?s response for one of their an-
swers (a la the game show ?Family Feud?).
Figure 1: Stills from three videos depicting the verbs ?receive,?
?hear,? and ?help.?
We initially found that subjects had difficulty distin-
guishing what things could do the action (subjects) vs.
what things the action could be done to (objects). In or-
der to suggest the appropriate syntactic frame, we use
javascript to form their inputs into protosentences as they
typed. For example, if they identified an action as ?pick-
ing? and suggested ?fruit? as a possible object, the pro-
tosentence ?it is picking fruit? is displayed below their
input (Figure 2). This helped ensure consistent answers.
The subject and object tasks were done separately, and
for the object task, users were allowed to say that there
is nothing the action can be done to (for example, for an
intransitive verb).
Figure 2: A screenshot of a user completing a task to find ob-
jects of a particular verb, where the verb is represented by a
film. After the user has written a verb and a noun, a protosen-
tence is formed and shown to ensure that the user is using the
words in the appropriate roles.
These subjects and objects we collected were then used
as inputs for a second task. We showed workers videos
with potential subjects and objects and asked them to
create pairs of sentences with opposite Transitivity at-
tributes. For example, Write a sentence where the thing
to which the action is done benefits and Write a sentence
where the thing to which the action is done is not affected
by the action. For both sides of the Transitivity dimen-
sion, we allowed users to say that writing such a sentence
is impossible. We discuss the initial results of this task in
Section 3.
2.2 Annotation
Our second task?one of annotation?depends on having
a corpus available in the language of interest. For con-
189
creteness and availability, we use Wikipedia, a free mul-
tilingual encyclopedia. We extract a large pool of sen-
tences from Wikipedia containing verbs of interest. We
apply light preprocessing to remove long, unclear (e.g.
starting with a pronoun), or uniquely Wikipedian sen-
tences (e.g. very short sentences of the form ?See List
of Star Trek Characters?). We construct tasks, each for a
single verb, that ask users to identify the subject and ob-
ject for the verb in randomly selected sentences.2 Users
were prompted by an interactive javascript guide (Fig-
ure 3) that instructed them to click on the first word of the
subject (or object) and then to click on the last word that
made up the subject (or object). After they clicked, a text
box was automatically populated with their answer; this
decreased errors and made the tasks easier to finish. For
quality control, each HIT has a simple sentence where
subject and object were already determined by the au-
thors; the user must match the annotation on that sentence
for credit. We ended up rejecting less than one percent of
submitted hits.
Figure 3: A screenshot of the subject identification task. The
user has to click on the phrase that they believe is the subject.
Once objects and subjects have been identified, other
users rate the sentence?s Transitivity by answering the
following questions like, where $VERB represents the
verb of interest, $SUBJ is its subject and $OBJ is its ob-
ject3:
? Aspect. After reading this sentence, do you know
that $SUBJ is done $VERBing?
? Affirmation. From reading the sentence, how cer-
tain are you that $VERBing happened?
? Benefit. How much did $OBJ benefit?
? Harm. How much was $OBJ harmed?
? Kinesis. Did $SUBJ move?
? Punctuality. If you were to film $SUBJ?s act of
$VERBing in its entirety, how long would the movie
be?
? Volition. Did the $SUBJ make a conscious choice
to $VERB?
The answers were on a scale of 0 to 4 (higher num-
bers meant the sentence evinced more of the property in
2Our goal of language independence and the unreliable correspon-
dence between syntax and semantic roles precludes automatic labeling
of the subjects and objects.
3These questions were developed using Greene and Resnik?s (2009)
surveys as a foundation.
question), and each point in the scale had a description to
anchor raters and to ensure consistent results.
2.3 Rewards
Table 2 summarizes the rewards for the tasks used in
these experiments. Rewards were set at the minimal rate
that could attract sufficient interest from users. For the
?Video Elicitation? task, where users wrote sentences
with specified Transitivity properties, we also offered
bonuses for clever, clear sentences. However, this was
our least popular task, and we struggled to attract users.
3 Results and Discussion
3.1 Creative but Unusable Elicitation Results
We initially thought that we would have difficulty coax-
ing users to provide full sentences. This turned out not
to be the case. We had no difficulty getting (very imag-
inative) sentences, but the sentences were often incon-
sistent with the Transitivity aspects we are interested in.
This shows both the difficulty of writing concise instruc-
tions for non-experts and the differences between every-
day meanings of words and their meaning in linguistic
contexts.
For example, the ?volitional? elicitation task asked
people to create sentences where the subject made a con-
scious decision to perform the action. In the cases where
we asked users to create sentences where the subject did
not make a conscious decision to perform an action, al-
most all of the sentences created by users focused on sen-
tences where a person (rather than employ other tactics
such as using a less individuated subject, e.g. replacing
?Bob? with ?freedom?) was performing the action and
was coerced into doing the action. For example:
? Sellers often give gifts to their clients when they are
trying to make up for a wrongdoing.
? A man is forced to search for his money.
? The man, after protesting profusely, picked an exer-
cise class to attend
? The vegetarian Sherpa had to eat the pepperoni pizza
or he would surely have died.
While these data are likely still interesting for other pur-
poses, their biased distribution is unlikely to be useful for
helping identify whether an arbitrary sentence in a text
expresses the volitional Transitivity attribute. The users
prefer to have an animate agent that is compelled to take
the action rather than create sentences where the action
happens accidentally or is undertaken by an abstract or
inanimate actor.
Similarly, for the aspect dimension, many users simply
chose to represent actions that had not been completed
190
Task Questions / Hit Pay Repetition Tasks Total
Video Object 5 0.04 5 10 $2.00
Video Subject 5 0.04 5 10 $2.00
Corpus Object 10 0.03 5 50 $7.50
Corpus Subject 10 0.03 5 50 $7.50
Video Elicitation 5 0.10 2 70 $14.00
Corpus Annotation 7 0.03 3 400 $36.00
Total $69.00
Table 2: The reward structure for the tasks presented in this paper (not including bonuses or MTurk overhead). ?Video Subject? and
?Video Object? are where users were presented with a video and supplied the subjects and objects of the depicted actions. ?Corpus
Subject? and ?Corpus Object? are the tasks where users identified the subject and objects of sentences from Wikipedia. ?Video
Elicitation? refers to the task where users were asked to write sentences with specified Transitivity properties. ?Corpus Annotation?
is where users are presented with sentences with previously identified subjects and objects and must rate various dimensions of
Transitivity.
using the future tense. For the kinesis task, users dis-
played amazing creativity in inventing situations where
movement was correlated with the action. Unfortunately,
as before, these data are not useful in generating predic-
tive features for capturing the properties of Transitivity.
We hope to improve experiments and instructions to
better align everyday intuitions with the linguistic proper-
ties of interest. While we have found that extensive direc-
tions tend to discourage users, perhaps there are ways in-
crementally building or modifying sentences that would
allow us to elicit sentences with the desired Transitivity
properties. This is discussed further in the conclusion,
Section 4.
3.2 Annotation Task
For the annotation task, we observed that users often had
a hard time keeping their focus on the words in question
and not incorporating additional knowledge. For exam-
ple, for each of the following sentences:
? Bonosus dealt with the eastern cities so harshly that
his severity was remembered centuries later .
? On the way there, however, Joe and Jake pick an-
other fight .
? The Black Sea was a significant naval theatre of
World War I and saw both naval and land battles
during World War II .
? Bush claimed that Zubaydah gave information that
lead to al Shibh ?s capture .
some users said that the objects in bold were greatly
harmed, suggesting that users felt even abstract concepts
could be harmed in these sentences. A rigorous inter-
pretation of the affectedness dimension would argue that
these abstract concepts were incapable of being harmed.
We suspect that the negative associations (severity, fight,
battles, capture) present in this sentence are causing users
to make connections to harm, thus creating these ratings.
Similarly, world knowledge flavored other questions,
such as kinesis, where users were able to understand from
context that the person doing the action probably moved
at some point near the time of the event, even if move-
ment wasn?t a part of the act of, for example, ?calling? or
?loving.?
3.3 Quantitative Results
For the annotation task, we were able to get consistent
ratings of transitivity. Table 3 shows the proportion of
sentences where two or more annotators agreed on the
a Transitivity label of the sentences for that dimension.
All of the dimensions were significantly better than ran-
dom chance agreement (0.52); the best was harm, which
has an accessible, clear, and intuitive definition, and the
worst was kinesis, which was more ambiguous and prone
to disagreement among raters.
Dimension Sentences
with Agreement
HARM 0.87
AFFIRMATION 0.86
VOLITION 0.86
PUNCTUALITY 0.81
BENEFIT 0.81
ASPECT 0.80
KINESIS 0.70
Table 3: For each of the dimensions of transitivity, the propor-
tion of sentences where at least two of three raters agreed on the
label. Random chance agreement is 0.52.
Figure 4 shows a distribution for each of the Transitiv-
ity data on the Wikipedia corpus. These data are consis-
tent with what one would expect from random sentences
from an encyclopedic dataset; most of the sentences en-
191
Median Score
Co
un
t
0
50
100
150
200
250
0
50
100
150
200
250
AFFIRMATION
KINESIS
0 1 2 3 4
ASPECT
PUNCTUALITY
0 1 2 3 4
BENEFIT
VOLITIONALITY
0 1 2 3 4
HARM
0 1 2 3 4
Figure 4: Histograms of median scores from raters by Transitivity dimension. Higher values represent greater levels of Transitivity.
code truthful statements, most actions have been com-
pleted, most objects are not affected, most events are over
a long time span, and there is a bimodal distribution over
volition. One surprising result is that for kinesis there
is a fairly flat distribution. One would expect a larger
skew toward non-kinetic words. Qualitative analysis of
the data suggest that raters used real-world knowledge to
associate motion with the context of actions (even if mo-
tion is not a part of the action), and that raters were less
confident about their answers, prompting more hedging
and a flat distribution.
3.4 Predicting Transitivity
We also performed an set of initial experiments to investi-
gate our ability to predict Transitivity values for held out
data. We extracted three sets of features from the sen-
tences: lexical features, syntactic features, and features
derived from WordNet (Miller, 1990).
Lexical Features A feature was created for each word
in a sentence after being stemmed using the Porter stem-
mer (Porter, 1980).
Syntactic Features We parsed each sentence using the
Stanford Parser (Klein and Manning, 2003) and used
heuristics to identify cases where the main verb is tran-
sitive, where the subject is a nominalization (e.g. ?run-
ning?), or whether the sentence is passive. If any of these
constructions appear in the sentence, we generate a corre-
sponding feature. These represent features identified by
Greene and Resnik (2009).
WordNet Features For each word in the sentence, we
extracted all the possible senses for each word. If any
possible sense was a hyponym (i.e. an instance of) one
of: artifact, living thing, abstract entity, location, or food,
we added a feature corresponding to that top level synset.
For example, the string ?Lincoln? could be an instance
of both a location (Lincoln, Nebraska) and a living thing
(Abe Lincoln), so a feature was added for both the loca-
tion and living thing senses. In addition to these noun-
based features, features were added for each of the pos-
sible verb frames allowed by each of a word?s possible
senses (Fellbaum, 1998).
At first, we performed simple 5-way classification and
found that we could not beat the most frequent class base-
line for any dimension. We then decided to simplify the
classification task to make binary predictions of low-vs-
high instead of fine gradations along the particular di-
mension. To do this, we took all the rated sentences for
each of the seven dimensions and divided the ratings into
low (ratings of 0-1) and high (ratings of 2-4) values for
that dimension. Table 4 shows the results for these bi-
nary classification experiments using different classifiers.
All of the classification experiments were conducted us-
ing the Weka machine learning toolkit (Hall et al, 2009)
and used 10-fold stratified cross validation.
Successfully rating Transitivity requires knowledge
beyond individual tokens. For example, consider kine-
sis. Judging kinesis requires lexical semantics to realize
whether a certain actor is capable of movement, pragmat-
ics to determine if the described situation permits move-
ment, and differentiating literal and figurative movement.
One source of real-world knowledge is WordNet;
adding some initial features from WordNet appears to
help aid some of these classifications. For example, clas-
sifiers trained on the volitionality data were not able to
do better than the most frequent class baseline before the
addition of WordNet-based features. This is a reasonable
result, as WordNet features help the algorithm generalize
which actors are capable of making decisions.
192
Dimension Makeup
Classifier Accuracy
Baseline NB VP SVM-WN +WN -WN +WN -WN +WN
HARM 269/35 88.5 83.9 84.9 87.2 87.8 88.5 88.5
AFFIRMATION 380/20 95.0 92.5 92.0 94.3 95.0 95.0 95.0
VOLITION 209/98 68.1 66.4 69.4 67.1 73.3 68.1 68.1
PUNCTUALITY 158/149 51.5 59.6 61.2 57.0 59.6 51.5 51.5
BENEFIT 220/84 72.4 69.1 65.1 73.4 71.4 72.4 72.4
ASPECT 261/46 85.0 76.5 74.3 81.1 84.7 85.0 85.0
KINESIS 160/147 52.1 61.2 61.2 56.4 60.9 52.1 52.1
Table 4: The results of preliminary binary classification experiments for predicting various transitivity dimensions using different
classifiers such as Naive Bayes (NB), Voted Perceptron (VP) and Support Vector Machines (SVM). Classifier accuracies for two
sets of experiments are shown: without WordNet features (-WN) and with WordNet features (+WN). The baseline simply predicts
the most frequent class. For each dimension, the split between low Transitivity (rated 0-1) and high Transitivity (rated 2-4) is shown
under the ?Makeup? column. All reported accuracies are using 10-fold stratified cross validation.
4 Conclusion
We began with the goal of capturing a subtle linguistic
property for which annotated datasets were not available.
We created a annotated dataset of 400 sentences taken
from the real-word dataset Wikipedia annotated for seven
different Transitivity properties. Users were able to give
consistent answers, and we collected results in a man-
ner that is relatively language independent. Once we ex-
pand and improve this data collection scheme for English,
we hope to perform similar data collection in other lan-
guages. We have available the translated versions of the
questions used in this study for Arabic and German.
Our elicitation task was not as successful as we had
hoped. We learned that while we could form tasks using
everyday language that we thought captured these sub-
tle linguistic properties, we also had many unspoken as-
sumptions that the creative workers on MTurk did not
necessarily share. As we articulated these assumptions
in increasingly long instruction sets to workers, the sheer
size of the instructions began to intimidate and scare off
workers.
While it seems unlikely we can strike a balance that
will give us the answers we want with the elegant instruc-
tions that workers need to feel comfortable for the tasks
as we currently defined them, we hope to modify the task
to embed further linguistic assumptions. For example, we
hope to pilot another version of the elicitation task where
workers modify an existing sentence to change one Tran-
sitivity dimension. Instead of reading and understanding
a plodding discussion of potentially irrelevant details, the
user can simply see a list of sentence versions that are not
allowed.
Our initial classification results suggest that we do not
yet have enough data to always detect these Transitiv-
ity dimensions from unlabeled text or that our algorithms
are using features that do not impart enough information.
It is also possible that using another corpus might yield
greater variation in Transitivity that would aid classifica-
tion; Wikipedia by design attempts to keep a neutral tone
and eschews the highly charged prose that would contain
a great deal of Transitivity.
Another possibility is that, instead of just the Transi-
tivity ratings alone, tweaks to the data collection process
could also help guide classification algorithms (Zaidan et
al., 2008). Thus, instead of clicking on a single annota-
tion label in our current data collection process, Turkers
would click on a data label and the word that most helped
them make a decision.
Our attempts to predict Transitivity are not exhaus-
tive, and there are a number of reasonable algorithms
and resources which could also be applied to the prob-
lem; for example, one might expect semantic role label-
ing or sense disambiguation to possibly aid the prediction
of Transitivity. Determining which techniques are effec-
tive and the reasons why they are effective would aid not
just in predicting Transitivity, which we believe to be an
interesting problem, but also in understanding Transitiv-
ity.
Using services like MTurk allows us to tighten the loop
between data collection, data annotation, and machine
learning and better understand difficult problems. We
hope to refine the data collection process to provide more
consistent results on useful sentences, build classifiers,
and extract features that are able to discover the Transi-
tivity of unlabeled text. We believe that our efforts will
help cast an interesting aspect of theoretical linguistics
into a more pragmatic setting and make it accessible for
use in more practical problems like sentiment analysis.
References
C. Fellbaum, 1998. WordNet : An Electronic Lexi-
cal Database, chapter A semantic network of English
193
verbs. MIT Press, Cambridge, MA.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 503?
511.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1).
Paul J. Hopper and Sandra A. Thompson. 1980.
Transitivity in grammar and discourse. Language,
(56):251?299.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the Asso-
ciation for Computational Linguistics, pages 423?430.
Xiaojuan Ma and Perry R. Cook. 2009. How well do
visual verbs work in daily communication for young
and old adults? In international conference on Human
factors in computing systems, pages 361?364.
George A. Miller. 1990. Nouns in WordNet: A lexical
inheritance system. International Journal of Lexicog-
raphy, 3(4):245?264.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Omar F. Zaidan, Jason Eisner, and Christine Piatko.
2008. Machine learning with annotator rationales
to reduce annotation cost. In Proceedings of the
NIPS*2008 Workshop on Cost Sensitive Learning,
Whistler, BC, December. 10 pages.
194
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 217?221,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Error Driven Paraphrase Annotation using Mechanical Turk
Olivia Buzek
Computer Science and Linguistics
University of Maryland
College Park, MD 20742, USA
olivia.buzek@gmail.com
Philip Resnik
Linguistics and UMIACS
University of Maryland
College Park, MD 20742, USA
resnik@umd.edu
Benjamin B. Bederson
Computer Science and HCIL
University of Maryland
College Park, MD 20742, USA
bederson@cs.umd.edu
Abstract
The source text provided to a machine translation
system is typically only one of many ways the input
sentence could have been expressed, and alternative
forms of expression can often produce a better trans-
lation. We introduce here error driven paraphras-
ing of source sentences: instead of paraphrasing a
source sentence exhaustively, we obtain paraphrases
for only the parts that are predicted to be problematic
for the translation system. We report on an Amazon
Mechanical Turk study that explores this idea, and
establishes via an oracle evaluation that it holds the
potential to substantially improve translation quality.
1 Introduction
The source text provided to a translation system is typ-
ically only one of many ways the input sentence could
have been expressed, and alternative forms of expression
can often produce better translation. This observation is
familiar to most statistical MT researchers in the form of
preprocessing choices ? for example, one segmentation
of a Chinese sentence might yield better translations than
another.1 Over the past several years, MT frameworks
have been developed that permit all the alternatives to be
used as input, represented efficiently as a confusion net-
work, lattice, or forest, rather than forcing selection of
a single input representation. This has improved perfor-
mance when applied to phenomena including segmenta-
tion, morphological analysis, and more recently source
langage word order (Dyer, 2007; Dyer et al, 2008; Dyer
and Resnik, to appear).
We have begun to explore the application of the same
key idea beyond low-level processing phenomena such
as segmentation, instead looking at alternative expres-
sions of meaning. For example, consider translating The
1Chinese is written without spaces, so most MT systems need to
segment the input into words as a preprocessing step.
Democratic candidates stepped up their attacks during
the debate. The same basic meaning could have been ex-
pressed in many different ways, e.g.:
? During the debate the Democratic candidates
stepped up their attacks.
? The Democratic contenders ratcheted up their at-
tacks during the debate.
? The Democratic candidates attacked more aggres-
sively during the debate.
? The candidates in the Democratic debate attacked
more vigorously.
These examples illustrate lexical variation, as well as syn-
tactic differences, e.g. whether the attacking or the in-
creasing serves as the main verb. We hypothesize that
variation of this kind holds a potential advantage for
translation systems, namely that some variations may be
more easily translated than others depending on the train-
ing data that was given to the system, and we can im-
prove translation quality by allowing a system to take best
advantage of the variations it knows about, at the sub-
sentential level, just as the systems described above can
take advantage of alternative segmentations.
Paraphrase lattices provide a way to make this hypoth-
esis operational. This idea is a variation on the uses of
paraphrase in translation introduced by Callison-Burch
and explored by others, as well (Callison-Burch et al,
2006; Madnani et al, 2007; Callison-Burch, 2008; Mar-
ton et al, 2009). These authors have shown that perfor-
mance improvements can be gained by exploiting para-
phrases using phrase pivoting. We have investigated us-
ing pivoting to create exhaustive paraphrase lattices, and
we have also investigated defining upper bounds by elic-
iting human sub-sentential paraphrases using Mechani-
cal Turk. Unfortunately, in both cases, we have found
the size of the paraphrase lattice prohibitive: there are
217
too many spans to paraphrase to make using Turk cost-
effective, and automatically generated paraphrase lattices
turn out to be too noisy to produce improved translations.
A potential solution to this problem comes from a dif-
ferent line of work we are pursuing, in which translation
is viewed as a collaborative process involving people and
machines (Bederson et al, 2010). Here, the idea is that
in translating from a source to a target language, source-
and target-language speakers who are not bilingual can
collaborate to improve the quality of automatic transla-
tion, via an iterative protocol involving translation, back
translation, and the use of a very rich user interface. For
example, consider the following translation from English
to French by an automatic MT system:
? Source: Polls indicate Brown, a state senator,
and Coakley, Massachusetts? Attorney General, are
locked in a virtual tie to fill the late Sen. Ted
Kennedy?s Senate seat.
? System: Les sondages indiquent Brown,
un se?nateur d?e?tat, et Coakley,
Massachusetts? Procureur ge?ne?ral, sont enferme?s
dans une cravate virtuel a` remplir le regrette?
se?nateur Ted Kennedy?s sie`ge au Se?nat.
Someone with only a semester of college French (one of
the authors) can look at this automatic translation, and
see that the underlined parts are probably wrong. Chang-
ing the source sentence to rephrase the underlined pieces
(e.g. changing Massachusetts? Attorney General to the
Attorney General of Massachusetts), we obtain a transla-
tion that is still imperfect but is more acceptable:
? System: Les sondages indiquent que Brown, un
se?nateur d?e?tat, et Coakley, le procureur ge?ne?ral
du Massachusetts, sont enferme?s dans une cravate
virtuel pourvoir le sige au Se?nat de Sen. Ted
Kennedy, qui est de?ce?de? re?cemment.
One could imagine (and, indeed, we are building) a vi-
sual interface that allows a human participant on the tar-
get side to communicate back to a source-side collabora-
tor, in effect saying, ?These underlined pieces look like
they were translated poorly; can you rephrase the rele-
vant parts of your sentence, and perhaps that will lead to
a better translation??2
Putting these ideas together ? source paraphrase and
identification of difficult regions of input for translation
? we arrive at the idea of error driven paraphrasing of
source sentences: instead of paraphrasing to introduce as
much variation as possible everywhere in the sentence,
we suggest that instead it makes sense to paraphrase only
2Communicating which parts of the sentence are relevant across lan-
guages is being done via projection across languages using word align-
ments; cf. (Hwa et al, 2001).
the parts of a source sentence that are problematic for the
translation system. In Section 2 we give a first-pass algo-
rithm for error driven paraphrasing, in Section 3 we de-
scribe how this was realized using MTurk, and Sections 4
and 5 provide an oracle evaluation, discussion, and con-
clusions.
2 Identifying source spans with errors
In error driven paraphrasing, the key idea is to focus on
source spans that are likely to be problematic for trans-
lation. Although in principle one could use human feed-
back from the target side to identify relevant spans, in
this paper we begin with an automatic approach, auto-
matically identifying that are likely to be incorrect via
a novel algorithm. Briefly, we automatically translate
source F to target E, then back-translate to produce F? in
the source language. We compare F and F? using TERp
(Snover et al, 2009), a form of string-edit distance that
identifies various categories of differences between two
sentences, and when at least two consecutive non-P (non-
paraphrase) edits are found, we flag their smallest con-
taining syntactic constituent.
In more detail, we posit that areas of F? where there
were many edits from F will correspond to areas in where
the target translation did not match the English very well.
Specifically, deletions (D), insertions (I), and shifts (S)
are likely to represent errors, while matches (M) and
paraphrases (P) probably represent a fairly accurate trans-
lation. Furthermore, we assume that while a single D, S,
or I edit might be fairly meaningless, a string of at least 2
of those types of edits is likely to represent a substantive
problem in the translation.
In order to identify reasonably meaningful paraphrase
units based on potential errors, we rely on a source lan-
guage constituency parser. Using the parse, we find the
smallest constituent of the sentence containing all of the
tokens in a particular error string. At times, these con-
stituents can be quite large, even the entire sentence. To
weed out these cases, we restrict constituent length to no
more than 7 tokens.
For example, given
F The most recent probe to visit Jupiter was the Pluto-
bound New Horizons spacecraft in late February 2007.
E La investigacio?n ma?s reciente fue la visita de Ju?piter a
Pluto?n de la envolvente sonda New Horizons a fines de
febrero de 2007.
F? The latest research visit Jupiter was the Pluto-bound New
Horizons spacecraft in late February 2007.
spans in the the bolded phrase in F would be identified,
based on the TERp alignment and smallest containing
constituent as shown in Figure 1.
218
NP PP 
NP 
Figure 1: TERp alignment of a source sentence and its back-translation in order to identify a problematic source span.
3 Error driven paraphrasing on MTurk
We chose to use translation from English to Chinese in
this first foray into Mechanical Turk for error driven para-
phrase. This made sense for a number of reasons: first,
because we expected to have a much easier time finding
Turkers; second, because we could make use of a high
quality English parser (in this case the Stanford parser);
and, third, because it meant that we as researchers could
easily read and judge the quality of Turkers? paraphrases.
To create an English-to-Chinese data set, we used the
Chinese-to-English data from the MT08 NIST machine
translation evaluation. We used English reference 0 as
the source sentence, and the original Chinese sentence as
the target. We chose reference 0 because on inspection
these references seemed most reflective of native English
grammar and usage. The data set comprises 1357 sen-
tence pairs. Using the the above described algorithm to
identify possible problem areas in the translation, with
the Google Translate API providing both the translation
and back-translation, we generated 1780 potential error
regions in 1006 of the sentences. Then we created HITs
both to obtain paraphrases, and to validate the quality of
paraphrase responses. Costs were $117.48 for obtaining
multiple paraphrases, and $44.06 for verification.
3.1 Obtaining paraphrases
Based on the phrases marked as problematic by our algo-
rithm, we created HITs asking for paraphrases within 5
sentences, as illustrated in Figure 2. Workers were given
60 minutes to come up with a single paraphrase for each
of the five indicated problematic regions, for a reward of
$0.10. If a worker felt they could not come up with an
alternate phrasing for the marked phrase, they had the
option of marking an ?Unable to paraphrase? checkbox.
We assigned each task to 3 workers, resulting in 3 para-
phrases for every marked phrase. From the 1780 errors,
we got 5340 responses. Of these, 4821 contained actual
paraphrase data, while the rest of the responses indicated
an inability to paraphrase, via the checkbox response. All
paraphrases were passed on to the verification phase.
3.2 Paraphrase Verification
In the verification phase, we generated alternative full
sentences based on the 4821 paraphrases. Workers were
shown an original sentence F and asked to compare it to at
most 5 alternatives, with a maximum of 20 comparisons
made in a HIT. (Recall that although F is the conven-
tional notation for source sentences in machine transla-
tion, in this study the F sentences are in English.) Re-
sponses were given in the form of radio buttons, mark-
ing ?Yes? for an alternate sentence if workers felt it was
grammatical and accurately reflected the content of the
219
original sentence, or ?No? if it did not meet both of those
criteria. Workers were given 30 minutes to make their
decisions, for a reward of $0.05. This task was also as-
signed to 3 workers, resulting in 3 judgments for every
paraphrase.
4 Evaluating Results
Using the paraphrase results from Mechanical Turk, we
constructed rephrased full sentences for every combina-
tion of paraphrase alternatives. For example, if a sentence
had 2 sub-spans paraphrased, and the two sub-spans had 2
and 3 unique paraphrasings, respectively, we would con-
struct 2 ? 3 = 6 alternative full sentences. From the
1780 predicted problematic phrases (within the 1006 au-
tomatically identified sentences with possible translation
errors), we generated 14,934 rephrased sentences. Each
rephrased English sentence was translated into a Chinese
sentence, again via the Google Translate API. We then
evaluated results for translation of the original sentences,
and of all their paraphrase alternatives, via the TER met-
ric, using the MT08 original Chinese sentence as the
target-language reference translation. The evaluation set
includes the 1000 sentence where at least one paraphrase
was provided.3
Our evaluation takes the form of an oracle study: if
we knew with perfect accuracy which variant of a sen-
tence to translate, i.e. among the original and all its para-
phrases, based on knowledge of the reference translation,
how well could we do? An ?oracle? telling us which vari-
ant is best is not available in the real world, of course, but
in situations like this one, oracle studies are often used
to establish the magnitude of the potential gain (Och et
al., 2004). In this case, the baseline is the average TER
score for the 1000 original sentences, 84.4. If an ora-
cle were permitted to choose which variant was the best
to translate, the average TER score would drop to 80.6.4
Drilling down a bit further, we find that a better-translated
paraphrase sentence is available in 313 of the 1000 cases,
or31.3%, and for those 313 cases, TER for the best para-
phrase alternative improves on the TER for the original
sentence by 12.16 TER points.
5 Conclusions
This annotation effort has produced gold standard sub-
sentential paraphrases and paraphrase quality ratings for
spans in a large number of sentences, where the choice
of spans to paraphrase is specifically focused on regions
of the sentence that are difficult to translate. In addi-
3For the other 6 sentences, all problematic spans were marked ?Un-
able to paraphrase? by all 3 MTurkers.
4TER measures errors, so lower is better. A reduction in TER of 3.8
for an MT evaluation dataset would be considered quite substantial; a
reduction of 1 point would typically be a publishable result.
tion, we have performed an initial analysis, using human-
generated paraphrases to provide an oracle evaluation of
how much could be gained in translation by translating
paraphrases of problematic regions in the source sen-
tence. The results suggest if paraphrasing is automati-
cally targeted to problematic source spans using a back-
translation comparison, good paraphrases of the problem-
atic spans could improve translation performance quite
substantially.
In future work, we will use a translation system sup-
porting lattice input (Dyer et al, 2008), rather than the
Google Translation API, in order to take advantage of
fully automatic error-driven paraphrasing, using pivot-
based approaches (e.g. (Callison-Burch et al, 2006)) to
complete the automation of the error-driven paraphrase
process. We will also investigate the use of human rather
than machine identification of likely translation prob-
lems, in the context of collaborative translation (Beder-
son et al, 2010).
References
Benjamin B. Bederson, Chang Hu, and Philip Resnik. 2010. Trans-
lation by iterative collaboration between monolingual users. In
Graphics Interface (GI) conference.
Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006.
Improved statistical machine translation using paraphrases. In
Robert C. Moore, Jeff A. Bilmes, Jennifer Chu-Carroll, and Mark
Sanderson, editors, HLT-NAACL. The Association for Computa-
tional Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints on paraphrases ex-
tracted from parallel corpora. In EMNLP, pages 196?205. ACL.
Chris Dyer and Philip Resnik. to appear. Forest translation. In
NAACL?10.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing word lattice
translation. In Proceedings of HLT-ACL, Columbus, OH.
C. Dyer. 2007. Noisier channel translation: translation from morpho-
logically complex languages. In Proceedings of the Second Work-
shop on Statistical Machine Translation, Prague, June.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan Kolak. 2001.
Evaluating translational correspondence using annotation projection.
In ACL ?02: Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 392?399, Morristown, NJ,
USA. Association for Computational Linguistics.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and Bonnie Dorr.
2007. Using paraphrases for parameter tuning in statistical ma-
chine translation. In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 120?127, Prague, Czech Republic,
June. Association for Computational Linguistics.
Yuval Marton, Chris Callison-Burch, and Philip Resnik. 2009. Im-
proved statistical machine translation using monolingually-derived
paraphrases. In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 381?390, Singa-
pore, August. Association for Computational Linguistics.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar,
Kenji Yamada, Alexander Fraser, Shankar Kumar, Libin Shen, David
Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir R. Radev.
2004. A smorgasbord of features for statistical machine translation.
In HLT-NAACL, pages 161?168.
Matt Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz.
2009. TER-Plus: Paraphrases, Semantic, and Alignment Enhance-
ments to Translation Edit Rate. Machine Translation.
220
Figure 2: HIT format 1: Obtaining sub-sentential paraphrases. Note that as the MTurker types a paraphrase into the box,what
is typed appears immediately (character by character) in the full-sentence context under ?New sentence?, so that they can see
immediately how the entire sentence looks with their paraphrase.
the press trust of india quoted
the government minister for relief and rehabilitation kadam
kadam, the governments relief and rehabilitation minister (2/3)
the government minister concerned with relief and rehabiliation kadam (1/3)
as revealing today that in the last week, the monsoon has started in
all of indias states one
every one of indias state, one (3/3)
each of Indias states one (2/3)
all states of india one (1/3)
after another, and that the financial losses and casualties have been serious in all areas. just in maharashtra, the state which
includes
mumbai, indias largest city,
india?s largest city, mumbai (3/3)
the largest city in India, Mumbai, (3/3)
mumbai, the largest city of india, (3/3)
the number of people
known to have died
who died (3/3)
identified to have died (2/3)
known to have passed away (2/3)
has now reached 358.
Figure 3: Example of error-driven paraphrases produced via HIT format 1, above, for a single sentence. The paraphrase spans
(indented) are shown with the number of MTurkers, out of 3, who labeled that paraphrase in context as acceptable using a ?vali-
dation? HIT.
221
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 72?76,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The University of Maryland Statistical Machine Translation System for
the Fifth Workshop on Machine Translation
Vladimir Eidelman?, Chris Dyer??, and Philip Resnik??
?UMIACS Laboratory for Computational Linguistics and Information Processing
?Department of Linguistics
University of Maryland, College Park
{vlad,redpony,resnik}@umiacs.umd.edu
Abstract
This paper describes the system we devel-
oped to improve German-English transla-
tion of News text for the shared task of
the Fifth Workshop on Statistical Machine
Translation. Working within cdec, an
open source modular framework for ma-
chine translation, we explore the benefits
of several modifications to our hierarchical
phrase-based model, including segmenta-
tion lattices, minimum Bayes Risk de-
coding, grammar extraction methods, and
varying language models. Furthermore,
we analyze decoder speed and memory
performance across our set of models and
show there is an important trade-off that
needs to be made.
1 Introduction
For the shared translation task of the Fifth Work-
shop on Machine Translation (WMT10), we par-
ticipated in German to English translation under
the constraint setting. We were especially inter-
ested in translating from German due to set of
challenges it poses for translation. Namely, Ger-
man possesses a rich inflectional morphology, pro-
ductive compounding, and significant word re-
ordering with respect to English. Therefore, we
directed our system design and experimentation
toward addressing these complications and mini-
mizing their negative impact on translation qual-
ity.
The rest of this paper is structured as follows.
After a brief description of the baseline system
in Section 2, we detail the steps taken to improve
upon it in Section 3, followed by experimental re-
sults and analysis of decoder performance metrics.
2 Baseline system
As our baseline system, we employ a hierarchical
phrase-based translation model, which is formally
based on the notion of a synchronous context-free
grammar (SCFG) (Chiang, 2007). These gram-
mars contain pairs of CFG rules with aligned non-
terminals, and by introducing these nonterminals
into the grammar, such a system is able to uti-
lize both word and phrase level reordering to cap-
ture the hierarchical structure of language. SCFG
translation models have been shown to be well
suited for German-English translation, as they are
able to both exploit lexical information for and ef-
ficiently compute all possible reorderings using a
CKY-based decoder (Dyer et al, 2009).
Our system is implemented within cdec, an ef-
ficient and modular open source framework for
aligning, training, and decoding with a num-
ber of different translation models, including
SCFGs (Dyer et al, 2010).1 cdec?s modular
framework facilitates seamless integration of a
translation model with different language models,
pruning strategies and inference algorithms. As
input, cdec expects a string, lattice, or context-free
forest, and uses it to generate a hypergraph repre-
sentation, which represents the full translation for-
est without any pruning. The forest can now be
rescored, by intersecting it with a language model
for instance, to obtain output translations. The
above capabilities of cdec allow us to perform the
experiments described below, which would other-
wise be quite cumbersome to carry out in another
system.
The set of features used in our model were the
rule translation relative frequency P (e|f), a target
n-gram language model P (e), a ?pass-through?
penalty when passing a source language word
to the target side without translating it, lexical
translation probabilities Plex(e|f) and Plex(f |e),
1http://cdec-decoder.org
72
a count of the number of times that arity-0,1, or 2
SCFG rules were used, a count of the total num-
ber of rules used, a source word penalty, a target
word penalty, the segmentation model cost, and a
count of the number of times the glue rule is used.
The number of non-terminals allowed in a syn-
chronous grammar rule was restricted to two, and
the non-terminal span limit was 12 for non-glue
grammars. The hierarchical phrase-base transla-
tion grammar was extracted using a suffix array
rule extractor (Lopez, 2007).
2.1 Data preparation
In order to extract the translation grammar nec-
essary for our model, we used the provided Eu-
roparl and News Commentary parallel training
data. The lowercased and tokenized training data
was then filtered for length and aligned using the
GIZA++ implementation of IBM Model 4 (Och
and Ney, 2003) to obtain one-to-many alignments
in both directions and symmetrized by combining
both into a single alignment using the grow-diag-
final-and method (Koehn et al, 2003). We con-
structed a 5-gram language model using the SRI
language modeling toolkit (Stolcke, 2002) from
the provided English monolingual training data
and the non-Europarl portions of the parallel data
with modified Kneser-Ney smoothing (Chen and
Goodman, 1996). Since the beginnings and ends
of sentences often display unique characteristics
that are not easily captured within the context of
the model, and have previously been demonstrated
to significantly improve performance (Dyer et al,
2009), we explicitly annotate beginning and end
of sentence markers as part of our translation
process. We used the 2525 sentences in news-
test2009 as our dev set on which we tuned the fea-
ture weights, and report results on the 2489 sen-
tences of the news-test2010 test set.
2.2 Viterbi envelope semiring training
To optimize the feature weights for our model,
we use Viterbi envelope semiring training (VEST),
which is an implementation of the minimum er-
ror rate training (MERT) algorithm (Dyer et al,
2010; Och, 2003) for training with an arbitrary
loss function. VEST reinterprets MERT within
a semiring framework, which is a useful mathe-
matical abstraction for defining two general oper-
ations, addition (?) and multiplication (?) over
a set of values. Formally, a semiring is a 5-tuple
(K,?,?, 0, 1), where addition must be commu-
nicative and associative, multiplication must be as-
sociative and must distribute over addition, and an
identity element exists for both. For VEST, hav-
ing K be the set of line segments, ? be the union
of them, and? be Minkowski addition of the lines
represented as points in the dual plane, allows us
to compute the necessary MERT line search with
the INSIDE algorithm.2 The error function we use
is BLEU (Papineni et al, 2002), and the decoder is
configured to use cube pruning (Huang and Chi-
ang, 2007) with a limit of 100 candidates at each
node. During decoding of the test set, we raise
the cube pruning limit to 1000 candidates at each
node.
2.3 Compound segmentation lattices
To deal with the aforementioned problem in Ger-
man of productive compounding, where words
are formed by the concatenation of several mor-
phemes and the orthography does not delineate the
morpheme boundaries, we utilize word segmen-
tation lattices. These lattices serve to encode al-
ternative ways of segmenting compound words,
and as such, when presented as the input to the
system allow the decoder to automatically choose
which segmentation is best for translation, leading
to markedly improved results (Dyer, 2009).
In order to construct diverse and accurate seg-
mentation lattices, we built a maximum entropy
model of compound word splitting which makes
use of a small number of dense features, such
as frequency of hypothesized morphemes as sep-
arate units in a monolingual corpus, number of
predicted morphemes, and number of letters in
a predicted morpheme. The feature weights are
tuned to maximize conditional log-likelihood us-
ing a small amount of manually created reference
lattices which encode linguistically plausible seg-
mentations for a selected set of compound words.3
To create lattices for the dev and test sets, a lat-
tice consisting of all possible segmentations for
every word consisting of more than 6 letters was
created, and the paths were weighted by the pos-
terior probability assigned by the segmentation
model. Then, max-marginals were computed us-
ing the forward-backward algorithm and used to
prune out paths that were greater than a factor of
2.3 from the best path, as recommended by Dyer
2This algorithm is equivalent to the hypergraph MERT al-
gorithm described by Kumar et al (2009).
3The reference segmentation lattices used for training are
available in the cdec distribution.
73
(2009).4 To create the translation model for lattice
input, we segmented the training data using the
1-best segmentation predicted by the segmenta-
tion model, and word aligned this with the English
side. This version of the parallel corpus was con-
catenated with the original training parallel cor-
pus.
3 Experimental variation
This section describes the experiments we per-
formed in attempting to assess the challenges
posed by current methods and our exploration of
new ones.
3.1 Bloom filter language model
Language models play a crucial role in transla-
tion performance, both in terms of quality, and in
terms of practical aspects such as decoder memory
usage and speed. Unfortunately, these two con-
cerns tend to trade-off one another, as increasing
to a higher-order more complex language model
improves performance, but comes at the cost of
increased size and difficulty in deployment. Ide-
ally, the language model will be loaded into mem-
ory locally by the decoder, but given memory con-
straints, it is entirely possible that the only option
is to resort to a remote language model server that
needs to be queried, thus introducing significant
decoding speed delays.
One possible alternative is a randomized lan-
guage model (RandLM) (Talbot and Osborne,
2007). Using Bloom filters, which are a ran-
domized data structure for set representation, we
can construct language models which signifi-
cantly decrease space requirements, thus becom-
ing amenable to being stored locally in memory,
while only introducing a quantifiable number of
false positives. In order to assess what the im-
pact on translation quality would be, we trained
a system identical to the one described above, ex-
cept using a RandLM. Conveniently, it is possi-
ble to construct a RandLM directly from an exist-
ing SRILM, which is the route we followed in us-
ing the SRILM described in Section 2.1 to create
our RandLM.5 Table 1 shows the comparison of
SRILM and RandLM with respect to performance
on BLEU and TER (Snover et al, 2006) on the test
set.
4While normally the forward-backward algorithm com-
putes sum-marginals, by changing the addition operator to
max, we can obtain max-marginals.
5Default settings were used for constructing the RandLM.
Language Model BLEU TER
RandLM 22.4 69.1
SRILM 23.1 68.0
Table 1: Impact of language model on translation
3.2 Minimum Bayes risk decoding
During minimum error rate training, the decoder
employs a maximum derivation decision rule.
However, upon exploration of alternative strate-
gies, we have found benefits to using a mini-
mum risk decision rule (Kumar and Byrne, 2004),
wherein we want the translation E of the input F
that has the least expected loss, again as measured
by some loss function L:
E? = argmin
E?
EP (E|F )[L(E,E
?)]
= argmin
E?
?
E
P (E|F )L(E,E?)
Using our system, we generate a unique 500-
best list of translations to approximate the poste-
rior distribution P (E|F ) and the set of possible
translations. Assuming H(E,F ) is the weight of
the decoder?s current path, this can be written as:
P (E|F ) ? exp?H(E,F )
where ? is a free parameter which depends on
the models feature functions and weights as well
as pruning method employed, and thus needs to
be separately empirically optimized on a held out
development set. For this submission, we used
? = 0.5 and BLEU as the loss function. Table 2
shows the results on the test set for MBR decod-
ing.
Language Model Decoder BLEU TER
RandLM
Max-D 22.4 69.1
MBR 22.7 68.8
SRILM
Max-D 23.1 68.0
MBR 23.4 67.7
Table 2: Comparison of maximum derivation ver-
sus MBR decoding
3.3 Grammar extraction
Although the grammars employed in a SCFG
model allow increased expressivity and translation
quality, they do so at the cost of having a large
74
Language Model Grammar Decoder Memory (GB) Decoder time (Sec/Sentence)
Local SRILM corpus 14.293 ? 1.228 5.254 ? 3.768
Local SRILM sentence 10.964 ? .964 5.517 ? 3.884
Remote SRILM corpus 3.771 ? .235 15.252 ? 10.878
Remote SRILM sentence .443 ? .235 14.751 ? 10.370
RandLM corpus 7.901 ? .721 9.398 ? 6.965
RandLM sentence 4.612 ? .699 9.561 ? 7.149
Table 3: Decoding memory and speed requirements for language model and grammar extraction varia-
tions
number of rules, thus efficiently storing and ac-
cessing grammar rules can become a major prob-
lem. Since a grammar consists of the set of rules
extracted from a parallel corpus containing tens of
millions of words, the resulting number of rules
can be in the millions. Besides storing the whole
grammar locally in memory, other approaches
have been developed, such as suffix arrays, which
lookup and extract rules on the fly from the phrase
table (Lopez, 2007). Thus, the memory require-
ments for decoding have either been for the gram-
mar, when extracted beforehand, or the corpus, for
suffix arrays. In cdec, however, loading grammars
for single sentences from a disk is very fast relative
to decoding time, thus we explore the additional
possibility of having sentence-specific grammars
extracted and loaded on an as-needed basis by the
decoder. This strategy is shown to massively re-
duce the memory footprint of the decoder, while
having no observable impact on decoding speed,
introducing the possibility of more computational
resources for translation. Thus, in addition to the
large corpus grammar extracted in Section 2.1,
we extract sentence-specific grammars for each of
the test sentences. We measure the performance
across using both grammar extraction mechanisms
and the three different language model configu-
rations: local SRILM, remote SRILM, and Ran-
dLM.
As Table 3 shows, there is a marked trade-
off between memory usage and decoding speed.
Using a local SRILM regardless of grammar in-
creases decoding speed by a factor of 3 compared
to the remote SRILM, and approximately a fac-
tor of 2 against the RandLM. However, this speed
comes at the cost of its memory footprint. With a
corpus grammar, the memory footprint of the lo-
cal SRILM is twice as large as the RandLM, and
almost 4 times as large as the remote SRILM. Us-
ing sentence-specific grammars, the difference be-
comes increasingly glaring, as the remote SRILM
memory footprint drops to ?450MB, a factor of
nearly 24 compared to the local SRILM and a fac-
tor of 10 compared to the process size with the
RandLM. Thus, using the remote SRILM reduces
the memory footprint substantially but at the cost
of significantly slower decoding speed, and con-
versely, using the local SRILM produces increased
decoder speed but introduces a substantial mem-
ory overhead. The RandLM provides a median
between the two extremes: reduced memory and
(relatively) fast decoding at the price of somewhat
decreased translation quality. Since we are using
a relatively large beam of 1000 candidates for de-
coding, the time presented in Table 3 does not rep-
resent an accurate basis for comparison of cdec to
other decoders, which should be done using the
results presented in Dyer et al (2010).
We also tried one other grammar extraction
configuration, which was with so-called ?loose?
phrase extraction heuristics, which permit un-
aligned words at the edges of phrases (Ayan and
Dorr, 2006). When decoded using the SRILM and
MBR, this achieved the best performance for our
system, with a BLEU score of 23.6 and TER of
67.7.
4 Conclusion
We presented the University of Maryland hier-
archical phrase-based system for the WMT2010
shared translation task. Using cdec, we experi-
mented with a number of methods that are shown
above to lead to improved German-to-English
translation quality over our baseline according to
BLEU and TER evaluation. These include methods
to directly address German morphological com-
plexity, such as appropriate feature functions, seg-
mentation lattices, and a model for automatically
constructing the lattices, as well as alternative de-
coding strategies, such asMBR.We also presented
75
several language model configuration alternatives,
as well as grammar extraction methods, and em-
phasized the trade-off that must be made between
decoding time, memory overhead, and translation
quality in current statistical machine translation
systems.
5 Acknowledgments
The authors gratefully acknowledge partial sup-
port from the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-2-001 and NSF award IIS0838801.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the view of the
sponsors.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In Proceedings
of the Joint Conference of the International Com-
mittee on Computational Linguistics and the As-
sociation for Computational Linguistics (COLING-
ACL?2006), pages 9?16, Sydney.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310?318.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In Computational Linguistics, volume 33(2),
pages 201?228.
Chris Dyer, Hendra Setiawan, Yuval Marton, and
P. Resnik. 2009. The University of Maryland sta-
tistical machine translation system for the Fourth
Workshop on Machine Translation. In Proceedings
of the EACL-2009 Workshop on Statistical Machine
Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL System Demonstrations.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for mt. In Proceedings
of NAACL-HLT.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144?151.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL 2004: Main Proceedings.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 163?171.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP,
pages 976?985.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics, volume 29(21), pages
19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Intl. Conf. on Spoken
Language Processing.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, June.
76
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 344?350,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Noisy SMS Machine Translation in Low-Density Languages
Vladimir Eidelman?, Kristy Hollingshead?, and Philip Resnik??
?UMIACS Laboratory for Computational Linguistics and Information Processing
?Department of Linguistics
University of Maryland, College Park
{vlad,hollingk,resnik}@umiacs.umd.edu
Abstract
This paper presents the system we developed
for the 2011 WMT Haitian Creole?English
SMS featured translation task. Applying stan-
dard statistical machine translation methods to
noisy real-world SMS data in a low-density
language setting such as Haitian Creole poses
a unique set of challenges, which we attempt
to address in this work. Along with techniques
to better exploit the limited available train-
ing data, we explore the benefits of several
methods for alleviating the additional noise
inherent in the SMS and transforming it to
better suite the assumptions of our hierarchi-
cal phrase-based model system. We show
that these methods lead to significant improve-
ments in BLEU score over the baseline.
1 Introduction
For the featured translation task of the Sixth Work-
shop on Statistical Machine Translation, we devel-
oped a system for translating Haitian Creole Emer-
gency SMS messages. Given the nature of the task,
translating text messages that were sent during the
January 2010 earthquake in Haiti to an emergency
response service called Mission 4636, we were not
only faced with the problem of dealing with a low-
density language, but additionally, with noisy, real-
world data in a domain which has thus far received
relatively little attention in statistical machine trans-
lation. We were especially interested in this task be-
cause of the unique set of challenges that it poses
for existing translation systems. We focused our re-
search effort on techniques to better utilize the lim-
ited available training resources, as well as ways in
which we could automatically alleviate and trans-
form the noisy data to our advantage through the
use of automatic punctuation prediction, finite-state
raw-to-clean transduction, and grammar extraction.
All these techniques contributed to improving trans-
lation quality as measured by BLEU score over our
baseline system.
The rest of this paper is structured as follows.
First, we provide a brief overview of our baseline
system in Section 2, followed by an examination of
issues posed by this task and the steps we have taken
to address them in Section 3, and finally we con-
clude with experimental results and additional anal-
ysis.
2 System Overview
Our baseline system is based on a hierarchical
phrase-based translation model, which can formally
be described as a synchronous context-free gram-
mar (SCFG) (Chiang, 2007). Our system is imple-
mented in cdec, an open source framework for align-
ing, training, and decoding with a number of differ-
ent translation models, including SCFGs. (Dyer et
al., 2010). 1 SCFG grammars contain pairs of CFG
rules with aligned nonterminals, where by introduc-
ing these nonterminals into the grammar, such a sys-
tem is able to utilize both word and phrase level re-
ordering to capture the hierarchical structure of lan-
guage. SCFG translation models have been shown
to produce state-of-the-art translation for most lan-
guage pairs, as they are capable of both exploit-
ing lexical information for and efficiently comput-
ing all possible reorderings using a CKY-based de-
coder (Dyer et al, 2009).
1http://cdec-decoder.org
344
One benefit of cdec is the flexibility allowed with
regard to the input format, as it expects either a
string, lattice, or context-free forest, and subse-
quently generates a hypergraph representing the full
translation forest without any pruning. This forest
can now be rescored, by intersecting it with a lan-
guage model for instance, to obtain output transla-
tions. These capabilities of cdec allow us to perform
the experiments described below, which may have
otherwise proven to be quite impractical to carry out
in another system.
The set of features used in our model were the
rule translation relative frequency P (e|f), a target
n-gram language model P (e), lexical translation
probabilities Plex(e|f) and Plex(f |e), a count of the
total number of rules used, a target word penalty,
and a count of the number of times the glue rule
is used. The number of non-terminals allowed in
a synchronous grammar rule was restricted to two,
and the non-terminal span limit was 12 for non-glue
grammars. The hierarchical phrase-based transla-
tion grammar was extracted using a suffix array rule
extractor (Lopez, 2007).
To optimize the feature weights for our model, we
used an implementation of the hypergraph minimum
error rate training (MERT) algorithm (Dyer et al,
2010; Och, 2003) for training with an arbitrary loss
function. The error function we used was BLEU (Pa-
pineni et al, 2002), and the decoder was configured
to use cube pruning (Huang and Chiang, 2007) with
a limit of 100 candidates at each node.
2.1 Data Preparation
The SMS messages were originally translated by
English speaking volunteers for the purpose of pro-
viding first responders with information and loca-
tions requiring their assistance. As such, in order to
create a suitable parallel training corpus from which
to extract a translation grammar, a number of steps
had to be taken in addition to lowercasing and tok-
enizing both sides of training data. Many of the En-
glish translations had additional notes sections that
were added by the translator to the messages with
either personal notes or further informative remarks.
As these sections do not correspond to any text on
the source side, and would therefore degrade the
alignment process, these had to be identified and re-
moved. Furthermore, the anonymization of the data
resulted in tokens such as firstname and phonenum-
ber which were prevalent and had to be preserved
as they were. Since the total amount of Haitian-
English parallel data provided is quite limited, we
found additional data and augmented the available
set with data gathered by the CrisisCommons group
and made it available to other WMT participants.
The combined training corpus from which we ex-
tracted our grammar consisted of 123,609 sentence
pairs, which was then filtered for length and aligned
using the GIZA++ implementation of IBM Model
4 (Och and Ney, 2003) to obtain one-to-many align-
ments in either direction and symmetrized using the
grow-diag-final-and method (Koehn et al, 2003).
We trained a 5-gram language model using the
SRI language modeling toolkit (Stolcke, 2002) from
the English monolingual News Commentary and
News Crawl language modeling training data pro-
vided for the shared task and the English portion of
the parallel data with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1996). We have previ-
ously found that since the beginnings and ends of
sentences often display unique characteristics that
are not easily captured within the context of the
model, explicitly annotating beginning and end of
sentence markers as part of our translation process
leads to significantly improved performance (Dyer
et al, 2009).
A further difficulty of the task stems from the fact
that there are two versions of the SMS test set, a raw
version, which contains the original messages, and a
clean version which was post-edited by humans. As
the evaluation of the task will consist of translating
these two versions of the test set, our baseline sys-
tem consisted of two systems, one built on the clean
data using the 900 sentences in SMS dev clean to
tune our feature weights, and evaluated using SMS
devtest clean, and one built analogously for the raw
data tuned on the 900 sentences in SMS dev raw and
evaluated on SMS devtest raw. We report results on
these sets as well as the 1274 sentences in the SMS
test set.
3 Experimental Variation
The results produced by the baseline systems are
presented in Table 1. As can be seen, the clean ver-
sion performs on par with the French-English trans-
345
BASELINE
Version Set BLEU TER
clean
dev 30.36 56.04
devtest 28.15 57.45
test 27.97 59.19
raw
dev 25.62 63.27
devtest 24.09 63.82
test 23.33 65.93
Table 1: Baseline system BLEU and TER scores
lation quality in the 2011 WMT shared translation
task,2 and significantly outperforms the raw version,
despite the content of the messages being identical.
This serves to underscore the importance of proper
post-processing of the raw data in order to attempt to
close the performance gap between the two versions.
Through analysis of the raw and clean data we iden-
tified several factors which we believe greatly con-
tribute to the difference in translation output. We
examine punctuation in Section 3.2, grammar post-
processing in Section 3.3, and morphological differ-
ences in Sections 3.4 and 3.5.
3.1 Automatic Resource Confidence Weighting
A practical technique when working with a low-
density language with limited resources is to du-
plicate the same trusted resource multiple times in
the parallel training corpus in order for the transla-
tion probabilities of the duplicated items to be aug-
mented. For instance, if we have confidence in the
entries of the glossary and dictionary, we can dupli-
cate them 10 times in our training data to increase
the associated probabilities. The aim of this strat-
egy is to take advantage of the limited resources and
exploit the reliable ones.
However, what happens if some resources are
more reliable than others? Looking at the provided
resources, we saw that in the Haitisurf dictionary,
the entry for paske is matched with for, while in
glossary-all-fix, paske is matched with because. If
we then consider the training data, we see that in
most cases, paske is in fact translated as because.
Motivated by this type of phenomenon, we em-
ployed an alternative strategy to simple duplication
which allows us to further exploit our prior knowl-
edge.
2http://matrix.statmt.org/matrix
First, we take the previously word-aligned base-
line training corpus and for each sentence pair and
word ei compute the alignment link count c(ei, fj)
over the positions j that ei is aligned with, repeating
for c(fi, ej) in the other direction. Then, we pro-
cess each resource we are considering duplicating,
and augment its score by c(ei, fj) for every pair of
words which was observed in the training data and
is present in the resource. This score is then normal-
ized by the size of the resource, and averaged over
both directions. The outcome of this process is a
score for each resource. Taking these scores on a
log scale and pinning the top score to associate with
20 duplications, the result is a decreasing number of
duplications for each subsequent resources, based on
our confidence in its entries. Thus, every entry in the
resource receives credit, as long as there is evidence
that the entries we have observed are reliable. On
our set of resources, the process produces a score of
17 for the Haitisurf dictionary and 183 for the glos-
sary, which is in line with what we would expect.
It may be that the resources may have entries which
occur in the test set but not in the training data, and
thus we may inadvertently skew our distribution in
a way which negatively impacts our performance,
however, overall we believe it is a sound assumption
that we should bias ourselves toward the more com-
mon occurrences based on the training data, as this
should provide us with a higher translation probabil-
ity from the good resources since the entries are re-
peated more often. Once we obtain a proper weight-
ing scheme for the resources, we construct a new
training corpus, and proceed forward from the align-
ment process.
Table 2 presents the BLEU and TER results of the
standard strategy of duplication against the confi-
dence weighting scheme outlined above. As can be
CONF. WT. X10
Version Set BLEU TER BLEU TER
clean
dev 30.79 55.71 30.61 55.31
devtest 27.92 57.66 28.22 57.06
test 27.97 59.65 27.74 59.34
raw
dev 26.11 62.64 25.72 62.99
devtest 24.16 63.71 24.18 63.71
test 23.66 65.69 23.06 66.78
Table 2: Confidence weighting versus x10 duplication
346
seen, the confidence weighting scheme substantially
outperforms the duplication for the dev set of both
versions, but these improvements do not carry over
to the clean devtest set. Therefore, for the rest of the
experiments presented in the paper, we will use the
confidence weighting scheme for the raw version,
and the standard duplication for the clean version.
3.2 Automatic Punctuation Prediction
Punctuation does not usually cause a problem in
text-based machine translation, but this changes
when venturing into the domain of SMS. Punctua-
tion is very informative to the translation process,
providing essential contextual information, much
as the aforementioned sentence boundary markers.
When this information is lacking, mistakes which
would have otherwise been avoided can be made.
Examining the data, we see there is substantially
more punctuation in the clean set than in the raw.
For example, there are 50% more comma?s in the
clean dev set than in the raw. A problem of lack of
punctuation has been studied in the context of spo-
ken language translation, where punctuation predic-
tion on the source language prior to translation has
been shown to improve performance (Dyer, 2007).
We take an analogous approach here, and train a hid-
den 5-gram model using SRILM on the punctuated
portion of the Haitian side of the parallel data. We
then applied the model to punctuate the raw dev set,
and tuned a system on this punctuated set. How-
ever, the translation performance did not improve.
This may have been do to several factors, including
the limited size of the training set, and the lack of
in-domain punctuated training data. Thus, we ap-
plied a self-training approach. We applied the punc-
tuation model to the SMS training data, which is
only available in the raw format. Once punctuated,
we re-trained our punctuation prediction model, now
including the automatically punctuated SMS data
AUTO-PUNC
Version Set BLEU TER
raw
dev 26.09 62.84
devtest 24.38 64.26
test 23.59 65.91
Table 3: Automatic punctuation prediction results
as part of the punctuation language model training
data. We use this second punctuation prediction
model to predict punctuation for the tuning and eval-
uation sets. We continue by creating a new parallel
training corpus which substitutes the original SMS
training data with the punctuated version, and build
a new translation system from it. The results from
using the self-trained punctuation method are pre-
sented in Table 3. Future experiments on the raw
version are performed using this punctuation.
3.3 Grammar Filtering
Although the grammars of a SCFG model per-
mit high-quality translation, the grammar extraction
procedure extracts many rules which are formally li-
censed by the model, but are otherwise incapable of
helping us produce a good translation. For example,
in this task we know that the token firstname must al-
ways translate as firstname, and never as phonenum-
ber. This refreshing lack of ambiguity allows us to
filter the grammar after extracting it from the train-
ing corpus, removing any grammar rule where these
conditions are not met, prior to decoding. Filtering
removed approximately 5% of the grammar rules.3
Table 4 shows the results of applying grammar fil-
tering to the raw and clean version.
GRAMMAR
Version Set BLEU TER
clean
dev 30.88 54.53
devtest 28.69 56.21
test 28.29 58.78
raw
dev 26.41 62.47
devtest 24.47 63.26
test 23.96 65.82
Table 4: Results of filtering the grammar in a post-
processing step before decoding
3.4 Raw-Clean Segmentation Lattice
As noted above, a major cause of the performance
degradation from the clean to the raw version is re-
lated to the morphological errors in the messages.
Figure 1 presents a segmentation lattice with two
versions of the same sentence; the first being from
3We experimented with more aggressive filtering based
on punctuation and numbers, but translation quality degraded
rapidly.
347
the raw version, and the second from the clean. We
can see that that Ilavach has been broken into two
segments, while ki sou has been combined into one.
Since we do not necessarily know in advance
which segmentation is the correct one for a better
quality translation, it may be of use to be able to
utilize both segmentations and allow the decoder to
learn the appropriate one. In previous work, word
segmentation lattices have been used to address the
problem of productive compounding in morphologi-
cally rich languages, such as German, where mor-
phemes are combined to make words but the or-
thography does not delineate the morpheme bound-
aries. These lattices encode alternative ways of seg-
menting compound words, and allow the decoder
to automatically choose which segmentation is best
for translation, leading to significantly improved re-
sults (Dyer, 2009). As opposed to building word
segmentation lattices from a linguistic morphologi-
cal analysis of a compound word, we propose to uti-
lize the lattice to encode all alternative ways of seg-
menting a word as presented to us in either the clean
or raw versions of a sentence. As the task requires
us to produce separate clean and raw output on the
test set, we tune one system on a lattice built from
the clean and raw dev set, and use the single system
to decode both the clean and raw test set separately.
Table 5 presents the results of using segmentation
lattices.
3.5 Raw-to-Clean Transformation Lattice
As can be seen in Tables 1, 2, and 3, system per-
formance on clean text greatly outperforms system
performance on raw text, with a difference of almost
5 BLEU points. Thus, we explored the possibility of
automatically transforming raw text into clean text,
based on the ?parallel? raw and clean texts that were
provided as part of the task.
One standard approach might have been to train
SEG-LATTICE
Version Set BLEU TER
raw
dev 26.17 61.88
devtest 24.64 62.53
test 23.89 65.27
Table 5: Raw-Clean segmentation lattice tuning results
FST-LATTICE
Version Set BLEU TER
raw
dev 26.20 62.15
devtest 24.21 63.45
test 22.56 67.79
Table 6: Raw-to-clean transformation lattice results
a Haitian-to-Haitian MT system to ?translate? from
raw text to clean text. However, since the training set
was only available as raw text, and only the dev and
devtest datasets had been cleaned, we clearly did not
have enough data to train a raw-to-clean translation
system. Thus, we created a finite-state transducer
(FST) by aligning the raw dev text to the clean dev
text, on a sentence-by-sentence basis. These raw-to-
clean alignments were created using a simple mini-
mum edit distance algorithm; substitution costs were
calculated according to orthographic match.
One option would be to use the resulting raw-to-
clean transducer to greedily replace each word (or
phrase) in the raw input with the predicted transfor-
mation into clean text. However, such a destructive
replacement method could easily introduce cascad-
ing errors by removing text that might have been
translated correctly. Fortunately, as mentioned in
Section 2, and utilized in the previous section, the
cdec decoder accepts lattices as input. Rather than
replacing raw text with the predicted transformation
into ?clean? text, we add a path to the input lat-
tice for each possible transform, for each word and
phrase in the input. We tune a system on a lattice
built from this approach on the dev set, and use the
FST developed from the dev set in order to create
lattices for decoding the devtest and test sets. An
example is shown in Figure 3.4. Note that in this
example, the transformation technique correctly in-
serted new paths for ilavach and ki sou, correctly
retained the single path for zile, but overgenerated
many (incorrect) options for nan. Note, though, that
the original path for nan remains in the lattice, de-
laying the ambiguity resolution until later in the de-
coding process. Results from creating raw-to-clean
transformation lattices are presented in Table 6.
By comparing the results in Table 6 to those in
Table 5, we can see that the noise introduced by the
finite-state transformation process outweighed the
348
1 2Eske?ske 3
nou ap kite nou mouri nan zile
4ila
5Ilavach
vach
6la 7
kisou
9
ki 8okay 10lasou
Figure 1: Partial segmentation lattice combining the raw and clean versions of the sentence:
Are you going to let us die on Ile a` Vaches which is located close the city of Les Cayes.
15
16
a
m
pa
te
l
17
an
nan
lan
nan
18
nanak
19
zile
20
tant
21
ila
22ilavach
nan vach
23
e
24
sa
la
lanan 25
ki
26kisou
sou
Figure 2: Partial input lattice for sentence in Figure 3.4, generated using the raw-to-clean transform technique
described in Section 3.5.
gains of adding new phrases for tuning.
4 System Comparison
Table 7 shows the performance on the devtest set
of each of the system variations that we have pre-
sented in this paper. From this table, we can see
that our best-performing system on clean data was
the GRAMMAR system, where the training data was
multiplied by ten as described in Section 3.1, then
the grammar was filtered as described in Section 3.3.
Our performance on clean test data, using this sys-
tem, was 28.29 BLEU and 58.78 TER. Table 7 also
demonstrates that our best-performing system on
raw data was the SEG-LATTICE system, where the
training data was confidence-weighted (Section 3.1),
the grammar was filtered (Section 3.3), punctuation
was automatically added to the raw data as described
in Section 3.2, and the system was tuned on a lattice
created from the raw and clean dev dataset. Our per-
formance on raw test data, using this system, was
23.89 BLEU and 65.27 TER.
5 Conclusion
In this paper we presented our system for the 2011
WMT featured Haitian Creole?English translation
task. In order to improve translation quality of low-
density noisy SMS data, we experimented with a
number of methods that improve performance on
both the clean and raw versions of the data, and help
clean raw
System BLEU TER BLEU TER
BASELINE 28.15 57.45 24.09 63.82
CONF. WT. 27.92 57.66 24.16 63.71
X10 28.22 57.06 24.18 63.71
GRAMMAR 28.69 56.21 24.47 63.26
AUTO-PUNC ? ? 24.38 64.26
SEG-LATTICE ? ? 24.64 62.53
FST-LATTICE ? ? 24.21 63.45
Table 7: Comparison of all systems? performance on
devtest set
close the gap between the post-edited and real-world
data according to BLEU and TER evaluation. The
methods employed were developed to specifically
address shortcomings we observed in the data, such
as segmentation lattices for morphological ambigu-
ity, confidence weighting for resource utilization,
and punctuation prediction for lack thereof. Overall,
this work emphasizes the feasibility of adapting ex-
isting translation technology to as-yet underexplored
domains, as well as the shortcomings that need to be
addressed in future work in real-world data.
6 Acknowledgments
The authors gratefully acknowledge partial support
from the DARPA GALE program, No. HR0011-06-
2-001. In addition, the first author was supported by
the NDSEG Fellowship. Any opinions or findings
do not necessarily reflect the view of the sponsors.
349
References
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting of
the Association for Computational Linguistics, pages
310?318.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In Computational Linguistics, volume 33(2),
pages 201?228.
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip
Resnik. 2009. The University of Maryland statistical
machine translation system for the Fourth Workshop
on Machine Translation. In Proceedings of the EACL-
2009 Workshop on Statistical Machine Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL System Demonstrations.
Chris Dyer. 2007. The University of Maryland Trans-
lation system for IWSLT 2007. In Proceedings of
IWSLT.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In Proceedings of
NAACL-HLT.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 144?151.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48?54.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP,
pages 976?985.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. In
Computational Linguistics, volume 29(21), pages 19?
51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. on Spoken Language
Processing.
350
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 399?404,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The Value of Monolingual Crowdsourcing in a Real-World Translation
Scenario: Simulation using Haitian Creole Emergency SMS Messages
Chang Hu?, Philip Resnik??, Yakov Kronrod?
Vladimir Eidelman?, Olivia Buzek??, Benjamin B. Bederson?
?UMIACS and Department of Linguistics
?UMIACS and Department of Computer Science
University of Maryland, College Park
{changhu,bederson}@cs.umd.edu
{resnik,vlad,buzek}@umiacs.umd.edu
yakov@umd.edu
Abstract
MonoTrans2 is a translation system that com-
bines machine translation (MT) with human
computation using two crowds of monolin-
gual source (Haitian Creole) and target (En-
glish) speakers. We report on its use in the
WMT 2011 Haitian Creole to English trans-
lation task, showing that MonoTrans2 trans-
lated 38% of the sentences well compared to
Google Translate?s 25%.
1 Introduction
One of the most remarkable success stories to come
out of the January 2010 earthquake in Haiti in-
volved translation (Munro, 2010). While other
forms of emergency response and communication
channels were failing, text messages were still get-
ting through, so a number of people came together to
create a free phone number for emergency text mes-
sages, which allowed earthquake victims to report
those who were trapped or in need of medical atten-
tion. The problem, of course, was that most people
were texting in Haitian Creole (Kreyol), a language
not many of the emergency responders understood,
and few, if any, professional translators were avail-
able. The availability of usable translations literally
became a matter of life and death.
In response to this need, Stanford University grad-
uate student Rob Munro coordinated the rapid cre-
ation of a crowdsourcing framework, which allowed
volunteers ? including, for example, Haitian expa-
triates and French speakers ? to translate messages,
providing responders with usable information in as
little as ten minutes. Translations may not have been
perfect, but to a woman in labor, it had to have made
a big difference for English-speaking responders to
see Undergoing children delivery Delmas 31 instead
of Fanm gen tranche pou fe` yon pitit nan Delmas 31.
What about a scenario, though, in which even am-
ateur bilingual volunteers are hard to find, or too
few in number? What about a scenario, e.g. the
March 2011 earthquake and tsunami in Japan, in
which there are many people worldwide who wish
to help but are not fluent in both the source and tar-
get languages?
For the last few years, we have been exploring the
idea of monolingual crowdsourcing for translation
? that is, technology-assisted collaborative transla-
tion involving crowds of participants who know only
the source or target language (Buzek et al, 2010;
Hu, 2009; Hu et al, 2010; Hu et al, 2011; Resnik
et al, 2010). Our MonoTrans2 framework has pre-
viously shown very promising results on children?s
books: on a test set where Google Translate pro-
duced correct translations for only 10% of the input
sentences, monolingual German and Spanish speak-
ers using our framework produced translations that
were fully correct (as judged by two independent
bilinguals) nearly 70% of the time (Hu et al, 2011).
We used the same framework in the WMT 2011
Haitian-English translation task. For this experi-
ment, we hired Haitian Creole speakers located in
Haiti, and recruited English speakers located in the
U.S., to serve as the monolingual crowds.
2 System
MonoTrans2 is a translation system that combines
machine translation (MT) with human computation
(Quinn et al, 2011) using two ?crowds? of mono-
lingual source (Haitian Creole) and target (English)
399
speakers.1 We summarize its operation here; see Hu
et al (2011) for details.
The Haitian Creole sentence is first automatically
translated into English and presented to the English
speakers. The English speakers then can take any of
the following actions for candidate translations:
? Mark a phrase in the candidate as an error
? Suggest a new translation candidate
? Vote candidates up or down
Identifying likely errors and voting for candidates
are things monolinguals can do reasonably well:
even without knowing the intended interpretation,
you can often identify when some part of a sentence
doesn?t make sense, or when one sentence seems
more fluent or plausible than another. Sometimes
rather than identifying errors, it is easier to suggest
an entirely new translation candidate based on the
information available on the target side, a variant
of monolingual post-editing (Callison-Burch et al,
2004).
Any new translation candidates are then back-
translated into Haitian Creole, and any spans marked
as translation errors are projected back to identify
the corresponding spans in the source sentence, us-
ing word alignments as the bridge (cf. Hwa et al
(2002), Yarowsky et al (2001)).2 The Haitian Cre-
ole speakers can then:
? Rephrase the entire source sentence (cf.
(Morita and Ishida, 2009))
? ?Explain? spans marked as errors
? Vote candidates up or down (based on the back-
translation)
Source speakers can ?explain? error spans by of-
fering a different way of phrasing that piece of the
source sentence (Resnik et al, 2010), in order to
produce a new source sentence, or by annotating the
spans with images (e.g. via Google image search)
or Web links (e.g. to Wikipedia). The protocol then
continues: new source sentences created via partial-
1For the work reported here, we used Google Translate as
the MT component via the Google Translate Research API.
2The Google Translate Research API provides alignments
with its hypotheses.
or full-sentence paraphrase pass back through MT
to the English side, and any explanatory annota-
tions are projected back to the corresponding spans
in the English candidate translations (where the er-
ror spans had been identified). The process is asyn-
chronous: participants on the Haitian Creole and
English sides can work independently on whatever
is available to them at any time. At any point, the
voting-based scores can be used to extract a 1-best
translation.
In summary, the MonoTrans2 framework uses
noisy MT to cross the language barrier, and supports
monolingual participants in doing small tasks that
gain leverage from redundant information, the hu-
man capacity for linguistic and real-world inference,
and the wisdom of the crowd.
3 Experiment
We recruited 26 English speakers and 4 Haitian Cre-
ole speakers. The Haitian Creole speakers were re-
cruited from Haiti and do not speak English. Five of
the 26 English speakers were paid UMD undergrad-
uates; the other 21 were volunteer researchers, grad-
uate students, and staff unrelated to this research. 3
Over a 13 day period, Haitian Creole and English
speaker efforts totaled 15 and 29 hours, respectively.
4 Data Sets
Our original goal of fully processing the entire SMS
clean test and devtest sets could not be realized in the
available time, owing to unanticipated reshuffling of
the data by the shared task organizers and logistical
challenges working with participants in Haiti. Ta-
ble 1 summarizes the data set sizes before and after
reshuffling. We put 1,224 sentences from the pre-
before after
test 1,224 1,274
devtest 925 900
Table 1: SMS clean data sets before and after reshuffling
reshuffling test set, interspersed with 123 of the 925
sentences from the pre-reshuffling devtest set, into
the system ? 1,347 sentences in total. We report
3These, obviously, did not include any of the authors.
400
results on the union of pre- and post-reshuffling de-
vtest sentences (Set A, |A| = 1516), and the post-
reshuffling test set (Set B, |B| = 1274 ).
5 Evaluation
Of the 1,347 sentences available for processing in
MonoTrans2, we define three subsets:
? Touched: Sentences that were processed by at
least one person (657 sentences)
? Each-side: Sentences that were processed by at
least one English speaker followed by at least
one Haitian Creole speaker (431 sentences)
? Full: Sentences that have at least three trans-
lation candidates, of which the most voted-for
one received at least three votes (207 sentences)
We intersect these three sets with sets A and B in or-
der to evaluate MonoTrans2 output against the pro-
vided references (Table 2).4
Set S |S| |S ?A| |S ?B|
Touched 657 162 168
Each-side 431 127 97
Full 207 76 60
Table 2: Data sets for evaluation and their sizes
Tables 3 and 4 report two automatic scoring met-
rics, uncased BLEU and TER, comparing Mono-
Trans2 (M2) against Google Translate (GT) as a
baseline.
Set Condition BLEU TER
Touched ?A
GT 21.75 56.99
M2 23.25 57.27
Each-side ?A
GT 21.44 57.51
M2 21.47 58.98
Full ?A
GT 25.05 54.15
M2 27.59 52.78
Table 3: BLEU and TER results for different levels of com-
pletion on the devtest set A
Since the number of sentences in each evaluated
set is different (Table 2), we cannot directly compare
4Note that according to these definitions, Touched contains
both Each-side and Full, but Each-side does not contain Full.
Set Condition BLEU TER
Touched ?B
GT 19.78 59.88
M2 24.09 58.15
Each-side ?B
GT 21.15 56.88
M2 23.80 57.19
Full ?B
GT 22.51 54.51
M2 28.90 52.22
Table 4: BLEU and TER results for different levels of com-
pletion on the test set B
scores between the sets. However, Table 4 shows
that when the MonoTrans2 process is run on test
items ?to completion?, in the sense defined by ?Full?
(i.e. Full?B), we see a dramatic BLEU gain of 6.39,
and a drop in TER of 2.29 points. Moreover, even
when only target-side or only source-side monolin-
gual participation is available we see a gain of 4.31
BLEU and a drop of 1.73 TER points (Touched?B).
By contrast, the results on the devtest data are en-
couraging, but arguably mixed (Table 3). In order to
step away from the vagaries of single-reference au-
tomatic evaluations, therefore, we also conducted an
evaluation based on human judgments. Two native
English speakers unfamiliar with the project were
recruited and paid for fluency and adequacy judg-
ments: for each target translation paired with its cor-
responding reference, each evaluator rated the tar-
get sentence?s fluency and adequacy on a 5-point
scale, where fluency of 5 indicates complete fluency
and adequacy of 5 indicates complete preservation
of meaning (Dabbadie et al, 2002).5
Sentences N Google MonoTrans2
Full ?A 76 18 (24%) 30 (39%)
Full ?B 60 15 (25%) 23 (38%)
Table 5: Number of sentences with maximum possible
adequacy (5) in Full ?A and Full ?B, respectively.
Similar to Hu et al (2011), we adopt the very con-
servative criterion that a translation output is consid-
ered correct only if both evaluators independently
give it a rating of 5. Unlike Hu et al (2011), for
whom children?s book translation requires both flu-
ency and adequacy, we make this a requirement only
5Presentation order was randomized.
401
for adequacy, since in this scenario what matters to
aid organizations is not whether a translation is fully
fluent, but whether it is correct. On this criterion,
the Google Translate baseline of around 25% cor-
rect improves to around 40% for Monotrans, con-
sistently for both the devtest and test data (Table 5).
Nonetheless, Figures 1 and 2 make it clear that the
improvements in fluency are if anything more strik-
ing.
5.1 Statistical Analysis
Variable Adequacy Fluency
Positive
mostSingleCandidateVote ** ***
candidateCount ** **
numOfAnswers * NS
Negative
roundTrips *** ***
voteCount * .
Table 6: Effects of independent variables in linear regres-
sion for 330 touched sentences
(Signif. codes: ?***? 0.001, ?**? 0.01, ?*? 0.05, ?.? 0.1)
In addition to the main evaluation, we investi-
gated the relationship between tasks performed in
the MonoTrans2 system and human judgments us-
ing linear regression and an analysis of variance.
We evaluate the set of all 330 touched sentences in
Touched?A and Touched?B in order to under-
stand which properties of the MonoTrans2 process
correlate with better translation outcomes.
Our analysis focused on improvement over the
Google Translate baseline, looking specifically at
the improvement based on the human evaluators? av-
eraged fluency and adequacy scores.
Table 6 summarizes the positive and negative
effects for five of six variables we considered that
came out significant for at least one of the measures.
6
The positive results were as expected. Having
more votes for the winning candidate (mostSingle-
CandidateVote) made it more successful, since this
means that more people felt it was a good represen-
tative translation. Having more candidates to choose
6A sixth, numOfVoters, was not significant in the linear re-
gression for either adequacy or fluency.
from (candidateCount) meant that more people had
taken the time to generate alternatives, reflecting at-
tention paid to the sentence. Also, the amount of
attention paid to target speakers? requests for clarifi-
cation (numOfAnswers) is as expected related to the
adequacy of the final translation, and perhaps as ex-
pected does not correlate with fluency of the output
since it helps with meaning and not actual target-side
wording.
We were, however, confused at first by the neg-
ative influence of the roundTrips measure and vote-
Count measures. We conjecture that the first effect
arises due to a correlation between roundTrips and
translation difficulty; much harder sentences would
have led to many more paraphrase requests, and
hence to more round trips. We attempted to inves-
tigate this hypothesis by testing correlation with a
naive measure of sentence difficulty, length, but this
was not fruitful. We suspect that inspecting use of
abbreviations, proper nouns, source-side mistakes,
and syntactic complexity would give us more insight
into this issue.
As for voteCount, the negative correlation is un-
derstandable when considered side by side with
the other vote-based measure, mostSingleCandidat-
eVote. Having a higher number of votes for the win-
ning candidate leads to improvement (strongly sig-
nificant for both adequacy and fluency), so a higher
general vote count means that people were also vot-
ing more times for other candidates. Hence, once the
positive winning vote count is taken into account,
the remaining votes actually represent disagreement
on the candidates, hence correlating negatively with
overall improvement over baseline.
It is important to note that when these measures
are all considered together, they show that there is a
clear correlation between the MonoTrans2 system?s
human processing and the eventual increase in both
quality and fluency of the sentences. As people give
more attention to sentences, these sentences show
better performance, as judged by increase over base-
line.
6 Discussion
Our experiment did not address acquisition of, and
incentives for, monolingual participants. In fact, get-
ting time from Haitian Creole speakers, even for pay,
402
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?s
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(a) Fluency Distribution
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?S
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(b) Adequacy Distribution
Figure 1: Human judgments for fluency and adequacy in fully processed devtest items (Full ?A)
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?s
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(a) Fluency Distribution
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?S
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(b) Adequacy Distribution
Figure 2: Human judgments for fluency and adequacy in fully processed test items (Full ?B)
created a large number of logistical challenges, and
was a contributing factor as to why we did not obtain
translations for the entire test set. However, avail-
ability of monolingual participants is not the issue
being addressed in this experiment: we are confi-
dent that in a real-world scenario like the Haitian
or Japanese earthquakes, large numbers of monolin-
gual volunteers would be eager to help, certainly in
larger total numbers than bilingual volunteers. What
matters here, therefore, is not how much of the test
set was translated in total, but how much the trans-
lations improved for the sentences where monolin-
gual crowdsourcing was involved, compared to the
MT baseline, and what throughput might be like in
a real-world scenario.
We also were interested in throughput, particu-
larly in comparison to bilingual translators. In previ-
ous experimentation (Hu et al, 2011), throughput in
MonoTrans2 extrapolated to roughly 800 words per
day, a factor of 2.5 slower than professional trans-
lators? typical speed of 2000 words per day. In
this experiment, overall translation speed averaged
about 300 words per day, a factor of more than 6
times slower. However, this is an extremely pes-
simistic estimate, for several reasons. First, our pre-
vious experiment had more than 20 users per side,
while here our Haitian crowd consisted of only four
people. Second, we discovered after beginning the
experiment that the translation of our instructions
into Haitian Creole had been done somewhat slop-
pily. And, third, we encountered a range of tech-
nical and logistical problems with our Haitian par-
ticipants, ranging from finding a location with In-
ternet access to do the work (ultimately an Internet
Cafe? turned out to be the best option), to slow and
sporadic connections (even in an Internet Cafe?), to
relative lack of motivation for part-time rather than
full-time work. It is fair to assume that in a real-
world scenario, some unanticipated problems like
these might crop up, but it also seems fair to assume
that many would not; for example, most people from
the Haitian Creole and French-speaking communi-
ties who volunteered using Munro et al?s system
in January 2010 were not themselves located in the
403
third world.
Finally, regarding quality, the results here are
promising, albeit not as striking as those Hu et al
(2011) obtained for Spanish-German translation of
children?s books. The nature of SMS messages
themselves may have been a contributing factor to
the lower translation adequacy: even in clean form,
these are sometimes written using shorthand (e.g.
?SVP?), and are sometimes not syntactically correct.
The text messages are seldom related to each other,
unlike sentences in larger bodies of text where even
partially translated sentences can be related to each
other to provide context, as is the case for children?s
books. One should also keep in mind that the under-
lying machine translation engine, Google Translate
between Haitian Creole and English, is still in an al-
pha phase.
Those considerations notwithstanding, it is en-
couraging to see a set of machine translations get
better without the use of any human bilingual exper-
tise. We are optimistic that with further refinements
and research, monolingual translation crowdsourc-
ing will make it possible to harness the vast num-
ber of technologically connected people who want
to help in some way when disaster strikes.
7 Acknowledgments
This research is supported by NSF contract
#BCS0941455 and by a Google Research Award.
References
Olivia Buzek, Philip Resnik, and Benjamin B. Bederson.
2010. Error driven paraphrase annotation using me-
chanical turk. In NAACL 2010 Workshop on Creating
Speech and Text Language Data With Amazon?s Me-
chanical Turk.
Chris Callison-Burch, Colin Bannard, , and Josh
Schroeder. 2004. Improving statistical translation
through editing. In Workshop of the European Asso-
ciation for Machine Translation.
Marianne Dabbadie, Anthony Hartley, Margaret King,
Keith J. Miller, Widad Mustafa El Hadi, Andrei
Popescu-Belis, Florence Reeder, and Michelle Vanni.
2002. A hands-on study of the reliability and coher-
ence of evaluation metrics. In Workshop at the LREC
2002 Conference, page 8. Citeseer.
Chang Hu, Benjamin B. Bederson, and Philip Resnik.
2010. Translation by iterative collaboration between
monolingual users. In Proceedings of Graphics Inter-
face 2010 on Proceedings of Graphics Interface 2010,
pages 39?46, Ottawa, Ontario, Canada. Canadian In-
formation Processing Society.
Chang Hu, Ben Bederson, Philip Resnik, and Yakov Kro-
nrod. 2011. Monotrans2: A new human computation
system to support monolingual translation. In Human
Factors in Computing Systems (CHI 2011), Vancou-
ver, Canada, May. ACM, ACM.
Chang Hu. 2009. Collaborative translation by monolin-
gual users. In Proceedings of the 27th international
conference extended abstracts on Human factors in
computing systems, pages 3105?3108, Boston, MA,
USA. ACM.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan
Kolak. 2002. Evaluating translational correspon-
dence using annotation projection. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 392?399, Philadelphia, Penn-
sylvania. Association for Computational Linguistics.
Daisuke Morita and Toru Ishida. 2009. Designing pro-
tocols for collaborative translation. In PRIMA ?09:
Proceedings of the 12th International Conference on
Principles of Practice in Multi-Agent Systems, pages
17?32, Berlin, Heidelberg. Springer-Verlag.
Robert Munro. 2010. Crowdsourced translation for
emergency response in haiti: the global collaboration
of local knowledge. In AMTA Workshop on Collabo-
rative Crowdsourcing for Translation. Keynote.
Alexander J. Quinn, Bederson, and Benjamin B. Beder-
son. 2011. Human computation: A survey and tax-
onomy of a growing field. In Human Factors in Com-
puting Systems (CHI 2011), Vancouver, Canada, May.
ACM, ACM.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alexander J. Quinn, and Benjamin B. Bederson. 2010.
Improving translation via targeted paraphrasing. In
EMNLP.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via
robust projection across aligned corpora. In HLT
?01: Proceedings of the first international conference
on Human language technology research, pages 1?8,
Morristown, NJ, USA. Association for Computational
Linguistics.
404
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 128?133,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Towards Efficient Large-Scale Feature-Rich Statistical Machine
Translation
Vladimir Eidelman1, Ke Wu1, Ferhan Ture1, Philip Resnik2, Jimmy Lin3
1 Dept. of Computer Science 2 Dept. of Linguistics 3 The iSchool
Institute for Advanced Computer Studies
University of Maryland
{vlad,wuke,fture,resnik,jimmylin}@umiacs.umd.edu
Abstract
We present the system we developed to
provide efficient large-scale feature-rich
discriminative training for machine trans-
lation. We describe how we integrate with
MapReduce using Hadoop streaming to
allow arbitrarily scaling the tuning set and
utilizing a sparse feature set. We report our
findings on German-English and Russian-
English translation, and discuss benefits,
as well as obstacles, to tuning on larger
development sets drawn from the parallel
training data.
1 Introduction
The adoption of discriminative learning methods
for SMT that scale easily to handle sparse and lex-
icalized features has been increasing in the last
several years (Chiang, 2012; Hopkins and May,
2011). However, relatively few systems take full
advantage of the opportunity. With some excep-
tions (Simianer et al, 2012), most still rely on
tuning a handful of common dense features, along
with at most a few thousand others, on a relatively
small development set (Cherry and Foster, 2012;
Chiang et al, 2009). While more features tuned
on more data usually results in better performance
for other NLP tasks, this has not necessarily been
the case for SMT.
Thus, our main focus in this paper is to improve
understanding into the effective use of sparse fea-
tures, and understand the benefits and shortcom-
ings of large-scale discriminative training. To
this end, we conducted experiments for the shared
translation task of the 2013 Workshop on Statis-
tical Machine Translation for the German-English
and Russian-English language pairs.
2 Baseline system
We use a hierarchical phrase-based decoder im-
plemented in the open source translation system
cdec1 (Dyer et al, 2010). For tuning, we use
Mr. MIRA2 (Eidelman et al, 2013), an open
source decoder agnostic implementation of online
large-margin learning in Hadoop MapReduce. Mr.
MIRA separates learning from the decoder, allow-
ing the flexibility to specify the desired inference
procedure through a simple text communication
protocol. The decoder receives input sentences
and weight updates from the learner, while the
learner receives k-best output with feature vectors
from the decoder.
Hadoop MapReduce (Dean and Ghemawat,
2004) is a popular distributed processing frame-
work that has gained widespread adoption, with
the advantage of providing scalable parallelization
in a manageable framework, taking care of data
distribution, synchronization, fault tolerance, as
well as other features. Thus, while we could oth-
erwise achieve the same level of parallelization, it
would be in a more ad-hoc manner.
The advantage of online methods lies in their
ability to deal with large training sets and high-
dimensional input representations while remain-
ing simple and offering fast convergence. With
Hadoop streaming, our system can take advantage
of commodity clusters to handle parallel large-
scale training while also being capable of running
on a single machine or PBS-managed batch clus-
ter.
System design To efficiently encode the infor-
mation that the learner and decoder require (source
sentence, reference translation, grammar rules) in
a manner amenable to MapReduce, i.e. avoiding
dependencies on ?side data? and large transfers
across the network, we append the reference and
1http://cdec-decoder.org
2https://github.com/kho/mr-mira
128
per-sentence grammar to each input source sen-
tence. Although this file?s size is substantial, it is
not a problem since after the initial transfer, it re-
sides on Hadoop distributed file system, and Map-
Reduce optimizes for data locality when schedul-
ing mappers.
A single iteration of training is performed as
a Hadoop streaming job. Each begins with a
map phase, with every parallel mapper loading the
same initial weights and decoding and updating
parameters on a shard of the data. This is followed
by a reduce phase, with a single reducer collect-
ing final weights from all mappers and computing
a weighted average to distribute as initial weights
for the next iteration.
Parameter Settings We tune our system toward
approximate sentence-level BLEU (Papineni et al,
2002),3 and the decoder is configured to use cube
pruning (Huang and Chiang, 2007) with a limit
of 200 candidates at each node. For optimiza-
tion, we use a learning rate of ?=1, regularization
strength of C=0.01, and a 500-best list for hope
and fear selection (Chiang, 2012) with a single
passive-aggressive update for each sentence (Ei-
delman, 2012).
Baseline Features We used a set of 16 stan-
dard baseline features: rule translation relative
frequency P (e|f), lexical translation probabilities
Plex(e|f) and Plex(f |e), target n-gram language
model P (e), penalties for source and target words,
passing an untranslated source word to the tar-
get side, singleton rule and source side, as well
as counts for arity-0,1, or 2 SCFG rules, the total
number of rules used, and the number of times the
glue rule is used.
2.1 Data preparation
For both languages, we used the provided Eu-
roparl and News Commentary parallel training
data to create the translation grammar neces-
sary for our model. For Russian, we addi-
tionally used the Common Crawl and Yandex
data. The data were lowercased and tokenized,
then filtered for length and aligned using the
GIZA++ implementation of IBM Model 4 (Och
and Ney, 2003) to obtain one-to-many align-
ments in both directions and symmetrized sing the
grow-diag-final-and method (Koehn et al, 2003).
3We approximate corpus BLEU by scoring sentences us-
ing a pseudo-document of previous 1-best translations (Chi-
ang et al, 2009).
We constructed a 5-gram language model us-
ing SRILM (Stolcke, 2002) from the provided
English monolingual training data and parallel
data with modified Kneser-Ney smoothing (Chen
and Goodman, 1996), which was binarized using
KenLM (Heafield, 2011). The sentence-specific
translation grammars were extracted using a suffix
array rule extractor (Lopez, 2007).
For German, we used the 3,003 sentences in
newstest2011 as our Dev set, and report results
on the 3,003 sentences of the newstest2012 Test
set using BLEU and TER (Snover et al, 2006).
For Russian, we took the first 2,000 sentences of
newstest2012 for Dev, and report results on the re-
maining 1,003. For both languages, we selected
1,000 sentences from the bitext to be used as an
additional testing set (Test2).
Compound segmentation lattices As German
is a morphologically rich language with produc-
tive compounding, we use word segmentation lat-
tices as input for the German translation task.
These lattices encode alternative segmentations of
compound words, allowing the decoder to auto-
matically choose which segmentation is best. We
use a maximum entropy model with recommended
settings to create lattices for the dev and test sets,
as well as for obtaining the 1-best segmentation of
the training data (Dyer, 2009).
3 Evaluation
This section describes the experiments we con-
ducted in moving towards a better understanding
of the benefits and challenges posed by large-scale
high-dimensional discriminative tuning.
3.1 Sparse Features
The ability to incorporate sparse features is the pri-
mary reason for the recent move away from Min-
imum Error Rate Training (Och, 2003), as well as
for performing large-scale discriminative training.
We include the following sparse Boolean feature
templates in our system in addition to the afore-
mentioned baseline features: rule identity (for ev-
ery unique rule in the grammar), rule shape (map-
ping rules to sequences of terminals and nontermi-
nals), target bigrams, lexical insertions and dele-
tions (for the top 150 unaligned words from the
training data), context-dependent word pairs (for
the top 300 word pairs in the training data), and
structural distortion (Chiang et al, 2008).
129
Dev Test Test2 5k 10k 25k 50k
en 75k 74k 27k 132k 255k 634k 1258k
de 74k 73k 26k 133k 256k 639k 1272k
Table 1: Corpus statistics in tokens for German.
Dev Test Test2 15k
ru 46k 24k 24k 350k
en 50k 27k 25k 371k
Table 2: Corpus statistics in tokens for
Russian.
Set # features Tune Test
?BLEU ?BLEU ?TER
de-en 16 22.38 22.69 60.61
+sparse 108k 23.86 23.01 59.89
ru-en 16 30.18 29.89 49.05
+sparse 77k 32.40 30.81 48.40
Table 3: Results with the addition of sparse fea-
tures for German and Russian.
All of these features are generated from the
translation rules on the fly, and thus do not have
to be stored as part of the grammar. To allow for
memory efficiency while scaling the training data,
we hash all the lexical features from their string
representation into a 64-bit integer.
Altogether, these templates result in millions of
potential features, thus how to select appropriate
features, and how to properly learn their weights
can have a large impact on the potential benefit.
3.2 Adaptive Learning Rate
The passive-aggressive update used in MIRA has a
single learning rate ? for all features, which along
with ? limits the amount each feature weight can
change at each update. However, since the typical
dense features (e.g., language model) are observed
far more frequently than sparse features (e.g., rule
identity), it has been shown to be advantageous
to use an adaptive per-feature learning rate that
allows larger steps for features that do not have
much support (Green et al, 2013; Duchi et al,
2011). Essentially, instead of having a single pa-
rameter ?,
?? min
(
C, cost(y
?)?w>(f(y+)? f(y?))
?f(y+)? f(y?)?2
)
w? w + ??
(
f(y+)? f(y?)
)
we instead have a vector ? with one entry for each
feature weight:
??1 ? ??1 + ?diag
(
ww>
)
w? w + ??1/2
(
f(y+)? f(y?)
)
?=1 
?=0.01 
?=0.1 
22.2 
22.4 
22.6 
22.8 
23 
23.2 
23.4 
23.6 
23.8 
24 
BLE
U 
Iteration 
Figure 1: Learning curves for tuning when using
a single step size (?) versus different per-feature
learning rates.
In practice, this update is very similar to that of
AROW (Crammer et al, 2009; Chiang, 2012).
Figure 1 shows learning curves for sparse mod-
els with a single learning rate, and adaptive learn-
ing with ?=0.01 and ?=0.1, with associated re-
sults on Test in Table 4.4 As can be seen, using
a single ? produces almost no gain on Dev. How-
ever, while both settings using an adaptive rate fare
better, the proper setting of ? is important. With
?=0.01 we observe 0.5 BLEU gain over ?=0.1 in
tuning, which translates to a small gain on Test.
Henceforth, we use an adaptive learning rate with
?=0.01 for all experiments.
Table 3 presents baseline results for both lan-
guages. With the addition of sparse features, tun-
ing scores increase by 1.5 BLEU for German, lead-
ing to a 0.3 BLEU increase on Test, and 2.2 BLEU
for Russian, with 1 BLEU increase on Test. The
majority of active features for both languages are
rule id (74%), followed by target bigrams (14%)
and context-dependent word pairs (11%).
3.3 Feature Selection
As the tuning set size increases, so do the num-
ber of active features. This may cause practi-
cal problems, such as reduced speed of computa-
tion and memory issues. Furthermore, while some
4All sparse models are initialized with the same tuned
baseline weights. Learning rates are local to each mapper.
130
Adaptive # feat. Tune Test
?BLEU ?BLEU ?TER
none 74k 22.75 22.87 60.19
?=0.01 108k 23.86 23.01 59.89
?=0.1 62k 23.32 22.92 60.09
Table 4: Results with different ? settings for using a per-feature learning rate with sparse features.
Set # feat. Tune Test
?BLEU ?BLEU ?TER
all 510k 32.99 22.36 59.26
top 200k 200k 32.96 22.35 59.29
all 373k 34.26 28.84 49.29
top 200k 200k 34.45 28.98 49.30
Table 5: Comparison of using all features versus
top k selection.
sparse features will generalize well, others may
not, thereby incurring practical costs with no per-
formance benefit. Simianer et al (2012) recently
explored `1/`2 regularization for joint feature se-
lection for SMT in order to improve efficiency and
counter overfitting effects. When performing par-
allel learning, this allows for selecting a reduced
set of the top k features at each iteration that are
effective across all learners.
Table 5 compares selecting the top 200k fea-
tures versus no selection for a larger German and
Russian tuning set (?3.4). As can be seen, we
achieve the same performance with the top 200k
features as we do when using double that amount,
while the latter becomes increasing cumbersome
to manage. Therefore, we use a top 200k selection
for the remainder of this work.
3.4 Large-Scale Training
In the previous section, we saw that learning
sparse features on the small development set leads
to substantial gains in performance. Next, we
wanted to evaluate if we can obtain further gains
by scaling the tuning data to learn parameters di-
rectly on a portion of the training bitext. Since the
bitext is used to learn rules for translation, using
the same parallel sentences for grammar extrac-
tion as well as for tuning feature weights can lead
to severe overfitting (Flanigan et al, 2013). To
avoid this issue, we used a jackknifing method to
split the training data into n = 10 folds, and built
a translation system on n?1 folds, while sampling
sentences from the News Commentary portion of
the held-out fold to obtain tuning sets from 5,000
to 50,000 sentences for German, and 15,000 sen-
tences for Russian.
Results for large-scale training for German are
presented in Table 6. Although we cannot com-
pare the tuning scores across different size sets,
we can see that tuning scores for all sets improve
substantially with sparse features. Unfortunately,
with increasing tuning set size, we see very little
improvement in Test BLEU and TER with either
feature set. Similar findings for Russian are pre-
sented in Table 7. Introducing sparse features im-
proves performance on each set, respectively, but
Dev always performs better on Test.
While tuning on Dev data results in better BLEU
on Test than when tuning on the larger sets, it is
important to note that although we are able to tune
more features on the larger bitext tuning sets, they
are not composed of the same genre as the Tune
and Test sets, resulting in a domain mismatch.
This phenomenon is further evident in German
when testing each model on Test2, which is se-
lected from the bitext, and is thus closer matched
to the larger tuning sets, but is separate from both
the parallel data used to build the translation model
and the tuning sets. Results on Test2 clearly show
significant improvement using any of the larger
tuning sets versus Dev for both the baseline and
sparse features. The 50k sparse setting achieves
almost 1 BLEU and 2 TER improvement, showing
that there are significant differences between the
Dev/Test sets and sets drawn from the bitext.
For Russian, we amplified the effects by select-
ing Test2 from the portion of the bitext that is sepa-
rate from the tuning set, but is among the sentences
used to create the translation model. The effects of
overfitting are markedly more visible here, as there
is almost a 7 BLEU difference between tuning on
Dev and the 15k set with sparse features. Further-
more, it is interesting to note when looking at Dev
that using sparse features has a significant nega-
tive impact, as the baseline tuned Dev performs
131
Tuning Test
?BLEU ?TER
5k 22.81 59.90
10k 22.77 59.78
25k 22.88 59.77
50k 22.86 59.76
Table 8: Results for German with 2 iterations of
tuning on Dev after tuning on larger set.
reasonably well, while the introduction of sparse
features leads to overfitting the specificities of the
Dev/Test genre, which are not present in the bitext.
We attempted two strategies to mitigate this
problem: combining the Dev set with the larger
bitext tuning set from the beginning, and tuning
on a larger set to completion, and then running 2
additional iterations of tuning on the Dev set using
the learned model. Results for tuning on Dev and a
larger set together are presented in Table 7 for Rus-
sian and Table 6 for German. As can be seen, the
resulting model improves somewhat on the other
genre and strikes a middle ground, although it is
worse on Test than Dev.
Table 8 presents results for tuning several ad-
ditional iterations after learning a model on the
larger sets. Although this leads to gains of around
0.5 BLEU on Test, none of the models outperform
simply tuning on Dev. Thus, neither of these two
strategies seem to help. In future work, we plan
to forgo randomly sampling the tuning set from
the bitext, and instead actively select the tuning
set based on similarity to the test set.
4 Conclusion
We explored strategies for scaling learning for
SMT to large tuning sets with sparse features.
While incorporating an adaptive per-feature learn-
ing rate and feature selection, we were able to
use Hadoop to efficiently take advantage of large
amounts of data. Although discriminative training
on larger sets still remains problematic, having the
capability to do so remains highly desirable, and
we plan to continue exploring methods by which
to leverage the power of the bitext effectively.
Acknowledgments
This research was supported in part by the DARPA
BOLT program, Contract No. HR0011-12-C-
0015; NSF under awards IIS-0916043 and IIS-
1144034. Vladimir Eidelman is supported by a
NDSEG Fellowship.
References
S. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
ACL.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation. In
NAACL-HLT.
D. Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. JMLR,
13:1159?1187.
K. Crammer, A. Kulesza, and M. Dredze. 2009. Adap-
tive regularization of weight vectors. In NIPS.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In OSDI.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese,
F. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and
P. Resnik. 2010. cdec: A decoder, alignment, and
learning framework for finite-state and context-free
translation models. In ACL System Demonstrations.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for mt. In Proceedings
of NAACL-HLT.
Vladimir Eidelman, Ke Wu, Ferhan Ture, Philip
Resnik, and Jimmy Lin. 2013. Mr. MIRA: Open-
source large-margin structured learning on map-
reduce. In ACL System Demonstrations.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In WMT.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In NAACL.
S. Green, S. Wang, D. Cer, and C. Manning. 2013.
Fast and adaptive online training of feature-rich
translation models. In ACL.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In WMT.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP.
132
Tuning # mappers # features Tune Test Test2
?BLEU ?BLEU ?TER ?BLEU ?TER
Dev 120 16 22.38 22.69 60.61 29.31 54.26
5k 120 16 32.60 22.14 59.60 29.69 52.96
10k 120 16 33.16 22.06 59.43 29.93 52.37
Dev+10k 120 16 19.40 22.32 59.37 30.17 52.45
25k 300 16 32.48 22.21 59.54 30.03 51.71
50k 600 16 32.21 22.21 59.39 29.94 52.55
Dev 120 108k 23.86 23.01 59.89 29.65 53.86
5k 120 159k 33.70 22.26 59.26 30.53 51.84
10k 120 200k 34.00 22.12 59.24 30.51 51.71
Dev+10k 120 200k 19.62 22.42 59.17 30.26 52.21
25k 300 200k 32.96 22.35 59.29 30.39 52.14
50k 600 200k 32.86 22.40 59.15 30.54 51.88
Table 6: German evaluation with large-scale tuning, showing numbers of mappers employed, number of
active features for best model, and test scores on Test and bitext Test2 domains.
Tuning # mappers # features Tune Test Test2
?BLEU ?BLEU ?TER ?BLEU ?TER
Dev 120 16 30.18 29.89 49.05 57.14 32.56
15k 200 16 34.65 28.60 49.63 59.64 30.65
Dev+15k 200 16 33.97 28.88 49.37 58.24 31.81
Dev 120 77k 32.40 30.81 48.40 52.90 36.85
15k 200 200k 35.05 28.34 49.69 59.81 30.59
Dev+15k 200 200k 34.45 28.98 49.30 57.61 32.71
Table 7: Russian evaluation with large-scale tuning, showing numbers of mappers employed, number of
active features for best model, and test scores on Test and bitext Test2 domains.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
P. Simianer, S. Riezler, and C. Dyer. 2012. Joint fea-
ture selection in distributed stochastic learning for
large-scale discriminative training in SMT. In ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In ICSLP.
133
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, page 72,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
?I Want to Talk About, Again, My Record On Energy ...?: Modeling
Agendas and Framing in Political Debates and Other Conversations
Philip Resnik
University of Maryland
Abstract
Computational social science has been emerging over the last several years as a hotbed of in-
teresting work, taking advantage of, to quote Lazer et al. (Science, v.323), ?digital traces that
can be compiled into comprehensive pictures of both individual and group behavior, with the
potential to transform our understanding of our lives, organizations, and societies.? Within that
larger setting, I?m interested in how language is used to influence people, with an emphasis on
computational modeling of agendas (who is most effectively directing attention, and toward what
topics?), framing or ?spin? (what underlying perspective does this language seek to encourage?),
and sentiment (how does someone feel, as evidenced in the language they use)? These questions
are particularly salient in political discourse. In this talk, I?ll present recent work looking at po-
litical debates and other conversations using Bayesian models to capture relevant aspects of the
conversational dynamics, as well as new methods for collecting people?s reactions to speeches,
debates, and other public conversations on a large scale.
This talk includes work done in collaboration with Jordan Boyd-Graber, Viet-An Nguyen, Deb-
orah Cai, Amber Boydstun, Rebecca Glazier, Matthew Pietryka, Tim Jurka, and Kris Miler.
72
