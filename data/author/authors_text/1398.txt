 
Spoken Dialogue for Simulation Control and Conversational Tutoring 
 
Elizabeth Owen Bratt Karl Schultz Brady Clark 
CSLI,  
Stanford University, 
Stanford, CA 94305 
CSLI,  
Stanford University, 
Stanford, CA 94305 
CSLI,  
Stanford University, 
Stanford, CA 94305 
ebratt@csli.stanford.edu schultzk@csli.stanford.edu bzack@csli.stanford.edu 
 
  
 
 
Abstract 
 
 
This demonstration shows a flexible tutoring 
system for studying the effects of different 
tutoring strategies enhanced by a spoken 
language interface.  The hypothesis is that 
spoken language increases the effectiveness 
of automated tutoring.  The domain is Navy 
damage control. 
1 Technical Content 
 
This demonstration shows a flexible tutoring 
system for studying the effects of different tutoring 
strategies enhanced by a spoken language interface.  
The hypothesis is that spoken language increases the 
effectiveness of automated tutoring.  Our focus is on 
the SCoT-DC spoken language tutor for Navy 
damage control; however,  because SCoT-DC 
performs reflective tutoring on DC-Train simulator 
sessions, we have also developed a speech interface 
for the existing DC-Train damage control simulator, 
to promote ease of use as well as consistency of 
interface. 
Our tutor is developed within the Architecture for 
Conversational Intelligence (Lemon et al 2001).  We 
use the Open Agent Architecture (Martin et al 1999) 
for communication between agents based on the 
Nuance speech recognizer, the Gemini natural 
language system (Dowding et al 1993), and Festival 
speech synthesis. Our tutor adds its own dialogue 
manager agent, for general principles of 
conversational intelligence, and a tutor agent, which 
uses tutoring strategies and tactics to plan out an 
appropriate review and react to the student's answers 
to questions and desired topics. 
  
The SCoT-DC tutor, in Socratic style, asks 
questions rather than giving explanations.  The tutor 
has a repertoire of hinting tactics to deploy in 
response to student answers to questions, and 
identifies and iscusses repeated mistakes.  The 
student is able to ask "why" questions after certain 
tutor explanations, and  to alter the tutorial plan by 
requesting that the tutor skip discussion of certain 
topics. In DC-Train, the system uses several windows 
to provide information graphically, in addition to the 
spoken messages.  In SCoT-DC, the Ship Display 
from DC-Train is used for both multimodal input and 
output.  
 
Both DC-Train and SCoT-DC use the same 
overall Gemini grammar, with distinct top-level 
grammars producing appropriate subsets for each 
application. Our Gemini grammar currently has 166 
grammar rules and 811 distinct words.  In a Nuance 
language model compiled from the Gemini grammar 
(Moore 1998), different top-level grammars are used 
in SCoT-DC to enhance speech recognition based on 
expected answers. 
 
2 Performance Assessment 
 
Experiments to assess the effectiveness of SCoT-
DC tutoring are underway in March 2004, with 15 
subjects currently scheduled.  In July 2003, students 
in the Repair Locker Head class at the Navy Fleet 
Training Center in San Diego ran 12 sessions with 
DC-Train. Sessions ranged from 1 to 65 user 
utterances, with an average of  21.  The average 
utterance length was 7 words. In speech recognition, 
about 22% of utterances were rejected, and the
sentences with a recognition hypothesis had a word 
error rate of 27%.  The transcribed data, combined 
with developer test run data, gave us 327 unique out-
of-grammar sentences.  Of these, we found 79 
examples where the automatic Nuance endpointing 
cut off an utterance too early, and 20 examples of 
disfluent speech.  118 sentences were determined to 
be potentially useful phrasings to add to the grammar, 
while 73 sentences were found to lie outside the 
scope of the application. 
 
To address these issues, w have added new 
phrasings to the grammar.  We also intend to use 
Nuance?s Listen & Learn offline grammar adaptation 
tool, to give higher probabilities to likely sentences 
while retaining broad grammar-based coverage.  We 
may also adjust endpointing time, based on partial 
speech recognition hypothesis, to give extra time to 
the kinds of sentences typically occurring with more 
internal pauses. Disfluencies may decrease as users 
become more familiar with DC-Train and SCoT-DC 
during the comparatively longer use xpected from 
each user in a typical tutoring session
The graphical interface for the DC-Train 
simulator is shown in Figure 1. 
 
 
 
Figure 1: DC-Train simulator GUI 
 
Each window on the screen is modeled on a 
source of information available to a real-lifeDCA on 
a ship, including as a detailed drawing of the several 
hundred compartments on the ship, a record of all 
communications to and from the DCA, a hazard 
detection panel showing the locations of alarms 
which have occurred, and a panel showing the 
firemain, i.e. the pipes carrying water throughout the 
ship, and the valves and pumps controlling the flow 
of the water.  The window depicting heads represents 
the other personnel in the same room as the DCA, 
who are available to receive and transmit messages. 
 While in the original version of DC-Train, 
the DCA?s orders and communications to other 
personnel on the ship took place through a menu 
system, this demo presents the newer spoken 
dialogue interface.  Spoken commands take the form 
of actual Navy commands, thus enabling the Navy 
student to train in the same manner as they would 
perform these duties through radio communications 
on a ship. 
The user clicks a button to begin speaking, 
and the speech is recognized by Nuance, using a 
grammar-based language model automatically 
derived from the Gemini grammar used for parsing 
and interpretation of the commands.  A dialogue 
manager then maps the Gemini logical forms into 
DC-Train commands.  To allow the student to 
monitor the success of the speech recognizer, the text 
of the utterance is displayed. Responses from the 
simulated personnel are spoken by Festival speech 
synthesis, and also displayed as text on the screen. 
Most spoken interactions with DC-Train 
involve the student DCA giving single commands 
without any use of dialogue structure; however, the 
system will query the student for missing  required 
parameters of commmands, such as the repair team 
who is to perform the action, or the number of  the 
pump to start on the firemain.  If the student does not 
respond to these queries, the system will provide the 
context of the command missing the parameter as 
part of a more informative request.  The student 
retains the ability to issue other commands at this 
time, and need not respond to the system if there is a 
more pressing crisis elsewhere. 
At the end of a DC-Train session, the 
student can then receive customized feedback and 
tutoring from SCoT-DC, based on a record of the 
student?s actions compared to what an expert DCA 
would have done at each point, based on  rules 
accounting for the state of the simulation. The goal of 
the tutorial interaction is to identify and remediate 
any gaps in the student?s understanding of damage 
control doctrine, and to improve the student?s 
performance in issuing the correct commands without 
hesitation.   
The graphical interface to the SCoT-DC 
tutor is shown in Figure 2. 
  
 
 
Figure 2: ScoT-DC tutor GUI 
 
SCoT-DC uses two instances of the Ship Display 
from DC-Train, seen in Figure 3, one to give an 
overall view of the ship and one to zoom in on 
affected compartments, with color indicating the type 
of crisis in a compartment and the state of damage 
control there.   The student can click on a 
compartment in the Ship Display as a way of 
indicating that compartment to the system. The 
automated tutor and the student communicate 
through speech, while the lower window displays the 
text of both sides of the interaction, and permits the 
user to scroll back through the entire tutorial session.   
 
Figure 3: Highlighted Compartment  
 
The tutor can also display bulkheads used to set 
boundaries for firefighting, as in Figure 4. 
 
Figure 4:  Highlighted Bulkhead Walls 
 
A third kind of graphical information that the 
tutor may convey to the student involves regions of 
jurisdiction for repair  teams, shown in Figure 5.
 
Figure 5: Repair Team Jurisdiction Regions 
As in DC-Train, the student clicks to begin 
speaking, then Nuance speech recognition provides a 
string of words to be interpreted by a Gemini 
grammar.  Also as in DC-Train, responses from the 
tutor are  synthesized by Festival, although the tutor 
speaks with a more natural voice provided by 
FestVox limited domain synthesis, in which large 
units of the tutor?s utterances may be taken from 
prompts recorded for this application.   
Interpretation of the Gemini interpreted 
forms is handled by a more complex dialogue 
manager in SCoT-DC than in DC-Train, with a 
structured representation of the dialogue, which is 
used to guide the system?s use of discourse markers, 
among other things.  The dialogue is mainly driven 
by the tutor agent?s strategies, though the student can 
request to move on to future topics without 
completing the current discussion, and also ask a 
?Why? question after some explanations. 
 Tutorial strategies generally guide the 
overall path of the conversation, such as choosing 
which crises to discuss based on the errors made by 
the student. Tutorial tactics apply at a lower-level 
throughout the dialogue, for example, when a student 
gives an incorrect answer, the tutor will give a 
general hint and repos the question.  If the student 
answers incorrectly a second time the tutor will give 
a more specific hint and ask the question again.  If 
the student fails a third time the tutor will give the 
correct answer, and proceed. 
 Running a  full DC-Train scenario takes 20-
40 minutes, and has the flavor of the following 
excerpt: 
 
 [buzzing alarm goes off, it is a fire alarm] 
 
DCCO: Fire in compartment 2-78-01-L. 
Student: Net80 to repair locker 2, investigate 
compartment 2-78-01-L. 
Repair Locker 2: Reports, fire in compartment 2-78-
01-L. 
Repair Locker 2: Reports, smoke in compartment 2-
78-01-L. 
Student: Net80 to repair locker 2, fight the fire in 
compartment 2-78-01-L. 
Student: Net80 to repair locker 2, set fire and smoke 
boundaries on primary forward 78, primary aft 126, 
secondary forward 42, secondary aft 174, above 1, 
below 2. 
 
A reflective dialogue with the tutor will takes 
around 10 minutes.  The following gives a sample of 
the kind of tutorial interaction.  
 
Tutor: Hello, we are about to review your session 
from earlier today. 
Tutor: All right, are you ready? 
Student: Yes. 
Tutor: In this session there were 5 primary crises and 
1 secondary crisis. 
Tutor: 5 of the 6 crises were resolved. 
Tutor: Most of your actions were correct, however 
you made several mistakes. 
Tutor: OK, let?s discuss a few of those crises.
Tutor: The first crisis was a fire here in Crew Living 
space No. 1. [centers on this compartment] 
Tutor: This fire was not contained early enough.
Tutor: As a result, the fire spread to the adjacent 
Combat sytem equipment room No. 2. [highlights 
2nd compartment] 
Tutor: OK, let?s review how to handle this crisis. 
[removes highlight from 2nd compartment] 
Tutor: What?s the first thing you should do when the 
fire alarm sounds? 
Student: Send repair two to investigate. 
Tutor: Yes, that?s right. 
Tutor: However, you failed to do this during the 
session. 
Tutor: This is a mistake you made more than once. 
Tutor: Remember, alarms need to be verified first. 
Tutor: OK, suppose that there is a fire. 
Tutor: What?s the next step? 
Student: Set fire and smoke boundaries. 
Tutor: Yes. That?s what you did during the session.
Tutor: OK, what else needs to be done? 
Student: The repair team should fight the fire. 
Tutor: No, you missed a step. 
Tutor: Remember, the crisis might interfere with 
other ship systems. 
Tutor: Let?s try again. 
Tutor: What else needs to be done?
Student: I don?t know. 
Tutor: Well. 
Tutor: You should order repair five to mechanically 
and electrically isolate the compartment. 
 
 
A video clip of an older version of the ScoT-DC 
system is available at http://www-
csli.stanford.edu/semlab/muri/November2002Demo.h
tml 
 
3 Acknowledgements 
 
This work is supported by the Department of the 
Navy under research grant N000140010660, a 
multidisciplinary university research initiative on 
natural language interaction with intelligent tutoring 
systems. 
 
3.1 References  
 
A. Black and K. Lenzo, 1999. Building Voices in the 
Festival Speech Synthesis System (DRAFT) 
Available at 
http://www.cstr.ed.ac.uk/projects/festival/papers.h
tml. 
A. Black and P. Taylor. 1997. Festival speech 
synthesis system: system documentation (1.1.1). 
Technical Report Technical Report HCRC/TR-
83, University of Edinburgh Human 
Communication Research Centre. 
V. V. Bulitko and D. C. Wilkins. 1999. Automated 
instructor assistant for ship damage control. In  
Proceedings of AAAI-99. 
J. Dowding, M. Gawron, D. Appelt, L. Cherny, R. 
Moore, and D. Moran. 1993. Gemini: A natural 
language system for spoken language 
understanding.  In Procdgs of ACL 31. 
Oliver Lemon, Alexander Gruenstein, and Stanley 
Peters. 2002. Collaborative Activities and Multi-
tasking in Dialogue Systems , Traitement 
Automatique des Langues (TAL), 43(2):131- 54, 
special issue on dialogue. 
D. Martin, A. Cheyer, and D. Moran. 1999. ``The 
open agent architecture: A framework for 
building distributed software systems,'' Applied 
Artificial Intelligence, v.13:91-128. 
Robert C. Moore. 1998. Using Natural Language 
Knowledge Sources in Speech Recognition." 
Proceedings of the NATO Advanced Study 
Institute. 
Karl Schultz, Elizabeth Owen Bratt, Brady Clark, 
Stanley Peters, Heather Pon-Barry, and Pucktada 
Treeratpituk. 2003. A Scalable, Reusable Spoken 
Conversational Tutor: SCoT. In AIED 2003 
Supplementary Procdgs. (V. Aleven et aleds). 
Univ. of Sydney. 367-377.  
 
 
Automated Tutoring Dialogues for Training in Shipboard
Damage Control
John Fry, Matt Ginzton, Stanley Peters, Brady Clark & Heather Pon-Barry
Stanford University
Center for the Study of Language Information
Stanford CA 94305-4115 USA
{fry,mginzton,peters,bzack,ponbarry}@csli.stanford.edu
Abstract
This paper describes an application
of state-of-the-art spoken language
technology (OAA/Gemini/Nuance)
to a new problem domain: engaging
students in automated tutorial dia-
logues in order to evaluate and im-
prove their performance in a train-
ing simulator.
1 Introduction
Shipboard damage control refers to the task of
containing the effects of fire, explosions, hull
breaches, flooding, and other critical events
that can occur aboard Naval vessels. The
high-stakes, high-stress nature of this task, to-
gether with limited opportunities for real-life
training, make damage control an ideal target
for AI-enabled educational technologies like
training simulators and tutoring systems.
This paper describes the spoken dialogue
system we developed for automated critiquing
of student performance on a damage control
training simulator. The simulator is DC-
Train (Bulitko and Wilkins, 1999), an im-
mersive, multimedia training environment for
damage control. DC-Train?s training sce-
narios simulate a mixture of physical phenom-
ena (e.g., fire, flooding) and personnel issues
(e.g., casualties, communications, standard-
ized procedures). Our current tutoring sys-
tem is restricted fire damage scenarios only,
and in particular to the twelve fire scenar-
ios available in DC-Train version 2.5, but
in future versions we plan to support post-
session critiques for all of the damage phe-
nomena that will be modeled by DC-Train
4.0: fire, flooding, missile damage, and wall
or firemain ruptures.
2 Previous Work
Eliciting self-explanation from a student has
been shown to be a highly effective tutoring
method (Chi et al, 1994). For this reason,
a number of automated tutoring systems cur-
rently use NLP techniques to engage students
in reflective dialogues. Three notable exam-
ples are the medical Circsim tutor (Zhou et
al., 1999); the Basic Electricity and Electron-
ics (BE&E) tutor (Rose? et al, 1999); and
the computer literacy AutoTutor (Wiemer-
Hastings et al, 1999).
Our system shares several features with
these three tutoring systems:
A knowledge base Our system encodes
all domain knowledge relevant to supporting
intelligent tutoring feedback into a structure
called an Expert Session Summary (Section
4). These expert summaries encode causal
relationships between events on the ship as
well as the proper and improper responses to
shipboard crises.
Tutoring strategies In our system, as in
those above, the flow of dialogue is controlled
by (essentially) a finite-state transition net-
work (Fig. 1).
An interpretation component In our
system, the student?s speech is recognized and
parsed into logical forms (Section 3). A dia-
logue manager inspects the current dialogue
information state to determine how best to
incorporate each new utterance into the dia-
logue (Lemon et al, 2001).
Prompt
student review
of actions
Correct
student?s
report Prompt for
reflection on
START
END
continue"
"OK, let?s
event N...
Summary
of damage
main points
Review
performance
student?s
Evaluate
reflections
Correct
student?s
"You handled
this one well"
event 1
of damage
Summary
Brief
summary of
session
errors
Figure 1: Post-session dialogue move graph (simplified)
However, an important difference is that
the three systems above are entirely text-
based, whereas ours is a spoken dialogue sys-
tem. Our speech interface offers greater natu-
ralness than keyboard-based input. In this re-
spect, our system is similar to cove (Roberts,
2000), a training simulator for conning Navy
ships that uses speech to interact with the
student. But whereas cove uses short conver-
sational exchanges to coach the student dur-
ing the simulation, our system engages in ex-
tended tutorial dialogues after the simulation
has ended. Besides being more natural, spo-
ken language systems are also better suited to
multimodal interactions (viz., one can point
and click while talking but not while typing).
An additional significant difference between
our system and a number of other automated
tutoring systems is our use of ?deep? process-
ing techniques. While other systems utilize
?shallow? statistical approaches like Latent Se-
mantic Analysis (e.g. AutoTutor), our system
utilizes Gemini, a symbolic grammar. This
approach enables us to provide precise and
reliable meaning representations.
3 Implementation
To facilitate the implementation of multi-
modal, mixed-initiative tutoring interactions,
we decided to implement our system within
the Open Agent Architecture (OAA) (Martin
et al, 1999). OAA is a framework for coor-
dinating multiple asynchronous communicat-
ing processes. The core of OAA is a ?facilita-
tor? which manages message passing between
a number of software agents that specialize
in certain tasks (e.g., speech recognition or
database queries). Our system uses OAA to
coordinate the following five agents:
1. The Gemini NLP system (Dowding et
al., 1993). Gemini uses a single unifi-
cation grammar both for parsing strings
of words into logical forms (LFs) and for
generating sentences from LF inputs.
2. A Nuance speech recognition server,
which converts spoken utterances to
strings of words. The Nuance server re-
lies on a language model, which is com-
piled directly from the Gemini grammar,
ensuring that every recognized utterance
is assigned an LF.
3. The Festival text-to-speech system,
which ?speaks? word strings generated by
Gemini.
4. A Dialogue Manager which coordi-
nates inputs from the user, interprets the
user?s dialogue moves, updates the dia-
logue context, and delivers speech and
graphical outputs to the user.
5. A Critique Planner, described below
in Section 4.
Agents 1-3 are reusable, ?off-the-shelf? dia-
logue system components (apart from the
Gemini grammar, which must be modified for
each application). We implemented agents 4
and 5 in Java specifically for this application.
Variants of this OAA/Gemini/Nuance ar-
chitecture have been deployed successfully in
other dialogue systems, notably SRI?s Com-
mandTalk (Stent et al, 1999) and an un-
Figure 2: Screen shot of post-session tutorial dialogue system
manned helicopter interface developed in our
laboratory (Lemon et al, 2001).
4 Planning the dialogue
Each student session with DC-Train pro-
duces a session transcript, i.e. a time-stamped
record of every event (both computer- and
student-initiated) that occurred during the
simulation. These transcripts serve as the
input to our post-session Critique Planner
(CP).
The CP plans a post-session tutorial di-
alogue in two steps. In the first step, an
Expert Session Summary (ESS) is cre-
ated from the session transcript. The ESS
is a tree whose parent nodes represent dam-
age events and whose leaves represent actions
taken in response to those damage events.
Each student-initiated action in the ESS is
evaluated as to its timeliness and conformance
to damage control doctrine. Actions that the
student should have taken but did not are also
inserted into the ESS and flagged as such.
Each action node in the ESS therefore falls
into one of three classes: (i) correct actions;
(ii) errors of commission (e.g., the student
sets fire containment boundaries incorrectly);
and (iii) errors of omission (e.g., the student
fails to secure permission from the captain be-
fore flooding certain compartments).
Our current tutoring system covers scenar-
ios generated by DC-Train 2.5, which covers
fire scenarios only. Future versions will use
scenarios generated by DC-Train 4.0, which
covers damage control scenarios involving fire,
smoke, flooding, pipe and hull ruptures, and
equipment deactivation. Our current tutor-
ing system is based on an ESS graph that is
generated by an expert model that consists
of an ad-hoc set of firefighting rules. Future
versions will be based on an ESS graph that
is generated by an successor to the Minerva-
DCA expert model (Bulitko and Wilkins,
1999), an extended Petri Net envisionment-
based reasoning system. The new expert
model is designed to produce an ESS graph
during the course of problem solving that con-
tains nodes for all successful and unsuccessful
plan and goal achievement events, along with
an explanation structure for each graph node.
The second step in planning the post-
session tutorial dialogue is to produce a di-
alogue move graph (Fig. 1). This is a di-
rected graph that encodes all possible configu-
rations of dialogue structure and content that
can be handled by the system.
Generating an appropriate dialogue move
graph from an ESS requires pedagogical
knowledge, and in particular a tutoring strat-
egy. The tutoring strategy we adopted is
based on our analysis of videotapes of fifteen
actual DC-Train post-session critiques con-
ducted by instructors at the Navy?s Surface
Warfare Officer?s School in Newport, RI. The
strategy we observed in these critiques, and
implemented in our system, can be outlined
as follows:
1. Summarize the results of the simulation
(e.g., the final condition of the ship).
2. For each major damage event in the ESS:
(a) Ask the student to review his ac-
tions, correcting his recollections as
necessary.
(b) Evaluate the correctness of each stu-
dent action.
(c) If the student committed errors,
ask him how these could have been
avoided, and evaluate the correct-
ness of his responses.
3. Finally, review each type of error that
arose in step (2c).
A screen shot of the tutoring system in
action is shown in Fig. 2. As soon as a
DC-Train simulation ends, the dialogue sys-
tem starts up and the dialogue manager be-
gins traversing the dialogue move graph. As
the dialogue unfolds, a graphical representa-
tion of the ESS is revealed to the student in
piecemeal fashion as depicted in the top right
frame of Fig. 2.
Acknowledgments
This work is supported by the Depart-
ment of the Navy under research grant
N000140010660, a multidisciplinary univer-
sity research initiative on natural language in-
teraction with intelligent tutoring systems.
References
V V. Bulitko and D C. Wilkins. 1999. Automated
instructor assistant for ship damage control. In
Proceedings of AAAI-99, Orlando, FL, July.
M. T. H. Chi, N. de Leeuw, M. Chiu, and C.
LaVancher. 1994. Eliciting self-explanations
improves understanding. Cognitive Science,
18(3):439?477.
J. Dowding, J. Gawron, D. Appelt, J. Bear, L.
Cherny, R. C. Moore, and D. Moran. 1993.
Gemini: A natural language system for spoken-
language understanding. In Proceedings of the
ARPA Workshop on Human Language Technol-
ogy.
O. Lemon, A. Bracy, A. Gruenstein, and S. Pe-
ters. 2001. A multi-modal dialogue system for
human-robot conversation. In Proceedings of
NAACL 2001.
D. Martin, A. Cheyer, and D. Moran. 1999.
The Open Agent Architecture: a framework for
building distributed software systems. Applied
Artificial Intelligence, 13(1-2).
B. Roberts. 2000. Coaching driving skills in
a shiphandling trainer. In Proceedings of the
AAAI Fall Symposium on Building Dialogue
Systems for Tutorial Applications.
C. P. Rose?, B. Di Eugenio, and J. D. Moore. 1999.
A dialogue based tutoring system for basic elec-
tricity and electronics. In S. P. Lajoie and
M. Vivet, editors, Artificial Intelligence in Ed-
ucation (Proceedings of AIED?99), pages 759?
761. IOS Press, Amsterdam.
A. Stent, J. Dowding, J. Gawron, E. O. Bratt, and
R. C. Moore. 1999. The CommandTalk spoken
dialogue system. In Proceedings of ACL ?99,
pages 183?190, College Park, MD.
P. Wiemer-Hastings, K. Wiemer-Hastings, and
A. Graesser. 1999. Improving an intelli-
gent tutor?s comprehension of students with la-
tent semantic analysis. In S. P. Lajoie and
M. Vivet, editors, Artificial Intelligence in Ed-
ucation (Proceedings of AIED?99), pages 535?
542. IOS Press, Amsterdam.
Y. Zhou, R. Freedman, M. Glass, J. A. Michael,
A. A. Rovick, and M. W. Evens. 1999. Deliver-
ing hints in a dialogue-based intelligent tutoring
system. In Proceedings of AAAI-99, Orlando,
FL, July.
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 104?111,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Semantic Density Analysis: 
Comparing word meaning across time and phonetic space 
 
Eyal Sagi 
Northwestern University 
Evanston, Illinois, USA 
eyal@u.northwestern.edu 
 
Stefan Kaufmann 
Northwestern University 
Evanston, Illinois, USA 
kaufmann@northwestern.edu 
Brady Clark 
Northwestern University 
Evanston, Illinois, USA 
bzack@northwestern.edu 
 
 
  
Abstract 
This paper presents a new statistical method 
for detecting and tracking changes in word 
meaning, based on Latent Semantic Analysis. 
By comparing the density of semantic vector 
clusters this method allows researchers to 
make statistical inferences on questions such 
as whether the meaning of a word changed 
across time or if a phonetic cluster is asso-
ciated with a specific meaning. Possible appli-
cations of this method are then illustrated in 
tracing the semantic change of ?dog?, ?do?, and 
?deer? in early English and examining and 
comparing phonaesthemes. 
1 Introduction 
The increase in available computing power over 
the last few decades has led to an explosion in 
the application of statistical methods to the anal-
ysis of texts. Researchers have applied these me-
thods to a wide range of tasks, from word-sense 
disambiguation (Levin et al, 2006) to the sum-
marization of texts (Marcu, 2003) and the auto-
matic scoring of student essays (Riedel et al, 
2006). However, some fields of linguistics that 
have traditionally employed corpora as their 
source material, such as historical semantics, 
have yet to benefit from the application of these 
statistical methods.  
In this paper we demonstrate how an existing 
statistical tool (Latent Semantic Analysis) can be 
adapted and used to automate and enhance some 
aspects of research in historical semantics and 
other fields whose focus is on the comparative 
analysis of word meanings within a corpus. Our 
method allows us to assess the semantic variation 
within the set of individual occurrences of a giv-
en word type. This variation is inversely related 
to a property of types that we call density ? intui-
tively, a tendency to occur in highly similar con-
texts. In terms of our LSA-based spatial semantic 
model, we calculate vectors representing the con-
text of each occurrence of a given term, and es-
timate the term?s cohesiveness as the density 
with which these token context vectors are 
?packed? in space. 
2 The method 
Latent Semantic Analysis (LSA) is a collective 
term for a family of related methods, all of which 
involve building numerical representations of 
words based on occurrence patterns in a training 
corpus. The basic underlying assumption is that 
co-occurrence within the same contexts can be 
used as a stand-in measure of semantic related-
ness (see Firth, 1957; Halliday and Hasan, 1976; 
Hoey, 1991, for early articulations of this idea). 
The success of the method in technical applica-
tions such as information retrieval and its popu-
larity as a research tool in psychology, education, 
linguistics and other disciplines suggest that this 
hypothesis holds up well for the purposes of 
those applications. 
The relevant notion of ?context? varies. The 
first and still widely used implementation of the 
idea, developed in Information Retrieval and 
originally known as Latent Semantic Indexing 
(Deerwester et al, 1990), assembles a term-
document matrix in which each vocabulary item 
(term) is associated with an n-dimensional vector 
recording its distribution over the n documents in 
the corpus. In contrast, the version we applied in 
this work measures co-occurrence in a way that 
is more independent of the characteristics of the 
documents in the training corpus, building in-
104
stead a term-term matrix associating vocabulary 
items with vectors representing their frequency 
of co-occurrence with each of a list of ?content-
bearing? words. This approach originated with 
the ?WordSpace? paradigm developed by 
Sch?tze (1996). The software we used is a ver-
sion of the ?Infomap? package developed at 
Stanford University and freely available (see also 
Takayama et al, 1999). We describe it and the 
steps we took in our experiments in some detail 
below. 
2.1 Word vectors 
The information encoded in the co-occurrence 
matrix, and thus ultimately the similarity meas-
ure depends greatly on the genre and subject 
matter of the training corpus (Takayama et al, 
1999; Kaufmann, 2000). In our case, we used the 
entire available corpus as our training corpus. 
The word types in the training corpus are ranked 
by frequency of occurrence, and the Infomap 
system automatically selects (i) a vocabulary ?  
for which vector representations are to be col-
lected, and (ii) a set ? of 1,000 ?content-bearing? 
words whose occurrence or non-occurrence is 
taken to be indicative of the subject matter of a 
given passage of text. Usually, these choices are 
guided by a stoplist of (mostly closed-class) lexi-
cal items that are to be excluded, but because we 
were interested in tracing changes in the meaning 
of lexical items we reduced this stoplist to a bare 
minimum. To compensate, we increased the 
number of ?content-bearing? words to 2,000. The 
vocabulary ? consisted of the 40,000 most fre-
quent non-stoplist words. The set ? of content-
bearing words contained the 50th through 2,049th 
most frequent non-stoplist words. This method 
may seem rather blunt, but it has the advantage 
of not requiring any human intervention or ante-
cedently given information about the domain. 
The cells in the resulting matrix of 40,000 
rows and 2,000 columns were filled with co-
occurrence counts recording, for each 
pair  ?, ? ? ? ? ?, the number of times a token 
of ? occurred in the context of a token of ? in 
the corpus.1 The ?context? of a token ??  in our 
                                                 
1 Two details are glossed over here: First, the Infomap sys-
tem weighs this raw count with a ??. ??? measure of the 
column label c, calculated as follows: ??. ??? ? = ?? ? ?
 ??? ? + 1 ? ??? ?? ?    where ?? and ?? are the number 
of occurrences of ? and the number of documents in which 
? occurs, respectively, and ? is the total number of docu-
ments. Second, the number in each cell is replaced with its 
square root, in order to approximate a normal distribution of 
counts and attenuate the potentially distorting influence of 
implementation is the set of tokens in a fixed-
width window from the 15th item preceding ??  
to the 15th item following it (less if a document 
boundary intervenes). The matrix was trans-
formed by Singular Value Decomposition 
(SVD), whose implementation in the Infomap 
system relies on the SVDPACKC package 
(Berry, 1992; Berry et al, 1993). The output was 
a reduced 40,000 ? 100 matrix. Thus each item 
? ? ? is associated with a 100-dimensional 
vector ?   . 
2.2 Context vectors 
Once the vector space is obtained from the 
training corpus, vectors can be calculated for any 
multi-word unit of text (e.g. paragraphs, queries, 
or documents), regardless of whether it occurs in 
the original training corpus or not, as the normal-
ized sum of the vectors associated with the words 
it contains. In this way, for each occurrence of a 
target word type under investigation, we calcu-
lated a context vector from the 15 words preced-
ing and the 15 words following that occurrence. 
Context vectors were first used in Word Sense 
Discrimination by Sch?tze (1998). Similarly to 
that application, we assume that these ?second-
order? vectors encode the aggregate meaning, or 
topic, of the segment they represent, and thus, 
following the reasoning behind LSA, are 
indicative of the meaning with which it is being 
used on that particular occurrence. Consequently, 
for each target word of interest, the context 
vectors associated with its occurrences constitute 
the data points. The analysis is then a matter of 
grouping these data points according to some 
criterion (e.g., the period in which the text was 
written) and conducting an appropriate statistical 
test. In some cases it might also be possible to 
use regression or apply a clustering analysis. 
2.3 Semantic Density Analysis 
Conducting statistical tests comparing groups of 
vectors is not trivial. Fortunately, some questions 
can be answered based on the similarity of vec-
tors within each group rather than the vectors 
themselves. The similarity between two vectors 
?   , ?  is measured as the cosine between them:2 
                                                                          
high base frequencies (cf. Takayama, et al 1998; Widdows, 
2004). 
2 While the cosine measure is the accepted measure of simi-
larity, the cosine function is non-linear and therefore prob-
lematic for many statistical methods. Several transforma-
tions can be used to correct this (e.g., Fisher?s z). In this 
paper we will use the angle, in degrees, between the two 
vectors (i.e., ????1) because it is easily interpretable. 
105
??? ?   , ?  =
?   ? ? 
 ?     ?  
 
 
The average similarity of a group of vectors is 
indicative of its density ? a dense group of highly 
similar vectors will have a high average cosine 
(and a correspondingly low average angle) 
whereas a sparse group of dissimilar vectors will 
have an average cosine that approaches zero (and 
a correspondingly high average angle).3 Thus 
since a word that has a single, highly restricted 
meaning (e.g. ?palindrome?) is likely to occur in 
a very restricted set of contexts, its context vec-
tors are also likely to have a low average angle 
between them, compared to a word that is highly 
polysemous or appears in a large variety of con-
texts (e.g. ?bank?, ?do?). From this observation, it 
follows that it should be possible to compare the 
cohesiveness of groups of vectors in terms of the 
average pairwise similarity of the vectors of 
which they are comprised. Because the number 
of such pairings tends to be prohibitively large 
(e.g., nearly 1,000,000 for a group of 1,000 vec-
tors), it is useful to use only a sub-sample in any 
single analysis. A Monte-Carlo analysis in which 
n pair-wise similarity values are chosen at ran-
dom from each group of vectors is therefore ap-
propriate.4 
However, there is one final complication to 
consider in the analysis. The passage of time in-
fluences not only the meaning of words, but also 
styles and variety of writing. For example, texts 
in the 11th century were much less varied, on av-
erage, than those written in the 15th century.5 
This will influence the calculation of context 
vectors as those depend, in part, on the text they 
are taken from. Because the document as a whole 
is represented by a vector that is the average of 
all of its words, it is possible to predict that, if no 
other factors exist, two contexts are likely to be 
related to one another to the same degree that 
their documents are. Controlling for this effect 
can therefore be achieved by subtracting from 
                                                 
3
 Since the cosine ranges from -1 to +1, it is possible in 
principle to obtain negative average cosines. In practice, 
however, the overwhelming majority of vocabulary items 
have a non-negative cosine with any given target word, 
hence the average cosine usually does not fall below zero. 
4
 It is important to note that the number of independent 
samples in the analysis is determined not by the number of 
similarity values compared but by the number of individual 
vectors used in the analysis. 
5 Tracking changes in the distribution of the document 
vectors in a corpus over time might itself be of interest to 
some researchers but is beyond the scope of the current 
paper. 
the angle between two context vectors the angle 
between the documents in which they appear.  
3 Applications to Research 
3.1 A Diachronic Investigation: Semantic 
Change 
One of the central questions of historical seman-
tics is the following (Traugott, 1999):6 
 
Given the form-meaning pair ? (lexeme) what 
changes did meaning ? undergo? 
 
For example, the form as long as underwent 
the change `equal in length? > `equal in time? > 
`provided that?. Evidence for semantic change 
comes from written records, cognates, and struc-
tural analysis (Bloomfield, 1933).  Traditional 
categories of semantic change include (Traugott, 
2005: 2-4; Campbell, 2004:254-262; Forston, 
2003: 648-650): 
? Broadening (generalization, extension, 
borrowing): A restricted meaning becomes less 
restricted (e.g. Late Old English docga `a (spe-
cific) powerful breed of dog? > dog `any member 
of the species Canis familiaris? 
? Narrowing (specialization, restriction): A 
relatively general meaning becomes more specif-
ic (e.g. Old English deor `animal? > deer) 
? Pejoration (degeneration): A meaning be-
comes more negative (e.g. Old English s?lig 
`blessed, blissful? > sely `happy, innocent, pitia-
ble? > silly `foolish, stupid?) 
 
Semantic change results from the use of lan-
guage in context, whether linguistic or extralin-
guistic. Later meanings of forms are connected to 
earlier ones, where all semantic change arises by 
polysemy, i.e. new meanings coexist with earlier 
ones, typically in restricted contexts. Sometimes 
new meanings split off from earlier ones and are 
no longer considered variants by language users 
(e.g. mistress `woman in a position of authority, 
head of household? > `woman in a continuing 
extra-marital relationship with a man?). 
Semantic change is often considered unsyste-
matic (Hock and Joseph, 1996: 252). However, 
recent work (Traugott and Dasher, 2002) sug-
gests that there is, in fact, significant cross-
linguistic regularity in semantic change. For ex-
                                                 
6 This is the semasiological perspective on semantic change. 
Other perspectives include the onomasiological perspective 
(?Given the concept ?, what lexemes can it be expressed 
by??). See Traugott 1999 for discussion. 
106
ample, in the Invited Inferencing Model of Se-
mantic Change proposed by Traugott and Dasher 
(2002) the main mechanism of semantic change 
is argued to be the semanticization of conversa-
tional implicatures, where conversational impli-
catures are a component of speaker meaning that 
arises from the interaction between what the 
speaker says and rational principles of communi-
cation (Grice, 1989 [1975]). Conversational im-
plicatures are suggested by an utterance but not 
entailed. For example, the utterance Some stu-
dents came to the party strongly suggests that 
some but not all students came to the party, even 
though the utterance would be true strictly speak-
ing if all students came to the party. According to 
the Invited Inferencing Model, conversational 
implicatures become part of the semantic poly-
semies of particular forms over time. 
Such changes in meaning should be evident 
when examining the contexts in which the lex-
eme of interest appears. In other words, changes 
in the meaning of a type should translate to dif-
ferences in the contexts in which its tokens are 
used. For instance, semantic broadening results 
in a meaning that is less restricted and as a result 
can be used in a larger variety of contexts. In a 
semantic space that encompasses the period of 
such a change, this increase in variety can be 
measured as a decrease in vector density across 
the time span of the corpus. This decrease trans-
lates into an increase in the average angle be-
tween the context vectors for the word. For in-
stance, because the Old English word ?docga? 
applied to a specific breed of dog, we predicted 
that earlier occurrences of the lexemes ?docga? 
and ?dog?, in a corpus of documents of the ap-
propriate time period, will show less variety than 
later occurrences. 
An even more extreme case of semantic broa-
dening is predicted to occur as part of the process 
of grammaticalization (Traugot and Dasher, 
2002) in which a content word becomes a func-
tion word. Because, as a general rule, a function 
word can be used in a much larger variety of 
contexts than a content word, a word that under-
went grammaticalization should appear in a sub-
stantially larger variety of contexts than it did 
prior to becoming a function word. One well stu-
died case of grammaticalization is that of periph-
rastic ?do?. While in Old English ?do? was used 
as a verb with a causative and habitual sense 
(e.g. ?do you harm?), later in English it took on a 
functional role that is nearly devoid of meaning 
(e.g. ?do you know him??). Because this change 
occurred in Middle English, we predicted that 
earlier occurrences of ?do? will show less variety 
than later ones. 
In contrast with broadening, semantic narrow-
ing results in a meaning that is more restricted, 
and is therefore applicable in fewer contexts than 
before. This decrease in variety results in an in-
crease in vector density and can be directly 
measured as a decrease in the average angle be-
tween the context vectors for the word. As an 
example, the Old English word ?deor? denoted a 
larger group of living creatures than does the 
Modern English word ?deer?. We therefore pre-
dicted that earlier occurrences of the lexemes 
?deor? and ?deer?, in a corpus of the appropriate 
time period, will show more variety than later 
occurrences. 
We tested our predictions using a corpus de-
rived from the Helsinki corpus (Rissanen, 1994). 
The Helsinki corpus is comprised of texts span-
ning the periods of Old English (prior to 
1150A.D.), Middle English (1150-1500A.D.), 
and Early Modern English (1500-1710A.D.). 
Because spelling in Old English was highly vari-
able, we decided to exclude that part of the cor-
pus and focused our analysis on the Middle Eng-
lish and Early Modern English periods. The re-
sulting corpus included 504 distinct documents 
totaling approximately 1.1 million words. 
To test our predictions regarding semantic 
change in the words ?dog?, ?do?, and ?deer?, we 
collected all of the contexts in which they appear 
in our subset of the Helsinki corpus. This re-
sulted in 112 contexts for ?dog?, 4298 contexts 
for ?do?, and 61 contexts for ?deer?. Because 
there were relatively few occurrences of ?dog? 
Table 1 - Mean angle between context vectors for target words in different periods in the Helsinki 
corpus (standard deviations are given in parenthesis) 
 
n 
Unknown composi-
tion date 
(<1250) 
Early Middle 
English 
(1150-1350) 
Late Middle 
English 
(1350-1500) 
Early Modern 
English 
(1500-1710) 
dog 112   15.47 (14.19) 24.73(10.43) 
do 4298  10.31(13.57) 13.02 (9.50) 24.54 (11.2) 
deer 61 38.72 (17.59) 20.6 (18.18)  20.5 (9.82) 
science 79   13.56 (13.33) 28.31 (12.24) 
 
107
and ?deer? in the corpus it was practical to com-
pute the angles between all possible pairs of con-
text vectors. As a result, we elected to forgo the 
Monte-Carlo analysis for those two words in fa-
vor of a full analysis. The results of our analysis 
for all three words are given in Table 1. These 
results were congruent with our prediction: The 
density of the contexts decreases over time for 
both ?dog? (t(110) = 2.17, p < .05) and ?do? 
(F(2,2997)=409.41, p < .01) while in the case of 
?deer? there is an increase in the density of the 
contexts over time (t(36) = 3.05, p < .01). 
Furthermore, our analysis corresponds with 
the data collected by Elleg?rd (1953). Elleg?rd 
traced the grammaticalization of ?do? by manual-
ly examining changes in the proportions of its 
various uses between 1400 and 1700. His data 
identifies an overall shift in the pattern of use 
that occurred mainly between 1475 and 1575. 
Our analysis identifies a similar shift in patterns 
between the time periods spanning 1350-1500 
and 1500-1570. Figure 1 depicts an overlay of 
both datasets. The relative scale of the two sets 
was set so that the proportions of ?do? uses at 
1400 and 1700 (the beginning and end of El-
leg?rd?s data, respectively) match the semantic 
density measured by our method at those times. 
Finally, our method can be used not only to 
test predictions based on established cases of 
semantic change, but also to identify new ones. 
For instance, in examining the contexts of the 
word ?science? we can identify that it underwent 
semantic broadening shortly after it first ap-
peared in the 14th century (t(77) = 4.51, p < .01). 
A subsequent examination of the contexts in 
which the word appears indicated that this is 
probably the result of a shift from a meaning re-
lated to generalized knowledge (e.g., ??and 
learn science of school?, John of Trevisa's Polyc-
hronicon, 1387) to one that can also be used to 
refer to more specific disciplines (e.g., ??of the 
seven liberal sciences?, Simon Forman?s Diary, 
1602). 
Our long term goal with respect to this type of 
analysis is to use this method in a computer-
based tool that can scan a diachronic corpus and 
automatically identify probable cases of semantic 
change within it. Researchers can then use these 
results to focus on identifying the specifics of 
such changes, as well as examine the overall pat-
terns of change that exist in the corpus. It is our 
belief that such a use will enable a more rigorous 
testing and refinement of existing theories of se-
mantic change. 
3.2 A Synchronic Investigation: Phonaes-
themes 
In addition to examining changes in meaning 
across time, it is also possible to employ our me-
thod to examine how the semantic space relates 
to other possible partitioning of the lexemes 
represented by it. For instance, while the rela-
tionship between the phonetic representation and 
semantic content is largely considered to be arbi-
trary, there are some notable exceptions. One 
interesting case is that of phonaesthemes (Firth, 
1930), sub-morphemic units that have a predict-
able effect on the meaning of the word as a 
whole. In English, one of the more frequently 
mentioned phonaesthemes is a word-initial gl- 
which is common in words related to the visual 
modality (e.g., ?glance?, ?gleam?). While there 
have been some scholastic explorations of these 
non-morphological relationships between sound 
and meaning, they have not been thoroughly ex-
plored by behavioral and computational research 
(with some notable exceptions; e.g. Hutchins, 
1998; Bergen, 2004). Recently, Otis and Sagi 
(2008) used the semantic density of the cluster of 
words sharing a phonaestheme as a measure of 
10
30
50
70
10
20
30
1200 1300 1400 1500 1600 1700
%
 o
f 
p
e
ri
p
h
ra
st
ic
 ?d
o
? u
se
s
(E
lle
g?
rd
, 1
9
5
3
)
M
e
an
 A
n
gl
e
 b
e
tw
e
e
n
 v
e
ct
o
rs
(c
u
rr
e
n
t 
st
u
d
y)
Year
Current Study Ellegard's data
Figure 1 ? A comparison of the rise of periphrastic 'do' as measured by semantic density in our study and 
the proportion of periphrastic 'do' uses by Elleg?rd (1953). 
 
108
the strength of the relationship between the pho-
netic cluster and its proposed meaning.  
Otis and Sagi used a corpus derived from 
Project Gutenberg (http://www.gutenberg.org/) 
as the basis for their analysis. Specifically, they 
used the bulk of the English language literary 
works available through the project?s website. 
This resulted in a corpus of 4034 separate docu-
ments consisting of over 290 million words.  
The bulk of the candidate phonaesthemes they 
tested were taken from the list used by Hutchins 
(1998), with the addition of two candidate pho-
naesthemes (kn- and -ign). Two letter combina-
tions that were considered unlikely to be pho-
naesthemes (br- and z-) were also included in 
order to test the method?s capacity for discrimi-
nating between phonaesthemes and non-
phonaesthemes. Overall Otis and Sagi (2008) 
examined 47 possible phonaesthemes. 
In cases where a phonetic cluster represents a 
phonaestheme, it intuitively follows that pairs of 
words sharing that phonetic cluster are more 
likely to share some aspect of their meaning than 
pairs of words chosen at random. Otis and Sagi 
tested whether this was true for any specific can-
didate phonaestheme using a Monte-Carlo analy-
sis. First they identified all of the words in the 
corpus sharing a conjectured phonaestheme7 and 
chose the most frequent representative word 
form for each stem, resulting in a cluster of word 
types representing each candidate phonaes-
theme.8 Next they tested the statistical signific-
ance of this relationship by running 100 t-test 
comparisons. Each of these tests compared the 
relationship of 50 pairs of words chosen at ran-
dom from the conjectured cluster with 50 pairs of 
words chosen at random from a similarly sized 
cluster, randomly generated from the entire cor-
pus. The number of times these t-tests resulted in 
a statistically significant difference (? = .05) was 
recorded. This analysis was repeated 3 times for 
each conjectured phonaestheme and the median 
value was used as the final result. 
To determine whether a conjectured phonaes-
theme was statistically supported by their analy-
sis Otis and Sagi compared the overall frequency 
                                                 
7 It is important to note that due to the nature of a written 
corpus, the match was orthographical rather than phonetic. 
However, in most cases the two are highly congruent. 
8 Because, in this case, Otis and Sagi were not interested in 
temporal changes in meaning, they used the overall word 
vectors rather than look at each context individually. As a 
result, each of the vectors used in the analysis is based on 
occurrences in many different documents and there was no 
need to control for the variability of the documents.  
of statistically significant t-tests with the binomi-
al distribution for their ? (.05). After applying a 
Bonferroni correction for performing 50 compar-
isons, the threshold for statistical significance of 
the binomial test was for 14 t-tests out of 100 to 
turn out as significant, with a frequency of 13 
being marginally significant. Therefore, if the 
significance frequency (#Sig below) of a candi-
date phonaestheme was 15 or higher, that pho-
naestheme was judged as being supported by 
statistical evidence. Significance frequencies of 
13 and 14 were considered as indicative of a 
phonaestheme for which there was only marginal 
statistical support. 
Among Hutchins? original list of 44 possible 
phonaesthemes, 26 were found to be statistically 
reliable and 2 were marginally reliable. Overall 
the results were in line with the empirical data 
collected by Hutchins. By way of comparing the 
two datasets, #Sig and Hutchins? average rating 
measure were well correlated (r = .53). Neither 
of the unlikely phonaestheme candidates we ex-
amined were statistically supported phonaes-
themes (#Sigbr- = 6; #Sigz- = 5), whereas both of 
our newly hypothesized phonaesthemes were 
statistically supported (#Sigkn- = 28; #Sig-ign = 
23). In addition to being able to use this measure 
as a decision criterion as to whether a specific 
phonetic cluster might be phonaesthemic, it can 
also be used to compare the relative strength of 
two such clusters. For instance, in the Gutenberg 
corpus the phonaesthemic ending ?owl (e.g., 
?growl?, ?howl?; #Sig=97) was comprised of a 
cluster of words that were more similar to one 
another than ?oop (e.g., ?hoop?, ?loop?; #Sig=32).  
Such results can then be used to test the cogni-
tive effects of phonaesthemes. For instance, fol-
lowing the comparison above, we might hypo-
thesize that the word ?growl? might be a better 
semantic prime for ?howl? than the word ?hoop? 
is for the word ?loop?. In contrast, because a 
word-initial br- is not phonaesthemic, the word 
?breeze? is unlikely to be a semantic prime for 
the word ?brick?. In addition, it might be interest-
ing to combine the diachronic analysis from the 
previous section with the synchronic analysis in 
this section to investigate questions such as when 
and how phonaesthemes come to be part of a 
language and what factors might affect the 
strength of a phonaestheme. 
4 Discussion 
While the method presented in this paper is 
aimed towards quantifying semantic relation-
109
ships that were previously difficult to quantify, it 
also raises an interesting theoretical issue, name-
ly the relationship between the statistically com-
puted semantic space and the actual semantic 
content of words. On the one hand, simulations 
based on Latent Semantic Analysis have been 
shown to correlate with cognitive factors such as 
the acquisition of vocabulary and the categoriza-
tion of texts (cf. Landauer & Dumais, 1997). On 
the other hand, in reality speakers? use of lan-
guage relies on more than simple patterns of 
word co-occurrence ? For instance, we use syn-
tactic structures and pragmatic reasoning to sup-
plement the meaning of the individual lexemes 
we come across (e.g., Fodor, 1995; Grice, 1989 
[1975]). It is therefore likely that while LSA cap-
tures some of the variability in meaning exhi-
bited by words in context, it does not capture all 
of it. Indeed, there is a growing body of methods 
that propose to integrate these two disparate 
sources of linguistic information (e.g., Pado and 
Lapata, 2007; Widdows, 2003) 
Certainly, the results reported in this paper 
suggest that enough of the meaning of words and 
contexts is captured to allow interesting infe-
rences about semantic change and the relatedness 
of words to be drawn with a reasonable degree of 
certainty. However, it is possible that some im-
portant aspects of meaning are systematically 
ignored by the analysis. For instance, it remains 
to be seen whether this method can distinguish 
between processes like pejoration and amerliora-
tion as they require a fine grained distinction be-
tween ?good? and ?bad? meanings. 
Regardless of any such limitations, it is clear 
that important information about meaning can be 
gathered through a systematic analysis of the 
contexts in which words appear. Furthermore, 
phenomena such as the existence of phonaes-
themes and the success of LSA in predicting vo-
cabulary acquisition rates, suggest that the acqui-
sition of new vocabulary involves the gleaning of 
the meaning of words through their context. The 
role of context in semantic change is therefore 
likely to be an active one ? when a listener en-
counters a word they are unfamiliar with they are 
likely to use the context in which it appears, as 
well as its phonetic composition, as clues to its 
meaning. Furthermore, if a word is likewise en-
countered in context in which it is unlikely, this 
unexpected observation may induce the listener 
to adjust their representation of both the context 
and the word in order to increase the overall co-
herence of the utterance or sentence. As a result, 
it is possible that examining the contexts in 
which a word is used in different documents and 
time periods might be useful not only as a tool 
for examining the history of a semantic change 
but also as an instrument for predicting its future 
progress. Overall, this suggests a dynamic view 
of the field of semantics ? semantics as an ever-
changing landscape of meaning. In such a view, 
semantic change is the norm as the perceived 
meaning of words keeps shifting to accommo-
date the contexts in which they are used. 
References  
Bergen, B. (2004). The Psychological Reality of 
Phonaesthemes.  Language, 80(2), 291-311. 
Berry, M. W. (1992) SVDPACK: A Fortran-77 
software library for the sparse singular value 
decomposition. Tech. Rep. CS-92-159, Knox-
ville, TN: University of Tennessee. 
Berry, M. W., Do, T., O?Brien, G. Vijay, K. Va-
radh an, S. (1993) SVDPACKC (Version 1.0) 
User?s Guide, Tech. Rep. UT-CS-93-194, 
Knoxville, TN: University of Tennessee. 
Bloomfield, L. (1933). Language. New York, 
NY: Holt, Rinehart and Winston.  
Campbell, L. (2004) Historical linguistics: An 
introduction 2nd ed. Cambridge, MA: The MIT 
Press.  
Deerwester, S., Dumais, S. T., Furnas, G. W., 
Landauer, T. K., and Harshman, R. (1990) In-
dexing by Latent Semantic Analysis. Journal 
of the American Society for Information 
Science, 41, 391-407. 
Elleg?rd, A. (1953) The Auxiliary Do: the Estab-
lishment and Regulation of its Use in English. 
Gothenburg Studies in English, 2. Stockholm: 
Almqvist and Wiksell. 
Firth, J. (1930) Speech. London: Oxford Univer-
sity Press. 
Firth, J. (1957) Papers in Linguistics, 1934-1951, 
Oxford University Press. 
Fodor, J. D. (1995) Comprehending sentence 
structure. In L. R. Gleitman and M. Liberman, 
(Eds.), Invitation to Cognitive Science, volume 
1. MIT Press, Cambridge, MA. 209-246. 
Forston, B. W. (2003) An Approach to Semantic 
Change. In B. D. Joseph and R. D. Janda 
(Eds.), The Handbook of Historical Linguis-
tics. Malden, MA: Blackwell Publishing. 648-
666. 
110
Grice, H. P. (1989) [1975]. Logic and Conversa-
tion. In Studies in the Way of Words. Cam-
bridge, MA: Harvard University Press. 22-40. 
Halliday, M. A. K., & Hasan, R. (1976) Cohe-
sion in English. London: Longman. 
Hock, H. H., and Joseph, B. D. (1996) Language 
History, Language Change, and Language Re-
lationship: An Introduction to Historical and 
Comparative Linguistics. Berlin: Mouton de 
Gruyter. 
Hoey, M. (1991) Patterns of Lexis in Text. Lon-
don: Oxford University Press. 
Hutchins, S. S. (1998). The psychological reality, 
variability, and compositionality of English 
phonesthemes. Dissertation Abstracts Interna-
tional, 59(08), 4500B. (University Microfilms 
No. AAT 9901857). 
Infomap [Computer Software]. (2007). 
http://infomap-nlp.sourceforge.net/ Stanford, 
CA. 
Kaufmann, S. (2000) Second-order cohesion. 
Computational Intelligence. 16, 511-524. 
Landauer, T. K., & Dumais, S. T. (1997). A solu-
tion to Plato's problem: The Latent Semantic 
Analysis theory of the acquisition, induction, 
and representation of knowledge. Psychologi-
cal Review, 104, 211-240. 
Levin, E., Sharifi, M., & Ball, J. (2006) Evalua-
tion of utility of LSA for word sense discrimi-
nation. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, 
Companion Volume: Short Papers, New York 
City. 77-80.  
Marcu, D (2003) Automatic Abstracting, Encyc-
lopedia of Library and Information Science, 
Drake, M. A., ed. 245-256. 
Otis K., & Sagi E. (2008) Phonaesthemes: A 
Corpora-based Analysis. In B. C. Love, K. 
McRae, & V. M. Sloutsky (Eds.), Proceedings 
of the 30th Annual Conference of the Cognitive 
Science Society. Austin, TX: Cognitive 
Science Society. 
Pado, S. & Lapata, M. (2007) Dependency-based 
Construction of Semantic Space Models. 
Computational Linguistics, 33, 161-199. 
Riedel E., Dexter S. L., Scharber C., Doering A. 
(2006) Experimental Evidence on the Effec-
tiveness of Automated Essay Scoring in 
Teacher Education Cases. Journal of Educa-
tional Computing Research, 35, 267-287. 
Rissanen, M. (1994) The Helsinki Corpus of 
English Texts. In Kyt?, M., Rissanen, M. and 
Wright S. (eds), Corpora Across the Centu-
ries: Proceedings of the First International 
Colloquium on English Diachronic Corpora. 
Amsterdam: Rodopi. 
Sch?tze, H. (1996) Ambiguity in language learn-
ing: computational and cognitive models. CA: 
Stanford. 
Sch?tze, H. (1998) Automatic word sense dis-
crimination. Computational Linguistics 
24(1):97-124. 
Takayama, Y., Flournoy R., & Kaufmann, S. 
(1998) Information Mapping: Concept-Based 
Information Retrieval Based on Word 
Associations. CSLI Tech Report. CA: 
Stanford. 
Takayama, Y., Flournoy, R., Kaufmann, S. & 
Peters, S. (1999). Information retrieval based 
on domain-specific word associations. In Cer-
cone, N. and Naruedomkul K. (eds.), Proceed-
ings of the Pacific Association for Computa-
tional Linguistics (PACLING?99), Waterloo, 
Canada. 155-161. 
Traugott, E. C. (1999) The Role of Pragmatics in 
Semantic Change. In J. Verschueren (ed.), 
Pragmatics in 1998:  Selected Papers from the 
6th International Pragmatics Conference, vol. 
II. Antwerp: International Pragmatics Associa-
tion. 93-102. 
Traugott, E. C. (2005) Semantic Change. In En-
cyclopedia of Language and Linguistics, 2nd 
ed., Brown K. ed. Oxford: Elsevier. 
Traugott, E. C., and Dasher R. B. (2002) Regu-
larity in Semantic Change. Cambridge: Cam-
bridge University Press. 
Widdows, D. (2003) Unsupervised methods for 
developing taxonomies by combining syntactic 
and statistical information. In Proceedings of 
the joint Human Language Technology Confe-
rence and Annual Meeting of the North Ameri-
can Chapter of the Association for Computa-
tional Linguistics. Edmonton, Canada: 
Wiemer-Hastings. 197-204. 
Widdows, D. (2004) Geometry and Meaning. 
CSLI Publications, CA: Stanford. 
 
111
