Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 61?66,
Sydney, July 2006. c?2006 Association for Computational Linguistics
On2L - A Framework for Incremental Ontology Learning in Spoken
Dialog Systems
Berenike Loos
European Media Laboratory GmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
berenike.loos@eml-d.villa-bosch.de
Abstract
An open-domain spoken dialog system has
to deal with the challenge of lacking lexi-
cal as well as conceptual knowledge. As
the real world is constantly changing, it is
not possible to store all necessary knowl-
edge beforehand. Therefore, this knowl-
edge has to be acquired during the run
time of the system, with the help of the
out-of-vocabulary information of a speech
recognizer. As every word can have var-
ious meanings depending on the context
in which it is uttered, additional context
information is taken into account, when
searching for the meaning of such a word.
In this paper, I will present the incremental
ontology learning framework On2L. The
defined tasks for the framework are: the
hypernym extraction from Internet texts
for unknown terms delivered by the speech
recognizer; the mapping of those and their
hypernyms into ontological concepts and
instances; and the following integration of
them into the system?s ontology.
1 Introduction
A computer system, which has to understand and
generate natural language, needs knowledge about
the real world. As the manual modeling and main-
tenance of those knowledge structures, i.e. ontolo-
gies, are both time and cost consuming, there ex-
ists a demand to build and populate them automat-
ically or at least semi automatically. This is possi-
ble by analyzing unstructured, semi-structured or
fully structured data by various linguistic as well
as statistical means and by converting the results
into an ontological form.
In an open-domain spoken dialog system the au-
tomatic learning of ontological concepts and cor-
responding relations between them is essential,
as a complete manual modeling of them is nei-
ther practicable nor feasible as the real world and
its objects, models and processes are constantly
changing and so are their denotations.
This work assumes that a viable approach to
this challenging problem is to learn ontological
concepts and relations relevant for a certain user
- and only those - incrementally, i.e. at the time
of the user?s inquiry. Hypernyms1 of terms that
are not part of the speech recognizer lexicon, i.e.
out-of-vocabulary (OOV) terms, and hence lack-
ing any mapping to the employed knowledge rep-
resentation of the language understanding compo-
nent, should be found in texts from the Internet.
That is the starting point of the proposed ontol-
ogy learning framework On2L (On-line Ontology
Learning). With the found hypernym On2L can
assign the place in the system?s ontology to add
the unknown term.
So far the work described herein refers to the
German language only. In a later step, the goal is
to optimize it for English as well.
2 Natural Language and Ontology
Learning
Before describing the actual ontology learning
process it is important to make a clear distinction
between the two fields involved: this is on the one
hand natural language and on the other hand onto-
logical knowledge.
As the Internet is a vast resource of up-to-date
1According to Lyons (1977) hyponymy is the relation
which holds between a more specific lexeme (i.e. a hyponym)
and a more general one (i.e. a hypernym). E.g. animal is a
hypernym of cat.
61
information, On2L employs it to search for OOV
terms and their corresponding hypernyms. The
natural language texts are rich in terms, which can
be used as labels of concepts in the ontology and
rich in semantic relations, which can be used as
ontological relations.
The two areas which are working on similar
topics but are using different terminology need
to be distinguished, so that the extraction of se-
mantic information from natural language is sep-
arated from the process of integrating this knowl-
edge into an ontology.
Figure 1: Natural Language and Ontology Learn-
ing
Figure 1 shows the process of ontology learning
from natural language text. On the left side natural
language lexemes are extracted. During a transfor-
mation process nouns, verbs and proper nouns are
converted into concepts, relations and instances of
an ontology2.
3 Related Work
The idea of acquiring knowledge exactly at the
time it is needed is new and became extremely
useful with the emergence of open-domain dia-
log systems. Before that, more or less complete
ontologies could be modeled for the few domains
covered by a dialog system. Nonetheless, many
ontology learning frameworks exist, which alle-
viate the work of an ontology engineer to con-
struct knowledge manually, e.g. ASIUM (Faure
and Nedellec, 1999), which helps an expert in ac-
quiring knowledge from technical text using syn-
tactic analysis for the extraction, a semantic simi-
larity measure and a clustering algorithm for the
2In our definition of the term ontology not only concepts
and relations are included but also instances of the real world.
conceptualization. OntoLearn (Missikoff et al,
2002) uses specialized web site texts as a corpus
to extract terminology, which is filtered by statis-
tical techniques and then used to create a domain
concept forest with the help of a semantic interpre-
tation and the detection of taxonomic and similar-
ity relations. KAON Text-To-Onto (Maedche and
Staab, 2004) applies text mining algorithms for
English and German texts to semi-automatically
create an ontology, which includes algorithms for
term extraction, for concept association extraction
and for ontology pruning.
Pattern-based approaches to extract hy-
ponym/hypernym relationships range from
hand-crafted lexico-syntactic patterns (Hearst,
1992) to the automatic discovery of such patterns
by e.g. a minimal edit distance algorithm (Pantel
et al, 2004).
The SmartWeb Project into which On2L will be
integrated as well, aims at constructing an open-
domain spoken dialog system (Wahlster, 2004)
and includes different techniques to learn ontolog-
ical knowledge for the system?s ontology. Those
methods work offline and not at the time of the
user?s inquiry in contrast to On2L:
C-PANKOW (Cimiano et al, 2005) puts a
named entity into several linguistic patterns that
convey competing semantic meanings. The pat-
terns, which can be matched most often on the web
indicate the meaning of the named entity.
RelExt (Schutz and Buitelaar, 2005) automat-
ically identifies highly relevant pairs of concepts
connected by a relation over concepts from an
existing ontology. It works by extracting verbs
and their grammatical arguments from a domain-
specific text collection and computing correspond-
ing relations through a combination of linguistic
and statistical processing.
4 The ontology learning framework
The task of the ontology learning framework
On2L is to acquire knowledge at run time. As
On2L will be integrated into the open-domain di-
alog system Smartweb (Wahlster, 2004), it will be
not only useful for extending the ontology of the
system, but to make the dialog more natural and
therefore user-friendly.
Natural language utterances processed by an
open-domain spoken dialog system may contain
words or parts of words which are not recognized
by the speech recognizer, as they are not contained
62
in the recognizer lexicon. The words not contained
are most likely not represented in the word-to-
concept lexicon as well3. In the presented ontol-
ogy learning framework On2L the corresponding
concepts of those terms are subject to a search on
the Internet. For instance, the unknown term Auer-
stein would be searched on the Internet (with the
help of a search engine like Google). By applying
natural language patterns and statistical methods
possible hypernyms of the term can be extracted
and the corresponding concept in the ontology of
the complete dialog system can be found. This
process is described in Section 4.5.
As a term often has more than one meaning
depending on the context in which it is uttered,
some information about this context is added for
the search4 as shown in Section 4.4.
Figure 2 shows the life cycle of the On2L frame-
work. In the middle of the diagram the question
example by a supposed user is: How do I get to
the Auerstein? The lighter fields in the figure mark
components of the dialog system, which are only
utilized by On2L, whereas the darker fields are es-
pecially built to complete the ontology learning
task.
Figure 2: The On2L Life Cycle
The sequential steps shown in Figure 2 are de-
scribed in more detail in the following paragraphs
starting with the processing of the user?s utterance
by the speech recognizer.
4.1 Speech Recognition
The speech recognizer classifies all words of the
user?s utterance not found in the lexicon as out-
3In case the speech recognizer of the system and the word-
to-concept lexicon are consistent.
4Of course, even in the same context a term can have more
than one meaning as discussed in Section 4.6.
of-vocabulary (OOV). That means an automatic
speech recognition (ASR) system has to process
words, which are not in the lexicon of the speech
recognizer (Klakow et al, 2004). A solution
for a phoneme-based recognition is the establish-
ment of corresponding best rated grapheme-chain
hypotheses (Gallwitz, 2002). These grapheme-
chains are constructed with the help of statistical
methods to predict the most likely grapheme order
of a word, not found in the lexicon. Those chains
are then used for a search on the Internet in the
final version of On2L. To evaluate the framework
itself adequately so far only a set of correctly writ-
ten terms is subject to search.
4.2 Language Understanding
In this step of the dialog system, all correctly
recognized terms of the user utterance are mapped
to concepts with the help of a word-to-concept lex-
icon. Such a lexicon assigns corresponding nat-
ural language terms to all concepts of an ontol-
ogy. This is not only a necessary step for the di-
alog system, but can assist the ontology learning
framework in a possibly needed semantic disam-
biguation of the OOV term.
Furthermore the information of the concepts of
the other terms of the utterance can help to evalu-
ate results: when there are more than one concept
proposal for an instance (i.e. on the linguistic side
a proper noun like Auerstein) found in the system?s
ontology, the semantic distance between each pro-
posed concept and the other concepts of the user?s
question can be calculated5 .
4.3 Preprocessing
A statistical part-of-speech tagging method de-
cides on the most probable part-of-speech of the
whole utterance with the help of the sentence con-
text of the question. In the On2L framework
we used the language independent tagger qtag6,
which we trained with the hand-tagged German
corpus NEGRA 27.
5E.g. with the single-source shortest path algorithm of
Dijkstra (Cormen et al, 2001).
6qtag exists as a downloadable JAR file and
can therefore be integrated into a platform inde-
pendent JAVA program. For more information, see
http://www.english.bham.ac.uk/staff/omason/software/qtag.html
(last access: 21st February 2006).
7The NEGRA corpus version 2 consists of 355,096 to-
kens (20,602 sentences) of German newspaper text, taken
from the Frankfurter Rundschau. For more information
visit: http://www.coli.uni-saarland.de/projects/sfb378/negra-
corpus/negra-corpus.html (last access: 21st February 2006).
63
With the help of this information, the part-of-
speech of the hypernym of the OVV term can be
predicted. Furthermore, the verb(s) of the utter-
ance can anticipate possible semantic relations for
the concept or instance to be integrated into the
ontology.
4.4 Context Module
To understand the user in an open-domain dialog
system it is important to know the extra-linguistic
context of the utterances. Therefore a context
module is applied in the system, which can give
information on the discourse domain, day and
time, current weather conditions and location of
the user. This information is important for On2L
as well. Here we make use of the location of the
user and the discourse domain so far, as this infor-
mation is most fruitful for a more specific search
on the Internet. The location is delivered by a GPS
component and the discourse domain is detected
with the help of the pragmatic ontology PrOnto
((Porzel et al, 2006)). Of course, the discourse
domain can only be detected for domains modeled
already in the knowledge base (Rueggenmann and
Gurevych, 2004).
The next section will show the application of the
context terms in more detail.
4.5 Hypernym extraction from the Internet
We apply the OOV term from the speech recog-
nizer as well as a context term for the search of
the most likely hypernym on the Internet.
For testing reasons a list of possible queries was
generated. Here are some examples to give an
idea:
(1) Auerstein ? Heidelberg
(2) Michael Ballack ? SportsDiscourse
(3) Lord of the Rings ? CinemaDiscourse
On the left side of the examples 1 to 3 is the
OOV term and on the right side the corresponding
context term as generated by the context module.
For searching, the part ?Discourse? is pruned.
The reason to lay the main focus of the evalu-
ation searches on proper nouns is, that those are
most likely not in the recognizer lexicon and not
as instances in the system?s ontology.
4.5.1 Global versus Local OOVs
To optimize results we make a distinction be-
tween global OOVs and local OOVs.
In the case of generally familiar proper nouns
like stars, hotel chains or movies (so to say global
OOVs), a search on Wikipedia can be quite suc-
cessful.
In the case of proper nouns, only common in
a certain country region, like Auerstein (Restau-
rant), Bierbrezel (Pub) and Lux (Cinema), which
are local OOVs, a search with Wikipedia is gener-
ally not fruitful. Therefore it is searched with the
help of the Google API.
As one can not know the kind of OOV before-
hand, the Wikipedia search is started before the
Google search. If no results are produced, the
Google search will deliver them hopefully. If re-
sults are found, Google search will be used to test
those.
4.5.2 Wikipedia Search
The structure of Wikipedia8 entries is preas-
signed. That means, the program can know, where
to find the most suitable information beforehand.
In the case of finding hypernyms the first sentence
in the encyclopedia description is most useful. To
give an example, here is the first sentence for the
search entry Michael Ballack:
(4) Michael Ballack (born September 26,
1976 in Grlitz, then East Germany) IS A
German football player.
With the help of lexico-syntactic patterns, the
hypernym can be extracted. Those so-called
Hearst patterns (Hearst, 1992) occur frequently in
lexicons for describing a term. In example 4 the
pattern X is a Y would be matched and the hyper-
nym football player9 of the term Michael Ballack
could be extracted.
4.5.3 Google Search
The search parameters in the Google API can
be adjusted for the corresponding search task. The
tasks we used for our framework are a search in
the titles of the web pages and a search in the text
of the web pages.
Adjusting the Google parameters The as-
sumption was, that depending on the task the
Google parameters should be adjusted. Four pa-
rameters were tested with the two tasks (Title and
8Wikipedia is a free encyclopedia, which is editable on
the Internet: www.wikipedia.org (last access: 22nd February
2006)
9In German compounds generally consist of only one
word, therefore it is easier to extract them than in the case
of English ones.
64
Page Search, as described in the next paragraphs)
and a combination thereof. The parameter default
is used, when no other parameters are assigned; in-
title is set, in case the search term should be found
in the title of the returned pages; allintext, when
the search term should be found in the text of the
pages; and inurl, when the search term should be
found in the URL.
In Figure 3 the outcome of the evaluation is
shown. The evaluation was done by students, who
scored the titles and pages with 1, when a possible
hypernym could be found and 0 if not. Surpris-
ingly, the default value delivered the best results
for all tasks, followed by the allintext parameter.
Figure 3: Evaluation of the Google parameters
Title Search To search only in the titles of the
web pages has the advantage, that results can be
generated relatively fast. This is important as time
is a relevant factor in spoken dialog systems. As
the titles often contain the hypernym but do not
consist of a full sentence, Hearst patterns cannot
be found. Therefore, an algorithm was imple-
mented, which searches for nouns in the title, ex-
tracts them and counts the occurrences. The noun
most frequently found in all the titles delivered
by Google is regarded as the hypernym. For the
counting we applied stemming and clustering al-
gorithms to group similar terms.
Page Search For Page Search Hearst patterns as
in Wikipedia Search were applied. In contrast to
encyclopedia entries the recall of those patterns
was not so high in the texts from the web pages.
Thus, we searched in the text surrounding of the
searched term for nouns. Equally to Title Search
we counted the occurrence of nouns. Different
evaluation steps showed, that the window size of
four words in front and after the term is most suc-
cessful.
With the help of machine learning algorithms
from the WEKA10 library we did a text mining to
10http://www.cs.waikato.ac.nz/ml/weka (last access: 21st
ameliorate the results as shown in Faulhaber et al
(2006).
4.5.4 Results
Of all 100 evaluated pages for Google parame-
ters only about 60 texts and about 40 titles con-
tained possible hypernyms (as shown in Figure 3).
This result is important for the evaluation of the
task algorithms as well. The outcome of the eval-
uation setup was nearly the same: 38 % precicion
for Title Search and about 58 % for Page Search
(see Faulhaber (2006)). These scores where eval-
uated with the help of forms asking students: Is X
a hypernym of Y?.
4.6 Disambiguation by the user
In some cases two or more hypernyms are scored
with the same ? or quite similar ? weights. An ob-
vious reason is, that the term in question has more
than one meaning in the same context. Here, only
a further inquiry to the user can help to disam-
biguate the OOV term. In the example from the
beginning a question like ?Did you mean the hotel
or the restaurant?? could be posed. Even though
the system would show the user that it did not per-
fectly understand him/her, the user might be more
contributory than in a question like ?What did you
mean??. The former question could be posed by
a person familiar with the place, to disambiguate
the question of someone in search for Auerstein as
well and would therefore mirror a human-human
dialog leading to more natural dialogs with the
machine.
4.7 Integration into the ontology
The foundational ontology (Cimiano et al, 2004)
integrated into the dialog system Smartweb is
based on the highly axiomatized Descriptive On-
tology for Linguistic and Cognitive Engineering
(DOLCE) 11. It features various extensions called
modules, e.g. Descriptions & Situations (Gangemi
and Mika, 2003). Additional to the foundational
ontology a domain-independent layer is included
which consists of a range of branches from the less
axiomatic SUMO (Suggested Upper Merged On-
tology (Niles and Pease, 2001)), which is known
for its intuitive and comprehensible structure. Cur-
rently, the dialog system features several domain
February 2006).
11More information on this descriptive and reductionistic
approach is found on the WonderWeb Project Homepage:
wonderweb.semanticweb.org.
65
ontologies, i.e. a SportEvent-, a Navigation-, a
WebCam-, a Media-, and a Discourse-Ontology.
According to this, it is possible that in some
cases there exists the corresponding concept to a
hypernym. This can be found out with the help
of a so-called term widening. The concept labels
in the SmartWeb Ontology are generally English
terms. Therefore the found German hypernym has
to be translated into English. An English thesaurus
is used to increase the chance of finding the right
label in the ontology.
5 Future Work
The work described here is still in process and not
evaluated in detail so far. Therefore, our goal is
to establish a task-oriented evaluation setup and to
ameliorate the results with various techniques.
As natural language texts are not only rich in hi-
erarchical relations but in other semantic relations
as well, it is advantageous to extend the ontology
by those relations.
As user contexts are an important part of a dia-
log system, we are planning to learn new user con-
texts, which can be represented in the ontology by
the DOLCE module Descriptions and Situations.
Furthermore our goal is, to integrate the on-
tology learning framework into the open-domain
spoken dialog system Smartweb.
References
Philipp Cimiano, Andreas Eberhart, Daniel Hitzler,
Pascal Oberle, Steffen Staab, and Rudi Studer.
2004. The smartweb foundational ontology.
SmartWeb Project Report.
Philipp Cimiano, Gu?nter Ladwig, and Steffen Staab.
2005. Gimme? the context: Context-driven auto-
matic semantic annotation with c-pankow. In Pro-
ceedings of the 14th World Wide Web Conference.
ACM Press.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Section 24.3:
Dijkstra?s algorithm. In Introduction to Algorithms,
Second Edition, pages 595?601. MIT Press and
McGraw-Hill.
Arndt Faulhaber, Berenike Loos, Robert Porzel, and
Rainer Malaka. 2006. Towards understanding the
unknown: Open-class named entity classification in
multiple domains. In Proceedings of the Ontolex
Workshop at LREC. Genoa, Italy.
David Faure and Claire Nedellec. 1999. Knowledge
acquisition of predicate argument structures from
technical texts using machine learning: The system
asium. In EKAW ?99: Proceedings of the 11th Eu-
ropean Workshop on Knowledge Acquisition, Mod-
eling and Management, London, UK. Springer-
Verlag.
Florian Gallwitz. 2002. Integrated Stochastic Models
for Spontaneous Speech Recognition. Logos, Berlin.
Aldo Gangemi and Peter Mika. 2003. Understand-
ing the semantic web through descriptions and situ-
ations. In Proceedings of the ODBASE Conference.
Springer.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING, Nantes, France.
Dietrich Klakow, Georg Rose, and Xavier Aubert.
2004. Oov-detection in a large vocabulary sys-
tem using automatically defined word-fragments as
filler. In Proceedings of EUROSPEECH?99, Bu-
dapest, Hungary.
John Lyons. 1977. Semantics. University Press, Cam-
bridge, MA.
Alexander Maedche and Steffen Staab. 2004. Ontol-
ogy learning. In Steffen Staab and Rudi Studer, ed-
itors, Handbook on Ontologies, International Hand-
books on Information Systems. Springer.
Michele Missikoff, Roberto Navigli, and Paola Velardi.
2002. Integrated approach to web ontology learning
and engineering. In IEEE Computer - November.
Ian Niles and Adam Pease. 2001. Towards a standard
upper ontology. In Chris Welty and Barry Smith,
editors, Workshop on Ontology Management, Ogun-
quit, Maine. Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001).
Patrick Pantel, Deepak Ravichandran, and Eduard
Hovy. 2004. Towards terascale semantic acquisi-
tion. In Proceedings of Coling, Geneva, Switzer-
land. COLING.
Robert Porzel, Hans-Peter Zorn, Berenike Loos, and
Rainer Malaka. 2006. Towards a separation of prag-
matic knowledge and contextual information. In
Proceedings of ECAI-06 Workshop on Contexts and
Ontologies, Lago di Garda, Italy.
Klaus Rueggenmann and Iryna Gurevych. 2004. As-
signing domains to speech recognition hypotheses.
In Proceedings of HLT-NAACL Workshop on Spoken
Language Understanding for Conversational Sys-
tems and Higher Level Linguistic Knowledge for
Speech Processing. Boston, USA.
Alexander Schutz and Paul Buitelaar. 2005. Relext: A
tool for relation extraction in ontology extension. In
Proceedings of the 4th International Semantic Web
Conference. Galway, Ireland.
Wolfgang Wahlster. 2004. SmartWeb: Mobile appli-
cations of the semantic web. In Proceedings of In-
formatik, Ulm, Germany.
66
Resolution of Lexical Ambiguities in Spoken Dialogue Systems
Berenike Loos Robert Porzel
European Media Laboratory, GmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
 
firstname.lastname@eml-d.villa-bosch.de 
Abstract
The development of conversational multi-
domain spoken dialogue systems poses new
challenges for the reliable processing of less re-
stricted user utterances. Unlike in controlled
and restricted dialogue systems a simple one-
to-one mapping from words to meanings is no
longer feasible here. In this paper two different
approaches to the resolution of lexical ambigu-
ities are applied to a multi-domain corpus of
speech recognition output produced from spon-
taneous utterances in a spoken dialogue sys-
tem. The resulting evaluations show that all
approaches yield significant gains over the ma-
jority class baseline performance of .68, i.e. f-
measures of .79 for the knowledge-driven ap-
proach and .86 for the supervised learning ap-
proach.
1 Introduction
Following Ide and Veronis (1998) we can distinguish
between data- and knowledge-driven word sense dis-
ambiguation (WSD). Given the basic distinction be-
tween written text and spoken utterances, we follow
Allen et al (2001) and differentiate further between con-
trolled and conversational spoken dialogue systems. Nei-
ther data- nor knowledge-driven word sense disambigua-
tion has been performed on speech data stemming from
human interactions with dialogue systems, since multi-
domain conversational spoken dialogue systems for hu-
man computer interaction (HCI) have not existed in the
past. Now that speech data from multi-domain systems
have become available, corresponding experiments and
evaluations have become feasible.
In this paper we present the results of first word
sense disambiguation annotation experiments on data
from spoken interactions with multi-domain dialogue
systems. Additionally, we describe the results of a cor-
responding evaluation of a data- and a knowledge-driven
word sense disambiguation system on that data. For
knowledge-driven disambiguation we examined whether
the ontology-based method for computing semantic co-
herence introduced by Gurevych et al (2003a) can be
employed to disambiguate between alternative interpre-
tations, i.e. concept representations, of a given speech
recognition hypothesis (SRH) at hand. We will show
the results of its evaluation in the semantic interpreta-
tion task of WSD. For example, in speech recognition
hypotheses containing forms of the German verb kom-
men, i.e. (to) come, a decision had to be made whether
its meaning corresponds to the motion sense or to the
showing sense, i.e. becoming mapped onto either a
MotionDirectedTransliteratedProcess or a
WatchPerceptualProcess in the terminology of
our spoken language understanding system. For a data-
driven approach we employed a highly supervised learn-
ing algorithm introduced by Brants (2000) and trained
it on a corpus of annotated data. A second set of se-
mantically annotated speech recognition hypotheses was
employed as a gold-standard for evaluating both the
ontology-based and supervised learning method. Both
data sets were annotated by separate human annotators.
All annotated data stems from log files of an auto-
matic speech recognition system that was implemented in
the SMARTKOM system (Wahlster et al, 2001; Wahlster,
2003). It is important to point out that there are at least
two essential differences between spontaneous speech
WSD and textual WSD, i.e.,
 a smaller size of processable context as well as
 imperfections, hesitations, disfluencies and speech
recognition errors.
Existing spoken language understanding systems,
that are not shallow and thusly produce deep syntac-
tic and semantic representations for multiple domains,
e.g. the production system approach described by
Engel (2002) or unification-based approaches described
by Crysmann et al (2002), have shown to be more suit-
able for well-formed input but less robust in case of im-
perfect input. For conversational and reliable dialogue
systems that achieve satisfactory scores in evaluation
frameworks such as proposed by Walker et al (2000) or
Beringer et al (2002) for multi-modal dialogue systems,
we need robust knowledge- or data-driven methods for
disambiguating the sometimes less than ideal output of
the large vocabulary spontaneous speech recognizers. In
the long run, we would also like to avoid expensive pre-
processing work, which is necessary for both ontology-
driven and supervised learning methods, i.e. labor in-
tensive ontology engineering and data annotation respec-
tively.
2 State of the Art
After work on WSD had overcome so-called early doubts
(Ide and Veronis, 1998) in the 1960?s, it was applied to
various NLP tasks, such as machine translation, informa-
tion retrieval, content and grammatical analysis and text
processing. Yarowsky (1995) used both supervised and
unsupervised WSD for correct phonetizitation of words
in speech synthesis. However, there is no recorded work
on processing speech recognition hypotheses resulting
from speech utterances as it is done in our research.
In general, following Ide and Veronis (1998) the various
WSD approaches of the past can be divided into two
types, i.e., data- and knowledge-based approaches.
2.1 Data-based Methods
Data-based approaches extract their information directly
from texts and are divided into supervised and unsuper-
vised methods (Yarowsky, 1995; Stevenson, 2003).
Supervised methods work with a given (and therefore
limited) set of potential classes in the learning process.
For example, Yarowsky (1992) used a thesaurus to gener-
ate 1042 statistical models of the most general categories.
Weiss (1973) already showed that disambiguation rules
can successfully be learned from hand-tagged corpora.
Despite the small size of his training and test corpus, an
accuracy of 90   was achieved. Even better results on
a larger corpus were obtained by Kelly and Stone 1975
who included collocational, syntactic and part of speech
information to yield an accuracy of 93   on a larger cor-
pus. As always, supervised methods require a manually
annotated learning corpus.
Unsupervised methods do not determine the set of
classes before the learning process, but through analysis
of the given data by identifying clusters of similar cases.
One example is the algorithm for clustering by commit-
tee described by Pantel and Lin (2003), which automati-
cally discovers word senses from text. Generally, unsu-
pervised methods require large amounts of data. In the
case of spoken dialogue and speech recognition output
sufficient amounts of data will hopefully become avail-
able once multi-domain spoken dialogue systems are de-
ployed in real world applications.
2.2 Knowledge-based Methods
Knowledge-based approaches work with lexica and/or
ontologies. The kind of knowledge varies widely and
machine-readable as well as computer lexica are em-
ployed. The knowledge-based approach employed herein
(Gurevych et al, 2003a) operates on an ontology partially
derived from FrameNet data (Baker et al, 1998) and de-
scribed by Gurevych et al (2003b).
In a comparable approach Sussna (1993) worked with
the lexical reference system WordNet and used a similar
metric for the calculation of semantic distance of a num-
ber of input lexemes. Depending on the type of semantic
relation (hyperonymy, synonymy etc.) different weights
are given and his metric takes account of the number of
arcs of the same type leaving a node and the depth of a
given edge in the overall tree. The disambiguation results
on textual data reported by Sussna (1993) turned out to
be significantly better than chance. In contrast to many
other work on WSD with WordNet he took into account
not only the isa hierarchy, but other relational links as
well. The method is, therefore, similar to the one used
in this evaluation, with the difference that this one uses a
semantic-web conform ontology instead of WordNet and
it is applied to speech recognition hypotheses. The fact,
that our WSD work is done on SRHs makes it difficult
to compare the results with methods evaluated on textual
data such as in the past SENSEVAL studies (Edmonds,
2002).
The ontology-based system has been successfully used
for a set of tasks such as finding the best speech recog-
nition hypotheses from sets of competing SRHs, labeling
SRHs as correct or incorrect representations of the users
intention and for scoring their degree of contextual co-
herence (Gurevych et al, 2003a; Porzel and Gurevych,
2003; Porzel et al, 2003). In general, the system offers
an additional way of employing ontologies, i.e. to use
the knowledge modeled therein as the basis for evaluat-
ing the semantic coherence of sets of concepts. It can be
employed independent of the specific ontology language
used, as the underlying algorithm operates only on the
nodes and named edges of the directed graph represented
by the ontology. The specific knowledge base, e.g. writ-
ten in OIL-RDFS, DAML+OIL or OWL,1 is converted
into a graph, consisting of the class hierarchy, with each
class corresponding to a concept representing either an
entity or a process and their slots, i.e. the named edges
of the graph corresponding to the class properties, con-
straints and restrictions.
1OIL-RDFS, DAML+OIL and OWL are frequently used
knowledge modeling languages originating in W3C and Se-
mantic Web projects. For more details, see www.w3c.org/RDF,
www.w3c.org/OWL and www.daml.org.
3 Data and Annotation Experiment
In this section we describe the data collection and anno-
tation experiments performed in order to obtain indepen-
dent data sets for training and evaluation.
3.1 Data Collection
The first data set was used for training the supervised
model is described in Gurevych et al (2002b) and was
collected using the so-called Hidden Operator Test (Rapp
and Strube, 2002). This procedure represents a simplifi-
cation of classical end-to-end experiments and Wizard-
of-Oz experiments (Francony et al, 1992) - as it is con-
ductible without the technically very complex use of a
real or a seemingly real conversational system. The sub-
jects are prompted to ask for specific information and the
system response is pre-manufactured. We had 29 subjects
prompted to say certain inputs in 8 dialogues. 1479 turns
were recorded. In our experimental setup each user-turn
in the dialogue corresponded to a single illocution, e.g.
route request or sights information request as described
by Gurevych et al (2002a).
The second data set was used for testing the data- and
ontology-based systems and thusly will be called the test
corpus. It was produced by means of Wizard-of-Oz ex-
periments (Francony et al, 1992). In this type of setting
a full-blown multimodal dialogue system is simulated by
a team of human hidden operators. A test person com-
municates with the supposed system and the dialogues
are recorded and filmed digitally. Here over 224 subjects
produced 448 dialogues (Schiel et al, 2002), employing
the same domains and tasks as in the first data collection.
3.2 Data Pre-Processing
After manual segmentation of the data into single utter-
ances. The resulting audio files were then manually tran-
scribed. The segmented audio files were handed to the
speech recognition engine integrated in the SMARTKOM
dialogue system (Wahlster, 2003). Employing the seman-
tic parsing system described by Engel (2002) the corre-
sponding speech recognition word lattices (Oerder and
Ney, 1993) were first transformed into n-best lists of so-
called hypotheses sequences. These were mapped onto
conceptual representations, which contain the multiple
semantic interpretations of the individual hypotheses se-
quences that arise due to lexical ambiguities.
For obtaining the training data, we used only the best,
correct and perfectly disambiguated speech recognition
hypotheses as described by Porzel et al (2003) from the
first data set of 552 utterances. For obtaining the test
data we took a random sample of 3100 utterances from
the second data set. This seeming discrepancy between
training and test data is due to the fact that only a part of
the test data set actually contains ambiguous lexical items
and many of the utterances quite similar to each other.
For example, given the utterance shown in its transcribed
form in example (1), we then obtained the sequence of
recognition hypotheses shown in examples (1a) - (1e).
1 wie
how
komme
can
ich
I
in
in
Heidelberg
Heidelberg
weiter.
continue.
1a Rennen
Race
Lied
song
Comedy
comedy
Show
show
Heidelberg
Heidelberg
weiter.
continue.
1b denn
then
wie
how
Comedy
comedy
Heidelberg
Heidelberg
weiter.
continue.
1c denn
then
wie
how
kommen
come
Show
show
weiter.
continue.
1d denn
then
wie
how
Comedy
comedy
weiter.
continue.
1e denn
then
wie
how
komme
can
ich
I
in
in
Heidelberg
Heidelberg
weiter.
continue.
3.3 Annotation
We employed VISTAE2 (Mu?ller, 2002) for annotat-
ing the data and for creating the corresponding gold-
standards for the training and test corpora. The annota-
tion of the data was done by two persons specially trained
for the annotation tasks, with different purposes:
 First of all, if humans are able to annotate the data
reliably, it is generally more feasible that machines
are able to do that as well. This was the case as
shown by the resulting inter annotator agreement of
78.89   .
 Secondly, a gold-standard is needed to evaluate the
systems? performances. For that purpose, the anno-
tators reached an agreement on annotated items of
the test data which had differed in the first place.
The resulting gold-standard represents the highest
degree of correctly disambiguated data and is used
for comparison with the tagged data produced by the
disambiguation systems.
 Thirdly, for the supervised learning another cor-
rectly disambiguated data set is needed for training
the statistical model.
2The acronym stands for Visualization Tool for Annotation
and Evaluation.
The class-based kappa statistic of (Cohen, 1960; Car-
letta, 1996) cannot be applied here, as the classes vary
depending on the number of ambiguities per entry in the
lexicon. Also an additional class, i.e., not-decidable
was allowed for cases as in SRH (1c), where it is impos-
sible to assign sensible meanings. The test data set alo-
gether was annotated with 2219 markables of ambiguous
tokens, stemming from 70 ambiguous words occurring in
the test corpus.
3.4 Calculating the Baselines
For calculating the majority class baseline, which in our
case corresponds to the performance of a unigram tagger,
we applied the method described in (Porzel and Malaka,
2004). Therefore, all markables in the gold-standard
were counted and, corresponding to the frequency of each
concept of each ambiguous lexeme, the percentage of
correctly chosen concepts by means of selecting the most
frequent meaning was calculated. This resulted in a base-
line of 52.48   for the test data set.
4 Word Sense Disambiguation Systems
Both word sense disambiguation systems described
herein were tested and developed with the SMARTKOM
research framework. As one of the most advanced current
systems, the SMARTKOM (Wahlster, 2003) comprises a
large set of input and output modalities together with an
efficient fusion and fission pipeline. SMARTKOM fea-
tures speech input with prosodic analysis, gesture input
via infrared camera, recognition of facial expressions and
their emotional states. On the output side, the system fea-
tures a gesturing and speaking life-like character together
with displayed generated text and multimedia graphical
output. It currently comprises nearly 50 modules running
on a parallel virtual machine-based integration software
called Multiplatform3 described in Herzog et al (2003).
4.1 The Knowledge-driven System
The ontology employed for the evaluation has about
800 concepts and 200 relations (apart from the isa-
relations defining the general taxonomy) and is described
by Gurevych et al (2003b). It includes a generic top-
level ontology whose purpose is to provide a basic struc-
ture of the world, i.e. abstract classes to divide the uni-
verse in distinct parts as resulting from the ontological
analysis.4 The modeling of Processes and Physical Ob-
jects as a kind of event that is continuous and homoge-
neous in nature, follows the frame semantic analysis used
for generating the FRAMENET data (Baker et al, 1998).
3The abbreviation stands for ?MUltiple Language / Target
Integration PLATform FOR Modules?.
4The top-level was developed following the procedure out-
lined in Russell and Norvig (1995).
The hierarchy of Processes is connected to the hierarchy
of Physical Objects via slot-constraint definitions herein
referred to as relations.
The system performs a number of processing steps. A
first preprocessing step is to convert each SRH into a
concept representation (CR). For that purpose the sys-
tem?s lexicon is used, which contains either zero, one
or many corresponding concepts for each entry. A sim-
ple vector of concepts - corresponding to the words in
the SRH for which entries in the lexicon exist - consti-
tutes each resulting CR. All other words with empty con-
cept mappings, e.g. articles, are ignored in the conver-
sion. Due to lexical ambiguity, i.e. the one to many
word - concept mappings, this processing step yields a
set  
	 of possible interpreta-
tions for each SRH.
For example, the words occurring in a SRH such as
(2) have the corresponding entries in the lexicon that are
shown below.
2 Ich
I
bin
am
auf
on
dem
the
Philosphenweg
Philosopher?s Walk

entry 

string  Ich  /string 

concept  Person  /concept 
 /entry 

entry 

string  bin  /string 

concept  StaticSpatialProcess  /concept 

concept  SelfIdentificationProcess  /concept 

concept  NONE  /concept 
 /entry 

entry 

string  auf  /string 

concept  TwoPointRelation  /concept 

concept  NONE  /concept 
 /entry 

entry 

string  Philosophenweg  /string 

concept  Location  /concept 
 /entry 
Since we have multiple concept entries for individual
words, i.e. lexical ambiguities, we get a resulting set  
of concept representations.
CR1  Person, StaticSpatialProcess, Location 
CR2  Person, StaticSpatialProcess,
TwoPointRelation, Location 
CR3  Person, SelfIdentificationProcess, Location 
CR4  Person, SelfIdentificationProcess,
TwoPointRelation, Location 
CR5  Person, TwoPointRelation, Location 
CR6  Person, Location 
The concept representations consist of a different num-
ber of concepts, because the concept none is not rep-
resented in the CRs. The concept none is assigned to
lexemes which have one (or more than one) meaning
outside the SmartKom domains or constitute functional
grammatical markers.
The system then converts the domain model, i.e. an
ontology, into a directed graph with concepts as nodes
and relations as edges. In order to find the shortest
path between two concepts, the ONTOSCORE system em-
ploys the single source shortest path algorithm of Dijk-
stra (Cormen et al, 1990). Thus, the minimal paths con-
necting a given concept   with every other concept in CR
(excluding   itself) are selected, resulting in an  ma-
trix of the respective paths. To score the minimal paths
connecting all concepts with each other in a given CR,
a method proposed by Demetriou and Atwell (1994) to
score the semantic coherence of alternative sentence in-
terpretations against graphs based on the Longman Dic-
tionary of Contemporary English (LDOCE) was used in
the original system.5
The new addition made for this evaluation was to as-
sign different weights to the individual relations found
by the algorithm, depending on their level of granularity
within the relation hierarchy. For example, a broad level
relation such as has-theme which is found in the class
statement of Process is weighted with negative 1 as it
has only one super-relation, i.e. has-role, whereas a more
specific relation such as has-actor is weighted with neg-
ative 4 because it has four super-relations, i.e. has-artist,
has-associated-person(s), has-attribute and has-role.
As before, the algorithm selects from the set of all
paths between two concepts the one with the smallest
weight, i.e. the cheapest. The distances between all con-
cept pairs in CR are summed up to a total score.6 The set
of concepts with the lowest aggregate score represents the
combination with the highest semantic relatedness.
4.2 The Data-driven System
In this section we describe the implementation of the sta-
tistical learning techniques employed for the task of per-
forming WSD on our corpus of spoken dialogue data.
For our experiments we took the general purpose sta-
tistical tagger (Brants, 2000), which is generally used for
part-of-speech tagging. It employs a VITERBI algorithm
for second order Markov models (Rabiner, 1989), linear
interpolation for smoothing and deleted interpolation for
5As defined by Demetriou and Atwell (1994),  	


is the set of direct relations (both isa and se-
mantic relations) that can connect two nodes (concepts); and

	





Making Relative Sense:
From Word-graphs to Semantic Frames
Robert Porzel Berenike Loos Vanessa Micelli
European Media Laboratory, GmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg, Germany
 
firstname.lastname@eml-d.villa-bosch.de 
Abstract
Scaling up from controlled single domain spo-
ken dialogue systems towards conversational,
multi-domain and multimodal dialogue sys-
tems poses new challenges for the reliable pro-
cessing of less restricted user utterances. In this
paper we explore the feasibility to employ a
general purpose ontology for various tasks in-
volved in processing the user?s utterances.
1 Introduction
We differentiate between controlled single-domain and
more conversational multi-domain spoken dialogue sys-
tems (Allen et al, 2001). The transition from the former
to the later can be regarded as a scaling process, since vir-
tually every processing technique applicable for restricted
single domain user utterances has to be adopted to new
challenges, i.e., varying context-dependencies (Porzel et
al., 2004) increasing levels of ambiguity (Gurevych et al,
2003a; Loos and Porzel, 2004) and less predictable input
(Loeckelt et al, 2002). Additionally, for conversational
multi-domain spoken dialogue systems tasks have to be
tackled that were by and large unnecessary in restricted
single-domain systems. In this exploration, we will focus
on a subset of these tasks, namely:
 hypotheses verification (HV) - i.e. finding the best
hypothesis out of a set of possible speech recogni-
tion hypotheses (SRH);
 sense disambiguation (SD) - i.e. determining the
best mapping of the lexically ambiguous linguistic
forms contained therein to their sense-specific se-
mantic representations;

relation tagging (RT) - i.e. determining adequate se-
mantic relations between the relevant sense-tagged
entities.
Many of these tasks have been addressed in other fields,
for example, hypothesis verification in the field of ma-
chine translation (Tran et al, 1996), sense disambigua-
tion in speech synthesis (Yarowsky, 1995), and relation
tagging in information retrieval (Marsh and Perzanowski,
1999). These challenges also apply for spoken dialogue
systems and arise when they are scaled up towards multi-
domain and more conversational settings.
In this paper we will address the utility of using on-
tologically modeled knowledge to assist in solving these
tasks in spoken dialogue systems. Following an overview
of the state of the art in Section 2 and the ontology-based
coherence scoring system in Section 3, we describe its
employment in the task of hypotheses verification in Sec-
tion 4. In Section 5 we describe the system?s employment
for the task of sense disambiguation and in Section 6 we
present first results of a study examining the performance
of the system for the task of relation tagging. An analy-
sis of the evaluation results and concluding remarks are
given in Section 7.
2 Related Work
2.1 Hypotheses Verification
While a simple one-best hypothesis interface between au-
tomatic speech recognition (ASR) and natural language
understanding (NLU) suffices for restricted dialogue sys-
tems, more complex systems either operate on n-best lists
as ASR output or convert ASR word graphs (Oerder and
Ney, 1993) into n-best lists. Usually, this task is per-
formed by combining the respective acoustic and lan-
guage model scores calculated by the speech recognition
system as described by Schwartz and Chow (1990).
Facing multiple representations of a single utterance
consequently poses the question, which one of the dif-
ferent hypotheses corresponds most likely to the user?s
utterance. Several ways of solving this problem have
been proposed and implemented in various systems. As
mentioned above, the scores provided by the ASR sys-
tem itself are used most frequently. Still, in recent works
also scores provided by the NLU system have been em-
ployed, e.g. parsing scores (Engel, 2002) or discourse
based scores (Pfleger et al, 2002). However, these meth-
ods are prone to assign very high scores to SRHs which
are semantically incoherent and low scores to semanti-
cally coherent ones, if faced with imperfect and unpre-
dicted input (Porzel et al, 2003a).
2.2 Sense Disambiguation
Employing the task categorization scheme proposed by
Stevenson (2003), the task of creating adequate seman-
tic representations of the individual entities occurring in
the SRHs can be regarded as a form of semantic dis-
ambiguation. Since, in our case, a fixed inventory of
senses is given by the lexicon and only the ambiguous
lexical forms have to be disambiguated, our task falls
into the corresponding subcategory of sense disambigua-
tion. Following Ide and Veronis (1998) we distinguish
between data- and knowledge-driven word sense disam-
biguation. Given the basic distinction between written
text and spoken utterances, the only sense disambiguation
results performed on speech data stemming from human
interactions with dialogue systems have been reported by
Loos and Porzel (2004), who compared both data- and
knowledge-driven sense disambiguation on the same set
of actual speech data.
Historically, after work on WSD had overcome so-
called early doubts (Ide and Veronis, 1998) in the 1960?s,
it was applied to various NLP tasks, such as machine
translation, information retrieval, content and grammat-
ical analysis and text processing. Yarowsky (1995)
used both supervised and unsupervised WSD for cor-
rect phonetizitation of words in speech synthesis. How-
ever, there is no recorded work on processing speech
recognition hypotheses resulting from speech utterances
as it is done in our research. In general, following
Ide and Veronis (1998) the various WSD approaches of
the past can be divided into two types, i.e., data- and
knowledge-based approaches.
Data-based Methods Data-based approaches extract
their information directly from texts and are divided into
supervised and unsupervised methods (Yarowsky, 1995;
Stevenson, 2003).
Supervised methods work with a given (and therefore
limited) set of potential classes in the learning process.
For example, Yarowsky (1992) used a thesaurus to gen-
erate 1042 statistical models of the most general cate-
gories. Weiss (1973) already showed that disambiguation
rules can successfully be learned from hand-tagged cor-
pora. However limited by the small size of his training
and test corpus, an accuracy of 90   was achieved. Even
better results on a larger corpus were obtained by Kelly
and Stone 1975 who included collocational, syntactic and
part of speech information to yield an accuracy of 93   on
a larger corpus. As always, supervised methods require a
manually annotated learning corpus.
Unsupervised methods do not determine the set of
classes before the learning process, but through analysis
of the given data by identifying clusters of similar cases.
One example is the algorithm for clustering by commit-
tee described by Pantel and Lin (2003), which automati-
cally discovers word senses from text. Generally, unsu-
pervised methods require large amounts of data. In the
case of spoken dialogue and speech recognition output
sufficient amounts of data will hopefully become avail-
able once multi-domain spoken dialogue systems are de-
ployed in real world applications.
Knowledge-based Methods Knowledge-based ap-
proaches work with lexica and/or ontologies. The kind
of knowledge varies widely and machine-readable lexica
are employed. The knowledge-based approach employed
herein (Gurevych et al, 2003a) operates on an ontology
partially derived from FrameNet data (Baker et al, 1998)
and is described by Gurevych et al (2003b).
In a comparable approach Sussna (1993) worked with
the lexical reference system WordNet and used a similar
metric for the calculation of semantic distance of a num-
ber of input lexemes. Depending on the type of semantic
relation (hyperonymy, synonymy etc.) different weights
are given and his metric takes account of the number of
arcs of the same type leaving a node and the depth of a
given edge in the overall tree. The disambiguation results
on textual data reported by Sussna (1993) turned out to
be significantly better than chance. In contrast to many
other work on WSD with WordNet he took into account
not only the isa hierarchy, but other relational links as
well. The method is, therefore, similar to the one used
in this evaluation, with the difference that this one uses a
semantic-web conform ontology instead of WordNet and
it is applied to speech recognition hypotheses. The fact,
that our WSD work is done on SRHs makes it difficult
to compare the results with methods evaluated on textual
data such as in the SENSEVAL studies (Edmonds, 2002).
2.3 Labeling Semantic Roles and Relations
The task of representing the semantic relations
that hold between the sense tagged entities can be
thought of as an extension of the work presented by
Gildea and Jurafsky (2002), where the tagset is defined
by entities corresponding to FrameNet frame elements
(Baker et al, 1998). Therein, for example, given the oc-
currence of a CommercialTransaction frame the
task lies in the appropriate labeling of the corresponding
roles, such as buyer, seller or goods.
Additionally the task discussed herein features sim-
ilarities to the scenario template task of the Message
Understanding Conferences (Marsh and Perzanowski,
1999). In this case predefined templates are given
(e.g. is-bought-by(COMPANY A,COMPANY B)
which have to instantiated correctly, i.e. in a phrase such
as ?Stocks sky-rocketed after Big Blue acquired Softsoft
. . . ? the specific roles, i.e. Big Blue as COMPANY B and
Softsoft as COMPANY A have to be put in their adequate
places within the overall template.
Now that speech data from the more conversational
multi-domain dialogue systems have become available,
we present the corresponding annotation experiments and
evaluation results of a knowledge-driven hypothesis ver-
ification, sense disambiguation and relation tagging sys-
tem, whose knowledge store and algorithm are presented
below.
3 Ontology-based Scoring and Tagging
The Ontology Used: The ontology used in the exper-
iments described herein was initially designed as a gen-
eral purpose component for knowledge-based NLP. It in-
cludes a top-level ontology developed following the pro-
cedure outlined by Russell and Norvig (1995) and orig-
inally covered the tourism domain encoding knowledge
about sights, historical persons and buildings. Then, the
existing ontology was adopted in the SMARTKOM project
(Wahlster et al, 2001) and modified to cover a number
of new domains, e.g., new media and program guides,
pedestrian and car navigation and more (Gurevych et al,
2003b). The top-level ontology was re-used with some
slight extensions. Further developments were motivated
by the need of a process hierarchy.
This hierarchy models processes which are domain-
independent in the sense that they can be relevant for
many domains, e.g., InformationSearchProcess. The
modeling of Process as a kind of event that is continuous
and homogeneous in nature, follows the frame seman-
tic analysis used in the FRAMENET project (Baker et al,
1998).
The role structure also reflects the general intention to
keep abstract and concrete elements apart. A set of most
general properties has been defined with regard to the
role an object can play in a process: agent, theme, ex-
periencer, instrument (or means), location, source, tar-
get, path. These general roles applied to concrete pro-
cesses may also have subroles: thus an agent in a pro-
cess of buying (TransactionProcess) is a buyer, the one
in the process of cognition is a cognizer. This way, roles
can also build hierarchical trees. The property theme in
the process of information search is a required piece-of-
information, in PresentationProcess it is a presentable-
object, i.e., the entity that is to be presented.
The OntoScore System: The ONTOSCORE software
runs as a module in the SMARTKOM multi-modal and
multi-domain spoken dialogue system (Wahlster, 2003).
The system features the combination of speech and ges-
ture as its input and output modalities. The domains of
the system include cinema and TV program information,
home electronic device control as well as mobile services
for tourists, e.g. tour planning and sights information.
ONTOSCORE operates on n-best lists of SRHs pro-
duced by the language interpretation module out of the
ASR word graphs. It computes a numerical ranking of
alternative SRHs and thus provides an important aid to
the spoken language understanding component. More
precisely, the task of ONTOSCORE in the system is to
identify the best SRH suitable for further processing and
evaluate it in terms of its contextual coherence against the
domain and discourse knowledge.
ONTOSCORE performs a number of processing steps.
At first each SRH is converted into a concept represen-
tation (CR). For that purpose we augmented the system?s
lexicon with specific concept mappings. That is, for each
entry in the lexicon either zero, one or many correspond-
ing concepts where added. A simple vector of concepts
- corresponding to the words in the SRH for which en-
tries in the lexicon exist - constitutes each resulting CR.
All other words with empty concept mappings, e.g. ar-
ticles and aspectual markers, are ignored in the conver-
sion. Due to lexical ambiguity, i.e. the one to many
word - concept mappings, this processing step yields a
set  
	 of possible interpreta-
tions for each SRH.
Next, ONTOSCORE converts the domain model, i.e. an
ontology, into a directed graph with concepts as nodes
and relations as edges. In order to find the shortest path
between two concepts, ONTOSCORE employs the single
source shortest path algorithm of Dijkstra (Cormen et al,
1990). Thus, the minimal paths connecting a given con-
cept Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 33?40,
New York City, June 2006. c?2006 Association for Computational Linguistics
Scaling Natural Language Understanding via User-driven Ontology
Learning
Berenike Loos
European Media Laboratory, GmbH
Schloss-Wolfsbrunnenweg 33, 69118 Heidelberg, Germany
firstname.lastname@eml-d.villa-bosch.de
Abstract
Non-statistical natural language under-
standing components need world knowl-
edge of the domain for which they are ap-
plied in a machine-readable form. This
knowledge can be represented by manu-
ally created ontologies. However, as soon
as new concepts, instances or relations
are involved in the domain, the manually
created ontology lacks necessary informa-
tion, i.e. it becomes obsolete and/or in-
complete. This means its ?world model?
will be insufficient to understand the user.
The scalability of a natural language un-
derstanding system, therefore, essentially
depends on its capability to be up to
date. The approach presented herein ap-
plies the information provided by the user
in a dialog system to acquire the knowl-
edge needed to understand him or her ad-
equately. Furthermore, it takes the posi-
tion that the type of incremental ontology
learning as proposed herein constitutes a
viable approach to enhance the scalability
of natural language systems.
1 Introduction
To let a computer system understand natural lan-
guage one needs knowledge about objects and their
relations in the real world. As the manual modeling
and maintenance of such knowledge structures, i.e.
ontologies, are not only time and cost consuming,
but also lead to not-scalable systems, there exists a
demand to build and populate them automatically or
at least semi automatically. This is possible by an-
alyzing unstructured, semi-structured or fully struc-
tured data by various linguistic as well as statistical
means and by converting the results into an ontolog-
ical form.
In an open-domain scalable natural language un-
derstanding (NLU) system the automatic learning
of ontological concepts and corresponding relations
between them is essential, as a complete modeling
of the world is neither practicable nor feasible, as the
real world and its objects, models and processes are
constantly changing along with their denotations.
This paper assumes that a viable approach to this
challenging problem is to learn ontological concepts
and relations relevant to a certain user in a given con-
text by the dialog system at the time of the user?s
inquiry. My central hypothesis is that the infor-
mation about terms that lack any mapping to the
employed knowledge representation of the language
understanding component can only be found in top-
ical corpora such as the Web. With the help of this
information one can find the right node in the on-
tology to append the concept corresponding to the
unknown term in case it is a noun or to insert it as an
instance in case it is a proper noun or another named
entity.
The goal of the ontology learning component is to
extend the knowledge base of the NLU system and
therefore it will gradually adapt to the user?s needs.
An example from the area of spoken dialog sys-
tems would be that of a user walking through the
city of Heidelberg and asking: ?How do I get to
33
the Auerstein?. This would lead to the detection
of Auerstein as being neither recognizable by the
speech recognizer nor mappable to the knowledge
representation of the system. Therefore, the cor-
responding hypernym of Auerstein has to be found
on the internet by recourse to additional information
about the context of the user. In this case, the ad-
ditional information consists of the location of the
user, namely Heidelberg. Once found, the hyper-
nym is mapped to a corresponding concept, which
already exists in the ontology. If there is no such
corresponding concept, the concept for the hyper-
nym thereof has to be determined. The formerly un-
known term is mapped to a concept and is integrated
into the system?s ontology as a child of the concept
for the found hypernym. In case the unknown term
is a proper noun, it is integrated as an instance of the
concept for the hypernym. So far, the research un-
dertaken is related to nouns and proper nouns, also
more generally referred to as terms in this paper.
In the following section, I will describe related
work undertaken to solve the task of ontology learn-
ing, followed by some remarks of the distinction
between ontology learning and natural language in
Section 3. Thereafter, I will sketch out the minimal
stages involved in the type of ontology learning pro-
posed herein in Section 4.
2 Related Work
The capability to acquire knowledge exactly at the
time it is needed can be regarded as an important
stepping stone towards scalable natural language un-
derstanding systems. The necessity of scalability
in NLU became more and more obvious in open-
domain dialog systems, as the knowledge base in-
tegrated into those can never be complete. Before
the emergence of open-domain systems, more or
less complete ontologies were modeled manually for
the domain needed in the NLU system and were
therefore not scalable to additional domains, un-
less modeled in advance in a manual fashion or by
means of off-line ontology learning. Nonetheless,
numerous off-line ontology learning frameworks ex-
ist, which alleviate the work of an ontology engineer
to construct knowledge manually (Maedche, 2002),
(Schutz and Buitelaar, 2005), (Cimiano et al, 2005).
Most of these frameworks apply hybrid methods to
optimize their learning results.
For example, the ontology population method On-
toLearn (Navigli et al, 2004) is based on text min-
ing and other machine learning techniques and starts
with a generic ontology like WordNet and docu-
ments in a given domain. The result is a domain
extended and trimmed version of the initial ontol-
ogy. For this, the system applies three phases to
learn concepts:
? First, a terminology extraction method, using
shallow techniques that range from stochas-
tic methods to more sophisticated syntactic ap-
proaches, is applied, which extracts a list of do-
main terms (mostly nouns and proper nouns)
from a set of documents representative for a
given domain.
? Second, a semantic interpretation takes place
which makes use of a compositional interpreta-
tion and structural semantic interconnections.
? After these two phases the extending and trim-
ming of the initial ontology takes place. With
the help of the semantic interpretation of the
terms they can be organized in sub-trees and
appended under the appropriate node of the ini-
tial ontology applying linguistic rules.
The text understanding system SYNDICATE
(SYNthesis of DIstributed Knowledge Acquired
from Texts) uses an integrated ontology learning
module (Hahn and Marko, 2002). In this approach
new concepts are learned with the help of text un-
derstanding, which applies two different sources of
evidence, namely, the prior knowledge of the topic
domain of the texts and grammatical constructions
in which unknown lexical items occur in the texts.
In an incremental process a given ontology is up-
dated as new concepts are acquired from real-world
texts. The acquisition process is centered on the lin-
guistic and conceptual ?quality? of various forms of
evidence underlying the generation and refinement
of concept hypotheses. On the basis of the quality of
evidence, concept hypotheses are ranked according
to credibility and the most credible ones are selected
for assimilation into the domain knowledge base.
The project Disciple (Stanescu et al, 2003) builds
agents which can be initially trained by a sub-
ject matter expert and a knowledge engineer, in
34
a way similar to how an expert would teach an
apprentice. A Disciple agent applies two differ-
ent methods for ontology learning, i.e. exception-
based and example-based ontology learning. The
exception-based learning approach consists of four
main phases:
? First, a candidate discovery takes place, in
which the agent analyzes a rule together with
its examples, exceptions and the ontology and
finds the most plausible types of extensions of
the latter that may reduce or eliminate the rule?s
exceptions.
? In the second phase the expert interacts with the
agent to select one of the proposed candidates.
? Afterwards the agent elicits the ontology exten-
sion knowledge from the expert and finally a
rule refinement takes place, in which the agent
updates the rule and eliminates its exceptions
based on the performed ontology extension.
? When the subject matter expert has to specify a
fact involving a new instance or new feature in
the agent teaching process, the example-based
learning method is invoked. In this process
the agent tries to find example sentences of
the words next to a new term through various
heuristics. For instance, he finds out that X is
member of Y, and consequently can ask the ex-
pert. If he affirms, the new term can be memo-
rized.
All of the approaches described above exhibit the-
oretical as well as practical (in the light of the task
undertaken herein) shortcomings. The theoretical
problems that have not been resolved in a satisfac-
tory manner by the works described above (as well
as numerous others) are:
? a clear separation of the linguistic and ontolog-
ical subtasks involved in the overall ontology
learning endeavor
? systematic ways and methods for evaluating the
individual learning results
? rigorously defined baselines against which to
evaluate the ensuing learning approaches.
In the following I will describe how these is-
sues can be addressed within the user-driven
ontology learning framework proposed herein.
3 Natural Language versus Ontology
Learning
Before describing the actual ontology learning
process it is important to make a clear distinction
between the two fields involved: This is on the one
hand natural language and on the other hand ontol-
ogy learning.
The corpora to extract knowledge from should
come from the internet as this source provides the
most up-to-date information. The natural language
texts are rich in terms, which can be used as labels
of concepts in the ontology and rich in semantic re-
lations, which can be used as ontological relations
(aka properties).
The connection between the two areas which are
working on similar topics but are using different ter-
minology needs a distinction between the extraction
of semantic information from natural language and
the final process of integrating this knowledge into
an ontology.
Figure 1: Natural Language versus Ontology Learn-
ing
Figure 1 shows the process of ontology learning
from natural language text. On the left side relevant
natural language terms are extracted. During a trans-
formation process they are converted into labels of
concepts and relations of an ontology. Proper nouns
are transfered into instance labels in the ontology1.
1In our understanding the term ontology denotes both the
instance model as well as the ground ontology.
35
4 Scaling NLU via User-driven Ontology
Learning
A user-driven ontology learning framework should
be able to acquire knowledge at the run time of the
NLU system. Therefore, terms which are not under-
stood by the system have to be identified. In dialog
systems this is true for all terms uttered or written by
a user, which are not presently contained in the lex-
icon or can be derived by means of derivational or
flexional morphology. In the following I will refer
to these terms as unknown terms2.
When a user of an open-domain spoken dialog
system makes an utterance, it happens regularly, that
the term is not represented in the system?s lexicon.
Since it is assumed, in this work, that the meaning
of terms is represented by means of a formal on-
tology, a user-driven ontology learning framework
is needed to determine the corresponding concepts
for these terms, e.g., via a search on topical corpora.
For instance, a term such as Auerstein could be em-
ployed to query a search engine. By applying natural
language patterns, as proposed by Hearst (1992) and
statistical methods, as proposed by Faulhaber et al
(2006) possible hypernyms or sets of hypernym can-
didates of the term can be extracted. For these a cor-
responding concept (or set of possible concepts) in
the ontology employed by the dialog system need to
be found. Last but not least the unknown term has to
be inserted into the ontology as either an instance or
a subclass of that concept. This process is described
in greater detail in Section 5.4).
It is important to point out that terms often have
more than one meaning, which can only be deter-
mined by recourse to the context in which it is ut-
tered/found (Widdows, 2003), (Porzel et al, 2006).
Therefore, information about this context needs to
be added in order to make searching for the right
hypernym feasible3 as shown in Section 5.3. For ex-
ample, the term Lotus can refer to a flower, a specific
type of car or among copious other real world enti-
ties to a restaurant in Heidelberg. Therefore, a scal-
able ontology learning framework in a dialog system
requires at least the following ingredients:
2This closely corresponds to what is termed out-of-
vocabulary (OOV) words in the automatic speech recognition
community.
3Of course, even in the same context a term can have more
than one meaning as discussed in Section 5.7.
? A formal explicit model of a shared conceptual-
ization of a specific domain of interest (Gruber,
1993), i.e. an ontology;
? processing methods which indicate the un-
known terms;
? a corpus, as the starting point to retrieve hyper-
nyms;
? methods for mapping hypernyms to concepts in
the ontology;
? an evaluation framework;
Figure 2 shows the steps involved in on-demand
ontology learning from the text to the knowledge
side.
Figure 2: From text to knowledge
5 On-demand learning
From the cognitive point of view learning makes
only sense when it happens on-demand. On-demand
means, that it occurs on purpose and that activity
is involved rather than passivity. As pointed out by
Spitzer (2002) for human beings activity is neces-
sary for learning. We cannot learn by drumming
data into our brain through listening cassettes when
sleeping or by similar fruitless techniques. The rea-
son for this is, that we need active ways of struc-
turing the data input into our brain. Furthermore, we
try only to learn what we need to learn and are there-
fore quite economic with the ?storage space? in our
brain.
36
It makes not only for humans sense to simply
learn whatever they need and what is useful for
them. Therefore, I propose that ontology learning,
as any other learning, is only useful and, in the end,
possible if it is situated and motivated by the given
context and the user needs. This can entail learning
missing concepts relevant to a domain or to learn
new concepts and instances which become neces-
sary due to changes in a domain.
However, the fundamental ontological commit-
ments should be adhered to. So, for example, the
decision between a revisionary and a descriptive on-
tology should be kept in the hand of the knowledge
engineer, as well as the choice between a multiplica-
tive and a reductionist modeling4. As soon as the
basic structure is given new knowledge can be in-
tegrated into this structure. Thus, for a reduction-
ist ontology a concept such as Hotel should be ap-
pended only once, e.g. to an ontological concept as
PhysicalObject rather than NonPhysicalObject.
In the following I will describe the various steps
and components involved in on-demand ontology
learning.
5.1 Unknown terms in dialog systems
In case the dialog system works with spoken lan-
guage one can use the out-of-vocabulary (OOV)
classification of the speech recognizer about all
terms not found in the lexicon (Klakow et al,
2004). A solution for a phoneme-based recog-
nition is the establishment of corresponding best-
rated grapheme-chain hypotheses (Gallwitz, 2002).
Those can be used for a search on the internet. In
case the dialog system only works with written lan-
guage it is easier to identify terms, which cannot be
mapped to ontological concepts, at least if they are
spelled correctly. To evaluate the framework itself
adequately it is useful to apply only correctly writ-
ten terms for a search.
Later on in both cases - i.e. in spoken and written
dialog systems - a ranking algorithm of the best, say
three, hypotheses should be selected to find the most
adequate term. Here methods like the one of Google
?Did you mean...? for spelling errors could be used.
4More information on these and other ontological choices
can be found summarized in (Cimiano et al, 2004)
5.2 Language Understanding
All correctly recognized terms of the user utterance
can be mapped to concepts with the help of an analy-
sis component. Frequently, production systems
(Engel, 2002), semantic chunkers (Bryant, 2004)
or simple word-to-concept lexica (Gurevych et al,
2003) are employed for this task. Such lexica assign
corresponding natural language terms to all concepts
of an ontology. This is especially important for a
later semantic disambiguation of the unknown term
(Loos and Porzel, 2004). In case the information of
the concepts of the other terms of the utterance can
help to evaluate results: When there is more than one
concept proposal for an instance (i.e. on the linguis-
tic side a proper noun like Auerstein) found in the
word-to-concept lexicon, the semantic distance be-
tween each proposed concept and the other concepts
of the user?s question can be calculated5 .
5.3 Linguistic and Extra-linguistic Context
Not only linguistic but also extra linguistic context
plays an important role in dialog systems. Thus, to
understand the user in an open-domain dialog sys-
tem it is important to know the extra-linguistic con-
text of the utterances. If there is a context module
or component in the system it can give information
on the discourse domain, time and location of the
user. This information can be used as a support for a
search on the internet. E.g. the location of the user
when searching for, say Auerstein, is advantageous,
as in the context of the city Heidelberg it has a dif-
ferent meaning than in the context of another city
(Bunt, 2000), (Porzel et al, 2006).
Part of the context information can be represented
by the ontology as well as patterns for grouping a
number of objects, processes and parameters for one
distinctive context (Loos and Porzel, 2005).
5.4 Finding the appropriate hypernym on the
internet
For this, the unknown term as well as an appropri-
ate context term (if available) needs to be applied
for searching possible hypernyms on the Web. As
mentioned before an example could be the unknown
term Auerstein and the context term Heidelberg.
5E.g. with the single-source shortest path algorithm of Dijk-
stra (Cormen et al, 2001).
37
For searching the internet different encyclopedias
and search engines can be used and the correspond-
ing results can be compared. After a distinction be-
tween different types of unknown terms, the search
methods are described.
Global versus local unknown terms: In the case
of generally familiar proper nouns like stars, hotel
chains or movies (so to say global unknown terms),
a search on a topical encyclopedia can be quite suc-
cessful. In the case of proper nouns, only common in
a certain country region, such as Auerstein (Restau-
rant), Bierbrezel (Pub) and Lux (Cinema), which are
local unknown terms, a search in an encyclopedia
is generally not fruitful. Therefore, one can search
with the help of a search engine.
As one can not know the kind of unknown terms
beforehand, the encyclopedia search should be ex-
ecuted before the one using the search engine. If
no results are produced, the latter will deliver them
(hopefully). In case results are retrieved by the for-
mer, the latter can still be used to test those.
Encyclopedia Search: The structure of Encyclo-
pedia entries is generally pre-assigned. That means,
a program can know, where to find the most suit-
able information beforehand. In the case of finding
hypernyms the first sentence in the encyclopedia de-
scription is often found to be the most useful. To
give an example from Wikipedia6 , here is the first
sentence for the search entry Michael Ballack:
(1) Michael Ballack (born September 26, 1976
in Grlitz, then East Germany) IS A German
football player.
With the help of lexico-syntactic patterns, the hy-
pernym can be extracted. These so-called Hearst
patterns (Hearst, 1992) can be expected to occur fre-
quently in lexicons for describing a term. In example
1 the pattern X is a Y would be matched and the hy-
pernym football player of the term Michael Ballack
could be extracted.
Title Search: To search only in the titles of web
pages might have the advantage, that results can be
6Wikipedia is a free encyclopedia, which is editable on the
internet: http://www.wikipedia.org (last access: 26th January
2006).
generated relatively fast. This is important as real-
time performance is an important usability factor in
dialog systems. When the titles contain the hyper-
nym it still is to be expected that they might not
consist of full sentences, Hearst patterns (Hearst,
1992) are, therefore, unlikely to be found. Alter-
natively, only the nouns in the title could be ex-
tracted and their occurrences counted. The noun
most frequently found in all the titles could then be
regarded as the most semantically connected term.
To aid such frequency-based approaches stemming
and clustering algorithms can be applied to group
similar terms.
Page Search: For a page search Hearst patterns as
in the encyclopedia search can almost certainly be
applied. In contrast to encyclopedia entries the recall
of those patterns is not so high in the texts from the
web pages.
Figure 3: Tasks for the evaluation of ontology learn-
ing
The text surrounding the unknown term is
searched for nouns. Equal to the title search the oc-
currence of nouns can then be counted. With the
help of machine learning algorithms a text mining
can be done to ameliorate the results.
5.5 Mapping text to knowledge by term
narrowing and widening
As soon as an appropriate hypernym is found in a
text the corresponding concept name should be de-
termined. For term narrowing, the term has to be
stemmed to its most general form. For the term
widening, this form is used to find synonyms. Those
38
are, in turn, used for searching ontological concept
names in the ontology integration phase. If the hy-
pernym found is in a language other than the one
used for the ontology, a translation of the terms has
to take place as well.
5.6 Integration into an ontology
After the mapping phase newly learned concepts,
instances or relations can be integrated into any
domain-independent or even foundational ontology.
If no corresponding concept can be found the next
more general concept has to be determined by the
techniques described above.
5.7 Evaluation
An evaluation of such a system can be divided into
two types: one for the performance of the algorithms
before the deployment of the system and one, which
can be performed by a user during the run time of
the system.
Methodological evaluation Before integrating
the framework into a dialog system or any other
NLU system an evaluation of the methods and their
results should take place. Therefore, a representative
baseline has to be established and a gold-standard
(Grefenstette, 1994) created, depending on the task
which is in the target of the evaluation. The ensuing
steps in this type of evaluation are shown in Figure
3 and described here in their order:
1. The extraction of hypernyms of unknown
words from text and the extraction of semantic
relations (other than is-a) between NLU terms.
2. The mapping of a linguistic term to an ontolog-
ical concept.
3. The integration of ontological concepts, in-
stances and relations into the system?s ontol-
ogy.
Depending on the three steps the most adequate
baseline method or algorithm for each of them has
to be identified. In step 1 for the extraction of hyper-
nyms a chance baseline as well as a majority class
baseline will not do the job, because their perfor-
mance would be too poor. Therefore, a well estab-
lished algorithm which, for example applies a set of
standard Hearst patterns (Hearst, 1992) would con-
stitute a potential candidate. For the mapping from
text to knowledge (see step 2) the baseline could be
a established by standard stemming combined with
string similarity metrics. In case of different source
and goal languages an additional machine transla-
tion step would also become necessary. For the base-
line of ontology evaluation a task-based framework
as proposed by (Porzel and Malaka, 2005) could be
employable.
Evaluation by the user As soon as the framework
is integrated into a dialog system the only way to
evaluate it is by enabling the user to browse the on-
tological additions at his or her leisure and to de-
cide whether terms have been understood correctly
or not. In case two or more hypernyms are scored
with the same ? or quite similar ? weights, this ap-
proach could also be quite helpful. An obvious rea-
son for this circumstance is, that the term in ques-
tion has more than one meaning in the same context.
Here, only a further inquiry to the user can help to
disambiguate the unknown term. In the Auerstein
example a question like ?Did you mean the hotel
or the restaurant?? could be posed. Even though
the system would show the user that it did not per-
fectly understand him/her, the user might be more
contributory and less annoyed than with a question
like ?What did you mean??. The former question
could also be posed by a person familiar with the
place, to disambiguate the question of someone in
search for Auerstein and would therefore mirror a
human-human dialogs, which in turn would further-
more lead to more natural human-computer dialogs.
6 Concluding Remarks
In this paper I have shown, that the scalability of
non-statistical natural language understanding sys-
tems essentially depends on its capability to be up to
date when it comes to understand language. Fur-
thermore, I took the position that ontology learn-
ing is viable, when it happens incrementally and in
a context-sensitive fashion. Future work will focus
on implementation and evaluation within a running
multi-modal dialog system. Additionally, a tight in-
tegration with automatic lexicon and grammar learn-
ing is of paramount importance.
39
References
John Bryant. 2004. Scalable construction-based parsing
and semantic analysis. In Proceedings of the 2nd In-
ternational Workshop on Scalable Natural Language
Understanding (ScaNaLU 2004) at HLT-NAACL 2004.
Harry Bunt. 2000. Dialogue pragmatics and con-
text specification. In H.C. Bunt and W.J. Black, ed-
itors, Computational Pragmatics, Abduction, Belief
and Context; Studies in Computational Pragmatics,
pages 81?150. John Benjamins, Amsterdam.
Philipp Cimiano, Andreas Eberhart, Daniel Hitzler, Pas-
cal Oberle, Steffen Staab, and Rudi Studer. 2004. The
SmartWeb foundational ontology. SmartWeb Project
Report.
Philipp Cimiano, Gu?nter Ladwig, and Steffen Staab.
2005. Gimme? the context: Context-driven automatic
semantic annotation with C-PANKOW. In Proceed-
ings of the 14th World Wide Web Conference. ACM
Press.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Section 24.3: Dijk-
stra?s algorithm. In Introduction to Algorithms, Sec-
ond Edition, pages 595?601. MIT Press and McGraw-
Hill.
Ralf Engel. 2002. SPIN: Language understanding for
spoken dialogue systems using a production system
approach. In Proceedings of the International Confer-
ence on Speech and Language Processing 2002, Den-
ver, USA.
Arndt Faulhaber, Berenike Loos, Robert Porzel, and
Rainer Malaka. 2006. Open-class named entity clas-
sification in multiple domains. In Proceedings of the
Ontolex Workshop. Genua, Italy.
Florian Gallwitz. 2002. Integrated Stochastic Models for
Spontaneous Speech Recognition. Logos, Berlin.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
USA.
Thomas Gruber. 1993. A translation approach to
portable ontology specifications. Knowledge Acqui-
sition (5).
Iryna Gurevych, Rainer Malaka, Robert Porzel, and
Hans-Peter Zorn. 2003. Semantic coherence scoring
using an ontology. In Proc. of the HLT/NAACL 2003,
page (in press), Edmonton, CN.
Udo Hahn and Kornl G. Marko. 2002. Ontology and lex-
icon evolution by text understanding. In Proceedings
of the ECAI 2002 Workshop on Machine Learning and
Natural Language Processing for Ontology Engineer-
ing (OLT?2002). Lyon, France.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING 92, Nantes, France.
Dietrich Klakow, Georg Rose, and Xavier Aubert. 2004.
Oov-detection in a large vocabulary system using au-
tomatically defined word-fragments as filler. In Pro-
ceedings of EUROSPEECH?99, Budapest, Hungary.
Berenike Loos and Robert Porzel. 2004. Resolution of
lexical ambiguities in spoken dialogue systems. In
Proceedings of the 5th Workshop on Diascourse and
Dialogue, Cambridge, Massachusetts, USA.
Berenike Loos and Robert Porzel. 2005. Towards
ontology-based pragmatic analysis. In Proceedings of
DIALOR?05, Nancy, France.
Alexander Maedche. 2002. Ontology Learning for the
Semantic Web. Kluwer Academic Publishers, USA.
Roberto Navigli, Paola Velardi, Alessandro Cucchiarelli,
and Francesca Neri. 2004. Extending and enriching
WordNet with OntoLearn. In Proceeedings of Inte-
grated Approach for Web Ontology Learning and En-
gineering. IEEE Computer.
Robert Porzel and Rainer Malaka. 2005. A task-based
framework for ontology learning, population and eval-
uation. Ontology Learning from Text: Methods, Eval-
uation and Applications Frontiers in Artificial Intelli-
gence and Applications Series, 123.
Robert Porzel, Iryna Gurevych, and Rainer Malaka.
2006. In context: Integrating domain- and situation-
specific knowledge. SmartKom Foundations of Mul-
timodal Dialogue Systems, Springer, Cognitive Tech-
nologies.
Alexander Schutz and Paul Buitelaar. 2005. RelExt:
A tool for relation extraction in ontology extension.
In Proceedings of the 4th International Semantic Web
Conference. Galway, Ireland.
Manfred Spitzer. 2002. Lernen. Spektrum Akademis-
cher Verlag.
Bogdan Stanescu, Cristina Boicu, Gabriel Balan, Mar-
cel Barbulescu, Mihai Boicu, and Gheorghe Tecuci.
2003. Ontologies for learning agents: Problems, solu-
tions and directions. In Proceedings of the 2003 IEEE
International Conference on Systems, Man and Cyber-
netics, Volume: 3. Washington D.C.
Dominic Widdows. 2003. A mathematical model for
context and word-meaning. In International and Inter-
disciplinary Conference on Modeling and Using Con-
text. Stanford, California.
40
