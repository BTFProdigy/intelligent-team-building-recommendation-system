Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1153?1160
Manchester, August 2008
Choosing the Right Translation:
A Syntactically Informed Classification Approach
Simon Zwarts
Centre for Language Technology
Macquarie University
Sydney, Australia
szwarts@ics.mq.edu.au
Mark Dras
Centre for Language Technology
Macquarie University
Sydney, Australia
madras@ics.mq.edu.au
Abstract
One style of Multi-Engine Machine
Translation architecture involves choos-
ing the best of a set of outputs from
different systems. Choosing the best
translation from an arbitrary set, even
in the presence of human references, is
a difficult problem; it may prove better
to look at mechanisms for making such
choices in more restricted contexts.
In this paper we take a classification-
based approach to choosing between
candidates from syntactically informed
translations. The idea is that using
multiple parsers as part of a classifier
could help detect syntactic problems in
this context that lead to bad transla-
tions; these problems could be detected
on either the source side?perhaps sen-
tences with difficult or incorrect parses
could lead to bad translations?or on
the target side?perhaps the output
quality could be measured in a more
syntactically informed way, looking for
syntactic abnormalities.
We show that there is no evidence that
the source side information is useful.
However, a target-side classifier, when
used to identify particularly bad trans-
lation candidates, can lead to signifi-
cant improvements in Bleu score. Im-
provements are even greater when com-
bined with existing language and align-
ment model approaches.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
1 Introduction
It is fairly safe to say that whenever there
are multiple approaches to solving a problem
in Artificial Intelligence, the idea of trying to
find a better solution by combining those ap-
proaches has been proposed: blackboard archi-
tectures, ensemble methods for machine learn-
ing, and so on.
In Machine Translation (MT), there is a
long tradition of combining multiple machine
translations, as through a Multi-Engine MT
(MEMT) architecture; the origins of this are
generally credited to Frederking and Nirenburg
(1994). One way of dividing up such systems
is into those that take the whole output from
multiple systems and judge between them to
select the best candidate, and those that com-
bine elements of the outputs to construct a
best candidate.
Deciding between whole sentence level out-
puts looks like a classical classification prob-
lem. Of course, deciding between MT out-
puts in the general case is a problem that cur-
rently has no good solution, and is unlikely
to in the near future: Bleu (and similar met-
rics) require one or more reference texts to dis-
tinguish between candidate outputs with the
level of accuracy that they achieve, and even
then they are open to substantial criticism
(Callison-Burch et al, 2006). However, there
are reasons to think that there is some promise
in considering this as a classification problem.
Corston-Oliver et al (2001) build a classifier to
distinguish between human and machine trans-
lations with an 80% accuracy. Several other
later systems have some success in distinguish-
ing between MT outputs using language mod-
els, alignment models, and voting schemes. In
addition, while the problem of deciding be-
tween arbitrary MT outputs is difficult, it may
1153
be feasible in specific cases. The classifier con-
structed by Corston-Oliver et al (2001) takes
advantage of characteristic mistakes found in
the output of the particular MT system used.
In general, we are interested in MT where
syntax is involved. The first part of the main
idea of this paper is that there are two ways
in which problematic translations might be de-
tected. One is on the source side: perhaps sen-
tences with difficult or incorrect parses could
lead to bad use of syntax and hence bad trans-
lations, and this could be detected by a clas-
sifier. The other is that on the target side,
perhaps the output quality could be measured
in a more syntactically informed way, looking
for syntactic abnormalities.
As to the particular system, in this paper
we look at a specific type of MT, the output of
systems that use syntactic reordering as pre-
processing (Collins et al, 2005; Wang et al,
2007; Zwarts and Dras, 2007). In these sys-
tems, the source language is reordered to mir-
ror the syntax of the target language in certain
respects, leading to an improvement in the ag-
gregate quality of the output over the base-
line, although it is not always the case that
each individual sentence in the reordered ver-
sion is better. This could then be framed as an
MEMT, where the reordered candidate is con-
sidered the default one, backing off to the base-
line where the reordered one is worse, based on
the decision of a classifier. Given the ?unnatu-
ral? order of the preprocessed source side, there
is reason to expect that bad or unsuccessful re-
ordered translations might be detectable.
The second part of the main idea of the pa-
per is that a classifier could use a combina-
tion of multiple parsers, as in Mutton et al
(2007), to indicate problems. In that work,
designed to assess fluency of output of gen-
eration systems, metrics were developed from
various parsers?log probability of most likely
parse, number of tree fragments, and so on?
that correlated with human judgements, and
that could be combined in a classifier to pro-
duce a better evaluation metric. We take such
an approach as a starting point for developing
classifiers to indicate problematic source and
target sides within a reordering MT system.
In Section 2 we review some related work. In
Section 3 we investigate the potential gain in
correctly choosing the better translation can-
didate in our context. In Section 4 we build
a classifier using an approximation to fairly
standard language and alignment model fea-
tures, mostly for use as a comparator, while
Sections 5 and 6 present our models based on
source and target language sides respectively.
Section 7 concludes.
2 Related work
In this section we briefly review some relevant
work on deciding between translation candi-
dates in ?sentence-level? MEMT.
Most common is the use of language models,
or voting which may be based on some kind of
alignment, or a combination. Callison-Burch
and Flournoy (2001) use a trigram language
model (LM) on MT outputs to decide the best
candidate, looking at nine systems across four
language directions and domains, and treating
them as black boxes; evaluation is by human
judges and on a fairly small data set. Akiba
et al (2002) score MT outputs by a combina-
tion of a standard LM and an alignment model
(here IBM 4), and then use statistical tests
to determine rankings of MT system outputs.
Eisele (2005) uses a heuristic voting scheme
based on n-gram overlap of the different out-
puts, and adds an LM to make decisions; the
LM reportedly achieves further improvement.
Rosti et al (2007) look at sentence-level com-
binations (as well as word- and phrase-level),
using reranking of n-best lists and confidence
scores derived from generalised linear models
with probabilistic features from n-best lists.
Huang and Papineni (2007) propose a hier-
archical model for word, phrase and sentence
level combination; they use LMs and inter-
estingly find that incorporating rudimentary
linguistic information like Part-of-Speech is
helpful. Riezler and Maxwell (2006) combine
transfer-based and statistical MT; they back
off to the SMT translation when the grammar
is inadequate, analysing the grammar to deter-
mine this.
Other work, like ours, uses a classifier. The
goal of Corston-Oliver et al (2001) is slightly
different, in that it aims to distinguish human
translations from MT output. The classifier
uses syntactic features derived from a manual
error analysis, taking advantage of character-
1154
istics specific to their MT system and parser.
Nomoto (2003) uses a LM and an IBM-based
alignment model, and then constructs sepa-
rate SVMs for regression based on these, each
with a single feature (i.e. the LM value or the
alignment model value); the SVM is thus not
strictly used as a classifier, but as a regression
tool. Nomoto (2004) extends this by decid-
ing on the best LM through a voting scheme.
Other related work not in an MEMT context
that uses parsers to distinguish better from
worse translations are on syntax-based lan-
guage models (Charniak et al, 2003) and on
syntactically informed reranking (Och et al,
2003). Both use only single parsers and work
only with candidate translations generated in-
side an SMT system (either all candidates or
n-best).
3 Potential Gain
The type of system we focus on in this pa-
per operates in two stages. First, syntac-
tically based reordering takes place to make
the source sentence more similar in struc-
ture to the syntax of the target language.
This is then passed to a Phrase-based SMT
(PSMT) component (Pharaoh (Koehn, 2004)
in the cited work). For German to English
(Collins et al, 2005) and Dutch to English
(Zwarts and Dras, 2007) this reordering in-
volves moving some long-distance dependen-
cies closer together, such as clause-final par-
ticiples and verb-second auxiliaries. This im-
proves translation quality by compensating for
the weakness in PSMT of long-distance word
reordering: Collins et al (2005) report a 1.6
Bleu percentage point improvement, Zwarts
and Dras (2007) a 1.0 Bleu percentage point
improvement.
However, individual sentences translated
from the original non-reordered source sen-
tences are sometimes better than their re-
ordered equivalent; examples are given in both
Collins et al (2005) and Zwarts and Dras
(2007). (We refer to these in the rest of the
paper as non-reordered translations and re-
ordered translations respectively.) For there to
be a point to constructing an MEMT-style sys-
tem where the reordered translation is the de-
fault translation and the non-reordered trans-
lation the fallback, it is necessary for the non-
reordered version to be better a reasonable
proportion of the time, allowing scope for a
Bleu improvement across the system.
To determine if this is the case, we construct
an approximate oracle to choose the better
of each pair of reordered and non-reordered
translation sentences. While Bleu is a rea-
sonable choice for evaluating the quality of the
overall composite set of translation sentences,
it is not suitable for sentence-level decisions.
However, in line with Nomoto (2003)?s moti-
vation for developing m-precision as an alter-
native to Bleu, we make the following obser-
vation.
The Bleu score (ignoring brevity) is an har-
monic mean between the different n-gram com-
ponents:
exp(
?
N
n=1
log p
n
)
Here p
n
is the precision for the different n-gram
overlap counts of a candidate sentence with a
gold standard sentence. If we want to glob-
ally optimise this score for an optimal Bleu
document score, we need to pick for each sen-
tence the n-gram counts that contribute most
to the overall score. For example, if we have
to pick between sentence A and sentence B,
where A has 2 unigram counts and 1 bigram
count, and B has 2 unigram counts only, A is
clearly preferred; however, for sentences C and
D, where C has 4 unigram counts and D has 2
unigram counts and 1 bigram count, we do not
know which eventually will lead to the global
maximum Bleu.
However we observe that because it is an
harmonic mean, small values are weighted ex-
ponentially heavier, due to the log operator.
Our heuristic to achieve the highest score is to
have the most extreme possible small values.
Since we know that an n-gram is always less
frequent than an (n? 1)-gram we concentrate
on the higher n-grams first. The decision pro-
cess between sentences is therefore to choose
the candidate with higher n-gram counts for
the maximum value of n, then n ? 1-gram
counts, and so on down to unigrams.
Here we will work with the Dutch?English
data used by Zwarts and Dras (2007). We use
the portions of the Europarl corpus (Koehn,
2003) that were used for training in that work;
and Bleu with 1 reference with n-grams up
to length 4. We then use our heuristic to se-
lect between the reordered and non-reordered
1155
Not-Bleu comparable
Identical 179,327
Undecidable 119,725
Total 299,052
Bleu comparable
Non-Reordered better 128,585
Reordered better 163,172
Total 291,757
Overall Total 590,809
Table 1: Comparing translation quality
Learner Baseline Accuracy
English ? Dutch
SVM - Polynomial 56.0% 56.6%
SVM - Polynomial 50.0% 51.2%
Maximum Entropy 50.0% 51.0%
Dutch ? English
SVM - Polynomial 50.0% 51.4%
Table 2: Results for internal language decider
translation candidates of Zwarts and Dras
(2007) for the language direction Dutch to En-
glish. Selecting the reordered translation as
default and backing off leads to a 1.1 Bleu
percentage point improvement over the 1.0 al-
ready mentioned. Results for English to Dutch
are similar.
In Table 1 we see the breakdown of the en-
tire corpus we work with. Some sentences are
identical, and some are different but with no
indication by our heuristic as to which of the
two is better. In the cases where we do have
an indication we see a sizeable 44% of the non-
reordered translations are better.
4 Internal Indicators
Before looking at our syntax-related ap-
proaches, it would be useful to have a com-
parison based on the approaches of previous
work. As noted in Section 2, these generally
use language models and alignment models, as
usual to estimate fluency and fidelity of candi-
date translations.
Because our two candidate solutions are
both ultimately produced by Pharaoh (Koehn,
2004), our quick-and-dirty solution can use
Pharaoh?s own final translation probabili-
ties, which capture language and alignment
model information. We build a classifier
Learner Baseline Accuracy
English ? Dutch
SVM - Polynomial 50.0% 50.1%
SVM - Radial 50.0% 49.7%
Maximum Entropy 50.0% 50.2%
Table 3: Results for Source language decider
that attempts to distinguish the better of a
pair of reordered and non-reordered transla-
tions. Denoting the non-reordered transla-
tion T
n
, and the reordered T
r
, we take as fea-
tures log(P (T
n
)), log(P (T
r
)), and log(P (T
n
))-
log(P (T
r
)). In addition, because the sentences
do not always have equal length and we do
not want to penalise longer sentences, we also
have three features describing the perplex-
ity: e
log(P (T
n
))/length(T
n
)
, e
log(P (T
r
))/length(T
r
)
,
and the difference between these two. Here
length is the function returning the length of
a sentence in tokens. Our training data we
get by partitioning the sentences according to
whether reordering is beneficial as measured by
our heuristic from Section 3. As machine learn-
ers we used SVM-light
1
(Joachims, 1998) and
the MaxEnt decider from the Stanford Classi-
fier
2
(Manning and Klein, 2003).
Table 2 shows the results the classifier pro-
duces on this data set. While the accuracy
rates for the classifiers are all statistically sig-
nificantly different (at a 95% confidence level)
from the baseline (using a standard test of pro-
portions), the results are not promising.
5 Source Language Indicators
5.1 All Data
The finding that almost half of the reordered
translations degrade the actual translation
quality raises the question of why. Our ini-
tial hypothesis is that because we use more
linguistic tools, this is likely to introduce new
errors. We hypothesise that one of the prob-
lems of reordering is either the parser getting
it wrong, or the rules getting it wrong because
of parse complexity. Our idea for estimating
the wrongness of a parse, or the complexity of
a parse that might lead to incorrect reordering
rule application, is to use ?side-effect? informa-
1
http://svmlight.joachims.org
2
http://nlp.stanford.edu/software/classifier.shtml
1156
Top Correct Accuracy
10 5 50%
50 23 46%
100 48 48%
200 100 50%
500 240 48%
1000 490 49%
Table 4: Accuracy range for Source Side Ex-
treme Predictions
tion from multiple parsers, in a modification of
an idea taken from Mutton et al (2007).
3
For
example, the parser of Collins (1999), in addi-
tion to the actual parse, gives a probability for
the most likely parse; if this most likely parse is
not at all likely, this may be because the parser
is having difficulty. The Link Parser (Grinberg
et al, 1995) produces dependency-style parses,
and gives an unlinked fragment count where a
complete parse cannot be made; this unlinked
fragment count may be indicative of parse dif-
ficulty. For this part, we therefore look only
at translations with English as source side and
Dutch as target, in order to be able to use mul-
tiple parsers on the source side sentences.
Again, we construct a machine learner to
predict which is the better of the reordered and
non-reordered translations. Our training data
is as in Section 4.
As a feature set we use: character and to-
ken length of the sentence, probability values
as supplied by the Collins parser, and the un-
linked fragment count as supplied by the Link
Parser. We used machine learners as in Sec-
tion 4. Both the SVM and the features are
similar to Mutton et al (2007).
The results are calculated on 39k examples,
split 30k training, 9k testing. Table 3 shows
the results for different learning techniques
with different settings. The accuracy scores
show selection no different from random: none
of the differences are statistically significant.
With such poor results, we do not bother to
calculate the Bleu effect of using the classifier
as a decider here.
3
Similar work is that of Albrecht and Hwa (2007);
however this requires human references unavailable
here.
Learner Baseline Accuracy
Dutch ? English
SVM - Polynomial 50.0% 52.3%
Maximum Entropy 50.0% 52.9%
Table 5: Results for target language decider
5.2 Thresholding
Because our MEMT uses the non-reordered
translations as a back-off, even if the classifier
is not accurate over the whole set of sentences,
it could still be useful to identify the poor-
est reordered translations and back off only in
those cases. SVM-light gives prediction scores
as part of its classification; data points that are
firmly within the positive (negative) classifica-
tion spaces are higher positive (negative) val-
ues, while border-line cases have a value very
close to 0. Here we interpret these as an es-
timate of the magnitude of the difference in
quality between reordered and non-reordered
translations. We calculated the accuracy over
the n most extreme predictions for different
values of n. The results in Table 4 show that
the ?extreme range? does not have a higher ac-
curacy either.
6 Target Language Indicators
6.1 All Data
We now consider our second approach, trying
to classify syntactic abnormality of the trans-
lations. Inspecting the sentences by hand,
we found that there are some sentences with
markedly poor grammaticality, even by the
standards of MT output. Examples of of-
ten reoccurring problems include verb posi-
tioning (often still sentence-final), positioning
of modals in the sentence, etc. Most are in the
realm of problems the reordering rules actually
try to target.
Here we use the multiple-parser approach in
a way more like that of Mutton et al (2007),
as an estimate of the fluency of the sentence
with a focus on syntactic characteristics. As in
Section 5, we construct a classifier using mul-
tiple parser outputs to distinguish the better
of a pair of reordered and non-reordered trans-
lations. Similarly, we use as features the most
likely parse probability of the Collins parser
(Collins, 1999) and unlinked fragment count
1157
Learner Baseline Accuracy
SVM - Complete 50.0% 52.3%
SVM - LargeDiff 50.0% 52.9%
SVM - HugeDiff 50.0% 51.2%
Table 6: Varying Bleu training data
from the Link parser (Grinberg et al, 1995).
We combine these with the sentences lengths
in both character count and token count of the
two candidate sentences.
Our translation direction in this section,
Dutch to English, is the opposite of Section 5,
for the same reason that we want to use multi-
ple parsers on the target side. The reordering
on the Dutch language is done on the results of
the Alpino (Bouma et al, 2000) parser. The
rules for reordering are found in Zwarts and
Dras (2006). Our training data is again as in
Section 4.
Table 5 shows the accuracy, calculated on
a 38k examples, split 30k training, 8k testing.
The accuracy again is close to baseline perfor-
mance, although it is clearly better than our
LM and alignment classifier of Section 4. Here
all the improvements are statistically signifi-
cant on a 95% confidence level. This is sur-
prising as Mutton et al (2007) on a somewhat
similar task was much more successful. Their
performance is expressed as a correlation with
human judgement rather than accuracy, but
compared to our performance where the im-
provement in accuracy is only a couple of times
the standard error, their approach performed
much better. A possible explanation could be
that the data we work on has much subtler dif-
ferences than their work. We know both trans-
lations are ultimately generated from the same
input, which makes our both candidates very
close.
6.2 Varying Training Data
In particular in (Mutton et al, 2007) the train-
ing data used human sentences as positive ex-
emplars and very simple bigram-generated sen-
tences as negative ones, so that there was a big
difference in quality between them. So per-
haps there are too many borderline cases in
the training data here.
Therefore we retrained the classifier of Sec-
tion 6.1, selecting only those sentence pairs
Top Correct Accuracy
10 9 90%
20 19 95%
50 40 80%
100 79 79%
200 145 72.5%
500 300 66.6%
1000 538 53.8%
Table 7: Accuracy of Prediction in the extreme
range
where the difference was more distinct. For
the LargeDiff set the difference was at least 4
or more unigrams or 3 or more bigrams; for
the HugeDiff set the difference was at least 6
or more unigrams or 5 or more bigrams.
Table 6 shows the results; all accuracy scores
are better than the baseline with 95% confi-
dence. For LargeDiff, there is an improvement
over using the complete data set. Surprisingly,
for the HugeDiff training data the gain is not
only gone, but this decider performs statis-
tically significantly worse than using all the
data.
We therefore conclude that the nature of
mistakes made when using reordering as a
preprocessing step is of a very subtle kind.
Very big mistakes are made as part of trans-
lation process completely independent of re-
ordering, while the improvement due to re-
ordering is only where subtly a small set of
words, compared to the reference, has been
changed for the better. The training size how-
ever is only reduced to three quarters of the
complete training size. It is therefore very un-
likely this sudden drop in performance is due
to data sparsity.
6.3 Thresholding
As in Section 5.2, we look at the cases where
our SVM gives a higher prediction score that
indicates a greater difference in quality of the
non-reordered translation over the reordered
one. Here we use as training data the LargeDiff
set from Section 6.2.
Results are in Table 7, which unlike the
thresholded results of Section 5.2 are quite
promising. There is a clear pattern here, with
very high accuracy scores in the top range,
slowly dropping to around overall performance
1158
System Bleu
Baseline 0.208
Reordered 0.221
SVM-pick 0.238
Table 8: Bleu results for the different selec-
tions
Features Accuracy
SVM - all 52.3%
SVM - length only 49.8%
SVM - length and Link 50.5%
SVM - length and Collins 50.1%
Table 9: Contribution of Parsers
after 1000 samples. This 1000 mark is out of
3461 negative samples in the test set range,
roughly marking the first third mark before
accuracy scores have reached average perfor-
mance.
Predictions with an extreme score on the
other side of the scale hardly show an improve-
ment. Because this subset of sentences shows
a higher accuracy, it is worthwhile to calculate
Bleu scores over the sentences in the test set
belonging to the top 500 SVM-predictions pos-
itive (reordered translation is better) and the
500 SVM predictions negative (non-reordered
is better). Table 8 shows the improvement of
Bleu scores.
4
The first interesting thing which can be seen
in the table is that this subset of sentences
already has higher improvement than is seen
in the whole data set simply by choosing the
reordered only, because the SVM is already
used to pick the most discriminating sentences.
We note that on this subset of sentences our
technique of picking the right sentence actu-
ally scores an improvement equal to the use of
reordering by itself.
6.4 Parser Contribution
In Table 9 we show the effects of individual
parsers, taking as the starting point the SVM
of Table 5. Clearly, combining parsers leads to
a much better decider.
Learner Baseline Accuracy
Dutch ? English
SVM Polynomial 50.0% 60.5%
Table 10: Combining internal features with
target side features
Top Reordering Non-reordered
10 9 90% 10 100%
20 18 90% 18 90%
50 33 66% 43 86%
100 61 61% 77 77%
200 114 57% 148 74%
500 289 58% 383 76%
1000 564 56% 748 75%
Table 11: Accuracy of the Combined model
6.5 Combining Models
As the results of classifying translation outputs
using features derived from multiple parsers
are promising, we next look at whether it
is useful to combine this information with
the language and alignment model information
from Section 4. Remarkably, as can be seen in
Table 10, the combination of these two fea-
tures has a much greater effect that the two
features sets individually. Comparing these
scores against 80% accuracy achieved in dis-
tinguish MT output from human output in the
work of Corston-Oliver et al (2001), this 60%
on a dataset with much more subtle differences
is quite promising.
Furthermore Table 11 shows the accuracy
ranking of the SVM for the combining model
for the extreme SVM-predictions, similar to
Tables 4 and 7. The last column of Table 11
matches previous tables, but now we also show
an improvement in correct prediction for the
reordered cases.
7 Conclusion
In this paper we have looked at a restricted
MEMT scenario, where we choose between
a syntactic-reordering-as-preprocessing trans-
lation candidate, in the style of (Collins et
al., 2005), and a baseline PSMT candidate.
We have shown that using a classifier built
around outputs of multiple parsers, to decide
4
Baseline here is the same baseline from Zwarts and
Dras (2007), which is the parser read-off of the tree.
1159
whether to back off to the baseline candidate,
can be successful in selecting the right candi-
date. There is no indication that classifying
information on the source side?looking to see
whether sentences with difficult or incorrect
parses could lead to bad reorderings and hence
bad translations?is useful; however, applying
such a classifier to the target side?looking to
see whether the output quality could be mea-
sured in a syntactically informed way, look-
ing for syntactic abnormalities?is successful
in detecting particularly bad translation can-
didates, and leads to an improvement in Bleu
score over the reordered translations equal to
the improvement gained by the reordering ap-
proach over the baseline. Multiple parsers
clearly improve the results over single parsers.
The target-side classifier can also be usefully
combined with language and alignment model
features, improving its accuracy substantially;
continuing with such an approach looks like a
promising direction. As a further step, the re-
sults are sufficiently positive to extend to other
sorts of syntactically informed SMT.
References
Akiba, Yasuhrio, Taro Watanabe, and Eiichiro Sumita.
2002. Using Language and Translation Models to
Select the Best among Outputs from Multiple MT
systems. In Proc. of Coling, pages 8?14.
Albrecht, Joshua S. and Rebecca Hwa. 2007. Regres-
sion for Sentence-Level MT Evaluation. In Proc. of
ACL, pages 296?303.
Bouma, Gosse, Gertjan van Noord, and Robert Mal-
ouf. 2000. Alpino: Wide Coverage Computational
Analysis of Dutch. In Computational Linguistics in
the Netherlands (CLIN).
Callison-Burch, Chris and Raymond S. Flournoy. 2001.
A Program for Automatically Selecting the Best
Output from Multiple Machine Translation Engines.
In Proc. MT Summit, pages 63?66.
Callison-Burch, Chris, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the Role of Bleu in
Machine Translation Research. In Proc. of EACL,
pages 249?256.
Charniak, Eugene, Kevin Knight, and Kenju Yamada.
2003. Syntax-based Language Models for Statistical
Machine Translation. In Proc. of MT Summit, pages
40?46.
Collins, Michael, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540, Ann
Arbor, Michigan, June.
Collins, Michael. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Corston-Oliver, Simon, Michael Gamon, and Chris
Brockett. 2001. A machine learning approach to
the automatic evaluation of machine translation. In
Proc. of ACL, pages 148?155.
Eisele, Andreas. 2005. First steps towards multi-engine
machine translation. In Proc. of the ACL Workshop
on Building and Using Parallel Texts, pages 155?158.
Frederking, Robert and Sergei Nirenburg. 1994. Three
Heads are Better than One. In Proc. of the ACL
Conference on Applied Natural Language Processing,
pages 95 ? 100.
Grinberg, Dennis, John Lafferty, and Daniel Sleator.
1995. A robust parsing algorithm for link grammars.
In Proc. of the International Workshop on Parsing
Technologies.
Huang, Fei and Kishore Papineni. 2007. Hierarchical
System Combination for Machine Translation. In
Proc. of EMNLP, pages 277?286.
Joachims, T. 1998. Making large-scale support vec-
tor machine learning practical. In B. Scho?lkopf,
C. Burges, A. Smola, editor, Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
Koehn, Philipp. 2003. Europarl: A Multilingual Cor-
pus for Evaluation of Machine Translation Philipp
Koehn, Draft, Unpublished.
Koehn, Philipp. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proc. of AMTA, pages 115?124.
Manning, Christopher and Dan Klein. 2003. Optimiza-
tion, Maxent Models, and Conditional Estimation
without Magic. Tutorial at HLT-NAACL 2003 and
ACL 2003.
Mutton, Andrew, Mark Dras, Stephan Wan, and
Robert Dale. 2007. Gleu: Automatic evaluation of
sentence-level fluency. In Proc of ACL, pages 344?
351.
Nomoto, Tadashi. 2003. Predictive Models of Per-
formance in Multi-Engine Machine Translation. In
Proc. of MT Summit, pages 269?276.
Nomoto, Tadashi. 2004. Multi-Engine Machine Trans-
lation with Voted Language Model. In Proc. of ACL,
pages 494?501.
Och, Franz Josef, Daniel Gildea, Sanjeev Khundanpur,
Anoop Sarkar, Kenju Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jainand Zhen Jin, and Dragomir Radev. 2003.
Final report of Johns Hopkins 2003 summer work-
shop on syntax for statistial machine translation.
Riezler, Stefan and John Maxwell, III. 2006. Gram-
matical machine translation. In Proc of NAACL,
pages 248?255.
Rosti, Antti-Veikko, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and B Bonnie
J. Dorr. 2007. Combining Outputs from Multiple
Machine Translation Systems? in Human Language.
In Proc. of NAACL, pages 228?235.
Wang, Chao, Michael Collins, and Philipp Koehn.
2007. Chinese Syntactic Reordering for Statistical
Machine Translation. In Proc of EMNLP, pages
737?745.
Zwarts, Simon and Mark Dras. 2006. This Phrase-
Based SMT System is Out of Order: Generalised
Word Reordering in Machine Translation. In Proc.
of the Australasian Language Technology Workshop,
pages 149?156.
Zwarts, Simon and Mark Dras. 2007. Syntax-Based
Word Reordering in Phrase-Based Statistical Ma-
chine Translation: Why Does it Work? In Proc.
of MT Summit, pages 559?566.
1160
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1371?1378,
Beijing, August 2010
Detecting Speech Repairs Incrementally
Using a Noisy Channel Approach
Simon Zwarts, Mark Johnson and Robert Dale
Centre for Language Technology
Macquarie University
{simon.zwarts|mark.johnson|robert.dale}@mq.edu.au
Abstract
Unrehearsed spoken language often
contains disfluencies. In order to cor-
rectly interpret a spoken utterance,
any such disfluencies must be identi-
fied and removed or otherwise dealt
with. Operating on transcripts of
speech which contain disfluencies, our
particular focus here is the identifica-
tion and correction of speech repairs
using a noisy channel model. Our aim
is to develop a high-accuracy mecha-
nism that can identify speech repairs
in an incremental fashion, as the ut-
terance is processed word-by-word.
We also address the issue of the evalu-
ation of such incremental systems. We
propose a novel approach to evalua-
tion, which evaluates performance in
detecting and correcting disfluencies
incrementally, rather than only assess-
ing performance once the processing of
an utterance is complete. This demon-
strates some shortcomings in our ba-
sic incremental model, and so we then
demonstrate a technique that improves
performance on the detection of disflu-
encies as they happen.
1 Introduction
One of the most obvious differences between
written language and spoken language is the
fact that the latter presents itself incremen-
tally over some time period. Most natural lan-
guage processing applications operate on com-
plete sentences; but for real time spontaneous
speech, there are potential benefits to incre-
mentally processing the input so that a system
can stay responsive and interact directly be-
fore a speaker?s utterance is complete. Work
in psycholinguistics supports the view that the
human parsing mechanism works incremen-
tally, with partial semantic interpretations be-
ing produced before the complete utterance
has been heard (Marslen-Wilson, 1973). Our
interest is in developing similarly incremental
processing techniques for natural language in-
terpretation, so that, for example, a speech
recognizer might be able to interject during
a long utterance to object, cut the speaker
short, or correct a mistaken assumption; such
a mechanism is even required for the appro-
priate timing of backchannel signals. Addi-
tionally the incremental nature of the model
allows potential application of this model in
speech recognition models.
Another feature of unrehearsed spoken lan-
guage that has no obvious correlate in written
language is the presence of disfluencies.1 Dis-
fluencies are of different types, ranging from
simple filled pauses (such as um and uh) to
more complicated structures where the se-
quence of words that make up the utterance is
?repaired? while it is being produced. Whereas
simpler disfluencies may be handled by sim-
ply deleting them from the sequence of words
under consideration, the editing terms in a
speech repair are part of the utterance, and
therefore require more sophisticated process-
ing.
There are three innovations in the present
paper. First, we demonstrate that a noisy
channel model of speech repairs can work ac-
curately in an incremental fashion. Second,
we provide an approach to the evaluation of
1Although some disfluencies can be considered
grammatical errors, they are generally quite distinct
in both cause and nature from the kinds of grammat-
ical errors found in written text.
1371
such an incremental model. Third, we tackle
the problem of the early detection of speech
repairs, and demonstrate a technique that de-
creases the latency (as measured in tokens)
involved in spotting that a disfluency has oc-
curred.
The rest of the paper is structured as fol-
lows. Section 2 provides some background
on speech repairs and existing approaches to
handling them, including Johnson and Char-
niak?s (2004) model, which we use as a start-
ing point for our incremental model. Section
3 describes our model in detail, focusing on
the noisy channel model and the incremental
component of this model. Section 4 introduces
some considerations that arise in the develop-
ment of techniques for the evaluation of in-
cremental disfluency detection; we then pro-
vide a quantitative assessment of our perfor-
mance using these techniques. Our evaluation
reveals that our basic incremental model does
not perform very well at detecting disfluencies
close to where they happen, so in Section 5 we
present a novel approach to optimise detection
of these disfluencies as early as possible. Fi-
nally Section 6 concludes and discusses future
work.
2 Speech Repairs
We adopt the terminology and definitions in-
troduced by Shriberg (1994) to discuss disflu-
ency. We are particularly interested in what
are called repairs. These are the hardest
types of disfluency to identify since they are
not marked by a characteristic vocabulary.
Shriberg (1994) identifies and defines three
distinct parts of a repair, referred to as the
reparandum, the interregnum and the re-
pair. Consider the following utterance:
I want a flight
reparandum? ?? ?
to Boston,
uh, I mean
? ?? ?
interregnum
to Denver
? ?? ?
repair
on Friday (1)
The reparandum to Boston is the part of the
utterance that is being edited out; the inter-
regnum uh is a filler, which may not always be
present; and the repair to Denver replaces the
reparandum.
Given an utterance that contains such a re-
pair, we want to be able to correctly detect
the start and end positions of each of these
three components. We can think of each word
in an utterance as belonging to one of four
categories: fluent material, reparandum, in-
terregnum, or repair. We can then assess the
accuracy of techniques that attempt to detect
disfluencies by computing precision and recall
values for the assignment of the correct cate-
gories to each of the words in the utterance,
as compared to the gold standard as indicated
by annotations in the corpus.
An alternative means of evaluation would
be to simply generate a new signal with the
reparandum and filler removed, and compare
this against a ?cleaned-up? version of the ut-
terance; however, Core and Schubert (1999)
argue that, especially in the case of speech
repairs, it is important not to simply throw
away the disfluent elements of an utterance,
since they can carry meaning that needs to
be recovered for proper interpretation of the
utterance. We are therefore interested in the
first instance in a model of speech error detec-
tion, rather than a model of correction.
Johnson and Charniak (2004) describe such
a model, using a noisy-channel based approach
to the detection of the start and end points of
reparanda, interregna and repairs. Since we
use this model as our starting point, we pro-
vide a more detailed explanation in Section 3.
The idea of using a noisy channel model
to identify speech repairs has been explored
for languages other than English. Honal and
Schultz (2003) use such a model, compar-
ing speech disfluency detection in spontaneous
spoken Mandarin against that in English. The
approach performs well in Mandarin, although
better still in English.
Both the models just described operate on
transcripts of completed utterances. Ideally,
however, when we deal with speech we would
like to process the input word by word as it is
received. Being able to do this would enable
tighter integration in both speech recognition
1372
and interpretation, which might in turn im-
prove overall accuracy.
The requirement for incrementality is recog-
nised by Schuler et al (2010), who employ
an incremental Hierarchical Hidden Markov
Model (HHMM) to detect speech disfluen-
cies. The HHMM is trained on manually an-
notated parse trees which are transformed by
a right corner transformation; the HHMM is
then used in an incremental fashion on un-
seen data, growing the parse structure each
time a new token comes in. Special subtrees
in this parse can carry a marker indicating
that the span of the subtree consists of tokens
corresponding to a speech disfluency. Schuler
et al?s approach thus provides scope for de-
tecting disfluencies in an incremental fashion.
However, their reported accuracy scores are
not as good as those of Johnson and Char-
niak (2004): they report an F-score of 0.690
for their HHMM+RCT model, as compared
to 0.797 for Johnson and Charniak?s parser
model.
Our aim in this paper, then, is to investigate
whether it is possible to adapt Johnson and
Charniak?s model to process utterances incre-
mentally, without any loss of accuracy. To
define the incremental component more pre-
cisely, we investigate the possibility of mark-
ing the disfluencies as soon as possible during
the processing of the input. Given two models
that provide comparable accuracy measured
on utterance completion, we would prefer a
model which detects disfluencies earlier.
3 The Model
In this section, we describe Johnson and Char-
niak?s (2004) noisy channel model, and show
how this model can be made incremental.
As a data set to work with, we use the
Switchboard part of the Penn Treebank 3 cor-
pus. The Switchboard corpus is a corpus of
spontaneous conversations between two par-
ties. In Penn Treebank 3, the disfluencies are
manually annotated. Following Johnson and
Charniak (2004), we use all of sections 2 and
3 for training; we use conversations 4[5-9]* for
a held-out training set; and conversations 40*,
41[0-4]* and 415[0-3]* as the held-out test set.
3.1 The Noisy Channel Model
To find the repair disfluencies a noisy channel
model is used. For an observed utterance with
disfluencies, y, we wish to find the most likely
source utterance, x?, where:
x? = argmaxx p(x | y) (2)
= argmaxx p(y | x) p(x)
Here we have a channel model p(y|x) which
generates an utterance y given a source x and
a language model p(x). We assume that x
is a substring of y, i.e., the source utterance
can be obtained by marking words in y as a
disfluency and effectively removing them from
this utterance.
Johnson and Charniak (2004) experiment
with variations on the language model; they
report results for a bigram model, a trigram
model, and a language model using the Char-
niak Parser (Charniak, 2001). Their parser
model outperforms the bigram model by 5%.
The channel model is based on the intuition
that a reparandum and a repair are generally
very alike: a repair is often almost a copy of
the reparandum. In the training data, over
60% of the words in a reparandum are lexically
identical to the words in the repair. Exam-
ple 1 provides an example of this: half of the
repair is lexically identical to the reparandum.
The channel model therefore gives the high-
est probability when the reparandum and re-
pair are lexically equivalent. When the poten-
tial reparandum and potential repair are not
identical, the channel model performs dele-
tion, insertion or substitution. The proba-
bilities for these operations are defined on a
lexical level and are derived from the training
data. This channel model is formalised us-
ing a Synchronous Tree Adjoining Grammar
(STAG) (Shieber and Schabes, 1990), which
matches words from the reparandum to the
repair. The weights for these STAG rules are
learnt from the training text, where reparanda
and repairs are aligned to each other using a
minimum edit-distance string aligner.
1373
For a given utterance, every possible ut-
terance position might be the start of a
reparandum, and every given utterance po-
sition thereafter might be the start of a re-
pair (to limit complexity, a maximum distance
between these two points is imposed). Ev-
ery disfluency in turn can have an arbitrary
length (again up to some maximum to limit
complexity). After every possible disfluency
other new reparanda and repairs might occur;
the model does not attempt to generate cross-
ing or nested disfluencies, although they do
very occasionally occur in practice. To find
the optimal selection for reparanda and re-
pairs, all possibilities are calculated and the
one with the highest probability is selected.
A chart is filled with all the possible start
and end positions of reparanda, interregna
and repairs; each entry consists of a tuple
?rmbegin, irbegin, rrbegin, rrend?, where rm is the
reparandum, ir is the interregnum and rr is
the repair. A Viterbi algorithm is used to find
the optimal path through the utterance, rank-
ing each chart entry using the language model
and channel model. The language model, a
bigram model, can be easily calculated given
the start and end positions of all disfluency
components. The channel model is slightly
more complicated because an optimal align-
ment between reparandum and repair needs
to be calculated. This is done by extending
each partial analysis by adding a word to the
reparandum, the repair or both. The start po-
sition and end position of the reparandum and
repair are given for this particular entry. The
task of the channel model is to calculate the
highest probable alignment between reparan-
dum and repair. This is done by initialising
with an empty reparandum and repair, and
?growing? the analysis one word at a time. Us-
ing a similar approach to that used in calculat-
ing the edit-distance between reparandum and
repair, the reparandum and repair can both be
extended with one of four operations: deletion
(only the reparandum grows), insertion (only
the repair grows), substitution (both grow),
or copy (both grow). When the reparandum
and the repair have their length correspond-
ing to the current entry in the chart, the chan-
nel probability can be calculated. Since there
are multiple alignment possibilities, we use dy-
namic programming to select the most proba-
ble solutions. The probabilities for insertion,
deletion or substitution are estimated from
the training corpus. We use a beam-search
strategy to find the final optimum when com-
bining the channel model and the language
model.
3.2 Incrementality
Taking Johnson and Charniak?s model as a
starting point, we would like to develop an in-
cremental version of that algorithm. We sim-
ulate incrementality by maintaining for each
utterance to be processed an end-of-prefix
boundary; tokens after this boundary are
not available for the model to use. At each
step in our incremental model, we advance this
boundary by one token (the increment), un-
til finally the entire utterance is available. We
make use of the notion of a prefix, which is
a substring of the utterance consisting of all
tokens up to this boundary marker.
Just as in the non-incremental model, we
keep track of all the possible reparanda and re-
pairs in a chart. Every time the end-of-prefix
boundary advances, we update the chart: we
add all possible disfluencies which have the
end position of the repair located one token
before the end-of-prefix boundary, and we add
all possible start points for the reparandum,
interregna and repair, and end points for the
reparandum and interregna, given the order-
ing constraints of these components.
In our basic incremental model, we leave the
remainder of the algorithm untouched. When
the end-of-prefix boundary reaches the end of
the utterance, and thus the entire utterance
is available, this model results in an iden-
tical analysis to that provided by the non-
incremental model, since the chart contains
identical entries, although calculated in a dif-
ferent order. Intuitively, this model should
perform well when the current prefix is very
close to being a complete utterance; and it
should perform less well when a potential dis-
1374
fluency is still under construction, since these
situations are not typically found in the train-
ing data. We will return to this point further
below.
We do not change the training phase of the
model and we assume that the optimal values
found for the non-incremental model are also
optimal for the incremental model, since most
weights which need to be learned are based on
lexical values. Other weights are bigram based
values, and values dealing with unknown to-
kens (i.e., tokens which occur in the test data,
but not in the training data); it is not unrea-
sonable to assume these weights are identical
or very similar in both the incremental and
the non-incremental model.
4 Evaluation Models and Their
Application
As well as evaluating the accuracy of the anal-
ysis returned at the end of the utterance, it
seems reasonable to also evaluate how quickly
and accurately an incremental algorithm de-
tects disfluencies on a word-by-word basis as
the utterance is processed. In this section, we
provide the methodological background to our
approach, and in Section 5.2 we discuss the
performance of our model when evaluated in
this way.
Incremental systems are often judged solely
on the basis of their output when the utter-
ance being processed is completed. Although
this does give an insight into how well a system
performs overall, it does not indicate how well
the incremental aspects of the mechanism per-
form. In this section we present an approach
to the evaluation of a model of speech repair
detection which measures the performance of
the incremental component.
One might calculate the accuracy over all
prefixes using a simple word accuracy score.
However, because each prefix is a superstring
of each previous prefix, such a calculation
would not be fair: tokens that appear in early
in the utterance will be counted more often
than tokens that appear later in the utterance.
In theory, the analysis of the early tokens can
change at each prefix, so arguably it would
make sense to reevaluate the complete analy-
sis so far at every step. In practice, however,
these changes do not happen, and so this mea-
surement would not reflect the performance of
the system correctly.
Our approach is to define a measure of re-
sponsiveness: that is, how soon is a dis-
fluency detected? We propose to measure
responsiveness in two ways. The time-to-
detection score indicates how many tokens
following a disfluency are read before the given
disfluency is marked as one; the delayed ac-
curacy score looks n tokens back from the
boundary of the available utterance and, when
there is a gold standard disfluency-marked to-
ken at that distance, counts how often these
tokens are marked correctly.
We measure the time-to-detection score by
two numbers, corresponding to the number of
tokens from the start of the reparandum and
the number of tokens from the start of the re-
pair. We do this because disfluencies can be of
different lengths. We assume it is unlikely that
a disfluency will be found before the reparan-
dum is completed, since the reparandum it-
self is often fluent. We measure the time-to-
detection by the first time a given disfluency
appears as one.
Since the model is a statistical model, it
is possible that the most probable analysis
marks a given word at position j as a disflu-
ency, while in the next prefix the word in the
same position is now no longer marked as be-
ing disfluent. A prefix later this word might
be marked as disfluent again. This presents
us with a problem. How do we measure when
this word was correctly identified as disfluent:
the first time it was marked as such or the sec-
ond time? Because of the possibility of such
oscillations, we take the first marking of the
disfluency as the measure point. Disfluencies
which are never correctly detected are not part
of the time-to-detection score.
Since the evaluation starts with disfluencies
found by the model, this measurement has
precision-like properties only. Consequently,
there are easy ways to inflate the score arti-
ficially at the cost of recall. We address this
1375
by also calculating the delayed accuracy. This
is calculated at each prefix by looking back n
tokens from the prefix boundary, where n = 0
for the prefix boundary. For each n we cal-
culate the accuracy score at that point over
all prefixes. Each token is only assessed once
given a set value of n, so we do not suffer
from early prefixes being assessed more often.
However, larger values of n do not take all to-
kens into account, since the last y tokens of
an utterance will not play a part in the ac-
curacy when y < n. Since we evaluate given
a gold standard disfluency, this measurement
has recall-like properties.
Together with the final accuracy score over
the entire utterance, the time-to-detection
and delayed accuracy scores provide different
insights and together give a good measure-
ment of the responsiveness and performance
of the model.
Our incremental model has the same fi-
nal accuracy as the original non-incremental
model; this corresponds to an F-score (har-
monic mean) of 0.778 on a word basis.
We found the average time to detection,
measured in tokens for this model to be 8.3
measured from the start of reparandum and
5.1 from the start of repair. There are situ-
ations where disfluencies can be detected be-
fore the end of the repair; by counting from
the start rather than the end of the disfluency
components, we provide a way of scoring in
such cases. To provide a better insight into
what is happening, we also report the average
distance since the start of the reparandum.
We find that the time to detect is larger than
the average repair length; this implies that,
under this particular model, most disfluencies
are only detected after the repair is finished.
In fact the difference is greater than 1, which
means that in most cases it takes one more to-
ken after the repair before the model identifies
the disfluency.
Table 1 shows the delayed accuracy. We can
see that the score first rises quickly after which
the increases become much smaller. As men-
tioned above, a given disfluency detection in
theory might oscillate. In practice, however,
oscillating disfluencies are very rare, possibly
because a bigram model operates on a very lo-
cal level. Given that oscillation is rare, a quick
stabilisation of the score indicates that, when
we correctly detect a disfluency, this happens
rather quickly after the disfluency has com-
pleted, since the accuracy for the large n is
calculated over the same tokens as the accu-
racy for the smaller n (although not in the
same prefix).
5 Disfluencies around Prefix
Boundaries
5.1 Early detection algorithm
Our model uses a language model and a chan-
nel model to locate disfluencies. It calculates
a language model probability for the utterance
with the disfluency taken out, and it calculates
the probability of the disfluency itself with the
STAG channel model.
Consider the following example utterance
fragment where a repair disfluency occurs:
. . . wi
reparandum? ?? ?rni+1 rni+2
repair? ?? ?rri+3 rri+4 wi+5 . . . (3)
Here, the subscripts indicate token position in
sequence; w is a token outside the disfluency;
and rn is a reparandum being repaired by
the repair rr. The language model estimates
the continuation of the utterance without the
disfluency. The model considers whether the
utterance continuation after the disfluency is
probable given the language model; the rel-
evant bigram here is p(rri+3|wi), continuing
with p(rri+4|rri+3). However, under the in-
cremental model, it is possible the utterance
has only been read as far as token i + 3, in
which case the probability p(wi+4|wi+3) is un-
defined.
We would like to address the issue of look-
ing beyond a disfluency under construction.
We assume the issue of not being able to look
for an utterance continuation after the repair
component of the disfluency can be found back
in the incremental model scores. A disfluency
is usually only detected after the disfluency is
completely uttered, and always requires one
1376
n tokens back 1 2 3 4 5 6
accuracy 0.500 0.558 0.631 0.665 0.701 0.714
Table 1: delayed accuracy, n tokens back from the end of prefixes
n tokens back 1 2 3 4 5 6
accuracy 0.578 0.633 0.697 0.725 0.758 0.770
Table 2: delayed accuracy under the updated model
more token in the basic model. In the given
instance this means it is unlikely that we will
detect the disfluency before i + 5.
In order to make our model more respon-
sive, we propose a change which makes it
possible for the model to calculate channel
probabilities and language model probabili-
ties before the repair is completed. Assum-
ing we have not yet reached the end of utter-
ance, we would like to estimate the continua-
tion of the utterance with the relevant bigram
p(rri+4|rri+3). Since rri+4 is not yet avail-
able we cannot calculate this probability. The
correct thing to do is to sum over all possible
continuations, including the end of utterance
token (for the complete utterance, as opposed
to the current prefix). This results in the fol-
lowing bigram estimation:
?
t?vocabulary
p(t|wi) (4)
This estimation is not one we need to derive
from our data set, since p is a true probability.
In this case, the sum over all possible continu-
ations (this might include an end of utterance
marker, in which case the utterance is already
complete) equals 1. We therefore modify the
algorithm so that it takes this into account.
This solves the problem of the language model
assessing the utterance with the disfluency cut
out, when nothing from the utterance contin-
uation after a disfluency is available.
The other issue which needs to be addressed
is the alignment of the reparandum with the
repair when the repair is not yet fully avail-
able. Currently the model is encouraged to
align the individual tokens of the reparandum
with those of the repair. The algorithm has
lower estimations when the reparandum can-
not be fully aligned with the repair because
the reparandum and repair differ considerably
in length.
We note that most disfluencies are very
short: reparanda and repairs are often only
one or two tokens each in length, and the inter-
regnum is often empty. To remove the penalty
for an incomplete repair, we allow the repair to
grow one token beyond the prefix boundary;
given the relative shortness of the disfluencies,
this seems reasonable. Since this token is not
available, we cannot calculate the lexical sub-
stitution value. Instead we define a new opera-
tion in the channel model: in addition to dele-
tion, insertion, copy, and substitution, we add
an additional substitution operation, the in-
cremental completion substitution. This
operation does not compete with the copy op-
eration or the normal substitution operation,
since it is only defined when the last token of
the repair falls at the prefix boundary.
5.2 Results for the Early detection
algorithm
The results of these changes are reflected
in new time-to-detection and delayed accu-
racy scores. Again we calculated the time-
to-detection, and found this to be 7.5 from
the start of reparandum and 4.6 from the
start of repair. Table 2 shows the results un-
der the new early completion model using the
delayed accuracy method. We see that the
updated model has lower time-to-detection
scores (close to a full token earlier); for de-
layed accuracy, we note that the scores sta-
bilise in a similar fashion, but the scores for
the updated model rise slightly more quickly.
1377
6 Conclusions and Future Work
We have demonstrated an incremental model
for finding speech disfluencies in spoken lan-
guage transcripts. When we consider com-
plete utterances, the incremental model pro-
vides identical results to those of a non-
incremental model that delivers state-of-the-
art accuracy in speech repair detection. We
have investigated a number of measures which
allow us to evaluate the model on an incremen-
tal level. Most disfluencies are identified very
quickly, typically one or two tokens after the
disfluency has been completed. We addressed
the problems of the model around the end of
prefix boundaries. These are repairs which are
either still in the process of being uttered or
have just been completed. We have addressed
this issue by making some changes to how the
model deals with prefix boundaries, and we
have shown that this improves the responsive-
ness of the model.
The work reported in this paper uses a n-
gram model as a language model and a STAG
based model for the repair. We would like
to replace the n-gram language model with a
better language model. Previous work (John-
son and Charniak, 2004) has shown that dis-
fluency detection can be improved by replac-
ing the n-gram language model with a statis-
tical parser. Besides a reported 5% accuracy
improvement, this also provides a structural
analysis, something which an n-gram model
does not. We would like to investigate a sim-
ilar extension in our incremental approach,
which will require the integration of an in-
cremental statistical parser with our noisy
channel model. While transcripts of spoken
texts come with manually annotated sentence
boundaries, real time spoken language does
not. The language model in particular takes
these sentence boundaries into account. We
therefore propose to investigate the proper-
ties of this model when sentence boundaries
are removed.
Acknowledgements
This work was supported by the Australian
Research Council as part of the Thinking
Head Project, ARC/NHMRC Special Re-
search Initiative Grant # TS0669874. We
thank the anonymous reviewers for their help-
ful comments.
References
Charniak, Eugene. 2001. Immediate-head pars-
ing for language models. In Proceedings of the
39th Annual Meeting on Association for Com-
putational Linguistics, pages 124?131.
Core, Mark and Lenhart Schubert. 1999. A model
of speech repairs and other disruptions. In
AAAI Fall Symposium on Psychological Mod-
els of Communication in Collaborative Systems,
pages 48?53.
Honal, Matthias and Tanja Schultz. 2003. Correc-
tion of Disfluencies in Spontaneous Speech us-
ing a Noisy-Channel Approach. In Proceedings
of the 8th Eurospeech Conference.
Johnson, Mark and Eugene Charniak. 2004. A
tag-based noisy channel model of speech repairs.
In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics,
pages 33?39.
Marslen-Wilson, W. 1973. Linguistic structure
and speech shadowing at very short latencies.
Nature, 244:522?533.
Schuler, William, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2010. Broad-
Coverage Parsing using Human-Like Mem-
ory Constraints. Computational Linguistics,
36(1):1?30.
Shieber, Stuart M. and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proceed-
ings of the 13th International Conference on
Computational Linguistics, pages 253?258.
Shriberg, Elizabeth. 1994. Preliminaries to a
Theory of Speech Disuencies. Ph.D. thesis, Uni-
versity of California, Berkeley.
1378
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 703?711,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
The impact of language models and loss functions on repair disfluency
detection
Simon Zwarts and Mark Johnson
Centre for Language Technology
Macquarie University
{simon.zwarts|mark.johnson|}@mq.edu.au
Abstract
Unrehearsed spoken language often contains
disfluencies. In order to correctly inter-
pret a spoken utterance, any such disfluen-
cies must be identified and removed or other-
wise dealt with. Operating on transcripts of
speech which contain disfluencies, we study
the effect of language model and loss func-
tion on the performance of a linear reranker
that rescores the 25-best output of a noisy-
channel model. We show that language mod-
els trained on large amounts of non-speech
data improve performance more than a lan-
guage model trained on a more modest amount
of speech data, and that optimising f-score
rather than log loss improves disfluency detec-
tion performance.
Our approach uses a log-linear reranker, oper-
ating on the top n analyses of a noisy chan-
nel model. We use large language models,
introduce new features into this reranker and
examine different optimisation strategies. We
obtain a disfluency detection f-scores of 0.838
which improves upon the current state-of-the-
art.
1 Introduction
Most spontaneous speech contains disfluencies such
as partial words, filled pauses (e.g., ?uh?, ?um?,
?huh?), explicit editing terms (e.g., ?I mean?), par-
enthetical asides and repairs. Of these, repairs
pose particularly difficult problems for parsing and
related Natural Language Processing (NLP) tasks.
This paper presents a model of disfluency detec-
tion based on the noisy channel framework, which
specifically targets the repair disfluencies. By com-
bining language models and using an appropriate
loss function in a log-linear reranker we are able to
achieve f-scores which are higher than previously re-
ported.
Often in natural language processing algorithms,
more data is more important than better algorithms
(Brill and Banko, 2001). It is this insight that drives
the first part of the work described in this paper. This
paper investigates how we can use language models
trained on large corpora to increase repair detection
accuracy performance.
There are three main innovations in this paper.
First, we investigate the use of a variety of language
models trained from text or speech corpora of vari-
ous genres and sizes. The largest available language
models are based on written text: we investigate the
effect of written text language models as opposed to
language models based on speech transcripts. Sec-
ond, we develop a new set of reranker features ex-
plicitly designed to capture important properties of
speech repairs. Many of these features are lexically
grounded and provide a large performance increase.
Third, we utilise a loss function, approximate ex-
pected f-score, that explicitly targets the asymmetric
evaluation metrics used in the disfluency detection
task. We explain how to optimise this loss func-
tion, and show that this leads to a marked improve-
ment in disfluency detection. This is consistent with
Jansche (2005) and Smith and Eisner (2006), who
observed similar improvements when using approx-
imate f-score loss for other problems. Similarly we
introduce a loss function based on the edit-f-score in
our domain.703
Together, these three improvements are enough to
boost detection performance to a higher f-score than
previously reported in literature. Zhang et al (2006)
investigate the use of ?ultra large feature spaces? as
an aid for disfluency detection. Using over 19 mil-
lion features, they report a final f-score in this task of
0.820. Operating on the same body of text (Switch-
board), our work leads to an f-score of 0.838, this is
a 9% relative improvement in residual f-score.
The remainder of this paper is structured as fol-
lows. First in Section 2 we describe related work.
Then in Section 3 we present some background on
disfluencies and their structure. Section 4 describes
appropriate evaluation techniques. In Section 5 we
describe the noisy channel model we are using. The
next three sections describe the new additions: Sec-
tion 6 describe the corpora used for language mod-
els, Section 7 describes features used in the log-
linear model employed by the reranker and Section 8
describes appropriate loss functions which are criti-
cal for our approach. We evaluate the new model in
Section 9. Section 10 draws up a conclusion.
2 Related work
A number of different techniques have been pro-
posed for automatic disfluency detection. Schuler
et al (2010) propose a Hierarchical Hidden Markov
Model approach; this is a statistical approach which
builds up a syntactic analysis of the sentence and
marks those subtrees which it considers to be made
up of disfluent material. Although they are inter-
ested not only in disfluency but also a syntactic anal-
ysis of the utterance, including the disfluencies be-
ing analysed, their model?s final f-score for disflu-
ency detection is lower than that of other models.
Snover et al (2004) investigate the use of purely
lexical features combined with part-of-speech tags
to detect disfluencies. This approach is compared to
approaches which use primarily prosodic cues, and
appears to perform equally well. However, the au-
thors note that this model finds it difficult to identify
disfluencies which by themselves are very fluent. As
we will see later, the individual components of a dis-
fluency do not have to be disfluent by themselves.
This can occur when a speaker edits her speech for
meaning-related reasons, rather than errors that arise
from performance. The edit repairs which are the fo-
cus of our work typically have this characteristic.
Noisy channel models have done well on the dis-
fluency detection task in the past; the work of John-
son and Charniak (2004) first explores such an ap-
proach. Johnson et al (2004) adds some hand-
written rules to the noisy channel model and use a
maximum entropy approach, providing results com-
parable to Zhang et al (2006), which are state-of-the
art results.
Kahn et al (2005) investigated the role of
prosodic cues in disfluency detection, although the
main focus of their work was accurately recovering
and parsing a fluent version of the sentence. They
report a 0.782 f-score for disfluency detection.
3 Speech Disfluencies
We follow the definitions of Shriberg (1994) regard-
ing speech disfluencies. She identifies and defines
three distinct parts of a speech disfluency, referred
to as the reparandum, the interregnum and the re-
pair. Consider the following utterance:
I want a flight
reparandum? ?? ?
to Boston,
uh, I mean
? ?? ?
interregnum
to Denver
? ?? ?
repair
on Friday
(1)
The reparandum to Boston is the part of the utterance
that is ?edited out?; the interregnum uh, I mean is a
filled pause, which need not always be present; and
the repair to Denver replaces the reparandum.
Shriberg and Stolcke (1998) studied the location
and distribution of repairs in the Switchboard cor-
pus (Godfrey and Holliman, 1997), the primary cor-
pus for speech disfluency research, but did not pro-
pose an actual model of repairs. They found that the
overall distribution of speech disfluencies in a large
corpus can be fit well by a model that uses only in-
formation on a very local level. Our model, as ex-
plained in section 5, follows from this observation.
As our domain of interest we use the Switchboard
corpus. This is a large corpus consisting of tran-
scribed telephone conversations between two part-
ners. In the Treebank III (Marcus et al, 1999) cor-
pus there is annotation available for the Switchboard
corpus, which annotates which parts of utterances
are in a reparandum, interregnum or repair.704
4 Evaluation metrics for disfluency
detection systems
Disfluency detection systems like the one described
here identify a subset of the word tokens in each
transcribed utterance as ?edited? or disfluent. Per-
haps the simplest way to evaluate such systems is
to calculate the accuracy of labelling they produce,
i.e., the fraction of words that are correctly labelled
(i.e., either ?edited? or ?not edited?). However,
as Charniak and Johnson (2001) observe, because
only 5.9% of words in the Switchboard corpus are
?edited?, the trivial baseline classifier which assigns
all words the ?not edited? label achieves a labelling
accuracy of 94.1%.
Because the labelling accuracy of the trivial base-
line classifier is so high, it is standard to use a dif-
ferent evaluation metric that focuses more on the de-
tection of ?edited? words. We follow Charniak and
Johnson (2001) and report the f-score of our disflu-
ency detection system. The f-score f is:
f =
2c
g + e
(2)
where g is the number of ?edited? words in the gold
test corpus, e is the number of ?edited? words pro-
posed by the system on that corpus, and c is the num-
ber of the ?edited? words proposed by the system
that are in fact correct. A perfect classifier which
correctly labels every word achieves an f-score of
1, while the trivial baseline classifiers which label
every word as ?edited? or ?not edited? respectively
achieve a very low f-score.
Informally, the f-score metric focuses more on
the ?edited? words than it does on the ?not edited?
words. As we will see in section 8, this has implica-
tions for the choice of loss function used to train the
classifier.
5 Noisy Channel Model
Following Johnson and Charniak (2004), we use a
noisy channel model to propose a 25-best list of
possible speech disfluency analyses. The choice of
this model is driven by the observation that the re-
pairs frequently seem to be a ?rough copy? of the
reparandum, often incorporating the same or very
similar words in roughly the same word order. That
is, they seem to involve ?crossed? dependencies be-
tween the reparandum and the repair. Example (3)
shows the crossing dependencies. As this exam-
ple also shows, the repair often contains many of
the same words that appear in the reparandum. In
fact, in our Switchboard training corpus we found
that 62reparandum also appeared in the associated
repair,
to Boston uh, I mean, to Denver? ?? ?
reparandum
? ?? ?
interregnum
? ?? ?
repair
(3)
5.1 Informal Description
Given an observed sentence Y we wish to find the
most likely source sentence X? , where
X? = argmax
X
P (Y |X)P (X) (4)
In our model the unobserved X is a substring of the
complete utterance Y .
Noisy-channel models are used in a similar way
in statistical speech recognition and machine trans-
lation. The language model assigns a probability
P (X) to the string X , which is a substring of the
observed utterance Y . The channel model P (Y |X)
generates the utterance Y , which is a potentially dis-
fluent version of the source sentence X . A repair
can potentially begin before any word of X . When
a repair has begun, the channel model incrementally
processes the succeeding words from the start of the
repair. Before each succeeding word either the re-
pair can end or else a sequence of words can be in-
serted in the reparandum. At the end of each re-
pair, a (possibly null) interregnum is appended to the
reparandum.
We will look at these two components in the next
two Sections in more detail.
5.2 Language Model
Informally, the task of language model component
of the noisy channel model is to assess fluency of
the sentence with disfluency removed. Ideally we
would like to have a model which assigns a very
high probability to disfluency-free utterances and a
lower probability to utterances still containing dis-
fluencies. For computational complexity reasons, as
described in the next section, inside the noisy chan-
nel model we use a bigram language model. This705
bigram language model is trained on the fluent ver-
sion of the Switchboard corpus (training section).
We realise that a bigram model might not be able
to capture more complex language behaviour. This
motivates our investigation of a range of additional
language models, which are used to define features
used in the log-linear reranker as described below.
5.3 Channel Model
The intuition motivating the channel model design
is that the words inserted into the reparandum are
very closely related to those in the repair. Indeed,
in our training data we find that 62% of the words
in the reparandum are exact copies of words in the
repair; this identity is strong evidence of a repair.
The channel model is designed so that exact copy
reparandum words will have high probability.
Because these repair structures can involve an un-
bounded number of crossed dependencies, they can-
not be described by a context-free or finite-state
grammar. This motivates the use of a more expres-
sive formalism to describe these repair structures.
We assume that X is a substring of Y , i.e., that the
source sentence can be obtained by deleting words
from Y , so for a fixed observed utterance Y there
are only a finite number of possible source sen-
tences. However, the number of possible source sen-
tences, X , grows exponentially with the length of Y ,
so exhaustive search is infeasible. Tree Adjoining
Grammars (TAG) provide a systematic way of for-
malising the channel model, and their polynomial-
time dynamic programming parsing algorithms can
be used to search for likely repairs, at least when
used with simple language models like a bigram
language model. In this paper we first identify the
25 most likely analyses of each sentence using the
TAG channel model together with a bigram lan-
guage model.
Further details of the noisy channel model can be
found in Johnson and Charniak (2004).
5.4 Reranker
To improve performance over the standard noisy
channel model we use a reranker, as previously sug-
gest by Johnson and Charniak (2004). We rerank a
25-best list of analyses. This choice is motivated by
an oracle experiment we performed, probing for the
location of the best analysis in a 100-best list. This
experiment shows that in 99.5% of the cases the best
analysis is located within the first 25, and indicates
that an f-score of 0.958 should be achievable as the
upper bound on a model using the first 25 best anal-
yses. We therefore use the top 25 analyses from the
noisy channel model in the remainder of this paper
and use a reranker to choose the most suitable can-
didate among these.
6 Corpora for language modelling
We would like to use additional data to model
the fluent part of spoken language. However, the
Switchboard corpus is one of the largest widely-
available disfluency-annotated speech corpora. It is
reasonable to believe that for effective disfluency de-
tection Switchboard is not large enough and more
text can provide better analyses. Schwartz et al
(1994), although not focusing on disfluency detec-
tion, show that using written language data for mod-
elling spoken language can improve performance.
We turn to three other bodies of text and investi-
gate the use of these corpora for our task, disfluency
detection. We will describe these corpora in detail
here.
The predictions made by several language models
are likely to be strongly correlated, even if the lan-
guage models are trained on different corpora. This
motivates the choice for log-linear learners, which
are built to handle features which are not necessar-
ily independent. We incorporate information from
the external language models by defining a reranker
feature for each external language model. The value
of this feature is the log probability assigned by the
language model to the candidate underlying fluent
substring X
For each of our corpora (including Switchboard)
we built a 4-gram language model with Kneser-Ney
smoothing (Kneser and Ney, 1995). For each analy-
sis we calculate the probability under that language
model for the candidate underlying fluent substring
X . We use this log probability as a feature in the
reranker. We use the SRILM toolkit (Stolcke, 2002)
both for estimating the model from the training cor-
pus as well as for computing the probabilities of the
underlying fluent sentences X of the different anal-
ysis.
As previously described, Switchboard is our pri-706
mary corpus for our model. The language model
part of the noisy channel model already uses a bi-
gram language model based on Switchboard, but in
the reranker we would like to also use 4-grams for
reranking. Directly using Switchboard to build a 4-
gram language model is slightly problematic. When
we use the training data of Switchboard both for lan-
guage fluency prediction and the same training data
also for the loss function, the reranker will overesti-
mate the weight associated with the feature derived
from the Switchboard language model, since the flu-
ent sentence itself is part of the language model
training data. We solve this by dividing the Switch-
board training data into 20 folds. For each fold we
use the 19 other folds to construct a language model
and then score the utterance in this fold with that
language model.
The largest widely-available corpus for language
modelling is the Web 1T 5-gram corpus (Brants and
Franz, 2006). This data set, collected by Google
Inc., contains English word n-grams and their ob-
served frequency counts. Frequency counts are pro-
duced from this billion-token corpus of web text.
Because of the noise1 present in this corpus there is
an ongoing debate in the scientific community of the
use of this corpus for serious language modelling.
The Gigaword Corpus (Graff and Cieri, 2003)
is a large body of newswire text. The corpus con-
tains 1.6 ? 109 tokens, however fluent newswire text
is not necessarily of the same domain as disfluency
removed speech.
The Fisher corpora Part I (David et al, 2004) and
Part II (David et al, 2005) are large bodies of tran-
scribed text. Unlike Switchboard there is no disflu-
ency annotation available for Fisher. Together the
two Fisher corpora consist of 2.2 ? 107 tokens.
7 Features
The log-linear reranker, which rescores the 25-best
lists produced by the noisy-channel model, can
also include additional features besides the noisy-
channel log probabilities. As we show below, these
additional features can make a substantial improve-
ment to disfluency detection performance. Our
reranker incorporates two kinds of features. The first
1We do not mean speech disfluencies here, but noise in web-
text; web-text is often poorly written and unedited text.
are log-probabilities of various scores computed by
the noisy-channel model and the external language
models. We only include features which occur at
least 5 times in our training data.
The noisy channel and language model features
consist of:
1. LMP: 4 features indicating the probabilities of
the underlying fluent sentences under the lan-
guage models, as discussed in the previous sec-
tion.
2. NCLogP: The Log Probability of the entire
noisy channel model. Since by itself the noisy
channel model is already doing a very good job,
we do not want this information to be lost.
3. LogFom: This feature is the log of the ?fig-
ure of merit? used to guide search in the noisy
channel model when it is producing the 25-best
list for the reranker. The log figure of merit is
the sum of the log language model probability
and the log channel model probability plus 1.5
times the number of edits in the sentence. This
feature is redundant, i.e., it is a linear combina-
tion of other features available to the reranker
model: we include it here so the reranker has
direct access to all of the features used by the
noisy channel model.
4. NCTransOdd: We include as a feature parts of
the noisy channel model itself, i.e. the channel
model probability. We do this so that the task
to choosing appropriate weights of the channel
model and language model can be moved from
the noisy channel model to the log-linear opti-
misation algorithm.
The boolean indicator features consist of the fol-
lowing 3 groups of features operating on words and
their edit status; the latter indicated by one of three
possible flags: when the word is not part of a dis-
fluency or E when it is part of the reparandum or I
when it is part of the interregnum.
1. CopyFlags X Y: When there is an exact copy
in the input text of length X (1 ? X ? 3) and
the gap between the copies is Y (0 ? Y ? 3)
this feature is the sequence of flags covering the
two copies. Example: CopyFlags 1 0 (E707
) records a feature when two identical words
are present, directly consecutive and the first
one is part of a disfluency (Edited) while the
second one is not. There are 745 different in-
stances of these features.
2. WordsFlags L n R: This feature records the
immediate area around an n-gram (n ? 3).
L denotes how many flags to the left and R
(0 ? R ? 1) how many to the right are includes
in this feature (Both L and R range over 0 and
1). Example: WordsFlags 1 1 0 (need
) is a feature that fires when a fluent word is
followed by the word ?need? (one flag to the
left, none to the right). There are 256808 of
these features present.
3. SentenceEdgeFlags B L: This feature indi-
cates the location of a disfluency in an ut-
terance. The Boolean B indicates whether
this features records sentence initial or sen-
tence final behaviour, L (1 ? L ? 3)
records the length of the flags. Example
SentenceEdgeFlags 1 1 (I) is a fea-
ture recording whether a sentence ends on an
interregnum. There are 22 of these features
present.
We give the following analysis as an example:
but E but that does n?t work
The language model features are the probability
calculated over the fluent part. NCLogP, Log-
Fom and NCTransOdd are present with their asso-
ciated value. The following binary flags are present:
CopyFlags 1 0 (E )
WordsFlags:0:1:0 (but E)
WordsFlags:0:1:0 (but )
WordsFlags:1:1:0 (E but )
WordsFlags:1:1:0 ( that )
WordsFlags:0:2:0 (but E but ) etc.2
SentenceEdgeFlags:0:1 (E)
SentenceEdgeFlags:0:2 (E )
SentenceEdgeFlags:0:3 (E )
These three kinds of boolean indicator features to-
gether constitute the extended feature set.
2An exhaustive list here would be too verbose.
8 Loss functions for reranker training
We formalise the reranker training procedure as fol-
lows. We are given a training corpus T containing
information about n possibly disfluent sentences.
For the ith sentence T specifies the sequence of
words xi, a set Yi of 25-best candidate ?edited? la-
bellings produced by the noisy channel model, as
well as the correct ?edited? labelling y?i ? Yi.3
We are also given a vector f = (f1, . . . , fm)
of feature functions, where each fj maps a word
sequence x and an ?edit? labelling y for x to a
real value fj(x, y). Abusing notation somewhat,
we write f(x, y) = (f1(x, y), . . . , fm(x, y)). We
interpret a vector w = (w1, . . . , wm) of feature
weights as defining a conditional probability distri-
bution over a candidate set Y of ?edited? labellings
for a string x as follows:
Pw(y | x,Y) =
exp(w ? f(x, y))
?
y??Y exp(w ? f(x, y?))
We estimate the feature weights w from the train-
ing data T by finding a feature weight vector w? that
optimises a regularised objective function:
w? = argmin
w
LT (w) + ?
m?
j=1
w2j
Here ? is the regulariser weight and LT is a loss
function. We investigate two different loss functions
in this paper. LogLoss is the negative log conditional
likelihood of the training data:
LogLossT (w) =
m?
i=1
? log P(y?i | xi,Yi)
Optimising LogLoss finds the w? that define (regu-
larised) conditional Maximum Entropy models.
It turns out that optimising LogLoss yields sub-
optimal weight vectors w? here. LogLoss is a sym-
metric loss function (i.e., each mistake is equally
weighted), while our f-score evaluation metric
weights ?edited? labels more highly, as explained
in section 4. Because our data is so skewed (i.e.,
?edited? words are comparatively infrequent), we
3In the situation where the true ?edited? labelling does not
appear in the 25-best list Yi produced by the noisy-channel
model, we choose y?i to be a labelling in Yi closest to the true
labelling.708
can improve performance by using an asymmetric
loss function.
Inspired by our evaluation metric, we devised an
approximate expected f-score loss function FLoss .
FLossT (w) = 1 ?
2Ew[c]
g + Ew[e]
This approximation assumes that the expectations
approximately distribute over the division: see Jan-
sche (2005) and Smith and Eisner (2006) for other
approximations to expected f-score and methods for
optimising them. We experimented with other asym-
metric loss functions (e.g., the expected error rate)
and found that they gave very similar results.
An advantage of FLoss is that it and its deriva-
tives with respect to w (which are required for
numerical optimisation) are easy to calculate ex-
actly. For example, the expected number of correct
?edited? words is:
Ew[c] =
n?
i=1
Ew[cy?i | Yi], where:
Ew[cy?i | Yi] =
?
y?Yi
cy?i (y) Pw(y | xi,Yi)
and cy?(y) is the number of correct ?edited? labels
in y given the gold labelling y?. The derivatives of
FLoss are:
?FLossT
?wj
(w) =
1
g + Ew[e]
(
FLossT (w)
?Ew[e]
?wj
? 2?Ew[c]
?wj
)
where:
?Ew[c]
?wj
=
n?
i=1
?Ew[cy?i | xi,Yi]
?wj
?Ew[cy? | x,Y]
?wj
=
Ew[fjcy? | x,Y] ? Ew[fj | x,Y] Ew[cy? | x,Y].
?E[e]/?wj is given by a similar formula.
9 Results
We follow Charniak and Johnson (2001) and split
the corpus into main training data, held-out train-
ing data and test data as follows: main training con-
sisted of all sw[23]?.dps files, held-out training con-
sisted of all sw4[5-9]?.dps files and test consisted of
all sw4[0-1]?.dps files. However, we follow (John-
son and Charniak, 2004) in deleting all partial words
and punctuation from the training and test data (they
argued that this is more realistic in a speech process-
ing application).
Table 1 shows the results for the different models
on held-out data. To avoid over-fitting on the test
data, we present the f-scores over held-out training
data instead of test data. We used the held-out data
to select the best-performing set of reranker features,
which consisted of features for all of the language
models plus the extended (i.e., indicator) features,
and used this model to analyse the test data. The f-
score of this model on test data was 0.838. In this
table, the set of Extended Features is defined as all
the boolean features as described in Section 7.
We first observe that adding different external lan-
guage models does increase the final score. The
difference between the external language models is
relatively small, although the differences in choice
are several orders of magnitude. Despite the pu-
tative noise in the corpus, a language model built
on Google?s Web1T data seems to perform very
well. Only the model where Switchboard 4-grams
are used scores slightly lower, we explain this be-
cause the internal bigram model of the noisy chan-
nel model is already trained on Switchboard and so
this model adds less new information to the reranker
than the other models do.
Including additional features to describe the prob-
lem space is very productive. Indeed the best per-
forming model is the model which has all extended
features and all language model features. The dif-
ferences among the different language models when
extended features are present are relatively small.
We assume that much of the information expressed
in the language models overlaps with the lexical fea-
tures.
We find that using a loss function related to our
evaluation metric, rather than optimising LogLoss ,
consistently improves edit-word f-score. The stan-
dard LogLoss function, which estimates the ?max-
imum entropy? model, consistently performs worse
than the loss function minimising expected errors.
The best performing model (Base + Ext. Feat.
+ All LM, using expected f-score loss) scores an f-
score of 0.838 on test data. The results as indicated
by the f-score outperform state-of-the-art models re-709
Model F-score
Base (noisy channel, no reranking) 0.756
Model log loss expected f-score loss
Base + Switchboard 0.776 0.791
Base + Fisher 0.771 0.797
Base + Gigaword 0.777 0.797
Base + Web1T 0.781 0.798
Base + Ext. Feat. 0.824 0.827
Base + Ext. Feat. + Switchboard 0.827 0.828
Base + Ext. Feat. + Fisher 0.841 0.856
Base + Ext. Feat. + Gigaword 0.843 0.852
Base + Ext. Feat. + Web1T 0.843 0.850
Base + Ext. Feat. + All LM 0.841 0.857
Table 1: Edited word detection f-score on held-out data for a variety of language models and loss functions
ported in literature operating on identical data, even
though we use vastly less features than other do.
10 Conclusion and Future work
We have described a disfluency detection algorithm
which we believe improves upon current state-of-
the-art competitors. This model is based on a noisy
channel model which scores putative analyses with
a language model; its channel model is inspired by
the observation that reparandum and repair are of-
ten very similar. As Johnson and Charniak (2004)
noted, although this model performs well, a log-
linear reranker can be used to increase performance.
We built language models from a variety of
speech and non-speech corpora, and examine the ef-
fect they have on disfluency detection. We use lan-
guage models derived from different larger corpora
effectively in a maximum reranker setting. We show
that the actual choice for a language model seems
to be less relevant and newswire text can be used
equally well for modelling fluent speech.
We describe different features to improve disflu-
ency detection even further. Especially these fea-
tures seem to boost performance significantly.
Finally we investigate the effect of different loss
functions. We observe that using a loss function di-
rectly optimising our interest yields a performance
increase which is at least at large as the effect of us-
ing very large language models.
We obtained an f-score which outperforms other
models reported in literature operating on identical
data, even though we use vastly fewer features than
others do.
Acknowledgements
This work was supported was supported under Aus-
tralian Research Council?s Discovery Projects fund-
ing scheme (project number DP110102593) and
by the Australian Research Council as part of the
Thinking Head Project the Thinking Head Project,
ARC/NHMRC Special Research Initiative Grant #
TS0669874. We thank the anonymous reviewers for
their helpful comments.
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Published by Linguistic Data Consortium,
Philadelphia.
Erik Brill and Michele Banko. 2001. Mitigating the
Paucity-of-Data Problem: Exploring the Effect of
Training Corpus Size on Classifier Performance for
Natural Language Processing. In Proceedings of the
First International Conference on Human Language
Technology Research.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118?126.
Christopher Cieri David, David Miller, and Kevin
Walker. 2004. Fisher English Training Speech Part
1 Transcripts. Published by Linguistic Data Consor-
tium, Philadelphia.710
Christopher Cieri David, David Miller, and Kevin
Walker. 2005. Fisher English Training Speech Part
2 Transcripts. Published by Linguistic Data Consor-
tium, Philadelphia.
John J. Godfrey and Edward Holliman. 1997.
Switchboard-1 Release 2. Published by Linguistic
Data Consortium, Philadelphia.
David Graff and Christopher Cieri. 2003. English gi-
gaword. Published by Linguistic Data Consortium,
Philadelphia.
Martin Jansche. 2005. Maximum Expected F-Measure
Training of Logistic Regression Models. In Proceed-
ings of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 692?699, Vancouver, British
Columbia, Canada, October. Association for Compu-
tational Linguistics.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy channel model of speech repairs. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 33?39.
Mark Johnson, Eugene Charniak, and Matthew Lease.
2004. An Improved Model for Recognizing Disfluen-
cies in Conversational Speech. In Proceedings of the
Rich Transcription Fall Workshop.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
Use of Prosody in Parsing Conversational Speech. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 233?240, Vancouver,
British Columbia, Canada.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing, pages 181?
184.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
Published by Linguistic Data Consortium, Philadel-
phia.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-Coverage Parsing us-
ing Human-Like Memory Constraints. Computational
Linguistics, 36(1):1?30.
Richard Schwartz, Long Nguyen, Francis Kubala,
George Chou, George Zavaliagkos, and John
Makhoul. 1994. On Using Written Language
Training Data for Spoken Language Modeling. In
Proceedings of the Human Language Technology
Workshop, pages 94?98.
Elizabeth Shriberg and Andreas Stolcke. 1998. How
far do speakers back up in repairs? A quantitative
model. In Proceedings of the International Confer-
ence on Spoken Language Processing, pages 2183?
2186.
Elizabeth Shriberg. 1994. Preliminaries to a Theory of
Speech Disuencies. Ph.D. thesis, University of Cali-
fornia, Berkeley.
David A. Smith and Jason Eisner. 2006. Minimum Risk
Annealing for Training Log-Linear Models. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, pages
787?794.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2004. A Lexically-Driven Algorithm for Disfluency
Detection. In Proceedings of Human Language Tech-
nologies and North American Association for Compu-
tational Linguistics, pages 157?160.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904.
Qi Zhang, Fuliang Weng, and Zhe Feng. 2006. A pro-
gressive feature selection algorithm for ultra large fea-
ture spaces. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 561?568.
711
