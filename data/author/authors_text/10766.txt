Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 921?928
Manchester, August 2008
Class-Driven Attribute Extraction
Benjamin Van Durme, Ting Qian and Lenhart Schubert
Department of Computer Science
University of Rochester
Rochester, NY 14627, USA
Abstract
We report on the large-scale acquisition
of class attributes with and without the
use of lists of representative instances, as
well as the discovery of unary attributes,
such as typically expressed in English
through prenominal adjectival modifica-
tion. Our method employs a system based
on compositional language processing, as
applied to the British National Corpus. Ex-
perimental results suggest that document-
based, open class attribute extraction can
produce results of comparable quality as
those obtained using web query logs, indi-
cating the utility of exploiting explicit oc-
currences of class labels in text.
1 Introduction
Recent work on the task of acquiring attributes
for concept classes has focused on the use of pre-
compiled lists of class representative instances,
where attributes recognized as applying to multi-
ple instances of the same class are inferred as be-
ing likely to apply to most, or all, members of
that class. For example, the class US President
might be represented as a list containing the en-
tries Bill Clinton, George Bush, Jimmy Carter, etc.
Phrases such as Bill Clinton?s chief of staff ..., or
search queries such as chief of staff bush, provide
evidence that the class US President has as an at-
tribute chief of staff.
Usually the focus of such systems has been on
on binary attributes, such as the example chief of
staff, while less attention has been paid to unary
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
class attributes such as illegal for the class Drug,
or warm-blooded for the class Animal.
1
These
attributes are most typically expressed in English
through prenominal adjectival modification, with
the nominal serving as a class designator. When
attribute extraction is based entirely on instances
and not the class labels themselves, this form of
modification goes undiscovered.
In what follows we explore both the impact of
gazetteers in attribute extraction as well as the
acquisition and filtering of unary class attributes,
through a process based on logical form genera-
tion from syntactic parses derived from the British
National Corpus.
2 Extraction Framework
Extraction was performed using a modified ver-
sion of the KNEXT system, a knowledge acquisi-
tion framework constructed for large scale genera-
tion of abstracted logical forms through composi-
tional linguistic analysis. The following provides
an overview of KNEXT and its target knowledge
representation, Episodic Logic.
2.1 Episodic Logic
Automatically acquiring general world knowledge
from text is not a task that provides an immedi-
ate solution to any real world problem.
2
Rather,
the motivation for acquiring large stores of back-
ground knowledge is to enable research within
other areas of artificial intelligence, e.g., the con-
struction of systems that can engage in dialogues
about everyday topics in unrestricted English, use
1
Almuhareb and Poesio (2004) treat unary attributes as
values of binary attributes; e.g., illegal might be the value of
a legality attribute. But for many unary attributes, this is a
stretch.
2
Unless one regularly needs reminding of facts such as, A
WOMAN MAY BOIL A GOAT.
921
(Some e0:
[e0 at-about Now0]
[(Many.det x :
[x ((attr athletic.a) (plur youngster.n))]
[x want.v
(Ka
(become.v
(plur
((attr professional.a) athlete.n))))])
** e0])
Figure 1: Example EL formula; square brackets indicate
a sentential infix syntax of form [subject pred object ...], Ka
reifies action predicates, and attr ?raises? adjectival predicates
to predicate modifiers; e0 is the situation characterized by the
sentence.
common sense in answering questions or solv-
ing problems, pursue intrinsic goals independently,
and show awareness of their own characteristics,
biography, and cognitive capacities and limita-
tions. An important challenge in the pursuit of
these long-range goals is the design and implemen-
tation of a knowledge representation that is as ex-
pressively rich as natural language and facilitates
language understanding and commonsense reason-
ing.
Episodic Logic (EL) (Schubert and Hwang,
2000), is a superset of FOL augmented with cer-
tain semantic features common to all human lan-
guages: generalized quantification, intensionality,
uncertainty, modification and reification of predi-
cates and propositions, and event characterization
by sentences. An implementation of EL exists as
the EPILOG system (Schaeffer et al, 1993), which
supports both forward and backward inference,
along with various specialized routines for dealing
with, e.g., color, time, class subsumption, etc. EPI-
LOG is under current development as a platform for
studying a notion of explicit self-awareness as de-
fined by Schubert (2005).
As an indication of EL?s NL-like syntax, figure
1 contains the output of EPILOG?s parser/logical-
form generator for the sentence, Many athletic
youngsters want to become professional athletes.
2.2 KNEXT
If ?deep? language understanding and common-
sense reasoning involve items as complex and
structured as seen in figure 1, then automated
knowledge acquisition cannot simply be a mat-
ter of accumulating rough associations between
word strings, along the lines ?(Youngster) (want
become) (professional athlete)?. Rather, acquired
knowledge needs to conform with a systematic,
highly expressive KR syntax such as EL.
The KNEXT project is aimed at extracting such
structured knowledge from text. One of the major
obstacles is that the bulk of commonsense knowl-
edge on which people rely is not explicitly written
down ? precisely because it is common. Even it
were written down, most of it could not be reliably
interpreted, because reliable interpretation of lan-
guage is itself dependent on commonsense knowl-
edge (among other things).
In view of these difficulties, KNEXT has initially
focused on attempting to abstract world knowl-
edge ?factoids? from texts, based on the logical
forms derived from parsed sentences. The idea is
that nominal pre- and post-modifiers, along with
subject-verb-object relations, captured in logical
forms similar to that in figure 1, give a glimpse
of the common properties and relationships in the
world ? even if the source sentences describe in-
vented situations. For example, the following were
extracted by KNEXT, then automatically verbal-
ized back into English for ease of readability:
? SOME NUMBER OF YOUNGSTERS MAY WANT
TO BECOME ATHLETES.
? YOUNGSTERS CAN BE ATHLETIC.
? ATHLETES CAN BE PROFESSIONAL.
2.3 Attribute Extraction via KNEXT
In order to study the contribution of lists of in-
stances (i.e., generalized gazetteers) to the task of
attribute extraction, the version of KNEXT as pre-
sented by Schubert (2002) was modified to provide
output of a form similar to that of the extraction
work of Pas?ca and Van Durme (2007).
KNEXT?s abstracted, propositional output was
automatically verbalized into English, with any re-
sultant statements of the form, A(N) X MAY HAVE
A(N) Y, taken to suggest that the class X has as an
attribute the property Y.
KNEXT was designed from the beginning to
make use of gazetteers if available, where a
phrase such as Bill Clinton vetoed the bill sup-
ports the (verbalized) proposition A PRESIDENT
MAY VETO A BILL. just as would The president
vetoed the bill. We instrumented the system to
record which propositions did or did not require
gazetteers in their construction, allowing for a nu-
merical breakdown of the respective contributions
of known instances of a class, versus the class label
itself.
922
Pas?ca and Van Durme (2007) described the re-
sults of an informal survey asking participants to
enumerate what they felt to be important attributes
for a small set of example classes. Some of these
resultant attributes were not of the form targeted
by the authors? system. For example, nonprofit
was given as an important potential attribute for
the class Company, as well as legal for the class
Drug. These attributes correspond to unary predi-
cates as compared to the targeted binary predicates
underlying such attributes as cost(X,Y) for the class
Drug.
We extracted such unary attributes by focusing
on verbalizations of the form, A(N) X CAN BE Y
as in AN ANIMAL CAN BE WARM-BLOODED.
3 Experimental Setting
3.1 Corpus Processing
Initial reports on the use of KNEXT were focused
on the processing of manually created parse trees,
on a corpus of limited size (the Brown corpus of
Kucera and Francis (1967)). Since that time the
system has been modified into a fully automatic
extraction system, making use of syntactic parse
trees generated by parsers trained on the Penn
Treebank.
For our studies here, the parser employed was
that of Collins (1997) applied to the sentences
of the British National Corpus (BNC Consortium,
2001). Our choice of the BNC was motivated by
its breadth of genre, its substantial size (100 mil-
lion words) and its familiarity (and accessibility)
to the community.
3.2 Gazetteers
KNEXT?s gazetteers were used as-is, and were
defined based on a variety of sources: miscella-
neous publicly available lists, as well as manual
enumeration. The classes covered can be seen in
the Results section in table 2, where the minimum,
maximum and mean size were 2, 249, and 41, re-
spectively.
3.3 Filtering out Non-predicative Adjectives
Beyond the pre-existing KNEXT framework, ad-
ditional processing was introduced for the extrac-
tion of unary attributes in order to filter out vacu-
ous or unsupported propositions derived from non-
compositional phrases.
This filtering was performed through the cre-
ation of three lists: a whitelist of accepted pred-
icative adjectives; a graylist containing such adjec-
tives that are meaningful as unary predicates only
when applied to plural nouns; and a blacklist de-
rived from Wikipedia topic titles, representing lex-
icalized, non-compositional phrases.
Whitelist The creation of the whitelist began with
calculating part-of-speech (POS) tagged bigram
counts using the Brown corpus. The advantage
of using a POS-tagged bigram model lies in the
saliency of phrase structures, which enabled fre-
quency calculations for both attributive and pred-
icative uses of a given adjective. Attributive counts
were based on instances when an adjective appears
in the pre-nominal position and modifies another
noun. Predicative counts were derived by sum-
ming over occurrences of a given adjective after
all possible copulas. These counts were used to
compute a p/a ratio - the quotient of predicative
count over attributive count - for each word classi-
fied by WordNet (Fellbaum, 1998) as a having an
adjectival use. After manual inspection, two cut-
off points were chosen at ratios of .06 and 0, as
seen in table 1.
Words not appearing in the Brown corpus (i.e.
having 0 count for both uses), were sampled and
inspected, with the decision made to place the
majority within the whitelist, excluding just those
with suffixes including -al, -c, -an, -st, -ion, -th, -o,
-ese, -er, -on, -i, -x, -v, and -ing.
This process resulted in a combined whitelist of
14,249 (usually) predicative adjectives.
p/a ratio (r) Cut-off decision
r ? .06 keep the adjective*
0 < r < .06 remove the adjective*
otherwise keep the adjective*
Table 1: Cut-off decision given the p/a ratio of an
adjective. *Note: except for hand-selected cases.
Graylist We manually constructed a short list (cur-
rently 33 words) containing adjectives that are gen-
erally inappropriate as whitelist entries, but could
be acceptable when applied to plurals. For exam-
ple, the verbalized proposition OBJECTS CAN BE
SIMILAR was deemed acceptable, while a state-
ment such as AN OBJECT CAN BE SIMILAR is
erroneous because of a missing argument.
Blacklist From an exhaustive set of Wikipedia
topic titles was derived a blacklist consisting of en-
tries that had to satisfy four criteria: 1) no more
than three words in length; 2) has no closed-class
923
words, such as prepositions or adverbs; 3) must
begin with an adjective and end with a noun (de-
termined by WordNet); and 4) does not contain
any numerical characters or miscellaneous sym-
bols that are usually not meaningful in English.
Therefore, each title in the resultant list is liter-
ally a short noun phrase with adjectives as pre-
modifiers. It was observed that in these encyclope-
dia titles, the role of adjectives is predominantly to
restrict the scope of the object that is being named
(e.g. CRIMINAL LAW), rather than to describe
its attributions or features (e.g. DARK EYES).
More often than not, only cases similar to the sec-
ond example can be safely verbalized as X CAN
BE Y from a noun phrase Y X, with Y being the
pre-nominal adjective.
We further refined this list by examining trigram
frequencies as reported in the web-derived n-gram
collection of Brants and Franz (2006). For each ti-
tle of the form (Adj N) ..., we gathered trigram fre-
quencies for adverbial modifications such as (very
Adj N) ..., and (truly Adj N) .... Intuitively, high rel-
ative frequency of such modification with respect
to the non-modified bigram supports removal of
the given title from the blacklist.
Trigram counts were collected using the modi-
fiers: absolutely, almost, entirely, highly, nearly,
perfectly, truly and very. These counts were
summed for a given title then divided by the afore-
mentioned bigram score. Upon sampled inspec-
tion, all three-word titles were kept on the black-
list, along with any two-word title with a resultant
ratio less than 0.028. For example, the titles Hardy
Fish, Young Galaxy, and Sad Book were removed,
while Common Cause, Bouncy Ball, and Heavy Oil
were retained.
4 Results
From the parsed BNC, 6,205,877 propositions
were extracted, giving an average of 1.396 propo-
sitions per input sentence.
3
These results were
then used to explore the necessity of gazetteers,
and the potential for extracting unary attributes.
Quality judgements were performed using a 5
point scale as seen in figure 2.
3
These approximately six million verbalized propo-
sitions, along with their underlying logical form and
respective source sentence(s), may be browsed in-
teractively through an online browser available at:
http://www.cs.rochester.edu/u/vandurme/epik
THE STATEMENT ABOVE IS A REASONABLY
CLEAR, ENTIRELY PLAUSIBLE GENERAL
CLAIM AND SEEMS NEITHER TOO SPECIFIC
NOR TOO GENERAL OR VAGUE TO BE USEFUL:
1. I agree.
2. I lean towards agreement.
3. I?m not sure.
4. I lean towards disagreement.
5. I disagree.
Figure 2: Instructions for scaled judging.
4.1 Necessity of Gazetteers
From the total set of extracted propositions,
638,809 could be verbalized as statements of the
form X MAY HAVE Y. There were 71,531 unique
classes (X) for which at least a single candidate at-
tribute (Y) was extracted, with 9,743 of those hav-
ing at least a single such attribute that was sup-
ported by a minimum of two distinct sentences.
Table 2 gives the number of attributes extracted
for the given classes when using only gazetteers,
when using only the given names as class labels,
and when using both together. While instance-
based extraction generated more unique attributes,
there were still a significant number of results de-
rived based exclusively on class labels. Further,
as can be seen for cases such as Artist, class-
driven extraction provided a large number of at-
tribute candidates not observed when relying only
on gazetteers (701 total candidate attributes were
gathered based on the union of 441 and 303 can-
didates respectively extracted with, and without a
gazetteer for Artist).
We note that this volume measure is potentially
biased against class-driven extraction, as no ef-
fort was made to pick an optimal label for a given
gazetteer, (the original hand-specified class labels
were retained). For example, one might expect the
label Drink to generate more, yet still appropriate,
propositions than Beverage, Actor and/or Actress
as compared to Show Biz Star, or the semantically
similar Book versus Literary Work. This is sug-
gested by the entries in the table based on using
supertypes of the given class, as well as in figure
3, which favorably compares top attributes discov-
ered for select classes against those reported else-
where in the literature.
Table 3 gives the assessed quality for the top ten
attributes extracted for five of the classes in table
2. As can be seen, class-driven extraction can pro-
duce attributes of quality assessed at par with at-
tributes extracted using only gazetteers.
924
BasicFood Religion
K (Food): quality, part, taste, value, portion.. K: basis, influence, name, truths, symbols, principles,
D: species, pounds, cup, kinds, lbs, bowl.. strength, practice, origin, adherent, god, defence..
Q: nutritional value, health benefits, glycemic index, D: teachings, practice, beliefs, religion spread,
varieties, nutrition facts, calories.. principles, emergence, doctrines..
Q: basic beliefs, teachings, holy book, practices, rise,
branches, spread, sects..
HeavenlyBody Painter
K
G
(Planet): surface, orbit, bars, history, atmosphere.. K
G
(Artist) : works, life, career, painting, impression,
K (Planet): surface, history, future, orbit, mass, field.. drawings, paintings, studio, exhibition..
K (Star): surface, mass, field, regions.. K (Artist): works, impression, career, life, studio..
D: observations, spectrum, planet, spectra, conjunction, K (Painter) : works, life, wife, eye..
transit, temple, surface.. Q?: paintings, works, portrait, death, style, artwork,
Q: atmosphere, surface, gravity, diameter, mass, bibliography, bio, autobiography, childhood..
rotation, revolution, moons, radius..
Figure 3: Qualitative comparison of top extracted attributes; K
G
is KNEXT using gazetteers, K (class) is KNEXT for a class
label similar to the heading, D and Q are document- and query-based results as reported in (Pas?ca et al, 2007), Q? is query-based
results reported in (Pas?ca and Van Durme, 2007).
The noticeable drop in quality for the class
Planet when only using gazetteers (3.2 mean
judged acceptability) highlights the recurring
problem of word sense ambiguity in extraction.
The names of Roman deities, such as Mars or Mer-
cury, are used to refer to a number of conceptu-
ally distinct items, such as planets within our so-
lar system. Two of the attributes judged as poor
quality for this class were bars and customers, re-
spectively derived from the noun phrases: (NP
(NNP Mars) (NNS bars)), and (NP (NNP Mer-
cury) (NNS customers)). Note that in both cases
the underlying extraction is correctly performed;
the error comes from abstracting to the wrong
class. These NPs may arguably support the ver-
balized propositions, e.g.: A CANDY-COMPANY
MAY HAVE BARS, and A CAR-COMPANY
MAY HAVE CUSTOMERS.
These examples point to additional areas for
improvement beyond sense disambiguation: non-
compositional phrase filtering for all NPs, rather
than just in the cases of adjectival modification
(Mars bar is a Wikipedia topic); and relative dis-
counting of patterns used in the extraction pro-
cess
4
. This later technique is commonly used in
specialized extraction systems, such as constructed
by Snow et al (2005) who fit a logistic regression
model for hypernym (X is-a Y) classification based
on WordNet, and Girju et al (2003) who trained a
classifier to look specifically for part-whole rela-
tions.
4
For example, (NP (NNP X) (NNS Y)) may be more se-
mantically ambiguous than, e.g., the possessive construction
(NP (NP (NNP X) (POS ?s)) (NP (NNS Y))).
4.2 Unary Attributes
Table 4 shows how filtering non-compositional
phrases from CAN BE propositions affects extrac-
tion volume. Table 5 shows the difference between
such post-filtered propositions and those that were
deleted. As our filter lists were not built fully au-
tomatically, evaluation was performed exclusively
by an author with negligible direct involvement in
the lists? creation (so-as to minimize judgement
bias).
As examples, the top ten unary attributes for
select classes are given in table 6, which the au-
thors believe to be high quality on average, with
some bad entries present. Attributes such as
pre-raphaelite for Painter are considered obscure,
while those such as favourite for Animal are con-
sidered unlikely to be useful as a unary predicate.
The importance of class-driven extraction can be
seen in results such as those given for the class Ap-
ple. Even if it were the case that gazetteer-based
extraction could deliver perfect results for those
classes whose instances occasionally appear ex-
plicitly in text, there are a number of classes for
which such instances are entirely lacking. For ex-
ample, there are many instances of the class Com-
pany which have been individually named and ap-
pear in text with some frequency, e.g., Microsoft,
Walmart, or Boeing. However, despite the many
real-world instantiations of the class Apple, this
does not translate into a list of individually named
members in text.
5
If our goal is to acquire at-
tributes for as many classes as possible, our results
5
Instances of Apple are referred to directly as such; ?an
apple.?
925
Class Both Gaz. Class Lbl.
Continent 777 698 96
Country 7,285 5,993 1,696
US State 1,289 1,286 609*
US City 2,216 2,120 813*
World City 4,780 4,747 813*
Beverage 53 53 0
Tycoon 19 10 10
TV Network 71 71 0
Artist 706 441 303
Medicine 29 2 27
Weekday 1,234 1,232 2
Month 2,282 1,875 474
Dictator 533 509 28
Conqueror 103 84 19
Philosopher 672 649 37
Conductor 118 74 45
Singer 220 179 49
Band 349 58 303
King 811 208 664
Queen 541 17 532
Religious Leader 127 127 0
Adventurer 32 27 5
Planet 289 163 141
Criminal/Outlaw 30 30 6/4*
Service Agency 85 83 2
Architect 72 67 63
Show Biz Star 82 82 0
Film Maker 42 33 9
Composer 722 651 98
Humanitarian 5 5 0
Pope 235 123 113
River 402 168 253
Company 3,968 1,553 2,941
Deity 1,037 1,027 19
Scientist 798 750 60
Religious Holiday 594 593 65*
Civic Holiday 3 3 65*
Military Commander 71 71 26*
Intl Political Entity 673 673 0
Sports Celebrity 45 45 0
Activist Organization 63 63 0
Martial Art 3 3 0
Government Agency 295 294 2
Criminal Organization 0 0 0
US President 596 596 1,421*
Political Leader 568 568 170*
Supreme Court Justice 0 0 18*
Emperor 436 211 259
Fictitious Character 227 227 180*
Literary Work 9 9 0
Engineer/Inventor 10 10 73/13*
Famous Lawyer 0 0 72*
Writer 1,116 957 236
TOTAL 35,723 29,518 8,506
Table 2: Extraction volume with and without using
gazetteers. *Note: When results are zero after gaz. omission,
values are reported for super-types, such as Holiday for the
sub-type Civic Holiday, or City for US City. A/B scores re-
ported for each class used separately, e.g., Engineer/Inventor.
Class Both Gazetteer Class Label
King 1.2 1.9 1.3
Composer 1.5 1.5 2.1
River 1.9 1.9 1.5
Continent 1.5 1.9 2.0
Planet 1.9 3.2 1.6
Table 3: Average judged acceptability for the top ten at-
tributes extracted for the given classes when using/not-using
gazetteer information.
Collection Size
% of
Original CAN BE
Original total 6,204,184 100 -
Filtered total 5,382,282 87 -
Original CAN BE 2,895,325 46 100
Filtered CAN BE 2,073,417 33 72
Whitelist 812,146 15 28
Blacklist 19,786 1< 1
Table 4: Impact of filtering on volume. For example, those
propositions removed because of the whitelist comprised 15%
of the total propositions extracted, or 28% of those specifi-
cally verbalized as X CAN BE Y.
indicate the benefits of exploiting the explicit ap-
pearance of class labels in text.
5 Related Work
Pas?ca and Van Durme (2007) presented an ap-
proach to attribute extraction based on the use
of search engine query logs, a previously unex-
plored source of data within information extrac-
tion. Results confirmed the intuition that a sig-
nificant number of high quality, characteristic at-
tributes for many classes may be derived based
on the relative frequency with which anonymous
users request particular pieces of information for
known instances of a concept class. Pas?ca et al
(2007) compared the quality of shallow attribute
extraction techniques as applied to documents ver-
sus search engine query logs, concluding that such
methods are more applicable to query logs than to
documents. We note that while search queries do
seem ideally suited for extracting class attributes,
existing large-scale collections of query logs are
proprietary and thus unavailable to the general re-
search community. At least until such a resource
becomes available, it is of interest to the commu-
nity that (qualitatively) similar extraction results
may be achieved exclusively using publicly avail-
able document collections.
Alternative approaches to harvesting large-scale
knowledge repositories based on logical forms in-
clude that reported by Suchanek et al (2007).
The authors used non-linguistic information avail-
926
1 10 100 1,000
Filtered 3.18 3.60 2.74 2.76
Blacklist 3.88 4.00 4.08 4.06
Whitelist 3.78 3.76 3.74 3.80
Table 5: Mean evaluated acceptability for 50 unary at-
tributes randomly sampled from each of the given levels of
support (attribute occurred once, less than 10 times, less than
100 times, ...). Filtered refers to the final ?clean? results,
Blacklist and Whitelist refer to propositions deleted due to
the given list.
Painter
famous, romantic, distinguished, celebrated,
well-known, pre-raphaelite, flemish, dutch,
abstract
Animal
dead, trapped, dangerous, unfortunate, intact,
hungry, wounded, tropical, sick, favourite
Drug
dangerous, powerful, addictive, safe, illegal,
experimental, effective, prescribed, harmful,
hallucinatory
Apple
red, juicy, fresh, bad, substantive, stuffed,
shiny, ripe, green, baked
Earthquake
disastrous, violent, underwater, prolonged,
powerful, popular, monstrous, fatal, famous,
epic
Table 6: Top ten unary attributes for select classes, gathered
exclusively without the use of gazetteers.
able via Wikipedia to populate a KB based on
a variant of the logic underlying the Web On-
tology Language (OWL). Results were limited to
14 predefined relation types, such as diedInYear
and politicianOf, with membership of instances
within particular concept classes inferred based on
Wikipedia?s category pages. Authors report 5 mil-
lion so-called ontological facts being extracted.
Almuhareb and Poesio (2004) performed at-
tribute extraction on webtext using simple extrac-
tion patterns (e.g., ?the * of the C [is|was]?, and
?[a|an|the] * C [is|was]?, which respectively match
The color of the rose was red and A red rose was
...), and showed that such attributes could improve
concept clustering. Subsequently they tested an
alternative approach to the same problem using a
dependency parser, extracting syntactic relations
such as (ncmod, rose, red) and (ncsubj, grow, rose)
(Almuhareb and Poesio, ). They concluded that
syntactic information is relatively expensive to de-
rive, and serves primarily to alleviate data spar-
sity problems (by capturing dependencies between
potentially widely separated words) that may no
longer be an issue given the scale of the Web. We
take a different view, first because attribute extrac-
tion is an offline task for which a 60% overhead
cost (reported by the authors) is not a major is-
sue, but more importantly because we regard ap-
proaches that process language compositionally as
ultimately necessary for deeper meaning represen-
tation and language understanding.
Following intuitions similar to those laid out by
Schubert (2002), Banko et al (2007) presented
TextRunner, the latest in a series of ever more so-
phisticated general information extraction systems
(Cafarella et al, 2005; Etzioni et al, 2004). The
authors constructed a non-parser based extractor
for open domain text designed to efficiently pro-
cess web-sized datasets. Results are in the form of
bracketed text sequences that hint at a sentence?s
underlying semantics. For example, (Bletchley
Park) was location of (Station X).
Cimiano et al (2005) performed a limited form
of class-driven extraction in order to induce class
hierarchies via the methods of Formal Concept
Analysis (FCA). For example, a car is both drive-
able and rentable based on its occurrence in object
position of the relevant verbs. A bike shares these
properties with car, as well as having the property
rideable, leading to these classes being near in the
resultant automatically constructed taxonomy. Ex-
periments were performed on limited domains for
which pre-existing ontologies existed for measur-
ing performance (tourism and finance).
Lin (1999) gave a corpus-based method for find-
ing various types of non-compositional phrases,
including the sort discussed in this paper. Identi-
fication was based on mutual information statistics
conditioned on a given syntactic context (such as
our targeted prenominal adjectival modification).
If the mutual information of, e.g., white house,
shows strong differences from that for construc-
tions with similar components, e.g., red house, and
white barn, then the given phrase was determined
to be non-compositional. The use of this method to
supplement that explored here is a matter of cur-
rent investigation. Early results confirm our in-
tuition regarding the correlation between such au-
tomatically discovered non-compositional phrases
and Wikipedia topic titles, where high scoring
phrases not already in our list tend to suggest miss-
Yes
cooking pot, magic flute, runny nose, skimmed milk,
acquired dyslexia, charged particles, earned income
No
causal connectives, golden oldies, ruling junta,
graduated pension, unsung heroes, viral rna
Table 7: Example high-scoring phrases as ranked by Lin?s
metric when applied to KNEXT logical forms, along with
whether there is, at the time of this writing, an associated
Wikipedia entry.
927
A CAR MAY HAVE A ...
back, boot, side, driver, front, roof, seat, end, interior,
owner, door, control, value, bonnet, wheel, window,
engine, headlights..
A CAR CAN BE ...
black, parked, red, white, armoured, nice, hired, bloody,
open, beautiful, wrecked, unmarked, secondhand,
powerful, brand-new, out-of use, damaged, heavy, dark,
competitive, broken-down..
A CAR MAY BE ... IN SOME WAY
parked, stolen, driven, damaged, serviced, stopped,
lost, clamped, overturned, locked, involved in an accident,
found, turned, transported..
Table 8: Top attributes extracted for the class Car, where
MAY BE relational properties (akin to those used by Cimi-
ano et al (2005)) are similarly acquired via verbalization of
abstracted logical forms.
ing entries in the encyclopedia (see table 7). The
ability to perform such ?missing topic discovery?
should be of interest to those within the emerging
community of Wikipedia-focused AI researchers.
6 Conclusion
We have shown that an open knowledge extraction
system can effectively yield class attributes, even
when named instances of the class are unavailable
or scarce (as a final example see table 8). We stud-
ied the quantitative contributions of instances (as
given in KNEXT gazetteers) and explicitly occur-
ring class nominals to the discovery of attributes,
and found both to be important. We paid partic-
ular attention to the acquisition of unary class at-
tributes, for which access to class labels is of par-
ticular importance because of their typical manner
of expression in text.
Acknowledgements The authors are grateful to
Daniel Gildea for contributing a parsed version of
the BNC. This work was supported by NSF grants
IIS-0328849 and IIS-0535105.
References
Almuhareb, Abdulrahman and Massimo Poesio. Finding con-
cept attributes in the web using a parser. In Proceedings
Corpus Linguistics Conference.
Almuhareb, Abdulrahman and Massimo Poesio. 2004.
Attribute-based and value-based clustering: an evaluation.
In Proceedings of EMNLP.
Banko, Michele, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open Infor-
mation Extraction from the Web. In Proceedings of IJCAI.
BNC Consortium. 2001. The British National Corpus, ver-
sion 2 (BNC World). Distributed by Oxford University
Computing Services.
Brants, Thorsten and Alex Franz. 2006. Web 1T 5-gram Ver-
sion 1. Distributed by the Linguistic Data Consortium.
Cafarella, Michael J., Doug Downey, Stephen Soderland, and
Oren Etzioni. 2005. KnowItNow: Fast, Scalable Infor-
mation Extraction from the Web. In Proceedings of HLT-
EMNLP.
Cimiano, Philipp, Andreas Hotho, and Steffen Stabb. 2005.
Learning concept hierarchies from text corpora using for-
mal concept analysis. Journal of Artificial Inteligence Re-
search.
Collins, Michael. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of ACL.
Etzioni, Oren, Michael Cafarella, Doug Downey, Stanley
Kok, AnaMaria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2004. Web-scale
Information Extraction in KnowItAll. In Proceedings of
WWW.
Fellbaum, Christiane. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Girju, R., A. Badulescu, and D. Moldovan. 2003. Learning
semantic constraints for the automatic discovery of part-
whole relations. In Proceedings of HLT-NAACL.
Kucera, H. and W. N. Francis. 1967. Computational Anal-
ysis of Present-Day American English. Brown University
Press, Providence, RI.
Lin, Dekang. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL.
Pas?ca, Marius and Benjamin Van Durme. 2007. What you
seek is what you get: Extraction of class attributes from
query logs. In Proceedings of IJCAI.
Pas?ca, Marius, Benjamin Van Durme, and Nikesh Garera.
2007. The role of documents vs. queries in extracting class
attributes from text. In Proceedings of CIKM.
Schaeffer, S.A., C.H. Hwang, J. de Haan, and L.K. Schubert.
1993. EPILOG, the computational system for episodic
logic: User?s guide. Technical report, Dept. of Comput-
ing Science, Univ. of Alberta, August.
Schubert, Lenhart K. and Chung Hee Hwang. 2000. Episodic
logic meets little red riding hood: A comprehensive, natu-
ral representation for language understanding. In Iwanska,
L. and S.C. Shapiro, editors, Natural Language Processing
and Knowledge Representation: Language for Knowledge
and Knowledge for Language. MIT/AAAI Press.
Schubert, Lenhart K. 2002. Can we derive general world
knowledge from texts? In Proceedings of HLT.
Schubert, Lenhart K. 2005. Some Knowledge Representa-
tion and Reasoning Requirements for Self-awareness. In
Proc. AAAI Spring Symposium on Metacognition in Com-
putation.
Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym dis-
covery. In Proceedings of NIPS 17.
Suchanek, Fabian M., Gjergji Kasneci, and Gerhard Weikum.
2007. YAGO: A core of semantic knowledge unifying
WordNet and Wikipedia. In Proceedings of WWW.
928
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 37?42,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Building a Semantic Lexicon of English Nouns via Bootstrapping
Ting Qian1, Benjamin Van Durme2 and Lenhart Schubert2
1Department of Brain and Cognitive Sciences
2Department of Computer Science
University of Rochester
Rochester, NY 14627 USA
ting.qian@rochester.edu, {vandurme, schubert}@cs.rochester.edu
Abstract
We describe the use of a weakly supervised
bootstrapping algorithm in discovering con-
trasting semantic categories from a source lex-
icon with little training data. Our method pri-
marily exploits the patterns in sentential con-
texts where different categories of words may
appear. Experimental results are presented
showing that such automatically categorized
terms tend to agree with human judgements.
1 Introduction
There are important semantic distinctions between
different types of English nouns. For example, some
nouns typically refer to a concrete physical object,
such as book, tree, etc. Others are used to represent
the process or the result of an event (e.g. birth, cele-
bration). Such information is useful in disambiguat-
ing syntactically similar phrases and sentences, so as
to provide more accurate semantic interpretations.
For instance, A MAN WITH HOBBIES and A MAN
WITH APPLES share the same structure, but convey
very different aspects about the man being referred
to (i.e. activities vs possessions).
Compiling such a lexicon by hand, e.g., WordNet
(Fellbaum, 1998), requires tremendous time and ex-
pertise. In addition, when new words appear, these
will have to be analyzed and added manually. Fur-
thermore, a single, global lexicon may contain er-
roneous categorizations when used within a specific
domain/genre; we would like a ?flexible? lexicon,
adaptable to a given corpus. Also, in adapting se-
mantic classifications of words to a particular genre
or domain, we would like to be able to exploit con-
tinuing improvements in methods of extracting se-
mantic occurrence patterns from text.
We present our initial efforts in discovering se-
mantic classes incrementally under a weakly super-
vised bootstrapping process. The approach is able
to selectively learn from its own discoveries, thereby
minimizing the effort needed to provide seed exam-
ples as well as maintaining a reasonable accuracy
rate. In what follows, we first focus on its appli-
cation to an event-noun classification task, and then
use a physical-object vs non-physical-object experi-
ment as a showcase for the algorithm?s generality.
2 Bootstrapping Algorithm
The bootstrapping algorithm discovers words with
semantic properties similar to a small set of labelled
seed examples. These examples can be manually se-
lected from an existing lexicon. By simply changing
the semantic property of the seed set, this algorithm
can be applied to the task of discovering a variety of
semantic classes.
Features Classification is performed using a
perceptron-based model (Rosenblatt, 1958) that ex-
amines features of each word. We use two kinds
of features in our model: morphological (affix and
word length), and contextual. Suffixes, such as -ion,
often reveal the semantic type that a noun belongs
to (e.g., destruction, explosion). Other suffixes like
-er typically suggest non-event nouns (e.g. waiter,
hanger). The set of affixes can be modified to re-
flect meaningful distinctions in the task at hand. Re-
garding word length, longer words tend to have more
37
syllables, and thus are more likely to contain affixes.
For example, if a word ends with -ment, its num-
ber of letters must be ? 5. We defined a partition
of words based on word length: shortest (fewer than
5 letters), short (5-7), medium (8-12), long (13-19),
and longest (> 19).
Besides morphological features, we also make use
of verbalized propositions resulting from the experi-
ments of Van Durme et al (2008) as contextual fea-
tures. These outputs are in the form of world knowl-
edge ?factoids? abstracted from texts, based on log-
ical forms from parsed sentences, produced by the
KNEXT system (see Schubert (2002) for details).
The followings are some sample factoids about the
word destruction, extracted from the British Na-
tional Corpus.
? A PERSON-OR-ORGANIZATION MAY UNDERGO A DE-
STRUCTION
? INDIVIDUAL -S MAY HAVE A DESTRUCTION
? PROPERTY MAY UNDERGO A DESTRUCTION
We take each verbalization (with the target word
removed) as a contextual feature, such as PROPERTY
MAY UNDERGO A . Words from the same seman-
tic category (e.g., event nouns) should have seman-
tic and syntactic similarities on the sentential level.
Thus their contextual features, which reflect the use
of words both semantically and syntactically, should
be similar. For instance, PROPERTY MAY UNDERGO
A PROTECTION is another verbalization produced
by KNEXT, suggesting the word protection may be-
long to the same category as destruction.
A few rough-and-ready heuristics are already em-
ployed by KNEXT to do the same task as we wish
to automate here. A built-in classifier judges nomi-
nals to be event or non-event ones based on analysis
of endings, plus a list of event nouns whose endings
are unrevealing, and a list of non-event nouns whose
endings tend to suggest they are event nouns. As a
result, the factoids used as contextual features in our
work already reflect the built-in classifier?s attempt
to distinguish event nouns from the rest. Thus, the
use of these contextual features may bias the algo-
rithm to perform seemingly well on event-noun clas-
sification. However, we will show that our algorithm
works for classification of other semantic categories,
for which KNEXT does not yet have discriminative
procedures.
Iterative Training We use a bootstrapping pro-
cedure to iteratively train a perceptron-based lin-
ear classifier. A perceptron algorithm determines
whether the active features of a test case are similar
to those learned from given categories of examples.
In an iterative training process, the classifier first
learns from a small seed set, which contains exam-
ples of all categories (in binary classification, both
positive and negative examples) manually selected
to reflect human knowledge of semantic categories.
The classifier then discovers new instances (and cor-
responding features) of each category. Based on
activation values, these newly discovered instances
are selectively admitted into the original training set,
which increases the size of training examples for the
next iteration.
The iterative training algorithm described above
is adopted from Klementiev and Roth (2006). The
advantage of bootstrapping is the ability to auto-
matically learn from new discoveries, which saves
both time and effort required to manually examine
a source lexicon. However, if implemented exactly
as described above, this process has two apparent
disadvantages: New examples may be wrongly clas-
sified by the model; and it is difficult to evaluate the
discriminative models produced in successive itera-
tions, as there are no standard data against which to
judge them (the new examples are by definition pre-
viously unexamined). We propose two measures to
alleviate these problems. First, we admit into the
training set only those instances whose activation
values are higher than the mean activation of their
corresponding categories in an iteration. This sets
a variable threshold that is correlated with the per-
formance of the model at each iteration. Second, we
evaluate iterative results post hoc, using a Bootstrap-
ping Score. This measures the efficacy of bootstrap-
ping (i.e. the ratio of correct newly discovered in-
stances to training examples) and precision (i.e. the
proportion of correct discoveries among all those re-
turned by the algorithm). We compute this score to
decide which iteration has yielded the optimal dis-
criminative model.
3 Building an Event-noun Lexicon
We applied the bootstrapping algorithm to the task
of discovering event nouns from a source lexicon.
38
Event nouns are words that typically describe the
occurrence, the process, or the result of an event.
We first explore the effectiveness of this algorithm,
and then describe a method of extracting the optimal
model. Top-ranked features in the optimal model are
used to find subcategories of event nouns.
Experimental Setup The WordNet noun-list is
chosen as the source lexicon (Fellbaum, 1998),
which consists of 21,512 nouns. The purpose of
this task is to explore the separability of event nouns
from this collection.
typical suffixes: appeasement, arrival, renewal,
construction, robbery, departure, happening
irregular cases: birth, collapse, crash, death, de-
cline, demise, loss, murder
Table 1: Examples of event-nouns in initial training set.
We manually selected 15 event nouns and 215
non-event nouns for the seed set. Event-noun exam-
ples are representative of subcategories within the
semantic class, as well as their commonly seen mor-
phological structures (Table 1). Non-event examples
are primarily exceptions to morphological regulari-
ties (to prevent the algorithm from overly relying on
affix features), such as, anything, ambition, diago-
nal. The subset of all contextual and morphological
features represented by both event and non-event ex-
amples are used to bootstrap the training process.
Event Noun Discovery Reducing the number of
working features is often an effective strategy in
training a perceptron. We experimented with two
cut-off thresholds for features: in Trial 1, features
must appear at least 10 times (55,354 remaining);
in Trial 2, features must appear at least 15 times
(35,457 remaining).
We set the training process to run for 20 iterations
in both trials. Classification results of each iteration
were collected. We expect the algorithm to discover
few event nouns during early iterations. But with
new instances found in each subsequent iteration,
it ought to utilize newly seen features and discover
more. Figure 1 confirms our intuition.
The number of classified event-noun instances in-
creased sharply at the 15th iteration in Trial 1 and the
11th iteration in Trial 2, which may suggest overfit-
ting of the training examples used in those iterations.
If so, this should also correlate with an increase of
error rate in the classification results (error rate de-
fined as the percentage of non-event nouns identi-
fied as event nouns in all discovered event nouns).
We manually marked all misclassified event noun in-
stances for the first 10 iterations in both trials. The
error rate in Trial 2 is expected to significantly in-
crease at the 10th iteration, while Trial 1 should ex-
hibit little increase in error rate within this interval.
This expectation is confirmed in Figure 2.
Extracting the Optimal Model We further pur-
sued the task of finding the iteration that has yielded
the best model. Optimality is judged from two as-
pects: 1) the number of correctly identified event
nouns should be significantly larger than the size of
seed examples; and 2) the accuracy of classification
results should be relatively high so that it takes lit-
tle effort to clean up the result. Once the optimal
model is determined, we analyze its most heavily
weighted features and try to derive finer categories
from them. Furthermore, the optimal model could
be used to discover new instances from other source
lexicons in the future.
We define a measure called the Bootstrapping
Score (BS), serving a similar purpose as an F-score.
BS is computed as in Formula (1).
BS = 2 ?BR ? PrecisionBR + Precision . (1)
Here the Bootstrapping Rate (BR) is computed as:
BR = |NEW ||NEW |+ |SEED| , (2)
where |NEW | is the number of correctly identi-
fied new instances (seed examples excluded), and
|SEED| is the size of seed examples. The rate
of bootstrapping reveals how large the effect of the
bootstrapping process is. Note that BR is different
from the classic measure recall, for which the total
number of relevent documents (i.e. true event nouns
in English) must be known a priori ? again, this
knowledge is what we are discovering. The score
is a post hoc solution; both BR and precision are
computed for analysis after the algorithm has fin-
ished. Combining Formulas (1) and (2), a higher
Bootstrapping Score means better model quality.
Bootstrapping scores of models in the first ten it-
erations are plotted in Figure 3. Model quality in
39
5 10 15 20
02
000
4000
6000
8000
10000
Iteration
Numb
er of E
vent N
ouns D
iscove
red
Trial 1Trial 2
Figure 1: Classification rate
2 4 6 8 100
.05
0.10
0.15
0.20
0.25
0.30
Iteration
Error r
ate
Trial 1Trial 2
Figure 2: Error rate
2 4 6 8 100
.82
0.84
0.86
0.88
0.90
0.92
0.94
Iteration
Boots
trappin
g Scor
e
Trial 1Trial 2
Figure 3: Bootstrapping score
1 . . . 6 . . . 10
incorrect 5 . . . 32 . . . 176
correct 79 . . . 236 . . . 497
error rate 5.9% . . . 11.9% . . . 26.2%
score 87.0% . . . 90.8% . . . 83.8%
Table 2: From iterations 1 to 10, comparison between
instance counts, error rates, and bootstrapping scores as
the measure of model quality.
Trial 2 is better than in Trial 1 on average. In ad-
dition, within Trial 2, Iteration 6 yielded the best
discriminative model with a bootstrapping score of
90.8%. Compared to instance counts and error rate
measures as shown in Table 2, this bootstrapping
score provides a balanced measure of model qual-
ity. The model at the 6th iteration (hereafter, Model
6) can be considered the optimal model generated
during the bootstrapping training process.
Top-ranked Features in the Optimal Model In
order to understand why Model 6 is optimal, we
extracted its top 15 features that activate the event-
noun target in Model 6, as listed in Table 3. Inter-
estingly, top-ranked features are all contextual ones.
In fact, in later models where the ranks of mor-
phological features are boosted, the algorithm per-
formed worse as a result of relying too much on
those context-insensitive features.
Collectively, top-ranked features define the con-
textual patterns of event nouns. We are interested
in finding semantic subcategories within the set of
event nouns (497 nouns, Trial 2) by exploiting these
features individually. For instance, some events typ-
ically happen to people only (e.g. birth, betrayal),
while others usually happen to inanimate objects
(e.g. destruction, removal). Human actions can also
be distinguished by the number of participants, such
as group activities (e.g. election) or individual ac-
tivities (e.g. death). It is thus worth distinguishing
nouns that describe different sorts of events.
Manual Classification We extracted the top 100
contextual features from Model 6 and grouped
them into feature classes. A feature class con-
sists of contextual features sharing similar mean-
ings. For instance, A COUNTRY MAY UNDERGO
and A STATE MAY UNDERGO both belong to the
class social activity. For each feature class, we enu-
merate all words that correspond to its feature in-
stances. Examples are shown in Table 4.
Not all events can be unambiguously classified
into one of the subcategories. However, this is also
not necessary because these categories overlap with
one another. For example, death describes an event
that tends to occur both individually and briefly. In
addition to the six categories listed here, new cate-
gories can be added by creating more feature classes.
Automatic Clustering Representing each noun as
a frequency vector over the top 100 most discrim-
inating contextual features, we employed k-means
clustering and compared the results to our manually
crafted subcategories.
Through trial-and-error, we set k to 12, with the
smallest resulting cluster containing 2 nouns (inter-
pretation and perception), while the biggest result-
ing cluster contained 320 event nouns (that seemed
to share no apparent semantic properties). Other
clusters varied from 5 to 50 words in size, with ex-
amples shown in Table 5.
The advantage of automatic clustering is that the
results may reflect an English speaker?s impression
of word similarity gained through language use. Un-
40
a person-or-organization may undergo a a state may undergo a a can be attempted
a country may undergo a a child may have a a can be for a country
a company may undergo a a project may undergo a authority may undergo a
an explanation can be for a an empire may undergo a a war may undergo a
days may have a a can be abrupt a can be rapid
Table 3: Top 15 features that promote activation of the event-noun target, ranked from most weighted to least.
human events: adoption, arrival, birth, betrayal,
death, development, disappearance, emancipation,
funeral . . .
events of inanimate objects: collapse, construc-
tion, definition, destruction, identification, incep-
tion, movement, recreation, removal . . .
individual activities: birth, death, execution, fu-
neral, promotion . . .
social activities: abolition, evolution, federation,
fragmentation, invasion . . .
lasting events: campaign, development, growth,
trial . . .
brief events: awakening, collapse, death, mention,
onset, resignation, thunderstorm . . .
Table 4: Six subcategories of event nouns.
fortunately, the discovered clusters do not typically
come with the same obvious semantic properties as
were defined in manual classification. In the exam-
ple given above, neither of Cluster 1 and Cluster 3
seems to have a centralized semantic theme. But
Cluster 2 seems to be mostly about human activities.
Comparison with WordNet To compare our re-
sults with WordNet resources, we enumerated all
children of the gloss ?something that happens at a
given place and time?, giving 7655 terms (phrases
excluded). This gave a broader range of event nouns,
such as proper nouns and procedures (e.g. 9/11, CT,
MRI), onomatopoeias (e.g. mew, moo), and words
whose event reading is only secondary (e.g. pic-
ture, politics, teamwork). These types of words tend
to have very different contextual features from what
our algorithm had discovered.
While our method may be limited by the choice of
seed examples, we were able to discover event nouns
not classified under this set by WordNet, suggest-
ing that the discovery mechanism itself is a robust
one. Among them were low-frequency nouns (e.g.
crescendo, demise, names of processes (e.g. absorp-
Cluster 1 (17): cancellation, cessation, closure,
crackle, crash, demise, disappearance, dismissal, dis-
solution, division, introduction, onset, passing, resig-
nation, reversal, termination, transformation
Cluster 2 (32): alienation, backing, betrayal, contem-
plation, election, execution, funeral, hallucination,
imitation, juxtaposition, killing, mention, moulding,
perfection, prosecution, recognition, refusal, removal,
resurrection, semblance, inspection, occupation, pro-
motion, trial . . .
Cluster 3 (7): development, achievement, arrival,
birth, death, loss, survival
Table 5: Examples resulting from automatic clustering.
tion, evolution), and particular cases like thunder-
storm.
4 Extension to Other Semantic Categories
To verify that our bootstrapping algorithm was not
simply relying on KNEXT?s own event classifica-
tion heuristics, we set the algorithm to learn the
distinction between physical and non-physical ob-
jects/entities.
(Non-)Physical-object Nouns 15 physical-object/
entity nouns (e.g. knife, ring, anthropologist) and
34 non-physical ones (e.g. happiness, knowledge)
were given to the model as the initial training set.
At the 9th iteration, the number of discovered physi-
cal objects (which form the minority group between
the two) approaches 2,200 and levels off. We ran-
domly sampled five 20-word groups (a subset of
these words are listed in Table 6) from this entire
set of discovered physical objects, and computed an
average error rate of 4%. Prominent features of the
model at the 9th iteration are shown in Table 7.
5 Related Work
The method of using distributional patterns in a
large text corpus to find semantically related En-
41
heifer, sheriff, collector, hippie, accountant, cape, scab,
pebble, box, dick, calculator, sago, brow, ship, ?john,
superstar, border, rabbit, poker, garter, grinder, million-
aire, ash, herdsman, ?cwm, pug, bra, fulmar, *cam-
paign, stallion, deserter, boot, tear, elbow, cavalry,
novel, cardigan, nutcase, ?bulge, businessman, cop, fig,
musician, spire, butcher, dog, elk, . . .
Table 6: Physical-object nouns randomly sampled from
results; words with an asterisk are misclassified, ones
with a question mark are doubtful.
a male-individual can be a a can be small
a person can be a a can be large
a can be young a can be german
-S*morphological feature a can be british
a can be old a can be good
Table 7: Top-10 features that promote activation of the
physical-object target in the model.
glish nouns first appeared in Hindle (1990). Roark
and Charniak (1998) constructed a semantic lexicon
using co-occurrence statistics of nouns within noun
phrases. More recently, Liakata and Pulman (2008)
induced a hierarchy over nominals using as features
knowledge fragments similar to the sort given by
KNEXT. Our work might be viewed as aiming for
the same goal (a lexico-semantic based partition-
ing over nominals, tied to corpus-based knowledge),
but allowing for an a priori bias regarding preferred
structure.
The idea of bootstrapping lexical semantic prop-
erties goes back at least to Hearst (1998), where the
idea is suggested of using seed examples of a rela-
tion to discover lexico-syntactic extraction patterns
and then using these to discover further examples
of the desired relation. The Basilisk system devel-
oped by Thelen and Riloff (2002) almost paralleled
our effort. However, negative features ? features
that would prevent a word from being classified into
a semantic category ? were not considered in their
model. In addition, in scoring candidate words, their
algorithm only looked at the average relevance of
syntactic patterns. Our perceptron-based algorithm
examines the combinatorial effect of those patterns,
which has yielded results suggesting improved ac-
curacy and bootstrapping efficacy.
Similar to our experiments here using k-means,
Lin and Pantel (2001) gave a clustering algorithm
for iteratively building semantic classes, using as
features argument positions within fragments from
a syntactic dependency parser.
6 Conclusion
We have presented a bootstrapping approach for cre-
ating semantically tagged lexicons. The method can
effectively classify nouns with contrasting semantic
properties, even when the initial training set is a very
small. Further classification is possible with both
manual and automatic methods by utilizing individ-
ual contextual features in the optimal model.
Acknowledgments
This work was supported by NSF grants IIS-
0328849 and IIS-0535105.
References
BNC Consortium. 2001. The British National Corpus,
version 2 (BNC World). Distributed by Oxford Uni-
versity Computing Services.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Marti A. Hearst. 1998. Automated discovery of Word-
Net relations. In (Fellbaum, 1998), pages 131?153.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In ACL.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In ACL.
Maria Liakata and Stephen Pulman. 2008. Auto-
matic Fine-Grained Semantic Classification for Do-
main Adaption. In Proceedings of Semantics in Text
Processing (STEP).
Dekang Lin and Patrick Pantel. 2001. Induction of se-
mantic classes from natural language text. In KDD.
Brian Roark and Eugene Charniak. 1998. Noun-phrase
co-occurrence statistics for semi-automatic semantic
lexicon construction. In ACL, pages 1110?1116.
Frank Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65(6):386?408.
Lenhart K. Schubert. 2002. Can we derive general world
knowledge from text? In HLT.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In EMNLP.
Benjamin Van Durme, Ting Qian, and Lenhart Schubert.
2008. Class-driven Attribute Extraction. In COLING.
42
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 18?26,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Is there syntactic adaptation in language comprehension?
Alex B. Fine, Ting Qian, T. Florian Jaeger and Robert A. Jacobs
Department of Brain and Cognitive Sciences
University of Rochester
Rochester, NY, USA
{afine, tqian, fjaeger, robbie}@bcs.rochester.edu
Abstract
In this paper we investigate the manner in
which the human language comprehension
system adapts to shifts in probability dis-
tributions over syntactic structures, given
experimentally controlled experience with
those structures. We replicate a classic
reading experiment, and present a model
of the behavioral data that implements a
form of Bayesian belief update over the
course of the experiment.
1 Introduction
One of the central insights to emerge from ex-
perimental psycholinguistics over the last half
century is that human language comprehension
and production are probability-sensitive. Dur-
ing language comprehension, language users ex-
ploit probabilistic information in the linguistic sig-
nal to make inferences about the speaker?s most
likely intended message. In syntactic compre-
hension specifically, comprehenders exploit statis-
tical information about lexical and syntactic co-
occurrence statistics. For instance, (1) is temporar-
ily ambiguous at the noun phrase the study, since
the NP can be parsed as either the direct object
(DO) of the verb acknowledge or as the subject
NP of a sentence complement (SC).
(1) The reviewers acknowledged the study...
? DO: ... in the journal.
? SC: ... had been revolutionary.
The ambiguity in the SC continuation is resolved
at had been, which rules out the direct object in-
terpretation of the study. Reading times at had
been?the so-called point of disambiguation?are
correlated with a variety of lexical-syntactic prob-
abilities. For instance, if the probability of a SC
is low, given the verb, subjects are garden-pathed
and will display longer reading times at had been.
Conversely, if the probability of a SC is high, the
material at the point of disambiguation is rela-
tively unsurprising (i.e. conveys less information),
and reading times will be short. Readers are also
sensitive to the probability of the post-verbal NP
occurring as the direct object of the verb. This is
often discussed in terms of plausibility?in (1), the
study is a plausible direct object of acknowledge
(relative to, say, the window), which will also con-
tribute to longer reading times in the event of a SC
continuation (Garnsey et al, 1997).
Thus, humans make pervasive use of proba-
bilistic cues in the linguistic signal. A question
that has received very little attention, however, is
how language users maintain or update their rep-
resentations of the probability distributions rele-
vant to language use, given new evidence?a phe-
nomenon we will call adaptation. That is, while
we know that language users have access to lin-
guistic statistics, we know little about the dynam-
ics of this knowledge in language users: is the
probabilistic information relevant to comprehen-
sion derived from experience during a critical pe-
riod of language acquisition, or do comprehenders
update their knowledge on the basis of experience
throughout adulthood? A priori, both scenarios
seem plausible?given the sheer number of cues
relevant to comprehension, it would be advanta-
geous to limit the resources devoted to acquiring
this knowledge; on the other hand, any learner?s
linguistic experience is bound to be incomplete, so
the ability to adapt to novel distributional patterns
in the linguistic input may prove to be equally use-
ful. The goal of this paper is to explore this is-
sue and to take an initial step toward providing a
computational framework for characterizing adap-
tation in language processing.
1.1 Adaptation in Sentence Comprehension
Both over time and across situations, humans are
exposed to linguistic evidence that, in principle,
18
ought to lead to shifts in our representations of the
relevant probability distributions. An efficient lan-
guage processing system is expected to take new
evidence into account so that behavior (decisions
during online production, predictions about up-
coming words, etc.) will be guided by accurate es-
timates of these probability distributions. At least
at the level of phonetic perception and produc-
tion, there is evidence that language users quickly
adapt to the statistical characteristics of the am-
bient language. For instance, over the course of
a single interaction, the speech of two interlocu-
tors becomes more acoustically similar, a phe-
nomenon known as spontaneous phonetic imita-
tion (Goldinger, 1998). Perhaps even more strik-
ingly, Clayards et al (2008) demonstrated that,
given a relatively small number of tokens, compre-
henders shift the degree to which they rely on an
acoustic cue as the variance of that cue changes,
reflecting adaptation to the distributions of proba-
bilistic cues in speech perception.
At the level of syntactic processing, belief up-
date/adaptation has only recently been addressed
(Wells et al, 2009; Snider and Jaeger, in prep). In
this study, we examine adaptation at the level of
syntactic comprehension. We provide a computa-
tional model of short- to medium-term adaptation
to local shifts in the statistics of the input. While
the Bayesian model presented can account for the
behavioral data, the quality of the model depends
on how control variables are treated. We discuss
the theoretical and methodological implications of
this result.
Section 2 describes the behavioral experiment,
a slight modification of the classic reading experi-
ment reported in Garnsey et al (1997). The study
reported in section 3 replicates the basic findings
of (Garnsey et al, 1997). In sections 4 and 5
we outline a Bayesian model of syntactic adapta-
tion, in which distributions over syntactic struc-
tures are updated at each trial based on the ev-
idence in that trial, and discuss the relationship
between the model results and control variables.
Section 6 concludes.
2 Behavioral Experiment
2.1 Participants
Forty-six members of the university community
participated in a self-paced reading study for pay-
ment. All were native speakers of English with
normal or corrected to normal vision, based on
self-report.
2.2 Materials
Subjects read a total of 98 sentences, of which 36
were critical items containing DO/SC ambiguities,
as in (1). These 36 sentences comprise a subset of
those used in Garnsey et al (1997). The stim-
uli were manipulated along two dimensions: first,
verbs were chosen such that the conditional prob-
ability of a SC, given the verb, varied. In Garnsey
et al (1997), this conditional probability was es-
timated from a norming study, in which subjects
completed sentence fragments containing DO/SC
verbs (e.g. the lawyer acknowledged...). We adopt
standard psycholinguistic terminology and refer
to this conditional probability as SC-bias. The
verbs used in the critical sentences in Garnsey et
al. (1997) were selected to span a wide range of
SC-bias values, from .01 to .9. Each sentence con-
tained a different DO/SC verb. In addition to SC-
bias, half of the sentences presented to each sub-
ject included the complementizer that, as in (2).
(2) The reviewers acknowledged that the
study had been revolutionary.
Sentences with a complementizer were included
as an unambiguous baseline (Garnsey et al 1997).
The presence of a complementizer was counter-
balanced, such that each subject saw half of the
sentences with a complementizer and all sen-
tences occurred with and without a complemen-
tizer equally often across subjects. All of the criti-
cal sentences contained a SC continuation. The 36
critical items were interleaved with 72 fillers that
included simple transitives and intransitives.
2.3 Procedure
Subjects read critical and filler sentences in a self-
paced moving window display (Just et al, 1982),
presented using the Linger experimental presen-
tation software (Rohde, 2005). Sentences were
presented in a noncumulative word-by-word self-
paced moving window. At the beginning of each
trial, the sentence appeared on the screen with all
non-space characters replaced by a dash. Using
their dominant hands, subjects pressed the space
bar to view each consecutive word in the sen-
tence. Durations between space bar presses were
recorded. At each press of the space bar, the
currently-viewed word reverted to dashes as the
next word was converted to letters. A yes/no com-
19
prehension question followed all experimental and
filler sentences.
2.4 Analysis
In keeping with standard procedure, we used
length-corrected residual per-word reading times
as our dependent measure. Following Garnsey et
al. (1997), we define the point of disambiguation
in the critical sentences as the two words follow-
ing the post-verbal NP (e.g. had been in (1) and
(2)). All analyses reported here were conducted on
residual reading times at this region. For a given
subject, residual reading times more than two stan-
dard deviations from that subject?s mean residual
reading time were excluded.
3 Study 1
Residual reading times at the point of disambigua-
tion were fit to a linear mixed effects regression
model. This model included the full factorial de-
sign (i.e. all main effects and all interactions) of
logged SC-bias (taken from the norming study re-
ported in Garnsey et al 1997) and complemen-
tizer presence. Additionally, the model included
random intercepts of subject and item. This was
the maximum random effect structure justified by
the data, based on comparison against more com-
plex models.1 All predictors in the model were
centered at zero in order to reduce collinearity.
P-values reported in all subsequent models were
calculated using MCMC sampling (where N =
10,000).
3.1 Results
This model replicated the findings reported by
Garnsey et al (1997). There was a significant
main effect of complementizer presence (? =
?3.2, t = ?2.5, p < .05)?reading times at
the point of disambiguation were lower when
the complementizer was present. Additionally,
there was a significant two-way interaction be-
tween complementizer presence and logged SC-
bias (? = 3.0, t = 2.5, p < .05)?SC-bias has a
stronger negative correlation with reading times in
the disambiguating region when the complemen-
tizer is absent, as expected. Additionally, Gar-
nsey et al (1997) found a main effect of SC-bias.
For us, this main effect did not reach significance
1For a detailed description of the procedure used,
see http://hlplab.wordpress.com/2009/05/14/random-effect-
should-i-stay-or-should-i-go/
(? = ?1.2, t = ?1.11, p = .5), possibly owing to
the fact that we tested a much smaller sample than
Garnsey et al (1997) (51 compared to 82 partici-
pants).
4 Study 2: Bayesian Syntactic
Adaptation
Reading times at the point of disambiguation in
these stimuli reflect, among other things, sub-
jects? estimates of the conditional probability
p(SC|verb) (Garnsey et al 1997), which we have
been calling SC-bias. Thus, we model the task fac-
ing subjects in this experiment as one of Bayesian
inference, where subjects are, when reading a sen-
tence containing the verb vi, inferring a posterior
probability P(SC|vi), i.e. the probability that a
sentence complement clause will follow a verb vi.
According to Bayes rule, we have:
p(SC|vi) =
p(vi|SC)p(SC)
p(vi)
(1)
In Equation (1), we use the relative frequency
of vi (estimated from the British National Corpus)
as the estimate for p(vi). The first term in the nu-
merator, p(vi|SC), is the likelihood, which we es-
timate by using the relative frequency of vi among
all verbs that can take a sentence complement as
their argument. These values are taken from the
corpus study by Roland et al (2007). Roland et al
(2007) report, among other things, the number of
times a SC occurs as the argument of roughly 200
English verbs. These values are reported across a
number of corpora. We use the values from the
BNC to compute p(vi|SC).
The prior probability of a sentence complement
clause, p(SC), is the estimate of interest in this
study. We hypothesize that, under the assumptions
of the current model, subjects update their esti-
mate for p(SC) based on the evidence presented
in each trial. As a result, the posterior probability
varies from trial to trial, not only because the verb
used in each stimulus is different, but also because
the belief about the probability of a sentence com-
plement is being updated based on the evidence in
each trial. We employ the beta-binomial model to
simulate this updating process, as described next.
4.1 Belief Update
We adopt an online training paradigm involving
an ideal observer learning from observations. Af-
ter observing a sentence containing a DO/SC verb,
20
we predict that subjects will update both the likeli-
hood p(vi|SC) for that verb, as well as the proba-
bility p(SC). Because each verb occurs only once
for a given subject, the effect of updating the first
quantity is impossible to measure in the current ex-
perimental paradigm. We therefore focus on mod-
eling how subjects update their belief of p(SC)
from trial to trial.
We make the simplifying assumption that the
only possible argument that DO/SC verbs can take
is either a direct object or a sentence complement
clause. Further, subjects are assumed to have an
initial belief about how probable a sentence com-
plement is, on a scale of 0 to 1. Let ? denote
this probability estimate, and p(?) the strength of
this estimate. From the perspective of an ideal
observer, p(?) will go up for ? > 0.5 when a
DO/SC verb is presented with a sentence comple-
ment as its argument. This framework assumes
that subjects do not compute ? by merely relying
on frequency (otherwise, ? will be simply the ra-
tio between SC and DO structures in a block of
trials), but they have a distribution P (?), where
each possible estimate of ? is associated with a
probability indicating the confidence on that es-
timate. In order to make our results comparable
to existing models, however, we use the expected
value of P (?) in each iteration of training as point
estimates. Therefore, for one subject, we have
36 estimated ?? values, each corresponding to the
changed belief after seeing a sentence containing
SC in an experiment of 36 trials. Because none
of the filler items included DO/SC verbs, we as-
sume that filler trials have no effect on subjects?
estimates of P (?).
Since all stimuli in our experiment have the SC
structure, the general expectation is the distribu-
tion P (?) will shift towards the end where ? = 1.
Our belief update model tries to capture the shape
of this shift during the course of the experiment.
Using Bayesian inference, we can describe the up-
dating process as the following, where ?i repre-
sents a particular belief of the value ?.
p(? = ?i|obs.) =
p(obs.|? = ?i)p(? = ?i)
p(obs.)
= p(obs.|? = ?i)p(? = ?i)? 1
0
p(obs.|?)p(?) d?
(2)
This posterior probability is hypothesized to re-
flect how likely a subject would consider the prob-
ability of SC to be ?i after being exposed to one
experimental item. We discretized ? to 100 evenly
spaced ?i values, ranging from 0 to 1. Thus, the
denominator can be calculated by marginalizing
over the 100 ?i values. The two terms in the nu-
merator in Equation (2) are estimated in the fol-
lowing manner.
Likelihood function p(obs.|? = ?i) is modeled
by a binomial distribution, where the parameters
are ?i (the probability of observing a SC clause)
and 1 ? ?i (the probability of observing a direct
object), and where the outcome is the experimen-
tal item presented to the subject. Therefore:
p(obs.|? = ?i) =
(nsc + ndo)!
nsc!ndo!
?nsci (1? ?i)ndo
(3)
In the current experiment, ndo is always 0 since
all stimuli contain the SC argument. In addition,
between-trial reading time differences are mod-
elled at one item a step for each subject so that nsc
is always 1 in each trial. It is in theory possible to
set nsc to other numbers.
The prior In online training, the posterior of the
previous iteration is used as the prior for the cur-
rent one. Nevertheless, the prior p(? = ?i) for
the very first iteration of training needs to be es-
timated. Here we assume a beta distribution with
parameters ? and ?. The probability of the prior
then is:
p(? = ?i) =
???1i (1? ?i)??1
B(?,?)
Intuitively, ? and ? capture the number of times
subjects have observed the SC and DO outcomes,
respectively, before the experiment. In the context
of our research, this model assumes that subjects?
beliefs about p(SC) and p(DO) are based on ??1
observations of SC and ? ? 1 observations of DO
prior to the experiment.
The values of the parameters of the beta distri-
bution were obtained by searching through the pa-
rameter space with an objective function based on
the Bayesian information criterion (BIC) score of
a regression model containing the log of the pos-
terior computed using the updated prior p(SC),
complementizer presence, and the two-way inter-
action. The BIC (Schwarz, 1978) is a measure
of model quality that weighs the models empirical
coverage against its parsimony (BIC = 2ln(L)+
21
k ? ln(n), where k is the number of parameters in
the model, n the number of data points, and L is
the models data likelihood). Smaller BIC indicate
better models. The ? and ? values yielding the
lowest BIC score are used.
In estimating ? and ?, we considered all pairs of
non-negative integers such that both values were
below 1000. The values of ? and ? used here were
1 and 177, respectively. These values do not im-
ply that subjects have seen only 1 SC and 177 DOs
prior to the experiment, but that only this many ob-
servations inform subjects? prior beliefs about this
distribution. The relationship between the choice
of the parameters of the beta distribution, ? and
?, and the BIC of the model used in the parameter
estimation is shown in Figure 1.
Beta
Alpha
BIC
Figure 1: The relationship between the BIC of the
model used in the parameter estimation step and
values of ? and ? in the beta distribution
Because we model subjects? estimates of
p(SC|vi) in terms of Bayesian inference, with a
continuously updated prior, p(SC), the value of
p(SC|vi) depends, in our model, on both verb-
specific statistics (i.e. the likelihood p(vi|SC) and
the probability of the verb p(vi)) and the point in
the experiment at which the trial containing that
verb is encountered. We can visualize this rela-
tionship in Figure 2, which shows the values given
by the model of p(SC|vi) for four particular dif-
ferent verbs, depending on the point in the experi-
ment at which the verb is seen.
The approach we take is hence fundamentally
Presentation Order
Pos
terio
r p(S
C|v)
0.2
0.4
0.6
0.8
0 10 20 30
confide deny
know
0 10 20 30
0.2
0.4
0.6
0.8
print
Figure 2: The relationship, for four of the verbs,
between the value of p(SC|vi) given by the model
as a function of when in the experiment vi is en-
countered
different from the approach commonly taken in
psycholinguistics, which is to use static estimates
of quantities such as p(SC|vi) derived from cor-
pora or norming studies.
4.2 Analysis
To test whether the model-derived values of
p(SC|vi) are a good fit for the behavioral data,
we fit residual reading times at the point of dis-
ambiguation using linear mixed effects regression.
The model included main effects of p(SC|vi)?as
given by the model just described?and comple-
mentizer presence, as well as the two-way inter-
action between these two predictors. Additionally,
there were random intercepts of subject and item.
p(SC|vi) was logged and centered at zero.
4.3 Results
There was a highly significant main effect of
the posterior probability p(SC|vi) yielded by the
beta-binomial model (? = ?40, t = ?21.2, p <
.001), as well as a main effect of complemen-
tizer presence (?4.5, t = ?3.7, p < .001).
The two-way interaction between complementizer
presence and the posterior probability from the
beta-binomial model did not reach significance
(? = 0.5, t = .5, p > .05). The reason is likely
that, in the analysis presented for Study 1, we can
interpret the interaction as indicating that when
22
SC-bias is high, the complementizer has less of
an effect; in our model, the posterior probabil-
ity p(SC|vi) is both generally higher and has less
variance than the same quantity when based on
corpus- or norming study estimates, since the prior
probability p(SC) is continuously increasing over
the course of the experiment. This would have the
effect of eliminating or at least obscuring the in-
teraction with complementizer presence.
The posterior p(SC|vi) has a much stronger
negative correlation with residual reading times
than the measure of SC-bias used in Study 1 (? =
?40 as opposed to ? = ?1.2).
4.4 Discussion
So far, we have replicated a classic finding in the
sentence processing literature (Study 1), provided
evidence that subjects? estimates of the conditional
probability p(SC|vi) change based on evidence
throughout the experiment, and that this process
is captured well by a model which implements a
form of incremental Bayesian belief update. We
take this as evidence that the language comprehen-
sion system is adaptive, in the sense that language
users continually update their estimates of proba-
bility distributions over syntactic structures.
5 Syntactic Adaptation vs. Motor
Adaptation
The results of the model presented in section 4
are amenable to (at least) two explanations. We
have hypothesized that, given exposure to new ev-
idence about probability distributions over syn-
tactic structures in English, subjects update their
beliefs about these probability distributions, re-
flected in reading times?a phenomenon we refer
to as syntactic adaptation. An alternative explana-
tion, however, is one that appeals to motor adap-
tation, rather than syntactic adaptation. Specifi-
cally, it could be that subjects are simply adapt-
ing to the task?rather than to changes in syntactic
distributions?as the experiment proceeds, lead-
ing to faster reading times.
We expect the effect of motor adaptation to
be captured by presentation order, or the point
in the experiment at which subjects encounter a
given stimulus. In particular, we predict a neg-
ative correlation between presentation order and
reading times. Unfortunately, in the current ex-
periment, presentation order and p(SC|vi) derived
from the Beta-binomial model are positively cor-
related (r = .6)?the latter increases with increas-
ing presentation order, since participants only see
SC continuations. The results we observed above
could hence also be due to an effect of presentation
order.
The expected shape of a possible effect of task
adaptation is not obvious. That is, it is not clear
whether the relationship between presentation or-
der and reading times will be linear. On the one
hand, linearity would be the default assumption
prior to theoretical considerations about the dis-
tributional properties of presentation order. On
the other hand, presentation order is a lower-
bounded variable, which often are distributed ap-
proximately log-normally. Additionally, it is pos-
sible that there may be a floor effect: participants
may get used to having to press the space bar to ad-
vance to the next word and may quickly get faster
at that procedure until RTs converge against the
minimal time it takes to program the motor move-
ment to press the space bar. Such an effect would
likely lead to an approximately log-linear effect of
presentation order.
We test for an effect of motor adaptation by ex-
amining the effect of presentation order on read-
ing times, comparing the effect of linear and log-
transformed presentation order.
5.1 Controlling for Presentation Order in the
Beta-binomial model
We test for separate effects of syntactic adaptation
and motor adaptation by conducting stepwise re-
gressions with two models containing the full fac-
torial design of the Beta-binomial posterior, com-
plementizer presence, and, for the first model, a
linear effect of presentation order and, for the
second model, log-transformed presentation order.
We conducted stepwise regressions using back-
ward elimination, starting with all predictors and
removing non-significant predictors (i.e. p > .1),
one at a time, until all non-significant predictors
are deleted.
For both the model including a linear effect
of presentation order and a model including log-
transformed presentation order, the final mod-
els resulting from the stepwise regression proce-
dure included only main effects of complemen-
tizer presence and log presentation order. These
models are summarized in Figure 1, which in-
cludes coefficient-based tests for significance of
each of the predictors (i.e. whether the coefficient
23
is significantly different from zero) as well as ?2-
based tests for significance (i.e. the difference be-
tween a model with that predictor and one with-
out). Comparing the two resulting models based
on the Bayesian Information Criterion, the model
containing log-transformed presentation order is a
better model than one with a linear effect of pre-
sentation order (BIClog = 37467; BICnon?log =
37510).
Pres. order untransformed
Coef. and ?2-based tests
Predictor ? p ?2 p
Comp. pres. ?4.3 < .05 4.9 < .05
Pres. order ?.7 < .001 28.2 < .001
Pres. order log-transformed
Coef. and ?2-based tests
Predictor ? p ?2 p
Comp. pres. ?4.3 < .05 4.8 < .05
Pres. order ?33.8 < .001 29.4 < .001
Table 1: Coefficient- and ?2-based tests for sig-
nificance of model resulting from stepwise regres-
sion
In sum, the beta-binomial derived posterior ap-
pears to have no predictive power after presenta-
tion order is controlled for. This result does not
depend on how presentation order is treated (i.e.
log-transformed or not).
5.2 The interaction between SC-bias and
presentation order
The results from the previous section suggest that
the Beta-binomial derived posterior carries no pre-
dictive power after presentation order is controlled
for. Is there any evidence at all for syntactic adap-
tation (as opposed to motor, or task, adaptation)?
To attempt to answer this, we analyzed the read-
ing data using the model reported in section 3,
with an additional main effect of presentation or-
der, as well as the interactions between presenta-
tion order and the other predictors in the model.
An overall decrease in reading times due to mo-
tor adaptation should surface as a main effect of
presentation order, as mentioned; syntactic adap-
tation, however, is predicted to show up as a two-
way interaction between SC-bias and presentation
order?since subjects only see SC continuations,
subjects should expect this outcome to become
more and more probable over the course of the ex-
periment, causing the correlation between SC-bias
and reading times to become weaker (thus we pre-
dict the interaction to have a positive coefficient).
To test for such an interaction, we performed
a stepwise regressions with two models contain-
ing the full factorial design of SC-bias, comple-
mentizer presence, and, for the first model, a lin-
ear effect of presentation order and, for the second
model, log-transformed presentation order. The
stepwise regression procedure here was identical
to the one reported in the previous section.
For both models, the remaining predictors were
main effects of presentation order, complemen-
tizer presence, and SC-bias, as well as a two-way
interaction between SC-bias and complementizer
presence and a two-way interaction between SC-
bias and presentation order. The results of these
models are given in Table 2.
Pres. order untransformed
Coef. and ?2-based tests
Predictor ? p ?2 p
SC-bias ?.4 = .8 11.5 < .001
Comp. pres. ?4.4 < .001 18.1 < .001
Pres. order ?.9 < .001 420.9 < .001
SC-bias:Comp. 2.6 < .05 5.3 < .05
SC-bias:Pres. Order .1 < .05 6.2 < .05
Pres. order log-transformed
Coef. and ?2-based tests
Predictor ? p ?2 p
SC-bias ?1.4 = .5 8.9 < .05
Comp. pres. ?4.6 < .001 19.3 < .001
Pres. order ?42.4 < .001 461.2 < .001
SC-bias:Comp. 2.6 < .05 5.2 < .05
SC-bias:Pres. Order 3.5 = .06 3.4 = .06
Table 2: Coefficient- and ?2-based tests for sig-
nificance of model resulting from stepwise regres-
sion
The main findings reported in Study 1 (i.e. a
main effect of complementizer presence and a
two-way interaction between SC-bias and com-
plementizer presence) are replicated here, and do
not depend on whether presentation order is log-
transformed. However, the interaction between
SC-bias and presentation order is less reliable
when presentation order is log-transformed, reach-
ing only marginal significance. In short, an ad-
equate account of the data requires reference to
both motor adaptation (in the form of a main effect
of presentation order, log-transformed) and syn-
tactic adaptation.
If subjects are improving at the task, and the
effect of presentation order represents a kind of
adaptation to the task of self-paced reading, we
would expect to find a main effect of presenta-
tion order on reading times at all regions. This
24
is the case?a strong negative correlation between
presentation order and reading times holds across
all regions. Evidence that the observed interac-
tion is due to syntactic belief update comes from
the fact that the interaction between SC-bias and
presentation order, unlike the main effect of pre-
sentation order, is limited to the disambiguating
region of the sentence. We performed the regres-
sion reported above on residual reading times at
the main verb (e.g. acknowledge), ambiguous (e.g.
the study), and disambiguating (e.g. had been) re-
gions. These analyses revealed, as expected, main
effects of presentation order across all regions. At
the verb and ambiguous regions, however, presen-
tation order did not interact with SC-bias.
Region ? p? value
Main effect of pres. order
Verb ?.95 < .001
Ambig. region ?.9 < .001
Disambig. region ?.9 < .001
Pres. order X SC-bias interaction
Verb .09 = .24
Ambig. region .04 = .37
Disambig. region .1 < .05
Table 3: Main effect of presentation order and in-
teraction of presentation order with SC-bias at dif-
ferent regions in the critical sentences
This finding provides initial evidence that sub-
jects adapt their linguistic expectations to the evi-
dence observed throughout the experiment. How-
ever, the interaction between presentation order
and SC-bias in this analysis is amenable to an al-
ternative interpretation: interactions between pre-
sentation order and other variables could emerge
if subjects? reaction times reach some minimum
value over the course of the experiment, causing
any other variable to become less strongly corre-
lated with the dependent measure as reaction times
approach that minimum value. Thus this interac-
tion could be an artefact of a floor effect.
To test the possibility that the SC-bias-
presentation order interaction is the result of a
floor effect, we compared the 1st, 5th, and 10th
fastest percentiles of residual reading times across
all regions. As shown in Figure 3, faster reading
times are observed at each quantile in at least one
other region. In other words, reading times in the
disambiguating region do not seem to be bounded
by motor demands associated with the task. We
hence tentatively conclude that the interaction be-
tween SC-bias and log-transformed presentation
order is not the result of a floor effect, although
this issue deserves further attention.
Figure 3: Minimum and upper boundary of 1st,
5th, and 10th percentile values of residual reading
times across all sentence regions
6 Conclusion
We hypothesized that the language comprehension
system rapidly adapts to shifts in the probability
distributions over syntactic structures on the ba-
sis of experience with those structures. To in-
vestigate this phenomenon, we modelled reading
times from a self-paced reading experiment us-
ing a Bayesian model of incremental belief up-
date. While an initial test of the Beta-binomial
model was encouraging, the predictions of the
Beta-binomial model are highly correlated with
presentation order in the current data set. This
means that it is hard to distinguish between adap-
tation to the task of self-paced reading and syntac-
tic adaptation. Indeed, model comparison suggests
that the Bayesian model does not explain a signif-
icant amount of the variance in reading times once
motor adaptation (as captured by stimulus presen-
tation order) is accounted for. In a secondary anal-
ysis, we did, however, find preliminary evidence
of syntactic adaptation. That is, while the Beta-
binomial model does not seem to capture syntac-
tic belief update adequately, there is evidence that
comprehenders continuously update their syntac-
tic distributions.
25
Teasing apart the effects of motor adaptation
and linguistic adaptation will require experimen-
tal designs in which these two factors are not as
highly correlated as in the present study. Ongoing
work addresses this issue.
Acknowledgements
The authors wish to thank Neal Snider and mem-
bers of the Human Language Processing lab, as
well as three anonymous ACL reviewers for help-
ful discussion and feedback. We are also very
grateful to Jeremy Ferris for help in collecting the
data reported here. This work was supported by
the University of Rochesters Provost Award for
Multidisciplinary Research and NSF grant BCS-
0845059 to TFJ.
References
John Anderson. 1990. The adaptive character of
thought. Lawrence Erlbaum.
Bock and Griffin. 2000. The persistence of structural
priming: Transient activation or implicit learning?
Journal of Experimental Psychology, 129(2):177?
192.
Chang, Dell, and Bock. 2006. Becoming syntactic.
Psychological Review, 113(2):234?272.
Clayards, Tanenhaus, Aslin, and Jacobs. 2008. Per-
ception of speech reflects optimal use of probabilis-
tic cues. Cognition, 108:804?809.
Garnsey, Pearlmutter, Myers, and Lotocky. 1997.
The contributions of verb bias and plausibility to
the comprehension of temporarily ambiguous sen-
tences. Journal of Memory and Language, (37):58?
93.
S.D. Goldinger. 1998. Echoes of echoes? an episodic
theory of lexical access. Psychological Review,
(105):251?279.
Florian Jaeger. in press. Redundancy and reduc-
tion: speakers manage syntactic information density.
Cognitive Psychology.
Just, Carpenter, and Woolley. 1982. Paradigms and
processes in reading comprehension. Journal of Ex-
perimental Psychology: General, 111:228?238.
Rohde. 2005. Linger experiment presentation soft-
ware. http://tedlab.mit.edu/ dr/Linger/.
Schwarz. 1978. Estimating the dimension of a model.
Annals of Statistics, 6:461?464.
Herbert Simon, 1987. 77K New Palgrave Dictionary
of Economics, chapter Bounded Rationality, pages
266?268. Macmillan, London.
Neal Snider and Florian Jaeger. in prep.
Thothathiri and Snedeker. 2008. Give and take:
Syntactic priming during language comprehension.
Cognition, 108:51?68.
Wells, Christiansen, Race, Acheson, and MacDonald.
2009. Experience and sentence comprehension:
Statistical learning and relative clause comprehen-
sion. Cognitive Psychology, 58:250?271.
26
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 45?53,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Close = Relevant? The Role of Context in Efficient Language Production
Ting Qian and T. Florian Jaeger
Department of Brain and Cognitive Sciences
University of Rochester
Rochester, NY 14627 United States
{tqian,fjaeger}@bcs.rochester.edu
Abstract
We formally derive a mathematical model
for evaluating the effect of context rele-
vance in language production. The model
is based on the principle that distant con-
textual cues tend to gradually lose their
relevance for predicting upcoming linguis-
tic signals. We evaluate our model against
a hypothesis of efficient communication
(Genzel and Charniak?s Constant Entropy
Rate hypothesis). We show that the devel-
opment of entropy throughout discourses
is described significantly better by a model
with cue relevance decay than by previ-
ous models that do not consider context ef-
fects.
1 Introduction
In this paper, we present a study on the effect
of context relevance decay on the entropy of lin-
guistic signals in natural discourses. Context rele-
vance decay refers to the phenomenon that contex-
tual cues that are distant from an upcoming event
(e.g. production of a new linguistic signal) are less
likely to be relevant to the event, as discourse con-
tents that are close to one another are likely to be
semantically related. One can also view the words
and sentences in a discourse as time steps, where
distant context becomes less relevant simply due
to normal forgetting over time (e.g. activation de-
cay in memory). The present study investigates
how this decaying property of discourse context
might affect the development of entropy of lin-
guistic signals in discourses. We first introduce
the background on efficient language production
and then propose our hypothesis.
1.1 Background on Efficient Language
Production
The metaphor ?communication channel?, bor-
rowed from Shannon?s information theory (Shan-
non, 1948), can be conceived of as an abstract en-
tity that defines the constraints of language com-
munication (e.g. ambient noise, distortions in ar-
ticulation). For error free communication to occur,
the ensemble of messages that a speaker may utter
must be encoded in a system of signals whose en-
tropy is under the capacity of the communication
channel. Entropy of these signals, in this context,
correlates with the average number of upcoming
messages that the speaker can choose from for a
particular signal (e.g. a word to be spoken) given
preceding discourse context. In other words, if
the average number of choices given any linguis-
tic signal exceeds the channel capacity, it cannot
be guaranteed that the receiver can correctly infer
the originally intended message. Such transmis-
sion errors will reduce the efficiency of language
communication.
Keeping the entropy of linguistic signals be-
low the channel capacity alone is not efficient, for
one can devise a code where each signal corre-
sponds to a distinct message. With a unique choice
per signal, this encoding achieves an entropy of
zero at the cost of requiring a look-up table that
is too large to be possible (cf. Zipf (1935), who
makes a similar argument for meaning and form).
In fact, the most efficient code requires language
users to encode messages into signals of the en-
tropy bounded by the capacity of the channel. One
implication of this efficient encoding is that over
time, the entropy of the signals is constant. One
of the first studies to investigate such constancy
is Genzel and Charniak (2002), in which the au-
thors proposed the Constant Entropy Rate (CER)
hypothesis: in written text, the entropy per sig-
nal symbol is constant across sentence positions in
discourses. That is, if we view sentence positions
as a measure of time steps, then the entropy per
word at each step should be the same in order to
achieve efficient communication (word is selected
as the unit of signal, although it does not have to
45
be case; cf. Qian and Jaeger (2009)).
The difficulty in testing this direct prediction is
computationally specifying the code used by hu-
man speakers to obtain a context-sensitive esti-
mate of the entropy per word. An ngram model
overestimates the entropy of upcoming messages
by relying on only the preceding n-1 words within
a sentence, while in reality the upcoming message
is also constrained by extra-sentential context that
accumulates within a discourse. The more extra-
sentential context that the ngram model ignores,
the higher estimate for entropy will be. Hence,
the CER hypothesis indirectly predicts that the
entropy of signals, as estimated by ngrams, will
increase across sentence positions. While some
studies have found the predicted positive correla-
tion between sentence position and the per-word
entropy of signals estimated by ngrams, most of
them assumed the correlation to be linear (Genzel
and Charniak, 2002; Genzel and Charniak, 2003;
Keller, 2004; Piantadosi and Gibson, 2008). How-
ever, in previous work, we found that a log-linear
regression model was a better fit for empirical data
than a simple linear regression model based on
data of 12 languages (Qian and Jaeger, under re-
view). Why this would be case remained a puzzle.
Our research question is closely related to this
indirect prediction of the Constant Entropy Rate
hypothesis. Intuitively, the number of possible
messages that a speaker can choose from for an
upcoming signal in a discourse is often restricted
by the presence of discourse context. Contex-
tual cues in the preceding discourse can make the
upcoming content more predictable and thus ef-
fectively reduces signal entropy. As previously
mentioned, however, different contextual cues, de-
pending on how long ago they were provided, have
various degrees of effectiveness in reducing sig-
nal entropy. Thus we ask the question whether
the decay of context relevance could explain the
sublinear relation between entropy and discourse
progress that has been observed in previous stud-
ies.
We formally derive two nonlinear models for
testing our Relevance Decay Hypothesis (intro-
duced next). In addition to the constant entropy as-
sumption in CER, our model assumed that the rel-
evance of early sentences in the discourse system-
atically decays as a function of discourse progress.
Our models provide the best fit to the distribution
of entropy of signals, suggesting the availability
of discourse context can affect the planning of the
rest of a discourse.
1.2 Relevance Decay Hypothesis
We hypothesize the sublinear relation between the
entropy of signals, when estimated out of dis-
course context (hereafter, out-of-context entropy
of signals) using an ngram model, and sentence
position (Piantadosi and Gibson, 2008; Qian and
Jaeger, under review) is due to the role of dis-
course context (hereafter, context). Consider the
following example. Assume that context at the kth
sentence position comes from the 1 . . . k ? 1 sen-
tences in the past. If k is large enough, context
from the early sentences 1 . . . i (i  k) is essen-
tially no longer relevant. Rather, the nearby k ? i
sentences are contributing most of the discourse
context. As a result, the constraint on the entropy
of signals at sentence position k is mostly due to
the nearby window of k ? i sentences. Then if we
look ahead to the (k + 1)th sentence position and
follow the same steps of reasoning, context at that
point also mostly comes from the nearby window
of k? i sentences (i.e. (k+ 1)? (i+ 1) = k? i).
Hence, for later sentence positions, the difference
in available context is minimal. Consequently,
their out-of-context entropy of signals increases
at a very small rate. On the other hand, when k
is fairly small, to the extent that the k ? i win-
dow covers the entire preceding discourse, all of
the 1 . . . k ? 1 sentences are contributing relevant
context. As k increases, the number of preced-
ing sentences increases, which results in a more
significant change in relevant context, but the rel-
evance of each individual sentence decreases with
its distance to k, which results in a sublinear pat-
tern of relevant context with respect to sentence
position overall. As we will show, the relation of
out-of-context entropy of signals to sentence posi-
tion follows from the relation of relevant context
to sentence position, exhibiting a sublinear form
as well.
The problem of interest here is to specify how
quickly the relevance of a preceding sentence de-
cays as a function of its distance to a target sen-
tence position k. We experimented with two forms
of decay functions ? power law decay and expo-
nential decay. It has been established that many
types of human behaviors can be well described by
the power function (Wixted and Ebbesen, 1991),
so we mainly focus on building a model under the
46
Language Training Data Test Data
in words in sentences in words in sentences per position
Danish 154,514 5,640 8,048 270 18
Dutch 50,309 3,255 2,105 90 6
English 597,698 23,295 31,276 1155 77
French 229,461 9,300 11,371 435 29
Italian 97,198 4,245 4,524 225 15
Mandarin Chinese 145,127 4,875 4,310 150 10
Norwegian 89,724 4,125 2,973 150 10
Portuguese 170,342 5,340 9,044 240 16
Russian 398,786 18,075 20,668 930 62
Spanish (Latin-American) 1,363,560 41,160 67,870 2,070 138
Spanish (European) 255,366 7,485 8,653 240 16
Swedish 266,348 11,535 13,369 555 37
Table 1: Number of words and sentences in the training and test data for each of the twelve languages.
The last column gives the number of sentences at each sentence position (which is identical to the number
of documents contained in the corpora).
power law, and examine if the model under the ex-
ponential law yields any difference. Under the as-
sumptions of true entropy rate is constant across
sentences, we predict that our models will bet-
ter characterize the changes in estimated entropy
of signals than general regression models that are
blind to the role of context.
2 Methods
2.1 Data
We used the Reuters Corpus Volume 1 and 2
(Lewis et al, 2004). The corpus contains about
810,000 English news articles and over 487,000
news articles in thirteen languages. Because of in-
consistent annotation, we excluded the data from
three languages, Chinese, German, and Japanese.
For Chinese, we substituted the Treebank Cor-
pus (Xue et al, 2005) for the Reuters data, leav-
ing us with twelve languages: Danish, Dutch,
English, French, Italian, Mandarin Chinese, Nor-
wegian, Portuguese, Russian, European Spanish,
Latin-American Spanish, and Swedish. In order
to estimate out-of-context entropy per word (i.e.
per signal symbol) for each sentence position, ar-
ticles were divided into a training set (95% of all
stories) for training language models and a test set
(the remaining 5%) for analysis (see Table 1 for
details). Out-of-context entropy per word was es-
timated by computing the average log probability
of sentences at that position, normalized by their
lengths in words (i.e. for an individual sentence
token s, the term to be averaged is ? log p(s)length(s) bits per
word). Standard trigram language models were
used to compute these probabilities (Clarkson and
Rosenfeld, 1997). The majority of the 12 lan-
guages belong to the Indo-European family, while
Mandarin Chinese is a Sino-Tibetan language.
2.2 Modeling Relevance Decay of Context
Formally, we define the relevance of context in the
same unit as entropy of signals ? bits per word.
Let r0 denote the entropy of signals that efficiently
encode the ensemble of messages a speaker can
choose from for any sentence position, a constant
under the assumption of CER. According to Infor-
mation Theory, r0 is equivalent to the uncertainty
associated with any sentence position if context is
considered. Thus, in error free communication,
linguistic signals presented at the kth sentence po-
sition are said to have resolved the uncertainty at
k and therefore are r0-bit relevant at the kth sen-
tence position. Then, at the (k+i)th sentence posi-
tion, these linguistic signals have become context
by definition and their relevance has decayed to
some r bits. Our models start from defining the
value of r as a function of the distance between
context and a target sentence position.
2.2.1 Power-law Decay Model
If the relevance of a cue q (e.g. a preceding sen-
tence), which is originally r0-bit relevant at po-
sition kq, decays at the rate following the power
function, its remaining relevance at target sentence
position k is:
relevancepow(k, q) = r0(k ? kq + 1)
?? (1)
In Equation (1), k > kq and ? is the decay rate.
This means at position k, the relevance of the cue
47
from the (k?1)th sentence is r0?2??-bit relevant;
the relevance of the cue from the (k?2)th sentence
is r0 ? 3??-bit relevant, and so on. As a result, the
relevance of discourse-specific context at position
k is the marginalization of all cues up to qk?1:
contextpow(k) = r0
?
qi?{q1...qk?1}
(k ? kqi + 1)
?? (2)
The general trend predicted by Equation (2)
is that discourse-specific context increases more
rapidly at the beginning of a discourse and much
more slowly towards the end due to the relevance
decay of distant cues. Rewriting Equation (2) in
a closed-form formula so that a model can be fit-
ted to data is not a trivial task without knowing the
rate ?, but the paradox is that ? has to be estimated
from the data. As a workaround, we approximated
the value of Equation (2) by computing a definite
integral of Equation (1), where ?i is a shorthand
for k ? kq + 1:
contextpow(k) ?
? k
1
r0?i
??d?i
= r0(
k1?? ? 1
1? ?
) (3)
Equation (3) uses an integral to approximate the
sum of a series defined as a function. The result
is usually acceptable as long as ? is greater than
1 so that the series defined by Equation 1 is con-
vergent (this assumption is empirically supported;
see Figure 5). Note that Equation (3) produces
the desirable effect that upon encountering the
first sentence of a discourse, no discourse-specific
contextual cues are available to the speaker (i.e.
context(1) = 0).
Now that we know the maximum relevance of
context at sentence position k, we can predict the
amount of out-of-context entropy of signals r(k)
based on the idea of uncertainty again. There are
new linguistic signals that are r0-bit relevant in
context at any sentence position. In addition, we
now know context(k) bits of relevant context are
also available. Thus, the sum of r0 and context(k)
defines the maximum amount of out-of-context
uncertainty that can be resolved at sentence posi-
tion k. Therefore, the out-of-context entropy of
signals at k is at most:
rpow(k) = context(k) + r0 (4)
= r0
k1?? ? 1
1? ?
+ r0
Whether speakers will utilize all available con-
text as predicted by Equation (4) is another de-
bate. Here we adopt the view that speakers are
maximally efficient in that they do make use of
all available context. Thus, we make the predic-
tion that out-of-context entropy of signals, as ob-
served empirically from data, can be described by
this model. Figure 1 shows the behavior of this
function with various parameter sets.
2 4 6 8 10 12 14
5
6
7
8
9
10
11
12
Sentence Position
Mod
el?P
redic
ted E
ntro
py p
er W
ord
r0 = 5.5,? = 2r0 = 5.5,? = 2.2r0 = 5,? = 2r0 = 5,? = 2.2
Figure 1: Schematic plots of the behavior of out-
of-context entropy of signals assuming the decay
of the relevance of context is a power function.
2.2.2 Exponential Decay Model
The second model assumes the relevance of con-
text decays exponentially. Following the same no-
tations as before, the relevance of a cue q at posi-
tion k is:
relevanceexp(k, q) = r0e
??(k?kq) (5)
The major difference between the power func-
tion and the exponential one is that the relevance
of a contextual cue drops more slowly in the expo-
nential case (Anderson, 1995). The relevance of
all discourse-specific context for a speaker at k is:
contextexp(k) = r0
k?1?
i=1
e??i (6)
48
Equation (6) is the sum of a geometric progres-
sion series. We can write Equation (6) in a closed-
form:
contextexp(k) =
r0
e? ? 1
(1? e?(k?1)?) (7)
As a result, the out-of-context entropy of signals
is:
rexp(k) =
r0
e? ? 1
(1? e?(k?1)?) + r0 (8)
Figure 2 schematically shows the behavior of
this function. One can notice this function con-
verges against a ceiling more quickly than the
power function. Thus, this model makes a slightly
different prediction from the power law model.
2 4 6 8 10 12 14
5
6
7
8
9
10
11
12
Sentence Position
Mod
el?P
redic
ted E
ntro
py p
er W
ord
r0 = 5.5,? = 0.6r0 = 5.5,? = 0.8r0 = 5,? = 0.6r0 = 5,? = 0.8
Figure 2: Schematic plots of the behavior of out-
of-context entropy of signals assuming the decay
of the relevance of context is an exponential func-
tion.
2.3 Nonlinear Regression Analysis
To test whether the proposed models (i.e. Equa-
tions 4 and 8) better characterize the data, we
built nonlinear regression models with document-
specific random effects, where the out-of-context
entropy of signals, rij , is regressed on sentence
position, kj . Based on the power law model, we
have
rij = (?1+b1i)
kj1??2 ? 1
1? ?2
+(?1+b1i)+ij (9)
where ?1 corresponds to r0, the theoretical con-
stant entropy of signals under an ideal encod-
ing. b1i represents the document-specific devia-
tions from the overall mean. ?2 corresponds to ?,
the mean rate at which the relevance of a past cue
decays, which is unfortunately not considered for
random effects for the practical purpose of making
computation feasible in the current work. Finally,
ij represents the errors independently distributed
as N (0, ?2), orthogonal to document specific de-
viations.
For the exponential model, the nonlinear model
is the following (symbols have the same interpre-
tations as in Equation 9):
rij =
(?1 + b1i)
e?2 ? 1
(1?e?(kj?1)?2)+(?1+b1i)+ij
(10)
Fitting data with the above nonlinear models
requires starting estimates for fixed-effect coeffi-
cients (i.e. ?1s and ?2s). Unfortunately, there are
no principled methods for selecting these values.
We heuristically selected 6 for ?1 and 2 for ?2 as
starting values for the power law model, and 4 and
0.5 as starting values for the exponential model.
3 Results
We examined the quality of the models and the pa-
rameters in the models: r0, the within-context en-
tropy rate, and ?, the rate of context decay.
3.1 Model Quality Comparison
The CER hypothesis indirectly predicts that out-
of-context entropy of signals of sentence positions
(bits per word) should increase throughout a dis-
course. The two models go one step further to
predict specific sublinear increase patterns, based
on the speaker?s considerations of the relevance of
past contextual cues. We compared the quality of
models in terms of Bayesian Information Criterion
(BIC) within languages. A lower BIC score indi-
cates a better fit. As shown by Figure 3, we find
our models best explain the data in 9 out of the 12
languages, reporting lower BIC scores than both
the linear and log-linear models as reported in our
previous work (Qian and Jaeger, under review).
For Danish, English and Italian, although neither
of our models produced a better score than the log-
linear model, the relative difference is small: 0.54
on average (comparing to BIC scores on the order
of 102 to 103).
49
Danish Dutch English French Italian Mandarin Norwegian Portuguese Russian E.Spanish L.Spanish Swedish
Power Law Exponential Loglinear Linear
?
20
?
10
0
10
20
Figure 3: Our models yield superior BIC scores in most languages. The y-axis shows the differences
between BIC scores of individual models for a language and mean BIC of the models for that language
(E.Spanish = European Spanish; L.Spanish = Latin-American Spanish).
Specifically, in terms of BIC scores, the power-
law model is better than the linear model (t(11) =
?3.98, p < 0.01), and the log-linear model
(t(11) = ?3.10, p < 0.05). The exponen-
tial model is also better than the linear model
(t(11) = ?3.98, p < 0.01), and the log-linear
model (t(11) = ?3.18, p < 0.01). The power-
law model and the exponential model are not sig-
nificantly different from each other (t(11) = 0.5,
p > 0.5).
3.2 Interpretation of Parameters
Constant Entropy of Signals r0. Both models
are constructed in such a way that the first param-
eter r0, in theory, corresponds to the theoretical
within-context entropy of signals of sentence po-
sitions. This parameter refers to how many bits per
word are needed to encode the ensemble of mes-
sages at a sentence position when context is taken
into account. The CER hypothesis directly pre-
dicts that this rate should be constant throughout
a discourse. Although we are unable to test this
prediction directly, it is nevertheless interesting to
compare whether these two independently devel-
oped models yield the same estimates for this pa-
rameter in each language.
Figure 4 shows encouraging results. Not only
the estimates made by the power model are well
correlated with those by the exponential model,
but also the slope of this correlation is equal to 1
(t(10) = 1.01, p < 0.0001). Since there are no
reasons a priori to suspect that these two models
l
l
4.0 4.5 5.0 5.5 6.0 6.5
4.0
4.5
5.0
5.5
6.0
6.5
Estimated Within?context Entropy per Word (Exponential Model)
Estim
ated
 With
in?c
onte
xt En
tropy
 per 
Word
 (Pow
er?
law M
odel
)
l
l
DanishDutchEnglishFrenchItalianMandarinNorwegianPortugueseRussianE. SpanishL. SpanishSwedish
Figure 4: Estimates of r0 correlate between both
models with a slope of 1.
would give the same estimates, this is a first step to
confirming the entropy per word in sentence pro-
duction is indeed a tractable constant throughout
discourses.
Among all languages, r0 has a mean of 5.0
bits in both models, and a variance of 0.46 in
the power-law model and 0.48 in the exponential
model, both remarkably small. The similarity in
r0 between languages may lead one to speculate
whether the amount of uncertainty per word in dis-
courses is largely the same regardless of the actual
language used by the speakers. On the other hand,
50
the differences in r0 may reveal the specific prop-
erties of different languages. Meanwhile, precau-
tions need to be taken in interpreting those esti-
mates given that the corpora are of different sizes,
and the ngram model is simplistic in nature.
Decay Rate ?. The second parameter ? corre-
sponds to the rate of relevance decay in both mod-
els. Since the base relevance r0 varies between
languages, ? can be more intuitively interpreted as
to indicate the percentage of the original relevance
of a contextual cue still remains in n positions. In
the power-law model, for example, the context in-
formation from a previous sentence in Danish, on
average, is only 11.6% (2?3.10 = 0.116) as rele-
vant. Hence, the relevance of a contextual cue de-
creases rather quickly for Danish. Table 2 shows
this is in fact the general picture for all languages
we tested.
Language Relevance of Context in Discourse (%)
1 pos. before 2 pos. before 3 pos. before
Danish 11.6 3.3 1.4
Dutch 10.4 2.8 1.1
English 0.1 0.0 0.0
French 8.5 2.0 0.7
Italian 10.2 2.7 1.0
Mandarin 7.7 1.7 0.6
Norwegian 18.9 7.1 3.6
Portuguese 5.5 1.0 0.3
Russian 12.7 3.8 1.6
E. Spanish 0.8 0.0 0.0
L. Spanish 2.7 0.3 0.1
Swedish 5.8 1.1 0.3
Table 2: In the power model, relevance of a con-
textual cue decays rather quickly for each lan-
guage.
The picture of ? looks a little different in the
exponential model. The relevance percentage on
average is significantly higher, which confirms an
earlier point that the power function decreases
more quickly than the exponential function. Table
3 shows a summary for the 12 languages.
One may note that the decay rate varies greatly
between languages under the prediction of both
models. However, these number are only approxi-
mations since the entropy estimated by the ngram
language model is far from psychological real-
ity. Furthermore, it is unlikely that speakers of
one language would exhibit the same decay rate
of context relevance in their production, let alne
speakers of different languages, who may be sub-
ject to language-specific constraints during pro-
Language Relevance of Context in Discourse (%)
1 pos. before 2 pos. before 3 pos. before
Danish 30.1 9.1 2.7
Dutch 28.7 8.2 2.4
English 9.6 0.9 0.1
French 26.7 7.1 1.9
Italian 28.7 8.2 2.4
Mandarin 25.7 6.6 1.7
Norwegian 42.3 17.9 7.6
Portuguese 22.5 5.1 1.1
Russian 34.6 12.0 4.2
E. Spanish 14.2 2.0 0.3
L. Spanish 18.6 3.5 0.6
Swedish 23.7 5.6 1.3
Table 3: In the exponential model, relevance of a
contextual cue decays more slowly.
duction. Therefore, the variation in estimates of
? seems reasonable.
Correlation between r0 and ?. Interestingly, r0
and ? are highly correlated (r2 = 0.39, p < 0.05
in the power model, Figure 5; r2 = 0.47, p < 0.01
in the exponential model, Figure 6): a high rel-
evance decay rate tends to be coupled with high
within-context entropy of signals. This unan-
ticipated observation is in fact compatible with
the account of efficient language production: a
high within-context entropy of signals indicates
the base relevance of a contextual cue (i.e. r0)
is high. It is then useful for its relevance to de-
cay more quickly to allow the speaker to inte-
grate context from other cues. Otherwise, the to-
tal amount of relevant context may presumably
overload working memory. However, our cur-
rent results come from only cross-linguistic sam-
ples. Cross-validation in within-language samples
is needed for confirming this hypothesis.
3.3 The Bigger Picture
Having obtained the estimates for r0 and ?, we are
now in a position to examine how out-of-context
entropy of signals increases as a function of sen-
tence positions, given the estimates of these two
parameters. As shown in Figure 7, the predictions
from both models are qualitative similar except
that 1) when the decay rate in the power-law model
is low, out-of-context entropy of signals converges
more slowly than in the exponential model (Figure
7, right panel); 2) when the decay rate in the power
model is high, it almost converges as quickly as
the exponential model, and only minor differences
exist in their predictions (Figure 7, left panel).
51
ll
4.0 4.5 5.0 5.5 6.0
4
6
8
10
Within?Context Entropy per Word
Rele
van
ce 
Dec
ay R
ate
l
l
DanishDutchEnglishFrenchItalianMandarin
NorwegianPortugueseRussianE. SpanishL. SpanishSwedish
Figure 5: The rate of relevance decay is corre-
lated with within-context entropy of signals in the
power-law model.
l
l
4.5 5.0 5.5 6.0
1.0
1.5
2.0
Within?Context Entropy per Word
Rele
van
ce 
Dec
ay R
ate
l
l
DanishDutchEnglishFrenchItalianMandarin
NorwegianPortugueseRussianE. SpanishL. SpanishSwedish
Figure 6: The rate of relevance decay is correlated
with within-context entropy of signals in the expo-
nential model.
Because of the nonlinearity in our models, it
is not possible to report the results in an intuitive
manner as in ?an increase in sentence position cor-
responds to an increase ofX bits of out-of-context
entropy per word?. Instead, we can analytically
solve for the derivative of the predicted out-of-
context entropy of signals with respect to sentence
position (Equation 4 and 8). This gives us:
rpower(k)
? = r0k
?? (11)
for the power-law model, showing the rate of in-
crease in predicted out-of-context entropy of sig-
nals is a monotonically decreasing power function,
and
l
l
l l
l
l
l
l
l
l l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l l
l
l
l
2 4 6 8 12
4
5
6
7
8
Sentence Position
Out
?of?
con
text
 En
trop
y pe
r W
ord 
in D
utch
Power:? = 3.27Exp:? = 1.25 l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l l l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l l
l
l
l
l
l
l
l
l l
l l
l
l
l
l l l
l
l
ll
l
l
l
l
l
l
l
l
l
2 4 6 8 12
4
5
6
7
8
Sentence Position
Out
?of?
con
text
 En
trop
y pe
r W
ord 
in N
orw
egia
n
Power:? = 2.4Exp:? = 0.86
Figure 7: Predicted out-of-context entropy of sig-
nals by the power-law model (solid) and the expo-
nential model (dashed) in Dutch and Norwegian,
with the actual distributions plotted on the back-
ground.
rexp(k)
? =
r0?
e? ? 1
(e?(k?1)?) (12)
for the exponential model, showing the rate of in-
crease is a monotonically decreasing exponential
function. These mathematical properties indeed
match our observations in Figure 7.
4 Discussion and Future Work
The models introduced in this paper try to answer
this question: if the relevance of a contextual cue
for predicting an upcoming linguistic signal de-
cays over the course of a discourse, how much un-
certainty (entropy) is associated with each individ-
ual sentence position? We have shown under that
models that incorporate (power law or exponen-
tial) cue relevance decay in most cases describe
the relation of out-of-context entropy of signals
to sentence position are better accounted for than
previously suggested models.
We are continuing to investigate along this line.
Specifically, we are interested in finding the role of
semantic memory in affecting the relevance decay
of context. To test that, we plan to implement a
probabilistic topic model, in which topic continu-
ity between a preceding sentence and an upcom-
ing sentence is quantitatively measured. Thus, the
decay of contextual cues can be based on the esti-
52
mated semantic relatedness between sentences, in
addition to the abstract notion of rate as used in
this paper.
Finally, our relevance decay model can be ap-
plied to the domain of language processing as
well. For instance, the distance between a con-
textual cue and the target word may affect how
quickly a comprehender can process the informa-
tion conveyed by the word. We plan to address
these question in future work.
5 Conclusion
We have presented a new approach for examin-
ing the distribution of entropy of linguistic sig-
nals in discourses, showing that not only the out-
of-context entropy of signals increases sublinearly
with sentence position, but also the sublinear trend
is better explained by our nonlinear models than
by log-linear models of previous work. Our mod-
els are built on the assumption that the relevance
of a contextual cue for predicting a linguistic sig-
nal in the future decays with its distance to the tar-
get, and predict the relation of out-of-context en-
tropy of signals to sentence position in discourses.
These results indirectly lend support to the hypoth-
esis that speakers maintain a constant entropy of
signals across sentence positions in a discourse.
Acknowledgements
We wish to thank Meredith Brown, Alex Fine and
three anonymous reviewers for their helpful com-
ments on this paper. This work was supported by
NSF grant BCS-0845059 to TFJ.
References
John R. Anderson. 1995. Learning and Memory: An
integrated approach. John Wiley & Sons.
Philip R. Clarkson and Roni Rosenfeld. 1997. Sta-
tistical language modeling using the cmu-cambridge
toolkit. In Proceedings of ESCA Eurospeech.
Dimitry Genzel and Eugene Charniak. 2002. Entropy
rate constancy in text. In ACL, pages 199?206.
Dimitry Genzel and Eugene Charniak. 2003. Variation
of entropy and parse trees of sentences as a function
of the sentence number. in. In EMNLP, pages 65?
72.
Frank Keller. 2004. The entropy rate principle as a
predictor of processing effort: An evaluation against
eye-tracking data. In EMNLP, pages 317?324.
D. D. Lewis, Y. Yang, T. Rose, and F Li. 2004. Rcv1:
A new benchmark collection for text categorization
research. J Mach Learn Res, 5:361?397.
Steve Piantadosi and Edwards Gibson. 2008. Uniform
information density in discourse: a cross-corpus
analysis of syntactic and lexical predictability. In
CUNY.
Ting Qian and T. Florian Jaeger. 2009. Evidence
for efficient language production in chinese. In
CogSci09, pages 851?856.
Ting Qian and T. Florian Jaeger. under review. En-
tropy profiles in language: A cross-linguistic inves-
tigation.
C. E. Shannon. 1948. A mathematical theory of com-
munications. Bell Labs Tech J, 27(4):623?656.
J. T. Wixted and E. B. Ebbesen. 1991. On the form of
forgetting. Psychological Science, 2:409?415.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Nat Lang
Eng, 11:207?238.
G. K. Zipf. 1935. Psycho-Biology of Languages.
Houghton-Mifflin.
53
