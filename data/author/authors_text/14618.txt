Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 666?676, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning Lexicon Models from Search Logs for Query Expansion 
 
Jianfeng Gao 
Microsoft Research, Redmond 
Washington 98052, USA 
jfgao@microsoft.com 
 
Shasha Xie 
Educational Testing Service, Princeton 
New Jersey 08540, USA 
sxie@ets.org 
 
Xiaodong He 
Microsoft Research, Redmond 
Washington 98052, USA 
xiaohe@microsoft.com 
 
 
Alnur Ali 
Microsoft Bing, Bellevue 
Washington 98004, USA 
alnurali@microsoft.com 
 
Abstract 
This paper explores log-based query expan-
sion (QE) models for Web search. Three 
lexicon models are proposed to bridge the 
lexical gap between Web documents and 
user queries. These models are trained on 
pairs of user queries and titles of clicked 
documents. Evaluations on a real world data 
set show that the lexicon models, integrated 
into a ranker-based QE system, not only 
significantly improve the document retriev-
al performance but also outperform two 
state-of-the-art log-based QE methods. 
1 Introduction 
Term mismatch is a fundamental problem in Web 
search, where queries and documents are com-
posed using different vocabularies and language 
styles. Query expansion (QE) is an effective strate-
gy to address the problem. It expands a query is-
sued by a user with additional related terms, called 
expansion terms, so that more relevant documents 
can be retrieved.  
In this paper we explore the use of clickthrough 
data and translation models for QE. We select ex-
pansion terms for a query according to how likely 
it is that the expansion terms occur in the title of a 
document that is relevant to the query. Assuming 
that a query is parallel to the titles of documents 
clicked for that query (Gao et al
icon models are trained on query-title pairs ex-
tracted from clickthrough data. The first is a word 
model that learns the translation probability be-
tween single words. The second model uses lexi-
calized triplets to incorporate word dependencies 
for translation. The third is a bilingual topic model, 
which represents a query as a distribution of hid-
den topics and learns the translation between a 
query and a title term at the semantic level. We 
will show that the word model provides a rich set 
of expansion candidates while the triplet and topic 
models can effectively select good expansion 
terms, and that a ranker-based QE system which 
incorporates all three of these models not only sig-
nificantly improves Web search result but outper-
forms other log-based QE methods that are state-
of-the-art. 
There is growing interest in applying user logs 
to improve QE. A recent survey is due to Baeze-
Yates and Ribeiro-Neto (2011). Below, we briefly 
discuss two log-based QE methods that are closest 
to ours and are re-implemented in this study for 
comparison. Both systems use the same type of log 
data that we used to train the lexicon models. The 
term correlation model of Cui et al
to our knowledge the first to explore query-
document relations for direct extraction of expan-
sion terms for Web search. The method outper-
forms traditional QE methods that do not use log 
data e.g. the local analysis model of Xu and Croft 
(1996). In addition, as pointed out by Cui et al
(2003) there are three important advantages that 
make log-based QE a promising technology to im-
prove the performance of commercial search en-
gines. First, unlike traditional QE methods that are 
based on relevance feedback, log-based QE derives 
expansion terms from search logs, allowing term 
correlations to be pre-computed offline. Compared 
to methods that are based on thesauri either com-
piled manually (Prager et alu-
666
tomatically from document collections (Jing and 
Croft 1994), the log-based method is superior in 
that it explicitly captures the correlation between 
query terms and document terms, and thus can 
bridge the lexical gap between them more effec-
tively. Second, since search logs retrain query-
document pairs clicked by millions of users, the 
term correlations reflect the preference of the ma-
jority of users. Third, the term correlations evolve 
along with the accumulation of user logs, thus can 
reflect updated user interests at a specific time. 
However, as pointed out by Riezler et al
(2008), Cui et ald method suf-
fers low precision of QE partly because the corre-
lation model does not explicitly capture context 
information and is susceptible to noise. Riezler et 
al. developed a QE system by retraining a standard 
phrase-based statistical machine translation (SMT) 
system using query-snippet pairs extracted from 
clickthrough data (Riezler et al
Liu 2010). The SMT-based system can produce 
cleaner, more relevant expansion terms because 
rich context information useful for filtering noisy 
expansions is captured by combining language 
model and phrase translation model in its decoder. 
Furthermore, in the SMT system all component 
models are properly smoothed using sophisticated 
techniques to avoid sparse data problems while the 
correlation model relies on pure counts of term 
frequencies. However, the SMT system is used as a 
black box in their experiments. So the relative con-
tribution of different SMT components is not veri-
fied empirically. In this study we break this black 
box in order to build a better, simpler QE system. 
We will show that the proposed lexicon models 
outperform significantly the term correlation mod-
el, and that a simpler QE system that incorporates 
the lexicon models can beat the sophisticated, 
black-box SMT system. 
2 Lexicon Models 
We view search queries and Web documents as 
two different languages, and cast QE as a means to 
bridge the language gap by translating queries to 
documents, represented by their titles. In this sec-
tion, we will describe three translation models that 
are based on terms, triplets, and topics, respective-
ly, and the way these models are learned from que-
ry-title pairs extracted from clickthrough data. 
2.1 Word Model 
The word model takes the form of IBM Model 1 
(Brown et alafferty 1999). Let 
            be a query,   be an expansion term 
candidate, the translation probability from   to   is 
defined as  
   |     ? ( |  ) (  | )
 
   
 (1) 
where    |   is the unsmoothed unigram proba-
bility of word   in query  . The word translation 
probabilities    |   are estimated on the query-
title pairs derived from the clickthrough data by 
assuming that the title terms are likely to be the 
desired expansions of the paired query. Our train-
ing method follows the standard procedure of 
training statistical word alignment models pro-
posed by Brown et al we opti-
mize the model parameters   by maximizing the 
probability of generating document titles from que-
ries over the entire training corpus: 
           ?    |     
 
   
 (2) 
where both the titles   and the paired queries   are 
viewed as bag of words. The translation probability 
    |      takes the form of IBM Model 1 as  
   |     
 
      
?? (  |    )
 
   
 
   
 (3) 
where   is a constant,   is the length of  , and   is 
the length of  . To find the optimal word transla-
tion probabilities of IBM Model 1, we used the EM 
algorithm, where the number of iterations is deter-
mined empirically on held-out data. 
2.2 Triplet Model 
The word model is context independent. The triplet 
model, which is originally proposed for SMT (Ha-
san et al to capture inter-term 
dependencies for selecting expansion terms. The 
model is based on lexicalized triplets (       ) 
which can be understood as two query terms trig-
gering one expansion term. The translation proba-
bility of   given   for the triplet model is parame-
terized as 
667
   |     
 
 
? ?  ( |     )
 
     
   
   
 (4) 
where Z is a normalization factor based on the cor-
responding query length, i.e.,   
      
 
, and 
 (  |     ) is the probability of translating    into 
   given another query word   . Since    can be 
any word in   that is not necessary to be adjacent 
to   , the triple model is able to combine local (i.e. 
word and phrase level) and global (i.e. query level) 
contextual information useful for word translation.  
Similar to the case of word model, we used the 
EM algorithm to estimate the translation probabili-
ties    |      on the query-title pairs.  Since the 
number of all possible triplets (      ) is large and 
as a consequence the model training could suffer 
the data sparseness problem, in our experiments 
count-based cutoff is applied to prune the model to 
a manageable size. 
2.3 Bilingual Topic Model (BLTM) 
The BLTM was originally proposed for Web doc-
ument ranking by Gao et aln-
derlying the model is that a search query and its 
relevant Web documents share a common distribu-
tion of (hidden) topics, but use different (probably 
overlapping) vocabularies to express these topics. 
Intuitively, BLTM-based QE works as follows. 
First, a query is represented as a vector of topics. 
Then, all the candidate expansion terms, which are 
selected from document, are ranked by how likely 
it is that these document terms are selected to best 
describe those topics. In a sense, BLTM is similar 
to the word model and the triplet model since they 
all map a query to a document word. BLTM differs 
in that the mapping is performed at the topic level 
(via a language independent semantic representa-
tion) rather than at the word level. In our experi-
ments BLTM is found to often select a different set 
of expansion terms and is complementary to the 
word model and the triplet model. 
Formally, BLTM-based QE assumes the follow-
ing story of generating   from  : 
1. First, for each topic  , a pair of different 
word distributions    
    
   are selected 
from a Dirichlet prior with concentration pa-
rameter ?, where  
 
 is a topic-specific query 
term distribution, and   
  a topic-specific 
document term distribution. Assuming there 
are   topics, we have two sets of distribu-
tions       
      
   and    
   
      
  . 
2. Given  , a topic distribution    is drawn 
from a Dirichlet prior with concentration pa-
rameter  . 
3. Then a document term (i.e., expansion term 
candidate)   is generated by first selecting a 
topic   according to the topic distribution   , 
and then drawing a word from   
 . 
By summing over all possible topics, we end up 
with the following model form 
       |   ?   |  
     |   
 
 (5) 
The BLTM training follows the method described 
in Gao et ale EM algorithm to 
estimate the parameters (         of BLTM by 
maximizing the joint log-likelihood of the query-
title pairs and the parameters. In training, we also 
constrain that the paired query and title have simi-
lar fractions of tokens assigned to each topic. The 
constraint is enforced on expectation using posteri-
or regularization (Ganchev et al
3 A Ranker-Based QE System 
This section describes a ranker-based QE system in 
which the three lexicon models described above 
are incorporated. The system expands an input 
query in two distinct stages, candidate generation 
and ranking, as illustrated by an example in Figure 
1. 
Original query jaguar locator 
Ranked expansion  jaguar finder 
candidates 
(altered words are in 
car locator 
jaguar location 
italic) jaguar directory 
 ? 
 jaguar list 
Expanded query OR(jaguar, car)  
(selected expansion 
terms are in italic) 
OR(locator, finder, location, 
directory) 
Figure 1. An example of an original query, its expan-
sion candidates and the expanded query generated by 
the ranker-based QE system. 
 
668
In candidate generation, an input query   is 
first tokenized into a sequence of terms. For each 
term   that is not a stop word, we consult a word 
model described in Section 2.1 to identify the best 
  altered words according to their word transla-
tion probabilities from  . Then, we form a list of 
expansion candidates, each of which contains all 
the original words in   except for the word that is 
substituted by one of its altered words. So, for a 
query with   terms, there are at most    candi-
dates. 
In the second stage, all the expansion candidates 
are ranked using a ranker that is based on the Mar-
kov Random Field (MRF) model in which the 
three lexicon models are incorporated as features.  
Expansion terms of a query are taken from those 
terms in the  -best (     in our experiments) 
expansion candidates of the query that have not 
been seen in the original query string. 
In the remainder of this section we will describe 
in turn the MRF-based ranker, the ranking features, 
and the way the ranker parameters are estimated. 
3.1 MRF-Based Ranker 
The ranker is based on the MRF model that models 
the joint distribution of         over a set of ex-
pansion term random variables             and 
a query random variable  . It is constructed from a 
graph   consisting of a query node and nodes for 
each expansion term. Nodes in the graph represent 
random variables and edges define the independ-
ence semantics between the variables. An MRF 
satisfies the Markov property (Bishop 2006), 
which states that a node is independent of all of its 
non-neighboring nodes given observed values of 
its neighbors, defined by the clique configurations 
of  . The joint distribution over the random varia-
bles in   is defined as  
        
 
  
?       
      
 (6) 
where      is the set of cliques in  , and each 
       is a non-negative potential function de-
fined over a clique configuration c that measures 
the compatibility of the configuration,   is a set of 
parameters that are used within the potential func-
tion, and    normalizes the distribution. For rank-
ing expansion candidates, we can drop the expen-
sive computation of    since it is independent of 
E, and simply rank each expansion candidate   by 
its unnormalized joint probability with   under the 
MRF. It is common to define MRF potential func-
tions of the exponential form as        
           , where      is a real-valued feature 
function over clique values and    is the weight of 
the feature function. Then, we can compute the 
posterior     |   as 
    |   
       
     
 (7) 
    
?   ?          
      
 ?       
      
  
which is essentially a weighted linear combination 
of a set of features. 
Therefore, to instantiate the MRF model, one 
needs to define a graph structure and a set of po-
tential functions. In this paper, the graphical model 
representation we propose for QE is a fully con-
nected graph shown in Figure 2, where all expan-
sion terms and the original query are assumed de-
pendent with each other. In what follows, we will 
define six types of cliques that we are interested in 
defining features (i.e., potential functions) over. 
3.2 Features 
The cliques and features are inspired by the com-
ponent models used in SMT systems. The cliques 
defined in   for MRF can be grouped into two cat-
egories. The first includes three types of cliques 
involving both the query node and one or more 
expansion terms. The potential functions defined 
over these cliques attempt to abstract the idea be-
hind the query to title translation models. The other 
three types, belonging to the second category, in-
volve only expansion terms. Their potential func-
 
 
Figure 2: The structure of the Markov random field for 
representing the term dependency among the query   
and the expansion terms        . 
669
tions attempt to abstract the idea behind the target 
language models.  
The first type of cliques involves a single ex-
pansion term and the query node. The potentials 
functions for these cliques are defined as 
                             (6) 
               
                   
where the three feature functions of the form 
       are defined as the log probabilities of 
translating   to   according to the word, triplet and 
topic models defined in Equations (1), (4) and (5), 
respectively. 
                   |    
                   |    
                       |    
The second type of cliques contains the query 
node and two expansion terms,    and     , which 
appear in consecutive order in the expansion. The 
potential functions over these cliques are defined 
as 
                                        (7) 
where the feature        is defined as the log prob-
ability of generating an expansion bigram given   
           |               |    
Unlike the language models used for document 
ranking (e.g., Zhai and Lafferty 2001), we cannot 
compute the bigram probability by simply counting 
the relative frequency of           in   because 
the query is usually very short and the bigram is 
unlikely to occur. Thus, we approximate the bi-
gram probability by assuming that the words in   
are independent with each other. We thus have 
         |   
            
    
 
 
           |   ?     |        
 
   
?      
 
   
  
where     |         is the translation probability 
computed using a variant of the triplet model de-
scribed  in Section 2.2. The model variation differs 
from the one of Equation (4) in two respects. First, 
it models the translation in a different direction i.e., 
from expansion to query. Second, we add a con-
straint to the triplets such that (       ) must be an 
ordered, contiguous bigram. The model variation is 
also trained using EM on query-title pairs.       
and       |    are assigned respectively by the 
unigram and bigram language models, estimated 
from the collection of document titles of the click-
through data, and        is the unigram probability 
of the query term, estimated from the collection of 
queries of the clickthrough data. 
The third type of cliques contains the query 
node and two expansion terms,    and   , which 
occur unordered within the expansion. The poten-
tial functions over these cliques are defined as 
                                    (8) 
where the feature        is defined as the log prob-
ability of generating a pair of expansion terms 
        given   
         |             |  .  
Unlike            |   defined in Equation (7), this 
class of features captures long-span term depend-
ency in the expansion candidate. Similar to the 
computation of          |   in Equation (7), we 
approximate        |   as     
       |   
          
    
 
 
         |   ?     |      
 
   
?      
 
   
  
where     |       is the translation probability 
computed using the triplet model described  in Sec-
tion 2.2, but in the expansion-to-query direction. 
      is assigned by a unigram language model 
estimated from the collection of document titles of 
the clickthrough data.     |    is assigned by a co-
occurrence model, estimated as  
    |    
        
?         
  
where         is the number of times that the two 
terms occur in the same title in clickthrough data.  
We now turn to the other three types of cliques 
that do not contain the query node. The fourth type 
of cliques contains only one expansion term. The 
potential functions are defined as 
670
                          (9) 
                  
where       is the unigram probability computed 
using a unigram language model trained on the 
collection of document titles. 
The fifth type of cliques contains a pair of terms 
appearing in consecutive order in the expansion. 
The potential functions are defined as 
                                    (10) 
                      |     
where       |    is the bigram probability com-
puted using a bigram language model trained on 
the collection of document titles. 
The sixth type of cliques contains a pair of 
terms appearing unordered within the expansion. 
The potential functions are defined as 
                                (11) 
                  |     
where     |    is the assigned by a co-occurrence 
model trained on the collection of document titles. 
3.3 Parameter Estimation 
The MRF model uses 8 classes of features defined 
on 6 types of cliques, as in Equations (6) to (11). 
Following previous work (e.g., Metzler and Croft 
2005; Bendersky et alhat all 
features within the same feature class are weighted 
by the same tied parameter   . Thus, the number of 
free parameters of the MRF model is significantly 
reduced. This not only makes the model training 
easier but also improves the robustness of the 
model. After tying the parameters and using the 
exponential potential function form, the MRF-
based ranker can be parameterized as  
    |  
    
?      ?          
 
   
  (12) 
   ?          
 
   
  
     ?            
 
   
  
   ?               
   
   
  
   ? ?             
 
     
   
   
  
   ?        
 
   
  
   ?             
   
   
  
   ? ?           
 
     
   
   
 
where there are in total 8  ?s to be estimated. 
Although the MRF is by nature a generative 
model, it is not always appropriate to train the pa-
rameters using conventional likelihood based ap-
proaches due to the metric divergence problem 
(Morgan et alaximum likelihood 
estimate is unlikely to be the one that optimizes the 
evaluation metric. In this study the effectiveness of 
a QE method is evaluated by first issuing a set of 
queries which are expanded using the method to a 
search engine and then measuring the Web search 
performance. Better QE methods are supposed to 
lead to better Web search results using the corre-
spondingly expanded query set. 
For this reason, the parameters of the MRF-
based ranker are optimized directly for Web 
search. In our experiments, the objective in train-
ing is Normalized Discounted Cumulative Gain 
(NDCG, Jarvelin and Kekalainen 2000), which is 
widely used as quality measure for Web search. 
Formally, we view parameter training as a multi-
dimensional optimization problem, with each fea-
ture class as one dimension. Since NDCG is not 
differentiable, we tried in our experiments numeri-
cal algorithms that do not require the computation 
of gradient. Among the best performers was the 
Powell Search algorithm (Press et al
constructs a set of   virtual directions that are con-
jugate (i.e., independent with each other), then it 
uses line search  times (    in our case), each 
on one virtual direction, to find the optimum. Line 
search is a one-dimensional optimization algo-
rithm. Our implementation follows the one de-
scribed in Gao et alsed to opti-
mize averaged precision. 
4 Experiments 
We evaluate the performance of a QE method by 
first issuing a set of queries which are expanded 
using the method to a search engine and then 
671
measuring the Web search performance. Better QE 
methods are supposed to lead to better Web search 
results using the correspondingly expanded query 
set.  
Due to the characteristics of our QE methods, 
we cannot conduct experiments on standard test 
collections such as the TREC data because they do 
not contain related user logs we need. Therefore, 
following previous studies of log-based QE (e.g., 
Cui et allthe 
proprietary datasets that have been developed for 
building a commercial search engine, and demon-
strate the effectiveness of our methods by compar-
ing them against previous state-of-the-art log-
based QE methods. 
The relevance judgment set consists of 4,000 
multi-term English queries. On average, each que-
ry is associated with 197 Web documents (URLs). 
Each query-URL pair has a relevance label. The 
label is human generated and is on a 5-level rele-
vance scale, 0 to 4, with 4 meaning document D is  
the  most  relevant  to  query Q  and 0 meaning  D 
is  not  relevant to Q.  
The relevance judgment set is constructed as 
follows. First, the queries are sampled from a year 
of search engine logs. Adult, spam, and bot queries 
are all removed. Queries are ?de-duped? so that 
only unique queries remain. To reflect a natural 
query distribution, we do not try to control the 
quality of these queries. For example, in our query 
sets, there are roughly 20% misspelled queries, 20% 
navigational queries, and 10% transactional que-
ries. Second, for each query, we collect Web doc-
uments to be judged by issuing the query to several 
popular search engines (e.g., Google, Bing) and 
fetching retrieval results from each. Finally, the 
query-document pairs are judged by a group of 
well-trained assessors. In this study all the queries 
are preprocessed as follows. The text is white-
space tokenized and lowercased, numbers are re-
tained, and no stemming/inflection treatment is 
performed. We split the judgment set into two non-
overlapping datasets, namely training and test sets, 
respectively. Each dataset contains 2,000 queries. 
The query-title pairs used for model training are 
extracted from one year of query log files using a 
procedure similar to Gao et al
periments we used a randomly sampled subset of 
20,692,219 pairs that do not overlap the queries 
and documents in the test set. 
Our Web document collection consists of ap-
proximately 2.5 billion Web pages. In the retrieval 
experiments we use the index based on the content 
fields (i.e., body and title text) of each Web page. 
The Web search performance is evaluated by 
mean NDCG. We report NDCG scores at trunca-
tion levels of 1, 3, and 10.  We also perform a sig-
nificance test using the paired t-test. Differences 
are considered statistically significant when p-
value is less than 0.05. 
4.1 Comparing Systems 
Table 1 shows the main document ranking results 
using different QE systems, developed and evalu-
ated using the datasets described above.  
NoQE (Row 1) is the baseline retrieval system 
that uses the raw input queries and the BM25 doc-
ument ranking model. Rows 2 to 4 are different QE 
systems. Their results are obtained by first expand-
ing a query, then using BM25 to rank the docu-
ments with respect to the expanded query.  
TC (Row 2) is our implementation of the corre-
lation-based QE system (Cui et al
takes the following steps to expand an input query 
 : 
# QE methods NDCG@1 NDCG@3 NDCG@10 
1 NoQE 34.70 36.50 41.54 
2 TC 33.78 36.57 42.33
 ? 
3 SMT 34.79
 ? 36.98 ?? 42.84 ?? 
4 MRF 36.10 ??? 38.06 ??? 43.71 ??? 
5 MRFum+bm+cm 33.31 36.12 42.26
 ? 
6 MRFtc 34.50
 ? 36.59 42.33 ? 
7 MRFwm 34.73
 ? 36.62 42.73 ?? 
8 MRFtm 35.13
 ?? 37.46 ??? 42.82 ?? 
9 MRFbltm 34.34
 ? 36.19 41.98 ? 
10 MRFwm+tm 35.21
 ??? 37.46 ??? 42.83 ?? 
11 MRFwm+tm+bltm 35.84
 ??? 37.70 ??? 43.14 ??? 
Table 1: Ranking results using BM25 with different 
query expansion systems. The superscripts      and    
indicate statistically significant improvements 
         over NoQE, TC, and SMT, respectively. 
Rows 5 to 11 are different versions of MRF in Row 5, 
They use the same candidate generator but use in the 
ranker different feature classes, as specified by the 
subscript. tc specifies the feature class defined as the 
scoring function in Equation (13). Refer to Equation 
(12) for the names of other feature classes. 
 
 
672
1. Extract all query terms   (eliminating 
stopwords) from  . 
2. Find all documents that have clicks on a 
query that contains one or more of these 
query terms. 
3. For each title term   in these documents, 
calculate its evidence of being selected as 
an expansion term according to the whole 
query via a scoring function        |   . 
4. Select n title terms with the highest score 
(where the value of n is optimized on train-
ing data) and formulate the expanded que-
ry by adding these terms into  . 
5. Use the expanded query to rank documents. 
The scoring function is based on the term correla-
tion model, and is defined as 
       |     (?   |    
   
) (13) 
   |   ?    |     |  
    
  
where    is the set of documents clicked for the 
queries containing the term   and is collected from 
search logs,    |   is a normalized tf-idf weight 
of the document term in  , and    |   is the rela-
tive occurrence of   among all the documents 
clicked for the queries containing  . Table 1 shows 
that TC leads to significant improvement over 
NoQE in NDCG@10, but not in NDCG@1 and 
NDCG@3 (Row 2 vs. Row 1). The result is not 
entirely consistent with what reported in Cui et al
(2003). A possible reason is that Cui et al
formed the evaluation using documents and search 
logs collected from the Encarta website, which is 
much cleaner and more homogenous than the data 
sets we used. The result suggests that although QE 
improves the recall of relevant documents, it is 
also likely to introduce noise that hurts the preci-
sion of document retrieval. 
SMT (Row 3) is a SMT-based QE system. Fol-
lowing Riezler et al is an im-
plementation of a phrase-based SMT system with a 
standard set of features for translation model and 
language model, combined under a log linear mod-
el framework (Koehn et alrom 
Riezler et al translation model 
is trained on query-snippet pairs and the language 
model on queries, in our implementation the trans-
lation model is trained on query-title pairs and the 
language model on titles. To apply the system to 
QE, expansion terms of a query are taken from 
those terms in the 10-best translations of the query 
that have not been seen in the original query string. 
We see that SMT significantly outperforms TC in 
NDCG at all levels. The result confirms the con-
clusion of Riezler et alt context 
information is crucial for improving retrieval pre-
cision by filtering noisy expansions.  
Both TC and SMT, considered as state-of-the-
art QE methods, have been frequently used for 
comparison in related studies. Thus, we also used 
them as baselines in our experiments. 
MRF (Row 4) is the ranker-based QE system 
described in Section 3, which uses a MRF-based 
ranker to incorporate all 8 classes of features de-
rived from a variety of lexicon translation models 
and language models as in Equation (12). Results 
show that the ranker-based QE system significantly 
outperforms both NoQE and the two state-of-the-
art QE methods. The fact that MRF beats SMT 
with a statistically significant margin although the 
former is a much simpler system indicates that text 
translation and QE are different tasks and some 
SMT components, designed for the task of regular 
text translation, are not as effective in selecting 
expansion terms. We will explore this in more de-
tail in the next section. 
4.2 Comparing Models 
The experiments presented in this section investi-
gate in detail the effectiveness of different models, 
e.g., the lexicon models and the language models 
described in Sections 2 and 3, in ranking expansion 
candidates for QE. The results are summarized in 
Rows 5 to 11 in Table 1, where a number of differ-
ent versions of the ranker-based QE system are 
compared. These versions, labeled as MRFf, use 
the same candidate generator, and differ in the fea-
ture classes (which are specified by the subscript f) 
incorporated in the MRF-based ranker. In what 
follows, we focus our discussion on the results of 
the three lexicon models. 
MRFwm (Row 7) uses the word translation 
model described in Section 2.1. Both the word 
model and term correlation model used in MRFtm 
(Row 6) are context independent. They differ 
mainly in the training methods. For the sake of 
comparison, in our experiment the word model is 
673
EM-trained with the correlation model as initial 
point. Rezler et al that statisti-
cal translation model is superior to correlation 
model because the EM training captures the hidden 
alignment information when mapping document 
terms to query terms, leading to a better smoothed 
probability distribution. Our result (Row 7 vs. Row 
6) verifies the hypothesis. Notice that MRFtc out-
performs TC in NDCG@1 (Row 6 vs. Row 2) 
mainly because in the former the expansion candi-
dates are generated by a word translation model 
and are less noisy. 
It is encouraging to observe that the rankers us-
ing the triplet model features achieve the QE per-
formance either in par with or better than that of 
SMT (Rows 8, 10 and 11 vs. Row 3), although the 
latter is a much more sophisticated system. The 
result suggests that not all SMT components are 
useful for QE. For example, language models are 
indispensable for translation but are less effective 
than word models for QE (Row 5 vs. Rows 6 and 
7). We also observe that the triplet model not only 
outperforms significantly the word model due to 
the use of contextual information (Row 8 vs. Row 
7), but also seems to subsume the latter in that 
combining the features derived from both models 
in the ranker leads to little improvement over the 
ranker that uses only the triplet model features 
(Row 10 vs. Row 8).  
The bilingual topic model underperforms the 
word model and the triplet model (Row 9 vs. Rows 
7 and 8). However, we found that the bilingual top-
ic model often selects a different set of expansion 
terms and is complementary to the other two lexi-
con models. As a result, unlike the case of combin-
ing the word model and triplet model features, in-
corporating the bilingual topic model features in 
the ranker leads to some visible improvement in 
NDCG at all positions (Row 11 vs. Row 10). 
To better understand empirically how the MRF-
based QE system achieves the improvement, we 
analyzed the expansions generated by our system 
in detail and obtained several interesting findings. 
First, as expected, in comparison with the word 
model, the triplet translation model is more effec-
tive in benefitting long queries, e.g., notably que-
ries containing questions and queries containing 
song lyrics. Second, unlike the two lexicon models, 
the bilingual topic model tends to generate expan-
sions that are more likely to relate to an entire que-
ry rather than individual query terms. Third, the 
features involving the order of the expansion terms 
benefitted queries containing named entities. 
5 Related Work 
In comparison with log-based methods studied in 
this paper, the QE methods based on automatic 
relevance feedback have been studied much more 
extensively in the information retrieval (IR) com-
munity, and have been proved useful for improving 
IR performance on benchmark datasets such as 
TREC (e.g., Rocchio 1971; Xu and Croft 1996; 
Lavrenko 2001; Zhai and Lafferty 2001). Howev-
er, these methods cannot be applied directly to a 
commercial Web search engine because the rele-
vant documents are not always available and gen-
erating pseudo-relevant documents requires multi-
phase retrieval, which is prohibitively expensive. 
Although automatic relevance feedback is not the 
focus of this study, our method shares a lot of simi-
larities with some of them. For example, similar to 
the way the parameters of our QE ranker are esti-
mated, Cao et alethod of se-
lecting expansion terms to directly optimize aver-
age precision. The MRF model has been previous-
ly used for QE, in the form of relevance feedback 
and pseudo-relevance feedback (Metzler et al
2007; Lang et al MRF models 
use the features derived from IR systems such as 
Indri, we use the SMT-inspired features.  
Using statistical translation models for IR is not 
new (e.g., Berger and Lafferty 1999; Jin et al
Xue et alveness of the statistical 
translation-based approach to Web search has been 
demonstrated empirically in recent studies where 
word-based and phrase-based translation models 
are trained on large amounts of clickthrough data 
(e.g., Gao et alwork extends 
these studies and constructs QE-oriented transla-
tion models that capture more flexible dependen-
cies. 
In addition to QE, search logs have also been 
used for other Web search tasks, such as document 
ranking (Joachims 2002; Agichtein et al
search query processing and spelling correction 
(Huang et alre-
trieval (Craswell and Szummer 2007), and user 
query clustering (Baeza-Yates and Tiberi 2007; 
Wen et al
6 Conclusions 
674
In this paper we extend the previous log-based QE 
methods in two directions. First, we formulate QE 
as the problem of translating a source language of 
queries into a target language of documents, repre-
sented as titles. This allows us to adapt the estab-
lished techniques developed for SMT to QE. Spe-
cially, we propose three lexicon models based on 
terms, lexicalized triplets, and topics, respectively. 
These models are trained on pairs of user queries 
and the titles of clicked documents using EM. Se-
cond, we present a ranker-based QE system, the 
heart of which is a MRF-based ranker in which the 
lexicon models are incorporated as features. We 
perform experiments on the Web search task using 
a real world data set. Results show that the pro-
posed system outperforms significantly other state-
of-the-art QE systems. 
This study is part of a bigger, ongoing project, 
aiming to develop a real-time QE system for Web 
search, where simplicity is the key to the success. 
Thus, what we learned from this study is particu-
larly encouraging. We demonstrate that with large 
amounts of clickthrough data for model training, 
simple lexicon models can achieve state-of-the-art 
QE performance, and that the MRF-based ranker 
provides a simple and flexible framework to incor-
porate a variety of features capturing different 
types of term dependencies in such an effective 
way that the Web search performance can be di-
rectly optimized. 
References  
Agichtein, E., Brill, E., and Dumais, S. 2006. Im-
proving web search ranking by incorporating us-
er behavior information. In SIGIR, pp. 19-26. 
Baeze-Yates, R., and Ribeiro-Neto, B. 2011. Mod-
ern Information Retrieval. Addison-Wesley. 
Baeza-Yates, R. and Tiberi, A. 2007. Extracting 
semantic relations from query logs. In SIGKDD, 
pp. 76-85. 
Bai, J., Song, D., Bruza, P., Nie, J-Y., and Cao, G. 
2005. Query expansion using term relationships 
in language models for information retrieval. In 
CIKM, pp. 688-695. 
Bendersky, M., Metzler, D., and Croft, B. 2010. 
Learning concept importance using a weighted 
dependence model. In WSDM, pp. 31-40.  
Berger, A., and Lafferty, J. 1999. Information re-
trieval as statistical translation. In SIGIR, pp. 
222-229. 
Bishop, C. M. 2006. Patten recognition and ma-
chine learning. Springer.  
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. 
Latent Dirichlet al Machine 
Learning Research, 3: 993-1022. 
Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., 
and Mercer, R. L. 1993. The mathematics of sta-
tistical machine translation: parameter estimation. 
Computational Linguistics, 19(2): 263-311. 
Cao, G., Nie, J-Y., Gao, J., and Robertson, S. 2008. 
Selecting good expansion terms for pseudo-
relevance feedback. In SIGIR, pp. 289-305. 
Craswell, N. and Szummer, M. 2007. Random 
walk on the click graph. In SIGIR. pp. 239-246. 
Cui, H., Wen, J-R., Nie, J-Y. and Ma, W-Y. 2002. 
Probabilistic query expansion using query logs. 
In WWW, pp. 325-332.  
Cui, H., Wen, J-R., Nie, J-Y. and Ma, W-Y. 2003. 
Query expansion by mining user log. IEEE 
Trans on Knowledge and Data Engineering. Vol. 
15, No. 4. pp. 1-11. 
Dempster, A., Laird, N., and Rubin, D. 1977. Max-
imum likelihood from incomplete data via the 
EM algorithm. Journal of the Royal Statistical 
Society, 39: 1-38. 
Ganchev, K., Graca, J., Gillenwater, J., and Taskar, 
B. 2010. Posterior regularization for structured 
latent variable models. Journal of Machine 
Learning Research, 11 (2010): 2001-2049. 
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web 
search. In SIGIR, pp. 675-684. 
Gao, J., He, X., and Nie, J-Y. 2010a. Clickthrough-
based translation models for web search: from 
word models to phrase models. In CIKM, pp. 
1139-1148. 
Gao, J., Li, X., Micol, D., Quirk, C., and Sun, X. 
2010b. A large scale ranker-based system for 
query spelling correction. In COLING, pp. 358-
366. 
675
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 
2009. Smoothing clickthrough data for web 
search ranking. In SIGIR, pp. 355-362. 
Gao, J., Qi, H., Xia, X., and Nie, J-Y. 2005. Linear 
discriminant model for information retrieval. In 
SIGIR, pp. 290-297. 
Hasan, S., Ganitkevitch, J., Ney, H., and Andres-
Fnerre, J. 2008. Triplet lexicon models for statis-
tical machine translation. In EMNLP, pp. 372-
381. 
Huang, J., Gao, J., Miao, J., Li, X., Wang, K., and 
Behr, F. 2010. Exploring web scale language 
models for search query processing. In WWW, pp. 
451-460. 
Jarvelin, K. and Kekalainen, J. 2000. IR evaluation 
methods for retrieving highly relevant docu-
ments. In SIGIR, pp. 41-48 
Jin, R., Hauptmann, A. G., and Zhai, C. 2002. Title 
language model for information retrieval. In 
SIGIR, pp. 42-48. 
Jing, Y., and Croft., B. 1994. An association 
thesaurus for information retrieval. In RIAO, pp. 
146-160. 
Joachims, T. 2002. Optimizing search engines us-
ing clickthrough data. In SIGKDD, pp. 133-142. 
Koehn, P., Och, F., and Marcu, D. 2003. Statistical 
phrase-based translation. In HLT/NAACL, pp. 
127-133. 
Lang, H., Metzler, D., Wang, B., and Li, J-T. 2010. 
Improving latent concept expansion using 
markov random fields. In CIKM, pp. 249-258. 
Lavrenko, V., and Croft, B. 2001. Relevance-based 
language models. In SIGIR, pp. 120-128. 
Lease, M. 2009. An improved markov random 
field model for supporting verbose queries. In 
SIGIR, pp. 476-483 
Metzler, D., and Croft, B. 2005. A markov random 
field model for term dependencies. In SIGIR, pp. 
472-479. 
Metzler, D., and Croft, B. 2007. Latent concept 
expansion using markov random fields. In 
SIGIR, pp. 311-318. 
Morgan, W., Greiff, W., and Henderson, J.  2004.  
Direct maximization of average precision by 
hill-climbing with a comparison to a maximum 
entropy approach.  Technical report.  MITRE. 
Och, F. 2002. Statistical machine translation: from 
single-word models to alignment templates. PhD 
thesis, RWTH Aachen. 
Prager, J., Chu-Carroll, J., and Czuba, K. 2001. 
Use of Wordnet hypernyms for answering what 
is questions. In TREC 10. 
Press, W. H., Teukolsky, S. A., Vetterling, W. T., 
and Flannery, B. P. 1992. Numerical Recipes in 
C. Cambridge Univ. Press. 
Rocchio, J. 1971. Relevance feedback in infor-
mation retrieval. In The SMART retrieval system: 
experiments in automatic document processing, 
pp. 313-323, Prentice-Hall Inc. 
Riezler, S., Liu, Y. and Vasserman, A. 2008. 
Translating queries into snippets for improving 
query expansion. In COLING 2008. 737-744. 
Riezler, S., and Liu, Y. 2010. Query rewriting us-
ing monolingual statistical machine translation. 
Computational Linguistics, 36(3): 569-582. 
Wen, J., Nie, J-Y., and Zhang, H. 2002. Query 
clustering using user logs. ACM TOIS, 20(1): 59-
81. 
Xu, J., and Croft, B. 1996. Query expansion using 
local and global document analysis. In SIGIR. 
Xue, X., Jeon, J., Croft, W. B. 2008. Retrieval 
models for Question and answer archives.  In 
SIGIR, pp. 475-482. 
Zhai, C., and Lafferty, J. 2001a. Model-based 
feedback in the kl-divergence retrieval model. In 
CIKM, pp. 403-410. 
Zhai, C., and Lafferty, J. 2001b. A study of 
smoothing methods for language models applied 
to ad hoc information retrieval. In SIGIR, pp. 
334-342. 
676
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 46?54,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Confusion Networks for Speech Summarization
Shasha Xie and Yang Liu
Department of Computer Science
The University of Texas at Dallas
{shasha,yangl}@hlt.utdallas.edu
Abstract
For extractive meeting summarization, previ-
ous studies have shown performance degrada-
tion when using speech recognition transcripts
because of the relatively high speech recogni-
tion errors on meeting recordings. In this pa-
per we investigated using confusion networks
to improve the summarization performance
on the ASR condition under an unsupervised
framework by considering more word candi-
dates and their confidence scores. Our ex-
perimental results showed improved summa-
rization performance using our proposed ap-
proach, with more contribution from leverag-
ing the confidence scores. We also observed
that using these rich speech recognition re-
sults can extract similar or even better sum-
mary segments than using human transcripts.
1 Introduction
Speech summarization has received increasing in-
terest recently. It is a very useful technique that
can help users to browse a large amount of speech
recordings. The problem we study in this paper is
extractive meeting summarization, which selects the
most representative segments from the meeting tran-
scripts to form a summary. Compared to text sum-
marization, speech summarization is more challeng-
ing because of not only its more spontaneous style,
but also word errors in automatic speech recogni-
tion (ASR) output. Intuitively the incorrect words
have a negative impact on downstream summariza-
tion performance. Previous research has evaluated
summarization using either the human transcripts or
ASR output with word errors. Most of the prior
work showed that performance using ASR output is
consistently lower (to different extent) comparing to
that using human transcripts no matter whether su-
pervised or unsupervised approaches were used.
To address the problem caused by imperfect
recognition transcripts, in this paper we investigate
using rich speech recognition results for summariza-
tion. N-best hypotheses, word lattices, and confu-
sion networks have been widely used as an inter-
face between ASR and subsequent spoken language
processing tasks, such as machine translation, spo-
ken document retrieval (Chelba et al, 2007; Chia
et al, 2008), and shown outperforming using 1-
best hypotheses. However, studies using these rich
speech recognition results for speech summariza-
tion are very limited. In this paper, we demonstrate
the feasibility of using confusion networks under an
unsupervised MMR (maximum marginal relevance)
framework to improve summarization performance.
Our experimental results show better performance
over using 1-best hypotheses with more improve-
ment observed from using confidence measure of the
words. Moreover, we find that the selected summary
segments are similar to or even better than those gen-
erated using human transcripts.
2 Related Work
Many techniques have been proposed for the meet-
ing summarization task, including both unsuper-
vised and supervised approaches. Since we use un-
supervised methods in this study, we will not de-
scribe previous work using supervised approaches
because of the space limit. Unsupervised meth-
46
ods are simple and robust to different corpora, and
do not need any human labeled data for training.
MMR was introduced in (Carbonell and Goldstein,
1998) for text summarization, and was used widely
in meeting summarization (Murray et al, 2005a; Xie
and Liu, 2008). Latent semantic analysis (LSA) ap-
proaches have also been used (Murray et al, 2005a),
which can better measure document similarity at the
semantic level rather than relying on literal word
matching. In (Gillick et al, 2009), the authors intro-
duced a concept-based global optimization frame-
work using integer linear programming (ILP), where
concepts were used as the minimum units, and the
important sentences were extracted to cover as many
concepts as possible. They showed better perfor-
mance than MMR. In a follow-up study, (Xie et al,
2009) incorporated sentence information in this ILP
framework. Graph-based methods, such as LexRank
(Erkan and Radev, 2004), have been originally used
for extractive text summarization, where the docu-
ment is modeled as a graph and sentences as nodes,
and sentences are ranked according to its similarity
with other nodes. (Garg et al, 2009) proposed Clus-
terRank, a modified graph-based method in order
to take into account the conversational speech style
in meetings. Recently (Lin et al, 2009) suggested
to formulate the summarization task as optimizing
submodular functions defined on the document?s se-
mantic graph, and showed better performance com-
paring to other graph-based approaches.
Rich speech recognition results, such as N-best
hypotheses and confusion networks, were first used
in multi-pass ASR systems to improve speech recog-
nition performance (Stolcke et al, 1997; Mangu et
al., 2000). They have been widely used in many sub-
sequent spoken language processing tasks, such as
machine translation, spoken document understand-
ing and retrieval. Confusion network decoding was
applied to combine the outputs of multiple machine
translation systems (Sim et al, 2007; Matusov et
al., 2006). In the task of spoken document retrieval,
(Chia et al, 2008) proposed to compute the expected
word counts from document and query lattices, and
estimate the statistical models from these counts,
and reported better retrieval accuracy than using
only 1-best transcripts. (Hakkani-Tur et al, 2006)
investigated using confusion networks for name en-
tity detection and extraction and user intent classifi-
cation. They also obtained better performance than
using ASR 1-best output.
There is very limited previous work using more
than 1-best ASR output for speech summarization.
Several studies used acoustic confidence scores in
the 1-best ASR hypothesis in the summarization sys-
tems (Valenza et al, 1999; Zechner and Waibel,
2000; Hori and Furui, 2003). (Liu et al, 2010) eval-
uated using n-best hypotheses for meeting summa-
rization, and showed improved performance with the
gain coming mainly from the first few candidates. In
(Lin and Chen, 2009), confusion networks and po-
sition specific posterior lattices were considered in
a generative summarization framework for Chinese
broadcast news summarization, and they showed
promising results by using more ASR hypotheses.
We investigate using confusion networks for meet-
ing summarization in this study. This work differs
from (Lin and Chen, 2009) in terms of the language
and genre used in the summarization task, as well
as the summarization approaches. We also perform
more analysis on the impact of confidence scores,
different pruning methods, and different ways to
present system summaries.
3 Summarization Approach
In this section, we first describe the baseline sum-
marization framework, and then how we apply it to
confusion networks.
3.1 Maximum Marginal Relevance (MMR)
MMR is a widely used unsupervised approach in
text and speech summarization, and has been shown
perform well. We chose this method as the basic
framework for summarization because of its sim-
plicity and efficiency. We expect this is a good
starting point for the study of feasibility of us-
ing confusion networks for summarization. For
each sentence segment Si in one document D, its
score (MMR(i)) is calculated using Equation 1
according to its similarity to the entire document
(Sim1(Si, D)) and the similarity to the already ex-
tracted summary (Sim2(Si, Summ)).
MMR(i) =
?? Sim1(Si, D)? (1? ?)? Sim2(Si, Summ)
(1)
47
where parameter ? is used to balance the two factors
to ensure the selected summary sentences are rel-
evant to the entire document (thus important), and
compact enough (by removing redundancy with the
currently selected summary sentences). Cosine sim-
ilarity can be used to compute the similarity of two
text segments. If each segment is represented as a
vector, cosine similarity between two vectors (V1,
V2) is measured using the following equation:
sim(V1, V2) =
?
i t1it2i??
i t
2
1i ?
??
i t
2
2i
(2)
where ti is the term weight for a word wi, for which
we can use the TFIDF (term frequency, inverse doc-
ument frequency) value, as widely used in the field
of information retrieval.
3.2 Using Confusion Networks for
Summarization
Confusion networks (CNs) have been used in many
natural language processing tasks. Figure 1 shows
a CN example for a sentence segment. It is a di-
rected word graph from the starting node to the end
node. Each edge represents a word with its associ-
ated posterior probability. There are several word
candidates for each position. ?-? in the CN repre-
sents a NULL hypothesis. Each path in the graph is
a sentence hypothesis. For the example in Figure 1,
?I HAVE IT VERY FINE? is the best hypothesis
consisting of words with the highest probabilities for
each position. Compared to N-best lists, confusion
networks are a more compact and powerful repre-
sentation for word candidates. We expect the rich in-
formation contained in the confusion networks (i.e.,
more word candidates and associated posterior prob-
abilities) can help to determine words? importance
for summarization.
Figure 1: An example of confusion networks.
The core problems when using confusion net-
works under the MMR summarization framework
are the definitions for Si, D, and Summ, as shown
in Equation 1. The extractive summary unit (for
each Si) we use is the segment provided by the rec-
ognizer. This is often different from syntactic or se-
mantic meaningful unit (e.g., a sentence), but is a
more realistic setup. Most of the previous studies
for speech summarization used human labeled sen-
tences as extraction units (for human transcripts, or
map them to ASR output), which is not the real sce-
nario when performing speech summarization on the
ASR condition. In the future, we will use automatic
sentence segmentation results, which we expect are
better units than pause-based segmentation used in
ASR. We still use a vector space model to represent
each summarization unit Si. The entire document
(D) and the current selected summary (Summ) are
formed by simply concatenating the corresponding
segments Si together. In the following, we describe
different ways to represent the segments and how to
present the final summary.
A. Segmentation representation
First, we construct the vector for each segment
simply using all the word candidates in the CNs,
without considering any confidence measure or pos-
terior probability information. The same TFIDF
computation is used as before, i.e., counting the
number of times a word appears (TF) and how many
documents it appears (used to calculate IDF).
Second, we leverage the confidence scores to
build the vector. For the term frequency of word wi,
we calculate it by summing up its posterior proba-
bilities p(wik) at each position k, that is,
TF (wi) =
?
k
p(wik) (3)
Similarly, the IDF values can also be computed us-
ing the confidence scores. The traditional method
for calculating a word?s IDF uses the ratio of the
total number of documents (N ) and the number of
documents containing this word. Using the confi-
dence scores, we calculate the IDF values as follows,
IDF (wi) = log(
N
?
D (maxk p(wik))
) (4)
If a word wi appears in the document, we find its
maximum posterior probability among all the posi-
tions it occurs in the CNs, which is used to signal
wi?s soft appearance in this document. We add these
soft counts for all the documents as the denomina-
tor in Equation 4. Different from the traditional IDF
48
calculation method, where the number of documents
containing a word is an integer number, here the de-
nominator can be any real number.
B. Confusion network pruning
The above vectors are constructed using the entire
confusion networks. We may also use the pruned
ones, in which the words with low posterior prob-
abilities are removed beforehand. This can avoid
the impact of noisy words, and increase the system
speed as well. We investigate three different pruning
methods, listed below.
? absolute pruning: In this method, we delete
words if their posterior probabilities are lower
than a predefined threshold, i.e., p(wi) < ?.
? max diff pruning: First for each position k,
we find the maximum probability among all
the word candidates: Pmaxk = maxj p(wjk).
Then we remove a word wi in this position if
the absolute difference of its probability with
the maximum score is larger than a predefined
threshold, i.e., Pmaxk ? p(wik) > ?.
? max ratio pruning: This is similar to the above
one, but instead of absolute difference, we use
the ratio of their probabilities, i.e., p(wik)Pmaxk < ?.
Again, for the last two pruning methods, the com-
parison is done for each position in the CNs.
C. Summary rendering
With a proper way of representing the text seg-
ments, we then extract the summary segments using
the MMR method described in Section 3.1. Once the
summary segments are selected using the confusion
network input, another problem we need to address
is how to present the final summary. When using
the human transcripts or the 1-best ASR hypothesis
for summarization, we can simply concatenate the
corresponding transcripts of the selected sentence
segments as the final summary for the users. How-
ever, when using the confusion networks as the rep-
resentation of each sentence segment, we only know
which segments are selected by the summarization
system. To provide the final summary to the users,
there are two choices. We can either use the best hy-
pothesis from CNs of those selected segments as a
text summary; or return the speech segments to the
users to allow them to play it back. We will evaluate
both methods in this paper. For the latter, in order to
use similar word based performance measures, we
will use the corresponding reference transcripts in
order to focus on evaluation of the correctness of the
selected summary segments.
4 Experiments
4.1 Corpus and Evaluation Measurement
We use the ICSI meeting corpus, which contains 75
recordings from natural meetings (most are research
discussions) (Janin et al, 2003). Each meeting is
about an hour long and has multiple speakers. These
meetings have been transcribed, and annotated with
extractive summaries (Murray et al, 2005b). The
ASR output is obtained from a state-of-the-art SRI
speech recognition system, including the confusion
network for each sentence segment (Stolcke et al,
2006). The word error rate (WER) is about 38.2%
on the entire corpus.
The same 6 meetings as in (Murray et al, 2005a;
Xie and Liu, 2008; Gillick et al, 2009; Lin et al,
2009) are used as the test set in this study. Fur-
thermore, 6 other meetings were randomly selected
from the remaining 69 meetings in the corpus to
form a development set. Each meeting in the de-
velopment set has only one human-annotated sum-
mary; whereas for the test meetings, we use three
summaries from different annotators as references
for performance evaluation. The lengths of the ref-
erence summaries are not fixed and vary across an-
notators and meetings. The average word compres-
sion ratio for the test set is 14.3%, and the mean de-
viation is 2.9%. We generated summaries with the
word compression ratio ranging from 13% to 18%,
and only provide the best results in this paper.
To evaluate summarization performance, we use
ROUGE (Lin, 2004), which has been widely used
in previous studies of speech summarization (Zhang
et al, 2007; Murray et al, 2005a; Zhu and Penn,
2006). ROUGE compares the system generated
summary with reference summaries (there can be
more than one reference summary), and measures
different matches, such as N-gram, longest com-
mon sequence, and skip bigrams. In this paper,
we present our results using both ROUGE-1 and
49
ROUGE-2 F-scores.
4.2 Characteristics of CNs
First we perform some analysis of the confusion net-
works using the development set data. We define
two measurements:
? Word coverage. This is to verify that CNs con-
tain more correct words than the 1-best hy-
potheses. It is defined as the percentage of
the words in human transcripts (measured us-
ing word types) that appear in the CNs. We
use word types in this measurement since we
are using a vector space model and the multi-
ple occurrence of a word only affects its term
weights, not the dimension of the vector. Note
that for this analysis, we do not perform align-
ment that is needed in word error rate measure
? we do not care whether a word appears in the
exact location; as long as a word appears in the
segment, its effect on the vector space model is
the same (since it is a bag-of-words model).
? Average node density. This is the average num-
ber of candidate words for each position in the
confusion networks.
Figure 2 shows the analysis results for these two
metrics, which are the average values on the devel-
opment set. In this analysis we used absolute prun-
ing method, and the results are presented for dif-
ferent pruning thresholds. For a comparison, we
also include the results using the 1-best hypotheses
(shown as the dotted line in the figure), which has an
average node density of 1, and the word coverage of
71.55%. When the pruning threshold is 0, the results
correspond to the original CNs without pruning.
We can see that the confusion networks include
much more correct words than 1-best hypotheses
(word coverage is 89.3% vs. 71.55%). When in-
creasing the pruning thresholds, the word coverage
decreases following roughly a linear pattern. When
the pruning threshold is 0.45, the word coverage of
the pruned CNs is 71.15%, lower than 1-best hy-
potheses. For node density, the non-pruned CNs
have an average density of 11.04. With a very small
pruning threshold of 0.01, the density decreases
rapidly to 2.11. The density falls less than 2 when
the threshold is 0.02, which means that for some
0123
4567
891011
12
0 0.01 0.02 0.03 0.04 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5Pruning Threshold
Node Density
7075
8085
90
Word Coverage 
(%)node density word coverage
Figure 2: Average node density and word coverage of the
confusion networks on the development set.
nodes there is only one word candidate preserved
after pruning (i.e., only one word has a posterior
probability higher than 0.02). When the threshold
increases to 0.4, the density is less than 1 (0.99),
showing that on average there is less than one candi-
date left for each position. This is consistent with the
word coverage results ? when the pruning thresh-
old is larger than 0.45, the confusion networks have
less word coverage than 1-best hypotheses because
even the top word hypotheses are deleted. There-
fore, for our following experiments we only use the
thresholds ? ? 0.45 for absolute pruning.
Note that the results in the figure are based on
absolute pruning. We also performed analysis us-
ing the other two pruning methods described in Sec-
tion 3.2. For those methods, because the decision
is made by comparing each word?s posterior proba-
bility with the maximum score for that position, we
can guarantee that at least the best word candidate is
included in the pruned CNs. We varied the pruning
threshold from 0 to 0.95 for these pruning methods,
and observed similar patterns as in absolute prun-
ing for the word coverage and node density analysis.
As expected, the fewer word candidates are pruned,
the better word coverage and higher node density the
pruned CNs have.
4.3 Summarization Results
4.3.1 Results on dev set using 1-best hypothesis
and human transcripts
We generate the baseline summarization result
using the best hypotheses from the confusion net-
50
works. The summary sentences are extracted using
the MMR method introduced in Section 3.1. The
term weighting is the traditional TFIDF value. The
ROUGE-1 and ROUGE-2 scores for the baseline are
listed in Table 1.
Because in this paper our task is to evaluate the
summarization performance using ASR output, we
generate an oracle result, where the summary ex-
traction and IDF calculation are based on the human
transcripts for each ASR segment. These results are
also presented in Table 1. Comparing the results for
the two testing conditions, ASR output and human
transcripts, we can see the performance degradation
due to recognition errors. The difference between
them seems to be large enough to warrant investiga-
tion of using rich ASR output for improved summa-
rization performance.
ROUGE-1 ROUGE-2
Baseline: best hyp 65.60 26.83
Human transcript 69.98 33.21
Table 1: ROUGE results (%) using 1-best hypotheses and
human transcripts on the development set.
4.3.2 Results on the dev set using CNs
A. Effect of segmentation representation
We evaluate the effect on summarization using
different vector representations based on confusion
networks. Table 2 shows the results on the develop-
ment set using various input under the MMR frame-
work. We also include the results using 1-best and
human transcripts in the table as a comparison. The
third row in the table uses the 1-best hypothesis, but
the term weight for each word is calculated by con-
sidering its posterior probability in the CNs (denoted
by ?wp?). We calculate the TF and IDF values us-
ing Equation 3 and 4 introduced in Section 3.2. The
other representations in the table are for the non-
pruned and pruned CNs based on different pruning
methods, and with or without using the posteriors to
calculate term weights.
In general, we find that using confusion networks
improves the summarization performance compar-
ing with the baseline. Since CNs contain more can-
didate words and posterior probabilities, a natural
segment representation ROUGE-1 ROUGE-2
Best hyp 65.60 26.83
Best hyp (wp) 66.83 29.84
Non-pruned CNs 66.58 28.22
Non-pruned CNs (wp) 66.47 29.27
Pruned CNs
Absolute 67.44 29.02
Absolute (wp) 66.98 29.99
Max diff 67.29 28.97
Max diff (wp) 67.10 29.76
Max ratio 67.43 28.97
Max ratio (wp) 67.06 29.90
Human transcript 69.98 33.21
Table 2: ROUGE results (%) on the development set us-
ing different vector representations based on confusion
networks: non-pruned and pruned, using posterior prob-
abilities (?wp?) and without using them.
question to ask is, which factor contributes more to
the improved performance? We can compare the re-
sults in Table 2 across different conditions that use
the same candidate words, one with standard TFIDF,
and the other with posteriors for TFIDF, or that use
different candidate words and the same setup for
TFIDF calculation. Our results show that there is
more improvement using our proposed method for
TFIDF calculation based on posterior probabilities,
especially ROUGE-2 scores. Even when just us-
ing 1-best hypotheses, if we consider posteriors, we
can obtain very competitive results. There is also
a difference in the effect of using posterior proba-
bilities. When using the top hypotheses representa-
tion, posteriors help both ROUGE-1 and ROUGE-2
scores; when using confusion networks, non-pruned
or pruned, using posterior probabilities improves
ROUGE-2 results, but not ROUGE-1.
Our results show that adding more candidates in
the vector representation does not necessarily help
summarization. Using the pruned CNs yields bet-
ter performance than the non-pruned ones. There is
not much difference among different pruning meth-
ods. Overall, the best results are achieved by using
pruned CNs: best ROUGE-1 result without using
posterior probabilities, and best ROUGE-2 scores
when using posteriors.
B. Presenting summaries using human tran-
scripts
51
segment representation ROUGE-1 ROUGE-2
Best hyp 68.26 32.25
Best hyp (wp) 69.16 33.99
Non-pruned CNs 69.28 33.49
Non-pruned CNs (wp) 67.84 32.95
Pruned CNs
Absolute 69.66 34.06
Absolute (wp) 69.37 34.25
Max diff 69.88 34.17
Max diff (wp) 69.38 33.94
Max ratio 69.76 34.06
Max ratio (wp) 69.44 34.39
Human transcript 69.98 33.21
Table 3: ROUGE results (%) on the development set
using different segment representations, with the sum-
maries constructed using the corresponding human tran-
scripts for the selected segments.
In the above experiments, we construct the final
summary using the best hypotheses from the con-
fusion networks once the summary sentence seg-
ments are determined. Although we notice obvious
improvement comparing with the baseline results,
the ROUGE scores are still much lower than using
the human transcripts. One reason for this is the
speech recognition errors. Even if we select the cor-
rect utterance segment as in the reference summary
segments, the system performance is still penalized
when calculating the ROUGE scores. In order to
avoid the impact of word errors and focus on evalu-
ating whether we have selected the correct segments,
next we use the corresponding human transcripts of
the selected segments to obtain performance mea-
sures. The results from this experiment are shown in
Table 3 for different segment representations.
We can see that the summaries formed using hu-
man transcripts are much better comparing with the
results presented in Table 2. These two setups use
the same utterance segments. The only difference
lies in the construction of the final summary for
performance measurement, using the top hypothe-
ses or the corresponding human transcripts for the
selected segments. We also notice that the differ-
ence between using 1-best hypothesis and human
transcripts is greatly reduced using this new sum-
mary formulation. This suggests that the incorrect
word hypotheses do not have a very negative im-
pact in terms of selecting summary segments; how-
ever, word errors still account for a significant part
of the performance degradation on ASR condition
when using word-based metrics for evaluation. Us-
ing the best hypotheses with their posterior proba-
bilities we can obtain similar ROUGE-1 score and
a little higher ROUGE-2 score comparing to the re-
sults using human transcripts. The performance can
be further improved using the pruned CNs.
Note that when using the non-pruned CNs and
posterior probabilities for term weighting, the
ROUGE scores are worse than most of other condi-
tions. We performed some analysis and found that
one reason for this is the selection of some poor
segments. Most of the word candidates in the non-
pruned CNs have very low confidence scores, result-
ing in high IDF values using our proposed methods.
Since some top hypotheses are NULL words in the
poorly selected summary segments, it did not affect
the results when using the best hypothesis for eval-
uation, but when using human transcripts, it leads to
lower precision and worse overall F-scores. This is
not a problem for the pruned CNs since words with
low probabilities have been pruned beforehand, and
thus do not impact segment selection. We will inves-
tigate better methods for term weighting to address
this issue in our future work.
These experimental results prove that using the
confusion networks and confidence scores can help
select the correct sentence segments. Even though
the 1-best WER is quite high, if we can con-
sider more word candidates and/or their confidence
scores, this will not impact the process of select-
ing summary segments. We can achieve similar
performance as using human transcripts, and some-
times even slightly better performance. This sug-
gests using more word candidates and their confi-
dence scores results in better term weighting and
representation in the vector space model. Some
previous work showed that using word confidence
scores can help minimize the WER of the extracted
summaries, which then lead to better summarization
performance. However, we think the main reason
for the improvement in our study is from selecting
better utterances, as shown in Table 3. In our ex-
periments, because different setups select different
segments as the summary, we can not directly com-
pare the WER of extracted summaries, and analyze
whether lower WER is also helpful for better sum-
52
output summary
best hypotheses human transcripts
R-1 R-2 R-1 R-2
Best hyp 65.73 26.79 68.60 32.03
Best hyp (wp) 65.92 27.27 68.91 32.69
Pruned CNs 66.47 27.73 69.53 34.05
Human transcript N/A N/A 69.08 33.33
Table 4: ROUGE results (%) on the test set.
marization performance. In our future work, we will
perform more analysis along this direction.
4.3.3 Experimental results on test set
The summarization results on the test set are pre-
sented in Table 4. We show four different evalua-
tion conditions: baseline using the top hypotheses,
best hypotheses with posterior probabilities, pruned
CNs, and using human transcripts. For each condi-
tion, the final summary is evaluated using the best
hypotheses or the corresponding human transcripts
of the selected segments. The summarization system
setups (the pruning method and threshold, ? value in
MMR function, and word compression ratio) used
for the test set are decided based on the results on
the development set.
For the results on the test set, we observe sim-
ilar trends as on the development set. Using the
confidence scores and confusion networks can im-
prove the summarization performance comparing
with the baseline. The performance improvements
from ?Best hyp? to ?Best hyp (wp)? and from ?Best
hyp (wp)? to ?Pruned CNs? using both ROUGE-1
and ROUGE-2 measures are statistically significant
according to the paired t-test (p < 0.05). When the
final summary is presented using the human tran-
scripts of the selected segments, we observe slightly
better results using pruned CNs than using human
transcripts as input for summarization, although the
difference is not statistically significant. This shows
that using confusion networks can compensate for
the impact from recognition errors and still allow us
to select correct summary segments.
5 Conclusion and Future Work
Previous research has shown performance degrada-
tion when using ASR output for meeting summa-
rization because of word errors. To address this
problem, in this paper we proposed to use confu-
sion networks for speech summarization. Under the
MMR framework, we introduced a vector represen-
tation for the segments by using more word can-
didates in CNs and their associated posterior prob-
abilities. We evaluated the effectiveness of using
different confusion networks, the non-pruned ones,
and the ones pruned using three different methods,
i.e., absolute, max diff and max ratio pruning. Our
experimental results on the ICSI meeting corpus
showed that even when we only use the top hypothe-
ses from the CNs, considering the word posterior
probabilities can improve the summarization perfor-
mance on both ROUGE-1 and ROUGE-2 scores.
By using the pruned CNs we can obtain further im-
provement. We found that more gain in ROUGE-
2 results was yielded by our proposed soft term
weighting method based on posterior probabilities.
Our experiments also demonstrated that it is pos-
sible to use confusion networks to achieve similar
or even better performance than using human tran-
scripts if the goal is to select the right segments. This
is important since one possible rendering of summa-
rization results is to return the audio segments to the
users, which does not suffer from recognition errors.
In our experiments, we observed less improve-
ment from considering more word candidates than
using the confidence scores. One possible reason is
that the confusion networks we used are too confi-
dent. For example, on average 90.45% of the can-
didate words have a posterior probability lower than
0.01. Therefore, even though the correct words were
included in the confusion networks, their contribu-
tion may not be significant enough because of low
term weights. In addition, low probabilities also
cause problems to our proposed soft IDF computa-
tion. In our future work, we will investigate prob-
ability normalization methods and other techniques
for term weighting to cope with these problems.
6 Acknowledgment
This research is supported by NSF award IIS-
0845484. Any opinions expressed in this work are
those of the authors and do not necessarily reflect
the views of NSF. The authors thank Shih-Hsiang
Lin and Fei Liu for useful discussions.
53
References
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
SIGIR.
Ciprian Chelba, Jorge Silva, and Alex Acero. 2007.
Soft indexing of speech content for search in spoken
documents. In Computer Speech and Language, vol-
ume 21, pages 458?478.
Tee Kiah Chia, Khe Chai Sim, Haizhou Li, and Hwee Tou
Ng. 2008. A lattice-based approach to query-by-
example spoken document retrieval. In Proceedings
of SIGIR.
Gunes Erkan and Dragomir R. Radev. 2004. LexRank:
graph-based lexical centrality as salience in text sum-
marization. Artificial Intelligence Research, 22:457?
479.
Nikhil Garg, Benoit Favre, Korbinian Reidhammer, and
Dilek Hakkani-Tur. 2009. ClusterRank: a graph based
method for meeting summarization. In Proceedings of
Interspeech.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-Tur. 2009. A global optimization
framework for meeting summarization. In Proceed-
ings of ICASSP.
Dilek Hakkani-Tur, Frederic Behet, Giuseppe Riccardi,
and Gokhan Tur. 2006. Beyond ASR 1-best: using
word confusion networks in spoken language under-
standing. Computer Speech and Language, 20(4):495
? 514.
Chiori Hori and Sadaoki Furui. 2003. A new approach to
automatic speech summarization. IEEE Transactions
on Multimedia, 5(3):368?378.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin, Thilo
Pfau, Elizabeth Shriberg, Andreas Stolcke, and Chuck
Wooters. 2003. The ICSI meeting corpus. In Pro-
ceedings of ICASSP.
Shih-Hsiang Lin and Berlin Chen. 2009. Improved
speech summarization with multiple-hypothesis repre-
sentations and Kullback-Leibler divergence measures.
In Proceedings of Interspeech.
Hui Lin, Jeff Bilmes, and Shasha Xie. 2009. Graph-
based submodular selection for extractive summariza-
tion. In Proceedings of ASRU.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In the Workshop on
Text Summarization Branches Out.
Yang Liu, Shasha Xie, and Fei Liu. 2010. Using n-best
recognition output for extractive summarization and
keyword extraction in meeting speech. In Proceedings
of ICASSP.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14:373?400.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proceedings of EACL.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005a.
Extractive summarization of meeting recordings. In
Proceedings of Interspeech.
Gabriel Murray, Steve Renals, Jean Carletta, and Johanna
Moore. 2005b. Evaluating automatic summaries of
meeting recordings. In Proceedings of the ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation.
Khe Chai Sim, William Byrne, Mark Gales, Hichem
Sahbi, and Phil Woodland. 2007. Consensus net-
work decoding for statistical machine translation sys-
tem combination. In Proceedings of ICASSP.
Andreas Stolcke, Yochai Konig, and Mitchel Weintraub.
1997. Explicit word error minimization in N-best list
rescoring. In Proceedings of Eurospeech.
Andreas Stolcke, Barry Chen, Horacio Franco,
Venkata Ra mana Rao Gadde, Martin Graciarena,
Mei-Yuh Hwang, Katrin Kirchhoff, Arindam Mandal,
Nelson Morgan, Xin Lei, Tim Ng, and et al 2006.
Recent innovations in speech-to-text transcription at
SRI-ICSI-UW. IEEE Transactions on Audio, Speech,
and Language Processing, 14(5):1729?1744.
Robin Valenza, Tony Robinson, Marianne Hickey, and
Roger Tucker. 1999. Summarization of spoken audio
through information extraction. In Proceedings of the
ESCA Workshop on Accessing Information in Spoken
Audio, pages 111?116.
Shasha Xie and Yang Liu. 2008. Using corpus
and knowledge-based similarity measure in maximum
marginal relevance for meeting summarization. In
Proceedings of ICASSP.
Shasha Xie, Benoit Favre, Dilek Hakkani-Tur, and Yang
Liu. 2009. Leveraging sentence weights in concept-
based optimization framework for extractive meeting
summarization. In Proceedings of Interspeech.
Klaus Zechner and Alex Waibel. 2000. Minimizing word
error rate in textual summaries of spoken language. In
Proceedings of NAACL.
Jian Zhang, Ho Yin Chan, Pascale Fung, and Lu Cao.
2007. A comparative study on speech summarization
of broadcast news and lecture speech. In Proceedings
of Interspeech.
Xiaodan Zhu and Gerald Penn. 2006. Summarization of
spontaneous conversations. In Proceedings of Inter-
speech.
54
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 103?111,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Exploring Content Features for Automated Speech Scoring
Shasha Xie, Keelan Evanini, Klaus Zechner
Educational Testing Service (ETS)
Princeton, NJ 08541, USA
{sxie,kevanini,kzechner}@ets.org
Abstract
Most previous research on automated speech
scoring has focused on restricted, predictable
speech. For automated scoring of unrestricted
spontaneous speech, speech proficiency has
been evaluated primarily on aspects of pro-
nunciation, fluency, vocabulary and language
usage but not on aspects of content and topi-
cality. In this paper, we explore features repre-
senting the accuracy of the content of a spoken
response. Content features are generated us-
ing three similarity measures, including a lex-
ical matching method (Vector Space Model)
and two semantic similarity measures (Latent
Semantic Analysis and Pointwise Mutual In-
formation). All of the features exhibit moder-
ately high correlations with human proficiency
scores on human speech transcriptions. The
correlations decrease somewhat due to recog-
nition errors when evaluated on the output of
an automatic speech recognition system; how-
ever, the additional use of word confidence
scores can achieve correlations at a similar
level as for human transcriptions.
1 Introduction
Automated assessment of a non-native speaker?s
proficiency in a given language is an attractive ap-
plication of automatic speech recognition (ASR) and
natural language processing (NLP) technology; the
technology can be used by language learners for
individual practice and by assessment providers to
reduce the cost of human scoring. While much
research has been done about the scoring of re-
stricted speech, such as reading aloud or repeating
sentences verbatim (Cucchiarini et al, 1997; Bern-
stein et al, 2000; Cucchiarini et al, 2000; Witt and
Young, 2000; Franco et al, 2000; Bernstein et al,
2010b), much less has been done about the scor-
ing of spontaneous speech. For automated scor-
ing of unrestricted, spontaneous speech, most auto-
mated systems have estimated the non-native speak-
ers? speaking proficiency primarily based on low-
level speaking-related features, such as pronuncia-
tion, intonation, rhythm, rate of speech, and fluency
(Cucchiarini et al, 2002; Zechner et al, 2007; Chen
et al, 2009; Chen and Zechner, 2011a), although a
few recent studies have explored features based on
vocabulary and grammatical complexity (Zechner et
al., 2007; Bernstein et al, 2010a; Bernstein et al,
2010b; Chen and Zechner, 2011b).
To date, little work has been conducted on au-
tomatically assessing the relatively higher-level as-
pects of spontaneous speech, such as the content
and topicality, the structure, and the discourse in-
formation. Automated assessment of these aspects
of a non-native speaker?s speech is very challeng-
ing for a number of reasons, such as the short length
of typical responses (approximately 100 words for
a typical 1 minute response, compared to over 300
words in a typical essay/written response), the spon-
taneous nature of the speech, and the presence of
disfluencies and possible grammatical errors. More-
over, the assessment system needs text transcripts
of the speech to evaluate the high level aspects, and
these are normally obtained from ASR systems. The
recognition accuracy of state-of-the-art ASR sys-
tems on non-native spontaneous speech is still rel-
atively low, which will sequentially impact the re-
103
liability and accuracy of automatic scoring systems
using these noisy transcripts. However, despite these
difficulties, it is necessary for an automated assess-
ment system to address the high level information
of a spoken response in order to fully cover all as-
pects that are considered by human raters. Thus, in
this paper we focus on exploring features to repre-
sent the high-level aspect of speech mainly on the
accuracy of the content.
As a starting point, we consider approaches that
have been used for the automated assessment of con-
tent in essays. However, due to the qualitative dif-
ferences between written essays and spontaneous
speech, the techniques developed for written texts
may not perform as well on spoken responses. Still,
as a baseline, we will evaluate the content features
used for essay scoring on spontaneous speech. In
addition to a straightforward lexical Vector Space
Model (VSM), we investigate approaches using two
other similarity measures, Latent Semantic Analysis
(LSA) and Pointwise Mutual Information (PMI), in
order to represent the semantic-level proficiency of
a speaker. All of the content features are analyzed
using both human transcripts and speech recognizer
output, so we can have a better understanding of the
impact of ASR errors on the performance of the fea-
tures. As expected, the results show that the per-
formance on ASR output is lower than when hu-
man transcripts are used. Therefore, we propose im-
proved content features that take into account ASR
confidence scores to emphasize responses whose es-
timated word accuracy is comparatively higher than
others. These improved features can obtain similar
performance when compared to the results using hu-
man transcripts.
This paper is organized as follows. In the next
section we introduce previous research on auto-
mated assessment of content in essays and spoken
responses. The content features we generated and
the model we used to build the final speaking scores
are described in Sections 3 and Section 4, respec-
tively. In Section 5 we show the performance of
all our proposed features. Finally, we conclude our
work and discuss potential future work in Section 6.
2 Related Work
Most previous research on assessment of non-native
speech has focused on restricted, predictable speech;
see, for example, the collection of articles in (Es-
kenazi et al, 2009). When assessing spontaneous
speech, due to relatively high word error rates of
current state-of-the-art ASR systems, predominantly
features related to low-level information have been
used, such as features related to fluency, pronuncia-
tion or prosody (Zechner et al, 2009).
For scoring of written language (automated essay
scoring), on the other hand, several features related
to the high level aspects have been used previously,
such as the content and the discourse information.
In one approach, the lexical content of an essay was
evaluated by using a VSM to compare the words
contained in each essay to the words found in a sam-
ple of essays from each score category (Attali and
Burstein, 2006). In addition, this system also used
an organization feature measuring the difference be-
tween the ideal structure of an essay and the actual
discourse elements found in the essay. The features
designed for measuring the overall organization of
an essay assumed a writing strategy that included
an introductory paragraph, at least a three-paragraph
body with each paragraph in the body consisting of
a pair of main point, supporting idea elements, and
a concluding paragraph. In another approach, the
content of written essays were evaluated using LSA
by comparing the test essays with essays of known
quality in regard of their degree of conceptual rele-
vance and the amount of relevant content (Foltz et
al., 1999).
There has been less work measuring spoken re-
sponses in terms of the higher level aspects. In
(Zechner and Xi, 2008), the authors used a content
feature together with other features related to vocab-
ulary, pronunciation and fluency to build an auto-
mated scoring system for spontaneous high-entropy
responses. This content feature was the cosine word
vector product between a test response and the train-
ing responses which have the highest human score.
The experimental results showed that this feature
did not provide any further contribution above a
baseline of only using non-content features, and
for some tasks the system performance was even
slightly worse after including this feature. However,
104
we think the observations about the content features
used in this paper were not reliable for the following
two reasons: the number of training responses was
limited (1000 responses), and the ASR system had a
relatively high Word Error Rate (39%).
In this paper, we provide further analysis on the
performance of several types of content features.
Additionally, we used a larger amount of training
data and a better ASR system in an attempt to extract
more meaningful and accurate content features.
3 Automatic Content Scoring
In automatic essay scoring systems, the content of an
essay is typically evaluated by comparing the words
it contains to the words found in a sample of es-
says from each score category (1-4 in our experi-
ments), where the scores are assigned by trained hu-
man raters. The basic idea is that good essays will
resemble each other in their word choice, as will
poor essays. We follow this basic idea when extract-
ing content features for spoken responses.
3.1 Scoring Features
For each test spoken response, we calculate its simi-
larity scores to the sample responses from each score
category. These scores indicate the degree of simi-
larity between the words used in the test response
and the words used in responses from different score
points. Using these similarity scores, 3 content fea-
tures are generated in this paper:
? Simmax: the score point which has the high-
est similarity score between test response and
score vector
? Sim4: the similarity score to the responses
with the highest score category (4 in our ex-
periments).
? Simcmb: the linear combination of the similar-
ity scores to each score category.
4?
i=1
wi ? Simi (1)
where wi is scaled to [-1, 1] to imply its positive
or negative impact.
3.2 Similarity Measures
There are many ways to calculate the similarity be-
tween responses. A simple and commonly used
method is the Vector Space Model, which is also
used in automated essay scoring systems. Under this
approach, all the responses are converted to vectors,
whose elements are weighted using TF*IDF (term
frequency, inverse document frequency). Then, the
cosine similarity score between vectors can be used
to estimate the similarity between the responses the
vectors originally represent.
Other than this lexical matching method, we also
try two additional similarity measures to better cap-
ture the semantic level information: Latent Semantic
Analysis (Landauer et al, 1998) and a corpus-based
semantic similarity measure based on pointwise mu-
tual information (Mihalcea et al, 2006). LSA has
been widely used for computing document similar-
ity and other information retrieval tasks. Under this
approach, Singular Value Decomposition (SVD) is
used to analyze the statistical relationship between a
set of documents and the words they contain. A m?n
word-document matrix X is first built, in which each
element Xij represents the weighted term frequency
of word i in document j. The matrix is decomposed
into a product of three matrices as follows:
X = U?V T (2)
where U is an m?m matrix of left-singular vectors,
? is an m?n diagonal matrix of singular values, and
V is the n? n matrix of right-singular vectors.
The top ranked k singular values in ? are kept,
and the left is set to be 0. So ? is reformulated as ?k.
The original matrix X is recalculated accordingly,
Xk = U?kV
T (3)
This new matrix Xk can be considered as a
smoothed or compressed version of the original ma-
trix. LSA measures the similarity of two documents
by calculating the cosine between the corresponding
compressed column vectors.
PMI was introduced to calculate the semantic
similarity between words in (Turney, 2001). It is
based on the word co-occurrence on a large corpus.
Given two words, their PMI is computed using:
PMI(w1, w2) = log2
p(w1&w2)
p(w1) ? p(w2)
(4)
105
This indicates the statistical dependency between w1
and w2, and can be used as a measure of the semantic
similarity of two words.
Given the word-to-word similarity, we can calcu-
late the similarity between two documents using the
following function,
sim(D1, D2) =
1
2
(
?
w?{D1} (maxSim(w,D2) ? idf(w))?
w?{D1} idf(w)
+
?
w?{D2}(maxSim(w,D1) ? idf(w)?
w?{D2} idf(w))
)
(5)
maxSim(w,Di) = maxwj?{Di}PMI(w,wj)
(6)
For each word w in document D1, we find the word
in document D2 which has the highest similarity
to w. Similarly, for each word in D2, we iden-
tify the most similar words in D1. The similarity
score between two documents is then calculated by
combining the similarity of the words they contain,
weighted by their word specificity (i.e., IDF values).
In this paper, we use these three similarity mea-
sures to calculate the similarity between the test re-
sponse and the training responses for each score cat-
egory. Using the VSM method, we convert all the
training responses in one score category into one big
vector, and for a given test response we calculate its
cosine similarity to this vector as its similarity to that
corresponding score point vector. For the other simi-
larity measures, we calculate the test response?s sim-
ilarity to each of the training responses in one score
category, and report the average score as its similar-
ity to this score point. We also tried using this av-
erage similarity score for the VSM method, but our
experimental results showed that this average score
obtained lower performance than using one big vec-
tor generated from all the training samples due to
data sparsity. After the similarity scores to each of
the four score categories are computed, the content
features introduced in Section 3.1 are then extracted
and are used to evaluate the speaking proficiency of
the speaker.
4 System Architecture
This section describes the architecture of our auto-
mated content scoring system, which is shown in
Figure 1. First, the test taker?s voice is recorded,
and sent to the automatic speech recognition system.
Second, the feature computation module takes the
output hypotheses from the speech recognizer and
generates the content features. The last component
considers all the scoring features, and produces the
final score for each spoken response.
Featu
re?
Comp
utatio
n
Reco
gnize
d?Wo
rds?
Scori
ng?
Comp
utatio
n?
Mod
ule
and?U
ttera
nces
Featu
res
Spee
ch
Scori
ngM
odel
Spee
ch?
Reco
gnize
r
Scori
ng?M
odel
Audio
Files
Spea
king?S
cores
Figure 1: Architecture of the automated content scoring
system.
While we are using human transcripts of spoken
responses as a baseline in this paper, we want to note
that in an operational system as depicted in this fig-
ure, the scoring features are computed and extracted
using the hypotheses from the ASR system, which
exhibits a relatively high word error rate. These
recognition errors will sequentially impact the pro-
cess of calculating the similarity and computing the
content scores, and decrease the performance of the
final speaking scores. In order to improve the system
performance in this ASR condition, we explore the
use of word confidence scores from the ASR system
during feature generation. In particular, the similar-
ity scores between the test response and each score
category are weighted using the recognition confi-
dence score of the response, so that the scores can
also contain information related to its acoustic accu-
racy. The confidence score for one response is the
average value of all the confidence scores for each
word contained in the response. In Section 5, we
will evaluate the performance of our proposed con-
tent features using both human transcripts and ASR
outputs, as well as the enhanced content features us-
106
ing ASR confidence scores.
5 Experimental Results
5.1 Data
The data we use for our experiments are from the
Test of English as a Foreign Language R? internet-
based test (TOEFL iBT) in which test takers respond
to several stimuli using spontaneous speech. This
data set contains 24 topics, of which 8 are opinion-
based tasks, and 16 are contextual-based tasks. The
opinion-based tasks ask the test takers to provide
information or opinions on familiar topics based
on their personal experience or background knowl-
edge. The purpose of these tasks is to measure the
speaking ability of examinees independent of their
ability to read or listen to English language. The
contextual-based tasks engage reading, listening and
speaking skills in combination to mimic the kinds
of communication expected of students in campus-
based situations and in academic courses. Test tak-
ers read and/or listen to some stimulus materials and
then respond to a question based on them. For each
of the tasks, after task stimulus materials and/or test
questions are delivered, the examinees are allowed a
short time to consider their response and then pro-
vide their responses in a spontaneous manner within
either 45 seconds (for the opinion-based tasks) or 60
seconds (for the contextual-based tasks).
For each topic, we randomly select 1800 re-
sponses for training, and 200 responses as develop-
ment set for parameter tuning. Our evaluation data
contains 1500 responses from the same English pro-
ficiency test, which contain the same 24 topics. All
of these data are scored on a 0-4 scale by expert hu-
man raters. In our automated scoring system, we use
a filtering model to identify responses which should
have a score of 0, such as responses with a technical
difficulty (e.g., equipment problems, high ambient
noise), responses containing uncooperative behavior
from the speakers (e.g., non-English speech, whis-
pered speech). So in this paper we only focused on
the responses with scores of 1-4. Statistics for this
data set are shown in Table 1. As the table shows,
the score distributions are similar across the train-
ing, development, and evaluation data sets.
5.2 Speech recognizer
We use an ASR system containing a cross-word
triphone acoustic model trained on approximately
800 hours of spoken responses from the same En-
glish proficiency test mentioned above and a lan-
guage model trained on the corresponding tran-
scripts, which contain a total of over 5 million
words. The Word Error Rate (WER) of this system
on the evaluation data set is 33%.
5.3 Evaluation metric
To measure the quality of the developed features, we
employ a widely used metric, the Pearson correla-
tion coefficient (r). In our experiments, we use the
value of the Pearson correlation between the feature
values and the human proficiency scores for each
spoken response.
5.4 Feature performance on transcripts
In Section 3.1, we introduced three features derived
from the similarity between the test responses and
the training responses for each score point. We first
build the training samples for each topic, and then
compare the test responses with their corresponding
models. Three similarity measures are used for cal-
culating the similarity scores, VSM, LSA, and the
PMI-based method. In order to avoid the impact of
recognition errors, we first evaluate these similarity
methods and content features using the human tran-
scripts. The Pearson correlation coefficients on the
evaluation data set for this experiment are shown in
Table 2. The parameters used during model build-
ing, such as the weights for each score category in
the feature Simcmb and the number of topics k in
LSA, are all tuned on the development set, and ap-
plied directly on the evaluation set.
The correlations show that even the simple vec-
tor space model can obtain a good correlation of
0.48 with the human rater scores. The feature
Simcmb performs the best across almost all the test
setups, since it combines the information from all
score categories. The PMI-based features outper-
form the other two similarity methods when evalu-
ated both on all responses or only on the contextual-
based topics. We also observe that the correlations
on contextual-based tasks are much higher than on
opinion-based tasks. The reason for this is that
107
Table 1: Summary statistics of training, development and evaluation data set.
Data sets Responses Speakers score avg score sd
Score distribution (percentage %)
1 2 3 4
Train 43200 8000 2.63 0.79 1750 (4.1) 15128 (35.0) 20828 (48.2) 4837 (11.2)
Dev 4800 3760 2.61 0.79 215 (4.5) 1719 (35.8) 2295 (47.8) 499 (10.4)
Eval 1500 250 2.57 0.81 95 (6.3) 549 (36.6) 685 (45.7) 152 (10.1)
Table 2: Pearson correlations of the content features using human transcripts.
VSM LSA PMI
Simmax Sim4 Simcmb Simmax Sim4 Simcmb Simmax Sim4 Simcmb
ALL 0.46 0.32 0.48 0.32 0.38 0.45 0.18 0.51 0.53
Contextual 0.50 0.51 0.58 0.36 0.55 0.57 0.21 0.57 0.62
Opinion 0.37 0.03 0.25 0.29 0.14 0.22 0.06 0.42 0.51
the contextual-based tasks are more constrained to
the materials provided with the test item, whereas
the opinion-based tasks are relatively open-ended.
Therefore, it is easier for the similarity measures to
track the content, the topics, or the vocabulary usage
of the contextual-based topics. Overall, the best cor-
relations are obtained using the feature combining
the similarity scores to each score category and the
PMI-based methods to calculate the similarity. Here,
the Pearson correlations are 0.53 for all responses,
and 0.62 for the contextual-based tasks only.
We also investigated whether additional perfor-
mance gains could be achieved by combining infor-
mation from the three different content features to
build a single overall content score, since the three
features may measure disparate aspects of the re-
sponse. The combination model we use is mul-
tiple regression, in which the score assigned to a
test response is estimated as a weighted linear com-
bination of a selected set of features. The fea-
tures are the similarity values to each score category
(Simi, i ? {1, 2, 3, 4}), calcuated using the three
similairty measures. In total we have 12 content
features. The regression model is also built on the
development set, and tested on the evaluation set.
The correlation for the final model is 0.60 on all
responses, which is significantly better than the in-
dividual models (0.48 for VSM, 0.45 for LSA, and
0.53 for PMI). Compared to results reported in pre-
vious work on similar speech scoring tasks but mea-
suring other aspects of speech, our correlation re-
sults are very competitive (Zechner and Xi, 2008;
Zechner et al, 2009).
5.5 Feature Performance on ASR output
The results shown in the previous section were ob-
tained using human transcripts of test responses, and
were reported in order to demonstrate the meaning-
fulness of the proposed features. However, in prac-
tical automated speech scoring systems, the only
available text is the output of the ASR system, which
may contain a large number of recognition errors.
Therefore, in this section we show the performance
of the content features extracted using ASR hy-
potheses. Note that we still use the human tran-
scripts of the training samples to train the models,
the parameter values and the regression weights;
however, we only use ASR output of the evaluation
data for testing the feature performance. These cor-
relations are shown in Table 3.
Compared to the results in Table 2, we find that
the VSM and LSA methods are very robust to recog-
nition errors, and we only observe slight correlation
decreases on these features. However, the decrease
for the PMI-based method is quite large. A possi-
ble reason for this is that this method is based on
word-to-word similarity computed on the training
data, so the mismatch between training and evalu-
ation set likely has a great impact on the computa-
tion of the similarity scores, since we train on human
transcripts, but test using ASR hypotheses. Likely
for the same reason, the regression model combining
all the features does not provide any further contri-
bution to the correlation result (0.44 when evaluated
108
Table 3: Pearson correlations of the content features using ASR output.
VSM LSA PMI
Simmax Sim4 Simcmb Simmax Sim4 Simcmb Simmax Sim4 Simcmb
ALL 0.43 0.34 0.48 0.30 0.37 0.43 0.11 0.24 0.42
Contextual 0.49 0.53 0.58 0.34 0.54 0.57 0.16 0.31 0.53
Opinion 0.30 0.05 0.07 0.25 0.12 0.15 0.05 0.17 0.27
Table 4: Pearson correlations of the content features using ASR output with confidence scores.
VSM LSA PMI
Simmax Sim4 Simcmb Simmax Sim4 Simcmb Simmax Sim4 Simcmb
ALL 0.43 0.36 0.48 0.30 0.40 0.45 0.11 0.39 0.51
Contextual 0.49 0.55 0.58 0.34 0.57 0.59 0.16 0.46 0.59
Opinion 0.30 0.24 0.25 0.25 0.18 0.20 0.05 0.32 0.40
on all responses).
In Section 4, we proposed using ASR confidence
scores during feature extraction to introduce acous-
tic level information and, thus, penalize responses
for which the ASR output is less likely to be correct.
Under this approach, all similarity scores are mul-
tiplied by the average word confidence score con-
tained in the test response. The performance of these
enhanced features is provided in Table 4. Compared
to the scores in Table 3, the enhanced features per-
form better than the basic features that do not take
the confidence scores into consideration. Using this
approach, we can improve the correlation scores for
most of the features, especially for the PMI-based
features. These features had lower correlations be-
cause of the recognition errors, but with the con-
fidence scores, they outperform the other features
when evaluated both on all responses or only on
contextual-based responses. Note that the correla-
tions for feature Simmax remains the same because
the same average confidence scores for each test re-
sponse is multiplied by the similarity scores to each
of the score points, so the score point obtaining the
highest similarity score is the same whether the con-
fidence scores are considered or not. The correlation
of the regression model also improves from 0.44 to
0.51 when the confidence scores are included. Over-
all, the best correlations for the individual similarity
features with the confidence scores are very close to
those obtained using human transcripts, as shown in
Tables 2 and 4: the difference is 0.53 vs. 0.51 for
all responses, and 0.62 vs. 0.59 for contextual-based
tasks only.
Because all models and parameter values are
trained on human transcripts, this experimental
setup might not be optimal for using ASR outputs.
For instance, the regression model does not outper-
form the results of individual features using ASR
outputs, although the confidence scores help im-
prove the overall correlation scores. We expect that
we can obtain better performance by using a regres-
sion model trained on ASR transcripts, which can
better model the impact of noisy data on the features.
In our future work, we will build sample responses
for each score category, tune the parameter values,
and train the regression model all on ASR hypothe-
ses. We hope this can solve the mismatch problem
during training and evaluation, and can provide us
even better correlation results.
6 Conclusion and Future Work
Most previous work on automated scoring of spon-
taneous speech used features mainly related to low-
level information, such as fluency, pronunciation,
prosody, as well as a few features measuring aspects
such as vocabulary diversity and grammatical accu-
racy. In this paper, we focused on extracting con-
tent features to measure the speech proficiency in
relatively higher-level aspect of spontaneous speech.
Three features were computed to measure the sim-
ilarity between a test response and a set of sam-
ple responses representing different levels of speak-
ing proficiency. The similarity was calculated using
different methods, including the lexical matching
109
method VSM, and two corpus-based semantic simi-
larity measures, LSA and PMI. Our experimental re-
sults showed that all the features obtained good cor-
relations with human proficiency scores if there are
no recognition errors in the text transcripts, with the
PMI-based method performing the best over three
similarity measures. However, if we used ASR tran-
scripts, we observed a marked performance drop for
the PMI-based method. Although we found that
VSM and LSA were very robust to ASR errors, the
overall correlations for the ASR condition were not
as good as using human transcripts. To solve this
problem, we proposed to use ASR confidence scores
to improve the feature performance, and achieved
similar results as when using human transcripts.
As we discussed in Section 5, all models were
trained using human transcripts, which might de-
crease the performance when these models are ap-
plied directly to the ASR outputs. In our future
work, we will compare models trained on human
transcripts and on ASR outputs, and investigate
whether we should use matching data for training
and evaluation, or whether we should not introduce
noise during training in order to maintain the validity
of the models. We will also investigate whether the
content features can provide additional information
for automated speech scoring, and help build better
scoring systems when they are combined with other
non-content features, such as the features represent-
ing fluency, pronunciation, prosody, vocabulary di-
versity information. We will also explore generating
other features measuring the higher-level aspects of
the spoken responses. For example, we can extract
features assessing the responses? relatedness to the
stimulus of an opinion-based task. For contextual-
based tasks, the test takers are asked to read or lis-
ten to some stimulus material, and answer a ques-
tion based on this information. We can build models
using these materials to check the correctness and
relatedness of the spoken responses.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater R? v.2. The Journal of Technology,
Learning, and Assessment, 4(3):3?30.
Jared Bernstein, John De Jong, David Pisoni, and Brent
Townshend. 2000. Two experiments on automatic
scoring of spoken language proficiency. In Proceed-
ings of Integrating Speech Tech. in Learning (InSTIL).
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010a. Fluency and structural complexity as predic-
tors of L2 oral proficiency. In Proceedings of Inter-
speech.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010b. Validating automated speaking tests. Lan-
guage Testing, 27(3):355?377.
Lei Chen and Klaus Zechner. 2011a. Applying rhythm
features to automatically assess non-native speech. In
Proceedings of Interspeech.
Miao Chen and Klaus Zechner. 2011b. Computing and
evaluating syntactic complexity features for automated
scoring of spontaneous non-native speech. In Pro-
ceedings of ACL-HLT.
Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven as-
sessment of non-native spontaneous speech. In Pro-
ceedings of NAACL-HLT.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997.
Automatic evaluation of Dutch pronunciation by using
speech recognition technology. In IEEE Workshop on
Auotmatic Speech Recognition and Understanding.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learners?
fluency by means of automatic speech recognition
technology. Journal of the Acoustical Society of Amer-
ica, 107:989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learners?
fluency: comparisons between read and spontaneous
speech. Journal of the Acoustical Society of America,
111(6):2862?2873.
Maxine Eskenazi, Abeer Alwan, and Helmer Strik. 2009.
Spoken language technology for education. Speech
Communication, 51(10):831?1038.
Peter W. Foltz, Darrell Laham, and Thomas K. Landauer.
1999. The Intelligent Essay Assessor: Applications to
educational technology. Interactive multimedia Elec-
tronic Journal of Computer-Enhanced Learning, 1(2).
Horacio Franco, Leonardo Neumeyer, Vassilios Di-
galakis, and Orith Ronen. 2000. Combination of ma-
chine scores for automatic grading of pronunciation
quality. Speech Communication, 30(1-2):121?130.
Thomas K Landauer, Peter W. Foltz, and Darrell Laham.
1998. Introduction to Latent Semantic Analysis. Dis-
course Processes, 25:259?284.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of
the American Association for Artificial Intelligence,
September.
110
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
ECML.
Silke M. Witt and Steve J. Young. 2000. Phone-
level pronunciation scoring and assessment for interac-
tive language learning. Speech Communication, 30(1-
2):95?108.
Klaus Zechner and Xiaoming Xi. 2008. Towards auto-
matic scoring of a test of spoken language with het-
erogeneous task types. In Proceedings of the Third
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications.
Klaus Zechner, Derrick Higgins, and Xiaoming Xi.
2007. SpeechraterTM: A construct-driven approach
to score spontaneous non-native speech. In Proceed-
ings of the 2007 Workshop of the International Speech
Communication Association (ISCA) Special Interest
Group on Speech and Language Technology in Edu-
cation (SLaTE).
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken En-
glish. Speech Communication, 51(10):883?895.
111
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 157?162,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Prompt-based Content Scoring for Automated Spoken Language Assessment
Keelan Evanini
Educational Testing Service
Princeton, NJ 08541, USA
kevanini@ets.org
Shasha Xie
Microsoft
Sunnyvale, CA 94089
shxie@microsoft.com
Klaus Zechner
Educational Testing Service
Princeton, NJ 08541, USA
kzechner@ets.org
Abstract
This paper investigates the use of prompt-
based content features for the automated as-
sessment of spontaneous speech in a spoken
language proficiency assessment. The results
show that single highest performing prompt-
based content feature measures the number
of unique lexical types that overlap with the
listening materials and are not contained in
either the reading materials or a sample re-
sponse, with a correlation of r = 0.450 with
holistic proficiency scores provided by hu-
mans. Furthermore, linear regression scor-
ing models that combine the proposed prompt-
based content features with additional spoken
language proficiency features are shown to
achieve competitive performance with scoring
models using content features based on pre-
scored responses.
1 Introduction
A spoken language proficiency assessment should
provide information about how well the non-native
speaker will be able to perform a wide range of tasks
in the target language. Therefore, in order to provide
a full evaluation of the non-native speaker?s speak-
ing proficiency, the assessment should include some
tasks eliciting unscripted, spontaneous speech. This
goal, however, is hard to achieve in the context of
a spoken language assessment which employs auto-
mated scoring, due to the difficulties in developing
accurate automatic speech recognition (ASR) tech-
nology for non-native speech and in extracting valid
and reliable features. Because of this, most spo-
ken language proficiency assessments which use au-
tomated scoring have focused on restricted speech,
and have included tasks such as reading a word / sen-
tence / paragraph out loud, answering single-word
factual questions, etc. (Chandel et al, 2007; Bern-
stein et al, 2010).
In order to address this need, some automated
spoken language assessment systems have also in-
cluded tasks which elicit spontaneous speech. How-
ever, these systems have focused primarily on a non-
native speaker?s pronunciation, prosody, and fluency
in their scoring models (Zechner et al, 2009), since
these types of features are relatively robust to ASR
errors. Some recent studies have investigated the
use of features related to a spoken response?s con-
tent, such as (Xie et al, 2012). However, the ap-
proach to content scoring taken in that study requires
a large amount of responses for each prompt to be
provided with human scores in order to train the
content models. This approach is not practical for a
large-scale, high-stakes assessment which regularly
introduces many new prompts into the assessment?
obtaining the required number of scored training re-
sponses for each prompt would be quite expensive
and could lead to potential security concerns for the
assessment. Therefore, it would be desirable to de-
velop an approach to content scoring which does not
require a large amount of actual responses to train
the models. In this paper, we propose such a method
which uses the stimulus materials for each prompt
contained in the assessment to evaluate the content
in a spoken response.
157
2 Related Work
There has been little prior work concerning auto-
mated content scoring for spontaneous spoken re-
sponses (a few recent studies include (Xie et al,
2012) and (Chen and Zechner, 2012)); however, sev-
eral approaches have been investigated for written
responses. A standard approach for extended writ-
ten responses (e.g., essays) is to compare the con-
tent in a given essay to the content in essays that
have been provided with scores by human raters us-
ing similarity methods such as Content Vector Anal-
ysis (Attali and Burstein, 2006) and Latent Semantic
Analysis (Foltz et al, 1999). This method thus re-
quires a relatively large set of pre-scored responses
for each test question in order to train the content
models. For shorter written responses (e.g., short an-
swer questions targeting factual content) approaches
have been developed that compare the similarity be-
tween the content in a given response and a model
correct answer, and thus do not necessarily require
the collection of pre-scored responses. These ap-
proaches range from fully unsupervised text-to-text
similarity measures (Mohler and Mihalcea, 2009) to
systems that incorporate hand-crafted patterns iden-
tifying specific key concepts (Sukkarieh et al, 2004;
Mitchell et al, 2002).
For extended written responses, it is less practical
to make comparisons with model responses, due to
the greater length and variability of the responses.
However, another approach that does not require
pre-scored responses is possible for test questions
that have prompts with substantial amounts of in-
formation that should be included in the answer. In
these cases, the similarity between the response and
the prompt materials can be calculated, with the hy-
pothesis that higher scoring responses will incorpo-
rate certain prompt materials more than lower scor-
ing responses. This approach was taken by (Gure-
vich and Deane, 2007) which demonstrated that
lower proficiency non-native essay writers tend to
use more content from the reading passage, which is
visually accessible and thus easier to comprehend,
than the listening passage. The current study inves-
tigates a similar approach for spoken responses.
3 Data
The data used in this study was drawn from TOEFL
iBT, an international assessment of academic En-
glish proficiency for non-native speakers. For this
study, we focus on a task from the assessment which
elicits a 60 second spoken response from the test
takers. In their response, the test takers are asked
to use information provided in reading and listen-
ing stimulus materials to answer a question concern-
ing specific details in the materials. The responses
are then scored by expert human raters on a 4-point
scale using a scoring rubric that takes into account
the following three aspects of spoken English pro-
ficiency: delivery (e.g., pronunciation, prosody, flu-
ency), language use (e.g., grammar, lexical choice),
and topic development (e.g., content, discourse co-
herence). For this study, we used a total of 1189
responses provided by 299 unique speakers to four
different prompts1 (794 responses from 199 speak-
ers were used for training and 395 responses from
100 speakers were used for evaluation).
4 Methodology
We investigated several variations of simple features
that compare the lexical content of a spoken re-
sponse to following three types of prompt materials:
1) listening passage: a recorded lecture or dialogue
containing information relevant to the test question
(the number of words contained in each of the four
listening passages used in this study were 213, 223,
234, and 318), 2) reading passage: an article or es-
say containing additional information relevant to the
test question (the number of words contained in the
two reading passages were 94 and 111), and 3) sam-
ple response: a sample response provided by the test
designers containing the main ideas expected in a
model answer (the number of words contained in the
four sample responses were 41, 74, 102, and 133).
The following types of features were investi-
gated for each of the materials: 1) stimulus cosine:
the cosine similarity between the spoken response
and the various materials, 2) tokens/response,
types/response: the number of word tokens / types
that occur in both the spoken response and each of
1Two out of the four tasks in this study had only listening
materials; responses to these tasks are not included in the results
for the features which require reading materials.
158
the materials, divided by the number of word to-
kens / types in the response,2 and 3) unique tokens,
unique types: the number of word tokens / types that
occur in both the spoken response and one or two
of the materials, but do not occur in the remaining
material(s).
As a baseline, we also compare the proposed
content features based on the prompt materials to
content features based on collections of scored re-
sponses to the same prompts. This type of feature
has been shown to be effective for content scoring
both in non-native essays (Attali and Burstein, 2006)
and spoken responses (Xie et al, 2012), and is com-
puted by comparing the content in a test response to
content models trained using responses from each of
the score points. It is defined as follows:
? Simi: the similarity score between the words
in the spoken response and a content model
trained from responses receiving score i (i ?
1, 2, 3, 4 in this study)
The Simi features were trained on a corpus of
7820 scored responses (1955 for each of the four
prompts), and we investigated two different meth-
ods for computing the similarity between the test
responses and the content models: Content Vector
Analysis using the cosine similarity metric (CVA)
and Pointwise Mutual Information (PMI).
The spoken responses were processed using an
HMM-based triphone ASR system trained on 800
hours of non-native speech (approximately 15% of
the training data consisted of responses to the four
test questions in this study), and the ASR hypothe-
ses were used to compute the content features.3
5 Results
We first examine the performance of each of the
individual features by calculating their correlations
with the holistic English speaking proficiency scores
provided by expert human raters. These results for
2Dividing the number of matching word tokens / types by
the number of word tokens in the response factors out the over-
all length of the response from the calculation of the feature.
3Transcriptions were not available for the spoken responses
used in this study, so the exact WER of the ASR system is un-
known. However, the WER of the ASR system on a comparable
set of spoken responses is 28%.
the training partition are presented in Table 1.4
Feature Set Feature r
stimulus cosine
listening 0.384
reading 0.176
sample 0.384
tokens/response
listening 0.022
reading 0.096
sample 0.121
types/response
listening 0.426
reading 0.142
sample 0.128
unique tokens
L?RS 0.116
L?RS? 0.162
LR?S 0.219
LR?S? 0.337
unique types
L?RS 0.140
L?RS? 0.166
LR?S 0.259
LR?S? 0.450
CVA
Sim1 0.091
Sim2 0.186
Sim3 0.261
Sim4 0.311
PMI
Sim1 0.191
Sim2 0.261
Sim3 0.320
Sim4 0.361
Table 1: Correlations of individual content features with
holistic human scores on the training partition
As Table 1 shows, some of the individual content
features based on the prompt materials obtain higher
correlations with human scores than the baseline
CVA and PMI features based on scored responses.
Next, we investigated the overall contribution of the
content features to a scoring model that takes into
account features from various aspects of speaking
proficiency. To show this, we built a baseline lin-
ear regression model to predict the human scores us-
ing 9 features from 4 different aspects of speaking
4For the unique tokens and unique types features, each row
lists how the prompt materials were used in the similarity com-
parison as follows: R = reading, L = listening, S = sample,
and ? indicates no lexical overlap between the spoken response
and the material. For example, L?RS indicates content from the
test response that overlapped with both the reading passage and
sample response but was not contained in the listening material.
159
proficiency (fluency, pronunciation, prosody, and
grammar) produced by SpeechRater, an automated
speech scoring system (Zechner et al, 2009), as
shown in Table 2.
Category Features
Fluency normalized number of silences
> 0.15 sec, normalized number
of silences > 0.495 sec, average
chunk length, speaking rate, nor-
malized number of disfluencies
Pronunciation normalzied Acoustic Model
score from forced alignment
using a native speaker AM,
average normalized phone du-
ration differnce compared to a
reference corpus
Prosody mean deviation of distance be-
tween stressed syllables
Grammar Language Model score
Table 2: Baseline speaking proficiency features used in
the scoring model
In order to investigate the contribution of the vari-
ous types of content features to the scoring model,
linear regression models were built by adding the
features from each of the feature sets in Table 1 to
the baseline features. The models were trained using
the 794 responses in the training set and evaluated
on the 395 responses in the evaluation set. Table 3
presents the resulting correlations both for the indi-
vidual responses (N=395) as well as the sum of all
four responses from each speaker (N=97).5
As Table 3 shows, all of the scoring models us-
ing feature sets with the proposed content features
based on the prompt materials outperform the base-
line model. While none of the models incorporat-
ing features from a single feature set outperforms
the baseline CVA model using features based on
scored responses, a model incorporating all of the
proposed prompt-based content features, all prompt-
based, does outperform this baseline. Furthermore,
a model incorporating all of the content features
(both the proposed features and the baseline CVA /
PMI features), all content, outperforms a model us-
5Three speakers were removed from the evaluation set for
this analysis since they provided fewer than four responses.
Feature Set response r speaker r
Baseline 0.607 0.687
+ types/response 0.612 0.701
+ tokens/response 0.615 0.700
+ unique tokens 0.616 0.695
+ stimulus cosine 0.630 0.716
+ unique types 0.658 0.761
+ CVA 0.665 0.762
+ all prompt-based 0.677 0.779
+ PMI 0.723 0.818
+ CVA and PMI 0.723 0.818
+ all content 0.742 0.838
Table 3: Performance of scoring models with the addition
of content features
ing only the baseline CVA and PMI features.6
6 Discussion and Conclusion
This paper has demonstrated that the use of content
scoring features based solely on the prompt stimu-
lus materials and a sample response is a viable al-
ternative to using features based on content mod-
els trained on large sets of pre-scored responses for
the automated assessment of spoken language profi-
ciency. Under this approach, automated scoring sys-
tems for large-scale spoken language assessments
involving spontaneous speech can begin to address
an area of spoken language proficiency (content ap-
propriateness) which has mostly been neglected in
systems that have been developed to date. Com-
pared to an approach using pre-scored responses for
training the content models, the proposed approach
is much more cost effective and reduces the risk
that test materials will be seen by test takers prior
to the assessment; both of these attributes are cru-
cial benefits for large-scale, high-stakes language as-
sessments. Furthermore, the proposed prompt-based
content features, when combined in a linear regres-
sion model with other speaking proficiency features,
outperform a baseline set of CVA content features
which use models trained on pre-scored responses,
6While the prompt-based content features do result in im-
provements, neither of these two differences are statistically sig-
nificant at ? = 0.05 using the Hotelling-Williams Test, since
both the magnitude of the increase and the size of the data set
are relatively small.
160
and they add further improvement to a model incor-
porating the higher performing baseline with PMI
content features.
The results in Table 1 indicate that the indi-
vidual features based on overlapping lexical types
(types/response and unique types) perform slightly
better than the ones based on overlapping lexical to-
kens (tokens/response and unique tokens). This sug-
gests that it is important for test takers to use a range
of concepts that are contained in the stimulus mate-
rials in their responses. Similarly to the result from
(Gurevich and Deane, 2007), Table 1 also shows that
the features measuring overlap between the response
and the listening materials typically perform better
than the features measuring overlap between the re-
sponse and the reading materials; the best individ-
ual feature, LR?S? for unique types, measures the
amount of overlap with lexical types that are con-
tained in the listening stimulus, but absent from the
reading stimulus and sample response. This indi-
cates that the use of content from the listening ma-
terials is a better differentiator among students of
differing language proficiency levels than reading
materials, likely because test takers generally have
more difficulty understanding the content from lis-
tening materials.
Table 1 also shows the somewhat counterintu-
itive result that features based on no lexical over-
lap with the sample response produce higher corre-
lations than features based on lexical overlap with
the sample response, when there is lexical overlap
with the listening materials and no overlap with the
reading materials. That is, the LR?S? feature out-
performs the LR?S feature for both the unique types
and unique tokens features sets. However, as shown
in Section 4, the sample responses varied widely
in length (ranging from 41 to 133 words), and all
were substantially shorter than the listening materi-
als, which ranged from 213 to 318 words. Therefore,
it is likely that many of the important lexical items
from the sample response are also contained in the
listening materials. Thus, the LR?S feature provided
less information than the LR?S? feature.
The features used in this study are all based on
simple lexical overlap statistics, and are thus triv-
ial to implement. Future research will investigate
more sophisticated methods of text-to-text similar-
ity for prompt-based content scoring, such as those
used in (Mohler and Mihalcea, 2009). Furthermore,
future research will address the validity of the pro-
posed features by ensuring that there are ways to fil-
ter out responses that are too similar to the stimulus
materials, and thus indicate that the test taker simply
repeated the source verbatim.
7 Acknowledgments
The authors would like to thank Yigal Attali for shar-
ing his ideas about prompt-based content scoring.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater R? V.2. The Journal of Technol-
ogy, Learning, and Assessment, 4(3):3?30.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010. Validating automated speaking tests. Language
Testing, 27(3):355?377.
Abhishek Chandel, Abhinav Parate, Maymon Madathin-
gal, Himanshu Pant, Nitendra Rajput, Shajith Ikbal,
Om Deshmuck, and Ashish Verma. 2007. Sensei:
Spoken language assessment for call center agents. In
Proceedings of ASRU.
Miao Chen and Klaus Zechner. 2012. Using an ontol-
ogy for improved automated content scoring of spon-
taneous non-native speech. In Proceedings of the
7th Workshop on Innovative Use of NLP for Build-
ing Educational Applications, NAACL-HLT, Montre?al,
Canada. Association for Computational Linguistics.
Peter W. Foltz, Darrell Laham, and Thomas K. Landauer.
1999. The intelligent essay assessor: Applications to
educational technology. Interactive Multimedia Elec-
tronic Journal of Computer-Enhanced Learning, 1(2).
Olga Gurevich and Paul Deane. 2007. Document
similarity measures to distinguish native vs. non-
native essay writers. In Proceedings of NAACL HLT,
Rochester, NY.
Tom Mitchell, Terry Russell, Peter Broomhead, and
Nicola Aldridge. 2002. Towards robust computerised
marking of free-text responses. In Proceedings of
the 6th International Computer Assisted Assessment
(CAA) Conference, Loughborough.
Michael Mohler and Rada Mihalcea. 2009. Text-to-
text semantic similarity for automatic short answer
grading. In Proceedings of the European Chapter of
the Association for Computational Linguistics (EACL
2009), Athens, Greece.
Jana Sukkarieh, Stephen Pulman, and Nicholas Raikes.
2004. Auto-marking 2: An update on the UCLES-
Oxford University research into using computational
linguistics to score short, free text responses. In
161
International Association of Educational Assessment,
Philadelphia.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech scor-
ing. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 103?111, Montre?al, Canada. Association
for Computational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken En-
glish. Speech Communication, 51(10):883?895.
162
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 288?292,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Evaluating Unsupervised Language Model Adaptation Methods for
Speaking Assessment
Shasha Xie
Microsoft
1020 Enterprise Way
Sunnyvale, CA 94089
shxie@microsoft.com
Lei Chen
Educational Testing Service
600 Rosedale Rd
Princeton, NJ
LChen@ets.org
Abstract
In automated speech assessment, adaptation of
language models (LMs) to test questions is im-
portant to achieve high recognition accuracy
However, for large-scale language tests, the
ordinary supervised training, which uses an
expensive and time-consuming manual tran-
scription process, is hard to utilize for LM
adaptation. In this paper, several LM adap-
tation methods that require either no manual
transcription process or just a small amount of
transcriptions have been evaluated. Our ex-
periments suggest that these LM adaptation
methods can allow us to obtain considerable
recognition accuracy gain with no or low hu-
man transcription cost.
Index Terms: language model adaptation, unsuper-
vised training, Web as a corpus
1 Introduction
Automated speech assessment, a fast-growing area
in the speech research field (Eskenazi, 2009), typ-
ically uses an automatic speech recognition (ASR)
system to recognize spontaneous speech responses
and use the recognition outputs to generate the fea-
tures for scoring. Since the recognition accuracy di-
rectly influences the quality of the speech features,
especially for the features related to word entities,
such as those measuring grammar accuracy and vo-
cabulary richness, it is important to use ASR sys-
tems with high recognition accuracy.
Adaptation of language models (LMs) to test re-
sponses is an effective method to improve recogni-
tion accuracy. However, it is difficult to only use
the ordinary supervised training to adapt LMs to test
questions. First, for high-stake tests administered
globally, a very large pool of test questions have to
be used to strengthen the tests? security and validity.
Since a large number of test questions have many
possible answers for each question, a large set of au-
dio files needs to be transcribed to cover response
content. Second, due to time and cost constraints,
it may not be practical to have a pre-test to collect
enough speech responses for adaptation purposes.
Therefore, it is important to pursue other methods to
obtain LM adaptation data in a faster and lower-cost
way than the ordinary supervised training.
As we will review in Section 2, some promising
technologies, such as unsupervised training, active
learning, and LM adaptation based on Web data,
have been utilized in broadcast news recognition, di-
alog system, and so on. In this paper on the LM
adaptation task used in automated speech scoring
systems, we will report our experiments to obtain
LM adaptation data in a faster and more economical
way that requires little human involvement. To our
knowledge, this is the first such work reported in the
automated speech assessment area.
The rest of the paper is organized as follows: Sec-
tion 2 reviews the related previous research results;
Section 3 describes the English test, the data used
in our experiments, and the ASR system used; Sec-
tion 4 reports the experiments of different methods
we tried to obtain LM adaptation data; Section 5 dis-
cusses our findings and plans for future research.
288
2 Previous Work
Unsupervised training is the method of using untran-
scribed audio to adapt a language model (LM). An
initial ASR model (seed model) is used to recognize
the untranscribed audio, and the obtained ASR out-
puts are used in the follow-up LM adaptation. (Chen
et al, 2003) utilized unsupervised LM adaptation
on broadcast news (BN) recognition. The unsuper-
vised adaptation method reduces the word error rate
(WER) by 2% relative to using the baseline LM.
(Bacchiani and Roark, 2003) reported that unsuper-
vised LM adaptation provided an absolute error rate
reduction of 3.9% over the un-adapted baseline per-
formance by using 17 hours of untranscribed adap-
tation data. This was 51% of the 7.7% adaptation
error rate reduction obtained by using an ordinary
supervised adaptation method.
Active learning is used to reduce the number of
training examples to be annotated by automatically
processing the unlabeled examples and then select-
ing the most informative ones with respect to a given
cost function. (Riccardi and Hakkani-Tur, 2003;
Tur et al, 2005) proposed using a combination of
unsupervised and active learning for ASR training
to minimize the workload of human transcription.
Their experiments showed that the amount of la-
beled data needed for a given recognition accuracy
can be reduced by 75% when combining these two
training approaches.
A recent trend in Natural Language Processing
(NLP) and speech recognition research is utilizing
Web data to improve the LMs, especially when in-
domain training material is limited. (Ng et al,
2005) investigated LM topic adaptation using Web
data. Experiments in recognizing Mandarin tele-
phone conversations showed that use of filtered Web
data leads to a 7% reduction in the character recog-
nition error rate. (Sarikaya et al, 2005) used Web
data to adapt LMs used in a spoken dialog system.
From a limited in-domain data set, they generated
a series of search queries and retrieved Web pages
from Google using these queries. In their recog-
nition experiment done on a dialog system, they
achieved a 5.2% word error reduction by using the
Web data, compared to a baseline LM trained on
1700 in-domain utterances.
3 Test, Data, and ASR
Our in-domain data was from The Test of English
for International Communication, TOEIC R?, which
tests non-native English speakers? basic speaking
ability required in international business communi-
cations. In our experiments, we focused on opinion
testing questions. An example question is: ?Do you
agree with the statement that a company should only
hire experienced employees? Use specific reasons to
support your answer?.
A state-of-the-art HMM LVCSR system, which
was provided by a leading ASR vendor, was used in
our experiments. It contains a cross-word tri-phone
acoustic model (AM) and a combination of bi-gram,
tri-gram, and up to four-gram LMs. The AM and
LM are trained by supervised training from about
800 hours of audio and manual transcriptions of
non-native English speaking data collected from the
Test Of English as a Foreign Language (TOEFL R?).
TOEFL R? is targeted to assess test-takers? ability
to use English to study in an institution using En-
glish as its primary teaching language. Speaking
content from TOEFL R? data is quite different from
the content shown in TOEIC R? data. When testing
this recognizer on a held-out evaluation set extracted
from the TOEFL R? test, a word error rate (WER) of
33.0% 1 is observed. This recognizer was used as
the seed recognizer in our experiments.
4 Experiments
We collected a set of audio responses from the
TOEIC R? test, focusing on opinion questions. This
data set was randomly selected from different first-
language (L1) and English speaking proficiency lev-
els. Then, these audio files were manually tran-
scribed. In our experiments, 1470 responses were
used for LM adaptation and the remaining 184 re-
sponses were used to evaluate speech recognition
1ASR on non-native speech is more difficult than on native
speech for various reasons (Livescu and Glass, 2000). How-
ever, a high WER does not rule out the possibility of using
ASR outputs for automated scoring, especially when relying
on delivery related features. For example, (Chen et al, 2009)
shows that several pronunciation features? contributions for as-
sessment, measured as Pearson correlations between the feat-
uers and human scores, only drop about 10% to 20% when us-
ing ASR outputs with a WER as high as 50% compared to using
human transcriptions.
289
accuracy. When using the seed recognizer with-
out any adaptation, the WER on the evaluation set
is 42.8%, which is much higher than the accuracy
achieved on the TOEFL R? data (33.0%). Using the
ordinary supervised training, adapting LMs using
these 1470 manual transcriptions, the WER is re-
duced to 34.7%, close to the performance on the
in-domain TOEFL R? data. Note that a fixed dictio-
nary with a vocabulary size of about 20, 000 words,
which in general is much larger than the vocabulary
mastered by non-native test takers, was used in our
experiment.
4.1 Unsupervised LM adaptation
Using the seed recognizer trained on the TOEFL R?
data, we recognized 1470 adaptation responses and
selected varying amounts of ASR outputs for LM
adaptation. From ASR outputs of all responses, we
selected the responses with high confidence scores
estimated by the seed recognizer so that we could
use the ASR outputs with higher recognition accu-
racy on the LM adaptation task. We used two meth-
ods to measure the confidence score for each re-
sponse from word-level confidence scores. First, we
took the average of all word confidence scores a re-
sponse contains, as shown in Equation 1.
ConfperWord =
1
N
N?
i=1
conf(wi) (1)
where conf(wi) is the confidence score of word, wi.
The other method we used considers each word?s du-
ration, as shown in Equation 2.
ConfperSec =
?N
i=1 d(wi) ? conf(wi)
?N
i=1 d(wi)
(2)
where d(wi) is the duration of wi.
In Figure 1, we showed the WER after running
unsupervised LM adaptation, where the adaptation
responses were selected if they had high word-based
(ConfperWord) or duration-based (ConfperSec)
confidence scores. The data sizes used for adapta-
tion vary from 0% (without any adaptation) to 100%
(using all adaptation data). We observe continuous
reduction of WER when using more and more adap-
tation data. Selecting responses by the word-based
confidence scores performs a little better than the se-
lection method based on the confidence scores nor-
malized by corresponding word durations. However,
there is no significant difference between these two
selection criteria.
Figure 1: Unsupervised LM adaptation performance us-
ing different sizes of development set data.
ASR accuracy may vary within each response.
Therefore, instead of using entire responses, we also
explored using smaller units for LM adaptation. All
of the ASR outputs were split into word sequences
with fixed lengths (10-15 words), and the ones with
higher per-word confidence scores (ConfperWord)
were extracted for model adaptation. Our experi-
ment shows that using word-sequence pieces rather
than entire responses leads to a faster WER reduc-
tion. When only using 5% of the adaptation data, we
obtained 3.5% absolute WER reduction compared to
the baseline result without adaptation. Note that we
only obtained 2.5% absolute WER reduction when
using entire responses in adaptation.
4.2 Web data LM adaptation
Given around 40% WER when using our seed ASR,
unsupervised learning faces the issue that many
recognition errors were included in model adapta-
tion. Can we find another source to obtain LM
adaptation inputs with fewer errors? To address
this question, we explored building a training cor-
pus from Web data based on test questions. We
used BootCat (Baroni and Bernardini, 2004), a cor-
pus building tool designed to collect data from the
Web, to collect our LM adaptation data. Based on
test prompts in the TOEIC R? test, we manually gen-
erated search queries. After receiving the search
queries, the BootCat tool searched the Web using
the Microsoft Bing search engine. Then, top-ranked
290
Web pages were downloaded and texts on these Web
pages were extracted. We examined the Web search
results (including URLs and texts) returned by the
BootCat tool. The returned Web data has varied
matching rates among these prompts and are gen-
erally noisy.
By using only the default setup provided by the
BootCat tool, we collected 5312 sentences in total.
After a simple text normalization, we used the ob-
tained Web data for LM adaptation, and the WER
on the evaluation data was 38.5%. This WER result
is a little higher than the WER result achieved by
unsupervised LM adaptation (38.1%). Without tran-
scribing any response from test-takers, the language
model adaptation using Web data already helps to
improve recognition accuracy. Then, we tried us-
ing both the Web data and the ASR hypotheses for
adaptation, and we can further decreased the WER
to 37.6%. This is lower than using the two LM adap-
tation data sets separately.
4.3 Semi-supervised approaches for LM
adaptation
For semi-supervised LM adaptation, we replaced the
speech responses of lower confidence scores with
their corresponding human transcripts. We hoped
that by using the responses with high confidence
scores together with a small amount of human tran-
scripts, we could get better performance by intro-
ducing less noise during adaptation. We set differ-
ent thresholds for selecting the low confidence re-
sponses and replacing them with human transcripts.
We find that just manually transcribing a limited
amount of audio data gives us further WER reduc-
tion, compared to using unsupervised learning. Af-
ter transcribing just 100 responses, 6.8% of 1470 re-
sponses in the adaptation data set, semi-supervised
learning can achieve 61.73% of the WER reduction
(8.1%) obtained by using the ordinary supervised
training that requires transcription of all 1470 re-
sponses.
4.4 Discussion
In Table 1, we compared the performance of all the
adaptation methods mentioned in this paper, includ-
ing two unsupervised methods adapted using the
ASR hypotheses and ?related? Web data, and one
semi-supervised method 2, replacing the ASR hy-
potheses of lower confidence scores with their corre-
sponding human transcripts. For a convenient com-
parison, we also include the baseline (without LM
adaptation) and the result of using the supervised
adaptation. All the proposed unsupervised/semi-
supervised methods can significantly improve the
ASR performance compared to the baseline result.
For projects with time limits, we can use these
unsupervised/semi-supervised methods to help us
get relatively good ASR outputs.
Table 1: The WER on the evaluation set using different
LM adaptation methods.
baseline
unsupervised
semi super.
ASR Web ASR&Web
42.8 38.1 38.5 37.6 37.8 34.7
5 Conclusions and Future Work
In this paper, we reported our experiments in ap-
plying several LM adaptation methods to automated
speech scoring systems that require few, if any, hu-
man transcripts, which are expensive and slow to
obtain for large-sized adaptation data sets. The un-
supervised training (using ASR transcriptions from
a seed ASR system) clearly shows higher accuracy
than a ASR system without any domain adaptation.
We also used test questions to collect related texts
from Web. Even though such Web data may be noisy
and its relatedness to real test responses is not al-
ways guaranteed, text data collected from the Web
is helpful to adapt LMs to better fit the responses to
test questions. To better cope with recognition er-
rors brought on by using the unsupervised training
method, we proposed using human transcriptions on
a small amount of poorly recognized responses. Us-
ing such little human involvement further helps to
obtain a lower WER. Therefore, based on the ex-
periments described in this paper, we conclude that
these novel LM adaptation methods provide promis-
ing solutions to let us skip the ordinary supervised
training for LM adaptation tasks frequently used in
automated speech scoring.
2The semi-supervised result was from replacing 100 low-
confidence responses with human transcripts.
291
The reported experiments in this paper were con-
ducted on a limited-size data set. We plan to increase
the testing data to a larger size and hope to cover
more types of test questions and spoken tests. In ad-
dition, we plan to investigate how to automatically
generate Web search queries based on test questions.
References
M. Bacchiani and B. Roark. 2003. Unsupervised lan-
guage model adaptation. In 2003 IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing, 2003. Proceedings.(ICASSP?03).
M. Baroni and S. Bernardini. 2004. BootCaT: bootstrap-
ping corpora and terms from the web. In Proceedings
of LREC, volume 2004, page 13131316.
L. Chen, J. L Gauvain, L. Lamel, and G. Adda. 2003.
Unsupervised language model adaptation for broad-
cast news. In 2003 IEEE International Conference on
Acoustics, Speech, and Signal Processing, 2003. Pro-
ceedings.(ICASSP?03).
L. Chen, K. Zechner, and X Xi. 2009. Improved pro-
nunciation features for construct-driven assessment of
non-native spontaneous speech. In NAACL-HLT.
M. Eskenazi. 2009. An overview of spoken language
technology for education. Speech Communication,
51(10):832?844.
K. Livescu and J. Glass. 2000. Lexical modeling
of non-native speech for automatic speech recogni-
tion. In Acoustics, Speech, and Signal Processing,
2000. ICASSP?00. Proceedings. 2000 IEEE Interna-
tional Conference on, volume 3, pages 1683?1686.
T. Ng, M. Ostendorf, M. Y Hwang, M. Siu, I. Bulyko, and
X. Lei. 2005. Web-data augmented language mod-
els for mandarin conversational speech recognition. In
Proc. ICASSP, volume 1.
G. Riccardi and D. Z Hakkani-Tur. 2003. Active and un-
supervised learning for automatic speech recognition.
In Proc. 8th European Conference on Speech Commu-
nication and Technology.
R. Sarikaya, A. Gravano, and Y. Gao. 2005. Rapid lan-
guage model development using external resources for
new spoken dialog domains. In Proc. ICASSP, vol-
ume 1, pages 573?576.
G. Tur, D. Hakkani-Tur, and R. E Schapire. 2005. Com-
bining active and semi-supervised learning for spo-
ken language understanding. Speech Communication,
45(2):171?186.
292
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 116?123,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Similarity-Based Non-Scorable Response Detection for Automated Speech
Scoring
Su-Youn Yoon
Educational Testing Service
Princeton, NJ, USA
syoon@ets.org
Shasha Xie
Microsoft
Sunnyvale, CA, USA
shxie@microsoft.com
Abstract
This study provides a method that iden-
tifies problematic responses which make
automated speech scoring difficult. When
automated scoring is used in the context
of a high stakes language proficiency as-
sessment, for which the scores are used to
make consequential decisions, some test
takers may have an incentive to try to game
the system in order to artificially inflate
their scores. Since many automated pro-
ficiency scoring systems use fluency fea-
tures such as speaking rate as one of the
important features, students may engage
in strategies designed to manipulate their
speaking rate as measured by the system.
In order to address this issue, we de-
veloped a method which filters out non-
scorable responses based on text similar-
ity measures. Given a test response, the
method generated a set of features which
calculated the topic similarity with the
prompt question or the sample responses
including relevant content. Next, an au-
tomated filter which identified these prob-
lematic responses was implemented us-
ing the similarity features. This filter im-
proved the performance of the baseline
filter in identifying responses with topic
problems.
1 Introduction
In spoken language proficiency assessment, some
responses may include sub-optimal characteristics
which make it difficult for the automated scor-
ing system to provide a valid score. For instance,
some test takers may try to game the system by
speaking in their native languages or by citing
memorized responses for unrelated topics. Oth-
ers may repeat questions or part of questions with
modifications instead of generating his/her own
response. Hereafter, we call these problematic
responses non-scorable (NS) responses. By us-
ing these strategies, test takers can generate flu-
ent speech, and the automated proficiency scoring
system, which utilizes fluency as one of the im-
portant factors, may assign a high score. In or-
der to address this issue, the automated proficiency
scoring system in this study used a two-step ap-
proach: these problematic responses were filtered
out by a ?filtering model,? and only the remaining
responses were scored using the automated scor-
ing model. By filtering out these responses, the
robustness of the automated scoring system can be
improved.
The proportion of NS responses, in the assess-
ment of which the responses are scored by human
raters, are likely to be low. For instance, the pro-
portion of NS responses in the international En-
glish language assessment used in this study was
2%. Despite this low proportion, it is a serious
problem which has a strong impact on the validity
of the test. In addition, the likelihood of students
engaging in gaming strategies may increase with
the use of automated scoring. Therefore, an au-
tomated filtering model with a high accuracy is a
necessary step to use the automated scoring sys-
tem as a sole rater.
Both off-topic and copy responses have topic-
related problems, although they are at the two ex-
tremes in the degree of similarity. Focusing on
the intermediate levels of similarity, Metzler et al.
(2005) presented a hierarchy of five similarity lev-
els: unrelated, on the general topic, on the spe-
cific topic, same facts, and copied. In the auto-
mated scoring of spontaneous speech, responses
that fell into unrelated can be considered as off-
topic, while the ones that fell into copied can be
considered as repetition or plagiarism. Follow-
ing this approach, we developed a non-scorable
response identification method utilizing similar-
116
Figure 1: A diagram of the overall architecture of
our method.
ity measures. We will show that this similarity
based method is highly efficient in identifying off-
topic or repetition responses. Furthermore, we
will show that the method can effectively detect
NS responses that are not directly related to the
topicality issue (e.g, non-English responses).
Figure 1 shows the overall architecture of our
method including the automated speech profi-
ciency scoring system. For a given spoken re-
sponse, the system performs speech processing in-
cluding speech recognition and generates a word
hypotheses and time stamps. In addition, the sys-
tem computes pitch and power; the system calcu-
lates descriptive statistics such as the mean and
standard deviation of pitch and power at both the
word level and response level. Given the word hy-
potheses and descriptive features of pitch/power,
it derives features for automated proficiency scor-
ing. In addition, the similarity features are gener-
ated based on the word hypotheses and topic mod-
els. Finally, given both sets of features, the filter-
ing model filters out non-scorable responses, and
the remainder of the responses are scored using a
scoring model. A detailed description of the sys-
tem is available from Zechner et al. (2009). In this
study, we will only focus on the filtering model.
This paper will proceed as follows: we first re-
view previous studies in section 2, then describe
the data in section 3, and present the method and
experiment set-up in sections 4 and 5. The results
and discussion are presented in section 6, and the
conclusions are presented in section 7.
2 Related Work
Filtering of NS responses for automated speech
scoring has been rarely recognized. Only a few
pieces of research have focused on this task,
and most studies have targeted highly restricted
speech. van Doremalen et al. (2009) and Lo et
al. (2010) used normalized confidence scores of
a speech recognizer in recasting speech. They
identified non-scorable responses with promising
performances (equal error rates ranged from 10
to 20%). Cheng and Shen (2011) extended these
studies and combined an acoustic model score, a
language model score, and a garbage model score
with confidence scores. They applied this new fil-
ter to less constrained items (e.g., picture descrip-
tion) and identified off-topic responses with an ac-
curacy rate of 90%with a false positive rate of 5%.
Although normalized confidence scores
achieved promising performances in restricted
speech, they may not be appropriate for the items
that elicit unconstrained spontaneous speech.
Low confidence scores signal the use of words
or phrases not covered by the language model
(LM) and this is strongly associated with off-topic
responses in restricted speech in which the target
sentence is given. However, in spontaneous
speech, this is not trivial; it may be associated
with not only off-topic speech but also mismatch
between the LM and speech input due to the low
coverage of the LM. Due to the latter case, the
decision based on the confidence score may not
be effective in measuring topic similarity.
The topic similarity between two documents
has been frequently modeled by relative-frequency
measures (Hoad and Zobel, 2003; Shivakumar and
Garcia-Molina, 1995), document fingerprinting
(Brin et al., 1995; Shivakumar and Garcia-Molina,
1995; Shivakumar and Garcia-Molina, 1996)), and
query based information retrieval methods using
vector space models or language model (Sander-
son, 1997; Hoad and Zobel, 2003).
Document similarity measures have been ap-
plied in automated scoring. Foltz et al. (1999)
evaluated the content of written essays using latent
semantic analysis (LSA) by comparing the test es-
says with essays of known quality in regard to their
degree of conceptual relevance and the amount of
relevant content. In another approach, the lexical
content of an essay was evaluated by comparing
the words contained in each essay to the words
found in a sample of essays from each score cat-
egory (Attali and Burstein, 2006). More recently,
Xie et al. (2012) used a similar approach in au-
tomated speech scoring; they measured the sim-
ilarity using three similarity measures, including
a lexical matching method (Vector Space Model)
and two semantic similarity measures (Latent Se-
mantic Analysis and Pointwise Mutual Informa-
tion). They showed moderately high correlations
117
between the similarity features and human profi-
ciency scores on even the output of an automatic
speech recognition system. Similarity measures
have also been used in off-topic detection for non-
native speakers? essays. Higgins et al. (2006) cal-
culated overlaps between the question and content
words from the essay and obtained an error rate of
10%.
Given the promising performance in both auto-
mated scoring and off-topic essay detection, we
will expand these similarity measures in NS re-
sponse detection for speech scoring.
3 Data
In this study, we used a collection of responses
from an international English language assess-
ment. The assessment was composed of items in
which speakers were prompted to provide sponta-
neous speech.
Approximately 48,000 responses from 8,000
non-native speakers were collected and used for
training the automated speech recognizer (ASR
set). Among 24 items in the ASR set, four items
were randomly selected. For these items, a total
of 11,560 responses were collected and used for
the training and evaluation of filtering model (FM
set). Due to the extremely skewed distribution of
NS responses (2% in the ASR set), it was not easy
to train and evaluate the filtering model. In or-
der to address this issue, we modified the distribu-
tion of NS responses in the FM set. Initially, we
collected 90,000 responses including 1,560 NS re-
sponses. While maintaining all NS responses, we
downsampled the scorable responses in the FM set
to include 10,000 responses. Finally, the propor-
tion of NS responses was 6 times higher in FM
set (13%) than ASR set. This artificial increase of
the NS responses reduces the current problem of
the skewed NS distribution and may make the task
easier. However, the likelihood of students engag-
ing in gaming strategies may increase with the use
of automated scoring, and this increased NS dis-
tribution may be close to this situation.
Each response was rated by trained human
raters using a 4-point scoring scale, where 1 indi-
cated a low speaking proficiency and 4 indicated a
high speaking proficiency. The raters also labeled
responses as NS, when appropriate. NS responses
are defined as responses that cannot be given a
score according to the rubrics of the four-point
scale. NS responses were responses with tech-
nical difficulties (TDs) that obscured the content
of the responses or responses that would receive
a score of 0 due to participants? inappropriate be-
haviors. The speakers, item information, and dis-
tribution of proficiency scores are presented in Ta-
ble 1. There was no overlap in the sets of speakers
in the ASR and FM sets.
In addition, 1,560 NS responses from the FM
set were further classified into six types by two
raters with backgrounds in linguistics using the
rubrics presented in Table 2. This annotation was
used for the purpose of analysis: to identify the
frequent types of NS responses and prioritize the
research effort.
Type Proportion
in total
NSs
Description
NR 73% No response. Test taker doesn?t
speak.
OR 16% Off-topic responses. The re-
sponse is not related to the
prompt.
TR 5% Generic responses. The re-
sponse only include filler words
or generic responses such as, ?I
don?t know, it is too difficult to
answer, well?, etc.
RE 4% Question copy. Full or partial
repetition of question.
NE 1% Non-English. Responses is in a
language other than English.
OT 1% Others
Table 2: Types of zero responses and proportions
Some responses belonged to more than one
type, and this increased complexity of the anno-
tation task. For instance, one response was com-
prised of a question copy and generic sentences,
while another response was comprised of a ques-
tion copy and off-topic sentences. An example of
this type was presented in Table 3. This was a re-
sponse for the question ?Talk about an interesting
book that you read recently. Explain why it was
interesting
1
.?
For these responses, annotators first segmented
them into sentences and assigned the type that was
most dominant.
Each rater annotated approximately 1,000 re-
sponses, and 586 responses were rated by both
1
In order to not reveal the real test question administered
in the operational test, we invented this question. Based on
the question, we also modified a sample response; the ques-
tion copy part was changed to avoid disclosure of the test
question, but the other part remained the same as the original
response.
118
Data set Num. responses Num. speakers Num. items Average score Score distribution
NS 1 2 3 4
ASR 48,000 8,000 24 2.63 773 1953 16834 23106 5334
2% 4% 35% 48% 11%
FM 11,560 11,390 4 2.15 1560 734 4328 4263 675
13% 6% 37% 37% 6%
Table 1: Data size and score distribution
Sentence Type
Well in my opinion are the inter-
esting books that I read recently
is.
RE
Talking about a interesting book. RE
One interesting book oh God in-
teresting book that had read re-
cently.
RE
Oh my God. TR
I really don?t know how to an-
swer this question.
TR
Well I don?t know. TR
Sorry. TR
Table 3: Manual transcription of complex-type re-
sponse
raters. The Cohen?s kappa between two raters was
0.76. Among five different NS responses, non-
response was the most frequent type (73%), fol-
lowed by off-topic (16%). The combination of the
two types was approximately 90% of the entire NS
responses.
4 Method
In this study, we generated two different types of
features. First, we developed similarity features
(both chunk-based and response-based) to identify
the responses with problems in topicality. Sec-
ondly, we generated acoustic, fluency, and ASR-
confidence features using a state-of-art automated
speech scoring system. Finally, using both feature
sets, classifiers were trained to make a binary dis-
tinction of NS response vs. scorable response.
4.1 Chunk-based similarity features
Some responses in this study included more than
two different types of the topicality problems. For
instance, the first three sentences in Table 3 be-
longed to the ?copied? category, while the other
sentences fell into ?unrelated?. If the similarity
features were calculated based on the entire re-
sponse, the feature values may fall into neither
the ?copied? nor ?unrelated? range because of the
trade-off between the two types at two extremes.
In order to address this issue, we calculated chunk-
based similarity features similar to Metzler et al.
(2005)?s sentence-based features.
First, the response was split into the chunks
which were surrounded by long silences with du-
rations longer than 0.6 sec. For each chunk,
the proportion of word overlap with the question
(WOL) was calculated based on the formula (1).
Next, chunks with a WOL higher than 0.5 were
considered as question copies.
WOL =
|S?Q|
|S|
where S is a response and Q is a question,
|S ? Q| is the number of word types that appear
both in S and Q,
|S| is the number of word types in S
(1)
Finally, the following three features were de-
rived for each response based on the chunk-based
WOL.
? numwds: the number of word tokens after re-
moving question copies, fillers, and typical
generic sentences
2
;
? copyR: the proportion of question copies in
the response in terms of number of word to-
kens;
? meanWOL: the mean ofWOLs for all chunks
in the response.
4.2 Response-based similarity features
We implemented three features based on a vector
space model (VSM) using cosine similarity and
term frequency-inverse document frequency (tf -
idf ) weighting to estimate the topic relevance at
the response-level.
2
Five sentences ?it is too difficult?, ?thank you?, ?I don?t
know?, ?I am sorry?, and ?oh my God? were stored as typical
sentences and removed from responses
119
Since the topics of each question were differ-
ent from each other, we trained a VSM for each
question separately. For the four items in the
FM set, we selected a total of 485 responses (125
responses per item) from the ASR set for topic
model training. Assuming that the responses with
the highest proficiency scores contain the most di-
verse and appropriate words related to the topic,
we only selected responses with a score of 4.
We obtained the manual transcriptions of the re-
sponses, and all responses about the same ques-
tion were converted into a single vector. In this
study, the term was a unigram word, and the doc-
ument was the response. idf was trained from the
entire set of 48,000 responses in the ASR training
partition, while tf was trained from the question-
specific topic model training set.
In addition to the response-based VSM, we
trained a question-based VSM. Each question was
composed of two sentences. Each question was
converted into a single vector, and a total of four
VSMs were trained. idf was trained in the same
way as the response-based VSMs, while tf was
trained only using the question sentences.
Using these two different types of VSMs, the
following three features were generated for each
response.
? sampleCosine: a similarity score based on
the response-based VSM. Assuming that two
documents with the same topic shared com-
mon words, it measured the similarity in the
words used in a test response and the sample
responses. The feature was implemented to
identify off-topic responses (OR);
? qCosine: a similarity score based on the
question-based VSM. It measured the simi-
larity between a test response and its ques-
tion. The feature was implemented to iden-
tify both off-topic responses (OR) and ques-
tion copy responses (RE); a low score is
highly likely to be an off-topic response,
while a high score signals a full or partial
copy;
? meanIDF : mean of idfs for all word tokens
in the response. Generic responses (TR) tend
to include many high frequency words such
as articles and pronouns, and the mean idf
value of these responses may be low.
4.3 Features from the automated speech
scoring system
A total of 61 features (hereafter, A/S features)
were generated using a state-of-the-art automated
speech scoring system. A detailed description
of the system is available from (Jeon and Yoon,
2012). Among these features, many features were
conceptually similar but based on different nor-
malization methods, and they showed a strong
inter-correlation. For this study, 30 features were
selected and classified into three groups according
to their characteristics: acoustic features, fluency
features, and ASR-confidence features.
The acoustic features were related to power,
pitch, and MFCC. First, power, pitch and
MFCC were extracted at each frame using
Praat (Boersma, 2002). Next, we generated
response-level features from these frame-level fea-
tures by calculating mean and variation. These
features captured the overall distribution of energy
and voiced regions in a speaker?s response. These
features are relevant since NS responses may have
an abnormal distribution in energy. For instance,
non-responses contain very low energy. In order
to detect these abnormalities in the speech signal,
pitch and power related features were calculated.
The fluency features measure the length of a re-
sponse in terms of duration and number of words.
In addition, this group contains features related
to speaking rate and silences, such as mean du-
ration and number of silences. In particular, these
features are effective in identifying non-responses
which contain zero or only a few words.
The ASR-confidence group contains features
predicting the performance of the speech recog-
nizer. Low confidence scores signal low speech
recognition accuracy.
4.4 Model training
Three filtering models were trained to investigate
the impact of each feature group: a filtering model
using similarity features (hereafter, the Similarity-
filter), a filtering model using A/S features (here-
after, the A/S-filter), and a filtering model using a
combination of the two groups of features (here-
after, the Combined-filter).
5 Experiments
AnHMM-based speech recognizer was trained us-
ing the ASR set. A gender independent triphone
acoustic model and a combination of bigram, tri-
120
gram, and four-gram language models were used.
A word error rate (WER) of 27% on the held-out
test dataset was observed.
For each response in the FM set, the word
hypotheses was generated using this recognizer.
From this ASR-based transcription, the six simi-
larity features were generated. In addition, the 30
A/S features described in 4.3 were generated.
Using these two sets of features, filtering mod-
els were trained using the Support Vector Ma-
chine algorithm (SVM) with the RBF kernel of
the WEKA machine-learning toolkit (Hall et al.,
2009). A 10 fold cross-validation was conducted
using the FM dataset.
6 Results and discussion
First, we will report the performance for the sub-
set only topic-related NS responses. The sim-
ilarity features were designed to detect NS re-
sponses with topicality issues, but the majority in
the FM set were non-response (73%). The topic-
related NS responses (off-topic responses, generic
responses, and question copy responses) were only
25%. In the entire set, the advantage of the simi-
larity features over the A/S features might not be
salient due to the high proportion of non-response.
In order to investigate the performance of the sim-
ilarity features in the topic related NS responses,
we excluded all responses other than ?OR?, ?TR?,
and ?RE? from the FM set and conducted a 10 fold
cross-validation.
Table 4 presents the average of the 10 fold
cross-validation results in this subset. In this set,
the total number of NS responses is 314, and the
accuracy of the majority voting (to classify all re-
sponses as scorable responses) is 0.962.
acc. prec. recall fscore
Similarity-
filter
0.975 0.731 0.548 0.626
A/S-filter 0.971 0.767 0.341 0.472
Combined-
filter
0.977 0.780 0.566 0.656
Table 4: Performance of filters in topic-related NS
detection
Not surprisingly, the Similarity-filter outper-
formed the A/S-filter: the F-score was approxi-
mately 0.63 which was 0.15 higher than that of
the A/S-filter in absolute value. The lack of fea-
tures specialized for detection of topic abnormal-
ity resulted in the low recall of the A/S-filter. The
combination of the two features achieved a slight
improvement: the F-score was 0.66 and it was 0.03
higher than the Similarity-filter.
In Metzler et al. (2005)?s study, the system us-
ing both sentence-based features and document-
based features did not achieve further improve-
ment over the system based on the document-
based features alone. In order to explore the im-
pact of chunk-based features, similarity features
were classified into two groups (chunk-based fea-
tures vs. document-based features), and two fil-
ters were trained using each group separately. Ta-
ble 5 compares the performance of the two filters
(Similarity-chunk and Similarity-doc) with the fil-
ter using all similarity features (Similarity).
acc. prec. recall fscore
Similarity-
chunk
0.972 0.700 0.442 0.542
Similarity-
doc
0.971 0.730 0.396 0.514
Similarity 0.975 0.731 0.548 0.626
Table 5: Comparison of chunk-based and
document-based similarity features
In this study, the chunk-based features were
comparable to the document-based features. Fur-
thermore, combination of the two features im-
proved F-score. The performance improvement
mostly resulted from higher recall.
Finally, Table 6 presents the results using the
entire FM set, including the OR, TR, and RE re-
sponses that were not included in the previous
experiment. The accuracy of the majority class
baseline (classifying all responses as scorable re-
sponses) is 0.865.
acc. prec. recall fscore
Similarity-
filter
0.976 0.926 0.895 0.910
A/S-filter 0.974 0.953 0.849 0.898
Combined-
filter
0.977 0.941 0.884 0.911
Table 6: Performance of filters in all types of NS
detection
Both the Similarity-filter and the A/S-
filter achieved high performance. Both accuracies
and F-scores were similar and the difference
121
between the two filters was approximately 0.01.
The Similarity-filter achieved better performance
than the A/S-filter in recall: it was 0.89, which
was substantially higher than the A/S-filter (0.85).
It is an encouraging result that the Similarity-
filter could achieve a performance comparable
to the A/S-filter, which was based on multi-
ple resources such as signal processing, forced-
alignment, and ASR. But, the combination of the
two feature groups did not achieve further im-
provement: the increase in both accuracy and F-
measure was less than 0.01.
7 Conclusions
In this study, filtering models were implemented
as a supplementary module for an automated
speech proficiency scoring system. In addition to
A/S features, which have shown promising perfor-
mance in previous studies, a set of similarity fea-
tures were implemented and a filtering model was
developed. The Similarity-filter was more accu-
rate than the A/S-filter in identifying the responses
with topical problems. This result is encouraging
since the proportion of these responses is likely to
increase when the automated speech scoring sys-
tem becomes a sole rater of the assessment.
Although the Similarity-filter achieved better
performance than the A/S-filter, it should be fur-
ther improved. The recall of the system was low,
and approximately 45% of NS responses could
not be identified. In addition, the model requires
substantial amount of sample responses for each
item, and it will cause serious difficulty when it is
used the real test situation. In future, we will ex-
plore the similarity features trained only using the
prompt question or the additional prompt materi-
als such as visual and audio materials.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e?rater R v.2. The Journal of Technol-
ogy, Learning, and Assessment, 4(3).
Paul Boersma. 2002. Praat, a system for doing phonet-
ics by computer. Glot International, 5(9/10):341?
345.
Sergey Brin, James Davis, and Hector Garcia-Molina.
1995. Copy detection mechanisms for digital docu-
ments. In ACM SIGMOD Record, volume 24, pages
398?409. ACM.
Jian Cheng and Jianqiang Shen. 2011. Off-topic detec-
tion in automated speech assessment applications.
In Proceedings of InterSpeech, pages 1597?1600.
IEEE.
Peter W. Foltz, Darrell Laham, and Thomas K. Lan-
dauer. 1999. The Intelligent Essay Assessor: Appli-
cations to educational technology. Interactive mul-
timedia Electronic Journal of Computer-Enhanced
Learning, 1(2).
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Derrick Higgins, Jill Burstein, and Yigal Attali. 2006.
Identifying off-topic student essays without topic-
specific training data. Natural Language Engineer-
ing, 12(02):145?159.
Timothy C Hoad and Justin Zobel. 2003. Meth-
ods for identifying versioned and plagiarized doc-
uments. Journal of the American society for infor-
mation science and technology, 54(3):203?215.
Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic
feature-based non-scorable response detection for an
automated speaking proficiency assessment. In Pro-
ceedings of the InterSpeech, pages 1275?1278.
Wai-Kit Lo, Alissa M Harrison, and Helen Meng.
2010. Statistical phone duration modeling to filter
for intact utterances in a computer-assisted pronun-
ciation training system. In Proceedings of Acous-
tics Speech and Signal Processing (ICASSP), 2010
IEEE International Conference on, pages 5238?
5241. IEEE.
Donald Metzler, Yaniv Bernstein, W Bruce Croft, Al-
istair Moffat, and Justin Zobel. 2005. Similarity
measures for tracking information flow. In Proceed-
ings of the 14th ACM international conference on In-
formation and knowledge management, pages 517?
524. ACM.
Mark Sanderson. 1997. Duplicate detection in the
reuters collection. ? Technical Report (TR-1997-5)
of the Department of Computing Science at the Uni-
versity of Glasgow G12 8QQ, UK?.
Narayanan Shivakumar and Hector Garcia-Molina.
1995. Scam: A copy detection mechanism for digi-
tal documents.
Narayanan Shivakumar and Hector Garcia-Molina.
1996. Building a scalable and accurate copy detec-
tion mechanism. In Proceedings of the first ACM
international conference on Digital libraries, pages
160?168. ACM.
Joost van Doremalen, Helmet Strik, and Cartia Cuc-
chiarini. 2009. Utterance verification in language
learning applications. In Proceedings of the SLaTE.
122
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 103?111. Association for Computa-
tional Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883?895.
123
