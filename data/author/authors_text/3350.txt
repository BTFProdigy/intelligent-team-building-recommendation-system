A Compact Architecture for Dialogue Management Based on 
Scripts ond Meta-Outputs 
MAnny Rayner ,  Beth  Ann Hockey ,  F rAnk ie  J~mes  
Research Inst i tute for Advanced Computer  Science 
Mail  Stop 19-39, NASA Ames Research Center  
Moffett  Field, CA  94035-1000 
{ mauny, bahockey, l~ ames} ~r iacs.edu 
Abst rac t  
We describe an architecture for spoken dialogue 
interfaces to semi-autonomous systems that trans- 
forms speech signals through successive r presenta- 
tions of linguistic, dialogue, and domain knowledge. 
Each step produces an output, and a meta-output 
describing the transformation, with an executable 
program in a simple scripting language as the fi- 
nal result. The output/meta-output distinction per- 
mits perspicuous treatment of diverse tasks such as 
resolving pronouns, correcting user misconceptions, 
and optimizing scripts. 
1 In t roduct ion  
The basic task we consider in this paper is that of 
using spoken language to give commands to a semi- 
autonomous robot or other similar system. As ev- 
idence of the importance of this task in the NLP 
community note that the early, influential system 
SHRDLU (Winograd, 1973) was intended to address 
just this type of problem. More recent work on spo- 
ken language interfaces to semi-antonomous robots 
include SRrs Flakey robot (Konolige et al, 1993) 
and NCARArs InterBOT project (Perzanowski et 
al., 1998; Perzanowski et al, 1999). A number of 
other systems have addressed part of the task. Com- 
mandTalk (Moore et al, 1997), Circuit Fix-It Shop 
(Smith, 1997) and TRAINS-96 (Traum and Allen, 
1994; Tranm and Andersen, 1999) are spoken lan- 
guage systems but they interface to simulation or 
help facilities rather than semi-autonomous agents. 
Jack's MOOse Lodge (Badler et al, 1999) takes text 
rather than speech as natural language input and the 
avatars being controlled are not semi-autonomous. 
Other researchers have considered particular aspects 
of the problem such as accounting for various aspects 
of actions (Webber, 1995; Pyre et al, 1995). In most 
of this and other related work the treatment is some 
variant of the following. If there is a speech inter- 
face, the input speech signal is converted into text. 
Text either from the recognizer or directly input by 
the user is then converted into some kind of logi- 
cal formula, which abstractly represents the user's 
intended command; this formula is then fed into a 
command interpreter, which executes the command. 
We do not think the standard treatment outlined 
above is in essence incorrect, but we do believe that, 
as it stands, it is in need of some modification. This 
paper will in particular make three points. First, we 
suggest that the output representation should not be 
regarded as a logical expression, but rather as a pro- 
gram in some kind of scripting language. Second, we 
argue that it is not merely the case that the process 
of converting the input signal to the final represen- 
tation can sometimes go wrong; rather, this is the 
normal course of events, and the interpretation pro- 
cess should be organized with that assumption in 
mind. Third, we claim, perhaps urprisingly, that 
the first and second points are related. These claims 
are elaborated in Section 2. 
The remainder of the paper describes an archi- 
tecture which addresses the issues outlined above, 
and which has been used to implement a prototype 
speech interface to a simulated semi-autonomous 
robot intended for deployment on the International 
Space Station. Sections 3 and 4 present an overview 
of the implemented interface, focussing on represen- 
tational issues relevant to dialogue management. Il-
lustrative xamples of interactions with the system 
are provided in Section 5. Section 6 concludes. 
2 Theoret i ca l  Ideas  
2.1 Scripts vs Logical Forms 
Let's first look in a little more detail at the question 
of what the output representation f a spoken lan- 
guage interface to a semi-autonomous robot/agent 
should be. In practice, there seem to be two main 
choices: atheoreticai representations, or some kind 
of logic. 
Logic is indeed an excellent way to think 
about representing static relationships like database 
queries, but it is much less clear that it is a good way 
to represent commands. In real life, when people 
wish to give a command to a computer, they usu- 
ally do so via its operating system; a complex com- 
mand is an expression in a scripting language like 
CSHELL, Perl, or VBScript. These languages are 
related to logical formalisms, but cannot be mapped 
112 
onto them in a simple way. Here are some of the 
obvious differences: 
? A scripting language is essentially imperative, 
rather than relational. 
? The notion of temporal sequence is fundamental 
to the language. "Do P and then Q" is not the 
same as "Make the goals P and Q true"; it is 
explicitly stated that P is to be done first. Simi- 
larly, "For each X in the list (A B C), do P(X)" 
is not the same as "For all X, make P(X) true"; 
once again, the scripting language defines an or: 
der, but not the logical anguage 1. 
? Scripting languages assume that commands do 
not always succeed. For example, UNIX-based 
scripting languages like CSHELL provide each 
script with the three predefined streams tdin, 
stdout and sl;derr. Input is read from std in 
and written to sCdout; error messages, warn- 
ings and other comments are sent to stderr .  
We do not think that these properties of scripting 
language are accidental. They have evolved as the 
result of strong selectional pressure from real users 
with real-world tasks that need to be carried out, 
and represent a competitive way to meet said users' 
needs. We consequently hink it is worth taking seri- 
ously the idea that a target representation produced 
by a spoken language interface should share many of 
these properties. 
2.2 Fall|ble Interpretat ion:  Outputs  and 
Meta-outputs  
We now move on to the question of modelling the in- 
terpretation process, that is to say the process that 
converts the input (speech) signal to the output (ex- 
ecutable) representation. As already indicated, we 
think it is important to realize that interpretation 
is a process which, like any other process, may suc- 
ceed more or less well in achieving its intended goals. 
Users may express themselves unclearly or incom- 
pletely, or the system may more or less seriously 
fail to understand exactly what they mean. A good 
interpretation architecture will keep these consider- 
ations in mind. 
Taking our lead from the description of scripting 
languages sketched above, we adapt the notion of 
the "error stream" to the interpretation process. In 
the course of interpreting an utterance, the system 
translates it into successively "deeper" levels of rep- 
resentation. Each translation step has not only an 
input (the representation consumed) and an output 
1In cases like these, the theorem prover or logic program- 
ruing interpreter used to evaluate he logical formula typically 
assigns a conventional order to the conjuncts; note however 
that this is part of the procedural semantics ofthe theorem 
prover/interpreter, and does not follow from the declarative 
semantics of the logical formalism. 
(the representation produced), but also something 
we will refer to as a "meta-output": his provides in- 
formation about how the translation was performed. 
At a high level of abstraction, our architecture will 
be as follows. Interpretation proceeds as a series 
of non-deterministic translation steps, each produc- 
ing a set of possible outputs and associated meta- 
outputs. The final translation step produces an ex- 
ecutable script. The interface attempts to simulate 
execution of each possible script produced, in or- 
der to determine what would happen if that script 
were selected; simulated execution can itself produce 
further meta-outputs. Finally, the system uses the 
meta-output information to decide what to do with 
the various possible interpretations it has produced. 
Possible actions include selection and execution of 
an output script, paraphrasing meta-output infor- 
mation back to the user, or some combination ofthe 
two. 
In the following section, we present a more de- 
tailed description showing how the output/meta- 
output distinction works in a practical system. 
3 A P ro to type  Imp lementat ion  
The ideas sketched out above have been realized as 
a prototype spoken language dialogue interface to a 
simulated version of the Personal Satellite Assistant 
(PSA; (PSA, 2000)). This section gives an overview 
of the implementation; i  the following section, we 
focus on the specific aspects of dialogue management 
which are facilitated by the output/meta-output ar- 
chitecture. 
3.1 Leve ls  o f  Representat ion  
The real PSA is a miniature robot currently being 
developed at NASA Ames Research Center, which 
is intended for deployment on the Space Shuttle 
and/or International Space Station. It will be ca- 
pable of free navigation in an indoor micro-gravity 
environment, and will provide mobile sensory capac- 
ity as a backup to a network of fixed sensors. The 
PSA will primarily be controlled by voice commands 
through a hand-held or head-mounted microphone, 
with speech and language processing being handled 
by an offboard processor. Since the speech process- 
ing units are not in fact physically connected to the 
PSA we envisage that they could also be used to con- 
trol or monitor other environmental functions. In 
particular, our simulation allows voice access to the 
current and past values of the fixed sensor eadings. 
The initial PSA speech interface demo consists of 
a simple simulation of the Shuttle. State parame- 
ters include the PSA's current position, some envi- 
ronmental variables uch as local temperature, pres- 
sure and carbon dioxide levels, and the status of the 
Shuttle's doors (open/closed). A visual display gives 
direct feedback on some of these parameters. 
113 
The speech and language processing architecture 
is based on that of the SRI CommandTalk sys- 
tem (Moore et al, 1997; Stent et al, 1999). The sys- 
tem comprises a suite of about 20 agents, connected 
together using the SPd Open Agent Architecture 
(OAA; (Martin et al, 1998)). Speech recognition 
is performed using a version of the Nuance recog- 
nizer (Nuance, 2000). Initial language processing is
carried out using the SRI Gemini system (Dowding 
et al, 1993), using a domain~independent unification 
grammar and a domain-specific lexicon. The lan- 
guage processing rammar is compiled into a recog- 
nition grarnm~kr using the methods of (Moore et al, 
1997); the net result is that only grammatically well- 
formed utterances can be recognized. Output from 
the initial language-processing step is represented 
in a version of Quasi Logical Form (van Eijck and 
Moore, 1992), and passed in that form to the dia- 
logue manager. We refer to these as linguistic level 
representations. 
The aspects of the system which are of primary in- 
terest here concern the dialogue manager (DM) and 
related modules. Once a linguistic level represen- 
tation has been produced, the following processing 
steps occur: 
? The linguistic level representation is converted 
into a discourse level representation. This pri- 
marily involves regularizing differences in sur- 
face form: so, for example, "measure the pres- 
sure" and '~hat is the pressure?" have differ- 
ent representations at the linguistic level, but 
the same representation at the discourse level. 
? If necessary, the system attempts to resolve in- 
stances of ellipsis and anaph*oric reference. For 
example, if the previous command was "mea- 
sure temperature at flight deck", then the new 
command "lower deck" will be resolved to an 
expression meaning "measure temperature at 
lower deck". Similarly, if the previous command 
was "move to the crew hatch", then the com- 
mand "open it" will be resolved to "open the 
crew hatch". We call the output of this step a 
resolved iscourse level representation. 
? The resolved discourse level representation is 
converted into an executable script in a lan- 
guage essentially equivalent o a subset of 
CSHELL. This involves two sub-steps. First, 
quantified variables are given scope: for exam- 
ple, "go to the flight deck and lower deck and 
measure pressure" becomes omething approxi- 
mately equivalent to the script 
foreach x ( f l ight_deck lower_deck) 
go_to $x 
measure  pressure 
end 
The point to note here is that the foreach has 
scope over both the go_to and the meeusmre ac- 
tions; an alternate (incorrect) scoping would be 
fo reachx  ( f l ight_deck lower_deck) 
go_to $x 
end 
measure  pressure 
The second sub-step is to attempt o optimize 
the plan. In the current example, this can 
be done by reordering the list ( f l ight .deck 
louer_deck). For instance, if the PSA is al- 
ready at the lower deck, reversing the list will 
mean that the robot only makes one trip, in- 
stead of two. 
The final step in the interpretation process is 
plan evaluation: the system tries to work out 
what will happen if it actually executes the 
plan. (The relationship between plan evaluation 
and plan execution is described in more detail 
in Section 4.1). Among other things, this gives 
the dialogue manager the possibility of compar- 
ing different interpretations of the original com- 
mand, and picking the one which is most effi- 
cient. 
3.2 How Meta-outputs Participate in the 
Tr---qlation , 
The above sketch shows how context-dependent 
interpretation is arranged as a series of non- 
deterministic translation steps; in each case, we have 
described the input and the output for the step in 
question. We now go back to the concerns of Sec- 
tion 2. First, note that each translation step is in 
general fallible. We give several examples: 
One of the most obvious cases arises when the 
user simply issues an invalid command, such as 
requesting the PSA to open a door D which is 
already open. Here, one of the meta-outputs 
issued by the plan evaluation step will be the 
term 
presupposition_failure(already_open(D)); 
the DM can decide to paraphrase this back to 
the user as a surface string of the form "D is 
already open". Note that plan evaluation does 
not involve actually executing the final script, 
which can be important. For instance, if the 
command is "go to the crew hatch and open it" 
and the crew hatch is already open, the interface 
has the option of informing the user that there 
is a problem without first carrying out the "go 
to" action. 
The resolution step can give rise to similar kinds 
of metaooutput. For example, a command may 
114 
include a referring expression that has no deno- 
tation, or an ambiguous denotation; for exam- 
ple, the user might say "both decks", presum- 
ably being unaware that there are in fact three 
of them. This time, the meta-output produced 
is 
presupposition_failure ( 
incorrect_size_of_set (2,3)) 
representing the user's incorrect belief about 
the number of decks. The DM then has the pos- 
sibility of informingthe user of this misconcelfi 
tion by realizing the meta-output term as the 
surface string "in fact there are three of them". 
Ambiguous denotation occurs when a descrip- 
tion is under-specified. For instance, the user 
might say "the deck" in a situation where there 
is no clearly salient deck, either in the discourse 
situation or in the simulated world: here, the 
meta-output will be 
presupposition_failure ( 
underspecif ied_def inite (deck)) 
which can be realized as the clarification ques- 
tion "which deck do you mean?" 
? A slightly more complex case involves plan 
costs. During plan evaluation, the system simu- 
lates execution of the output script while keep- 
ing track of execution cost. (Currently, the cost 
is just an estimate of the time required to exe- 
cute the script). Execution costs are treated as 
meta-outputs of the form 
cost (C) 
and passed back through the interpreter so that 
the plan optimization step can make use of 
them. 
? Finally, we consider what happens when the 
system receives incorrect input from the speech 
recognizer. Although the recognizer's language 
model is constrained so that it can only pro- 
duce grammatical utterances, it can still misrec- 
ognize one grammatical string as another one. 
Many of these cases fall into one of a small 
number of syntactic patterns, which function as 
fairly reliable indicators of bad recognition. A 
typical example is conjunction involving a pro- 
noun: if the system hears "it and flight deck", 
this is most likely a misrecognition fsomething 
like "go to flight deck". 
During the processing phase which translates 
linguistic level representations into discourse 
level representations, the system attempts to 
match each misrecognition pattern against he 
input linguistic form, and if successful produces 
a meta-output of the form 
presupposi t ion_fa i lure ( 
dubious_If (<Type>)) 
These meta-outputs are passed down to the 
DM, which in the absence of sufficiently com- 
pelling contrary evidence will normally issue a 
response of the form "I'm sorry, I think I mis- 
heard you". 
4 A Compact  Arch i tec ture  for  
D ia logue  Management  Based  on  
Scr ip ts  and  Meta -Outputs  
None of the individual functionalities outlined above 
are particularly novel in themselves. What we find 
new and interesting is the fact that they can all 
be expressed in a uniform way in terms of the 
script output/meta-output architecture. This sec- 
tion presents three examples illustrating how the ar- 
chitecture can be used to simplify the overall orga- 
nization of the system. 
4.1 Integration of plan evaluation, plan 
execution and dialogue management 
Recall that the DM simulates evaluation of the plan 
before running it, in order to obtain relevant meta- 
information. At plan execution time, plan actions 
result in changes to the world; at plan evaluation 
time, they result in simulated changes to the world 
and/or produce meta-outputs. 
Conceptualizing plans as scripts rather than log- 
icai formulas permits an elegant reatment of the 
execution/evaluation dichotomy. There is one script 
interpreter, which functions both as a script exec- 
utive and a script evaluator, and one set of rules 
which defines the procedural semantics of script ac- 
tions. Rules are parameterized by execution type 
which is either "execute" or "evaluate". In "evalu- 
ate" mode, primitive actions modify a state vector 
which is threaded through the interpreter; in "ex- 
ecute" mode, they result in commands being sent 
to (real or simulated) effector agents. Conversely, 
"meta-information" actions, such as presupposition 
failures, result in output being sent to the meta- 
output stream in "evaluate" mode, and in a null ac- 
tion in "execute" mode. The upshot is that a simple 
semantics can be assigned to rules like the following 
one, which defines the action of attempting to open 
a door which may already be open: 
procedure ( 
open_door (D), 
i f_then_else (status (D, open_closed, open), 
presupposi t ion_fa i lure (already_open(D)), 
change_status (D, open_closed, open) ) ) 
4.2 Using meta-outputs to choose between 
interpretations 
As described in the preceding section, the resolution 
step is in general non-deterministic and gives rise to 
115 
meta-outputs which describe the type of resolution 
carried out. For example, consider a command in- 
volving a definite description, like "open the door". 
Depending on the preceding context, resolution will 
produce a number of possible interpretations; "the 
door" may be resolved to one or more contextually 
available doors, or the expression may be left un- 
resolved. In each case, the type of resolution used 
appears as a meta-output, and is available to the di- 
alogue manager when it decides which interpretation 
is most felicitous. By default, the DM's strategy isto 
attempt to supply antecedents for referring expre~.. 
sious, preferring the most recently occurring sortally 
appropriate candidate. In some cases, however, it is 
desirable to allow the default strategy to be over- 
ridden: for instance, it may result in a script which 
produces a presupposition failure during plan eval- 
uation. Treating resolution choices and plan evalu- 
ation problems as similar types of objects makes it 
easy to implement this kind of idea. 
4.3 Us ing meta-outputs  to choose between 
dialogue management  moves 
Perhaps the key advantage ofour architecture is that 
collecting together several types of information as a 
bag of meta-outputs simplifies the top-level struc- 
ture of the dialogue manager. In our application, 
the critical choice of dialogue move comes after the 
dialogue manager has selected the most plausible in- 
terpretation. It now has to make two choices. First, 
it must decide whether or not to paraphrase any of 
the meta-outputs back to the user; for example, if
resolution was unable to fill some argument posi- 
tion or find an antecedent for a pronoun, it may be 
appropriate to paraphrase the corresponding meta- 
output as a question, e.g. "where do you mean?", or 
"what do you mean by 'it' ?' .  Having all the meta- 
outputs available together means that the DM is 
able to plan a coherent response: so if there are sev- 
eral recta-outputs which could potentially be worth 
paraphrasing, it typically realizes only the most im- 
portant one. 
Second, if interpretation was able to produce a 
well-formed plan, the DM currently has the three 
options of executing it, paraphrasing it back to the 
user as a confirmation question, or doing nothing. 
Once again, this decision often requires global infor- 
mation about what has happened uring the inter- 
pretation process. For example, knowing that plan 
evaluation showed that the plan would take signif- 
icant time to execute makes it more plausible that 
the user would prefer to receive a confirmation. 
5 Examples  
This section presents a commented sample dialogue 
with the implemented PSA interface demo chosen 
to illustrate the points discussed in the previous ec- 
tions. We focus in particular on the role that meta- 
outputs play in interpretation. Text preceded by 
USER represents spoken utterances from the user. 
Text preceded by PSA represents synthesized spoken 
responses from the PSA. Text inside square brackets 
describes the actions carried out by the PSA. 
1. The PSA can move to various locations and 
measure nvironmental f ctors. In the first ex- 
change, reference resolution is used to interpret 
the phrase "all three decks" as referring to the 
lower deck, mid deck and flight deck. Plan op- 
timiT.ation finds the most efficient way to or- 
der the locations, and paraphrases the resulting 
plan back to the user. 
USER: Go to all three decks and measure car- 
bon dioxide. 
PSA: I will move to flight deck, mid deck and 
then lower deck and I will measure carbon 
dioxide level, okay? 
USER: Okay. \[PSA moves to flight deck\] 
PSA: The carbon dioxide level at the flight 
deck is one percent. \[PSA moves to mid 
deck\] 
PSA: The carbon dioxide level at the mid deck 
is one percent. \[PSA moves to lower deck\] 
PSA: The carbon dioxide level at the lower 
deck is one percent. 
2. The next example involves resolution of ellip- 
sis. The relevant context is recovered from the 
preceding dialogue. 
USER: Do the same for the pilot's seat. 
PSA: I will move to the pilot's seat and I will 
measure carbon dioxide level, okay? 
USER: Right. \[PSA moves to pilot's seat\] 
PSA: The carbon dioxide level at the pilot's 
seat is one percent. 
3. Resolution of ellipsis can involve filling in un- 
specified semantic parameters toa syntactically 
complete command. It is not meaningful to talk 
about a sensor eading without reference to the 
place at which that reading was carried out; in 
the following example, resolution assumes that 
the location in question is the one most recently 
mentioned in the dialogue. 
Note also that since the PSA does not have to 
move in order to carry out the command, and 
hence can execute it immediately, the DM rea- 
sonably decides to skip the confirmation ques- 
tion. This decision is made based on the meta- 
output encoding the estimated cost of executing 
the script. 
116 
USER: What was the carbon dioxide level at 
fifteen oh five according to the fixed sen- 
sors? 
PSA: According to the fixed sensors, at fifteen 
oh five the carbon dioxide level at the pi- 
lot's seat was one percent. 
4. The system encounters a user misconception, 
which results in a "presupposition failure" 
meta-output being produced uring reference 
resolution. It consequently decides to alert the 
user to this, rather than attempt to carry out 
the command. 
USER: Close both doors. 
PSA: There are in fact three of them. 
5. The user utterance includes a pronoun, "it". 
The system resolves "it" to "the crew hatch", 
and issues an appropriate paraphrase in order 
to confirm that this is what the user meant. 
USER: Go to crew hatch and close it. 
PSA: I will go to crew hatch and I will close 
crew hatch, okay? 
USER: Yeah. \[PSA moves to crew hatch and 
closes it\] 
6. The user asks the system to close the door, but 
it is unclear which door they are referring to. 
An "ambiguous reference" meta-output is pro- 
duced by reference resolution, resulting in the 
DM issuing a clarification question. 
USER: Close the door. 
PSA: Which door do you mean? 
7. The user response is successfully interpreted by 
reference resolution as an instance of ellipsis. 
Trying to close the crew hatch a second time 
however causes a presupposition failure meta- 
output o be generated during plan evaluation. 
The system informs the user of this and takes 
no action. 
USER: The crew hatch. 
PSA: The crew hatch is already closed. 
8. The user presents three locations in a subopti- 
mal order. Plan evaluation finds a more efficient 
ordering. 
USER: Move to storage lockers, commander's 
seat and flight deck and measure tempera- 
ture. 
PSA: I will move to flight deck, commander's 
seat and then storage lockers and I will 
measure temperature, okay? 
USER: Sure. 
\[PSA moves to flight deck, commander's 
seat and storage lockers in that order and 
reports temperature at each location\] 
9. The user asks the system to repeat he previous 
command. Since the PSA is now starting from 
a different position, its most efficient plan is no 
longer the same, and it informs the user of this. 
USER: Do that again. 
PSA: I will move to storage lockers, comman- 
der's seat and then flight deck and I will 
measure temperature, okay? 
USER: Yes. \[PSA moves to storage lockers\] 
PSA: The temperature at the storage lockers 
is 19.9 degrees Celsius. \[PSA starts moving 
to commander's seat\] 
10. The user interrupts the PSA in the middle of the 
plan, and asks it to go back to its last stopping 
place. Since "stop" may involve a potentially 
hazardous condition, the interface obeys imme- 
diately, without attempting to confirm first. 
USER: Stop. \[PSA stops\] 
USER: Go back. \[PSA returns to storage lock- 
ers/ 
6 Summary  
We have described a generic architecture for spoken 
language dialogue interfaces to semi-autonomous 
agents, based on the standard notion of translating 
to successively deeper levels of representation. The 
novel aspects of our scheme center around two ideas: 
first, that the final output representations are best 
conceptualized not as logical expressions but rather 
as programs in a scripting language; second, that 
steps in the translation process hould produce not 
only a simple output, but also meta-information de-
scribing how the output was produced. We have pre- 
sented examples suggesting how several apparently 
diverse types of dialogue behavior can be captured 
simply within our framework, and outlined a proto- 
type implementation f the scheme. 
References  
N. Badler, R. Bindiganavale, J. Bourne, J. Allbeck, 
J. Shi, and M. Palmer. 1999. Real time virtual 
humans. In International Conference on Digital 
Media Futures. 
J. Dowding, M. Gawron, D. Appelt, L. Cherny, 
R. Moore, and D. Moran. 1993. Gemini: A nat- 
ural language system for spoken language un- 
derstanding. In Proceedings of the Thirty-First 
Annual Meeting of the Association for Computa- 
tional Linguistics. 
117 
K. Konolige, K. Myers, E. Ruspini, and A. Saf- 
fiotti. 1993. Flakey in action: The 1992 AAAI  
robot competition. Technical Report SRI Techni- 
cal Note 528, SKI, AI Center, SKI International, 
333 Ravenswood Ave., Menlo Park, CA  94025. 
D. Martin, A. Cheyer, and D. Moran. 1998. Build- 
ing distributed software systems with the open 
agent architecture. In Proceedings of the Third 
International Conference on the Practical Appli- 
cation of Intelligent Agenta nd Multi-Agent Tech- 
nalogy. 
R. Moore, J. Dowding, H. Bratt, J. Gawron~- 
Y. Gorfu, and A. Cheyer. 1997. CommandTalk: 
A spoken-language interface for battlefield simu- 
lations. In Proceedings ofthe Fifth Conference on 
Applied Natural Language Processing, pages 1-7. 
Nuance, 2000. Nuance Communications, Inc. 
http://www.nuance.com. As of 9 March 2000. 
D. Perzanowski, A. Schnltz, and W. Adams. 1998. 
Integrating natural language and gesture in a 
robotics domain. In IEEE International Sympo- 
sium on Intelligent Control." ISIC/CIRA/ISAS 
Joint Conference, pages 247-252, Gaithersburg, 
MD: National Institute of Standards and Tech- 
nology. 
D. Perzanowski, A. Schnltz, W. Adams, and 
E. Marsh. 1999. Goal tracking in s natural lan- 
guage interface: Towards achieving adjustable au- 
tonomy. In ISIS/CIRA99 Conference, Monterey, 
CA. IEEE. 
PSA, 2000. Personal Satellite Assistant (PSA) 
Project. http://ic.arc.nasa.gov/ic/psa/. As of 9 
March 2000. 
D. Pyre, L. Pryor, and D. Murphy. 1995. Actions 
as processes: a position on planning. In Working 
Notes, AAAI Symposium on Eztending Theories 
of Action, pages 169-173. 
R. W. Smith. 1997. An evaluation of strategies for 
selective utterance verification for spoken atural 
language dialog. In Proceedings of the Fifth Con- 
\]erence on Applied Natural Language Processing, 
pages 41-48. 
A. Stent, J. Dowding, J. Gawron, E. Bratt, and 
R. Moore. 1999. The CommandTalk spoken di- 
alogue system. In Proceedings of the Thirty- 
Seventh Annual Meeting of the Association for 
Computational Linguistics, pages 183-190. 
D. R. Tranm and J. Allen. 1994. Discourse obliga- 
tions in dialogue processing. In Proceedings ofthe 
Thirty-Second Annual Meetiitg of the Association 
for Computational Linguistics, pages 1-8. 
D. R. Traum and C. F. Andersen. 1999. Represen- 
tations of dialogue state for domain and task inde- 
pendent meta-dialogue. In Proceedings of the IJ- 
CAI'gg Workshop on Knowledge and Reasoning 
in Practical Dialogue Systems, pages 113-120. 
J. van Eijck and R. Moore. 1992. Semantic rules 
for English. In H. Alshawi, editor, The Core Lan- 
guage Engine. MIT Press. 
B. Webber. 1995. Instructing animated agents: 
Viewing language inbehavioral terms. In Proceed- 
ings of the International Conference on Coopera- 
tive Multi-modal Communication. 
T. A. Winograd. 1973. A procedural model of lan- 
guage understanding. In R. C. Shank and K. M. 
Colby, editors, Computer Models of Thought and 
Language. Freeman, San Francisco, CA. 
118 
Compi l ing  Language Mode ls  f rom a L ingu is t i ca l ly  Mot ivated  
Un i f i ca t ion  Grammar  
Manny Rayner t>, Beth Ann Hockey t, Frankie James t 
Elizabeth Owen Bratt ++, Sharon Goldwater ++ and Jean Mark Gawron ~ 
tResea.rch Inst i tute for 
Advanced Computer  Science 
Mail Stop 19-39 
NASA Ames Research Center 
Moffett Field, CA 94035-1000 
Abstract 
Systems now exist which are able to con:pile 
unification gralmnars into language models that 
can be included in a speech recognizer, but it 
is so far unclear whether non-trivial linguisti- 
cally principled gralnlnars can be used for this 
purpose. We describe a series of experiments 
which investigate the question empirica.lly, by 
incrementally constructing a grammar and dis- 
covering what prot)lems emerge when succes- 
sively larger versions are compiled into finite 
state graph representations and used as lan- 
guage models for a medium-vocabulary recog- 
nition task. 
1 Introduction ~ 
Construction of speech recognizers for n:ediuln- 
vocabulary dialogue tasks has now becolne an 
important I)ractical problem. The central task 
is usually building a suitable language model, 
and a number of standard methodologies have 
become established. Broadly speaking, these 
fall into two main classes. One approach is 
to obtain or create a domain corpus, and froln 
it induce a statistical anguage model, usually 
some kind of N-gram grammar; the alternative 
is to manually design a grammar which specifies 
the utterances the recognizer will accept. There 
are many theoretical reasons to prefer the first 
course if it is feasible, but in practice there is of- 
ten no choice. Unless a substantial domain cor- 
pus is available, the only method that stands a 
chance of working is hand-construction f an ex- 
i The majority of the research reported was performed 
at I{IACS under NASA Cooperative Agreement~ Number 
NCC 2-1006. The research described in Section 3 was 
supported by the Defense Advanced Research Projects 
Agency under Con~racl~ N66001-94 C-6046 with the 
Naval Command, Control, and Ocean Surveillance Cen- 
ter. 
SRI International  
333 Ravenswood Ave 
Menlo Park, CA 94025 
*netdecisions 
Well ington House 
East Road 
Cambr idge CB1 1BH 
England 
plicit grammar based on the grammar-writer's 
intuitions. 
If the application is simple enough, experi- 
ence shows that good grammars of this kind 
can be constructed quickly and efficiently using 
commercially available products like ViaVoice 
SDK (IBM 1999) or the Nuance Toolkit (Nu- 
ance 1999). Systems of this kind typically al- 
low specification of some restricted subset of the 
class of context-free grammars, together with 
annotations that permit the grammar-writer to
associate selnantic values with lexical entries 
and rules. This kind of framework is fl:lly ad- 
equate for small grammars. As the gran:mars 
increase in size, however, the limited expres- 
sive power of context-free language notation be- 
conies increasingly burdensome. The grainn:a,r 
tends to beconie large and unwieldy, with many 
rules appearing in multiple versions that con- 
stantly need to be kept in step with each other. 
It represents a large developn:ent cost, is hard 
to maintain, and does not usually port well to 
new applications. 
It is tempting to consider the option of mov- 
ing towards a :::ore expressive grammar tbrmal- 
isln, like unification gramnm.r, writing the orig- 
inal grammar in unification grammar form and 
coml)iling it down to the context-free notation 
required by the underlying toolkit. At least 
one such system (Gemilfi; (Moore ct al 1997)) 
has been implemented and used to build suc- 
cessful and non-trivial applications, most no- 
tably ComnmndTalk (Stent ct al 1999). Gem- 
ini accepts a slightly constrained version of the 
unification grammar formalism originally used 
in the Core Language Engine (Alshawi 1992), 
and compiles it into context-free gran:nmrs in 
the GSL formalism supported by the Nuance 
Toolkit. The Nuance Toolkit con:piles GSL 
gran:mars into sets of probabilistic finite state 
670 
gra.phs (PFSGs), which form the final bmguage 
model. 
The relative success of the Gemilfi system 
suggests a new question. Ulfification grammars 
ha.re been used many times to build substantial 
general gramlnars tbr English and other na.tu- 
ra\[ languages, but the language model oriented 
gra.mln~rs o far developed fi)r Gemini (includ- 
ing the one for ColnmandTalk) have a.ll been 
domain-sl)ecific. One naturally wonders how 
feasible it is to take yet another step in the di- 
rection of increased genera.lity; roughly, what 
we want to do is start with a completely gen- 
eral, linguistically motivated gramma.r, combine 
it with a domain-specific lexicon, and compile 
the result down to a domain-specitic context- 
free grammar that can be used as a la.nguage 
model. If this 1)tetra.mine can be rea.lized, it is 
easy to believe that the result would 1)e a.n ex- 
tremely useful methodology tbr rapid construc- 
tion of la.nguage models. It is i lnportant o note 
tha.t there are no obvious theoretical obstacles 
in our way. The clailn that English is context- 
free has been respectable since a.t least the early 
8(Is (Pullum and Gazda.r 1982) 'e, and the idea. 
of using unification grammar as a. compact wa 5, 
of tel)resenting an ulMerlying context-fl'e~e, lan- 
guage is one of the main inotivations for GPSG 
(Gazdar et al1985) and other formalislns based 
on it. The real question is whether the goal is 
practically achievable, given the resource limi- 
tations of current technology. 
In this l)a.1)er, we describe work aimed at the 
target outlined above, in which we used the 
Gemini system (described in more detail in Sec- 
tion 2) to a.ttempt o compile a. va.riety of lin- 
guistically principled unification gralnlna.rs into 
la.ngua.ge lnodels. Our first experiments (Sec- 
tion 3) were pertbrmed on a. large pre-existing 
unification gramlna.r. These were unsuccessful, 
for reasons that were not entirely obvious; in 
order to investigate the prol)lem more system- 
atically, we then conducted a second series of 
experilnents (Section 4), in which we increlnen- 
tally 1)uilt up a smMler gra.lnlna.r. By monitor- 
ing; the behavior of the compilation process and 
the resulting langua.ge model as the gra.lmnar~s 
2~1e m'e aware l, hal, this claim is most~ 1)robably not 
l;rue for natural languages ill gelmraI (lh'csnall cl al 
1987), but furl~hcr discussion of t.his point is beyond I.he 
scope of t, llC paper. 
cover~ge was expanded, we were a.ble to iden- 
tit~ the point a,t which serious problems began 
to emerge (Section 5). In the fina.1 section, we 
summarize and suggest fltrther directions. 
2 Tile Genfini Language Model  
Compi le r  
To lnake the paper nlore self-contained, this sec- 
tion provides some background on the method 
used by Gemini to compile unifica.tion grain- 
mars into CFGs, and then into language mod- 
els. The ha.sic idea. is the obvious one: enu- 
mera.te all possible instantiations of the feal;ures 
in the grammar rules and lexicon entries, and 
thus tra.nsform esch rule and entry in the ()rig- 
inal unification grammar into a set of rules in 
the derived CFG. For this to be possible, the 
relevant fe~ttul'es Inust be constrained so that 
they can only take values in a finite predefined 
range. The finite range restriction is inconve- 
nient for fea.tures used to build semantic repre- 
sentations, and the tbrmalism consequently dis- 
tinguishes syntactic and semantic features; se- 
lmmtic features axe discarded a.t the start of the 
compilation process. 
A naive iml)lelnentation of the basic lnethod 
would be iml)raetical for any but the small- 
est a.nd simplest grammars, and considera.ble 
ingemfity has been expended on various opti- 
mizations. Most importantly, categories axe ex- 
panded in a demand-driven fa.shion, with infer  
lnatiotl being percolated 1)oth t)otton>up (from 
the lexicon) and top-down (fl'om the grammar's 
start symbol). This is done in such a. way 
that potentially valid colnl)inations of feature 
instantiations in rules are successively filtered 
out if they are not licensed by the top-down 
and bottom-ul) constra.ints. Ranges of feature 
values are also kept together when possible, so 
that sets of context-free rules produced by the 
mdve algorithm may in these cases be merged 
into single rules. 
By exploiting the structure of the gram- 
mar a.nd lexicon, the demand-driven expansion 
lnethod can often effect substa.ntial reductions 
in the size of the derived CFG. (For the type 
of grammar we consider in this paper, the re- 
duction is typically by ,~ fa.etor of over 102?). 
The downside is that even an app~trently slnall 
cha.nge in the syntactic t>atures associated with 
a. rule may have a large eIfect on the size of 
671 
the CFG, if it opens up or blocks an impor- 
tant percolation path. Adding or deleting lexi- 
con entries can also have a significant effect on 
the size of the CFG, especially when there are 
only a small number of entries in a given gram- 
matical category; as usual, entries of this type 
behave from a software ngineering standpoint 
like grammar ules. 
The language model compiler also performs 
a number of other non-trivial transformations. 
The most important of these is related to the 
fact that Nuance GSL grammars are not al- 
lowed to contain left-recursive rules, and left- 
recursive unification-grammar rules must con- 
sequently be converted into a non-left-recursive 
fort::. Rules of this type do not however occur 
in the gramlnars described below, and we conse- 
quently omit further description of the method. 
3 Initial Experiments 
Our initial experiments were performed on a 
recent unification grammar in the ATIS (Air 
Travel Information System) domain, developed 
as a linguistically principled grammar with a 
domain-specific lexicon. This grammar was 
cre~ted for an experiment COl::t)aring cover- 
age and recognition performance of a hand- 
written grammar with that of a.uto:::atically de- 
rived recognition language models, as increas- 
ing amounts of data from the ATIS corpus 
were made available for each n:ethod. Exam- 
ples of sentences covered by this gralnlnar are 
"yes", "on friday", "i want to fly from boston 
to denver on united airlines on friday septem- 
ber twenty third", "is the cheapest one way 
fare from boston to denver a morning flight", 
and "what flight leaves earliest from boston to 
san francisco with the longest layover in den- 
ver". Problems obtaining a working recognition 
grammar from the unification grammar ended 
our original experiment prematurely, and led 
us to investigate the factors responsible for the 
poor recognition performance. 
We explored several ikely causes of recogni- 
tion trouble: number of rules, ::umber of vocab- 
ulary items, size of node array, perplexity, and 
complexity of the grammar, measured by aver- 
age and highest number of transitions per graph 
in the PFSG form of the grammar. 
We were able to in:mediately rule out sim- 
ple size metrics as the cause of Nuance's diffi- 
culties with recognition. Our smallest air travel 
grammar had 141 Gemini rules and 1043 words, 
producing a Nuance grammar with 368 rules. 
This compares to the Con:mandTalk grammar, 
which had 1231 Gemini rules and 1771 words, 
producing a Nuance gran:n:ar with 4096 rules. 
To determine whether the number of the 
words in the grammar or the structure of 
the phrases was responsible for the recognition 
problems, we created extreme cases of a Word+ 
grammar (i.e. a grammar that constrains the 
input to be any sequence of the words in the 
vocabulary) and a one-word-per-category gram- 
mar. We found that both of these variants 
of our gralmnar produced reasonable recogni- 
tion, though the Word+ grammar was very in- 
accurate. However, a three-words-per-category 
grammar could not produce snccessflfl speech 
recognition. 
Many thature specifications can lnake a gram- 
mar ::tore accurate, but will also result in a 
larger recognition grammar due to multiplica- 
tion of feature w~lues to derive the categories 
of the eontext-fl'ee grammar. We experimented 
with various techniques of selecting features to 
be retained in the recognition grammar. As de- 
scribed in the previous ection, Gemini's default 
method is to select only syntactic features and 
not consider semantic features in the recogni- 
tion grammar. We experimented with selecting 
a subset of syntactic features to apply and with 
applying only se:nantic sortal features, and no 
syntactic features. None of these grammars pro- 
duced successful speech recognition. 
/.Fro::: these experiments, we were unable to 
isolate any simple set of factors to explain which 
grammars would be problematic for speech 
recognition. However, the numbers of transi- 
tions per graph in a PFSG did seem suggestive 
of a factor. The ATIS grammar had a high of 
1184 transitions per graph, while the semantic 
grammar of CommandTalk had a high of 428 
transitions per graph, and produced very rea- 
sonable speech recognition. 
Still, at; the end of these attempts, it beca.me 
clear that we did not yet know the precise char- 
acteristic that makes a linguistically motivated 
grammar intractable for speech recognition, nor 
the best way to retain the advantages of the 
hand-written grammar approach while provid- 
ing reasonable speech recognition. 
672 
4 Incrementa l  Grammar  
Deve lopment  
In our second series of experiments, we in- 
crelnenta.lly developed a. new grammar front 
s('ra.tch. The new gra.mma.r is basica.lly a s('a.led- 
down and a.dapted version of tile Core Lan- 
guage Engine gramme\ for English (Puhnan 
1!)92; Rayner 1993); concrete development work 
a.nd testing were organized a.round a. speech in- 
terfa c(; to a. set; of functionalities oflhred by a 
simple simula,tion of the Space Shuttle (Rather, 
Hockey gll(l James 2000). Rules and lexical 
entries were added in sma.ll groups, typically 
2-3 rules or 5 10 lexical entries in one incre- 
ment. After each round of exl)a.nsion , we tested 
to make sure that the gramlnar could still 1)e 
compiled into a. usa.bh; recognizer, a.nd a.t sev- 
ere.1 points this suggested changes in our iln- 
1)\]ementation strategy. The rest of this section 
describes tile new grmmnar in nlore detail. 
4.1 Overv iew of  Ru les  
The current versions of the grammar and lexi- 
con contain 58 rules a.nd 30J. Ulfinflectesl entries 
respectively. They (:over the tbllowing phenom- 
el i  :~IZ 
1. Top-level utl;er~tnces: declarative clauses, 
WH-qtlestions, Y-N questions, iml)erat;ives, 
etlil)tical NPs and I)Ps, int(;rject.ions. 
~.. / \ ]  9 \,~ H-lnovement of NPs and PPs. 
3. The fbllowing verb types: intr~nsi- 
tive, silnple transitive, PP con:plen-mnt, 
lnodaJ/a.uxiliary, -ing VP con-q)len:ent, par- 
ticleq-NP complement, sentential comple- 
lnent, embedded question complement. 
4. PPs: simple PP, PP with postposition 
("ago")~ PP lnodifica,tion of VP and NP. 
5. Relat;ive clauses with both relative NP pro- 
1101111 ("tit(; telnperature th,tt I measured )
and relative PP ("the (loci: where I am"). 
6. Numeric determiners, time expressions, 
and postmodification of NP 1)y nun:eric ex- 
pressions. 
7. Constituent conjunction of NPs and 
cl~ulses. 
Tilt following examl)le sentences illustrate 
current covera,ge: 3 '-. , ':how ~d)out scenario 
three.?", "wha, t is the temperature?", "mea- 
sure the pressure a,t flight deck", "go to tile 
crew ha.tch a.nd (:lose it", "what were ten:per- 
a.tttt'e a, nd pressure a.t iifteen oh five?", "is the 
telnpera.ture going ttp'. ~', "do the fi?ed sensors 
sa.y tha.t the pressure is decreasing. , "find out 
when the pressure rea.ched fifteen p s i . . . .  wh~t 1 
is the pressure that you mea.sured?", "wha.t is 
the tempera.lure where you a.re?", ?~(:a.n you find 
out when the fixed sensors ay the temperature 
at flight deck reached thirty degrees celsius?". 
4.2 Unusua l  Features  o f  the  Grammar  
Most of the gramn:~u', as already sta.ted, is 
closely based on the Core Language Eng!ne 
gra.nlnla.r. \?e briefly sllnllna.rize the main di- 
vergences between the two gramnlars. 
4.2.1 I nvers ion  
The new gramlna, r uses a. novel trea.tment of 
inversion, which is p~trtly designed to simplify 
the l)l'ocess of compiling a, fea,ture gl'anllna, r into 
context-free form. The CLE grammar's trea.t- 
l l tent of invers ion uses a, movement account, in 
which the fronted verb is lnoved to its notional 
pla.ce in the VP through a feature. So, tbr 
example, the sentence "is pressure low?" will 
in the origina.1 CLE gramma.r ha.re the phrase- 
structure 
::\[\[iS\]l" \ [p ressure \ ]N / ,  \[\[\]V \[IO\V\]AI),\]\]V'\]'\],'g" 
in whk:h the head of th(, VP is a V gap coin- 
dexed with tile fronted main verb 1,~ . 
Our new gra.mn:ar, in contrast, hal:dles in- 
version without movement, by making the con> 
bination of inverted ver\]) and subject into a. 
VBAR constituent. A binary fea.ture invsubj  
picks o:ll; these VBARs, a.nd there is a. question- 
forma,tion rule of tilt form 
S --> VP : E invsub j=y\ ]  
Continuing the example, the new gram- 
mar a.ssigns this sentence tilt simpler phrase- 
structure 
"\[\[\[is\] v \[press:ire\] N*'\] v .A .  \[\[low\] J\] V.\] S" 
4.2.2 Sorta l  Const ra in ts  
Sortal constra,ints are coded into most gr~un:nnr 
rules as synta.ctic features in a straight-forward 
lna.nner, so they are available to the compilation 
673 
process which constructs the context-free gram- 
mar, ~nd ultimately tile language model. The 
current lexicon allows 11 possible sortal values 
tbr nouns, and 5 for PPs. 
We have taken the rather non-standard step 
of organizing tile rules for PP modification so 
that a VP or NP cannot be modified by two 
PPs  of the same sortal type. The principal mo- 
tivation is to tighten the language model with 
regard to prepositions, which tend to be pho- 
netically reduced and often hard to distinguish 
from other function words. For example, with- 
out this extra constraint we discovered that an 
utterance like 
measure temperature at flight deck 
and lower deck 
would frequently be misrecognized as 
measure temperature at flight deck in 
lower deck 
5 Exper iments  with Incremental  
G r am 111 ar  S 
Our intention when developing the new gram- 
mar was to find out just when problems began 
to emerge with respect to compilation of tan- 
gm~ge models. Our initial hypothesis was that 
these would l)robably become serious if the rules 
for clausal structure were reasonably elaborate; 
we expected that the large number of possible 
ways of combining modal and auxiliary verbs, 
question forlnation, movement, and sentential 
complements would rapidly combine to produce 
an intractably loose language model. Interest- 
ingly, this did not prove to be the case. In- 
stead, the rules which appear to be the primary 
ca.use of difficulties are those relating to relative 
clauses. We describe the main results in Sec- 
tion 5.1; quantitative results on recognizer per- 
tbrmance are presented together in Section 5.2. 
5.1 Main Findings 
We discovered that addition of the single rule 
which allowed relative clause modification of an 
NP had a dr~stic effect on recognizer perfor- 
lnance. The most obvious symptoms were that 
recognition became much slower and the size of 
the recognition process much larger, sometimes 
causing it to exceed resource bounds. The false 
reject rate (the l)roportion of utterances which 
fell below the recognizer's mininmnl confidence 
theshold) also increased substantially, though 
we were surprised to discover no significant in- 
crea.se in the word error rate tbr sentences which 
did produce a recognition result. To investi- 
gate tile cause of these effects, we examined the 
results of perfornfing compilation to GSL and 
PFSG level. The compilation processes are such 
that symbols retain mnemonic names, so that it 
is relatively easy to find GSL rules and gral)hs 
used to recognize phrases of specified gralnmat- 
ical categories. 
At the GSL level, addition of the relative 
clause rule to the original unification grammar 
only increased the number of derived Nuance 
rules by about 15%, from 4317 to 4959. The av- 
erage size of the rules however increased much 
more a. It, is easiest o measure size at the level of 
PFSGs, by counting nodes and transitions; we 
found that the total size of all the graphs had in- 
creased from 48836 nodes and 57195 tra.nsitions 
to 113166 nodes and 140640 transitions, rather 
more than doubling. The increase was not dis- 
tributed evenly between graphs. We extracted 
figures for only the graphs relating to specific 
grammatical categories; this showed that, the 
number of gra.1)hs fbr NPs had increased from 
94 to 258, and lnoreover that the average size 
of each NP graph had increased fronl 21 nodes 
and 25.5 transitions to 127 nodes and 165 tra.nsi- 
tions, a more than sixfold increase. The graphs 
for clause (S) phrases had only increased in 
number froln 53 to 68. They ha.d however also 
greatly increased in average size, from 171 nodes 
and 212 transitions to 445 nodes and 572 tran- 
sitions, or slightly less than a threefold increase. 
Since NP and S are by far the most important 
categories in the grammar, it is not strange that 
these large changes m~tke a great difference to 
the quality of the language model, and indi- 
rectly to that of speech recognition. 
Colnparing the original unification gramlnar 
and the compiled CSL version, we were able to 
make a precise diagnosis. The problem with the 
relative clause rules are that they unify feature 
values in the critical S and NP subgralnlnars; 
this means that each constrains the other, lead- 
ing to the large observed increase in the size 
and complexity of the derived Nuance grammar. 
aGSL rules are written in all notat ion which allows 
disjunction and Klccne star. 
674 
Specifically, agreement ilffbrmation and sortal 
category are shared between the two daugh- 
ter NPs in the relative clause modification rule, 
which is schematically as follows: 
Igp: \[agr=A, sort=S\]  --+ 
NP: \[agr=A, sort=S\] 
REL:\[agr=A, sort=S\]  
These feature settings ~re needed in order to get 
tile right alternation in pairs like 
the robot that *measure/measures 
the teml)erature \[agr\] 
the *deck/teml)era.ture tha.t you 
measured \[sort\] 
We tested our hypothesis by colnlnenting ()lit 
the agr and sor t  features in the above rule. 
This completely solves the main 1)robh;in of ex- 
1)lesion in the size of the PFSG representation; 
tile new version is only very slightly larger than 
tile one with no relative clause rule (50647 nodes 
and 59322 transitions against 48836 nodes and 
57195 transitions) Most inL1)ortantty, there is 
no great increase in the number or average size 
of the NP and S graphs. NP graphs increase in 
number froin 94 to 130, and stay constant in a.v- 
era ge size.; S graphs increase in number f}om 53 
to 64, and actually decrease, in aa;erage size to 
13,5 nodes and 167 transitions. Tests on st)eech 
(l~t;a. show that recognition quality is nea~rly :lie 
sa.me as for the version of the recognizer which 
does not cover relative clauses. Although speed 
is still significantly degraded, the process size 
has been reduced sufficiently that the 1)roblen:s 
with resource bounds disappear. 
It would be rea.sonal)le 1:o expect tim: remov- 
ing the explosion in the PFSG ret)resentation 
would result in mL underconstrained language 
model for the relative clause paxt of the gram- 
mar, causing degraded 1)erformance on utter- 
ances containing a, relative clause. Interestingly, 
this does not appear to hapl)en , though recog- 
nition speed under the new grammar is signif- 
icaatly worse for these utterances COml)ared to 
utterances with no relative clause. 
5.2 Recogn i t ion  Resu l ts  
This section summarizes our empirical recog- 
nition results. With the help of the Nuance 
Toolkit batchrec  tool, we evah:ated three ver- 
sions of the recognizer, which differed only with 
respect to tile language model, no_re ls  used 
the version of the language model derived fl'onI a 
granLn:a.r with the relative clause rule removed; 
re l s  is the version derived from the fltll gram- 
lnar; and un l inked  is the colnl)romise version, 
which keeps the relative clause rule but removes 
the critical features. We constructed a corpus 
of 41 utterances, of mean length 12.1 words. 
The utterances were chosen so that the first, 31 
were within the coverage of all three versions 
of the grammar; the last 10 contained relative 
clauses, and were within the coverage of re :s  
and un: inked but :tot of no_rels .  Each utter- 
anee was recorded by eight different subjects, 
none of whom had participated in development 
of the gra.mmar or recognizers. Tests were run 
on a dual-processor SUN Ultra60 with 1.5 GB 
of RAM. 
The recognizer was set, to reject uttera.nces if 
their a.ssociated confidence measure fell under 
the default threshold. Figures 1 and 2 sum- 
marize the re.suits for the first 31 utterances 
(no relative clauses) and the last 10 uttera:Lces 
(relative clauses) respectively. Under '?RT', 
we give inean recognition speed (averaged over 
subjects) e?pressed as a multiple of real time; 
'PRe.j' gives the false reject rate, the :heart l)er - 
centage of utterances which were reiected ue to 
low confidence measures; 'Me:n' gives the lnean 
1)ercentage of uttera.nces which fhiled due to the. 
recognition process exceeding inemory resource 
bounds; and 'WER,' gives the mean word er- 
ror rate on the sentences that were neither re- 
jected nor failed due to resource bound prob- 
lems. Since the distribution was highly skewed, 
all mea.ns were calculated over the six subjects 
renm.i:fing after exclusion of the extreme high 
and low values. 
Looking first at Figure 1, we see that re l s  is 
clearly inferior to no_re ls  on tile subset of the 
corpus which is within the coverage of both ver- 
sions: nea.rly twice as many utterances are re- 
jected due to low confidence values or resource 
1)roblems, and recognition speed is about five 
times slower, un l inked  is in contrast :tot sig- 
nificantly worse than no_re ls  in terms of recog- 
nition performance, though it is still two and a 
half times slower. 
Figure 2 compares re l s  and un l inked on the 
utterances containing a relative clause. It seems 
reasona.ble to say that recognition performance 
675 
I C4ran"nar I I FR .i I IWER 1 
no_rels 1.04 9.0% - 6.0% 
re l s  4.76 16.1% 1.1% 5.7% 
un l inked  2.60 9.6% - 6.5% 
Figure 1: Evaluation results for 31 utterances 
not containing relative clauses, averaged across 
8 subjects excluding extreme values. 
Grammar xRT FRej Men: WER\ ]  
re l s  4.60 26.7% 1.6% 3.5%\] 
un l inked 5.29 20.0% - 5.4%J 
Figure 2: Evaluation results for i0 utter~mces 
containing relative clauses, averaged across 8 
subjects excluding extreme values. 
is comparable for the two versions: rels has 
lower word error rate, but also rqjects more 
utterances. Recognition speed is marginally 
lower for unl inked,  though it is not clear to us 
whether the difference is significant given the 
high variability of the data. 
6 Conc lus ions  and  Fur ther  
D i rec t ions  j 
We found the results presented above surpris- 
ing and interesting. When we 1)egal: our pro- 
gramme of attempting to compile increasingly 
larger linguistically based unification grammars 
into language models, we had expected to see a 
steady combinatorial increase, which we guessed 
would be most obviously related to complex 
clause structure. This did not turn out to be the 
case. Instead, the serious problems we encoun- 
tered were caused by a small number of crit- 
ical rules, of which the one for relative clause 
modification was by the far the worst. It was 
not immediately obvious how to deal with the 
problem, but a careful analysis revealed a rea- 
sonable con:promise solution, whose only draw- 
back was a significant but undisastrous degra- 
dation in recognition speed. 
It seems optimistic to hope that the rela- 
tive clause problem is the end of the story; the 
obvious way to investigate is by continuing to 
expand the gramlnar in the same incremental 
fashion, and find out what happens next. We 
intend to do this over the next few months, and 
expect in due course to be able to l)resent fur- 
ther results. 
References  
H. Alshawi. 1992. The Core Language Engine. 
Cambridge, Massachusetts: The MIT Press. 
J. Bresnan, R.M. Kapla.n, S. Peters and A. Za- 
enen. Cross-Serial Dependencies in Dutch. 
1987. In W. J. Savitch et al(eds.), The For- 
real Complexity of Natural Languagc, Reidel, 
Dordrecht, pages 286-319. 
G. Gazdar, E. Klein, G. Pullum and I. Sag. 
1985. Generalized Phrase Structure Gram- 
mar Basil Blackwell. 
IBM. 1999. ViaVoice SDK tbr Windows, ver- 
sion 1.5. 
R. Moore, J. Dowding, H. Bratt, J.M. Gawron, 
Y. Gorfl:, and A. Cheyer. 1997. Com- 
mandTalk: A Spoken-Language Interface 
tbr Battlefield Simulations. Proceedings 
of the Fifth Conference on Applied Nat- 
uraI Languagc Processing, pages 1-7, 
Washington, DC. Available online from 
http ://www. ai. sri. com/natural-language 
/project s/arpa-sl s / commandt alk. html. 
Nuance Communications. 1999. Nuance Speech 
Recognition System Developer's Manv, aI, Ver- 
sion 6.2 
G. Pullum and G. Gazdar. 1982. Natural Lan- 
guages and Context-Free Languages. Lin- 
guistics and Philosophy, 4, pages 471-504. 
S.G. Puhnan. 1992. Unification-Based Synta.c- 
tic Analysis. In (Alshawi 1992) 
M. Rayner. 1993. English Linguistic Coverage. 
In M.S. Agn~s et al 1993. Spoken Language 
Translator: First Year Report. SRI Techni- 
cal Report CRC-043. Available online from 
http ://www. sri. com. 
M. Rayner, B.A. Hockey and F. James. 2000. 
Turning Speech into Scripts. To appear in 
P~vceedings of the 2000 AAAI Spring Sym- 
posium on Natural Language Dialogues with 
Practical Robotic Devices 
A. Stent, J. Dowding, J.M. Gawron, E.O. 
Bratt, and R. Moore. 1999. The Coin- 
mandTalk Spoken Dialogue System. P'rv- 
cecdings of the 37th Annual Meeting of the 
ACL, pages 183-190. Available online from 
ht tp  ://www. a i .  s r i .  com/natura l - language 
/p ro jec t  s /a rpa-s  :s  / commandt a:k.  html. 
676 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 713?720
Manchester, August 2008
Almost Flat Functional Semantics for Speech Translation
Manny Rayner1, Pierrette Bouillon1, Beth Ann Hockey2, Yukie Nakao3
1 University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
Emmanuel.Rayner@issco.unige.ch
Pierrette.Bouillon@issco.unige.ch
2 UCSC UARC, Mail Stop 19-26
NASA Ames Research Center
Moffet Field, CA 94035
bahockey@ucsc.edu
3 University of Nantes, LINA
2, rue de la Houssinie`re
BP 92208 44322 Nantes Cedex 03
yukie.nakao@univ-nantes.fr
Abstract
We introduce a novel semantic represen-
tation formalism, Almost Flat Functional
semantics (AFF), which is designed as an
intelligent compromise between linguis-
tically motivated predicate/argument se-
mantics and ad hoc engineering solutions
based on flat feature/value lists; the cen-
tral idea is to tag each semantic element
with the functional marking which most
closely surrounds it. We argue that AFF is
well-suited for medium-vocabulary speech
translation applications, and describe sim-
ple and general algorithms for parsing,
generating and performing transfer using
AFF representations. The formalism has
been fully implemented within a medium-
vocabulary interlingua-based Open Source
speech translation system which translates
between English, French, Japanese and
Arabic.
1 Introduction
Many speech translation architectures require
some way to represent the meaning of spoken ut-
terances, but even a brief review of the literature
reveals a serious divergence of opinion as to how
this may best be done. At risk of oversimplifying
a little, there are two competing heritages. On the
one hand, there is the mainstream computational
semantics approach, which ultimately goes back
to philosophers like Montague, Russell and Frege
and views predicate calculus as the paradigm rep-
resentation language. On this view of things, a
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
suitable way to represent meaning is to use com-
plex structures, in which components and relation-
ships are based on deep grammatical functions.
Typical ways to realise this strategy are unscoped
logical forms, neo-Davidsonian semantics, mini-
mal recursion semantics, and similar formalisms.
Thus a sentence like ?I want a pepperoni pizza?
might be represented as something like
want1(E, X, Y),
ref(X, pronoun(i)),
quant(Y, indef), pizza1(Y),
pepperoni1(Z), nn(Z, Y)
Approaches based in the linguistic tradition were
dominant about 10 to 15 years ago, when they were
used in major systems like Germany?s Verbmo-
bil (Wahlster, 2000) and SRI?s Spoken Language
Translator (Rayner et al, 2000). They are still
reasonably popular today, as exemplified by major
systems like PARC?s XLE (Riezler et al, 2002).
The competing heritage has its roots in engi-
neering approaches to spoken language systems,
which historically have been intimately connected
with Machine Learning. On this view of things,
a typical semantic representation is a flat list of
feature-value pairs, with the features represent-
ing semantic concepts: here, ?I want a pepperoni
pizza? would be represented as something like
[utterance_type=request,
food=pizza, type=pepperoni]
It is interesting to see how little contact there
has been between these two traditions. Writers
on formal semantics usually treat ad hoc feature-
value representations as not even worthy of serious
discussion. Conversely, proponents of engineer-
ing/machine learning approaches often assume in
practice that all semantic representations will be
some version of a flat feature-value list; a good
713
example of this tendency is Young?s widely cited
2002 survey of machine learning approaches to
spoken dialogue (Young, 2002).
Trying to be as neutral as possible, it is reason-
able to argue that both approaches have important
things to offer, and that it is worth trying to find
some compromise between them. Other things be-
ing equal, flat feature-value representations have
desirable formal properties: they are simple, and
easy to manipulate and reason with. Their draw-
back is that they are an impoverished represen-
tation language, which can lose important infor-
mation. This means that concepts may be im-
possible to represent, or, alternately viewed, that
the representation format may conflate concepts
which we would prefer to distinguish. In the other
direction, hierarchical logic-based representations
are highly expressive, but pose much more serious
challenges in terms of formal manipulability. Al-
though they are more easily capable of represent-
ing semantic distinctions, it is harder to use them to
perform concrete reasoning operations. In transla-
tion systems, these abstract issues manifest them-
selves in a tradeoff between complexity of trans-
lation rules, and ambiguity of semantic represen-
tations. A flat semantic representation formalism
means that translation rules are simple to write;
however, it also means that the semantic represen-
tations they operate on are more likely to be am-
biguous.
In this paper, we will explore the tradeoffs be-
tween the two competing positions outlined above
in the context of a concrete Open Source sys-
tem, the MedSLT medical speech translator. Pre-
vious versions of MedSLT have used a represen-
tation strategy intermediate between the ?logic-
based semantics? and the ?flat semantics? ap-
proaches, though much closer to the ?flat? end of
the scale. We will discuss the strengths and weak-
nesses of the original MedSLT representation for-
malism, and then present a revised version, ?Al-
most Flat Functional Semantics? (AFF). As the
name suggests, AFF incorporates functional mark-
ings, characteristic of a logic-based semantics ap-
proach, into a representation formalism which still
mainly consists of flat list structures. We will show
how grammars using semantics written in AFF can
be compiled into parsers and generators, and de-
scribe a simple formalism that can be used to spec-
ify rules for translating AFF expressions into AFF
expressions. Finally, we will show how use of AFF
in MedSLT has allowed us to address in a princi-
pled way most of the examples which are problem-
atic for the original version of the system, while
still retaining a simple and transparent framework
for writing translation rules.
2 The MedSLT System
MedSLT (Bouillon et al, 2005) is a medium-
vocabulary Open Source speech translation system
for medical domains, implemented using the Open
Source Regulus compiler (Rayner et al, 2006)
and the Nuance recognition platform. Process-
ing is primarily rule-based. Recognition uses a
grammar-based language model, which produces
a source-langage semantic representation. This is
first translated by one set of rules into an interlin-
gual form, and then by a second set into a target
language representation. A target-language gram-
mar, compiled into generation form, turns this into
one or more possible surface strings, after which
a set of generation preferences picks one out. Fi-
nally, the selected string is realised in spoken form.
There is also some use of corpus-based statistical
methods, both to tune the language model (Rayner
et al, 2006, Section 11.5) and to drive a robust em-
bedded help system (Chatzichrisafis et al, 2006).
The treatment of syntactic structure is a care-
fully thought-out compromise between linguistic
and engineering traditions. All grammars used
are extracted from general linguistically motivated
resource grammars, using corpus-based methods
driven by small sets of examples (Rayner et al,
2006, Chapter 9). This results in a simpler and flat-
ter grammar specific to the domain, whose struc-
ture is similar to the ad hoc phrasal grammars typ-
ical of engineering approaches. The treatment of
semantics is however less sophisticated, and ba-
sically represents a minimal approach in the en-
gineering tradition. Each lexicon item contributes
a set of zero or more feature-value pairs (in most
cases exactly one pair). Most of the grammar rules
simply concatenate the sets of pairs received from
their daughters. A small number of rules, primarily
those for subordinate clauses, create a nested sub-
structure representing the embedded clause. Fig-
ure 1 shows an example representation.
It should be obvious from the example that the
flat representation is potentially very ambiguous,
since nearly all information about grammatical
functions has been lost. The example also illus-
trates, however, why this is often unimportant in
714
[[utterance_type,sentence],
[pronoun,vous],
[path_proc,avoir],
[voice,active],
[tense,present],
[cause,nause?e], [sc,quand],
[clause,
[[pronoun,vous],
[symptom,mal],
[path_proc,avoir],
[voice,active],
[tense,present]]])]
Figure 1: Semantic representation produced by the
current MedSLT system for the French sentence
Avez-vous des nause?es quand vous avez mal? (?Do
you have nausea when you have the pain??)
practice. From a purely syntactic point of view,
the fragment
[[pronoun,vous],
[path_proc,avoir],
[cause,nause?e]]
could either represent vous avez des nause?es (?you
have nausea?) or des nause?es vous ont (?nausea
has you?). Except, possibly, in certain kinds of
literary contexts, the second realisation is so im-
plausible that it can be discounted. It is thus rea-
sonable to add sortal constraints to the lexical en-
tries involved, which permit des nause?es to occur
in well-formed utterances as the object of avoir,
but not as its subject. Thus the representation is
in fact unambiguous, and will only generate one
surface realisation.
With the moderate vocabularies used by Med-
SLT (for example, the current French module has
a vocabulary of about 1 100 surface words), the
vast majority of constructions can be rendered un-
ambiguous using similar strategies. The result is
that most translation rules are easy to write, since
they have to do no more than map lists of feature-
value pairs to lists of feature-value pairs. To take
a typical example, the Japanese question itami wa
koutoubu desu ka (?pain-TOPIC back-part-head is-
Q?) receives the representation
[[utterance_type,sentence],
[symptom,itami],
[body_part,koutoubu],
[verb,desu], [tense,present]]
which we wish to map to the interlingua represen-
tation
[[utterance_type,ynq], [verb,be],
[tense,present], [voice,active],
[symptom,pain], [prep,in_loc],
[part,back], [body_part,head]]
(?is the pain in the back of the head?). In a more
expressive semantic framework, the structural mis-
matches here would be non-trivial to resolve. In
the flat MedSLT notation, we only need the fol-
lowing two list-to-list translation rules:
transfer_rule(
[[body_part,koutoubu]],
[[prep,in_loc], [part,back],
[body_part,head]]).
(koutoubu ? ?in the back of the head?) and
transfer_rule(
[[verb, desu]],
[[verb, be]]).
(desu ? ?is?).
As usual, however, we pay a price for simplicity.
In the terminology of Statistical Machine Transla-
tion, what we are essentially doing here is weaken-
ing the channel model, and relying on the strength
of the target language model. This is a reason-
able strategy partly because of the restricted nature
of the domain, and partly because of the fact that
the initial parsing stage makes it possible for us
to work with bags of concepts rather than bags of
words; clearly, bags of concepts are more expres-
sive.
None the less, it is normal to expect the un-
derspecified channel model to cause some prob-
lems, and this indeed proves to be the case. Al-
though most semantic relationships in the domain
are unambiguous even as bags of concepts (?back
of the head? is possible; ?head of the back? isn?t),
there are unpleasant counterexamples. For in-
stance, {?visit?, ?doctor?, ?patient?} can be re-
alised as either ?patient visits doctor? or ?doc-
tor visits patient?. Similarly, {?precede?, ?nau-
sea?, ?headache?} can be either ?nausea precedes
headache? or ?headache precedes nausea?. Cases
like these must be dealt with using ad hoc solu-
tions based on domain pragmatics. In the current
version of the system, ?patient visits doctor? is
forced by producing both surface realisations, and
defining a generation preference. In the case of
{?precede?, ?nausea?, ?headache?}, the problem
is addressed by dividing symptoms into ?primary?
(the symptom the patient is being examined for,
e.g. ?headache?) and ?secondary? (other possibly
715
utterance:[sem=concat(Verb, [[tag, obj, Np]])] -->
verb:[sem=Verb], np:[sem=Np].
np:[sem=concat(Adj, Noun)] -->
spec:[], ?adj:[sem=Adj], noun:[sem=Noun].
np:[sem=concat(Np, PP)] -->
np:[sem=Np], pp:[sem=PP].
pp:[sem=[[tag, Tag, Np]]] -->
prep:[sem=Tag], np:[sem=Np].
verb:[sem=[[action, grasp]]] --> grasp.
noun:[sem=[[thing, block]]] --> block.
noun:[sem=[[loc, table]]] --> table.
adj:[sem=[[colour, red]]] --> red.
spec:[] --> the.
prep:[sem=on] --> on.
Figure 2: Toy grammar with nested predicate-argument semantics.
related symptoms, e.g. ?nausea?). It is reasonable
in practice to assume that the doctor will only be
interested in secondary symptoms that may cause
primary ones, and hence will precede them.
Although each language in the current version
of MedSLT only contains a handful of similar
cases, solutions like those outlined above are both
inelegant and brittle. It would be desirable to find
some more principled way to deal with them; we
would, however, like to do this without sacrificing
the appealing simplicity of the translation rule for-
malism. In the next section, we will show how it is
possible to reconcile these two conflicting goals.
3 Almost Flat Functional Semantics
As we have seen, the problem with a simple bag-
of-concepts representation is its ambiguity; what
we would like to do is find some principled way to
reduce that ambiguity, without greatly increasing
the formalism?s representational complexity. At
this point, a linguistic intuition is helpful. The
bag-of-concepts representation can reasonably be
thought of as an artificial free word-order lan-
guage. There are many natural free word-order
languages; the reason why they are in general no
more ambiguous than fixed word-order languages
is that they use case-marking to convey functional
information which constrains the space of pos-
sible interpretations. For speakers of European
languages, the best-known example will proba-
bly be Classical Latin. For instance, when St.
Jerome wrote Amor ordinem nescit (?love-NOM
order-ACC not-know-PRES-3-SING?), the case-
markings make it clear that he meant ?Love does
not know order? rather than ?Order does not know
love?.
The comparison with free word-order languages
suggests a natural extension of the original bag-
of-concepts representation, where each element is
associated with an additional functional tag which
does the work that a case-marking would do in a
natural free word-order language. It also suggests
a simple construction which can be used to cre-
ate an unordered linear representation that includes
functional tags. We start by defining a standard
nested predicate-argument semantics; we then flat-
ten the representation of each clause S, marking
each primitive semantic element with the imme-
diately surrounding functional tag in S, or with a
null marking if there is no such tag. The resulting
semantic representations still represent each clause
as an unordered list, but in contrast to the MedSLT
bag-of-concepts representation now include func-
tional information. We will call this style of rep-
resentation Almost Flat Functional (AFF) seman-
tics; the ?almost? comes from the fact that there is
still a minimal amount of nested structure, repre-
senting the distinction between main and embed-
ded clauses.
Figures 2 and 3 give a concrete illustration of the
716
[[action, grasp], [null=[action, grasp],
[tag, obj, obj=[colour, red],
[[colour, red], obj=[thing, block],
[thing, block], on=[loc, table]]
[tag, on,
[[loc, table]]]]]]
Figure 3: Construction of AFF representation for ?grasp the red block on the table?. The AFF represen-
tation (right) is a flattened version of the original nested predicate-argument one (left).
AFF construction. Figure 2 presents a toy Regulus
grammar, which allows a few sentences like ?grasp
the red block on the table? and assigns a nested
functional semantics to them. The representations
of most constituents are unordered lists. In the case
of utterance and np, these are formed by con-
catenating the representations of their daughters.
There are two examples of functional markings:
the rule for utterance wraps an [tag, obj
...] around its np daughter, and the rule for pp
wraps a tag around its np daughter, whose label is
determined by the semantic value of the p daugh-
ter.
Figure 3 introduces the AFF construction itself.
The left-hand side of the figure shows the nested
predicate-argument representation of the sentence,
in which elements of the form
[tag, Tag, Arg]
represent tags and their associated arguments. The
right-hand side shows the derived AFF representa-
tion, where each element that is within the scope of
a [tag ...] has been marked with the tag that
would be immediately above it in the nested ver-
sion. Thus the element [loc, table] is inside
the scope of both the obj tag and the on tag; how-
ever, the AFF version assigns it the on tag, since
this is the innermost one.
In the rest of this section, we will describe how
we can parse surface strings into AFF represen-
tations, generate surface strings from AFF repre-
sentations, and define translation rules which map
AFF representations to AFF representations.
3.1 Analysis and generation
For both analysis and generation, the starting point
is a grammar with a nested predicate-argument se-
mantics like the one shown in the left half of Fig-
ure 3. Analysis is straightforward. We first use a
standard parser-generator to compile the grammar
into a parser; the nested predicate-argument repre-
sentations it produces are then subjected to a post-
processing phase, which flattens them in the way
illustrated in the figure.
This simple approach is however not feasible for
generation, since the flattening operation is highly
non-deterministic in the reverse direction; finding
all possible ?unflattenings? and then attempting
to generate from each one would in most cases
be hopelessly inefficient. A better solution is to
transform the original grammar into one with AFF
semantics, where the current functional marking
is specified as an extra features on relevant con-
stituents, and percolated through the rules. In ef-
fect, the ?unflattening? and generation operations
can now proceed simultaneously, with each one
constraining the other.
Figure 4 presents an example, showing the re-
sult of performing this transformation on the toy
Regulus grammar from Figure 2. Here, the origi-
nal [tag, ...] wrappers have been removed,
and replaced by the new feature tag, which has
been added to all constituents whose semantics is
a list of items of the form Tag=Value. The value
of the tag feature on each constituent where it
is defined is the tag for the most closely enclos-
ing [tag, ...] in the original grammar; these
values are percolated down to the lexical rules,
where they unify with the tags on the semantic
fragment contributed by the rule. The transforma-
tion is straightforward to define in its general form,
and the transformed grammars can be readily com-
piled into efficient generators by standard feature-
grammar generator-compiler algorithms like Se-
mantic Head-Driven Generation (Shieber et al,
1990). For the concrete experiments described
later, we used a slightly extended version of the
Open Source Regulus generator compiler.
3.2 Transfer
Our basic strategy for defining transfer between
AFF expressions is to make it as close as pos-
sible to transfer on the original bag-of-concepts
717
utterance:[sem=concat(Verb, Np)] -->
verb:[sem=Verb, tag=null], np:[sem=Np, tag=obj].
np:[sem=concat(Adj, Noun), tag=Tag] -->
spec:[], ?adj:[sem=Adj, tag=Tag], noun:[sem=Noun, tag=Tag].
np:[sem=concat(Np, PP), tag=Tag] -->
np:[sem=Np, tag=Tag], pp:[sem=PP, tag=Tag].
pp:[sem=Np] -->
prep:[sem=Tag], np:[sem=Np, tag=Tag].
verb:[sem=[Tag=[action, grasp]], tag=Tag] --> grasp.
noun:[sem=[Tag=[thing, block]], tag=Tag] --> block.
noun:[sem=[Tag=[loc, table]], tag=Tag] --> table.
adj:[sem=[Tag=[colour, red]], tag=Tag] --> red.
spec:[] --> the.
prep:[sem=on] --> on.
Figure 4: Version of grammar from Figure 2 after transformation to AFF semantics.
representations, which is conditional mapping of
lists to lists. Since AFF is an extension of bag-
of-concepts, and bag-of-concepts is usually suffi-
ciently unambiguous as it stands, we only want
to add the functional markings in the cases where
they are required. Most of our rules will thus still
be transfer rules like the ones shown in Sec-
tion 2, except that they now map lists of function-
marking-tagged items to lists of function-marking-
tagged items; however, in accordance with the
stated design principles, we allow tags to be omit-
ted when desired, with the convention that an omit-
ted tag denotes an uninstantiated tag value.
One of the underlying linguistic intuitions be-
hind AFF is that there are correspondences be-
tween functional markings in different languages,
with each given functional marking f
s
in the
source language typically mapping to a specific
functional marking f
t
in the target language.
For this reason, it would be highly unnatural
only to specify transformations of tag values us-
ing transfer rules. We consequently intro-
duce a second kind of rule, which we call a
tag transfer rule; as the name suggests,
this defines a direct mapping from tags to tags.
Given the fact that functional tags have some claim
to universality, it is reasonable to hope that many
tags will map onto themselves. Thus a typical tag
rule might map the English subj tag to the Arabic
subj tag, which we write as
tag_transfer_rule(subj, subj).
Most tag transfer rules will be of the above
simple form. However, there are always cases
where languages diverge structurally, and here it
will be necessary to make the tag transfer rule con-
ditional on its surrounding context. For example,
English constructions with the verb ?last? (?Does
the headache last more than ten minutes??) are
realised differently in Arabic, using the transitive
verb tahus bi (?feel?), thus here hal tahus bi al
soudaa li akthar min achr daqayq? (?Do (you)
feel the headache during more than ten minutes??).
Here, ?headache? is marked as subj in English,
but the correspoding Arabic word, soudaa, is the
obj of tahus bi. We express the general fact that
we wish to map subj to obj in the context of the
verb ?last? using the rule
tag_transfer_rule(subj, obj) :-
context([state, last]).
We also require a normal transfer rule
which maps ?last? to tahus bi. This also has to
introduce an implicit second person subject, so the
full rule is
transfer_rule(
[[state, last]],
[[state, tahus_bi],
subj=[pronoun, anta]]).
(anta = ?you?). Related sets of rules of this kind
can be written more concisely with a small exten-
718
sion to the formalism, as follows:
transfer_rule(
[[state, last]],
[[state, tahus_bi],
subj=[pronoun, anta]],
[subj:obj]).
An important question we have so far postponed
discussing is how to fill in unspecified tag values
on the RHS of a transfer rule application.
At first, we believed that several possible strate-
gies were feasible; rather to our surprise, examina-
tion of some examples convinced us that only one
of these strategies actually made sense. The algo-
rithm is as follows. We assume a transfer -
rule R, whose LHS has successfully matched a
set of tag/concept pairs, and consider the following
cases:
1. R explicitly assigns values to all of the tags
on its RHS. There is nothing more to do.
2. Not all of the tags on the RHS are assigned
values by R. Apply tag transfer rules
to all the matched tags on the LHS which
were not originally assigned values by R, giv-
ing a set of tags {T
1
...T
n
}. There are now two
subcases:
(a) n = 1, i.e. only one transferred tag is
produced. Set the values of all the unin-
stantiated tags on the transferred RHS to
T
1
.
(b) n > 1, i.e. several different transferred
tags are produced. Leave the values of
the ininstantiated tags on the transferred
RHS uninstantiated.
The least obvious part of this is (2a), which
is easier to understand when we consider some
more specific cases. The simplest and most com-
mon example is the case where R is a ?lex-
ical? transfer rule which contains exactly
one tag/concept pair on each side, each tag be-
ing left unspecified. We evidently need to apply
a tag transfer rule to the tag matched by the
single pair on the LHS, to get the value of the tag
attached to the transferred RHS.
To take a slightly more complex case, consider
an English ? Japanese rule which maps the ex-
pression ?back of the head? to the single word
koutoubu. We could write this as
transfer_rule(
[[part, back],
of=[body_part,head]],
[[body_part, koutoubu]])
Here, it is clear that we want to translate the tag
on the source-language pair that matches [part,
back], and assign it to the target-language ele-
ment [body part, koutoubu]. The transla-
tion of the tag of is irrelevant.
4 Using AFF in MedSLT
We have implemented and tested a version of AFF
inside the Open Source MedSLT system, building
AFF versions of the grammars for English, French
Japanese, Arabic and the Interlingua. We also cre-
ated AFF versions of the translation rules between
the four surface languages and the Interlingua, in
both directions. Coverage and performance of the
two versions of the system on development data
were essentially the same; the key differences were
architectural in nature. We now briefly summarise
these differences.
The basic tradeoff is between analysis and gen-
eration on one hand, and translation on the other.
The more expressive AFF formalism implies that
representations are less ambiguous, which means
fewer problems in the analysis and generation
components. The downside is that the translation
rules become more complex. On the positive side,
switching from bag-of-concepts to AFF allowed us
to implement clean solutions to a substantial num-
ber of problems which were previously handled in
an ad hoc manner. As previously noted, English
constructions using verbs like ?precede?, ?cause?,
?accompany?, ?visit? and ?be in contact with? are
in general ambiguous in the bag-of-words repre-
sentation, and had to be solved by artificially con-
straining their arguments; AFF makes it possible
to do this by simply differentiating between subj
and obj tags. Similar considerations applied to
constructions in the other two languages. For ex-
ample, using bag-of-words, the Arabic frequency
expressions thalath marrat fi al ousbou (?three
times a week?) and marra kul thalathat assabii
(?once every three weeks?) were previously rep-
resented in the same way, necessitating addition of
a brittle generation preference. AFF once again
allows the two expressions to be cleanly distin-
guished.
It was evident from the start that we would
win on this kind of example; what was less clear
719
was the price we would have to pay, in terms
of increased complexity of the transfer rule set.
Gratifyingly, the conservative nature of the ex-
tension meant that this price turned out to be
quite low. We had originally wondered whether
it would be necessary to write many condi-
tional tag transfer rules, or add functional
tags to a large proportion of the transfer -
rules. In fact, out of the total of 4444 rules
used by the eight language pairs together, only
39 (0.9%) were conditional tag transfer -
rules, and 524 (11.8%) were transfer rules
containing at least one functional tag. A fur-
ther 120 rules (2.7%) were unconditional tag -
transfer rules. The remaining 3761 rules
(84.6%) were transfer rules which did not
explicitly mention functional tags, and were thus
essentially bag-of-concepts mapping rules. To
summarise, less than a sixth of the rules were af-
fected by moving to the new framework.
5 Summary and Conclusions
We have described Almost Flat Functional seman-
tics, a formalism which adds functional markings
to a flat atheoretical feature/value representation.
The additional functional information in AFF is
sufficient to resolve nearly all of the representa-
tional ambiguities which caused problems for the
flat bag-of-concepts formalism. In terms of repre-
sentational complexity, however, the AFF formal-
ism appears to be only slightly less tractable than
bag-of-concepts. It seems reasonable to us that,
like bag-of-concepts, it could also support learn-
able surface-oriented parsing; this could be com-
bined with statistical recognition to provide a ro-
bust back-up to grammar-based speech processing
(Rayner et al, 2005), a claim that we hope to inves-
tigate empirically in the near future. It is much less
clear that full logic-based representations could be
used for such purposes.
What we find interesting here, from a general
perspective, is that we were able to create a re-
duced, but still essentially clean, form of a main-
stream linguistic treatment, and incorporate it into
an ad hoc engineering framework in a way that
only marginally affected that framework?s perfor-
mance characteristics. Without wishing to exag-
gerate the importance of our results, we think ex-
amples like AFF suggest that the gulf between
these two types of approach is not, perhaps, as
wide as is sometimes suggested.
References
Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
Machine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
Chatzichrisafis, N., P. Bouillon, M. Rayner, M. Santa-
holma, M. Starlander, and B.A. Hockey. 2006. Eval-
uating task performance for a unidirectional con-
trolled language medical speech translation system.
In Proceedings of the HLT-NAACL International
Workshop on Medical Speech Translation, pages 9?
16, New York.
Rayner, M., D. Carter, P. Bouillon, V. Digalakis, and
M. Wire?n, editors. 2000. The Spoken Language
Translator. Cambridge University Press.
Rayner, M., P. Bouillon, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, H. Isahara,
K. Kanzaki, and Y. Nakao. 2005. A methodol-
ogy for comparing grammar-based and robust ap-
proaches to speech understanding. In Proceedings
of the 9th International Conference on Spoken Lan-
guage Processing (ICSLP), pages 1103?1107, Lis-
boa, Portugal.
Rayner, M., B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
Riezler, S., T.H. King, R.M. Kaplan, R. Crouch, J.T.
Maxwell, and M. Johnson. 2002. Parsing the
wall street journal using a lexical-functional gram-
mar and discriminative estimation techniques. In
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (demo track),
Philadelphia, PA.
Shieber, S., G. van Noord, F.C.N. Pereira, and R.C.
Moore. 1990. Semantic-head-driven generation.
Computational Linguistics, 16(1).
Wahlster, W., editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
Young, S. 2002. Talking to machines (statistically
speaking). In Proceedings of the 7th International
Conference on Spoken Language Processing (IC-
SLP), pages 9?16, Denver, CO.
720
147
148
149
150
151
152
153
154
299
300
301
302
303
304
305
306
187
188
189
190
223
224
225
226
 
	ffPractical Issues in Compiling Typed Unification Grammars for Speech
Recognition
John Dowding Beth Ann Hockey
RIACS RIALIST Group
NASA Ames Research Center
Moffett Field, CA 94035
jdowding@riacs.edu
bahockey@riacs.edu
Jean Mark Gawron
Dept. of Linguistics
San Diego State University
San Diego, CA
gawron@mail.sdsu.edu
Christopher Culy
SRI International
333 Ravenswood Avenue
Menlo Park, CA 94025
culy@ai.sri.com
Abstract
Current alternatives for language mod-
eling are statistical techniques based
on large amounts of training data, and
hand-crafted context-free or finite-state
grammars that are difficult to build
and maintain. One way to address
the problems of the grammar-based ap-
proach is to compile recognition gram-
mars from grammars written in a more
expressive formalism. While theoreti-
cally straight-forward, the compilation
process can exceed memory and time
bounds, and might not always result in
accurate and efficient speech recogni-
tion. We will describe and evaluate two
approaches to this compilation prob-
lem. We will also describe and evalu-
ate additional techniques to reduce the
structural ambiguity of the language
model.
1 Introduction
Language models to constrain speech recogni-
tion are a crucial component of interactive spo-
ken language systems. The more varied the lan-
guage that must be recognized, the more critical
good language modeling becomes. Research in
language modeling has heavily favored statisti-
cal approaches (Cohen 1995, Ward 1995, Hu et
al. 1996, Iyer and Ostendorf 1997, Bellegarda
1999, Stolcke and Shriberg 1996) while hand-
coded finite-state or context-free language models
dominate the commercial sector (Nuance 2001,
SpeechWorks 2001, TellMe 2001, BeVocal 2001,
HeyAnita 2001, W3C 2001). The difference re-
volves around the availability of data. Research
systems can achieve impressive performance us-
ing statistical language models trained on large
amounts of domain-targeted data, but for many
domains sufficient data is not available. Data may
be unavailable because the domain has not been
explored before, the relevant data may be con-
fidential, or the system may be designed to do
new functions for which there is no human-human
analog interaction. The statistical approach is un-
workable in such cases for both the commercial
developers and for some research systems (Moore
et al 1997, Rayner et al 2000, Lemon et al
2001, Gauthron and Colineau 1999). Even in
cases for which there is no impediment to col-
lecting data, the expense and time required to col-
lect a corpus can be prohibitive. The existence
of the ATIS database (Dahl et al 1994) is no
doubt a factor in the popularity of the travel do-
main among the research community for exactly
this reason.
A major problem with grammar-based finite-
state or context-free language models is that they
can be tedious to build and difficult to maintain,
as they can become quite large very quickly as
the scope of the grammar increases. One way
to address this problem is to write the gram-
mar in a more expressive formalism and gener-
ate an approximation of this grammar in the for-
mat needed by the recognizer. This approach
has been used in several systems, CommandTalk
(Moore et al 1997), RIALIST PSA simula-
tor (Rayner et al 2000), WITAS (Lemon et al
2001), and SETHIVoice (Gauthron and Colin-
eau 1999). While theoretically straight-forward,
this approach is more demanding in practice, as
each of the compilation stages contains the po-
tential for a combinatorial explosion that will ex-
ceed memory and time bounds. There is also no
guarantee that the resulting language model will
lead to accurate and efficient speech recognition.
We will be interested in this paper in sound ap-
proximations (Pereira and Wright 1991) in which
the language accepted by the approximation is
a superset of language accepted by the original
grammar. While we conceed that alternative tech-
niques that are not sound (Black 1989, (Johnson
1998, Rayner and Carter 1996) may still be useful
for many purposes, we prefer sound approxima-
tions because there is no chance that the correct
hypothesis will be eliminated. Thus, further pro-
cessing techniques (for instance, N-best search)
will still have an opportunity to find the optimal
solution.
We will describe and evaluate two compilation
approaches to approximating a typed unification
grammar with a context-free grammar. We will
also describe and evaluate additional techniques
to reduce the size and structural ambiguity of the
language model.
2 Typed Unification Grammars
Typed Unification Grammars (TUG), like HPSG
(Pollard and Sag 1994) and Gemini (Dowding et
al. 1993) are a more expressive formalism in
which to write formal grammars1. As opposed to
atomic nonterminal symbols in a CFG, each non-
terminal in a TUG is a complex feature structure
(Shieber 1986) where features with values can be
attached. For example, the rule:
s[]   np:[num=N] vp:[num=N]
can be considered a shorthand for 2 context free
rules (assuming just two values for number):
s
 
np singular vp singular
s
 
np plural vp plural
1This paper specifically concerns grammars written in
the Gemini formalism. However, the basic issues involved in
compiling typed unification grammars to context-free gram-
mars remain the same across formalisms.
This expressiveness allows us to write grammars
with a small number of rules (from dozens to a
few hundred) that correspond to grammars with
large numbers of CF rules. Note that the approx-
imation need not incorporate all of the features
from the original grammar in order to provide a
sound approximation. In particular, in order to de-
rive a finite CF grammar, we will need to consider
only those features that have a finite number of
possible values, or at least consider only finitely
many of the possible values for infinitely valued
features. We can use the technique of restriction
(Shieber 1985) to remove these features from our
feature structures. Removing these features may
give us a more permissive language model, but it
will still be a sound approximation.
The experimental results reported in this pa-
per are based on a grammar under development
at RIACS for a spoken dialogue interface to a
semi-autonomous robot, the Personal Satellite
Assistant (PSA). We consider this grammar to be
medium-sized, with 61 grammar rules and 424
lexical entries. While this may sound small, if
the grammar were expanded by instantiating vari-
ables in all legal permutations, it would contain
over 	 context-free rules.
3 The Compilation Process
We will be studying the compilation process
to convert typed unification grammars expressed
in Gemini notation into language models for
use with the Nuance speech recognizer (Nuance,
2001). We are using Nuance in part because it
supports context-free language models, which is
not yet industry standard.2 Figure 1 illustrates the
stages of processing: a typed unification grammar
is first compiled to a context-free grammar. This
is in turn converted into a grammar in Nuance?s
Grammar Specification Language (GSL), which
is a form of context-free grammar in a BNF-like
notation, with one rule defining each nonterminal,
and allowing alternation and Kleene closure on
the right-hand-side. Critically, the GSL must not
contain any left-recursion, which must be elimi-
nated before the GSL representation is produced.
2The standard is moving in the direction of context-
free language models, as can be seen in the draft standard
for Speech Recognition Grammars being developed by the
World Wide Web Consortium (W3C 2001).
Context Free Grammar
TUG to CFG Compiler
nuance_compiler
GSL Grammar
CFG to GSL Conversion
Recognition System
        Package
Typed Unification Grammar (TUG)
Figure 1: Compilation Process
The GSL representation is then compiled into a
Nuance package with the nuance compiler.
This package is the input to the speech recognizer.
In our experience, each of the compilation stages,
as well as speech recognition itself, has the po-
tential to lead to a combinatorial explosion that
exceeds practical memory or time bounds.
We will now describe implementations of the
first stage, generating a context-free grammar
from a typed unification grammar, by two differ-
ent algorithms, one defined by Kiefer and Krieger
(2000) and one by Moore and Gawron, described
in Moore (1998) The critical difficulty for both
of these approaches is how to select the set of
derived nonterminals that will appear in the final
CFG.
3.1 Kiefer&Krieger?s Algorithm
The algorithm of Kiefer&Krieger (K&K) divides
this compilation step into two phases: first, the
set of context-free nonterminals is determined by
iterating a bottom-up search until a least fixed-
point is reached; second, this least fixed-point is
used to instantiate the set of context-free produc-


 for each l 
A Limited-Domain English to Japanese Medical Speech Translator
Built Using REGULUS 2
Manny Rayner
Research Institute for Advanced
Computer Science (RIACS),
NASA Ames Research Center,
Moffet Field, CA 94035
mrayner@riacs.edu
Pierrette Bouillon
University of Geneva
TIM/ISSCO,
40, bvd du Pont-d?Arve,
CH-1211 Geneva 4,
Switzerland
pierrette.bouillon@issco.unige.ch
Vol Van Dalsem III
El Camino Hospital
2500 Grant Road
Mountain View, CA 94040
vvandal3@aol.com
Hitoshi Isahara, Kyoko Kanzaki
Communications Research Laboratory
3-5 Hikaridai
Seika-cho, Soraku-gun
Kyoto, Japan 619-0289
{isahara,kanzaki}@crl.go.jp
Beth Ann Hockey
Research Institute for Advanced
Computer Science (RIACS),
NASA Ames Research Center,
Moffet Field, CA 94035
bahockey@riacs.edu
Abstract
We argue that verbal patient diagnosis is a
promising application for limited-domain
speech translation, and describe an ar-
chitecture designed for this type of task
which represents a compromise between
principled linguistics-based processing on
the one hand and efficient phrasal transla-
tion on the other. We propose to demon-
strate a prototype system instantiating this
architecture, which has been built on top
of the Open Source REGULUS 2 platform.
The prototype translates spoken yes-no
questions about headache symptoms from
English to Japanese, using a vocabulary of
about 200 words.
1 Introduction and motivation
Language is crucial to medical diagnosis. Dur-
ing the initial evaluation of a patient in an emer-
gency department, obtaining an accurate history of
the chief complaint is of equal importance to the
physical examination. In many parts of the world
there are large recent immigrant populations that re-
quire medical care but are unable to communicate
fluently in the local language. In the US these im-
migrants are especially likely to use emergency fa-
cilities because of insurance issues. In an emer-
gency setting there is acute need for quick accurate
physician-patient communication but this communi-
cation is made substantially more difficult in cases
where there is a language barrier. Our system is
designed to address this problem using spoken ma-
chine translation.
Designing a spoken translation system to obtain
a detailed medical history would be difficult if not
impossible using the current state of the art. The
reason that the use of spoken translation technol-
ogy is feasible is because what is actually needed in
the emergency setting is more limited. Since medi-
cal histories traditionally are obtained through two-
way physician-patient conversations that are mostly
physician initiative, there is a preestablished limiting
structure that we can follow in designing the trans-
lation system. This structure allows a physician to
sucessfully use one way translation to elicit and re-
strict the range of patient responses while still ob-
taining the necessary information.
Another helpful constraint on the conversational
requirements is that the majority of medical condi-
tions can be initiatlly characterized by a relatively
small number of key questions about quality, quan-
tity and duration of symptoms. For example, key
questions about chest pain include intensity, loca-
tion, duration, quality of pain, and factors that in-
crease or decrease the pain. These answers to these
questions can be sucessfully communicated by a
limited number of one or two word responses (e.g.
yes/no, left/right, numbers) or even gestures (e.g.
pointing to an area of the body). This is clearly a
domain in which the constraints of the task are suf-
ficient for a limited domain, one way spoken trans-
lation system to be a useful tool.
2 An architecture for limited-domain
speech translation
The basic philosophy behind the architecture of the
system is to attempt an intelligent compromise be-
tween fixed-phrase translation on one hand (e.g.
(IntegratedWaveTechnologies, 2002)) and linguisti-
cally motivated grammar-based processing on the
other (e.g. VERBMOBIL (Wahlster, 2000) and Spo-
ken Language Translator (Rayner et al, 2000a)).
At run-time, the system behaves essentially like a
phrasal translator which allows some variation in the
input language. This is close in spirit to the approach
used in most normal phrase-books, which typically
allow ?slots? in at least some phrases (?How much
does ? cost??; ?How do I get to ? ??). However,
in order to minimize the overhead associated with
defining and maintaining large sets of phrasal pat-
terns, these patterns are derived from a single large
linguistically motivated unification grammar; thus
the compile-time architecture is that of a linguisti-
cally motivated system. Phrasal translation at run-
time gives us speed and reliability; the linguistically
motivated compile-time architecture makes the sys-
tem easy to extend and modify.
The runtime system comprises three main mod-
ules. These are respectively responsible for source
language speech recognition, including parsing and
production of semantic representation; transfer and
generation; and synthesis of target language speech.
The speech processing modules (recognition and
synthesis) are implemented on top of the standard
Nuance Toolkit platform (Nuance, 2003). Recogni-
tion is constrained by a CFG language model written
in Nuance Grammar Specification Language (GSL),
which also specifies the semantic representations
produced. This language model is compiled from
a linguistically motivated unification grammar us-
ing the Open Source REGULUS 2 platform (Rayner
et al, 2003; Regulus, 2003); the compilation pro-
cess is driven by a small corpus of examples. The
language processing modules (transfer and genera-
tion) are a suite of simple routines written in SICStus
Prolog. The speech and language processing mod-
ules communicate with each other through a mini-
mal file-based protocol.
The semantic representations on both the source
and target sides are expressed as attribute-value
structures. In accordance with the generally mini-
malistic design philosophy of the project, semantic
representations have been kept as simple as possi-
ble. The basic principle is that the representation of
a clause is a flat list of attribute-value pairs: thus for
example the representation of ?Did your headache
start suddenly?? is the attribute-value list
[[utterance_type,ynq],[tense,past],
[symptom,headache],[state,start],
[manner,suddenly]]
In a broad domain, it is of course trivial to con-
struct examples where this kind of representation
runs into serious problems. In the very narrow do-
main of a phrasebook translator, it has many desir-
able properties. In particular, operations on semantic
representations typically manipulate lists rather than
trees. In a broad domain, we would pay a heavy
price: the lack of structure in the semantic represen-
tations would often make them ambiguous. The very
simple ontology of the phrasebook domain however
means that ambiguity is not a problem; the compo-
nents of a flat list representation can never be de-
rived from more than one functional structure, so
this structure does not need to be explicitly present.
Transfer rules define mappings of sets of attribute-
value pairs to sets of attribute-value pairs; the ma-
jority of the rules map single attribute-value pairs
to single attribute-value pairs. Generation is han-
dled by a small Definite Clause Grammar (DCG),
which converts attribute-value structures into sur-
face strings; its output is passed through a minimal
post-transfer component, which applies a set of rules
which map fixed strings to fixed strings. Speech syn-
thesis is performed either by the Nuance Vocalizer
TTS engine or by concatenation of recorded wave-
files, depending on the output language.
One of the most important questions for a med-
ical translation system is that of reliability; we ad-
dress this issue using the methods of (Rayner and
Bouillon, 2002). The GSL form of the recognition
grammar is run in generation mode using the Nu-
ance generate utility to generate large numbers
of random utterances, all of which are by construc-
tion within system coverage. These utterances are
then processed through the system in batch mode us-
ing all-solutions versions of the relevant processing
algorithms. The results are checked automatically
to find examples where rules are either deficient or
ambiguous. With domains of the complexity under
consideration here, we have found that it is feasible
to refine the rule-sets in this way so that holes and
ambiguities are effectively eliminated.
3 A medical speech translation system
We have built a prototype medical speech transla-
tion system instantiating the functionality outlined
in Section 1 and the architecture of Section 2. The
system permits spoken English input of constrained
yes/no questions about the symptoms of headaches,
using a vocabulary of about 200 words. This is
enough to support most of the standard examina-
tion questions for this subdomain. There are two
versions of the system, producing spoken output in
French and Japanese respectively. Since English ?
Japanese is distinctly the more interesting and chal-
lenging language pair, we will focus on this version.
Speech recognition and source language analy-
sis are performed using REGULUS 2. The grammar
is specialised from the large domain-independent
grammar using the methods sketched in Section 2.
The training corpus has been constructed by hand
from an initial corpus supplied by a medical pro-
fessional; the content of the questions was kept un-
changed, but where necessary the form was revised
to make it more appropriate to a spoken dialogue.
When we felt that it would be difficult to remem-
ber what the canonical form of a question would
be, we added two or three variant forms. For exam-
ple, we permit ?Does bright light make the headache
worse?? as a variant for ?Is the headache aggra-
vated by bright light??, and ?Do you usually have
headaches in the morning?? as a variant for ?Does
the headache usually occur in the morning??. The
current training corpus contains about 200 exam-
ples.
The granularity of the phrasal rules learned by
grammar specialisation has been set so that the con-
stituents in the acquired rules are VBARs, post-
modifier groups, NPs and lexical items. VBARs
may include both inverted subject NPs and adverbs1.
Thus for example the training example ?Are the
headaches usually caused by emotional upset?? in-
duces a top-level rule whose context-free skeleton is
UTT --> VBAR, VBAR, POSTMODS
For the training example, the first VBAR in the in-
duced rule spans the phrase ?are the headaches usu-
ally?, the second VBAR spans the phrase ?caused?,
and the POSTMODS span the phrase ?by emotional
upset?. The same rule could potentially be used to
cover utterances like ?Is the pain sometimes pre-
ceded by nausea?? and ?Is your headache ever as-
sociated with blurred vision??. The same training
example will also induce several lower-level rules,
the least trivial of which are rules for VBAR and
POSTMODS with context-free skeletons
VBAR --> are, NP, ADV
POSTMODS --> P, NP
The grammar specialisation method is described in
full detail in (Rayner et al, 2000b).
With regard to the transfer component, we have
had two main problems to solve. Firstly, it is well-
known that translation from English to Japanese re-
quires major reorganisation of the syntactic form.
Word-order is nearly always completely different,
and category mismatches are very common. It is
mainly for this reason that we chose to use a flat
semantic representation. As long as the domain is
simple enough that the flat representations are un-
ambiguous, transfer can be carried out by mapping
lists of elements into lists of elements. For example,
we translate ?are your headaches caused by fatigue?
as ?tsukare de zutsu ga okorimasu ka? (lit. ?fatigue-
CAUSAL headache-SUBJ occur-PRESENT QUES-
TION?). Here, the source-language representation is
[[utterance_type,ynq],
[tense,present],
[symptom,headache],
[event,cause],
[cause,fatigue]]
and the target-language one is
[[utterance_type,sentence],
[tense,present],
[symptom,zutsu],
1This non-standard definition of VBAR has technical advan-
tages discussed in (Rayner et al, 2000c)
do your headaches often appear at night ?
yoku yoru ni zutsu ga arimasu ka
(often night-AT headache-SUBJ is-PRES-Q)
is the pain in the front of the head ?
itami wa atama no mae no hou desu ka
(pain-TOPIC head-OF front side is-PRES-Q)
did your headache start suddenly ?
zutsu wa totsuzen hajimari mashita ka
(headache-TOPIC sudden start-PRES-Q)
have you had headaches for weeks ?
sushukan zutsu ga tsuzuite imasu ka
(weeks headache-SUBJ have-CONT-PRES-Q)
is the pain usually superficial ?
itsumo itami wa hyomenteki desu ka
(usually pain-SUBJ superficial is-PRES-Q)
is the severity of the headaches increasing ?
zutsu wa hidoku natte imasu ka
(headache-TOPIC severe becoming is-PRES-Q)
Table 1: Examples of utterances covered by the pro-
totype
[event,okoru],[postpos,causal],
[cause,tsukare]]
Each line in the source representation maps into the
corresponding one in the target in the obvious way.
The target-language grammar is constrained enough
that there is only one Japanese sentence which can
be generated from the given representation.
The second major problem for transfer relates to
elliptical utterances. These are very important due
to the one-way character of the interaction: instead
of being able to ask a WH-question (?What does
the pain feel like??), the doctor needs to ask a se-
ries of Y-N questions (?Is the pain dull??, ?Is the
pain burning??, ?Is the pain aching??, etc). We
rapidly found that it was much more natural for
questions after the first one to be phrased ellipti-
cally (?Is the pain dull??, ?Burning??, ?Aching??).
English and Japanese have however different con-
ventions as to what types of phrase can be used
elliptically. Here, for example, it is only pos-
sible to allow some types of Japanese adjectives
to stand alone. Thus we can grammatically and
semantically say ?hageshii desu ka? (lit. ?burn-
ing is-QUESTION?) but not ?*uzukuyona desu
ka? (lit. ?*aching is-QUESTION?). The prob-
lem is that adjectives like ?uzukuyona? must com-
bine adnominally with a noun in this context:
thus we in fact have to generate ?uzukuyona itami
desu ka? (?aching-ADNOMINAL-USAGE pain is-
QUESTION?). Once again, however, the very lim-
ited domain makes it practical to solve the problem
robustly. There are only a handful of transforma-
tions to be implemented, and the extra information
that needs to be added is always clear from the sortal
types of the semantic elements in the target represen-
tation.
Table 1 gives examples of utterances covered by
the system, and the translations produced.
References
IntegratedWaveTechnologies, 2002. http://www.i-w-
t.com/investor.html. As of 15 Mar 2002.
Nuance, 2003. http://www.nuance.com. As of 25 Febru-
ary 2003.
M. Rayner and P. Bouillon. 2002. A phrasebook style
medical speech translator. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (demo track), Philadelphia, PA.
M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and
M. Wire?n, editors. 2000a. The Spoken Language
Translator. Cambridge University Press.
M. Rayner, D. Carter, and C. Samuelsson. 2000b. Gram-
mar specialisation. In Rayner et al (Rayner et al,
2000a).
M. Rayner, B.A. Hockey, and F. James. 2000c. Compil-
ing language models from a linguistically motivated
unification grammar. In Proceedings of the Eighteenth
International Conference on Computational Linguis-
tics, Saarbrucken, Germany.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
Regulus, 2003. http://sourceforge.net/projects/regulus/.
As of 24 April 2003.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
An Intelligent Procedure Assistant
Built Using REGULUS 2 and ALTERF
Manny Rayner, Beth Ann Hockey, Jim Hieronymus, John Dowding, Greg Aist
Research Institute for Advanced Computer Science (RIACS)
NASA Ames Research Center
Moffet Field, CA 94035
{mrayner,bahockey,jimh,jdowding,aist}@riacs.edu
Susana Early
DeAnza College/NASA Ames Research Center
searly@mail.arc.nasa.gov
Abstract
We will demonstrate the latest version of
an ongoing project to create an intelli-
gent procedure assistant for use by as-
tronauts on the International Space Sta-
tion (ISS). The system functionality in-
cludes spoken dialogue control of nav-
igation, coordinated display of the pro-
cedure text, display of related pictures,
alarms, and recording and playback of
voice notes. The demo also exempli-
fies several interesting component tech-
nologies. Speech recognition and lan-
guage understanding have been devel-
oped using the Open Source REGULUS
2 toolkit. This implements an approach
to portable grammar-based language mod-
elling in which all models are derived
from a single linguistically motivated uni-
fication grammar. Domain-specific CFG
language models are produced by first
specialising the grammar using an au-
tomatic corpus-based method, and then
compiling the resulting specialised gram-
mars into CFG form. Translation between
language centered and domain centered
semantic representations is carried out by
ALTERF, another Open Source toolkit,
which combines rule-based and corpus-
based processing in a transparent way.
1 Introduction
Astronauts aboard the ISS spend a great deal of their
time performing complex procedures. This often in-
volves having one crew member reading the proce-
dure aloud, while while the other crew member per-
forms the task, an extremely expensive use of as-
tronaut time. The Intelligent Procedure Assistant is
designed to provide a cheaper alternative, whereby a
voice-controlled system navigates through the pro-
cedure under the control of the astronaut perform-
ing the task. This project has several challenging
features including: starting the project with no tran-
scribed data for the actual target input language, and
rapidly changing coverage and functionality. We
are using REGULUS 2 and ALTERF to address these
challenges. Together, they provide an example-
based framework for constructing the portion of the
system from recognizer through intepretation that
allows us to make rapid changes and take advan-
tage of both rule-base and corpus-based information
sources. In this way, we have been able to extract
maximum utility out of the small amounts of data
initial available to the project and also smoothly ad-
just as more data has been accumulated in the course
of the project.
The following sections describe the procedure as-
sistant application and domain, REGULUS 2 and AL-
TERF.
2 Application and domain
The system, an early version of which was described
in (Aist et al, 2002), is a prototype intelligent voice
enabled personal assistant, intended to support astro-
nauts on the International Space Station in carrying
out complex procedures. The first production ver-
sion is tentatively scheduled for introduction some
time during 2004. The system reads out each pro-
cedure step as it reaches it, using a TTS engine, and
also shows the corresponding text and supplemen-
tary images in a visual display. Core functionality
consists of the following types of commands:
? Navigation: moving to the following step or
substep (?next?, ?next step?, ?next substep?),
going back to the preceding step or substep
(?previous?, ?previous substep?), moving to a
named step or substep (?go to step three?, ?go
to step ten point two?).
? Visiting non-current steps, either to preview fu-
ture steps or recall past ones (?read step four?,
?read note before step nine?). When this func-
tionality is invoked, the non-current step is dis-
played in a separate window, which is closed
on returning to the current step.
? Recording, playing and deleting voice notes
(?record voice note?, ?play voice note on step
three point one?, ?delete voice note on substep
two?).
? Setting and cancelling alarms (?set alrm for
five minutes from now?, ?cancel alarm at ten
twenty one?).
? Showing or hiding pictures (?show the small
waste water bag?, ?hide the picture?).
? Changing the TTS volume (?increase/decrease
volume?).
? Querying status (?where are we?, ?list voice
notes?, ?list alarms?).
? Undoing and correcting commands (?go back?,
?no I said increase volume?, ?I meant step
four?).
The system consists of a set of modules, written
in several different languages, which communicate
with each other through the SRI Open Agent Ar-
chitecture (Martin et al, 1998). Speech recogni-
tion is carried out using the Nuance Toolkit (Nuance,
2003).
3 REGULUS 2
REGULUS 2 (Rayner et al, 2003; Regulus, 2003)
is an Open Source environment that supports effi-
cient compilation of typed unification grammars into
speech recognisers. The basic intent is to provide
a set of tools to support rapid prototyping of spo-
ken dialogue applications in situations where little
or no corpus data exists. The environment has al-
ready been used to build over half a dozen appli-
cations with vocabularies of between 100 and 500
words.
The core functionality provided by the REGU-
LUS 2 environment is compilation of typed unifi-
cation grammars into annotated context-free gram-
mar language models expressed in Nuance Gram-
mar Specification Language (GSL) notation (Nu-
ance, 2003). GSL language models can be con-
verted into runnable speech recognisers by invoking
the Nuance Toolkit compiler utility, so the net result
is the ability to compile a unification grammar into
a speech recogniser.
Experience with grammar-based spoken dialogue
systems shows that there is usually a substantial
overlap between the structures of grammars for dif-
ferent domains. This is hardly surprising, since they
all ultimately have to model general facts about the
linguistic structure of English and other natural lan-
guages. It is consequently natural to consider strate-
gies which attempt to exploit the overlap between
domains by building a single, general grammar valid
for a wide variety of applications. A grammar of this
kind will probably offer more coverage (and hence
lower accuracy) than is desirable for any given spe-
cific application. It is however feasible to address
the problem using corpus-based techniques which
extract a specialised version of the original general
grammar.
REGULUS implements a version of the grammar
specialisation scheme which extends the Explana-
tion Based Learning method described in (Rayner
et al, 2002). There is a general unification gram-
mar, loosely based on the Core Language Engine
grammar for English (Pulman, 1992), which has
been developed over the course of about ten individ-
ual projects. The semantic representations produced
by the grammar are in a simplified version of the
Core Language Engine?s Quasi Logical Form nota-
tion (van Eijck and Moore, 1992).
A grammar built on top of the general grammar is
transformed into a specialised Nuance grammar in
the following processing stages:
1. The training corpus is converted into a ?tree-
bank? of parsed representations. This is done
using a left-corner parser representation of the
grammar.
2. The treebank is used to produce a specialised
grammar in REGULUS format, using the EBL
algorithm (van Harmelen and Bundy, 1988;
Rayner, 1988).
3. The final specialised grammar is compiled into
a Nuance GSL grammar.
4 ALTERF
ALTERF (Rayner and Hockey, 2003) is another Open
Source toolkit, whose purpose is to allow a clean
combination of rule-based and corpus-driven pro-
cessing in the semantic interpretation phase. There
is typically no corpus data available at the start
of a project, but considerable amounts at the end:
the intention behind ALTERF is to allow us to shift
smoothly from an initial version of the system which
is entirely rule-based, to a final version which is
largely data-driven.
ALTERF characterises semantic analysis as a task
slightly extending the ?decision-list? classification
algorithm (Yarowsky, 1994; Carter, 2000). We start
with a set of semantic atoms, each representing a
primitive domain concept, and define a semantic
representation to be a non-empty set of semantic
atoms. For example, in the procedure assistant do-
main we represent the utterances
please speak up
show me the sample syringe
set an alarm for five minutes from now
no i said go to the next step
respectively as
{increase volume}
{show, sample syringe}
{set alrm, 5, minutes}
{correction, next step}
where increase volume, show,
sample syringe, set alrm, 5, minutes,
correction and next step are semantic
atoms. As well as specifying the permitted semantic
atoms themselves, we also define a target model
which for each atom specifies the other atoms with
which it may legitimately combine. Thus here, for
example, correction may legitimately combine
with any atom, but minutes may only combine
with correction, set alrm or a number.1.
Training data consists of a set of utterances, in
either text or speech form, each tagged with its in-
tended semantic representation. We define a set of
feature extraction rules, each of which associates an
utterance with zero or more features. Feature ex-
traction rules can carry out any type of processing.
In particular, they may involve performing speech
recognition on speech data, parsing on text data, ap-
plication of hand-coded rules to the results of pars-
ing, or some combination of these. Statistics are
then compiled to estimate the probability p(a | f)
of each semantic atom a given each separate feature
f , using the standard formula
p(a | f) = (Naf + 1)/(Nf + 2)
where Nf is the number of occurrences in the train-
ing data of utterances with feature f , and N af is the
number of occurrences of utterances with both fea-
ture f and semantic atom a.
The decoding process follows (Yarowsky, 1994)
in assuming complete dependence between the fea-
tures. Note that this is in sharp contrast with the
Naive Bayes classifier (Duda et al, 2000), which as-
sumes complete independence. Of course, neither
assumption can be true in practice; however, as ar-
gued in (Carter, 2000), there are good reasons for
preferring the dependence alternative as the better
option in a situation where there are many features
extracted in ways that are likely to overlap.
We are given an utterance u, to which we wish to
assign a representation R(u) consisting of a set of
semantic atoms, together with a target model com-
prising a set of rules defining which sets of seman-
1The current system post-processes Alterf semantic atom
lists to represent domain dependancies between semantic
atoms more directly before passing on the result. e.g.
(correction, set alrm, 5, minutes) is repack-
aged as (correction(set alrm(time(0,5))))
tic atoms are consistent. The decoding process pro-
ceeds as follows:
1. Initialise R(u) to the empty set.
2. Use the feature extraction rules and the statis-
tics compiled during training to find the set of
all triples ?f, a, p? where f is a feature associ-
ated with u, a is a semantic atom, and p is the
probability p(a | f) estimated by the training
process.
3. Order the set of triples by the value of p, with
the largest probabilities first. Call the ordered
set T .
4. Remove the highest-ranked triple ?f, a, p? from
T . Add a to R(u) iff the following conditions
are fulfilled:
? p ? pt for some pre-specified threshold
value pt.
? Addition of a to R(u) results in a set
which is consistent with the target model.
5. Repeat step (4) until T is empty.
Intuitively, the process is very simple. We just
walk down the list of possible semantic atoms, start-
ing with the most probable ones, and add them to
the semantic representation we are building up when
this does not conflict with the consistency rules in
the target model. We stop when the atoms suggested
are too improbable, that is, they have probabilies be-
low a cut-off threshold.
5 Summary and structure of demo
We have described a non-trivial spoken language di-
alogue application built using generic Open Source
tools that combine rule-based and corpus-driven
processing. We intend to demo the system with par-
ticular reference to these tools, displaying intermedi-
ate results of processing and showing how the cover-
age can be rapidly reconfigured in an example-based
fashion.
References
G. Aist, J. Dowding, B.A. Hockey, and J. Hieronymus.
2002. An intelligent procedure assistant for astro-
naut training and support. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (demo track), Philadelphia, PA.
D. Carter. 2000. Choosing between interpretations. In
M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and
M. Wire?n, editors, The Spoken Language Translator.
Cambridge University Press.
R.O. Duda, P.E. Hart, and H.G. Stork. 2000. Pattern
Classification. Wiley, New York.
D. Martin, A. Cheyer, and D. Moran. 1998. Building
distributed software systems with the open agent ar-
chitecture. In Proceedings of the Third International
Conference on the Practical Application of Intelligent
Agents and Multi-Agent Technology, Blackpool, Lan-
cashire, UK.
Nuance, 2003. http://www.nuance.com. As of 25 Febru-
ary 2003.
S.G. Pulman. 1992. Syntactic and semantic process-
ing. In H. Alshawi, editor, The Core Language En-
gine, pages 129?148. MIT Press, Cambridge, Mas-
sachusetts.
M. Rayner and B.A. Hockey. 2003. Transparent com-
bination of rule-based and data-driven approaches in a
speech understanding architecture. In Proceedings of
the 10th EACL, Budapest, Hungary.
M. Rayner, B.A. Hockey, and J. Dowding. 2002. Gram-
mar specialisation meets language modelling. In Pro-
ceedings of the 7th International Conference on Spo-
ken Language Processing (ICSLP), Denver, CO.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
M. Rayner. 1988. Applying explanation-based general-
ization to natural-language processing. In Proceedings
of the International Conference on Fifth Generation
Computer Systems, pages 1267?1274, Tokyo, Japan.
Regulus, 2003. http://sourceforge.net/projects/regulus/.
As of 24 April 2003.
J. van Eijck and R. Moore. 1992. Semantic rules for
English. In H. Alshawi, editor, The Core Language
Engine, pages 83?116. MIT Press.
T. van Harmelen and A. Bundy. 1988. Explanation-
based generalization = partial evaluation (research
note). Artificial Intelligence, 36:401?412.
D. Yarowsky. 1994. Decision lists for lexical ambiguity
resolution. In Proceedings of the 32nd Annual Meet-
ing of the Association for Computational Linguistics,
pages 88?95, Las Cruces, New Mexico.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 29?32, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Voice Enabled Procedure Browser
for the International Space Station
Manny Rayner, Beth Ann Hockey, Nikos Chatzichrisafis, Kim Farrell
ICSI/UCSC/RIACS/NASA Ames Research Center
Moffett Field, CA 94035?1000
mrayner@riacs.edu, bahockey@email.arc.nasa.gov
Nikos.Chatzichrisafis@web.de, kfarrell@email.arc.nasa.gov
Jean-Michel Renders
Xerox Research Center Europe
6 chemin de Maupertuis, Meylan, 38240, France
Jean-Michel.Renders@xrce.xerox.com
Abstract
Clarissa, an experimental voice enabled
procedure browser that has recently been
deployed on the International Space Sta-
tion (ISS), is to the best of our knowl-
edge the first spoken dialog system in
space. This paper gives background
on the system and the ISS procedures,
then discusses the research developed to
address three key problems: grammar-
based speech recognition using the Regu-
lus toolkit; SVM based methods for open
microphone speech recognition; and ro-
bust side-effect free dialogue management
for handling undos, corrections and con-
firmations.
1 Overview
Astronauts on the International Space Station (ISS)
spend a great deal of their time performing com-
plex procedures. Crew members usually have to
divide their attention between the task and a pa-
per or PDF display of the procedure. In addition,
since objects float away in microgravity if not fas-
tened down, it would be an advantage to be able
to keep both eyes and hands on the task. Clarissa,
an experimental speech enabled procedure navigator
(Clarissa, 2005), is designed to address these prob-
lems. The system was deployed on the ISS on Jan-
uary 14, 2005 and is scheduled for testing later this
year; the initial version is equipped with five XML-
encoded procedures, three for testing water quality
and two for space suit maintenance. To the best of
our knowledge, Clarissa is the first spoken dialogue
application in space.
The system includes commands for navigation:
forward, back, and to arbitrary steps. Other com-
mands include setting alarms and timers, record-
ing, playing and deleting voice notes, opening and
closing procedures, querying system status, and in-
putting numerical values. There is an optional mode
that aggressively requests confirmation on comple-
tion of each step. Open microphone speech recog-
nition is crucial for providing hands free use. To
support this, the system has to discriminate between
speech that is directed to it and speech that is not.
Since speech recognition is not perfect, and addi-
tional potential for error is added by the open micro-
phone task, it is also important to support commands
for undoing or correcting bad system responses.
The main components of the Clarissa system are
a speech recognition module, a classifier for exe-
cuting the open microphone accept/reject decision,
a semantic analyser, and a dialogue manager. The
rest of this paper will briefly give background on the
structure of the procedures and the XML representa-
tion, then describe the main research content of the
system.
2 Voice-navigable procedures
ISS procedures are formal documents that typically
represent many hundreds of person hours of prepa-
ration, and undergo a strict approval process. One
requirement in the Clarissa project was that the pro-
cedures should be displayed visually exactly as they
29
Figure 1: Adding voice annotations to a group of
steps
appear in the original PDF form. However, reading
these procedures verbatim would not be very useful.
The challenge is thus to let the spoken version di-
verge significantly from the written one, yet still be
similar enough in meaning that the people who con-
trol the procedures can be convinced that the two
versions are in practice equivalent.
Figure 1 illustrates several types of divergences
between the written and spoken versions, with
?speech bubbles? showing how procedure text is ac-
tually read out. In this procedure for space suit main-
tenance, one to three suits can be processed. The
group of steps shown cover filling of a ?dry LCVG?.
The system first inserts a question to ask which suits
require this operation, and then reads the passage
once for each suit, specifying each time which suit is
being referred to; if no suits need to be processed, it
jumps directly to the next section. Step 51 points the
user to a subprocedure. The spoken version asks if
the user wants to execute the steps of the subproce-
dure; if so, it opens the LCVG Water Fill procedure
and goes directly to step 6. If the user subsequently
goes past step 17 of the subprocedure, the system
warns that the user has gone past the required steps,
and suggests that they close the procedure.
Other important types of divergences concern en-
try of data in tables, where the system reads out an
appropriate question for each table cell, confirms the
value supplied by the user, and if necessary warns
about out-of-range values.
Rec Patterns Errors
Reject Bad Total
Text LF 3.1% 0.5% 3.6%
Text Surface 2.2% 0.8% 3.0%
Text Surface+LF 0.8% 0.8% 1.6%
SLM Surface 2.8% 7.4% 10.2%
GLM LF 1.4% 4.9% 6.3%
GLM Surface 2.9% 4.8% 7.7%
GLM Surface+LF 1.0% 5.0% 6.0%
Table 1: Speech understanding performance on six
different configurations of the system.
3 Grammar-based speech understanding
Clarissa uses a grammar-based recognition architec-
ture. At the start of the project, we had two main rea-
sons for choosing this approach over the more popu-
lar statistical one. First, we had no available training
data. Second, the system was to be designed for ex-
perts who would have time to learn its coverage, and
who moreover, as former military pilots, were com-
fortable with the idea of using controlled language.
Although there is not much to be found in the litera-
ture, an earlier study in which we had been involved
(Knight et al, 2001) suggested that grammar-based
systems outperformed statistical ones for this kind
of user. Given that neither of the above arguments is
very strong, we wanted to implement a framework
which would allow us to compare grammar-based
methods with statistical ones, and retain the option
of switching from a grammar-based framework to a
statistical one if that later appeared justified. The
Regulus and Alterf platforms, which we have devel-
oped under Clarissa and other earlier projects, are
designed to meet these requirements.
The basic idea behind Regulus (Regulus, 2005;
Rayner et al, 2003) is to extract grammar-based lan-
guage models from a single large unification gram-
mar, using example-based methods driven by small
corpora. Since grammar construction is now a
corpus-driven process, the same corpora can be used
to build statistical language models, facilitating a di-
rect comparison between the two methodologies.
On its own, however, Regulus only permits com-
parison at the level of recognition strings. Alterf
(Rayner and Hockey, 2003) extends the paradigm to
30
ID Rec Features Classifier Error rates
Classification Task
In domain Out Av
Good Bad
1 SLM Confidence Threshold 5.5% 59.1% 16.5% 11.8% 10.1%
2 GLM Confidence Threshold 7.1% 48.7% 8.9% 9.4% 7.0%
3 SLM Confidence + Lexical Linear SVM 2.8% 37.1% 9.0% 6.6% 7.4%
4 GLM Confidence + Lexical Linear SVM 2.8% 48.5% 8.7% 6.3% 6.2%
5 SLM Confidence + Lexical Quadratic SVM 2.6% 23.6% 8.5% 5.5% 6.9%
6 GLM Confidence + Lexical Quadratic SVM 4.3% 28.1% 4.7% 5.5% 5.4%
Table 2: Performance on accept/reject classification and the top-level task, on six different configurations.
the semantic level, by providing a trainable seman-
tic interpretation framework. Interpretation uses a
set of user-specified patterns, which can match ei-
ther the surface strings produced by both the statisti-
cal and grammar-based architectures, or the logical
forms produced by the grammar-based architecture.
Table 1 presents the result of an evaluation, car-
ried out on a set of 8158 recorded speech utterances,
where we compared the performance of a statisti-
cal/robust architecture (SLM) and a grammar-based
architecture (GLM). Both versions were trained off
the same corpus of 3297 utterances. We also show
results for text input simulating perfect recognition.
For the SLM version, semantic representations are
constructed using only surface Alterf patterns; for
the GLM and text versions, we can use either sur-
face patterns, logical form (LF) patterns, or both.
The ?Error? columns show the proportion of ut-
terances which produce no semantic interpretation
(?Reject?), the proportion with an incorrect seman-
tic interpretation (?Bad?), and the total.
Although the WER for the GLM recogniser is
only slightly better than that for the SLM recogniser
(6.27% versus 7.42%, 15% relative), the difference
at the level of semantic interpretation is considerable
(6.3% versus 10.2%, 39% relative). This is most
likely accounted for by the fact that the GLM ver-
sion is able to use logical-form based patterns, which
are not accessible to the SLM version. Logical-form
based patterns do not appear to be intrinsically more
accurate than surface (contrast the first two ?Text?
rows), but the fact that they allow tighter integration
between semantic understanding and language mod-
elling is intuitively advantageous.
4 Open microphone speech processing
The previous section described speech understand-
ing performance in terms of correct semantic inter-
pretation of in-domain input. However, open micro-
phone speech processing implies that some of the in-
put will not be in-domain. The intended behaviour
for the system is to reject this input. We would
also like it, when possible, to reject in-domain input
which has not been correctly recognised.
Surface output from the Nuance speech recog-
niser is a list of words, each tagged with a confidence
score; the usual way to make the accept/reject deci-
sion is by using a simple threshold on the average
confidence score. Intuitively, however, we should be
able to improve the decision quality by also taking
account of the information in the recognised words.
By thinking of the confidence scores as weights,
we can model the problem as one of classifying doc-
uments using a weighted bag of words model. It
is well known (Joachims, 1998) that Support Vec-
tor Machine methods are very suitable for this task.
We have implemented a version of the method de-
scribed by Joachims, which significantly improves
on the naive confidence score threshold method.
Performance on the accept/reject task can be eval-
uated directly in terms of the classification error. We
can also define a metric for the overall speech under-
standing task which includes the accept/reject deci-
sion, as a weighted loss function over the different
types of error. We assign weights of 1 to a false re-
ject of a correct interpretation, 2 to a false accept of
an incorrectly interpreted in-domain utterance, and 3
to a false accept of an out-of-domain utterance. This
31
captures the intuition that correcting false accepts is
considerably harder than correcting false rejects, and
that false accepts of utterances not directed at the
system are worse than false accepts of incorrectly
interpreted utterances.
Table 2 summarises the results of experiments
comparing performance of different recognisers and
accept/reject classifiers on a set of 10409 recorded
utterances. ?GLM? and ?SLM? refer respectively to
the best GLM and SLM recogniser configurations
from Table 1. ?Av? refers to the average classi-
fier error, and ?Task? to a normalised version of the
weighted task metric. The best SVM-based method
(line 6) outperforms the best naive threshold method
(line 2) by 5.4% to 7.0% on the task metric, a relative
improvement of 23%. The best GLM-based method
(line 6) and the best SLM-based method (line 5) are
equally good in terms of accept/reject classification
accuracy, but the GLM?s better speech understand-
ing performance means that it scores 22% better on
the task metric. The best quadratic kernel (line 6)
outscores the best linear kernel (line 4) by 13%. All
these differences are significant at the 5% level ac-
cording to the Wilcoxon matched-pairs test.
5 Side-effect free dialogue management
In an open microphone spoken dialogue application
like Clarissa, it is particularly important to be able
to undo or correct a bad system response. This
suggests the idea of representing discourse states
as objects: if the complete dialogue state is an ob-
ject, a move can be undone straightforwardly by
restoring the old object. We have realised this idea
within a version of the standard ?update seman-
tics? approach to dialogue management (Larsson
and Traum, 2000); the whole dialogue management
functionality is represented as a declarative ?update
function? relating the old dialogue state, the input
dialogue move, the new dialogue state and the out-
put dialogue actions.
In contrast to earlier work, however, we include
task information as well as discourse information in
the dialogue state. Each state also contains a back-
pointer to the previous state. As explained in detail
in (Rayner and Hockey, 2004), our approach per-
mits a very clean and robust treatment of undos, cor-
rections and confirmations, and also makes it much
simpler to carry out systematic regression testing of
the dialogue manager component.
Acknowledgements
Work at ICSI, UCSC and RIACS was supported
by NASA Ames Research Center internal fund-
ing. Work at XRCE was partly supported by the
IST Programme of the European Community, un-
der the PASCAL Network of Excellence, IST-2002-
506778. Several people not credited here as co-
authors also contributed to the implementation of
the Clarissa system: among these, we would par-
ticularly like to mention John Dowding, Susana
Early, Claire Castillo, Amy Fischer and Vladimir
Tkachenko. This publication only reflects the au-
thors? views.
References
Clarissa, 2005. http://www.ic.arc.nasa.gov/projects/clarissa/.
As of 26 April 2005.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proceedings of the 10th European Conference on
Machine Learning, Chemnitz, Germany.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: a case
study. In Proceedings of Eurospeech 2001, pages
1779?1782, Aalborg, Denmark.
S. Larsson and D. Traum. 2000. Information state and
dialogue management in the TRINDI dialogue move
engine toolkit. Natural Language Engineering, Spe-
cial Issue on Best Practice in Spoken Language Dia-
logue Systems Engineering, pages 323?340.
M. Rayner and B.A. Hockey. 2003. Transparent com-
bination of rule-based and data-driven approaches in a
speech understanding architecture. In Proceedings of
the 10th EACL (demo track), Budapest, Hungary.
M. Rayner and B.A. Hockey. 2004. Side effect free
dialogue management in a voice enabled procedure
browser. In Proceedings of INTERSPEECH 2004, Jeju
Island, Korea.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL, Budapest, Hungary.
Regulus, 2005. http://sourceforge.net/projects/regulus/.
As of 26 April 2005.
32
A Compact  Arch i tec ture  for D ia logue  Management  Based  on 
Scr ip ts  and  Meta -Outputs  
Manny Rayner,  Beth  Ann  Hockey~ Frankie James .: : 
Research Institute for Advanced Computer Science 
Mail Stop 19-39, NASA Ames Research Center 
Moffett Field, CA 94035-1000 
{manny, bahockey, fjames} @riacs.edu 
Abstract  
We describe an architecture for spoken dialogue 
interfaces to semi-autonomous systems that trans- 
forms speech~signals through successive representa- 
tions of linguistic, dialogue, and domain knowledge. 
Each step produces an output, and a meta-output 
describing the transformation, with an executable 
program in a simple scripting language as the fi- 
nal result. The output/meta-output distinction per- 
mits perspicuous treatment of diverse tasks such as 
resolving pronouns, correcting user misconceptions, 
and optimizing scripts. 
1 Introduction 1 
The basic task we consider in this paper is that of 
using spoken language to give commands to a semi- 
autonomous robot or other similar system. As ev- 
idence of the importance of this ta~k in the NLP 
community note that the early, influential system 
SHRDLU (Winograd, 1973) was intended to address 
just this type of problem. More recent work on spo- 
ken language interfaces to semi-autonomous robots 
include SRI's Flakey robot (Konolige et al, 1993) 
and NCARAI's InterBOT project (Perzanowski et 
al., 1998; Perzanowski et al, 1999). A number of 
other systems have addressed part of the task. Com- 
mandTalk (Moore et al, 1997), Circuit Fix-It Shop 
(Smith, 1997) and Tl:tAINS-96 (Traum and Allen, 
1994; Traum and Andersen, 1999) are spoken lan- 
guage systems but they interface to simulation or 
help facilities rather than semi-autonomous agents. 
Jack's MOOse Lodge (Badler et al, 1999) takes text 
rather than speech as natural anguage input and the 
avatars being controlled are not semi-autonomous. 
Other researchers have considered particular aspects 
of the problem such as accounting for various aspects 
of actions (Webber, 1995; Pym et al, 1995). In most 
of this and other related work the treatment is some 
variant of the following. If there is a speech inter- 
face, the input speech signal is converted into text. 
Text either from the recognizer or directly input by 
IThis paper also appears in the proceedings of the Sixth 
International Conference on Applied Natural Language Pro- 
cessing~ Seattle, WA, April 2000. 
the user is then converted into some kind of logi- 
cal formula, which abstractly represents the user's 
intended command; this formula is.then fed into a 
command interpreter, which execdtes the command. 
We do not think the standard treatment outlined 
above is in essence incorrect, but we do believe that, 
as it stands, it is in-need of some modification. This 
paper will in particular make three points. First, we 
suggest that the output representation should not be 
regarded as a logical expression, but: rather as a pro- 
gram in some kind of scripting language. Second, we 
argue that it is not merely the case that the process 
of converting the input signal to the final represen- 
tation can sometimes go wrong; rather, this is the 
normal course of events, and the inferpretatiofi pro- 
cess should be organized with that assumption in 
mind. Third, we claim, perhaps urprisingly, that 
the first and second points are related. These claims 
are elaborated in Section 2. 
The remainder of the paper describes an archi- 
tecture which addresses the issues outlined above, 
and which has been used to implement a prototype 
speech interface to a simulated semi-autonomous 
robot intended for deployment on the International 
Space Station. Sections 3 and 4 present an overview 
of the implemented interface, focussingon represen- 
tational issues relevant o dialogue management. Il-
lustrative xamples of interactions with the system 
are provided in Section 5. Section 6 concludes. 
2 Theoretical Ideas, 
2.1 Scripts vs Logical Forms 
Let's first look in a little more detail at the question 
of what the output representation f a spoken lan- 
guage interface to a semi-autonomous robot/agent 
should be. In practice, there seem to be two main 
choices: atheoretical representations, or some kind 
of logic. 
Logic is indeed an excellent way to  think 
about representing static relationships like database 
queries, but it is much less clear that it is a good way 
to represent commands. In real life, when people 
wish to give a command to a computer, they usu- 
ally do so via its operating system; a complex com- 
54 
mand is an expression in a scripting language like 
CSHELL, Perl, or VBScript. These languages are 
related to logical formalisms, but cannot be mapped 
onto them in a simple way. Here are some of the 
obvious differences: 
? A scripting language is essentially imperative, 
rather than relational. 
? The notion of temporal sequence is fundamental 
to the language. "Do P and then Q" is not the 
same as "Make the goals P and Q true"; it is 
explicitly stated that P is to be done first. Simi~ 
larly, "For each X in the list (A B C), do P (X)"  
is not the same as "For all X, make P(X) true"; 
once again, the scripting language defines an or- 
der, but no~ the logical language 2.
? Scripting languages assume that commands do 
not always succeed. For example, UNIX-based 
scripting languages like CSHELL provide each 
script with the three predefined streams td in ,  
s tdout  and s tder r .  Input is read from s td in  
and written to stdout;  error messages, warn- 
ings and other comments are sent to s tder r .  
Wedo not think that these properties of scripting 
language are accidental. They have evolved as the 
result of strong selectional pressure from real users 
with real-world tasks that need to be carried out, 
and represent a competitive way to meet said users' 
needs. We consequently think it is worth taking seri- 
ously the idea that a target representation produced 
by a spoken language interface should share many of 
these properties. 
2.2 Fall ible In terpretat ion :  Outputs  and 
Meta -outputs  
We now move on to the question of modelling the in- 
terpretation process, that is to say the process that 
converts the input (speech) signal to the output (ex- 
ecutable) representation. As already indicated, we 
think it is important o realize that interpretation 
is a process which, like any other process, may suc- 
ceed more or less well in achieving its intended goals. 
Users may express themselves unclearly or incom- 
pletely, or the system may more or less seriously 
fail to understand exactly what they mean. A good 
interpretation architecture will keep these consider- 
ations in mind. 
Taking our lead from the description of scripting 
languages ketched above, we adapt the notion of 
the "error stream" to the interpretation process. In 
the course of  interpreting an utterance, the system 
2In cases like these, the theorem prover or logic program- 
ming interpreter used to evaluate the logical formula typically 
assigns a conventional order to the conjuncts; note however 
that this is part of the procedural semantics of the theorem 
prover/interpreter, and does not follow from the declarative 
semantics of the logical formalism. 
55 
translates it into successively "deeper" levels of rep- 
resentation. Each translation step has not only an 
input (the representation consumed) and an output 
(the representation produced), but also something 
we will refer to as a "meta-output':  this provides in- 
formation about how the translation was performed. 
At a high level of abstraction, our architecture will 
be as follows. Interpretation proceeds as a series 
of non-deterministic ranslation steps, each produc- 
ing a set of possible outputs and associated meta- 
outputs. The final translation step produces an ex- 
ecutable script. The interface attempts to simulate 
execution of each possible script produced, in or- 
der to determine what would happen if that script 
were selected; simulated execution can itself produce 
further meta-outputs. Finally, the system uses the 
meta-output information to decide what to do with 
the various possible interpretations it has produced. 
Possible actions incl~tde selection and execution of 
an output script, paraphrasing meta-output infor- 
mation back to the user, or some combination of the 
two. 
In the following section, we present a more de- 
tailed description showing how the output/meta- 
output'distinction works in a practical system. 
3 A Prototype Implementation 
The ideas sketched out above have been realized as 
a prototype spoken language dialogue interface to a 
simulated version of the Personal Satellite Assistant 
(PS i ;  (PSA, 2000)). This section gives an overview 
of the implementation; in the following section, we 
focus on the specific aspects of dialogue management 
which are facilitated by the output/meta-output ar- 
chitecture. 
3.1 Levels of  Representat ion  
The real PSA is a miniature robot currently being 
developed at NASA Ames Research Center, which 
is intended for deployment on the Space Shuttle 
and/or International Space Station. It will be ca- 
pable of free navigation in an indoor micro-gravity 
environment, and will provide mobile sensory capac- 
ity as a backup to a network of fixed sensors. The 
PSA will primarily be controlled by voice commands 
through a hand-held or head-mounted microphone, 
with speech and language processing being handled 
by an offboard processor. Since the speech process- 
ing units are not in fact physically connected to the 
PSA we envisage that they could also be used to con- 
trol or monitor other environmental functions. In 
particular, our simulation allows voice access to the 
current and past values of the fixed sensor eadings. 
The initial PSA speech interface demo consists of 
a simple simulation of the Shuttle. State parame- 
ters include the PSA's current position, some envi- 
ronmental variables uch as local temperature, pres- 
sure and carbon dioxide levels, and the status of the 
Shuttle's doors (open/closed). A visual display gives 
direct feedback on some of these parameters. 
The speech and language processing architecture 
is based on that of the SRI CommandTalk sys- 
tem (Moore et al, 1997; Stent et al, 1999). The sys- 
tem comprises a suite of about 20 agents, connected 
together using the SRI Open Agent Architecture 
(OAA; (Martin et al, 1998)). Speech recognition 
is performed using a version of the Nuance recog- 
nizer (Nuance, 2000). Initial language processing is
carried out Using the SRI Gemini system (Dowding 
et al, 1993), using a domain-independent u ification. 
grammar and a domain-specific lexicon. The lan- 
guage processing rammar is compiled into a recog- 
nition grammar using the methods of (Moore et al, 
1997); the n~ resnlt is that only grammatically well- 
formed utterances Gan be recognized. Output from 
the initial language-processing step is represented 
in a version of Quasi Logical Form (van Eijck and 
Moore, 1992), and passed in that form to the dia- 
logue manager. We refer to these as linguistic level 
representations. 
The aspects of the system which are of primary in- 
terest here concern the dialogue manager (DM) and 
related modules. Once a linguistic level represen- 
tation has been produced, the following processing 
steps occur: 
? The linguistic level representation is converted 
into a discourse level representation. This pri- 
marily involves regularizing differences in sur- 
face form: so, for example, "measure the pres- 
sure" and "what is the pressure?" have differ- 
ent representations at the linguistic level, but 
the same representation at the discourse level. 
? If necessary, the system attempts to resolve in- 
stances of ellipsis and anaphoric reference. For 
example, if the previous command was "mea- 
sure temperature at flight deck", then the new 
command "lower deck" will be resolved to an 
expression meaning "measure temperature at 
lower deck". Similarly, if the previous command 
was "move to the crew hatch", then the com- 
mand "open it" will be resolved to "open the 
crew hatch". We call the output of this step a 
resolved discourse level representation. 
? The resolved discourse level representation is 
converted into an executable script in a lan- 
guage essentially equivalent o a subset of 
CSHELL. This involves two sub-steps. First, 
quantified variables are given scope: for exam- 
ple, "go to the flight deck and lower deck and 
measure pressure" becomes omething approxi- 
mately equivalent to the script 
foreach x ( f l ight_deck  lower_deck) 
go_to Sx 
measure  pressure  
end 
The point to note here is that :the foreach has 
scope over both the go_to and the measure ac- 
tions; an alternate (incorrect) sCoping would be 
foreach x (flight_deck lower_deck) 
go_to Sx 
end 
measure  pressure 
The second sub-step is to attempt o optimize 
the plan. In the current example, this can 
be done by reordering the list ( f l ight_deck 
lowerAeck). For instance, if the PSA is al- 
ready at the lower deck, reversing the list will 
mean that the robot only makes one trip, in- 
stead of two. ,, 
The final step in the 'interlJretation process is 
plan evaluation: the syStem tries to work out 
what will happen if it actually executes the 
plan. (The relationship between plan evaluation 
and plan execution is described in more detail 
in Section 4.1). Among other things, this gives 
the dialogue manager the possibility of compar- 
ing different interpretations of the original com- 
mand, and picking the one which is most effi- 
cient. 
3.2 How Meta-outputs  Part ic ipate in the 
Translation 
The above sketch shows how context-dependent 
interpretation is arranged as a series of non- 
deterministic translation stepS; in each case, we have 
described the input and the output for the step in 
question. We now go back to the concerns of Sec- 
tion 2. First, note that each translation step is in 
general fallible. We give severalexamples: 
? One of the most obvious cases arises when the 
user simply issues an invalid command, such as 
requesting the PSA to open a door D which is 
already open. Here, one of the meta~outputs 
issued by the plan evaluation step will be the 
term 
presupposition_failure(already_open(D)); 
the DM can decide to paraphrase this back to 
the user as a surface string of the form "D is 
already open". Note that plan evaluation does 
not involve actually executing the final script, 
which can be important. For instance, if the 
command is "go to the crew hatch and open it" 
and the crew hatch is already open, the interface 
has the option of informing the user that there 
is a problem without-first carrying out the "go 
to" action. 
56 
? The resolution step can give rise to similar kinds 
of meta-output. For example, a command may 
include a referring expression that has no deno- 
tation, or an ambiguous denotation; for exam- 
ple, the user might say "both decks", presum- 
ably being unaware that there are in fact three 
of them. This time, the meta-output produced 
is 
presupposition_failure ( 
incorrect_size_of_set (2,3)) 
representing the user'sincorrect belief abou\[  
the number of decks. The DM then has the pos- 
sibility of informing the user of this misconcep- 
tion by realizipg the meta-output term as the 
surface stung "in. fact there are three of them". 
Ambiguous denotation occurs when a descrip- 
tion is under-specified. For instance, the user 
might say "the deck" in a situation where there 
is no clearly salient deck, either in the discourse 
situation :or in the simulated world: here, the 
meta-output will be 
presupposition_failure ( 
under specif ied_def inite (deck)) 
which can be realized as the clarification ques- 
tion "which deck do you mean?" 
? A slightly more complex case involves plan 
costs. During plan evaluation, the system simu- 
lates execution of the output script while keep- 
ing track of execution cost. (Currently, the cost 
is just an estimate of the time required to exe- 
cute the script). Execution costs are treated as 
meta-outputs of the form 
cost(C) 
and passed back through the interpreter so that 
the plan optimization step can make use of 
them. 
Finally,. we consider what happens when the 
system receives incorrect input from the speech 
recognizer. Although the recognizer's language 
model is constrained so that it can only pro- 
duce grammatical utterances, it can still misrec- 
ognize one grammatical string as another one. 
Many of these cases fall into one of a small 
number of syntactic patterns, which function as 
fairly reliable indicators of bad recognition. A 
typical example is conjunction i volving a pro- 
noun: if the system hears "it and flight deck", 
this is most likely a misrecognition f something 
like "go to flight deck". 
During the processing phase which translates 
linguistic level representations into discourse 
level representations, the system attempts to 
match each misrecognition pattern against he 
input linguistic form, and if successful produces 
a meta-output of the form 
presupposition_failure ( 
dubious_if (<Type>)) 
These mete-outputs are passed down to the 
DM, which in the absence of sufficiently com- 
pelling contrary evidence will normally issue a 
response of the form "I'm sorry, I think I mis- 
heard you". 
4 A Compact  Arch i tec ture  for 
D ia logue  Management  Based  on  
Scr ip ts  and  Meta -Outputs  
None of the individual functionalities outlined above 
are particularly novel in themselves. What we find 
new and interesting-is the fact that they can all 
be expressed in a uniform way in terms of the 
script output/meta-output architecture. This sec- 
tion presents three examples illustrating how the ar- 
chitecture can be used to simplify the overall orga- 
nization of the system. 
4.1 Integrat ion of plan evaluation, plan 
execution and dialogue management .  
Recall that the DM simulates evaluation of the plan 
before running it, in order to obtain relevant meta- 
information. At plan execution time, plan actions 
result in changes to the world; at plan evaluation 
time, they result in simulated changes to the world 
and/or produce meta-outputs. 
Conceptualizing plans as scripts rather than log- 
ical formulas permits an elegant reatment of the 
execution/evaluation dichotomy. There is one script 
interpreter, which functions both as a script exec- 
utive and a script evaluator, and one set of rules 
which defines the procedural semantics of script ac- 
tions. Rules are parameterized by execution type 
which is either "execute" or "evaluate". In "evalu- 
ate" mode, primitive actions modify a state vector 
which is threaded through the interpreter; in "ex- 
ecute" mode, they result in commands being sent 
to (real or simulated) effector agents. Conversely, 
"meta-information" actions, such as presupposition 
failures, result in output being sent to the meta- 
output stream in "evaluate" mode, and in a null ac- 
tion in "execute" mode. The upshot is that a simple 
semantics can be assigned to rules like the following 
one, which defines the action of attempting to open 
a door which may already be open: 
procedure ( 
open_door (D) , 
if_then_else (status (D, open_closed, open), 
presupposition_failure (already_open(D)), 
change_status (D, open_closed, open))) 
57 
4.2 Using meta-outputs  o choose between 
interpretat ions 
As described in the preceding section, the resolution 
step is in general non-deterministic and gives rise to 
meta-outputs which describe the type of resolution 
carried out. For example, consider a command in- 
volving a definite description, like "open the door". 
Depending on the preceding context, resolution will 
produce a number of possible interpretations; "the 
door" may be resolved to one or more contextually 
available doors, or the expression may be left un- 
resolved. In each case, the type of resolution used 
appears as a meta-output, and is available to the di- 
alogue manager when it decides which interpretation 
is most felicitous. By default, the DM's strategy is to 
attempt to~pp ly  antecedents for referring expres- 
sions, preferring h~ most recently occurring sortally 
appropriate candidate. In some cases, however, it is 
desirable to allow the default strategy to be over- 
ridden: for instance, it may result in a script which 
produces a presupposition failure during plan eval- 
uation. Treating resolution choices and plan evalu- 
ation problems as similar types of objects makes it 
easy to implement this kind of idea. 
4.3 Using meta-outputs  o choose between 
dialogue management  moves 
Perhaps the key advantage ofour architecture is that 
collecting together several types of information as a 
bag of meta-outputs simplifies the top-level struc- 
ture of the dialogue manager. In our application, 
the critical choice of dialogue move comes after the 
dialogue manager has selected the most plausible in- 
terpretation. It now has to make two choices. First, 
it must decide whether or not to paraphrase any of 
the meta-outputs back to the user; for example, if 
resolution was unable to fill some argument posi- 
tion or find an antecedent for a pronoun, it may be 
appropriate to paraphrase the corresponding meta- 
output as a question, e.g. "where do you mean?", or 
"what do you mean by 'it' ?". Having all the meta- 
outputs available together means that the DM is 
able to plan a coherent response: so if there are sev- 
eral meta-outputs which could potentially be worth 
paraphrasing, it typically realizes only the most im- 
portant one. 
Second, if interpretation was able to produce a 
well-formed plan, the DM currently has the three 
options of executing it, paraphrasing it back to the 
user as a confirmation question, or doing nothing. 
Once again, this decision often requires global infor- 
mation about what has happened uring the inter- 
pretation process. For example, knowing that plan 
evaluation showed that the plan would take signif- 
icant time to execute makes it more plausible that 
the user would prefer to receive a confirmation. 
58 
5 Examples  
This section presents a Commented sample dialogue 
with the implemented PSA interface demo chosen 
to illustrate the points discussed inthe previous ec- 
tions. We focus in particular On ,the :role that meta- 
outputs play in interpretation. Text preceded by 
USER represents spoken utterances from the user. 
Text preceded by PSA represents synthesized spoken 
responses from the PSA. Text inside square brackets 
describes the actions carried out bythe PSA. 
1. The PSA can move to various locations and 
measure nvironmental f ctors. In the first ex- 
zhange, reference resolution is used to interpret 
the phrase "all three decks" as referring to the 
lower deck, mid deck and flight deck. Plan op- 
timization finds the most efficient way to or- 
der the locations, an d paraphrases the resulting 
plan back to the user. 
USER: Go to all three decks and measure car- 
bon dioxide. ~ ' 
PSA: I will move to flight,deck, mid deck and 
then lower deck and I will measure carbon 
' dioxide level, okay? 
USER: Okay. \[PSA moves to flight deck\] 
PSA: The carbon dioxide level at the flight 
deck is one percent. \[PSA: moves to mid 
deck\] 
PSA.: The carbon dioxide level at the mid deck 
is one percent. \[PSA moves to lower deck\] 
PSA: The carbon dioxide level at the lower 
deck is one percent. '
2. The next example involves resolution of ellip- 
sis. The relevant context is recovered from the 
preceding dialogue. 
USER: Do the same for the pilot's seat. 
PSA: I will move to the pilot's seat and I will 
measure carbon dioxide level, okay? 
USER: Right. \[PSA moves to pilot's seat\] 
PSA: The carbon dioxide level at the pilot's 
seat is one percent. 
3. Resolution of ellipsis can involve filling in un- 
specified semantic parameters toa syntactically 
complete command. It is not meaningful to talk 
about a sensor eading without reference to the 
place at which that reading was carried out; in 
the following example, resolution assumes that 
the location in question is the one most recently 
mentioned in the dialogue. 
Note also that since the PSA does not have to 
move in order to carry out the command, and 
. 
. 
. 
. 
. 
hence can execute it immediately, the DM rea- 
sonably decides to skip the confirmation ques- 
tion. This decision is made based on the meta- 
Output encoding the estimated cost of executing 
the :script. 
' i  
USER: What was the carbon dioxide level at 
fifteen oh five according to the fixed sen- 
sors? 
PSA: According to the fixed sensors, at fifteen 
oh five the carbon dioxide level at the pi- 
lot's seat was one percent. -. 
The system encounters a user misconception, 
which results in a "presupposition failure" 
meta-output being produced during reference 
resolutiom It ?onsequently decides to alert the 
user to this, rather than attempt o carry out 
the command. 
USER: Close both doors. 
PSA: There are in fact three of them. 
The user utterance includes a pronoun, "it". 
The system resolves "it" to. "the crew hatch", 
and issues an appropriate paraphrase in order 
to confirm that this is what the user meant. 
USER: Go to crew hatch and close it. 
PSA: I will go to crew hatch and I will close 
crew hatch, okay? 
USER: Yeah. \[PSA moves to crew hatch and 
closes it\] 
The user asks the system to close the door, but 
it is unclear which door they are referring to. 
An "ambiguous reference" meta-output is pro- 
duced by reference resolution, resulting in the 
DM issuing a clarification question. 
USER: Close the door. 
PSA: Which door do you mean? 
The user response is successfully interpreted by 
reference resolution as an instance of ellipsis. 
Trying to close the crew hatch a second time 
however causes a presupposition failure meta- 
output o be generated during plan evaluation. 
The system informs the user of this and takes 
no action. 
USER: The crew hatch. 
PSA: The crew hatch is already closed. 
The user presents three locations in a subopti- 
mal order. Plan evaluation finds a more efficient 
ordering. 
USER: Move to storage lockers, commander's 
seat and flight deck and measure tempera- 
ture. 
PSA: I will move to flight deck, commander's 
seat and then storage lockers and I will 
measure temperature, okay? 
USER: Sure. 
\[PSA moves to flight deck, commander's 
seat and storage lockers in that order and 
reports temperature at each location\] 
9. The user asks the system to repeat he previous 
command. Since the PSA is now starting from 
a different position, its most efficient plan is no 
longer the same, and it informs the user of this. 
USER: Do that again. 
' PSA:  I will move to storage lockers, comman- 
der's seat and then flight deck and I will 
measure temperature, okay? 
USER: Yes. \[PSA moves to storage lockers\] 
PSA: The temperature at the storage lockers 
is 19.9 degrees Celsius. \[PSA starts moving 
to commander's seat\] 
10. The user interrupts the PSA in the middle of the 
plan, and asks it to go back to its last stopping 
place. Since "stop" may involve a potentially 
hazardous condition, the interface obeys imme- 
diately, without attempting to confirm first. 
USER: Stop. \[PSA stops\] 
USER: Go back. \[PSA returns to storage lock- 
ers\] 
6 Summary  
We have described a generic architecture for spoken 
language dialogue interfaces to semi-autonomous 
agents, based on the standard notion of translating 
to successively deeper levels of representation. The 
novel aspects of our scheme center around two ideas: 
first, that the final output representations are best 
conceptualized not as logical expressions but rather 
as programs in a scripting language; second, that 
steps in the translation process hould produce not 
only a simple output, but also meta-information de-
scribing how the output was produced. We have pre- 
sented examples uggesting how several apparently 
diverse types of dialogue behavior can be captured 
simply within our framework, and outlined a proto- 
type implementation f the scheme. 
References 
N. Badler, R. Bindiganavale, J. Bourne, J. Allbeck, 
J. Shi, and M. Palmer. 1999. Real time virtual 
humans. In International Conference on Digital 
Media Futures. 
J. Dowding, M. Gawron, D. Appelt, L. Cherny, 
R. Moore, and D. Moran. 1993. Gemini: A nat- 
ural language system for spoken language un- 
derstanding. In Proceedings of the Thirty-First 
59 
Annual Meeting of the Association for Computa- 
tional Linguistics. 
K. Konolige, K. Myers, E. Ruspini, and A. Saf- 
fiotti. 1993. Flakey in action: The 1992 AAAI 
robot competition. Technical Report SRI Techni- 
cal Note 528, SRI, AI Center, SRI International, 
333 Ravenswood Ave., Menlo Park, CA 94025. 
D. Martin, A. Cheyer, and D. Moran. 1998. Build- 
ing distributed software systems with the open 
agent architecture. In Proceedings of the Third 
International Conference on the Practical Appli- 
cation of Intelligent Agents and Multi-Agent Tech~ 
nology. 
R. Moore, J. Dowding, H. Bratt, J. Gawron, 
Y. Gorfu, and A. Cheyer. 1997. CommandTalk: 
A spoken:language interface for battlefield simu- 
lations. In ProCeedings of the Fifth Conference on 
Applied NaturaiLanguage Processing, pages 1-7. 
Nuance, 2000. Nuance Communications, Inc. 
http://www.nuance.com. As of 9 March 2000. 
D. Perzanowski, A. Schultz, and W. Adams. 1998. 
Integrating natural language and gesture in a 
robotics domain. In IEEE International Sympo- 
sium on Intelligent Control: ISIC/CIRA//ISAS 
Joint Conference, pages 247-252, Gaithersburg, 
MD: National Institute of Standards and Tech- 
nology. 
D. Perzanowski, A. Schultz, W. Adams, and 
E. Marsh. 1999. Goal tracking in a natural an- 
guage interface: Towards achieving adjustable au- 
tonomy. In ISIS/CIRA99 Conference, Monterey, 
CA. IEEE. 
PSA, 2000. Personal Satellite Assistant (PSA) 
Project. http://ic.arc.nasa.gov/ic/psa/. As of 9 
March 2000. 
D. Pym, L. Pryor, and D. Murphy. 1995. Actions 
as processes: a position on planning. In Working 
Notes, AAAI Symposium on Extending Theories 
of Action, pages 169-173. 
R. W. Smith. 1997. An evaluation of strategies for 
selective utterance verification for spoken natural 
language dialog. In Proceedings ofthe Fifth Con- 
ference on Applied Natural Language Processing, 
pages 41-48. 
A. Stent, J. Dowding, J. Gawron, E. Bratt, and 
R. Moore. 1999. The CommandTalk spoken di- 
alogue system. In Proceedings of the Thirty- 
Seventh Annual Meeting of the Association for 
Computational Linguistics, pages 183-190. 
D. R. Traum and J. Allen. 1994. Discourse obliga- 
tions in dialogue processing. In Proceedings ofthe 
Thirty-Second Annual Meeting of the Association 
for Computational Linguistics, pages 1-8. 
D. R. Traum and C. F. Andersen. 1999. Represen- 
tations of dialogue state for domain and task inde- 
pendent meta-dialogue. In Proceedings of the I J- 
CAI'99 Workshop on KnowTedge and Reasoning 
in Practical Dialogue Systems, pages 113-120. 
J. van Eijck and R. Moore. 1992. Semantic rules 
for English. In H. Alshawi, editor, The Core Lan- 
guage Engine. MIT Press. ~ 
B. Webber. 1995. Instructing animated agents: 
Viewing language in behavioral terms. In Proceed- 
ings of the International Conference on Coopera- 
tive Multi-modal Communication. 
T. A. Winograd. 1973. A procedural model of lan- 
guage understanding. In R. C. Shank and K. M. 
Colby, editors, Computer Models of Thought and 
Language. Freeman, San Francisco, CA. 
i 
60 
A procedure assistant for astronauts
in a functional programming architecture,
with step previewing and spoken correction of dialogue moves
Gregory Aist
1
, Manny Rayner
1
, John Dowding
1
,
Beth Ann Hockey
1
, Susana Early
2
, and Jim Hieronymus
3
1
Research Institute for Advanced Computer Science
2
Foothill/DeAnza College
3
NASA Ames Research Center
M/S T35B-1, Moffett Field CA 94035
{aist, mrayner, jdowding, bahockey, jimh}@riacs.edu; searly@mail.arc.nasa.gov
Abstract
We present a demonstration of a proto-
type system aimed at providing support
with procedural tasks for astronauts on
board the International Space Station.
Current functionality includes navigation
within the procedure, previewing steps,
requesting a list of images or a particular
image, recording voice notes and spoken
alarms, setting parameters such as audio
volume. Dialogue capabilities include
handling spoken corrections for an entire
dialogue move, reestablishing context in
response to a user request, responding to
user barge-in, and help on demand. The
current system has been partially reim-
plemented for better efficiency and in re-
sponse to feedback from astronauts and
astronaut training personnel. Added fea-
tures include visual and spoken step pre-
viewing, and spoken correction of
dialogue moves. The intention is to intro-
duce the system into astronaut training as
a prelude to flight on board the Interna-
tional Space Station.
1 Introduction
Astronauts on board the International Space Sta-
tion engage in a wide variety of tasks on orbit in-
cluding medical procedures, extra vehicular
activity (E V A), scientific payloads, and station
repair and maintenance. These human space flight
activities require extensive and thorough proce-
dures. These procedures are written down in the
form of a number of steps and, with various notes,
cautions, and warnings interspersed throughout the
procedure. Each step may have one or more sub
steps.  Procedures also include branch points, call-
outs to other procedures, and instructions to com-
municate with mission control.  Since December
2001, the RIALIST group has been developing a
spoken dialogue system for providing assistance
with space station procedures. Aist and Hockey
(2002) and Aist et al (2002) described the first
version of the system, which operated on a simpli-
fied (and invented) procedure for unpacking and
operating a digital camera and included speech
input and speech output only. Aist et al (2003)
described a second version of the system with an
XML-based display, and that included support for
not only procedures, but also voice notes and re-
corded alarms, and parameter settings such as in-
creasing and decreasing volume. In this paper, we
describe the third version of the system, with a
reimplemented architecture based on a functional
specification of the domain-specific aspects of the
system combined with an event-driven generic ar-
chitectural framework. We also describe two new
features: previewing of steps, and spoken correc-
tion of dialogue moves.
2 System Description
The March 2003 version of the Intelligent Proce-
dure Assistant is shown in Figure 1, just after
loading a procedure. The March 2003 version pro-
vides the following functions:
Loading a procedure by specifying its name, for
example, ?Load water procedure.?
Sequential navigation through individual steps, for
example, ?Next step? or ?Previous step.?
Navigation to arbitrary steps, for example, ?Go to
step two point one.?
Setting system parameters, such as ?Increase vol-
ume? or ?Decrease volume.?
Handling annotations, such as voice notes or
alarms (?Record a voice note?), or pictures (?Show
the small waste water bag.?).
Previewing steps; for example, ?Read step three?.
Issuing spoken corrections (of entire commands),
for example, ?I meant go to step two.?
We will discuss previewing steps and issuing spo-
ken corrections in turn.
2.1 Previewing steps (Reading mode)
Besides acting on the current step, astronauts indi-
cated that they would like a spoken preview of the
next step. Currently this functionality is imple-
mented as displaying a second procedure window
in the upper right corner of the screen. Further-
more, steps are prefixed with a spoken indication
of previewing, for example, ?Reading mode. Note
before step two?? To transition back into normal
(execution) mode, the user may say ?Stop read-
ing.? Figure 2 shows the resulting display for the
reading mode.  
2.2 Issuing spoken corrections
In the March 2003 version of the Checklist system,
the user may issue a spoken correction in the case
of an incorrectly given command, or in the case of
a speech recognition error (e.g. ?read me step
three? ? ?repeat step three?). The dialogue history
is represented as a list of the prior dialogue states.
Currently we model a correction as a change in the
information state, a rollback of the previous action
plan, and then an application of the new action
plan. Figure 3 shows the display after issuing a
correction, ?I meant the wash cloth?. Reading
mode has been exited, and a picture of the wash-
cloth is displayed.
Figure 1. Loading a procedure.
Figure 2. Preview mode, step three.
Figure 3. A subsequent correction, resulting in a
return to execution mode, and the implementation
of the other command.
Figure 4. Checklist dialogue system architecture.
3  Architecture, or, How to write
a dialogue system in three easy steps
There are three main sections to the dialogue han-
dling code: the input manager, dialogue manager,
and the output manager (Figure 4). These are
similar divisions to those proposed in Allen et al
(2000). Here, we also adopt a further division of
the code into application-specific code and generic
code. Application-specific code computes the fol-
lowing function for each component, as a compila-
tion step:
Input manager: Input ? Event
Dialogue manager: (Event, State)
? (Action, State)
Output manager: Action ? (Output, Inverse)
The Output and Inverse computed by the Input
manager are the multimodal output plans and their
multimodal inverses, respectively.  The multi-
modal inverses are used when applying a correc-
tion ? in conjunction with a return to a previous
state on the history list.
The generic code is an interpretation (or execu-
tion) step; the input manager?s code collects in-
coming events and dispatches the events to the
dialogue manager. The dialogue manager?s code
collects the incoming events, retrieves the previous
state, applies the application-specific function,
saves the new state, and then dispatches the new
action. The output manager takes the action, ap-
plies the application-specific function to compute
the output and its inverse, and then dispatches the
output plan one action at a time. Each action is rep-
resented as an OAA solvable, and dispatched se-
quentially to be handled by the appropriate agent
such as the text-to-speech agent.
The entire dialogue manager is side-effectfree.
(With the minor exception of loading a procedure
file, which causes a change in the ?last accessed?
time of the file.) In a more typical dialogue system
architecture such as that shown in Figure 5, the
side effects are represented separately. The inte-
gration of side effects into the output plan has
positive benefits for robustness, since they will be
represented in one place (and thus modified at the
same time when programming changes are made).
Figure 5.  A more typical dialogue system ar-
chitecture, with the side effects executed separately
from the spoken output.
4 Related Research and Future Work
Rudnicky, Reed, and Thayer (1996) describe a
system for supporting vehicle maintenance with
speech interfaces. Schreckenghost et al (2003)
describe a scenario involving similar tasks (life
Speech
Recognizer
Parser
Input
Manager
Output
Manager
Speech
Synthesizer
Visual
Display
Dialogue
Manager
I: input ? event
D: (event, state)
? (action, state)
O: action
? (output, inverse)
Speech
Recognizer
Parser Input
Manager
Output
Manager
Speech
Synthesizer
DB
Dialogue
Manager
support / maintenance related) but with the com-
puter in more control of the actual task. S & K
Electronics (n.d.) mention a procedure develop-
ment environment for rapidly developing and veri-
fying on-orbit procedures
(http://sk-web.sk-tech.com/proj.html).
Possible future work includes adding procedures
involving inventory management and robot arm
assistance, automating dialogue system construc-
tion from XML procedures, integrating with te-
lemetry to monitor execution of the procedure and
develop error recovery options, improving natural-
ness of the speech output, modeling dialogue to
include dialogue moves and expected user re-
sponses, and improving speech recognition to be
robust to ISS noise.
References
G. Aist. J. Dowding, B. A. Hockey, and J. Hieronymus.
2002. An intelligent procedure assistant for astronaut
training and support. Proceedings of the 40
th
 Annual
Meeting of the Association for Computational Lin-
guistics, refereed demonstration track.
G. Aist and B. A. Hockey. 2002. Generating Training
and Assistive Dialogues for Astronauts from Interna-
tional Space Station Technical Documentation. ITS
2002 Workshop on Integrating Technical and Train-
ing Documentation. Presented along with system
demonstration.
G. Aist, J. Dowding, B. A. Hockey, M. Rayner, J. Hi-
eronymus, D. Bohus, B. Boven, N. Blaylock, E.
Campana, S. Early, G. Gorrell, and S. Phan. 2003.
European Association for Computational Linguistics
(EACL) 2003 meeting, Software Demonstration, Bu-
dapest, Hungary, April 2003.
J. Allen, D. Byron, M. Dzikovska, G. Ferguson, L.
Galescu, and A. Stent. 2000. An architecture for a
generic dialogue shell. Natural Language Engineer-
ing, Special issue on Best Practice in Spoken Lan-
guage Dialogue Systems Engineering, pp. 323-340.
A. Rudnicky, S. Reed, and E. H. Thayer. 1996.
SpeechWear: A mobile speech system.
http://www.speech.cs.cmu.edu/air/papers/speechwear.ps
D. Schreckenghost, C. Thronesbery, P. Bonasso, D.
Kortenkamp and C. Martin, Intelligent Control of
Life Support for Space Missions, in IEEE Intelligent
Systems Magazine, September/October, 2002.
Portions of the dialogue systems described in this paper
were constructed with Rayner, Hockey, and Dowding?s
Regulus open source toolkit. Interested readers may find
the toolkit and supporting documentation online at:
http://sourceforge.net/projects/regulus/.
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 26?27,
Vancouver, October 2005.
Japanese Speech Understanding Using Grammar Specialization
Manny Rayner, Nikos Chatzichrisafis, Pierrette Bouillon
University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
mrayner@riacs.edu
{Pierrette.Bouillon,Nikolaos.Chatzichrisafis}@issco.unige.ch
Yukie Nakao, Hitoshi Isahara, Kyoko Kanzaki
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
yukie-n@khn.nict.go.jp, {isahara,kanzaki}@nict.go.jp
Beth Ann Hockey
UCSC/NASA Ames Research Center
Moffet Field, CA 94035
bahockey@riacs.edu
Marianne Santaholma, Marianne Starlander
University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
Marianne.Santaholma@eti.unige.ch
Marianne.Starlander@eti.unige.ch
The most common speech understanding archi-
tecture for spoken dialogue systems is a combination
of speech recognition based on a class N-gram lan-
guage model, and robust parsing. For many types
of applications, however, grammar-based recogni-
tion can offer concrete advantages. Training a
good class N-gram language model requires sub-
stantial quantities of corpus data, which is gen-
erally not available at the start of a new project.
Head-to-head comparisons of class N-gram/robust
and grammar-based systems also suggest that users
who are familiar with system coverage get better re-
sults from grammar-based architectures (Knight et
al., 2001). As a consequence, deployed spoken dia-
logue systems for real-world applications frequently
use grammar-based methods. This is particularly
the case for speech translation systems. Although
leading research systems like Verbmobil and NE-
SPOLE! (Wahlster, 2000; Lavie et al, 2001) usu-
ally employ complex architectures combining sta-
tistical and rule-based methods, successful practical
examples like Phraselator and S-MINDS (Phrasela-
tor, 2005; Sehda, 2005) are typically phrasal trans-
lators with grammar-based recognizers.
Voice recognition platforms like the Nuance
Toolkit provide CFG-based languages for writing
grammar-based language models (GLMs), but it is
challenging to develop and maintain grammars con-
sisting of large sets of ad hoc phrase-structure rules.
For this reason, there has been considerable inter-
est in developing systems that permit language mod-
els be specified in higher-level formalisms, normally
some kind of unification grammar (UG), and then
compile these grammars down to the low-level plat-
form formalisms. A prominent early example of this
approach is the Gemini system (Moore, 1998).
Gemini raises the level of abstraction signifi-
cantly, but still assumes that the grammars will be
domain-dependent. In the Open Source REGULUS
project (Regulus, 2005; Rayner et al, 2003), we
have taken a further step in the direction of increased
abstraction, and derive all recognizers from a sin-
gle linguistically motivated UG. This derivation pro-
cedure starts with a large, application-independent
UG for a language. An application-specific UG is
then derived using an Explanation Based Learning
(EBL) specialization technique. This corpus-based
specialization process is parameterized by the train-
ing corpus and operationality criteria. The training
corpus, which can be relatively small, consists of ex-
amples of utterances that should be recognized by
the target application. The sentences of the corpus
are parsed using the general grammar, then those
parses are partitioned into phrases based on the op-
erationality criteria. Each phrase defined by the
operationality criteria is flattened, producing rules
of a phrasal grammar for the application domain.
This application-specific UG is then compiled into
26
a CFG, formatted to be compatible with the Nuance
recognition platform. The CFG is compiled into the
runtime recognizer using Nuance tools.
Previously, the REGULUS grammar specialization
programme has only been implemented for English.
In this demo, we will show how we can apply the
same methodology to Japanese. Japanese is struc-
turally a very different language from English, so it
is by no means obvious that methods which work
for English will be applicable in this new context:
in fact, they appear to work very well. We will
demo the grammars and resulting recognizers in the
context of Japanese ? English and Japanese ?
French versions of the Open Source MedSLT medi-
cal speech translation system (Bouillon et al, 2005;
MedSLT, 2005).
The generic problem to be solved when building
any sort of recognition grammar is that syntax alone
is insufficiently constraining; many of the real con-
straints in a given domain and use situation tend to
be semantic and pragmatic in nature. The challenge
is thus to include enough non-syntactic constraints
in the grammar to create a language model that can
support reliable domain-specific speech recognition:
we sketch our solution for Japanese.
The basic structure of our current general
Japanese grammar is as follows. There are four main
groups of rules, covering NP, PP, VP and CLAUSE
structure respectively. The NP and PP rules each as-
sign a sortal type to the head constituent, based on
the domain-specific sortal constraints defined in the
lexicon. VP rules define the complement structure
of each syntactic class of verb, again making use of
the sortal features. There are also rules that allow
a VP to combine with optional adjuncts, and rules
which allow null constituents, in particular null sub-
jects and objects. Finally, clause-level rules form a
clause out of a VP, an optional subject and optional
adjuncts. The sortal features constrain the subject
and the complements combining with a verb, but the
lack of constraints on null constituents and optional
adjuncts still means that the grammar is very loose.
The grammar specialization mechanism flattens the
grammar into a set of much simpler structures, elim-
inating the VP level and only permitting specific pat-
terns of null constituents and adjuncts licenced by
the training corpus.
We will demo several different versions of the
Japanese-input medical speech translation system,
differing with respect to the target language and
the recognition architecture used. In particular, we
will show a) that versions based on the specialized
Japanese grammar offer fast and accurate recogni-
tion on utterances within the intended coverage of
the system (Word Error Rate around 5%, speed un-
der 0.1?RT), b) that versions based on the original
general Japanese grammar are much less accurate
and more than an order of magnitude slower.
References
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: a case
study. In Proceedings of Eurospeech 2001, pages
1779?1782, Aalborg, Denmark.
A. Lavie, C. Langley, A. Waibel, F. Pianesi, G. Lazzari,
P. Coletti, L. Taddei, and F. Balducci. 2001. Ar-
chitecture and design considerations in NESPOLE!:
a speech translation system for e-commerce applica-
tions. In Proceedings of HLT: Human Language Tech-
nology Conference, San Diego, California.
MedSLT, 2005. http://sourceforge.net/projects/medslt/.
As of 9 June 2005.
R. Moore. 1998. Using natural language knowledge
sources in speech recognition. In Proceedings of the
NATO Advanced Studies Institute.
Phraselator, 2005. http://www.phraselator.com/. As of 9
June 2005.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
Regulus, 2005. http://sourceforge.net/projects/regulus/.
As of 9 June 2005.
Sehda, 2005. http://www.sehda.com/. As of 9 June 2005.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
27
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 14?21,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Paraphrases of Deep Semantic Representions
to Support Regression Testing in Spoken Dialogue Systems
Beth Ann Hockey
UC Santa Cruz and BAHRC LLC
Mail Stop 19-26, UCSC UARC
NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@bahrc.net
Manny Rayner
University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
Emmanuel.Rayner@unige.ch
Abstract
Rule-based spoken dialogue systems require
a good regression testing framework if they
are to be maintainable. We argue that there
is a tension between two extreme positions
when constructing the database of test exam-
ples. On the one hand, if the examples con-
sist of input/output tuples representing many
levels of internal processing, they are fine-
grained enough to catch most processing er-
rors, but unstable under most system modi-
fications. If the examples are pairs of user
input and final system output, they are much
more stable, but too coarse-grained to catch
many errors. In either case, there are fairly
severe difficulties in judging examples cor-
rectly. We claim that a good compromise can
be reached by implementing a paraphrasing
mechanism which maps internal semantic rep-
resentations into surface forms, and carrying
out regression testing using paraphrases of se-
mantic forms rather than the semantic forms
themselves. We describe an implementation
of the idea using the Open Source Regulus
toolkit, where paraphrases are produced us-
ing Regulus grammars compiled in generation
mode. Paraphrases can also be used at run-
time to produce confirmations. By compiling
the paraphrase grammar a second time, as a
recogniser, it is possible in a simple and nat-
ural way to guarantee that confirmations are
always within system coverage.
1 Introduction
Design features that enable important functionality
in medium vocabulary, mixed-initiative spoken dia-
logue systems also create challenges for the project
cycle, and in particular for regression testing. Two
issues that make regression testing particularly dif-
ficult are the need for context dependent interpre-
tation, and the use of multiple levels of representa-
tion. Both of these features are typically necessary
for non-trivial dialogue systems of this type. Mul-
tiple levels of processing, as usual, provide neces-
sary modularity. Context dependent interpretation
enables responses that are tuned to the current cir-
cumstances of the interaction or the world, and fre-
quently helps resolve ambiguity.
The implications for regression testing, though,
are less happy. The context of each interaction in
the test suite needs to be stored as part of the inter-
action. Multiple levels of representation that are, for
example, useful for doing ellipsis resolution or ref-
erence resolution, also complicate testing. If regres-
sion testing is done on each separate level of pro-
cessing, or involves internal representations, small
changes to a representation at one level can mean
having to revise and rejudge the entire test suite to
keep it up to date.
This paper discusses the methodology we have
developed to address regression testing issues within
the Regulus framework. Regulus (Rayner et al,
2006) is an Open Source toolkit for builting medium
14
vocabulary spoken dialogue and translation appli-
cations, and has been used to build a number of
non-trivial spoken dialogue systems. Prominent ex-
amples include NASA?s Clarissa Procedure Navi-
gator (Rayner et al, 2005), Geneva University?s
multi-modal mobile-platform Calendar application
(Tsourakis et al, 2008), SDS, a prototype in-car sys-
tem developed by UC Santa Cruz in collaboration
with Ford Motors Research which was voted first
in Ford?s 2007 internal technology fair, and Taxi,
a speech-enabled game in which the user interacts
with a simulated cab driver to navigate around a map
of Manhattan. It has also been used to build the
MedSLT medical speech translation system (Bouil-
lon et al, 2008).
The Regulus platform includes tools for develop-
ing feature grammars, and compiling them in var-
ious ways. In particular, it is possible to compile
grammars into generators, and use them to support
paraphrasing from the internal semantic representa-
tions created during dialogue processing. This ca-
pability is key to the newest part of our regression
testing approach, and is discussed in detail in Sec-
tion 3. First, though, Section 2 gives an overview of
Regulus and the architecture of Regulus-based sys-
tems; we discuss features that complicate regression
testing, and how to address these problems within
this type of architecture. Section 4 discusses how
test suites are constructed and what types of items
they may contain. In Section 5 we show how para-
phrases can also be included in the run-time archi-
tecture. The final section concludes.
2 The Regulus platform
Regulus is an Open Source toolkit for building
medium vocabulary grammar-based spoken dia-
logue and translation systems. The central idea is to
base run-time processing on efficient, task-specific
grammars derived from general, reusable, domain-
independent core grammars. Early versions of Reg-
ulus used a single core grammar per language; a de-
tailed description of the core grammar for English
can be found in (Rayner et al, 2006, Chapter 9).
More recently, there have been attempts to go fur-
ther, and merge together core grammars for closely
related languages (Bouillon et al, 2007).
The core grammars are automatically specialised,
Figure 1: The Regulus compilation path. The general
unification grammar is first transformed into a specialised
feature grammar. This can then be transformed either into
a CFG grammar and Nuance recogniser, or into a gener-
ator. and a Nuance recogniser.
using corpus-driven methods based on small cor-
pora, to derive simpler grammars. Specialisation
is both with respect to task (recognition, analysis,
generation) and to application domain. The special-
isation process uses the Explanation Based Learning
algorithm (van Harmelen and Bundy, 1988; Rayner,
1988). It starts with a parsed treebank derived from
the training corpus, and then divides the parse tree
created from each training example into a set of one
or more subtrees, following a set of domain- and
grammar-specific rules conventionally known in the
Machine Learning literature as operationality crite-
ria. The rules in each subtree are then combined, us-
ing the unification operation, into a single rule. The
set of all such rules constitutes a specialised unifica-
tion grammar. Each of these specialised unification
grammars is then subjected to a second compila-
tion step, which converts it into its executable form.
For analysis and generation, this form is a standard
parser or generator. For recognition, it is a semanti-
cally annotated CFG grammar in the form required
by the Nuance engine, which is then subjected to
further Nuance-specific compilation steps to derive
a speech recognition package. Figure 1 summarises
compile-time processing.
The Regulus platform also contains infrastructure
to support construction of applications which use the
recognisers, parsers and generators as components.
In this paper, we will only discuss spoken dialogue
system applications. (There is also an elaborate in-
frastructure to support speech translation systems).
15
Figure 2: Top-level architecture for Regulus-based spo-
ken dialogue system
At a high level of generality, the architecture is a
standard one (Figure 2; cf. for example (Allen et
al., 2000)). The central component is the Dialogue
Manager (DM), which receives dialogue moves and
produces abstract actions. It also manipulates an in-
formation state, which maintains context; process-
ing will generally be context-dependent. The DM is
bracketed between two other components, the Input
Manager (IM) and the Output Manager (OM). The
IM receives logical forms, and non-speech inputs if
there are any, and turns them into dialogue moves.
The OM received abstract actions and turns them
into concrete actions. Usually, these actions will be
either speaking, though TTS or recorded speech, or
manipulation of a GUI?s screen area.
In the next section, we examine in more detail
how the various components are constructed, and
what the implications are for the software develop-
ment cycle. We will in particular be interested in
regression testing.
3 Context, regression testing and
paraphrasing
The three main components of the spoken dia-
logue system ? the IM, DM and OM ? all trans-
form one or more inputs into one or more outputs.
With the current focus on machine learning tech-
niques, a natural thought is to learn the relevant
tranformations from examples. Implemented mainly
through Partially Observable Markov Decision Pro-
cesses (POMDPs), this idea is attractive theoreti-
cally, but has been challenging to scale up. Systems
have been restricted to very simple domains (Roy
et al, 2000; Zhang et al, 2001) and only recently
have techniques been developed that show promise
for use in real-world systems (Williams and Young,
2007; Gasic? et al, 2008). The representations re-
quired in many systems are more complex than those
employed even in the more recent POMDP based
work, and there is also the usual problem that it is not
easy to obtain training data. In practice, most peo-
ple are forced to construct the transformation rules
by hand; the Regulus framework assumes this will
be the case. Hand-coding of dialogue processing
components involves the usual software engineering
problems that arise when building and maintaining
substantial rule-sets. In particular, it is necessary to
have a framework that supports efficient regression
testing.
As everyone who has tried will know, the thing
that makes regression testing difficult for this kind
of application is context-dependency. In the worst
case, the context is the whole world, or at least the
part of it that the system is interacting with, and re-
gression testing is impossible. In more typical cases,
however, good architectural choices can make the
problem reasonably tractable. In particular, things
become enormously simpler if it is possible to en-
capsulate all the context information in a datastruc-
ture that can be passed around. In the dialogue man-
agement architecture realised in Regulus (Rayner et
al., 2006, Chapter 5), the assumption is that this is
the case; it is then possible to use a version of ?up-
date semantics? (Larsson and Traum, 2000). The
central concepts are those of dialogue move, infor-
mation state and dialogue action. At the beginning
of each turn, the dialogue manager is in an infor-
mation state. Inputs to the dialogue manager are by
definition dialogue moves, and outputs are dialogue
actions. The behaviour of the dialogue manager over
a turn is completely specified by an update function
f of the form
f : State?Move ? State?Actions
Thus if a dialogue move is applied in a given infor-
mation state, the result is a new information state
and a set of zero or more dialogue actions.
3.1 Regression testing
Using the side-effect free framework is certainly a
large step in the right direction; it is in principle pos-
16
sible to construct a regression test suite consisting of
4-tuples of the form
?InState,Move,OutState,Actions?
There are however several problems. First, pro-
cessing consists of much more than just the update
function. It is optimistic to assume that the speech
recogniser will be able to produce dialogue moves
directly. In simple cases, this may be possible. In
more complex cases, extra levels of processing be-
come necessary; in other words, the IM component
will generally have a substantial amount of structure.
There are several reasons for this. The representa-
tion delivered by the grammar-based speech recog-
niser is syntax-oriented; it needs to be translated
into a semantic form. Again, because of context-
dependency, this translation often needs to be car-
ried out in more than one step. For example, in the
Calendar application, a question like ?When is the
next meeting in Switzerland?? might be followed by
the elliptical utterance ?In England??. Some kind
of mechanism is needed in order to resolve this to
a representation meaning ?When is the next meet-
ing in England?? A separate mechanism is used to
perform reference resolution. For instance, the de-
fault database for the Calendar application contains
one person called ?Nikos? and two called ?Mari-
anne?. The question ?Is Nikos attending the meet-
ing?? needs to be converted into a database query
that looks up the appropriate record; however, the
structurally similar query ?Is Marianne attending the
meeting?? should produce a disambiguation query.
Examples like these motivate the introduction of yet
another processing step, which carries out reference
resolution.
Of course, different systems will address these is-
sues in different ways; but, whatever the solution,
the general point remains that there will usually be
many layers of representation. From a system devel-
opment point of view, the problem is how to struc-
ture the regression testing needed in order to main-
tain the stability of each processing step. The most
cautious and direct way to do this is to have a corpus
of input/output tuples representing each individual
step, but experience shows that this type of solution
places an enormous burden on the annotators who
are required to judge the correctness or otherwise of
the tuples. First of all, under this approach the anno-
tators must be experts capable of reading and under-
standing internal representations. Second, even very
small changes in the system often require complete
reannotation of the test corpus; for example, some
data structure may have been changed so as to in-
clude an extra field. If constant rejudging is required
to keep the test suite coherent with the current ver-
sion of the system, either the testing is abandoned as
overly difficult and time consuming, or it is done in
a less careful way in order to speed up the process.
Neither outcome is satisfactory.
If annotation uses input/output tuples referring to
internal representations, the problems we have just
named appear inescapable. At the opposite end of
the spectrum, a common approach is not to look
at internal representations at all, but only at in-
put/output pairs consisting of top-level inputs and
outputs. For example, we can pair ?When is the next
meeting in Geneva?? with ?March 31 2009?, and
?Is Marianne attending the meeting?? with ?Which
Marianne do you mean?? This is generally, in prac-
tice, easier than doing regression testing on internal
representations; the key advantages are that, since
we are only dealing with pre-theoretical notions, an-
notation can be performed by non-experts, and an-
notations remain stable across most changes to in-
ternal representations.
Unfortunately, however, new problems arise.
First, determining the correct output response for a
given input is often tedious and slow. For example,
in the Calendar application, this generally involves
carrying out a database search. Suitable annotation
tools can alleviate the pain here, but then a worse
problem arises; it is often possible to produce a cor-
rect system response, even if processing is incorrect.
For instance, even if the system correctly answers
?No? to a yes/no question, this proves very little;
the question could have been interpreted in a mul-
titude of ways, and still produced a negative answer.
Knowing that a WH-question provides a correct an-
swer says more, but can still often be misleading.
Suppose, for example, that we know that the Calen-
dar system correctly answers ?None? to the question
?What meetings are there during the next week??
and there are no meetings for the next 15 days. We
will be unable to tell whether the question has been
interpreted as ?What meetings are there during the
17
World Context time=2008-10-14 14:34, speaker=mike
Last Para (none)
Input when is the next meeting with mark
Paraphrase when is [ the next meeting attended by mark green ]
World Context time=2008-10-16 09:47, speaker=mike
Last Para (none)
Input when is my next meeting with mark
Paraphrase when is [ the next meeting attended by mark green and mike jones ]
World Context time=2007-07-08 15:03, speaker=susan
Last Para (none)
Input is there a meeting next week
Paraphrase are there meetings between Mon Jul 9 2007 and Sun Jul 15 2007
World Context time=2008-11-17 18:20, speaker=mike
Last Para (none)
Input do i have a meeting on friday morning this week
Paraphrase are there meetings between 06:00 and 12:00 on Fri Nov 21 2008 attended by mike jones
World Context time=2008-11-12 10:19, speaker=mike
Last Para when is [ the next meeting attended by mike jones ]
Input will alex participate
Paraphrase will that meeting be attended by alex miller
World Context time=2007-07-08 15:56), speaker=susan
Last Para are there meetings on Mon Jul 9 2007
Input how about on tuesday
Paraphrase are there meetings on Tue Jul 10 2007
Table 1: Examples of regression testing tuples in the English Calendar system. Each tuple shows the current world
context (timestamp and speaker), the preceding paraphrase, the input, and the paraphrase produced from it.
next 7 days??, as ?What meetings are there during
the 7 day period starting this Sunday?? or as ?What
meetings are there during the 7 day period starting
this Monday?? Examples like these mean that re-
gression testing often fails to catch bugs introduced
by system changes.
3.2 Paraphrasing dialogue moves
To summarise: when carrying out regression testing,
we have two competing requirements. First, we need
to be able to access internal representations, since
they are so informative. At the same time, we prefer
to work with human-readable, pretheorically mean-
ingful objects, which will be stable at least under
most small changes in underlying representations.
There is, in fact, a good compromise between these
goals: we define a transformation which realises the
dialogue move as a human-readable string, which
we call a dialogue move paraphrase. So, for ex-
ample, consider the possible interpretations when,
on March 6 2009, a user asks ?What meetings are
there during the next week??. If ?What meetings
are there during the next week?? is interpreted as
?What meetings are there during the next 7 days??,
then the paraphrase might be ?What meetings are
there between Fri Mar 6 and Thu Mar 12 2009??; if
the interpretation is ?What meetings are there dur-
ing the 7 day period starting this Monday??, then
the corresponding paraphrase would be ?What meet-
ings are there between Mon Mar 9 and Sun Mar 15
2009?? Regression testing can be carried out using
paraphrases of dialogue moves, rather than the dia-
logue moves themselves.
The paraphrase mechanism is implemented as a
Regulus grammar, compiled in generation mode,
which directly relates a dialogue move and its sur-
face form. We have found that it is not hard to de-
sign ?paraphrase grammars? which produce outputs
18
fulfilling the main design requirements. Regression
testing is carried out on tuples consisting of the pre-
ceding paraphrase, the world context (if any), the in-
put, and the resulting paraphrase. Examples of such
tuples for the English Calendar grammar are shown
in Table 1; in Calendar, the world context consists of
the utterance time-stamp and the speaker.
A tuple combines the results of IM and DM
(but not OM) processing for a given example, and
presents them in a pre-theoretically meaningful way.
Although they are not as fine-grained as tuples for
individual processing steps, they are stable over
most system changes. In the opposite direction, they
are far more fine-grained than straight system in-
put/system output tuples. They are much easier to
judge than both of the other types of tuple. The bot-
tom line, at least as far as we are concerned, is that a
regression testing database of paraphrase-based tu-
ples can actually be maintained without inordinate
effort, implying corresponding gains for system sta-
bility. Previously, this was impossible.
The idea of creating paraphrases from dialogue
moves is of course not new; in previous work, how-
ever, they have generally been used at runtime to
provide feedback to the user as to how their input has
been interpreted by the system. Although in the cur-
rent discussion we have been more concerned with
their use in regression testing, we have in fact also
employed them for the more traditional purpose.
We return to this theme in Section 5. First, we
describe in more detail where our test suites come
from.
4 Collecting test suites
The tradition in the speech engineering community
is that a test suite consists of a list of recorded wav-
files, together with accompanying transcriptions.
The Nuance platform contains a fair amount of in-
frastructure, in particular the batchrec utility, for
processing lists of wavfiles. These tools are very
useful for computing measures like WER, and there
is a strong temptation to try to build on top of
them. After a while, however, we discovered that
they meshed poorly with the the basic goals of re-
gression testing in a spoken dialogue system, which
revolve around speech understanding rather than
speech recognition. There are two central problems.
One of them is context-dependence, which we have
already discussed at length. The other is the fact that
many applications require that the IM process both
speech and non-speech actions, with the sequence
and even the timing of actions being important.
For example, as we have already seen, time is
a central concept in the Calendar system. If the
user says ?Are there any meetings this afternoon??
the system interprets her as meaning ?Are there any
meetings from now until the end of the afternoon??
This means that the exact clock time for each utter-
ance is important. In the Taxi application, the taxi is
continually in motion, even when the user is not talk-
ing. The simulator sends the IM an non-speech mes-
sage several times a second, giving the taxi?s new
position and heading. This information is passed to
the DM, updating its state, and is essential for cor-
rect interpretation of commands like ?Turn right at
the next corner?.
Considerations like these finally convinced us to
move to a different strategy, in which offline regres-
sion testing more closely models the runtime be-
haviour of the application. At runtime, the system
produces a time-stamped log of all input passed to
the IM, including both speech and non-speech mes-
sages, in the sequence in which they were received.
Each speech message is paired with a pointer to the
recorded wavfile which produced it. Sets of such
logs make up the test suite. Offline testing essen-
tially re-runs the sequence of time-stamped records.
Wavfiles are passed to a recognition server, which
returns recognition results; time-stamps are used to
set a notional internal clock, which replaces the real
one for the purposes of performing temporal calcula-
tions. The test harness was quite easy to implement,
and solves all the problems that arose from close ad-
herence to a more speech recognition oriented test
framework.
5 Using paraphrases at run-time
As mentioned in Section 3.2, paraphrase grammars
can also be used at runtime, in order to provide
a direct confirmation to the user showing how the
system has interpreted what they have said. This
is not a compelling design for every system; in a
speech-only system, constant direct confirmation us-
ing paraphrases is in most cases unnatural and te-
19
dious. It is, however, a potentially valid strategy in a
multi-modal system where it is possible to present a
visual display of the paraphrase. In such a system, if
paraphrases are regularly displayed to a user, there
is, however, a good possibility of lexical and/or syn-
tactic entrainment. Entrainment increases the likeli-
hood that the user will produce the paraphrase lan-
guage, which means that it would be valuable to be
able to process that language through the system.
In the Regulus framework, this problem can be
very straightforwardly addressed. Since the para-
phrase grammar is a valid Regulus grammar, it can
be compiled into a Nuance grammar, and hence into
a second recognition package. At runtime, this pack-
age can be used in parallel with the main system
recogniser. Because the paraphrase grammar is de-
signed to directly relate surface language to dialogue
moves, dialogue moves are generated directly, skip-
ping the Input Manager processing. In particular,
since the original point of the paraphrase grammar
is to restate the user?s content in a way that resolves
and disambiguates underspecified material, there is
no need for resolution processing. Figure 3 shows
the dialogue system architecture with the additional
paraphrase processing path.
Input
Manager
Dialogue
Manager
Output
Manager
Main
Recognizer
Paraphrase
Recognizer
Playback
or TTS
GUI
Logical Form
Dialogue
Move
Dialogue
Move
Abstract
Action
Concrete
Action
Concrete
Action
Figure 3: Regulus dialogue architecture with a processing
path for paraphrases added. The paraphrase recognizer
sends a dialogue move directly to the Dialogue Manager.
Although it may seem preferable to include the
paraphrase coverage in the main recogniser cover-
age, we have found, somewhat to our surprise, that
this is not nearly as straightforward as it first ap-
pears. The problem is that the two grammars are de-
signed for very different tasks; the recognition gram-
mar is intended to capture natural user language,
while the paraphrase grammar?s job is to produce
unambiguous surface renderings of resolved seman-
tic representations. Although we have endeavoured
to make the paraphrase language as natural as pos-
sible, it is hard to avoid at least a few marginal
constructions, which do not fit well into the struc-
ture of the normal recognition grammar; even if we
did try to include them, the burden of keeping the
two different grammars in synch would be consider-
able. From a software engineering point of view, it
is far simpler just to maintain the two grammars sep-
arately, with each of them generating its own version
of the recogniser.
We tested the paraphrase grammar recognizer for
the Calendar application using paraphrases taken
from a previous run log and recorded by the two
authors. There were 249 recorded paraphrases to-
tal used. Because the Calendar paraphrase grammar
had originally been designed with only visual dis-
play in mind, some augmentation of the paraphrase
grammar was needed to cover the spoken versions
of the paraphrases. There is often more than one
possible spoken version corresponding to a written
representation as was the case for this data. For ex-
ample with a paraphrase such as ?when are meet-
ings on Sat Jan 3 2009?, ?Sat? could be pronounced
?sat? or ?Saturday?, ?3? could be ?third? or ?three?,
?Jan? could be produced as either ?jan? or ?January?
and ?2009? could be ?two thousand nine? or ?two
thousand and nine?. With the paraphrase compo-
nent structured as a standard Regulus grammar, all
that was needed was to add lexical items to cover
the spoken variants. These additions were restricted
to the recognition use of the paraphrase grammar
and not used for generation. Word Error (WER) was
4.43% for the paraphrase grammar recognizer, Sen-
tence Error (SER)was 34.53% and Semantic Error
(SemER)was 17.9%. This SemER was calculated
on untuned n-best. Clearly it is not possible to com-
pare with the main recognizer on the same data, but
for a rough comparison, we can look at numbers re-
ported for the Calendar application in (Georgescul
et al, 2008). That paper reports WER of 11.17%
and SemER of 18.85% for the 1-best baseline. The
SemER on the paraphrase grammar is 21.5% for 1-
best. The paraphrase grammar recognizer has much
better WER because it is so much more restricted
than the main recognizer. However, the sentences
covered by the paraphrase grammar are much longer
than those covered in the main grammar, and this
difference is reflected in the poorer performance by
paraphrase grammar when measured in terms of Se-
20
mER. The paraphrase language is long, very unnatu-
ral, yet we are able to produce a level of recognition
performance that is quite usable.
Given the ability to recognize with the paraphrase
grammar, a question which we hope to be able
to investigate empirically is the effect that entrain-
ment from exposure to the longer and less natural
paraphrases actually has on user language, which
initially tends to be biased towards short, natural-
sounding utterances, with frequent use of ellipsis.
This is a interesting topic for future research.
6 Conclusions
The dialogue move paraphrase mechanism provides
a useful approach to streamlining regression testing
without abandoning necessary detail. In non-trivial
spoken dialogue systems, it is generally necessary
to have a number of levels of representation. Our
approach provides a middle ground between track-
ing each of these levels in the test suites, creating a
excessive maintenance burden, and keeping only top
level inputs and outputs, which is too coarse-grained
to catch many errors. The Regulus framework pro-
vides the opportunity to implement this mechanism
as a Regulus grammar, which makes the compilation
into recognisers, parsers and generators available.
While generation with the paraphrase grammar sup-
ports the described improvement in regression test-
ing methodology, compiling the paraphrase gram-
mar into a recogniser allows us to ensure that para-
phrases used as confirmations can also be processed
if directed at the dialogue system. The framework
has been used with several fairly different kinds of
applications, and appears to have a major impact on
the overhead associated with maintenance of a use-
ful regression testing regime.
References
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2000. An architecture for
a generic dialogue shell. Natural Language Engineer-
ing, Special Issue on Best Practice in Spoken Lan-
guage Dialogue Systems Engineering, pages 1?16.
P. Bouillon, M. Rayner, B. Novellas, M. Starlander,
M. Santaholma, Y. Nakao, and N. Chatzichrisafis.
2007. Une grammaire partage?e multi-ta?che pour le
traitement de la parole: application aux langues ro-
manes. TAL.
P. Bouillon, G. Flores, M. Georgescul, S. Halimi, B.A.
Hockey, H. Isahara, K. Kanzaki, Y. Nakao, M. Rayner,
M. Santaholma, M. Starlander, and N. Tsourakis.
2008. Many-to-many multilingual medical speech
translation on a PDA. In Proceedings of The Eighth
Conference of the Association for Machine Translation
in the Americas, Waikiki, Hawaii.
Milica Gasic?, Simon Keizer, Francois Mairesse, Jost
Schatzmann, Blaise Thomson, Kai Yu, and Steve
Young. 2008. Training and evaluation of the his
pomdp dialogue system in noise. In Proceedings of the
9th SIGDIAL Workshop on Discourse and Dialogue.
Maria Georgescul, Manny Rayner, Pierrette Bouillon,
and Nikos Tsourakis. 2008. Discriminative learning
using linguistic features to rescore n-best speech hy-
potheses. In The IEEE Workshop on Spoken Language
Technology, Goa, India.
S. Larsson and D. Traum. 2000. Information state and
dialogue management in the TRINDI dialogue move
engine toolkit. Natural Language Engineering, Spe-
cial Issue on Best Practice in Spoken Language Dia-
logue Systems Engineering, pages 323?340.
M. Rayner, B.A. Hockey, J.M. Renders,
N. Chatzichrisafis, and K. Farrell. 2005. A voice
enabled procedure browser for the international space
station. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(interactive poster and demo track), Ann Arbor, MI.
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Putting
Linguistics into Speech Recognition: The Regulus
Grammar Compiler. CSLI Press, Chicago.
M. Rayner. 1988. Applying explanation-based general-
ization to natural-language processing. In Proceedings
of the International Conference on Fifth Generation
Computer Systems, pages 1267?1274, Tokyo, Japan.
N. Roy, J Pineau, and S. Thrun. 2000. Spoken dia-
logue management using probabilistic reasoning. In
Proceedings of ACL, Hong Kong.
N. Tsourakis, M. Georghescul, P. Bouillon, and
M. Rayner. 2008. Building mobile spoken dialogue
applications using regulus. In Proceedings of LREC
2008, Marrakesh, Morocco.
T. van Harmelen and A. Bundy. 1988. Explanation-
based generalization = partial evaluation (research
note). Artificial Intelligence, 36:401?412.
JD Williams and SJ Young. 2007. Partially observable
markov decision processes for spoken dialog systems.
Computer Speech and Language.
B. Zhang, Q Cai, J. Mao, E. Chang, and B Guo. 2001.
Spoken dialogue management as planning and acting
under uncertainty. In Proceedings of Eurospeech, Aal-
borg, Denmark.
21
Evaluating Task Performance for a Unidirectional Controlled Language 
Medical Speech Translation System 
 
 
Nikos Chatzichrisafis, Pierrette Bouillon, Manny Rayner, Marianne Santaholma, 
Marianne Starlander 
University of Geneva, TIM/ISSCO 
40 bvd du Pont-d'Arve, CH-1211 Geneva 4, Switzerland 
 
Nikos.Chatzichrisafis@vozZup.com, Pierrette.Bouillon@issco.unige.ch, 
Emmanuel.Rayner@issco.unige.ch, Marianne.Santaholma@eti.unige.ch, 
Marianne.Starlander@eti.unige.ch  
 
Beth Ann Hockey 
UCSC 
NASA Ames Research Center 
Moffett Field, CA 94035 
bahockey@email.arc.nasa.gov 
 
  
Abstract 
We present a task-level evaluation of the 
French to English version of MedSLT, a 
medium-vocabulary unidirectional con-
trolled language medical speech transla-
tion system designed for doctor-patient 
diagnosis interviews. Our main goal was 
to establish task performance levels of 
novice users and compare them to expert 
users. Tests were carried out on eight 
medical students with no previous expo-
sure to the system, with each student us-
ing the system for a total of three 
sessions. By the end of the third session, 
all the students were able to use the sys-
tem confidently, with an average task 
completion time of about 4 minutes. 
1 Introduction 
Medical applications have emerged as one of the 
most promising application areas for spoken lan-
guage translation, but there is still little agreement 
about the question of architectures. There are in 
particular two architectural dimensions which we 
will address: general processing strategy (statistical 
or grammar-based), and top-level translation func-
tionality (unidirectional or bidirectional transla-
tion). Given the current state of the art in 
recognition and machine translation technology, 
what is the most appropriate combination of 
choices along these two dimensions? 
Reflecting current trends, a common approach 
for speech translation systems is the statistical one. 
Statistical translation systems rely on parallel cor-
pora of source and target language texts, from 
which a translation model is trained. However, this 
is not necessarily the best alternative in safety-
critical medical applications. Anecdotally, many 
doctors express reluctance to trust a translation 
device whose output is not readily predictable, and 
most of the speech translation systems which have 
reached the stage of field testing rely on various 
types of grammar-based recognition and rule-based 
translation (Phraselator, 2006; S-MINDS, 2006; 
MedBridge, 2006). Even though statistical systems 
exhibit many desirable properties (purely data-
driven, domain independence), grammar-based 
systems utilizing probabilistic context-free gram-
mar tuning appear to deliver better results when 
training data is sparse (Rayner et al, 2005a). 
One drawback of grammar-based systems is that 
out-of-coverage utterances will be neither recog-
nized nor translated, an objection that critics have 
sometimes painted as decisive. It is by no means 
obvious, however, that restricted coverage is such 
a serious problem. In text processing, work on sev-
eral generations of controlled language systems has 
developed a range of techniques for keeping users 
within the bounds of system coverage (Kittredge, 
2003; Mitamura, 1999). If these techniques work 
for text processing, it is surely not inconceivable 
that variants of them will be equally successful for 
spoken language applications. Users are usually 
able to adapt to a controlled language system given 
enough time. The critical questions are how to 
provide efficient support to guide them towards the 
system's coverage, and how much time they will 
then need before they have acclimatized. 
With regard to top-level translation functional-
ity, the choice is between unidirectional and bidi-
rectional systems. Bidirectional systems are 
certainly possible today1, but the arguments in fa-
vor of them are not as clear-cut as might first ap-
pear. Ceteris paribus, doctors would certainly 
prefer bidirectional systems; in particular, medical 
students are trained to conduct examination dia-
logues using ?open questions? (WH-questions), 
and to avoid leading the patient by asking YN-
questions. 
The problem with a bidirectional system is, 
however, that open questions only really work well 
if the system can reliably handle a broad spectrum 
of replies from the patients, which is over-
optimistic given the current state of the art. In prac-
tice, the system's coverage is always more or less 
restricted, and some experimentation is required 
before the user can understand what language it is 
capable of handling. A doctor, who uses the system 
regularly, will acquire the necessary familiarity. 
The same might be true for a few patients, if spe-
cial circumstances mean that they encounter 
speech translation applications reasonably fre-
quently. Most patients, however, will have had no 
previous exposure to the system, and may be un-
willing to use a type of technology which they 
have trouble understanding.  
A unidirectional system, in which the doctor 
mostly asks YN-questions, will never be ideal. If, 
                                                          
1
 For example, the S-MINDS system (S-MINDS, 2006) 
offers bidirectional translation. 
however, the doctor can become proficient in using 
it, it may still be very much better than the alterna-
tive of no translation assistance at all.  
To summarize, today?s technology definitely 
lets us build unidirectional grammar-based medical 
speech translation systems which work for regular 
users who have had time to adapt to their limita-
tions. While bidirectional systems are possible, the 
case for them is less obvious, since users on the 
patient side may not in practice be able to use them 
effectively. 
In this paper, we will empirically investigate the 
ability of medical students to adapt to the coverage 
of unidirectional spoken language translation sys-
tem. We report a series of experiments, carried out 
using a French to English speech translation sys-
tem, in which medical students with no previous 
experience to the system were asked to use it to 
carry out a series of verbal examinations on sub-
jects who were simulating the symptoms of various 
types of medical conditions. Evaluation will be 
focused on usability. We primarily want to know 
how quickly subjects learn to use the system, and 
how their performance compares to that of expert 
users. 
2 The MedSLT system 
MedSLT (MedSLT, 2005; Bouillon et al, 2005) 
is a unidirectional, grammar-based medical speech 
translation system intended for use in doctor-
patient diagnosis dialogues. The system is built on 
top of Regulus (Regulus, 2006), an Open Source 
platform for developing grammar-based speech 
applications. Regulus supports rapid construction 
of complex grammar-based language models using 
an example-based method (Rayner et al, 2003; 
Rayner et al, 2006), which extracts most of the 
structure of the model from a general linguistically 
motivated resource grammar. Regulus-based rec-
ognizers are reasonably easy to maintain, and 
grammar structure is shared automatically across 
different subdomains. Resource grammars are now 
available for several languages, including English, 
Japanese (Rayner et al, 2005b), French (Bouillon 
et al, 2006) and Spanish. 
MedSLT includes a help module, whose purpose 
is to add robustness to the system and guide the 
user towards the supported coverage. The help 
module uses a second backup recognizer, equipped 
with a statistical language model; it matches the 
results from this second recognizer against a cor-
pus of utterances, which are within system cover-
age and have already been judged to give correct 
translations. In previous studies (Rayner et al, 
2005a; Starlander et al, 2005), we showed that the 
grammar-based recognizer performs much better 
than the statistical one on in-coverage utterances, 
and rather worse on out-of-coverage ones. We also 
found that having the help module available ap-
proximately doubled the speed at which subjects 
learned to use the system, measured as the average 
difference in semantic error rate between the re-
sults for their first quarter-session and their last 
quarter-session. It is also possible to recover from 
recognition errors by selecting one of the displayed 
help sentences; in the cited studies, we found that 
this increased the number of acceptably processed 
utterances by about 10%. 
The version of MedSLT used for the experi-
ments described in the present paper was config-
ured to translate from spoken French into spoken 
English in the headache subdomain. Coverage is 
based on standard headache-related examination 
questions obtained from a doctor, and consists 
mostly of yes/no questions. WH-questions and el-
liptical constructions are also supported. A typical 
short session with MedSLT might be as follows: 
- is the pain in the side of the head? 
- does the pain radiate to the neck? 
- to the jaw? 
- do you usually have headaches in the morn-
ing ?  
The recognizer?s vocabulary is about 1000 sur-
face words; on in-grammar material, Word Error 
Rate is about 8% and semantic error rate (per ut-
terance) about 10% (Bouillon et al, 2006). Both 
the main grammar-based recognizer and the statis-
tical recognizer used by the help system were 
trained from the same corpus of about 975 utter-
ances. Help sentences were also taken from this 
corpus. 
3 Experimental Setup 
In previous work, we have shown how to build a 
robust and extendable speech translation system. 
We have focused on performance metrics defined 
in terms of recognition and translation quality, and 
tested the system on na?ve users without any medi-
cal background (Bouillon et al, 2005; Rayner et 
al., 2005a; Starlander et al, 2005). 
In this paper, our primary goal was rather to fo-
cus on task performance evaluation using plausible 
potential users. The basic methodology used is 
common in evaluating usability in software sys-
tems in general, and spoken language systems in 
particular (Cohen et. al 2000). We defined a simu-
lated situation, where a French-speaking doctor 
was required to carry out a verbal examination of 
an English-speaking patient who claimed to be suf-
fering from a headache, using the MedSLT system 
to translate all their questions. The patients were 
played by members of the development team, who 
had been trained to answer questions consistently 
with the symptoms of different medical conditions 
which could cause headaches. We recruited eight 
native French-speaking medical students to play 
the part of the doctor. All of the students had com-
pleted at least four years of medical school; five of 
them were already familiar with the symptoms of 
different types of headaches, and were experienced 
in real diagnosis situations. 
The experiment was designed to study how well 
users were able to perform the task using the 
MedSLT system. In particular, we wished to de-
termine how quickly they could adapt to the re-
stricted language and limited coverage of the 
system. As a comparison point, representing near-
perfect performance, we also carried out the same 
test on two developers who had been active in im-
plementing the system, and were familiar with its 
coverage. 
Since it seemed reasonable to assume that most 
users would not interact with the system on a daily 
basis, we conducted testing in three sessions, with 
an interval of two days between each session. At 
the beginning of the first session, subjects were 
given a standardized 10-minute introduction to the 
system. This consisted of instruction on how to set 
up the microphone, a detailed description of the 
MedSLT push-to-talk interface, and a video clip 
showing the system in action. At the end of the 
presentation, the subject was given four sample 
sentences to get familiar with the system. 
After the training was completed, subjects were 
asked to play the part of a doctor, and conduct an 
examination through the system. Their task was to 
identify the headache-related condition simulated 
by the ?patient?, out of nine possible conditions. 
Subjects were given definitions of the simulated 
headache types, which included conceptual infor-
mation about location, duration, frequency, onset 
and possible other symptoms the particular type of 
headache might exhibit. 
Subjects were instructed to signal the conclusion 
of their examination when they were sure about the 
type of simulated headache. The time required to 
reach a conclusion was noted in the experiment 
protocols by the experiment supervisor. 
The subjects repeated the same diagnosis task on 
different predetermined sets of simulated condi-
tions during the second and third sessions. The ses-
sions were concluded either when a time limit of 
30 minutes was reached, or when the subject com-
pleted three headache diagnoses. At the end of the 
third session, the subject was asked to fill out a 
questionnaire. 
4 Results 
Performance of a speech translation system is 
best evaluated by looking at system performance 
as a whole, and not separately for each subcompo-
nent in the systems processing pipeline (Rayner et. 
al. 2000, pp. 297-pp. 312). In this paper, we conse-
quently focus our analysis on objective and subjec-
tive usability-oriented measures. 
In Section 4.1, we present objective usability 
measures obtained by analyzing user-system inter-
actions and measuring task performance. In Sec-
tion 4.2, we present subjective usability figures and 
a preliminary analysis of translation quality. 
4.1 Objective Usability Figures 
4.1.1 Analysis of User Interactions 
Most of our analysis is based on data from the 
MedSLT system log, which records all interactions 
between the user and the system. An interaction is 
initiated when the user presses the ?Start Recogni-
tion? button. The system then attempts to recog-
nize what the user says. If it can do so, it next 
attempts to show the user how it has interpreted the 
recognition result, by first translating it into the 
Interlingua, and then translating it back into the 
source language (in this case, French). If the user 
decides that the back-translation is correct, they 
press the ?Translate? button. This results in the 
system attempting to translate the Interlingua rep-
resentation into the target language (in this case, 
English), and speak it using a Text-To-Speech en-
gine. The system also displays a list of ?help sen-
tences?, consisting of examples that are known to 
be within coverage, and which approximately 
match the result of performing recognition with the 
statistical language model. The user has the option 
of choosing a help sentence from the list, using the 
mouse, and submitting this to translation instead.  
We classify each interaction as either ?success-
ful? or ?unsuccessful?. An interaction is defined to 
be unsuccessful if either 
i) the user re-initiates recognition without 
asking the system for a translation, or 
ii) the system fails to produce a correct 
translation or back translation. 
Our definition of ?unsuccessful interaction? in-
cludes instances where users accidentally press the 
wrong button (i.e. ?Start Recognition? instead of 
?Translate?), press the button and then say nothing, 
or press the button and change their minds about 
what they want to ask half way through. We ob-
served all of these behaviors during the tests. 
Interactions where the system produced a trans-
lation were counted as successful, irrespective of 
whether the translation came directly from the 
user?s spoken input or from the help list. In at least 
some examples, we found that when the translation 
came from a help sentence it did not correspond 
directly to the sentence the user had spoken; to our 
surprise, it could even be the case that the help sen-
tence expressed the directly opposite question to 
the one the user had actually asked. This type of 
interaction was usually caused by some deficiency 
in the system, normally bad recognition or missing 
coverage. Our informal observation, however, was 
that, when this kind of thing happened, the user 
perceived the help module positively: it enabled 
them to elicit at least some information from the 
patient, and was less frustrating than being forced 
to ask the question again. 
Table I to Table III show the number of total in-
teractions per session, the proportion of successful 
interactions, and the proportion of interactions 
completed by selecting a sentence from the help 
list. The total number of interactions required to 
complete a session decreased over the three ses-
sions, declining from an average of 98.6 interac-
tions in the first session to 63.4 in the second (36% 
relative) and 53.9 in the third (45% relative). It is 
interesting to note that interactions involving the 
help system did not decrease in frequency, but re-
mained almost constant over the first two sessions 
(15.5% and 14.0%), and were in fact most com-
mon during the third session (21.7%). 
 
Session 1 
Subject Interactions % Successful % Help 
User 1 57 56.1% 0.0% 
User 2 98 52.0% 25.5% 
User 3 91 63.7% 15.4% 
User 4 156 69.9% 10.3% 
User 5 86 64.0% 22.1% 
User 6 134 47.0% 19.4% 
User 7 56 53.6% 5.4% 
User 8 111 63.1% 26.1% 
AVG 98.6 58.7% 15.5% 
Table I Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 1st session 
 
Session 2 
Subject Interactions % Successful % Help 
User 1 50 74.0% 2.0% 
User 2 63 55.6% 27.0% 
User 3 34 88.2% 23.5% 
User 4 96 57.3% 17.7% 
User 5 64 65.6% 21.9% 
User 6 93 68.8% 10.8% 
User 7 48 60.4% 4.2% 
User 8 59 79.7% 5.1% 
AVG 63.4 68.7% 14.0% 
Table II Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 2nd session 
 
Session 3 
Subject Interactions % Successful % Help 
User 1 33 90.9% 33.3% 
User 2 57 56.1% 22.8% 
User 3 48 72.9% 29.2% 
User 4 67 70.2% 16.4% 
User 5 68 73.5% 27.9% 
User 6 60 70.0% 6.7% 
User 7 41 65.9% 14.6% 
User 8 57 56.1% 22.8% 
AVG 53.9 69.5% 21.7% 
Table III Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 3rd session 
In order to establish a performance baseline, we 
also analyzed interaction data for two expert users, 
who performed the same experiment. The expert 
users were two native French-speaking system de-
velopers, which were both familiar with the diag-
nosis domain. Table IV summarizes the results of 
those users. One of our expert users, listed as Ex-
pert 2, is the French grammar developer, and had 
no failed interactions. This confirms that recogni-
tion is very accurate for users who know the cov-
erage. 
 
Session 1 / Expert Users 
Subject Interactions % Successful % Help 
Expert 1 36 77.8% 13.9% 
Expert 2 30 100.0% 3.3% 
AVG 33 88.9% 8.6% 
Table IV Number of interactions, and percentages 
of successful interactions, and interactions 
involving the help component 
 
The expert users were able to complete the ex-
periment using an average of 33 interaction rounds. 
Similar performance levels were achieved by some 
subjects during the second and third session, which 
suggests that it is possible for at least some new 
users to achieve performance close to expert level 
within a few sessions. 
4.1.2 Task Level Performance 
One of the important performance indicators for 
end users is how long it takes to perform a given 
task. During the experiments, the instructors noted 
completion times required to reach a definite diag-
nosis in the experiment log. Table VI shows task 
completion times, categorized by session (col-
umns) and task within the session (rows).  
 Session 1 Session 2 Session 3 
Diagnosis 1 17:00 min 11:00 min 7:54 min 
Diagnosis 2 11:00 min 6:18 min 5:34 min 
Diagnosis 3 7:54 min 4:10 min 4:00 min 
Table V Average time required by subjects to 
complete diagnoses 
 
In the last two sessions, after subjects had ac-
climatized to the system, a diagnosis takes an aver-
age of about four minutes to complete. This 
compares to a three-minute average required to 
complete a diagnosis by our expert users. 
4.1.3 System coverage 
Table VI shows the percentage of in-coverage 
sentences uttered by the users on interactions that 
did not involve invocation of the help component. 
 
 IN-COVERAGE SENTENCES 
Session 1 54.9% 
Session 2 60.7% 
Session 3 64.6% 
Table VI Percentage of in-coverage sentences 
 
This indicates that subjects learn and adapt to 
the system coverage as they use the system more. 
The average proportion of in-coverage utterances 
is 10 percent higher during the third session than 
during the first session. 
4.2 Subjective Usability Measures 
4.2.1 Results of Questionnaire 
After finishing the third session, subjects were 
asked to fill in a short questionnaire, where re-
sponses were on a five-point scale ranging from 1 
(?strongly disagree?) to 5 (?strongly agree?). The 
results are presented in Table VIII. 
 
STATEMENT SCORE 
I quickly learned how to use the system. 4.4 
System response times were generally 
satisfactory. 
4.5 
When the system did not understand me, 
the help system usually showed me an-
other way to ask the question. 
4.6 
When I knew what I could say, the sys-
tem usually recognized me correctly. 
4.3 
I was often unable to ask the questions I 
wanted. 
3.8 
I could ask enough questions that I was 
sure of my diagnosis. 
4.3 
This system is more effective than non-
verbal communication using gestures. 
4.3 
I would use this system again in a simi-
lar situation. 
4.1 
Table VIII Subject responses to questionnaire. 
Scores are on a 5-point scale, averaged over all 
answers. 
 
Answers were in general positive, and most of 
the subjects were clearly very comfortable with the 
system after just an hour and a half of use. Interest-
ingly, even though most of the subjects answered 
?yes? to the question ?I was often unable to ask the 
questions I wanted?, the good performance of the 
help system appeared to compensate adequately for 
missing coverage. 
4.2.2 Translation Performance 
In order to evaluate the translation quality of the 
newly developed French-to-English system, we 
conducted a preliminary performance evaluation, 
similar to the evaluation method described in 
(Bouillon 2005). 
We performed translation judgment in two 
rounds. In the first round, an English-speaking 
judge was asked to categorize target utterances as 
comprehensible or not without looking at corre-
sponding source sentences. 91.1% of the sentences 
were judged as comprehensible. The remaining 
8.9% consisted of sentences where the terminology 
used was not familiar to the judge and of sentences 
where the translation component failed to produce 
a sufficiently good translation. An example sen-
tence is 
- Are the headaches better when you experi-
ence dark room? 
which stems from the French source sentence 
- Vos maux de t?te sont ils soulag?s par obs-
curit?? 
In the second round, English-speaking judges, 
sufficiently fluent in French to understand source 
language utterances, were shown the French source 
utterance, and asked to decide whether the target 
language utterance correctly reflected the meaning 
of the source language utterance. They were also 
asked to judge the style of the target language ut-
terance. Specifically, judges were asked to classify 
sentences as ?BAD? if the meaning of the English 
sentence did not reflect the meaning of the French 
sentence. Sentences were categorized as ?OK? if 
the meaning was transferred correctly and the sen-
tence was comprehensible, but the style of the re-
sulting English sentence was not perfect. Sentences 
were judged as ?GOOD? when they were compre-
hensible, and both meaning and style were consid-
ered to be completely correct. Table VIII 
summarizes results of two judges. 
 
 Good OK Bad  
Judge 1 15.8% 73.80% 10.3% 
Judge 2 46.6% 47.1% 6.3% 
Table VIII Judgments of the quality of the transla-
tions of 546 utterances 
 
It is apparent that translation judging is a highly 
subjective process. When translations were marked 
as ?bad?, the problem most often seemed to be re-
lated to lexical items where it was challenging to 
find an exact correspondence between French and 
English. Two common examples were ?troubles de 
la vision?, which was translated as ?blurred vi-
sion?, and ?faiblesse musculaire?, which was trans-
lated as ?weakness?. It is likely that a more careful 
choice of lexical translation rules would deal with 
at least some of these cases. 
5 Summary 
We have presented a first end-to-end evaluation 
of the MedSLT spoken language translation sys-
tem. The medical students who tested it were all 
able to use the system well, with performance in 
some cases comparable to that of that of system 
developers after only two sessions. At least for the 
fairly simple type of diagnoses covered by our sce-
nario, the system?s performance appeared clearly 
adequate for the task.  
This is particularly encouraging, since the 
French to English version of the system is quite 
new, and has not yet received the level of attention 
required for a clinical system. The robustness 
added by the help system was sufficient to com-
pensate for that, and in most cases, subjects were 
able to find ways to maneuver around coverage 
holes and other problems. It is entirely reasonable 
to hope that performance, which is already fairly 
good, would be substantially better with another 
couple of months of development work. 
In summary, we feel that this study shows that 
the conservative architecture we have chosen 
shows genuine potential for use in medical diagno-
sis situations. Before the end of 2006, we hope to 
have advanced to the stage where we can start ini-
tial trials with real doctors and patients. 
 
 
Acknowledgments 
We would like to thank Agnes Lisowska, Alia 
Rahal, and Nancy Underwood for being impartial 
judges over our system?s results. 
This work was funded by the Swiss National 
Science Foundation. 
References 
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. 
Hockey, M. Santaholma, M. Starlander, Y. Nakao, K. 
Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain 
medical speech translation. In Proceedings of the 
10th Conference of the European Association for 
Machine Translation (EAMT), Budapest, Hungary. 
P. Bouillon, M. Rayner, B. Novellas, Y. Nakao, M. San-
taholma, M. Starlander, and N. Chatzichrisafis. 2006. 
Une grammaire multilingue partag?e pour la recon-
naissance et la g?n?ration. In Proceedings of TALN 
2006, Leuwen, Belgium. 
M. Cohen, J. Giangola, and J. Balogh. 2004, Voice User 
Interface Design. Addison Wesley Publishing. 
R. I. Kittredge. 2003. Sublanguages and comtrolled 
languages. In R. Mitkov, editor, The Oxford Hand-
book of Computational Linguistics, pages 430?447. 
Oxford University Press. 
MedBridge, 2006. http://www.medtablet.com/. As of 
15th March 2006. 
MedSLT, 2005. http://sourceforge.net/projects/medslt/. 
As of 15th March 2006. 
T. Mitamura. 1999. Controlled language for multilin-
gual machine translation. In Proceedings of Machine 
Translation Summit VII, Singapore. 
Phraselator, 2006. http://www.phraselator.com. As of 
15 February 2006. 
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An 
open source environment for compiling typed unifi-
cation grammars into speech recognisers. In Pro-
ceedings of the 10th EACL (demo track), Budapest, 
Hungary. 
M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao, 
H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b. 
Japanese speech understanding using grammar spe-
cialization. In HLT-NAACL 2005: Demo Session, 
Vancouver, British Columbia, Canada. Association 
for Computational Linguistics. 
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. 
Hockey, M. Santaholma,M. Starlander, H. Isahara, 
K. Kankazi, and Y. Nakao. 2005a. A methodology for 
comparing grammar-based and robust approaches to 
speech understanding. In Proceedings of the 9th In-
ternational Conference on Spoken Language Process-
ing (ICSLP), Lisboa, Portugal. 
M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and M. 
Wir?n. 2000. The Spoken Language Translator, 
Cambridge University Press.  
M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao, 
H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b. 
Japanese speech understanding using grammar spe-
cialization. In HLT-NAACL 2005: Demo Session, 
Vancouver, British Columbia, Canada. Association 
for Computational Linguistics. 
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Put-
ting Linguistics into Speech Recognition: The 
Regulus Grammar Compiler. CSLI Press, Chicago.  
Regulus, 2006. http://sourceforge.net/projects/regulus/. 
As of 15 March 2006. 
S-MINDS, 2006. http://www.sehda.com/. As of 15 
March 2006. 
M. Starlander, P. Bouillon, N. Chatzichrisafis, M. San-
taholma, M. Rayner, B.A. Hockey, H. Isahara, K. 
Kanzaki, and Y. Nakao. 2005. Practicing controlled 
language through a help system integrated into the 
medical speech translation system (MedSLT). In Pro-
ceedings of the MT Summit X, Phuket, Thailand 
 
MedSLT: A Limited-Domain Unidirectional Grammar-Based Medical
Speech Translator
Manny Rayner, Pierrette Bouillon, Nikos Chatzichrisafis, Marianne Santaholma, Marianne Starlander
University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
Emmanuel.Rayner@issco.unige.ch
Pierrette.Bouillon@issco.unige.ch, Nikos.Chatzichrisafis@vozZup.com
Marianne.Santaholma@eti.unige.ch, Marianne.Starlander@eti.unige.ch
Beth Ann Hockey
UCSC/NASA Ames Research Center, Moffet Field, CA 94035
bahockey@email.arc.nasa.gov
Yukie Nakao, Hitoshi Isahara, Kyoko Kanzaki
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
yukie-n@khn.nict.go.jp, {isahara,kanzaki}@nict.go.jp
Abstract
MedSLT is a unidirectional medical
speech translation system intended for
use in doctor-patient diagnosis dialogues,
which provides coverage of several differ-
ent language pairs and subdomains. Vo-
cabulary ranges from about 350 to 1000
surface words, depending on the language
and subdomain. We will demo both the
system itself and the development envi-
ronment, which uses a combination of
rule-based and data-driven methods to
construct efficient recognisers, generators
and transfer rule sets from small corpora.
1 Overview
The mainstream in speech translation work is for the
moment statistical, but rule-based systems are still a
very respectable alternative. In particular, nearly all
systems which have actually been deployed are rule-
based. Prominent examples are (Phraselator, 2006;
S-MINDS, 2006; MedBridge, 2006).
MedSLT (MedSLT, 2005; Bouillon et al, 2005)
is a unidirectional medical speech translation system
for use in doctor-patient diagnosis dialogues, which
covers several different language pairs and subdo-
mains. Recognition is performed using grammar-
based language models, and translation uses a rule-
based interlingual framework. The system, includ-
ing the development environment, is built on top of
Regulus (Regulus, 2006), an Open Source platform
for developing grammar-based speech applications,
which in turn sits on top of the Nuance Toolkit.
The demo will show how MedSLT can be used
to carry out non-trivial diagnostic dialogues. In par-
ticular, we will demonstrate how an integrated intel-
ligent help system counteracts the brittleness inher-
ent in rule-based processing, and rapidly leads new
users towards the supported system coverage. We
will also demo the development environment, and
show how grammars and sets of transfer rules can be
efficiently constructed from small corpora of a few
hundred to a thousand examples.
2 The MedSLT system
The MedSLT demonstrator has already been exten-
sively described elsewhere (Bouillon et al, 2005;
Rayner et al, 2005a), so this section will only
present a brief summary. The main components are
a set of speech recognisers for the source languages,
a set of generators for the target languages, a transla-
tion engine, sets of rules for translating to and from
interlingua, a simple discourse engine for dealing
with context-dependent translation, and a top-level
which manages the information flow between the
other modules and the user.
MedSLT also includes an intelligent help mod-
ule, which adds robustness to the system and guides
the user towards the supported coverage. The help
module uses a backup recogniser, equipped with a
statistical language model, and matches the results
from this second recogniser against a corpus of utter-
ances which are within system coverage and trans-
late correctly. In previous studies, we showed that
the grammar-based recogniser performs much bet-
ter than the statistical one on in-coverage utterances,
but worse on out-of-coverage ones. Having the help
system available approximately doubled the speed
at which subjects learned, measured as the average
difference in semantic error rate between the results
for their first quarter-session and their last quarter-
session (Rayner et al, 2005a). It is also possible to
recover from recognition errors by selecting a dis-
played help sentence; this typically increases the
number of acceptably processed utterances by about
10% (Starlander et al, 2005).
We will demo several versions of the system, us-
ing different source languages, target languages and
subdomains. Coverage is based on standard exami-
nation questions obtained from doctors, and consists
mainly of yes/no questions, though there is also sup-
port for WH-questions and elliptical utterances. Ta-
ble 1 gives examples of the coverage in the English-
input headache version, and Table 2 summarises
recognition performance in this domain for the three
main input languages. Differences in the sizes of the
recognition vocabularies are primarily due to differ-
ences in use of inflection. Japanese, with little in-
flectional morphology, has the smallest vocabulary;
French, which inflects most parts of speech, has the
largest.
3 The development environment
Although the MedSLT system is rule-based, we
would, for the usual reasons, prefer to acquire these
rules from corpora using some well-defined method.
There is, however, little or no material available for
most medical speech translation domains, including
ours. As noted in (Probst and Levin, 2002), scarcity
of data generally implies use of some strategy to ob-
tain a carefully structured training corpus. If the cor-
pus is not organised in this way, conflicts between
alternate learned rules occur, and it is hard to in-
Where?
?do you experience the pain in your jaw?
?does the pain spread to the shoulder?
When?
?have you had the pain for more than a month?
?do the headaches ever occur in the morning?
How long?
?does the pain typically last a few minutes?
?does the pain ever last more than two hours?
How often?
?do you get headaches several times a week?
?are the headaches occurring more often?
How?
?is it a stabbing pain?
?is the pain usually severe?
Associated symptoms?
?do you vomit when you get the headaches?
?is the pain accompanied by blurred vision?
Why?
?does bright light make the pain worse?
?do you get headaches when you eat cheese?
What helps?
?does sleep make the pain better?
?does massage help?
Background?
?do you have a history of sinus disease?
?have you had an e c g?
Table 1: Examples of English MedSLT coverage
duce a stable set of rules. As Probst and Levin sug-
gest, one obvious way to attack the problem is to
implement a (formal or informal) elicitation strat-
egy, which biases the informant towards translations
which are consistent with the existing ones. This is
the approach we have adopted in MedSLT.
The Regulus platform, on which MedSLT
is based, supports rapid construction of com-
plex grammar-based language models; it uses an
example-based method driven by small corpora
of disambiguated parsed examples (Rayner et al,
2003; Rayner et al, 2006), which extracts most of
the structure of the model from a general linguis-
tically motivated resource grammar. The result is
a specialised version of the general grammar, tai-
lored to the example corpus, which can then be com-
piled into an efficient recogniser or into a genera-
Language Vocab WER SemER
English 441 6% 18%
French 1025 8% 10%
Japanese 347 4% 4%
Table 2: Recognition performance for English,
French and Japanese headache-domain recognisers.
?Vocab? = number of surface words in source lan-
guage recogniser vocabulary; ?WER? = Word Error
Rate for source language recogniser, on in-coverage
material; ?SemER? = semantic error rate for source
language recogniser, on in-coverage material.
tion module. Regulus-based recognisers and gen-
erators are easy to maintain, and grammar struc-
ture is shared automatically across different subdo-
mains. Resource grammars are available for several
languages, including English, Japanese, French and
Spanish.
Nuance recognisers derived from the resource
grammars produce both a recognition string and a
semantic representation. This representation con-
sists of a list of key/value pairs, optionally including
one level of nesting; the format of interlingua and
target language representations is similar. The for-
malism is sufficiently expressive that a reasonable
range of temporal and causal constructions can be
represented (Rayner et al, 2005b). A typical exam-
ple is shown in Figure 1. A translation rule maps
a list of key/value pairs to a list of key/value pairs,
optionally specifying conditions requiring that other
key/value pairs either be present or absent in the
source representation.
When developing new coverage for a given lan-
guage pair, the developer has two main tasks. First,
they need to add new training examples to the
corpora used to derive the specialised grammars
used for the source and target languages; second,
they must add translation rules to handle the new
key/value pairs. The simple structure of the Med-
SLT representations makes it easy to support semi-
automatic acquisition of both of these types of in-
formation. The basic principle is to attempt to find
the minimal set of new rules that can be added to the
existing set, in order to cover the new corpus exam-
ple; this is done through a short elicitation dialogue
with the developer. We illustrate this with a simple
example.
Suppose we are developing coverage for the En-
glish ? Spanish version of the system, and that
the English corpus sentence ?does the pain occur at
night? fails to translate. The acquisition tool first
notes that processing fails when converting from in-
terlingua to Spanish. The interlingua representation
is
[[utterance_type,ynq],
[pronoun,you],
[state,have_symptom],
[symptom,pain],[tense,present],
[prep,in_time],[time,night]]
Applying Interlingua ? Spanish rules, the result is
[[utterance_type,ynq],
[pronoun,usted],
[state,tener],[symptom,dolor],
[tense,present],
[prep,por_temporal],
failed:[time,night]]
where the tag failed indicates that the element
[time,night] could not be processed. The tool
matches the incomplete transferred representation
against a set of correctly translated examples, and
shows the developer the English and Spanish strings
for the three most similar ones, here
does it appear in the morning
-> tiene el dolor por la man?ana
does the pain appear in the morning
-> tiene el dolor por la man?ana
does the pain come in the morning
-> tiene el dolor por la man?ana
This suggests that a translation for ?does the pain
occur at night? consistent with the existing rules
would be ?tiene el dolor por la noche?. The devel-
oper gives this example to the system, which parses
it using both the general Spanish resource grammar
and the specialised grammar used for generation in
the headache domain. The specialised grammar fails
to produce an analysis, while the resource grammar
produces two analyses,
[[utterance_type,ynq],
[pronoun,usted],
[state,tener],[symptom,dolor],
[[utterance_type,ynq],[pronoun,you],[state,have_symptom],
[tense,present],[symptom,headache],[sc,when],
[[clause,[[utterance_type,dcl],[pronoun,you],
[action,drink],[tense,present],[cause,coffee]]]]
Figure 1: Representation of ?do you get headaches when you drink coffee?
[tense,present],
[prep,por_temporal],
[temporal,noche]]
and
[[utterance_type,dcl],
[pronoun,usted],
[state,tener],[symptom,dolor],
[tense,present],
[prep,por_temporal],
[temporal,noche]]
The first of these corresponds to the YN-question
reading of the sentence (?do you have the pain at
night?), while the second is the declarative reading
(?you have the pain at night?). Since the first (YN-
question) reading matches the Interlingua represen-
tation better, the acquisition tool assumes that it is
the intended one. It can now suggest two pieces of
information to extend the system?s coverage.
First, it adds the YN-question reading of ?tiene
el dolor por la noche? to the corpus used to train
the specialised generation grammar. The piece
of information acquired from this example is that
[temporal,noche] should be realised in this
domain as ?la noche?. Second, it compares the cor-
rect Spanish representation with the incomplete one
produced by the current set of rules, and induces a
new Interlingua to Spanish translation rule. This will
be of the form
[time,night] -> [temporal,noche]
In the demo, we will show how the development
environment makes it possible to quickly add new
coverage to the system, while also checking that old
coverage is not broken.
References
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
MedBridge, 2006. http://www.medtablet.com/index.html.
As of 15 March 2006.
MedSLT, 2005. http://sourceforge.net/projects/medslt/.
As of 15 March 2005.
Phraselator, 2006. http://www.phraselator.com. As of 15
March 2006.
K. Probst and L. Levin. 2002. Challenges in automatic
elicitation of a controlled bilingual corpus. In Pro-
ceedings of the 9th International Conference on The-
oretical and Methodological Issues in Machine Trans-
lation.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, H. Isahara, K. Kankazi,
and Y. Nakao. 2005a. A methodology for comparing
grammar-based and robust approaches to speech un-
derstanding. In Proceedings of the 9th International
Conference on Spoken Language Processing (ICSLP),
Lisboa, Portugal.
M. Rayner, P. Bouillon, M. Santaholma, and Y. Nakao.
2005b. Representational and architectural issues in a
limited-domain medical speech translator. In Proceed-
ings of TALN/RECITAL, Dourdan, France.
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Putting
Linguistics into Speech Recognition: The Regulus
Grammar Compiler. CSLI Press, Chicago.
Regulus, 2006. http://sourceforge.net/projects/regulus/.
As of 15 March 2006.
S-MINDS, 2006. http://www.sehda.com. As of 15
March 2006.
M. Starlander, P. Bouillon, N. Chatzichrisafis, M. Santa-
holma, M. Rayner, B.A. Hockey, H. Isahara, K. Kan-
zaki, and Y. Nakao. 2005. Practicing controlled lan-
guage through a help system integrated into the medi-
cal speech translation system (MedSLT). In Proceed-
ings of the MT Summit X, Phuket, Thailand.
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 41?48,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Adapting a Medical Speech to Speech Translation System (MedSLT) to 
Arabic 
Pierrette Bouillon 
University of Geneva, TIM/ISSCO, ETI 
40, Bd. Du Pont d'Arve 
CH-1211 Geneva 4, Switzerland 
Pierrette.Bouillon@issco.unige.ch 
 
Manny Rayner 
Powerset Inc 
475 Brannan Str. 
San Francisco 
CA 94107, USA 
manny@powerset.com 
 
Sonia Halimi 
University of Geneva, TIM/ISSCO, ETI 
40, Bd. Du Pont d'Arve 
CH-1211 Geneva 4, Switzerland 
Sonia.Halimi@eti.unige.ch 
 
Beth Ann Hockey 
Mail Stop 19-26, UCSC UARC, NASA 
Ames Research Center, Moffett Field, 
CA 94035-1000 
bahockey@ucsc.edu 
 
 
 
Abstract 
We describe the adaptation for Arabic of 
the grammar-based MedSLT medical 
speech system. The system supports simple 
medical diagnosis questions about head-
aches using vocabulary of 322 words. We 
show that the MedSLT architecture based 
on motivated general grammars produces 
very good results, with a limited effort. 
Based on the grammars for other languages 
covered by the system, it is in fact very 
easy to develop an Arabic grammar and to 
specialize it efficiently for the different 
system tasks. In this paper, we focus on 
generation. 
1 Introduction 
MedSLT is a medical speech translation system. It 
allows a doctor to ask diagnosis questions in medi-
cal subdomains, such as headaches, abdominal 
pain, etc, covering a wide range of questions that 
doctors generally ask their patients. The grammar-
based architecture, built using specialization from 
reusable general grammars, is designed to allow a 
rapid development of different domains and lan-
guages. Presently, it supports English, French, 
Japanese, Spanish and Catalan. This article focuses 
on the system development for Arabic.  
In general, translation in this context raises two 
specific questions: 1) how to achieve recognition 
quality that is good enough for translation, and 2) 
how to get translations to be as idiomatic as possi-
ble so they can be understood by the patient. For 
close languages and domains where accuracy is not 
very important (e.g. information requests), it may 
be possible to combine a statistical recognizer with 
a commercial translation system as it is often done 
in commercial tools such as SpokenTranslation 
(Seligman and Dillinger, 2006). However, for this 
specific application in a multilingual context, this 
solution is not applicable at all: even if perfect rec-
ognition were possible (which is far from being the 
case), current commercial tools for translating to 
Arabic do not guarantee good quality. The domain 
dealt with here contains, in fact, many structures 
specific to this type of oral dialogue that can not be 
handled by these systems. For example, all the 
doctor?s interactions with the MedSLT system 
consist of questions whose structures differ from 
one language to another, with each language hav-
ing its own constraints. Consequently, two types of 
errors occur in Arabic translation systems. Either 
they do not recognize the interrogative structure as 
in example (1), or they produce ungrammatical 
sentences by copying the original structure as in 
example (2): 
 
 
41
(1) was the pain severe?  
 ? ? ?
?  (Google) 
(kana al alam chadid)  
 ?have-past-3 the pain severe? 
 
is the pain aggravated by exertion? 
 
 ? (Systran) 
(al alam yufaqim bi juhd)  
?the pain escalate-3 with effort? 
 
(2) is the headache aggravated by bright light? 
  ? 
 Proceedings of SPEECHGRAM 2007, pages 41?48,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Bidirectional Grammar-Based Medical Speech Translator
Pierrette Bouillon1, Glenn Flores2, Marianne Starlander1, Nikos Chatzichrisafis1
Marianne Santaholma1, Nikos Tsourakis1, Manny Rayner1,3, Beth Ann Hockey4
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
Pierrette.Bouillon@issco.unige.ch
Marianne.Starlander@eti.unige.ch, Nikos.Chatzichrisafis@vozZup.com
Marianne.Santaholma@eti.unige.ch, Nikolaos.Tsourakis@issco.unige.ch
2 Medical College of Wisconsin, 8701 Watertown Plank Road, Milwaukee, WI 53226
gflores@mcw.edu
3 Powerset, Inc., 475 Brannan Street, San Francisco, CA 94107
manny@powerset.com
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
Abstract
We describe a bidirectional version of the
grammar-based MedSLT medical speech
system. The system supports simple medi-
cal examination dialogues about throat pain
between an English-speaking physician and
a Spanish-speaking patient. The physician?s
side of the dialogue is assumed to consist
mostly of WH-questions, and the patient?s of
elliptical answers. The paper focusses on the
grammar-based speech processing architec-
ture, the ellipsis resolution mechanism, and
the online help system.
1 Background
There is an urgent need for medical speech trans-
lation systems. The world?s current population
of 6.6 billion speaks more than 6,000 languages
(Graddol, 2004). Language barriers are associated
with a wide variety of deleterious consequences in
healthcare, including impaired health status, a lower
likelihood of having a regular physician, lower rates
of mammograms, pap smears, and other preven-
tive services, non-adherence with medications, a
greater likelihood of a diagnosis of more severe psy-
chopathology and leaving the hospital against med-
ical advice among psychiatric patients, a lower like-
lihood of being given a follow-up appointment af-
ter an emergency department visit, an increased risk
of intubation among children with asthma, a greater
risk of hospital admissions among adults, an in-
creased risk of drug complications, longer medical
visits, higher resource utilization for diagnostic test-
ing, lower patient satisfaction, impaired patient un-
derstanding of diagnoses, medications, and follow-
up, and medical errors and injuries (Flores, 2005;
Flores, 2006). Nevertheless, many patients who
need medical interpreters do not get them. For ex-
ample, in the United States, where 52 million peo-
ple speak a language other than English at home
and 23 million people have limited English profi-
ciency (LEP) (Census, 2007), one study found that
about half of LEP patients presenting to an emer-
gency department were not provided with a medical
interpreter (Baker et al, 1996). There is thus a sub-
stantial gap between the need for and availability of
language services in health care, a gap that could be
bridged through effective medical speech translation
systems.
An ideal system would be able to interpret ac-
curately and flexibly between patients and health
care professionals, using unrestricted language and
a large vocabulary. A system of this kind is, un-
fortunately, beyond the current state of the art.
It is, however, possible, using today?s technol-
ogy, to build speech translation systems for specific
scenarios and language-pairs, which can achieve
acceptable levels of reliability within the bounds
41
of a well-defined controlled language. MedSLT
(Bouillon et al, 2005) is an Open Source system
of this type, which has been under construction at
Geneva University since 2003. The system is built
on top of Regulus (Rayner et al, 2006), an Open
Source platform which supports development of
grammar-based speech-enabled applications. Regu-
lus has also been used to build several other systems,
including NASA?s Clarissa (Rayner et al, 2005b).
The most common architecture for speech trans-
lation today uses statistical methods to perform both
speech recognition and translation, so it is worth
clarifying why we have chosen to use grammar-
based methods. Even though statistical architec-
tures exhibit many desirable properties (purely data-
driven, domain independent), this is not necessar-
ily the best alternative in safety-critical medical ap-
plications. Anecdotally, many physicians express
reluctance to trust a translation device whose out-
put is not readily predictable, and most of the
speech translation systems which have reached the
stage of field testing rely on various types of
grammar-based recognition and rule-based transla-
tion (Phraselator, 2007; Fluential, 2007).
Statistical speech recognisers can achieve impres-
sive levels of accuracy when trained on enough data,
but it is a daunting task to collect training mate-
rial in the requisite quantities (usually, tens of thou-
sands of high-quality utterances) when trying to
build practical systems. Considering that the medi-
cal speech translation applications we are interested
in constructing here need to work for multiple lan-
guages and subdomains, the problem becomes even
more challenging. Our experience is that grammar-
based systems which also incorporate probabilistic
context-free grammar tuning deliver better results
than purely statistical ones when training data are
sparse (Rayner et al, 2005a).
Another common criticism of grammar-based
systems is that out-of-coverage utterances will
neither be recognized nor translated, an objec-
tion that critics have sometimes painted as de-
cisive. It is by no means obvious, however,
that restricted coverage is such a serious prob-
lem. In text processing, work on several gener-
ations of controlled language systems has devel-
oped a range of techniques for keeping users within
the bounds of system coverage (Kittredge, 2003;
Mitamura, 1999), and variants of these methods can
also be adapted for spoken language applications.
Our experiments with MedSLT show that even a
quite simple help system is enough to guide users
quickly towards the intended coverage of a medium-
vocabulary grammar-based speech translation appli-
cation, with most users appearing confident after just
an hour or two of exposure (Starlander et al, 2005;
Chatzichrisafis et al, 2006).
Until recently, the MedSLT system only sup-
ported unidirectional processing in the physician
to patient direction. The assumption was that the
physician would mostly ask yes/no questions, to
which the patient would respond non-verbally, for
example by nodding or shaking their head. A uni-
directional architecture is easier to make habitable
than a bidirectional one. It is reasonable to as-
sume that the physician will use the system regu-
larly enough to learn the coverage, but most patients
will not have used the system before, and it is less
clear that they will be able to acclimatize within the
narrow window at their disposal. These consider-
ations must however be balanced against the fact
that a unidirectional system does not allow for a
patient-centered interaction characterized by mean-
ingful patient-clinician communication or shared de-
cision making. Multiple studies in the medical lit-
erature document that patient-centeredness, effec-
tive patient-clinician communication, and shared de-
cision making are associated with significant im-
provements in patient health outcomes, including
reduced anxiety levels, improved functional sta-
tus, reduced pain, better control of diabetes melli-
tus, blood pressure reduction among hypertensives,
improved adherence, increased patient satisfaction,
and symptom reduction for a variety of conditions
(Stewart, 1995; Michie et al, 2003). A bidirectional
system is considered close to essential from a health-
care perspective, since it appropriately addresses the
key issues of patient centeredness and shared de-
cision making. For these reasons, we have over
the last few months developed a bidirectional ver-
sion of MedSLT, initially focussing on a throat pain
scenario with an English-speaking physician and a
Spanish-speaking patient. The physician uses full
sentences, while the patient answers with short re-
sponses.
One of the strengths of the Regulus approach is
42
that it is very easy to construct parallel versions of
a grammar; generally, all that is required is to vary
the training corpus. (We will have more to say about
this soon). We have exploited these properties of
the platform to create two different configurations
of the bidirectional system, so that we can compare
competing approaches to the problem of accommo-
dating patients unfamiliar with speech technology.
In Version 1 (less restricted), the patient is allowed
to answer using both elliptical utterances and short
sentences, while in Version 2 (more restricted) they
are only permitted to use elliptical utterances. Thus,
for example, if the physician asks the question ?How
long have you had a sore throat??, Version 1 allows
the patient to respond both ?Desde algunos d??as?
(?For several days?) and ?Me ha dolido la garganta
desde algunos d??as? (?I have had a sore throat for
several days?), while Version 2 only allows the first
of these. Both the short and the long versions are
translated uniformly, with the short version resolved
using the context from the preceding question.
In both versions, if the patient finds it too chal-
lenging to use the system to answer WH-questions
directly, it is possible to back off to the earlier di-
alogue architecture in which the physician uses Y-
N questions and the patient responds with simple
yes/no answers, or nonverbally. Continuing the ex-
ample, if the patient is unable to find an appro-
priate way to answer the physician?s question, the
physician could ask ?Have you had a sore throat for
more than three days??; if the patient responds nega-
tively, they could continue with the follow-on ques-
tion ?More than a week??, and so on.
In the rest of the paper, we first describe the
system top-level (Section 2), the way in which
grammar-based processing is used (Section 3), the
ellipsis processing mechanism (Section 4), and the
help system (Section 5). Section 6 presents an ini-
tial evaluation, and the final section concludes.
2 Top-level architecture
The system is operated through the graphical user
interface (GUI) shown in Figures 1 and 2. In
accordance with the basic principles of patient-
centeredness and shared decision-making outlined
in Section 1, the patient and the physician each have
their own headset, use their own mouse, and share
the same view of the screen. This is in sharp contrast
to the majority of the medical speech translation sys-
tems described in the literature (Somers, 2006).
As shown in the screenshots, the main GUI win-
dow is separated into two tabbed panes, marked
?Doctor? and ?Patient?. Initially, the ?Doctor? view
(the one shown in Figure 1) is active. The physician
presses the ?Push to talk? button, and speaks into
the headset microphone. If recognition is success-
ful, the GUI displays four separate results, listed on
the right side of the screen. At the top, immediately
under the heading ?Question?, we can see the actual
words returned by speech recognition. Here, these
words are ?Have you had rapid strep test?. Below,
we have the help pane: this displays similar ques-
tions taken from the help corpus, which are known to
be within system coverage. The pane marked ?Sys-
tem understood? shows a back-translation, produced
by first translating the recognition result into inter-
lingua, and then translating it back into English. In
the present example, this corrects the minor mistake
the recogniser has made, missing the indefinite ar-
ticle ?a?, and confirms that the system has obtained
a correct grammatical analysis and interpretation at
the level of interlingua. At the bottom, we see the
target language translation. The left-hand side of the
screen logs the history of the conversation to date, so
that both sides can refer back to it.
If the physician decides that the system has cor-
rectly understood what they said, they can now press
the ?Play? button. This results in the system produc-
ing a spoken output, using the Vocalizer TTS engine.
Simultaneously with speaking, the GUI shifts to the
?Patient? configuration shown in Figure 2. This dif-
fers from the ?Doctor? configuration in two respects:
all text is in the patient language, and the help pane
presents its suggestions immediately, based on the
preceding physician question. The various process-
ing components used to support these functionalities
are described in the following sections.
3 Grammar-based processing
Grammar-based processing is used for source-
language speech recognition and target-side genera-
tion. (Source-language analysis is part of the recog-
nition process, since grammar-based recognition in-
cludes creating a parse). All of these functionalities
43
Figure 1: Screenshot showing the state of the GUI after the physician has spoken, but before he has pressed
the ?Play? button. The help pane shows similar queries known to be within coverage.
Figure 2: Screenshot showing the state of the GUI after the physician has pressed the ?Play? button. The
help pane shows known valid responses to similar questions.
44
are implemented using the Regulus platform, with
the task-specific grammars compiled out of general
feature grammar resources by the Regulus tools. For
both recognition and generation, the first step is
to extract a domain-specific feature grammar from
the general one, using a version of the Explanation
Based Learning (EBL) algorithm.
The extraction process is driven by a corpus of ex-
amples and a set of ?operationality criteria?, which
define how the rules in the original resource gram-
mar are recombined into domain-specific ones. It is
important to realise that the domain-specific gram-
mar is not merely a subset of the resource grammar;
a typical domain-specific grammar rule is created by
merging two to five resource grammar rules into a
single ?flatter? rule. The result is a feature gram-
mar which is less general than the original one, but
more efficient. For recognition, the grammar is then
processed further into a CFG language model, using
an algorithm which alternates expansion of feature
values and filtering of the partially expanded gram-
mar to remove irrelevant rules. Detailed descrip-
tions of the EBL learning and feature grammar ?
CFG compilation algorithms can be found in Chap-
ters 8 and 10 of (Rayner et al, 2006). Regulus fea-
ture grammars can also be compiled into generators
using a version of the Semantic Head Driven algo-
rithm (Shieber et al, 1990).
The English (physician) side recogniser is com-
piled from the large English resource grammar de-
scribed in Chapter 9 of (Rayner et al, 2006), and
was constructed in the same way as the one de-
scribed in (Rayner et al, 2005a), which was used for
a headache examination task. The operationality cri-
teria are the same, and the only changes are a differ-
ent training corpus and the addition of new entries
to the lexicon. The same resources, with a differ-
ent training corpus, were used to build the English
language generator. It is worth pointing out that, al-
though a uniform method was used to build these
various grammars, the results were all very differ-
ent. For example, the recognition grammar from
(Rayner et al, 2005a) is specialised to cover only
second-person questions (?Do you get headaches
in the mornings??), while the generator grammar
used in the present application covers only first-
person declarative statements (?I visited the doctor
last Monday.?). In terms of structure, each gram-
mar contains several important constructions that the
other lacks. For example, subordinate clauses are
central in the headache domain (?Do the headaches
occur when you are stressed??) but are not present
in the sore throat domain; this is because the stan-
dard headache examination questions mostly focus
on generic conditions, while the sore throat exami-
nation questions only relate to concrete ones. Con-
versely, relative clauses are important in the sore
throat domain (?I have recently been in contact with
someone who has strep throat?), but are not suffi-
ciently important in the headache domain to be cov-
ered there.
On the Spanish (patient) side, there are four
grammars involved. For recognition, we have
two different grammars, corresponding to the two
versions of the system; the grammar for Ver-
sion 2 is essentially a subset of that for Version
1. For generation, there are two separate and
quite different grammars: one is used for trans-
lating the physician?s questions, while the other
produces back-translations of the patient?s ques-
tions. All of these grammars are extracted from
a general shared resource grammar for Romance
languages, which currently combines rules for
French, Spanish and Catalan (Bouillon et al, 2006;
Bouillon et al, to appear 2007b).
One interesting consequence of our methodology
is related to the fact that Spanish is a prodrop lan-
guage, which implies that many sentences are sys-
tematically ambiguous between declarative and Y-N
question readings. For example, ?He consultado un
me?dico? could in principle mean either ?I visited a
doctor? or ?Did I visit a doctor??. When training the
specialised Spanish grammars, it is thus necessary to
specify which readings of the training sentences are
to be used. Continuing the example, if the sentence
occurred in training material for the answer gram-
mar, we would specify that the declarative reading
was the intended one1.
4 Ellipsis processing and contextual
interpretation
In Version 1 of the system, the patient is per-
mitted to answer using elliptical phrases; in Ver-
1The specification can be formulated as a preference that
applies uniformly to all the training examples in a given group.
45
sion 2, she is obliged to do so. Ability to pro-
cess elliptical responses makes it easier to guide the
patient towards the intended coverage of the sys-
tem, without degrading the quality of recognition
(Bouillon et al, to appear 2007a). The downside is
that ellipses are also harder to translate than full sen-
tences. Even in a limited domain like ours, and in a
closely related language-pair, ellipsis can generally
not be translated word for word, and it is necessary
to look at the preceding context if the rules are to
be applied correctly. In examples 1 and 2 below,
the locative phrase ?In your stomach? in the English
source becomes the subject in the Spanish transla-
tion. This implies that the translation of the ellipsis
in the second physician utterance needs to change
syntactic category: ?In your head? (PP) becomes
?La cabeza? (NP).
(1) Doctor: Do you have a pain in your
stomach?
(Trans): Le duele el estomago?
(2) Doctor: In your head?
(Trans): *En la cabeza?
Since examples like this are frequent, our sys-
tem implements a solution in which the patient?s
replies are translated in the context of the preced-
ing utterance. If the patient-side recogniser?s output
is classified as an ellipsis (this can done fairly reli-
ably thanks to use of suitably specialised grammars;
cf. Section 3), we expand the incomplete phrase
into a full sentence structure by adding appropriate
structural elements from the preceding physician-
side question; the expanded semantic structure is the
one which is then translated into interlingual form,
and thence back to the physician-side language.
Since all linguistic representations, including
those of elliptical phrases and their contexts, are rep-
resented as flat attribute-value lists, we are able to
implement the resolution algorithm very simply in
terms of list manipulation. In YN-questions, where
the elliptical answer intuitively adds information to
the question (?Did you visit the doctor??; ?El lunes?
? ?I visited the doctor on Monday?), the repre-
sentations are organised so that resolution mainly
amounts to concatenation of the two lists2. In WH-
questions, where the answer intuitively substitutes
the elliptical answer for the WH-phrase (?What is
2It is also necessary to replace second-person pronouns with
first-person counterparts.
your temperature??; ?Cuarenta grados?? ?My tem-
perature is forty degrees?), resolution substitutes the
representation of the elliptical phrase for that of a
semantically similar element in the question.
The least trivial aspect of this process is provid-
ing a suitable definition of ?semantically similar?.
This is done using a simple example-based method,
in which the grammar developer writes a set of dec-
larations, each of which lists a set of semantically
similar NPs. At compile-time, the grammar is used
to parse each NP, and extract a generalised skele-
ton, in which specific lexical information is stripped
away; at run-time, two NPs are held to be semanti-
cally similar if they can each be unified with skele-
tons in the same equivalence class. This ensures that
the definition of the semantic similarity relation is
stable across most changes to the grammar and lex-
icon. The issues are described in greater detail in
(Bouillon et al, to appear 2007a).
5 Help system
Since the performance of grammar-based speech un-
derstanding is only reliable on in-coverage mate-
rial, systems based on this type of architecture must
necessarily use a controlled language approach, in
which it is assumed that the user is able to learn the
relevant coverage. As previously noted, the Med-
SLT system addresses this problem by incorporat-
ing an online help system (Starlander et al, 2005;
Chatzichrisafis et al, 2006).
On the physician side, the help system offers, af-
ter each recognition event, a list of related ques-
tions; similarly, on the patient side, it provides ex-
amples of known valid answers to the current ques-
tion. In both cases, the help examples are extracted
from a precompiled corpus of question-answer pairs,
which have been judged for correctness by system
developers. The process of selecting the examples
is slightly different on the two sides. For questions
(physician side), the system performs a second par-
allel recognition of the input speech, using a sta-
tistical recogniser. It then compares the recogni-
tion result, using an N-gram based metric, against
the set of known correct in-coverage questions from
the question-answer corpus, to extract the most sim-
ilar ones. For answers (patient side), the help sys-
tem searches the question-answer corpus to find the
46
questions most similar to the current one, and shows
the list of corresponding valid answers, using the
whole list in the case of Version 1 of the system, and
only the subset consisting of elliptical phrases in the
case of Version 2.
6 Evaluation
In previous studies, we have evaluated speech
recognition and speech understanding per-
formance for physician-side questions in
English (Bouillon et al, 2005) and Spanish
(Bouillon et al, to appear 2007b), and investi-
gated the impact on performance of the help system
(Rayner et al, 2005a; Starlander et al, 2005). We
have also carried out recent evaluations designed to
contrast recognition performance on elliptical and
full versions of the same utterance; here, our results
suggest that elliptical forms of (French-language)
MedSLT utterances are slightly easier to recognise
in terms of semantic error rate than full sentential
forms (Bouillon et al, to appear 2007a). Our initial
evaluation studies on the bidirectional system have
focussed on a specific question which has particular
relevance to this new version of MedSLT. Since
we are assuming that the patient will respond
using elliptical utterances, and that these utterances
will be translated in the context of the preceding
physician-side question, how confident can we
be that this context-dependent translation will be
correct?
In order to investigate these issues, we performed
a small data-collection using Version 2 of the sys-
tem, whose results we summarise here. One of the
authors of the paper played the role of an English-
speaking physician, in a simulated medical exam-
ination scenario where the goal was to determine
whether or not the ?patient? was suffering from a
viral throat infection. The six subjects playing the
role of the patient were all native speakers of Span-
ish, and had had no previous exposure to the system,
or indeed any kind of speech technology. They were
given cards describing the symptoms they were sup-
posed to be displaying, on which they were asked
to based their answers. From a total of 92 cor-
rectly recognised patient responses, we obtained 50
yes/no answers and 42 examples of real elliptical ut-
terances. Out of these, 36 were judged to have been
translated completely correctly, and a further 3 were
judged correct in terms of meaning, but less than flu-
ent. Only 3 examples were badly translated: of these
two were caused by problems in a translation rule,
and one by incorrect treatment of ellipsis resolution.
We show representative exchanges below; the last of
these is the one in which ellipsis processing failed to
work correctly.
(3) Doctor: For how long have you
had your sore throat?
Patient: Desde hace ma?s de
una semana
(Trans): I have had a sore
throat for more than one week
(4) Doctor: What were the results?
Patient: Negativo
(Trans): The results were negative
(5) Doctor: Have you seen a doctor
for your sore throat?
Patient: S?? el lunes
(Trans): I visited the doctor
for my sore throat monday
(6) Doctor: Have you been with anyone
recently who has a strep throat?
Patient: Si ma?s de dos semanas
(Trans): I was in contact with someone
more than two weeks recently
who had strep throat
7 Conclusions
We have presented a bidirectional grammar-based
English ? Spanish medical speech translation sys-
tem built using a linguistically motivated archi-
tecture, where all linguistic information is ulti-
mately derived from two resource grammars, one
for each language. We have shown how this en-
ables us to derive the multiple grammars needed,
which differ both with respect to function (recog-
nition/generation) and to domain (physician ques-
tions/patient answers). The system is currently un-
dergoing initial lab testing; we hope to advance to
initial trials on real patients some time towards the
end of the year.
References
[Baker et al1996] D.W. Baker, R.M. Parker, M.V.
Williams, W.C. Coates, and Kathryn Pitkin. 1996.
47
Use and effectiveness of interpreters in an emer-
gency department. Journal of the American Medical
Association, 275:783?8.
[Bouillon et al2005] P. Bouillon, M. Rayner,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara.
2005. A generic multi-lingual open source platform
for limited-domain medical speech translation. In
Proceedings of the 10th Conference of the European
Association for Machine Translation (EAMT), pages
50?58, Budapest, Hungary.
[Bouillon et al2006] P. Bouillon, M. Rayner, B. Novel-
las Vall, Y. Nakao, M. Santaholma, M. Starlander, and
N. Chatzichrisafis. 2006. Une grammaire multilingue
partage?e pour la traduction automatique de la parole.
In Proceedings of TALN 2006, Leuwen, Belgium.
[Bouillon et alto appear 2007a] P. Bouillon, M. Rayner,
M. Santaholma, and M. Starlander. to appear 2007a.
Les ellipses dans un syste`me de traduction automa-
tique de la parole. In Proceedings of TALN 2006,
Toulouse, France.
[Bouillon et alto appear 2007b] P. Bouillon, M. Rayner,
B. Novellas Vall, Y. Nakao, M. Santaholma, M. Star-
lander, and N. Chatzichrisafis. to appear 2007b. Une
grammaire partage?e multi-ta?che pour le traitement de
la parole : application aux langues romanes. Traite-
ment Automatique des Langues.
[Census2007] U.S. Census, 2007. Selected Social Char-
acteristics in the United States: 2005. Data Set: 2005
American Community Survey. Available here.
[Chatzichrisafis et al2006] N. Chatzichrisafis, P. Bouil-
lon, M. Rayner, M. Santaholma, M. Starlander, and
B.A. Hockey. 2006. Evaluating task performance for
a unidirectional controlled language medical speech
translation system. In Proceedings of the HLT-NAACL
International Workshop on Medical Speech Transla-
tion, pages 9?16, New York.
[Flores2005] G. Flores. 2005. The impact of medical in-
terpreter services on the quality of health care: A sys-
tematic review. Medical Care Research and Review,
62:255?299.
[Flores2006] G. Flores. 2006. Language barriers to
health care in the united states. New England Journal
of Medicine, 355:229?231.
[Fluential2007] Fluential, 2007.
http://www.fluentialinc.com. As of 24 March
2007.
[Graddol2004] D. Graddol. 2004. The future of lan-
guage. Science, 303:1329?1331.
[Kittredge2003] R. I. Kittredge. 2003. Sublanguages and
comtrolled languages. In R. Mitkov, editor, The Ox-
ford Handbook of Computational Linguistics, pages
430?447. Oxford University Press.
[Michie et al2003] S. Michie, J. Miles, and J. Weinman.
2003. Patient-centeredness in chronic illness: what is
it and does it matter? Patient Education and Counsel-
ing, 51:197?206.
[Mitamura1999] T. Mitamura. 1999. Controlled lan-
guage for multilingual machine translation. In Pro-
ceedings of Machine Translation Summit VII, Singa-
pore.
[Phraselator2007] Phraselator, 2007.
http://www.voxtec.com/. As of 24 March 2007.
[Rayner et al2005a] M. Rayner, P. Bouillon,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, H. Isahara, K. Kanzaki, and Y. Nakao.
2005a. A methodology for comparing grammar-based
and robust approaches to speech understanding. In
Proceedings of the 9th International Conference
on Spoken Language Processing (ICSLP), pages
1103?1107, Lisboa, Portugal.
[Rayner et al2005b] M. Rayner, B.A. Hockey, J.M. Ren-
ders, N. Chatzichrisafis, and K. Farrell. 2005b. A
voice enabled procedure browser for the International
Space Station. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (interactive poster and demo track), Ann Arbor,
MI.
[Rayner et al2006] M. Rayner, B.A. Hockey, and
P. Bouillon. 2006. Putting Linguistics into Speech
Recognition: The Regulus Grammar Compiler. CSLI
Press, Chicago.
[Shieber et al1990] S. Shieber, G. van Noord, F.C.N.
Pereira, and R.C. Moore. 1990. Semantic-head-driven
generation. Computational Linguistics, 16(1).
[Somers2006] H. Somers. 2006. Language engineering
and the path to healthcare: a user-oriented view. In
Proceedings of the HLT-NAACL International Work-
shop on Medical Speech Translation, pages 32?39,
New York.
[Starlander et al2005] M. Starlander, P. Bouillon,
N. Chatzichrisafis, M. Santaholma, M. Rayner, B.A.
Hockey, H. Isahara, K. Kanzaki, and Y. Nakao. 2005.
Practising controlled language through a help system
integrated into the medical speech translation system
(MedSLT). In Proceedings of MT Summit X, Phuket,
Thailand.
[Stewart1995] M.A. Stewart. 1995. Effective physician-
patient communication and health outcomes: a review.
Canadian Medical Association Journal, 152:1423?
1433.
48
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 80?86,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Zero to Spoken Dialogue System in One Quarter: Teaching Computational
Linguistics to Linguists Using Regulus
Beth Ann Hockey
Department of Linguistics,
UARC
UC Santa Cruz
Mail-Stop 19-26, NASA Ames
Moffett Field, CA 94035-1000
bahockey@ucsc.edu
Gwen Christian
Department of Linguistics
UCSC
Santa Cruz, CA 95064, USA
jchristi@ucsc.edu
Abstract
This paper describes a Computational Lin-
guistics course designed for Linguistics stu-
dents. The course is structured around the ar-
chitecture of a Spoken Dialogue System and
makes extensive use of the dialogue system
tools and examples available in the Regu-
lus Open Source Project. Although only a
quarter long course, students learn Computa-
tional Linguistics and programming sufficient
to build their own Spoken Dialogue System as
a course project.
1 Introduction
Spoken Dialogue Systems model end-to-end ex-
ecution of conversation and consequently require
knowledge of many areas of computational linguis-
tics, speech technology and linguistics. The struc-
ture of Spoken Dialogue Systems offers a ready-
made structure for teaching a computational linguis-
tics course. One can work through the components
and cover a broad range of material in a grounded
and motivating way. The course described in this
paper was designed for linguistics students, upper-
division undergraduate and graduate, many having
limited experience with programming or computer
science. By the end of a quarter long course, stu-
dents were able to build a working spoken dialogue
systems and had a good introductory level under-
standing of the related computational linguistics top-
ics.
When this course was first being contemplated,
it became apparent that there were a number of
somewhat unusual properties that it should have,
and a number of useful goals for it to accomplish.
The Linguistics Department in which this course is
given had only sporadic offerings of computational
courses, due in part to having no faculty with a pri-
mary focus in Computational Linguistics. linguis-
tics students are very interested in having courses in
this area, but even in the University as a whole avail-
ability is limited. A course on information extraction
is offered in the Engineering School and while some
linguistics students are equipped to take that course,
many do not have sufficient computer science back-
ground or programming experience to make that a
viable option.
This course, in the Linguistics department,
needed to be for linguistics students, who might not
have well-developed computer skills. It needed to
fit into a single quarter, be self-contained, depend
only on linguistics courses as prerequisites, and give
students at least an overview of a number of areas
of CL. These students are also interested in con-
nections with industry; now that there are industry
jobs available for linguists, students are eager for in-
ternships and jobs where they can apply the skills
learned in their linguistics courses. Given this, it
was also important that the students learn to program
during the course, both to make engineering courses
more accessible, and to attract potential employers.
In addition, since the department was interested
in finding ways to expand computational linguistics
offerings, it clearly would be good if the course ap-
pealed to the students, the department?s faculty and
to higher levels of the University administration.
80
2 Class Demographics
Students in the course are a mix of graduates and
upper-division undergraduates with a solid back-
ground in syntax and semantics but are not expected
to have much in the way of programming experi-
ence. Familiarity with Windows, Unix and some
minimal experience with shell scripting are recom-
mended but not required. Students have been very
successful in the course starting with no program-
ming experience at all. Because the Linguistics de-
partment is especially strong in formal linguistics,
and the courses typically require extensive problem
sets, linguistics students have good aptitude for and
experience working with formal systems and this ap-
titude and skill set seems to transfer quite readily to
programming.
3 Regulus Open Source Platform
The Regulus Open Source Platform is a major re-
source for the course. Regulus is designed for
corpus-based derivation of efficient domain-specific
speech recognisers from general linguistically-
motivated unification grammars. The process of
creating an application-specific Regulus recogniser
starts with a general unification grammar (UG), to-
gether with a supplementary lexicon containing ex-
tra domain-specific vocabulary. An application-
specific UG is then automatically derived using Ex-
planation Based Learning (EBL) specialisation tech-
niques (van Harmelen and Bundy, 1988). This
corpus-based EBL method is parameterised by 1) a
small domain-specific training corpus, from which
the system learns the vocabulary and types of
phrases that should be kept in the specialised gram-
mar, and 2) a set of ?operationality criteria?, which
control the specialised grammar?s generality. The
application-specific UG is then compiled into a
Nuance-compatible CFG. Processing up to this point
is all carried out using Open Source Regulus tools.
Two Nuance utilities then transform the output CFG
into a recogniser. One of these uses the training cor-
pus a second time to convert the CFG into a PCFG;
the second performs the PCFG-to-recogniser com-
pilation step. This platform has been used the base
for an number of applications including The Clarissa
Procedure Browser (Clarissa, 2006) and MedSLT
(Bouillon et al, 2005)
The Regulus website (Regulus, 2008) makes
available a number of resources, including compil-
ers, an integrated development environment, a Reg-
ulus resource grammar for English, online docu-
mentation and a set of example dialogue and trans-
lation systems. These examples range from com-
pletely basic to quite complex. This material is all
described in detail in the Regulus book (Rayner et
al., 2006), which documents the system and pro-
vides a tutorial. As noted in reviews of the book,
(Roark, 2007) (Bos, 2008) it is very detailed. To
quote Roark, ?the tutorial format is terrifically ex-
plicit which will make this volume appropriate for
undergraduate courses looking to provide students
with hands-on exercises in building spoken dialog
systems.? Not only does the Regulus-based dia-
logue architecture supply an organizing principle for
the course but a large proportion of the homework
comes from the exercises in the book. The exam-
ples serve as starting points for the students projects,
give good illustrations of the various dialogue com-
ponents and are nice clean programming examples.
The more research-oriented material in the Regulus
book also provides opportunities for discussion of
topics such as unification, feature grammars, ellip-
sis processing, dialogue-state update, Chomsky hi-
erarchy and compilers. Reviewers of the book have
noted a potential problem: although Regulus itself
is open source it is currently dependent on two com-
mercial pieces of software, SICStus Prolog and Nu-
ance speech recognition platform (8.5). Nuance 8.5
is a speech recognition developer platform that is
widely used for build telephone call centers. This
developer kit supplies the acoustic models which
model the sounds of the language, the user supplies
a language model which defines the range of lan-
guage that will be recognized for a particular appli-
cation. This dependance on these commercial prod-
ucts has turned out not to be a serious problem for
us since we were able to get a research license from
Nuance and purchase a site license for SICStus Pro-
log. However, beyond the fact that we were able to
get licenses, we are not convinced that eliminating
the commercial software would be an educational
win. While, for example, SWI Prolog might work
as well in the course the commercial SISCtus Pro-
log given a suitable port of Regulus, we think that
having the students work with a widely used com-
81
mercial speech recognition product such as Nuance,
is beneficial training for students looking for jobs
or internships. Using Nuance also avoids frustration
because its performance is dramatically better than
the free alternatives.
4 Other Materials
The course uses a variety of materials in addition to
the Regulus platform and book. For historical and
current views of research in dialogue and speech,
course sessions typically begin with an example
project or system, usually with a video or a runnable
version. Examples of system web materials
that we use include: (Resurrected)SHRDLU
(http://www.semaphorecorp.com/
misc/shrdlu.html), TRIPS and TRAINS
(http://www.cs.rochester.edu/
research/cisd/projects/trips/
movies/TRIPS\ Overview/), Galaxy
(http://groups.csail.mit.edu/sls/
/applications/jupiter.shtml), Vo-
calJoyStick (http://ssli.ee.washington.
edu/vj/), and ProjectListen (http://
www.cs.cmu.edu/?listen/mm.html)and
NASA?s Clarissa Procedure Browser (http://
ti.arc.nasa.gov/projects/clarissa/
gallery.php?ta\=\&gid\=\&pid\=).
Jurasfsky and Martin (Jurafsky and Martin, 2000)
is used as an additional text and various research pa-
pers are given as reading in addition to the Regulus
material. Jurafsky and Martin is also good source
for exercises. The Jurafsky and Martin material and
the Regulus material are fairly complementary and
fit together well in the context of this type of course.
Various other exercises are used, including two stu-
dent favorites: a classic ?construct your own ELIZA?
task, and a exercise in reverse engineering a tele-
phone call center, which is an original created for
this course.
5 Programming languages
Prolog is used as the primary language in the course
for several reasons. First, Prolog was built for pro-
cessing language and consequently has a natural fit
to language processing tasks. Second, as a high-
level language, Prolog allows students to stay on a
fairly conceptual level and does not require them to
spend time learning how to handle low-level tasks.
Prolog is good for rapid prototyping; a small amount
of Prolog code can do a lot of work and in a one
quarter class this is an important advantage. Also,
Prolog is very close to predicate logic, which the lin-
guistics students already know from their semantics
classes. When the students look at Prolog and see
something familiar, it builds confidence and helps
make the task of learning to program seem less
daunting. The declarative nature of Prolog, which
often frustrates computer science students who were
well trained in procedural programming, feels natu-
ral for the linguists. And finally, the Regulus Open
Source System is written mainly in Prolog, so using
Prolog for the course makes the Regulus examples
maximally accessible.
Note that Regulus does support development of
Java dialogue processing components, and provides
Java examples. However, the Java based examples
are two to three times longer, more complicated and
less transparent than their Prolog counterparts, for
the same functionality. We believe that the Java
based materials would be very good for a more ad-
vanced course on multimodal applications, where
the advantages of Java would be evident, but in a
beginning course for linguists, we find Prolog peda-
gogically superior.
A potential downside to using Prolog is that it
is not a particularly mainstream programming lan-
guage. If the course was solely about technical train-
ing for immediate employment, Java or C++ would
probably be better. However, because most students
enter the course with limited programming expe-
rience, the most important programming outcomes
for the course are that they end up with evidence
that they can complete a non-trivial programming
project, that they gain the experience of debugging
and structuring code and that they end up better
able to learn additional computer science subsequent
to the course. The alternative these students have
for learning programming is to take traditional pro-
gramming courses, starting with an extremely basic
introduction to computers course and taking 1-2 ad-
ditional quarter long courses to reach the level of
programming sophistication that they reach in one
quarter in this course. In addition, taking the alterna-
tive route, they would learn no Computational Lin-
guistics, and would likely find those courses much
82
less engaging.
6 Course Content
Figure 6, depicts relationships between the dialogue
system components and related topics both in Lin-
guistics and in Computational Linguistics and/or
Computer Science. The course follows the flow of
the Dialogue System processing through the various
components, discussing topics related to each com-
ponent. The first two weeks of the course are used
as an overview. Spoken Dialogue Systems are put
in the context of Computational Linguistics, Speech
Technology, NLP and current commercial and re-
search state of the art. General CL tools and tech-
niques are introduced and a quick tour is made of the
various dialogue system components. In addition to
giving the students background about the field, we
want them to be functioning at a basic level with the
software at the end of two weeks so that they can be-
gin work on their projects. Following the two week
introduction, about two weeks are devoted to each
component.
The speech recognition discussion is focused
mainly on language modeling. This is an area of par-
ticular strength for Regulus and the grammar-based
modeling is an easy place for linguists to start. Cov-
ering the details of speech recognition algorithms in
addition to the other material being covered would
be too much for a ten week course. In addition, the
department has recently added a course on speech
recognition and text-to-speech, so this is an obvi-
ous thing to omit from this course. With the Nu-
ance speech recognition platform, there is plenty for
the students to learn as users rather than as speech
recognition implementers. In practice, it is not un-
usual for a Spoken Dialogue System implementer to
use a speech recognition platform rather than build-
ing their own, so the students are getting a realistic
experience.
For the Input Management, Regulus has imple-
mented several types of semantic representations,
from a simple linear list representation that can be
used with the Alterf robust semantics tool, to one
that handles complex embedding. So the Input Man-
ager related component can explore the trade offs in
processing and representation, using Regulus exam-
ples.
The Dialogue Managment section looks at simple
finite state dialogue management as well as the dia-
logue state update approach that has typically been
used in Regulus based applications. Many other top-
ics are possible depending on the available time.
The Output Management unit looks at various as-
pects of generation, timing of actions and could also
discuss paraphrase generation or prosodic mark up.
Other topics of a system wide nature such as N-
best processing or help systems can be discussed at
the end of the course if time allows.
7 Improvements for ?08
The course is currently being taught for Spring quar-
ter and a number of changes have been implemented
to address what we felt were the weak points of the
course as previously taught. It was generally agreed
that the first version of the course was quite success-
ful and had many of the desired properties. Students
learned Computational Linguistics and they learned
how to program. The demo session of the students?
projects held at the end of the course was attended by
much of the linguistics department, plus a few out-
side visitors. Attendees were impressed with how
much the students had accomplished. In building on
that success, we wanted to improve the following ar-
eas: enrollment, limiting distractions from the spo-
ken dialogue material, building software engineer-
ing skills, making connections with industry and/or
research, and visibility.
The first time the course was given, enrollment
was six students. This level of enrollment was no
doubt in part related to the fact that the course was
announced relatively late and students did not have
enough lead time to work it into their schedules. The
small size was intimate, but it seemed as though it
would be better for more students to be able to ben-
efit from the course. For the current course, students
knew a year and a half in advance that it would be
offered. We also had an information session about
the course as part of an internship workshop, and
apparently word of mouth was good. With addition
of a course assistant the maximum we felt we could
handle without compromising the hands-on experi-
ence was twenty. Demand greatly exceeded supply
and we ended up with twenty two students initially
enrolled. As of the deadline for dropping without
83
Figure 1: Course Schematic: Architecture of Dialogue System with associated linguistic areas/topic at above and
Computational Linguistics and/or Computer Science topics below
penalty course enrollment is 16. The course is cur-
rently scheduled to be taught every other year but we
are considering offering it in summer school in the
non-scheduled years.
Two activities in the first incarnation of the course
were time-consuming without contributing directly
to learning about CL and Dialogue Systems. First,
students spent considerable time getting the soft-
ware, particularly the Nuance speech recognition
software and the Nuance text-to-speech, installed on
their personal machines. The variability across their
machines and fact we did not at that time have a
good way to run the software on Machintoshes con-
tributed to the problem. This made the speech as-
pects seem more daunting than they should have,
and delayed some of the topics and exercises.
For the current course, we arranged to have all
of the software up and running for them on day
one, in an instructional lab on campus. Mandatory
lab sessions were scheduled for the course in the
instructional lab, starting on the first day of class,
so that we could make sure that students were able
to run the software from the very beginning of the
course. These arrangements did not work out quite
as smoothly as we had hoped, but was still an im-
provement over the first time the course was taught.
Rather than being completely dependent on stu-
dents? personal machines, the labs, combined with
a strategy we worked out for running the software
from external USB drives, provide students with a
way to do their assignments even if they have un-
suitable personal machines. In the labs, students are
able to see how the software should behave if prop-
erly installed, and this is very helpful to them when
installing on their personal machines. We refined the
installation instructions considerably, which seemed
to improve installation speed. The Macintosh prob-
lem has been solved, at least for Intel Macs, since we
84
have been successful in running the software with
BootCamp. The twice weekly lab sessions also give
students a chance do installation and practical lab
exercises in an environment in which the course as-
sistant is able see what they are doing, and give them
assistance. Observing and getting help from more
computationally savy classmates is also common in
the labs. Athough the measures taken to reduce the
software installation burden still leave some room
for improvement, students were able to use Regulus
and Nuance successfully, on average, in less than
half the time required the first time the course was
taught.
The other distracting activity was building the
backend for the course projects. Spoken Dialogue
Systems are usually an interface technology, but the
students in the first offering of the course had to
build their projects end to end. While this was not
a complete loss, since they did get useful program-
ming experience, it seemed as though it would be
an improvement if students could focus more on the
spoken dialogue aspects. The approach for doing
this in the current course is to recruit Project Part-
ners from industry, government, academic research
projects and other university courses. Our students
build the spoken dialogue system components and
then work with their Project Partner to connect to
the Project Partner?s system. The students will then
demonstrate the project as a whole, that is, their di-
alogue system working with the Project Partner?s
material/system, at the course end demo sessions.
We have project partners working in areas such as:
robotics, telephone based services, automotive in-
dustry, and virtual environments. There are a num-
ber of potential benefits to this approach. Students
are able to spend most of their time on the spoken
dialogue system and yet have something interest-
ing to connect to. In fact, they have access to sys-
tems that are real research projects, and real com-
mercial products that are beyond what our students
would be capable of producing on their own. Stu-
dents gain the experience of doing a fairly realistic
software collaboration, in which they are the spo-
ken dialogue experts. Project partners are enthu-
siastic because they get to try projects they might
not have time or resources to do. Industry partners
get to check out potential interns and research part-
ners may find potential collaborators. In the previ-
ous version of the course, half of the students who
finished the course subsequently worked on Spoken
Dialogue oriented research projects connected with
the department. One of the students had a successful
summer internship with Ford Motors as a result of
having taken the course. The research and industry
connection was already there, but the Project Partner
program strengthens it and expands the opportuni-
ties beyond projects connected with the department.
One enhancement to students? software engineer-
ing skills in the current version of the course is that
students are using version control from day one.
Each student in the course is being provided with
a Subversion repository with a Track ticket system
hosted by Freepository.com. Part of the incentive
for doing this was to protect Project Partners? IP, so
that materials provided by (particularly commercial)
Project Partners would not be housed at the Univer-
sity, and would only be accessible to relevant stu-
dent(s), the Project Partner, the instructor and the
course assistant. The repositories also support re-
mote collaboration making a wider range of orga-
nizations workable as project partners. With the
repositories the students gain experience with ver-
sion control and bug-tracking. Having the version
control and ticket system should also make the de-
velopment of their projects easier. Another way we
are hoping to enhance the students software skills is
through simply having more assistance available for
students in this area. We have added the previously
mentioned lab sections in the instructional labs, we
have arranged for the course assistant to have sub-
stantial time available for tutoring, and we are post-
ing tutorials as needed on the course website.
The final area of improvement that we wanted to
address is visibility. This is a matter of some prac-
tical importance for the course, the addition of CL
to the department?s offerings, and the students. Vis-
ibility among students has improved with word of
mouth and with the strategically timed information
session held the quarter prior to holding the course.
The course end demo session in the first offering of
the course did a good job of bringing it to the at-
tention of the students and faculty in the Linguis-
tics Department. For the current course, the Project
Partner program provides considerable visibility for
students, the department, and the University, among
industry, government and other Universities. We are
85
also expanding the demo session at the end of the
course. This time the demo session will be held as a
University wide event, and will be held at the main
UC Santa Cruz campus and a second time at the Uni-
versity?s satellite Silicon Valley Center, in order to
tap into different potential audiences. The session
at the Silicon Valley Center has potential for giving
students good exposure to potential employers, and
both sessions have good potential for highlighting
the Linguistics department.
8 Summary and Conclusion
The course presented in this paper has three key fea-
tures. First it is designed for linguistics students.
This means having linguistics and not computer sci-
ence as prerequisites and necessitates teaching stu-
dents programming and computer science when they
may start with little or no background. Second,
the course takes the architecture of a Spoken Dia-
logue System as the structure of the course, working
through the components and discussing CL topics
as they relate to the components. The third feature is
the extensive use of the Regulus Open Source plat-
form as key resource for the course. Regulus ma-
terial is used for exercises, as a base for construc-
tion of students? course projects, and for introducing
topics such as unification, feature grammars, Chom-
sky hierarchy, and dialogue management. We have
found this combination excellent for teaching CL to
linguistics students. The grammar-based language
modeling in Regulus, the use of Prolog and relat-
ing linguistic topics as well as computational ones to
the various dialogue system components, gives lin-
guistics students familiar material to build on. The
medium vocabulary type of Spoken Dialogue sys-
tem supported by the Regulus platform, makes a
very motivating course project and students are able
to program by the end of the course.
We discuss a number of innovations we have in-
troduced in the latest version of the course, such as
the Project Partner program, use of instructional labs
and subversion repositories, and expanded course
demo session. Since we are teaching the course for
the second time during Spring Quarter, we will be
able to report on the outcome of these innovations at
the workshop.
Acknowledgments
We would like to thank Nuance, for giving us the
research licenses for Nuance 8.5 and Vocalizer that
helped make this course and this paper possible.
References
Johan Bos. 2008. A review of putting linguistics into
speech recognition. the regulus grammar compiler.
Natural Language Engineering, 14(1).
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
Clarissa, 2006. http://www.ic.arc.nasa.gov/projects/clarissa/.
As of 1 Jan 2006.
D. Jurafsky and J. H. Martin. 2000. Speech and
Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics and
Speech Recognition. Prentice Hall Inc, New Jersey.
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Putting
Linguistics into Speech Recognition: The Regulus
Grammar Compiler. CSLI Press, Chicago.
Regulus, 2008. http://www.issco.unige.ch/projects/regulus/,
http://sourceforge.net/projects/regulus/. As of 1 Jan
2008.
Brian Roark. 2007. A review of putting linguistics into
speech recognition: The regulus grammar compiler.
Computational Linguistics, 33(2).
T. van Harmelen and A. Bundy. 1988. Explanation-
based generalization = partial evaluation (research
note). Artificial Intelligence, 36:401?412.
86
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 54?62,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Using Artiially Generated Data
to Evaluate Statistial Mahine Translation
Manny Rayner, Paula Estrella, Pierrette Bouillon
University of Geneva, TIM/ISSCO
40 bvd du Pont-d'Arve, CH-1211 Geneva 4, Switzerland
fEmmanuel.Rayner,Paula.Estrella,Pierrette.Bouillongunige.h
Beth Ann Hokey
Mail Stop 19-26, UCSC UARC
NASA Ames Researh Center, Moffett Field, CA 94035?1000
bahokeyus.edu
Yukie Nakao
LINA, Nantes University, 2, rue de la Houssiniere, BP 92208 44322 Nantes Cedex 03
yukie.nakaouniv-nantes.fr
Abstrat
Although Statistial Mahine Translation
(SMT) is now the dominant paradigm
within Mahine Translation, we argue that
it is far from lear that it an outperform
Rule-Based Mahine Translation (RBMT)
on small- to medium-voabulary applia-
tions where high preision is more impor-
tant than reall. A partiularly important
pratial example is medial speeh trans-
lation. We report the results of exper-
iments where we ongured the various
grammars and rule-sets in an Open Soure
medium-voabulary multi-lingual medial
speeh translation system to generate large
aligned bilingual orpora for English !
Frenh and English ! Japanese, whih
were then used to train SMT models based
on the ommon ombination of Giza++,
Moses and SRILM. The resulting SMTs
were unable fully to reprodue the per-
formane of the RBMT, with performane
topping out, even for English ! Frenh,
with less than 70% of the SMT translations
of previously unseen sentenes agreeing
with RBMT translations. When the out-
puts of the two systems differed, human
judges reported the SMT result as fre-
quently being worse than the RBMT re-
sult, and hardly ever better; moreover, the
added robustness of the SMT only yielded
a small improvement in reall, with a large
penalty in preision.
1 Introdution
When Statistial Mahine Translation (SMT) was
rst introdued in the early 90s, it enountered a
hostile reeption, and many people in the researh
ommunity were unwilling to believe it ould ever
be a serious ompetitor to symboli approahes
(f. for example (Arnold et al, 1994)). The pendu-
lum has now swung all the way to the other end of
the sale; right now, the prevailing wisdom within
the researh ommunity is that SMT is the only
truly viable arhiteture, and that rule-based ma-
hine translation (RBMT) is ultimately doomed to
failure. In this paper, one of our initial onerns
will be to argue for a ompromise position. In our
opinion, the initial septiism about SMT was not
groundless; the arguments presented against it of-
ten took the form of examples involving deep lin-
guisti reasoning, whih, it was laimed, would be
hard to address using surfae methods. Proponents
of RBMT had, however, greatly underestimated
the extent to whih SMT would be able to takle
the problem of robustness, where it appears to be
far more powerful than RBMT. For most mahine
translation appliations, robustness is the entral
issue, so SMT's urrent preeminene is hardly sur-
prising.
Even for the large-voabulary tasks where SMT
does best, the situation is by no means as lear as
one might imagine: aording to (Wilks, 2007),
purely statistial systems are still unable to out-
perform SYSTRAN. In this paper, we will how-
ever be more onerned with limited-domain MT
tasks, where robustness is not the key requirement,
and auray is paramount. An immediate exam-
54
ple is medial speeh translation, whih is estab-
lishing itself as an an appliation area of some sig-
niane (Bouillon et al, 2006; Bouillon et al,
2008a). Translation in medial appliations needs
to be extremely aurate, sine mistranslations
an have serious or even fatal onsequenes. At
the panel disussion at the 2008 COLING work-
shop on safety-ritial speeh translation (Rayner
et al, 2008), the onsensus opinion, based on in-
put from pratising physiians, was that an appro-
priate evaluation metri for medial appliations
would be heavily slanted towards auray, as op-
posed to robustness. If the metri is normalised so
as to award 0 points for no translation, and 1 point
for a orret translation, the estimate was that a
suitable sore for an inorret translation would
be something between ?25 and ?100 points. With
these requirements, it seems unlikely that a robust,
broad-overage arhiteture has muh hane of
suess. The obvious strategy is to build a limited-
domain ontrolled-language system, and tune it to
the point where auray reahes the desired level.
For systems of this kind, it is at least oneiv-
able that RBMT may be able to outperform SMT.
The next question is how to investigate the issues
in a methodologially even-handed way. A few
studies, notably (Seneff et al, 2006), suggest that
rule-based translation may in fat be preferable in
these ases. (Another related experiment is de-
sribed in (Dugast et al, 2008), though this was
arried out in a large-voabulary system). These
studies, however, have not been widely ited. One
possible explanation is suspiion about method-
ologial issues. Seneff and her olleagues trained
their SMT system on 20 000 sentene pairs, a
small number by the standards of SMT. It is a pri-
ori not implausible that more training data would
have enabled them to reate an SMT system that
was as good as, or better than, the rule-based sys-
tem.
In this paper, our primary goal is to take this
kind of objetion seriously, and develop a method-
ology designed to enable a tight omparison be-
tween rule-based and statistial arhitetures. In
partiular, we wish to examine the widely be-
lieved laim that SMT is now inherently better
than RBMT. In order to do this, we start with a
limited-domain RBMT system; we use it to auto-
matially generate a large orpus of aligned pairs,
whih is used to train a orresponding SMT sys-
tem. We then ompare the performane of the two
systems.
Our argument will be that this situation essen-
tially represents an upper bound for what is possi-
ble using the SMT approah in a limited domain.
It has been widely remarked that quality, as well
as quantity, of training data is important for good
SMT; in many projets, signiant effort is ex-
pended to lean the original training data. Here,
sine the data is automatially generated by a rule-
based system, we an be sure that it is already
ompletely lean (in the sense of being internally
onsistent), and we an generate as large a quan-
tity of it as we require. The appliation, more-
over, uses only a smallish voabulary and a fairly
onstrained syntax. If the derived SMT system is
unable to math the original RBMT system's per-
formane, it seems reasonable to laim that this
shows that there are types of appliations where
RBMT arhitetures are superior.
The experiments desribed have been arried
out using MedSLT, an Open Soure interlingua-
based limited-domain medial speeh translation
system. The rest of the paper is organised as fol-
lows. Setion 2 provides bakground on the Med-
SLT system. Setion 3 desribes the experimen-
tal framework, and Setion 4 the results obtained.
Setion 5 onludes.
2 The MedSLT System
MedSLT (Bouillon et al, 2005; Bouillon et al,
2008b) is a medium-voabulary interlingua-based
Open Soure speeh translation system for dotor-
patient medial examination questions, whih
provides any-language-to-any-language transla-
tion apabilities for all languages in the set En-
glish, Frenh, Japanese, Arabi, Catalan. Both
speeh reognition and translation are rule-based.
Speeh reognition runs on the Nuane 8.5 reog-
nition platform, with grammar-based language
models built using the Open Soure Regulus om-
piler. As desribed in (Rayner et al, 2006),
eah domain-spei language model is extrated
from a general resoure grammar using orpus-
based methods driven by a seed orpus of domain-
spei examples. The seed orpus, whih typi-
ally ontains between 500 and 1500 utteranes,
is then used a seond time to add probabilisti
weights to the grammar rules; this substantially
improves reognition performane (Rayner et al,
2006, x11.5). Voabulary sizes and performane
measures for speeh reognition in the three lan-
55
guages where serious evaluations have been ar-
ried out are shown in Figure 1.
Language Voab WER SemER
English 447 6% 11%
Frenh 1025 8% 10%
Japanese 422 3% 4%
Table 1: Reognition performane for English,
Frenh and Japanese MedSLT reognisers. ?Vo-
ab? = number of surfae words in soure lan-
guage reogniser voabulary; ?WER? = Word Er-
ror Rate for soure language reogniser, on in-
overage material; ?SemER? = semanti error rate
(proportion of utteranes failing to produe orret
interlingua) for soure language reogniser, on in-
overage material.
At run-time, the reogniser produes a soure-
langage semanti representation. This is rst
translated by one set of rules into an interlingual
form, and then by a seond set into a target lan-
guage representation. A target-language Regu-
lus grammar, ompiled into generation form, turns
this into one or more possible surfae strings, af-
ter whih a set of generation preferenes piks
one out. Finally, the seleted string is realised in
spoken form. Robustness issues are addressed by
means of a bak-up statistial reogniser, whih
drives a robust embedded help system. The pur-
pose of the help system (Chatzihrisas et al,
2006) is to guide the user towards supported ov-
erage; it performs approximate mathing of out-
put from the statistial reogniser again a library
of sentenes whih have been marked as orretly
proessed during system development, and then
presents the losest mathes to the user.
Examples of typial English domain sentenes
and their translations into Frenh and Japanese are
shown in Figure 2.
3 Experimental framework
In the literature on language modelling, there is
a known tehnique for bootstrapping a statisti-
al language model (SLM) from a grammar-based
language model (GLM). The grammar whih
forms the basis of the GLM is sampled randomly
in order to reate an arbitrarily large orpus of ex-
amples; these examples are then used as a train-
ing orpus to build the SLM (Jurafsky et al, 1995;
Jonson, 2005). We adapt this proess in a straight-
forward way to onstrut an SMT for a given
language pair, using the soure language gram-
mar, the soure-to-interlingua translation rules, the
interlingua-to-target-language rules, and the tar-
get language generation grammar. We start in the
same way, using the soure language grammar to
build a randomly generated soure language or-
pus; as shown in (Hokey et al, 2008), it is im-
portant to have a probabilisti grammar. We then
use the omposition of the other omponents to
attempt to translate eah soure language sentene
into a target language equivalent, disarding the
examples for whih no translation is produed.
The result is an aligned bilingual orpus of ar-
bitrary size, whih an be used to train an SMT
model.
We used this method to generate aligned or-
pora for the two MedSLT language pairs English
! Frenh and English ! Japanese. For eah lan-
guage pair, we rst generated one million soure-
language utteranes; we next ltered them to keep
only examples whih were full sentenes, as op-
posed to elliptial phrases, and nally used the
translation rules and target-language generators to
attempt to translate eah sentene. This reated
approximately 305K aligned sentene-pairs for
English ! Frenh (1901K words English, 1993K
words Frenh), and 311K aligned sentene-pairs
for English ! Japanese (1941K words English,
2214K words Japanese). We held out 2.5% of
eah set as development data, and 2.5% as test
data. Using Giza++, Moses and SRILM (Oh and
Ney, 2000; Koehn et al, 2007; Stolke, 2002), we
trained SMT models from inreasingly large sub-
sets of the training portion, using the development
portion in the usual way to optimize parameter val-
ues. Finally, we used the resulting models to trans-
late the test portion.
Our primary goal was to measure the extent to
whih the derived versions of the SMT were able
to approximate the original RBMT on data whih
was within the RBMT's overage. There is a sim-
ple and natural way to perform this measurement:
we apply the BLEU metri (Papineni et al, 2001),
with the RBMT's translation taken as the refer-
ene. This means that perfet orrespondene be-
tween the two translations would yield a BLEU
sore of 1.0.
This raises an important point. The BLEU
sores we are using here are non-standard; they
measure the extent to whih the SMT approxi-
mates the RBMT, rather than, as usual, measuring
56
English Is the pain above your eye?
Frenh Avez-vous mal au dessus des yeux?
Japanese Itami wa me no ue no atari desu ka?
English Have you had the pain for more than a month?
Frenh Avez-vous mal depuis plus d'un mois?
Japanese Ikkagetsu ijou itami wa tsuzuki mashita ka?
English Is the pain assoiated with nausea?
Frenh Avez-vous des nause?es quand vous avez la douleur?
Japanese Itamu to hakike wa okori masu ka?
English Does bright light make the pain worse?
Frenh La douleur est-elle aggrave?e par une lumiere forte?
Japanese Akarui hikari wo miru to zutsu wa hidoku nari masu ka?
Table 2: Examples of English domain sentenes, and the system's translations into Frenh and Japanese.
the extent to whih it approximates human trans-
lations. It is important to bring in human judge-
ment, to evaluate the ases where the SMT and
RBMT differ. If, in these ases, it transpired that
human judges typially thought that the SMT was
as good as the RBMT, then the differene would
be purely aademi. We need to satisfy ourselves
that human judges typially asribe differenes be-
tween SMT and RBMT to shortomings in the
SMT rather than in the RBMT.
Conretely, we olleted all the different
hSoure, SMT-translation, RBMT-translationi
triples produed during the ourse of the ex-
periments, and extrated those where the two
translations were different. We randomly seleted
a set of examples for eah language pair, and
asked human judges to lassify them into one of
the following ategories:
 RBMT better: The RBMT translation was
better, in terms of preserving meaning and/or
being grammatially orret;
 SMT better: The SMT translation was bet-
ter, in terms of preserving meaning and/or be-
ing grammatially orret;
 Similar: Both translations were about
equally good OR the soure sentene was
meaningless in the domain.
In order to show that our metris are intuitively
meaningful, it is sufient to demonstrate that the
frequeny of ourrene of RBMT better is both
large in omparison to that of SMT better, and
aounts for a substantial proportion of the total
population.
Finally, we onsider the question of whether
the SMT, whih is apable of translating out-of-
grammar sentenes, an add useful robustness to
the base system. We olleted, from the set used in
the experiments desribed in (Rayner et al, 2005),
all the English sentenes whih failed to be trans-
lated into Frenh. We used the best version of
the English ! Frenh SMT to translate eah of
these sentenes, and asked human judges to eval-
uate the translations as being learly aeptable,
learly unaeptable, or borderline.
In the next setion, we present the results of the
various experiments we have just desribed.
4 Results
We begin with Figure 1, whih shows non-
standard BLEU sores for versions of the English
! Frenh SMT system trained on quantities of
data inreasing from 14 287 to 285 740 pairs. As
an be seen, translation performane improves up
to about 175 000 pairs. After this, it levels out
at around BLEU = 0.90, well below that of the
RBMT system with whih it is being ompared.
A more diret way to report the result is simply to
ount the proportion of test sentenes that are not
in the training data, whih are translated similarly
by the SMT and the RBMT. This gure tops out at
around 68%.
The results strongly suggest that the SMT is
unable to repliate the RBMT's performane at
all losely even in an easy language-pair, irre-
spetive of the amount of training data available.
Out of uriosity, and to reassure ourselves that the
automati generation proedure was doing some-
thing useful, we also tried training the English !
Frenh SMT on pairs derived from the 669 ut-
57
Figure 1: Non-standard BLEU sores against
number of pairs of training sentenes for English
! Frenh; training and test data both indepen-
dently generated, hene overlapping.
terane ?seed orpus? used to generate the gram-
mar (f. Setion 2). This produed utterly dis-
mal performane, with BLEU = 0.52. The result is
more interesting than it may rst appear, sine, in
speeh reognition, the differene in performane
between the SLMs trained from seed orpora and
large generated orpora is fairly small (Hokey et
al., 2008).
It seemed possible that the improvement in per-
formane with inreased quantities of training data
might, in effet, only be due to the SMT fun-
tioning as a translation memory; sine training
and test data are independently generated by the
same random proess, they overlap, with the de-
gree of overlap inreasing as the training set gets
larger. In order to investigate this hypothesis,
we repeated the experiments with data whih had
been uniqued, so that the training and test sets
were ompletely disjoint, and neither ontained
any dupliate sentenes
1
. In fat, Figure 2 show
that the graph for uniqued English ! Frenh data
are fairly similar to the one for the original non-
uniqued data shown in Figures 1. The main differ-
ene is that the non-standard BLEU sore for the
1
Our opinion is that this is not a realisti way to evaluate
the performane of a small-voabulary system; for example,
in MedSLT, one expets that at least some training sentenes,
e.g. ?Where is the pain??, will also our frequently in test
data.
Figure 2: Non-standard BLEU sores against
number of pairs of training sentenes for English
! Frenh; training and test data both indepen-
dently generated, then uniqued to remove dupli-
ates and overlapping items.
uniqued data, unsurprisingly, tops out at a lower
level, reeting the fat that a ?translation mem-
ory? effet does indeed our to some extent.
Results for English ! Japanese showed the
same trends as English ! Frenh, but were more
pronouned. Table 3 ompares the performane
of the best versions of the SMTs for the two
language-pairs, using both plain and artiially
uniqued data. We see that, with plain data, the
English ! Japanese SMT falls even further short
of repliating the performane of the RBMT than
was the ase for English ! Frenh; BLEU is
only 0.76. The differene between the plain and
uniqued versions is also more extreme. BLEU
(0.64) is onsiderably lower for the version trained
on uniqued data, suggesting that the SMT for this
language pair is nding it harder to generalise,
and is in effet loser to funtioning as a trans-
lation memory. This is onrmed by ounting
the sentenes in test data and not in training data
whih were translated similarly by the SMT and
the RBMT; we nd that the gure tops out at the
very low value of 26%.
As noted in our disussion of the experimental
framework, the non-standard BLEU sores only
address the question of whether the performane
of the SMT and RBMT systems is the same. It is
58
Training data Test data BLEU
English ! Frenh
Generated Generated 0.90
Gen/uniqued Gen/uniqued 0.85
English ! Japanese
Generated Generated 0.76
Gen/uniqued Gen/uniqued 0.64
Table 3: Translation performane, in terms of non-
standard BLEU metri, for different ongura-
tions, training on all available data of the spe-
ied type. ?Generated? = data randomly gener-
ated; ?Gen/uniqued? = data randomly generated,
then uniqued so that dupliates are removed and
test and training pairs do not overlap.
neessary to establish what the differenes mean
in terms of human judgements. We onsequently
turn to evaluation of the pairs for whih the SMT
and the RBMT systems produed different trans-
lation results.
Table 4 shows the ategorisation, aording to
the riteria outlined at the end of Setion 3, for 500
English ! Frenh pairs randomly seleted from
the set of examples where RBMT and SMT gave
different results; we asked three judges to evalu-
ate them independently, and ombined their judg-
ments by majority deision where appropriate. We
observed a very heavy bias towards the RBMT,
with unanimous agreement among the judges that
the RBMT translation was better in 201/500 ases,
and 2-1 agreement in a further 127. In ontrast,
there were only 4/500 ases where the judges
unanimously thought that the SMT translation was
preferable, with a further 12 supported by a ma-
jority deision. The rest of the table gives the
ases where the RBMT and SMT translations were
judged the same or ases in whih the judges dis-
agreed; there were only 41/500 ases where no
majority deision was reahed. Our overall on-
lusion is that we are justied in evaluating the
SMT by using the BLEU sores with the RBMT as
the referene. Of the ases where the two systems
differ, only a tiny fration, at most 16/500, indi-
ate a better translation from the SMT, and well
over half are translated better by the RBMT. Ta-
ble 5 presents typial examples of bad SMT trans-
lations in the English ! Frenh pair, ontrasted
with the translations produed by the RBMT. The
rst two are grammatial errors (a superuous ex-
tra verb in the rst, and agreement errors in the
seond). The third is an bad hoie of tense and
preposition; although grammatial, the target lan-
guage sentene fails to preserve the meaning, and,
rather than referring to a 20 day period ending
now, instead refers to a 20 day period some time
in the past.
Result Agreement Count
RBMT better all judges 201
RBMT better majority 127
SMT better all judges 4
SMT better majority 12
Similar all judges 34
Similar majority 81
Unlear disagree 41
Total 500
Table 4: Comparison of RBMT and SMT perfor-
mane on 500 randomly hosen English ! Frenh
translation examples, evaluated independently by
three judges.
Table 6 shows a similar evaluation for the En-
glish ! Japanese. Here, the differene between
the SMT and RBMT versions was so pronouned
that we felt justied in taking a smaller sample, of
only 150 sentenes. This time, 92/150 ases were
unanimously judged as having a better RBMT
translation, and there was not a single ase where
even a majority found that the SMT was better.
Agreement was good here too, with only 8/150
ases not yielding at least a majority deision.
Result Agreement Count
RBMT better all judges 92
RBMT better majority 32
SMT better all judges 0
SMT better majority 0
Similar all judges 2
Similar majority 16
Unlear disagree 8
Total 150
Table 6: Comparison of RBMT and SMT per-
formane on 150 randomly hosen English !
Japanese translation examples, evaluated indepen-
dently by three judges.
Finally, we look at the performane of the SMT
on material whih the RBMT is not able to trans-
late. This would seem to be a situation where
59
English does a temperature hange ause the headahe
RBMT Frenh vos maux de t?ete sont-ils ause?s par des hangements de tempe?rature
(your headahes are-they aused by hanges of temperature)
SMT Frenh avez-vous vos maux de t?ete sont-ils ause?s par des hangements de tempe?rature
(have-you your headahes are-they aused by hanges of temperature)
English are headahes relieved in the afternoon
RBMT Frenh vos maux de t?ete diminuent-ils l'apres-midi
(your headahes (MASC-PLUR) derease-MASC-PLUR the afternoon)
SMT Frenh vos maux de t?ete diminue-t-elle l'apres-midi
(your headahes (MASC-PLUR) derease-FEM-SING the afternoon)
English have you had them for twenty days
RBMT Frenh avez-vous vos maux de t?ete depuis vingt jours
(have-you your headahes sine twenty days)
SMT Frenh avez-vous eu vos maux de t?ete pendant vingt jours
(have-you had your headahes during twenty days)
Table 5: Examples of inorret SMT translations from English into Frenh. Errors are highlighted in
bold.
the SMT ould have an advantage; robustness is
generally a strength of statistial approahes. We
return to English ! Frenh in Table 7, whih
presents the result of running the best SMT model
on the 357 examples from the test set in (Rayner
et al, 2005) whih failed to be translated by the
RBMT. We divide the set into ategories based on
the reason for failure of the RBMT.
In the most populous group, translations that
failed due to out of voabulary items, the SMT
was, more or less by onstrution, also unable
to produe a translation. For the 110 items that
were out of grammar overage for the RBMT, the
SMT produed 38 good translations, and another 4
borderline translations. There were 50 items that
were within the soure grammar overage of the
RBMT, but failed somewhere in transfer and gen-
eration proessing. Of those, the majority (32)
represented ?bad? soure sentenes, onsidered as
ill-formed for the purposes of this experiment. Out
of the remaining items that were within RBMT
grammar overage, the SMT managed to produe
5 good translations and 1 borderline translation. In
total, on the most lenient interpretation, the SMT
produed 48 additional translations out of 357.
While this improvement in reall is arguably worth
having, it would ome at the prie of a substantial
deline in preision.
5 Disussion and Conlusions
We have presented a novel methodology for om-
paring RBMT and SMT, and tested it on a spe-
Result Count
Out of voabulary
Bad translation 187
Out of soure grammar overage
Good translation 38
Bad translation 44
Borderline translation 4
Bad soure sentene 34
In soure grammar overage
Good translation 5
Bad translation 12
Borderline translation 1
Bad soure sentene 32
Total 357
Table 7: English ! Frenh SMT performane on
examples from the test set whih failed to be trans-
lated by the RBMT, evaluated by one judge.
i pair of RBMT and SMT arhitetures. Our
laim is that these results show that the version
of SMT used here is not in fat apable of repro-
duing the output of the RBMT system. Although
there has been some interest in attempting to train
SMT systems from RBMT output, the evaluation
issues that arise when omparing SMT and RBMT
versions of a high-preision limited-domain sys-
tem are different from those arising in most MT
tasks, and neessitate a orrespondingly different
methodology. It is easy to gain the impression that
it is unsound, and that the experiment has been set
60
up in suh a way that only one result is possible.
This is not, in fat, true.
When we have disussed the methodology with
people who work primarily with SMT, we have
heard two main objetions. The rst is that the
SMT is being trained on RBMT output, and hene
an only be worse; a ommon suggestion is that
a system trained on human-produed translations
ould yield better results. It is not at all implau-
sible that an SMT trained on this kind of data
might perform better on material whih is outside
the overage of the RBMT system. In this do-
main, however, the important issue is preision,
not reall; what is ritial is the ability to trans-
late aurately on material that is within the on-
strained language dened by the RBMT overage.
The RBMT engine gives very good performane
on in-overage data, as has been shown in other
evaluations of the MedSLT system, e.g. (Rayner et
al., 2005); over 97% of all in-overage sentenes
are orretly translated. Human-generated transla-
tions would often, no doubt, be more natural than
those produed by the RBMT, and there would be
slightly fewer outright mistranslations. But the
primary reason why the SMT is doing badly is
not that the training material ontains bad trans-
lations, but rather that the SMT is inapable of
orretly reproduing the translations it sees in the
training data. Even in the easy English ! Frenh
language-pair, the SMT often produes a different
translation from the RBMT. It ould a priori have
been oneivable that the differenes were unin-
teresting, in the sense that SMT outputs different
from RBMT outputs were as good, or even better.
In fat, Table 4 show that this is not true; when the
two translations differ, although the SMT transla-
tion an oasionally be better, it is usually worse.
Table 6 shows that this problem is onsiderably
more aute in English ! Japanese. Thus the
SMT system's inability to model the RBMT sys-
tem points to a real limitation.
If the SMT had instead been trained on human-
generated data, its performane on in-overage
material ould only have improved substantially if
the SMT for some reason found it easier to learn to
reprodue patterns in human-generated data than
in RBMT-generated data. This seems unlikely.
The SMT is being trained from a set of translation
pairs whih are guaranteed to be ompletely on-
sistent, sine they have been automatially gener-
ated by the RBMT; the fat that the RBMT system
only has a small voabulary should also work in
its favour. If the SMT is unable to reprodue the
RBMT's output, it is reasonable to assume it will
have even greater difulty reproduing transla-
tions present in normal human-generated training
data, whih is always far from onsistent, and will
have a larger voabulary.
The seond objetion we have heard is that the
non-standard BLEU sores whih we have used to
measure performane use the RBMT translations
as a referene. People are quik to point out that,
if real human translations were sored in this way,
they would do less well on the non-standard met-
ris than the RBMT translations. This is, indeed,
absolutely true, and explains why it was essential
to arry out the omparison judging shown in Ta-
bles 4 and 6. If we had ompared human transla-
tions with RBMT translations in the same way, we
would have found that human translations whih
differed from RBMT translations were sometimes
better, and hardly ever worse. This would have
shown that the non-standard metris were inap-
propriate for the task of evaluating human trans-
lations. In the atual ase onsidered in this paper,
we nd a ompletely different pattern: the differ-
enes are one-sided in the opposite diretion, in-
diating that the non-standard metris do in fat
agree with human judgements here.
A general objetion to all these experiments is
that there may be more powerful SMT arhite-
tures. We used the Giza++/Moses/SRILM om-
binination beause it is the de fato standard. We
have posted the data we used at http://www.
bahr.net/geaf2009; this will allow other
groups to experiment with alternate arhitetures,
and determine whether they do in fat yield sig-
niant improvements. For the moment, however,
we think it is reasonable to laim that, in domains
where high auray is required, it remains to be
shown that SMT approahes are apable of ahiev-
ing the levels of performane that rule-based sys-
tems an deliver.
61
Referenes
D. Arnold, L. Balkan, S. Meijer, R.L. Humphreys, and
L. Sadler. 1994. Mahine Translation: An Introdu-
tory Guide. Blakwell, Oxford.
P. Bouillon, M. Rayner, N. Chatzihrisas, B.A.
Hokey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generi multi-
lingual open soure platform for limited-domain
medial speeh translation. In Proeedings of the
10th Conferene of the European Assoiation for
Mahine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
P. Bouillon, F. Ehsani, R. Frederking, and M. Rayner,
editors. 2006. Proeedings of the HLT-NAACL In-
ternational Workshop on Medial Speeh Transla-
tion, New York.
P. Bouillon, F. Ehsani, R. Frederking, M. MTear,
and M. Rayner, editors. 2008a. Proeedings of
the COLING Workshop on Speeh Proessing for
Safety Critial Translation and Pervasive Applia-
tions, Manhester.
P. Bouillon, G. Flores, M. Georgesul, S. Halimi,
B.A. Hokey, H. Isahara, K. Kanzaki, Y. Nakao,
M. Rayner, M. Santaholma, M. Starlander, and
N. Tsourakis. 2008b. Many-to-many multilingual
medial speeh translation on a PDA. In Proeed-
ings of The Eighth Conferene of the Assoiation
for Mahine Translation in the Amerias, Waikiki,
Hawaii.
N. Chatzihrisas, P. Bouillon, M. Rayner, M. San-
taholma, M. Starlander, and B.A. Hokey. 2006.
Evaluating task performane for a unidiretional
ontrolled language medial speeh translation sys-
tem. In Proeedings of the HLT-NAACL Interna-
tional Workshop on Medial Speeh Translation,
pages 9?16, New York.
L. Dugast, J. Senellart, and P. Koehn. 2008. Can we
relearn an RBMT system? In Proeedings of the
Third Workshop on Statistial Mahine Translation,
pages 175?178, Columbus, Ohio.
B.A. Hokey, M. Rayner, and G. Christian. 2008.
Training statistial language models from grammar-
generated data: A omparative ase-study. In Pro-
eedings of the 6th International Conferene on Nat-
ural Language Proessing, Gothenburg, Sweden.
R. Jonson. 2005. Generating statistial language mod-
els from interpretation grammars in dialogue sys-
tems. In Proeedings of the 11th EACL, Trento,
Italy.
A. Jurafsky, C. Wooters, J. Segal, A. Stolke, E. Fos-
ler, G. Tajhman, and N. Morgan. 1995. Us-
ing a stohasti ontext-free grammar as a language
model for speeh reognition. In Proeedings of
the IEEE International Conferene on Aoustis,
Speeh and Signal Proessing, pages 189?192.
P. Koehn, H. Hoang, A. Birh, C. Callison-Burh,
M. Federio, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open soure
toolkit for statistial mahine translation. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 2.
F.J. Oh and H. Ney. 2000. Improved statistial align-
ment models. In Proeedings of the 38th Annual
Meeting of the Assoiation for Computational Lin-
guistis, Hong Kong.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
BLEU: a method for automati evaluation of ma-
hine translation. Researh Report, Computer Si-
ene RC22176 (W0109-022), IBM Researh Divi-
sion, T.J.Watson Researh Center.
M. Rayner, P. Bouillon, N. Chatzihrisas, B.A.
Hokey, M. Santaholma, M. Starlander, H. Isahara,
K. Kanzaki, and Y. Nakao. 2005. A methodol-
ogy for omparing grammar-based and robust ap-
proahes to speeh understanding. In Proeedings
of the 9th International Conferene on Spoken Lan-
guage Proessing (ICSLP), pages 1103?1107, Lis-
boa, Portugal.
M. Rayner, B.A. Hokey, and P. Bouillon. 2006.
Putting Linguistis into Speeh Reognition: The
Regulus Grammar Compiler. CSLI Press, Chiago.
M. Rayner, P. Bouillon, G. Flores, F. Ehsani, M. Star-
lander, B. A. Hokey, J. Brotanek, and L. Biewald.
2008. A small-voabulary shared task for medial
speeh translation. In Proeedings of the COLING
Workshop on Speeh Proessing for Safety Criti-
al Translation and Pervasive Appliations, Manh-
ester.
S. Seneff, C. Wang, and J. Lee. 2006. Combining lin-
guisti and statistial methods for bi-diretional En-
glish Chinese translation in the ight domain. In
Proeedings of AMTA 2006.
A. Stolke. 2002. SRILM - an extensible language
modeling toolkit. In Seventh International Confer-
ene on Spoken Language Proessing. ISCA.
Y. Wilks. 2007. Stone soup and the Frenh room. In
K. Ahmad, C. Brewster, and M. Stevenson, editors,
Words and Intelligene I: Seleted Papers by Yorik
Wilks, pages 255?265.
62
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 32?35
Manchester, August 2008
The 2008 MedSLT System
Manny Rayner1, Pierrette Bouillon1, Jane Brotanek2, Glenn Flores2
Sonia Halimi1, Beth Ann Hockey3, Hitoshi Isahara4, Kyoko Kanzaki4
Elisabeth Kron5, Yukie Nakao6, Marianne Santaholma1
Marianne Starlander1, Nikos Tsourakis1
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
{Emmanuel.Rayner,Pierrette.Bouillon,Nikolaos.Tsourakis}@issco.unige.ch
{Sonia.Halimi,Marianne.Santaholma,Marianne.Starlander}@eti.unige.ch
2 UT Southwestern Medical Center, Children?s Medical Center of Dallas
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
3 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
4 NICT, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
{isahara,kanzaki}@nict.go.jp
5 3 St Margarets Road, Cambridge CB3 0LT, England
elisabethkron@yahoo.co.uk
6 University of Nantes, LINA, 2, rue de la Houssinie`re, BP 92208 44322 Nantes Cedex 03
yukie.nakao@univ-nantes.fr
Abstract
MedSLT is a grammar-based medical
speech translation system intended for
use in doctor-patient diagnosis dialogues,
which provides coverage of several dif-
ferent subdomains and multiple language
pairs. Vocabulary ranges from about 350 to
1000 surface words, depending on the lan-
guage and subdomain. We will demo three
different versions of the system: an any-
to-any multilingual version involving the
languages Japanese, English, French and
Arabic, a bidirectional English ? Span-
ish version, and a mobile version run-
ning on a hand-held PDA. We will also
demo the Regulus development environ-
ment, focussing on features which sup-
port rapid prototyping of grammar-based
speech translation systems.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1 Introduction
MedSLT is a medium-vocabulary grammar-based
medical speech translation system built on top of
the Regulus platform (Rayner et al, 2006). It is
intended for use in doctor-patient diagnosis dia-
logues, and provides coverage of several subdo-
mains and a large number of different language-
pairs. Coverage is based on standard examina-
tion questions obtained from physicians, and fo-
cusses primarily on yes/no questions, though there
is also support for WH-questions and elliptical ut-
terances.
Detailed descriptions of MedSLT can be found
in earlier papers (Bouillon et al, 2005; Bouil-
lon et al, 2008)1. In the rest of this note, we
will briefly sketch several versions of the system
that we intend to demo at the workshop, each of
which displays new features developed over the
last year. Section 2 describes an any-language-to-
any-language multilingual version of the system;
Section 3, a bidirectional English ? Spanish ver-
sion; Section 4, a version running on a mobile PDA
1All MedSLT publications are available on-line
at http://www.issco.unige.ch/projects/
medslt/publications.shtml.
32
platform; and Section 5, the Regulus development
environment.
2 A multilingual version
During the last few months, we have reorganised
the MedSLT translation model in several ways2. In
particular, we give a much more central role to the
interlingua; we now treat this as a language in its
own right, defined by a normal Regulus grammar,
and using a syntax which essentially amounts to
a greatly simplified form of English. Making the
interlingua into another language has made it easy
to enforce tight constraints on well-formedness of
interlingual semantic expressions, since checking
well-formedness now just amounts to performing
generation using the interlingua grammar.
Another major advantage of the scheme is that
it is also possible to systematise multilingual de-
velopment, and only work with translation from
source language to interlingua, and from interlin-
gua to target language; here, the important point
is that the human-readable interlingua surface syn-
tax makes it feasible in practice to evaluate transla-
tion between normal languages and the interlingua.
Development of rules for translation to interlingua
is based on appropriate corpora for each source
language. Development of rules for translating
from interlingua uses a corpus which is formed by
merging together the results of translating each of
the individual source-language corpora into inter-
lingua.
We will demonstrate our new capabilities in
interlingua-based translation, using a version of
the system which translates doctor questions in the
headache domain from any language to any lan-
guage in the set {English, French, Japanese, Ara-
bic}. Table 1 gives examples of the coverage of the
English-input headache-domain version, and Ta-
ble 2 summarises recognition performance in this
domain for the three input languages where we
have so far performed serious evaluations. Differ-
ences in the sizes of the recognition vocabularies
are primarily due to differences in use of inflec-
tion.
3 A bidirectional version
The system from the preceding section is unidi-
rectional; all communication is in the doctor-to-
patient direction, the expectation being that the pa-
2The ideas in the section are described at greater length in
(Bouillon et al, 2008).
Language Vocab WER SemER
English 447 6% 11%
French 1025 8% 10%
Japanese 422 3% 4%
Table 2: Recognition performance for English,
French and Japanese headache-domain recognis-
ers. ?Vocab? = number of surface words in source
language recogniser vocabulary; ?WER? = Word
Error Rate for source language recogniser, on in-
coverage material; ?SemER? = semantic error rate
for source language recogniser, on in-coverage
material.
tient will respond non-verbally. Our second demo,
an early version of which is described in (Bouillon
et al, 2007), supports bidirectional translation for
the sore throat domain, in the English ? Spanish
pair. Here, the English-speaking doctor typically
asks WH-questions, and the Spanish-speaking pa-
tient responds with elliptical utterances, which are
translated as full sentence responses. A short ex-
ample dialogue is shown in Table 3.
Doctor: Where is the pain?
?Do?nde le duele?
Patient: En la garganta.
I experience the pain in my throat.
Doctor: How long have you had a pain
in your throat?
?Desde cua?ndo le duele la garganta?
Patient: Ma?s de tres d??as.
I have experienced the pain in my
throat for more than three days.
Table 3: Short dialogue with bidirectional English
? Spanish version. System translations are in ital-
ics.
4 A mobile platform version
When we have shown MedSLT to medical profes-
sionals, one of the most common complaints has
been that a laptop is not an ideal platform for use
in emergency medical situations. Our third demo
shows an experimental version of the system us-
ing a client/server architecture. The client, which
contains the user interface, runs on a Nokia Linux
N800 Internet Tablet; most of the heavy process-
ing, including in particular speech recognition, is
hosted on the remote server, with the nodes com-
municating over a wireless network. A picture of
33
Where? Is the pain above your eye?
When? Have you had the pain for more than a month?
How long? Does the pain typically last a few minutes?
How often? Do you get headaches several times a week?
How? Is it a stabbing pain?
Associated symptoms? Do you vomit when you get the headaches?
Why? Does bright light make the pain worse?
What helps? Does sleep make the pain better?
Background? Do you have a history of sinus disease?
Table 1: Examples of English MedSLT coverage
the tablet, showing the user interface, is presented
in Figure 1. The sentences appearing under the
back-translation at the top are produced by an on-
line help component, and are intended to guide the
user into the grammar?s coverage (Chatzichrisafis
et al, 2006).
The architecture is described further in
(Tsourakis et al, 2008), which also gives perfor-
mance results for another Regulus applications.
These strongly suggest that recognition perfor-
mance in the client/server environment is no
worse than on a laptop, as long as a comparable
microphone is used.
5 The development environment
Our final demo highlights the new Regulus devel-
opment environment (Kron et al, 2007), which has
over the last few months acquired a large amount
of new functionality designed to facilitate rapid
prototyping of spoken language applications3 . The
developer initially constructs and debugs her com-
ponents (grammar, translation rules etc) in a text
view. As soon as they are consistent, she is able
to compile the source-language grammar into a
recogniser, and combine this with other compo-
nents to run a complete speech translation system
within the development environment. Connections
between components are defined by a simple con-
fig file. Figure 2 shows an example.
References
Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
3This work is presented in a paper currently under review.
Machine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
Bouillon, P., G. Flores, M. Starlander,
N. Chatzichrisafis, M. Santaholma, N. Tsourakis,
M. Rayner, and B.A. Hockey. 2007. A bidirectional
grammar-based medical speech translator. In Pro-
ceedings of the ACL Workshop on Grammar-based
Approaches to Spoken Language Processing, pages
41?48, Prague, Czech Republic.
Bouillon, P., S. Halimi, Y. Nakao, K. Kanzaki, H. Isa-
hara, N. Tsourakis, M. Starlander, B.A. Hockey, and
M. Rayner. 2008. Developing non-european trans-
lation pairs in a medium-vocabulary medical speech
translation system. In Proceedings of LREC 2008,
Marrakesh, Morocco.
Chatzichrisafis, N., P. Bouillon, M. Rayner, M. Santa-
holma, M. Starlander, and B.A. Hockey. 2006. Eval-
uating task performance for a unidirectional con-
trolled language medical speech translation system.
In Proceedings of the HLT-NAACL International
Workshop on Medical Speech Translation, pages 9?
16, New York.
Kron, E., M. Rayner, P. Bouillon, and M. Santa-
holma. 2007. A development environment for build-
ing grammar-based speech-enabled applications. In
Proceedings of the ACL Workshop on Grammar-
based Approaches to Spoken Language Processing,
pages 49?52, Prague, Czech Republic.
Rayner, M., B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
Tsourakis, N., M. Georghescul, P. Bouillon, and
M. Rayner. 2008. Building mobile spoken dialogue
applications using regulus. In Proceedings of LREC
2008, Marrakesh, Morocco.
34
Figure 1: Mobile version of the MedSLT system, running on a Nokia tablet.
Figure 2: Speech to speech translation from the development environment, using a Japanese to Arabic
translator built from MedSLT components. The user presses the Recognise button (top right), speaks in
Japanese, and receives a spoken translation in Arabic together with screen display of various processing
results. The application is defined by a config file which combines a Japanese recogniser and analy-
sis grammar, Japanese to Interlingua and Interlingua to Arabic translation rules, an Arabic generation
grammar, and recorded Arabic wavfiles used to construct a spoken result.
35
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 60?63
Manchester, August 2008
A Small-Vocabulary Shared Task for Medical Speech Translation
Manny Rayner1, Pierrette Bouillon1, Glenn Flores2, Farzad Ehsani3
Marianne Starlander1, Beth Ann Hockey4, Jane Brotanek2, Lukas Biewald5
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
{Emmanuel.Rayner,Pierrette.Bouillon}@issco.unige.ch
Marianne.Starlander@eti.unige.ch
2 UT Southwestern Medical Center, Children?s Medical Center of Dallas
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
3 Fluential, Inc, 1153 Bordeaux Drive, Suite 211, Sunnyvale, CA 94089, USA
farzad@fluentialinc.com
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
5 Dolores Labs
lukeab@gmail.com
Abstract
We outline a possible small-vocabulary
shared task for the emerging medical
speech translation community. Data would
consist of about 2000 recorded and tran-
scribed utterances collected during an eval-
uation of an English ? Spanish version
of the Open Source MedSLT system; the
vocabulary covered consisted of about 450
words in English, and 250 in Spanish. The
key problem in defining the task is to agree
on a scoring system which is acceptable
both to medical professionals and to the
speech and language community. We sug-
gest a framework for defining and admin-
istering a scoring system of this kind.
1 Introduction
In computer science research, a ?shared task? is a
competition between interested teams, where the
goal is to achieve as good performance as possible
on a well-defined problem that everyone agrees to
work on. The shared task has three main compo-
nents: training data, test data, and an evaluation
metric. Both test and training data are divided
up into sets of items, which are to be processed.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
The evaluation metric defines a score for each pro-
cessed item. Competitors are first given the train-
ing data, which they use to construct and/or train
their systems. They are then evaluated on the test
data, which they have not previously seen.
In many areas of speech and language process-
ing, agreement on a shared task has been a major
step forward. Often, it has in effect created a new
subfield, since it allows objective comparison of
results between different groups. For example, it
is very common at speech conference to have spe-
cial sessions devoted to recognition within a par-
ticular shared task database. In fact, a conference
without at least a couple of such sessions would
be an anomaly. A recent success story in language
processing is the Recognizing Textual Entailment
(RTE) task1. Since its inception in 2004, this has
become extremely popular; the yearly RTE work-
shop now attracts around 40 submissions, and error
rates on the task have more than halved.
Automatic medical speech translation would
clearly benefit from a shared task. As was made
apparent at the initial 2006 workshop in New
York2, nearly every group has both a unique ar-
chitecture and a unique set of data, essentially
making comparisons impossible. In this note, we
will suggest an initial small-vocabulary medical
1http://www.pascal-network.org/
Challenges/RTE/
2http://www.issco.unige.ch/pub/
SLT workshop proceedings book.pdf
60
shared task. The aspect of the task that is hard-
est to define is the evaluation metric, since there
unfortunately appears to be considerable tension
between the preferences of medical professionals
and speech system implementers. Medical profes-
sionals would prefer to carry out a ?deep? evalu-
ation, in terms of possible clinical consequences
following from a mistranslation. System evalua-
tors will on the other hand prefer an evaluation
method that can be carried out quickly, enabling
frequent evaluations of evolving systems. The plan
we will sketch out is intended to be a compromise
between these two opposing positions.
The rest of the note is organised as follows.
Section 2 describes the data we propose to use,
and Section 3 discusses our approach to evaluation
metrics. Section 4 concludes.
2 Data
The data we would use in the task is for the English
? Spanish language pair, and was collected us-
ing two different versions of the MedSLT system3.
In each case, the scenario imagines an English-
speaking doctor conducting a verbal examination
of a Spanish-speaking patient, who was assumed
to be have visited the doctor because they were
displaying symptoms which included a sore throat.
The doctor?s task was to use the translation sys-
tem to determine the likely reason for the patient?s
symptoms.
The two versions of the system differed in
terms of the linguistic coverage offered. The
more restricted version supported a minimal range
of English questions (vocabulary size, about 200
words), and only allowed the patient to respond
using short phrases (vocabulary size, 100 words).
Thus for example the doctor could ask ?How long
have you had a sore throat??, and the patient would
respond Hace dos d??as (?for two days?). The
less restricted version supported a broader range
of doctor questions (vocabulary size, about 450
words), and allowed the patient to respond using
both short phrases and complete sentences (vocab-
ulary size, about 225 words). Thus in response
to ?How long have you had a sore throat??, the
patient could say either Hace dos d??as (?for two
days?) or Tengo dolor en la garganta hace dos d??as
(?I have had a sore throat for two days?).
Data was collected in 64 sessions, carried out
3http://www.issco.unige.ch/projects/
medslt/
over two days in February 2008 at the University
of Texas Medical Center, Dallas. In each session,
the part of the ?doctor? was played by a real physi-
cian, and the part of the ?patient? by a Spanish-
speaking interpreter. This resulted in 1005 En-
glish utterances, and 967 Spanish utterances. All
speech data is available in SPHERE-headed form,
and totals about 90 MB. A master file, organised in
spreadsheet form, lists metadata for each recorded
file. This includes a transcription, a possible valid
translation (verified by a bilingual translator), IDs
for the ?doctor?, the ?patient?, the session and the
system version, and the preceding context. Con-
text is primarily required for short answers, and
consists of the most recent preceding doctor ques-
tion.
3 Evaluation metrics
The job of the evaluation component in the shared
task is to assign a score to each translated utter-
ance. Our basic model will be the usual one for
shared tasks in speech and language. Each pro-
cessed utterance will be assigned to a category;
each category will be associated with a specified
score; the score for a complete testset will the sum
of the scores for all of its utterances. We thus have
three sub-problems: deciding what the categories
are, deciding how to assign a category to a pro-
cessing utterance, and deciding what scores to as-
sociate with each category.
3.1 Defining categories
If the system attempts to translate an utterance,
there are a priori three things that can happen:
it can produce a correct translation, an incorrect
translation, or no translation. Medical speech
translation is a safety-critical problem; a mistrans-
lation may have serious consequences, up to and
including the death of the patient. This implies
that the negative score for an incorrect translation
should be high in comparison to the positive score
for a correct translation. So a naive scoring func-
tion might be ?1 point for a correct translation, 0
points for no translation, ?1000 points for an in-
correct translation.?
However, since the high negative score for a
mistranslation is justified by the possible serious
consequences, not all mistranslations are equal;
some are much more likely than others to result in
clinical consequences. For example, consider the
possible consequences of two different mistrans-
61
lations of the Spanish sentence La penicilina me
da alergias. Ideally, we would like the system to
translate this as ?I am allergic to penicillin?. If it
instead says ?I am allergic to the penicillin?, the
translation is slightly imperfect, but it is hard to see
any important misunderstanding arising as a result.
In contrast, the translation ?I am not allergic to
penicillin?, which might be produced as the result
of a mistake in speech recognition, could have very
serious consequences indeed. (Note in passing that
both errors are single-word insertions). Another
type of result is a nonsensical translation, perhaps
due to an internal system error. For instance, sup-
pose the translation of our sample sentence were
?The allergy penicillin does me?. In this case, it
is not clear what will happen. Most users will
probably dismiss the output as meaningless; a few
might be tempted to try and decipher it, with un-
predictable results.
Examples like these show that it is important for
the scoring metric to differentiate between differ-
ent classes of mistranslations, with the differentia-
tion based on possible clinical consequences of the
error. For similar reasons, it is important to think
about the clinical consequences when the system
produces correct translations, or fails to produce
a translation. For example, when the system cor-
rectly translates ?Hello? as Buenas d??as, there are
not likely to be any clinical consequences, so it is
reasonable to reward it with a lower score than the
one assigned to a clinically contentful utterance.
When no translation is produced, it also seems cor-
rect to distinguish the case where the user was able
recover by a suitably rephrasing the utterance from
the one where they simply gave up. For example,
if the system failed to translate ?How long has this
cough been troubling you??, but correctly handled
the simpler formulation ?How long have you had a
cough??, we would give this a small positive score,
rather than a simple zero.
Summarising, we propose to classify transla-
tions into the following seven categories:
1. Perfect translation, useful clinical conse-
quences.
2. Perfect translation, no useful clinical conse-
quences.
3. Imperfect translation, but not dangerous in
terms of clinical consequences.
4. Imperfect translation, potentially dangerous.
5. Nonsense.
6. No translation produced, but later rephrased
in a way the system handled adequately.
7. No translation produced, but not rephrased in
a way the system handled adequately.
3.2 Assigning utterances to categories
At the moment, medical professionals will only
accept the validity of category assignments made
by trained physicians. In the worst case, it is
clearly true that a layman, even one who has re-
ceived some training, will not be able to determine
whether or not a mistranslation has clinical signif-
icance.
Physician time is, however, a scarce and valu-
able resource, and, as usual, typical case and worst
case may be very different. Particularly for routine
testing during system development, it is clearly not
possible to rely on expert physician assessments.
We consequently suggest a compromise strategy.
We will first carry out an evaluation using medical
experts, in order to establish a gold standard. We
will then repeat this evaluation using non-experts,
and determine how large the differential is in prac-
tice.
We initially intend to experiment with two dif-
ferent groups of non-experts. At Geneva Uni-
versity, we will use students from the School of
Translation. These students will be selected for
competence in English and Spanish, and will re-
ceive a few hours of training on determination of
clinical significance in translation, using guide-
lines developed in collaboration with Glenn Flores
and his colleagues at the UT Southwestern Medi-
cal Center, Texas. Given that the corpus material
is simple and sterotypical, we think that this ap-
proach should yield a useful approximation to ex-
pert judgements.
Although translation students are far cheaper
than doctors, they are still quite expensive, and
evaluation turn-around will be slow. For these rea-
sons, we also propose to investigate the idea of per-
forming evaluations using Amazon?s Mechanical
Turk4. This will be done by Dolores Labs, a new
startup specialising in Turk-based crowdsourcing.
3.3 Scores for categories
We have not yet agreed on exact scores for the
different categories, and this is something that is
4http://www.mturk.com/mturk/welcome
62
probably best decided after mutual discussion at
the workshop. Some basic principles will be evi-
dent from the preceding discussion. The scale will
be normalised so that failure to produce a trans-
lation is counted as zero; potentially dangerous
mistranslations will be associated with a negative
score large in comparison to the positive score for
a useful correct translation. Inability to communi-
cate can certainly be dangerous (this is the point of
having a translation system in the first place), but
mistakenly believing that one has communicated
is usually much worse. As Mark Twain put it: ?It
ain?t what you don?t know that gets you into trou-
ble. It?s what you know for sure that just ain?t so?.
3.4 Discarding uncertain responses
Given that both speech recognition and machine
translation are uncertain technologies, a high
penalty for mistranslations means that systems
which attempt to translate everything may eas-
ily end up with an average negative score - in
other words, they would score worse than a system
which did nothing! For the shared task to be in-
teresting, we must address this problem, and in the
doctor to patient direction there is a natural way
to do so. Since the doctor can reasonably be as-
sumed to be a trained professional who has had
time to learn to operate the system, we can say that
he has the option of aborting any translation where
the machine does not appear to have understood
correctly.
We thus relativise the task with respect to a ?fil-
ter?: for each utterance, we produce both a transla-
tion in the target language, and a ?reference trans-
lation? in the source language, which in some way
gives information about what the machine has un-
derstood. The simplest way to produce this ?ref-
erence translation? is to show the words produced
by speech recognition. When scoring, we evaluate
both translations, and ignore all examples where
the reference translation is evaluated as incorrect.
To go back to the ?penicillin? example, suppose
that Spanish source-language speech recognition
has incorrectly recognised La penicilina me da
alergias as La penicilina no me da alergias. Even
if this produces the seriously incorrect translation
?I am not allergic to penicillin?, we can score it
as a zero rather than a negative, on the grounds
that the speech recognition result already shows
the Spanish-speaking doctor that something has
gone wrong before any translation has happened.
The reference translation may also be produced in
a more elaborate way; a common approach is to
translate back from the target language result into
the source language.
Although the ?filtered? version of the medical
speech translation task makes good sense in the
doctor to patient direction, it is less clear how
meaningful it is in the patient to doctor direction.
Most patients will not have used the system before,
and may be distressed or in pain. It is consequently
less reasonable to expect them to be able to pay at-
tention to the reference translation when using the
system.
4 Summary and conclusions
The preceding notes are intended to form a frame-
work which will serve as a basis for discussion at
the workshop. As already indicated, the key chal-
lenge here is to arrive at metrics which are ac-
ceptable to both the medical and the speech and
language community. This will certainly require
more negotiation. We are however encouraged by
the fact that the proposal, as presented here, has
been developed jointly by representatives of both
communities, and that we appear to be fairly near
agreement. Another important parameter which
we have intentionally left blank is the duration of
the task; we think it will be more productive to de-
termine this based on the schedules of interested
parties.
Realistically, the initial definition of the metric
can hardly be more than a rough guess. Experi-
mentation during the course of the shared task will
probably show that some adjustment will be desir-
able, in order to make it conform more closely to
the requirements of the medical community. If we
do this, we will, in the interests of fairness, score
competing systems using all versions of the metric.
63
