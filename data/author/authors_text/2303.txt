Coling 2008: Companion volume ? Posters and Demonstrations, pages 173?176
Manchester, August 2008
Entailment-based Question Answering  
for Structured Data 
Bogdan Sacaleanu?, Constantin Orasan?, Christian Spurk?, Shiyan 
Ou?, Oscar Ferrandez?, Milen Kouylekov? and Matteo Negri?  
?LT-Lab, DFKI GmbH / Saarbr?cken, Germany 
?RIILP, University of Wolverhampton / Wolverhampton, UK 
?Fondazione Bruno Kessler (FBK) / Trento, Italy 
?University of Alicante / Alicante, Spain 
 
 
Abstract  
This paper describes a Question Answer-
ing system which retrieves answers from 
structured data regarding cinemas and 
movies. The system represents the first 
prototype of a multilingual and multi-
modal QA system for the domain of tour-
ism. Based on specially designed domain 
ontology and using Textual Entailment as 
a means for semantic inference, the sys-
tem can be used in both monolingual and 
cross-language settings with slight ad-
justments for new input languages. 
1 Introduction 
Question Answering over structured data has 
been traditionally addressed through a deep 
analysis of the question in order to reconstruct a 
logical form, which is then translated in the query 
language of the target data (Androutsopoulos et 
al, 1995, Popescu et al 2003). This approach im-
plies a complex mapping between linguistic ob-
jects (e.g. lexical items, syntactic structures) and 
against data objects (e.g. concepts and relations 
in a knowledge base). Unfortunately, such a 
mapping requires extensive manual work, which 
in many cases represents a bottleneck preventing 
the realization of large scale and portable natural 
language interfaces to structured data.  
This paper presents the first prototype of a 
question answering system which can answer 
questions in several languages about movies and 
cinema using a multilingual ontology and textual 
                                                 
 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
entailment. The remainder of the paper is struc-
tured as follows: Section 2 presents the concept 
of entailment-based question answering; Section 
3 describes our prototype which implements this 
concept; A brief evaluation is presented in Sec-
tion 4, followed by conclusions in Section 5. 
2 Entailment-based QA 
Recently Textual Entailment (TE) has been pro-
posed as a unifying framework for applied se-
mantics (Dagan and Glickman, 2004), where the 
need for an explicit representation of a mapping 
between linguistic objects and data objects can 
be, at least partially, bypassed through the defini-
tion of semantic inferences at a textual level. In 
this framework, a text (T) is said to entail a hy-
pothesis (H) if the meaning of H can be derived 
from the meaning of T. 
On the basis of the TE framework, the QA 
problem can be recast as an entailment problem, 
where the text (T) is the question (or its affirma-
tive version) and the hypothesis (H) is a rela-
tional answer pattern, which is associated to in-
structions for retrieving the answer to the input 
question. In this framework, given a question Q 
and a set of relational answer patterns P, a QA 
system needs to select those patterns in P that are 
entailed by Q. Instructions associated to answer 
patterns may be viewed as high precision proce-
dures for answer extraction, which are dependent 
on the specific source which is asked for. In case 
of QA over structured data, instructions could be 
queries to a database; whilst in case of QA on the 
Web, an instruction could be the URL of a Web 
page containing the answer to a question or some 
form of IR query to a search engine. 
Therefore, the underlying idea of an entail-
ment-based QA system is to match the user?s re-
quest to a set of predefined question patterns in 
order to get some kind of analysis for the request. 
173
As an example consider the question ?Where 
can I watch the movie ?Dreamgirls? next Satur-
day?? and the predefined question patterns: 
? Which movies are currently running in 
[CINEMA]?  EAT = [MOVIE] 
? Where can I watch the movie [MOVIE] 
on [WEEKDAY]?  EAT = [CINEMA] 
? Where can I see [MOVIE]? 
 EAT = [CINEMA] 
In the example, each of the patterns contains 
placeholders for relevant named entities and has 
an expected answer type (EAT) associated with 
it. The entailment-based QA system should re-
turn that pattern (2) is entailed by the question 
and as a result the retrieval instructions associ-
ated to it will be used to answer the question. 
3 Description of system 
Our question answering system implements the 
concept of entailment-based question answering 
described in the previous section. The overall 
structure of our system is presented in Figure 1.  
Given a question asked by a user of the system 
in a known location, the QA planner forwards it 
to the Instance Annotator in order to find any 
concepts that might be related to the targeted 
domain (i.e. cinema, city, movie). The result is 
then analyzed by the Relation Matcher, which on 
the basis of entailment can either select the most 
appropriate interpretation of the question and im-
plicitly its associated procedure of answering the 
question, or decide that the user request is out-of-
coverage if no such interpretation is available. 
The cross-linguality of our system and, to a 
certain extent, the interaction between its compo-
nents is ensured by a domain ontology which is 
used for all four languages involved in the pro-
ject: English, German, Italian and Spanish, and 
its modules (Ou et al, 2008). Concepts from the 
ontology are used to annotate the user questions 
as well as data from which the answer is ex-
tracted. In the current stage of the project, the 
answers are contained in databases obtained from 
content provides or built from structured web 
pages. As a result, the information in the database 
tables was annotated with concepts from the on-
tology and then converted into an RDF graph to 
Figure 1. System Architecture 
174
facilitate retrieval using SPARQL query lan-
guage (Prud'hommeaux and Seaborne, 2006). 
Question patterns corresponding to one or several 
ontological relations were produced after ques-
tions for users were collected and used in the en-
tailment module. The question patterns used by 
the system are very similar to those presented in 
the previous section and contain placeholders for 
the actual entities that are expected to appear in a 
question. 
The SPARQL query associated with a pattern 
selected for a user question is used to retrieve the 
answers from the knowledge base and prepare for 
presentation. Given that our system is not limited 
to returning only textual information, further 
processing can be applied to the retrieved data. 
For example, for proximity questions the list of 
answers consists of cinema names and their GPS-
coordinates, which are used by the Answer Sort-
ing component to reorder the list of answers on 
the basis of their distance to the user?s location. 
Besides presenting the possible answers to a 
given question, the system can offer additional 
information based on the answer?s type: 
? a map for answers that are location 
names, 
? a route description for answers that are 
cinema names, 
? a video-trailer for answers that are movie 
names and 
? an image for answers that are person 
names. 
Due to the fact that a common semantics is 
shared by all four languages by way of a domain 
ontology, the system can be used not only in a 
monolingual setting, but also in a cross-language 
setting. This corresponds to a user-scenario 
where a tourist asks for information in their own 
language in a foreign location (i.e. English 
speaker in Italy). The only difference between 
monolingual and cross-language scenarios is that 
in the cross-language setting, the QA Core sub-
system (Figure 1) selects a Find Entailed Rela-
tion component according to the user input?s lan-
guage. This is due to the entailment algorithms 
that tend to use language specific resources in 
order to attain high accuracy results of matching 
the user request with one of the lexicalized rela-
tions (patterns). It is only the entailment compo-
nent that has to be provided in order to adapt the 
system to new input languages, once the lexical-
ized relations have been translated either manual 
or automatically. 
Both the Instance Annotator and the Answer 
Retriever are language independent, but location 
dependent (Figure 2). The Answer Retriever de-
pends on the location since it is querying data 
found at that place (i.e. Italy), while the Instance 
Annotator looks up instances of the data in the 
user?s question (i.e. annotates an English ques-
tion). They are language independent since they 
are working with data abstractions like SPARQL 
queries (Answer Retriever) or work at character 
level and do not consider language specific as-
pects, like words, in their look-up process (In-
stance Annotator). 
The current version of the system1 is designed 
according to the SOA (Service Oriented Archi-
tecture) and is implemented as point-to-point in-
tegrated web services. Any of the system?s com-
ponents can be substituted by alternative imple-
mentations with no need for further changes as 
long as the functionality remains the same. 
                                                 
1
 http://attila.dfki.uni-sb.de:8282/ QallMe_Proto-
type_WEB_Update/faces/Page6.jsp 
Figure 2. Cross-language Setting 
175
4 Evaluation 
A preliminary evaluation of the first prototype 
was carried out on randomly selected questions 
from a benchmark specifically designed for the 
project. This benchmark was developed to con-
tain questions about various aspects from the 
domain of tourism and for this reason we filtered 
out questions not relevant to cinema or movies. 
The evaluation of the system did not assess 
whether it can extract the correct answer. Instead, 
it measured to what extent the system can select 
the right SPARQL pattern. The explanation for 
this can be found in the fact that once a correct 
question pattern is selected, the extraction of the 
answer requires only retrieval of the answer from 
the database. Moreover, it should be pointed out 
that the main purpose of this preliminary evalua-
tion was to test the interaction between compo-
nents and indicate potential problems, and it was 
less about their performances.  
Table 1 summarises the results of the evalua-
tion. The number of questions used in the evalua-
tion is different from one language to another. 
This can be explained by the fact that for each 
language a number of questions (in general 500) 
was randomly selected from the benchmark and 
only the ones which referred to cinema or movies 
were selected. The column Questions indicates 
the number of questions assessed. The Correct 
column indicates for how many questions a cor-
rect SPARQL was generated. The Wrong column 
corresponds to the number of questions where a 
wrong or incomplete SPARQL was generated. 
This number also includes cases where no 
SPARQL was generated due to lack of corre-
sponding answer pattern. 
 
 Questions Correct Wrong 
English 167 74 (44.31%) 93 (55.68%) 
German 214 120 (56.04%) 94 (43.92%) 
Spanish 58 50 (86.20%) 8 (13.79%) 
Italian 99 46 (46.46%) 53 (53.53%) 
Table 1: Evaluation results 
As can be seen, the results are very different 
from one language to another. This can be ex-
plained by the fact that different entailment en-
gines are used for each language. In addition, 
even though the benchmark was built using a 
common set of guidelines, the complexity of 
questions varies from one language to another. 
For this reason, for some questions it is more dif-
ficult to find the correct pattern than for others.  
Analysis of the results revealed that one of the 
easiest ways to improve the performance of the 
system is to increase the number of patterns. Cur-
rently the average number of patterns per lan-
guage is 42. Improvement of the entailment en-
gines is another direction which needs to be pur-
sued. Most of the partners involved in the project 
have more powerful entailment engines than 
those integrated in the prototype which were 
ranked highly in RTE competitions. Unfortu-
nately, many of these engines cannot be used di-
rectly in our system due to their slow speed. Our 
system is supposed to give users results in real 
time which imposes some constraints on the 
amount of processing that can be done. 
5 Conclusions 
This paper presented the first prototype of an 
entailment-based QA system, which can answer 
questions about movies and cinema. The use of a 
domain ontology ensures that the system is cross-
language and can be extended to new languages 
with slight adjustments at the entailment engine. 
The system is implemented as a set of web ser-
vices and along a Service Oriented Architecture. 
6 Acknowledgements 
This work is supported by the EU-funded pro-
ject QALL-ME (FP6 IST-033860).  
References 
Androutsopoulos, I. and G.D. Ritchie and P. Thanisch. 
1995. Natural Language Interfaces to Databases -- 
An Introduction, Journal of Natural Language En-
gineering, vol.1, no.1, Cambridge University Press. 
Popescu Ana-Marie, Oren Etzioni, and Henry Kautz. 
2003. Towards a theory of natural language inter-
faces to databases. In Proceedings of the confer-
ence on Intelligent User Interfaces. 
Dagan Ido and Oren Glickman. 2004. Probabilistic 
textual entailment: Generic applied modeling of 
language variability. In PASCAL Workshop on 
Learning Methods for Text Understanding and 
Mining, Grenoble. 
Ou Shiyan, Viktor Pekar, Constantin Orasan, Chris-
tian Spurk, Matteo Negri. 2008. Development and 
alignment of a domain-specific ontology for ques-
tion answering. In Proceedings of the 6th Edition of 
the Language Resources and Evaluation Confer-
ence (LREC-08).  
Prud'hommeaux Eric, Andy Seaborne (eds.). 2006. 
SPARQL Query Language for RDF. RDF Data Ac-
cess Working Group. 
176
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 73?80,
Rochester, April 2007 c?2007 Association for Computational Linguistics
DLSITE-2: Semantic Similarity Based on Syntactic
Dependency Trees Applied to Textual Entailment
Daniel Micol, O?scar Ferra?ndez, Rafael Mun?oz, and Manuel Palomar
Natural Language Processing and Information Systems Group
Department of Computing Languages and Systems
University of Alicante
San Vicente del Raspeig, Alicante 03690, Spain
{dmicol, ofe, rafael, mpalomar}@dlsi.ua.es
Abstract
In this paper we attempt to deduce tex-
tual entailment based on syntactic depen-
dency trees of a given text-hypothesis pair.
The goals of this project are to provide an
accurate and fast system, which we have
called DLSITE-2, that can be applied in
software systems that require a near-real-
time interaction with the user. To accom-
plish this we use MINIPAR to parse the
phrases and construct their correspond-
ing trees. Later on we apply syntactic-
based techniques to calculate the seman-
tic similarity between text and hypothe-
sis. To measure our method?s precision we
used the test text corpus set from Second
PASCAL Recognising Textual Entailment
Challenge (RTE-2), obtaining an accuracy
rate of 60.75%.
1 Introduction
There are several methods used to determine tex-
tual entailment for a given text-hypothesis pair. The
one described in this paper uses the information
contained in the syntactic dependency trees of such
phrases to deduce whether there is entailment or
not. In addition, semantic knowledge extracted from
WordNet (Miller et al, 1990) has been added to
achieve higher accuracy rates.
It has been proven in several competitions and
other workshops that textual entailment is a complex
task. One of these competitions is PASCAL Recog-
nising Textual Entailment Challenge (Bar-Haim et
al., 2006), where each participating group develops a
textual entailment recognizing system attempting to
accomplish the best accuracy rate of all competitors.
Such complexity is the reason why we use a combi-
nation of various techniques to deduce whether en-
tailment is produced.
Currently there are few research projects related
to the topic discussed in this paper. Some systems
use syntactic tree matching as the textual entailment
decision core module, such as (Katrenko and Adri-
aans, 2006). It is based on maximal embedded syn-
tactic subtrees to analyze the semantic relation be-
tween text and hypothesis. Other systems use syn-
tactic trees as a collaborative module, not being the
core, such as (Herrera et al, 2006). The application
discussed in this paper belongs to the first set of sys-
tems, since syntactic matching is its main module.
The remainder of this paper is structured as fol-
lows. In the second section we will describe the
methods implemented in our system. The third one
contains the experimental results, and the fourth and
last discusses such results and proposes future work
based on our actual research.
2 Methods
The system we have built aims to provide a good
accuracy rate in a short lapse of time, making it
feasible to be included in applications that require
near-real-time responses due to their interaction with
the user. Such a system is composed of few mod-
ules that behave collaboratively. These include tree
construction, filtering, embedded subtree search and
graph node matching. A schematic representation of
the system architecture is shown in Figure 1.
73
Figure 1: DLSITE-2 system architecture.
Each of the steps or modules of DLSITE-2 is de-
scribed in the following subsections, that are num-
bered sequentially according to their execution or-
der.
2.1 Tree generation
The first module constructs the corresponding syn-
tactic dependency trees. For this purpose, MINI-
PAR (Lin, 1998) output is generated and afterwards
parsed for each text and hypothesis of our corpus.
Phrase tokens, along with their grammatical infor-
mation, are stored in an on-memory data structure
that represents a tree, which is equivalent to the men-
tioned syntactic dependency tree.
2.2 Tree filtering
Once the tree has been constructed, we may want
to discard irrelevant data in order to reduce our sys-
tem?s response time and noise. For this purpose we
have generated a database of relevant grammatical
categories, represented in Table 1, that will allow
us to remove from the tree all those tokens whose
category does not belong to such list. The result-
ing tree will have the same structure as the original,
but will not contain any stop words nor irrelevant to-
kens, such as determinants or auxiliary verbs. The
whole list of ignored grammatical categories is rep-
resented in Table 2.
We have performed tests taking into account and
discarding each grammatical category, which has al-
lowed us to generate both lists of relevant and ig-
nored grammatical categories.
Verbs, verbs with one argument, verbs with two ar-
guments, verbs taking clause as complement, verb
Have, verb Be
Nouns
Numbers
Adjectives
Adverbs
Noun-noun modifiers
Table 1: Relevant grammatical categories.
2.3 Graph embedding detection
The next step of our system consists in determining
whether the hypothesis? tree is embedded into the
text?s. Let us first define the concept of embedded
tree (Katrenko and Adriaans, 2006).
Definition 1: Embedded tree A tree
T1 = (V1, E1) is embedded into another
one T2 = (V2, E2) iff
1. V1 ? V2, and
2. E1 ? E2
where V1 and V2 represent the vertices,
and E1 and E2 the edges.
In other words, a tree, T1, is embedded into an-
other one, T2, if all nodes and branches of T1 are
present in T2.
We believe that it makes sense to reduce the strict-
ness of such a definition to allow the appearance
of intermediate nodes in the text?s branches that are
74
Determiners
Pre-determiners
Post-determiners
Clauses
Inflectional phrases
Preposition and preposition phrases
Specifiers of preposition phrases
Auxiliary verbs
Complementizers
Table 2: Ignored grammatical categories.
not present in the corresponding hypothesis? branch,
which means that we allow partial matching. There-
fore, a match between two branches will be pro-
duced if all nodes of the first one, namely ?1 ? E1,
are present in the second, namely ?2 ? E2, and their
respective order is the same, allowing the possibil-
ity of appearance of intermediate nodes that are not
present in both branches. This is also described in
(Katrenko and Adriaans, 2006).
To determine whether the hypothesis? tree is em-
bedded into the text?s, we perform a top-down
matching process. For this purpose we first compare
the roots of both trees. If they coincide, we then pro-
ceed to compare their respective child nodes, which
are the tokens that have some sort of dependency
with their respective root token.
In order to add more flexibility to our system,
we do not require the pair of tokens to be ex-
actly the same, but rather set a threshold that rep-
resents the minimum similarity value between them.
This is a difference between our approach and the
one described in (Katrenko and Adriaans, 2006).
Such a similarity is calculated by using the Word-
Net::Similarity tool (Pedersen et al, 2004), and,
concretely, the Wu-Palmer measure, as defined in
Equation 1 (Wu and Palmer, 1994).
Sim(C1, C2) =
2N3
N1 +N2 + 2N3
(1)
where C1 and C2 are the synsets whose similarity
we want to calculate, C3 is their least common su-
perconcept, N1 is the number of nodes on the path
from C1 to C3, N2 is the number of nodes on the
path from C2 to C3, and N3 is the number of nodes
on the path from C3 to the root. All these synsets
and distances can be observed in Figure 2.
Figure 2: Distance between two synsets.
If the similarity rate is greater or equal than the
established threshold, which we have set empirically
to 80%, we will consider the corresponding hypoth-
esis? token as suitable to have the same meaning
as the text?s token, and will proceed to compare its
child nodes in the hypothesis? tree. On the other
hand, if such similarity value is less than the cor-
responding threshold, we will proceed to compare
the children of such text?s tree node with the actual
hypothesis? node that was being analyzed.
The comparison between the syntactic depen-
dency trees of both text and hypothesis will be com-
pleted when all nodes of either tree have been pro-
cessed. If we have been able to find a match for all
the tokens within the hypothesis, the corresponding
tree will be embedded into the text?s and we will be-
lieve that there is entailment. If not, we will not be
able to assure that such an implication is produced
and will proceed to execute the next module of our
system.
Next, we will present a text-hypothesis pair sam-
ple where the syntactic dependency tree of the hy-
pothesis (Figure 3(b)) is embedded into the text?s
(Figure 3(a)). The mentioned text-hypothesis pair
is the following:
Text: Mossad is one of the world?s most
well-known intelligence agencies, and is
often viewed in the same regard as the CIA
and MI6.
Hypothesis: Mossad is an intelligence
agency.
75
(a) Mossad is one of the world?s most well-known intelligence agencies, and is often viewed
in the same regard as the CIA and MI6.
(b) Mossad is an intelligence
agency.
Figure 3: Representation of a hypothesis? syntactic dependency tree that is embedded into the text?s.
As one can see in Figure 3, the hypothesis? syn-
tactic dependency tree represented is embedded into
the text?s because all of its nodes are present in
the text in the same order. There is one exception
though, that is the word an. However, since it is a
determinant, the filtering module will have deleted
it before the graph embedding test is performed.
Therefore, in this example the entailment would be
recognized.
2.4 Graph node matching
Once the embedded subtree comparison has fin-
ished, and if its result is negative, we proceed to per-
form a graph node matching process, termed align-
ment, between both the text and the hypothesis. This
operation consists in finding pairs of tokens in both
trees whose lemmas are identical, no matter whether
they are in the same position within the tree. We
would like to point out that in this step we do not
use the WordNet::Similarity tool.
Some authors have already designed similar
matching techniques, such as the ones described in
(MacCartney et al, 2006) and (Snow et al, 2006).
However, these include semantic constraints that we
have decided not to consider. The reason of this
decision is that we desired to overcome the textual
entailment recognition from an exclusively syntactic
perspective. Therefore, we did not want this module
to include any kind of semantic knowledge.
The weight given to a token that has been found
in both trees will depend on the depth in the hypoth-
esis? tree and the token?s grammatical relevance.
The first of these factors depends on an empirically-
calculated weight that assigns less importance to a
node the deeper it is located in the tree. This weight
is defined in Equation 2. The second factor gives
different relevance depending on the grammatical
category and relationship. For instance, a verb will
have the highest weight, while an adverb or an ad-
jective will have less relevance. The values assigned
to each grammatical category and relationship are
also empirically-calculated and are shown in Tables
3 and 4, respectively.
Grammatical category Weight
Verbs, verbs with one argument, verbs
with two arguments, verbs taking
clause as complement
1.0
Nouns, numbers 0.75
Be used as a linking verb 0.7
Adjectives, adverbs, noun-noun mod-
ifiers
0.5
Verbs Have and Be 0.3
Table 3: Weights assigned to the grammatical cate-
gories.
76
Grammatical relationship Weight
Subject of verbs, surface subject, ob-
ject of verbs, second object of ditran-
sitive verbs
1.0
The rest 0.5
Table 4: Weights assigned to the grammatical rela-
tionships.
Let ? and ? represent the text?s and hypothesis?
syntactic dependency trees, respectively. We as-
sume we have found members of a synset, namely ?,
present in both ? and ?. Now let ? be the weight as-
signed to ??s grammatical category (defined in Table
3), ? the weight of ??s grammatical relationship (de-
fined in Table 4), ? an empirically-calculated value
that represents the weight difference between tree
levels, and ?? the depth of the node that contains
the synset ? in ?. We define the function ?(?) as
represented in Equation 2.
?(?) = ? ? ? ? ???? (2)
The value obtained by calculating the expression
of Equation 2 would represent the relevance of a
synset in our system. The experiments performed
reveal that the optimal value for ? is 1.1.
For a given pair (? , ?), we define the set ? as the
one that contains the synsets present in both trees:
? = ? ? ? ?? ? ?, ? ? ? (3)
Therefore, the similarity rate between ? and ?, de-
noted by the symbol ?, would be defined as:
?(?, ?) =
?
???
?(?) (4)
One should note that a requirement of our sys-
tem?s similarity measure would be to be independent
of the hypothesis length. Thus, we must define the
normalized similarity rate, as shown in Equation 5.
?(?, ?) =
?(?, ?)
?
???
?(?)
=
?
???
?(?)
?
???
?(?)
(5)
Once the similarity value, ?(?, ?), has been cal-
culated, it will be provided to the user together with
the corresponding text-hypothesis pair identifier. It
will be his responsibility to choose an appropriate
threshold that will represent the minimum similarity
rate to be considered as entailment between text and
hypothesis. All values that are under such a thresh-
old will be marked as not entailed. For this purpose,
we suggest using a development corpus in order to
obtain the optimal threshold value, as it is done in
the RTE challenges.
3 Experimental results
The experimental results shown in this paper were
obtained processing a set of text-hypothesis pairs
from RTE-2. The organizers of this challenge pro-
vide development and test corpora to the partic-
ipants, both of them containing 800 pairs manu-
ally annotated for logical entailment. It is com-
posed of four subsets, each of them correspond-
ing to typical true and false entailments in different
tasks, such as Information Extraction (IE), Informa-
tion Retrieval (IR), Question Answering (QA), and
Multi-document Summarization (SUM). For each
task, the annotators selected the same amount of true
entailments as negative ones (50%-50% split).
The organizers have also defined two measures to
evaluate the participating systems. All judgments
returned by the systems will be compared to those
manually assigned by the human annotators. The
percentage of matching judgments will provide the
accuracy of the system, i.e. the percentage of cor-
rect responses. As a second measure, the average
precision will be computed. This measure evaluates
the ability of the systems to rank all the pairs in the
corpus according to their entailment confidence, in
decreasing order from the most certain entailment to
the least. Average precision is a common evaluation
measure for system rankings that is defined as shown
in Equation 6.
AP =
1
R
n?
i=1
E(i)
#correct up to pair i
i
(6)
where n is the amount of the pairs in the test corpus,
R is the total number of positive pairs in it, i ranges
over the pairs, ordered by their ranking, and E(i) is
defined as follows:
77
E(i) =
?
?
?
1 if the i? th pair is positive,
0 otherwise.
(7)
As we previously mentioned, we tested our sys-
tem against RTE-2 development corpus, and used
the test one to evaluate it.
First, Table 5 shows the accuracy (ACC) and av-
erage precision (AP), both as a percentage, obtained
processing the development corpus from RTE-2 for
a threshold value of 68.9%, which corresponds to
the highest accuracy that can be obtained using our
system for the mentioned corpus. It also provides
the rate of correctly predicted true and false entail-
ments.
Task ACC AP TRUE FALSE
IE 52.00 51.49 54.00 50.00
IR 55.50 58.99 32.00 79.00
QA 57.50 54.72 53.00 62.00
SUM 65.00 81.35 39.00 91.00
Overall 57.50 58.96 44.50 70.50
Table 5: Results obtained for the development cor-
pus.
Next, let us show in Table 6 the results obtained
processing the test corpus, which is the one used
to compare the different systems that participated in
RTE-2, with the same threshold as before.
Task ACC AP TRUE FALSE
IE 50.50 47.33 75.00 26.00
IR 64.50 67.67 59.00 70.00
QA 59.50 58.16 80.00 39.00
SUM 68.50 75.86 49.00 88.00
Overall 60.75 57.91 65.75 55.75
Table 6: Results obtained for the test corpus.
As one can observe in the previous table, our
system provides a high accuracy rate by using
mainly syntactical measures. The number of text-
hypothesis pairs that succeeded the graph embed-
ding evaluation was three for the development cor-
pus and one for the test set, which reflects the strict-
ness of such module. However, we would like to
point out that the amount of pairs affected by the
mentioned module will depend on the corpus na-
ture, so it can vary significantly between different
corpora.
Let us now compare our results with the ones that
were achieved by the systems that participated in
RTE-2. One should note that the criteria for such
ranking is based exclusively on the accuracy, ignor-
ing the average precision value. In addition, each
participating group was allowed to submit two dif-
ferent systems to RTE-2. We will consider here the
best result of both systems for each group. The men-
tioned comparison is shown in Table 7, and contains
only the systems that had higher accuracy rates than
our approach.
Participant Accuracy
(Hickl et al, 2006) 75.38
(Tatu et al, 2006) 73.75
(Zanzotto et al, 2006) 63.88
(Adams, 2006) 62.62
(Bos and Markert, 2006) 61.62
DLSITE-2 60.75
Table 7: Comparison of some of the teams that par-
ticipated in RTE-2.
As it is reflected in Table 7, our system would
have obtained the sixth position out of twenty-four
participants, which is an accomplishment consider-
ing the limited number of resources that it has built-
in.
Since one of our system?s modules is based on
(Katrenko and Adriaans, 2006), we will compare
their results with ours to analyze whether the modi-
fications we introduced perform correctly. In RTE-
2, they obtained an accuracy rate of 59.00% for the
test corpus. The reason why we believe we have
achieved better results than their system is due to
the fact that we added semantic knowledge to our
graph embedding module. In addition, the syntactic
dependency trees to which we have applied such a
module have been previously filtered to ensure that
they do not contain irrelevant words. This reduces
the system?s noise and allows us to achieve higher
accuracy rates.
In the introduction of this paper we mentioned
that one of the goals of our system was to provide
78
a high accuracy rate in a short lapse of time. This is
one of the reasons why we chose to construct a light
system where one of the aspects to minimize was its
response time. Table 8 shows the execution times1
of our system for both development and test text cor-
pora from RTE-2. These include total and average2
response times.
Development Test
Total 1045 1023
Average 1.30625 1.27875
Table 8: DLSITE-2 response times (in seconds).
As we can see, accurate results can be obtained
using syntactic dependency trees in a short lapse of
time. However, there are some limitations that our
system does not avoid. For instance, the tree em-
bedding test is not applicable when there is no verb
entailment. This is reflected in the following pair:
Text: Tony Blair, the British Prime Minis-
ter, met Jacques Chirac in London.
Hypothesis: Tony Blair is the British
Prime Minister.
The root node of the hypothesis? tree would be
the one corresponding to the verb is. Since the en-
tailment here is implicit, there is no need for such a
verb to appear in the text. However, this is not com-
patible with our system, since is would not match
any node of the text?s tree, and thus the hypothesis?
tree would not be found embedded into the text?s.
The graph matching process would not behave
correctly either. This is due to the fact that the main
verb, which has the maximum weight because it is
the root of the hypothesis? tree and its grammatical
category has the maximum relevance, is not present
in the text, so the overall similarity score would have
a considerable handicap.
The example of limitation of our system that we
have presented is an apposition. To avoid this spe-
cific kind of situations that produce an undesired be-
havior in our system, we could add a preprocess-
ing module that transforms the phrases that have the
1The machine we used to measure the response times had an
Intel Core 2 Duo processor at 2GHz.
2Average response times are calculated diving the totals by
the number of pairs in the corpus.
structureX , Y , Z intoX is Y , andZ. For the shown
example, the resulting text and hypothesis would be
as follows:
Text: Tony Blair is the British Prime Min-
ister, and met Jacques Chirac in London.
Hypothesis: Tony Blair is the British
Prime Minister.
The transformed text would still be syntactically
correct, and the entailment would be detected since
the hypothesis? syntactic dependency tree is embed-
ded into the text?s.
4 Conclusions and future work
The experimental results obtained from this research
demonstrate that it is possible to apply a syntactic-
based approach to deduce textual entailment from a
text-hypothesis pair. We can obtain good accuracy
rates using the discussed techniques with very short
response times, which is very useful for assisting
different kinds of tasks that demand near-real-time
responses to user interaction.
The baseline we set for our system was to achieve
better results than the ones we obtained with our last
participation in RTE-2. As it is stated in (Ferra?ndez
et al, 2006), the maximum accuracy value obtained
by then was 55.63% for the test corpus. Therefore,
our system is 9.20% more accurate compared to the
one that participated in RTE-2, which represents a
considerable improvement.
The authors of this paper believe that if higher ac-
curacy rates are desired, a step-based systemmust be
constructed. This would have several preprocessing
units, such as negation detectors, multi-word associ-
ators and so on. The addition of these units would
definitely increase the response time preventing the
system from being used in real-time tasks.
Future work can be related to the cases where no
verb entailment is produced. For this purpose we
propose to extract a higher amount of semantic in-
formation that would allow us to construct a charac-
terized representation based on the input text, so that
we can deduce entailment even if there is no appar-
ent structure similarity between text and hypothesis.
This would mean to create an abstract conceptual-
ization of the information contained in the analyzed
phrases, allowing us to deduce ideas that are not
79
explicitly mentioned in the parsed text-hypothesis
pairs.
In addition, the weights and thresholds defined
in our system have been established empirically. It
would be interesting to calculate those values by
means of a machine learning algorithm and com-
pare them to the ones we have obtained empirically.
Some authors have already performed this compari-
son, being one example the work described in (Mac-
Cartney et al, 2006).
Acknowledgments
The authors of this paper would like to thank pro-
fessors Borja Navarro and Rafael M. Terol for their
help and critical comments.
This research has been supported by the under-
graduate research fellowships financed by the Span-
ish Ministry of Education and Science, the project
TIN2006-15265-C06-01 financed by such ministry,
and the project ACOM06/90 financed by the Span-
ish Generalitat Valenciana.
References
Rod Adams. 2006. Textual Entailment Through Ex-
tended Lexical Overlap. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The Second PASCAL Recognising Textual En-
tailment Challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Johan Bos, and Katja Markert. 2006. When logical infer-
ence helps determining textual entailment (and when it
doesnt). In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
O?scar Ferra?ndez, Rafael M. Terol, Rafael Mun?oz, Patri-
cio Mart??nez-Barco, and Manuel Palomar. 2006. An
approach based on Logic Forms andWordNet relation-
ships to Textual Entailment performance. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
Jesu?s Herrera, Anselmo Pen?as, A?lvaro Rodrigo, and Fe-
lisa Verdejo. 2006. UNED at PASCAL RTE-2 Chal-
lenge. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing Textual Entailment with LCC?s GROUNDHOG
System. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
Sophia Katrenko, and Pieter Adriaans. 2006. Using
Maximal Embedded Syntactic Subtrees for Textual En-
tailment Recognition. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems, Granada, Spain.
Bill MacCartney, Trond Grenager, Marie-Catherine de
Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the North American
Association of Computational Linguistics (NAACL-
06), NewYork City, NewYork, United States of Amer-
ica.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990. In-
troduction to WordNet: An On-line Lexical Database.
International Journal of Lexicography 1990 3(4):235-
244.
Ted Pedersen, Siddhart Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concepts. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-04), Boston, Massachus-
sets, United States of America.
Rion Snow, Lucy Vanderwende, and Arul Menezes.
2006. Effectively using syntax for recognizing false
entailment. In Proceedings of the North American As-
sociation of Computational Linguistics (NAACL-06),
New York City, New York, United States of America.
Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi,
and DanMoldovan. 2006. COGEX at the Second Rec-
ognizing Textual Entailment Challenge. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
Zhibiao Wu, and Martha Palmer. 1994. Verb Semantics
and Lexical Selection. In Proceedings of the 32nd An-
nual Meeting of the Associations for Computational
Linguistics, pages 133-138, Las Cruces, New Mexico,
United States of America.
Fabio M. Zanzotto, Alessandro Moschitti, Marco Pen-
nacchiotti, and Maria T. Pazienza. 2006. Learning
textual entailment from examples. In Proceedings of
the Second PASCAL Challenges Workshop on Recog-
nising Textual Entailment, Venice, Italy.
80
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 66?71,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Perspective-Based Approach for Solving Textual Entailment Recognition
O?scar Ferra?ndez, Daniel Micol, Rafael Mun?oz, and Manuel Palomar
Natural Language Processing and Information Systems Group
Department of Computing Languages and Systems
University of Alicante
San Vicente del Raspeig, Alicante 03690, Spain
{ofe, dmicol, rafael, mpalomar}@dlsi.ua.es
Abstract
The textual entailment recognition system
that we discuss in this paper represents
a perspective-based approach composed of
two modules that analyze text-hypothesis
pairs from a strictly lexical and syntactic
perspectives, respectively. We attempt to
prove that the textual entailment recognition
task can be overcome by performing indi-
vidual analysis that acknowledges us of the
maximum amount of information that each
single perspective can provide. We compare
this approach with the system we presented
in the previous edition of PASCAL Recognis-
ing Textual Entailment Challenge, obtaining
an accuracy rate 17.98% higher.
1 Introduction
Textual entailment recognition has become a popu-
lar Natural Language Processing task within the last
few years. It consists in determining whether one
text snippet (hypothesis) entails another one (text)
(Glickman, 2005). To overcome this problem sev-
eral approaches have been studied, being the Recog-
nising Textual Entailment Challenge (RTE) (Bar-
Haim et al, 2006; Dagan et al, 2006) the most re-
ferred source for determining which one is the most
accurate.
Many of the participating groups in previous edi-
tions of RTE, including ourselves (Ferra?ndez et al,
2006), designed systems that combined a variety of
lexical, syntactic and semantic techniques. In our
contribution to RTE-3 we attempt to solve the tex-
tual entailment recognition task by analyzing two
different perspectives separately, in order to ac-
knowledge the amount of information that an indi-
vidual perspective can provide. Later on, we com-
bine both modules to obtain the highest possible ac-
curacy rate. For this purpose, we analyze the pro-
vided corpora by using a lexical module, namely
DLSITE-1, and a syntactic one, namely DLSITE-2.
Once all results have been obtained we perform a
voting process in order to take into account all sys-
tem?s judgments.
The remainder of this paper is structured as fol-
lows. Section two describes the system we have
built, providing details of the lexical and syntactic
perspectives, and explains the difference with the
one we presented in RTE-2. Third section presents
the experimental results, and the fourth one provides
our conclusions and describes possible future work.
2 System Specification
This section describes the systemwe have developed
in order to participate in RTE-3. It is based on sur-
face techniques of lexical and syntactic analysis. As
the starting point we have used our previous system
presented in the second edition of the RTE Chal-
lenge (Ferra?ndez et al, 2006). We have enriched
it with two independent modules that are intended
to detect some misinterpretations performed by this
system. Moreover, these new modules can also rec-
ognize entailment relations by themselves. The per-
formance of each separate module and their combi-
nation with our previous system will be detailed in
section three.
Next, Figure 1 represents a schematic view of the
system we have developed.
66
Figure 1: System architecture.
As we can see in the previous Figure, our sys-
tem is composed of three modules that are coordi-
nated by an input scheduler. Its commitment is to
provide the text-hypothesis pairs to each module in
order to extract their corresponding similarity rates.
Once all rates for a given text-hypothesis pair have
been calculated, they will be processed by an output
gatherer that will provide the final judgment. The
method used to calculate the final entailment deci-
sion consists in combining the outputs of both lex-
ical and syntactic modules, and these outputs with
our RTE-2 system?s judgment. The output gatherer
will be detailed later in this paper when we describe
the experimental results.
2.1 RTE-2 System
The approach we presented in the previous edition of
RTE attempts to recognize textual entailment by de-
termining whether the text and the hypothesis are re-
lated using their respective derived logic forms, and
by finding relations between their predicates using
WordNet (Miller et al, 1990). These relations have
a specific weight that provide us a score represent-
ing the similarity of the derived logic forms and de-
termining whether they are related or not.
For our participation in RTE-3 we decided to ap-
ply our previous system because it allows us to han-
dle some kinds of information that are not correctly
managed by the new approaches developed for the
current RTE edition.
2.2 Lexical Module
This method relies on the computation of a wide va-
riety of lexical measures, which basically consists of
overlap metrics. Although in other related work this
kind of metrics have already been used (Nicholson
et al, 2006), the main contribution of this module is
the fact that it only deals with lexical features with-
out taking into account any syntactic nor semantic
information. The following paragraphs list the con-
sidered lexical measures.
Simple matching: initialized to zero. A boolean
value is set to one if the hypothesis word appears in
the text. The final weight is calculated as the sum of
all boolean values and normalized dividing it by the
length of the hypothesis.
Levenshtein distance: it is similar to simple match-
ing. However, in this case we use the mentioned
distance as the similarity measure between words.
When the distance is zero, the increment value is
one. On the other hand, if such value is equal to one,
the increment is 0.9. Otherwise, it will be the inverse
of the obtained distance.
Consecutive subsequence matching: this measure
assigns the highest relevance to the appearance of
consecutive subsequences. In order to perform this,
we have generated all possible sets of consecutive
subsequences, from length two until the length in
words, from the text and the hypothesis. If we pro-
ceed as mentioned, the sets of length two extracted
from the hypothesis will be compared to the sets of
the same length from the text. If the same element is
present in both the text and the hypothesis set, then
a unit is added to the accumulated weight. This pro-
cedure is applied for all sets of different length ex-
tracted from the hypothesis. Finally, the sum of the
weight obtained from each set of a specific length is
normalized by the number of sets corresponding to
67
this length, and the final accumulated weight is also
normalized by the length of the hypothesis in words
minus one. This measure is defined as follows:
CSmatch =
|H|?
i=2
f(SHi)
|H| ? 1
(1)
where SHi contains the hypothesis? subsequences
of length i, and f(SHi) is defined as follows:
f(SHi) =
?
j?SHi
match(j)
|H| ? i+ 1
(2)
being match(j) equal to one if there exists an ele-
ment k that belongs to the set that contains the text?s
subsequences of length i, such that k = j.
One should note that this measure does not con-
sider non-consecutive subsequences. In addition, it
assigns the same relevance to all consecutive sub-
sequences with the same length. Furthermore, the
longer the subsequence is, the more relevant it will
be considered.
Tri-grams: two sets containing tri-grams of letters
belonging to the text and the hypothesis were cre-
ated. All the occurrences in the hypothesis? tri-
grams set that also appear in the text?s will increase
the accumulated weight in a factor of one unit. The
weight is normalized by the size of the hypothesis?
tri-grams set.
ROUGE measures: considering the impact of n-
gram overlap metrics in textual entailment, we be-
lieve that the idea of integrating these measures1 into
our system is very appealing. We have implemented
them as defined in (Lin, 2004).
Each measure is applied to the words, lemmas and
stems belonging to the text-hypothesis pair. Within
the entire set of measures, each one of them is con-
sidered as a feature for the training and test stages
of a machine learning algorithm. The selected one
was a Support Vector Machine due to the fact that its
properties are suitable for recognizing entailment.
2.3 Syntactic Module
The syntactic module we have built is composed of
few submodules that operate collaboratively in order
1The considered measures were ROUGE-N with n=2 and
n=3, ROUGE-L, ROUGE-W and ROUGE-S with s=2 and s=3.
to obtain the highest possible accuracy by using only
syntactic information.
The commitment of the first two submodules is
to generate an internal representation of the syntac-
tic dependency trees generated by MINIPAR (Lin,
1998). For this purpose we obtain the output of such
parser for the text-hypothesis pairs, and then process
it to generate an on-memory internal representation
of the mentioned trees. In order to reduce our sys-
tem?s noise and increase its accuracy rate, we only
keep the relevant words and discard the ones that we
believe do not provide useful information, such as
determinants and auxiliary verbs. After this step has
been performed we can proceed to compare the gen-
erated syntactic dependency trees of the text and the
hypothesis.
The graph node matching, termed alignment, be-
tween both the text and the hypothesis consists in
finding pairs of words in both trees whose lemmas
are identical, no matter whether they are in the same
position within the tree. Some authors have already
designed similar matching techniques, such as the
one described in (Snow et al, 2006). However, these
include semantic constraints that we have decided
not to consider. The reason of this decision is that we
desired to overcome the textual entailment recogni-
tion from an exclusively syntactic perspective. The
formula that provides the similarity rate between the
dependency trees of the text and the hypothesis in
our system, denoted by the symbol ?, is shown in
Equation 3:
?(?, ?) =
?
???
?(?) (3)
where ? and ? represent the text?s and hypothesis?
syntactic dependency trees, respectively, and ? is the
set that contains all synsets present in both trees, be-
ing ? = ? ? ? ?? ? ?, ? ? ?. As we can observe in
Equation 3, ? depends on another function, denoted
by the symbol ?, which provides the relevance of
a synset. Such a weight factor will depend on the
grammatical category and relation of the synset. In
addition, we believe that the most relevant words of
a phrase occupy the highest positions in the depen-
dency tree, so we desired to assign different weights
depending on the depth of the synset. With all these
factors we define the relevance of a word as shown
68
in Equation 4:
?(?) = ? ? ? ? ???? (4)
where ? is a synset present in both ? and ?, ? rep-
resents the weight assigned to ??s grammatical cat-
egory (Table 1), ? the weight of ??s grammatical
relationship (Table 2), ? an empirically calculated
value that represents the weight difference between
tree levels, and ?? the depth of the node that contains
the synset ? in ?. The performed experiments reveal
that the optimal value for ? is 1.1.
Grammatical category Weight
Verbs, verbs with one argument, verbs with
two arguments, verbs taking clause as com-
plement
1.0
Nouns, numbers 0.75
Be used as a linking verb 0.7
Adjectives, adverbs, noun-noun modifiers 0.5
Verbs Have and Be 0.3
Table 1: Weights assigned to the relevant grammati-
cal categories.
Grammatical relationship Weight
Subject of verbs, surface subject, object of
verbs, second object of ditransitive verbs
1.0
The rest 0.5
Table 2: Weights assigned to the grammatical rela-
tionships.
We would like to point out that a requirement of
our system?s similarity measure is to be independent
of the hypothesis length. Therefore, we must de-
fine the normalized similarity rate, as represented in
Equation 5:
?(?, ?) =
?
???
?(?)
?
???
?(?)
(5)
Once the similarity value has been calculated, it
will be provided to the user together with the cor-
responding text-hypothesis pair identifier. It will be
his responsibility to choose an appropriate threshold
that will represent the minimum similarity rate to be
considered as entailment between text and hypothe-
sis. All values that are under such a threshold will
be marked as not entailed.
3 System Evaluation
In order to evaluate our system we have generated
several results using different combinations of all
three mentioned modules. Since the lexical one uses
a machine learning algorithm, it has to be run within
a training environment. For this purpose we have
trained our system with the corpora provided in the
previous editions of RTE, and also with the develop-
ment corpus from the current RTE-3 challenge. On
the other hand, for the remainder modules the devel-
opment corpora was used to set the thresholds that
determine if the entailment holds.
The performed tests have been obtained by per-
forming different combinations of the described
modules. First, we have calculated the accuracy
rates using only each single module separately.
Later on we have combined those developed by our
research group for this year?s RTE challenge, which
are DLSITE-1 (the lexical one) and DLSITE-2 (the
syntactic one). Finally we have performed a voting
process between these two systems and the one we
presented in RTE-2.
The combination of DLSITE-1 and DLSITE-2 is
described as follows. If both modules agree, then the
judgement is straightforward, but if they do not, we
then decide the judgment depending on the accuracy
of each one for true and false entailment situations.
In our case, DLSITE-1 performs better while dealing
with negative examples, so its decision will prevail
over the rest. Regarding the combination of the three
approaches, we have developed a voting strategy.
The results obtained by our system are represented
in Table 3. As it is reflected in such table, the high-
est accuracy rate obtained using the RTE-3 test cor-
pus was achieved applying only the lexical module,
namely DLSITE-1. On the other hand, the syntac-
tic one had a significantly lower rate, and the same
happened with the system we presented in RTE-2.
Therefore, a combination of them will most likely
produce less accurate results than the lexical mod-
ule, as it is shown in Table 3. However, we would
like to point out that these results depend heavily on
the corpus idiosyncrasy. This can be proven with the
results obtained for the RTE-2 test corpus, where the
grouping of the three modules provided the highest
accuracy rates of all possible combinations.
69
RTE-2 test RTE-3 dev RTE-3 test
Overall Overall Overall IE IR QA SUM
RTE-2 system 0.5563 0.5523 0.5400 0.4900 0.6050 0.5100 0.5550
DLSITE-1 0.6188 0.7012 0.6563 0.5150 0.7350 0.7950 0.5800
DLSITE-2 0.6075 0.6450 0.5925 0.5050 0.6350 0.6300 0.6000
DLSITE-1&2 0.6212 0.6900 0.6375 0.5150 0.7150 0.7400 0.5800
Voting 0.6300 0.6900 0.6375 0.5250 0.7050 0.7200 0.6000
Table 3: Results obtained with the corpora from RTE-2 and RTE-3.
3.1 Results Analysis
We will now perform an analysis of the results
shown in the previous section. First, we would like
to mention the fact that our system does not be-
have correctly when it has to deal with long texts.
Roughly 11% and 13% of the false positives of
DLSITE-1 and DLSITE-2, respectively, are caused
by misinterpretations of long texts. The underlying
reason of these failures is the fact that it is easier to
find a lexical and syntactic match when a long text
is present in the pair, even if there is not entailment.
In addition, we consider very appealing to show
the accuracy rates corresponding to true and false
entailment pairs individually. Figure 2 represents the
mentioned rates for all system combinations that we
displayed in Table 3.
Figure 2: Accuracy rates obtained for true and false
entailments using the RTE-3 test corpus.
As we can see in Figure 2, the accuracy rates
for true and false entailment pairs vary significantly.
The modules we built for our participation in RTE-3
obtained high accuracy rates for true entailment text-
hypothesis pairs, but in contrast they behaved worse
in detecting false entailment pairs. This is the oppo-
site to the system we presented in RTE-2, since it has
a much higher accuracy rate for false cases than true
ones. When we combinedDLSITE-1 andDLSITE-2,
their accuracy rate for true entailments diminished,
although, on the other hand, the rate for false ones
raised. The voting between all three modules pro-
vided a higher accuracy rate for false entailments be-
cause the system we presented at RTE-2 performed
well in these cases.
Finally, we would like to discuss some examples
that lead to failures and correct forecasts by our two
new approaches.
Pair 246 entailment=YES task=IR
T: Overall the accident rate worldwide for commercial aviation
has been falling fairly dramatically especially during the period
between 1950 and 1970, largely due to the introduction of new
technology during this period.
H: Airplane accidents are decreasing.
Pair 246 is incorrectly classified by DLSITE-1
due to the fact that some words of the hypothesis do
not appear in the same manner in the text, although
they have similar meaning (e.g. airplane and
aviation). However, DLSITE-2 is able to establish a
true entailment for this pair, since the hypothesis?
syntactic dependency tree can be matched within the
text?s, and the similarity measure applied between
lemmas obtains a high score. This fact produces
that, in this case, the voting also achieves a correct
prediction for pair 246.
Pair 736 entailment=YES task=SUM
T: In a security fraud case, Michael Milken was sentenced to 10
years in prison.
H: Milken was imprisoned for security fraud.
Pair 736 is correctly classified by DLSITE-1 since
there are matches for all hypothesis? words (except
imprisoned) and some subsequences. In contrast,
DLSITE-2 does not behave correctly with this exam-
ple because the main verbs do not match, being this
fact a considerable handicap for the overall score.
70
4 Conclusions and Future Work
This research provides independent approaches con-
sidering mainly lexical and syntactic information. In
order to achieve this, we expose and analyze a wide
variety of lexical measures as well as syntactic struc-
ture comparisons that attempt to solve the textual en-
tailment recognition task. In addition, we propose
several combinations between these two approaches
and integrate them with our previous RTE-2 system
by using a voting strategy.
The results obtained reveal that, although the
combined approach provided the highest accuracy
rates for the RTE-2 corpora, it has not accom-
plished the expected reliability in the RTE-3 chal-
lenge. Nevertheless, in both cases the lexical-based
module achieved better results than the rest of the in-
dividual approaches, being the optimal for our par-
ticipation in RTE-3, and obtaining an accuracy rate
of about 70% and 65% for the development and test
corpus, respectively. One should note that these re-
sults depend on the idiosyncrasies of the RTE cor-
pora. However, these corpora are the most reliable
ones for evaluating textual entailment recognizers.
Future work can be related to the development
of a semantic module. Our system achieves good
lexical and syntactic comparisons between texts, but
we believe that we should take advantage of the se-
mantic resources in order to achieve higher accuracy
rates. For this purpose we plan to build a module
that constructs characterized representations based
on the text using named entities and role labeling in
order to extract semantic information from a text-
hypothesis pair. Another future research line could
consist in applying different recognition techniques
depending on the type of entailment task. We have
noticed that the accuracy of our approach differs
when the entailment is produced mainly by lexical
or syntactic implications. We intend to establish an
entailment typology and tackle each type by means
of different points of view or approaches.
Acknowledgments
This research has been partially funded by the
QALL-ME consortium, which is a 6th Framework
Research Programme of the European Union (EU),
contract number FP6-IST-033860 and by the Span-
ish Government under the project CICyT number
TIN2006-1526-C06-01. It has also been supported
by the undergraduate research fellowships financed
by the Spanish Ministry of Education and Science,
and the project ACOM06/90 financed by the Span-
ish Generalitat Valenciana.
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The Second PASCAL Recognising Textual En-
tailment Challenge. Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment, pages 1?9.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In Quin?onero-Candela et al, edi-
tors, MLCW 2005, LNAI Volume 3944, pages 177?190.
Springer-Verlag.
Oscar Ferra?ndez, Rafael M. Terol, Rafael Mun?oz, Patri-
cio Mart??nez-Barco, and Manuel Palomar. 2006. An
approach based on Logic forms and wordnet relation-
ships to textual entailment performance. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 22?26, Venice,
Italy.
Oren Glickman. 2005. Applied Textual Entailment Chal-
lenge. Ph.D. thesis, Bar Ilan University.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems, Granada, Spain.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Stan Szpakow-
icz Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
pages 74?81, Barcelona, Spain, July. Association for
Computational Linguistics.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990. In-
troduction to WordNet: An On-line Lexical Database.
International Journal of Lexicography, 3(4):235?244.
Jeremy Nicholson, Nicola Stokes, and Timothy Baldwin.
2006. Detecting Entailment Using an Extended Imple-
mentation of the Basic Elements Overlap Metrics. In
Proceedings of the Second PASCAL Challenges Work-
shop on Recognising Textual Entailment, pages 122?
127, Venice, Italy.
Rion Snow, Lucy Vanderwende, and Arul Menezes.
2006. Effectively using syntax for recognizing false
entailment. In Proceedings of the North American
Association of Computational Linguistics, New York
City, New York, United States of America.
71
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 65?72,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
A Hybrid Stepwise Approach for De-identifying Person Names  in Clinical Documents   Oscar Ferr?ndez1,2, Brett R. South1,2, Shuying Shen1,2, St?phane M. Meystre1,2 1 Department of Biomedical Informatics, University of Utah, Salt Lake City, Utah, USA 2 IDEAS Center SLCVA Healthcare System, Salt Lake City, Utah, USA oscar.ferrandez@utah.edu, {brett.south,shuying.shen,stephane.meystre}@hsc.utah.edu   Abstract 
As Electronic Health Records are growing ex-ponentially along with large quantities of un-structured clinical information that could be used for research purposes, protecting patient privacy becomes a challenge that needs to be met. In this paper, we present a novel hybrid system designed to improve the current strate-gies used for person names de-identification. To overcome this task, our system comprises several components designed to accomplish two separate goals: 1) achieve the highest re-call (no patient data can be exposed); and 2) create methods to filter out false positives. As a result, our system reached 92.6% F2-measure when de-identifying person names in Veteran?s Health Administration clinical notes, and considerably outperformed other existing ?out-of-the-box? de-identification or named entity recognition systems.  
 1 Introduction Electronic Healthcare Records are invaluable re-sources for clinical research, however they contain highly sensitive Protected Health Information (PHI) that must remain confidential. In the United States, patient confidentiality is regulated by the Health Insurance Portability and Accountability Act (HIPAA). To share and use clinical documents for research purposes without patient consent, HIPAA requires prior removal of PHI. More spe-cifically, the HIPAA ?Safe Harbor?1 determines 18                                                 1 GPO US: 45 C.F.R. ? 164 Security and Privacy. http://www.access.gpo.gov/nara/cfr/waisidx_08/45cfr164_08.html Further details about the 18 HIPAA Safe Harbor PHI identifi-ers can be also found in (Meystre et al, 2010). 
PHI categories that have to be obscured in order to consider clinical data de-identified. An ideal de-identification system should recog-nize PHI accurately, but also preserve relevant non-PHI clinical data, so that clinical records can later be used for various clinical research tasks. Of the 18 categories of PHI listed by HIPAA, one of the most sensitive is patient names, and all person names in general. Failure to de-identify such PHI involves a high risk of re-identification, and jeopardizes patient privacy. In this paper, we describe our effort to satisfac-torily de-identify person names in Veteran?s Health Administration (VHA) clinical documents. We propose improvements in person names de-identification with a pipeline of processes tailored to the idiosyncrasies of clinical documents. This effort was realized in the context of the develop-ment of a best-of-breed clinical text de-identification system (nicknamed ?BoB?), which will be released as an open source software pack-age, and it started with the implementation and evaluation of several existing de-identification and Named Entity Recognition (NER) systems recog-nizing person names. We then devised a novel methodology to better tackle this task and improve performance.  2 Background and related work  In many aspects de-identification resembles tradi-tional NER tasks (Grishman and Sundheim, 1996). NER involves detecting entities such as person names, locations, and organizations. Consequently, given the similar entities targeted by both tasks, NER systems can be relevant to de-identify docu-ments. However, most named entity recognizers were developed for newswire articles, and not for clinical narratives. Clinical records are character-
65
ized by fragmented and incomplete utterances, lack of punctuation marks and formatting, as well as domain specific language. These complications, in addition to the fact that some entities can appear both as PHI and non-PHI in the same document (e.g., ?Mr. Epley? vs. ?the Epley maneuver?), make clinical text de-identification a challenging task. Therefore, although person names de-identification is essentially NER, the unique char-acteristics of clinical texts make it more interesting and challenging than recognizing names in news articles, which also enhance the motivation for this study. Several different approaches were proposed to deal with de-identification of clinical documents, and for named entity recognition of person names. These approaches are mainly focused on either pattern matching techniques, or statistical methods (Meystre et al, 2010), as exemplified below. Beckwith et al (2006) developed a de-identification system for pathology reports. This system implemented some patterns to detect dates, locations, and ID numbers, as well as a database of proper names and well-known markers such as ?Mr.? and ?PhD? to find person names. Friedlin and McDonald (2008) described the Medical De-identification System (MeDS). It used a combination of methods including heuristics, pattern matching, and dictionary lookups to identi-fy PHI. Pattern matching through regular expres-sions was used to detect numerical identifiers, dates, addresses, ages, etc.; while for names, MeDS used lists of proper names, common usage words and predictive markers, as well as a text string nearness algorithm to deal with typograph-ical errors.  Neamatullah et al (2008) proposed another rule-based de-identification approach focused on pat-tern matching via dictionary lookups, regular ex-pressions and context checks heuristics denoting PHI. Dictionaries made up of ambiguous names and locations that could also be non-PHI, as well as dictionaries of common words were used by this system to disambiguate PHI terms. Other de-identification systems such as (Aberdeen et al, 2010; Gardner and Xiong, 2009) use machine learning algorithms to train models and predict new annotations. The key aspect of these systems is the selection of the learning algo-rithm and features. Both (Aberdeen et al, 2010) and (Gardner and Xiong, 2009) use an implemen-
tation of Conditional Random Fields (CRF) and a set of learning features based on the morphology of the terms and their context. One disadvantage of these systems is the need for large amounts of an-notated training examples. As mentioned previously, for detecting person names, we could also use traditional newswire-trained NER systems. NER has long been studied by the research community and many different ap-proaches have been developed (Tjong Kim Sang and De Meulder, 2003; Doddington et al, 2004). One successful and freely available named entity recognizer is the Stanford NER system (Finkel et al, 2005), which provides an implementation of linear chain CRF sequence models, coupled with well-engineered feature extractors for NER, and trained with newswire documents.  3 Methods As already mentioned, we first selected and ran several existing de-identification and NER systems detecting person names in our clinical documents. Afterwards, we devised and present here a novel pipeline of processes designed to improve the PHI recognition task. 3.1 Existing de-identification and NER sys-tems Five available de-identification systems, as well as one newswire-trained named entity recognizer, were selected for an ?out-of-the-box? evaluation. The aim of this evaluation was to compare the per-formance of the various methods and resources when de-identifying person names in our clinical documents. We included three rule-based de-identification approaches:  ? HMS Scrubber (Beckwith et al, 2006); ? MeDS (Friedlin and McDonald, 2008); and ? MIT deid system (Neamatullah et al, 2008).  And two systems based on machine learning classifiers: ? The MITRE Identification Scrubber Toolkit (MIST) (Aberdeen et al, 2010); and  ? The Health Information DE-identification (HIDE) system (Gardner and Xiong, 2009).  
66
Regarding NER systems, we chose the Stanford NER system (Finkel et al, 2005), which has re-ported successful results when detecting person names. These systems were described in Section 2, when we presented related work. 3.2 Our best-of-breed approach Our names de-identification approach consists of a novel pipeline of processes designed to improve the current strategies for person names de-identification. This system is being developed as an Apache UIMA2 pipeline, with two main goals:   1) Obtain the highest recall (i.e., sensitivity), re-gardless of the impact on precision; and  2) Improve overall precision by filtering out the false positives produced previously.   These goals correspond to the implementation of the main components of our system. When we tested existing systems (we will present results for these systems in Table 1), we observed that recall was better addressed by rule-based approaches, while precision was higher applying machine learning-based algorithms. We therefore used this knowledge for the design of our system: goal#1 is then accomplished mainly using rule-based tech-niques, and goal#2 implementing machine learn-ing-based approaches. Moreover, recall is of paramount importance in de-identification (patient PHI cannot be disclosed). And this was also a reason that motivated us to first focus on achieving high recall, and filtering out false positives afterwards as a separate proce-dure.  Unlike other de-identification and NER systems that tackle the classification problem from one per-spective (i.e., rule-based or machine learning-based) or from a limited combined approach (e.g., learning features extracted using regular expres-sions), the design of our system allows us to take advantage of the strong points of both techniques separately. And more importantly, our classifiers for filtering out false-positives (goal#2) are trained using correct and incorrect annotations derived from previous modules implemented in goal#1. Thus, they do not predict if every token in the doc-ument is or belongs to a PHI identifier, they in-stead decide if an actual annotation is a false or                                                 2 http://uima.apache.org/ 
true positive. This design makes our classifiers better with less learning examples, which is a re-striction we have to deal with, and it also allows us to create methods that can be only focused on max-imizing recall regardless of the amount of false-positives introduced (goal#1). To the best of our knowledge, this perspective has not been exploited before, and as we will show in the evaluation sec-tion, it empirically demonstrates more robustness than previous approaches. 
The design of our system integrates different components described below. Figure 1 depicts an overview of our system?s architecture and work-flow.  
  Figure 1. System?s architecture.  3.2.1 NLP preprocessing steps This NLP preprocessing prepares the input for the main components of our system. It includes sen-tence segmentation, tokenization, part-of-speech tagging, chunking, and word normalization based 
67
on Lexical Variant Generation (LVG)3. The output of this preprocessing will be used by subsequent pattern matching techniques and features for ma-chine learning algorithms. For these processes, we adapted several cTAKES (Savova et al, 2010) components. 3.2.2 Rules and dictionary lookups We created a pattern matching component support-ed by contextual keyword searches (e.g., ?Dr.?, ?Mr.?, ?M.D.?, ?R.N.?, ?L.C.S.W.?), dictionaries of person names4, and a simple disambiguation procedure based on a list of common words and the capitalization of the entity. We adapted some of the techniques implemented in (Beckwith et al, 2006; Friedlin and McDonald, 2008; Neamatullah et al, 2008) to our documents, and developed new patterns. For dictionary lookups, we used Lucene5 indexing, experimenting with keyword and fuzzy dictionary searches. Each word token is compared with our indexed dictionary of names (last and first names from the 1990 US Census4), considering all matches as candidate name annotations. However, candidates that also match with an entry in our dic-tionary of common words6 and do not contain an initial capital letter are discarded from this set of candidate name annotations. With this component, we attempt to maximize recall, even if precision is altered. 3.2.3 CRF-based predictions To further enhance recall, we created another com-ponent based on CRF models. We incorporated this component in our system considering that ma-chine learning classifiers are more generalizable and can detect instances of names that are not sup-ported by our rules or dictionaries. Therefore, alt-hough we knew the individual results of a CRF classifier at this level were not enough for de-identification, at this point our main concern is to obtain the highest recall. Thus, adding a machine learning classifier into this level we could help the system predicting the PHI formats and instances                                                 3 http://lexsrv2.nlm.nih.gov/LexSysGroup/Projects/lvg/      current/web/index.html 4 Frequently Occurring Names from the 1990 Census. http://www.census.gov/genealogy/names. 5 http://lucene.apache.org/java/docs/index.html 6 We used the dictionary of common words from Neamatullah et al (2008). 
that could not be covered by our patterns and dic-tionaries. To develop this component, we used the CRF classifier implementation provided by the Stanford NLP group7. We carried out a feature selection procedure using greedy forward selection. It pro-vided us with the best learning feature set, which consisted of: the target word, 2-grams of letters, position in the document, part-of-speech tag, lem-ma, widely-used word-shape features (e.g., initial capitals, all capitals, digits inside, etc.), features from dictionaries of names and common words, a 2-word context window, and combinations of words, word-shapes and part-of-speech tags of the word and its local context. The learning features considered before and af-ter the selection procedure are shown in Table 1. 3.2.4 False-positive filtering The two previous components? objective is maxi-mal recall, producing numerous false positives. The last component of our pipeline was therefore designed to filter out these false positives and con-sequently increase overall precision. We built a machine learning classifier for this task, based on LIBSVM (Chang and Lin, 2001), a library for Support Vector Machines (SVM), with the RBF (Radial Basis Function) kernel. We then trained this classifier with reference standard text annota-tions, as well as the correct and incorrect annota-tions made by the previous components. We used our training document set (section 4.1) for this purpose. Features for the LIBSVM machine learning model were: the LVG normalized form of the target anno-tation, three words before and after, part-of-speech tags of the words within the annotation and the local context, number of tokens within the annota-tion, position in the document, 40 orthographic features (denoting capitals, digits, special charac-ters, etc.), features from dictionaries of names and common words, and the previous strategy used to make the annotation (i.e., rules, dictionary lookups or CRF-based predictions). 
                                                7 http://nlp.stanford.edu/software/corenlp.shtml 
68
Feature Description Selected* target word The word to classify as person name Yes 2-grams of letters Features from the 2-grams of letters from the word Yes 3-grams of letters Features from the 3-grams of letters from the word No 4-grams of letters Features from the 4-grams of letters from the word No lowercase n-grams Features from the n-grams of letters from the word in  lowercase (considering 2-, 3-, and 4-grams separately) No position Position of the word within a sentence Yes PoS Part-of-speech tag of the word Yes lemma Lemma of the word Yes 
word shape 
Initial capital 
Yes 
All capitals Mix of uppercase and lowercase letters Digits inside All digits Has dash End dash Alpha-numeric Numeric-alpha (starts with a number) Contains punctuation mark 
dictionaries Does the word match with an entry of the dictionary of names? Yes Does the word match with an entry of the dictionary of  common words? Yes 2-word window The two preceding and following words in the context Yes 3-,4-,5-word window The three, four and five preceding and following words in the context No 
word-pairs Combinations of the word and the next and previous words in the  context window, preserving direction but not position  (considering separate features for the different combinations  of the context and the target word) No titles Match the word against a list of name titles (Mr, Mrs, etc.) No lemma_context Lemma of the words inside the contextual window No PoS_context Individual features from the part-of-speech tags  of the contextual window Yes PoS_sequence Sequence of the part-of-speech tags of the 2-word contextual window and the target word Yes word_shape_context Word shape features of the contextual window Yes word-tag Combination of the word and part-of-speech No Table 1. Set of learning features for the CRF-based prediction module. (* = selected in the best learning features set)  4 Evaluation and discussion Our evaluation consists of: 1) ?out-of-the-box? evaluation of the systems presented in Section 3.1; and 2) evaluation of the performance of our person names de-identification pipeline.     
4.1 Data We manually annotated all person names (includ-ing patients, relatives, health care providers, and other persons) in a corpus of various types of Vet-eran?s Health Administration (VHA) clinical notes. These notes were selected using a stratified ran-dom sampling approach with documents longer than 500 words. Then, the 100 most frequent VHA note types were used as strata for sampling, and the 
69
same number of notes was randomly selected in each stratum. Two reviewers independently anno-tated each document, a third reviewer adjudicated their disagreements, and a fourth reviewer eventu-ally examined ambiguous and difficult adjudicated cases. The evaluation corpus presented here comprises a subset of 275 VHA clinical notes from the aforementioned corpus. For training, 225 notes were randomly selected (contained 748 person name annotations), and the remaining 50 notes (with 422 name annotations) were used for testing the systems. 4.2 Experiments and results We present results in terms of precision, recall and F-measure (harmonic mean of recall and preci-sion). We used a weight of 2 when calculating the F2-measure giving recall more (twice) importance than precision (Jurafsky and Martin, 2009). This reflects our emphasis on recall for de-identification. To our understanding, due to legal and privacy issues, a good de-identification system should be tailored to prioritize recall, and conse-quently patient confidentiality. It is not the scope of this paper to judge or modify the development design adopted by other de-identification systems. Moreover, we considered correct predictions at least overlapping with the entire PHI annotation in the reference standard (i.e., exact match with the reference annotation, or more than the exact match). We can therefore assure complete redac-tion of PHI. Table 2 illustrates ?out-of-the-box? evaluation results of the systems described in Section 3.1. For this evaluation, we trained MIST and HIDE with our 225 notes training corpus, while the Stanford NER was run using the trained models available with its distribution8. Testing was realized using our 50 notes testing corpus. Table 3 shows the performance of our names de-identification approach. We provide results for dif-ferent configurations of our pipeline:  ? Rules & Dictionaries. Results of the rules and dictionary lookups component de-scribed in Section 3.2.2, in this case using a                                                 8 Further details about these models can be found at http://nlp.stanford.edu/software/CRF-NER.shtml 
keyword-search strategy for dictionary lookups. ? R&D with fuzzy searches. Results from the rules and dictionary lookups component us-ing Lucene?s Fuzzy Query engine for dic-tionary searches. It implements a fuzzy search based on the Levenshtein (edit dis-tance) algorithm9 (Levenshtein, 1966), which has to surpass a similarity threshold in order to produce a match. We carried out a greedy search on the training corpus for the best similarity threshold. We found 0.74 to be the best threshold. ? CRF-based w/FS. The CRF-based predic-tions component results after selecting the best set of features (see Section 3.2.3). The CRF classifier was trained using our 225-document training corpus. ? R&D + CRF w/FS. The cumulative results from the rules and dictionary lookups (not implementing fuzzy dictionary searches) and the CRF-based predictions components. ? R&D + CRF w/FS + FP-filtering. Includes all components together, adding the false-positive filtering component (Section 3.2.4) at the end of the pipeline. The SVM model for this last component was created using our training corpus.  System Prec. Rec. F2 HMS Scrubber 0.150 0.675 0.397 MeDS 0.149 0.768 0.419 MIT deid 0.636 0.893 0.826 MIST 0.865 0.319 0.356 HIDE 0.975 0.376 0.429 Stanford NER 0.692 0.723 0.716 Table 2. ?Out-of-the-box? evaluation of existing de-identification and NER systems (Prec.=precision; Rec.=recall; F2= F2-measure).  System Prec. Rec. F2 Rules & Dictionaries 0.360 0.962 0.721 R&D + fuzzy 0.171 0.969 0.502 CRF-based w/FS 0.979 0.874 0.893 R&D + CRF w/FS 0.360 0.988 0.732 R&D + CRF w/FS + FP-filtering 0.774 0.974 0.926 Table 3. Cumulative results of our pipeline of processes.                                                 9 http://www.merriampark.com/ld.htm 
70
4.3 Analysis Our novel names de-identification pipeline signifi-cantly outperforms all other systems we evaluated ?out-of-the-box? or trained with our VHA notes corpus. Among the five existing systems we evaluated (Table 1), only one achieved noteworthy recall around 89%. However, none of them obtained any remarkable F2-measure.  Most errors produced by the pattern matching systems (i.e., HMS Scrubber, MeDS, and MIT deid system) were due to false positive annotations of medical eponyms (e.g., ?Achilles?, ?Guyon?, etc.), as well as acronyms denoting medical facilities (e.g., ?ER? and ?HCS?). The false negatives consisted of ambigu-ous person names (e.g., ?Bill? and ?Chase?), some formats not covered by the patterns (e.g., ?[Last-Name], [FirstName] [Initial]?), and a few names not found in the dictionaries. Among machine learning-based systems, the two de-identification applications (i.e., MIST and HIDE) obtained good precision, but quite low re-call. The size of our training corpus was somewhat limited, and these results probably indicate a need for more sophisticated learning features, as well as feature selection procedures (rather than using the ?out-of-the-box? feature specification that comes with these systems) for better performance. With improved learning features, we could mitigate the relative lack of training examples. Interestingly, the NER system, which was trained on newswire documents, performed even better than some de-identification systems, although a need for im-provement is still present. We acknowledge that the comparison with Stan-ford NER is not completely fair due to the different source of documents used for training. However, we considered it interesting information, and alt-hough clinical notes contain characteristics not present in newswire corpora, they also have simi-larities regarding person names (e.g., titles ?Mr.?, ?Dr.?, ?PhD?, part-of-speech, verb tenses). There-fore, we think that only for names recognition, a newswire trained NER can provide interesting re-sults, and this was actually what we observed. Table 2 points out that the combination of our components produces successful cumulative re-sults. Using the training corpus to create a simple component made up of rules, dictionary lookups, and few heuristics for disambiguation allowed for 
recall values of 0.96. This demonstrates the need to adapt these techniques to the target documents, instead of employing systems ?out-of-the-box?. Our experiments with fuzzy dictionary lookups did not allow for a significant increase in recall, but caused a decrease in precision (-19%). It sug-gests that there was no need for considering person name misspellings. The component based on CRF predictions alone achieved good performance, especially in preci-sion. It obtained the best F2-measure (0.89), clearly higher than the other ?out-of-the-box? systems based on CRF models. It proves that selecting suit-able learning features mitigates to some extent the scarcity of training examples.  Our next experiment combined the rules and dictionaries and CRF components. It improved the overall recall to about 0.99, which means that CRF-based predictions recognized some person names that were missed by our pattern matching components, but didn?t increase the precision. We reached here our first goal of high recall or sensi-tivity. Finally, we added the false-positive filtering component to our system. This component was able to filter out 622 (84%) false positives from a total of 742, improving the precision to 0.77 (+41%); but also causing a slight decrease in recall (-1.4%). This application of our pipeline was suc-cessful, reaching an F2-mesure of 0.93, and was an effective way of training the SVM model for false-positives filtering.  5 Conclusions We designed and evaluated a novel person names de-identification system with VHA clinical docu-ments. We also presented an ?out-of-the-box? evaluation of several available de-identification and NER systems; all of them were surpassed by our approach. With our proposal, we showed that it is possible to improve the recognition of person names in clin-ical records, even when the corpus for training ma-chine learning classifiers is limited. Furthermore, the workflow of our pipeline allowed us to tackle the de-identification task from an intuitive but powerful perspective, i.e. facing the achievement of high recall and precision as two separate goals implementing specific techniques and components. 
71
Packaging this two-step procedure as a boot-strapping learning or adding the rules to define learning features would not allow us to use the qualities of the R&D and CRF components (i.e., obtain the highest recall by any means). Moreover, considering the small size of our manually anno-tated examples, these approaches would not work much better than existing systems.  As future efforts, we plan to improve the preci-sion of the rules and dictionary lookups component by adding more sophisticated person names disam-biguation procedures. Such procedures should deal with the peculiar formatting of clinical records as well as integrate enriched knowledge from bio-medical resources. We also plan to evaluate the portability of our approach by using other sets of clinical documents, such as the 2006 i2b2 de-identification challenge corpus (Uzuner et al, 2007).  Acknowledgments Funding provided by the Department of Veterans Affairs Health Services Research & Development Services Consortium for Healthcare Informatics Research grant (HIR 08-374).  References  John Aberdeen, Samuel Bayer, Reyyan Yeniterzi, Ben Wellner, Cheryl Clark, David Hanauer, Bradley Ma-lin, and Lynette Hirschman. 2010. The MITRE Iden-tification Scrubber Toolkit: design, training, and assessment. International journal of medical infor-matics, 79 (12) (December): 849-59. Bruce A. Beckwith, Rajeshwarri Mahaadevan, Ulysses J. Balis, and Frank Kuo. 2006. Development and evaluation of an open source software tool for deidentification of pathology reports. BMC medical informatics and decision making, 6 (1) (January): 12. Chih-Chung Chang and Chih-Jen Lin. (2001). LIBSVM: a library for support vector machines. Computer, 1-30. George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The Automatic Content Extraction (ACE) Program - Tasks, Data, and Evaluation. Pro-ceedings of LREC 2004: 837-840. Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sam-
pling. Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics: 363-370. F. Jeff Friedlin and Clement J. McDonald. 2008. A software tool for removing patient identifying infor-mation from clinical documents. Journal of the American Medical Informatics Association??: JAMIA 15 (5) (January 1): 601-10. James Gardner and Li Xiong. 2009. An integrated framework for de-identifying unstructured medical data. Data & Knowledge Engineering 68 (12) (De-cember): 1441-1451. Ralph Grishman and Beth Sundheim. 1996. Message understanding conference-6: A brief history. Pro-ceedings of the 16th conference on Computational linguistics - Volume 1: 466-471. Association for Computational Linguistics, Copenhagen, Denmark. Daniel Jurafsky and James H. Martin. 2009. Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. 2nd edition. Prentice-Hall. Upper Saddle River, NJ, USA. V.I. Levenshtein. 1966. Binary Codes Capable of Cor-recting Deletions, Insertions and Reversals. Soviet Physics - Doklady 10: 707?710. Stephane M. Meystre, F. Jeffrey Friedlin, Brett R. South, Shuying Shen, and Matthew H. Samore. 2010. Automatic de-identification of textual documents in the electronic health record: a review of recent re-search. BMC medical research methodology 10 (1) (January): 70. Ishna Neamatullah, Margaret M. Douglass, Li-wei H. Lehman, Andrew Reisner, Mauricio Villarroel, Wil-liam J. Long, Peter Szolovits, George B. Moody, Roger G. Mark, and Gari D. Clifford. 2008. Auto-mated de-identification of free-text medical records. BMC medical informatics and decision making 8 (1) (January): 32. Guergana K. Savova, James J. Masanz, Philip V. Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-Schuler, and Christopher G. Chute. 2010. Mayo clin-ical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. Journal of the American Medical In-formatics Association??: JAMIA 17 (5): 507-13. Erik F. Tjong Kim Sang, and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Lan-guage-Independent Named Entity Recognition. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003 - Volume 4: 142-147. ?zlem Uzuner, Yuan Luo, and Peter Szolovits. 2007. Evaluating the State-of-the-Art in Automatic De-identification. Journal of the American Medical In-formatics Association??: JAMIA 14(5):550-563. 
72
