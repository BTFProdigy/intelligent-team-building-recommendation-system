Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 334?342,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
It?s Not You, it?s Me: Detecting Flirting and its Misperception in
Speed-Dates
Rajesh Ranganath
Computer Science Department
Stanford University
rajeshr@cs.stanford.edu
Dan Jurafsky
Linguistics Department
Stanford University
jurafsky@stanford.edu
Dan McFarland
School of Education
Stanford University
dmcfarla@stanford.edu
Abstract
Automatically detecting human social in-
tentions from spoken conversation is an
important task for dialogue understand-
ing. Since the social intentions of the
speaker may differ from what is perceived
by the hearer, systems that analyze human
conversations need to be able to extract
both the perceived and the intended social
meaning. We investigate this difference
between intention and perception by using
a spoken corpus of speed-dates in which
both the speaker and the listener rated the
speaker on flirtatiousness. Our flirtation-
detection system uses prosodic, dialogue,
and lexical features to detect a speaker?s
intent to flirt with up to 71.5% accuracy,
significantly outperforming the baseline,
but also outperforming the human inter-
locuters. Our system addresses lexical fea-
ture sparsity given the small amount of
training data by using an autoencoder net-
work to map sparse lexical feature vectors
into 30 compressed features. Our analy-
sis shows that humans are very poor per-
ceivers of intended flirtatiousness, instead
often projecting their own intended behav-
ior onto their interlocutors.
1 Introduction
Detecting human social meaning is a difficult task
for automatic conversational understanding sys-
tems. One cause of this difficulty is the pervasive
difference between intended social signals and the
uptake by the perceiver. The cues that a speaker
may use to attempt to signal a particular social
meaning may not be the cues that the hearer fo-
cuses on, leading to misperception.
In order to understand the impact of this dif-
ference between perception and intention, in this
paper we describe machine learning models that
can detect both the social meaning intended by the
speaker and the social meaning perceived by the
hearer. Automated systems that detect and model
these differences can lead both to richer socially
aware systems for conversational understanding
and more sophisticated analyses of conversational
interactions like meetings and interviews.
This task thus extends the wide literature on
social meaning and its detection, including the
detection of emotions such as annoyance, anger,
sadness, or boredom (Ang et al, 2002; Lee and
Narayanan, 2002; Liscombe et al, 2003), speaker
characteristics such as charisma (Rosenberg and
Hirschberg, 2005), personality features like ex-
troversion or agreeability (Mairesse et al, 2007;
Mairesse and Walker, 2008), speaker depression
or stress (Rude et al, 2004; Pennebaker and Lay,
2002; Cohn et al, 2004), and dating willingness
or liking (Madan et al, 2005; Pentland, 2005).
We chose to work on the domain of flirtation
in speed-dating. Our earlier work on this cor-
pus showed that it is possible to detect whether
speakers are perceived as flirtatious, awkward, or
friendly with reasonable accuracy (Jurafsky et al,
2009). In this paper we extend that work to de-
tect whether speakers themselves intended to flirt,
explore the differences in these variables, and ex-
plore the ability and inability of humans to cor-
rectly perceive the flirtation cues.
While many of the features that we use to build
these detectors are drawn from the previous liter-
ature, we also explore new features. Conventional
methods for lexical feature extraction, for exam-
ple, generally consist of hand coded classes of
words related to concepts like sex or eating (Pen-
nebaker et al, 2007). The classes tend to per-
form well in their specific domains, but may not
be robust across domains, suggesting the need for
unsupervised domain-specific lexical feature ex-
traction. The naive answer to extracting domain-
334
specific lexical features would just be to throw
counts for every word into a huge feature vector,
but the curse of dimensionality rules this method
out in small training set situations. We propose
a new solution to this problem, using an unsuper-
vised deep autoencoder to automatically compress
and extract complex high level lexical features.
2 Dataset
Our experiments make use of the SpeedDate Cor-
pus collected by the third author, and described
in Jurafsky et al (2009). The corpus is based
on three speed-dating sessions run at an Ameri-
can university in 2005, inspired by prior speed-
dating research (Madan et al, 2005). The grad-
uate student participants volunteered to be in the
study and were promised emails of persons with
whom they reported mutual liking. All partici-
pants wore audio recorders on a shoulder sash,
thus resulting in two audio recordings of the ap-
proximately 1100 4-minute dates. Each date was
conducted in an open setting where there was sub-
stantial background noise. This noisy audio was
thus hand-transcribed and turn start and end were
hand-aligned with the audio. In addition to the au-
dio, the corpus includes various attitude and de-
mographic questions answered by the participants.
Each speaker was also asked to report how of-
ten their date?s speech reflected different conver-
sational styles (awkward, flirtatious, funny, as-
sertive) on a scale of 1-10 (1=never, 10=con-
stantly): ?How often did the other person behave
in the following ways on this ?date???. In addition
they were also asked to rate their own intentions:
?How often did you behave in the following ways
on this ?date??? on a scale of 1-10.
In this study, we focus on the flirtation ratings,
examining how often each participant said they
were flirting, as well as how often each participant
was judged by the interlocutor as flirting.
Of the original 1100 dates only 991 total dates
are in the SpeedDate corpus due to various losses
during recording or processing. The current study
focuses on 946 of these, for which we have com-
plete audio, transcript, and survey information.
3 Experiment
To understand how the perception of flirting dif-
fers from the intention of flirting, we trained bi-
nary classifiers to predict both perception and in-
tention. In each date, the speaker and the inter-
locutor both labeled the speaker?s behavioral traits
on a Likert scale from 1-10. To generate binary
responses we took the top ten percent of Likert
ratings in each task and labeled those as positive
examples. We similarly took the bottom ten per-
cent of Likert ratings and labeled those as negative
examples. We ran our binary classification exper-
iments to predict this output variable. Our experi-
ments were split by gender. For the female exper-
iment the speaker was female and the interlocu-
tor was male, while for the male experiment the
speaker was male and the interlocutor was female.
For each speaker side of each 4-minute conver-
sation, we extracted features from wavefiles and
transcripts, as described in the next section. We
then trained four separate binary classifiers (for
each gender for both perception and intention).
4 Feature Descriptions
We used the features reported by Jurafsky et
al. (2009), which are briefly summarized here.
The features for a conversation side thus indicate
whether a speaker who talks a lot, laughs, is more
disfluent, has higher F0, etc., is more or less likely
to consider themselves flirtatious, or be considered
flirtatious by the interlocutor. We also computed
the same features for the alter interlocutor. Al-
ter features thus indicate the conversational behav-
ior of the speaker talking with an interlocutor they
considered to be flirtatious or not.
4.1 Prosodic Features
F0 and RMS amplitude features were extracted us-
ing Praat scripts (Boersma and Weenink, 2005).
Since the start and end of each turn were time-
marked by hand, each feature was easily extracted
over a turn, and then averages and standard devia-
tions were taken over the turns in an entire conver-
sation side. Thus the feature F0 MIN for a conver-
sation side was computed by taking the F0 min of
each turn in that side (not counting zero values of
F0), and then averaging these values over all turns
in the side. F0 MIN SD is the standard deviation
across turns of this same measure.
4.2 Dialogue and Disfluency Features
A number of discourse features were extracted,
following Jurafsky et al (2009) and the dialogue
literature. The dialog acts shown in Table 2
were detected by hand-built regular expressions,
based on analyses of the dialogue acts in the
335
F0 MIN minimum (non-zero) F0 per turn, averaged
over turns
F0 MIN SD standard deviation from F0 min
F0 MAX maximum F0 per turn, averaged over turns
F0 MAX SD standard deviation from F0 max
F0 MEAN mean F0 per turn, averaged over turns
F0 MEAN SD standard deviation (across turns) from F0
mean
F0 SD standard deviation (within a turn) from F0
mean, averaged over turns
F0 SD SD standard deviation from the f0 sd
PITCH RANGE f0 max - f0 min per turn, averaged over
turns
PITCH RANGE SD standard deviation from mean pitch range
RMS MIN minimum amplitude per turn, averaged
over turns
RMS MIN SD standard deviation from RMS min
RMS MAX maximum amplitude per turn, averaged
over turns
RMS MAX SD standard deviation from RMS max
RMS MEAN mean amplitude per turn, averaged over
turns
RMS MEAN SD standard deviation from RMS mean
TURN DUR duration of turn in seconds, averaged over
turns
TIME total time for a speaker for a conversation
side, in seconds
RATE OF
SPEECH
number of words in turn divided by dura-
tion of turn in seconds, averaged over turns
Table 1: Prosodic features from Jurafsky et al
(2009) for each conversation side, extracted using
Praat from the hand-segmented turns of each side.
hand-labeled Switchboard corpus of dialog acts.
Collaborative completions, turns where a speaker
completes the utterance begun by the alter, were
detected by finding sentences for which the first
word of the speaker was extremely predictable
from the last two words of the previous speaker,
based on a trigram grammar trained on the Tree-
bank 3 Switchboard transcripts. Laughter, disflu-
encies, and overlap were all marked in the tran-
scripts by the transcribers.
4.3 Lexical Features
We drew our lexical features from the LIWC lex-
icons of Pennebaker et al (2007), the standard
for social psychological analysis of lexical fea-
tures. We chose ten LIWC categories that have
proven useful in detecting personality-related fea-
tures (Mairesse et al, 2007): Anger, Assent, In-
gest, Insight, Negemotion, Sexual, Swear, I, We,
and You. We also added two new lexical features:
?past tense auxiliary?, a heuristic for automati-
cally detecting narrative or story-telling behavior,
and Metadate, for discussion about the speed-date
itself. The features are summarized in Table 3.
4.4 Inducing New Lexical Features
In Jurafsky et al (2009) we found the LIWC lex-
ical features less useful in detecting social mean-
ing than the dialogue and prosodic features, per-
haps because lexical cues to flirtation lie in differ-
ent classes of words than previously investigated.
We therefore investigated the induction of lexical
features from the speed-date corpus, using a prob-
abilisitic graphical model.
We began with a pilot investigation to see
whether lexical cues were likely to be useful; with
a small corpus, it is possible that lexical fea-
tures are simply too sparse to play a role given
the limited data. The pilot was based on us-
ing Naive Bayes with word existence features (bi-
nomial Naive Bayes). Naive Bayes assumes all
features are conditionally independent given the
class, and is known to perform well with small
amounts of data (Rish, 2001). Our Naive Bayes
pilot system performed above chance, suggesting
that lexical cues are indeed informative.
A simple approach to including lexical fea-
tures in our more general classification system
would be to include the word counts in a high di-
mensional feature vector with our other features.
This method, unfortunately, would suffer from
the well-known high dimensionality/small train-
ing set problem. We propose a method for build-
ing a much smaller number of features that would
nonetheless capture lexical information. Our ap-
proach is based on using autoencoders to con-
struct high level lower dimension features from the
words in a nonlinear manner.
A deep autoencoder is a hierarchichal graphical
model with multiple layers. Each layer consists of
a number of units. The input layer has the same
number of units as the output layer, where the out-
put layer is the model?s reconstruction of the input
layer. The number of units in the intermediate lay-
ers tends to get progressively smaller to produce a
compact representation.
We defined our autoencoder with visible units
modeling the probabilities of the 1000 most com-
mon words in the conversation for the speaker
and the probabilities of the 1000 most common
words for the interlocutor (after first removing
a stop list of the most common words). We
train a deep autoencoder with stochastic nonlin-
ear feature detectors and linear feature detectors
in the final layer. As shown in Figure 1, we used
a 2000-1000-500-250-30 autoencoder. Autoen-
336
BACKCHANNELS number of backchannel utterances in side (Uh-huh., Yeah., Right., Oh, okay.)
APPRECIATIONS number of appreciations in side (Wow, That?s true, Oh, great)
QUESTIONS number of questions in side
NTRI repair question (Next Turn Repair Indicator) (Wait, Excuse me)
COMPLETION (an approximation to) utterances that were ?collaborative completions?
LAUGH number of instances of laughter in side
TURNS total number of turns in side
DISPREFERRED (approximation to) dispreferred responses, beginning with discourse marker well
UH/UM total number of filled pauses (uh or um) in conversation side
RESTART total number of disfluent restarts in conversation side
OVERLAP number of turns in side which the two speakers overlapped
Table 2: Dialog act and disfluency features from Jurafsky et al (2009).
TOTAL WORDS total number of words
PAST TENSE uses of past tense auxiliaries was, were, had
METADATE horn, date, bell, survey, speed, form, questionnaire, rushed, study, research
YOU you, you?d, you?ll, your, you?re, yours, you?ve (not counting you know)
WE lets, let?s, our, ours, ourselves, us, we, we?d, we?ll, we?re, we?ve
I I?d, I?ll, I?m, I?ve, me, mine, my, myself (not counting I mean)
ASSENT yeah, okay, cool, yes, awesome, absolutely, agree
SWEAR hell, sucks, damn, crap, shit, screw, heck, fuck*
INSIGHT think*/thought, feel*/felt, find/found, understand*, figure*, idea*, imagine, wonder
ANGER hate/hated, hell, ridiculous*, stupid, kill*, screwed, blame, sucks, mad, bother, shit
NEGEMOTION bad, weird, hate, crazy, problem*, difficult, tough, awkward, boring, wrong, sad, worry,
SEXUAL love*, passion*, virgin, sex, screw
INGEST food, eat*, water, bar/bars, drink*, cook*, dinner, coffee, wine, beer, restaurant, lunch, dish
Table 3: Lexical features from Jurafsky et al (2009). Each feature value is a total count of the words in
that class for each conversation side; asterisks indicate including suffixed forms (e.g., love, loves, loving).
All except the first three are from LIWC (Pennebaker et al, 2007) (modified slightly, e.g., by removing
you know and I mean). The last five classes include more words in addition to those shown.
coders tend to perform poorly if they are initialized
incorrectly, so we use the Restricted Boltzmann
Machine (RBM) pretraining procedure described
in Hinton and Salakhutdinov (2006) to initialize
the encoder. Each individual RBM is trained using
contrastive divergence as an update rule which has
been shown to produce reasonable results quickly
(Hinton et al, 2006). Finally, we use backpropa-
gation to fine tune the weights of our encoder by
minimizing the cross entropy error. To extract fea-
tures from each conversation, we sample the code
layer (30 unit layer in our encoder) with the visi-
ble units corresponding to the most common word
probabilities from that document, creating 30 new
features that we can use for classification. The
conditional distributions of the first layer features
can be given by the softmax of the activations for
each gender:
p(v
i
|h) =
exp(bias
i
+
?
j
h
j
? w
ij
)
?
k?K
exp(bias
k
+
?
j
v
j
? w
kj
)
(1)
p(h
j
|v) =
1
1 + exp(bias
(
j) +
?
i
v
i
? w
ij
)
(2)
where K is the set of all the units representing the
same speaker as i
1
, v
i
is the ith visible unit, h
j
is
the jth hidden unit, w
ij
is the weight between visi-
ble unit i and hidden unit j, and bias
m
is the offset
of unit m. Intuitively, this means that the proba-
bility that a hidden unit is activated by the visible
layer is sigmoid of the weighted sum of all the vis-
ible units plus the unit?s bias term. Similarly, the
visible units are activated through a weighted sum
of the hidden units, but they undergo an additional
normalization (softmax) over all the other visible
units from the speaker to effectively model the
multinomial distribution from each speaker. Since
in a RBM hidden units are conditionally indepen-
dent given the visible units, and visible units are
1
The visible unit i models word probabilities of either the
speaker or the interlocutor, so the softmax is done over the
distribution of words for the speaker that unit i is modeling.
337
conditionally independent given hidden layer, the
above equations completely specify the first layer
of the model.
To account for the fact that each visible unit in
the first layer contained 1000 observations from
the underlying distribution we upweighted our fea-
tures by that factor. During pretraining the ?train-
ing data? for the higher layers is the activation
probabilities of the hidden units of layer directly
below when driven by that layer?s input data. The
intermediate layers in the model are symmetric
where the activation probabilities for both the vis-
ible and hidden units are of the same form as
p(h
j
|v) in layer 1. To produce real valued features
in the code layer we used linear hidden units. In
addition to the likelihood portion of the objective
we penalized large weights by using l2 regulariza-
tion and penalize all weights by applying a small
constant weight cost that gets applied at every up-
date. After training to find a good initial point
for the autoencoder we unroll the weights and use
backpropogation to fine tune our autoencoder.
While interpreting high level nonlinear features
can be challenging, we did a pilot analysis of one
of the 30 features fixing a large (positive or neg-
ative) weight on the feature unit (code layer) and
sampling the output units.
The top weighted words for a positive weight
are: O did, O live, S did, S friends, S went,
O live, S lot, S wait, O two, and O wasn?t (S for
speaker and O for interlocutor). The top weighted
words for a negative weight are: S long, O school,
S school, S phd, O years, S years, O stanford,
S lot, O research, O interesting and O education.
At least for this one feature, a large positive value
seemed to indicate the prevalence of questions
(wait, did) or storytelling (
em live, wasn?t). A large negative weight indicates
the conversation focused on the mundane details
of grad student life.
5 Classification
Before performing the classification task, we pre-
processed the data in two ways. First, we stan-
dardized all the variables to have zero mean and
unit variance. We did this to avoid imposing a
prior on any of the features based on their numer-
ical values. Consider a feature A with mean 100
and a feature B with mean .1 where A and B are
correlated with the output. Since the SVM prob-
lem minimizes the norm of the weight vector, there
2000
2000
1000
500
500
250
250
30
1000
2000
1000
500
500
250
250
30
1000
2000
1000
500
500
250
250
30
1000
Dialogue:
F: ...
M: ...
F: ...
Dialogue: 
F: ...
M: ...
F: ...
Diaogue:
F: ...
M: ...
F: ...
Reconstruct
Dialogue:
F: ...
M: ...
Reconstruct
Dialogue:
F: ...
M: ...
W
1
W
2
W
3
W
4
W
5
W
1
W
2
W
3
W
4
W
T
5
W
T
4
W
5
2000 2000
W
T
3
W
T
2
RBM
RBM
RBM
 RBM
Code layer
Decoder
Encoder
Pretraining
Unrolling
Fine-tuning
W
T
1
W
1
+?
1
W
2
+?
2
W
3
+?
3
W
4
+?
4
W
5
+?
5
W
T
5
+?
6
W
T
4
+?
7
W
T
3
+?
8
W
T
2
+?
9
W
T
1
+?
10
Figure 1: Pretraining is a fully unsupervised pro-
cedure that trains an RBM at each layer. Once the
pretraining of one layer is complete, the top layer
units are used as input to the next layer. We then
fine-tune our weights using backprop. The 30 fea-
tures are extracted from the code layer.
is a bias to put weight on feature A because intu-
itively the weight on feature B would need to be
1000 times larger to carry the same effect. This
argument holds similarly for the reduction to unit
variance. Second, we removed features correlated
greater than .7. One goal of removing correlated
features was to remove as much colinearity as pos-
sible from the regression so that the regression
weights could be ranked for their importance in the
classification. In addition, we hoped to improve
classification because a large number of features
require more training examples (Ng, 2004). For
example for perception of female flirt we removed
the number of turns by the alter (O turns) and the
number of sentence from the ego (S sentences) be-
cause they were highly correlated with S turns.
To ensure comparisons (see Section 7) between
the interlocutors? ratings and our classifier (and
because of our small dataset) we use k-fold cross
validation to learn our model and evaluate our
model. We train our binary model with the top
ten percent of ratings labeled as positive class ex-
amples and bottom ten percent of ratings as the
negative class examples. We used five-fold cross
validation in which the data is split into five equal
folds of size 40. We used four of the folds for
training and one for test. K-fold cross validation
does this in a round robin manner so every exam-
338
ple ends up in the test set. This yields a datasplit
of 160 training examples and 40 test examples. To
ensure that we were not learning something spe-
cific to our data split, we randomized our data or-
dering.
For classification we used a support vector ma-
chine (SVM). SVMs generally do not produce ex-
plicit feature weights for analysis because they are
a kernelized classifier. We solved the linear C-
SVM problem. Normally the problem is solved
in the dual form, but to facilitate feature analysis
we expand back to the primal form to retrieve w,
the weight vector. Our goal in the C-SVM is to
solve, in primal form,
min
?,w,b
1
2
||w||
2
+ C
m
?
i=1
?
i
s.t. y
(i)
(w
T
x
(i)
+ b) ? 1? ?
i
, i = 1, . . . ,m
?
i
? 0, i = 1, . . . ,m (3)
where m is the number of training examples, x
(i)
is the ith training examples, and y
(i)
is the ith class
(1 for the positive class, -1 for the negative class).
The ?
i
are the slack variables that allow this algo-
rithm to work for non linearly separable datasets.
A test example is classified by looking at the
sign of y(x) = w
T
x
(test)
+ b. To explore mod-
els that captured interactions, but do not allow for
direct feature analysis we solved the C-SVM prob-
lem using a radial basis function (RBF) as a kernel
(Scholkopf et al, 1997). Our RBF kernel is based
on a Gaussian with unit variance.
K(x
(i)
, x
(j)
) = exp(
?||x
(i)
? x
(j)
||
2
2?
) (4)
In this case predictions can be made by looking
at y(x
(test)
) =
?
m
i=1
?
(i)
y
(i)
rbf(x
(i)
, t
(test)
) + b,
where each ?
(i)
, for i = 1, . . . ,m is a member
of the set of dual variables that comes from trans-
forming the primal form into the dual form. The
SVM kernel trick allows us to explore higher di-
mensions while limiting the curse of dimensional-
ity that plagues small datasets like ours.
We evaluated both our linear C-SVM and our
radial basis function C-SVM using parameters
learned on the training sets by computing the ac-
curacy on the test set. Accuracy is the number of
correct examples / total number of test examples.
We found that the RBM classifier that handled in-
teraction terms outperformed linear methods like
logistic regression.
For feature weight extraction we aggregated the
feature weights calculated from each of the test
folds by taking the mean between them.
2
6 Results
We report in Table 4 the results for detecting flirt
intention (whether a speaker said they were flirt-
ing) as well as flirt perception (whether the listener
said the speaker was flirting).
Flirt Intention Flirt Perception
by M by F of M of F
RBM SVM 61.5% 70.0% 77.0% 59.5%
+autoencoder 69.0% 71.5% 79.5% 68.0%
features
Table 4: Accuracy of binary classification of each
conversation side, where chance is 50%. The first
row uses all the Jurafsky et al (2009) features for
both the speaker and interlocutor. The second row
adds the new autoencoder features.
In our earlier study of flirt perception, we
achieved 71% accuracy for men and 60% for
women (Jurafsky et al, 2009). Our current num-
bers for flirt perception are much better for both
men (79.5%), and women (68.0%). The improve-
ment is due both to the new autoencoder features
and the RBF kernel that considers feature inter-
actions (feature interactions were not included in
the logistic regression classifiers of Jurafsky et al
(2009)).
Our number for flirt intention are 69.0% for men
and 71.5% for women. Note that our accuracies
are better for detecting women?s intentions as well
as women?s perceptions (of men) than men?s in-
tentions and perceptions.
7 Feature Analysis
We first considered the features that helped clas-
sification of flirt intention. Table 5 shows feature
weights for the features (features were normed so
weights are comparable), and is summarized in the
following paragraphs:
? Men who say they are flirting ask more ques-
tions, and use more you and we. They laugh more,
and use more sexual, anger, and negative emo-
tional words. Prosodically they speak faster, with
higher pitch, but quieter (lower intensity min).
2
We could not use the zero median criteria used in Juraf-
sky et al (2009) because C-SVMs under the l-2 metric pro-
vide no sparse weight guarantees.
339
FEMALE FLIRT MALE FLIRT
O backchannel -0.0369 S you 0.0279
S appreciation -0.0327 S negemotion 0.0249
O appreciation -0.0281 S we 0.0236
O question 0.0265 S anger 0.0190
O avimin -0.0249 S sexual 0.0184
S turns -0.0247 O negemotion 0.0180
S backchannel -0.0245 O avpmax 0.0174
O you 0.0239 O swear 0.0172
S avtndur 0.0229 O laugh 0.0164
S avpmin -0.0227 O wordcount 0.0151
O rate 0.0212 S laugh 0.0144
S laugh 0.0204 S rate 0.0143
S wordcount 0.0192 S well 0.0131
S well 0.0192 S question 0.0131
O negemotion 0.019 O sexual 0.0128
S repair q 0.0188 S completion 0.0128
O sexual 0.0176 S avpmax 0.011
O overlap -0.0176 O completion 0.010
O sdpmean 0.0171 O sdimin 0.010
O avimax -0.0151 O metatalk -0.012
S avpmean -0.015 S sdpsd -0.015
S question -0.0146 S avimin -0.015
O sdimin 0.0136 S backchannel -0.022
S avpmax 0.0131
S we -0.013
S I 0.0117
S assent 0.0114
S metatalk -0.0107
S sexual 0.0105
S avimin -0.0104
O uh -0.0102
Table 5: Feature weights (mean weights of the ran-
domized runs) for the predictors with |weight| >
0.01 for the male and female classifiers. An S pre-
fix indicates features of the speaker (the candidate
flirter) while an O prefix indicates features of the
other. Weights for autoencoder features were also
significant but are omitted for compactness.
Features of the alter (the woman) that helped
our system detect men who say they are flirting
include the woman?s laughing, sexual words or
swear words, talking more, and having a higher
f0 (max).
?Women who say they are flirting have a much
expanded pitch range (lower pitch min, higher
pitch max), laugh more, use more I and well, use
repair questions but not other kinds of questions,
use more sexual terms, use far less appreciations
and backchannels, and use fewer, longer turns,
with more words in general. Features of the alter
(the man) that helped our system detect women
who say they are flirting include the male use of
you, questions, and faster and quieter speech.
We also summarize here the features for the per-
ception classification task; predicting which peo-
ple will be labeled by their dates as flirting. Here
the task is the same as for Jurafsky et al (2009)
and the values are similar.
? Men who are labeled by their female date as
flirting present many of the same linguistic behav-
iors as when they express their intention to flirt.
Some of the largest differences are that men are
perceived to flirt when they use less appreciations
and overlap less, while these features were not sig-
nificant for men who said they were flirting. We
also found that fast speech and more questions are
more important features for flirtation perception
than intention.
? Women who are labeled by their male date
as flirting also present much of the same linguis-
tic behavior as women who intend to flirt. Laugh-
ter, repair questions, and taking fewer, longer turns
were not predictors of women labeled as flirting,
although these were strong predictors of women
intending to flirt.
Both genders convey intended flirtation by
laughing more, speaking faster, and using higher
pitch. However, we do find gender differences;
men ask more questions when they say they are
flirting, women ask fewer, although they do use
more repair questions, which men do not. Women
use more ?I? and less ?we?; men use more ?we?
and ?you?. Men labeled as flirting are softer, but
women labeled as flirting are not. Women flirting
use much fewer appreciations; appreciations were
not a significant factor in men flirting.
8 Human Performance on this task
To evaluate the performance of our classifiers we
compare against human labeled data.
We used the same test set as for our machine
classifier; recall that this was created by taking the
top ten percent of Likert ratings of the speaker?s
intention ratings by gender and called those posi-
tive for flirtation intention. We constructed nega-
tive examples by taking the bottom ten percent of
intention Likert ratings. We called the interlocu-
tor correct on the positive examples if the inter-
locutor?s rating was greater than 5. Symmetrically
for the negative examples, we said the interlocutor
was correct if their rating was less than or equal
to 5. Note that this metric is biased somewhat to-
ward the humans and against our systems, because
we do not penalize for intermediate values, while
the system is trained to make binary predictions
only on extremes. The results of the human per-
ceivers on classifying flirtation intent are shown in
Table 6.
340
Male speaker Female speaker
(Female perceiver) (Male perceiver)
62.2% 56.2%
Table 6: Accuracy of human listeners at labeling
speakers as flirting or not.
We were quite surprised by the poor quality of
the human results. Our system outperforms both
men?s performance in detecting women flirters
(system 71.5% versus human 56.2%) and also
women?s performance in detecting male flirters
(system 69.0% versus human 62.2%).
Why are humans worse than machines at detect-
ing flirtation? We found a key insight by examin-
ing how the participants in a date label themselves
and each other. Table 7 shows the 1-10 Likert val-
ues for the two participants in one of the dates,
between Male 101 and Female 127. The two par-
ticipants clearly had very different perspectives on
the date. More important, however, we see that
each participant labels their own flirting (almost)
identically with their partner?s flirting.
I am flirting Other is flirting
Male 101 says: 8 7
Female 127 says: 1 1
Table 7: Likert scores for the date between Female
127 and Male 101.
We therefore asked whether speakers in general
tend to assign similar values to their own flirting
and their partner?s flirting. The Pearson correla-
tion coefficient between these two variables (my
perception of my own flirting, and my perception
of other?s flirting) is .73. By contrast, the poor per-
formance of subjects at detecting flirting in their
partners is coherent with the lower (.15) correla-
tion coefficient between those two variables (my
perception of the other?s flirting, and the other?s
perception of their own flirting). This discrepancy
is summarized in boldface in Table 8.
Since the speed-date data was also labeled for
three other variables, we then asked the same
question about these variables. As Table 8 shows,
for all four styles, speakers? perception of others
is strongly correlated with the speakers? percep-
tion of themselves, far more so than with what the
others actually think they are doing.
3
3
This was true no matter how the correlations were run,
whether with raw Likert values, with ego-centered (trans-
formed) values and with self ego-centered but other raw.
Variable Self-perceive-Other
& Self-perceive-Self
Self-perceive-Other &
Other-perceive-Other
Flirting .73 .15
Friendly .77 .05
Awkward .58 .07
Assertive .58 .09
Table 8: Correlations between speaker intentions
and perception for all four styles.
Note that although perception of the other does
not correlate highly with the other?s intent for any
of the styles, the correlations are somewhat bet-
ter (.15) for flirting, perhaps because in the speed-
date setting speakers are focusing more on detect-
ing this behavior (Higgins and Bargh, 1987). It is
also possible that for styles with positive valence
(friendliness and flirting) speakers see more simi-
larity between the self and the other than for nega-
tive styles (awkward and assertive) (Krah?e, 1983).
Why should this strong bias exist to link self-
flirting with perception of the other? One pos-
sibility is that speakers are just not very good at
capturing the intentions of others in four minutes.
Speakers instead base their judgments on their
own behavior or intentions, perhaps because of a
bias to maintain consistency in attitudes and rela-
tions (Festinger, 1957; Taylor, 1970) or to assume
there is reciprocation in interpersonal perceptions
(Kenny, 1998).
9 Conclusion
We have presented a new system that is able to
predict flirtation intention better than humans can,
despite humans having access to vastly richer in-
formation (visual features, gesture, etc.). This sys-
tem facilitates the analysis of human perception
and human interaction and provides a framework
for understanding why humans perform so poorly
on intention prediction.
At the heart of our system is a core set of
prosodic, dialogue, and lexical features that al-
low for accurate prediction of both flirtation inten-
tion and flirtation perception. Since previous word
lists don?t capture sufficient lexical information,
we used an autoencoder to automatically capture
new lexical cues. The autoencoder shows potential
for being a promising feature extraction method
for social tasks where cues are domain specific.
Acknowledgments: Thanks to the anonymous review-
ers and to a Google Research Award for partial funding.
341
References
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and
A. Stolcke. 2002. Prosody-Based Automatic De-
tection of Annoyance and Frustration in Human-
Computer Dialog. In INTERSPEECH-02.
Paul Boersma and David Weenink. 2005. Praat: doing
phonetics by computer (version 4.3.14). [Computer
program]. Retrieved May 26, 2005, from http://
www.praat.org/.
M. A. Cohn, M. R. Mehl, and J. W. Pennebaker.
2004. Linguistic markers of psychological change
surrounding September 11, 2001. Psychological
Science, 15:687?693.
Leon Festinger. 1957. A Theory of Cognitive Disso-
nance. Row, Peterson, Evanston, IL.
E. Tory Higgins and John A. Bargh. 1987. Social cog-
nition and social perception. Annual Review of Psy-
chology, 38:369?425.
G. E. Hinton and R. R Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
G. E. Hinton, S. Osindero, and Y. Teh. 2006. A
fast learning algorithm for deep belief nets. Neural
Computation, 18:1527?1554.
Dan Jurafsky, Rajesh Ranganath, and Dan McFarland.
2009. Extracting social meaning: Identifying inter-
actional style in spoken conversation. In NAACL
HLT 2009, Boulder, CO.
David Kenny. 1998. Interpersonal Perception: A So-
cial Relations Analysis. Guilford Press, New York,
NY.
B. Krah?e. 1983. Self-serving biases in perceived simi-
larity and causal attributions of other people?s per-
formance. Social Psychology Quarterly, 46:318?
329.
C. M. Lee and Shrikanth S. Narayanan. 2002. Com-
bining acoustic and language information for emo-
tion recognition. In ICSLP-02, pages 873?876,
Denver, CO.
Jackson Liscombe, Jennifer Venditti, and Julia
Hirschberg. 2003. Classifying Subject Ratings
of Emotional Speech Using Acoustic Features. In
INTERSPEECH-03.
Anmol Madan, Ron Caneel, and Alex Pentland. 2005.
Voices of attraction. Presented at Augmented Cog-
nition, HCI 2005, Las Vegas.
Franc?ois Mairesse and Marilyn Walker. 2008. Train-
able generation of big-five personality styles through
data-driven parameter estimation. In ACL-08,
Columbus.
Franc?ois Mairesse, Marilyn Walker, Matthias Mehl,
and Roger Moore. 2007. Using linguistic cues for
the automatic recognition of personality in conver-
sation and text. Journal of Artificial Intelligence Re-
search (JAIR), 30:457?500.
Andrew Y. Ng. 2004. Feature selection, L1 vs. L2
regularization, and rotational invariance. In ICML
2004.
J. W. Pennebaker and T. C. Lay. 2002. Language use
and personality during crises: Analyses of Mayor
Rudolph Giuliani?s press conferences. Journal of
Research in Personality, 36:271?282.
J. W. Pennebaker, R.E. Booth, and M.E. Francis. 2007.
Linguistic inquiry and word count: LIWC2007 op-
erator?s manual. Technical report, University of
Texas.
Alex Pentland. 2005. Socially aware computation and
communication. Computer, pages 63?70.
Irina Rish. 2001. An empirical study of the naive
bayes classifier. In IJCAI 2001 Workshop on Em-
pirical Methods in Artificial Intelligence.
Andrew Rosenberg and Julia Hirschberg. 2005.
Acoustic/prosodic and lexical correlates of charis-
matic speech. In EUROSPEECH-05, pages 513?
516, Lisbon, Portugal.
S. S. Rude, E. M. Gortner, and J. W. Pennebaker.
2004. Language use of depressed and depression-
vulnerable college students. Cognition and Emo-
tion, 18:1121?1133.
B. Scholkopf, K.K. Sung, CJC Burges, F. Girosi,
P. Niyogi, T. Poggio, and V. Vapnik. 1997. Com-
paring support vector machines with Gaussian ker-
nels to radialbasis function classifiers. IEEE Trans-
actions on Signal Processing, 45(11):2758?2765.
Howard Taylor. 1970. Chapter 2. In Balance in
Small Groups. Von Nostrand Reinhold Company,
New York, NY.
342
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 638?646,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Extracting Social Meaning: Identifying Interactional Style in Spoken
Conversation
Dan Jurafsky
Linguistics Department
Stanford University
jurafsky@stanford.edu
Rajesh Ranganath
Computer Science Department
Stanford University
rajeshr@cs.stanford.edu
Dan McFarland
School of Education
Stanford University
dmcfarla@stanford.edu
Abstract
Automatically extracting social meaning and
intention from spoken dialogue is an impor-
tant task for dialogue systems and social com-
puting. We describe a system for detecting
elements of interactional style: whether a
speaker is awkward, friendly, or flirtatious.
We create and use a new spoken corpus of 991
4-minute speed-dates. Participants rated their
interlocutors for these elements of style. Us-
ing rich dialogue, lexical, and prosodic fea-
tures, we are able to detect flirtatious, awk-
ward, and friendly styles in noisy natural con-
versational data with up to 75% accuracy,
compared to a 50% baseline. We describe sim-
ple ways to extract relatively rich dialogue fea-
tures, and analyze which features performed
similarly for men and women and which were
gender-specific.
1 Introduction
How can we extract social meaning from speech, de-
ciding if a speaker is particularly engaged in the con-
versation, is uncomfortable or awkward, or is partic-
ularly friendly and flirtatious? Understanding these
meanings and how they are signaled in language is
an important sociolinguistic task in itself. Extracting
them automatically from dialogue speech and text
is crucial for developing socially aware computing
systems for tasks such as detection of interactional
problems or matching conversational style, and will
play an important role in creating more natural dia-
logue agents (Pentland, 2005; Nass and Brave, 2005;
Brave et al, 2005).
Cues for social meaning permeate speech at every
level of linguistic structure. Acoustic cues such as
low and high F0 or energy and spectral tilt are impor-
tant in detecting emotions such as annoyance, anger,
sadness, or boredom (Ang et al, 2002; Lee and
Narayanan, 2002; Liscombe et al, 2003), speaker
characteristics such as charisma (Rosenberg and
Hirschberg, 2005), or personality features like extro-
version (Mairesse et al, 2007; Mairesse and Walker,
2008). Lexical cues to social meaning abound.
Speakers with links to depression or speakers who
are under stress use more first person singular pro-
nouns (Rude et al, 2004; Pennebaker and Lay, 2002;
Cohn et al, 2004), positive emotion words are cues
to agreeableness (Mairesse et al, 2007), and neg-
ative emotion words are useful cues to deceptive
speech (Newman et al, 2003). The number of words
in a sentence can be a useful feature for extroverted
personality (Mairesse et al, 2007). Finally, dia-
log features such as the presence of disfluencies
can inform listeners about speakers? problems in ut-
terance planning or about confidence (Brennan and
Williams, 1995; Brennan and Schober, 2001).
Our goal is to see whether cues of this sort are
useful in detecting particular elements of conversa-
tional style and social intention; whether a speaker
in a speed-dating conversation is judged by the in-
terlocutor as friendly, awkward, or flirtatious.
2 The Corpus
Our experiments make use of a new corpus we have
collected, the SpeedDate Corpus. The corpus is
based on three speed-dating sessions run at an elite
638
private American university in 2005 and inspired
by prior speed-dating research (Madan et al, 2005;
Pentland, 2005). The graduate student participants
volunteered to be in the study and were promised
emails of persons with whom they reported mutual
liking. Each date was conducted in an open setting
where there was substantial background noise. All
participants wore audio recorders on a shoulder sash,
thus resulting in two audio recordings of the approx-
imately 1100 4-minute dates. In addition to the au-
dio, we collected pre-test surveys, event scorecards,
and post-test surveys. This is the largest sample we
know of where audio data and detailed survey infor-
mation were combined in a natural experiment.
The rich survey information included date per-
ceptions and follow-up interest, as well as gen-
eral attitudes, preferences, and demographic infor-
mation. Participants were also asked about the
conversational style and intention of the interlocu-
tor. Each speaker was asked to report how of-
ten their date?s speech reflected different conversa-
tional styles (awkward, friendly, flirtatious, funny,
assertive) on a scale of 1-10 (1=never, 10=con-
stantly): ?How often did the other person behave in
the following ways on this ?date???. We chose three
of these five to focus on in this paper.
We acquired acoustic information by taking the
acoustic wave file from each recorder and manually
segmenting it into a sequence of wavefiles, each cor-
responding to one 4-minute date. Since both speak-
ers wore microphones, most dates had two record-
ings, one from the male recorder and one from the
female recorder. Because of mechanical, opera-
tor, and experimenter errors, some recordings were
lost, and thus some dates had only one recording.
Transcribers at a professional transcription service
used the two recordings to create a transcript for
each date, and time-stamped the start and end time
of each speaker turn. Transcribers were instructed
to mark various disfluencies as well as some non-
verbal elements of the conversation such as laughter.
Because of noise, participants who accidentally
turned off their mikes, and some segmentation and
transcription errors, a number of dates were not pos-
sible to analyze. 19 dates were lost completely, and
for an additional 130 we lost one of the two audio
tracks and had to use the remaining track to extract
features for both interlocutors. The current study fo-
cuses on the 991 remaining clean dates for which
we had usable audio, transcripts, and survey infor-
mation.
3 The Experiments
Our goal is to detect three of the style variables, in
particular awkward, friendly, or flirtatious speakers,
via a machine learning classifier. Recall that each
speaker in a date (each conversation side) was la-
beled by his or her interlocutor with a rating from
1-10 for awkward, friendly, or flirtatious behavior.
For the experiments, the 1-10 Likert scale ratings
were first mean-centered within each respondent so
that the average was 0. Then the top ten percent of
the respondent-centered meaned Likert ratings were
marked as positive for the trait, and the bottom ten
percent were marked as negative for a trait. Thus
each respondent labels the other speaker as either
positive, negative, or NA for each of the three traits.
We run our binary classification experiments to
predict this output variable.
For each speaker side of each 4-minute conversa-
tion, we extracted features from the wavefiles and
the transcript, as described in the next section. We
then trained six separate binary classifiers (for each
gender for the 3 tasks), as described in Section 5.
4 Feature Extraction
In selecting features we drew on previous research
on the use of relatively simple surface features that
cue social meaning, described in the next sections.
Each date was represented by the two 4-minute
wavefiles, one from the recorder worn by each
speaker, and a single transcription. Because of the
very high level of noise, the speaker wearing the
recorder was much clearer on his/her own recording,
and so we extracted the acoustic features for each
speaker from their own microphone (except for the
130 dates for which we only had one audio file). All
lexical and discourse features were extracted from
the transcripts.
All features describe the speaker of the conversa-
tion side being labeled for style. The features for
a conversation side thus indicate whether a speaker
who talks a lot, laughs, is more disfluent, has higher
F0, etc., is more or less likely to be considered flir-
tatious, friendly, or awkward by the interlocutor. We
639
also computed the same features for the alter inter-
locutor. Alter features thus indicate the conversa-
tional behavior of the speaker talking with an inter-
locutor they considered to be flirtatious, friendly, or
awkward.
4.1 Prosodic Features
F0 and RMS amplitude features were extracted us-
ing Praat scripts (Boersma and Weenink, 2005).
Since the start and end of each turn were time-
marked by hand, each feature was easily extracted
over a turn, and then averages and standard devia-
tions were taken over the turns in an entire conversa-
tion side. Thus the feature F0 MIN for a conversation
side was computed by taking the F0 min of each turn
in that conversation side (not counting zero values of
F0), and then averaging these values over all turns in
the side. F0 MIN SD is the standard deviation across
turns of this same measure.
Note that we coded four measures of f0 varia-
tion, not knowing in advance which one was likely
to be the most useful: F0 MEAN SD is the deviation
across turns from the global F0 mean for the con-
versation side, measuring how variable the speakers
mean f0 is across turns. F0 SD is the standard devia-
tion within a turn for the f0 mean, and then averaged
over turns, hence measures how variable the speak-
ers f0 is within a turn. F0 SD SD measures how much
the within-turn f0 variance varies from turn to turn,
and hence is another measure of cross-turn f0 vari-
ation. PITCH RANGE SD measures how much the
speakers pitch range varies from turn to turn, and
hence is another measure of cross-turn f0 variation.
4.2 Lexical Features
Lexical features have been widely explored in the
psychological and computational literature. For
these features we drew mainly on the LIWC lexicons
of Pennebaker et al (2007), the standard for social
psychological analysis of lexical features. From the
large variety of lexical categories in LIWC we se-
lected ten that the previous work of Mairesse et al
(2007) had found to be very significant in detect-
ing personality-related features. The 10 LIWC fea-
tures we used were Anger, Assent, Ingest, Insight,
Negemotion, Sexual, Swear, I, We, and You. We also
added two new lexical features, ?past tense auxil-
iary?, a heuristic for automatically detecting narra-
F0 MIN minimum (non-zero) F0 per turn, av-
eraged over turns
F0 MIN SD standard deviation from F0 min
F0 MAX maximum F0 per turn, averaged over
turns
F0 MAX SD standard deviation from F0 max
F0 MEAN mean F0 per turn, averaged over turns
F0 MEAN SD standard deviation (across turns) from
F0 mean
F0 SD standard deviation (within a turn)
from F0 mean, averaged over turns
F0 SD SD standard deviation from the f0 sd
PITCH RANGE f0 max - f0 min per turn, averaged
over turns
PITCH RANGE
SD
standard deviation from mean pitch
range
RMS MIN minimum amplitude per turn, aver-
aged over turns
RMS MIN SD standard deviation from RMS min
RMS MAX maximum amplitude per turn, aver-
aged over turns
RMS MAX SD standard deviation from RMS max
RMS MEAN mean amplitude per turn, averaged
over turns
RMS MEAN SD standard deviation from RMS mean
TURN DUR duration of turn in seconds, averaged
over turns
TIME total time for a speaker for a conversa-
tion side, in seconds
RATE OF
SPEECH
number of words in turn divided by
duration of turn in seconds, averaged
over turns
Table 1: Prosodic features for each conversation side,
extracted using Praat from the hand-segmented turns of
each side.
tive or story-telling behavior, and Metadate, for dis-
cussion about the speed-date itself. The features are
summarized in Table 2.
4.3 Dialogue Act and Adjacency Pair Features
A number of discourse features were extracted,
drawing from the conversation analysis, disfluency
and dialog act literature (Sacks et al, 1974; Juraf-
sky et al, 1998; Jurafsky, 2001). While discourse
features are clearly important for extracting social
meaning, previous work on social meaning has met
with less success in use of such features (with the
exception of the ?critical segments? work of (Enos
et al, 2007)), presumably because discourse fea-
640
TOTAL WORDS total number of words
PAST TENSE uses of past tense auxiliaries was, were, had
METADATE horn, date, bell, survey, speed, form, questionnaire, rushed, study, research
YOU you, you?d, you?ll, your, you?re, yours, you?ve (not counting you know)
WE lets, let?s, our, ours, ourselves, us, we, we?d, we?ll, we?re, we?ve
I I?d, I?ll, I?m, I?ve, me, mine, my, myself (not counting I mean)
ASSENT yeah, okay, cool, yes, awesome, absolutely, agree
SWEAR hell, sucks, damn, crap, shit, screw, heck, fuck*
INSIGHT think*/thought, feel*/felt, find/found, understand*, figure*, idea*, imagine, wonder
ANGER hate/hated, hell, ridiculous*, stupid, kill*, screwed, blame, sucks, mad, bother, shit
NEGEMOTION bad, weird, hate, crazy, problem*, difficult, tough, awkward, boring, wrong, sad, worry,
SEXUAL love*, passion*, loves, virgin, sex, screw
INGEST food, eat*, water, bar/bars, drink*, cook*, dinner, coffee, wine, beer, restaurant, lunch, dish
Table 2: Lexical features. Each feature value is a total count of the words in that class for each conversation side;
asterisks indicate that suffixed forms were included (e.g., love, loves, loving). All except the first three are from LIWC
(Pennebaker et al, 2007) (modified slightly, for example by removing you know and I mean). The last five classes
include more words in addition to those shown.
tures are expensive to hand-label and hard to auto-
matically extract. We chose a suggestive discourse
features that we felt might still be automatically ex-
tracted.
Four particular dialog acts were chosen as shown
in Table 3. Backchannels (or continuers) and ap-
preciations (a continuer expressing positive affect)
were coded by hand-built regular expressions. The
regular expressions were based on analysis of the
backchannels and appreciations in the hand-labeled
Switchboard corpus of dialog acts (Jurafsky et al,
1997). Questions were coded simply by the pres-
ence of question marks.
Finally, repair questions (also called NTRIs; next
turn repair indicators) are turns in which a speaker
signals lack of hearing or understanding (Schegloff
et al, 1977). To detect these, we used a simple
heuristic: the presence of ?Excuse me? or ?Wait?, as
in the following example:
FEMALE: Okay. Are you excited about that?
MALE: Excuse me?
A collaborative completion is a turn where a
speaker completes the utterance begun by the alter
(Lerner, 1991; Lerner, 1996). Our heuristic for iden-
tifying collaborative completions was to select sen-
tences for which the first word of the speaker was
extremely predictable from the last two words of the
previous speaker. We trained a word trigram model1
1interpolated, with Good Turing smoothing, trained on the
Treebank 3 Switchboard transcripts after stripping punctuation.
and used it to compute the probability p of the first
word of a speaker?s turn given the last two words
of the interlocutor?s turn. We arbitrarily chose the
threshold .01, labeling all turns for which p > .01 as
collaborative completions and used the total number
of collaborative completions in a conversation side
as our variable. This simple heuristic was errorful,
but did tend to find completions beginning with and
or or (1 below) and wh-questions followed by an NP
or PP phrase that is grammatically coherent with the
end of the question (2 and 3):
(1) FEMALE: The driving range.
(1) MALE: And the tennis court, too.
(2) MALE: What year did you graduate?
(2) FEMALE: From high school?
(3) FEMALE: What department are you in?
(3) MALE: The business school.
We also marked aspects of the preference struc-
ture of language. A dispreferred action is one in
which a speaker avoids the face-threat to the inter-
locutor that would be caused by, e.g., refusing a
request or not answering a question, by using spe-
cific strategies such as the use of well, hesitations, or
restarts (Schegloff et al, 1977; Pomerantz, 1984).
Finally, we included the number of instances of
laughter for the side, as well as the total number of
turns a speaker took.
4.4 Disfluency Features
A second group of discourse features relating to re-
pair, disfluency, and speaker overlap are summarized
641
BACKCHANNELS number of backchannel utterances in side (Uh-huh., Yeah., Right., Oh, okay.)
APPRECIATIONS number of appreciations in side (Wow, That?s true, Oh, great)
QUESTIONS number of questions in side
NTRI repair question (Next Turn Repair Indicator) (Wait, Excuse me)
COMPLETION (an approximation to) utterances that were ?collaborative completions?
DISPREFERRED (an approximation to) dispreferred responses, beginning with discourse marker well
LAUGH number of instances of laughter in side
TURNS total number of turns in side
Table 3: Dialog act/adjacency pair features.
in Table 4. Filled pauses (um, uh) were coded by
UH/UM total number of filled pauses (uh or
um) in conversation side
RESTART total number of disfluent restarts in
conversation side
OVERLAP number of turns in side which the two
speakers overlapped
Table 4: Disfluency features
regular expressions (the transcribers had been in-
structed to transcribe all filled pauses). Restarts are
a type of repair in which speakers begin a phrase,
break off, and then restart the syntactic phrase. The
following example shows a restart; the speaker starts
a sentence Uh, I and then restarts, There?s a group...:
Uh, I?there?s a group of us that came in?
Overlaps are cases in which both speakers were
talking at the same time, and were marked by the
transcribers in the transcripts:
MALE: But-and also obviously?
FEMALE: It sounds bigger.
MALE: ?people in the CS school are not
quite as social in general as other?
5 Classifier Training
Before performing the classification task, we prepro-
cessed the data in two ways. First, we standardized
all the variables to have zero mean and unit variance.
We did this to avoid imposing a prior on any of the
features based on their numerical values.2 Second,
2Consider a feature A with mean 100 and a feature B with
mean .1 where A and B are correlated with the output. Since
regularization favors small weights there is a bias to put weight
on feature A because intuitively the weight on feature B would
we removed features correlated greater than .7. One
goal of removing correlated features was to remove
as much colinearity as possible from the regression
so that the regression weights could be ranked for
their importance in the classification. In addition,
we hoped to improve classification because a large
number of features require more training examples
(Ng, 2004). For example for male flirt we removed
f0 range (highly correlated with f0 max), f0 min sd
(highly correlated with f0 min), and Swear (highly
correlated with Anger).
For each classification task due to the small
amounts of data we performed k-fold cross vali-
dation to learn and evaluate our models. We used
a variant of k-fold cross validation with five folds
where three folds are used for training, one fold is
used for validation, and one fold is used as a test set.
This test fold is not used in any training step. This
yields a datasplit of 60% for training, 20% for val-
idation, and 20% for testing, or 120 training exam-
ples, 40 validation examples, and 40 test examples.
To ensure that we were not learning something spe-
cific to our data split, we randomized our data order-
ing and repeated the k-fold cross validation variant
25 times.
We used regularized logistic regression for clas-
sification. Recall that in logistic regression we train
a vector of feature weights ? ? Rn so as to make
the following classification of some output variable
y for an input observation x:3
p(y|x; ?) = 11 + exp(??Tx) (1)
In regularized logistic regression we find the
need to be 1000 times larger to carry the same effect. This ar-
gument holds similarly for the reduction to unit variance.
3Where n is the number of features plus 1 for the intercept.
642
weights ? which maximize the following optimiza-
tion problem:
argmax
?
?
i
log p(yi|xi; ?)? ? ?R(?) (2)
R(?) is a regularization term used to penalize
large weights. We chose R(?), the regularization
function, to be the L1 norm of ?. That is, R(?) =
||?||1 =?ni=1 |?i|.
In our case, given the training set Strain, test set
Stest, and validation set Sval, we trained the weights
? as follows:
argmax
?
accuracy(??, Sval) (3)
where for a given sparsity parameter ?
?? = argmax
?
?
i
log p(yi|xi; ?)? ? ?R(?) (4)
We chose L1-regularization because the number of
training examples to learn well grows logarithmi-
cally with the number of input variables (Ng, 2004),
and to achieve a sparse activation of our features
to find only the most salient explanatory variables.
This choice of regularization was made to avoid the
problems that often plague supervised learning in
situations with large number of features but only a
small number of examples. The search space over
the sparsity parameter ? is bounded around an ex-
pected sparsity to prevent overfitting.
Finally, to evaluate our model on the learned ?
and ?? we used the features X of the test set Stest to
compute the predicted outputs Y using the logistic
regression model. Accuracy is simply computed as
the percent of correct predictions.
To avoid any data ordering bias, we calculated
a ?? for each randomized run. The output of the
runs was a vector of weights for each feature. We
kept any feature if the median of its weight vector
was nonzero.4 A sample boxplot for the highest
weighted ego features for predicting male flirt can
be found in Figure 1.
4We also performed a t-test to find salient feature values
significantly different than zero; the non-zero median method
turned out to be a more conservative measure in practice (intu-
itively, because L1 normed regression pushes weights to 0).
-1   -0.8  -0.6  -0.4  -0.2    0    0.2    0.4   0.6   0.8    1
question
f0 mean std
you
rate
intensity min
backchannel
appreciation
repair quest
intensity max
laugh
I
Figure 1: An illustrative boxplot for flirtation in men
showing the 10 most significant features and one not
significant (?I?). Shown are median values (central red
line), first quartile, third quartile, outliers (red X?s) and
interquartile range (filled box).
6 Results
Results for the 6 binary classifiers are presented in
Table 5.
Awk Flirt Friendly
M F M F M F
Speaker 63% 51% 67% 60% 72% 68%
+other 64% 64% 71% 60% 73% 75%
Table 5: Accuracy of binary classification of each con-
versation side, where chance is 50%. The first row uses
features only from the single speaker; the second adds all
the features from the interlocutor as well. These accu-
racy results were aggregated from 25 randomized runs of
5-fold cross validation.
The first row shows results using features ex-
tracted from the speaker being labeled. Here, all
conversational styles are easiest to detect in men.
The second row of table 5 shows the accuracy
when using features from both speakers. Not sur-
prisingly, adding information about the interlocutor
tends to improve classification, and especially for
women, suggesting that male speaking has greater
sway over perceptions of conversational style. We
discuss below the role of these features.
We first considered the features that helped clas-
sification when considering only the ego (i.e., the re-
sults in the first row of Table 5). Table 6 shows fea-
ture weights for the features (features were normed
so weights are comparable), and is summarized in
the following paragraphs:
? Men who are labeled as friendly use you, col-
643
MALE FRIENDLY MALE FLIRT
backchannel -0.737 question 0.376
you 0.631 f0 mean sd 0.288
intensity min sd 0.552 you 0.214
f0 sd sd -0.446 rate 0.190
intensity min -0.445 intensity min -0.163
completion 0.337 backchannel -0.142
time -0.270 appreciation -0.136
Insight -0.249 repair question 0.128
f0 min -0.226 intensity max -0.121
intensity max -0.221 laugh 0.107
overlap 0.213 time -0.092
laugh 0.192 overlap -0.090
turn dur -0.059 f0 min 0.089
Sexual 0.059 Sexual 0.082
appreciation -0.054 Negemo 0.075
Anger -0.051 metadate -0.041
FEMALE FRIENDLY FEMALE FLIRT
intensity min sd 0.420 f0 max 0.475
intensity max sd -0.367 rate 0.346
completion 0.276 intensity min sd 0.269
repair question 0.255 f0 mean sd 0.21
appreciation 0.253 Swear 0.156
f0 max 0.233 question -0.153
Swear -0.194 Assent -0.127
wordcount 0.165 f0 min -0.111
restart 0.172 intensity max 0.092
uh 0.241 I 0.073
I 0.111 metadate -0.071
past -0.060 wordcount 0.065
laugh 0.048 laugh 0.054
Negemotion -0.021 restart 0.046
intensity min -0.02 overlap -0.036
Ingest -0.017 f0 sd sd -0.025
Assent 0.0087 Ingest -0.024
f0 max sd 0.0089
MALE AWK
restart 0.502 completion -0.141
f0 sd sd 0.371 intensity max -0.135
appreciation -0.354 f0 mean sd -0.091
turns -0.292 Ingest -0.079
uh 0.270 Anger 0.075
you -0.210 repair question -0.067
overlap -0.190 Insight -0.056
past -0.175 rate 0.049
intensity min sd -0.173
Table 6: Feature weights (median weights of the random-
ized runs) for the non-zero predictors for each classifier.
Since our accuracy for detecting awkwardness in women
based solely on ego features is so close to chance, we
didn?t analyze the awkwardness features for women here.
laborative completions, laugh, overlap, but don?t
backchannel or use appreciations. Their utterances
are shorter (in seconds and words) and they are qui-
eter and their (minimum) pitch is lower and some-
what less variable.
? Women labeled as friendly have more collab-
orative completions, repair questions, laughter, and
appreciations. They use more words overall, and use
I more often. They are more disfluent (both restarts
and uh) but less likely to swear. Prosodically their f0
is higher, and there seems to be some pattern involv-
ing quiet speech; more variation in intensity mini-
mum than intensity max.
? Men who are labeled as flirting ask more ques-
tions, including repair questions, and use you. They
don?t use backchannels or appreciations, or overlap
as much. They laugh more, and use more sexual and
negative emotional words. Prosodically they speak
faster, with higher and more variable pitch, but qui-
eter (lower intensity max).
? The strongest features for women who are la-
beled as flirting are prosodic; they speak faster and
louder with higher and more variable pitch. They
also use more words in general, swear more, don?t
ask questions or use Assent, use more I, laugh more,
and are somewhat more disfluent (restarts).
?Men who are labeled as awkward are more dis-
fluent, with increased restarts and filled pauses (uh
and um). They are also not ?collaborative? conversa-
tionalists; they don?t use appreciations, repair ques-
tions, collaborative completions, past-tense, or you,
take fewer turns overall, and don?t overlap. Prosod-
ically the awkward labels are hard to characterize;
there is both an increase in pitch variation (f0 sd sd)
and a decrease (f0 mean sd). They don?t seem to get
quite as loud (intensity max).
The previous analysis showed what features of the
ego help in classification. We next asked about fea-
tures of the alter, based on the results using both
ego and alter features in the second row of Table 5.
Here we are asking about the linguistic behaviors of
a speaker who describes the interlocutor as flirting,
friendly, or awkward.
While we don?t show these values in a table, we
offer here an overview of their tendencies. For
example for women who labeled their male in-
terlocutors as friendly, the women got much qui-
eter, used ?well? much more, laughed, asked more
644
repair questions, used collaborative completions,
and backchanneled more. When a man labeled a
woman as friendly, he used an expanded intensity
range (quieter intensity min, louder intensity max).
laughed more, used more sexual terms, used less
negative emotional terms, and overlapped more.
When women labeled their male interlocutor as
flirting, the women used many more repair ques-
tions, laughed more, and got quieter (lower intensity
min). By contrast, when a man said his female inter-
locutor was flirting, he used more Insight and Anger
words, and raised his pitch.
When women labeled their male interlocutor as
awkward, the women asked a lot of questions, used
well, were disfluent (restarts), had a diminished
pitch range, and didn?t use I. In listening to some
of these conversations, it was clear that the conver-
sation lagged repeatedly, and the women used ques-
tions at these points to restart the conversations.
7 Discussion
The results presented here should be regarded with
some caution. The sample is not a random sample of
English speakers or American adults, and speed dat-
ing is not a natural context for expressing every con-
versational style. Therefore, a wider array of studies
across populations and genres would be required be-
fore a more general theory of conversational styles is
established.
On the other hand, the presented results may
under-reflect the relations being captured. The qual-
ity of recordings and coarse granularity (1 second)
of the time-stamps likely cloud the relations, and as
the data is cleaned and improved, we expect the as-
sociations to only grow stronger.
Caveats aside, we believe the evidence indicates
that the perception of several types of conversational
style have relatively clear signals across genders, but
with some additional gender contextualization.
Both genders convey flirtation by laughing more,
speaking faster, and using higher and more variable
pitch. Both genders convey friendliness by laughing
more, and using collaborative completions.
However, we do find gender differences; men asl
more questions when (labeled as) flirting, women
ask fewer. Men labeled as flirting are softer, but
women labeled as flirting are louder. Women flirt-
ing swear more, while men are more likely to use
sexual vocabulary. Gender differences exist as well
for the other variables. Men labeled as friendly use
you while women labeled as friendly use I. Friendly
women are very disfluent; friendly men are not.
While the features for friendly and flirtatious
speech overlap, there are clear differences. Men
speaker faster and with higher f0 (min) in flirtatious
speech, but not faster and with lower f0 (min) in
friendly speech. For men, flirtatious speech involves
more questions and repair questions, while friendly
speech does not. For women, friendly speech is
more disfluent than flirtatious speech, and has more
collaborative style (completions, repair questions,
appreciations).
We also seem to see a model of collaborative con-
versational style (probably related to the collabo-
rative floor of Edelsky (1981) and Coates (1996)),
cued by the use of more collaborative completions,
repair questions and other questions, you, and laugh-
ter. These collaborative techniques were used by
both women and men who were labeled as friendly,
and occurred less with men labeled as awkward.
Women themselves displayed more of this collab-
orative conversational style when they labeled the
men as friendly. For women only, collaborative style
included appreciations; while for men only, collabo-
rative style included overlaps.
In addition to these implications for social sci-
ence, our work has implications for the extraction of
meaning in general. A key focus of our work was on
ways to extract useful dialog act and disfluency fea-
tures (repair questions, backchannels, appreciations,
restarts, dispreferreds) with very shallow methods.
These features were indeed extractable and proved
to be useful features in classification.
We are currently extending these results to predict
date outcomes including ?liking?, extending work
such as Madan and Pentland (2006).
Acknowledgments
Thanks to three anonymous reviewers, Sonal Nalkur and
Tanzeem Choudhury for assistance and advice on data
collection, Sandy Pentland for a helpful discussion about
feature extraction, and to Google for gift funding.
645
References
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-
cke. 2002. Prosody-Based Automatic Detection of
Annoyance and Frustration in Human-Computer Dia-
log. In INTERSPEECH-02.
P. Boersma and D. Weenink. 2005. Praat: doing pho-
netics by computer (version 4.3.14). [Computer pro-
gram]. Retrieved May 26, 2005, from http://www.
praat.org/.
S. Brave, C. Nass, and K. Hutchinson. 2005. Comput-
ers that care: Investigating the effects of orientation
of emotion exhibited by an embodied conversational
agent. International Journal of Human-Computer
Studies, 62(2):161?178.
S. E. Brennan and M. F. Schober. 2001. How listen-
ers compensate for disfluencies in spontaneous speech.
Journal of Memory and Language, 44:274?296.
S. E. Brennan and M. Williams. 1995. The feeling of
another?s knowing: Prosody and filled pauses as cues
to listeners about the metacognitive states of speakers.
Journal of Memory and Language, 34:383?398.
J. Coates. 1996. Women Talk. Blackwell.
M. A. Cohn, M. R. Mehl, and J. W. Pennebaker. 2004.
Linguistic markers of psychological change surround-
ing September 11, 2001. Psychological Science,
15:687?693.
C. Edelsky. 1981. Who?s got the floor? Language in
Society, 10:383?421.
F. Enos, E. Shriberg, M. Graciarena, J. Hirschberg, and
A. Stolcke. 2007. Detecting Deception Using Critical
Segments. In INTERSPEECH-07.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL Labeling Project Coder?s Man-
ual, Draft 13. Technical Report 97-02, University of
Colorado Institute of Cognitive Science.
D. Jurafsky, E. Shriberg, B. Fox, and T. Curl. 1998. Lex-
ical, prosodic, and syntactic cues for dialog acts. In
Proceedings, COLING-ACL Workshop on Discourse
Relations and Discourse Markers, pages 114?120.
D. Jurafsky. 2001. Pragmatics and computational lin-
guistics. In L. R. Horn and G. Ward, editors, Hand-
book of Pragmatics. Blackwell.
C. M. Lee and S. S. Narayanan. 2002. Combining acous-
tic and language information for emotion recognition.
In ICSLP-02, pages 873?876, Denver, CO.
G. H. Lerner. 1991. On the syntax of sentences-in-
progress. Language in Society, 20(3):441?458.
G. H. Lerner. 1996. On the ?semi-permeable? character
of grammatical units in conversation: Conditional en-
try into the turn space of another speaker. In E. Ochs,
E. A. Schegloff, and S. A. Thompson, editors, Interac-
tion and Grammar, pages 238?276. Cambridge Uni-
versity Press.
J. Liscombe, J. Venditti, and J. Hirschberg. 2003. Clas-
sifying Subject Ratings of Emotional Speech Using
Acoustic Features. In INTERSPEECH-03.
A. Madan and A. Pentland. 2006. Vibefones: Socially
aware mobile phones. In Tenth IEEE International
Symposium on Wearable Computers.
A. Madan, R. Caneel, and A. Pentland. 2005. Voices
of attraction. Presented at Augmented Cognition, HCI
2005, Las Vegas.
F. Mairesse and M. Walker. 2008. Trainable generation
of big-five personality styles through data-driven pa-
rameter estimation. In ACL-08, Columbus.
F. Mairesse, M. Walker, M. Mehl, and R. Moore. 2007.
Using linguistic cues for the automatic recognition of
personality in conversation and text. Journal of Artifi-
cial Intelligence Research (JAIR), 30:457?500.
C. Nass and S. Brave. 2005. Wired for speech: How
voice activates and advances the human-computer re-
lationship. MIT Press, Cambridge, MA.
M. L. Newman, J. W. Pennebaker, D. S. Berry, and J. M.
Richards. 2003. Lying words: Predicting deception
from linguistic style. Personality and Social Psychol-
ogy Bulletin, 29:665?675.
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regulariza-
tion, and rotational invariance. In ICML 2004.
J. W. Pennebaker and T. C. Lay. 2002. Language use and
personality during crises: Analyses of Mayor Rudolph
Giuliani?s press conferences. Journal of Research in
Personality, 36:271?282.
J. W. Pennebaker, R. Booth, and M. Francis. 2007. Lin-
guistic inquiry and word count: LIWC2007 operator?s
manual. Technical report, University of Texas.
A. Pentland. 2005. Socially aware computation and
communication. Computer, pages 63?70.
A. M. Pomerantz. 1984. Agreeing and disagreeing with
assessment: Some features of preferred/dispreferred
turn shapes. In J. M. Atkinson and J. Heritage, edi-
tors, Structure of Social Action: Studies in Conversa-
tion Analysis. Cambridge University Press.
A. Rosenberg and J. Hirschberg. 2005. Acous-
tic/prosodic and lexical correlates of charismatic
speech. In EUROSPEECH-05, pages 513?516, Lis-
bon, Portugal.
S. S. Rude, E. M. Gortner, and J. W. Pennebaker. 2004.
Language use of depressed and depression-vulnerable
college students. Cognition and Emotion, 18:1121?
1133.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696?735.
E. A. Schegloff, G. Jefferson, and H. Sacks. 1977. The
preference for self-correction in the organization of re-
pair in conversation. Language, 53:361?382.
646
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 124?132,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
A Study of Academic Collaboration in Computational Linguistics with
Latent Mixtures of Authors
Nikhil Johri, Daniel Ramage
Department of Computer Science
Stanford University
Stanford, CA, USA
Daniel A. McFarland
School of Education
Stanford University
Stanford, CA, USA
{njohri2,dramage,dmcfarla,jurafsky}@stanford.edu
Daniel Jurafsky
Department of Linguistics
Stanford University
Stanford, CA, USA
Abstract
Academic collaboration has often been at
the forefront of scientific progress, whether
amongst prominent established researchers, or
between students and advisors. We suggest a
theory of the different types of academic col-
laboration, and use topic models to computa-
tionally identify these in Computational Lin-
guistics literature. A set of author-specific
topics are learnt over the ACL corpus, which
ranges from 1965 to 2009. The models are
trained on a per year basis, whereby only pa-
pers published up until a given year are used
to learn that year?s author topics. To determine
the collaborative properties of papers, we use,
as a metric, a function of the cosine similarity
score between a paper?s term vector and each
author?s topic signature in the year preceding
the paper?s publication. We apply this metric
to examine questions on the nature of collabo-
rations in Computational Linguistics research,
finding that significant variations exist in the
way people collaborate within different sub-
fields.
1 Introduction
Academic collaboration is on the rise as single au-
thored work becomes less common across the sci-
ences (Rawlings and McFarland, 2011; Jones et al,
2008; Newman, 2001). In part, this rise can be at-
tributed to the increasing specialization of individual
academics and the broadening in scope of the prob-
lems they tackle. But there are other advantages to
collaboration, as well: they can speed up produc-
tion, diffuse knowledge across authors, help train
new scientists, and are thought to encourage greater
innovation. Moreover, they can integrate scholarly
communities and foster knowledge transfer between
related fields. But all collaborations aren?t the same:
different collaborators contribute different material,
assume different roles, and experience the collabo-
ration in different ways. In this paper, we present
a new frame for thinking about the variation in col-
laboration types and develop a computational metric
to characterize the distinct contributions and roles of
each collaborator within the scholarly material they
produce.
The topic of understanding collaborations has at-
tracted much interest in the social sciences over the
years. Recently, it has gained traction in computer
science, too, in the form of social network analysis.
Much work focuses on studying networks formed
via citations (Radev et al, 2009; White and Mccain,
1998), as well as co-authorship links (Nascimento
et al, 2003; Liu et al, 2005). However, these works
focus largely on the graphical structure derived from
paper citations and author co-occurrences, and less
on the textual content of the papers themselves. In
this work, we examine the nature of academic col-
laboration using text as a primary component.
We propose a theoretical framework for determin-
ing the types of collaboration present in a docu-
ment, based on factors such as the number of es-
tablished authors, the presence of unestablished au-
thors and the similarity of the established authors?
past work to the document?s term vector. These col-
laboration types attempt to describe the nature of co-
authorships between students and advisors (e.g. ?ap-
prentice? versus ?new blood?) as well as those solely
between established authors in the field. We present
a decision diagram for classifying papers into these
types, as well as a description of the intuition behind
each collaboration class.
124
We explore our theory with a computational
method to categorize collaborative works into their
collaboration types using an approach based on topic
modeling, where we model every paper as a la-
tent mixture of its authors. For our system, we use
Labeled-LDA (LLDA (Ramage et al, 2009)) to train
models over the ACL corpus for every year of the
words best attributed to each author in all the papers
they write. We use the resulting author signatures
as a basis for several metrics that can classify each
document by its collaboration type.
We qualitatively analyze our results by examin-
ing the categorization of several high impact papers.
With consultation from prominent researchers and
textbook writers in the field, we demonstrate that our
system is able to differentiate between the various
types of collaborations in our suggested taxonomy,
based only on words used, at low but statistically
significant accuracy. We use this same similarity
score to analyze the ACL community by sub-field,
finding significant deviations.
2 Related Work
In recent years, popular topic models such as La-
tent Dirichlet Allocation (Blei et al, 2003) have
been increasingly used to study the history of sci-
ence by observing the changing trends in term based
topics (Hall et al, 2008), (Gerrish and Blei, 2010).
In the case of Hall et al, regular LDA topic mod-
els were trained over the ACL anthology on a per
year basis, and the changing trends in topics were
studied from year to year. Gerrish and Blei?s work
computed a measure of influence by using Dynamic
Topic Models (Blei and Lafferty, 2006) and study-
ing the change of statistics of the language used in a
corpus.
These models propose interesting ideas for utiliz-
ing topic modeling to understand aspects of scien-
tific history. However, our primary interest, in this
paper, is the study of academic collaboration be-
tween different authors; we therefore look to learn
models for authors instead of only documents. Pop-
ular topic models for authors include the Author-
Topic Model (Rosen-Zvi et al, 2004), a simple
extension of regular LDA that adds an additional
author variable over the topics. The Author-Topic
Model learns a distribution over words for each
topic, as in regular LDA, as well as a distribution
over topics for each author. Alternatively, Labeled
LDA (Ramage et al, 2009), another LDA variation,
offers us the ability to directly model authors as top-
ics by considering them to be the topic labels for the
documents they author.
In this work, we use Labeled LDA to directly
model probabilistic term ?signatures? for authors. As
in (Hall et al, 2008) and (Gerrish and Blei, 2010),
we learn a new topic model for each year in the cor-
pus, allowing us to account for changing author in-
terests over time.
3 Computational Methodology
The experiments and results discussed in this paper
are based on a variation of the LDA topic model run
over data from the ACL corpus.
3.1 Dataset
We use the ACL anthology from years 1965 to 2009,
training over 12,908 papers authored by over 11,355
unique authors. We train our per year topic mod-
els over the entire dataset; however, when evaluating
our results, we are only concerned with papers that
were authored by multiple individuals as the other
papers are not collaborations.
3.2 Latent Mixture of Authors
Every abstract in our dataset reflects the work, to
some greater or lesser degree, of all the authors of
that work. We model these degrees explicitly us-
ing a latent mixture of authors model, which takes
its inspiration from the learning machinery of LDA
(Blei et al, 2003) and its supervised variant La-
beled LDA (Ramage et al, 2009). These models
assume that documents are as a mixture of ?topics,?
which themselves are probability distributions over
the words in the vocabulary of the corpus. LDA
is completely unsupervised, assuming that a latent
topic layer exists and that each word is generated
from one underlying topic from this set of latent top-
ics. For our purposes, we use a variation of LDA in
which we assume each document to be a latent mix-
ture of its authors. Unlike LDA, where each docu-
ment draws a multinomial over all topics, the latent
mixture of authors model we use restricts a docu-
ment to only sample from topics corresponding to
125
its authors. Also, unlike models such as the Author-
Topic Model (Rosen-Zvi et al, 2004), where au-
thors are modeled as distributions over latent top-
ics, our model associates each author to exactly one
topic, modeling authors directly as distributions over
words.
Like other topic models, we will assume a genera-
tive process for our collection of D documents from
a vocabulary of size V . We assume that each docu-
ment d has Nd terms and Md authors from a set of
authors A. Each author is described by a multino-
mial distribution ?a over words V , which is initially
unobserved. We will recover for each document a
hidden multinomial ?(d) of length Md that describes
which mixture of authors? best describes the doc-
ument. This multinomial is in turn drawn from a
symmetric Dirichlet distribution with parameter ?
restrict to the set of authors ?(d) for that paper. Each
document?s words are generated by first picking an
author zi from ?(d) and then drawing a word from
the corresponding author?s word distribution. For-
mally, the generative process is as follows:
? For each author a, generate a distribution ?a over
the vocabulary from a Dirichlet prior ?
? For each document d, generate a multinomial mix-
ture distribution ?(d) ? Dir(?.1?(d))
? For each document d,
? For each i ? {1, ..., Nd}
? Generate zi ? {?
(d)
1 , ..., ?
(d)
Md
} ?
Mult(?(d))
? Generate wi ? {1, ..., V } ?Mult(?zi)
We use Gibbs sampling to perform inference in
this model. If we consider our authors as a label
space, this model is equivalent to that of Labeled
LDA (Ramage et al, 2009), which we use for in-
ference in our model, using the variational objec-
tive in the open source implementation1. After in-
ference, our model discovers the distribution over
terms that best describes that author?s work in the
presence of other authors. This distribution serves
as a ?signature? for an author and is dominated by
the terms that author uses frequently across collabo-
rations. It is worth noting that this model constrains
the learned ?topics? to authors, ensuring directly in-
terpretable results that do not require the interpreta-
1http://nlp.stanford.edu/software/tmt/
tion of a latent topic space, such as in (Rosen-Zvi et
al., 2004).
To imbue our model with a notion of time, we
train a separate LLDA model for each year in the
corpus, training on only those papers written before
and during the given year. Thus, we have separate
?signatures? for each author for each year, and each
signature only contains information for the specific
author?s work up to and including the given year.
Table 1 contains examples of such term signatures
computed for two authors in different years. The top
terms and their fractional counts are displayed.
4 Studying Collaborations
There are several ways one can envision to differen-
tiate between types of academic collaborations. We
focus on three factors when creating collaboration
labels, namely:
? Presence of unestablished authors
? Similarity to established authors
? Number of established authors
If an author whom we know little about is present
on a collaborative paper, we consider him or her to
be a new author. We threshold new authors by the
number of papers they have written up to the pub-
lication year of the paper we are observing. De-
pending on whether this number is below or above a
threshold value, we consider an author to be estab-
lished or unestablished in the given year.
Similarity scores are measured using the trained
LLDA models described in Section 3.2. For any
given paper, we measure the similarity of the pa-
per to one of its (established) authors by calculating
the cosine similarity of the author?s signature in the
year preceding the paper?s publication to the paper?s
term-vector.
Using the aforementioned three factors, we define
the following types of collaborations:
? Apprenticeship Papers are authored by one or
more established authors and one or more un-
established authors, such that the similarity of
the paper to more than half of the established
authors is high. In this case, we say that the
new author (or authors) was an apprentice of
126
Philipp Koehn, 2002 Philipp Koehn, 2009 Fernando Pereira, 1985 Fernando Pereira, 2009
Terms Counts Terms Counts Terms Counts Terms Counts
word 3.00 translation 69.78 grammar 14.99 type 40.00
lexicon 2.00 machine 34.67 phrase 10.00 phrase 30.89
noun 2.00 phrase 26.85 structure 7.00 free 23.14
similar 2.00 english 23.86 types 6.00 grammar 23.10
translation 1.29 statistical 19.51 formalisms 5.97 constraint 23.00
purely 0.90 systems 18.32 sharing 5.00 logical 22.41
accuracy 0.90 word 16.38 unification 4.97 rules 21.72
Table 1: Example term ?signatures? computed by running a Labeled LDA model over authors in the ACL corpus on a
per year basis: top terms for two authors in different years are shown alongside their fractional counts.
the established authors, continuing in their line
of work.
? New Blood Papers are authored by one estab-
lished author and one or more unestablished au-
thors, such that the similarity of the paper to the
established author is low. In this case, we say
that the new author (or authors) provided new
ideas or worked in an area that was dissimilar to
that which the established author was working
in.
? Synergistic Papers are authored only by es-
tablished authors such that it does not heavily
resemble any authors? previous work. In this
case, we consider the paper to be a product of
synergy of its authors.
? Catalyst Papers are similar to synergistic
ones, with the exception that unestablished au-
thors are also present on a Catalyst Paper. In
this case, we hypothesize that the unestablished
authors were the catalysts responsible for get-
ting the established authors to work on a topic
dissimilar to their previous work.
The decision diagram in Figure 1 presents an easy
way to determine the collaboration type assigned to
a paper.
5 Quantifying Collaborations
Following the decision diagram presented in Figure
1 and using similarity scores based on the values
returned by our latent author mixture models (Sec-
tion 3.2), we can deduce the collaboration type to
assign to any given paper. However, absolute cate-
gorization requires an additional thresholding of au-
thor similarity scores. To avoid the addition of an
arbitrary threshold, instead of directly categorizing
papers, we rank them based on the calculated sim-
ilarity scores on three different spectra. To facili-
tate ease of interpretation, the qualitative examples
we present are drawn from high PageRank papers as
calculated in (Radev et al, 2009).
5.1 The MaxSim Score
To measure the similarity of authors? previous work
to a paper, we look at the cosine similarity between
the term vector of the paper and each author?s term
signature. We are only interested in the highest co-
sine similarity score produced by an author, as our
categories do not differentiate between papers that
are similar to one author and papers that are sim-
ilar to multiple authors, as long as high similarity
to any single author is present. Thus, we choose
our measure, the MaxSim score, to be defined as:
max
a?est
cos(asig, paper)
We choose to observe the similarity scores only
for established authors as newer authors will not
have enough previous work to produce a stable term
signature, and we vary the experience threshold by
year to account for the fact that there has been a large
increase in the absolute number of papers published
in recent years.
Depending on the presence of new authors and
the number of established authors present, each pa-
per can be placed into one of the three spectra: the
Apprenticeship-New Blood spectrum, the Synergy
spectrum and the Apprenticeship-Catalyst spectrum.
Apprenticeship and Low Synergy papers are those
with high MaxSim scores, while low scores indicate
New Blood, Catalyst or High Synergy papers.
5.2 Examples
The following are examples of high impact papers
as they were categorized by our system:
127
Figure 1: Decision diagram for determining the collaboration type of a paper. A minimum of 1 established author is
assumed.
5.2.1 Example: Apprenticeship Paper
Improvements in Phrase-Based Statistical Ma-
chine Translation (2004)
by Richard Zens and Hermann Ney
This paper had a high MaxSim score, indicating high
similarity to established author Hermann Ney. This
categorizes the paper as an Apprenticeship Paper.
5.2.2 Example: New Blood Paper
Thumbs up? Sentiment Classification using
Machine Learning Techniques (2002)
by Lillian Lee, Bo Pang and Shivakumar
Vaithyanathan
This paper had a low MaxSim score, indicating
low similarity to established author Lillian Lee.
This categorizes the paper as a New Blood Pa-
per, with new authors Bo Pang and Shivakumar
Vaithyanathan. It is important to note here that new
authors do not necessarily mean young authors or
grad students; in this case, the third author on the
paper was experienced, but in a field outside of
ACL.
5.2.3 Example: High Synergy Paper
Catching the Drift: Probabilistic Content
Models, with Applications to Generation and
Summarization (2003)
by Regina Barzilay and Lillian Lee
This paper had low similarity to both established
authors on it, making it a highly synergistic paper.
Synergy here indicates that the work done on this
paper was mostly unlike work previously done by
either of the authors.
5.2.4 Example: Catalyst Paper
Answer Extraction (2000)
by Steven Abney, Michael Collins, Amit Singhal
This paper had a very low MaxSim score, as well
as the presence of an unestablished author, making
it a Catalyst Paper. The established authors (from
an ACL perspective) were Abney and Collins, while
Singhal was from outside the area and did not have
many ACL publications. The work done in this pa-
per focused on information extraction, and was un-
like that previously done by either of the ACL estab-
lished authors. Thus, we say that in this case, Sing-
hal played the role of the catalyst, getting the other
two authors to work on an area that was outside of
their usual range.
5.3 Evaluation
5.3.1 Expert Annotation
To quantitatively evaluate the performance of
our system, we prepared a subset of 120 papers
from among the highest scoring collaborative papers
based on the PageRank metric (Radev et al, 2009).
Only those papers were selected which had at least a
128
single established author. One expert in the field was
asked to annotate each of these papers as being ei-
ther similar or dissimilar to the established authors?
prior work given the year of publication, the title of
the publication and its abstract.
We found that the MaxSim scores of papers la-
beled as being similar to the established authors
were, on average, higher than those labeled as dis-
similar. The average MaxSim score of papers anno-
tated as low MaxSim collaboration types (High Syn-
ergy, New Blood or Catalyst papers) was 0.15488,
while that of papers labeled as high MaxSim types
(Apprentice or Low Synergy papers) had a mean
MaxSim score of 0.21312. The MaxSim scores of
the different sets were compared using a t-test, and
the difference was found to be statistically signifi-
cant with a two-tailed p-value of 0.0041.
Framing the task as a binary classification prob-
lem, however, did not produce very strong results.
The breakdown of the papers and success rates (as
determined by a tuned threshold) can be seen in Ta-
ble 3. The system had a relatively low success rate of
62.5% in its binary categorization of collaborations.
5.3.2 First Author Prediction
Studies have suggested that authorship order,
when not alphabetical, can often be quantified and
predicted by those who do the work (Sekercioglu,
2008). Through a survey of all authors on a sam-
ple of papers, Slone (1996) found that in almost all
major papers, ?the first two authors are said to ac-
count for the preponderance of work?. We attempt
to evaluate our similarity scores by checking if they
are predictive of first author.
Though similarity to previous work is only a small
contributor to determining author order, we find that
using the metric of cosine similarity between author
signatures and papers performs significantly better
at determining the first author of a paper than ran-
dom chance. Of course, this feature alone isn?t ex-
tremely predictive, given that it?s guaranteed to give
an incorrect solution in cases where the first author
of a paper has never been seen before. To solve the
problem of first author prediction, we would have
to combine this with other features. We chose two
other features - an alphabetical predictor, and a pre-
dictor based on the frequency of an author appearing
as first author. Although we don?t show the regres-
Predictor Feature Accuracy
Random Chance 37.35%
Author Signature Similarity 45.23%
Frequency Estimator 56.09%
Alphabetical Ordering 43.64%
Table 2: Accuracy of individual features at predicting the
first author of 8843 papers
sion, we do explore these two other features and find
that they are also predictive of author order.
Table 2 shows the performance of our prediction
feature alongside the others. The fact that it beats
random chance shows us that there is some infor-
mation about authorial efforts in the scores we have
computed.
6 Applications
A number of questions about the nature of collabo-
rations may be answered using our system. We de-
scribe approaches to some of these in this section.
6.1 The Hedgehog-Fox Problem
From the days of the ancient Greek poet
Archilochus, the Hedgehog-Fox analogy has
been frequently used (Berlin, 1953) to describe two
different types of people. Archilochus stated that
?The fox knows many things; the hedgehog one big
thing.? A person is thus considered a ?hedgehog?
if he has expertise in one specific area and focuses
all his time and resources on it. On the other hand,
a ?fox? is a one who has knowledge of several
different fields, and dabbles in all of them instead of
focusing heavily on one.
We show how, using our computed similarity
scores, one can discover the hedgehogs and foxes
of Computational Linguistics. We look at the top
100 published authors in our corpus, and for each
author, we compute the average similarity score the
author?s signature has to each of his or her papers.
Note that we start taking similarity scores into ac-
count only after an author has published 5 papers,
thereby allowing the author to stablize a signature
in the corpus and preventing the signature from be-
ing boosted by early papers (where author similarity
would be artificially high, since the author was new).
We present the authors with the highest average
similarity scores in Table 4. These authors can be
129
Collaboration Type True Positives False Positives Accuracy
New Blood, Catalyst or High Synergy Papers 43 23 65.15%
Apprentice or Low Synergy Papers 32 22 59.25%
Overall 75 45 62.50%
Table 3: Evaluation based on annotation by one expert
considered the hedgehogs, as they have highly sta-
ble signatures that their new papers resemble. On
the other hand, Table 5 shows the list of foxes, who
have less stable signatures, presumably because they
move about in different areas.
Author Avg. Sim. Score
Koehn, Philipp 0.43456
Pedersen, Ted 0.41146
Och, Franz Josef 0.39671
Ney, Hermann 0.37304
Sumita, Eiichiro 0.36706
Table 4: Hedgehogs - authors with the highest average
similarity scores
Author Avg. Sim. Score
Marcus, Mitchell P. 0.09996
Pustejovsky, James D. 0.10473
Pereira, Fernando C. N. 0.14338
Allen, James F. 0.14461
Hahn, Udo 0.15009
Table 5: Foxes - authors with the lowest average similar-
ity scores
6.2 Similarity to previous work by sub-fields
Based on the different types of collaborations dis-
cussed in, a potential question one might ask is
which sub-fields are more likely to produce appren-
tice papers, and which will produce new blood pa-
pers. To answer this question, we first need to deter-
mine which papers correspond to which sub-fields.
Once again, we use topic models to solve this prob-
lem. We first filter out a subset of the 1,200 highest
page-rank collaborative papers from the years 1980
to 2007. We use a set of topics built by running a
standard LDA topic model over the ACL corpus, in
which each topic is hand labeled by experts based on
the top terms associated with it. Given these topic-
term distributions, we can once again use the cosine
similarity metric to discover the highly associated
Topic Score
Statistical Machine Translation 0.2695
Prosody 0.2631
Speech Recognition 0.2511
Non-Statistical Machine Translation 0.2471
Word Sense Disambiguation 0.2380
Table 6: Topics with highest MaxSim scores (papers are
more similar to the established authors? previous work)
Topic Score
Question Answering 0.1335
Sentiment Analysis 0.1399
Dialog Systems 0.1417
Spelling Correction 0.1462
Summarization 0.1511
Table 7: Topics with lowest MaxSim scores (papers are
less similar to the established authors? previous work)
topics for each given paper from our smaller sub-
set, by choosing topics with cosine similarity above
a certain threshold ? (in this case 0.1).
Once we have created a paper set for each topic,
we can measure the ?novelty? for each paper by look-
ing at their MaxSim score. We can now find the av-
erage MaxSim score for each topic. This average
similarity score gives us a notion of how similar to
the established author (or authors) a paper in the sub
field usually is. Low scores indicate that new blood
and synergy style papers are more common, while
higher scores imply more non-synergistic or appren-
ticeship style papers. This could indicate that topics
with lower scores are more open ended, while those
with higher scores require more formality or train-
ing. The top five topics in each category are shown
in Tables 6 and 7. The scores of the papers from
the two tables were compared using a t-test, and the
difference in the scores of the two tables was found
to be very statistically significant with a two-tailed p
value << 0.01.
130
7 Discussion and Future Work
Once we have a robust way to score different kinds
of collaborations in ACL, we can begin to use these
scores as a quantitative tool to study phonemena in
the computational linguistics community. With our
current technique, we discovered a number of nega-
tive results; however, given that our accuracy in bi-
nary classification of categories is relatively low, we
cannot state for sure whether these are true negative
results or a limitation of our model.
7.1 Tentative Negative Results
Among the questions we looked into, we found the
following results:
? There was no signal indicating that authors
who started out as new blood authors were any
more or less likely to survive than authors who
started out as apprentices. Survival was mea-
sured both by the number of papers eventually
published by the author as well as the year of
the author?s final publication; however, calcu-
lations by neither measure correlated with the
MaxSim scores of the authors? early papers.
? Each author in the corpus was labeled for gen-
der. Gender didn?t appear to differentiate how
people collaborated. In particular, there was no
difference between men and women based on
how they started their careers. Women and men
are equally likely to begin as new blood authors
as they are to begin as apprentices.
? On a similar note, established male authors are
equally likely to partake in new blood or ap-
prentice collaborations as their female counter-
parts.
? No noticeable difference existed between aver-
age page rank scores of a certain categorization
of collaborative papers (e.g. high synergy pa-
pers vs. low synergy papers).
It is difficult to conclusively demonstrate negative
results, particularly given that our MaxSim scores
are by themselves not particularly strong discrimi-
nators in the binary classification tasks. We consider
these findings to be tentative and an opportunity to
explore in the future.
8 Conclusion
Not everything we need to know about academic
collaborations can be found in the co-authorship
graph. Indeed, as we have argued, not all types
of collaborations are equal, as embodied by differ-
ing levels of seniority and contribution from each
co-author. In this work, we have taken a first step
toward computationally modeling these differences
using a latent mixture of authors model and ap-
plied it to our own field, Computational Linguistics.
We used the model to examine how collaborative
works differ by authors and subfields in the ACL an-
thology. Our model quantifies the extent to which
some authors are more prone to being ?hedgehogs,?
whereby they heavily focus on certain specific ar-
eas, whilst others are more diverse with their fields
of study and may be analogized with ?foxes.?
We also saw that established authors in certain
subfields have more deviation from their previous
work than established authors in different subfields.
This could imply that the former fields, such as
?Sentiment Analysis? or ?Summarization,? are more
open to new blood and synergistic ideas, while other
latter fields, like ?Statistical Machine Translation?
or ?Speech Recognition? are more formal or re-
quire more training. Alternatively, ?Summarization?
or ?Sentiment Analysis? could just still be younger
fields whose language is still evolving and being in-
fluenced by other subareas.
This work takes a first step toward a new way of
thinking about the contributions of individual au-
thors based on their network of areas. There are
many design parameters that still exist in this space,
including alternative text models that take into ac-
count richer structure and, hopefully, perform bet-
ter at discriminating between the types of collabo-
rations we identified. We intend to use the ACL an-
thology as our test bed for continuing to work on tex-
tual models of collaboration types. Ultimately, we
hope to apply the lessons we learn on modeling this
familiar corpus to the challenge of answering large-
scale questions about the nature of collaboration as
embodied by large scale publication databases such
as ISI and Pubmed.
131
Acknowledgments
This research was supported by NSF grant NSF-
0835614 CDI-Type II: What drives the dynamic cre-
ation of science? We thank our anonymous review-
ers for their valuable feedback and the members of
the Stanford Mimir Project team for their insights
and engagement.
References
Isaiah Berlin. 1953. The hedgehog and the fox: An essay
on Tolstoy?s view of history. Simon & Schuster.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd international
conference on Machine learning, ICML ?06, pages
113?120, New York, NY, USA. ACM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
Sean M. Gerrish and David M. Blei. 2010. A language-
based approach to measuring scholarly impact. In Pro-
ceedings of the 26th International Conference on Ma-
chine Learning.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 363?371, Stroudsburg, PA, USA.
Association for Computational Linguistics.
B. F. Jones, S. Wuchty, and B. Uzzi. 2008. Multi-
university research teams: Shifting impact, geography,
and stratification in science. Science, 322:1259?1262,
November.
Xiaoming Liu, Johan Bollen, Michael L. Nelson, and
Herbert Van de Sompel. 2005. Co-authorship net-
works in the digital library research community. In-
formation Processing & Management, 41(6):1462 ?
1480. Special Issue on Infometrics.
Mario A. Nascimento, Jo?rg Sander, and Jeffrey Pound.
2003. Analysis of sigmod?s co-authorship graph. SIG-
MOD Rec., 32:8?10, September.
M. E. J. Newman. 2001. From the cover: The struc-
ture of scientific collaboration networks. Proceedings
of the National Academy of Science, 98:404?409, Jan-
uary.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network cor-
pus. In Proceedings of the 2009 Workshop on Text
and Citation Analysis for Scholarly Digital Libraries,
NLPIR4DL ?09, pages 54?61, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled lda: a super-
vised topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 1 - Volume 1, EMNLP ?09, pages 248?256.
Craig M. Rawlings and Daniel A. McFarland. 2011. In-
fluence flows in the academy: Using affiliation net-
works to assess peer effects among researchers. Social
Science Research, 40(3):1001 ? 1017.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for au-
thors and documents. In Proceedings of the 20th con-
ference on Uncertainty in artificial intelligence, UAI
?04, pages 487?494.
Cagan H. Sekercioglu. 2008. Quantifying coauthor con-
tributions. Science, 322(5900):371.
RM Slone. 1996. Coauthors? contributions to major
papers published in the ajr: frequency of undeserved
coauthorship. Am. J. Roentgenol., 167(3):571?579.
Howard D. White and Katherine W. Mccain. 1998. Visu-
alizing a discipline: An author co-citation analysis of
information science. Journal of the American Society
for Information Science, 49:1972?1995.
132
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 13?21,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Towards a Computational History of the ACL: 1980?2008
Ashton Anderson
Stanford University
ashtona@stanford.edu
Dan McFarland
Stanford University
dmcfarla@stanford.edu
Dan Jurafsky
Stanford University
jurafsky@stanford.edu
Abstract
We develop a people-centered computational
history of science that tracks authors over top-
ics and apply it to the history of computa-
tional linguistics. We present four findings
in this paper. First, we identify the topical
subfields authors work on by assigning auto-
matically generated topics to each paper in the
ACL Anthology from 1980 to 2008. Next, we
identify four distinct research epochs where
the pattern of topical overlaps are stable and
different from other eras: an early NLP pe-
riod from 1980 to 1988, the period of US
government-sponsored MUC and ATIS eval-
uations from 1989 to 1994, a transitory period
until 2001, and a modern integration period
from 2002 onwards. Third, we analyze the
flow of authors across topics to discern how
some subfields flow into the next, forming dif-
ferent stages of ACL research. We find that the
government-sponsored bakeoffs brought new
researchers to the field, and bridged early top-
ics to modern probabilistic approaches. Last,
we identify steep increases in author retention
during the bakeoff era and the modern era,
suggesting two points at which the field be-
came more integrated.
1 Introduction
The rise of vast on-line collections of scholarly pa-
pers has made it possible to develop a computational
history of science. Methods from natural language
processing and other areas of computer science can
be naturally applied to study the ways a field and
its ideas develop and expand (Au Yeung and Jatowt,
2011; Gerrish and Blei, 2010; Tu et al, 2010; Aris et
al., 2009). One particular direction in computational
history has been the use of topic models (Blei et al,
2003) to analyze the rise and fall of research top-
ics to study the progress of science, both in general
(Griffiths and Steyvers, 2004) and more specifically
in the ACL Anthology (Hall et al, 2008).
We extend this work with a more people-centered
view of computational history. In this framework,
we examine the trajectories of individual authors
across research topics in the field of computational
linguistics. By examining a single author?s paper
topics over time, we can trace the evolution of her
academic efforts; by superimposing these individual
traces over each other, we can learn how the entire
field progressed over time. One goal is to investi-
gate the use of these techniques for computational
history in general. A second goal is to use the ACL
Anthology Network Corpus (Radev et al, 2009) and
the incorporated ACL Anthology Reference Corpus
(Bird et al, 2008) to answer specific questions about
the history of computational linguistics. What is the
path that the ACL has taken throughout its 50-year
history? What roles did various research topics play
in the ACL?s development? What have been the piv-
otal turning points?
Our method consists of four steps. We first run
topic models over the corpus to classify papers into
topics and identify the topics that people author in.
We then use these topics to identify epochs by cor-
relating over time the number of persons that topics
share in common. From this, we identify epochs as
sustained patterns of topical overlap.
Our third step is to look at the flow of authors be-
tween topics over time to detect patterns in how au-
thors move between areas in the different epochs.
We group topics into clusters based on when au-
thors move in and out of them, and visualize the flow
13
of people across these clusters to identify how one
topic leads to another.
Finally, in order to understand how the field grows
and declines, we examine patterns of entry and exit
within each epoch, studying how author retention
(the extent to which authors keep publishing in the
ACL) varies across epochs.
2 Identifying Topics
Our first task is to identify research topics within
computational linguistics. We use the ACL Anthol-
ogy Network Corpus and the incorporated ACL An-
thology Reference Corpus, with around 13,000 pa-
pers by approximately 11,000 distinct authors from
1965 to 2008. Due to data sparsity in early years, we
drop all papers published prior to 1980.
We ran LDA on the corpus to produce 100 genera-
tive topics (Blei et al, 2003). Two senior researchers
in the field (the third author and Chris Manning) then
collaboratively assigned a label to each of the 100
topics, which included marking those topics which
were non-substantive (lists of function words or af-
fixes) to be eliminated. They produced a consensus
labeling with 73 final topics, shown in Table 1 (27
non-substantive topics were eliminated, e.g. a pro-
noun topic, a suffix topic, etc.).
Each paper is associated with a probability distri-
bution over the 100 original topics describing how
much of the paper is generated from each topic. All
of this information is represented by a matrix P ,
where the entry Pij is simply the loading of topic
j on paper i (since each row is a probability distri-
bution,
?
j Pij = 1). For ease of interpretation, we
sparsify the matrix P by assigning papers to topics
and thus set al entries to either 0 or 1. We do this
by choosing a threshold T and setting entries to 1 if
they exceed this threshold. If we call the new ma-
trix Q, Qij = 1 ?? Pij ? T . Throughout all
our analyses we use T = 0.1. This value is approx-
imately two standard deviations above P , the mean
of the entries in P . Most papers are assigned to 1
or 2 topics; some are assigned to none and some are
assigned to more.
This assignment of papers to topics also induces
an assignment of authors to topics: an author is as-
signed to a topic if she authored a paper assigned
to that topic. Furthermore, this assignment is natu-
rally dynamic: since every paper is published in a
particular year, authors? topic memberships change
over time. This fact is at the heart of our methodol-
ogy ? by assigning authors to topics in this princi-
pled way, we can track the topics that authors move
through. Analyzing the flow of authors through top-
ics enables us to learn which topics beget other top-
ics, and which topics are related to others by the peo-
ple that author across them.
3 Identifying Epochs
What are the major epochs of the ACL?s history? In
this section, we seek to partition the years spanned
by the ACL?s history into clear, distinct periods of
topical cohesion, which we refer to as epochs. If
the dominant research topics people are working on
suddenly change from one set of topics to another,
we view this as a transition between epochs.
To identify epochs that satisfy this definition, we
generate a set of matrices (one for each year) de-
scribing the number of people that author in every
pair of topics during that year. For year y, let Ny
be a matrix such that Nyij is the number of people
that author in both topics i and j in year y (where
authoring in topic j means being an author on a pa-
per p such that Qpj = 1). We don?t normalize by
the total number of people in each topic, thus pro-
portionally representing bigger topics since they ac-
count for more research effort than smaller topics.
Each matrix is a signature of which topic pairs have
overlapping author sets in that year.
From these matrices, we compute a final matrix
C of year-year correlations. Cij is the Pearson cor-
relation coefficient between N i and N j . C captures
the degree to which years have similar patterns of
topic authorship overlap, or the extent to which a
consistent pattern of topical research is formed. We
visualize C as a thermal in Figure 1.
To identify epochs in ACL?s history, we ran hier-
archical complete link clustering on C. This resulted
in a set of four distinct epochs: 1980?1988, 1989?
1994, 1995?2001, and 2002?2008. For three of
these periods (all except 1995?2001), years within
each of these ranges are much more similar to each
other than they are to other years. During the third
period (1995?2001), none of the years are highly
similar to any other years. This is indicative of a
14
Number Name Topics
1 Big Data
NLP
Statistical Machine Translation (Phrase-Based): bleu, statistical, source, target, phrases, smt, reordering
Dependency Parsing: dependency/ies, head, czech, depen, dependent, treebank
MultiLingual Resources: languages, spanish, russian, multilingual, lan, hindi, swedish
Relation Extraction: pattern/s, relation, extraction, instances, pairs, seed
Collocations/Compounds: compound/s, collocation/s, adjectives, nouns, entailment, expressions, MWEs
Graph Theory + BioNLP: graph/s, medical, edge/s, patient, clinical, vertex, text, report, disease
Sentiment Analysis: question/s, answer/s, answering, opinion, sentiment, negative, positive, polarity
2 Probabilistic
Methods
Discriminative Sequence Models: label/s, conditional, sequence, random, discriminative, inference
Metrics + Human Evaluation: human, measure/s, metric/s, score/s, quality, reference, automatic, correlation, judges
Statistical Parsing: parse/s, treebank, trees, Penn, Collins, parsers, Charniak, accuracy, WSJ
ngram Language Models: n-gram/s, bigram/s, prediction, trigram/s, unigram/s, trigger, show, baseline
Algorithmic Efficiency: search, length, size, space, cost, algorithms, large, complexity, pruning
Bilingual Word Alignment: alignment/s, align/ed, pair/s, statistical, source, target, links, Brown
ReRanking: score/s, candidate/s, list, best, correct, hypothesis, selection, rank/ranking, scoring, top, confidence
Evaluation Metrics: precision, recall, extraction, threshold, methods, filtering, extract, high, phrases, filter, f-measure
Methods (Experimental/Evaluation): experiments, accuracy, experiment, average, size, 100, baseline, better, per, sets
Machine Learning Optimization: function, value/s, parameter/s, local, weight, optimal, solution, criterion, variables
3 Linguistic
Supervision
Biomedical Named Entity Recognition: biomedical, gene, term, protein, abstracts, extraction, biological
Word Segmentation: segment/ation, character/s, segment/s, boundary/ies, token/ization
Document Retrieval: document/s, retrieval, query/ies, term, relevant/ance, collection, indexing, search
SRL/Framenet: argument/s, role/s, predicate, frame, FrameNet, predicates, labeling, PropBank
Wordnet/Multilingual Ontologies: ontology/ies, italian, domain/s, resource/s, i.e, ontological, concepts
WebSearch + Wikipedia: web, search, page, xml, http, engine, document, wikipedia, content, html, query, Google
Clustering + Distributional Similarity: similar/ity, cluster/s/ing, vector/s, distance, matrix, measure, pair, cosine, LSA
Word Sense Disambiguation: WordNet, senses, disambiguation, WSD, nouns, target, synsets, Yarowsky
Machine Learning Classification: classification, classifier/s, examples, kernel, class, SVM, accuracy, decision
Linguistic Annotation: annotation/s/ed, agreement, scheme/s, annotators, corpora, tools, guidelines
Tutoring Systems: student/s, reading, course, computer, tutoring, teaching, writing, essay
Chunking/Memory Based Models: chunk/s/ing, pos, accuracy, best, memory-based, Daelemans
Named Entity Recognition: entity/ies, name/s/d, person, proper, recognition, location, organization, mention
Dialog: dialogue, utterance/s, spoken, dialog/ues, act, interaction, conversation, initiative, meeting, state, agent
Summarization: topic/s, summarization, summary/ies, document/s, news, articles, content, automatic, stories
4 Discourse Multimodal (Mainly Generation): object/s, multimodal, image, referring, visual, spatial, gesture, reference, description
Text Categorization: category/ies, group/s, classification, texts, categorization, style, genre, author
Morphology: morphological, arabic, morphology, forms, stem, morpheme/s, root, suffix, lexicon
Coherence Relations: relation, rhetorical, unit/s, coherence, texts, chains
Spell Correction: error/s, correct/ion, spelling, detection, rate
Anaphora Resolution: resolution, pronoun, anaphora, antecedent, pronouns, coreference, anaphoric
Question Answering Dialog System: response/s, you, expert, request, yes, users, query, question, call, database
UI/Natural Language Interface: users, database, interface, a71, message/s, interactive, access, display
Computational Phonology: phonological, vowel, syllable, stress, phonetic, phoneme, pronunciation
Neural Networks/Human Cognition: network/s, memory, acquisition, neural, cognitive, units, activation, layer
Temporal IE/Aspect: event/s, temporal, tense, aspect, past, reference, before, state
Prosody: prosody/ic, pitch, boundary/ies, accent, cues, repairs, phrases, spoken, intonation, tone, duration
5 Early
Probability
Lexical Acquisition Of Verb Subcategorization: class/es, verb/s, paraphrase/s, subcategorization, frames
Probability Theory: probability/ies, distribution, probabilistic, estimate/tion, entropy, statistical, likelihood, parameters
Collocations Measures: frequency/ies, corpora, statistical, distribution, association, statistics, mutual, co-occurrences
POS Tagging: tag/ging, POS, tags, tagger/s, part-of-speech, tagged, accuracy, Brill, corpora, tagset
Machine Translation (Non Statistical + Bitexts): target, source, bilingual, translations, transfer, parallel, corpora
6 Automata Automata Theory: string/s, sequence/s, left, right, transformation, match
Tree Adjoining Grammars : trees, derivation, grammars, TAG, elementary, auxiliary, adjoining
Finite State Models (Automata): state/s, finite, finite-state, regular, transition, transducer
Classic Parsing: grammars, parse, chart, context-free, edge/s, production, CFG, symbol, terminal
Syntactic Trees: node/s, constraints, trees, path/s, root, constraint, label, arcs, graph, leaf, parent
7 Classic
Linguistics
Planning/BDI: plan/s/ning, action/s, goal/s, agent/s, explanation, reasoning
Dictionary Lexicons: dictionary/ies, lexicon, entry/ies, definition/s, LDOCE,
Linguistic Example Sentences: John, Mary, man, book, examples, Bill, who, dog, boy, coordination, clause
Syntactic Theory: grammatical, theory, functional, constituent/s, constraints, LFG
Formal Computational Semantics: semantics, logic/al, scope, interpretation, meaning, representation, predicate
Speech Acts + BDI: speaker, utterance, act/s, hearer, belief, proposition, focus, utterance
PP Attachment: ambiguity/ies/ous, disambiguation, attachment, preference, preposition
Natural Language Generation: generation/ing, generator, choice, generated, realization, content
Lexical Semantics: meaning/s, semantics, metaphor, interpretation, object, role
Categorial Grammar/Logic: proof, logic, definition, let, formula, theorem, every, iff, calculus
Syntax: clause/s, head, subject, phrases, object, verbs, relative, nouns, modifier
Unification Based Grammars: unification, constraints, structures, value, HPSG, default, head
Concept Ontologies / Knowledge Rep: concept/s, conceptual, attribute/s, relation, base
8 Government MUC-Era Information Extraction: template/s, message, slot/s, extraction, key, event, MUC, fill/s
Speech Recognition: recognition, acoustic, error, speaker, rate, adaptation, recognizer, phone, ASR
ATIS dialog: spoken, atis, flight, darpa, understanding, class, database, workshop, utterances
9 Early NLU 1970s-80s NLU Work: 1975-9, 1980-6, computer, understanding, syntax, semantics, ATN, Winograd, Schank, Wilks, lisp
Code Examples: list/s, program/s, item/s, file/s, code/s, computer, line, output, index, field, data, format
Speech Parsing And Understanding: frame/s, slot/s, fragment/s, parse, representation, meaning
Table 1: Results of topic clustering, showing some high-probability representative words for each cluster.
15
Figure 1: Year-year correlation in topic authoring
patterns. Hotter colors indicate high correlation,
colder colors denote low correlation.
state of flux in which authors are constantly chang-
ing the topics they are in. As such, we refer to
this period as a transitory epoch. Thus our analysis
has identified four main epochs in the ACL corpus
between 1980 and 2008: three focused periods of
work, and one transitory phase.
These epochs correspond to natural eras in the
ACL?s history. During the 1980?s, there were co-
herent communities of research on natural language
understanding and parsing, generation, dialog, uni-
fication and other grammar formalizations, and lex-
icons and ontologies.
The 1989?1994 era corresponds to a number of
important US government initiatives: MUC, ATIS,
and the DARPA workshops. The Message Under-
standing Conferences (MUC) were an early initia-
tive in information extraction, set up by the United
States Naval Oceans Systems Center with the sup-
port of DARPA, the Defense Advanced Research
Projects Agency. A condition of attending the MUC
workshops was participation in a required evalua-
tion (bakeoff) task of filling slots in templates about
events, and began (after an exploratory MUC-1 in
1987) with MUC-2 in 1989, followed by MUC-3
(1991), MUC-4 (1992), MUC-5 (1993) and MUC-
6 (1995) (Grishman and Sundheim, 1996). The
Air Travel Information System (ATIS) was a task
for measuring progress in spoken language under-
standing, sponsored by DARPA (Hemphill et al,
1990; Price, 1990). Subjects talked with a system
to answer questions about flight schedules and air-
line fares from a database; there were evaluations
in 1990, 1991, 1992, 1993, and 1994 (Dahl et al,
1994). The ATIS systems were described in pa-
pers at the DARPA Speech and Natural Language
Workshops, a series of DARPA-sponsored worksh-
sop held from 1989?1994 to which DARPA grantees
were strongly encouraged to participate, with the
goal of bringing together the speech and natural lan-
guage processing communities.
After the MUC and ATIS bakeoffs and the
DARPA workshops ended, the field largely stopped
publishing in the bakeoff topics and transitioned to
other topics; participation by researchers in speech
recognition also dropped off significantly. From
2002 onward, the field settled into the modern era
characterized by broad multilingual work and spe-
cific areas like dependency parsing, statistical ma-
chine translation, information extraction, and senti-
ment analysis.
In summary, our methods identify four major
epochs in the ACL?s history: an early NLP period,
the ?government? period, a transitory period, and a
modern integration period. The first, second, and
fourth epochs are periods of sustained topical co-
herence, whereas the third is a transitory phase dur-
ing which the field moved from the bakeoff work to
modern-day topics.
4 Identifying Participant Flows
In the previous section, we used topic co-
membership to identify four coherent epochs in the
ACL?s history. Now we turn our attention to a finer-
grained question: How do scientific areas or move-
ments arise? How does one research area develop
out of another as authors transition from a previous
research topic to a new one? We address this ques-
tion by tracing the paths of authors through topics
over time, in aggregate.
4.1 Topic Clustering
We first group topics into clusters based on how au-
thors move through them. To do this, we group years
into 3-year time windows and consider adjacent time
periods. We aggregate into 3-year windows because
16
the flow across adjacent single years is noisy and of-
ten does not accurately reflect shifts in topical fo-
cus. For each adjacent pair of time periods (for ex-
ample, 1980?1982 and 1983?1985), we construct a
matrix S capturing author flow between each topic
pair, where the Sij entry is the number of authors
who authored in topic i during the first time period
and authored in topic j during the second time pe-
riod. These matrices capture people flow between
topics over time.
Next we compute similarity between topics. We
represent each topic by its flow profile, which is sim-
ply the concatenation of all its in- and out-flows in
all of the S matrices. More formally, let Fi be the re-
sulting vector after concatenating the i-th row (trans-
posed into a column) and i-th column of every S
matrix. We compute a topic-topic similarity matrix
T where Tij is the Pearson correlation coefficient
between Fi and Fj . Two topics are then similar
if they have similar flow profiles. Note that topics
don?t need to share authors to be similar ? authors
just need to move in and out of them at roughly the
same times. Through this approach, we identify top-
ics that play similar roles in the ACL?s history.
To find a grouping of topics that play similar roles,
we perform hierarchical complete link clustering on
the T matrix. The goal is to identify clusters of
topics that are highly similar to each other but are
dissimilar from those in other clusters. Hierarchi-
cal clustering begins with every topic forming a sin-
gleton cluster, then iteratively merges the two most
similar clusters at every step until there is only one
cluster of all topics remaining. Every step gives
a different clustering solution, so we assess clus-
ter fitness using Krackhard and Stern?s E-I index,
which measures the sum of external ties minus the
sum of internal ties divided by the sum of all ties.
Given T as an input, the E-I index optimizes iden-
tical profiles as clusters (i.e., topic stages), not dis-
crete groups. The optimal solution we picked using
the E-I index entails 9 clusters (shown in Table 1),
numbered roughly backwards from the present to the
past. We?ll discuss the names of the clusters in the
next section.
4.2 Flows Between Topic Clusters
Now that we have grouped topics into clusters by
how authors flow in and out of them, we can com-
pute the flow between topics or between topic clus-
ters over time. First we define what a flow between
topics is. We use the same flow matrix used in the
above topic clustering: the flow between topic i in
one time period and topic j in the following time pe-
riod is simply the number of authors present in both
at the respective times. Again we avoid normaliz-
ing because the volume of people moving between
topics is relevant.
Now we can define flow between clusters. Let A
be the set of topics in cluster C1 and let B be the set
of topics in cluster C2. We define the flow between
C1 and C2 to be the average flow between topics in
A and B:
f(C1, C2) =
?
A?A,B?B f(A,B)
|A| ? |B|
(where f(A,B) represents the topic-topic flow
defined above). We also tried defining cluster-
cluster flow as the maximum over all topic-topic
flows between the clusters, and the results were
qualitatively the same.
Figure 2 shows the resulting flows between clus-
ters. Figure 2a shows the earliest period in our
(post-1980) dataset, where we see reflections of ear-
lier natural language understanding work by Schank,
Woods, Winograd, and others, quickly leading into
a predominance of what we?ve called ?Classic Lin-
guistic Topics?. Research in this period is charac-
terized by a more linguistically-oriented focus, in-
cluding syntactic topics like unification and catego-
rial grammars, formal syntactic theory, and preposi-
tional phrase attachments, linguistic semantics (both
lexical semantics and formal semantics), and BDI
dialog models. Separately we see the beginnings of
a movement of people into phonology and discourse
and also into the cluster we?ve called ?Automata?,
which at this stage includes (pre-statistical) Parsing
and Tree Adjoining Grammars.
In Figure 2b we see the movement of people
into the cluster of government-sponsored topics: the
ATIS and MUC bakeoffs, and speech.
In Figure 2c bakeoff research is the dominant
theme, but people are also beginning to move in and
out of two new clusters. One is Early Probabilistic
Models, in which people focused on tasks like Part
of Speech tagging, Collocations, and Lexical Acqui-
17
87 Classic Ling.
9 Early NLU
6
Automata
4
Discourse
5
2
3
1
6.9
2.6
0.8
1.0
1.7
1.4
0.6
0.8
0.6
0.7
(a) 1980?1983 ? 1984?1988
8
Gov?t
7
9 Early NLU
6
Automata
4
Discourse
5
2
3
1
3.1
1.8
1.6
0.7
0.7
1.2
1.0
0.9
1.8
(b) 1986?1988 ? 1989?1991
8
7
Classic Ling.
9
Early NLU
6
Automata
4
5
Early Prob.
2
Prob. Methods
3
1
19.6
3.1
2.6
2.8
2.7
2.1
2.7
2.4
2.3
2.1
(c) 1989?1991?1992?1994
8
7
Classic Ling.
9
6Automata
4
5Early Prob.
2
3
Ling. Supervision
1
3.7
1.2
2.7
1.5
1.8
1.1
1.0
3.4
0.9
0.9
(d) 1992?1994?1995?1998
8
7
9
6
4
5Early Prob.
2
Prob. Methods
3 Ling. Supervision
1 Big Data NLP6.7
3.3
3.2
3.7
3.6
2.6
5.7
4.2
2.9
2.7
(e) 2002?2004?2005?2007
Figure 2: Author flow between topic clusters in five key time periods. Clusters are sized according to how
many authors are in those topics in the first time period of each diagram. Edge thickness is proportional to
volume of author flow between nodes, relative to biggest flow in that diagram (i.e. edge thicknesses in are
not comparable across diagrams).
18
sition of Verb Subcategorization. People also begin
to move specifically from the MUC Bakeoffs into a
second cluster we call Probabilistic Methods, which
in this very early stage focused on Evaluations Met-
rics and Experimental/Evaluation Methods. People
working in the ?Automata? cluster (Tree Adjoining
Grammar, Parsing, and by this point Finite State
Methods) continue working in these topics.
By Figure 2d, the Early Probability topics are
very central, and probabilistic terminology and early
tasks (tagging, collocations, and verb subcategoriza-
tion) are quite popular. People are now moving
into a new cluster we call ?Linguistic Supervised?, a
set of tasks that apply supervised machine learning
(usually classification) to tasks for which the gold la-
bels are created by linguists. The first task to appear
in this area was Named Entity Recognition, popu-
lated by authors who had worked on MUC, and the
core methods topics of Machine Learning Classifi-
cation and Linguistic Annotation. Other tasks like
Word Sense Disambiguation soon followed.
By Figure 2e, people are leaving Early Probabil-
ity topics like part of speech tagging, collocations,
and non-statistical MT and moving into the Linguis-
tic Supervised (e.g., Semantic Role Labeling) and
Probabilistic Methods topics, which are now very
central. In Probabilistic Methods, there are large
groups of people in Statistical Parsing and N-grams.
By the end of this period, Prob Methods is sending
authors to new topics in Big Data NLP, the biggest of
which are Statistical Machine Translation and Sen-
timent Analysis.
In sum, the patterns of participant flows reveal
how sets of topics assume similar roles in the his-
tory of the ACL. In the initial period, authors move
mostly between early NLP and classic linguistics
topics. This period of exchange is then transformed
by the arrival of government bakeoffs that draw au-
thors into supervised linguistics and probabilistic
topics. Only in the 2000?s did the field mature and
begin a new period of cohesive exchange across a
variety of topics with shared statistical methods.
5 Member Retention and Field Integration
How does the ACL grow or decline? Do authors
come and go, or do they stay for long periods? How
much churn is there in the author set? How do these
1980 1985 1990 1995 2000 2005
First year of time frame
0.2
0.3
0.4
0.5
0.6
0.7
A
u
t
h
o
r
 
r
e
t
e
n
t
i
o
n
Figure 3: Overlap of authors in successive 3-year
time periods over time. The x-axis indicates the
first year of the 6-year time window being consid-
ered. Vertical dotted lines indicate epoch bound-
aries, where a year is a boundary if the first time
period is entirely in one epoch and the second is en-
tirely in the next.
trends align with the epochs we identified? To ad-
dress these questions, we examine author retention
over time ? how many authors stay in the field ver-
sus how many enter or exit.
In order to calculate membership churn, we cal-
culate the Jaccard overlap in the sets of people that
author in adjacent 3-year time periods. This met-
ric reflects the author retention from the first period
to the second, and is inherently normalized by the
number of authors (so the growing number of au-
thors over time doesn?t bias the trend). We use 3-
year time windows since it?s not unusual for authors
to not publish in some years while still remaining ac-
tive. We also remove the bulk of one-time authors by
restricting the authors under consideration to those
who have published at least 10 papers, but the ob-
served trend is similar for any threshold (including
no threshold). The first computation is the Jaccard
overlap between those who authored in 1980?1982
and those who authored in 1983?1985; the last is
between the author sets of the 2003?2005 and 2006?
2008 time windows. The trend is shown in Figure 3.
The author retention curve shows a clear align-
ment with the epochs we identified. In the first
19
epoch, the field is in its infancy: authors are work-
ing in a stable set of topics, but author retention is
relatively low. Once the bakeoff epoch starts, au-
thor retention jumps significantly ? people stay in
the field as they continue to work on bakeoff pa-
pers. As soon as the bakeoffs end, the overlap in
authors drops again. The fact that author retention
rocketed upwards during the bakeoff epoch is pre-
sumably caused by the strong external funding in-
centive attracting external authors to enter and re-
peatedly publish in these conferences.
To understand whether this drop in overlap of au-
thors was indeed indicative of authors who entered
the field mainly for the bakeoffs, we examined au-
thors who first published in the database in 1989. Of
the 50 most prolific such authors (those with more
than 8 publications in the database), 25 (exactly
half) were speech recognition researchers. Of those
25 speech researchers, 16 exited (never published
again in the ACL conferences) after the bakeoffs.
But 9 (36%) of them remained, mainly by adapting
their (formerly speech-focused) research areas to-
ward natural language processing topics. Together,
these facts suggest that the government-sponsored
period led to a large influx of speech recognition
researchers coming to ACL conferences, and that
some fraction of them remained, continuing with
natural language processing topics.
Despite the loss of the majority of the speech
recognition researchers at the end of the bakeoff
period, the author retention curve doesn?t descend
to pre-bakeoff levels: it stabilizes at a consistently
higher value during the transitory epoch. This may
partly be due to these new researchers colonizing
and remaining in the field. Or it may be due to the
increased number of topics and methods that were
developed during the government-sponsored period.
Whichever it is, the fact that retention didn?t return
to its previous levels suggests that the government
sponsorship that dominated the second epoch had a
lasting positive effect on the field.
In the final epoch, author retention monotonically
increases to its highest-ever levels; every year the
rate of authors publishing continuously rises, as does
the total number of members, suggesting that the
ACL community is coalescing as a field. It is plau-
sible that this final uptick is due to funding ? gov-
ernmental, industrial, or otherwise ? and it is an in-
teresting direction for further research to investigate
this possibility.
In sum, we observe two epochs where member
retention increases: the era of government bakeoffs
(1989?1994) and the more recent era where NLP
has received significantly increased industry interest
as well as government funding (2002?2008). These
eras may thus both be ones where greater external
demand increased retention and cohesion.
6 Conclusion
We offer a new people-centric methodology for
computational history and apply it to the AAN to
produce a number of insights about the field of com-
putational linguistics.
Our major result is to elucidate the many ways
in which the government-sponsored bakeoffs and
workshops had a transformative effect on the field
in the early 1990?s. It has long been understood that
the government played an important role in the field,
from the early support of machine translation to the
ALPAC report. Our work extends this understand-
ing, showing that the government-supported bake-
offs and workshops from 1989 to 1994 caused an in-
flux of speech scientists, a large percentage of whom
remained after the bakeoffs ended. The bakeoffs
and workshops acted as a major bridge from early
linguistic topics to modern probabilistic topics, and
catalyzed a sharp increase in author retention.
The significant recent increase in author overlap
also suggests that computational linguistics is in-
tegrating into a mature field. This integration has
drawn on modern shared methodologies of statistical
methods and their application to large scale corpora,
and may have been supported by industry demands
as well as by government funding. Future work will
be needed to see whether the current era is one much
like the bakeoff era with an outflux of persons once
funding dries up, or if it has reached a level of matu-
rity reflective of a well-established discipline.
Acknowledgments
This research was generously supported by the Of-
fice of the President at Stanford University and the
National Science Foundation under award 0835614.
Thanks to the anonymous reviewers, and to Steven
Bethard for creating the topic models.
20
References
A. Aris, B. Shneiderman, V. Qazvinian, and D. Radev.
2009. Visual overviews for discovering key papers and
influences across research fronts. Journal of the Amer-
ican Society for Information Science and Technology,
60(11):2219?2228.
C. Au Yeung and A. Jatowt. 2011. Studying how the
past is remembered: towards computational history
through large scale text mining. In Proceedings of
the 20th ACM international conference on Information
and knowledge management, pages 1231?1240. ACM.
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M. Joseph, M.Y.
Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan.
2008. The acl anthology reference corpus: A refer-
ence dataset for bibliographic research in computa-
tional linguistics. In Proc. of the 6th International
Conference on Language Resources and Evaluation
Conference (LREC?08), pages 1755?1759.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning Re-
search, 3(5):993?1022.
D.A. Dahl, M. Bates, M. Brown, W. Fisher, K. Hunicke-
Smith, D. Pallett, C. Pao, A. Rudnicky, and
E. Shriberg. 1994. Expanding the scope of the atis
task: The atis-3 corpus. In Proceedings of the work-
shop on Human Language Technology, pages 43?48.
Association for Computational Linguistics.
S. Gerrish and D.M. Blei. 2010. A language-based ap-
proach to measuring scholarly impact. In Proceed-
ings of the 26th International Conference on Machine
Learning.
T.L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences of the United States of America, 101(Suppl
1):5228.
R. Grishman and B. Sundheim. 1996. Message under-
standing conference-6: A brief history. In Proceedings
of COLING, volume 96, pages 466?471.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using topic
models. In Proceedings of EMNLP 2008.
C.T. Hemphill, J.J. Godfrey, and G.R. Doddington. 1990.
The atis spoken language systems pilot corpus. In Pro-
ceedings of the DARPA speech and natural language
workshop, pages 96?101.
P. Price. 1990. Evaluation of spoken language systems:
The atis domain. In Proceedings of the Third DARPA
Speech and Natural Language Workshop, pages 91?
95. Morgan Kaufmann.
D.R. Radev, P. Muthukrishnan, and V. Qazvinian. 2009.
The acl anthology network corpus. In Proceedings of
the 2009 Workshop on Text and Citation Analysis for
Scholarly Digital Libraries, pages 54?61. Association
for Computational Linguistics.
Y. Tu, N. Johri, D. Roth, and J. Hockenmaier. 2010. Ci-
tation author topic model in expert search. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics: Posters, pages 1265?1273. Asso-
ciation for Computational Linguistics.
21
