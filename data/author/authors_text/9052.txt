Paraphrasing of Chinese Utterances
Yujie Zhang?
Communications Research Laboratory
2-2-2, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
yujie@crl.go.jp
Kazuhide Yamamoto
ATR Spoken Language Translation Research Laboratories
2-2-2, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0288 Japan
yamamoto@fw.ipsj.or.jp
Abstract
One of the key issues in spoken language trans-
lation is how to deal with unrestricted expres-
sions in spontaneous utterances. This research
is centered on the development of a Chinese
paraphraser that automatically paraphrases ut-
terances prior to transfer in Chinese-Japanese
spoken language translation. In this paper, a
pattern-based approach to paraphrasing is pro-
posed for which only morphological analysis is
required. In addition, a pattern construction
method is described through which paraphras-
ing patterns can be efficiently learned from a
paraphrase corpus and human experience. Us-
ing the implemented paraphraser and the ob-
tained patterns, a paraphrasing experiment was
conducted and the results were evaluated.
1 Introduction
In spoken language translation one of the key
issues is how to deal with unrestricted expres-
sions in spontaneous utterances. To resolve this
problem, we have proposed a paraphrasing ap-
proach in which the utterances are automati-
cally paraphrased prior to transfer (Yamamoto
et al, 2001; Yamamoto, 2002). The paraphras-
ing process aims to bridge the gap between the
unrestricted expressions in the input and the
limited expressions that the transfer can trans-
late. In fact, paraphrasing actions are often seen
in daily communication. When a listener can-
not understand what a speaker said, the speaker
usually says it again using other words, i.e., he
paraphrases. In a Chinese-Japanese spoken lan-
guage translation system, the pre-processing of
Chinese utterances is involved and we attempt
to apply a paraphrasing approach. This paper
? This work was done when the author stayed at ATR
Spoken Language Translation Research Laboratories.
is focused on the paraphrasing of Chinese utter-
ances.
Some cases of paraphrasing research with cer-
tain targets have been reported. For example,
there has been work on rewriting the source
language in machine translation with a focus
on reducing syntactic ambiguities (Shirai et al,
1993), research on paraphrasing paper titles
with a focus on transforming syntactic struc-
tures to achieve readability (Sato, 1999), and
research on paraphrasing Japanese in summa-
rization with a focus on transforming a noun
modifier into a noun phrase (Kataoka et al,
1999). We have reported some research on
Chinese paraphrasing (Zhang and Yamamoto,
2001; Zhang et al, 2001; Zong et al, 2001). The
techniques of paraphrasing natural language
can be applied not only to the pre-processing
of machine translation but also to information
retrieval and summarization.
2 Goals and Approach
In the pre-processing stage of translation, Chi-
nese paraphrasing focuses on
(1) transforming the expressions of spoken lan-
guage into formal expressions,
(2) reducing syntactic and semantic ambigui-
ties,
(3) generating as many different expressions as
possible in order to include expressions that
can be translated by the transfer, and
(4) paraphrasing the main constituents of the
utterance in case the paraphrasing of the
whole utterance has no effect.
The aim of paraphrasing types (1), (2) and (4)
is to simplify the expressions of utterances, and
that of paraphrasing type (3) is to increase the
variations of utterances. At present, we focus
on paraphrasing types (1), (2) and (3).
Paraphrasing is a process that automatically
generates new expressions that have the same
meaning as the input sentence. At first glance
one would think that the problem could be re-
solved by separating it into two processes: the
parsing process that analyzes the input sentence
and obtains its meaning, and the generation
process that generates sentences from the ob-
tained meaning. However, this solution is not
practicable for the following reasons.
? At present, the techniques of parsing and
semantics analysis of the Chinese language
are far below the level needed for appli-
cation. When studying spoken language,
research on parsing and research on se-
mantics analysis are major themes them-
selves. For automatic paraphrasing, we
should first determine what kind of anal-
ysis is required and then start to develop a
parser or a semantics analyzer.
? Even if meanings can be obtained, goal (3)
cannot be achieved if only one sentence is
generated. Here, the demand that para-
phrasing should generate multiple expres-
sions is the most important. This focus is
different from that of conventional sentence
generation.
In fact, the paraphrasing can be conducted
at many different levels, for instance, words,
phrases, or larger constituents. Although the
paraphrasing of such constituents is probably
related to context, it is not true that paraphras-
ing is impossible without being able to under-
stand the whole sentence (Kataoka et al, 1999).
The paraphrasing process encounters the fol-
lowing problems. (i) How to identify objects,
i.e., which components of an input sentence will
be paraphrased, (ii) how to generate new sen-
tences, and (iii) how to ensure that the gener-
ated sentences have the same meaning as the
input sentence. In order to avoid the large cost
of syntax and semantics analysis, we propose
a pattern-based approach to paraphrasing in
which only morphological analysis is required.
The focus is placed on how to generate as many
different expressions as possible and how to get
paraphrasing patterns from a paraphrasing cor-
pus.
Table 1. Part of the part-of-speech tag set of
the Penn Chinese Treebank
Symbol Explanation
NN common noun
NR proper noun
PN pronoun
DT determiner
DEC  in a relative-clause
DEG associative 
M measure word
JJ other noun-modifier
VA predicative adjective
VC 
VE  as the main verb
VV other verb
AD adverb
P preposition excl.  and 
LC localizer
CD cardinal number
OD ordinal number
SP sentence-final particle
BA  in ba-construction
CC coordinating conjunction
3 Paraphrasing Pattern
The paraphrase corpus of the spoken Chinese
language consists of 20,000 original sentences
and 44,480 paraphrases, one original sentence
having at least two paraphrases (Zhang et al,
2001). The paraphrases were obtained by the
manual rephrasing of the original sentences:
words may be reordered, some words may be
substituted with synonyms, or the syntactic
structures may be changed. Such a paraphrase
corpus contains the knowledge of how to gener-
ate paraphrases for one sentence. We intend to
get paraphrasing patterns from the corpus. By
pairing each paraphrase with its corresponding
original sentence, 44,480 pairs were obtained.
Hereafter, we call such pairs paraphrase pairs.
Word segmentation and part-of-speech tagging
were carried out on the paraphrase pairs. The
part-of-speech tagger accepted the Penn Chi-
nese Treebank tag set, which comprises 33
parts-of-speech (Xia, 2000). A part of the Penn
Chinese Treebank tag set is shown in Table 1.
3.1 Extraction of Instances
For one paraphrase pair, the paraphrase may
differ from its original sentence in one of the
following paraphrasing phenomena: (1) word
order, (2) substitution of synonyms, and (3)
change of syntactic structure. For most para-
phrase pairs, the paraphrases contain a mixture
of the above phenomena. We need to classify
the paraphrasing phenomena and learn the rela-
tive paraphrasing patterns. In this way, we can
restrict the paraphrasing process to some lan-
guage phenomena and summarize the changes
in the information of the resultant paraphrases.
The following paraphrasing phenomena were
considered and related paraphrase pairs were
extracted.
3.1.1 Word Order
Word order in the spoken Chinese is com-
paratively free. In the paraphrase corpus,
quite a large proportion of the paraphrases
is created by word reordering. We extracted
the paraphrase pairs in which the morpheme
number of the original sentence is equal to that
of the paraphrase and each morpheme of the
original sentence appears in the paraphrase and
vice versa. One example is shown in 3-1.
[3? 1] An extracted paraphrase pair.
Original:  /VV  /AD  /VV
 /P  /PN  /VA 	 /SP
(Please call me again, could you?)
Paraphrase:  /VV  /AD  /P  /PN
 /VV  /VA 	 /SP
Guided by the extracted paraphrase pair, we
can in fact paraphrase the original sentence by
reordering its words according to the word order
of the paraphrase. The extracted paraphrase
pairs of this kind provided instances for learning
word order paraphrasing patterns.
3.1.2 Negative Expressions
In some paraphrase pairs, we observed that
paraphrasing phenomena were related to
negative expressions. For example, original
sentences include negative words ? (do not
)? or ? (did not)? , but their corresponding
paraphrases appear as affirmative forms with-
out these negative words. This fact implied
that the sentences could be simplified by delet-
ing the negative expressions. For this purpose,
the paraphrase pairs were extracted in which
the original sentences included the words ??
or ?? and the corresponding paraphrases did
not. One example is shown in 3-2.
[3? 2]
Original: ? /VV ? /AD ?? /VV  /PN 

/DEG ?? /NN
(Do you know my telephone number?)
Paraphrase: ?? /VV  /PN 
 /DEG ??
/NN 	 /SP
3.1.3 Expression of ??
The Chinese language has a few grammatical
markers. The particle ?? is one of such mark-
ers. The sentences with the form ?S(subject)
V(verb) O(object) C(complement)? may be
changed into the form ?S  O V C? by
inserting the particle ?? (Zhang and Sato,
1999). The usage of ?? emphasizes the
object by moving it before the verb. When
the particle ?? is in a sentence, it is easier
to identify the object. So the insertion of ??
will supply more information about syntactic
structure and reduce syntactic ambiguities.
Moreover, paraphrasing the sentences with
particle ?? may be more exact because the
identification of the object is more accurate.
We extracted the paraphrase pairs in which the
original sentences included the particle ??
and the corresponding paraphrases did not.
See example 3-3 below.
[3? 3]
Original:  /DT  /M  /NN  /VV 
/PN  /VV
(Could you fill out this form, please.)
Paraphrase:  /VV  /PN  /BA  /DT
 /M  /NN  /VV
(Could you make this form filled out, please.)
3.2 Automatic Generalization of
Instances
Then we attempted to generalize the extracted
instances in order to obtain paraphrasing pat-
terns. For each extracted paraphrase pair, the
original sentence is generalized to make the
matching part of the pattern, and the para-
phrase is generalized to make the generation
part of the pattern. The matching part spec-
ifies the components that will be paraphrased
as well as the context conditions. The genera-
tion part defines how to construct a paraphrase.
When the constituted pattern is applied to one
input sentence, if the input matches with the
matching part, a new sentence will be generated
according to the generation part.
In fact, the purpose of generalization is to
get a regular expression from the original sen-
tence and to get an operation expression con-
taining substitutions from the paraphrase. As
shown in 3-3, both the original sentence and the
paraphrase are series of morphemes, and each
morpheme consists of a part-of-speech and an
orthographic expression. The important thing
in paraphrasing is to maintain meaning. To
what extent the series of morphemes will be
generalized depends on each paraphrasing pair.
First, parts-of-speech keep the syntactic infor-
mation and therefore they should be kept. Sec-
ond, orthographic expressions of verbs, auxil-
iary verbs, adverbs, etc., are important in de-
ciding the main meaning of the sentence and
therefore they should also be kept. The ortho-
graphic expressions of other categories, such as
nouns, pronouns and numerals, can be general-
ized to an abstract level by replacing each or-
thographic expression with a wild card.
The pattern generalized from 3-3 is illus-
trated in 3-4. The left part is the matching part
and the right part is the generation part. The
lexical information may be an orthographic
expression or a variable represented by symbol
Xi. Xi in the matching part is in fact a
wild card, which means it can match with
any orthographic expression in the matching
operation. Xi in the generation part defines a
substitution operation.
[3? 4] A generalized pattern.
 /DT  /M X1/NN  /VV X2/PN  /VV
?  /VV X2/PN  /BA  /DT  /M X1/NN
 /VV
However, we found two problems in this kind
of automatic generalization. The first is that re-
strictions on the patterns generalized from long
sentences are too specific at the lexical level. In
fact, the clauses and noun phrases used as modi-
fiers have no effect on the considered paraphras-
ing phenomena and can be generalized further.
The second is that some orthographic expres-
sions with important meanings are generalized
to wild cards, for instance, the numeral ?	

(how many)? may imply that the sentence is
interrogative. Therefore, a method is needed
to prevent some orthographic expressions from
being automatically replaced with wild cards.
3.3 Semi-Automatic Generalization of
Instances
Specifying which morphemes should be general-
ized and which orthographic expressions should
be kept requires human experience. In order to
integrate human experience into automatic gen-
eralization, we developed a semi-automatic gen-
eralization tool. The tool consists of description
symbols and a transformation program. The
description symbols are designed for people to
define generalization information on instances,
and the transformation program automatically
transforms the defined instances into patterns.
Three description symbols are defined as fol-
lows.
[ ]: This symbol is followed by a numeral
and is used to enclose a sequence of mor-
phemes. The enclosed part is a syntac-
tic component, e.g., a noun phrase or a
clause. Except for the part-of-speech of the
last morpheme, the enclosed part will be
replaced with a variable. In the Chinese
language, the syntactic property of a se-
quence of words is most likely reflected in
the last word, so we keep the part-of-speech
of the last morpheme. The enclosed parts
in the original sentence and the paraphrase
denoted by the same numerals will be re-
placed with the same variables.
{ }: This symbol is used to enclose a mor-
pheme. The orthographic expression of the
morpheme will be kept. In this way, the
lexical information of morphemes can be
utilized to define the context. A few or-
thographic expressions can be defined in-
side one symbol so that words that can be
paraphrased in the same way can be stored
as one pattern.
? ?: This symbol is used to enclose a mor-
pheme. The orthographic expression of the
morpheme will be replaced with a variable.
In this way, the orthographic expressions of
verbs or adverbs can also be generalized.
The usage of the symbols is explained in 3-5 and
3-6. Example 3-5 is a paraphrase pair in which
description symbols are defined. Example 3-6 is
the paraphrasing pattern generalized from 3-5.
[3? 5] A defined instance.
Original:  /VV  /VV  /PN  /CD ?
/M? [ /NN 
 /DEG]1 [ /NN  /NN]2
(Could you give me two copies of the Japanese
pamphlet, please?)
Paraphrase: [ /NN 
 /DEG]1 [Machine Translation by Interaction
between Paraphraser and Transfer
Kazuhide Yamamoto
ATR Spoken Language Translation Research Laboratories
2-2-2, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288 Japan
yamamoto@fw.ipsj.or.jp
Abstract
A machine translation model has been pro-
posed where an input is translated through
both source-language and target-language para-
phrasing processes. We have implemented our
prototype model for the Japanese-Chinese lan-
guage pair. This paper describes our core idea
of translation, where a source language para-
phraser and a language transfer cooperates in
translation by exchanging information about
the source input.
1 Introduction
Humans generally have language capability,
mostly for their mother tongue and to a lesser
extent for foreign languages. This leads us
to making the most of our mother language,
even in conducting translation. That is, when
we translate our language into a foreign one
unfamiliar to us, we may try to paraphrase
the source input into easier expressions we can
translate.
In contrast, there is no such machine trans-
lation (MT) model so far proposed where the
source language module is biased over the bilin-
gual language module. All of the MT models are
either those where the bilingual processor takes
the initiative over the source language analyzer
(conventional analyze-transfer-generate model)
or integration models of analyzer and transfer,
such as example-based or statistical models. Al-
though some MT models have a paraphraser
(also called a ?pre-editor?), such as that of Shirai
et al (1993), paraphrasing is performed in these
models because it is necessary to prepare for the
subsequent bilingual process. In other words,
the paraphraser operates as a sub-module for
successful transfer.
We have proposed a new MT model that
is more similar to the human translation pro-
cess than other MT systems (Yamamoto et
al., 2001). This model, called the Sandglass
model, is designed so that the system can gener-
ate a translation through source language para-
phrasing, even if the system does not have
sufficient bilingual knowledge. In this sense,
our model design can be considered a non-
professional translator?s model.
From the engineering point of view, our
model has an advantage in language portability;
it is easy to construct an MT for a new language,
since our model depends only on source lan-
guage and thus can reduce dependence on bilin-
gual knowledge. Moreover, the better source
language paraphraser we make, the easier the
implementation of other language MT becomes.
Another advantage is task portability, since all
of the paraphrasing knowledge, except for lex-
ical paraphrasing knowledge, is independent of
the task, so we do not need to fit most of the
paraphrasing knowledge to the required task. It
is also significant that this model?s paraphraser
can be employed not only for MT but also for
most natural language processing (NLP) appli-
cations. This is possible because both the input
and output of a paraphraser is the same natural
language.
We have been building the Sandglass MT
system for the Japanese-Chinese, Chinese-
Japanese language pairs (Yamamoto et al,
2001; Zhang and Yamamoto, 2002). We have
already constructed a prototype for Japanese-
Chinese. In this paper, we report the core con-
cepts of this prototype and discuss issues of both
our principle and our implementation.
2 Sandglass Translation Model
Figure 1 shows our paradigm for a translation
model. In the conventional MT model, the pro-
cess load and the information used to deal with
Conventional paradigm
Sandglass paradigm
translation process
load
maximum load in a
bilingual transfer 
process
maximum load in a
monolingual process
by paraphrasing
Figure 1: Comparison of the two MT paradigms
it are maximized in the transfer module; how-
ever, we propose that they be minimized in the
transfer in consideration of language portability
and task portability.
This translation approach is effective in MT
where neither the source- nor target-language
is English. Although there are a large number
of bilingual corpora currently available, most of
them are between English and other language.
This suggests that it is not useful to apply
bilingual-corpus-based approaches to situations
not involving English. Moreover, conventional
approaches based on hand-written rules are also
unsuccessful due to lack of bilingual speakers of
non-English pairs.
We also assume that reduction of bilingual
processing costs is crucial for multilingual MT
construction. Although both interlingual MT
and MT with controlled language satisfies this
request, our MT paradigm has an advantage
in that it does not require design of interlin-
gua/controlled language, which can be a critical
problem.
2.1 Modularity and paraphrasing
strategy
The Sandglass translation model has a
source language paraphraser (hereafter the
paraphraser) and a bilingual language transfer
(hereafter the transfer), which have high modu-
larity with each other in order to develop them
as independently as possible. One of our aims
in this model is to develop a general-purpose
paraphraser that can also be used in other NLP
applications.
When the system has modularity, the para-
phraser does not need to consider the knowl-
edge or translation capability of the transfer.
However, the paraphraser has trouble in plan-
ning a paraphrasing strategy, since the purpose
of paraphrasing in this model is to help small-
knowledge transfer. One may think of it as a
solution to generate all possible paraphrases,
transfer them into the target language, and se-
lect the best one among the successful outputs.
We believe that, although this strategy works,
it is not practical due to the computation cost.
In many cases, there are local paraphrases pos-
sible for one input, which may result in com-
binatorial explosion for generating paraphrases.
Moreover, this strategy leads to a more serious
problem in speech translation that requires real-
time computation.
As an alternative, we propose the following
strategy for planning paraphrases. We first
put the controller between the paraphraser and
the transfer. The controller communicates with
both the paraphraser and the transfer and ex-
changes information between them on the target
sentence be translated. As opposed to the one-
way information path from the paraphraser to
the transfer, a bi-directional information flow
enables cooperation by allowing each module
to provide its counterpart with information on
what is possible and what is impossible.
This kind of process is not necessary in the
typical MT model, since each process has the re-
sponsibility to perform its mission successfully
and giving up is never allowed. If one of the
processes gives up its mission, the entire transla-
tion process also gives up and fails. On the con-
trary, our model (sometimes) allows the transfer
to give up generating the target language. Al-
though this responsibility continuously enlarges
the transfer knowledge, it is one of the critical
problems of the typical MT. In general, in or-
der to avoid a fatty transfer, we propose shifting
the responsibility of generating the target lan-
guage from the transfer process to monolingual
processes.
<1> "W1 W2 W3 W4"
<2>  W3 -> W3+ ?
<3> "W1 W2 W3+ W4"
W3 -> W3+
W5 -> W5+ W6
W10 W11-> W12
...
  W1 W2 W3+ W4 -> ...
  W1 W5 W3 -> ...
  W7 W8 W9 W10 -> ...
      ...
Transfer 
Knowledge
Paraphrasing 
Knowledge
Controller TransferParaphraser
NG
OK
To Target 
Language
Paraphraser 
<5>
<4>
OK
Figure 2: Translation strategy by interaction of
the two modules
2.2 Interaction between paraphrasing
and transfer
Figure 2 illustrates our translation strategy.
The translator mainly consists of the para-
phraser and the transfer, where a controller
is located between the two modules in order
to control the information flow1. This model
has the following characteristics: (1) the para-
phraser and the transfer are equivalent in terms
of process sequences, i.e., the process flow is not
an assembly line type, and (2) the knowledge for
paraphrasing and that for transferring are sep-
arated so that the paraphraser and the trans-
fer are responsible for monolingual and bilingual
processing, respectively.
The translation process is achieved as follows.
The output of word segmentation and part-
of-speech (POS) tagging is first attempted to
transfer to the target language through the con-
troller. Assume that a sequence of morphemes
W1W2W3W4, where Wi is each morpheme, fails
to transfer (process ?1? in the figure).
In this case, the transfer may obtain informa-
tion on the failed input morphemes that is use-
ful for the paraphrasing strategy, such as similar
morpheme sequences that can be transferred or
1For simplicity, other parts of the translator are hid-
den in the figure.
parts of the input that are impossible to trans-
fer. Our transfer can obtain expressions similar
to the input, if any exists, when the transfer
fails. In this example, the transfer found that
the morpheme sequence W1W2W3+W4 is simi-
lar to one in its knowledge, i.e., it understands
that the input can be transferred if W3 can be
paraphrased into W3+. Accordingly, the trans-
fer provides this information to the paraphraser
as a paraphrasing hint (process ?2?).
Then the paraphraser attempts to use this
suggestion prior to other paraphrasing trials. It
judges whetherW3 is replaceable byW3+, and if
it has such knowledge, it paraphrases based on
the transfer hint and returns this paraphrase to
the transfer (process ?3?). Again, the transfer
carries out a new trial and it succeeds in transla-
tion this time (process ?4?). Finally, the target
language expression is passed to the subsequent
processes (process ?5?).
Among the possibilities other than those
shown in the figure, if the transfer cannot find
any similar expression, the paraphraser then
attempts to rewrite the input by utilizing its
own paraphrasing knowledge. Similarly, if the
paraphraser cannot accept the rewriting hint
that the transfer suggests, the paraphraser also
thinks by itself.
2.3 Paraphraser
Currently, our paraphraser can deal with six
paraphrasing types: (1) verification of the
transfer?s suggestion, (2) input segmentation,
(3) reduction of honorific expressions (Ohtake
and Yamamoto, 2001), (4) simplification of
functional words (Yamamoto, 2001), (5) chunk-
ing of noun phrases, and (6) deletion of minor
elements. Paraphrasing is conducted in this or-
der. If one of the pattern conditions in this
paraphrasing knowledge is matched, the para-
phraser then finishes and returns its paraphrase;
no other paraphrase is pursued.
(1) As the first type of the paraphrasing, the
paraphraser verifies the paraphrasing hint that
the transfer suggests, if any. In our model, all of
the suggested paraphrasing rules are formed as
single-morpheme replacements, most of which
are functional words. Therefore, the para-
phraser has a list of these types of rephrasing
rules in advance to verify the suggestion. We
have built a list that contains 175 replacement
patterns.
Ex.1 	

?	
(It seems interesting.)
Ex.2 
?
(Until what time is it?)
In the above two examples, a sentence-final
particle and an auxiliary verb are replaced, re-
spectively. These slight differences should be
merged before bilingual processing in order to
restrict unnecessary combinations in the target
language.
(2) If the verification fails, the paraphraser
then attempts to split the input utterance ac-
cording to the pre-defined segmentation rules.
This is necessary because we are dealing with
spoken language, which has no clear sentence
boundaries. The segmentation rules, consisting
of 30 rules, are defined by checking sequences
of either word or POS. For example, in many
cases, if there is a sentence-final particle, then
the input is segmented after that word. In the
following example, a segmentation border is de-
scribed by the symbol ?;?.
Ex.3 
?;
(So, see you!)
Ex.4 
?;
(How much? That one.)
It is possible to regard the above two exam-
ples as single sentences, so it is difficult in gen-
eral in Japanese speech to determine whether
to segment them or not. However, this is not
a problem in the proposed method because our
segmentation is conducted only if the transfer
fails to deal with the input as a single sentence.
(3) Honorific expressions are seen in Japanese
speech very frequently. These expressions in-
volve many variations for expressing one sense,
so they should be unified before the transfer to
avoid the great amount of increase in unnec-
essary bilingual knowledge that would be ex-
pected. Our paraphraser for honorifics, which
was proposed by Ohtake and Yamamoto (2001),
reduces such variations to as few as possible. We
have 212 paraphrasing patterns for honorific ex-
pressions.
Ex.5 Acquisition of Lexical Paraphrases from Texts
Kazuhide Yamamoto
ATR Spoken Language Translation Research Laboratories
2-2-2, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288 Japan
yamamoto@fw.ipsj.or.jp
Abstract
Automatic acquisition of paraphrase knowledge
for content words is proposed. Using only a
non-parallel text corpus, we compute the para-
phrasability metrics between two words from
their similarity in context. We then filter words
such as proper nouns from external knowledge.
Finally, we use a heuristic in further filtering to
improve the accuracy of the automatic acqui-
sition. In this paper, we report the results of
acquisition experiments.
1 Introduction
Paraphrasing research has attracted increased
attention, and the work in this field has become
more active recently. Paraphrasing involves var-
ious types of transformations of expressions into
the same language, and thus there is generally
no all-purpose design and information resource.
Among the many types of paraphrasing, a hand-
written construction may be best for syntactic
paraphrasing knowledge or knowledge of func-
tional words because the number of resulting
phenomena can be counted. On the other hand,
we need to acquire lexical paraphrasing knowl-
edge automatically or efficiently, since there is
an enormous number of phenomena observed for
an enormous number of content words.
Some works, such as Barzilay and McKeown
(2001), have acquired paraphrasing knowledge
automatically. All of those works found dif-
ferences from a paraphrase corpus, where each
expression is aligned to another expression (or
more) with the same meaning and in the same
language. Unfortunately, there is no paraphrase
corpus widely available except for a few collec-
tions such as those prepared by Shirai et al
(2001) and Zhang et al (2001). Most of those
works collected paraphrase corpora by employ-
ing special situations, such as multiple news re-
sources from the same events or multiple trans-
lations of the same (and well-known) story in
other languages. However, since these situa-
tions seem to be really special, we believe that
the collection of many paraphrase corpora in the
near future is quite hopeless. Consequently, it
is necessary to conduct a feasibility study on
collecting various kinds of paraphrase knowl-
edge from non-paraphrase corpora, particularly
from raw text corpora. Although we have al-
ready reported extracting paraphrasing knowl-
edge of Japanese noun modifiers from a raw cor-
pus (Kataoka et al, 1999), we need to explore
other types of expressions.
With this motivation, we have attempted
to acquire paraphrasing knowledge on content
words, mainly nouns and verbs. As a knowl-
edge source, we use newspaper articles collected
over one year in text format, which is regarded
as the most generally used corpus. In this trial,
we propose the following two principles of ac-
quisition:
? Conditions for applying each type of para-
phrasing knowledge should be obtained.
? Paraphrasing knowledge should have direc-
tionality.
In other words, all of the paraphrasing pat-
terns obtained by the conventional methods
seem to have been applied unconditionally, that
is, conventional approaches tend to target only
unconditional patterns. However, most para-
phrasing phenomena depend on their context;
paraphrasing can be possible only if a para-
phrased expression fits the context.
Moreover, directionality of the rules is an
important issue for paraphrasing, although no
other works have discussed this. Despite the
existence of synonymy, even if expression E
1
can be replaced by expression E
2
, it is unsure
whether E
2
can be paraphrased by using E
1
.
We discuss this feature in the experiments.
2 Contextual Similarity vs.
Synonymy
Paraphrasability is the degree of replacability
for two expressions E
1
and E
2
, which are re-
garded as different from each other in some
sense. This definition implies the notion that E
1
should not be judged as the same (or similar) as
E
2
in the sense of meaning. Of course, similarity
of meaning and paraphrasability are very closely
correlated with each other, and Kurohashi and
Sakai (1999) utilize this feature to paraphrase
Japanese expressions (in order to comprehend
them more easily). They use a Japanese dic-
tionary written for humans (or more precisely,
children) to replace a part of the target expres-
sion with a different one by judging its local
context computed by a thesaurus.
We propose that replacability (obtained by
the corpus, for example) is a more important
factor in judging the paraphrasability of expres-
sions than their meaning as defined in a dictio-
nary. For example, words used only in some spe-
cial situations, such as for children or in ancient
documents, should not be used in a paraphrase
even though it has synonymy.
On the contrary, even if synonymy is not sat-
isfied, we still focus on expressions that are re-
placeable. Hypernymy is one example of this.
A hypernym is not a paraphrase in a strict
sense due to the loss of information. However,
this kind of paraphrasing is still useful from
the engineering point of view. For instance,
these loose (and therefore many) paraphrases
are more effective in the case of reluctant pro-
cessing, where we must necessarily change an
expression for various reasons such as our re-
quirement in paraphrase-based machine trans-
lation (Yamamoto, 2002). Moreover, this kind
of paraphrase loses nothing when it is used as
anaphora or when it is trivial and out of major
interest in the context used. Not all hypernyms
are always paraphrasable, so we cannot list this
type of paraphrase from only a thesaurus.
3 Approach and Implementation
This section describes our approach to acquiring
paraphrase knowledge from a text corpus. We
use Perl programming language to implement
all of the following processes and experiments.
3.1 Collection of context from corpus
We first define the term context in this paper.
The context of a certain content word is de-
fined as direct dependency relations between the
word and the words that surround it in texts.
That is, the context of a content word c is, in our
sense, defined as the collection of words upon
which c directly depends as well as the collec-
tion of words that directly depend on c.
Under this definition, we first collected all of
the dependency relations observed in the cor-
pus. Each article is segmented and part-of-
speech tagged by the morphological analyzer
JUMAN1 and then parsed by the KNP2 parser.
We then obtained a relation triplet (c
1
, r, c
2
)
from each article, where a word c
1
depends on
a word c
2
with the relation r. A complete list
of r types and their examples is shown below:
? (Noun, r
1
, Noun)
e.g. ??? (this time)? (of)?? (law)?
??? (terrorism)?? (bill)?
? (Adjective, r
2
, Noun)
e.g. ???? (new)?? (law)?
? (Noun, r
3
, Verb)
e.g. ???? (Lower House)? (SUB)???
? (approve)?
? (Verb, r
4
, Noun)
e.g. ????? (bombing)?? (U.S. army)?
In this list, r
i
can be a particle, such as a
case particle (r
3
) or an associative particle ???
(r
1
). Another type of possible r
i
in the list is
a syntactic relation expressed without any par-
ticle or other functional marker. For instance,
a verb or an adjective directly modifies a noun
without using any functional words in Japanese.
In this case, we introduce the notion of a con-
stituent boundary proposed by Furuse and Iida
1http://www-nagao.kuee.kyoto-u.ac.jp
/nl-resource/juman-e.html
2http://www-nagao.kuee.kyoto-u.ac.jp
/nl-resource/knp-e.html
(1994), which is a virtual functional marker in-
serted between two consecutive content words,
in order to more easily analyze a sentence. For
instance, if there are two consecutive nouns, we
assume that ?nn? is inserted between the two
nouns, and consequently the relation r
i
is ?nn?.
3.2 Bigraph construction
We then transform the collection of triplets into
a bigraph (2-partite graph). In the first step,
each triplet in the collection is converted into
two couplets consisting of a content word and
an operator by the following definition: an op-
erator consists of a content word c and a rela-
tion with directionality r. It is defined as either
r ? c (something depends on c by r) or r ? c
(c depends on something by r). For instance,
suppose that a triplet is (c
1
, r, c
2
), then both
a couplet for the first content word c
1
, i.e., (c
1
,
r ? c
2
) and a couplet for the second content
word c
2
, i.e., (c
2
, r ? c
1
) are extracted in this
operation.
We perform this conversion for all of the
triplets, and a list of couplets is obtained. From
the viewpoint of graph theory, this couplet list
is a bigraph, such as figure 1, which consists
of two sets (content word set and operator set)
and a list of edges, where each edge connects an
element in one set to an element on the other
side. This bigraph is a weighted graph, and each
weight expresses the frequency of appearing in
the corpus.
3.3 Paraphrasability computation
In the next step, we compute paraphrasability.
In this work, the paraphrasability P for any two
content words c
i
and c
j
is defined in the follow-
ing formula:
P (c
i
, c
j
) =
?
m?M(c
i
)?M(c
j
)
p(m, c
i
)
?
m?M(c
i
)
p(m, c
i
)
(1)
M(c
i
) = {m|f(m, c
i
) > 1} (2)
p(m, c
i
) =
f(m, c
i
)
?
c
f(m, c)
(3)
In this formula, let f(m
0
, c
0
) be frequency of
content word c
0
with operator m
0
. In other
content
 words operators
m5
c2
c4
m3
Figure 1: Example of a bigraph (2-partite
graph)
words, f(m
0
, c
0
) is a weight of edge (c
0
, m
0
)
in the bigraph.
This formulation can be explained as follows.
Paraphrasability between two content words c
i
and c
j
increases if these words behave similarly
in terms of their dependency relations. That is,
this metrics compares the similarity of the con-
textual situations of the two input words. The
definition states that paraphrasability computes
the number of operators that c
j
links among the
operators that c
i
links.
However, we believe that the importance of
each operator m is not equivalent to that of the
others. For example, in figure 1, the operator
m
3
is linked by only two words, c
2
and c
4
, while
the operator m
5
is linked by almost all of the
words. In this situation, it is not reasonable
to handle the two operators equally, since m
3
may confirm that the two words are similar or
paraphrasable, whereas m
5
may be a general
operator widely used in various situations. In
other words, when we compute paraphrasability
from c
2
to c
4
, the edge (c
4
, m
5
) is judged as less
important than the edge (c
4
, m
3
) or (c
2
, m
3
).
Consequently, each operator is weighted by the
definition of formula (3). Moreover, instances
of low frequency are regarded as accidental and
insignificant, so we filter out links where an in-
stance appears only once.
It is obvious in the definition of (1) that
0 ? P (c
i
, c
j
) ? 1, and a higher score expresses
a higher possibility of paraphrasing. More im-
portantly, the definition indicates the relation
P (c
i
, c
j
) = P (c
j
, c
i
), i.e., there is a directional-
ity that gives larger differences than any similar-
ity metrics. Even if an expression E
1
has a large
paraphrasability for an expression E
2
, it is com-
pletely uncertain whether the paraphrasability
of E
2
into E
1
is high or low.
3.4 Paraphrase knowledge filtering
By only taking the discussion of the last sub-
section into account, we can compute para-
phrasability between any two content words.
However, this measure is not the final judgment
of paraphrasability: some pairs score very high
even though they are not paraphrasable. For ex-
ample, the pair three and four may have a very
high score but are of course not paraphrasable.
In our observation, the following kinds are found
to be misjudged as paraphrasable by our defini-
tions.
1. number, e.g., ???(three) ????(four)
2. proper noun,
e.g., ????(Beijing) ?????(Taipei)
3. antonym, e.g., ???(right) ????(left)
Obviously, these errors occurred due to one
of the limitations of our approach; since the for-
mula only has an interest in the context of the
words found in the corpus, not in the sense of
the words found in a dictionary.
However, we can filter out these kinds of word
pairs by introducing language resources exter-
nal to the corpus. First, we can judge whether
the word is a number by applying some sim-
ple rules. Second, we can now easily obtain ex-
tensive lists of both major proper nouns and
antonyms. We obtain the proper noun list from
GoiTaikei3, one of the largest Japanese elec-
tronic thesauri, in which 169,682 proper noun
entries are extracted. We obtain the antonym
list from both Gakken Kokugo Daijiten (a
Japanese word dictionary) and Kadokawa Ruigo
Shinjiten (a Japanese thesaurus), which have
11,981 antonym pairs in total.
3http://www.kecl.ntt.co.jp/icl/mtg/resources
/GoiTaikei/
c1
c2
c3
(a)
(b)
(c)
:judged as paraphrasable
Figure 2: Heuristic by number of links
In fact, further filtering is necessary in order
to reduce errors. For example, in English, gui-
tar, piano, and flute have very similar contexts,
such as ?to play the ,? ?an electric ,? ?a
violin and a ,? and so on, although they are
naturally not paraphrasable. We predict that in
order to use lexical paraphrase collection for fil-
tering, future research will need to concentrate
on how to collect word pairs that are not para-
phrasable but have the same context.
3.5 Further filtering by heuristic
method
In the final process, we filter the pairs further
by using our proposed heuristic to improve the
acquisition accuracy.
From our observations of the results obtained
by the above operations, we found a clear ten-
dency in words that have a very high frequency
or very broad sense: these words tend to be
judged as having a high paraphrasability from
many words or to many words, even if they are
not actually paraphrasable. For example, in fig-
ure 2 (a), a content word such as c
1
tends to be
misjudged as paraphrasable if c
1
links to many
words and/or if c
1
is linked by many words.
In other words, case (b) of the figure, where a
word c
2
connects to only one word, would more
likely have its paraphrasing judged as proper.
We also build a hypothesis that case (c), where
Table 1: Evaluation for Content Words
Case 1 Case 2 Total
Extracted 668 1149 1684
Paraphrasable 422 780 1117
Accuracy 63.2% 67.9% 66.3%
Case 1: a word paraphrases to one word
Case 2: a word is paraphrased from one word
two words are exchangeable, has more accuracy
than the other two cases, which are evaluated
in the next section.
We assume these errors occurred because
such words can have dependency relations with
many words, i.e., such words are general and
frequently appearing. Consequently, such cases
are unexpectedly judged as being highly para-
phrasable from or to many words. As these
words are used many times in many contexts,
the possibility of inserted noises also increases.
Therefore, distinguishing noises from real para-
phrases becomes difficult.
These spurious paraphrases should not re-
main in the final results, so we conduct another
filtering according to the above analysis. The
actual process is conducted as follows. For each
c
i
, we count the number of c
j
that satisfies the
relation P (c
i
, c
j
) > P
const
. If there is only one
word c
j
that satisfies this relation, we finally de-
termine that c
i
can paraphrase to c
j
. Similarly,
for each c
j
, we count the number of c
i
that sat-
isfies the relation P (c
i
, c
j
) > P
const
. If there is
only one word c
i
that satisfies the relation, we
finally determine that c
i
can paraphrase to c
j
.
In the experiment below, we set P
const
= 0.1.
In this heuristic filtering, some word pairs
that are actually paraphrasable may, unfortu-
nately, also be lost. The problem of saving them
remains for our future work.
4 Knowledge Acquisition
Experiment
4.1 Experiment on content word
paraphrasing
We have conducted an experiment of paraphras-
ing knowledge acquisition in the following con-
4These two words have the same string but different
part-of-speech, so our tagger judges these two as differ-
ent.
Table 2: Highest Paraphrasable Pairs
Paraphrase pair P P (?)
?? (anecdote) ?? (story) 1 .0015
??????????? 1 .2539?
??? (only) ??? (only) 1 .1877?
?? (scheme) ??? (form) .9982 .2671?
???? (panic) ??? (confusion) .9978 .0496
?? (win) ??? (win)4 .9802 .5176?
???? (hockey) ??? (baseball) .9752 .0286
?? (formation) ??? (formation) .9672 .0449
??? (incongruity) ??? (pain) .9667 .0352
?? (drastic change) ??? (change) .9582 .0177
ditions. The corpus we used was all articles of
The Mainichi Shimbun, which is one of the na-
tional daily newspapers of Japan, published in
the year 1995. The size of the corpus is 87.3
MB, consisting of 1.33 million sentences.
Table 1 illustrates evaluation results of knowl-
edge acquisition. The results show that our pro-
posed process can choose approximately 1,700
paraphrase pairs that have 66% accuracy. Al-
though this accuracy is not satisfactory for an
automatic process, it is already helpful from
the engineering point of view; accordingly, we
can obtain a large amount of high-quality para-
phrase pairs with a minimum human check in
significantly less time than one day.
We also show the acquired paraphrase pairs
with the highest paraphrasabilities in Table 2.
Note that P (?) in the table denotes the para-
phrasability of the inverted paraphrases, from
right to left direction, and the symbol ? indi-
cates that this direction is also judged as para-
phrasable, i.e., these two words are determined
to be paraphrasable with each other. We found
that most of the entries in the list are correctly
judged to be paraphrasable, even though some
of them cannot be paraphrasable, such as ???
??? (underarm throwing)? into ??????
(overarm throwing)?5.
We can also confirm that the directionality of
the proposed measure works quite well. For ex-
ample, we can paraphrase the term ??? (anec-
dote)? with the more general term ?? (story),?
but it is impossible to replace the latter with
the former except in some restricted context.
The outputs seen in this table illustrate such an
intuition.
5Both are names of techniques in sumo wrestling.
Table 3: Paraphrasability of Operators
Paraphrase pair P
?? (request) ? ??? (order) ? 1
?? (director) ? ??? (professor) ? .9940
?? (branch) ???? (district court) ? .9334
?? (high court) ???? ? .8734
?nn??? (short term) ??nn??? (college) .8723
?nn?? (Swallows) ??nn?? (Giants) .8553
?? (every week) ?nn???nn?? (night) .8123
?? (city councillor) ?nn???? ?nn? .8063
?? (several) ?nn??? (several) ?nn? .7961
?? (pref. assemblyman) ?nn???? ?nn? .7859
If the process judges that the two words can
paraphrase each other, these words are consid-
ered to be a paraphrase in a narrow sense. In
this experiment, we can extract 114 pairs that
satisfy this relation, and 75 of these pairs are
evaluated as being correct, for an accuracy of
65.8%.
4.2 Experiment for acquisition of
operator paraphrase
So far in this paper, we have been using an op-
erator set to compute any of two words in the
content word set in the bigraph. We found that
we can also do this in the reverse way: comput-
ing any of two operators by using the content
word set. This is possible because even if we
turn a bigraph upside-down, it is still a bigraph.
In this subsection, we report an experiment on
computing the paraphrasability of operators by
the same procedure as above.
After multiple filtering, 432 pairs were judged
as paraphrasable. From these we found that
the number of correct pairs was 312 (72.2%
accuracy). Table 3 illustrates the final para-
phrasable pairs with the highest paraphrasabil-
ity.
Unfortunately, these pairs include errors,
so their performance in an automatic process
should be improved. However, this performance
is still promising for a human-assisted tool.
We investigated the pairs and found that
there were various kinds of paraphrasing knowl-
edge obtained in this process. Not only para-
phrases of content words but also paraphrase
knowledge of the following types were obtained
in this experiment.
? insertion and deletion of the particle ???
in noun-noun sequences
? paraphrasing for case particles; in
Japanese, it may be possible to change a
particle under a certain context.
? voice conversion
? different description of the same word, e.g.,
from a Chinese-origin word to a native
Japanese word
5 Related Works
Lexical paraphrasing is very useful in infor-
mation retrieval, since it is necessary to ex-
pand terms for improving retrieval coverage.
Jacquemin et al (1997) have proposed acquir-
ing syntactic and morpho-syntactic variations
of the multi-word terms using a corpus-based
approach. They have searched for variation,
i.e., similar expressions using (a part of) the in-
put words, such as technique for measurement
against measurement technique, while our tar-
get is the paraphrase of a single content word.
The goal of our work is to obtain lexical
knowledge for paraphrasing. For this purpose
we use contextual similarity, which is also used
in the sense similarity computation task in the
fields of natural language processing, artificial
intelligence, and cognitive science. Moreover,
the idea of corpus-based context extraction is
basically the same and also used in the task of
automatic construction of thesauri or sense de-
termination of unknown words.
Although this is the first work to use con-
text for paraphrase knowledge extraction, many
previously reported works have used context
for similarity calculation. Paraphrasability and
word sense similarity may seem like similar met-
rics, but there are critical differences between
the two tasks. First, similarity satisfies the sym-
metrical property while paraphrasability does
not (explained in 3.3). Second, similarity is
a relative measure while paraphrasability is an
absolute measure; in many cases, we can answer
?Can E
1
paraphrase to E
2
??, but it is hard to
answer ?Is E
1
similar to E
2
??. In other words,
it is important to collect paraphrases while it
may be pointless to collect similar words, since
the border for the former is clearer than that of
the latter.
The kind of information used for defining con-
text is important. For this question, Nagamatsu
and Tanaka (1996) used a deep case (seen in a
semantically tagged corpus), and Kanzaki et al
(2000) only extracted relations of nominal modi-
fication. The most closely related work in terms
of similarity source is the work of Grefenstette
(1994), where they obtained subject-verb, verb-
object, adjective-noun, and noun-noun relations
from a corpus. In contrast, as discussed in sub-
section 3.1, we propose extracting all of the de-
pendency relations around content words, i.e.,
nouns, verbs, and adjectives. This is the first
attempt to introduce these features into a con-
text definition, and it is obvious that coverage
of extracted pairs becomes wider by using var-
ious features. However, we have not conducted
enough experiments to prove that these factors
are effective. This remains for our future work.
6 Conclusions
We propose a process to acquire paraphras-
ing pairs of content words from a non-parallel
raw corpus. We utilize contextual similarity,
obtained from the corpus, to compute para-
phrasability between any two content words.
Some of the word pairs that unexpectedly have
high paraphrasability are filtered out by us-
ing external linguistic knowledge such as proper
nouns and antonyms. Moreover, our proposed
heuristic, obtained through observation, can in-
crease acquisition accuracy. These processes in
combination are able to obtain more than 1,700
paraphrase pairs with approximately 66% accu-
racy.
Our interest in this research is not to pursue
higher accuracy in automatic processing but to
obtain any kind of paraphrasing knowledge as
fast as possible. From this point of view, the
coverage of the acquisition process is a more se-
rious problem for us than accuracy. Our prelim-
inary experiment showed that a drastic drop in
accuracy is observed even if we increase cover-
age gradually. We need to find another filtering
criterion to avoid this problem.
Acknowledgment
The research reported here was supported in part by
a contract with the Telecommunications Advance-
ment Organization of Japan entitled, ?A study of
speech dialogue translation technology based on a
large corpus.?
References
Regina Barzilay and Kathleen R. McKeown.
2001. Extracting paraphrases from a parallel
corpus. In Proc. of ACL-2001, pages 50?57.
Osamu Furuse and Hitoshi Iida. 1994. Con-
stituent boundary parsing for example-based
machine translation. In Proc. of Coling?94,
pages 105?111.
Gregory Grefenstette. 1994. Corpus-derived
first, second, third-order word affinities. In
Proc. of EURALEX?94.
Christian Jacquemin, Judith L. Klavans, and
Evelyne Tzoukermann. 1997. Expansion of
multi-word terms for indexing and retrieval
using morphology and syntax. In Proc. of
ACL-EACL?97, pages 24?31.
Kyoto Kanzaki, Qing Ma, and Hitoshi Isahara.
2000. Similarities and differences among se-
mantic behaviors of Japanese adnominal con-
stituents. In Proc. of ANLP/NAACL 2000
Workshop on Syntactic and Semantic Com-
plexity in Natural Language Processing Sys-
tem, pages 59?68.
Akira Kataoka, Shigeru Masuyama, and
Kazuhide Yamamoto. 1999. Summarization
by shortening a Japanese noun modifier into
expression ?A no B?. In Proc. of NLPRS?99,
pages 409?414.
Sadao Kurohashi and Yasuyuki Sakai. 1999.
Semantic analysis of Japanese noun phrases:
A new approach to dictionary-based under-
standing. In Proc. of ACL?99, pages 481?488.
Kenji Nagamatsu and Hidehiko Tanaka. 1996.
Estimating point-of-view-based similarity us-
ing POV reinforcement and similarity prop-
agation. In Proc. of Pacific Asia Conference
on Language, Information, and Computation
(PACLIC), pages 373?382.
Satoshi Shirai, Kazuhide Yamamoto, and Fran-
cis Bond. 2001. Japanese-English paraphrase
corpus. In Proc. of NLPRS2001 Workshop on
Language Resources in Asia, pages 23?30.
Kazuhide Yamamoto. 2002. Machine transla-
tion by interaction between paraphraser and
transfer. In Proc. of COLING2002.
Yujie Zhang, Kazuhide Yamamoto, and
Masashi Sakamoto. 2001. Paraphrasing
utterances by reordering words using semi-
automatically acquired patterns. In Proc. of
NLPRS2001, pages 195?202.
Detecting Transliterated Orthographic Variants
via Two Similarity Metrics
Kiyonori Ohtake
ATR SLT
Keihanna Science City
Kyoto 619-0288,
Japan
kiyonori.ohtake@atr.jp
Youichi Sekiguchi
Nagaoka Univ. of Tech.
Nagaoka City,
Niigata 940-2188,
Japan
sekiguti@nlp.nagaokaut.ac.jp
Kazuhide Yamamoto
Nagaoka Univ. of Tech.
Nagaoka City,
Niigata 940-2188,
Japan
yamamoto@fw.ipsj.or.jp
Abstract
We propose a detection method for or-
thographic variants caused by translit-
eration in a large corpus. The method
employs two similarities. One is string
similarity based on edit distance. The
other is contextual similarity by a vec-
tor space model. Experimental results
show that the method performed a 0.889
F-measure in an open test.
1 Introduction
This paper discusses a detection method for
transliterated orthographic variants of foreign
words. Transliteration of foreign words causes
orthographic variants because there are several
conditions required for transliterating. One may
person transliterate to approximate pronunciation,
whereas another one may conduct transliteration
based on spelling. For example, the English
word report can be transliterated into two Japanese
words, ?
  (ripooto)? and ?   (re-
pooto).? The former ?ripooto? is based on an ap-
proximation of its pronunciation, while ?repooto?
is transliterated from its spelling.
In addition, several source languages can be
transliterated. For instance, the English word virus
corresponds to the Japanese words: ? 	

(uirusu)? from Latin, ?    (biirusu)? and ?


 (viirusu)? from German, while ? 

 (bairasu)? or ?  
 (vairasu)? are
also possible as transliterations that approximate
the English pronunciation. Moreover, some for-
eign words end up in different forms in Japanese
because of variation in English pronunciation; e.g.,
between British and American. For example, the
English word body corresponds to two words: ? A System to Solve Language Tests for Second Grade Students 
Manami Saito 
Nagaoka University of Technology 
saito@nlp.nagaokaut.ac.jp 
Kazuhide Yamamoto 
Nagaoka University of Technology 
yamamoto@fw.ipsj.or.jp 
Satoshi Sekine 
New York University 
Language Craft 
sekine@cs.nyu.edu 
Hitoshi Isahara 
National Institute of Information and Com-
munications Technology 
isahara@nict.go.jp 
 
 
Abstract 
This paper describes a system which 
solves language tests for second grade 
students (7 years old). In Japan, there 
are materials for students to measure 
understanding of what they studied, 
just like SAT for high school students 
in US. We use textbooks for the stu-
dents as the target material of this study. 
Questions in the materials are classified 
into four types: questions about Chi-
nese character (Kanji), about word 
knowledge, reading comprehension, 
and composition. This program doesn?t 
resolve the composition and some other 
questions which are not easy to be im-
plemented in text forms. We built a 
subsystem for each finer type of ques-
tions. As a result, we achieved 55% - 
83% accuracy in answering questions 
in unseen materials. 
1 Introduction 
This paper describes a system which solves lan-
guage tests for second grade students (7 years 
old). We have the following two objections. 
First, we aim to realize the NLP technologies 
into the form which can be easily observed by 
ordinary people. It is difficult to evaluate NLP 
technology clearly by ordinary people. Thus, we 
set the target to answer second grade Japanese 
language test, as an example of intelligible ap-
plication to ordinary people. The ability of this 
program will be shown by scores which are fa-
miliar to ordinary people. 
Second aim is to observe the problems of the 
NLP technologies by degrading the level of tar-
get materials. Those of the current NLP research 
are usually difficult, such as newspapers or tech-
nological texts. They require high accuracy lan-
guage processing, complex world knowledge or 
semantic processing. The NLP problems would 
become more apparent when we degrade the 
target materials. Although questions for second 
grade students also require world knowledge, it 
is expected that the questions become simpler 
and are resolved without tangled techniques.  
2 Related Works 
Hirschman et al (1999) and Charniak et al 
(2000) proposed systems to solve ?Reading 
Comprehension.? Hirschman et al (1999) de-
veloped ?Deep Read,? which is a system to se-
lect sentences in the text which include answers 
to a question. In their experiments, the types of 
questions are limited to ?When,? ?Who? and so 
on. The system is basically an information re-
trieval system which selects a sentence, instead 
of a document, based on the bag-of-words 
method. That system retrieves the sentence con-
taining the answer at 30-40% of the time on the 
tests of third to sixth grade materials. In short, 
Deep Read is very restricted compared to our 
system. Charniak et al (2000) built a system 
improved over the Deep Read by giving more 
weights for verb and subject, and introduced 
heuristic rules for ?Why? question. Though, the 
essential target and method are the same as that 
of Deep Read. 
43
3 Question Classification 
First, we bought five language test books for 
second grade students and one of them, pub-
lished by KUMON, was used as a training text 
to develop our system. The other four books are 
referred occasionally. Second, we classified the 
questions in the training text into four types: 
questions about Chinese character (Kanji), ques-
tions on word knowledge, reading comprehen-
sion, and composition. We will call these types 
as major types. Each of the ?major types? is 
classified into several ?minor types.? Table 1 
shows four major types and their minor types. In 
practice, each minor type farther has different 
style of questions; such as description question, 
choice question, and true-false question. The 
questions can be classified into approximately 
100 categories. We observed that some ques-
tions in other books are mostly similar; however 
there are several questions which are not cov-
ered by the categories. 
 
Major type Minor type 
Kanji Reading, Writing, Radical, The 
order of writing, Classification 
Word knowl-
edge 
Katakana, How to use Kana, Ap-
propriate Noun, To fill blanks for 
Verb, Adjective, and Adjunct, 
Synonym, Antonym, Particle, 
Conjunction, Onomatopoeia, Po-
lite Expression, Punctuation mark
Reading com-
prehension 
Who, What, When, Where, How, 
Why question, Extract specific 
phrases, Progress order of a story, 
Prose and Verse  
Composition Constructing sentence, How to 
write composition 
Table 1. Question types 
4 Answering questions and evaluation 
result 
In this section, we describe the programs to 
solve the questions for each of the 100 catego-
ries and these evaluation results. First we classi-
fied questions. Some questions are difficult to 
cover by the system such as the stroke order of 
writing Kanji. For about 90% of all the question 
types other than such questions, we created pro-
grams for basically one or two categories of 
questions. There are 47 programs for the catego-
ries found in the training data. 
In each section of 4.1 to 4.3, we describe 
how to solve questions of typical types and the 
evaluation results. The evaluation results of the 
total system will be reported in the following 
section. 
Table 2 to 4 show the distributions of minor 
types in each major type, ?Kanji,? ?Word 
knowledge,? and ?Reading comprehension,? in 
the training data and the evaluation results. 
Training and evaluation data have no overlap.  
4.1 Kanji questions 
Most of the Kanji questions are categorized into 
?reading? and ?writing.? Morphological analysis 
is used in the questions of both reading and writ-
ing Kanji; we found that large corpus is effec-
tive to choose the answer from Kanji candidates 
given from dictionary. Table 2 shows it in detail. 
This system cannot answer to the questions 
which are asking the order of writing Kanji, be-
cause it is difficult to put it into digital format.  
The system made 7 errors out of 334 ques-
tions. The most of the questions are the errors in 
reading Kanji by morphological analysis. 
In particular, morphological analysis is the 
only effective method to answer questions on 
this type. It would be very helpful, if we had a 
large corpus considering reading information, 
but there is no such corpus.  
 
Training 
data 
Test data Ques-
tion type
The rate 
of Q in 
training 
data[%]
The used 
knowledge 
and tools Correct 
ans. 
(total) 
Correct ans. 
(known type 
Q., total) 
Reading 27 Kanji dictionary, 
Morphological 
analysis 
 
96(100) 
 
6(8,8) 
Writing 61 Word diction-
ary, Large 
corpus 
 
220(222) 
 
63(66,66) 
Order of 
writing 
6 -  
0(20) 
 
0(0,2) 
Combi-
nation 
of Kanji 
parts 
3 -  
0(10) 
 
0(0,0) 
Classify 
Kanji 
3 Word diction-
ary, Thesaurus 
 
11(12) 
 
0(0,0) 
Total 100 - 327(364) 69(74,76) 
Table 2. Question types for Kanji 
4.2  Word knowledge questions 
The word knowledge question dealt with vo-
cabulary, different kinds of words and the struc-
ture of sentence. These don?t include Kanji 
questions and reading comprehension. Table 3 
shows different types of questions in this type. 
44
For the questions on antonym, the system can 
answer correctly by choosing most relevant an-
swer candidate using the large corpus out of 
multiple candidates found in the antonym dic-
tionary. 
The questions about synonyms ask relations 
of priority/inferiority between words and choos-
ing the word in a different group. These ques-
tions can usually be answered using thesaurus. 
Ex.1 shows a question about particle, Japa-
nese postposition, which asks to select the most 
appropriate particle for the sentence. 
The system produces all possible sentences 
with the particle choices, and finds most likely 
sentences in a corpus. In Ex.1, all combinations 
are not in a corpus, therefore shorter parts of the 
sentence are used to find in the corpus (e.g. ??
?? (1) ????, ???? (2) ????). In this 
case, the most frequent particle for (1) is ??? in 
a corpus, so this system outputs incorrect answer. 
Ex.1 [?/?/?]????()??????? 
?????? 
 Select particle which fits the sentence from 
{wo,to,ni} 
[1] ??? (1) ??? (2) ??? 
apple-(1) orange-(2) buy 
(1) correct=? (to) system=? (wo) 
(2) correct=? (wo) system=? (wo) 
 
The questions of Katakana can be answered 
mostly by the dictionary. The accuracy of this 
type is not so high, found in Table 2, because 
there are questions asking the origin of the 
words, most Katakana words in Japanese has a 
few origins: borrowed words, onomatopoeia, 
and others. Because we don?t have such knowl-
edge, we could not answer those questions.   
The questions of onomatopoeia include those 
shown in Ex.2. The system uses co-occurrence 
of words in the given sentence and each answer 
candidate to choose the correct answer in Ex.2, 
?????.? However, it was not chosen be-
cause the co-occurrence frequency of ????
?,? the word in the sentence, and ?????,? 
incorrect answer, is higher.  
Ex.2 ??? ???? ???? ??? 
? [ ] ?? ??????? ??????? 
Choose the most appropriate onomatopoeia 
(1) ??? ??? ???? ????  
????(A large object is rowing slowly) 
[??????????????] 
The questions of word knowledge are classi-
fied into 29 types. We made a subsystem for 
each type. As there are possibly more types in 
other books, making a subsystem for each type 
is very costly. One of the future directions of 
this study is to solve this problem. 
Training 
data 
Test data Question 
type 
The rate 
of Q in 
training 
data[%] 
The used 
knowledge and 
tools Correct 
ans. 
(total) 
Correct ans. 
(known type 
Q., total) 
Anonym 18 Antonym 
dictionary, 
Large corpus 
26(27) 12(15,21) 
Synonym 11 Thesaurus 14(17) 34(44,83) 
Particle 19 Large corpus 25(28) 16(17,17) 
Katakana 25 Word diction-
ary, Morpho-
logical analysis 
18(37) 19(22,52) 
Onomato-
poeia 
19 Large corpus, 
Morphological 
analysis 
18(29) 16(20,31) 
Structure 
of sen-
tence 
5 Morphological 
analysis 
7(7) 20(22,22) 
How to 
use kana 
2 - 0(3) 0(0,19) 
Dictation 
of verb 
2 - 0(3) 0(0,0) 
Total 100 - 108(151) 117(140,245)
Table 3. Question types for Word knowledge 
4.3 Reading comprehension questions 
The reading comprehension questions need to 
read a story and answer questions on the story. 
We will describe five typical techniques that are 
used at different types of questions, shown in 
Table 4. 
Pattern matching (a) is used in questions to 
fill blanks in an expression which describes a 
part of the story. In general, the sequence of 
word used in the matching is the entire expres-
sion, but if no match was found, smaller por-
tions are used in the matching. 
Ex.3 Fill blanks in the expression 
Story?partial??????? ???? 
?? ?? ????????? ????  
?? ???? ?????? 
(In a few days, the flower withers and gradu-
ally changes its color to black.) 
Expression???? (1)?(2) ?? ????? 
(The flower (1) and change its color to (2).) 
Answer?(1) ???? (withers) 
(2) ???? (black) 
The effectiveness of this technique is found 
in this example. The other methods will be 
needed when questions will be more difficult. At 
the time, this technique is very useful to solve 
many questions in reading comprehension.  
45
When the question is ?Why? question, key-
words such as ????  (thus)? and ????
(because)? are used. 
For example, when questions start with 
?When (??)? and ?Where (??),? we can 
restrict the type of answer word to time or loca-
tion, respectively. If the question includes the 
expression of ????? (??? is a particle to 
indicate direction/specification), the answer is 
also likely to be expressed with ?ni? right after 
the location in the story. (The kind of NE 
(Named Entity) and particle right after word 
(b)) 
For the questions asking the time or location 
about the entire story, this system outputs the 
appropriate type of the word which appeared 
first in the story. Although there are mistakes 
due to morphological analysis and NE extraction, 
this technique is also consistently very useful.  
The technique which is partial matching 
with keywords (c) is used to seek an answer 
from story for ?how,? ?why? or ?of what? ques-
tions. Keywords from the question are used to 
locate the answer in the story. 
Ex.4 ???????? ?????????
? ???
Frequency in the large corpus is used to 
find the appropriate sentence conjunction. (d) 
Answer is chosen by comparing the mutual in-
formation of the candidate conjunctions and the 
last basic block of the previous sentence. How-
ever, this technique turns out to be not effective. 
Discourse analysis considering wider context is 
required to solve this question. 
The technique which uses distance between 
keywords in question and answers (e) is sup-
plementary to the abovementioned methods. If 
multiple answers are found, the answer candi-
date that is the closest in the story text to the 
keywords in questions is generated. These key-
words are content words and unknown words in 
the text. This technique is found very effective. 
In Table 4, the column ?Used method? shows 
the techniques used to solve the type of ques-
tions, in the order of priority. ?f? in the table 
denotes means that we use a method which was 
not mentioned above.  
????(How big are chicks when 
they hatched?) 
Text?partial??????????? ??  
 ???????????? ??????? 
(The size of chicks when they hatched is about 
the size of your thumb.) 
 
 
 Answer?????? ???? (size of 
thumb)  
 
The rate of Training data Test data
questions in Used corrent wns. corrent ans.
training data [%] methods (total) (known type Q, total)
Who said 5 b,a,f
The others 0 b,e,f
Like what c,b,e
Of what c,f,e
What doing c,a,f
What is a,e
What do A say b,a,f,e
Whole story b
Part of story -
Whole story b
Part of story b,f,c
16 c,f 11(18) 0(1, 1)
10 c 8(11) 0(0, 1)
2 b,c,f 1(2) 0(0, 0)
10 a 10(12) 4(9, 9)
4 - 0(5) 0(0, 0)
2 d 1(2) 1(3, 3)
10 f 8(11) 0(0, 0)
10 f 7(12) 3(3, 3)
1 - 0(1) 0(0, 6)
100 - 74(116) 10(22, 34)
Why
How
How long, how often, how large
Total
Paragraph
The others
To fill blanks
Not have interrogative pronoun
Conjunction
Progress order of a story
Where 4 3(5) 0(1
When 4 3(5) 0(0
Who
Question type
17(26) 1(1, 6)What 22
5(6) 1(4, 4)
, 1)
, 0)
 
Table 4. Question types for Reading comprehension 
46
5 Evaluation 
We collected questions for the test from differ-
ent books of the training data. The proposition 
of the number of questions for different sections 
is not the same as that of the training data. Table 
2 to 4 show the evaluation results in the test data 
for each type. Table 5 shows the summary of the 
evaluation result. In the test, we use only the 
questions of the type in training data. The tables 
also show the total number of questions, the 
number of questions which are solved correctly, 
and the number of questions which are not one 
of the types the system targeted (not a type in 
the training data). 
The ratio of the questions covered by the sys-
tem, questions in test data which have the same 
type in the training data are 97.4% in Kanji, 
57.1% in word knowledge, and 64.7% in read-
ing comprehension. It indicates that about a half 
of the questions in word knowledge isn?t cov-
ered. As the result, accuracy on the questions of 
the covered types in word knowledge is 83.6%, 
but it drops to 47.8% for the entire questions. It 
is because our system classified the questions 
into many small types and builds a subsystem 
for each type. 
The accuracy for the questions of covered 
type is 83.4%. In particular, for the questions of 
Kanji and word knowledge, the scores in the test 
data are nearly the same as those in the training 
data. It presents that the accuracy of the system 
is provided that the question is in the covered 
type. However, the score of reading comprehen-
sion is lower in the test data. We believe that 
this is mainly due to the small test data of read-
ing comprehension (only 34) and that the accu-
racy for ?Who? questions and the questions to 
fill blanks in the test data are quite difficult com-
pared to the training data. 
Num. Num. Num. RCA * RCA* RCA*
of of of (known (total? in total
all Q known corrent type Q) [%] of known
type Q ans. [%] type Q [%]
Kanji 76 74 69 93.2 90.8 89.8
Word knowledge 245 140 117 83.6 47.8 71.5
Reading 
Comprehension
Total 355 235 196 83.4 55.2 80.3
45.5 29.4 63.834 22 10
 
Table 5. Evaluations at test data 
 
* Rate of Correct Answer 
6 Discussions 
We will discuss the problems and future direc-
tions found by the experiments. 
6.1 Recognition and Classification of Ques-
tions 
In order to solve a question in language test, 
students have to recognize the type of the ques-
tion. The current system skips this process. In 
this system, we set up about 100 types of ques-
tions referring the training data and a subpro-
gram solves questions corresponding to each 
type. There are two problems to be solved. First, 
we have to design the appropriate classification 
and avoid unknown types in the test data. From 
the experiment, we found that the current types 
are not enough to solve this problem. Second, 
the program has to classify the questions auto-
matically. We are building this system and are 
forecasting it quite optimistically once a good 
format is provided. 
6.2 Effectiveness of Large Corpus 
The large corpus of newspapers and the Web are 
used effectively in many different cases. We 
will describe several examples. 
In Japanese, there are different Kanji for the 
same reading. For example, Kanji for ???(au: 
to see, to solve correctly) are ???(to see)? or 
???(to solve correctly)? for ?????(to see 
people)? and ??????(to solve an answer 
correctly),? respectively. This type of questions 
can be solved by counting the expressions with 
Kanji in the corpus. It is similar to word sense 
disambiguation. 
In the questions of particle complement, such 
as ??? (umbrella) ??/?/? (locative-, con-
junctive-, and objective particles) ?? (home) 
??/?/?????? (to left) ? (Intentional 
sentence is ?I left the umbrella at home?)?, it can 
be solved by counting the expressions with each 
particle in a corpus. This method is mentioned in 
Matsui?2004?but the evaluation result was 
not reported. When the answer is not found for 
the entire expression, the answer is searched by 
deleting some contexts. Most questions of filling 
blank types, similar strategy is helpful to find 
the correct answer. 
In summary, the experiments showed that the 
large corpus is quite useful in several types of 
47
questions. We believe it would be quite difficult 
to achieve the same accuracy by compiled 
knowledge, such as a dictionary of verbs, anto-
nyms, synonyms, and relation words, and a the-
saurus.  
6.3 World Knowledge 
The questions sometimes need various types of 
world knowledge. For example, ?A student en-
ters junior high school after graduated from 
elementary school.? And ?People become happy, 
if he receives something nice from someone.? It 
is a difficult problem how to describe and how 
use that knowledge. Another type of world 
knowledge includes origin of words, such as 
foreign borrowed word or onomatopoeia. As far 
as we know, there is no comprehensive knowl-
edge of such in electronic form. It is required to 
design attributes of world knowledge and to use 
them flexibly when applying then to solve the 
questions. 
6.4 Difference between Reading Compre-
hension and Question Answering 
The current QA systems identify the NE type of 
questions and seek the answer candidate of the 
type. However, the questions in the reading 
comprehension don?t limit the answer types to 
person and organization, even if the question is 
?Who? type question. For example, ?raccoon 
dog behind our house? or ?the moon? can be the 
answer. Also, the answer is not always a noun 
phrase, but can be a clause, for example, ?the 
time when new leaves growing on a branch? for 
questions asking time. There are different kinds 
of questions, which are asking not the time of 
specific event but the time or season of the en-
tire story. For example ?When is this story 
about?? In this case, the question can?t be an-
swered by just extracting a noun phrase.  
However, at the moment, we can?t conclude 
if the question can or cannot be answered with-
out really understanding it. Sometime, we can 
find a correct answer without reading the story 
down the line or understanding the story per-
fectly. It is one of the future works. 
6.5 Other techniques: discourse and 
anaphora 
Some techniques other than morphological 
analysis, frequency of appearance in a corpus, 
and question answering methods are used in our 
system. We raise two issues. One of those is the 
discourse analysis. It is required in the questions 
to assign the order of paragraphs, and to select 
appropriate sentence conjunction. The other is 
anaphora analysis, which is very important, not 
only to indicate the antecedent, but also to find 
the link of mentions of entities.  
7 Conclusion 
 several inter-
esting NLP problems were found. 
Hi
Comprehension system?.  
Ch
er-based Language Understanding 
K.
of 
K. 
atural Language Processing, 2004, 
 
We develop a system to solve questions of sec-
ond grade language tests. Our objectives are to 
demonstrate the NLP technologies to ordinary 
people, and to observe the problems of NLP by 
degrading the level of target materials. We 
achieved 55% - 83% accuracy and
References 
rschman, L., Light, M., Breck, E. and Burger, J. D. 
?Deep READ: a Reading 
ACL, 1999, pp 325-332. 
arniak et al, ?Reading Comprehension Programs 
in a Statistical-language-Processing Class?. Work-
shop on Reading Comprehension Tests as Evalua-
tion for Comput
Systems. 2000. 
 Matsui: ?Search Technologies on WWW which 
utilize search engines?. (In Japanese) Journal 
Japanese Language, February, 2004, pp 34-43. 
Yoshihira, Y. Takeda, S. Sekine: ?KWIC System 
on WEB documents?, (In Japanese) 10th Annual 
Meeting of N
pp 137-139. 
 
F  
(http://languagecraft.jp/dennou/) 
igure 1. A Snapshot of the system
48
Transforming a Sentence End into News Headline Style
Satoshi Ikeda and Kazuhide Yamamoto
Dept. of Electrical Engineering, Nagaoka University of Technology
ikeda@nlp.nagaokaut.ac.jp, yamamoto@fw.ipsj.or.jp
Abstract
News on electrical bulletin boards con-
sist of high density expressions. Many
sentences end with unique expressions
that consist of nouns and case parti-
cles. This paper focuses on expressions
used at the end of sentences and at-
tempts to summarize them by forming
noun or case particle endings. We sum-
marize the news sentence through pat-
tern matching approach. Our evalua-
tion illustrates that the summarizer re-
duces 2.50 characters per sentence on
average; the reduction ratio is 6%. We
also show that people perceive the cor-
rect meanings of the summarized sen-
tences with 95% accuracy.
1 Introduction
Electrical bulletin board displays the latest
news headlines which each newspaper office an-
nounced. News headlines are shorter than news-
papers? news with laconicism because they are
summarized to transfer in limited space.
One of a characteristics of Japanese news head-
lines can be seen at sentence ends (Exp.1).
Exp.1)????????????????
(Countermeasure for alleged abduction is judged
after the movement of administration party.)
Although the end of sentence in Exp.1 is omit-
ted, we have no difficulty to understand the mean-
ing. We unconsciously complete the sentence by
guessing what is omitted without a mistake. With-
out unnecessary ends, these type of sentences are
short and nonredundant.
The final purpose of this work is to transform
news sentences into a news headline style. In
Japanese, sentences end with nouns or case par-
ticles are grammatically incorrect, however, this
kind of expressions are shorter than grammati-
cally perfect sentences, and hence often used to
meet the limited length. We believe many sen-
tences have semantically redundant expressions
in the end, which needs to be focused in summa-
rization.
In this paper we present a list of deletable ex-
pressions at Japanese sentence ends. We have
to carefully investigate which sentence ends are
deletable, and how to change into the headline
style. We present the concrete expressions of
deletion with examples and illustrate effects of
deletion.
2 Related Works
As the most similar work to ours, Sato et al[6]
tries to extract paraphrasing patterns of sentence
end by preparing a lot of alignment pairs between
news sentences and their headline versions. They
compare the sentences from the ends and obtain
many correspondences between the two. How-
ever, they have no proposals on how to use these
one-to-N correspondences, i.e., the way to select
one from many candidates. Our approach is to ob-
tain many transformation patterns as well, but we
do not use aligned corpus; we use a large collec-
tion of news headlines instead and find patterns
by our thorough observation.
Wakao et al[7] compares newscast and corre-
sponding subtitle expressions to investigate the
differences of them. One of the observation tar-
gets is sentence end, and they have shown us
some typical patterns of conversion into a short
news. This enumerates phraseologies which are
able to be cut down and investigates the frequency
of use. In news subtitles nouns or case particles
are used at the sentence end. This work is drew
upon literature [7] while we investigated in our
own right. We shaded light on the phraseologies
which do not exist literature [7] such as?????????????? (There seems to sur-
render.)?. We examined the phraseologies which
41
are disposed by machine. Fukushima et al [1] cut
off the unnecessary part from literature [7].
There are investigations to summarize text
by confining the number of characters [2,3,5].
Ishizako et al[2] cut off areas of overlap. Ohmori
et al[5] and Mikami et al[3] summarized text al-
together, but these investigations do not focus on
sentence ends.
3 News Headlines and Their Sentence
Ends
There is an email service that delivers Japanese
news headlines three times a day on week-
days. That is Nikkei news mail(1) provided by
NIKKEI-goo. We have been collecting them
since December 1999. Table1 shows the statistics
we have obtained.
Table 1: Statistical datum which are collected
number of mails 3365
number of stories 21127
number of sentences 40374
News headlines are more distinctive than news
stories in sentence end. Therefore, we investi-
gated part of speech on both news headlines and
newspaper(Nihon Keizai Shimbun(2)). Table2
shows the comparison.
Table 2: Occurrence ratio of POS in sentence end
occurrence ratio[%]
POS newspaper headlines
noun 23.70 55.92
(verbal noun) (5.00) (39.90)
verb 28.66 15.91
adjective 1.80 0.19
adverb 0.20 0.22
particle 1.56 8.83
(case particles) (0.34) (6.41)
auxiliary verb 38.59 18.52
symbol 5.42 0.40
In the newspaper, declinable words are respon-
sible for the majority of sentence ends. In news
headlines, there are in fact many verbal nouns in
sentence ends.
Japanese words are classified broadly into two
types; one derived from China and another origi-
nated in Japan. News headlines contain the for-
mer more than the latter because words from
China carry more information in fewer characters.
We investigated news headlines and news on a pa-
per which contained words of both Chinese and
Japanese origins. The result is shown in Table3.
In fact, news headlines preferably use the words
of Chinese origin about three times as much as
that of Japanese origin.
Table 3: Ratios of Chinese and Japanese origin
words in (a) newspaper and (b) headlines.
ratios [%]
Japanese Chinese (a) (b) a/b???? ?? (to be found) 1.059 2.658 0.398??? ?? (to decide) 0.622 2.184 0.285?? ?? (to elect) 0.210 2.643 0.079??? ?? (to find out) 0.181 2.875 0.063??? ?? (to order) 1.132 3.841 0.295??? ?? (to say) 0.456 0.181 2.493??? ?? (to investigate) 6.284 53.333 0.118
total 2.712 7.271 0.373
We can imagine that a short phraseology
is preferably used when the phraseology has
the same information. We estimate that the
news headlines are high density phraseology than
newspaper.
4 Method of Summarization
In order to transform a sentence end into a
shorter one, we have conducted three kinds of
procedures:
(1) Deletion of target words at sentence end
(2) Deletion with minor transformation after the
target words
(3) Transformation of sentence end
More precisely, we have proposed conduct-
ing the following 10 procedures for transforming
Japanese sentence ends into a news headline style:
1. Cut off dictum and honorific phraseology (1)
2. Cut off???? (wo shimesu:show)? (1)
3. Change verbal noun(2)
4. Cut off??? (naru)?(2)
5. Cut off the part which follows????? (akirakani)?
(2)
6. Change words of Japanese origin(2)
7. Cut off????? (teshimau)?(1)
8. Cut off??? (tatu)?(2)
9. Transform phraseology indicated the action in the fu-
ture (2)
42
10. Change to compound noun (3)
We summarized in this order, and process 3.?
9. can be switched.
4.1 Cut Off Dictum and Honorific Phraseol-
ogy
Phraseologies shown below are dictum or hon-
orific phraseology. These phraseology in sentence
end is cut off because these are not necessary to
understand the meaning.
? dictum phraseology: ???? (datta)? ????
(dearu)??? (da)?
? honorific phraseology:??? (masu)???? (desu)?
4.2 Cut Off???? (wo simesu:show)?
When a sentence end is ???? (wo
shimesu)?or????? (wo shimeshita)?, this
phraseology is cut off because??? (shimesu)?
has little meaning in that sentence. The main verb
of the sentence is the verbal noun before???? (wo shimesu)?.
4.3 Change Verbal Nouns
The expression after the verbal noun closest
to the main verb of the sentence is deleted. In
Japanese, we put a word??? (suru)? after a
verbal noun to make a verb, but in the summary
it can be deleted since we can still understand the
usage.
When a self-sufficient word exists following a
verbal noun, we do not dispose this.
Step 1 The part following??? (suru)? is cut.
Nominalized verbal noun to cut??? (suru)? is
the verbal noun in this arrangement.
Step 2 When the cut part contains an estimation
phraseology????? (mirareru)? or ???? (daou)?, tack on?? (ka))? and finish.
Exp.2)??????????????????
?????????????
(He seemed to surrender in trouble with escape fund.)
Step 3 When the cut part contains a contradic-
tion phraseology??? (nai)? or ?? (nu)?,
tack on??? (sezu)?at the sentence end and fin-
ish. When this part concurrently contains a pas-
sive phraseology??? (reru)?, tack on???? (sarezu)? and finish.
Step 4 When a sentence end is?noun?? (wo)? verbal noun?,?? (wo)?is cut to become a
compound noun?noun? verbal noun?.
Exp.3)?????????????????????????????????????????????????
(Starting this month, Japanese chess problems are seen in
ads of each station and in trains.)
Step 5 When a sentence end is ?particle1? noun ????? (surukoto) ? particle2 ?
noun?, ????? (surukoto)? is cut. If the
particle1 is?? (wo)?or?? (ka)?, this parti-
cle changes to?? (no)?.
If the cut part contains ???? (ha-
jimete:first)?, procedures from Step 2 is different
as follows.
Step 2 When the cut part contains?????
(surunoha)? or????? (shitanoha)?, ???? (hajimete:first)? is tacked on before verbal
noun. When the part of cut contains?????
(mirareru)?, tack on?? (ka)? in sentence end
and finish.
Step 3 When the cut part contains ???
(shite)?, ??? (go hatsu)?is tacked on in the
sentence end. When the term just before noun
is particle ?? (ka)?, this particle ?? (ka)?
changes into particle?? (no)?.
Exp.4)????????????????????
????????
?????????????????????
?????
(He first acceded the interview since Karmapa Seventeenth
left China.)
Step 4 .1 When a verbal noun is ??? (hat-
sugen:delivery)?or??? (genkyuu:citation)?,???? (hajimete:first)? is tacked on before the
verbal noun.
Exp.5)????????????????????
?????????????????
(Russian troop?s cadre first adverted to retreat.)
Step 4.2 When a verbal noun is not??? (hat-
sugen:delivery)?or??? (genkyuu:citation)?,
the term before the verbal noun is checked. The
sentence end is processed the following.
? particle?? (no)?,?? (ga)?? verbal noun? particle?? (no)?? verbal noun ???? (ha
hatsu)?
? particle?? (wo)?,?? (mo)?? verbal noun? particle?? (wo)?,?? (mo)?? verbal noun
? otherwise?verbal noun ???? verbal noun ???? (ha
hatsu)?
43
Step 5 When the cut part contains?????
(mirareru)?, ?? (ka)?is tacked on in the sen-
tence end.
4.4 Cut Off??? (naru)?
When ?particle ??? (naru)? exists in a
sentence, this part and the following are cut off.
When a self-sufficient word exists in the cut part,
the meaning changes or we do not understand the
meaning.
Therefore, when a self-sufficient word exists?particle??? (naru)?following, the sentence
is not disposed this arrangement.
The?particle??? (naru)? and the follow-
ing are cut off. When the particle is?? (ni)? or?? (to)?,?? (ni)? is tacked on in the sentence
end.
Exp.6)????????????????????
???????
?????????????????????
????
(The accord became the bare adoption because this arranged
after three and half months of general election ballot )
When the cut part contains a contradiction
phraseology??? (nai)? or?? (nu)?,???? (narazu)? is tacked on in the sentence end.
Exp.7)???????????????????
????????
????????????????????
?????
(Almost all detonating agents did not work because they
seemed to wet)
4.5 Cut Off the Part After??????
When????? (akirakani:out of doubt)?ex-
ists in a sentence, the part which follows????? (akirakani)? is cut off. When the cut part con-
tains a self-sufficient word, the meaning changes
or we do not understand the meaning.
Then, when a self-sufficient word exists in the
sentence, the sentence is not disposed this ar-
rangement.
Step 1 The part which follows????? (aki-
rakani)? is cut off.
Step 2 Research the part of cut and dispose the
cut part.
? Contradiction phraseology??? (nai)?or?? (nu)?
and passive phraseology??? (reru)? exist.????? (sarezu)?is tacked on in the sentence end.
? The contradiction phraseology??? (nai)? or??
(nu)?exists.???? (sezu)? is tacked on in the sentence end.
Exp.8)????????????????
??????????? ???(The amount of
loss is not announced.)
Step 3 When?????? (surukoto wo)? ex-
ists before????? (akirakani)?,?????
(surukoto wo)?is cut off. When the part before
the cut is?particle?? (ni)?? verbal noun?,?? (ni)? is changed to?? (e)?. When the part
before the cut part is?particle?? (wo)?? ver-
bal noun?,?? (wo)? is changed to?? (no)?.
4.6 Change Words of Japanese Origin
When a Japanese origin word by Table3 exists
in a sentence, the part before it is cut off. Then
the Japanese origin word is replaced by Chinese
one.
When a self-sufficient word exists following
Japanese origin word, the sentence is not disposed
of this arrangement. We changed the word which
shows Table3.
Step 1 Japanese origin word and following are
cut off.
Step 2 When sentence end is ?????? (surukoto wo)?, cut off ????? (su-
rukoto:doing)?, tack on the correspondent Chi-
nese origin word, and finish the arrangement.
Exp.9)?????????????????????
???????
????????????????????
????
(They have decided to start making an ?instruction book on
extensive assistance for disaster?.)
Step 3 When a sentence condition is followed,
the sentence is disposed.
? A sentence end is a particle?? (ga)? and Japanese
origin word is???? (wakaru:understand)?? tack on??? (hanmei:understand)? and finish.
? A sentence end is a particle?? (ga)? and Japanese
origin word is not???? (siraberu:census)?? The particle?? (ga)?is changed to a particle??
(wo)?.
? A sentence end is?? (ga)? noun?? (de)????
(no)? noun?? (wo)?
? A sentence end is a particle?? (ha)? and Japanese
origin word is???? (wakaru:understand)?? Get the former sentence back again and finish.
? Japanese origin word is ???? (siraberu:census)?
and the cut part contains????? (siteiru)?
44
? tack on???? (tyousa tyu:under survey)?at the
sentence end and finish.
Step 4 Chinese origin word which corresponds
Japanese origin word tacked on the sentence end.
Exp.10)????????????????
??????????????
(The total of 359 counterfeit coins were found.)
4.7 Cut Off????? (teshimau)?
When a sentence contains????? (teshi-
mau)?, we feel that the sentence is negative and????? (teshimau)? is not necessary to un-
derstand the meaning of the sentence. Thus we
cut off????? (teshimau)? in the headline.
This arrangement is used not only the sentence
ends but middle of the sentence. When the term
after the cut part is ?? (ba)?, we do not dis-
pose it. When the sentence end is?????
(teshimau)?, change the term before?????
(teshimau)?to primitive form and finish.
When????? (teshimau)? exists without
the sentence end, ????? (teshimau)? and
the character before this phraseology is cut off.
4.8 Cut off??? (tatsu)?
When a sentence contains??? (tatsu)?,??? (tatsu)?, the part following it is cut off. When
the following part contains the self-sufficient
word, the meaning changes or we do not under-
stand the meaning.
Therefore, when a self-sufficient word exists in
the following part, the sentence is not disposed
this arrangement. When??? (tatsu)? is a part
of idiom, the sentence is not disposed of this ar-
rangement.
Step 1 ??? (tatsu)? and the following part
are cut off.
Exp.11)?????????????????????
??????
??????????????????????
????
(?Top boy? is acme in TV game retail business)
Step 2 When a contradiction phraseology???
(nai)? or ?? (nu)?exists in the cut part,???? (tatazu)? is tacked on at the sentence end.
4.9 Phraseology of Words Implying Future
When a phraseology which indicate the action
in the future such as ??? (keikaku:attempt)?
or ??? (yotei:plan)? exists in the sentence,
the phraseology can changed to ?? (he)? in
Japanese. Therefore, the terms listed below are
the phraseology of indicated the action in the fu-
ture. When ??? (suru) ? this phraseology?
exists in the sentence, this part and following are
changed to?? (he)?.
??? (yotei:plan)???? (keikaku:attempt)????
(houshin:policy)???? (houkou:future direction)?
When??? (suru)? this phraseology?exists
in a sentence and the following contains a contra-
diction phraseology??? (nai)?or?? (nu)?,
the sentence is not disposed of this arrangement.
When the following contains the???? (toiu)?
or ???, the sentence is not disposed of this ar-
rangement.
??? (suru)? this phraseology?and follow-
ing are cut off. when the sentence end is particle,
the particle is cut off.?? (he)? is tacked on the
sentence end.
4.10 Change to a Compound Noun
When a sentence end is?noun? particle?
verbal noun? after the above arrangements, the
particle cut off to become a compound noun.
When the noun is neither pronoun, person name,
unique noun nor postfix for Chasen(3), this ar-
rangement is not disposed. When the particle is??? (kara)?,?? (de)? or?? (mo)?, this
arrangement is not disposed.
We make a compound noun dictionary for The
Mainichi Newspapers(4) to check the adequacy of
compound nouns. When?noun particle? (ni)
verbal noun? and the dictionary contains?noun? verbal noun? which is cut of???,?noun? particle? (ni)? verbal noun?is changed to?noun? verbal noun?. When the particle is not?? (ni)?,?noun? particle? verbal noun?is
changed to?noun? verbal noun?.
Exp.12)????????????????????
??????????????????
?????????????????
(A man?s body was found on the third floor of burned-out
site.)
5 Experiments
We implemented the proposed technique with
Perl programming language to measure the ade-
45
quacy of proposed technique. We summary with
this program. Then input sentence are all sen-
tences seen in the newspaper corpus. The number
of input sentences is 232,038, and 73,512 outputs
are somehow summarized in our method.
5.1 Summarization Ratio
We calculated a sentence ratio and number of
reduced characters in a sentence. This result of
experiment is shown in Table4. The method of
Table4 shows the section number. This Table4
shows the result which used the only one method.
The summarization ratio is 94%. In fact, this
method is reduced the 6% about one sentence.
Table 4: Summarization ratio
process 4.1 4.2 4.3 4.4 4.5
# sentence 16825 1313 37995 7510 199
summ. ratio 0.94 0.94 0.94 0.93 0.90
# reduced char. 1.60 4.00 2.56 3.12 5.41
process 4.6 4.7 4.8 4.9 total
# sentence 7194 600 197 848 72681
summ. ratio 0.96 0.89 0.92 0.87 0.94
# reduced char. 2.20 3.93 3.28 6.57 2.45
5.2 Subjective Evaluation
We also evaluated the proposed technique by
human judgment. We picked up 1,000 sentences
at random from summary sentences, and three ex-
aminees individually accounted them. The sen-
tences are measured by majority decision. As-
sessment criterion is: (1) same meaning without
context, and (2) low unnaturalness. The result is
shown in Table5. The numbers in the table de-
note the section numbers explaining the process
of transformation.
Table 5: Correctness of each process
method 4.1 4.2 4.3 4.4 4.5
# sentence 231 19 492 107 9
# correct 205 18 481 106 8
ratio 0.89 0.95 0.98 0.99 0.89
method 4.6 4.7 4.8 4.9 total
# sentence 116 21 3 13 1000
# correct 113 17 3 12 952
ratio 0.97 0.81 1 0.92 0.95
We have also computed the influence of per-
sonal difference. In this kind of subjective evalua-
tion different person may answer difference judg-
ment. We have evaluated our results in three cri-
teria: (1) at least one said correct, (2) at least two
said correct, and (3) all three said correct. This re-
sult is shown in Table6. The Table illustrates that
correctness is more than 90% in all cases.
Table 6: Correctness changes by personal differ-
ences.
? 1 ? 2 = 3
correctness 0.98 0.95 0.91
5.3 Comparison to the Human Summaries
We compare summaries of the proposed
method and by the human. We picked up 100 sen-
tences in summary sentences at random. One ex-
aminee summarized the original sentences which
corresponded the pick up the summary sentences.
We computed the summarization ratio about these
sentences. The result is shown in Table7.
Table 7: Comparison of summaries by proposed
technique and manual summary
machine human
# sentence 72727 100
summ. ratio 0.94 0.92
# reduced characters 2.45 3.87
Although the sentence ratio of machine sum-
mary is close to the manual summary?s one, num-
ber of reduced characters are approximately one
character different. This indicates that human try
to change many parts of sentence according to
the change of the sentence end, while the ma-
chine does not consider such influence. Change
of sentence end often requires transforming the
whole syntax structure, such as change of aspect
or form. We need more investigations on this is-
sue.
6 Discussions
6.1 Discussion of Erroneous Summaries
In this section we describe some erroneous
summaries by our method and discuss the rea-
sons.
Exp.13)????????????????????
???? 15.5mm???????????
46
?*1????????????????????
????????????
(The face show the character like an annual ring.)
Exp.13 is error example in arrangement ?cut
off the ???? (wo shimesu:show)?? When
the term before ???? (wo shimesu)?is the
noun, the sentence does not have main verb. The
main verb which does not exist in the sentence
is not right in Japanese. when the term before???? (wo shimesu)? is noun, this arrange-
ment does not disposed. This kind of error is
covered. But when the noun is ??? (kan-
gae:concept)?,??? (ikou:disposition)?or???? (mitooshi:forecast)?, this arrangement is
correct.
Exp.14)?????????????????????
?*????????????????
(It is decided to caution the overuse to user.)
Exp.14 is the error example ?change the word
of Japanese origin. When the cut off????? (surukoto:doing)?, the modification relation
is changed. Therefore, the modification relation
is a wrong one. When the particle?? (wo)? is
changed to particle?? (no)?, this kind of error
is covered(Exp.15).
Exp.15)??????????????????????????????????????
Exp.16)?????????????????
?*?????????????
(He thinks that his mother killed.)
Exp.16 is an error example in ?cut off????? (teshimau)??. When????? (tesimau)?is
cut off, it is not congruent inflected forms of????? (teshimau)? and the verb. When????? (teshimau)? is cut off, the inflected forms
must be congruent.
6.2 Verbalness/Nominalness of Verbal Noun
The sentence end is???? (ha hatsu:first)?
in Section4.3. There are a big differences by hu-
mans in degree of accepting this expression. We
thus change expression???? (ha hatsu)?into????? (hajimete:first)??. The example be-
fore changed is shown in Exp.17.
Exp.17)??????????????????
??????????
????????????????????
?????
1symbol ?*? indicates that the sentence is wrong.
(It is the first time that President Putin has a talk to the cap-
tain of Arab Crown)
Some people feel unnatural or wrong in this
example. But when the original sentences do
not have ???? (hajimete:first)?, the sum-
mary sentences are correct. The example is shown
Exp.18 without???? (hajimete:first)?.
Exp.18)??????????????????
?????
??????????????????????
This example gives us no unnaturalness. We
think the verbal noun affect this. The verbal noun
represents that indicates the kind. The verbal op-
eration of verbal noun is varied by humans.
We think concretely about ???
(kaidan:meeting)? of Exp.17and Exp.18.
First, we think that ??? (kaidan:meeting)?
is complemented the verbal noun ?????
(kaidan suru:have a talk)?. The predicate is
generally at sentence end in Japanese. When
the predicate does not exist in a sentence, it is
inclinable in human thought that sentence end
term is predicate. The other hand, we think
that ??? (kaidan:colloquy)? is nominal
or verbal operation in Exp.18 because ???
(kaidan:meeing)? is not sentence end. then
when ??? (kaidan:meeting)? is nominal,
human have unnaturalness. And when ???
(kaidan:meeting)? is verbal, human do not feel
unnatural.
We cite the error summary which sentence end
is noun other than verbal noun in this paper but
the verbal operation of noun is pertained in these
sentences. And the noun of operation verbal is??? (kangae:concept)?other than verbal noun.
6.3 Comparison of Machine and Manual
Summaries
We examine the machine and manual sum-
maries. Although many sentences are not much
different, some sentences have big differences for
summarization. One example is shown as fol-
lows, original sentence, its machine summary and
its manual summary respectively.
Exp.19)?????????????????????
????????????????????
??????????????????
(There is graph used the color picture.)
Exp.19 is cut off the honorific phraseology but
the manual summary is cut off??? (aru)? too.
47
This is shown that??? (aru)?is dictum phrase-
ology. And the sentence end is?? (mo)?. This
is often seen in the news headline. But the pro-
posed technique do not deal with them.
6.4 Summarization Failure
We examine the sentences which are not sum-
marized by the method. We picked up the 200
sentences at random and examine whether or not
it should be summarized. This results is that 9
sentences are missing. The example is shown be-
low with the supposed summary.
Exp.20)???????????????
????
?????????????????
(Mr. Ikemoto?s blob was found from the burned-out site.)
Exp.20 is not summarized. The reason of this
error is caused by an error of the morphological
analysis.
7 Conclusion
In order to generate short and smart style seen
in news headlines, this paper presents a method of
transforming Japanese sentence end expressions
into short style. Our observation reveals that the
end of sentence in the headlines are either nouns
or case particles in many sentences, we thus at-
tempt to summarize them as short as possible. We
have implemented the approach and evaluated in
summarization ratio and their correctness. The
results illustrates that the reduction ratio is 6%
against overall sentence length, and the sentence
is expected to be cut off 2.50 characters per sen-
tence. The length of automatic shortening is ap-
proximately the same as manual summarization.
We also confirmed that 95% of the summaries
were judged to be correct.
Acknowledgment
This work was supported in part by MEXT Grants-in-Aid
for Young Scientists (B) 16700134, and for Scientific Re-
search (A) 16200009, Japan.
Tools and language resources
(1) Nikkei news mail, NIKKEI-goo,
http://nikkeimail.goo.ne.jp/
(2) Nihon Keizai Shimbun Newspaper Corpus,
year 2000, Nihon Keizai Shimbun, Inc.
(3) Chasen, Ver.2.3.3, Matsumoto Lab, Nara In-
stitute of Science and Technology.
http://chasen.naist.jp/hiki/ChaSen/
(4) The Mainichi Newspapers Corpus, year 2000,
Mainichi Newspaper Co., Ltd.
References
[1] Takahiro Fukushima, Terumasa Ehara and Kat-
suhiko Shirai. 1999. Regulation for Reducing Num-
ber of Characters for Sentence Simplification, Pro-
ceedings of The Fifth Annual Meeting of The As-
sociation for Natural Language Processing, pp.221?
224. (in Japanese)
[2] Yuko Ishizako, Akira Kataoka, Shigeru Masuyama
and Seiichi Nakagawa. 1999. Summarization by
Reducing Overlaps and Its Application to TV News
Texts. IPSJ SIG Technical Reports 99-NL-133(7),
pages 45?52. Information Processing Society of
Japan. (in Japanese)
[3] Makoto Mikami, Shigeru Masuyama and Seiichi
Nakagawa. 1999. A Summarization Method by Re-
ducing Redundancy of Each Sentence for Mak-
ing Captions of Newscasting. Journal of Natural
Language Processing Vol.6, No.6, pp.65?81. (in
Japanese)
[4] Kiyonori Ohtake and Kazuhide Yamamoto. 2001.
Paraphrasing Honorifics. Proc. of NLPRS2001
Post-Conference Workshop on Automatic Para-
phrasing: Theories and Applications, pp. 13?20.
[5] Takefumi Oomori, Hidetaka Masuda and Hiroshi
Nakagawa. 2003. Web News Articles Summariza-
tion and its Evaluation using Articles for Mobile
Terminals, IPSJ SIG Technical Reports 2003-NL-
153(1). pages 1?8. Information Processing Society
of Japan. (in Japanese)
[6] Dai Sato, Moritaka Iwakoshi, Hidetaka Masuda
and Hiroshi Nakagawa. 2004. Extraction of Para-
phrasing Patterns from Aligned Corpora of Web
and Mobile Terminal News Articles. IPSJ SIG
Technical Reports 2004-NL-159(27). pages 193?
200. Information Processing Society of Japan. (in
Japanese)
[7] Takahiro Wakao, Terumasa Ehara and Katsuhiko
Shirai. 1997. Summarization Methods Used for
Caption in TV News Programs, IPSJ SIG Technical
Reports 97-NL-122(13). pages 83?89. Information
Processing Society of Japan. (in Japanese)
48
Summarization by Analogy:
An Example-based Approach for News Articles
Megumi Makino and Kazuhide Yamamoto
Dept. of Electrical Engineering, Nagaoka University of Technology
1603-1 Kamitomioka, Nagaoka, Niigata 940-2188 Japan
{makino,ykaz}@nlp.nagaokaut.ac.jp
Abstract
Automatic summarization is an important
task as a form of human support technology.
We propose in this paper a new summariza-
tion method that is based on example-based
approach. Using example-based approach
for the summarization task has the following
three advantages: high modularity, absence
of the necessity to score importance for each
word, and high applicability to local con-
text. Experimental results have proven that
the summarization system attains approxi-
mately 60% accuracy by human judgment.
1 Introduction
The example-based approach generates language by
imitating instances, which originated in the machine
translation method based on the analogy (Nagao,
1984). The idea is derived from the observation that
a human being translates according to past transla-
tion experiences. In the machine translation task,
this approach has been implemented, and has so far
achieved efficient results (Sumita, 1998; Imamura,
2004).
In summarization, a human being also summa-
rizes with his own knowledge and experiences. For
this reason, we focus on a summarization method
which is based on analogy, example-based summa-
rization. The example-based method summarizes
the input text in three steps. First, it retrieves a simi-
lar instance to the input text. Second, it links equiv-
alent phrases between the input text and the similar
instance. Finally, a summary is acquired with com-
bination of some corresponding phrases. Here, we
employed a Japanese news article as the input text
and utilized news headlines as the instances. The
news headline consists of one brief sentence which
describes the main point.
We assert that the example-based summarization
has the following advantages:
(1)High modularity
Easy improvement and maintenance are required
to formulate a useful system in general. An
example-based framework makes it easy for us to
improve a system by only adding instances. Besides,
the addition of instances causes few side-effects.
(2)Use of similarity rather than importance
Almost all previous work on summarization has
focused on a sentence extraction. These works com-
pute importance for each word to extract a sentence.
However, it is difficult to compute the importance
which correlates with human sense. Example-based
summarization means there is no need to measure
the importance, and it computes the similarity in-
stead. We think it is easier to assess the similarity
between two expressions rather than the importance
of one expression.
(3)High applicability to local context
The statistical method, in general, attempts to
compute the probability of each word appearing in
the summary corpus (Knight and Marcu, 2002; Wit-
brock and Mittal, 1999). This may increase difficul-
ties in maintaining local context, since the statistical
approach focuses on the global probability. How-
ever, the example-based approach attempts to find
most locally similar instance out of the instance col-
lection, which may increase the fitness of input con-
texts.
For the three reasons given above, this paper
explains the system which summarizes a Japanese
news article to a one-sentence summary by imitat-
ing the similar instance.
739
As related work, Nguyen et al (2004) have pro-
posed an example-based sentence reduction model.
They deal with the compression of one sentence,
while we summarize some sentences into a one-
sentence summary. Thus, our summarization ratio
is inevitably lower than theirs, as it is considered to
be more difficult as a summarization task.
Many studies have summarized some sentences,
such as a news article, into a one-sentence summary.
Most of them extract the important sentence and
contract it. In contrast, our method generates a one-
sentence summary by combining phrases in some
sentences. Consequently, we can obtain high com-
pression summaries that include information from
many positions of the source.
2 Instance Collection
Our example-based summarization regards news
headlines as the instance collection. A news head-
line is a short sentence in which the primary point
is written. The following example is Japanese news
headlines:
Example (1) :
????????????????????
(Mitsubishi Motors Corp. produces passenger cars
in China.)
We use Japanese news headlines, like the above
examples, as instances. Besides, as we have noted,
only news headlines are used as instances; that is,
the pairs formed by an original sentence and its sum-
marized sentence are not used.
3 Example-based Summarization
3.1 Overview
Our example-based summarization system summa-
rizes a lengthy news article into a one-sentence sum-
mary by using instances. The overall process is il-
lustrated in figure 1. The system is composed of the
following three processes in this order:
1. Retrieve a similar instance to an input news ar-
ticle from the instance collection.
2. Align corresponding phrases between the input
news article and the similar instance.
3. Combine the corresponding phrases to form a
summary.
Detail of each process is described hereafter.
3.2 Retrieval of Similar Instance
The system measures a similarity between the input
and each instance in the instance collection when
it retrieves a similar instance. If many words are
shared between two expressions, we regard two ex-
pressions as similar. Hence, the similarity is calcu-
lated on basis of the overlaps of content words be-
tween the input news article I and the instance E ,
defined as follows:
Sim(E, I)=
n
?
i=1
Score(i)? {w ? ||T v1(E)?Tvi(I)||
+||To1(E)?Toi(I)||} (1)
where,
- n : the number of sentences in input,
- Tvi(?) : the verbs set in the last phrase of the i-th
sentence,
- Toi(?) : the set of content words in the i-th sen-
tence,
- ||Tv1(E)? Tvi(I)|| : the number of overlaps be-
tween Tv1(E) and Tvi(I).
In the equation, Score(i) and w are designed to give
a higher score if words indicating the main topic of
the input article are matched with words in the in-
stance. We have found that words have different
contributions, depending on the sentence position,
to the main topic. Therefore, we apply Score(i)
which depends on the sentence position i, and we
use the following experimentally-determined score
as Score(i).
Score(i) =
{
5.15 if i = 1
2.78/i0.28 otherwise (2)
The score indicates an agreement rate of content
words depending on the sentence position, which is
calculated by using 5000 pairs of newspaper?s body
and its title1 We have also found that the verbs in
the last phrase are appropriate for the main topic of
the input article. For that reason, we determine the
weight w=3 by our experiment.
Example 2 shows the similar instance obtained by
measuring the similarity.
Example (2) :
Input news article
?????????????????????
?????? 24??????????(skip the
1We used the same kind of newspaper as data set in section
4.1 for calculating Score(i).
740
Figure 1: Overview of example-based summarization
rest.)
(The Manufacturing Council held a meeting on the
24th, which discusses the hard-hitting strategy for
quality management. ...)
Obtained similar instance
????????? 18???????????
????
(The committee for the privatization of the Public
Roads Administration held the first meeting on the
18th at the prime minister?s office.)
3.3 Phrase Alignment
We compare the phrases in the input with those in
the similar instance, and the system aligns the corre-
sponding phrases. Here, the correspondence refers
to the link of the equivalent phrases between the in-
put and its similar instance. The detail of phrase
alignment procedures are shown in the following.
To begin with, sentences both in the input and in
the similar instance are analyzed using a Japanese
syntactic parser CaboCha1). The sentences are split
into phrases and named entities (NEs), such as PER-
SON, LOCATION, DATE, are recognized by the
tool.
Then the adnominal phrases in the similar in-
stance are deleted. This is because the adnomi-
nal phrases are of many types, depending on the
modified noun; accordingly, the adnominal phrase
should be used only if the modified nouns are ex-
actly matched between the input and the similar in-
stance.
Finally, the system links the corresponding
phrases. Here, phrase correspondence is one-to-
many, not one-to-one, and therefore a phrase in a
similar instance has some corresponding phrases in
the input. In order to compare phrases, the following
four measures are employed: (i) agreement of gram-
matical case, (ii) agreement of NE, (iii) similarity
with enhanced edit distance, and (iv) similarity by
means of mutual information. The measure of (i)
focuses on functional words, whereas the measures
of (ii)-(iv) note content words. Let us explain the
measures using example 2.
(i) Agreement of Grammatical Case
If there is a phrase which has the same grammati-
cal case2 in the input and in the similar instance, we
regard the phrase as the corresponding phrase. In
example 2, for example, the phrases ????? ?
(the hard-hitting strategy obj3), ??? (the meet-
ing obj)? in the input corresponds the phrase ???
??(the first meeting obj)? in the similar instance.
(ii) Agreement of Named Entity
Provided the input has the same NE tag as the sim-
ilar instance, the phrase involving its tag links the
corresponding phrase. For example, in example 2,
the phrase ?24? [DATE] (on the 24th.)? in the in-
put corresponds the phrase ?18? [DATE] (on the
18th.)? in the similar instance.
(iii) Similarity with Enhanced Edit Distance
We adopt the enhanced edit distance to link phrases
including the same characters, because Japanese ab-
breviation tends to include the same characters as
the original. For example, the abbreviation of ??
2Comma is also regarded as grammatical case (i.e., null
case) here.
3
?obj? is an object case marker.
741
??? (Bank of Japan)? is ????. The enhanced
edit distance is proposed by Yamamoto et al (2003).
The distance is a measure of similarity by counting
matching characters between two phrases. More-
over, the distance is assigned a different similarity
weight according to the type of matched characters.
We apply 1.0 to the weight only if Chinese-derived
characters (Kanji) are matched. We link phrases as
corresponding phrases, where the phrases are the top
three similar to a phrase in the similar instance.
(iv) Similarity with Mutual Information
We finally compute the similarity with mutual in-
formation to link syntactically similar phrases. For
example, given the following two expressions: ??
???? (to hold a meeting)? and ?????? (to
hold a convention)?, we regard?? (a meeting) and
?? (a convention) as similar. We use the similar-
ity proposed by Lin (1998). The method uses mu-
tual information and dependency relationships as the
phrase features. We extend the method to Japanese
by using a particle as the dependency relationships.
We link phrases as corresponding phrases, where the
phrases are the top three similar to a phrase in the
similar instance.
3.4 Combination of the Corresponding Phrases
Our system forms the one-sentence summary by
combining the corresponding phrases. Let us ex-
plain this process by using figure 2. We arrange the
phrase of the input on the node, where the phrases
is judged as the correspondence to the phrase in the
similar instance. For example, in figure 2, the sec-
ond nodes e and d denote the corresponding phrases
in the input, which correspond to the second phrase
had in the similar instance.
We assign the similarity between corresponding
phrases as the weight of node. In addition to this,
we employ phrase connection score to the weight of
edge. The score indicates the connectivity of con-
secutive two phrases, e.g. two nodes such as node
d and node e in figure 2. If you want to obtain a
fine summary, i.e., a summary that contains similar
phrases to the similar instance, and that is correct
grammatically, you have to search the best path ?Wp
for path sequence Wp = {w0,w1,w2, ? ? ? ,wm}, where
the best path maximizes the score.
?Wp =Wp s.t. argmax
p
Scorep(Wp) (3)
Figure 2: Optimal path problem that depends on
combination of the corresponding phrases4.
The best path ?Wp is a one-sentence summary which
is generated by our system. Take the case of the
thick line in figure 2, ?Wp is indicated as ?Wp =
{a,d,e,g,k,m,n}, namely, generated summary is
formed the phrases {a,d,e,g,k,m,n}. In eq.3,
Scorep(Wp) is given by
Scorep(Wp)=?
m
?
i=0
N(wi)+(1??)
m
?
i=1
E(wi?1,wi) (4)
where ? is the balancing factor among the
weights of node and edge. We score ? = 0.6 by
our experiment. m indicates the last number of the
phrase in the similar instance, N(wi) is given as fol-
lows:
N(wi)=max
{ 0.5 if (grammatical case or
NE tag is matched)
1/rank otherwise
(5)
where, rank indicates the rank order of the similarity
with the enhanced edit distance or mutual informa-
tion to the phrase wi. N(wi) illustrates the similar-
ity between corresponding two phrases. The node
score, shown above, is determined by the prelim-
inary experiment. The edge score E(wi?1,wi) is
given by
E(wi?1,wi) =
1
|loc(wi?1)? loc(wi)|+1
(6)
where, loc(wi) denotes where the location of the
sentence contains the phrase wi in the input. The
edge score means that if wi?1 and wi are located
closely to each other, a higher score is given, since a
good connection is expected in this case.
4The nodes, a, b, c,? ? ? , n, indicate the corresponding
phrases to the phrase in the similar sentence. For example, the
nodes, b, c, d correspond to ?The PRA Committee.? i is a phrase
number in the similar sentence.
742
4 Evaluation and Discussion
4.1 The Corpus
We used 26,784 news headlines as instances, which
were collected from the Nikkei-goo mail service2)
for 2001-2006. In order to adjust the weight w in the
eq.1 and the balancing parameter ? in eq.4, 150 in-
put news articles were used as the tuning set. A dif-
ferent group of 134 news articles were used for eval-
uation. We used Nihon Keizai Shimbun, a Japanese
newspaper 3) , from 1999 through 2000 as tuning and
test data.
4.2 Summarization Ratio
To calculate summarization ratio, we have compared
the number of characters in the input news articles
with that in the output summary. As the result,
we obtained a summarization ratio of 5%; namely,
95% characters in the input were reduced. From the
summarization ratio, our approach made it possible
to summarize sentences into one-sentence summary
with high compression.
4.3 Sectional Evaluation
We evaluated each part of our system by human
judgment5. We first evaluated the process by retriev-
ing similar instance. Next, we evaluated the pro-
cesses of phrase alignment and the combination by
assessing whether the output summaries were appro-
priate.
? Retrieving Process
An examinee evaluated the similar instances ob-
tained. Given an input news article and the similar
instance to the input, the examinee rates the follow-
ing scale from one to four, based on how similar the
similar instance obtained is to the summary which
the examinee generated from the input news article:
1) quite similar 2) slightly similar
3) not very similar 4) not similar
Out of 134 input articles, 77 inputs were ranked
either 1) quite similar or 2) slightly similar. As a
consequence, the accuracy of similar instance ob-
tained is approximately 57%, which indicates that
the similarity calculation for obtaining similar in-
stance is feasible.
5One examinee judged the parts of our system.
? Phrase Alignment and Combination
We also evaluated parts of phrase alignment and
the combination by human judgment. The exami-
nee compared 77 output summaries with their input.
Here, we limited 77 outputs judged as good similar
instances in evaluation of the process of retrieving
similar instance, because we evaluate specifically
the parts of phrase alignment and combination.
The examinee categorized them based on how
proper the output summary is to the input news arti-
cle:
1) quite proper 2) slightly proper
3) not very proper 4) not proper
As a result of judgment, 48 outputs out of 77 are
evaluated either 1) quite proper or 2) slightly proper.
Both a statistical method by Knight and
Marcu (2002) and an example-based method by
Nguyen et al (2004) contracted one-sentence with
a summarization ratio of approximately 60-70%.
Both papers indicated that a score of 7-8 on a scale
from one to ten was obtained. They deal with the
compression of one sentence, while we summarize
some sentences into a one-sentence summary. Thus,
our summarization ratio is lower than theirs, as it is
considered to be more difficult as a summarization
task. Despite this, we obtained the ratio that 62%
(48 out of 77 results) were judged proper. Although
direct comparison of the performance is impossible,
it is considered that our proposed method obtains a
competitive accuracy.
4.4 Discussions
? Examples of Output Summary
Figure 3 shows some examples of the output sum-
mary.
From figure 3, we can see that the similar in-
stances were effectively used, and the appropriate
summaries to the input are generated. For example,
the second summary in the figure is judged as a fine
summary contracting information of two sentences
according to the similar instance.
? Analysis of Summarization Errors
In the course of our summarization, we have ob-
served errors due to erroneous correspondences. In
Japanese, sometimes two or more phrases are con-
tracted into one phrase, as in the example below. We
now only attempt to correspond two phrases one by
743
Input news article?
??????????????????????????
??????????????????????????
??????????????????????????
??????????????????????????
???????????????????????????
?????????????????(skip the rest.)
(The prosecution made Kawano?s closing arguments on the 21st
in the trial at the Yokohama District Court. The ex-sergeant
Suguru Kawano is accused of gang-bashing by Atsugi Police
Station?s patrol group in a string of scandals of Kanagawa Pre-
fectural Police. The prosecutors demanded one and half year in
a prison. ...)
Obtained similar instance?
????? 22??8?????????????????
??????????????????????????
???
(The prosecution made Takuma?s closing arguments on the
22nd in the trial at the Osaka District Court, and asked for the
death penalty.)
Output summary?
??????????????????????????
????????????????
(The prosecution made Kawano?s closing arguments on the 21st
in the trial and demanded one and half years in prison.)
Figure 3: The examples of generated summary
one, and we thus can not deal with many-to-one cor-
respondences.
Example (3) :
?????/6????????/
(compare with the same month last year)
5??/???? 5???/
(in May)
We expect that this kind of phenomenon can
be solved by paraphrasing an input summary as
well as summary instance. Recently, several works
on paraphrasing techniques have been proposed in
Japanese, hence such pre-processing before align-
ment would be feasible.
5 Conclusion and Future Work
We have presented an example-based technique that
has been applied to the summarization task. The
essence of the proposed method is to generate a one-
sentence summary by combining instances each of
which imitates the given input.
As the result of human judgment, the retrieval
process of a similarity sentence attained 57% accu-
racy. And our method generated summary in which
62% were judged proper. We have confirmed by
our observation that the summaries were generated
by combining the phrases in many positions of the
input, while those summaries are not given just by
6
?/? indicates a phrase boundary.
common methods such as sentence extraction meth-
ods and sentence compression methods.
The sectional evaluation and the inspection of
example output show that this system works well.
However, larger scale evaluation and comparison of
its accuracy remain to be future work.
Tools and language resources
1) CaboCha, Ver.0.53, Matsumoto Lab., Nara Institute of
Science and Technology.
http://chasen.org/?taku/software/cabocha/
2) Nikkei News Mail, NIKKEI-goo,
http://nikkeimail.goo.ne.jp/
3) Nihon Keizai Shimbun Newspaper Corpus, years 1999?
2000, Nihon Keizai Shimbun, Inc.
References
Kenji Imamura. 2004. Automatic Construction of Trans-
lation Knowledge for Corpus-based Machine Transla-
tion. Ph.D. thesis, Nara Institute of Science and Tech-
nology.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion Beyond Sentence Extraction: A Probabilistic Ap-
proach to Sentence Compression. Artificial Intelli-
gence, 139(1):91?107.
Dekang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of COLING-ACL98,
pages 768?773.
Makoto Nagao. 1984. A Framework of a Mechanical
Translation Between Japanese and English By Anal-
ogy Principle. In Artificial and Human Intelligence,
pages 173?180.
Minh Le Nguyen, Susumu Horiguchi, Akira Shimazu,
and Bao Tu Ho. 2004. Example-Based Sentence
Reduction Using the Hidden Markov Model. ACM
Transactions on Asian Language Information Process-
ing, 3(2):146?158.
Eiichiro Sumita. 1998. An Example-Based Approach
to Transfer and Structural Disambiguation within Ma-
chine Translation. Ph.D. thesis, Kyoto University.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
Summarization: A Statistical Approach to Generat-
ing Highly Condensed Non-Extractive Summaries. In
Research and Development in Information Retrieval,
pages 315?316.
Eiko Yamamoto, Masahiro Kishida, Yoshinori Takenami,
Yoshiyuki Takeda, and Kyoji Umemura. 2003. Dy-
namic Programming Matching for Large Scale Infor-
mation Retrieval. In Proceedings of the 6th Interna-
tional Workshop on Information Retrieval with Asian
Languages, pages 100?108.
744
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 133?136,
New York, June 2006. c?2006 Association for Computational Linguistics
Using Phrasal Patterns to Identify Discourse Relations  
Manami Saito 
Nagaoka University of  
Technology 
Niigata, JP 9402188 
saito@nlp.nagaokaut.ac.jp 
Kazuhide Yamamoto 
Nagaoka University of 
Technology 
Niigata, JP 9402188 
yamamoto@fw.ipsj.or.jp
Satoshi Sekine 
New York University 
New York, NY 10003 
sekine@cs.nyu.edu 
 
Abstract 
This paper describes a system which 
identifies discourse relations between two 
successive sentences in Japanese. On top 
of the lexical information previously 
proposed, we used phrasal pattern 
information. Adding phrasal information 
improves the system's accuracy 12%, 
from 53% to 65%. 
1 Introduction 
Identifying discourse relations is important for 
many applications, such as text/conversation 
understanding, single/multi-document 
summarization and question answering. (Marcu 
and Echihabi 2002) proposed a method to identify 
discourse relations between text segments using 
Na?ve Bayes classifiers trained on a huge corpus. 
They showed that lexical pair information 
extracted from massive amounts of data can have a 
major impact. 
We developed a system which identifies the 
discourse relation between two successive 
sentences in Japanese. On top of the lexical 
information previously proposed, we added phrasal 
pattern information. A phrasal pattern includes at 
least three phrases (bunsetsu segments) from two 
sentences, where function words are mandatory 
and content words are optional. For example, if the 
first sentence is ?X should have done Y? and the 
second sentence is ?A did B?, then we found it 
very likely that the discourse relation is 
CONTRAST (89% in our Japanese corpus). 
2 Discourse Relation Definitions 
There have been many definitions of discourse 
relation, for example (Wolf 2005) and (Ichikawa 
1987) in Japanese. We basically used Ichikawa?s 
classes and categorized 167 cue phrases in the 
ChaSen dictionary (IPADIC, Ver.2.7.0), as shown 
in Table 1. Ambiguous cue phrases were 
categorized into multiple classes. There are 7 
classes, but the OTHER class will be ignored in the 
following experiment, as its frequency is very 
small. 
Table 1. Discourse relations 
Discourse     
relation 
Examples of cue phrase 
(English translation) 
Freq. in  
corpus [%]
ELABORATION and, also, then, moreover 43.0 
CONTRAST although, but, while 32.2 
CAUSE-
EFFECT 
because, and so, thus, 
therefore 12.1 
EQUIVALENCE in fact, alternatively, similarly 6.0 
CHANGE-
TOPIC 
by the way, incidentally, 
and now, meanwhile, well 5.1 
EXAMPLE for example, for instance 1.5 
OTHER most of all, in general 0.2 
 
3 Identification using Lexical Information 
The system has two components; one is to identify 
the discourse relation using lexical information, 
described in this section, and the other is to 
identify it using phrasal patterns, described in the 
next section. 
A pair of words in two consecutive sentences 
can be a clue to identify the discourse relation of 
those sentences. For example, the CONTRAST 
relation may hold between two sentences which 
133
have antonyms, such as ?ideal? and ?reality? in 
Example 1. Also, the EXAMPLE relation may 
hold when the second sentence has hyponyms of a 
word in the first sentence. For example, ?gift shop?, 
?department store?, and ?supermarket? are 
hyponyms of ?store? in Example 2.  
Ex1) 
a. It is ideal that people all over the world 
accept independence and associate on an 
equal footing with each other. 
b. (However,) Reality is not that simple. 
Ex2) 
a. Every town has many stores.  
b. (For example,) Gift shops, department 
stores, and supermarkets are the main 
stores. 
 
In our experiment, we used a corpus from the 
Web (about 20G of text) and 38 years of 
newspapers. We extracted pairs of sentences in 
which an unambiguous discourse cue phrase 
appears at the beginning of the second sentence. 
We extracted about 1,300,000 sentence pairs from 
the Web and about 150,000 pairs from newspapers.  
300 pairs (50 of each discourse relation) were set 
aside as a test corpus. 
3.1 Extracting Word Pairs 
Word pairs are extracted from two sentences; i.e. 
one word from each sentence. In order to reduce 
noise, the words are restricted to common nouns, 
verbal nouns, verbs, and adjectives. Also, the word 
pairs are restricted to particular kinds of POS 
combinations in order to reduce the impact of word 
pairs which are not expected to be useful in 
discourse relation identification. We confined the 
combinations to the pairs involving the same part 
of speech and those between verb and adjective, 
and between verb and verbal noun. 
All of the extracted word pairs are used in base 
form. In addition, each word is annotated with a 
positive or negative label. If a phrase segment 
includes negative words like ?not?, the words in 
the same segment are annotated with a negative 
label. Otherwise, words are annotated with a 
positive label. We don?t consider double negatives. 
In Example 1-b, ?simple? is annotated with a 
negative, as it includes ?not? in the same segment. 
3.2 Score Calculation 
All possible word pairs are extracted from the 
sentence pairs and the frequencies of pairs are 
counted for each discourse relation. For a new 
(test) sentence pair, two types of score are 
calculated for each discourse relation based on all 
of the word pairs found in the two sentences. The 
scores are given by formulas (1) and (2). Here 
Freq(dr, wp) is the frequency of word pair (wp) in 
the discourse relation (dr). Score1 is the fraction of 
the given discourse relation among all the word 
pairs in the sentences. Score2 incorporates an 
adjustment based on the rate (RateDR) of the 
discourse relation in the corpus, i.e. the third 
column in Table 1. The score actually compares 
the ratio of a discourse relation in the particular 
word pairs against the ratio in the entire corpus. It 
helps the low frequency discourse relations get 
better scores.  
 
( )
( )
?
?
=
wpdr
wp
wpdrFreq
wpDRFreq
DRScore
,
1 ),(
,
                 (1) 
 
( )
( )
DR
wpdr
wp
RatewpdrFreq
wpDRFreq
DRScore ?= ?
?
,
2 ),(
,
 (2) 
 
4 Identification using Phrasal Pattern 
We can sometimes identify the discourse relation 
between two sentences from fragments of the two 
sentences. For example, the CONTRAST relation 
is likely to hold between the pair of fragments ?? 
should have done ?.? and ?? did ?.?, and the 
EXAMPLE relation is likely to hold between the 
pair of fragments ?There is?? and ?Those are ? 
and so on.?.  Here ??? represents any sequence of 
words. The above examples indicate that the 
discourse relation between two sentences can be 
recognized using fragments of the sentences even 
if there are no clues based on the sort of content 
words involved in the word pairs.  Accumulating 
such fragments in Japanese, we observe that these 
fragments actually form a phrasal pattern. A phrase 
(bunsetsu) in Japanese is a basic component of 
sentences, and consists of one or more content 
words and zero or more function words. We 
134
specify that a phrasal pattern contain at least three 
subphrases, with at least one from each sentence. 
Each subphrase contains the function words of the 
phrase, and may also include accompanying 
content words. We describe the method to create 
patterns in three steps using an example sentence 
pair (Example 3) which actually has the 
CONTRAST relation. 
Ex3)  
a. ?kanojo-no kokoro-ni donna omoi-ga at-ta-
ka-ha wakara-nai.? (No one knows what 
feeling she had in her mind.) 
b. ?sore-ha totemo yuuki-ga iru koto-dat-ta-
ni-chigai-nai.? (I think that she must have 
needed courage.) 
 
1) Deleting unnecessary phrases 
Noun modifiers using ?no? (a typical particle for a 
noun modifier) are excised from the sentences, as 
they are generally not useful to identify a discourse 
relation. For example, in the compound phrase 
?kanozyo-no (her) kokoro (mind)? in Example 3, 
the first phrase (her), which just modifies a noun 
(mind), is excised. Also, all of the phrases which 
modify excised phrases, and all but the last phrase 
in a conjunctive clause are excised.  
 
2) Restricting phrasal pattern 
In order to avoid meaningless phrases, we restrict 
the phrase participants to components matching the 
following regular expression pattern. Here, noun-x 
means all types of nouns except common nouns, i.e. 
verbal nouns, proper nouns, pronouns, etc. 
 
?(noun-x | verb | adjective)? (particle | auxiliary 
verb | period)+$?, or ?adverb$? 
 
3) Combining phrases and selecting words in a 
phrase 
All possible combinations of phrases including at 
least one phrase from each sentence and at least 
three phrases in total are extracted from a pair of 
sentences in order to build up phrasal patterns. For 
each phrase which satisfies the regular expression 
in 2), the subphrases to be used in phrasal patterns 
are selected based on the following four criteria (A 
to D). In each criterion, a sample of the result 
pattern (using all the phrases in Example 3) is 
expressed in bold face. Note that it is quite difficult 
to translate those patterns into English as many 
function words in Japanese are encoded as a 
position in English. We hope readers understand 
the procedure intuitively. 
 
A) Use all components in each phrase 
kanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai. 
sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai. 
 
B) Remove verbal noun and proper noun 
kanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai. 
sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai. 
 
C) In addition, remove verb and adjective 
kanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai. 
sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai. 
 
D) In addition, remove adverb and remaining noun 
kanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai. 
sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai. 
 
4.1 Score Calculation 
By taking combinations of 3 or more subphrases 
produced as described above, 348 distinct patterns 
can be created for the sentences in Example 3; all 
of them are counted with frequency 1 for the 
CONTRAST relation. Like the score calculation 
using lexical information, we count the frequency 
of patterns for each discourse relation over the 
entire corpus. Patterns appearing more than 1000 
times are not used, as those are found not useful to 
distinguish discourse relations. 
The scores are calculated replacing Freq(dr, 
wp) in formulas (1) and (2) by Freq(dr, pp). Here, 
pp is a phrasal pattern and Freq(dr, pp) is the 
number of times discourse relation dr connects 
sentences for which phrasal pattern pp is matched. 
These scores will be called Score3 and Score4, 
respectively. 
5 Evaluation 
The system identifies one of six discourse relations, 
described in Table 1, for a test sentence pair. Using 
the 300 sentence pairs set aside earlier (50 of each 
discourse relation type), we ran two experiments 
for comparison purposes: one using only lexical 
information, the other using phrasal patterns as 
well. In the experiment using only lexical 
information, the system selects the relation 
maximizing Score2 (this did better than Score1).  In 
the other, the system chooses a relation as follows: 
if one relation maximizes both Score1 and Score2, 
135
choose that relation; else, if one relation maximizes 
both Score3 and Score4, choose that relation; else 
choose the relation maximizing Score2. 
Table 2 shows the result. For all discourse relations, 
the results using phrasal patterns are better or the 
same. When we consider the frequency of 
discourse relations, i.e. 43% for ELABORATION, 
32% for CONTRAST etc., the weighted accuracy 
was 53% using only lexical information, which is 
comparable to the similar experiment by (Marcu 
and Echihabi 2002) of 49.7%. Using phrasal 
patterns, the accuracy improves 12% to 65%. Note 
that the baseline accuracy (by always selecting the 
most frequent relation) is 43%, so the improvement 
is significant. 
Table 2. The result 
Discourse relation Lexical info. Only 
With phrasal 
pattern 
ELABORATION 44% (22/50) 52% (26/50) 
CONTRAST 62% (31/50) 86% (43/50) 
CAUSE-EFFECT 56% (28/50) 56% (28/50) 
EQUIVALENCE 58% (29/50) 58% (29/50) 
CHANGE-TOPIC 66% (33/50) 72% (36/50) 
EXAMPLE 56% (28/50) 60% (30/50) 
Total 57% (171/300) 64% (192/300)
Weighted accuracy 53% 65% 
 
Since they are more frequent in the corpus, 
ELABORATION and CONTRAST are more 
likely to be selected by Score1 or Score3. But 
adjusting the influence of rate bias using Score2 
and Score4, it sometimes identifies the other 
relations.  
The system makes many mistakes, but people 
also may not be able to identify a discourse 
relation just using the two sentences if the cue 
phrase is deleted. We asked three human subjects 
(two of them are not authors of this paper) to do 
the same task. The total (un-weighted) accuracies 
are 63, 54 and 48%, which are about the same or 
even lower than the system performance. Note that 
the subjects are allowed to annotate more than one 
relation (Actually, they did it for 3% to 30% of the 
data). If the correct relation is included among 
their N choices, then 1/N is credited to the accuracy 
count. We measured inter annotator agreements. 
The average of the inter-annotator agreements is 
69%. We also measured the system performance 
on the data where all three subjects identified the 
correct relation, or two of them identified the 
correct relation and so on (Table 3). We can see 
the correlation between the number of subjects 
who answered correctly and the system accuracy. 
In short, we can observe from the result and the 
analyses that the system works as well as a human 
does under the condition that only two sentences 
can be read. 
Table 3. Accuracy for different agreements 
# of  subjects correct 3 2 1 0 
System accuracy 71% 63% 60% 47%
. 
6 Conclusion 
In this paper, we proposed a system which 
identifies discourse relations between two 
successive sentences in Japanese. On top of the 
lexical information previously proposed, we used 
phrasal pattern information. Using phrasal 
information improves accuracy 12%, from 53% to 
65%. The accuracy is comparable to human 
performance. There are many future directions, 
which include 1) applying other machine learning 
methods, 2) analyzing discourse relation 
categorization strategy, and 3) including a longer 
context beyond two sentences. 
Acknowledgements 
This research was partially supported by the 
National Science Foundation under Grant IIS-
00325657. This paper does not necessarily reflect 
the position of the U.S. Government. We would 
like to thank Prof. Ralph Grishman, New York 
University, who provided useful suggestions and 
discussions. 
References 
Daniel Marcu and Abdessamad Echihabi. 2002. An 
Unsupervised Approach to Recognizing Discourse 
Relations, Proceedings of the 40th Annual Meeting of 
the Association for Computational Linguistics, 368-
375. 
Florian Wolf and Edward Gibson. 2005. Representing 
Discourse Coherence: A Corpus-Based Study, 
Computational Linguistics, 31(2):249-287. 
Takashi Ichikawa. 1978. Syntactic Overview for 
Japanese Education, Kyo-iku publishing, 65-67 (in 
Japanese). 
136
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 713?720,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Clustered Global Phrase Reordering Model
for Statistical Machine Translation
Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Souraku-gun
Kyoto, 619-0237 Japan
nagata.masaaki@labs.ntt.co.jp,
Kuniko Saito
NTT Cyber Space Laboratories
1-1 Hikarinooka, Yokoshuka-shi
Kanagawa, 239-0847 Japan
saito.kuniko@labs.ntt.co.jp
Kazuhide Yamamoto, Kazuteru Ohashi  
Nagaoka University of Technology
1603-1, Kamitomioka, Nagaoka City
Niigata, 940-2188 Japan
ykaz@nlp.nagaokaut.ac.jp, ohashi@nlp.nagaokaut.ac.jp
Abstract
In this paper, we present a novel global re-
ordering model that can be incorporated
into standard phrase-based statistical ma-
chine translation. Unlike previous local
reordering models that emphasize the re-
ordering of adjacent phrase pairs (Till-
mann and Zhang, 2005), our model ex-
plicitly models the reordering of long dis-
tances by directly estimating the parame-
ters from the phrase alignments of bilin-
gual training sentences. In principle, the
global phrase reordering model is condi-
tioned on the source and target phrases
that are currently being translated, and
the previously translated source and tar-
get phrases. To cope with sparseness, we
use N-best phrase alignments and bilin-
gual phrase clustering, and investigate a
variety of combinations of conditioning
factors. Through experiments, we show,
that the global reordering model signifi-
cantly improves the translation accuracy
of a standard Japanese-English translation
task.
1 Introduction
Global reordering is essential to the translation of
languages with different word orders. Ideally, a
model should allow the reordering of any distance,
because if we are to translate from Japanese to En-
glish, the verb in the Japanese sentence must be
moved from the end of the sentence to the begin-
ning just after the subject in the English sentence.

Graduated in March 2006
Standard phrase-based translation systems use
a word distance-based reordering model in which
non-monotonic phrase alignment is penalized
based on the word distance between successively
translated source phrases without considering the
orientation of the phrase alignment or the identi-
ties of the source and target phrases (Koehn et al,
2003; Och and Ney, 2004). (Tillmann and Zhang,
2005) introduced the notion of a block (a pair of
source and target phrases that are translations of
each other), and proposed the block orientation
bigram in which the local reordering of adjacent
blocks are expressed as a three-valued orienta-
tion, namely Right (monotone), Left (swapped),
or Neutral. A block with neutral orientation is sup-
posed to be less strongly linked to its predecessor
block: thus in their model, the global reordering is
not explicitly modeled.
In this paper, we present a global reordering
model that explicitly models long distance re-
ordering1. It predicts four type of reordering
patterns, namely MA (monotone adjacent), MG
(monotone gap), RA (reverse adjacent), and RG
(reverse gap). There are based on the identities of
the source and target phrases currently being trans-
lated, and the previously translated source and tar-
get phrases. The parameters of the reordering
model are estimated from the phrase alignments of
training bilingual sentences. To cope with sparse-
ness, we use N-best phrase alignments and bilin-
gual phrase clustering.
In the following sections, we first describe the
global phrase reordering model and its param-
1It might be misleading to call our reordering model
?global? since it is at most considers two phrases. A truly
global reordering model would take the entire sentence struc-
ture into account.
713
eter estimation method including N-best phrase
alignments and bilingual phrase clustering. Next,
through an experiment, we show that the global
phrase reordering model significantly improves
the translation accuracy of the IWSLT-2005
Japanese-English translation task (Eck and Hori,
2005).
2 Baseline Translation Model
In statistical machine translation, the translation of
a source (foreign) sentence   is formulated as the
search for a target (English) sentence  that max-
imizes the conditional probability  
 	
, which
can be rewritten using the Bayes rule as,






 	

Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 11?18,
Beijing, August 2010
Using Goi-Taikei as an Upper Ontology to Build a Large-Scale Japanese
Ontology from Wikipedia
Masaaki Nagata
NTT Communication Science
Laboratories
nagata.masaaki@labs.ntt.co.jp
Yumi Shibaki and Kazuhide Yamamoto
Nagaoka University of
Technology
{shibaki,yamamoto}@jnlp.org
Abstract
We present a novel method for build-
ing a large-scale Japanese ontology from
Wikipedia using one of the largest
Japanese thesauri, Nihongo Goi-Taikei
(referred to hereafter as ?Goi-Taikei?) as
an upper ontology. First, The leaf cat-
egories in the Goi-Taikei hierarchy are
semi-automatically aligned with seman-
tically equivalent Wikipedia categories.
Then, their subcategories are created au-
tomatically by detecting is-a links in the
Wikipedia category network below the
junction using the knowledge defined in
Goi-Taikei above the junction. The re-
sulting ontology has a well-defined taxon-
omy in the upper level and a fine-grained
taxonomy in the lower level with a large
number of up-to-date instances. A sam-
ple evaluation shows that the precisions of
the extracted categories and instances are
92.8% and 98.6%, respectively.
1 Introduction
In recent years, we have become increasingly
aware of the need for up-to-date knowledge bases
offering broad-coverage in order to implement
practical semantic inference engines for advanced
applications such as question answering, summa-
rization and textual entailment recognition. One
promising approach involves automatically ex-
tracting a large comprehensive ontology from
Wikipedia, a freely available online encyclopedia
with a wide variety of information. One problem
with previous such efforts is that the resulting on-
tology is either fragmentary or trivial.
Ponzetto and Strube (2007) presents a set of
lightweight heuristics such as head matching and
modifier matching for distinguishing between is-
a and not-is-a links in the Wikipedia category
network. The most powerful heuristics is head
matching in which a category link is labeled as
is-a if the two categories share the same head
lemma, such as CAPITALS IN ASIA and CAPI-
TALS. For Japanese, Sakurai et al (2008) present
a method equivalent to head matching in Japanese.
As Japanese is a head final language, they intro-
duced a heuristics called suffix matching in which
a category link is labeled as is-a if one category
is the suffix of the other category, such as  
 (airports in Japan) and  (airports). The
problem with the ontology extracted by these two
methods is that it is not a single interconnected
taxonomy, but a set of taxonomic trees.
One way to make a single taxonomy is to use
an existing large-scale taxonomy as a core for the
resulting ontology. In YAGO, Suchanek et al
(2007) merged English WordNet and Wikipedia
by adding instances (namely Wikipedia articles)
to the is-a hierarchy of WordNet. Of the cate-
gories assigned to a Wikipedia article, they re-
garded one with a plural head noun as the article?s
hypernym, which is called a conceptual category.
They then linked the conceptual category to a
WordNet synset by heuristic rules including head
matching. For Japanese, Kobayashi et al (2008)
present an attempt equivalent to YAGO, where
they merged Goi-Taikei and Japanese Wikipedia.
The problem with these two methods is that the
core taxonomy is extended only one level al-
though many new instances are added. They can-
not make the most of the fine-grained taxonomic
11
information contained in the Wikipedia category
network.
In this paper, we present a novel method for
building a single interconnected ontology from
Wikipedia, with a fine-grained taxonomy in the
lower level, by using a manually constructed the-
saurus as its upper ontology. In the following
sections, we first describe the language resources
used in this work. We then describe a semi-
automatic method for building the ontology and
report our experimental results.
2 Language Resources
2.1 Nihongo Goi-Taikei
Nihongo Goi-Taikei (     , ?compre-
hensive outline of Japanese vocabulary?)1 is one
of the largest and best known Japanese thesauri
(Ikehara et al, 1997). It was originally developed
as a dictionary for a Japanese-to-English machine
translation system in the early 90?s. It was then
published as a book in 5 volumes in 1997 and as
a CD-ROM in 1999. It contains about 300,000
Japanese words and the meanings of each word
are described by using 2,715 hierarchical seman-
tic categories. Each word has up to 5 semantic
categories in order of frequency in use, and each
category is assigned with a unique ID number and
category name such as 4:person and 388:place2.
Goi-Taikei has different semantic category hi-
erarchies for common nouns, proper nouns, and
verbs, respectively. We used only the common
noun category in this work. For simplicity, we
mapped all proper nouns in the proper noun cate-
gory to the equivalent common noun category us-
ing the category mapping table shown in the Goi-
Taikei book.
Figure 1 shows the top three layers for common
nouns3. For example, the transliterated Japanese
word raita ( 
	 ) has two semantic cate-
gories 353:author and 915:household appli-
ance. The former originates with the English
1Referred to as ?Goi-Taikei? unless otherwise noted.
2We use Sans Serif for the Goi-Taikei category and
SMALL CAPS for the Wikipedia category. The Goi-Taikei
category is prefixed with ID number.
3The maximum depth of the common noun hierarchy is
12. Most links are is-a relations, but some are part-of rela-
tions, which are explicitly marked
word ?writer? while the latter originates with En-
glish word ?lighter?. By climbing up the Goi-
Taikei category hierarchy, we can infer that the
former refers to a human being (4:person) while
the latter refers to a physical object (533:con-
crete object).
2.2 Japanese Wikipedia
Wikipedia is a free, multilingual, on-line ency-
clopedia actively developed by a large number of
volunteers. Japanese Wikipedia now has about
500,000 articles. Figure 2 shows examples of an
article page and a category page. An article page
has a title, body, and categories. In most articles,
the first sentence of the body gives the definition
of the title. A category also has a title, body, and
categories. Its title is prefixed with ?Category:?
and its body includes a list of articles that belong
to the category.
Although the Wikipedia category system is or-
ganized in a hierarchal manner, it is not a tax-
onomy but a thematic classification. An article
could belong to many categories and the category
network has loops. The relations between linked
categories are chaotic, but the lower the category
link is in the hierarchy, the more it is likely to be
an is-a relation. For example, the category link
between  (COCKTAIL) and  (ALCO-
HOLIC BEVERAGE) is an is-a relation. Although
the article 	 (shaker) is in the category
Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 18?27,
Beijing, August 2010
Textual Entailment Recognition using Word Overlap, 
Mutual Information and Subpath Set 
 Yuki Muramatsu 
Nagaoka University of 
Technology 
muramatsu@jnlp.org 
Kunihiro Udaka 
Nagaoka University of 
Technology 
udaka@jnlp.org 
Kazuhide Yamamoto 
Nagaoka University of 
Technology 
yamamoto@jnlp.org 
 
 
 
Abstract 
When two texts have an inclusion 
relation, the relationship between them is 
called entailment. The task of 
mechanically distinguishing such a 
relation is called recognising textual 
entailment (RTE), which is basically a 
kind of semantic analysis. A variety of 
methods have been proposed for RTE. 
However, when the previous methods 
were combined, the performances were 
not clear. So, we utilized each method as 
a feature of machine learning, in order to 
combine methods. We have dealt with 
the binary classification problem of two 
texts exhibiting inclusion, and proposed 
a method that uses machine learning to 
judge whether the two texts present the 
same content. We have built a program 
capable to perform entailment judgment 
on the basis of word overlap, i.e. the 
matching rate of the words in the two 
texts, mutual information, and similarity 
of the respective syntax trees (Subpath 
Set). Word overlap was calclated by 
utilizing BiLingual Evaluation 
Understudy (BLEU). Mutual information 
is based on co-occurrence frequency, and 
the Subpath Set was determined by using 
the Japanise WordNet. A Confidence-
Weighted Score of 68.6% was obtained 
in the mutual information experiment on 
RTE. Mutual information and the use of 
three methods of SVM were shown to be 
effective.  
1 Introduction 
This paper can help solve textual entailment 
problems. Researchers of natural language 
processing have recently become interested in 
the automatic recognition of textual entailment 
(RTE), which is the task of mechanically 
distinguishing an inclusion relation. Text 
implication recognition is the task of taking a 
text (T) and a hypothesis (H), and judging 
whether one (the text) can be inferred from the 
other (hypothesis). Here below is an example 
task. In case of entailment, we call the relation to 
be ?true?). 
 
Example 1: Textual entailment recognition.  
T: Google files for its long-awaited IPO.  
H: Google goes public.  
Entailment Judgment: True. 
 
For such a task, large applications such as 
question answering, information extraction, 
summarization and machine translation are 
involved. A large-scale evaluation workshop has 
been conducted to stimulate research on 
recognition of entailment (Dagan et al, 2005). 
These authors divided the RTE methods into six 
methods. We focused on 3 methods of them. 
P?rez and Alfonseca?s method (P?rez and 
Alfonseca, 2005) used Word Overlap. This 
method is assumed to have taken place when 
words or sentences of the text and the hypothesis 
are similar, hence the relation should be true. 
P?rez and Alfonseca used  the BLEU algorithm 
to calculate the entailment relationship. 
Glickman et als. method was considered as 
using statistical lexical relations. These authors 
assumed that the possibility of entailment were 
high when the co-occurrence frequency of the 
word in the source and the target were high. 
18
While this may be correct, we believe 
nevertheless that it problematic not to consider 
the co-occurrence of the hypothesis words. This 
being so, we proposed to use mutual information. 
Finally, Herrera et als. method is based on 
Syntactic matching. They calculated the degree 
of similarity of the syntax tree.  
We combined these three methods using 
machine learning techniques. 
2 Related Works 
Dagan et al (Dagan et al 2005) conducted 
research in 2005 on how to evaluate data of 
RTE; the authors insisted on the need of 
semantic analysis. As a first step, they 
considered the problem of textual entailment, 
proposing how to build evaluation data. Theu 
also organised and workshop on this topic. Their 
evaluation data are problems of binary 
classification of the texts to be compared. They 
used a sentence extracted from a newspaper 
corpus, and built a hypothesis from this text 
using one of seven methods: question answering, 
sentence comprehension, information extraction, 
machine translation, paraphrasing, information 
retrieval and comparable documents. They 
proposed a method of evaluation using RTE, and 
they introduced several RTE methods. 
Odani et al (Odani et al 2005) did research on 
the construction of evaluation data in Japan, 
mentionning that there was a problem in the 
evaluation data of Dagan et al For example, 
they stated that ?The evaluation data that he 
constructed are acting some factors. So it is 
difficult to discuss the problem?. Next, they did 
an RTE evaluation data using Japanese. The 
inference factors for judging entailment 
judgment were divided into five categories: 
inclusion, lexicon (words that can?t be declined), 
lexicon (declinable words), syntax and inference. 
The subclassification was set for each 
classification, and Japanese RTE evaluation data 
was constructed. In addition, a dictionary and 
Web text were used for the entailment judgment. 
The authors were able to solve entailment 
judgment with words or phrases containing 
synonyms and/or a super-sub type relation. 
However, this classification lacks precision.  
For example, they defined the term ?lexicon 
(words that cannot be declined)? as ?The 
meaning and the character of the noun that exists 
in text are data from which information on the 
truth of hypothesis is given?. Given this lack of 
clarity, we considered this method to be difficult 
to reproduce.  
However, the evaluation data they built is 
general and available for public use. Regarding 
the research using the evaluation data of such 
RTE, there have been many reports in the 
workshop.  
For example, P?rez and Alfonseca (P?rez and 
Alfonseca, 2005) assumed that the possibility of 
entailment was high when the text matched the 
hypothesis. The concordance rate of the text and 
the hypothesis was then calculated for judging 
the text and the hypothesis of the inclusion 
relation. In their research, they used BiLingual 
Evaluation Understudy (BLEU) to evaluate 
machine translation. An entailment judgment of 
?true? was given when the BLEU score was 
above than a given threshold decided in advance. 
The evaluation data of Dagan et al was used in 
the experiment, and its accuracy was about 50%. 
The evaluation data of comparable document 
types were the results with the highest accuracy. 
Hence the authors concluded that this method 
can be considered as a baseline of RTE. We 
dealt with it as word overlap. 
Glickman et al (Glickman et al 2005) 
conducted research using co-occurring words. 
They assumed that the entailment judgment was 
?true? when the probability of co-occurrence 
between the text and the hypothesis was high. In 
addition, the content word of the text with the 
highest co-occurrence probability was calculated 
from the content word of all of the hypotheses, 
and it was proposed as a method for entailment 
judgment. A Web search engine was used to 
calculate in the co-occurrence probability. This 
experiment yielded an accuracy of 
approximately 58%, while the evaluation data of 
comparable document types was about 83%. 
This being so, the authors concluded that they 
have been able to improve the results with the 
help of other deep analytical tools. We improved 
this method, and used it as mutual information. 
Herrera et al (Herrera et al, 2005) focused on 
syntactic similarity. They assumed that the 
entailment judgment was ?true? when the 
syntactic similarity of the text and the hypothesis 
was high. In addition, they used WordNet for 
considering identifiable expressions. The results 
19
of the experiment yielded an accuracy of 
approximately 57%. We improved this method, 
and used it then as subpath set. 
Prodromos Malakasiotis and Ion 
Androutsopoulos (Prodromos Malakasiotis and 
Ion Androutsopoulos, 2007) used Support 
Vector Machines. They assumed that the 
entailment judgment was ?true? when the 
similarity of words, POS tags and chunk tags 
were high. The results of the experiment yielded 
an accuracy of approximately 62%. However, 
they forgot to combine past RTE methods as 
feature of SVM. 
The authors of this paper present a new RTE 
method. We propose to combine word overlap, 
mutual information and subpath sets. We dealt 
with SVM by using 3 methods equally as 
features, and we estimated higher precision than 
when using individual; independent methods. 
3 Textual Entailment Evaluation Data 
We used the textual entailment evaluation data 
of Odani et al for the problem of RTE. This 
evaluation data is generally available to the 
public at the Kyoto University1.  
The evaluation data comprises the inference 
factor, subclassification, entailment judgment, 
text and hypothesis. Table 1 gives an example. 
The inference factor is divided into five 
categories according to the definition provided 
by Odani et al: inclusion, lexicon (indeclinable 
word), lexicon (declinable word), syntax and 
inference. They define the classification 
viewpoint of each inference factor as follows: 
 
Example 2: Classification criteria of inference 
factors 
? Inclusion: The text almost includes the 
hypothesis. 
 
Table 1: RTE Evaluation data of Odani et al 
 
 
                                                 
1 http://www.nlp.kuee.kyoto-u.ac.jp/nl-resource 
? Lexicon (Indeclinable Word): Information of 
the hypothesis is given by the meaning or the 
behaviour of the noun in the text.  
? Lexicon (Declinable Word): Information of 
the hypothesis is given by the meaning or the 
behaviour of the declinable word in the text. 
? Syntax: The text and the hypothesis have a 
relation of syntactic change.  
?Inference: Logical form. 
 
They divided the data into 166 subclasses,  
according to each inference factor. The 
entailment judgment is a reliable answer in the 
text and the hypothesis. It is a difficult problem 
to entailment judgment for the criteria answer. 
Therefore, when they reported on the RTE 
workshop, they assumed the following 
classification criteria: 
 
Example 3: Classification criteria of entailment 
determination. 
??(Talw): When the text is true, the hypothesis 
is always true. 
??(Talm): When the text is true, the hypothesis 
is almost true. 
??(Fmay): When the text is true, the hypothesis 
may be true. 
??(Falw): When the text is true, the hypothesis 
is false. 
 
In terms of the text and the hypothesis, when 
we observed the evaluation data, the evaluation 
data accounted for almost every sentence in both 
the texts and the hypotheses, and also the 
hypotheses were shorter than the texts. 
 There is a bias in the number of problems 
evaluated by the inference factor and by the 
subclassification. The number of evaluation data 
open to the public now stands at 2471. 
 
 
 
 
 
 
 
 
 
Inference Factor Sub- 
Classification 
Entailment 
Judgment 
Text Hypothesis 
Lexicon 
(Indeclinable Word) 
Behavior ? Toyota opened 
a luxury car  shop. 
Lexus is 
a luxury car. 
20
4 Proposal Method 
Up now, a number of methods have been 
proposed for RTE. However, when the previous 
methods were combined, the performances were 
hard to judge. Hence, we used each method as a 
feature of machine learning, and combined them 
then. 
The input text and the hypothesis were 
considered as a problem of binary classification 
(?true? or ?false?). Therefore, we employed 
support vector machines (Vapnik, 1998), which 
are often used to address binary classification 
problems (in fact, we implemented our system 
with Tiny SVM). With this method we achieved 
higher precision than with individual 
independent methods.  
Figure 1 shows our proposed method.  
 
 
 
Figure 1: Our Proposed Method 
 
In the following sections, we will describe the 
three features used in machine learning.  
4.1 Word Overlap 
It is assumed that when words or sentences of 
the text and the hypothesis are similar, the 
relation should be true. P?rez and Alfonseca 
used a BLEU algorithm to calculate the 
entailment between the text and the hypothesis. 
BLEU is often used to evaluate the quality of 
machine translation. Panieni et al provided the 
following definition of BLEU. In particular, the 
BLEU score between length r of the sentence B 
and length c of the sentence A is given by the 
formulas (1) and (2): 
( )
1
( , ) exp( log( ) / ) (1)
1 {1, / } (2)
n
i
i
Bleu A B BP p n
BP exp max r c
=
=
= ?
?
  
 
where pi represents the matching rate of n-gram. 
The n-gram of this method was calculated as 
word n-gram. We assumed n = 1 and used the 
public domain program NTCIR7 2 . Here is an 
example of the calculation. 
  
Example 4: Calculation by BLEU. 
T:??????????? (The moon is 
Earth's satellite.) 
H:??????????? (The moon is 
around the Earth.) 
BLEU:0.75 
 
We estimated n = 1 for the following reasons: 
 
1. The reliability of word overlap is not high 
when n is large. 
2. The calculated result of BLEU often 
becomes 0 when n is large. 
 
First, we will explain the reason 1 mentioned 
above. The report of Kasahara et al (Kasahara et 
al., 2010) is a reproduction of the one provided 
by P?rez et al(P?rez et al, 2005). They prepared 
an original RTE evaluation set of reading 
comprehension type, and proposed a new RTE 
system using a BLEU algorithm. When they 
experimented by increasing the maximum 
number of elements n of word n-gram from 1 to 
4, the optimum maximum number of elements n 
is 3. They proposed the following analysis: if the 
hypothesis is shorter than the text, with n = 4, 
then the frequency is low in word 4-gram. 
However, the accidental coincidence of the word 
4-gram significantly affected BLEU. When n is 
large, the reliability of the word overlap 
decreases. 
Next, as an explanation of reason 2, when the 
length of the targeted sentence is short, the 
numerical result of BLEU sometimes becomes 0. 
For example, the number of agreements of 4-
gram becomes 0 when calculating with n = 4, 
and the BLEU value sometimes becomes 0. 
                                                 
2 http://www.nlp.mibel.cs.tsukuba.ac.jp/bleu_kit/ 
Word 
Overlap 
Subpath 
Set 
SVM 
True 
False 
T:xxxxx       
H:yyyyy 
Mutual 
Information 
Evaluation Data 
Score Calculation 
Resource 
Processing 
 
21
Such calculations accounted for approximately 
69% of the Odani et al evaluation set. 
4.2 Mutual Information 
Glickman et al (Glickman et al 2005) assumed 
that the possibility of entailment is high when 
the co-occurrence frequency of the word in the 
text and the hypothesis is high. Therefore, they 
proposed a method of total multiplication, by 
searching for the word with the highest co-
occurrence frequency from all the words of the 
hypothesis, as shown in formulas (3) and (4): 
 
,
( 1| ) max ( , ) (3)
( , ) (4)
u h V t
u v
v
P Trh t lep u v
n
lep u v
n
? ?= = ?
?
?
?
 
 
P(Trh=1|t) expresses the probability of 
entailment between the text and the hypothesis. 
In these formulas, u is the content word of the 
hypothesis (noun, verb, adjective or unknown 
word); v the content word of the text; n 
represents the number of Web search hits; nu, v is 
the number of hits when the words u and v are 
searched on the Web. But, when the content 
word of the text is low frequency, the numerical 
result of the lep(u, v) increases for P(Trh=1|t). 
We believe that it was a problem not to take into 
account the co-occurrence of the hypothesis 
words. In addition, their method to handle long 
sentences and reaching the conclusion ?false? is 
problematic. This is why, we considered Rodney 
et als. method (Rodney et al 2006) and 
proposed the use of mutual information, which is 
calculates on the basis of the formulas (5) and 
(6):  
,
1
( 1| ) max ( , ) (5)
( )
( , ) log (6)
( ) ( )
u h V t
u v
u v
P Trh t lep u v
p n
lep u v
p n p n
? ?= = ?
? ?
?
u
 
 
u is the number of the content words of the 
hypothesis. Hence, 1/u averages product of max 
lep(u,v). This being so we considered that this 
model can do entailment judgments 
independantly of the length of the hypothesis. 
It searches for the word of the text considering 
that the mutual information reaches the 
maximum value from each of the hypothesis 
words. When P(Trh=1|t) is higher than an 
arbitrary threshold value, it is judged to be 
? true? , and ?false? in the opposite case. 
Glickman assumed the co-occurrence frequency 
to be the number of Web-search hits. However, 
we estimated that the reliability of the co-
occurrence frequency was low, because the co-
occurrence of the Web search engine was a wide 
window. This is why, we used the Japanese Web 
N-gram3. In particular, we used 7-gram data, and 
calculated the co-occurrence frequency nu, v, 
frequency nu and nv of the word. p(ni) was 
calculated by (?) the frequency ni divided the 
number of all words. Japanese Web N-gram was 
made from 20,036,793,177 sentences, including 
255,198,240,937 words. The unique number of 
7-gram is 570,204,252. 
To perform morphological analysis, we used 
Mecab4, for example: 
 
Example 5: Calculation by mutual information. 
T:????????????????(The 
air conditioner works in this room.) 
H:????(It is cool.) 
Mutual Information:10.0 
 
( )
( ) ( )
,
1
( 1| ) max ( , )
1
( , )
u V tP Trh t lep u v
p n
lep u v
p n p n
? ?= = ?
= ?
?
???????(cool,the air conditioner)
???(cool) ????(the air conditioner)
?(7)
-log 10.0 (8)
 
 
This method actually standardises the result by 
dividing by the maximum value of lep(u, v). As 
a result, p reaches the value 1 from 0. We used 
the discounting for nu, nv,and nu, v,, because a 
zero-frequency problem had occurred when 
calculating the frequency. There are some 
methods for discounting. We used the additive 
method reported by Church and Gale (Church 
and Gale, 1991). They compared some 
discounting methods by using the newspaper 
corpus. The addition method is shown as follows.  
 
( ) 1
( ) (9)
C wP w
" V
+=
+
 
 
                                                 
3 http://www.gsk.or.jp/catalog/GSK-2007-C/ 
4 http://mecab.sourceforge.net/ 
22
The additive method assumed N to be the 
number of all words in a corpus. C(w) is the 
frequency of word w in the corpus. V is a 
constant to adjust the total of the appearance 
probability to 1. It is equal to the unique number 
of words w. The additive method is very simple, 
it adds a constant value to occurrence count 
C(w). The method of adding 1 to the occurrence 
count is called Laplace method also. 
4.3 Subpath Set 
Herrera et al (Herrera et al, 2005) parsed the 
hypothesis and the text, and they calculated the 
degree of similarity of the syntax tree from both. 
Our method also deals with the degree of 
similarity of the syntax tree. The tree kernel 
method of Collins and Duffy (M. Collins and 
N. Duffy, 2002) shows the degree of similarity 
of the syntax tree; however, it requires much 
time to calculate the degree of similarity. 
Therefore, we employed the subpath set of 
Ichikawa et al This latter calculates partial 
routes from the root to the leaf of the syntax tree. 
Our method assumes the node to be a content 
word (noun, verb, adjective or unknown word) 
in the syntax tree, while the branch is a 
dependency relation. For parsing we relied on 
Cabocha5 . 
The frequency vector was assumed to comprise 
a number of partial routes, similar to the 
approach of Ichikawa et al (Ichikawa et al, 
2005). The number of partial routes is unique. 
However, even if the same expression is shown 
for the word with a different surface form, it is 
not possible to recognise it as the same node. 
Therefore, we used the Japanese version of 
WordNet (Bond et al, 2009), in which a word 
with a different surface can be treated as the 
same expression, because Japanese WordNet 
contains synonyms. The same expressions of our 
method were hypernym words, hyponym words 
and synonym words in Japanese Word Net, 
because RTE sometimes considered the 
hierarchical dictionary of the hypernym and the 
hyponym word to be the same expression. 
However, our hypernym and hyponym words 
were assumed to be a parent and a child node of 
the object word, as shown in Figure 3. 
                                                 
5 http://chasen.org/~taku/software/cabocha/ 
Example 6: Calculation by subpath set. 
T:???????????????? 2 ?
??? 
(T:The point adheres by the twice because it is 
campaigning.) 
H:??????????????????
?? 2???? 
(H:The point adheres usually by the twice 
because it is campaigning.) 
Subpath:0.86 
 
 
Figure 2: Partial route chart of subpath set. 
 
The number of partial routes is 7, and 6 partial 
routes overlap in T and H. So, the subpath is 
0.86 (6/7).  
5 Evaluation 
The textual entailment evaluation data of Odani 
et al, described in Section 3, was used in the 
experiment. The entailment judgment of four 
values is manually given to the textual 
entailment evaluation data. In our experiment we 
considered ?Talw? and ?Talm? to be ?true? and 
? Fmay?  and ? Falw?  as ? false? . The 
evaluation method used was a Confidence-
Weight Score (CWS, also known as Average 
Precision), proposed by Dagan et al. As for the 
closed test, the threshold value with the 
maximum CWS was used. 
 
1
1
/
1
( )
1
( )
k
i All
i
i k
Accuracy Correct All
CWS r precision k
k
precision k r
k
? ?
? ?
=
?
?
(10)
= ? (11)
= (12)
 
All = Number of all evaluation data. Correct = 
Number of correct answer data. If k is a correct 
answer, rk = 1. If k is an incorrect answer, rk = 0. 
23
When the Entailment judgment annotated in 
evaluation data matches with the Entailment 
judgment of our method, the answer is true. 
 The threshold of the Closed test was set 
beforehand (0?th?1). When it was above the 
threshold, it was judged ?true?. When it was 
higher than the threshold, it was judged ?false?. 
SVM was used to calculate the value of three 
methods (word overlap, mutual information and 
subpath set) as the features for learning data, was 
experimented.  
Open test was experimented 10-fold cross-
validations. 9 of the data divided into 10 were 
utilized as the learning data. Remaining 1 was 
used as an evaluation data. It looked for the 
threshold that CWS becomes the maximum from 
among the learning data. It experimented on the 
threshold for which it searched by the learning 
data to the evaluation data. It repeats until all 
data that divides this becomes an evaluation data, 
averaged out. (Or we experimented Leave-one-
out cross validation.) 
Using the SVM, experiments were conducted 
on the numerical results of Sections 4.1 to 4.3 as 
the features. 
 The textual entailment evaluation data 
numbered 2472: ?Talw?: 924, ?Talm?: 662, ?Fmay?: 
262 and ?Falw?: 624, and there were 4356 words. 
The total number of words was 43421. Tables 2 
and 3 show the results of the experiment, which 
focused respectively on the closed and open 
tests,. When the ?true? textual entailment 
evaluation data ?Talw? only and ?Talw and Talm? 
was used, mutual information achieved the best 
performance. When the true data ?Talm? only was 
used, SVM achieved the best performance.  
 
 
Table 2: Results of the RTE experiments 
6 Discussion 
In this section, we discuss the relation between 
each 3 method value assumed to be the criterion 
of judgment and CWS in the closed test. When 
the ?true? evaluation data was assumed to be 
?Talm? only in the open test, the result of SVM 
exceeded the results of the closed test. We then 
consider the relation between SVM and CWS.  
6.1 Close Test of Ward Overlap 
We believe that the results of the experiments of 
word overlap were more effective than other 
methods, because they achieved the best 
performance excluding ?Talm? and ?Talw and Talm? 
in 3 methods. Figure 3 shows the relation to 
CWS when BLEU value changes.  
 
0
10
20
30
40
50
60
70
80
90
100
0 0.2 0.4 0.6 0.8 1
BLEU
CW
S[
%] Talw
Tmay
Talw and Tmay
 
Figure 3: Results of the closed test of the 
RTE experiments by word overlap. 
 
The tendency shown in Figure 3 did not change 
much when the relation between the threshold 
value and CWS was observed, even though the 
?true? evaluation data was changed. 
 
 
 
 
 
 CWS 
Closed Test Open Test   
Talw Talm Talw and Talm Talw Talm Talw and Talm 
Word Overlap 53.0% 57.9% 62.1% 39.0% 60.2% 59.3% 
Mutual Informaition 55.9% 52.9% 68.6% 53.4% 55.6% 67.4% 
Subpath Set 54.5% 57.0% 61.8% 45.0% 59.7% 61.1% 
SVM 51.4% 61.2% 63.5% 49.9% 61.9% 64.1% 
24
However, the entailment judgment of the word 
overlap method becomes nearly ?false? when the 
BLEU value is 1 (or ?true? when BLEU score is 
0.) Table 3 shows the entailment judgment when 
the BLEU value is 0 or 1.  
We assumed that BLEU value that CWS 
becomes the maximum depends on the ratio of 
number of T and F in the evaluation set. 
However, when true condition is ?Talw? only, T 
is more than F (T:924,F:886). And when true 
condition is ?Talm? only, F is more than T 
(T:662,F:886). For this reason, The possibility of 
our assumption is low because both true 
conditions are BLEU value that CWS becomes 
the maximum is 1. 
6.2 Close Test of Mutual Information 
We believe that the results of the experiments of 
mutual information were more effective than 
other methods, because they achieved the best 
performance excluding ?Talm? in 3 methods. 
Figure 4 shows the relation to CWS when 
mutual information value changes.  
The tendency shown in Figure 4 did not change 
much when the relation between mutual 
information value and CWS was observed, even 
though the ?true? evaluation data was changed. 
When mutual information values are from 0.2 
(or 0.3) to 1, CWS increased. However, the 
entailment judgment of the mutual information 
method becomes almost ?true? when mutual 
information score is near 1 (or ?false? when 
mutual information score value is near 0.)  
 
 
Table 3: Entailment judgment in closed test 
of word overlap (T=True, F=False). 
 
Table 4: Entailment judgment in closed test 
of mutual information (T=True, F=False, 
MI=mutual information). 
0
10
20
30
40
50
60
70
80
90
100
0 0.2 0.4 0.6 0.8 1
Mutual Information
CW
S[
%] Talw
Talm
Talw and Talm
 
Figure 4: Results of the closed test of the RTE 
experiments by mutual information. 
 
Table 4 shows the entailment judgment when the 
mutual information value is near 0 or 1. Our 
results showed most entailment judgment results 
to be almost ?true? (or almost ?false?) for the 
optimal threshold value in the evaluation data. 
Therefore, we considered that the method of 
RTE using mutual information should be 
reviewed.  
6.3  Close Test of Subpath Set 
We believe that the results of the experiments of 
subpath set were not better than other methods. 
Figure 5 shows the relation to CWS when 
subpath set (SS) value changes.  
The tendency shown in Figure 5 changed much 
when the relation between the threshold value 
and CWS was observed, even though the ?true? 
evaluation data was changed. When the true 
conditions are ?Talw? and ?Talm?, the tendencies 
were very near. 
 
 
 
 
 
 
 
  Answer/System T/T T/F F/T F/F CWS 
Talw               (Bleu=1) 5 919 12 874 53.0 
Talm               (Bleu=1) 0 662 12 874 57.9 True Condition 
Talw and Talm (Bleu=0) 1586 0 886 0 62.1 
  Answer/System T/T T/F F/T F/F CWS 
Talw               (MI=0.72) 924 0 884 2 55.9 
Talm               (MI=0) 0 2 662 884 52.9 
True 
Condition 
Talw and Talm (MI=0.68) 1586 0 884 2 68.6 
25
However, when the true conditions were ?Talw? 
and ?Talw and Talm?, the tendencies were different. 
The tendency of ?Talw? was rising. The tendency 
of ?Talw and Talm? was dropping until the subpath 
set value was 0.2. The entailment judgment of 
the mutual information method becomes almost 
?true? when subpath set value was near 1)  
 
0
10
20
30
40
50
60
70
80
90
100
0 0.2 0.4 0.6 0.8 1
Subpath Set
CW
S[
%] Talw
Talm
Talw and Talm
 
Figure 5: Results of the closed test of the RTE 
experiments by subpath set.  
 
Table 5 shows the entailment judgment when the 
threshold value is near 0 or 1. Our results 
showed most entailment judgment results to be 
almost ?true? (or almost ?false?) for the optimal 
subpath set value in the evaluation data.  
6.4 Open Test of SVM 
The open tests were conducted in 10-fold cross-
validation , and the experimental result is their 
average. Figure 6 shows the related chart 10-fold 
cross-validation.  
When the true data were assumed to be ?Talm? 
only, the maximum value of CWS was 70.3%. 
As a result, the result of 10?fold cross validation 
exceeded the closed test. 
 
 
 
 
Table 5: Entailment judgment in closed test of 
subpath set (T=True, F=False, SS=subpath set). 
0
10
20
30
40
50
60
70
80
90
100
1 3 5 7 9
Fold N
CW
S[
%] Talw
Tmay
Talw and Tmay
 
Figure 6: Results of the open test of the RTE 
experiments by SVM. 
 
When the true data was assumed to be ?Talw? 
only, the minimum value of CWS was 42.7%. 
We focused on the difference between the 
maximum and minimum value in 10-fold cross-
validation. When the true answer was assumed 
to be ?Talm?, the difference between the 
maximum and minimum value is the greatest 
(15.3 points) in the open tests, and ?Talw and 
Talm? was the lowest with 11.6 points.  
We believe that when the result ?Talm? was 
?true?, it was consequently more unstable than 
?Talw and Talm?, because there was a larger 
amount of evaluation data ?Talw and Talm?.  
7 Conclusion 
We built a Japanese textual entailment 
recognition system based on the past methods of 
RTE. We considered the problem of RTE as a 
problem of binary classification, and built a new 
model of RTE for machine learning. We 
proposed machine learning to consider the 
matching rate of the words of the text and the 
hypothesis, using mutual information and 
similarity of the syntax tree. The method of 
using mutual information and the use of three 
methods of SVM tunrned out to be effective. 
 
 
 
 
  Answer/System T/T T/F F/T F/F CWS 
Talw               (SS=1) 9 915 14 872 54.5 
Talm               (SS=1) 1 661 14 872 57.0 True Condition 
Talw and Talm (SS=0) 1586 0 886 0 61.8 
26
In the future, we will consider changing the 
domain of the evaluation data and the 
experiment. Moreover, we will propose a new 
method for the feature of machine learning. 
We will also consider to expand WordNet. 
Shnarch et al (Shnarch et al, 2009) researched 
the extraction from Wikipedia of lexical 
reference rules, identifying references to term 
meaning triggered by other terms. They 
evaluated their lexical reference relation for RTE. 
They improved previous RTE methods. We will 
use their method for ours in order to expand 
Japanese WordNet. We believe that this can help 
us improve our method/results. 
References 
Michitaka Odani, Tomohide Shibata, Sadao 
Kurohashi, Takayuki Nakata, Building data of 
japanese Text Entailment and recognition of 
inferencing relation based on automatic achieved 
similar expression. In Proceeding of 14th Annual 
Meeting of the Association for "atural Language 
Processing, pp. 1140-1143, 2008 (in Japanese) 
 
Diana P?rez and Enrique Alfonseca. Application of 
the Bleu algorithm for recognising textual entailment. 
In Proceedings of the first PASCAL Recognizing 
Textual Entailment Challenge, pp. 9-12, 2005 
 
Oren Glickman, Ido Dagan and Moshe Koppel. 
Web Based Probabilistic Textual Entailment. In 
Proceedings of the PASCAL Recognizing Textual 
Entailment Challenge, pp. 33-36, 2005 
 
Francis Bond, Hitoshi Isahara, Sanae Fujita, 
Kiyotaka Uchimoto, Takayuki Kuribayashi and 
Kyoko Kanzaki. Enhancing the Japanese WordNet. 
In the 7th Workshop on Asian Language Resources, 
in conjunction with ACL-IJCNLP, pp. 1-8, 2009  
 
Hiroshi Ichikawa, Taiichi Hashimoto, Takenobu 
Tokunaka and Hodumi Tanaka. New methods to 
retrieve sentences based on syntactic similarity. 
Information Processing Society of Japan SIG"L 
"ote, pp39-46, 2005(in Japanese) 
 
Kishore Panieni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. BLEU: a Method for  Automatic 
Evaluation of Machine Translation. In Proceedings 
of the Annual Meeting of the Association for 
Computational Linguistics, pp. 311-318, 2002 
 
Ido Dagan, Oren Glickman and Bernardo Magnini. 
The PASCAL Recognizing Textual Entailment 
Challenge. In Proceedings of the first PASCAL 
Recognizing Textual Entailment Challenge, pp. 1-
8,2005 
 
Jes?s Herrera, Anselmo Pe?as and Felisa Verdejo, 
Textual Entailment Recognition Based on 
Dependency Analysis and WordNet. In Proceedings 
of the first PASCAL Recognizing Textual Entailment 
Challenge, pp. 21-24, 2005 
 
Kaname Kasahara, Hirotoshi Taira and Masaaki 
Nagata, Consider of the possibility Textual 
Entailment applied to Reading Comprehension Task 
consisted of multi documents. In Proceeding of 14th 
Annual Meeting of the Association for "atural 
Language Processing, pp. 780-783, 2010 (in 
Japanese) 
 
M. Collins and N. Duffy. Convolution kernel for 
natural language. In Advances in "eural Information 
Proccessing Systems ("IPS), volume 16, pages 625?
632, 2002. 
 
Prodromos Malakasiotis and Ion Androutsopoulos. 
Learning Textual Entailment using SVMs and String 
Similarity Measures. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and 
Paraphrasing, pp. 42-47, 2007 
 
Vladimir N. Vapnik, The Statisitcal Learning 
Theory. Springer, 1998. 
 
Church, K. W. & Gale, W. A.. A comparison of the 
enhanced Good-Turing and deleted estimation 
methods for estimating probabilities of English 
bigrams. Computer Speech and Language, volume 5, 
19-54. 
 
Rodney D. Nielsen, Wayne Ward and James H. 
Martin. Toward Dependency Path based Entailment. 
In Proceedings of the second PASCAL Recognizing 
Textual Entailment Challenge, pp. 44-49, 2006 
 
Eyal Shnarch, Libby barak, Ido Dagan. Extracting 
Lexical Reference Rules from Wikipedia. In 
Proceedings of the Annual Meeting of the 
Association for Computational Linguistics, pp. 450-
458, 2009 
 
 
 
27
Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 1?9,
Beijing, August 2010
Constructing Large-Scale Person Ontology from Wikipedia 
Yumi Shibaki 
Nagaoka University of 
Technology 
shibaki@jnlp.org 
Masaaki Nagata 
NTT Communication 
Science Laboratories 
nagata.masaaki@ 
labs.ntt.co.jp 
Kazuhide Yamamoto 
Nagaoka University of 
Technology 
yamamoto@jnlp.org 
Abstract 
This paper presents a method for con-
structing a large-scale Person Ontology 
with category hierarchy from Wikipe-
dia. We first extract Wikipedia category 
labels which represent person (hereafter, 
Wikipedia Person Category, WPC) by 
using a machine learning classifier. We 
then construct a WPC hierarchy by de-
tecting is-a relations in the Wikipedia 
category network. We then extract the 
titles of Wikipedia articles which 
represent person (hereafter, Wikipedia 
person instance, WPI). Experiments 
show that the accuracy of WPC extrac-
tion is 99.3% precision and 98.4% re-
call, while that of WPI extraction is 
98.2% and 98.6%, respectively. The ac-
curacies are significantly higher than 
the previous methods. 
1  Introduction 
In recent years, we have become increasingly 
aware of the need for, up-to-date knowledge 
bases offering broad coverage in order to im-
plement practical semantic inference engines 
for advanced applications such as question 
answering, summarization and textual entail-
ment recognition. General ontologies, such as 
WordNet (Fellbaum et al, 1998), and Nihongo 
Goi-Taikei (Ikehara et al, 1997), contain gen-
eral knowledge of wide range of fields. How-
ever, it is difficult to instantly add new know-
ledge, particularly proper nouns, to these gen-
eral ontologies. Therefore, Wikipedia has 
come to be used as a useful corpus for know-
ledge extraction because it is a free and large-
scale online encyclopedia that continues to be 
actively developed. For example, in DBpedia 
(Bizer et al 2009), RDF triples are extracted 
from the Infobox templates within Wikipedia 
articles. In YAGO (Suchanek et al 2007), an 
appropriate WordNet synset (most likely cate-
gory) is assigned to a Wikipedia category as a 
super-category, and Wikipedia articles are ex-
tracted as instances of the category.  
As a first step to make use of proper noun 
and related up-to-date information in Wikipedia, 
we focus on person names and the articles and 
categories related to them because it contains a 
large number of articles and categories that in-
dicate person, and because large-scale person 
ontology is useful for applications such as per-
son search and named entity recognition. Ex-
amples of a person article are personal name 
and occupational title such as ?Ichiro? and ?Fi-
nancial planner,? while an example of a person 
category is occupational title such as 
?Sportspeople.? 
The goal of this study is to construct a large-
scale and comprehensive person ontology by 
extracting person categories and is-a relations1 
among them. We first apply a classifier based 
on machine learning to all Wikipedia categories 
to extract categories that represent person. If 
both of the linked Wikipedia categories are per-
son categories, the category link is labeled as 
an is-a relation. We then use a heuristic-based 
rule to extract the title of articles that represent 
person as person instance from the person cate-
gories. 
In the following sections, we first describe 
the language resources and the previous works. 
We then introduce our method for constructing 
the person ontology and report our experimen-
tal results. 
                                                 
1 ?is-a relation? is defined as a relation between A and B 
when ?B is a (kind of) A.? 
1
2 Language Resources 
2.1  Japanese Wikipedia 
Wikipedia is a free, multilingual, on-line en-
cyclopedia that is being actively developed by a 
large number of volunteers. Wikipedia has ar-
ticles and categories. The data is open to the 
public as XML files2. Figure 1 shows an exam-
ple of an article. An article page has a title, 
body, and categories. In most articles, the first 
sentence of the body is the definition sentence 
of the title.  Although the Wikipedia category 
system is organized in a hierarchal manner, it is 
a thematic classification, not a taxonomy. The 
relation between category and subcategory and 
that between a category and articles listed on it 
are not necessarily an is-a relation. A category 
could have two or more super categories and 
the category network could have loops.  
 
?????????Michelle Wie, 1989?10?11?- ?
???????????
Michelle Wie (Michelle Wie, born October 11, 
1989 ) is a golf player. 
Category : American golfers | 1989 births
Mi lle Wie
category
title of article
definitio  sentence
 
Figure 1: Example of title, body (definition 
sentence), and categories for article page in 
Japanese Wikipedia (top) and its translation 
(bottom) 
2.2 Nihongo Goi-Taikei  
To construct the ontology, we first apply a ma-
chine learning based classifier to determine if a 
category label indicates a person or not. A Wi-
kipedia category label is often a common com-
pound noun or a noun phrase, and the head 
word of a Japanese compound noun and noun 
phrase is usually the last word. We assume the 
semantic category of the last word is an impor-
tant feature for classification.  
Nihongo Goi-Taikei (hereafter, Goi-Taikei) 
is one of the largest and best known Japanese 
thesauri. Goi-Taikei contains different semantic 
category hierarchies for common nouns, proper 
nouns, and verbs. In this work, we use only the 
                                                 
2http://download.wikimedia.org/jawiki 
common noun category (Figure 2). It consists 
of approximately 100,000 Japanese words (he-
reafter, instance) and the meanings of each 
word are described by using about 2,700 hie-
rarchical semantic categories. Words (In-
stances) with multiple meanings (ambiguous 
words) are assigned multiple categories in Goi-
Taikei. For example, the transliterated Japanese 
word (instance) raita (???? ) has two 
meanings of ?writer? and ?lighter,? and so be-
longs to two categories, ?353:author 3 ? and 
?915:household.?  
Japanese WordNet (approximately 90,000 
entries as of May 2010), which has recently 
been released to the public (Bonds et al, 2008), 
could be an alternative to Goi-Taikei as a large-
scale Japanese thesaurus. We used Goi-Taikei 
in this work because Japanese WordNet was 
translated from English WordNet and it is not 
known whether it covers the concepts unique to 
Japanese. 
3 Previous Works 
3.1 Ponzetto?s method and Sakurai?s me-
thod 
Ponzetto et al (2007) presented a set of 
lightweight heuristics such as head matching 
and modifier matching for distinguishing is-a 
links from not-is-a links in the Wikipedia cate-
gory network. The main heuristic, ?Syntax-
based methods? is based on head matching, in 
which a category link is labeled as is-a relation 
if the two categories share the same head lem-
ma, such as CAPITALS IN ASIA and CAPI-
TALS. Sakurai et al (2008) presented a method 
equivalent to head matching for Japanese Wi-
kipedia. As Japanese is a head final language, 
they introduced the heuristic called suffix 
matching; it labels a category link as a is-a rela-
tion if one category is the suffix of the other 
category, such as ?????(airports in Ja-
pan) and ??(airports). In the proposed me-
thod herein, if a Wikipedia category and its 
parent category are both person categories, the 
category link is labeled as is-a relation. There-
fore, is-a relations, which cannot be extracted 
by Ponzetto?s or Sakurai?s method, can be ex-
tracted. 
                                                 
3 The Goi-Taikei category is prefixed with ID number. 
2
246:personalities
and competitors
5:humans 223:officials 219:semi-man
249:actor 251:competitor
453:shrine 221:spirit
4:people
151:ethnic group
152:ethnic group 153:race 55:boy 56:girl
1:common noun
2:concrete
1000:abstract
3:agents 388:places 533:objects
362:organizations 389:facilities 468:nature 534:animate
1235:events
1936:job
1937:business1939:occupation 
1065:title
1069:number1066:name
2483:nature 2507:state
385:nation383:assembly
Writer?????
353:author 915:household appliance
lighter?????
706:inanimate
Semantic category hierarchy for common nouns
1236:human
activities
1001:abstract
things
2422:abstract
relationship
About 2,700 categories
About 100,000 instances
 
Figure 2: Part of a category hierarchy for common nouns in Nihongo Goi-Taikei 
3.2 Kobayashi?s method 
Kobayashi et al (2008) presented a tech-
nique to make a Japanese ontology equivalent 
to YAGO; it assigns Goi-Taikei categories to 
Japanese Wikipedia categories. These two me-
thods and our method are similar in that a Wi-
kipedia category and the title of an article are 
regarded as a category and an instance, respec-
tively. Kobayashi et al automatically extract 
hypernyms from the definition sentence of each 
article in advance (referred to hereafter as ?D-
hypernym.?) They apply language-dependent 
lexico-syntactic patterns to the definition sen-
tence to extract the D-hypernym. Here are some 
examples. 
 
??[hypernym]?????? <EOS> 
one of [hypernym] 
 
??[hypernym]???<EOS> 
is a [hypernym] 
 
 [hypernym] <EOS> 
is a [hypernym] ? 
 
where <EOS> refer to the beginning of a 
sentence 
For example, from the article in Figure 1, the 
words ?????? (golf player)? is extracted 
as the D-hypernym of the article ??????
??? (Michelle Wie).? 
Figure 3 outlines the Kobayashi?s method. 
First, for a Wikipedia category, if its last word 
matches an instance of Goi-Taikei category, all 
such Goi-Taikei categories are extracted as a 
candidate of the Wikipedia category?s super-
class. If the last word of the D-hypernym of the 
Wikipedia article listed on the Wikipedia cate-
gory matches an instance of the Goi-Taikei cat-
egory, the Goi-Taikei category is extracted as 
the super-class of the Wikipedia category and 
its instances (Wikipedia articles) (Figure 3). 
Although the Kobayashi?s method is a general 
one, it can be used to construct person ontology 
if the super-class candidates are restricted to 
those Goi-Taikei categories which represent 
person. Titl             ????????
Michelle Wie
Hypernym   ?????
Golf player
?????????????
American golfers
Wikipedia category
Match
last word
Person category
?_person
?????_golfer
??_player
???_artist
?
?
?
Goi-Taikei
Wikipedia article
Title            ALPG???
ALPG Tour
Hypernym   ??????
Golf tour
Doesn?t 
match
Wikipedia article ?
 
Figure 3: The outline of Kobayashi?s method 
3.3 Yamashita?s method 
Yamashita made an open source software 
which extracts personal names from Japanese 
Wikipedia4. He extracted the titles of articles 
listed on the categories ???(? births) (e.g., 
2000 births). As these categories are used to 
sort the names of people, horses, and dogs by 
born year, he used a simple pattern matching 
                                                 
4http://coderepos.org/share/browser/lang/perl/misc/wikipe
jago 
3
rules to exclude horses and dogs. In the expe-
riment in Section 5, we implemented his me-
thod by using not only ??? (births)? but also 
???  (deaths)? and ????  (th-century 
deaths),? ????  (s deaths),? ????  (s 
births),? and ????  (th births)? to extract 
personal names. As far as we know, it is the 
only publicly available software to extract a 
large number of person names from the Japa-
nese Wikipedia. For the comparison with our 
method, it should be noted that his method 
cannot extract person categories. 
4 Ontology Building Method 
4.1 Construction of Wikipedia person cat-
egory hierarchy (WPC) 
We extract the WPC by using a machine learn-
ing classifier. If a Wikipedia category and its 
parent category are both person categories, the 
category link is labeled as an is-a relation. This 
means that all is-a relations in our person on-
tology are extracted from the original Wikipe-
dia category hierarchy using only a category 
classifier. This is because we investigated 
1,000 randomly sampled links between person 
categories and found 98.7% of them were is-a 
relations. Figure 4 shows an example of the 
Wikipedia category hierarchy and the con-
structed WPC hierarchy. 
 
Music Technology
Composers
Broadcasting
Wikipedia person
category (WPC)
Announcer productions
Announcers
is-a
is-a
is-a
is-a
Category without
parent and child
R ot category
Musicians
Conductors
Engineers
Announcers
Musicians
Conductors
Composers
Japanese conductors
Engineers
Japanese conductors
 
Figure 4: Example of Wikipedia category hie-
rarchy (top) and constructed Wikipedia person 
category hierarchy (bottom) 
We detect whether the Wikipedia category 
label represents a person by using Support Vec-
tor Machine (SVM). The semantic category of 
the words in the Wikipedia category label and 
those in the neighboring categories are used for 
the features. We use the following three aspects 
of the texts that exist around the target category 
for creating the features: 
 
1. Structural relation between the target cat-
egory and the text in Wikipedia.  (6 kinds) 
 
2.  Span of the text.  (2 kinds) 
 
3. Semantic category of the text derived 
from Goi-Taikei. (4 kinds) 
 
We examined 48 features by combining the 
above three aspects (6*2*4). 
   The following are the six structural relations 
in Wikipedia between the target category and 
the text information: 
 
Structural relation 
A. The target Wikipedia category label.  
 
B. All parent category labels of the target cat-
egory.  
 
C. All child category labels of the target   cat-
egory.  
 
D. All sibling category labels of the target 
category.  
 
E. All D-hypernym5 from each article listed on 
the target category.  
 
F. All D-hypernyms extracted from the ar-
ticles with the same name as the target cate-
gory. 
 
As for F, for example, when the article ??
???(bassist) is listed on the category: ?
? ? ? ? (bassist), we regard the D-
hypernym of the article as the hypernym of  
the category. 
 
As most category labels and D-hypernyms are 
common nouns, they are likely to match in-
stances in Goi-Taikei which lists possible se-
mantic categories of words.  
                                                 
5As for D-hypernym extraction patterns, we used almost 
the same patterns described in previous works on Japa-
nese sources such as (Kobayashi et al 2008; Sumida et al, 
2008), which are basically equivalent to the works on 
English sources such as (Hearst, 1992). 
4
   After the texts located at various structural 
relations A-F are collected, they are matched to 
the instances of Goi-Taikei in two different 
spans: 
 
Span of the text 
?. All character strings of the text 
 
?. The last word of the text 
 
For the span ?, the text is segmented into 
words using a Japanese morphological analyzer. 
The last word is used because the last word 
usually represents the meaning of the entire 
noun phrase (semantic head word) in Japanese.  
In the proposed method, hierarchical seman-
tic categories of Goi-Taikei are divided into 
two categories; ?Goi-Taikei person categories? 
and other categories. Goi-Taikei person catego-
ry is defined as those categories that represent 
person, that is, all categories under ?5:humans? 
and ?223:officials,? and ?1939: occupation? 
and ?1066:name? in Goi-Taikei hierarchy as 
shown in Figure 1.  
For each structural relation A-F  and span ? 
and ?, we calculate four relative frequencies 
a-d, which represents the manner in which the 
span of texts match the instance of Goi-Taikei 
person category. It basically indicates the de-
gree to which the span of text is likely to mean 
a person.  
 
Semantic type 
a. The span of text matches only instances of 
Goi-Taikei person categories. 
 
b. The span of text matches only instances of 
categories other than Goi-Taikei person cat-
egories. 
 
c. The span of text matches both instances of 
Goi-Taikei person categories and those of 
other categories. 
 
d. The span of text does not match any in-
stances of Goi-Taikei. 
 
For example, when the target category is ???
?? (musicians) in Figure 5 and the feature in 
question is B-? (the last word of its parent 
categories), the word ??? (whose senses are 
family and house) falls into semantic type c, 
and the word ???? (music) falls into seman-
tic type b. Therefore, the frequency of semantic 
types a, b, c, d are 0, 1, 1, 0, respectively, in the 
features related to B-?, and the relative fre-
quencies used for the feature value related B-? 
are 0, 0.5, 0.5, 0, respectively. In this way, we 
use 48 relative frequencies calculated from the 
combinations of structural relation A-F, span 
? and ?, and semantic type a-d, as the feature 
vector for the SVM.  
 ?Target category
?Similar category
?Last word
???_Artists ??_Music
???_Musicians
??????_Jazz composers
???_Composers
???_Musicians by instrument
?? Art ??????_People by occupation
 
Figure 5: Example of Wikipedia category hie-
rarchy when the target category is ????? 
4.2  Similar category 
In Wikipedia, there are categories that do not 
have articles and those with few neighboring 
categories. Here, we define the neighboring 
categories for a category as those categories 
that can be reached through a few links from 
the category. In these cases, there is a possibili-
ty that there is not enough text information 
from which features (mainly semantic category 
of words) can be extracted, which could de-
grade the accuracy. 
The proposed method overcomes this prob-
lem by detecting categories similar to the target 
category (the category in question) from its 
neighboring categories for extracting sufficient 
features to perform classification. Here, "simi-
lar category" is defined as parent, child, and 
sibling categories whose last word matches the 
last word of the target category. This is because 
there is a high possibility that the similar cate-
gories and the target category have similar 
meaning if they share the same last word in the 
category labels. If the parent (child) category is 
determined as a similar category, its parent 
(child) category is also determined as a similar 
category if the last word is the same. The pro-
cedure is repeated as long as they share the 
same last word.  
Figure 5 shows an example of similar cate-
gories when the target category is ?Musicians.? 
In this case, features extracted from A-F of 
5
similar categories are added to features ex-
tracted using A-F of the target category, ?Mu-
sicians.? For example, similar category ?Art-
ists? has ?Art? and ?People by occupation? as 
B (parent categories of the target category) in 
Figure 5, therefore ?Art? and ?People by occu-
pation? are added to B of ?Musicians.? 
4.3 Extracting Wikipedia person instance 
(WPI) 
The proposed method extracts, as WPIs the 
titles of articles listed as WPCs that meet the 
following four requirements.  
 
1. The last word of the D-hypernym of the 
title of the Wikipedia article matches an in-
stance of Goi-Taikei person category.  
 
2. The last word of the title of Wikipedia ar-
ticle matches an instance of Goi-Taike per-
son category. 
 
3. At least one of the Wikipedia categories as-
signed to the Wikipedia article matches the 
following patterns: 
 
(??|???|???|??|???|???)<EOS> 
( deaths | th-century deaths | ?s deaths | births | th-births | ?s 
births ) <EOS> 
 
These categories are used to sort a large 
number of person names by year.  
 
 
4. Wikipedia categories assigned to the Wiki-
pedia article satisfy the following condition: 
 
   
5.0categories  Wikipediaofnumber  All
4.1Section in   WPCsextracted ofNumber ???
 
 
This condition is based on the observation 
that the more WPCs a Wikipedia article is 
assigned to, the more it is likely to be a WPI. 
We set the threshold 0.5 from the results of a 
preliminary experiment.  
5 Experiments  
5.1 Experimental setup 
We used the XML file of the Japanese Wiki-
pedia as of July 24, 2008. We removed irrele-
vant pages by using keywords (e.g., ?image:,? 
?Help:?) in advance. This cleaning yielded 
477,094 Wikipedia articles and 39,782 Wiki-
pedia categories. We manually annotated each 
category to indicate whether it represents per-
son (positive) or not (negative). For ambiguous 
cases, we used the following criteria:  
 
?Personal name by itself (e.g., Michael Jack-
son) is not regarded as WPC because usually 
it does not have instances. (Note: personal 
name as article title is regarded as WPI. )  
 
?Occupational title (e.g., Lawyers) is regarded 
as WPC because it represents a person. 
 
?Family (e.g., Brandenburg family) and Eth-
nic group (e.g., Sioux) are regarded as WPC. 
 
?Group name (e.g., The Beatles) is not re-
garded as WPC. 
 
In order to develop a person category classifier, 
we randomly selected 2,000 Wikipedia catego-
ries (positive:435, negative:1,565) from all cat-
egories for training6. We used the remaining 
37,767 categories for evaluation. To evaluate 
WPI extraction accuracy, we used Wikipedia 
articles not listed on the Wikipedia categories 
used for training. 417,476 Wikipedia articles 
were used in the evaluation.  
To evaluate our method, we used TinySVM-
0.09 7  with a linear kernel for classification, 
and the Japanese morphological analyzer JU-
MAN-6.0 8  for word segmentation. The com-
parison methods are Kobayashi?s method and 
Yamashita?s method under the same conditions 
as our method. 
5.2 Experimental results  
Table 1 shows the WPCs extraction accuracy. 
Precision and recall of proposed method are 6.5 
points and 14.8 points better than those of Ko-
bayashi's method, respectively. 
 
Precision Recall F-measure
Kobayashi?s
method
92.8%
(6727/7247)
83.6%
(6727/8050)
88.0%
Proposed
method
99.3%
(7922/7979)
98.4%
(7922/8050)
98.8%
 
Table 1: The Wikipedia person categories 
(WPCs) extraction accuracy 
                                                 
6We confirmed that the accuracy will level off about 
2,000 training data by experiment. Details will be de-
scribed in Section 6. 
7http://chasen.org/~taku/software/TinySVM/ 
8http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/juman.html 
6
To confirm our assumption on the links be-
tween WPCs, we randomly selected 1,000 pairs 
of linked categories from extracted WPCs, and 
manually investigated whether both 
represented person and were linked by is-a re-
lation. We found that precision of these pairs 
was 98.3%. 
Errors occurred when the category link be-
tween  person categories in the Wikipedia cate-
gory network was not an is-a relation, such as 
???(Chiba clan) ? ????(Ohsuga clan). 
However, this case is infrequent, because 
98.7% of the links between person categories 
did exhibit an is-a relation (as described in Sec-
tion 4.1).  
Table 2 shows the WPIs extraction accuracy. 
We randomly selected 1,000 Wikipedia articles 
from all categories in Wikipedia, and manually 
created evaluation data (positive:281, nega-
tive:719). The recall of the proposed method 
was 98.6%, 21.0 points higher than that of Ya-
mashita?s method. Our method topped the F-
measure of Kobayashi?s method by 3.4 points. 
Among 118,552 extracted as WPIs by our me-
thod, 116,418 articles were expected be correct. 
In our method, errors occurred when WPI was 
not listed on any WPCs. However, this case is 
very rare. Person instances are almost always 
assigned to at least one WPC. Thus, we can 
achieve high coverage for WPIs even if we fo-
cus only on WPCs. We randomly selected 
1,000 articles from all articles and obtained 277 
person instances by a manual evaluation. Fur-
thermore, we investigated the 277 person in-
stances, and found that only two instances were 
not classified into any WPCs (0.7%). 
 
Precision Recall F-measure
Yamashita's
method
100.0%
(218/218)
77.6%
(218/281)
87.4%
Kobayashi's
method
96%
(264/275)
94.0%
(264/281)
95.0%
Proposed
method
98.2%
(277/282)
98.6%
(277/281)
98.4%
Table 2: The Wikipedia person instance 
(WPIs) extraction accuracy 
 
Table 3 shows the extracted WPC-WPI pairs 
(e.g., American golfers-Michelle Wie, Artists-
Meritorious Artist) extraction accuracy. We 
randomly selected 1,000 pairs of Wikipedia 
category and Wikipedia article from all such 
pairs in Wikipedia, and manually investigated 
whether both category and article represented a 
person and whether they were linked by an is-a 
relation (positive:296, negative:704). Precision 
and recall of proposed method are 2.1 points 
and 11.8 points higher than those of Kobaya-
shi's method, respectively. Among all 274,728 
extracted as WPC-WPI pairs by our method, 
269,233 was expected be correct. 
 
Precision Recall F-measure
Kobayashi?s
method
95.9%
(259/270)
87.5%
(259/296)
91.5%
Proposed
method
98.0%
(294/300)
99.3%
(294/296)
98.7%
Table 3: The extraction accuracy of the pairs 
of Wikipedia person category and person in-
stance (WPC-WPI) 
6 Discussions 
We constructed a WPC hierarchy using the 
8,357 categories created by combining ex-
tracted categories and training categories. The 
resulting WPC hierarchy has 224 root catego-
ries (Figure 4). Although the majority of the 
constructed ontology is interconnected, 194 
person categories had no parent or child (2.3 % 
of all person categories). In rare cases, the cat-
egory network has loops (e.g., ?Historians? and 
?Scholars of history? are mutually interlinked).  
Shibaki et al (2009) presented a method for 
building a Japanese ontology from Wikipedia 
using Goi-Taikei, as its upper ontology. This 
method can create a single connected taxono-
my with a single root category. We also hope 
to create a large-scale, single-root, and inter-
connected person ontology by using some up-
per ontology.   
Our method is able to extract WPCs that do 
not match any Goi-Taikei instance (e.g., Vi-
olinists and Animators). Furthermore, our me-
thod is able to detect many ambiguous Wikipe-
dia category labels correctly as person category. 
For example, ?????????? (fashion 
model)? is ambiguous because the last word 
???? (model)? is ambiguous among three 
senses: person, artificial object, and abstract 
relation. Kobayashi?s method cannot extract a 
WPC if the last word of the category label does 
not match any instance in Goi-Taikei. Their 
method is error-prone if the last word has mul-
7
tiple senses in Goi-Taikei because it is based on 
simple pattern matching. Our method can han-
dle unknown and ambiguous category labels 
since it uses machine learning-based classifiers 
whose features are extracted from neighboring 
categories. 
Our method can extract is-a person category 
pairs that could not be extracted by Ponzetto et 
al. (2007) and Sakurai et al (2008). Their me-
thods use head matching in which a category 
link is labeled as an is-a relation only if the 
head words of category labels are matched. 
However, our method can extract is-a relations 
without reference to surface character strings, 
such as ????????(Journalists)? and 
?????????(Sports writers).? Among 
all 14,408 Wikipedia category pairs extracted 
as is-a relations in our method, 5,558 (38.6%) 
did not match their head words.  
We investigated the learning curve of the 
machine learning-based classifier for extracting 
WPCs, in order to decide the appropriate 
amount of training data for future updates.  
As we have already manually tagged all 
39,767 Wikipedia categories, we randomly se-
lected 30,000 categories and investigated the 
performance of our method when the number 
of the training data was changed from 1,000 to 
30,000. The evaluation data was the remaining 
9,767 categories.  
 
precision
recall
f-value
100.0
99.0
98.0
97.0
Precision
Recall
F-measure
The number of training data
P
r
e
c
i
s
i
o
n
 /
 R
e
c
a
l
l
 /
 F
-
m
e
a
s
u
r
e
 [
%
]
0 10k 20k 30k
 
Figure 6: The effect of training data size to 
WPC extraction accuracy 
 
Figure 6 shows the precision, recall, and F-
measure for different training data sizes. F-
measure differed only 0.4 points from 1,000 
samples (98.5%) to 30,000 samples (98.9%). 
Figure 6 shows that the proposed method of-
fers high accuracy in detecting WPCs with only 
a few thousand training examples.  
Our method uses similar categories for 
creating features as well as the target Wikipe-
dia category (Section 4.1). We compared the 
proposed method to a variant that does not use 
similar categories to confirm the effectiveness 
of this technique. Furthermore, our method 
uses the Japanese thesaurus, Goi-Taikei, to 
look up the semantic category of the words for 
creating the features for machine learning. We 
also compared the proposed method with the 
one that does not use semantic category (de-
rived from Goi-Taikei) but instead uses word 
surface form for creating features (This one 
uses similar categories).  
   Figure 7 shows the performance of the clas-
sifiers for each type of features. We can clearly 
observe that using similar categories results in 
higher F-measure, regardless of the training 
data size. We also observe that when there is 
little training data, the method using word sur-
face form as features results in drastically low-
er F-measures. In addition, its accuracy was 
consistently lower than the others even if the 
training data size was increased. Therefore, we 
can conclude that using similar category and 
Goi-Taikei are very important for creating good 
features for classification. 
 
????
????
????
Proposed method
Without using similar category
Without using Goi-Taikei
F
-
m
e
a
s
u
r
e
 [
%
]
The number of training data
100.0
96.0
98.0
94.0
0
92.0
90.0
10k 20k 30k
 
Figure 7: The effects of using similar catego-
ries and Goi-Taikei 
 
In future, we will attempt to apply our method 
to other Wikipedia domains, such as organiza-
tions and products. We will also attempt to use 
other Japanese thesauri, such as Japanese 
WordNet. Furthermore, we hope to create a 
large-scale and single connected ontology. As a 
final note, we plan to open the person ontology 
constructed in this paper to the public on Web 
in the near future. 
 
8
 References 
Bizer, C., J. Lehmann, G. Kobilarov, S. Auer, C. 
Becker, R. Cyganiak, and S. Hellmann. 2009.  
?DBpedia - A crystallization point for the web of 
data,? Web Semantics: Science, Services and 
Agents on the World Wide Web, vol. 7, No.3,  
pages 154-165. 
Bond, Francis, Hitoshi Isahara, Kyoko Kanzaki, and 
Kiyotaka Uchimoto. 2008. Boot-strapping a 
wordnet using multiple existing wordnets. In 
Proceedings of the 6th International Conference 
on Language Resources and Evaluation (LREC), 
pages 28-30. 
Fellbaum, Christiane. 1998. WordNet: An Electron-
ic Lexical Database, Language, Speech, and 
Communication Series. MIT Press. 
Hearst, Marti A. 1992. Automatic acquisition of 
hyponyms from large text corpora. In Proceed-
ings of the 14th Conference on Computational 
Linguistics (COLING), pages 539-545. 
Ikehara, Satoru, Masahiro Miyazaki, Satoshi Shi-
rai,Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogu-
ra, Yoshifumi Ooyama, and Yoshihiko Hayashi, 
editors. 1997. Nihongo Goi-Taikei ? a Japanese 
Lexicon. Iwanami Shoten. (in Japanese). 
Kobayashi, Akio, Shigeru Masuyama, and Satoshi 
Sekine. 2008. A method for automatic construc-
tion of general ontology merging goitaikei and 
Japanese Wikipedia. In Information Processing 
Society of Japan (IPSJ) SIG Technical Re-
port2008-NL-187 (in Japanese), pages 7-14. 
Ponzetto, S. P. and Michael Strube. 2007. Deriving 
a large scale taxonomy from Wikipedia. In Pro-
ceedings of the 22nd Conference on the Ad-
vancement of Artificial Intelligence (AAAI), pag-
es 1440?1445. 
Sakurai, Shinya, Takuya Tejima, Masayuki Ishika-
wa, Takeshi Morita, Noriaki Izumi, and Takahira 
Yamaguchi. 2008. Applying Japanese Wikipedia 
for building up a general ontology. In Japanese 
Society of Artificial Intelligence (JSAI) Technical 
Report SIG-SWO-A801-06 (in Japanese), pages 
1-8. 
 Shibaki, Yumi, Masaaki Nagata and Kazuhide Ya-
mamoto. 2009. Construction of General Ontolo-
gy from Wikipedia using a Large-Scale Japanese 
Thesaurus. In Information Processing Society of 
Japan (IPSJ) SIG Technical Report2009-NL-
194-4. (in Japanese). 
 
Suchanek, Fabian M., Gjergji Kasneci, and Ger-
hardWeikum. 2007. Yago: A core of semantic 
knowledge unifying wordnet and Wikipedia. In 
Proceedings of the 16th International Conference 
on World Wide Web (WWW), pages 697-706. 
Sumida, Asuka, Naoki Yoshinaga, and Kentaro To-
risawa. 2008. Boosting precision and recall of 
hyponymy relation acquisition from hierarchical 
layouts in Wikipedia. In Proceedings of the Sixth 
Language Resources and Evaluation Confe-
rence(LREC), pages 28?30. 
9
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 32?39,
Beijing, August 2010
Even Unassociated Features Can Improve
Lexical Distributional Similarity
Kazuhide Yamamoto and Takeshi Asakura
Department of Electrical Engineering
Nagaoka University of Technology
{yamamoto, asakura}@jnlp.org
Abstract
This paper presents a new computation
of lexical distributional similarity, which
is a corpus-based method for computing
similarity of any two words. Although
the conventional method focuses on em-
phasizing features with which a given
word is associated, we propose that even
unassociated features of two input words
can further improve the performance in
total. We also report in addition that
more than 90% of the features has no
contribution and thus could be reduced
in future.
1 Introduction
Similarity calculation is one of essential tasks in
natural language processing (1990; 1992; 1994;
1997; 1998; 1999; 2005). We look for a seman-
tically similar word to do corpus-driven summa-
rization, machine translation, language genera-
tion, recognition of textual entailment and other
tasks. In task of language modeling and disam-
biguation we also need to semantically general-
ize words or cluster words into some groups. As
the amount of text increases more and more in
the contemporary world, the importance of sim-
ilarity calculation also increases concurrently.
Similarity is computed by roughly two ap-
proaches: based on thesaurus and based on cor-
pus. The former idea uses thesaurus, such as
WordNet, that is a knowledge resource of hi-
erarchical word classification. The latter idea,
that is the target of our work, originates from
Harris?s distributional hypothesis more than four
decades ago (1968), stating that semantically
similar words tend to appear in similar contexts.
In many cases a context of a word is represented
as a feature vector, where each feature is another
expression that co-occurs with the given word in
the context.
Over a long period of its history, in partic-
ular in recent years, several works have been
done on distributional similarity calculation. Al-
though the conventional works have attained the
fine performance, we attempt to further improve
the quality of this measure. Our motivation of
this work simply comes from our observation
and analysis of the output by conventional meth-
ods; Japanese, our target language here, is writ-
ten in a mixture of four scripts: Chinese char-
acters, Latin alphabet, and two Japanese-origin
characters. In this writing environment some
words which have same meaning and same pro-
nunciation are written in two (or more) different
scripts. This is interesting in terms of similarity
calculation since these two words are completely
same in semantics so the similarity should be
ideally 1.0. However, the reality is, as far as we
have explored, that the score is far from 1.0 in
many same word pairs. This fact implies that the
conventional calculation methods are far enough
to the goal and are expected to improve further.
The basic framework for computing distribu-
tional similarity is same; for each of two input
words a context (i.e., surrounding words) is ex-
tracted from a corpus, a vector is made in which
an element of the vector is a value or a weight,
and two vectors are compared with a formula to
compute similarity. Among these processes we
have focused on features, that are elements of
32
the vector, some of which, we think, adversely
affect the performance. That is, traditional ap-
proaches such as Lin (1998) basically use all of
observed words as context, that causes noise in
feature vector comparison. One may agree that
the number of the characteristic words to deter-
mine the meaning of a word is some, not all, of
words around the target word. Thus our goal is
to detect and reduce such noisy features.
Zhitomirsky-Geffet and Dagan (2009) have
same motivation with us and introduced a boot-
strapping strategy that changes the original fea-
tures weights. The general idea here is to pro-
mote the weights of features that are common
for associated words, since these features are
likely to be most characteristic for determining
the word?s meaning. In this paper, we propose
instead a method to using features that are both
unassociated to the two input words, in addition
to use of features that are associated to the input.
2 Method
The lexical distributional similarity of the input
two words is computed by comparing two vec-
tors that express the context of the word. In this
section we first explain the feature vector, and
how we define initial weight for each feature of
the vector. We then introduce in Subsection 2.3
the way to compute similarity by two vectors.
After that, we emphasize some of the features by
their association to the word, that is explained in
Subsection 2.4. We finally present in Subsection
2.5 feature reduction which is our core contribu-
tion of this work. Although our target language
is Japanese, we use English examples in order to
provide better understanding to the readers.
2.1 Feature Vector
We first explain how to construct our feature vec-
tor from a text corpus.
A word is represented by a feature vector,
where features are collection of syntactically de-
pendent words co-occurred in a given corpus.
Thus, we first collect syntactically dependent
words for each word. This is defined, as in
Lin (1998), as a triple (w, r,w?), where w and
w? are words and r is a syntactic role. As for
definition of word, we use not only words given
by a morphological analyzer but also compound
words. Nine case particles are used as syntactic
roles, that roughly express subject, object, modi-
fier, and so on, since they are easy to be obtained
from text with no need of semantic analysis. In
order to reduce noise we delete triples that ap-
pears only once in the corpus.
We then construct a feature vector out of col-
lection of the triples. A feature of a word is an
another word syntactically dependent with a cer-
tain role. In other words, given a triple (w, r,w?),
a feature of w corresponds to a dependent word
with a role (r,w?).
2.2 (Initial) Filtering of Features
There are several weighting functions to deter-
mine a value for each feature element. As far
as we have investigated the literature the most
widely used feature weighting function is point-
wise mutual information (MI), that is defined as
follows:
MI(w, r,w?) = log2
freq(w, r,w?)S
freq(w)freq(r,w?) (1)
where freq(r,w?) is the frequency of the co-
occurrence word w? with role r, freq(w)
is the independent frequency of a word w,
freq(w, r,w?) is the frequency of the triples
(w, r,w?), and S is the number of all triples.
In this paper we do not discuss what is the
best weighting functions, since this is out of tar-
get. We use mutual information here because it
is most widely used, i.e., in order to compare per-
formance with others we want to adopt the stan-
dard approach.
As other works do, we filter out features that
have a value lower than a minimal weight thresh-
olds ?. The thresholds are determined according
to our preliminary experiment, that is explained
later.
2.3 Vector Similarity
Similarity measures of the two vectors are com-
puted by various measures. Shibata and Kuro-
hashi (2009) have compared several similarity
measures including Cosine (Ruge, 1992), (Lin,
33
 
(input word) w: boy
(feature) v: guardOBJ
(synonyms of w, shown with its similarity to w) Syn(w) =
{ child(0.135), girl(0.271), pupil(0.143), woman(0.142), young people(0.147) } 
(feature vectors V ):
V(boy) = { parentsMOD, runawaySUBJ, reclaimOBJ, fatherMOD, guardOBJ , ? ? ? }
V(child) = { guardOBJ, lookOBJ, bringOBJ, give birthOBJ, careOBJ , ? ? ? }
V(girl) = { parentsMOD, guardOBJ, fatherMOD, testifySUBJ, lookOBJ, ? ? ? }
V(pupil) = { targetOBJ, guardOBJ, careOBJ, aimOBJ, increaseSUBJ, ? ? ? }
V(woman) = { nameMOD, give birthOBJ, groupMOD, together+with, parentsMOD , ? ? ? }
V(young people) = { harmfulTO, globalMOD, reclaimOBJ, wrongdoingMOD , ? ? ? } 
(words that has feature v) Asc(v) = {boy, child, girl, pupil, ? ? ?}
weight(w, v) = weight (boy, guardOBJ) =
?
wf?Asc(v)?Syn(w) sim(w,wf )
= 0.135 + 0.271 + 0.143 = 0.549 
Figure 1: Example of feature weighting for word boy.
1998), (Lin, 2002), Simpson, Simpson-Jaccard,
and conclude that Simpson-Jaccard index attains
best performance of all. Simpson-Jaccard index
is an arithmetic mean of Simpson index and Jac-
card index, defined in the following equation:
sim(w1, w2) =
1
2(simJ (w1, w2)+simS(w1, w2))(2)
simJ(w1, w2) =
|V1 ? V2|
|V1 ? V2|
(3)
simS(w1, w2) =
|V1 ? V2|
min(|V1|, |V2|)
(4)
where V1 and V2 is set of features for w1 and
w2, respectively, and |A| is the number of set A.
It is interesting to note that both Simpson and
Jaccard compute similarity according to degree
of overlaps of the two input sets, that is, the re-
ported best measure computes similarity by ig-
noring the weight of the features. In this paper
we adopt Simpson-Jaccard index, sim, which
indicates that the weight of features that is ex-
plained below is only used for feature reduction,
not for similarity calculation.
2.4 Feature Weighting by Association
We then compute weights of the features of the
word w according to the degree of semantic as-
sociation to w. The weight is biased because all
of the features, i.e., the surrounding words, are
not equally characteristic to the input word. The
core idea for feature weighting is that a feature
v in w is more weighted when more synonyms
(words of high similarity) of w also have v.
Figure 1 illustrates this process by examples.
Now we calculate a feature guardOBJ for a word
boy, we first collect synonyms of w, denoted by
Syn(w), from a thesaurus. We then compute
similarities between w and each word in Syn(w)
by Equation 2. The weight is the sum of the sim-
ilarities of words in Syn(w) that have feature v,
defined in Equation 5.
weight(w, v) =
?
wf?Asc(v)?Syn(w)
sim(w,wf )
(5)
34
Figure 2: An illustration of similarity calculation of Zhitomirsky-Geffet and Dagan (2009) (a) and
the proposed method (b1 and b2) in feature space. In order to measure the distance of the two words
(shown in black dots) they use only associated words, while we additionally use unassociated words
in which the distances to the words are similar.
2.5 Feature Reduction
We finally reduce features according to the dif-
ference of weights of each feature in words we
compare. In computing similarity of two words,
w1 and w2, a feature v satisfying Equation 6 is
reduced.
abs(weight(w1, v) ? weight(w2, v)) > ? (6)
where abs() is a function of absolute value, and
? is a threshold for feature reduction.
Figure 2 illustrates our idea and compares
the similar approach proposed by Zhitomirsky-
Geffet and Dagan (2009). Roughly speaking,
Zhitomirsky-Geffet and Dagan (2009) compute
similarity of two words, shown as black dots
in (a), mainly according to associated features
(dark-colored circle), or features that has high
weights in Equation 5. And the associated fea-
tures are determined word by word indepen-
dently.
In contrast, the proposed method relatively re-
duces features, depending on location of input
two words. At (b1) in the figure, not only asso-
ciated (high-colored area) but unassociated fea-
tures (light-colored area) are used for similar-
ity computation in our method. As Equation 6
shows, regardless of how much a feature is as-
sociated to the word, the feature is not reduced
when it has similar weight to both w1 and w2,
located at the middle area of the two words in
the figure.
This idea seems to work more effectively,
compared with Zhitomirsky-Geffet and Da-
gan (2009), in case that input two words are not
so similar, that is shown at (b2) of the figure.
As they define associated features independently,
it is likely that the overlapped area is little or
none between the two words. In contrast, our
method uses features at the middle area of two
input words, where there is always certain fea-
tures provided for similarity computation, shown
in case (b2). Simplified explanation is that our
similarity is computed as the ratio of the associ-
ated area to the unassociated area in the figure.
We will verify later if the method works better in
low similarity calculation.
2.6 Final Similarity
The final similarity of two words are calculated
by two shrunk vectors (or feature sets) and Equa-
tion 2, that gives a value between 0 and 1.
35
3 Evaluation
3.1 Evaluation Method
In general it is difficult to answer how similar
two given words are. Human have no way to
judge correctness if computed similarity of two
words is, for instance, 0.7. However, given two
word pairs, such as (w,w1) and (w,w2), we may
answer which of two words, w1 or w2, is more
similar to w than the other one. That is, degree
of similarity is defined relatively hence accuracy
of similarity measures is evaluated by way of rel-
ative comparisons.
In this paper we employ an automatic eval-
uation method in order to reduce time, human
labor, and individual variations. We first col-
lect four levels of similar word pairs from a the-
saurus1. Thesaurus is a resource of hierarchi-
cal words classification, hence we can collect
several levels of similar word pairs according
to the depth of common parent nodes that two
words have. Accordingly, we constructed four
levels of similarity pairs, Level 0, 1, 2, and 3,
where the number increases as the similarity in-
creases. Each level includes 800 word pairs that
are randomly selected. The following examples
are pairs with word Asia in each Level. 
Example: Four similarity levels for pair of
Asia.
Level 3(high): Asia vs. Europe
Level 2: Asia vs. Brazil
Level 1: Asia vs. my country
Level 0(low): Asia vs. system 
We then combine word pairs of adjacent sim-
ilarity Levels, such as Level 0 and 1, that is a
test set to see low-level similarity discrimination
power. The performance is calculated in terms
of how clearly the measure distinguishes the dif-
ferent levels. In a similar fashion, Level 1 and 2,
as well as 2 and 3, are combined and tested for
middle-level and high-level similarity discrimi-
nation, respectively. The number of pairs in each
1In this experiment we use Bunrui Goi Hyo also for
evaluation. Therefore, this experimental setting is a kind
of closed test. However, we see that the advantage to use
the same thesaurus in the evaluation seems to be small.
Figure 3: Relation between threshold ? and per-
formance in F-measures for Level 3+2 test set.
test set is 1,600 as two Levels are combined.
3.2 Experimental Setting
The corpus we use in this experiment is all the
articles in The Nihon Keizai Shimbun Database,
a Japanese business newspaper corpus cover-
ing the years 1990 through 2004. As morpho-
logical analyzer we use Chasen 2.3.3 with IPA
morpheme dictionary. The number of collected
triples is 2,584,905, that excludes deleted ones
due to one time appearance and words including
some symbols.
In Subsection 2.4 we use Bunrui Goi Hyo, a
Japanese thesaurus for synonym collection. The
potential target words are all content words, ex-
cept words that have less than twenty features.
The number of words after exclusion is 75,530.
Moreover, words that have four or less words in
the same category in the thesaurus are regarded
as out of target in this paper, due to limitation
of Syn(w) in Subsection 2.4. Also, in order
to avoid word sense ambiguity, words that have
more than two meanings, i.e., those classified in
more than two categories in the thesaurus, also
remain to be solved.
3.3 Threshold for Initial Filtering
Figure 3 shows relation between threshold ? and
the performance of similarity distinction that is
drawn in F-measures, for Level 3+2 test set. As
can be seen, the plots seem to be concave down
36
Figure 4: Threshold vs. accuracy in Level 3+2
set.
Figure 5: Threshold vs. accuracy in Level 2+1
set.
and there is a clear peak when ? is between 2
and 3.
In the following experiments we set ? the
value where the best performance is given for
each test set. We have observed similar phenom-
ena in other test sets. The thresholds we use is
2.1 for Level 3+2, 2.4 for Level 2+1, and 2.4 for
Level 1+0.
3.4 Threshold for Weighting Function
Figure 4, 5, and 6 show relation between thresh-
old ? and performance in Level 3+2, 2+1, 1+0
test set, respectively. The threshold at the point
where highest performance is obtained greatly
depends on Levels: 0.3 in Level 3+2, 0.5 in Level
2+1, and 0.9 in Level 1+0. Comparison of these
three figures indicates that similarity distinction
Figure 6: Threshold vs. accuracy in Level 1+0
set.
Table 1: Performance comparison of three meth-
ods in each task (in F-measures).
Level S&K ZG&D proposed
Lvl.3+Lvl.2 0.702 0.791 0.797
Lvl.2+Lvl.1 0.747 0.771 0.773
Lvl.1+Lvl.0 0.838 0.789 0.840
power in higher similarity region requires lower
threshold, i.e., fewer features. In contrast, con-
ducting fine distinction in lower similarity level
requires higher threshold, i.e., a lot of features
most of which may be unassociated ones.
3.5 Performance
Table 1 shows performance of the pro-
posed method, compared with Shibata
and Kurohashi (2009) (S&K in the table)
and Zhitomirsky-Geffet and Dagan (2009)
(ZG&D)2. The method of Shibata and Kuro-
hashi (2009) here is the best one among those
compared. It uses only initial filtering described
in Subsection 2.2. The method of Zhitomirsky-
Geffet and Dagan (2009) in addition emphasize
associated features as explained in Subsection
2.4. All of the results in the table are the best
ones among several threshold settings.
The result shows that the accuracy is 0.797
(+0.006) in Level 3+2, 0.773 (+0.002) in Level
2The implementations of providing associated words
and the bootstrapping are slightly different to Zhitomirsky-
Geffet and Dagan (2009).
37
2+1, and 0.840 (+0.001) in Level 1+0, where the
degree of improvement here are those compared
with best ones except our proposed method. This
confirms that our method attains equivalent or
better performance in all of low, middle, and
high similarity levels.
We also see in the table that S&K and ZG&D
show different behavior according to the Level.
However, it is important to note here that our
proposed method performs equivalent or outper-
forms both methods in all Levels.
4 Discussions
4.1 Behavior at Each Similarity Level
As we have discussed in Subsection 2.5, our
method is expected to perform better than
Zhitomirsky-Geffet and Dagan (2009) in distinc-
tion in lower similarity area. Roughly speak-
ing, we interpret the results as follows. Shi-
bata and Kurohashi (2009) always has many fea-
tures that degrades the performance in higher
similarity level, since the ratio of noisy fea-
tures may throw into confusion. Zhitomirsky-
Geffet and Dagan (2009) reduces such noise
that gives better performance in higher similarity
level and is stable in all levels. And our proposed
method maintains performance of Zhitomirsky-
Geffet and Dagan (2009) in higher level while
improves performance that is close to Shibata
and Kurohashi (2009) in lower level, utilizing
fewer features. We think our method can include
advantages over the two methods.
4.2 Error Analysis
We overview the result and see that the major er-
rors are NOT due to lack of features. Table 2
illustrates the statistics of words with a few fea-
tures (less than 50 or 20). This table clearly tells
us that, in the low similarity level (Level 1+0) in
particular, there are few pairs in which the word
has less than 50 or 20, that is, these pairs are con-
sidered that the features are erroneously reduced.
4.3 Estimation of Potential Feature
Reduction
It is interesting to note that we may reduce 81%
of features in Level 3+2 test set while keeping
Table 2: Relation of errors and words with a few
features. In the table, (h) and (l) shows pairs that
are judged higher (lower) by the system. Column
of < 50 (< 20) means number of pairs each of
which has less than 50 (20) features.
Level #errs < 50 fea. < 20 fea.
Lvl.3+2 (h) 125 76 (61%) 32 (26%)
Lvl.3+2 (l) 220 150 (68%) 60 (27%)
Lvl.2+1 (h) 137 75 (55%) 32 (23%)
Lvl.2+1 (l) 253 135 (53%) 52 (21%)
Lvl.1+0 (h) 149 23 (15%) 4 ( 3%)
Lvl.1+0 (l) 100 17 (17%) 3 ( 3%)
the performance, if we can reduce them prop-
erly. In a same way, 87% of features in Level
2+1 set, and 52% of features in Level 1+0 set,
may also be reduced. These numbers are given
at the situation in which F-measure attains best
performance. Here, it is not to say that we are
sure to reduce them in future, but to estimate how
many features are really effective to distinguish
the similarity.
Here we have more look at the statistics. The
number of initial features on average is 609 in
Level 3+2 test set. If we decrease threshold by
0.1, we can reduce 98% of features at the thresh-
old of 0.8, where the performance remains best
(0.791). This is a surprising fact for us since
only 12 (; 609?(1?0.98)) features really con-
tribute the performance. Therefore, we estimate
that there is a lot to be reduced further in order
to purify the features.
5 Conclusion and Future Work
This paper illustrates improvement of lexical
distributional similarity by not only associated
features but also utilizing unassociated features.
The core idea is simple, and is reasonable when
we look at machine learning; in many cases we
use training instances of not only something pos-
itive but something negative to make the distinc-
tion of the two sides clearer. Similarly, in our
task we use features of not only associated but
unassociated to make computation of similarity
(or distance in semantic space) clearer. We as-
38
sert in this work that a feature that has similar
weight to two given words also plays important
role, regardless of how much it is associated to
the given words.
Among several future works we need to fur-
ther explore reduction of features. It is reported
by some literature such as Hagiwara et al (2006)
that we can reduce so many features while pre-
serving the same accuracy in distributional sim-
ilarity calculation. This implies that, some of
them are still harmful and are expected to be re-
duced further.
List of Tools and Resources
1. Chasen, a morphological analyzer,
Ver.2.3.3. Matsumoto Lab., Nara Institute
of Science and Technology. http://chasen-
legacy.sourceforge.jp/
2. IPADIC, a dictionary for morphologi-
cal analyzer. Ver.2.7.0. Information-
Technology Promotion Agency, Japan.
http://sourceforge.jp/projects/ipadic/
3. Bunrui Goihyo, a word list by semantic
principles, revised and enlarged edi-
tion. The National Institute for Japanese
Language. http://www.kokken.go.jp
/en/publications/bunrui goihyo/
4. Nihon Keizai Shimbun Newspaper Corpus,
years 1990-2004, Nihon Keizai Shimbun,
Inc.
References
Dagan, Ido, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based Models of Co-occurrence Proba-
bilities. Machine Learning, 34(1-3):43?69.
Grefenstette, Gregory. 1994. Exploration in Auto-
matic Thesaurus Discovery. Kluwer Academic
Publishers. Norwell, MA.
Hagiwara, Masato, Yasuhiro Ogawa, Katsuhiko
Toyama. 2006. Selection of Effective Contextual
Information for Automatic Synonym Acquisition.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pp.353?360.
Harris, Zelig S. 1968. Mathematical Structures of
Language. Wiley, New Jersey.
Hindle, Donald. 1990. Noun Classification from
Predicate-Argument Structures. In Proceedings
of the 28th Annual Meeting of the Association for
Computational Linguistics, pp.268?275.
Lee, Lillian. 1997. Similarity-Based Approaches to
Natural Language Processing. Ph.D. thesis, Har-
vard University, Cambridge, MA.
Lee, Lillian. 1999. Measures of distributional simi-
larity. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics,
pp. 25?32, College Park, MD.
Lin, Dekang. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of the 36th
Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Confer-
ence on Computational Linguistics, pp.768?774.
Montreal.
Lin, Dekang and and Patrick Pantel. 2002. Con-
cept Discovery from Text. In Proceedings of 19th
International Conference on Computational Lin-
guistics, pp.577?583. Taipei.
Ruge, Gerda. 1992. Experiments of Linguistically-
based Term Associations. Information Processing
& Management, 28(3):317?332.
Shibata, Tomohide and Sadao Kurohashi. 2009. Dis-
tributional similarity calculation using very large
scale Web corpus. In Proceedings of Annual Meet-
ing of Association for Natural Language Process-
ing. pp. 705?708.
Weeds, Julie and David Weir. 2005. Co-occurrence
retrieval: A Flexible Framework for Lexical Dis-
tributional Similarity. Computational Linguistics.
31(4):439?476.
Zhitomirsky-Geffet, Maayan and Ido Dagan. 2009.
Bootstrapping Distributional Feature Vector Qual-
ity. Computational Linguistics, 35(3):435?461.
39
