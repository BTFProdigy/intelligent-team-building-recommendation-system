Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 73?78,
Baltimore, Maryland USA, June 23-24, 2014.
c
?2014 Association for Computational Linguistics
A Rule-Augmented Statistical Phrase-based Translation System
Cong Duy Vu Hoang
?
, AiTi Aw
?
and Nhung T. H. Nguyen
??
?
Human Language Technology Dept.
Institute for Infocomm Research (I
2
R), A*STAR, Singapore
{cdvhoang, aaiti}@i2r.a-star.edu.sg
?
School of Information Science
Japan Advanced Institute of Science and Technology (JAIST), Japan
nthnhung@jaist.ac.jp
Abstract
Interactive or Incremental Statistical Ma-
chine Translation (IMT) aims to provide a
mechanism that allows the statistical mod-
els involved in the translation process to be
incrementally updated and improved. The
source of knowledge normally comes from
users who either post-edit the entire trans-
lation or just provide the translations for
wrongly translated domain-specific termi-
nologies. Most of the existing work on
IMT uses batch learning paradigm which
does not allow translation systems to make
use of the new input instantaneously. We
introduce an adaptive MT framework with
a Rule Definition Language (RDL) for
users to amend MT results through trans-
lation rules or patterns. Experimental re-
sults show that our system acknowledges
user feedback via RDL which improves
the translations of the baseline system on
three test sets for Vietnamese to English
translation.
1 Introduction
In current Statistical Machine Translation (SMT)
framework, users are often seen as passive con-
tributors to MT performance. Even if there is a
collaboration between the users and the system, it
is carried out in a batch learning paradigm (Ortiz-
Martinez et al., 2010), where the training of the
SMT system and the collaborative process are car-
ried out in different stages. To increase the produc-
tivity of the whole translation process, one has to
incorporate human correction activities within the
translation process. Barrachina et al. (2009) pro-
posed an iterative process in which the translator
activity is used by the system to compute its best
?
Work done during an internship at I
2
R, A*STAR.
(or n-best) translation suffix hypotheses to com-
plete the prefix. Ortiz-Martinez et al. (2011) pro-
posed an IMT framework that includes stochas-
tic error-correction models in its statistical formal-
ization to address the prefix coverage problems
in Barrachina et al. (2009). Gonzalez-Rubio et
al. (2013) proposed a similar approach with a spe-
cific error-correction model based on a statistical
interpretation of the Levenshtein distance (Leven-
shtein, 1966). On the other hand, Ortiz-Martinez
et al. (2010) presented an IMT system that is able
to learn from user feedback by incrementally up-
dating the statistical models used by the system.
The key aspect of this proposed system is the use
of HMM-based alignment models trained by an in-
cremental EM algorithm.
Here, we present a system similar to Ortiz-
Martinez et al. (2010). Instead of updating the
translation model given a new sentence pair, we
provide a framework for users to describe trans-
lation rules using a Rule Definition Language
(RDL). Our RDL borrows the concept of the rule-
based method that allows users to control the
translation output by writing rules using their lin-
guistic and domain knowledge. Although statis-
tical methods pre-dominate the machine transla-
tion research currently, rule-based methods are
still promising in improving the translation qual-
ity. This approach is especially useful for low
resource languages where large training corpus
is not always available. The advantage of rule-
based methods is that they can well handle par-
ticular linguistic phenomena which are peculiar to
languages and domains. For example, the TCH
MT system at IWSLT 2008 (Wang et al., 2008)
used dictionary and hand-crafted rules (e.g. regu-
lar expression) to process NEs. Their experiments
showed that handling NE separately (e.g., person
name, location name, date, time, digit) results in
translation quality improvement.
In this paper, we present an adaptive and in-
73
Figure 1: The proposed rule-augmented SMT
framework.
teractive MT system that allows users to correct
the translation and integrate the adaptation into
the next translation cycle. Our experiments show
that the system is specifically effective in han-
dling translation errors related to out of vocabulary
words (OOVs), language expressions, name enti-
ties (NEs), abbreviations, terminologies, idioms,
etc. which cannot be easily addressed in the ab-
sence of in-domain parallel data.
2 System Overview
Figure 1 shows the translation and interactive pro-
cess of our system. The system is trained with a
batch of parallel texts to create a baseline model.
Users improve the translation by adding RDL
rules to change or correct the unsatisfactory trans-
lation. New RDL rules are tested in a working
environment before uploading to the production
environment where they would be used by subse-
quent translation requests.
In our system, RDL Management checks, vali-
dates and indexes the translation rules. The Rule-
Augmented Decoder has two components: (1) the
RDL Matcher to find applicable RDL rules for a
given source text to create dynamic translation hy-
potheses; and (2) the Augmented Decoder to pro-
duce the final consensus translation using both dy-
namic hypotheses and static hypotheses from the
baseline model.
3 Rule Definition Language (RDL)
The Rule Definition Language (RDL) comprises a
RDL grammar, a RDL parser and a RDL matching
algorithm.
3.1 RDL Grammar
Our RDL grammar is represented with a Backus-
Naur Form (BNF)s syntax. The major feature of
Node Type Description
Token Any string of characters in the defined
basic processing unit of the language.
String A constant string of characters.
Identifier A term represents a pre-defined role
(e.g. integer, date, sequence, . . . ).
Meta-node A term executes a specific function
(e.g. casing, selection/option, con-
nection).
Context cue A term describes source context?s ex-
istence.
Function A term executes a pre-defined task.
Table 1: A brief description of RDL nodes.
Figure 2: An Example of RDL Rule.
RDL grammar is the support of pre-defined identi-
fiers and meta-operators which go beyond the nor-
mal framework of regular expression. We also
included a set of pre-defined functions to further
constraint the application and realization of the
rules. This framework allows us to incorporate
semantic information into the rule definition and
derive translation hypotheses using both semantic
and lexical information. A RDL rule is identified
by a unique rule ID and five constituents, includ-
ing Source pattern, rule Condition, Target transla-
tion, Reordering rule and user ConFidence. The
source pattern and target translation can be con-
structed using different combination of node types
as described in Table 1. The rules can be further
conditioned by using some pre-defined functions
and the system allows users to reorder the transla-
tion of the target node. Figure 2 gives an example
of a RDL rule where identifier @Num is used.
3.2 RDL Parsing and Indexing
The RDL Parser checks the syntax of the rules
before indexing and storing them into the rule
database. We utilize the compiler generator (WoB
et al., 2003) to generate a RDL template parser and
then embed all semantic parsing components into
the template to form our RDL Parser.
As rule matching is performed during transla-
tion, searching of the relevant rules have to be very
fast and efficient. We employed the modified ver-
sion of an inverted index scheme (Zobel and Mof-
fat, 2006) for our rule indexing. The algorithm is
74
Figure 3: A linked item chain for a rule source
(@a @b [c] [?d e?] [?f g h?] (?i? | ?j k?)).
represented in Algorithm 1.
Data: ruleID & srcPatn
Result: idxTbl
// To build data structure ? Forward Step
doForward(srcPatn, linkedItmChain);
// To create index table ? Backward Step
doBackward(linkedItmChain, ruleID, idxTbl);
Algorithm 1: Algorithm for RDL rule indexing.
The main idea of the rule indexing algorithm is
to index all string-based nodes in the source pat-
tern of the RDL rule. Each node is represented
using 3-tuple. They are ruleID, number of nodes
in source pattern and all plausible positions of the
node during rule matching. The indexing is car-
ried out via a Forward Step and Backward Step.
The Forward Step builds a linked item chain which
traverses all possible position transitions from one
node to another as illustrated in Figure 3. Note that
S and E are the Start and End Node. The link indi-
cates the order of transition from a node to another.
The numbers refer to the possible positions of an
item in source. The Backward Step starts at the
end of the source pattern; traverses back the link
to index each node using the 3-tuple constructed
in the Forward Step. This data structure allows us
to retrieve, add or update RDL rules efficiently and
incrementally without re-indexing.
3.3 RDL Matching Algorithm
Each word in the source string will be matched
against the index table to retrieve relevant RDL
rules during decoding. The aim is to retrieve all
RDL rules in which the word is used as part of
the context in the source pattern. We sort all the
rules based on the word positions recorded dur-
ing indexing, match their source patterns against
the input string within the given span, check the
conditions and generate the hypotheses if the rules
fulfill all the constraints.
4 Rule-Augmented Decoder
The rule-augmented decoder integrates the dy-
namic hypotheses generated during rule match-
ing with the baseline hypotheses during decoding.
Given a sentence f from a source language F, the
fundamental equation of SMT (Brown et al., 1993)
to translate it into a target sentence e of a target
language E is stated in Equation 1.
e
best
= argmax
e
P
r
(e|f)
= argmax
e
P
r
(f |e)P
r
(e)
= argmax
e
N
?
n=1
?
n
h
n
(e, f)
(1)
Here, P
r
(f |e) is approximated by a translation
model that represents the correlation between the
source and the target sentence and P
r
(e) is ap-
proximated by a language model presenting the
well-formedness of the candidate translation e.
Most of the SMT systems follow a log-linear ap-
proach (Och and Ney, 2002), where direct mod-
elling of the posterior probabilityP
r
(f |e) of Equa-
tion 1 is used. The decoder searches for the best
translation given a set of model h
m
(e, f) by max-
imizing the log-linear feature score (Och and Ney,
2004) as in Equation 1.
For each hypothesis generated by the RDL rule,
an appropriate feature vector score is needed to en-
sure that it will not disturb the probability distribu-
tion of each model and contributes to hypothesis
selection process of SMT decoder.
4.1 Model Score Estimation
The aim of the RDL implementation is to address
the translation of language-specific expressions
(such as date-time, number, title, etc.) and do-
main-specific terminologies. Sometimes, transla-
tion rules and bilingual phrases can be easily ob-
served and obtained from experienced translators
or linguists. However, it is difficult to estimate the
probability of the RDL rules manually to reflect
the correct word or phrase distribution in real data.
Many approaches have been proposed to solve the
OOV problem and estimate word translation prob-
abilities without using parallel data. Koehn et
al. (2000) estimated word translation probabilities
from unrelated monolingual corpora using the EM
algorithm. Habash et al. (2008) presented differ-
ent techniques to extend the phrase table for on-
line handling of OOV. In their approach, the ex-
tended phrases are added to the baseline phrase
75
table with a default weight. Arora et al. (2008)
extended the phrase table by adding new phrase
translations for all source language words that do
not have a single-word entry in the original phrase-
table, but appear in the context of larger phrases.
They adjusted the probabilities of each entry in the
extended phase table.
We performed different experiments to estimate
the lexical translation feature vector for each dy-
namic hypothesis generated by our RDL rules. We
obtain the best performance by estimating the fea-
ture vector score using the baseline phrase table
through context approximation. For each hypoth-
esis generated by the RDL rule, we retrieve en-
tries from the phrase table which have at least one
similar word with the source of the generated hy-
pothesis. We sort the entries based on the sim-
ilarities between the generated and retrieved hy-
potheses using both source and target phrase. The
medium score of the sorted list is assigned to the
generated hypothesis.
5 System Features
The main features of our system are (1) the flexi-
bilities provided to the user to create different lev-
els of translation rules, from simple one-to-one
bilingual phrases to complex generalization rules
for capturing the translation of specific linguis-
tic phenomena; and (2) the ability to validate and
manage translation rules online and incrementally.
5.1 RDL Rule Management
Our system framework is language independent
and has been implemented on a Vietnamese to En-
glish translation project. Figure 4 shows the RDL
Management Screen where a user can add, mod-
ify or delete a translation rule using RDL. A RDL
rule can be created using nodes. Each node can
be defined using string or system predefined meta-
identifiers with or without meta-operators as de-
scribed in Table 1. Based on the node type selected
by the user, the system further restricts the user to
appropriate conditions and translation functions.
The user can define the order of the translation out-
put of each node and at the same time, inform the
system whether to use a specific RDL exclusively
during decoding, in which any phrases from the
baseline phrase table overlapping with that span
will be ignored
1
. The system also provides an edi-
1
Similar to Moses XML markup exclusive feature
http://www.statmt.org/moses/?n=Moses.
Figure 4: RDL Management screen with identi-
fiers & meta-functions supported.
tor for expert users to code the rules using the RDL
controlled language. Each rule is validated by the
RDL parser (discussed in section 3.2), which will
display errors or warning messages when an in-
valid syntax is encountered.
5.2 RDL Rule Validation
Our decoder manages two types of phrase table.
One is the static phrase-table obtained through
the SMT training in parallel texts; the other is
the dynamic table that comprises of the hypothe-
ses generated on-the-fly during RDL rule match-
ing. To ensure only fully tested rules are used in
the production environment, the system supports
two types of dynamic phrase table. The work-
ing phrase-table holds the latest updates made by
the users. The users can test the translation with
these latest modifications using a specific transla-
tion protocol. When users are satisfied with these
modifications, they can perform an operation to
upload the RDL rules to the production phrase-
table, where the RDLs are used for all translation
AdvancedFeatures#ntoc9
76
Named Entity Category Number of Rules
Date-time 120
Measurement 92
Title 13
Designation 12
Number 19
Terminology 178
Location 13
Organization 48
Total 495
Table 2: Statistics of created RDL rules for
Vietnamese-to-English NE Translation.
requests. Uploaded rules can be deleted, modified
and tested again in the working environment be-
fore updated to the production environment. Fig-
ure 5b and Figure 5c show the differences in trans-
lation output before and after applied the RDL rule
in Figure 5a.
6 A Case Study for Vietnamese?English
Translation
We performed an experiment using the proposed
RDL framework for a Vietnamese to English
translation system. As named entity (NE) con-
tributes to most of the OOV occurrences and im-
pacts the system performance for out-of-domain
test data in our system, we studied the NE usage
in a large Vietnamese monolingual corpus com-
prising 50M words to extract RDL rules. We cre-
ated RDL rules for 8 popular NE types including
title, designation, date-time, measurement, loca-
tion, organization, number and terminology. We
made use of a list of anchor words for each NE
category and compiled our RDL rules based on
these anchor words. As a result, we compiled a
total of 495 rules for 8 categories and it took about
3 months for the rule creation. Table 2 shows the
coverage of our compiled rules.
6.1 Experiment & Results
Our experiments were performed on a training set
of about 875K parallel sentences extracted from
web news and revised by native linguists over 2
years. The corpus has 401K and 225K unique En-
glish and Vietnamese tokens. We developed 1008
and 2548 parallel sentences, each with 4 refer-
ences, for development and testing, respectively.
All the reference sentences are created and revised
by different native linguists at different times. We
also trained a very large English language model
using data from Gigaword, Europarl and English
Figure 5: Translation Demo with RDL rules.
Data Set nS nT nMR
TrainFull (VN) 875,579 28,251,775 627,125
TrainFull (EN) 875,579 20,191,526 -
Test1 (VN) 1009 34,717 737
Test1 (4 refs) (EN) 1009 ?25,713 -
Test2 (VN) 1033 29,546 603
Test2 (4 refs) (EN) 1033 ?22,717 -
Test3 (VN) 506 16,817 344
Test3 (4 refs) (EN) 506 ?12,601 -
Dev (VN) 1008 34,803 -
Dev (4 refs) (EN) 1008 ?25,631 -
Table 3: Statistics of Vietnamese-to-English paral-
lel data. nS, nT, and nMR are number of sentence
pairs and tokens, and count of matched rules, re-
spectively.
web texts of Vietnamese authors to validate the
impact of RDL rules on large-scale and domain-
rich corpus. The experimental results show that
created RDL rules improve the translation perfor-
mance on all 3 test sets. Table 3 and Table 4 show
respective data statistics and results of our evalua-
tion. More specifically, the BLEU scores increase
3%, 3.6% and 1.4% on the three sets, respectively.
7 Conclusion
We have presented a system that provides a con-
trol language (Kuhn, 2013) specialized for MT for
users to create translation rules. Our RDL differs
from Moses?s XML mark-up in that it offers fea-
77
Data Set System BLEU NIST METEOR
Set 1 Baseline 39.21 9.2323 37.81
+RDL (all) 39.51 9.2658 37.98
Set 2 Baseline 40.25 9.5174 38.24
+RDL (all) 40.61 9.6092 38.84
Set 3 Baseline 36.77 8.6953 37.65
+RDL (all) 36.91 8.7062 37.69
Table 4: Experimental results with RDL rules.
tures that go beyond the popular regular expres-
sion framework. Without restricting the mark-up
on the source text, we allow multiple translations
to be specified for the same span or overlapping
span.
Our experimental results show that RDL
rules improve the overall performance of the
Vietnamese-to-English translation system. The
framework will be tested for other language pairs
(e.g. Chinese-to-English, Malay-to-English) in
the near future. We also plan to explore advanced
methods to identify and score ?good? dynamic
hypotheses on-the-fly and integrate them into cur-
rent SMT translation system (Simard and Foster,
2013).
Acknowledgments
We would like to thank the reviewers of the paper
for their helpful comments.
References
Paul M. Sumita E. Arora, K. 2008. Translation
of unknown words in phrase-based statistical ma-
chine translation for languages of rich morphol-
ogy. In In Proceedings of the Workshop on Spoken
Language Technologies for Under-Resourced Lan-
guages, SLTU 2008.
Sergio Barrachina, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi,
Antonio Lagarda, Hermann Ney, Jes?us Tom?as, En-
rique Vidal, and Juan-Miguel Vilar. 2009. Sta-
tistical approaches to computer-assisted translation.
Comput. Linguist., 35(1):3?28, March.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263?
311, June.
Jes?us Gonz?alez-Rubio, Daniel Ort??z-Martinez, Jos?e-
Miguel Bened??, and Francisco Casacuberta. 2013.
Interactive machine translation using hierarchical
translation models. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 244?254, Seattle, Washington,
USA, October.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In Proceedings of
ACL: Short Papers, HLT-Short ?08, pages 57?60,
Stroudsburg, PA, USA.
Philipp Koehn and Kevin Knight. 2000. Estimating
word translation probabilities from unrelated mono-
lingual corpora using the em algorithm. In Proceed-
ings of the Seventeenth National Conference on Ar-
tificial Intelligence and Twelfth Conference on Inno-
vative Applications of Artificial Intelligence, pages
711?715. AAAI Press.
Tobias Kuhn. 2013. A survey and classification of con-
trolled natural languages. Computational Linguis-
tics.
VI Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet
Physics Doklady, 10:707.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In In Proceedings of
ACL, pages 295?302, Stroudsburg, PA, USA.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417?449, Decem-
ber.
Daniel Ortiz-Martinez, Ismael Garcia-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for inter-
active statistical machine translation. In In Proceed-
ings of NAACL, HLT ?10, pages 546?554, Strouds-
burg, PA, USA.
Daniel Ortiz-Mart??nez, Luis A. Leiva, Vicent Alabau,
Ismael Garc??a-Varea, and Francisco Casacuberta.
2011. An interactive machine translation system
with online learning. In In Proceedings of ACL:
Systems Demonstrations, HLT ?11, pages 68?73,
Stroudsburg, PA, USA.
Michel Simard and George Foster. 2013. Pepr: Post-
edit propagation using phrase-based statistical ma-
chine translation. Proceedings of the XIV Machine
Translation Summit, pages 191?198.
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi Liu,
Jianfeng Li, Dengjun Ren, and Zhengyu Niu. 2008.
The tch machine translation system for iwslt 2008.
In In Proceedings of IWSLT 2008, Hawaii, USA.
Albrecht WoB, Markus Loberbauer, and Hanspeter
Mossenbock. 2003. Ll(1) conflict resolution in a
recursive descent compiler generator. In Modular
Programming Languages, volume 2789 of Lecture
Notes in Computer Science, pages 192?201.
Justin Zobel and Alistair Moffat. 2006. Inverted files
for text search engines. ACM Comput. Surv., 38,
July.
78
Proceedings of BioNLP Shared Task 2011 Workshop, pages 94?101,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Extracting Bacteria Biotopes with Semi-supervised Named Entity
Recognition and Coreference Resolution
Nhung T. H. Nguyen and Yoshimasa Tsuruoka
School of Information Science
Japan Advanced Institute of Science and Technology
1-1 Asahidai, Nomi, Ishikawa 923-1292 Japan
{nthnhung,tsuruoka}@jaist.ac.jp
Abstract
This paper describes our event extraction sys-
tem that participated in the bacteria biotopes
task in BioNLP Shared Task 2011. The sys-
tem performs semi-supervised named entity
recognition by leveraging additional informa-
tion derived from external resources including
a large amount of raw text. We also perform
coreference resolution to deal with events hav-
ing a large textual scope, which may span over
several sentences (or even paragraphs). To
create the training data for coreference resolu-
tion, we have manually annotated the corpus
with coreference links. The overall F-score of
event extraction was 33.2 at the official eval-
uation of the shared task, but it has been im-
proved to 33.8 thanks to the refinement made
after the submission deadline.
1 Introduction
In this paper, we present a machine learning-based
approach for bacteria biotopes extraction of the
BioNLP Shared Task 2011 (Bossy et al , 2011).
The task consists of extracting bacteria localization
events, namely, mentions of given species and the
place where it lives. Places related to bacteria lo-
calization events range from plant or animal hosts
for pathogenic or symbiotic bacteria to natural envi-
ronments like soil or water1. This task also targets
specific environments of interest such as medical en-
vironments (hospitals, surgery devices, etc.), pro-
cessed food (dairy) and geographical localizations.
1https://sites.google.com/site/bionlpst/
home/bacteria-biotopes
The task of extracting bacteria biotopes involves
two steps: Named Entity Recognition (NER) and
event detection. The current dominant approach to
NER problems is to use supervised machine learning
models such as Maximum Entropy Markov Models
(MEMMs), Support Vector Machines (SVMs) and
Conditional Random Fields (CRFs). These models
have been shown to work reasonably well when a
large amount of training data is available (Nadeau
and Sekine, 2007). However, because the anno-
tated corpus delivered for this particular subtask in
the shared task is very small (78 documents with
1754 sentences), we have decided to use a semi-
supervised learning method in our system. Our NER
module uses a CRF model with enhanced features
created from external resources. More specifically,
we use additional features created from the output
of HMM clustering performed on a large amount of
raw text, and word senses from WordNet for tag-
ging.
The target events in this shared task are divided
into two types. The first is Localization events
which relates a bacterium to the place where it lives.
The second is PartOf events which denotes an or-
gan that belongs to an organism. As in Bossy et
al. (2010), the largest possible scope of the men-
tion of a relation is the whole document, and thus
it may span over several sentences (or even para-
graphs). This observation motivated us to perform
coreference resolution as a pre-processing step, so
that each event can be recognized within a narrower
textual scope. There are two common approaches to
coreference resolution: one mainly relies on heuris-
tics, and the other employs machine learning. Some
94
instances of the heuristics-based approach are de-
scribed in (Harabagiu et al, 2001; Markert and
Nissim, 2005; Yang and Su, 2007), where they
use lexical and encyclopedic knowledge. Machine
learning-based methods (Soon and Ng, 2001; Ng
and Cardie, 2002; Yang et al , 2003; Luo et al
, 2004; Daume and Marcu, 2005) train a classi-
fier or search model using a corpus annotated with
anaphoric pairs. In our system, we employ the sim-
ple supervised method presented in Soon and Ng
(2001). To create the training data, we have man-
ually annotated the corpus with coreference infor-
mation about bacteria.
Our approach, consequently, has three processes:
NER, coreference resolution of bacterium entities,
and event extraction. The latter two processes can be
formulated as classification problems. Coreference
resolution is to determine the relation between can-
didate noun phrases and bacterium entities, and the
event extraction is to detect the relation between two
entities. It should be noted that our official submis-
sion in the shared task was carried out without using
a coreference resolution module, and the system has
been improved after the submission deadline.
Our contribution in this paper is two-fold. In the
methodology aspect, we use an unsupervised learn-
ing method to create additional features for the CRF
model and perform coreference resolution to narrow
the scope of events. In the resource aspect, the man-
ual annotations for training our coreference resolu-
tion module will be made available to the research
community.
The remainder of this paper is organized as fol-
lowed. Section 2, 3 and 4 describe details about the
implementation of our system. Section 5 presents
the experimental results with some error analysis.
Finally, we conclude our approach and discuss fu-
ture work in section 6.
2 Semi-supervised NER
According to the task description, the NER task
consists of detecting the phrases that denote bacte-
rial taxon names and localizations which are bro-
ken into eight types: Host, HostPart, Geographical,
Food, Water, Soil, Medical and Environment. In
this work, we use a CRF model to perform NER.
CFRs (Lafferty et. al., 2001) are a sequence model-
ing framework that not only has all the advantages
of MEMMs but also solves the label bias problem
in a principled way. This model is suitable for la-
beling sequence data, especially for NER. Based on
this model, our CRF tagger is trained with a stochas-
tic gradient descent-based method described in Tsu-
ruoka et al (2009), which can produce a compact
and accurate model.
Due to the small size of the training corpus and
the complexity of their category, the entities cannot
be easily recognized by standard supervised learn-
ing. Therefore, we enhance our learning model by
incorporating related information from other exter-
nal resources. On top of the lexical and syntactic
features, we use two additional types of information,
which are expected to alleviate the data sparseness
problem. In summary, we use four types of features
including lexical and syntactic features, word clus-
ter and word sense features as the input for the CRF
model.
2.1 Word cluster features
The idea of enhancing a supervised learning model
with word cluster information is not new. Kamaza
et. al. (2001) use a hidden Markov model (HMM)
to produce word cluster features for their maximum
entropy model for part-of-speech tagging. Koo et al
(2008) implement the Brown clustering algorithm
to produce additional features for their dependency
parser. For our NER task, we use an HMM to pro-
duce word cluster features for our CRF model.
We employed an open source library2 for learn-
ing HMMs with the online Expectation Maximiza-
tion (EM) algorithm proposed by Liang and Klein
(2009). The online EM algorithm is much more ef-
ficient than the standard batch EM algorithm and al-
lows us to use a large amount of data. For each hid-
den state, words that are produced by this state with
the highest probability are written. We use this result
of word clustering as a feature for NER. The optimal
number of hidden states is selected by evaluating its
effectiveness on NER using the development set.
To prepare the raw text for HMM clustering, we
downloaded 686 documents (consisting of both full
documents and abstracts) about bacteria biotopes
2http://www-tsujii.is.s.u-tokyo.ac.jp/
?hillbig/ohmm.htm
95
Figure 1: Sample of HMM clustering result.
from MicrobeWiki, JGI Genome Portal, Genoscope,
2Can bacteria pages at EBI and NCBI Genome
Project (the training corpus is also downloaded from
these five webpages). In addition, we use the
100,000 latest MEDLINE abstracts containing the
string ?bacteri? in our clustering. In total, the raw
text consists of more than 100,000 documents with
more than 2 million sentences.
A part of the result of HMM clustering is shown
in Figure 1. According to this result, the word ?Bi-
fidobacterium? belongs to cluster number 9, and its
feature value is ?Cluster-9?. The word cluster fea-
tures of the other words are extracted in the same
way.
2.2 Word sense features
We used WordNet to produce additional features on
word senses. Although WordNet3 is a large lexi-
cal database, it only comprises words in the general
genre, to which only the localization entities belong.
Since it does not contain the bacterial taxon names,
the most important entities in this task, we used an-
other dictionary for bacteria names. The dictionary
was extracted from the genomic BLAST page of
NCBI 4. To connect these two resources, we simply
place all entries from the NCBI dictionary under the
?bacterium? sense of WordNet. Table 1 illustrates
some word sense features employed in our model.
2.3 Pre-processing for bacteria names
In biomedical documents, the bacteria taxon names
are written in many forms. For example, they are
3http://wordnet.princeton.edu/
4http://www.ncbi.nlm.nih.gov/sutils/
genom_table.cgi
Word POS Sense
chromosome NN body
colonize VBP social
detected VBN perception
fly NN animal
gastrointestinal JJ pert
infant NN person
longum FW bacterium
maintaining VBG stative
milk NN food
onion NN plant
proterins NNS substance
USA NNP location
Table 1: Sample of word sense features given by Word-
Net and NCBI dictionary.
presented in a full name like ?Bacillius cereus?, or
in a short form such as ?B. cereus?, or even in an ab-
breviation as ?GSB? (green sulfur bacteria). More-
over, the bacteria names are often modified with
some common strings such as ?strain?, ?spp.?, ?sp.?,
etc. ?Borrelia hermsii strain DAH?, ?Bradyrhizo-
bium sp. BTAi1?, and ?Spirochaeta spp.? are ex-
amples of this kind. In order to tackle this prob-
lem, we apply a pre-processing step before NER. Al-
though there are many previous studies solving this
kind of problem, in our system, we apply a simple
method for this step.
? Retrieving the full form of bacteria names. We
assume that (a) both short form and full form
must occur in the same document; (b) a token
is considered as an abbreviation if it is writ-
ten in upper case and its length is shorter than
4 characters. When a token satisfies condition
(b) (which means it is an abbreviation), the pro-
cessing retrieves its full form by identifying all
sequences containing tokens initialized by its
abbreviated character. In case of short form
like ?B. cereus?, the selected sequence must in-
clude the right token (which is ?cereus? in ?B.
cereus?).
? Making some common strings transparent. As
our observation on the training data, there are
8 common strings in bacteria names, including
?strain?, ?str?, ?str.?, ?subsp?, ?spp.?, ?spp?,
?sp.?, ?sp?. All of these strings will be removed
before NER and recovered after that.
96
3 Coreference Resolution as Binary
Classification
Coreference resolution is the process of determin-
ing whether different nominal phrases are used to
refer to the same real world entity or concept. Our
approach basically follows the learning method de-
scribed in Soon and Ng (2001). In this approach,
we build a binary classifier using the coreferring en-
tities in the training corpus. The classifier takes a
pair of candidates and returns true if they refer to
the same real world entity and false otherwise. In
this paper, we limit our module to detecting the bac-
teria?s coreference, and hence the candidates consist
of noun phrases (NPs) (starting by a determiner),
pronouns, possessive adjective and name of bacte-
ria.
In addition to producing the candidates, the pre-
processing step creates a set of features for each
anaphoric pair. These features are used by the clas-
sifier to determine if two candidates have a corefer-
ence relation or not.
The following features are extracted from each
candidate pair.
? Pronoun: 1 if one of the candidates is a pro-
noun; 0 otherwise.
? Exact or Partial Match: 1 if the two strings of
the candidates are identical, 2 if they are partial
matching; 0 otherwise.
? Definite Noun Phrase: 1 if one of the candi-
dates is a definite noun phrases; 0 otherwise.
? Demonstrative Noun Phrase: 1 if one of the
candidates is a demonstrative noun phrase; 0
otherwise.
? Number Agreement: 1 if both candidates are
singular or plural; 0 otherwise.
? Proper Name: 1 if both candidates are bac-
terium entities or proper names; 0 otherwise.
? Character Distance: count the number of the
characters between two candidates.
? Possessive Adjective: 1 if one of the candidates
is possessive adjective; 0 otherwise.
Figure 2: Example of annotating coreference resolution.
T16 is a bacterium which is delivered in *.a2 file, T24
and T25 are anaphoric expressions. There are two coref-
erence relations of T16 and T24, T16 and T25.
? Exist in Coreference Dictionary: 1 if the candi-
date exists in the dictionary extracted from the
training data; 0 otherwise. This feature aims to
remove noun phrases which are unlikely to be
related to the bacterium entities.
The first five features are exactly the same as those
in Soon and Ng (2001), while the others are refined
or added to make it suitable for our specific task.
In the testing phase, we used the best-first
clustering as in Ng and Cardie (2002). Rather
than performing a right-to-left search from each
anaphoric NP for the first coreferent NP, a right-to-
left search for a highly likely antecedent was per-
formed. Hence, the classifier was modified to select
the antecedent of NP with the coreference likelihood
score above a threshold. This threshold was tuned by
evaluating it on the development set.
3.1 Corpus annotation
To create the training data for coreference resolu-
tion, we have manually annotated the corpus based
on the gold-standard named entity annotations deliv-
ered by the organizer. Due to our decision to focus
on bacteria names, only the coreference of these en-
tities are labeled. We use a format similar to those of
the organizer, i.e. the standoff presentation and text-
bound annotations. The coreference annotation file
consists of two parts, one part for anaphoric expres-
sions and the other for coreference relation. Figure 2
shows an example of a coreference annotation with
the original text.
97
4 Event Extraction
The bacteria biotopes, as mentioned earlier, are di-
vided into two types. The first type of events,
namely localization events, relates a bacterium to
the place where it lives, and has two mandatory ar-
guments: a Bacterium type and a localization type.
The second type of events, i.e. PartOf events, de-
note an organ that belongs to an organism, and has
two mandatory arguments of type HostPart and Host
respectively. We view this step as determining the
relationship between two specific entities. Because
of no ambiguity between the two types of event, the
event extraction can be solved as the binary classifi-
cation of pairs of entities. The classifier is trained on
the training data with four types of feature extracted
from the context between two entities: distance in
sentences, the number of entities, the nearest left and
right verbs.
Generating Training Examples. Given the
coreference information on bacterium entities, the
system considers all the entities belonging to the
coreference chains as real bacteria and generates
event instances. Since about 96% of all annotated
events occur in the same paragraph, we restrict our
method to detecting events within one paragraph.
? Localization Event. The system creates a rela-
tionship between a bacterium and a localization
entity with minimum distance between them
by the following priorities:
(1) The bacterium precedes the localization en-
tity in the same sentence.
(2) The bacterium precedes the localization en-
tity in the same paragraph.
? PartOf Event. All possible relationships be-
tween Host and HostPart entities are generated
if they are in the same paragraph.
5 Experiments and Discussion
The training and evaluation data used in these exper-
iments are provided by the shared task organizers.
The token and syntactic information are extracted
from the supporting resources (Stenetorp et. al. ,
2011). More detail, the tokenized text was done by
GENIA tools, and the syntactic analyses was cre-
ated by the McClosky-Charinak parser (McClosky
Experiment Acc. Pre. Re. F-score
Baseline 94.28 76.32 35.51 48.47
Word cluster 94.46 78.23 39.59 52.57
Word sense 94.63 74.15 44.49 55.61
All Features 94.70 77.62 45.31 57.22
Table 2: Performance of Named Entity Recognition in
terms of Accuracy, Precision, Recall and F-score with
different features on the development set.
and Charniak, 2008), trained on the GENIA Tree-
bank corpus (Tateisi et al, 2005), which is one of the
most accurate parsers for biomedical documents.
For both classification of anaphoric pairs in coref-
erence resolution and determining relationship of
two entites, we used the SVMlight library 5, a state-
of-the-art classifier, with the linear kernel.
In order to find the best parameters and features
for our final system, we conducted a series of exper-
iments at each step of the approach.
5.1 Named Entity Recognition
We evaluated the impact of additional featues on
NER by running four experiments. The Baseline ex-
periment was conducted by using the original CRF
tagger, which did not use any additional features de-
rived from external resources. The other three ex-
periments were conducted by incrementally adding
more features to the CRF tagger. Table 2 shows the
results on the development set6.
Through these experiments we have realized that
using the external resources is very effective. The
word cluster and word sense features are used like
a dictionary. The first one can be considered as the
dictionary of specific classes of entity in the same
domain with this task, which mainly supports the
precision, whereas the latter is a general dictionary
boosting the recall. With regard to F-score, the word
sense features outperform the word cluster features.
When we combine all of them, the F-score is im-
proved significantly by nearly 9 points.
The detailed results of individual classes in Ta-
ble 3 show that the Environment entities are the
hardest to recognize. Because of their general char-
acteristic, these entities are often confused with Host
5http://svmlight.joachims.org/
6These scores were generated by using the CoNLL 2000
evaluation script.
98
Class Gold Pre. Re. F-score
Bacterium 86 70.00 40.23 51.09
Host 78 78.57 56.41 65.67
HostPart 44 91.67 50.00 64.71
Geographical 8 71.43 62.50 66.67
Environment 8 0.00 0.00 0.00
Food 0 N/A N/A N/A
Medical 2 100.00 50.00 66.67
Water 17 100.00 17.65 30.00
Soil 1 100.00 100.00 100.00
All 244 77.62 45.31 57.22
Table 3: Results of NER using all features on the de-
velopment set. The ?Gold? column shows the number
of entities of that class in the gold-standard corpus. The
score of Food entities is not available because there is no
positive instance in the development set.
Detection Linking
Precision 24.18 20.48
Recall 91.36 33.71
F-score 38.24 25.48
Table 4: Result of coreference resolution on the develop-
ment set achieved with gold-standard named entity anno-
tations.
or Water. In contrast, the Geographical category is
easier than the others if we have gazetteers and ad-
ministrative name lists.
5.2 Coreference Resolution
We next evaluated the accuracy of coreference reso-
lution for bacterium entities. The evaluation7 is car-
ried out in two steps: evaluation of mention detec-
tion, and evaluation of mention linking to produce
coreference links. The exact matching criterion was
used when evaluating the accuracy of the two steps.
Table 4 shows the performance of the coreference
resolution module when taking annotated entites as
input. As mentioned in section 3, the first step of this
module considers all NPs beginning with a deter-
miner and bacterium entities as candidates. There-
fore, the number of the candidate NPs is vastly larger
than that of the positive ones. This is the reason
why the precision of mention detection is low, while
the recall is high. This high recall leads to a large
number of generated linkings and raises the com-
7http://sites.google.com/site/bionlpst/
home/protein-gene-coreference-task
Experiment Pre. Re. F-score
No Coref. 42.11 27.34 33.15
With Coref. 43.40 27.64 33.77
Table 5: Comparative results of event extraction with and
without coreference information on the test set.
Type of event
Num. of addition Num. of ruled out
True False True False
Localization 17 1 6 20
PartOf 6 5 1 0
Total 29 27
Table 6: Contribution of coreference resolution to event
extraction.
plexity of linking detection. In order to obtain more
accurate results, we had to remove weak linkings
whose classification score is under 0.7 (this is the
best threshold on the development set). However, as
shown in Table 4, the performance of mention link-
ing was not satisfactory.
5.3 Event Extraction
Finally, we carried out two experiments on the test
set to investigate the effect of coreference resolution
on event extraction. The results shown in Table 5 in-
dicate that the contribution of coreference resolution
in this particular experiment is not significant. The
coreference information helps the module to add 29
more events (23 true and 6 false events) and rule out
27 events (20 false and 7 true events) compared with
the experiment with no coreference resolution. De-
tail about this contribution is presented in Table 6.
We further analyzed the result of event extraction
and found that there exist two kinds of Localization
events, which we call direct and indirect events. The
direct events are the ones that are easily recogniz-
able on the surface level of textual expressions. The
three Localization events in Figure 3 belong to this
type. Our module is able to detect most of the di-
rect events, especially when we have the coreference
information on bacteria ? it is straight-forward be-
cause the two arguments of the event occur in the
same sentence. In constrast, the indirect events
are more complicated. They appear implicitly in the
document and we need to infer them through an in-
termediate agent. For example, a bacterium causes
a disease, and this disease infects the humans or an-
99
Figure 3: Example of direct events. The solid line is the
Localization event, the dash line is the PartOf event.
Figure 4: Example of indirect events. The solid line is
the Localization event, the arrow shows the causative re-
lation.
imals. Therefore, it can be considered that the bac-
terium locates in the humans or animals. Figure 4
illustrates this case. In this example, the Bacillus
anthracis causes Anthrax, Humans contract the dis-
ease (which refers to Anthrax), and the Bacillus an-
thracis locates in Humans. These events are very
difficult to recognize since, in this context, we do
not have any information about the disease. Events
of this type provide an interesting challenge for bac-
teria biotopes extraction.
6 Conclusion and Future Work
We have presented our machine learning-based ap-
proach for extracting bacteria biotopes. The system
is implemented with modules for three tasks: NER,
coreference resolution and event extraction.
For NER, we used a CRF tagger with four types
of features: lexical and syntactic features, the word
cluster and word sense extracted from the external
resources. Although we achieved a significant im-
provement by employing WordNet and the HMM
clustering on raw text, there is still much room for
improvement. For example, because all extracted
knowledge used in this NER module belongs to the
general knowlegde, its performance is not as good as
our expectation. We envisage that the performance
of the module will be improved if we can find useful
biological features.
We have attempted to use the information ob-
tained from the coreference resolution of bacteria to
narrow the event?s scope. On the test set, although it
does not improve the system significantly, the coref-
erence information has shown to be useful in event
extraction. 8
In this work, we simply used binary classifiers
with standard features for both coreference resolu-
tion and event detection. More advanced machine
learning approaches for structured prediction may
lead to better performance, but we leave it for future
work.
References
Robert Bossy, Claire Nedellec, and Julien Jourde. 2010.
Guidelines for Annotation of Bacteria Biotopes.
Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteen
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope, In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task. Portland, Oregon, Association for Com-
putational Linguistics.
Hal Daume? III and Daniel Marcu. 2005. A Large-scale
Exploration of Effective Global Features for a Joint
Entity Detection and Tracking Model. In Proceedings
of HLT-EMNLP 2005, pp. 97-104.
Sanda M. Harabagiu, Razvan C. Bunescu and Steven J.
Maiorano. 2001. Text and Knowlegde Mining for Co-
reference Resolution. In Proceedings of NAACL 2001,
pp. 1-8.
Jun?ichi Kazama, Yusuke Miyao, and Jun?ichi Tsujii.
2001. A Maximum Entropy Tagger with Unsuper-
vised Hidden Markov Models. In Proceedings of NL-
PRS 2001, pp. 333-340.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In Pro-
ceedings of ACL-08: HLT, pp. 595-603.
John Lafferty, Andrew McCallum and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of ICML?01, pp. 282-289.
Percy Liang and Dan Klein. 2009. Online EM for Unsu-
pervised Models. In Proceedings of NAACL 2009, pp.
611-619.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla and Salim Roukos. 2004. A
Mention-Synchronous Co-reference Resolution Algo-
rithm based on the Bell Tree. In Proceedings of ACL
2004, pp. 135-142.
Katja Markert and Malvina Nissim. 2005. Comparing
Knowledge Sources for Nominal Anaphora Resolu-
tion. In Computational Linguistics, Volume 31 Issue
3, pp. 367-402.
8If you are interesting in the annotated corpus used for our
coreference resolution model, please request us by email.
100
David McClosky and Eugene Charniak. 2008. Self-
Training for Biomedical Parsing. Proceedings of the
Association for Computational Linguistics (ACL 2008,
short papers), Columbus, Ohio, pp. 101-104.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Linguisti-
cae Investigationes, Volume 30(1), pp. 326.
Vincent Ng and Claire Cardie. 2002. Improving Ma-
chine Learning Approach to Co-reference Resolution.
In Proceedings of ACL 2002, pp. 104-111.
Wee Meng Soon and Hwee Tou Ng. 2001. A Ma-
chine Learning Approach to Co-reference Resolution
of Noun Phrases. Computational Linguistics 2001,
Volume 27 Issue 4, pp. 521-544.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources.
InProceedings of the BioNLP 2011 Workshop Com-
panion Volume for Shared Task, Portland, Oregon, As-
sociation for Computational Linguistics.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta and Junichi
Tsujii. 2005. Syntax Annotation for the GENIA cor-
pus. In Proceedings of IJCNLP 2005 (Companion vol-
ume), pp. 222-227.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic Gradient Descent Training
for L1-regularized Log-linear Models with Cumulative
Penalty. In Proceedings of ACL-IJCNLP, pp. 477-485.
Xiaofeng Yang, Guodong Zhou, Jian Su and Chew Lim
Tan. 2003. Co-reference Resolution using Competi-
tion Learning Approach. In Proceedings of ACL 2003,
pp. 176-183.
Xiaofeng Yang and Jian Su. 2007. Coreference Reso-
lution Using Semantic Relatedness Information from
Automatically Discovered Patterns. In Proceedings of
ACL 2007, pp. 528-535.
101
