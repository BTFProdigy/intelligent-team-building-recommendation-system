Coling 2008: Companion volume ? Posters and Demonstrations, pages 173?176
Manchester, August 2008
Entailment-based Question Answering  
for Structured Data 
Bogdan Sacaleanu?, Constantin Orasan?, Christian Spurk?, Shiyan 
Ou?, Oscar Ferrandez?, Milen Kouylekov? and Matteo Negri?  
?LT-Lab, DFKI GmbH / Saarbr?cken, Germany 
?RIILP, University of Wolverhampton / Wolverhampton, UK 
?Fondazione Bruno Kessler (FBK) / Trento, Italy 
?University of Alicante / Alicante, Spain 
 
 
Abstract  
This paper describes a Question Answer-
ing system which retrieves answers from 
structured data regarding cinemas and 
movies. The system represents the first 
prototype of a multilingual and multi-
modal QA system for the domain of tour-
ism. Based on specially designed domain 
ontology and using Textual Entailment as 
a means for semantic inference, the sys-
tem can be used in both monolingual and 
cross-language settings with slight ad-
justments for new input languages. 
1 Introduction 
Question Answering over structured data has 
been traditionally addressed through a deep 
analysis of the question in order to reconstruct a 
logical form, which is then translated in the query 
language of the target data (Androutsopoulos et 
al, 1995, Popescu et al 2003). This approach im-
plies a complex mapping between linguistic ob-
jects (e.g. lexical items, syntactic structures) and 
against data objects (e.g. concepts and relations 
in a knowledge base). Unfortunately, such a 
mapping requires extensive manual work, which 
in many cases represents a bottleneck preventing 
the realization of large scale and portable natural 
language interfaces to structured data.  
This paper presents the first prototype of a 
question answering system which can answer 
questions in several languages about movies and 
cinema using a multilingual ontology and textual 
                                                 
 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
entailment. The remainder of the paper is struc-
tured as follows: Section 2 presents the concept 
of entailment-based question answering; Section 
3 describes our prototype which implements this 
concept; A brief evaluation is presented in Sec-
tion 4, followed by conclusions in Section 5. 
2 Entailment-based QA 
Recently Textual Entailment (TE) has been pro-
posed as a unifying framework for applied se-
mantics (Dagan and Glickman, 2004), where the 
need for an explicit representation of a mapping 
between linguistic objects and data objects can 
be, at least partially, bypassed through the defini-
tion of semantic inferences at a textual level. In 
this framework, a text (T) is said to entail a hy-
pothesis (H) if the meaning of H can be derived 
from the meaning of T. 
On the basis of the TE framework, the QA 
problem can be recast as an entailment problem, 
where the text (T) is the question (or its affirma-
tive version) and the hypothesis (H) is a rela-
tional answer pattern, which is associated to in-
structions for retrieving the answer to the input 
question. In this framework, given a question Q 
and a set of relational answer patterns P, a QA 
system needs to select those patterns in P that are 
entailed by Q. Instructions associated to answer 
patterns may be viewed as high precision proce-
dures for answer extraction, which are dependent 
on the specific source which is asked for. In case 
of QA over structured data, instructions could be 
queries to a database; whilst in case of QA on the 
Web, an instruction could be the URL of a Web 
page containing the answer to a question or some 
form of IR query to a search engine. 
Therefore, the underlying idea of an entail-
ment-based QA system is to match the user?s re-
quest to a set of predefined question patterns in 
order to get some kind of analysis for the request. 
173
As an example consider the question ?Where 
can I watch the movie ?Dreamgirls? next Satur-
day?? and the predefined question patterns: 
? Which movies are currently running in 
[CINEMA]?  EAT = [MOVIE] 
? Where can I watch the movie [MOVIE] 
on [WEEKDAY]?  EAT = [CINEMA] 
? Where can I see [MOVIE]? 
 EAT = [CINEMA] 
In the example, each of the patterns contains 
placeholders for relevant named entities and has 
an expected answer type (EAT) associated with 
it. The entailment-based QA system should re-
turn that pattern (2) is entailed by the question 
and as a result the retrieval instructions associ-
ated to it will be used to answer the question. 
3 Description of system 
Our question answering system implements the 
concept of entailment-based question answering 
described in the previous section. The overall 
structure of our system is presented in Figure 1.  
Given a question asked by a user of the system 
in a known location, the QA planner forwards it 
to the Instance Annotator in order to find any 
concepts that might be related to the targeted 
domain (i.e. cinema, city, movie). The result is 
then analyzed by the Relation Matcher, which on 
the basis of entailment can either select the most 
appropriate interpretation of the question and im-
plicitly its associated procedure of answering the 
question, or decide that the user request is out-of-
coverage if no such interpretation is available. 
The cross-linguality of our system and, to a 
certain extent, the interaction between its compo-
nents is ensured by a domain ontology which is 
used for all four languages involved in the pro-
ject: English, German, Italian and Spanish, and 
its modules (Ou et al, 2008). Concepts from the 
ontology are used to annotate the user questions 
as well as data from which the answer is ex-
tracted. In the current stage of the project, the 
answers are contained in databases obtained from 
content provides or built from structured web 
pages. As a result, the information in the database 
tables was annotated with concepts from the on-
tology and then converted into an RDF graph to 
Figure 1. System Architecture 
174
facilitate retrieval using SPARQL query lan-
guage (Prud'hommeaux and Seaborne, 2006). 
Question patterns corresponding to one or several 
ontological relations were produced after ques-
tions for users were collected and used in the en-
tailment module. The question patterns used by 
the system are very similar to those presented in 
the previous section and contain placeholders for 
the actual entities that are expected to appear in a 
question. 
The SPARQL query associated with a pattern 
selected for a user question is used to retrieve the 
answers from the knowledge base and prepare for 
presentation. Given that our system is not limited 
to returning only textual information, further 
processing can be applied to the retrieved data. 
For example, for proximity questions the list of 
answers consists of cinema names and their GPS-
coordinates, which are used by the Answer Sort-
ing component to reorder the list of answers on 
the basis of their distance to the user?s location. 
Besides presenting the possible answers to a 
given question, the system can offer additional 
information based on the answer?s type: 
? a map for answers that are location 
names, 
? a route description for answers that are 
cinema names, 
? a video-trailer for answers that are movie 
names and 
? an image for answers that are person 
names. 
Due to the fact that a common semantics is 
shared by all four languages by way of a domain 
ontology, the system can be used not only in a 
monolingual setting, but also in a cross-language 
setting. This corresponds to a user-scenario 
where a tourist asks for information in their own 
language in a foreign location (i.e. English 
speaker in Italy). The only difference between 
monolingual and cross-language scenarios is that 
in the cross-language setting, the QA Core sub-
system (Figure 1) selects a Find Entailed Rela-
tion component according to the user input?s lan-
guage. This is due to the entailment algorithms 
that tend to use language specific resources in 
order to attain high accuracy results of matching 
the user request with one of the lexicalized rela-
tions (patterns). It is only the entailment compo-
nent that has to be provided in order to adapt the 
system to new input languages, once the lexical-
ized relations have been translated either manual 
or automatically. 
Both the Instance Annotator and the Answer 
Retriever are language independent, but location 
dependent (Figure 2). The Answer Retriever de-
pends on the location since it is querying data 
found at that place (i.e. Italy), while the Instance 
Annotator looks up instances of the data in the 
user?s question (i.e. annotates an English ques-
tion). They are language independent since they 
are working with data abstractions like SPARQL 
queries (Answer Retriever) or work at character 
level and do not consider language specific as-
pects, like words, in their look-up process (In-
stance Annotator). 
The current version of the system1 is designed 
according to the SOA (Service Oriented Archi-
tecture) and is implemented as point-to-point in-
tegrated web services. Any of the system?s com-
ponents can be substituted by alternative imple-
mentations with no need for further changes as 
long as the functionality remains the same. 
                                                 
1
 http://attila.dfki.uni-sb.de:8282/ QallMe_Proto-
type_WEB_Update/faces/Page6.jsp 
Figure 2. Cross-language Setting 
175
4 Evaluation 
A preliminary evaluation of the first prototype 
was carried out on randomly selected questions 
from a benchmark specifically designed for the 
project. This benchmark was developed to con-
tain questions about various aspects from the 
domain of tourism and for this reason we filtered 
out questions not relevant to cinema or movies. 
The evaluation of the system did not assess 
whether it can extract the correct answer. Instead, 
it measured to what extent the system can select 
the right SPARQL pattern. The explanation for 
this can be found in the fact that once a correct 
question pattern is selected, the extraction of the 
answer requires only retrieval of the answer from 
the database. Moreover, it should be pointed out 
that the main purpose of this preliminary evalua-
tion was to test the interaction between compo-
nents and indicate potential problems, and it was 
less about their performances.  
Table 1 summarises the results of the evalua-
tion. The number of questions used in the evalua-
tion is different from one language to another. 
This can be explained by the fact that for each 
language a number of questions (in general 500) 
was randomly selected from the benchmark and 
only the ones which referred to cinema or movies 
were selected. The column Questions indicates 
the number of questions assessed. The Correct 
column indicates for how many questions a cor-
rect SPARQL was generated. The Wrong column 
corresponds to the number of questions where a 
wrong or incomplete SPARQL was generated. 
This number also includes cases where no 
SPARQL was generated due to lack of corre-
sponding answer pattern. 
 
 Questions Correct Wrong 
English 167 74 (44.31%) 93 (55.68%) 
German 214 120 (56.04%) 94 (43.92%) 
Spanish 58 50 (86.20%) 8 (13.79%) 
Italian 99 46 (46.46%) 53 (53.53%) 
Table 1: Evaluation results 
As can be seen, the results are very different 
from one language to another. This can be ex-
plained by the fact that different entailment en-
gines are used for each language. In addition, 
even though the benchmark was built using a 
common set of guidelines, the complexity of 
questions varies from one language to another. 
For this reason, for some questions it is more dif-
ficult to find the correct pattern than for others.  
Analysis of the results revealed that one of the 
easiest ways to improve the performance of the 
system is to increase the number of patterns. Cur-
rently the average number of patterns per lan-
guage is 42. Improvement of the entailment en-
gines is another direction which needs to be pur-
sued. Most of the partners involved in the project 
have more powerful entailment engines than 
those integrated in the prototype which were 
ranked highly in RTE competitions. Unfortu-
nately, many of these engines cannot be used di-
rectly in our system due to their slow speed. Our 
system is supposed to give users results in real 
time which imposes some constraints on the 
amount of processing that can be done. 
5 Conclusions 
This paper presented the first prototype of an 
entailment-based QA system, which can answer 
questions about movies and cinema. The use of a 
domain ontology ensures that the system is cross-
language and can be extended to new languages 
with slight adjustments at the entailment engine. 
The system is implemented as a set of web ser-
vices and along a Service Oriented Architecture. 
6 Acknowledgements 
This work is supported by the EU-funded pro-
ject QALL-ME (FP6 IST-033860).  
References 
Androutsopoulos, I. and G.D. Ritchie and P. Thanisch. 
1995. Natural Language Interfaces to Databases -- 
An Introduction, Journal of Natural Language En-
gineering, vol.1, no.1, Cambridge University Press. 
Popescu Ana-Marie, Oren Etzioni, and Henry Kautz. 
2003. Towards a theory of natural language inter-
faces to databases. In Proceedings of the confer-
ence on Intelligent User Interfaces. 
Dagan Ido and Oren Glickman. 2004. Probabilistic 
textual entailment: Generic applied modeling of 
language variability. In PASCAL Workshop on 
Learning Methods for Text Understanding and 
Mining, Grenoble. 
Ou Shiyan, Viktor Pekar, Constantin Orasan, Chris-
tian Spurk, Matteo Negri. 2008. Development and 
alignment of a domain-specific ontology for ques-
tion answering. In Proceedings of the 6th Edition of 
the Language Resources and Evaluation Confer-
ence (LREC-08).  
Prud'hommeaux Eric, Andy Seaborne (eds.). 2006. 
SPARQL Query Language for RDF. RDF Data Ac-
cess Working Group. 
176
139
140
141
142
135
136
137
138
Learning to identify animate references
Constantin Ora?san
School of Humanities, Languages
and Social Sciences
University of Wolverhampton
C.Orasan@wlv.ac.uk
Richard Evans
School of Humanities, Languages
and Social Sciences
University of Wolverhampton
R.J.Evans@wlv.ac.uk
Abstract
Information about the animacy of
nouns is important for a wide range of
tasks in NLP. In this paper, we present
a method for determining the animacy
of English nouns using WordNet and
machine learning techniques. Our
method firstly categorises the senses
from WordNet using an annotated
corpus and then uses this information
in order to classify nouns for which
the sense is not known. Our evaluation
results show that the accuracy of the
classification of a noun is around 97%
and that animate entities are more
difficult to identify than inanimate
ones.
1 Introduction
Information on the gender of noun phrase (NP)
referents can be exploited in a range of NLP
tasks including anaphora resolution and the
applications that can benefit from it such as
coreference resolution, information retrieval,
information extraction, machine translation,
etc. The gender of NP referents is explicitly
realised morphologically in languages such as
Romanian, French, Russian, etc. in which the
head of the NP or the NP?s determiner undergoes
predictable morphological transformation or
affixation to reflect its referent?s gender. In the
English language, the gender of NPs? referents is
not predictable from the surface morphology.
Moreover, in (Evans and Ora?san, 2000) it was
argued that it is not always desirable to obtain
information concerning the specific gender of
a NP?s referent in English. Instead, it is more
effective to obtain the animacy of each NP. We
define animacy as the property of a NP whereby
its referent, in singular rather than plural number,
can be referred to using a pronoun in the set
fhe, him, his, himself, she, her, hers, herselfg.
During the course of this paper, we will discuss
animate and inanimate senses of nouns and verbs.
We use these expressions to denote the senses
of nouns that are the heads of NPs referring to
animate/inanimate entities and the senses of verbs
whose agents are typically animate/inanimate
entities.
In our previous work, we investigated the use
of WordNet in order to determine the animacy of
entities in discourse. There, we used the fact that
each noun and verb sense is derived from unique
classes called unique beginners. We classified
each unique beginner as being a hypernym of a
set of senses that were for the most part either
animate or inanimate (in the case of nouns) or
indicative of animacy/inanimacy in their subjects
(in the case of verbs). In classifying a noun, the
number of its senses that belong to an animate
class is compared with the number belonging
to an inanimate class, and this information is
used to make the final classification. In addition,
if the noun is the head of a subject, the same
information is computed for the verb. Our
assumption was that a noun with many animate
senses is likely to be used to refer to an animate
entity. For subjects, the information from the
main verb was used to take into consideration the
context of the sentence. That system, referred
to in this paper as the previous system also used
a proper name gazetteer and some simple rules
which mainly assisted in the classification of
named entities. For reasons explained in Section
4.2, these additions to the basic algorithm were
ignored in the comparative evaluation described
there.
Experiments with that algorithm showed it to
be useful. Applied to a system for automatic
pronominal anaphora resolution, it led to a
substantial improvement in the ratio of suitable
and unsuitable candidates in the sets considered
by the anaphora resolver (Evans and Ora?san,
2000).
However, the previous system has two main
weaknesses. The first one comes from the fact
that the classes used to determine the number
of animate/inanimate senses are too general, and
in most cases they do not reliably indicate the
animacy of each sense in the class. The second
weakness is due to the naive nature of the rules
that decide if a NP is animate or not. Their
application is simple and involves a comparison
of values obtained for a NP with threshold values
that were determined on the basis of a relatively
small number of experiments.
In this paper, we present a new method for
animacy identification which uses WordNet and
machine learning techniques. The remainder
of the paper is structured as follows. Section
2 briefly describes some concepts concerning
WordNet that are used in this paper. In Section 3,
our two step method is described. An evaluation
of the method and discussion of the results is
presented in Section 4. We end the paper by
reviewing previous related work and drawing
some conclusions.
2 Background information
As previously mentioned, in this research
WordNet (Fellbaum, 1998) is used to identify
the animacy of a noun. In this section several
important concepts from WordNet are explained.
WordNet is an electronic lexical resource
organized hierarchically by relations between
sets of synonyms or near-synonyms called
synsets. Each of the four primary classes of
content-words, nouns, verbs, adjectives and
adverbs are arranged under a small set of so-
called unique beginners. In the case of nouns
and verbs, which are the concern of the present
paper, the unique beginners are the most general
concepts under which the entire set of entries is
organized on the basis of hyponymy/hypernymy
relations. Hypernymy is the relation that holds
between such word senses as vehicle
1
-ship
1
or
human
1
-politician
1
, in which the first items
in the pairs are more general than the second.
Conversely, the second items are more specific
than the first, and are their hyponyms.
It is usual to regard hypernymy as a vertically
arranged relationship, with general senses
positioned higher than more specific ones in an
ontology. In WordNet, the top-most senses are
called unique beginners. Senses at the same
vertical level in the ontology are also clustered
horizontally through the synonymy relation in
synsets. In this paper, the term node is used
interchangeably with synset.
As explained in Section 3.1, our method
requires that the nodes in WordNet are classified
according to their animacy. Given the size of
WordNet, this task cannot be done manually and
a corpus where words are annotated with their
senses was necessary. A corpus that meets these
requirements is SEMCOR (Landes et al, 1998),
a subset of the Brown Corpus in which the nouns
and the verbs have been manually annotated with
their senses from WordNet.
3 The method
In this section a two step method used to classify
words according to their animacy is presented. In
Section 3.1, we present an automatic method for
determining the animacy of senses from WordNet
on the basis of an annotated corpus. Once the
senses from WordNet have been classified, a
classical machine learning technique uses this
information to determine the animacy of a noun
for which the sense is not known. This technique
is presented in Section 3.2.
3.1 The classification of the senses
As previously mentioned, the unique beginners
are too general to be satisfactorily classified as
animate or inanimate. However, this does not
26
6
4
HYPERNYM
ani
h
= ani
i
inani
h
= inani
i
3
7
7
5
2
6
6
4
Sense
1
ani
1
inani
1
3
7
7
5
2
6
6
4
Sense
2
ani
2
inani
2
3
7
7
5
2
6
6
4
Sense
3
ani
3
inani
3
3
7
7
5
  
2
6
6
4
Sense
n
ani
n
inani
n
3
7
7
5
Figure 1: Example of hypernymy relation between senses in WordNet
Sense
1
Sense
2
Sense
3
... Sense
n
Observed ani
1
ani
2
ani
3
... ani
n
Expected ani
1
+ inani
1
ani
2
+ inani
2
ani
3
+ inani
3
... ani
n
+ inani
n
Table 1: Contingency table for testing if a hypernym is animate
mean that it is not possible to uniquely classify
more specific senses as animate or inanimate. In
this section, we present a corpus-based method
which classifies the synsets from WordNet
according to their animacy.
The NPs in a 52 file subset of the SEMCOR
corpus were manually annotated with animacy
information and then used by an automatic system
to classify the nodes. These 52 files contain 2512
animate entities and 17514 inanimate entities.
The system attempts to classify the senses
from WordNet that explicitly appear in the
corpus directly, on the basis of their frequency.1
However, our goal is to design a procedure which
is also able to classify senses that are not found
in the corpus. To this end, we decided to use a
bottom up procedure which starts by classifying
the terminal nodes and then continues with more
general nodes. The terminal nodes are classified
using the information straight from the annotated
files. When classifying a more general node,
the following hypothesis is used: ?if all the
1Due to linguistic ambiguities and tagging errors, not all
the senses at this level can be classified adequately in this
way.
hyponyms of a sense are animate, then the sense
itself is animate?. However, this does not always
hold because of annotation errors or rare uses of
a sense and instead, a statistical measure must be
used to test the animacy of a more general node.
Several measures were considered and the most
appropriate one seemed to be chi-square.
Chi-square is a non-parametric test which can
be used for estimating whether or not there is
any difference between the frequencies of items
in frequency tables (Oakes, 1998). The formula
used to calculate chi-square is:

2
=
X
(O  E)
2
E
(1)
where O is the observed number of cases and E
the expected number of cases. If 2 is less than
or equal to a critical level, we may conclude that
the observed and expected values do not differ
significantly.
Each time that a more general node is to be
classified, its hyponyms are considered. If all the
hyponyms observed in the corpus2 are annotated
as either animate or inanimate (but not both), the
2Either directly or indirectly via the hyponymy relations.
Generalisation rejected.... for hypernym Def:(any living entity)
Ani 16 Inani 3 person (sense 1)
++++Def: (a human being; "there was too much for one person to do")
Ani 0 Inani 11 animal (sense 1)
++++Def: (a living organism characterized by voluntary movement)
Figure 2: Example of generalisation rejected
Generalisation accepted .... for hypernym Def:(the continuum of
experience in which events pass from the future through the
present to the past)
Ani 0 Inani 9 past (sense 1)
++++Def: (the time that has elapsed; "forget the past")
Ani 0 Inani 6 future (sense 1)
++++Def: (the time yet to come)
Figure 3: Example of generalisation accepted
more general node is classified as its hyponyms
are. However, for the aforementioned reasons,
this rule does not apply in all cases. In the
remaining cases the chi-square test is applied.
For each more general node which is about to
be classified, two hypotheses are tested: the first
one considers the node animate and the second
one inanimate. The system classifies the node
according to which test is passed. If neither are
passed, it means that the node is too general and
it and all its hypernyms can equally refer to both
animate and inanimate entities.
For example, a more general node can have
several hyponyms as shown in Figure 1. In that
case, the hypernym has n hyponyms. We consider
each sense to have two attributes: the number
of times it has been annotated as animate (ani
i
)
and the number of times it has been annotated
as inanimate (inani
i
). For more general nodes,
these attributes are the sum of the number of
animate/inanimate instances of its hyponyms.
When the node is tested to determine whether or
not it is animate, a contingency table like Table
1 is built. Given that we are testing to see if the
more general node is animate or not, for each of
its hyponyms, the total number of occurrences of
a sense in the annotated corpus is the expected
value (meaning that all the instances should be
animate) and the number of times the hyponym is
annotated as referring to an animate entity is the
observed value. Formula 1 is used to compute
chi-square, and the result is compared with the
critical level obtained for n-1 degrees of freedom
and a significance level of .05. If the test is
passed, the more general node is classified as
animate. In a similar way, more general nodes
are tested for inanimacy. Figures 2 and 3 show
two small examples in which the generalisation
is rejected and accepted, respectively.
In order to be a valid test of significance, chi-
square usually requires expected frequencies to be
5 or more. If the contingency table is larger than
two-by-two, some few exceptions are allowed as
long as no expected frequency is less than one and
no more than 20% of the expected frequencies are
less than 5 (Sirkin, 1995). In our case it is not
possible to have expected frequencies less than
one because this would entail no presence in the
corpus. If, when the test is applied, more than
20% of the senses have an expected frequency
less than 5, the two similar senses with the lowest
frequency are merged and the test is repeated.3 If
no senses can be merged and still more than 20%
of the expected frequencies are less than 5, the
test is rejected.
3.2 The classification of a word
The classification described in the previous
section is useful for determining the animacy of a
sense, even for those which were not previously
found in the annotated corpus, but which are
hyponyms of a node that has been classified.
However, nouns whose sense is unknown cannot
be classified directly and therefore an additional
level of processing is necessary. In this section,
we show how TiMBL (Daelemans et al, 2000)
3Two senses are considered similar if they both have the
same attribute equal to zero.
was used to determine the animacy of nouns.
TiMBL is a program which implements several
machine learning techniques. After trying the
algorithms available in TiMBL with different
configurations, the best results were obtained
using instance-based learning with gain ratio as
the weighting measure (Quinlan, 1993; Mitchell,
1997). In this type of learning, all the instances
are stored without trying to infer anything from
them. At the classification stage, the algorithm
compares a previously unseen instance with
all the data stored at the training stage. The
most frequent class in the k nearest neighbours
is assigned as the class to which that instance
belongs. After experimentation, it was noticed
that the best results were obtained when k=3.
In our case the instances used in training
and classification consist of the following
information:
 The lemma of the noun which is to be
classified.
 The number of animate and inanimate senses
of the word. As we mentioned before, in
the cases where the animacy of a sense is
not known, it is inferred from its hypernyms.
If this information cannot be found for any
of a word?s hypernyms, information on the
unique beginners for the word?s sense is
used, in a manner similar to that used in
(Evans and Ora?san, 2000).
 If the word is the head of a subject, the
number of animate/inanimate senses of
its verb. For those senses for which the
classification is not known, an algorithm
similar to the one described for nouns is
employed. These values are 0 for heads of
non-subjects.
 The ratio of the number of animate singular
pronouns (e.g he or she) to inanimate
singular pronouns (e.g. it) in the whole text.
The output of this stage is a list of nouns
classified according to their animacy.
4 Evaluation and discussion
In this section we examine the performance
of the system, particularly with respect to the
classification of nouns; investigate sources of
errors; and highlight directions for future research
and improvements to the system.
4.1 The performance of the system
The system was evaluated with respect to two
corpora. The first one consists of the files selected
from the SEMCOR corpus stripped of the sense
annotation. The second one is a selection of
texts from Amnesty International (AI) used in our
previous research. These texts have been selected
because they include a relatively large number of
references to animate entities. By including the
texts from the second corpus we could compare
the results of our previous system with those
obtained here. In addition, we can assess the
results of the algorithm on data which was not
used to determine the animacy of the senses. The
characteristics of the two corpora are presented in
Table 2.
In this research three measures were used
to assess the performance of the algorithm:
accuracy, precision and recall. The accuracy is
the ratio between the number of items correctly
classified and the total number of items to be
classified. This measure assesses the performance
of the classification algorithm, but can be slightly
misleading because of the greater number of
inanimate entities in texts. In order to alleviate
this problem, we computed the precision and
recall for each type of classification. The
precision with which the method classifies
animate entities is defined as the ratio between
the number of entities it correctly classifies
as animate and the total number of entities it
classifies as animate (including the ones wrongly
assigned to this class). The method?s recall
over this task is defined as the ratio between the
number of entities correctly classified as animate
by the method and the total number of animate
entities to be classified. The precision and recall
for inanimate entities is defined in a similar
manner.
We consider that by using recall and precision
for each type of entity we can better assess the
performance of the algorithms. This is mainly
because the large number of inanimate entities are
considered separately from the smaller number of
animate entities. In addition to this, by separating
Corpus No of words No. of animate entities No of inanimate entities
SEMCOR 104612 2512 17514
AI 15767 537 2585
Table 2: The characteristics of the two corpora used
Animacy Inanimacy
Experiment Accuracy Precision Recall Precision Recall
Baseline on SEMCOR 37.62% 8.40% 74.44% 88.41% 31.64%
Baseline on AI 31.01% 18.07% 76.48% 79.27% 20.60%
Previous system on AI 64.87% 93.88% 36.09% 81.00 % 99.14%
New System on SEMCOR 97.51% 88.93% 91.03% 98.74% 98.41%
New System on AI 97.69% 94.28% 92.17% 98.38% 98.83%
Table 3: The results of the evaluation
the evaluation of the classification of animate
entities from the one for inanimate entities we can
assess the difficulty of each classification.
Table 3 presents the results of the method on
the two data sets. For the experiment with the
SEMCOR corpus, we evaluated it using five-fold
cross-validation. We randomly split the whole
corpus into five disjoint parts, using four parts for
training and one for evaluation. We repeated the
training-evaluation cycle five times, making sure
that the whole corpus was used. Note that for
each iteration of the cross-validation, the learning
process begins from scratch. The results reported
were obtained by averaging the error rates from
each of the 5 runs. In the second experiment, all
52 files from the SEMCOR corpus were used for
training and the texts from Amnesty International
for testing.
In addition to the results of the method
presented in this paper, Table 3 presents the
results of a baseline method and of the method
previously proposed in (Evans and Ora?san, 2000).
In the baseline method, the probability that an
entity is classified as animate is proportional
to the number of animate third person singular
pronouns in the text.
As can be seen in Table 3 the accuracy of the
baseline is very low. The results of our previous
method are considerably higher, but still poor
in the case of animate entities with many of
these being classified as inanimate.4 This can
4Due to time constraints and the large amount of effort
be explained by the fact that most of the unique
beginners were classified as inanimate, and
therefore there is a tendency to classify entities
as inanimate. The best results were obtained
by the new method over both corpora, the main
improvement being noticed in the classification
of animate entities.
Throughout this section we referred to the
classification of ambiguous nouns without trying
to assess how successful the classification of the
synsets in WordNet was. Such an assessment
would be interesting, but would require manual
classification of the nodes in WordNet, and
therefore would be somewhat time consuming.
Even though this evaluation was not carried out,
the high accuracy of the system suggests that the
current classification is useful.
4.2 Comments and error analysis
During the training phase of TiMBL, the program
computes the importance of each feature for
the classification. The most important feature
according to the gain ratio is the number of
animate senses of a noun followed by the number
of inanimate senses of the noun. This was
expected given that our method is based on the
idea that in most of the cases the number of
animate and inanimate senses determines the
animacy of a noun. However, this would mean
that the same noun will be classified in the same
required to transform the input data into a format usable
by the previous method, it was not possible to assess its
performance with respect to the SEMCOR corpus.
way regardless of the text. Therefore, three text
dependent features were introduced. They are the
number of animate and inanimate senses of the
predicate of the sentence if the noun is a subject,
and the ratio between the number of animate
third-person singular pronouns and inanimate
third-person singular pronouns in the text. In
terms of importance, gain ratio ranks them fourth,
fifth and sixth, respectively, after the lemma of
the noun. The lemma of the noun was included
because it was noticed that this improves the
accuracy of the method.
During the early stages of the evaluation, the
classification of personal names proved to be a
constant source of errors. Further investigation
showed that the system performed poorly on all
types of named entities. For the named entities
referring to companies, products, etc. this can
be explained by the fact that in many cases they
are not found in WordNet. However, in most
cases the system correctly classified them as
inanimate, having learned that most unknown
words belong to this class. Entities denoted by
personal names were constantly misclassified
either because the names were not in WordNet or
else they appeared with a substantial number of
inanimate senses (e.g. the names Bob and Maria
do not have any senses in WordNet which could
relate them to animate entities). In light of these
errors we decided not to present our system with
named entities. With no access to more accurate
techniques, we considered non-sentence-initial
capitalised words as named entities and removed
them from the evaluation data. Even when this
crude filtering was applied, we still presented
a significant number of proper names to our
system. This partially explains its lower accuracy
with respect to the classification of animate
entities.
By attempting to filter proper names, we
could not compare the new system with the one
referred to as the extended algorithm in (Evans
and Ora?san, 2000). In future, we plan to address
the problem of named entities by using gazetteers
or, alternatively, developing more sophisticated
named entity recognition methods.
Another source of errors is the unusual usage
of senses. For example someone can refer to their
pet with he or she, and therefore according to
our definition they should be considered animate.
However, given the way the algorithm is designed
there is no way to take these special uses into
consideration.5
Another problem with the method is the fact
that all the senses have the same weight. This
means that a word like pupil, which has two
animate senses and one inanimate, is highly
unlikely to be classified as inanimate, even if
it used to refer to a specific part of the eye.6
The ideal solution to this problem would be to
disambiguate the words, but this would require an
accurate disambiguation method. An alternative
solution is to weight the senses with respect to
the text. In this way, if a sense is more likely to
be used in a text, its animacy/inanimacy will have
greater influence on the classification process. At
present, we are trying to integrate the word sense
disambiguation method proposed in (Resnik,
1995) into our system. We hope that this will
particularly improve the classification of animate
entities.
5 Related work
Most of the work on animacy/gender recognition
has been done in the field of anaphora resolution.
The automatic recognition of NP gender on
the basis of statistical information has been
attempted before (Hale and Charniak, 1998).
That method operates by counting the frequency
with which a NP is identified as the antecedent of
a gender-marked pronoun by a simplistic pronoun
resolution system. It is reported that by using
the syntactic Hobbs algorithm (Hobbs, 1976)
for pronoun resolution, the method was able to
assign the correct gender to proper nouns in a
text with 68.15% precision, though the method
was not evaluated with respect to the recognition
of gender in common NPs. The method has
two main drawbacks. Firstly, it is likely to be
ineffective over small texts. Secondly, it seems
5However, it is possible to reclassify the nodes from
WordNet using an annotated corpus where the pets are
animate, but this would make the system consider all the
animals which can be pets animate.
6Actually the only way this word would be classified as
inanimate is if it is in the subject position, and most of the
senses of its main verb are inanimate. This is explained by
the way the senses are weighted by the machine learning
algorithm.
that the approach makes the assumption that
anaphora resolution is already effective, even
though, in general, anaphora resolution systems
rely on gender filtering.
In (Denber, 1998), WordNet was used to
determine the animacy of nouns and associate
them with gender-marked pronouns. The details
presented are sparse and no evaluation is given.
Cardie and Wagstaff (1999) combined the use of
WordNet with proper name gazetteers in order
to obtain information on the compatibility of
coreferential NPs in their clustering algorithm.
Again, no evaluation was presented with respect
to the accuracy of this animacy classification
task.
6 Conclusions and future work
In this paper, a two step method for animacy
recognition was proposed. In the first step, it
tries to determine the animacy of senses from
WordNet on the basis of an annotated corpus. In
the second step, this information is used by an
instance based learning algorithm to determine
the animacy of a noun. This area has been
relatively neglected by researchers, therefore a
comparison with other methods is difficult to
make. The accuracy obtained is around 97%,
more than 30% higher than that obtained by our
previous system.
Investigation of the results showed that in
order to obtain accuracy close to 100%, several
resources have to be used. As we point out in
Section 4.2, a method which is able to weight
the senses of a noun according to the text,
and a named entity recogniser are necessary.
The requirement for such components helps to
emphasise the problematic nature of NP animacy
recognition. We believe that such an investment
should be made in order to go forward with this
useful enterprise.
References
Claire Cardie and Kiri Wagstaff. 1999. Noun
phrase coreference as clustering. In Proceedings of
the 1999 Joint SIGDAT conference on Emphirical
Methods in NLP and Very Large Corpora (ACL?99),
pages 82 ? 89, University of Maryland, USA.
Walter Daelemans, Jakub Zavarel, Ko van der Sloot,
and Antal van den Bosch. 2000. Timbl: Tilburg
memory based learner, version 3.0, reference guide,
ilk technical report 00-01. ILK 00-01, Tilburg
University.
Michael Denber. 1998. Automatic resolution of
anaphora in english. Technical report, Eastman
Kodak Co, Imaging Science Division.
Richard Evans and Constantin Ora?san. 2000.
Improving anaphora resolution by identifying
animate entities in texts. In Proceedings of
the Discourse Anaphora and Reference Resolution
Conference (DAARC2000), pages 154 ? 162,
Lancaster, UK, 16 ? 18 November.
Christiane Fellbaum, editor. 1998. WordNet: An
Eletronic Lexical Database. The MIT Press.
John Hale and Eugene Charniak. 1998. Getting useful
gender statistics from english textx. Technical
Report CS-98-06, Brown University.
Jerry Hobbs. 1976. Pronoun resolution. Research
report 76-1, City College, City University of New
York.
Shari Landes, Claudia Leacock, and Randee I. Tengi.
1998. Building semantic concordances. In
Fellbaum (Fellbaum, 1998), pages 199 ? 216.
Tom M. Mitchell. 1997. Machine learning. McGraw-
Hill.
Michael P. Oakes. 1998. Statistics for Corpus
Linguistics. Edinburgh Textbooks in Empirical
Linguistics. Edinburgh University Press.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
Philip Resnik. 1995. Disambiguating noun groupings
with respect to Wordnet senses. In David Yarovsky
and Kenneth Church, editors, Proceedings of the
Third Workshop on Very Large Corpora, pages
54?68, Somerset, New Jersey. Association for
Computational Linguistics.
R. Mark Sirkin. 1995. Statistics for the social
sciences. SAGE Publications.
An evolutionary approach for improving the quality of automatic summaries
Constantin Ora?san
Research Group in Computational Linguistics
School of Humanities, Languages and Social Sciences
University of Wolverhampton
C.Orasan@wlv.ac.uk
Abstract
Automatic text extraction techniques
have proved robust, but very often their
summaries are not coherent. In this
paper, we propose a new extraction
method which uses local coherence as a
means to improve the overall quality of
automatic summaries. Two algorithms
for sentence selection are proposed
and evaluated on scientific documents.
Evaluation showed that the method
ameliorates the quality of summaries,
noticeable improvements being obtained
for longer summaries produced by an
algorithm which selects sentences using
an evolutionary algorithm.
1 Introduction
It is generally accepted that there are two main
approaches for producing automatic summaries.
The first one is called extract and rearrange because
it extracts the most important sentences from a text
and tries to arrange them in a coherent way. These
methods were introduced in the late 50s (Luhn,
1958) and similar methods are still widely used.
The second approach attempts to understand the
text and, then, generates its abstract, for this reason
it is referred to as understand and generate. The
best-known method that uses such an approach is
described in (DeJong, 1982). Given that the methods
which ?understand? a text are domain dependent,
whenever robust methods are required, extraction
methods are preferred.
Even though the extraction methods currently
used are more advanced than the one proposed in
(Luhn, 1958), many still produce summaries which
are not very coherent, making their reading difficult.
This paper presents a novel summarisation approach
which tries to improve the quality of the produced
summaries by ameliorating their local cohesion.
This paper is structured as follows: In Section
2 we present our hypothesis: it is possible
to produce better summaries by enforcing the
continuity principle (see next section for a definition
of this principle) . A corpus of scientific abstracts
is analysed in Section 3 to learn whether this
principle holds in human produced summaries.
In Section 4, we present two algorithms which
combine traditional techniques with information
provided by the continuity principle. Several
criteria are used to evaluate these algorithms on
scientific articles in Section 5. We finish with
concluding remarks, which also indicate possible
future research avenues.
2 How to ensure local cohesion
In the previous section we already mentioned
that we are trying to improve the automatic
summaries by using the continuity principle defined
in Centering Theory (CT) (Grosz et al, 1995). This
principle, requires that two consecutive utterances
have at least one entity in common. Even though
it sounds very simple, this principle is important for
the rest of the principles defined in the CT because
if it does not hold, none of the other principles
can be satisfied. Given that only the continuity
principle will be used in this paper and due to space
limits, the rest of these principles are not discussed
here. Their description can be found in (Kibble and
Power, 2000). For the same reason we will not go
into details about the CT.
In this paper, we take an approach similar to
(Karamanis and Manurung, 2002) and try to produce
summaries which do not violate the continuity
principle. In this way, we hope to produce
summaries which contain sequences of sentences
that refer the same entity, and therefore will be more
coherent. Before we can test if the principle is
satisfied, it is necessary to define certain parameters
on which the principle relies. As aforementioned,
the principle is tested on pairs of consecutive
utterances. In general utterances are clauses or
sentences. Given that the automatic identification of
clauses is not very accurate, we consider sentences
as utterances. An advantage of using sentences is
that most summarisation methods extract sentences,
which makes it easier to integrate them with our
method.
In this paper, we consider that two utterances
have an entity in common if the same head noun
phrase appears in both utterances. In order to
determine the head of noun phrases we use the FDG
tagger (Tapanainen and Ja?rvinen, 1997) which also
provides partial dependency relations between the
constituents of a sentence. At this stage we do not
employ any other method to determine whether two
noun phrases are semantically related.
3 Corpus investigation
Before we implemented our method, we wanted
to learn if the continuity principle holds in human
produced summaries. In order to perform this
analysis we investigated a corpus of 146 human
produced abstracts from the Journal of Artificial
Intelligence Research (JAIR). 1
Most of the processing was done automatically
using a simple script which tests if the principle
is satisfied by pairs of consecutive utterances (i.e.
if the pair has at least one head noun phrase in
common). Those pairs which violate the principle
were manually analysed.
In our corpus almost 75% of the pairs of
1The full articles and their abstracts are freely available at
http://www.jair.org
consecutive utterances (614 out of 835) satisfy the
principle. In terms of summaries, it was noticed
that 44 out of 146 do not have any such pairs which
violate the principle.
After analysing the violations, we can explain
them in one of the following ways:
? In 126 out of 221 cases (57%) the link between
utterances is realised by devices such as rhetorical
relations.
? In 76 cases (34%) the continuity principle was
realised, but was not identified by the script because
of words were replaced by semantic equivalents. In
only 17 of these cases pronouns were used.
? Ramifications in the discourse structure violate
the principle in 19 cases (9%). These ramifications
are usually explicitly marked by phrases such as
firstly, secondly.
After investigating our corpus we can definitely
say that the continuity principle is present in
human produced abstracts, and therefore by trying
to enforce it in automatic summaries, we might
produce better summaries. However, by using
such approach we cannot be sure that the produced
summaries are coherent, being known that it is
possible to produce cohesive texts, but which are
incoherent. In Section 4 we present a method which
uses the continuity principle to score the sentences.
This method is then evaluated in Section 5.
We also have to emphasise that we do not
claim that humans consciously apply the continuity
principle when they produce summaries or any other
texts. The presence of the violations identified in our
corpus is an indication for this.
4 The method
Karamanis and Manurung (2002) used the
continuity principle in text generation to choose
the most coherent text from several produced by
their generation system. In their case, the candidate
texts were sequences of facts, their best ordering
was determined by an evolutionary algorithm which
tried to minimise the number of violations of the
continuity principle they contained.
We take a similar approach in our attempt to
produce coherent summaries, trying to minimise the
number of violations of the principle they contain.
However, our situation is more difficult because
a summarisation program needs firstly to identify
the important information in the document and
then present it in a coherent way, whereas in text
generation the information to be presented is already
known. ?Understand and generate? methods would
be appropriate, but they can only be applied to
restricted domains. Instead, we employ a method
which scores a sentence not only using its content,
but also considering the context in which the
sentence would appear in a summary. Two different
algorithms are proposed. Both algorithms use the
same content-based scoring method (see Section
4.1), but they use different approaches to extract
sentences. As a result, the way the context-based
scoring method defined in Section 4.2 is applied
differs. The first algorithm is a greedy algorithm
which does not always produce the best summary,
but it is simple and fast. The second algorithm
employs an evolutionary technique to determine the
best set of sentences to be extracted.
We should point out that another difference
between our method and the ones used in text
generation is that we do not intend to change the
order of the extracted sentences. Such an addition
would be interesting, but preliminary experiments
did not lead to any promising results.
4.1 Content-based scoring method
We rely on several existing scoring methods to
determine the importance of a sentence on the basis
of its content. In this section we briefly describe how
this score is computed. The heuristics employed to
compute the score are:
Keyword method: uses the TF-IDF scores of words
to compute the importance of sentences. The score
of a sentence is the sum of words? scores from that
sentence (Zechner, 1996)
Indicator phrase method: Paice (1981) noticed
that in scientific papers it is possible to identify
phrases such as in this paper, we present,
in conclusion, which are usually meta-discourse
markers. A list of such phrases has been built and
all the sentences which contain an indicating phrase
have their scores boosted or penalised depending on
the phrase.
Location method: In scientific papers important
sentences tend to appear at the beginning and end of
the document. For this reason sentences in the first
and the last 13 paragraphs have their scores boosted.
This value was determined through experiments.
Title and headers method: Words in the title
and headers are usually important, so sentences
containing these words have their scores boosted.
Special formatting rules: Quite often certain
important or unimportant information is marked in
texts in a special way. In scientific paper it is
common to find equations, but they rarely appear in
the abstracts. For this reason sentences that contain
equations are excluded.
The score of a sentence is a weighted function
of these parameters, the weights being established
through experiments. As already remarked by other
researchers, one of the most important heuristics
proved to be the indicating phrase method.
4.2 Context-based scoring method
Depending on the context in which a sentence
appears in a summary, its score can be boosted
or penalised. If the sentence which is considered
satisfies the continuity principle with either the
sentence that precedes or follows it in the summary
to be produced, its score is boosted.2 If
the continuity principle is violated the score is
penalised. After experimenting with different values
we decided to boost the sentence?s score with the
TF-IDF scores of the common NPs? heads and
penalise with the highest TF-IDF score in the
document.
While analysing our corpus we noticed that large
number of violations of the continuity principle are
due to utterances in different segments. Usually this
is explicitly marked by a phrase. We extracted a list
of such phrases from our corpus and decided not to
penalise those sentences which violate the continuity
principle, but contain one of these phrases.
4.3 The greedy algorithm
The first of the two sentence selection algorithms is a
greedy algorithm which always extracts the highest
scored sentence from those not extracted yet. The
sentences? scores are computed in the way described
2The way the sentences which precedes and follows it is
determined depends very much on the algorithm used (see
Sections 4.3 and 4.4 for details). If the sentence is the first or
the last in a summary (i.e. there is no preceding or following
sentence) the score is not changed.
Given an extract
 
	
,

,...,

and S the sentence which is considered for
extraction
1. Find

and
PALinkA: A highly customisable tool for discourse annotation
Constantin Ora?san
Research Group in Computational Linguistics
School of Humanities, Languages and Social Sciences
University of Wolverhampton
United Kingdom
C.Orasan@wlv.ac.uk
Abstract
Annotation of discourse phenomena is a
notoriously difficult task which cannot
be carried out without the help of
annotation tools. In this paper we present
a Perspicuous and Adjustable Links
Annotator (PALinkA), a tool successfully
used in several of our projects. We also
briefly describe three types of discourse
annotations applied using the tool.
1 Introduction
Annotation of discourse phenomena is a notoriously
difficult task which cannot be carried out without the
help of annotation tools. In this paper, we present
an annotation tool successfully employed in three
tasks which capture various discourse phenomena.
In addition it proved useful in several other simpler
tasks. Even though the annotation still needs to be
done by humans, the features of the tool facilitate
the process.
The structure of this paper is as follows: In
Section 2 we discuss some of the requirements of
annotation tools. Several such tools are discussed
in Section 3 explaining why we decided to develop
our own annotator. A brief description of it is
presented in Section 4, followed by a three case
studies briefly showing how the tool was used for
marking different discourse phenomena. The article
finishes with conclusions indicating ways to further
develop the tool.
2 Requirements of annotation tools
In recent years the need to produce reusable corpora
led to an increasing use of XML encoding in
annotation. As a result, the annotation cannot
be applied using simple text editors. In addition,
the discourse annotation is usually complicated
requiring specialised tools. In this section, we
present the most important characteristics of a
discourse annotation tool.
An annotation tool needs to be easy to use; with
a minimum time required to learn how it works.
It should also hide unnecessary details from the
annotator (e.g. XML tags which are not directly
linked to the task).
Usually the annotators are linguists with little or
no experience of computers or annotation schemes.
Because of this, an annotation tool has to be
designed so that humans provide the information in
a very simple and friendly way. In addition, the
tool needs to ensure that no illegal information is
introduced during the process (e.g. illegal XML
constructions, wrong values for the attributes, etc.).
Last, but not least, it is desirable that a tool can
be used for more than one task, so the annotators do
not need to learn a new tool every time the task is
changed. Moreover, in projects which build corpora
in several languages, one way to ensure consistency
between the annotations in the different languages is
by using the same tool. Therefore, it is desired that
a tool is language independent.
PALinkA, the tool presented in this paper
meets all these requirements, being appropriate for
discourse annotation.
3 Existing annotation tools
A large number of the existing annotation tools are
for specific purposes only (e.g. for coreference
(Garside and Rayson, 1997; Ora?san, 2000), for
Rhetorical Structure Theory (Marcu, RSTTool)).
Due to space limits we will not refer to them. In
this section we briefly present few tools which can
be used for a wide range of annotations tasks.
Day et. al. (1998) present Alembic Workbench,
a tool developed by MITRE Corporation and
used in the MUC conferences. The tool is
highly customisable and features machine learning
algorithms which facilitate the annotation process.
Unfortunately the support seems to be discontinued
and the documentation how to use the machine
learning algorithms is sparse. When we tried to
process texts with rich annotation it became slow.
Other tools which can be used to annotate a
large range of discourse phenomena are MATE
(McKelvie et al, 2001), ATLAS (Laprun et al,
2002) and MMAX (Mu?ller and Strube, 2001).
All these tools provide advanced frameworks for
annotating text and speech, allowing customisation
according to the task. They are very powerful, but
they also require advanced computing knowledge
in order to install and take full advantage of
the facilities they provide. We consider that the
installation and customisation process needs to be
simple, so that people without much knowledge
about computers can use them.
In the next section, we present PALinkA, a tool
which requires little computing knowledge to install
and customise, and can be employed in a large
number of annotation tasks.
4 Perspicuous and Adjustable Links
Annotator (PALinkA)
Our corpus annotated for coreference (Mitkov et
al., 2000) was produced using Coreferential Links
Annotator (CLinkA) (Ora?san, 2000). Even though
the tool was useful for the annotation, we noticed
that it has limitations. For example it does not allow
to annotate texts which already contained other type
of annotation and the annotation scheme it built in
the tool which means that it cannot be changed.
We started to develop PALinkA as a replacement
of CLinkA, trying to address its shortcomings. Soon
we realised that it is easy to make a multipurpose
annotation tool, which can be adjusted to the
requirements of the task, without losing its ease of
use, keeping it perspicuous.
The underlying idea of PALinkA is that it is
possible to decompose most of the annotation tasks
using three types of basic operations:
  Insertion of information not explicitly marked in
the text (e.g. ellipsis, zero pronouns)
  Marking of the elements in a text (e.g. noun
phrases, utterances, sentences)
  Marking the links between the elements (e.g.
coreferential links)
We should emphasise that these three operation do
not correspond to only three XML tags. The number
of tags which can be inserted in a text is unlimited,
for each one being possible to specify its name and
attributes. However, for each tag it is necessary to
define the type of operation attached to it, so that the
tool will know how to handle it. For example, for
missing information the tool will insert a marker in
the text, whereas for a link it will ask the annotator to
specify the referred element. The set of tags which
can be used to annotate is loaded from a preferences
file. Figure 1 shows a small part of a preferences
file used to annotate coreference. It could look
complicated for a non-expert in computers, but its
syntax relies on a limited number of rules which are
described in the documentation.
[MARKER]
;<EXP ID="#" COMMENT="">...</EXP>
NAME:EXP
BGCOLOR:23,255,254
FGCOLOR:123,111,10
ATTR:ID=# ;unique id
ATTR:COMMENT=!
INSERT_BEFORE:[
INSERT_AFTER:]
Figure 1: Part of the preferences file
used to annotate the coreference
As can be seen in Figure 2 in the main screen
of the tool does not display the XML tags, so the
text can be easily read. In order to identify the tags
present in the text, the user can specify colours to
display the annotated text and can require to have
the boundaries explicitly marked (in our example
Figure 2: The main screen of the tool during annotation of coreference
with square brackets). PALinkA can be used to add
annotation to files which already contain some sort
of annotation. However, if the existing annotation is
not relevant for the task, it does not appear on the
screen at all.
The annotation process is kept as simple as
possible; the boundaries of tags and the links
between them being indicated with the help of the
mouse. The tags which need to be linked require a
unique ID. These IDs are generated and managed by
the program allowing the annotator to concentrate
on the annotation process. In addition to this, the
tool has all the normal operations an annotation tool
has: it can insert, delete or change tags.
The output of the program is a well-formed XML,
the tool making sure that the human annotator does
not produce invalid XML constructions. At present
the tool supports only in-line annotation, but in
the long term, we intent to offer the possibility of
producing stand-off annotation.
Given that PALinkA is implemented in Java it
is platform independent, running on any computer
with a Virtual Java Machine installed. The tool is
also language independent. In order to keep the tool
as flexible as possible, it does not has a tokeniser.
Instead, the tokens in the input text have to be
explicitly marked using XML.
Due to space restrictions, we cannot present
all the features of PALinkA and how it
operates in more detail. More information
can be found at the project?s web page:
http://clg.wlv.ac.uk/projects/PALinkA/. At the
same address it is possible to download the tool for
free.
5 Case studies
In this section, we show how PALinkA was used to
create annotated corpora for coreference resolution,
automatic summarisation and centering theory. We
finish the section with few examples of simpler
annotation tasks where PALinkA proved useful.
5.1 Coreference annotation
Annotating coreference is a notoriously time-
consuming and labor-intensive task. In this task,
the annotators have to mark the coreferential links
between entities in a text. Usually, each entity
receives a unique ID, and a link between two
entities is marked using these IDs. These IDs are
automatically managed by PALinkA. Some of the
links refer to more than one entity. This fact can
also be encoded using the tool.
For this annotation we extended the scheme
presented in (Tutin et al, 2000). Even though this
scheme is not similar with the one used in the MUC,
it can be easily converted to the MUC scheme.
PALinkA is currently used in the Alliance Project1
to produce coreferentially annotated corpora for
English and French.
The coreferential chains can be quickly identified
by using the entities? tree in the right hand side of
the screen (see Figure 2) or by highlighting them.
5.2 Annotation for automatic summarisation
Automatic summarisation is not part of the discourse
analysis field, but it can use discourse information
in order to produce high quality summaries. A
corpus of news was annotated with information
useful for automatic summarisation (Hasler et al,
2003). In addition to indicating the importance
of each sentence, we enhanced the corpus with
additional information which allows to measure the
conciseness and the coherence of summaries. In
order to be able to measure the conciseness of a
summary, we indicated in the important sentences
which parts can be removed without losing
important information. For coherence, we used
a simplified version of the coreference annotation
task. For each important sentence containing a
referential expression with the antecedent in another
sentence, we indicated the link between sentences.
As for other tasks, the tool eased the annotation
thanks to its friendly interface. In addition,
PALinkA has two features which made the task
much easier. One of these features indicates how
much of the text is marked with a certain tag. We
asked our annotators to mark 15% of the text as
1More details about the Alliance project are available at:
http://clg.wlv.ac.uk/projects/Alliance/
essential and another 15% as important. Using
PALinkA it was possible to keep these length
restrictions.
The time necessary to annotate a text was another
parameter we wanted to record. With PALinkA it is
possible to record this time. If the annotator needs to
take a break during the process, this can be indicated
by pressing the Pause button, in this way recording
the actual time required by the annotation.
The corpus annotated for automatic
summarisation is part of the Computer-Aided
Summarisation Tool (CAST) project.2
5.3 Annotating centering
Centering Theory (CT) characterises the local
coherence of a text on the basis of the discourse
entities in a text and the way in which they are
introduced (Grosz et al, 1995). CT was developed
and demonstrated on simple texts. In order to test
if the theory holds for real texts and gain insights
into how the theory can be applied to them, 60
news reports and encyclopedic texts were annotated
by several annotators. The number of annotated
texts may seem small, but given the difficulty of the
annotation and the fact that six versions of Centering
Theory were marked for each text, it is impossible to
produce large corpora.
In Centering Theory the discourse consists of a
sequence of utterances. Each utterance has several
forward looking centers and at most one backward
looking center. One of the forward looking center
is called preferred center and indicates the topic of
the utterance. Due to space limits, Centering Theory
cannot be discussed here, more details can be found
in (Grosz et al, 1995; Walker et al, 1998).
The main difficulty when annotating centering
comes from the number of embedded tags which
have to be marked. Each utterance contains several
centers, some of these also embedding other centers.
Given this richness of tags the main advantage of
using PALinkA is that it hides the XML tags, using
colours for each tag. In addition to this, it is possible
to configure the program to mark the beginning and
end of each tag using a character chosen by the
user. This feature proved also useful for coreference
annotation. It is possible to notice it in Figure 2
2http://clg.wlv.ac.uk/projects/CAST/
where the boundaries of each NP are marked by
square brackets. The user friendly interface facilitate
the annotation process and does not distract the
annotator with technical details.
5.4 Other tasks
In addition to annotating the aforementioned
discourse phenomena, the tool was also employed
in several other simpler tasks. It proved useful to
annotate named entities in a corpus of Romanian
news, mark noun phrases, prepositional phrases
and their attachment in Romanian texts. The tool
was also used to post-edit the output of automatic
programs which identify the layout of scientific
articles (e.g. headings, footnotes, citations).
6 Conclusions and future work
In this paper, we briefly presented a multipurpose
annotation tool used in several of our projects
which annotated the structure of the discourse. The
tool is freely available for research purposes at
http://clg.wlv.ac.uk/projects/PALinkA/.
In the future we intend add two new features
to PALinkA. The first one will enable automating
certain tasks with the possibility of post-editing the
output of the automatic methods. We are currently
working on an API which will allow the users to
plug their modules into PALinkA. However, given
that these modules will have to be written in Java,
this function will be available only for programmers.
The second feature which we want to add to the
system is to allow annotation of cross-document
links. Such an option will prove very useful for
cross-document coreference research.
7 Acknowledgements
The development of this tool was supported by
the Arts and Humanities Research Board (AHRB)
through the CAST project and by the British Council
through the Alliance Project.
References
David Day, John Aberdeen, Sasha Caskey, Lynette
Hirschman, Patricia Robinson, and Marc Vilain. 1998.
Alembic workbench corpus development tool. In
Proceedings of the First International Conference on
Language Resource&Evaluation, pages 1021 ? 1028.
Roger Garside and Paul Rayson. 1997. Higher-level
annotation tools. In Roger Garside, Geoffrey Leech,
and Anthony McEnery, editors, Corpus Annotation:
Linguistic Information from Computer Text Corpora,
pages 179 ? 193. Addison Wesley Longman.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modelling the local
coherence of discourse. Computational Linguistics,
21(2):203 ? 225.
Laura Hasler, Constantin Ora?san, and Ruslan Mitkov.
2003. Building better corpora for summarisation. In
Proceedings of Corpus Linguistics 2003, pages 309 ?
319, Lancaster, UK, March.
Christophe Laprun, Jonathan G. Fiscus, John Garofolo,
and Sylvain Pajot. 2002. A practical introduction to
ATLAS. In Proceedings of LREC2003, pages 1928 ?
19932, Las Palmas de Gran Canaria, Spain.
Daniel Marcu. RSTTool. RST Annotation
Tool. Availabe at: http://www.isi.edu/licensed-
sw/RSTTool/index.html.
David McKelvie, Amy Isard, Andreas Mengel, Morten B.
Moeller, Michael Grosse, and Marion Klein. 2001.
The MATE Workbench - an annotation tool for XML
coded speech corpora. Speech Communication, 33(1?
2):97 ? 112.
Ruslan Mitkov, Richard Evans, Constantin Ora?san,
Ca?ta?lina Barbu, Lisa Jones, and Violeta Sotirova.
2000. Coreference and anaphora: developing
annotating tools, annotated resources and annotation
strategies. In Proceedings of the Discourse, Anaphora
and Reference Resolution Conference (DAARC2000),
pages 49?58, Lancaster, UK.
Christoph Mu?ller and Michael Strube. 2001. MMAX:
a tool for the annotation of multi-modal corpora. In
Proceedings of the 2nd IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systems, pages
45 ? 50, Seattle, Washington, 5th August.
Constantin Ora?san. 2000. CLinkA a coreferential links
annotator. In Proceedings of LREC?2000, pages 491 ?
496, Athens, Greece.
Agnes Tutin, Francois Trouilleux, Catherine Clouzot,
Eric Gaussier, Annie Zaenen, Stephanie Rayot, and
Georges Antoniadis. 2000. Annotating a large
corpus with anaphoric links. In Proceedings of
the Discourse Anaphora and Reference Resolution
Conference (DAARC2000), pages 28 ? 38, Lancaster,
UK, 16th ? 18th November.
Marilyn A. Walker, Aravind K. Joshi, and Ellen Prince,
editors. 1998. Centering Theory in Discourse.
Oxford University Press.
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 107?108,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
WLV: A confidence-based machine learning method for the
GREC-NEG?09 task
Constantin Ora?san
RIILP
University of Wolverhampton, UK
C.Orasan@wlv.ac.uk
Iustin Dornescu
RIILP
University of Wolverhampton, UK
I.Dornescu@wlv.ac.uk
Abstract
This article presents the machine learning
approach used by the University of
Wolverhampton in the GREC-NEG?09
task. A classifier based on J48 decision
tree and a meta-classifier were used to
produce two runs. Evaluation on the
development set shows that the meta-
classifier achieves a better performance.
1 Introduction
The solution adopted by the University of
Wolverhampton to solve the GREC-NEG task
relies on machine learning. To this end, we
assumed that it is possible to learn which is the
correct form for a referential expression given the
context in which it appears. The remainder of the
paper is structured as follows: Section 2 presents
the method used in this paper. Section 3 presents
the evaluation results on the development set. The
paper finishes with conclusions.
2 Method
The method used to solve the GREC-NEG task
was inspired by the machine learning approaches
employed for coreference resolution. In these
methods, pairs of entities are classified as
coreferential or not on the basis of a set of features
(Mitkov, 2002). In the same manner, each REF
element from the text to be processed is paired
with all the REFEX elements in its chain and
machine learning is used to determine the lexical
form of which candidate REFEX element can be
used in the given context. To achieve this, a set of
features was derived after a corpus investigation.
As can be seen, some of these features are
similar to those used by resolution algorithms
(e.g. distance between entities), whilst others are
specific for the task (e.g. empty markers). The
features used for a (REF, REFEX) pair are:
? Whether the REF element is the first mention
in the chain. We noticed that in most cases
it corresponds to the longest REFEX element
in the plain case.
? Whether the REFEX element is the longest
string.
? Whether the REF element is the first word in
the sentence as this word is very likely to be
the subject (i.e. nominative or plain case).
? Whether the words before the REF element
can signal a possible empty element.
Example of such phrases are ?, but? and ?and
then?. These phrases were extracted after
analysing the training corpus.
? The distance in sentences to the previous
REF element in the chain. This feature was
used because a pronoun is more likely to
be used when several mentions are in the
same sentence, whilst full noun phrases are
normally used if the mentions are far away or
in different paragraphs.
? The REG08-TYPE of the REFEX tags
that were assigned by the program to the
previous 2 REF elements in the chain. This
information can prove useful in conjunction
with the previous feature.
? The part-of-speech tags of the four words
before and three words after the REF element
as a way to indicate the context in which the
element appears.
? A compatibility feature which indicates pairs
of SYNFUNC and CASE that are highly
correlated. This correlation was determined
by extracting the most frequent SYNFUNC
and CASE pairs from the training corpus.
107
? The size of the chain in elements as longer
chains are more likely to contain pronouns.
? The values of SEMCAT, SYNCAT and
SYNFUNC attributes of REF element
and REG08-TYPE and CASE of REFEX
element.
? The number of words in the REFEX value.
? Whether REF is in the first chain of the
document.
The last two features were introduced in order
to discriminate between candidate REFEX values
that have the same type and case. For example,
the number of words proved very useful when
selecting genitive case names and chi-squared
statistic ranks it as one of the best features together
with the compatibility feature, information about
previous elements in the chain and the longest
REFEX candidate.
Before the features are calculated, the text is
split into sentences and enriched with part-of-
speech information using the OpenNLP library.
1 The instances are fed into a binary classifier
that indicates whether the (REF, REFEX) pair is
good (i.e. the REFEX element is a good filler for
the REF element). Since each pair is classified
independently, it is possible to have zero, one or
more good REFEX candidates for a given REF.
Therefore, the system uses the confidence returned
by the classifier to rank the candidates and selects
the one that has the highest probability of being
good, regardless of the class assigned by the
classifier. In this way the system selects exactly
one REFEX for each REF.
3 Evaluation
The method proposed in this paper was evaluated
using two classifiers, both trained on the same
set of features. The first classifier is the standard
J48 decision tree algorithm implemented in Weka
(Witten and Frank, 2005). The run that used this
classifier is referred to in the rest of the paper as
standard run. Given the large number of negative
examples present in our training data, a meta-
classifier that is cost-sensitive was used for the
second run. In our case, the meta-classifier relies
on J48 and reweights training instances according
to the total cost assigned to each class. After
1http://opennlp.sourceforge.net/
experimenting with different cost matrices, we
decided to assign a cost of 3 to false negatives
and 1 to false positives, in this way biasing the
classifier towards a higher recall for YES answers.
The results obtained using this meta-classifier are
referred to as biased run. Our results on the
development set are presented in Table 1.
Measure Standard Biased
classification accuracy 94.40% 92.09%
total pairs 907 907
reg08 type matches 621 728
reg08 type accuracy 68.46% 80.26%
reg08 type precision 68.46% 80.26%
reg08 type recall 66.20% 77.61%
string matches 568 667
string accuracy 62.62% 73.53%
mean edit distance 0.845 0.613
mean normalised edit distance 0.351 0.239
Table 1: The evaluation results on the
development set
The first row in the table presents the accuracy
of the classifier on the training data using 10-fold
cross-validation. The very high accuracy is due
to the large number of negative instances in the
training data: assigning all the instances to the
class NO achieves a baseline accuracy of 88.96%.
The rest of the table presents the accuracy of the
system on the development set using the script
provided by the GREC-NEG organisers. As can
be seen, the best results are obtained by the biased
classifier despite performing worse at the level
of classification accuracy. This can be explained
by the fact that we do not use the output of the
classifier directly, instead using the classification
confidence.
4 Conclusions
This paper has presented our participation in the
GREC-NEG task with a machine learning system.
Currently the system tries to predict whether a
(REF, REFEX) pair is valid, but in the future
we plan to approach the task by using machine
learning methods to determine the values of
REG08-TYPE and CASE attributes.
References
Ruslan Mitkov. 2002. Anaphora resolution.
Longman.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann Publishers.
108
Book Reviews
Interactive Multi-Modal Question-Answering
? 2012 Association for Computational Linguistics
Antal van den Bosch? and Gosse Bouma? (editors)
(?Tilburg University and ?University of Groningen)
Berlin: Springer (Theory and Applications of Natural Language Processing series,
edited by Eduard Hovy), 2011, xii+279 pp; hardbound, ISBN 978-3-642-17524-4,
$124.00; e-book, ISBN 978-3-642-17525-1; paperbound, $24.95 or ?24.95 to members
of subscribing institutions
Reviewed by
Constantin Ora?san
University of Wolverhampton
Processing and presentation of multimodal information was one of the important di-
rections pursued by researchers in the areas related to information processing and
management in the first decade of this century (Stock and Zancanaro 2005; Maragos,
Potamianos, and Gros 2008; Lalanne et al 2009). The Interactive Multimodal Informa-
tion eXtraction (IMIX) Programme, a research program that ran between 2004 and 2009
and was funded by the Netherlands Organisation for Scientific Research (NWO), ad-
hered to this direction of research. This book contains a collection of articles describing
research carried out in the IMIX Programme. Given the large scale of the program, the
book covers only parts of it, arguably the most important ones: question answering,
(spoken) dialogue systems, and human?machine interaction.
The book is organized into four parts and an epilogue. The first part introduces
the IMIX Programme and the demonstrator developed by it. The main purpose of
the program was to bring together research groups from the Netherlands to build an
interactive multimodal question answering (QA) system that is able to answer gen-
eral encyclopedic medical questions. The IMIX Programme funded seven individual
projects that worked in a common field and contributed to a common demonstrator.
The fact that these projects ran largely independently is also apparent from the book
because there are few links between its chapters.
The architecture of the demonstrator is presented in the second article of the book,
?The IMIX Demonstrator? (Dennis Hofs, Boris van Schooten, and Rieks op den Akker).
The demonstrator showed users a fully functional system and allowed them to ask
questions using text, speech, and gestures. The answers produced by the system were
presented in the form of text, speech, or images, and could be used in follow-up ques-
tions. The article features a detailed description of the architecture, as well as several
screenshots and diagrams; these can be useful to researchers who want to find out
more about the demonstrator. In addition to the technical details, there is an interesting
discussion about the role of demonstrators in large projects and problems that need to
be addressed when building them. I think this brief discussion could be very useful for
anyone involved in a medium or large project that includes several research groups and
needs to build demonstrators.
The second part of the book focuses on dialog managers and covers most of the
interaction discussed in this book. First, the Vidiam (DIAlogue Management and the
VIsual channel) project is described in the article entitled ?Corpus-Based Develop-
ment of a Dialogue Manager for Multimodal Question Answering? (Van Schooten and
Computational Linguistics Volume 38, Number 2
Op den Akker). In addition to the corpora built in the project and the dialog manager
developed on the basis of these corpora, the article also contains a very good discussion
about how it is possible to integrate a dialog manager with a QA engine as a way of
developing an interactive QA system. I am not aware of any other articles that contain
all the information presented here in one place and in such detail. The second article in
this part, ?Multidimensional Dialogue Management? (Simon Keizer, Harry Bunt, and
Volha Petukhova), is more theoretical and presents a dialog manager built using the
framework of Dynamic Interpretation Theory (Bunt 2000) which is able to both interpret
and generate utterances using dialog acts. The article also presents briefly the way in
which this dialog manager was integrated in the IMIX demonstrator.
In my opinion, the editors of the book could have chosen a better title for the third
part of the book: ?Fusing Text, Speech, and Images.? Both articles in this part present
work done in the IMOGEN (Interactive Multimodal Output GENeration) project,1 one
of the subprojects embedded in the IMIX Programme that focused on producing mul-
timodal presentations that combine text, speech, and graphics. Only the first article
focuses on the multimodal aspect of the project, however. The other one discusses
only text processing. The article ?Experiments inMultimodal Information Presentation?
(Charlotte van Hooijdonk, Wauter Bosma, Emiel Krahmer, Alfons Maes, and Marie?t
Theune) presents three experiments for finding the appropriate way of combining
text and images when answering questions from the medical domain. In one of these
experiments, the multimodal answers are produced automatically. The other article,
?Text-to-Text Generation for Question Answering? (Bosma, Erwin Marsi, Krahmer, and
Theune), discusses sentence fusion and could fit very well in a book dedicated to text
summarization, as the method presented there is tested not only on data specific to
IMIX, but also on the DUC 2005 data.2
The fourth and the largest part of the book is ?Text Analysis for Question An-
swering.? It contains five articles, none of which describe a full QA system. Instead,
as the title suggests, they focus on various ways of processing texts that can help
with answering questions. One common feature of these articles is that they describe
methods to extract entities or relations between entities from texts. Most of the articles
also briefly discuss how this information is used in QA systems.
Most of the methods described in the fourth part of the book are now widely used
in computational linguistics, but when they were proposed a few years ago many
of them were rather innovative. For brevity, I give only a succinct indication of the
methods presented in the articles. ?Automatic Extraction of Medical Term Variants
from Multilingual Parallel Translations? (Lonneke van der Plas, Jo?rg Tiedemann, and
Ismail Fahmi) describes how to acquire medical terms and their variants from parallel
corpora. ?Relation Extraction for Open and Closed Domain Question Answering?
(Bouma, Fahmi, and Jori Mur) shows how it is possible to extract relations between
entities using dependency paths in a large collection of newspaper articles and in a
much smaller and closed domain corpus of medical documents. A sequence label-
ing method for entity recognition is presented in the article ?Constraint-Satisfaction
Inference for Entity Recognition? (Sander Canisius, Antal van den Bosch, and Walter
Daelemans). Large newspaper corpora and the Web are used in ?Extraction of Hyper-
nymy Information from Texts? (Erik Tjong Kim Sang, Katja Hofmann, and Maarten de
Rijke) to determine hypernymy relations between entities. The last article in the fourth
1 http://wwwhome.cs.utwente.nl/?theune/IMOGEN/.
2 http://www-nlpir.nist.gov/projects/duc/intro.html.
452
Book Reviews
part, ?Towards a Discourse-Driven Taxonomic Inference Model? (Piroska Lendvai)
looks at how the structure of discourse can be used for knowledge discovery from
encyclopedic texts. All the articles are well written and could be very interesting for
researchers working on information extraction.
The book finishes with an epilogue written by three members of the international
review panel of IMIX (Eduard Hovy, Jon Oberlander, and Norbert Reithinger) who give
a very good overview of the project, providing information that is not covered in any
other article of the book. For example, it expands on themultimodal research carried out
in the program and presents some details from the point of view of project management.
An objective evaluation of the overall program is also included.
The book is interesting and I enjoyed reading it. I have to point out, however, that
the research presented here is rather old. The IMIX Programme effectively ended in
2008, so it can be argued that most of the articles refer to work that is more than 5
years old. The authors of the epilogue praise the researchers involved in IMIX for the
large number of publications they produced. This means that most of the information
presented in the book was already published in one form or another somewhere else.
Despite this, the book compiles in one place information about the IMIX Programwhich
otherwise could take a while to collect.
When I started reading the book, I expected to find more about interactive multi-
modal question answering. Each of these topics is presented individually, but with the
exception of the article about the IMIX demonstrator, they are not discussed as a
whole. I was particularly disappointed by how little space was dedicated to multimodal
processing.
The articles in the book are written by different groups of authors and they are more
or less stand-alone. To achieve this, they all present brief background information about
the IMIX Programme. Despite the extra space required for it and the overlap between
the information presented in the articles, this is not necessarily bad because it means
that researchers who do not have the time to read the whole book can focus on only the
articles that are most relevant for them.
The potential readers of this book are likely to be researchers interested in the
processing of Dutch texts. Researchers in question answering, dialog processing, and
information extraction would also benefit from the book.
References
Bunt, Harry. 2000. Dialogue pragmatics and
context specification. In Abduction, Belief
and Context in Dialogue. John Benjamins,
Amsterdam.
Lalanne, Denis, Laurence Nigay, Philippe
Palanque, Peter Robinson, Jean
Vanderdonckt, and Jean-Franc?ois Ladry.
2009. Fusion engines for multimodal
input: a survey. In Proceedings of the
2009 International Conference on Multimodal
Interfaces, ICMI-MLMI ?09, pages 153?160,
New York, NY.
Maragos, Petros, Alexandros Potamianos,
and Patrick Gros, editors. 2008.Multimodal
Processing and Interaction: Audio, Video,
Text. Springer, Berlin.
Stock, Oliviero and Massimo Zancanaro,
editors. 2005.Multimodal Intelligent
Information Presentation. Springer, Berlin.
Constantin Ora?san is a Senior Lecturer in Computational Linguistics at the University of
Wolverhampton, UK. His current research interests include anaphora and coreference resolution,
automatic summarization, and question answering. Ora?san?s address is RIILP, Wulfruna St.,
University of Wolverhampton, WV1 1LY, UK; e-mail: C.Orasan@wlv.ac.uk.
453
