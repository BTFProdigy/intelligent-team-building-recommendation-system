Rapid Prototyping of Robust Language Understanding Modules
for Spoken Dialogue Systems
?Yuichiro Fukubayashi, ?Kazunori Komatani, ?Mikio Nakano,
?Kotaro Funakoshi, ?Hiroshi Tsujino, ?Tetsuya Ogata, ?Hiroshi G. Okuno
?Graduate School of Informatics, Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto
606-8501, Japan
{fukubaya,komatani}@kuis.kyoto-u.ac.jp
{ogata,okuno}@kuis.kyoto-u.ac.jp
?Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako, Saitama
351-0188, Japan
nakano@jp.honda-ri.com
{funakoshi,tsujino}@jp.honda-ri.com
Abstract
Language understanding (LU) modules for
spoken dialogue systems in the early phases
of their development need to be (i) easy
to construct and (ii) robust against vari-
ous expressions. Conventional methods of
LU are not suitable for new domains, be-
cause they take a great deal of effort to
make rules or transcribe and annotate a suf-
ficient corpus for training. In our method,
the weightings of the Weighted Finite State
Transducer (WFST) are designed on two
levels and simpler than those for conven-
tional WFST-based methods. Therefore,
our method needs much fewer training data,
which enables rapid prototyping of LU mod-
ules. We evaluated our method in two dif-
ferent domains. The results revealed that our
method outperformed baseline methods with
less than one hundred utterances as training
data, which can be reasonably prepared for
new domains. This shows that our method
is appropriate for rapid prototyping of LU
modules.
1 Introduction
The language understanding (LU) of spoken dia-
logue systems in the early phases of their devel-
opment should be trained with a small amount of
data in their construction. This is because large
amounts of annotated data are not available in the
early phases. It takes a great deal of effort and time
to transcribe and provide correct LU results to a
Figure 1: Relationship between our method and con-
ventional methods
large amount of data. The LU should also be robust,
i.e., it should be accurate even if some automatic
speech recognition (ASR) errors are contained in its
input. A robust LU module is also helpful when col-
lecting dialogue data for the system because it sup-
presses incorrect LU and unwanted behaviors. We
developed a method of rapidly prototyping LU mod-
ules that is easy to construct and robust against var-
ious expressions. It makes LU modules in the early
phases easier to develop.
Several methods of implementing an LU mod-
ule in spoken dialogue systems have been proposed.
Using grammar-based ASR is one of the simplest.
Although its ASR output can easily be transformed
into concepts based on grammar rules, complicated
grammars are required to understand the user?s ut-
terances in various expressions. It takes a great deal
of effort to the system developer. Extracting con-
210
Figure 2: Example of WFST for LU
cepts from user utterances by keyword spotting or
heuristic rules has also been proposed (Seneff, 1992)
where utterances can be transformed into concepts
without major modifications to the rules. However,
numerous complicated rules similarly need to be
manually prepared. Unfortunately, neither method
is robust against ASR errors.
To cope with these problems, corpus-based (Su-
doh and Tsukada, 2005; He and Young, 2005) and
Weighted Finite State Transducer (WFST)-based
methods (Potamianos and Kuo, 2000; Wutiwi-
watchai and Furui, 2004) have been proposed as LU
modules for spoken dialogue systems. Since these
methods extract concepts using stochastic analy-
sis, they do not need numerous complicated rules.
These, however, require a great deal of training data
to implement the module and are not suitable for
constructing new domains.
Here, we present a new WFST-based LU module
that has two main features.
1. A statistical language model (SLM) for ASR
and a WFST for parsing that are automatically
generated from the domain grammar descrip-
tion.
2. Since the weighting for the WFST is simpler
than that in conventional methods, it requires
fewer training data than conventional weight-
ing schemes.
Our method accomplishes robust LU with less ef-
fort using SLM-based ASR and WFST parsing. Fig-
ure 1 outlines the relationships between our method
and conventional schemes. Since rule- or grammar-
based approaches do not require a large amount of
data, they take less effort than stochastic techniques.
However, they are not robust against ASR errors.
Stochastic approaches, on the contrary, take a great
deal of effort to collect data but are robust against
ASR errors. Our method is an intermediate approach
that lies between these. That is, it is more robust than
rule- or grammar-based approaches and takes less
effort than stochastic techniques. This characteristic
makes it easier to rapidly prototype LU modules for
a new domain and helps development in the early
phases.
2 Related Work and WFST-based
Approach
A Finite State Transducer (FST)-based LU is ex-
plained here, which accepts ASR output as its in-
put. Figure 2 shows an example of the FST for a
video recording reservation domain. The input, ?,
means that a transition with no input is permitted at
the state transition. In this example, the LU mod-
ule returns the concept [month=2, day=22] for the
utterance ?It is February twenty second please?.
Here, a FILLER transition in which any word is ac-
cepted is appropriately allowed between phrases. In
Figure 2, ?F? represents 0 or more FILLER tran-
sitions. A FILLER transition from the start to the
end is inserted to reject unreliable utterances. This
FILLER transition enables us to ignore unnecessary
words listed in the example utterances in Table 1.
The FILLER transition helps to suppress the inser-
tion of incorrect concepts into LU results.
However, many output sequences are obtained for
one utterance due to the FILLER transitions, be-
cause the utterance can be parsed with several paths.
We used a WFST to select the most appropriate
path from several output sequences. The path with
the highest cumulative weight, w, is selected in a
211
Table 2: Many LU results for input ?It is February twenty second please?
LU output LU result w
It is February twenty second please month=2, day=22 2.0
It is FILLER twenty second please day=22 1.0
It is FILLER twenty second FILLER day=22 1.0
FILLER FILLER FILLER FILLER FILLER FILLER n/a 0
Table 1: Examples of utterances with FILLERs
ASR output
Well, it is February twenty second please
It is uhm, February twenty second please
It is February, twe-, twenty second please
It is February twenty second please, OK?
(LU result = [month=2, day=22])
WFST-based LU. In the example in Table 2, the
concept [month=2, day=22] has been selected, be-
cause its cumulative weight, w, is 2.0, which is the
highest.
The weightings of conventional WFST-based ap-
proaches used an n-gram of concepts (Potamianos
and Kuo, 2000) and that of word-concept pairs (Wu-
tiwiwatchai and Furui, 2004). They obtained the
n-grams from several thousands of annotated ut-
terances. However, it takes a great deal of ef-
fort to transcribe and annotate a large corpus. Our
method enables prototype LU modules to be rapidly
constructed that are robust against various expres-
sions with SLM-based ASR and WFST-based pars-
ing. The SLM and WFST are generated automat-
ically from a domain grammar description in our
toolkit. We need fewer data to train WFST, because
its weightings are simpler than those in conventional
methods. Therefore, it is easy to develop an LU
module for a new domain with our method.
3 Domain Grammar Description
A developer defines grammars, slots, and concepts
in a domain in an XML file. This description en-
ables an SLM for ASR and parsing WFST to be au-
tomatically generated. Therefore, a developer can
construct an LU module rapidly with our method.
Figure 3 shows an example of a descrip-
tion. A definition of a slot is described in
keyphrase-class tags and its keyphrases and
...
<keyphrase-class name="month">
...
<keyphrase>
<orth>February</orth>
<sem>2</sem>
</keyphrase>
...
</keyphrase-class>
...
<action type="specify-attribute">
<sentence> {It is} [*month] *day [please]
</sentence>
</action>
Figure 3: Example of a grammar description
the values are in keyphrase tags. The month is
defined as a slot in this figure. February and 2 are
defined as one of the phrases and values for the slot
month. A grammar is described in a sequence of
terminal and non-terminal symbols. A non-terminal
symbol represents a class of keyphrases, which is
defined in keyphrase-class. It begins with an
asterisk ?*? in a grammar description in sentence
tags. Symbols that can be skipped are enclosed
by brackets []. The FILLER transition described
in Section 2 is inserted between the symbols un-
less they are enclosed in brackets [] or braces {}.
Braces are used to avoid FILLER transitions from
being inserted. For example, the grammar in Figure
3 accepts ?It is February twenty second please.? and
?It is twenty second, OK??, but rejects ?It is Febru-
ary.? and ?It, uhm, is February twenty second.?.
A WFST for parsing can be automatically gener-
ated from this XML file. The WFST in Figure 2 is
generated from the definition in Figure 3. Moreover,
we can generate example sentences from the gram-
mar description. The SLM for the speech recognizer
is generated with our method by using many exam-
ple sentences generated from the defined grammar.
212
4 Weighting for ASR Outputs on Two
Levels
We define weights on two levels for a WFST. The
first is a weighting for ASR outputs, which is set to
select paths that are reliable at a surface word level.
The second is a weighting for concepts, which is
used to select paths that are reliable on a concept
level. The weighting for concepts reflects correct-
ness at a more abstract level than the surface word
level. The weighting for ASR outputs consists of
two categories: a weighting for ASR N-best outputs
and one for accepted words. We will describe the
definitions of these weightings in the following sub-
sections.
4.1 Weighting for ASR N-Best Outputs
The N-best outputs of ASR are used for an input of
a WFST. Weights are assigned to each sentence in
ASR N-best outputs. Larger weights are given to
more reliable sentences, whose ranks in ASR N-best
are higher. We define this preference as
wis =
e??scorei
?N
j e??scorej
,
where wis is a weight for the i-th sentence in ASRN-best outputs, ? is a coefficient for smoothing, and
scorei is the log-scaled score of the i-th ASR out-put. This weighting reflects the reliability of the
ASR output. We set ? to 0.025 in this study after
a preliminary experiment.
4.2 Weighting for Accepted Words
Weights are assigned to word sequences that have
been accepted by the WFST. Larger weights are
given to more reliable sequences of ASR outputs at
the surface word level. Generally, longer sequences
having more words that are not fillers and more re-
liable ASR outputs are preferred. We define these
preferences as the weights:
1. word(const.): ww = 1.0,
2. word(#phone): ww = l(W ), and
3. word(CM): ww = CM(W ) ? ?w.
The word(const.) gives a constant weight to
all accepted words. This means that sequences
with more words are simply preferred. The
word(#phone) takes the length of each accepted
word into consideration. This length is measured by
its number of phonemes, which are normalized by
that of the longest word in the vocabulary. The nor-
malized values are denoted as l(W ) (0 < l(W ) ?
1). By adopting word(#phone), the length of se-
quences is represented more accurately. We also
take the reliability of the accepted words into ac-
count as word(CM). This uses confidence measures
(Lee et al, 2004) for a word, W , in ASR outputs,
which are denoted as CM(W ). The ?w is the thresh-old for determining whether word W is accepted or
not. The ww obtains a negative value for an unreli-able word W when CM(W ) is lower than ?w. Thisrepresents a preference for longer and more reliable
sequences.
4.3 Weighting for Concepts
In addition to the ASR level, weights on a concept
level are also assigned. The concepts are obtained
from the parsing results by the WFST, and contain
several words. Weights for concepts are defined by
using the measures of all words contained in a con-
cept.
We prepared three kinds of weights for the con-
cepts:
1. cpt(const.): wc = 1.0,
2. cpt(avg):
wc =
?
W (CM(W ) ? ?c)
#W , and
3. cpt(#pCM(avg)):
wc =
?
W (CM(W ) ? l(W ) ? ?c)
#W ,
where W is a set of accepted words, W , in the corre-
sponding concept, and #W is the number of words
in W .
The cpt(const.) represents a preference for
sequences with more concepts. The cpt(avg)
is defined as the weight by using the CM(W )
of each word contained in the concept. The
cpt(#pCM(avg)) represents a preference for longer
and reliable sequences with more concepts. The ?cis the threshold for the acceptance of a concept.
213
Table 3: Examples of weightings when parameter set is: word(CM) and cpt(#pCM(avg))
ASR onput No, it is February twenty second
LU output FILLER it is February twenty second
CM(W ) 0.3 0.7 0.6 0.9 1.0 0.9
l(W ) 0.3 0.2 0.2 0.9 0.6 0.5
Concept - - - month=2 day=22
word - 0.7 ? ?w 0.6 ? ?w 0.9 ? ?w 1.0 ? ?w 0.9 ? ?wcpt - - - (0.9 ? 0.9 ? ?c)/1 (1.0 ? 0.6 ? ?c + 0.9 ? 0.5 ? ?c)/2
'
&
$
%
Reference From June third please
ASR output From June third uhm FIT please LU result
CM(W ) 0.771 0.978 0.757 0.152 0.525 0.741
LU reference From June third FILLER FILLER FILLER month:6, day:3
Our method From June third FILLER FILLER FILLER month:6, day:3
Keyword spotting From June third FILLER FIT please month:6, day:3, car:FIT
(?FIT? is the name of a car.)
Figure 4: Example of LU with WFST
4.4 Calculating Cumulative Weight and
Training
The LU results are selected based on the weighted
sum of the three weights in Subsection 4.3 as
wi = wis + ?w
?
ww + ?c
?
wc
The LU module selects an output sequence with
the highest cumulative weight, wi, for 1 ? i ? N .
Let us explain how to calculate cumulative weight
wi by using the example specified in Table 3. Here,
word(CM) and cpt(#pCM(avg)) are selected as pa-
rameters. The sum of weights in this table for ac-
cepted words is ?w(4.1 ? 5?w), when the input se-quence is ?No, it is February twenty second.?.
The sum of weights for concepts is ?c(1.335 ? 2?c)because the weight for ?month=2? is ?c(0.81 ? ?c)and the weight for ?day=22? is ?c(0.525 ? ?c).Therefore, cumulative weight wi for this input se-
quence is wis + ?w(4.1 ? 5?w) + ?c(1.335 ? 2?c).In the training phase, various combinations of pa-
rameters are tested, i.e., which weightings are used
for each of ASR output and concept level, such as
N = 1 or 10, coefficient ?w,c = 1.0 or 0, and thresh-old ?w,c = 0 to 0.9 at intervals of 0.1, on the train-ing data. The coefficient ?w,c = 0 means that acorresponding weight is not added. The optimal pa-
rameter settings are obtained after testing the various
combinations of parameters. They make the concept
error rate (CER) minimum for a training data set.
We calculated the CER in the following equation:
CER = (S +D + I)/N , where N is the number of
concepts in a reference, and S, D, and I correspond
to the number of substitution, deletion, and insertion
errors.
Figure 4 shows an example of LU with our
method, where it rejects misrecognized concept
[car:FIT], which cannot be rejected by keyword
spotting.
5 Experiments and Evaluation
5.1 Experimental Conditions
We discussed our experimental investigation into the
effects of weightings in Section 4. The user utter-
ance in our experiment was first recognized by ASR.
Then, the i-th sentence of ASR output was input to
WFST for 1 ? i ? N , and the LU result for the
highest cumulative weight, wi, was obtained.
We used 4186 utterances in the video recording
reservation domain (video domain), which consisted
of eight different dialogues with a total of 25 differ-
ent speakers. We also used 3364 utterances in the
rent-a-car reservation domain (rent-a-car domain) of
214
eight different dialogues with 23 different speakers.
We used Julius 1 as a speech recognizer with an
SLM. The language model was prepared by using
example sentences generated from the grammars of
both domains. We used 10000 example sentences in
the video and 40000 in the rent-a-car domain. The
number of the generated sentences was determined
empirically. The vocabulary size was 209 in the
video and 891 in the rent-a-car domain. The average
ASR accuracy was 83.9% in the video and 65.7%
in the rent-a-car domain. The grammar in the video
domain included phrases for dates, times, channels,
commands. That of the rent-a-car domain included
phrases for dates, times, locations, car classes, op-
tions, and commands. The WFST parsing mod-
ule was implemented by using the MIT FST toolkit
(Hetherington, 2004).
5.2 Performance of WFST-based LU
We evaluated our method in the two domains: video
and rent-a-car. We compared the CER on test data,
which was calculated by using the optimal settings
for both domains. We evaluated the results with 4-
fold cross validation. The number of utterances for
training was 3139 (=4186*(3/4)) in the video and
2523 (=3364*(3/4)) in the rent-a-car domain.
The baseline method was simple keyword spot-
ting because we assumed a condition where a large
amount of training data was not available. This
method extracts as many keyphrases as possible
from ASR output without taking speech recogni-
tion errors and grammatical rules into consideration.
Both grammar-based and SLM-based ASR outputs
are used for input in keyword spotting (denoted as
?Grammar & spotting? and ?SLM & spotting? in
Table 4). The grammar for grammar-based ASR
was automatically generated by the domain descrip-
tion file. The accuracy of grammar-based ASR was
66.3% in the video and 43.2% in the rent-a-car do-
main.
Table 4 lists the CERs for both methods. In key-
word spotting with SLM-based ASR, the CERs were
improved by 5.2 points in the video and by 22.2
points in the rent-a-car domain compared with those
with grammar-based ASR. This is because SLM-
based ASR is more robust against fillers and un-
1http://julius.sourceforge.jp/
Table 4: Concept error rates (CERs) in each domain
Domain Grammar &spotting SLM &spotting Ourmethod
Video 22.1 16.9 13.5
Rent-a-car 51.1 28.9 22.0
known words than grammar-based ASR. The CER
was improved by 3.4 and 6.9 points by optimal
weightings for WFST. Table 5 lists the optimal pa-
rameters in both domains. The ?c = 0 in the videodomain means that weights for concepts were not
used. This result shows that optimal parameters de-
pend on the domain for the system, and these need
to be adapted for each domain.
5.3 Performance According to Training Data
We also investigated the relationship between the
size of the training data for our method and the CER.
In this experiment, we calculated the CER in the
test data by increasing the number of utterances for
training. We also evaluated the results by 4-fold
cross validation.
Figures 5 and 6 show that our method outper-
formed the baseline methods by about 80 utterances
in the video domain and about 30 utterances in
the rent-a-car domain. These results mean that our
method can effectively be used to rapidly prototype
LU modules. This is because it can achieve robust
LU with fewer training data compared with conven-
tional WFST-based methods, which need over sev-
eral thousand sentences for training.
6 Conclusion
We developed a method of rapidly prototyping ro-
bust LU modules for spoken language understand-
ing. An SLM for a speech recognizer and a WFST
for parsing were automatically generated from a do-
main grammar description. We defined two kinds
of weightings for the WFST at the word and con-
cept levels. These two kinds of weightings were
calculated by ASR outputs. This made it possi-
ble to create an LU module for a new domain with
less effort because the weighting scheme was rel-
atively simpler than those of conventional methods.
The optimal parameters could be selected with fewer
training data in both domains. Our experiment re-
215
Table 5: Optimal parameters in each domain
Domain N ?w ww ?c wc
Video 1 1.0 word(const.) 0 -
Rent-a-car 10 1.0 word(CM)-0.0 1.0 cpt(#pCM(avg))-0.8
 0
 5
 10
 15
 20
 25
 30
 35
 40
 3000 1000 500 250 100 50 10
C
E
R
#utt. for training
Grammar-based ASR & keyword spotting
SLM-based ASR & keyword spotting
Our method
Figure 5: CER in video domain
 0
 5
 10
 15
 20
 25
 30
 50
 55
 3000 1000 500 250 100 50 10
C
E
R
#utt. for training
Grammar-based ASR & keyword spotting
SLM-based ASR & keyword spotting
Our method
Figure 6: CER in rent-a-car domain
vealed that the CER could be improved compared to
the baseline by training optimal parameters with a
small amount of training data, which could be rea-
sonably prepared for new domains. This means that
our method is appropriate for rapidly prototyping
LU modules. Our method should help developers
of spoken dialogue systems in the early phases of
development. We intend to evaluate our method on
other domains, such as database searches and ques-
tion answering in future work.
Acknowledgments
We are grateful to Dr. Toshihiko Ito and Ms. Yuka
Nagano of Hokkaido University for constructing the
rent-a-car domain system.
References
Yulan He and Steve Young. 2005. Spoken Language
Understanding using the Hidden Vector State Model.Speech Communication, 48(3-4):262?275.
Lee Hetherington. 2004. The MIT finite-state trans-ducer toolkit for speech and language processing. InProc. 6th International Conference on Spoken Lan-guage Processing (INTERSPEECH-2004 ICSLP).
Akinobu Lee, Kiyohiro Shikano, and Tatsuya Kawahara.
2004. Real-time word confidence scoring using lo-cal posterior probabilities on tree trellis search. InProc. 2004 IEEE International Conference on Acous-tics, Speech, and Signal Processing (ICASSP 2004),volume 1, pages 793?796.
Alexandors Potamianos and Hong-Kwang J. Kuo. 2000.
Statistical recursive finite state machine parsingfor speech understanding. In Proc. 6th Interna-tional Conference on Spoken Language Processing(INTERSPEECH-2000 ICSLP), pages 510?513.
Stephanie Seneff. 1992. TINA: A natural language sys-tem for spoken language applications. ComputationalLinguistics, 18(1):61?86.
Katsuhito Sudoh and Hajime Tsukada. 2005. Tightly in-
tegrated spoken language understanding using word-to-concept translation. In Proc. 9th European Con-ference on Speech Communication and Technology(INTERSPEECH-2005 Eurospeech), pages 429?432.
Chai Wutiwiwatchai and Sadaoki Furui. 2004. Hybridstatistical and structural semantic modeling for Thaimulti-stage spoken language understanding. In Proc.HLT-NAACL Workshop on Spoken Language Under-standing for Conversational Systems and Higher LevelLinguistic Information for Speech Processing, pages
2?9.
216
Proceedings of NAACL HLT 2009: Short Papers, pages 133?136,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Speech Understanding Framework
that Uses Multiple Language Models and Multiple Understanding Models
?Masaki Katsumaru, ?Mikio Nakano, ?Kazunori Komatani,
?Kotaro Funakoshi, ?Tetsuya Ogata, ?Hiroshi G. Okuno
?Graduate School of Informatics, Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto
606-8501, Japan
{katumaru, komatani}@kuis.kyoto-u.ac.jp
{ogata, okuno}@kuis.kyoto-u.ac.jp
?Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako, Saitama
351-0188, Japan
{nakano, funakoshi}@jp.honda-ri.com
Abstract
The optimal combination of language model
(LM) and language understanding model
(LUM) varies depending on available training
data and utterances to be handled. Usually, a
lot of effort and time are needed to find the op-
timal combination. Instead, we have designed
and developed a new framework that uses
multiple LMs and LUMs to improve speech
understanding accuracy under various situa-
tions. As one implementation of the frame-
work, we have developed a method for select-
ing the most appropriate speech understand-
ing result from several candidates. We use
two LMs and three LUMs, and thus obtain six
combinations of them. We empirically show
that our method improves speech understand-
ing accuracy. The performance of the oracle
selection suggests further potential improve-
ments in our system.
1 Introduction
The speech understanding component in a spoken
dialogue system consists of an automatic speech
recognition (ASR) component and a language un-
derstanding (LU) component. To develop a speech
understanding component, we need to prepare an
ASR language model (LM) and a language under-
standing model (LUM) for the dialogue domain
of the system. There are many types of LMs
such as finite-state grammars and N-grams, and
many types of LUMs such as finite-state transduc-
ers (FST), weighted finite-state transducers (WFST),
and keyphrase-extractors (extractor). Selecting a
suitable combination of LM and LUM is necessary
for robust speech understanding against various user
utterances.
Conventional studies of speech understanding
have investigated which LM and LUM give the best
performance by using fixed training and test data
such as the Air Travel Information System (ATIS)
corpus. However, in real system development, re-
sources such as training data for statistical models
and efforts to write finite-state grammars vary ac-
cording to the available human resources or budgets.
Domain-dependent training data are particularly dif-
ficult to obtain. Therefore, in conventional system
development, system developers determine the types
of LM and LUM by trial and error. Every LM and
LUM has some advantages and disadvantages, so it
is difficult for a single combination of LM and LUM
to gain high accuracy except in a situation involv-
ing a lot of training data and effort. Therefore, using
multiple speech understanding methods is a more ef-
fective approach.
In this paper, we propose a speech understand-
ing framework called ?Multiple Language models
and Multiple Understanding models (MLMU)?, in
which multiple LMs and LUMs are used, to achieve
better performance under the various development
situations. It selects the best speech understanding
result from the multiple results generated by arbi-
trary combinations of LMs and LUMs.
So far there have been several attempts to im-
prove ASR and speech understanding using mul-
tiple speech recognizers and speech understanding
modules. ROVER (Fiscus, 1997) tried to improve
ASR accuracy by integrating the outputs of multi-
ple ASRs with different acoustic and language mod-
133
LM 1utterance LM 2
LM n 
resultconfidenceintegrationcomponent
LUcomponent speechunderstandingresultsASRcomponent
LUM n LUM 2
LUM 1
LM: Language Model
LUM: Language Understanding Model
Figure 1: Flow of speech understanding in MLMU
els. The work is different from our study in the fol-
lowing two points: it does not deal with speech un-
derstanding, and it assumes that each ASR is well-
developed and achieves high accuracy for a variety
of speech inputs. Eckert et al (1996) used multiple
LMs to deal with both in-grammar utterances and
out-of-grammar utterances, but did not mention lan-
guage understanding. Hahn et al (2008) used mul-
tiple LUMs, but just a single language model.
2 Speech Understanding Framework
MLMU
MLMU is a framework by which system developers
can use multiple speech understanding methods by
preparing multiple LMs and multiple LUMs. Fig-
ure 1 illustrates the flow of speech understanding in
MLMU. System developers list available LMs and
LUMs for each system?s domain, and the system
understands utterances by using these models. The
framework selects one understanding result from
multiple results or calculates a confidence score of
the result by using the generated multiple under-
standing results.
MLMU can improve speech understanding for the
following reason. The performance of each speech
understanding (a combination of LM and LUM)
might not be very high when either training data for
the statistical model or available expertise and ef-
fort for writing grammar are insufficient. In such
cases, some utterances might not be covered by the
system?s finite-state grammar LM, and probability
estimation in the statistical models may not be very
good. Using multiple speech understanding mod-
els is expected to solve this problem because each
model has different specialities. For example, finite-
state grammar LMs and FST-based LUMs achieve
high accuracy in recognizing and understanding in-
grammar utterances, whereas out-of-grammar utter-
ances are covered by N-gram models and LUMs
based on WFST and keyphrase-extractors. There-
fore it is more possible that the understanding results
of MLMU will include the correct result than a case
when a single understanding model is used.
The understanding results of MLMU will be help-
ful in many ways. We used them to achieve better
understanding accuracy by selecting the most reli-
able one. This selection is based on features con-
cerning ASR results and language understanding re-
sults. It is also possible to delay the selection, hold-
ing multiple understanding result candidates that
will be disambiguated as the dialogue proceeds (Bo-
hus, 2004). Furthermore, confidence scores, which
enable an efficient dialogue management (Komatani
and Kawahara, 2000), can be calculated by ranking
these results or by voting on them, by using multi-
ple speech understanding results. The understanding
results can be used in the discourse understanding
module and the dialogue management module. They
can choose one of the understanding results depend-
ing on the dialogue situation.
3 Implementation
3.1 Available Language Models and Language
Understanding Models
We implemented MLMU as a library of RIME-
TK, which is a toolkit for building multi-domain
spoken dialogue systems (Nakano et al, 2008).
With the current implementation, developers can use
the following LMs:
1. A LM based on finite-state grammar (FSG)
2. A domain-dependent statistical N-gram model
(N-gram)
and the following LUMs:
1. Finite-state transducer (FST)
2. Weighted FST (WFST)
3. Keyphrase-extractor (extractor).
System developers can use multiple finite-state-
grammar-based LMs or N-gram-based LMs, and
134
also multiple FSTs and WFSTs. They can specify
the combination for each domain by preparing LMs
and LUMs. They can specify grammar models when
sufficient human labor is available for writing gram-
mar, and specify statistical models when a corpus for
training models is available.
3.2 Selecting Understanding Result based on
ASR and LU Features
We also implemented a mechanism for selecting one
of the understanding results as the best hypothesis.
The mechanism chooses the result with the highest
estimated probability of correctness. Probabilities
are estimated for each understanding result by using
logistic regression, which uses several ASR and LU
features.
We define Pi as the probability that speech under-
standing result i is correct, and we select one result
based on argmax
i
Pi. We denote each speech un-
derstanding result as i (i = 1,. . . ,6). We constructed
a logistic regression model for Pi. The regression
function can be written as:
Pi = 11 + exp(?(ai1Fi1 + . . . + aimFim + bi)) .(1)
The coefficients ai1, . . . , aim, bi were fitted us-
ing training data. The independent variables
Fi1, Fi2, ..., Fim are listed in Table 1. In the table,
n indicates the number of understanding results, that
is, n = 6 in this paper?s experiment. Here, we denote
the features as Fi1, Fi2, ..., Fim.
Features from Fi1 to Fi3 represent characteristics
of ASR results. The acoustic scores were normal-
ized by utterance durations in seconds. These fea-
tures are used for verifying its ASR result. Features
from Fi4 to Fi9 represent characteristics of LU re-
sults. Features from Fi4 to Fi6 are defined on the
basis of the concept-based confidence scores (Ko-
matani and Kawahara, 2000).
4 Preliminary Experiment
We conducted a preliminary experiment to show the
potential of the framework by using the two LMs
and three LUMs noted in Section 3.1.
Table 1: Features from speech understanding result i
Fi1: acoustic score of ASR
Fi2: difference between Fi1 and acoustic score
of ASR for utterance verification
Fi3: utterance duration [sec.]
Fi4: average confidence scores for concepts in i
Fi5: average of Fi4 ( 1n
?n
i Fi4)
Fi6: proportion of Fi4 (Fi4 /?ni Fi5)
Fi7: average # concepts ( 1n
?n
i #concepti)
Fi8: max. # concepts (max (#concepti) )
Fi9: min. # concepts (min (#concepti) )
4.1 Preparing LMs and LUMs
The finite-state grammar rules were written in sen-
tence units manually. A domain-dependent statisti-
cal N-gram model was trained on 10,000 sentences
randomly generated from the grammar. The vocab-
ulary sizes of the grammar LM and the domain-
dependent statistical LM were both 278. We
also used a domain-independent statistical N-gram
model for obtaining acoustic scores for utterance
verification, which was trained onWeb texts (Kawa-
hara et al, 2004). Its vocabulary size was 60,250.
The grammar used in the FST was the same as the
FSG used as one of the LMs, which was manually
written by a system developer. The WFST-based LU
was based on a method to estimate WFST parame-
ters with a small amount of data (Fukubayashi et al,
2008). Its parameters were estimated by using 105
utterances of just one user. The keyphrase extrac-
tor extracts as many concepts as possible from an
ASR result on the basis of a grammar while ignor-
ing words that do not match the grammar.
4.2 Target Data for Evaluation
We used 3,055 utterances in the rent-a-car reserva-
tion domain (Nakano et al, 2007). We used Julius
(ver. 4.0.2) as the speech recognizer and a 3000-
state phonetic tied-mixture (PTM) triphone model
as the acoustic model1. ASR accuracy in mora ac-
curacy when using the FSG and the N-gram model
were 71.9% and 75.5% respectively. We used con-
cept error rates (CERs) to represent the speech un-
derstanding accuracy, which is calculated as fol-
1http://julius.sourceforge.jp/
135
Table 2: CERs [%] for each speech understanding
method
speech understanding method
(LM + LUM) CER
(1) FSG + FST 26.9
(2) FSG + WFST 29.9
(3) FSG + extractor 27.1
(4) N-gram + FST 35.2
(5) N-gram + WFST 25.3
(6) N-gram + extractor 26.0
selection from (1) through (6) (our method) 22.7
oracle selection 13.5
lows:
CER = # error concepts#concepts in utterances . (2)
We manually annotated whether an understanding
result of each utterance was correct or not, and
used them as training data to fit the coefficients
ai1, . . . , aim, bi.
4.3 Evaluation in Concept Error Rates
We fitted the coefficients of regression functions and
selected understanding results with a 10-fold cross
validation. Table 2 lists the CERs based on combi-
nations of single LM and LUM and by our method.
Of all combinations of single LM and LUM, the best
accuracy was obtained with (5) (N-gram + WFST).
Our method improved by 2.6 points over (5). Al-
though we achieved a lower CER, we used a lot
of data to estimate logistic regression coefficients.
Such a large amount of data may not be available in a
real situation. We will conduct more experiments by
changing the amount of training data. Table 2 also
shows the accuracy of the oracle selection, which
selected the best speech understanding result man-
ually. The CER of the oracle selection was 13.5%,
a significant improvement compared to all combina-
tions of a LM and LUM. There is no combination of
a LM and LUM whose understanding results were
not selected at all in the oracle selection and our
method?s selection. These results show that using
multiple LMs and multiple LUMs can potentially
improve speech understanding accuracy.
5 Ongoing work
We will conduct more experiments in other domains
or with other resources to evaluate the effectiveness
of our framework. We plan to investigate the case
in which a smaller amount of the training data is
used to estimate the coefficients of the logistic re-
gressions. Furthermore, finding a way to calculate
confidence scores of speech understanding results is
on our agenda.
References
Dan Bohus. 2004. Error awareness and recovery in
task-oriented spoken dialogue systems. Ph.D. thesis,
Carnegie Mellon University.
Wieland Eckert, Florian Gallwitz, and Heinrich Nie-
mann. 1996. Combining stochastic and linguistic lan-
guage models for recognition of spontaneous speech.
In Proc. ICASSP, pages 423?426.
Jonathan G. Fiscus. 1997. A post-processing system
to yield reduced word error rates: Recognizer Out-
put Voting Error Reduction (ROVER). In Proc. ASRU,
pages 347?354.
Yuichiro Fukubayashi, Kazunori Komatani, Mikio
Nakano, Kotaro Funakoshi, Hiroshi Tsujino, Tetsuya
Ogata, and Hiroshi G. Okuno. 2008. Rapid prototyp-
ing of robust language understanding modules for spo-
ken dialogue systems. In Proc. IJCNLP, pages 210?
216.
Stefan Hahn, Patrick Lehnen, and Hermann Ney. 2008.
System combination for spoken language understand-
ing. In Proc. Interspeech, pages 236?239.
Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-
sunobu Itou, and Kiyohiro Shikano. 2004. Recent
progress of open-source LVCSR Engine Julius and
Japanese model repository. In Proc. ICSLP, pages
3069?3072.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management using
concept-level confidence measures of speech recog-
nizer output. In Proc. COLING, volume 1, pages 467?
473.
Mikio Nakano, Yuka Nagano, Kotaro Funakoshi, Toshi-
hiko Ito, Kenji Araki, Yuji Hasegawa, and Hiroshi Tsu-
jino. 2007. Analysis of user reactions to turn-taking
failures in spoken dialogue systems. In Proc. SIGdial,
pages 120?123.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and
Hiroshi Tsujino. 2008. A framework for building con-
versational agents based on a multi-expert model. In
Proc. SIGdial, pages 88?91.
136
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 9?17,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multi-Domain Spoken Dialogue System
with Extensibility and Robustness against Speech Recognition Errors
Kazunori Komatani Naoyuki Kanda Mikio Nakano?
Kazuhiro Nakadai? Hiroshi Tsujino? Tetsuya Ogata Hiroshi G. Okuno
Kyoto University, Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
{komatani,ogata,okuno}@i.kyoto-u.ac.jp
? Honda Research Institute Japan Co., Ltd., 8-1 Honcho, Wako, Saitama 351-0188, Japan
{nakano,nakadai,tsujino}@jp.honda-ri.com
Abstract
We developed a multi-domain spoken dia-
logue system that can handle user requests
across multiple domains. Such systems
need to satisfy two requirements: extensi-
bility and robustness against speech recog-
nition errors. Extensibility is required to
allow for the modification and addition
of domains independent of other domains.
Robustness against speech recognition er-
rors is required because such errors are
inevitable in speech recognition. How-
ever, the systems should still behave ap-
propriately, even when their inputs are er-
roneous. Our system was constructed on
an extensible architecture and is equipped
with a robust and extensible domain selec-
tion method. Domain selection was based
on three choices: (I) the previous domain,
(II) the domain in which the speech recog-
nition result can be accepted with the high-
est recognition score, and (III) other do-
mains. With the third choice we newly
introduced, our system can prevent dia-
logues from continuously being stuck in
an erroneous domain. Our experimental
results, obtained with 10 subjects, showed
that our method reduced the domain selec-
tion errors by 18.3%, compared to a con-
ventional method.
1 Introduction
Many spoken dialogue systems have been devel-
oped for various domains, including: flight reser-
vations (Levin et al, 2000; Potamianos and Kuo,
2000; San-Segundo et al, 2000), train travel in-
formation (Lamel et al, 1999), and bus informa-
tion (Komatani et al, 2005b; Raux and Eskenazi,
2004). Since these systems only handle a sin-
gle domain, users must be aware of the limita-
tions of these domains, which were defined by
the system developer. To handle various domains
through a single interface, we have developed a
multi-domain spoken dialogue system, which is
composed of several single-domain systems. The
system can handle complicated tasks that contain
requests across several domains.
Multi-domain spoken dialogue systems need to
satisfy the following two requirements: (1) exten-
sibility and (2) robustness against speech recog-
nition errors. Many such systems have been de-
veloped on the basis of a master-slave architec-
ture, which is composed of a single master module
and several domain experts handling each domain.
This architecture has the advantage that each do-
main expert can be independently developed, by
modifying existing experts or adding new experts
into the system. In this architecture, the master
module needs to select a domain expert to which
response generation and dialogue management for
the user?s utterance are committed. Hereafter, we
will refer to this selecting process domain selec-
tion.
The second requirement is robustness against
speech recognition errors, which are inevitable in
systems that use speech recognition. Therefore,
these systems must robustly select domains even
when the input may be incorrect due to speech
recognition errors.
We present an architecture for a multi-domain
spoken dialogue system that incorporates a new
domain selection method that is both extensi-
ble and robust against speech recognition errors.
Since our system is based on extensible architec-
ture similar to that developed by O?Neill (O?Neill
et al, 2004), we can add and modify the domain
9
Speech recognition
Domain selection
Utterance generation
Speech synthesis
User
utterance
System
response
User System
Central module Expert for domain A
method
Language understanding
Dialogue state update
Dialogue management
Dialogue statesvariable
Expert for domain B
method
variable
Expert for domain B
method
variable
Expert for domain B
method
Language understanding
Dialogue state update
Dialogue management
Dialogue statesvariable
Figure 1: Distributed-type architecture for multi-domain spoken dialogue systems
experts easily. In order to maintain robustness,
domain selection takes into consideration vari-
ous features concerning context and situations of
the dialogues. We also designed a new selection
framework that satisfies the extensibility issue by
abstracting the transitions between the current and
next domains. Specifically, our system selects the
next domain based on: (I) the previous domain,
(II) the domain in which the speech recognition
result can be accepted with the highest recognition
score, and (III) other domains. Conventional meth-
ods cannot select the correct domain when neither
the previous domain nor the speech recognition re-
sults for a current utterance are correct. To over-
come this drawback, we defined another choice as
(III) that enables the system to detect an erroneous
situation and thus prevent the dialogue from con-
tinuing to be incorrect. We modeled this frame-
work as a classification problem using machine
learning, and showed it is effective by perform-
ing an experimental evaluation of 2,205 utterances
collected from 10 subjects.
2 Architecture used for Multi-Domain
Spoken Dialogue Systems
In multi-domain spoken dialogue systems, the sys-
tem design is more complicated than in single do-
main systems. When the designed systems are
closely related to each other, a modification in a
certain domain may affect the whole system. This
type of a design makes it difficult to modify ex-
isting domains or to add new domains. Therefore,
a distributed-type architecture has been previously
proposed (Lin et al, 2001), which enables system
developers to design each domain independently.
In this architecture, the system is composed of
two kinds of components: a part that can be de-
signed independently of all other domains, and a
part in which relations among domains should be
considered. By minimizing the latter component,
a system developer can design each domain semi-
independently, which enables domains to be eas-
ily added or modified. Many existing systems are
based on this architecture (Lin et al, 2001; O?Neill
et al, 2004; Pakucs, 2003; Nakano et al, 2005).
Thus, we adopted the distributed-type architec-
ture (Nakano et al, 2005). Our system is roughly
composed of two parts, as shown in Figure 1: sev-
eral experts that control dialogues in each domain,
and a central module that controls each expert.
When a user speaks to the system, the central mod-
ule drives a speech recognizer, and then passes
the result to each domain expert. Each expert,
which controls its own domains, executes a lan-
guage understanding module, updates its dialogue
states based on the speech recognition result, and
returns the information required for domain selec-
tion1. Based on the information obtained from
the experts, the central module selects an appro-
priate domain for giving the response. An expert
then takes charge of the selected domain and deter-
mines the next dialogue act based on its dialogue
state. The central module generates a response
based on the dialogue act obtained from the expert,
and outputs the synthesized speech to the user.
Communications between the central module and
each expert are realized using method-calls in the
central module. Each expert is required to have
several methods, such as utterance understanding
or response selection, to be considered an expert
1Dialogue states in a domain that are not selected during
domain selection are returned to their previous states.
10
in this architecture.
As was previously described, the central mod-
ule is not concerned with processing the speech
recognition results; instead, the central module
leaves this task to each expert. Therefore, it is
important that the central module selects an ex-
pert that is committed to the process of the speech
recognition result. Furthermore, information used
during domain selection should also be domain
independent, because this allows easier domain
modification and addition, which is, after all, the
main advantage of distributed-type architecture.
3 Extensible and Robust Domain
Selection
Domain selection in the central module should
also be performedwithin an extensible framework,
and also should be robust against speech recogni-
tion errors.
In many conventional methods, domain selec-
tion is based on estimating the most likely do-
mains based on the speech recognition results.
Since these methods are heavily dependent on
the performance of the speech recognizers, they
are not robust because the systems will fail when
a speech recognizer fails. To behave robustly
against speech recognition errors, the success of
speech recognition and of domain selection should
be treated separately. Furthermore, in some con-
ventional methods, accurate language models are
required to construct the domain selection parts
before new domains are added to a multi-domain
system. This means that they are not extensible.
When selecting a domain, other studies have
used the information on the domain in which a pre-
vious response was made. Lin et al (2001) gave
preference to the domain selected in the previous
turn by adding a certain score as an award when
comparing the N-best candidates of the speech
recognition for each domain. Lane and Kawa-
hara (2005) also assigned a similar preference in
the classification with Support Vector Machine
(SVM). A system described in (O?Neill et al,
2004) does not change its domain until its sub-task
is completed, which is a constraint similar to keep-
ing dialogue in one domain. Since these methods
assume that the previous domain is most likely the
correct domain, it is expected that these methods
keep a system in the domain despite errors due
to speech recognition problems. Thus, should do-
main selection be erroneous, the damage due to the
Same domain as
previous response
Domain having 
the highest score in
speech recognizer
User utterancePrevious turn Current turn
(I)
(II)
(III)
?????
????
?
???
Other domains
except (I), (II)
Selected
domain
Figure 2: Overview of domain selection
error is compounded, as the system assumes that
the previous domain is always correct. Therefore,
we solve this problem by considering features that
represent the confidence of the previously selected
domain.
We define domain selection as being based on
the following 3-class categorization: (I) the previ-
ous domain, (II) the domain in which the speech
recognition results can be accepted with the high-
est recognition score, which is different from the
previous domain, and (III) other domains. Figure
2 depicts the three choices. This framework in-
cludes the conventional methods as choices (I) and
(II). Furthermore, it considers the possibility that
the current interpretations may be wrong, which
is represented as choice (III). This framework also
has extensibility for adding new domains, since it
treats domain selection not by detecting each do-
main directly, but by defining only a relative re-
lationship between the previous and current do-
mains.
Since our framework separates speech recogni-
tion results and domain selection, it can keep di-
alogues in the correct domain even when speech
recognition results are wrong. This situation is
represented as choice (I). An example is shown
in Figure 3. Here, the user?s first utterance (U1)
is about the restaurant domain. Although the sec-
ond utterance (U2) is also about the restaurant do-
main, an incorrect interpretation for the restaurant
domain is obtained because the utterance contains
an out-of-vocabulary word and is incorrectly rec-
ognized. Although a response for utterance U2
should ideally be in the restaurant domain, the sys-
tem control shifts to the temple sightseeing infor-
mation domain, in which an interpretation is ob-
tained based on the speech recognition result. This
11
? ?
U1: Tell me bars in Kawaramachi area.
(domain: restaurant)
S1: Searching for bars in Kawaramachi area.
30 items found.
U2: I want Tamanohikari (name of liquor).
(domain: restaurant)
Tamanohikari is out-of-vocabulary word, and
misrecognized as Tamba-bashi (name of place).
(domain: temple)
S2 (bad): Searching spots near Tamba-bashi. 10 items
found. (domain: temple)
S2 (good): I do not understand what you said. Do you
have any other preferences? (domain: restaurant)
? ?
Figure 3: Example in which choice (I) is appropri-
ate in spite of speech recognition error
is shown as utterance S2 (bad). In such cases, our
framework is capable of behaving appropriately.
This is shown as S2 (good), which is made by
selecting choice (I). Accepting erroneous recogni-
tion results is more harmful than rejecting correct
ones for the following reasons: 1) a user needs to
solve the misunderstanding as a result of the false
acceptance, and 2) an erroneous utterance affects
the interpretation of the utterances following it.
Furthermore, we define choice (III), which de-
tects the cases where normal dialogue manage-
ment is not suitable, in which case the central
module selects an expert based on either the pre-
vious domain or the domain based on the speech
recognition results. The situation corresponds to
a succession of recognition errors. However, this
problem is more difficult to solve than merely de-
tecting a simple succession of the errors because
the system needs to distinguish between speech
recognition errors and domain selection errors in
order to generate appropriate next utterances. Fig-
ure 4 shows an example of such a situation. Here,
the user?s utterances U1 and U2 are about the tem-
ple domain, but a speech recognition error oc-
curred in U2, and system control shifts to the hotel
domain. The user again says (U3), but this results
in the same recognition error. In this case, a do-
main that should ideally be selected is neither the
domain in the previous turn nor the domain deter-
mined based on the speech recognition results. If
this situation can be detected, the system should be
able to generate an appropriate response, like S3
(good), and prevent inappropriate responses based
? ?
U1: Tell me the address of Horin-ji (temple name).
(domain: temple)
S1: The address of Horin-ji is ...
U2: Then, what is the fee for Horin-ji?
(domain: temple)
misrecognized as ?the fee of Holiday Inn?.
(domain: hotel)
S2: The fee of Holiday Inn is ...
U3: The fee of Horin-ji. (domain: temple)
again misrecognized as ?the fee of Holiday Inn?.
(domain: hotel)
S3 (bad): The fee of Holiday Inn is ...
S3 (good): Are you asking about hotel information?
U4: No.
S4: Would you like to return to the temple information
service?
? ?
Figure 4: Example in which choice (III) should be
selected
on an incorrect domain determination. It is pos-
sible for the system to restart from two utterances
before (U1), after asking a confirmatory question
(S4) about whether to return to it or not. After that,
repetition of similar errors can also be avoided if
the system prohibits transition to the hotel domain.
4 Domain Selection using Dialogue
History
We constructed a classifier that selects the appro-
priate domains using various features, including
dialogue histories. The selected domain candi-
dates are based on: (I) the previous domain, (II)
the domain in which the speech recognition results
can be accepted with the highest recognition score,
or (III) other domains. Here, we describe the fea-
tures present in our domain selection method.
In order to not spoil the system?s extensibility,
an advantage of the distributed-type architecture,
the features used in the domain selection should
not depend on the specific domains. We categorize
the features used into three categories listed below:
? Features representing the confidence with
which the previous domain can be considered
correct (Table 1)
? Features about a user?s speech recognition re-
sult (Table 2)
12
Table 1: Features representing confidence in pre-
vious domain
P1: number of affirmatives after entering the domain
P2: number of negations after entering the domain
P3: whether tasks have been completed in the domain
(whether to enter ?requesting detailed information?
in database search task)
P4: whether the domain appeared before
P5: number of changed slots after entering the domain
P6: number of turns after entering the domain
P7: ratio of changed slots (= P5/P6)
P8: ratio of user?s negative answers (= P2/(P1 + P2))
P9: ratio of user?s negative answers in the domain (=
P2/P6)
P10: states in tasks
Table 2: Features of speech recognition results
R1: best posteriori probability of the N-best candidates
interpreted in the previous domain
R2: best posteriori probability for the speech recogni-
tion result interpreted in the domain, that is the do-
main with the highest score
R3: average of word?s confidence scores for the best
candidate of speech recognition results in the do-
main, that is, the domain with the highest score
R4: difference of acoustic scores between candidates
selected as (I) and (II)
R5: ratio of averages of words? confidence scores be-
tween candidates selected as (I) and (II)
? Features representing the situation after do-
main selection (Table 3)
We can take into account the possibility that a
current estimated domain might be erroneous, by
using features representing the confidence in the
previous domain. Each feature from P1 to P9 is
defined to represent the determination of whether
an estimated domain is reliable or not. Specifi-
cally, if there are many affirmative responses from
a user or many changes of slot values during in-
teractions in the domain, we regard the current do-
main as reliable. Conversely, the domain is not
reliable if there are many negative answers from a
user after entering the domain.
We also adopted the feature P10 to represent
the state of the task, because the likelihood that
a domain is changed depends on the state of the
task. We classified the tasks that we treat into two
categories using the following classifications first
made by Araki et al (1999). For a task catego-
rized as a ?slot-filling type?, we defined the di-
alogue states as one of the following two types:
?not completed?, if not all of the requisite slots
have been filled; and ?completed?, if all of the
Table 3: Features representing situations after do-
main selection
C1: dialogue state after the domain selection after se-
lecting previous domain
C2: whether the interpretation of the user?s utterance is
negative in previous domain
C3: number of changed slots after selecting previous
domain
C4: dialogue state after selecting the domain with the
highest speech recognition score
C5: whether the interpretation of the user?s utterance
is negative in the domain with the highest speech
recognition score
C6: number of changed slots after selecting the domain
with the highest speech recognition score
C7: number of common slots (name of place, here)
changed after selecting the domain with the high-
est speech recognition score
C8: whether the domain with the highest speech recog-
nition score has appeared before
requisite slots have been filled. For a task catego-
rized as a ?database search type?, we defined the
dialogue states as one of the following two types:
?specifying query conditions? and ?requesting de-
tailed information?, which were defined in (Ko-
matani et al, 2005a).
The features which represent the user?s speech
recognition result are listed in Table 2 and corre-
spond to those used in conventional studies. R1
considers the N-best candidates of speech recogni-
tion results that can be interpreted in the previous
domain. R2 and R3 represent information about a
domain with the highest speech recognition score.
R4 and R5 represent the comparisons between the
above-mentioned two groups.
The features that characterize the situations af-
ter domain selection correspond to the information
each expert returns to the central module after un-
derstanding the speech recognition results. These
are listed in Table 3. Features listed from C1 to
C3 represent a situation in which the previous do-
main (choice (I)) is selected. Those listed from
C4 to C8 represent a situation in which a domain
with the highest recognition score (choice (II)) is
selected.
Note that these features listed here have sur-
vived after feature selection. A feature survives
if the performance in the domain classification is
degradedwhen it is removed from a feature set one
by one. We had prepared 32 features for the initial
set.
13
Table 4: Specifications of each domain
Name of Class of # of vocab. # of
domain task in ASR slots
restaurant database search 1,562 10
hotel database search 741 9
temple database search 1,573 4
weather slot filling 87 3
bus slot filling 1,621 3
total - 7,373 -
5 Experimental Evaluation
5.1 Implementation
We implemented a Japanese multi-domain spoken
dialogue system with five domain experts: restau-
rant, hotel, temple, weather, and bus. Specifica-
tions of each expert are listed in Table 4. If there
is any overlapping slot between the vocabularies
of the domains, our architecture can treat it as a
common slot, whose value is shared among the
domains when interacting with the user. In our
system, place names are treated as a common slot.
We adopted Julian as the grammar-based
speech recognizer (Kawahara et al, 2004). The
grammar rules for the speech recognizer can be
automatically generated from those used in the
language understanding modules in each domain.
As a phonetic model, we adopted a 3000-states
PTM triphone model (Kawahara et al, 2004).
5.2 Collecting Dialogue Data
We collected dialogue data using a baseline sys-
tem from 10 subjects. First, the subjects used the
system by following a sample scenario, to get ac-
customed to the timing to speak. They, then, used
the system by following three scenarios, where at
least three domains were mentioned, but neither
an actual temple name nor domain was explicitly
mentioned. One of the scenarios is shown in Fig-
ure 5. Domain selection in the baseline system
was performed on the basis of the baseline method
that will be mentioned in Section 5.4, in which ?
was set to 40 after preliminary experiments.
In the experiments, we obtained 2,205 utter-
ances (221 per subject, 74 per dialogue). The
accuracy of the speech recognition was 63.3%,
which was rather low. This was because the sub-
jects tended to repeat similar utterances even after
misrecognition occurred due to out-of-grammar or
out-of-vocabulary utterances. Another reason was
that the dialogues for subjects with worse speech
recognition results got longer, which resulted in an
increase in the total number of misrecognition.
? ?
Tomorrow or the day after, you are planning a sightsee-
ing tour of Kyoto. Please find a shrine you want to visit
in the Arashiyama area, and determine, after consider-
ing the weather, on which day you will visit the shrine.
Please, ask for a temperature on the day of travel. Also
find out how to go to the shrine, whether you can take a
bus from the Kyoto station to there, when the shrine is
closing, and what the entrance fee is.
? ?
Figure 5: Example of scenarios
5.3 Construction of the Domain Classifier
We used the data containing 2,205 utterances col-
lected using the baseline system, to construct a do-
main classifier. We used C5.0 (Quinlan, 1993) as
a classifier. The features used were described in
Section 4. Reference labels were given by hand
for each utterance based on the domains the sys-
tem had selected and transcriptions of the user?s
utterances, as follows2.
Label (I): When the correct domain for a user?s
utterance is the same as the domain in which
the previous system?s response was made.
Label (II): Except for case (I), when the correct
domain for a user?s utterance is the domain
in which a speech recognition result in the N-
best candidates with the highest score can be
interpreted.
Label (III): Domains other than (I) and (II).
5.4 Evaluation of Domain Selection
We compared the performance of our domain se-
lection with that of the baseline method described
below.
Baseline method: A domain having an interpre-
tation with the highest score in the N-best
candidates of the speech recognition was se-
lected, after adding ? for the acoustic likeli-
hood of the speech recognizer if the domain
was the same as the previous one. We calcu-
lated the accuracies of domain selections for
various ?.
2Although only one of the authors assigned the labels,
they could be easily assigned without ambiguity, since the
labels were automatically defined as previously described.
Thus, the annotator only needs to judge whether a user?s re-
quest was about the same domain as the previous system?s re-
sponse or whether it was about a domain in the speech recog-
nition result.
14
0100
200
300
400
500
600
700
800
900
0 10 20 30 40 50 60
?
#
 
o
f
 
e
r
r
o
r
s
 
i
n
 
d
o
m
a
i
n
 
s
e
l
e
c
t
i
o
n
 total
 domain in previous utt.
 domain with highest score
 other domain
Figure 6: Accuracy of domain selection in the
baseline method
Our method: A domain was selected based on
our method. The performance was calculated
with a 10-fold cross validation, that is, one
tenth of the 2,205 utterances were used as test
data, and the remainder was used as training
data. The process was repeated 10 times, and
the average of the accuracies was computed.
Accuracies for domain selection were calculated
per utterance. When there were several domains
that had the same score after domain selection, one
domain was randomly selected among them as an
output.
Figure 6 shows the number of errors for do-
main selection in the baseline method, categorized
by their reference labels as ? changed. As ? in-
creases, so does the system desire to keep the pre-
vious domain. A condition where ? = 0 cor-
responds to a method in which domains are se-
lected based only on the speech recognition re-
sults, which implies that there are no constraints
on keeping the current domain. As we can see
in Figure 6, the number of errors whose refer-
ence labels are ?a domain in the previous response
(choice (I))? decreases as ? gets larger. This is be-
cause incorrect domain transitions due to speech
recognition errors were suppressed by the con-
straint to keep the domains. Conversely, we can
see an increase in errors whose labels are ?a do-
main with the highest speech recognition score
(choice (II))?. This is because there is too much
incentive for keeping the previous domain. The
smallest number of errors was 634 when ? = 35,
and the error rate of domain selection was 28.8%
(= 634/2205). There were 371 errors whose refer-
ence labels were neither ?a domain in the previous
response? nor ?a domain with the highest speech
recognition score?, which cannot be detected even
when ? is changed based on conventional frame-
works.
We also calculated the classification accuracy of
our method. Table 5 shows the results as a con-
fusion matrix. The left hand figure denotes the
number of outputs in the baseline method, while
the right hand figure denotes the number of out-
puts in our method. Correct outputs are in the
diagonal cells, while the domain selection errors
are in the off diagonal cells. Total accuracy in-
creased by 5.3%, from 71.2% to 76.5%, and the
number of errors in domain selection was reduced
from 634 to 518, so the error reduction rate was
18.3% (= 116/634). There was no output in the
baseline method for ?other domains (III)?, which is
in the third column, because conventional frame-
works have not taken this choice into considera-
tion. Our method was able to detect this kind of
error in 157 of 371 utterances, which allows us
to prevent further errors from continuing. More-
over, accuracies for (I) and (II) did not get worse.
Precision for (I) improved from 0.77 to 0.83, and
the F-measure for (I) also improved from 0.83 to
0.86. Although recall for (II) got worse, its preci-
sion improved from 0.52 to 0.62, and consequently
the F-measure for (II) improved slightly from 0.61
to 0.62. These results show that our method can
detect choice (III), which was newly introduced,
without degrading the existing classification accu-
racies.
The features that follow played an important
role in the decision tree. The features that repre-
sent confidence in the previous domain appeared
in the upper part of the tree, including ?the num-
ber of affirmatives after entering the domain (P1)?,
?the ratio of user?s negative answers in the do-
main (P9)?, ?the number of turns after entering the
domain (P6)?, and ?the number of changed slots
based on the user?s utterances after entering the
domain (P5)?. These were also ?whether a domain
with the highest score has appeared before (C8)?
and ?whether an interpretation of a current user?s
utterance is negative (C2)?.
6 Conclusion
We constructed a multi-domain spoken dialogue
system using an extensible framework. Domain
selection in conventional studies is based on ei-
ther the domain based on the speech recognition
15
Table 5: Confusion matrix in domain selection (baseline / our method)
reference label \ output in previous response (I) with highest score (II) others (III) # total label (recall)
in previous response (I) 1289 / 1291 162 / 85 0 / 75 1451 (0.89 / 0.89)
with highest score (II) 84 / 99 299? / 256? 0 / 28 383 (0.74 / 0.62)
others (III) 293 / 172 78 / 42 0 / 157 371 ( 0 / 0.42)
total 1666 / 1562 539 / 383 0 / 260 2205
(precision) (0.77) / (0.83) (0.52) / (0.62) ( - ) / (0.60) (0.712 / 0.765)
?: These include 17 errors because of random selection when there were several domains having the same highest scores.
results or the previous domain. However, we no-
ticed that these conventional frameworks cannot
cope with situations where neither of these do-
mains is correct. Detection of such situations
can prevent dialogues from staying in the incor-
rect domain, which allows our domain selection
method to be robust against speech recognition er-
rors. Furthermore, our domain selection method
is also extensible. Our method does not select the
domains directly, but, by categorizing them into
three classes, it can cope with an increase or de-
crease in the number of domains. Based on the re-
sults of an experimental evaluation using 10 sub-
jects, our method was able to reduce domain se-
lection errors by 18.3% compared to a baseline
method. This means our system is robust against
speech recognition errors.
There are still some issues that could make
our system more robust, and this is included in
future work. For example, in this study, we
adopted a grammar-based speech recognizer to
construct each domain expert easily. However,
other speech recognition methods could be used,
such as a statistical language model. As well,
multiple speech recognizers employing different
domain-dependent grammars could be run in par-
allel. Thus, we need to investigate how to integrate
these approaches into our framework, without de-
stroying the extensibility.
References
Masahiro Araki, Kazunori Komatani, Taishi Hirata,
and Shuji Doshita. 1999. A dialogue library for
task-oriented spoken dialogue systems. In Proc.
IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, pages 1?7.
Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-
sunobu Itou, and Kiyohiro Shikano. 2004. Re-
cent progress of open-source LVCSR engine Julius
and japanese model repository. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), pages 3069?
3072.
Kazunori Komatani, Naoyuki Kanda, Tetsuya Ogata,
and Hiroshi G. Okuno. 2005a. Contextual
constraints based on dialogue models in database
search task for spoken dialogue systems. In Proc.
European Conf. Speech Commun. & Tech. (EU-
ROSPEECH), pages 877?880, Sep.
Kazunori Komatani, Shinichi Ueno, Tatsuya Kawa-
hara, and Hiroshi G. Okuno. 2005b. User model-
ing in spoken dialogue systems to generate flexible
guidance. User Modeling and User-Adapted Inter-
action, 15(1):169?183.
Lori Lamel, Sophie Rosset, Jean-Luc Gauvain, and
Samir Bennacef. 1999. The LIMSI ARISE sys-
tem for train travel information. In IEEE Int?l Conf.
Acoust., Speech & Signal Processing (ICASSP),
pages 501?504, Phoenix, AZ.
Ian R. Lane and Tatsuya Kawahara. 2005. Utterance
verification incorporating in-domain confidence and
discourse coherence measures. In Proc. European
Conf. Speech Commun. & Tech. (EUROSPEECH),
pages 421?424.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov,
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky,M. Rahim, P. Ruscitti, andM.Walker.
2000. The AT&T-DARPA communicator mixed-
initiative spoken dialogue system. In Proc. Int?l
Conf. Spoken Language Processing (ICSLP).
Bor-shen Lin, Hsin-min Wang, and Lin-shan Lee.
2001. A distributed agent architecture for intelli-
gent multi-domain spoken dialogue systems. IEICE
Trans. on Information and Systems, E84-D(9):1217?
1230, Sept.
Mikio Nakano, Yuji Hasegawa, Kazuhiro Nakadai,
Takahiro Nakamura, Johane Takeuchi, Toyotaka
Torii, Hiroshi Tsujino, Naoyuki Kanda, and Hi-
roshi G. Okuno. 2005. A two-layer model for be-
havior and dialogue planning in conversational ser-
vice robots. In 2005 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems (IROS),
pages 1542?1548.
Ian O?Neill, Philip Hanna, Xingkun Liu, and Michael
McTear. 2004. Cross domain dialogue modelling:
An object-based approach. In Proc. Int?l Conf. Spo-
ken Language Processing (ICSLP).
Botond Pakucs. 2003. Towards dynamic multi-
domain dialogue processing. In Proc. European
16
Conf. Speech Commun. & Tech. (EUROSPEECH),
pages 741?744.
Alexandros Potamianos and Hong-Kwang J. Kuo.
2000. Statistical recursive finite state machine pars-
ing for speech understanding. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), volume 3,
pages 510?513.
J. Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo,
CA. http://www.rulequest.com/see5-info.html.
Antoine Raux and Maxine Eskenazi. 2004. Non-
native users in the let?s go!! spoken dialogue sys-
tem: Dealing with linguistic mismatch. In Proc. of
HLT/NAACL.
Ruben San-Segundo, Bryan Pellom, Wayne Ward, and
Jose M. Pardo. 2000. Confidence measures for di-
alogue management in the CU communicator sys-
tem. In IEEE Int?l Conf. Acoust., Speech & Signal
Processing (ICASSP).
17
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 88?91,
Columbus, June 2008. c?2008 Association for Computational Linguistics
A Framework for Building Conversational Agents Based on a Multi-Expert
Model
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, Hiroshi Tsujino
Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako, Saitama 351-0188, Japan
{nakano, funakoshi, yuji.hasegawa, tsujino}@jp.honda-ri.com
Abstract
This paper presents a novel framework for
building symbol-level control modules of an-
imated agents and robots having a spoken di-
alogue interface. It features distributed mod-
ules called experts each of which is special-
ized to perform certain kinds of tasks. A com-
mon interface that all experts must support is
specified, and any kind of expert can be incor-
porated if it has the interface. Several modules
running in parallel coordinate the experts by
accessing them through the interface, so that
the whole system can achieve flexible control,
such as interruption handling and parallel task
execution.
1 Introduction
As much attention is recently paid to autonomous
agents such as robots and animated agents, spoken
dialogue is expected to be a natural interface be-
tween users and such agents. Our objective is to es-
tablish a framework for developing the intelligence
module of such agents.
In establishing such a framework, we focus on
achieving the following features. (1) Multi-domain
dialogue: Since agents are usually expected to per-
form multiple kinds of tasks, they need to work in
multiple domains and switch domains according to
user utterances. (2) Interruption handling: It is cru-
cial for human-agent interaction to be able to handle
users? interrupting utterances while speaking or per-
forming tasks. (3) Parallel task execution: Agents,
especially robots that perform physical actions, are
expected to be able to execute multiple tasks in par-
allel when possible. For example, robots should be
able to engage in a dialogue while moving. (4) Ex-
tensibility: Since the agents can be used for a vari-
ety of tasks, various strategies for dialogue and task
planning should be able to be incorporated.
Although a number of models for conversational
agents have been proposed, no model has all of the
above properties. Several multi-domain dialogue
system models have been proposed and they are ex-
tensible, but it is not clear how they handle interrup-
tions to system utterances and actions (e.g., O?Neill
et al (2004), Lin et al (1999), and Hartikainen et al
(2004)). There are several spoken dialogue agents
and robots that can handle interruptions thanks to
their asynchronous control (Asoh et al, 1999; Boye
et al, 2000; Blaylock et al, 2002; Lemon et al,
2002), they do not focus on making it easy to add
new dialogue domains with a variety of dialogue
strategies.
This paper presents a framework called RIME
(Robot Intelligence based on Multiple Experts),
which employs modules called experts.1 Each ex-
pert is specialized for achieving certain kinds of
tasks by performing physical actions and engaging
in dialogues. It corresponds to the symbol-level con-
trol module of a system that can engage in tasks in
a single small domain, and it employs fixed con-
trol strategies. Only some of the experts take charge
in understanding user utterances and decide actions.
The basic idea behind RIME is to specify a com-
mon interface of experts for coordinating them and
to achieve flexible control. In RIME, several mod-
1RIME is an improved version of our previous model
(Nakano et al, 2005), whose interruption handling was too sim-
ple and which could not achieve parallel task execution.
88
ules run in parallel for coordinating experts. They
are understander, which is responsible for speech
understanding, action selector, which is responsible
for selecting actions, and task planner, which is re-
sponsible for deciding which expert should work to
achieve tasks.
RIME achieves the above mentioned features.
Multi-domain dialogues are possible by selecting an
appropriate expert which is specialized to dialogues
in a certain domain. Interruption handling is possi-
ble because each expert must have methods to de-
tect interruptions and decide actions to handle in-
terruptions, and coordinating modules can use these
methods. Parallel task execution is possible because
experts have methods for providing information to
decide which experts can take charge at the same
time, and the task planner utilizes that information.
Extensibility is achieved because any kind of expert
can be incorporated if it supports the common inter-
face. This makes it possible for agent developers to
build a variety of conversational agents.
2 Multi-Expert Model
This section explains RIME in detail. Fig. 1 depicts
its module architecture.
2.1 Experts
Each expert is a kind of object in the object-oriented
programming framework. In this paper, we call
tasks performed by one expert primitive tasks. Ex-
perts should be prepared for each primitive task type.
For example, if there is an expert for a primitive task
type ?telling someone?s extension number?, ?telling
person A?s extension number? is a primitive task.
By performing a series of primitive tasks, a com-
plicated task can be performed. For example, a mu-
seum guide robot can perform ?explaining object B?
by executing ?moving to B? and ?giving an explana-
tion on B?. Among the experts, a small number of
experts can perform tasks at one time. Such experts
are called being in charge.
Each expert holds information on the progress of
the primitive task. It includes task-type-independent
information, such as which action in this primitive
task is being performed and whether the previous
robot action finished, and task-type-dependent in-
formation such as the user intention understanding
understander
expert 1
expert 2
expert 3
expert n
action 
selector
task 
planner
global 
context
input 
processor
action 
executor
speech 
recognition
result
score
expert 
selection 
information
speech 
recognition
result
action
(from experts 
in charge)
action
execution
report
exec. report (to the expert 
that selected the action)
charge 
/discharge new task information
across tasks
.
.
.
.
.
microphone etc. agent & speech synthesizer
Figure 1: Architecture for RIME-Based Systems
results and dialogue history. The contents and the
data structure for the task-type-dependent informa-
tion for each expert can be designed by the system
developer.
Experts are classified into system-initiative task
experts and user-initiative task experts. In this pa-
per, the initiative of a task means who can initiate
the task. For example, the task ?understanding a
request for weather information? is a user-initiative
task, and the task ?providing weather information?
is a system-initiative task.
In RIME, executing multiple tasks in parallel be-
comes possible by making multiple experts take
charge. To check whether two experts can take
charge simultaneously, we currently use two fea-
tures verbal and physical. Two experts having the
same feature cannot take charge simultaneously.
The interface of experts consists of methods for
accessing its internal state. Below are some of the
task-type-dependent methods, which need to be im-
plemented by system developers.
The understand method updates the internal state
based on the user speech recognition results, us-
ing domain-dependent sentence patterns for utter-
ance understanding. This method returns a score
which indicates the plausibility the user utterance
should be dealt with by the expert. Domain selection
techniques in multi-domain spoken dialogue sys-
tems (Komatani et al, 2006) can be applied to obtain
the score. The select-action method outputs one ac-
tion based on the content of the internal state. Here,
an action is a multimodal command which includes
a text to speak and/or a physical action command.
89
The action can be an empty action, which means do-
ing nothing. The detect-interruption method returns
a Boolean value that indicates whether the previous
user utterance is an interruption to the action being
performed when this expert is being in charge. The
handle-interruption method returns the action to be
performed after an interruption is detected. For ex-
ample, an instruction to stop the utterance can be
returned.
In the definition of these methods, experts can
access a common database called global context to
store and utilize information across domains, such
as information on humans, information on the envi-
ronment, and past dialogue topics.
2.2 Modules Coordinating Experts
To exploit experts, three processes, namely the un-
derstander, the action selector, and the task planner,
work in parallel.
The understander receives output of an input pro-
cessor, which typically performs speech recogni-
tion. Each time the understander receives a user
speech recognition result from the input processor,
it performs the following process. First it dispatches
the speech recognition result to the experts in charge
and the user-initiative experts with their understand
methods, which then returns the scores mentioned
above. The expert that returns the highest score is
selected as the expert to take charge. If the selected
expert is not in charge, it tells the task planner that
the expert is selected as the user-initiative expert to
take charge. If the selected expert is in charge, it
calls the detect-interruption method of the expert. If
true is returned, it tells the action selector that an
interruption utterance is detected.
The action selector repeats the following process
for each expert being in charge in a short cycle.
When an interruption for the expert is detected, it
calls the expert?s handle-interruption method, and
it then sends the returned action to the action ex-
ecutor, which is assumed to execute multimodal ac-
tions by controlling agents, speech synthesizers, and
other modules. Otherwise, unless it is not waiting
for a user utterance, it calls the expert?s select-action
methods, and then sends the returned action to the
action executor. The returned action can be an empty
action. Note that it is assumed that the action execu-
tor can perform two or more actions in parallel when
verbalagentexplaining placesG
physicalagentmoving to show the way F
verbaluserunderstanding requests for guiding to placesE
verbalagentproviding extension numbersD
verbaluserunderstanding extension number requestsC
verbalagentproviding weather informationB
verbaluserunderstanding weather information requestsA 
featureinitiativetask typeID 
Table 1: Experts in the Example Robotic System
Human: "Where is the meeting 
room?"
Robot: "Would you like to know 
where the meeting room is?"
Human: "yes."
Human: "Tell me A's extension 
number."
Robot: "Please come this way."
(start moving)
Robot: "A's extension number is 
1234."
Robot: (stop moving)
Expert E
Expert G
Expert C
Expert D
understand request
to show the way
show the way
tell A's ext. 
number
understand 
request for A's 
ext. number
Robot: "The meeting room is over
there."
Utterances and physical actions Experts in charge and tasks
move to 
show the 
way
Expert F
Figure 2: Expert Selection in a Parallel Task Execution
Example
possible.
The task planner is responsible for deciding which
experts take charge and which experts do not. It
sometimes makes an expert take charge by setting
a primitive task, and sometimes it discharges an ex-
pert to cancel the execution of its primitive task. To
make such decisions, it receives several pieces of in-
formation from other modules. First it receives from
the understander information on which expert is se-
lected to understand a new utterance. It also receives
information on the finish of the primitive task from
an expert being in charge. In addition, it receives
new tasks from the experts that understand human
requests. The task planner also consults the global
context to access the information shared by the ex-
perts and the task planner. In this paper we do not
discuss the details of task planning algorithms, but
we have implemented a task planner with a simple
hierarchical planning mechanism.
There can be other processes whose output is
written in the global context. For example, a robot
and human localization process using image pro-
cessing and other sensor information processing can
be used.
90
3 Implementation as a Toolkit
The flexibility of designing experts increases the
amount of effort for programming in building ex-
perts. We therefore developed RIME-TK (RIME-
ToolKit), which provides libraries that facilitate
building systems based on RIME. It is implemented
in Java, and contains an abstract expert class hier-
archy. The system developers can create new ex-
perts by extending those abstract classes. Those ab-
stract classes have frequently used functions such
as WFST-based language understanding, template-
based language generation, and frame-based dia-
logue management. RIME-TK also contains the im-
plementations of the understander and the action se-
lector. In addition, it specifies the interfaces for the
input processor, the action executor, and the task
planner. Example implementations of these mod-
ules are also included in RIME-TK. Using RIME-
TK, conversational agents can be built by creating
experts, an input processor, an action executor, and
a task planner.
As an example, we have built a robotic system,
which is supposed to work at a reception, and can
perform several small tasks such as providing ex-
tension numbers of office members and guiding to
several places near the reception such as a meeting
room and a restroom. Some experts in the system
are listed in Table 1. Fig. 2 shows an example inter-
action between a human and the robotic system that
includes parallel task execution and how experts are
charged. The detailed explanation is omitted for the
lack of the space.
By developing several other robotic systems and
spoken dialogue systems (e.g., Komatani et al
(2006), Nakano et al (2006), and Nishimura et al
(2007)), we have confirmed that RIME and RIME-
TK are viable.
4 Concluding Remarks
This paper presented RIME, a framework for build-
ing conversational agents. It is different from pre-
vious frameworks in that it makes it possible to
build agents that can handle interruptions and exe-
cute multiple tasks in parallel by employing experts
which have a common interface. Although the cur-
rent implementation is useful for building various
kinds of systems, we believe that preparing more
kinds of expert templates and improving expert se-
lection for understanding utterances facilitate build-
ing a wider variety of systems.
Acknowledgments We would like to thank all
people who helped us to build RIME-TK and its ap-
plications.
References
H. Asoh, T. Matsui, J. Fry, F. Asano, and S. Hayamizu.
1999. A spoken dialog system for a mobile office
robot. In Proc. Eurospeech-99, pages 1139?1142.
N. Blaylock, J. Allen, and G. Ferguson. 2002. Synchro-
nization in an asynchronous agent-based architecture
for dialogue systems. In Proc. Third SIGdial Work-
shop, pages 1?10.
J. Boye, B. A. Hockey, and M. Rayner. 2000. Asyn-
chronous dialogue management: Two case-studies. In
Proc. Go?talog-2000.
M. Hartikainen, M. Turunen, J. Hakulinen, E.-P. Salo-
nen, and J. A. Funk. 2004. Flexible dialogue manage-
ment using distributed and dynamic dialogue control.
In Proc. Interspeech-2004, pages 197?200.
K. Komatani, N. Kanda, M. Nakano, K. Nakadai, H. Tsu-
jino, T. Ogata, and H. G. Okuno. 2006. Multi-domain
spoken dialogue system with extensibility and robust-
ness against speech recognition errors. In Proc. 7th
SIGdial Workshop, pages 9?17.
O. Lemon, A. Gruenstein, A. Battle, and S. Peters. 2002.
Multi-tasking and collaborative activities in dialogue
systems. In Proc. Third SIGdial Workshop, pages
113?124.
B. Lin, H. Wang, and L. Lee. 1999. Consistent dialogue
across concurrent topics based on an expert system
model. In Proc. Eurospeech-99, pages 1427?1430.
M. Nakano, Y. Hasegawa, K. Nakadai, T. Nakamura,
J. Takeuchi, T. Torii, H. Tsujino, N. Kanda, and H. G.
Okuno. 2005. A two-layer model for behavior and
dialogue planning in conversational service robots. In
Proc. 2005 IEEE/RSJ IROS, pages 1542?1547.
M. Nakano, A. Hoshino, J. Takeuchi, Y. Hasegawa,
T. Torii, K. Nakadai, K. Kato, and H. Tsujino. 2006.
A robot that can engage in both task-oriented and non-
task-oriented dialogues. In Proc. 2006 IEEE/RAS Hu-
manoids, pages 404?411.
Y. Nishimura, S. Minotsu, H. Dohi, M. Ishizuka,
M. Nakano, K. Funakoshi, J. Takeuchi, Y. Hasegawa,
and H. Tsujino. 2007. A markup language for describ-
ing interactive humanoid robot presentations. In Proc.
IUI-07.
I. O?Neill, P. Hanna, X. Liu, and M. McTear. 2004.
Cross domain dialogue modelling: an object-based ap-
proach. In Proc. Interspeech-2004, pages 205?208.
91
Proceedings of the 12th European Workshop on Natural Language Generation, pages 191?194,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Probabilistic Model of Referring Expressions for Complex Objects
Kotaro Funakoshi? Philipp Spanger?
?Honda Research Institute Japan Co., Ltd.
Saitama, Japan
funakoshi@jp.honda-ri.com
nakano@jp.honda-ri.com
Mikio Nakano? Takenobu Tokunaga?
?Tokyo Institute of Technology
Tokyo, Japan
philipp@cl.cs.titech.ac.jp
take@cl.cs.titech.ac.jp
Abstract
This paper presents a probabilistic model
both for generation and understanding of
referring expressions. This model intro-
duces the concept of parts of objects, mod-
elling the necessity to deal with the char-
acteristics of separate parts of an object in
the referring process. This was ignored or
implicit in previous literature. Integrating
this concept into a probabilistic formula-
tion, the model captures human character-
istics of visual perception and some type
of pragmatic implicature in referring ex-
pressions. Developing this kind of model
is critical to deal with more complex do-
mains in the future. As a first step in our
research, we validate the model with the
TUNA corpus to show that it includes con-
ventional domain modeling as a subset.
1 Introduction
Generation of referring expressions has been stud-
ied for the last two decades. The basic orientation
of this research was pursuing an algorithm that
generates a minimal description which uniquely
identifies a target object from distractors. Thus
the research was oriented and limited by two con-
straints: minimality and uniqueness.
The constraint on minimality has, however,
been relaxed due to the computational complexity
of generation, the perceived naturalness of redun-
dant expressions, and the easiness of understand-
ing them (e.g., (Dale and Reiter, 1995; Spanger et
al., 2008)). On the other hand, the other constraint
of uniqueness has not been paid much attention
to. One major aim of our research is to relax this
constraint on uniqueness because of the reason ex-
plained below.
The fundamental goal of our research is to deal
with multipartite objects, which have constituents
with different attribute values. Typical domain set-
tings in previous literature use uniform objects like
the table A shown in Figure 1. However, real life
is not so simple. Multipartite objects such as ta-
bles B and C can be found easily. Therefore this
paper introduces the concept of parts of objects to
deal with more complex domains containing such
objects. Hereby the constraint on uniqueness be-
comes problematic because people easily generate
and understand logically ambiguous expressions
in such domains.
For example, people often use an expression
such as ?the table with red corners? to identify
table B. Logically speaking, this expression is
equally applicable both to A and to B, that is, vio-
lating the constraint on uniqueness. And yet peo-
ple seem to have no problem identifying the in-
tended target correctly and have little reluctance to
use such an expression (Evidence is presented in
Section 3). We think that this reflects some type of
pragmatic implicature arising from human charac-
teristics of visual perception and that is important
both for understanding human-produced expres-
sions and for generating human-friendly expres-
sions in a real environment. This paper proposes a
model of referring expressions both for generation
and understanding. Our model uses probabilities
to solve ambiguity under the relaxed constraint on
uniqueness while considering human perception.
No adequate data is currently available in or-
der to provide a comprehensive evaluation of our
model. As a first step in our research, we validate
the model with the TUNA corpus to show that it
includes conventional domain modeling.
Figure 1: An example scene
191
2 Related work
Horacek (2005) proposes to introduce probabili-
ties to overcome uncertainties due to discrepan-
cies in knowledge and cognition between subjects.
While our model shares the same awareness of is-
sues with Horacek?s work, our focus is on rather
different issues (i.e., handling multipartite objects
and relaxing the constraint on uniqueness). In
addition, Horacek?s work is concerned only with
generation while our model is available both for
generation and understanding. Roy (2002) also
proposes a probabilistic model for generation but
presupposes uniform objects.
Horacek (2006) deals with references for struc-
tured objects such as documents. Although it con-
siders parts of objects, the motivation and focus of
the work are on quite different aspects from ours.
3 Evidence against logical uniqueness
We conducted two psycholinguistic experiments
using the visual stimulus shown in Figure 1.
In the first experiment, thirteen Japanese sub-
jects were presented with an expression ?kado no
akai tukue (the table with red corners)? and asked
to choose a table from the three in the figure.
Twelve out of the thirteen chose table B. Seven
out of the twelve subjects answered that the given
expression was not ambiguous.
In the second experiment, thirteen different
Japanese subjects were asked to make a descrip-
tion for table B without using positional relations.
Ten out of the thirteen made expressions seman-
tically equivalent to the expression used in the
first experiment. Only three subjects made log-
ically discriminative expressions such as ?asi to
yotu kado dake akai tukue (the table whose four
corners and leg only are red).?
These results show that people easily gener-
ate/understand logically ambiguous expressions.
4 Proposed model
We define pi = {p1, p2, . . . , pk} as the set of k
parts of objects (classes of sub-parts) that appears
in a domain. Here p1 is special and always means
the whole of an object. In a furniture domain, p1
means a piece of furniture regardless of the kind
of the object (chair, table, whatever). pi(i 6= 1)
means a sub-part class such as leg. Note that pi is
defined not for each object but for a domain. Thus,
objects may have no part corresponding to pi (e.g.,
some chairs have no leg.).
A referring expression e is represented as a set
of n pairs of an attribute value expression eaj and a
part expression epj modified by eaj as
e = {(ep1, e
a
1), (e
p
2, e
a
2), . . . , (epn, ean)}. (1)
For example, an expression ?the white table with
a red leg? is represented as
{(?table?, ?white?), (?leg?, ?red?)}.
Given a set of objects ? and a referring ex-
pression e, the probability with which the expres-
sion e refers to an object o ? ? is denoted as
Pr(O = o|E = e,? = ?). If we seek to provide
a more realistic model, we can model a probabilis-
tic distribution even for ?. In this paper, however,
we assume that ? is fixed to ? and it is shared by
interlocutors exactly. Thus, hereafter, Pr(o|e) is
equal to Pr(o|e, ?).
Following the definition (1), we estimate
Pr(o|e) as follows:
Pr(o|e) ? N
?
i
Pr(o|epi , e
a
i ). (2)
Here, N is a normalization coefficient. According
to Bayes? rule,
Pr(o|epi , e
a
i ) =
Pr(o)Pr(epi , eai |o)
Pr(epi , eai )
. (3)
Therefore,
Pr(o|e) ? N
?
i
Pr(o)Pr(epi , eai |o)
Pr(epi , eai )
. (4)
We decompose Pr(epi , eai |o) as
?
u
?
v
Pr(epi |pu, o)Pr(e
a
i |av, o)Pr(pu, av|o)
(5)
where pu is one of parts of objects that could be
expressed with epi , and av is one of attribute val-
ues1 that could be expressed with eai . Under the
simplifying assumption that epi and eai are not am-
biguous and are single possible expressions for
a part of objects and an attribute value indepen-
dently of objects 2,
Pr(o|e) ? N
?
i
Pr(o)Pr(pi, ai|o)
Pr(pi, ai)
(6)
? N
?
i
Pr(o|pi, ai) (7)
1Each attribute value belongs to an attribute ?, a set of
attribute values. E.g., ?color = {red, white, . . .}.
2That is, we ignore lexical selection matters in this paper,
although our model is potentially able to handle those matters
including training from corpora.
192
Pr(o|p, a) concerns attribute selection in gen-
eration of referring expressions. Most attribute
selection algorithms presented in past work are
based on set operations over multiple attributes
with discrete (i.e., symbolized) values such as col-
ors (red, brown, white, etc) to find a uniquely dis-
tinguishing description. The simplest estimation
of Pr(o|p, a) following this conventional Boolean
domain modeling is
Pr(o|p, a) ?
{
|??|?1 (p in o has a)
0 (p in o does not have a) (8)
where ?? is the subset of ?, each member of which
has attribute value a in its part of p.
As Horacek (2005) pointed out, however, this
standard approach is problematic in a real envi-
ronment because many physical attributes are non-
discrete and the symbolization of these continuous
attributes have uncertainties. For example, even
if two objects are blue, one can be more blueish
than the other. Some subjects may say it?s blue
but others may say it?s purple. Moreover, there
is the problem of logical ambiguity pointed out
in Section 1. That is, even if an attribute itself
is equally applicable to several objects in a logi-
cal sense, other available information (such as vi-
sual context) might influence the interpretation of
a given referring expression.
Such phenomena could be captured by estimat-
ing Pr(o|p, a) as
Pr(o|p, a) ? Pr(a|p, o)Pr(p|o)Pr(o)
Pr(p, a)
. (9)
Pr(a|p, o) represents the relevance of attribute
value a to part p in object o. Pr(p|o) represents
the salience of part p in object o. The underlying
idea to deal with the problem of logical ambiguity
is ?If some part of an object is mentioned, it should
be more salient than other parts.? This is related
to Grice?s maxims in a different way from mat-
ters discussed in (Dale and Reiter, 1995). Pr(p|o)
could be computed in some manner by using the
saliency map (Itti et al, 1998). Pr(o) is the prior
probability that object o is chosen. If potential
functions (such as used in (Tokunaga et al, 2005))
are used for computing Pr(o), we can naturally
rank objects, which are equally relevant to a given
referring expression, according to distances from
interlocutors.
5 Algorithms
5.1 Understanding
Understanding a referring expression e is identify-
ing the target object o? from a set of objects ?. This
is formulated in a straightforward way as
o? = argmax
o??
Pr(o|e). (10)
5.2 Generation
Generation of a referring expression is choosing
the best appropriate expression e? to discriminate a
given object o? from a set of distractors. A simple
formulation is
e? = argmax
e??
Pr(e)Pr(o?|e). (11)
? is a pre-generated set of candidate expressions
for o?. This paper does not explain how to generate
a set of candidates.
Pr(e) is the generation probability of an ex-
pression e independent of objects. This probabil-
ity can be learned from a corpus. In the evaluation
described in Section 6, we estimate Pr(e) as
Pr(e) ? Pr(|e|)
?
i
Pr(?i). (12)
Here, Pr(|e|) is the distribution of expression
length in terms of numbers of attributes used.
Pr(?) is the selection probability of a specific at-
tribute ? (SP (a) in (Spanger et al, 2008)).
6 Preliminary evaluation
As mentioned above, no adequate corpus is cur-
rently available in order to provide an initial vali-
dation of our model which we present in this pa-
per. In this section, we validate our model us-
ing the TUNA corpus (the ?Participant?s Pack?
available for download as part of the Generation
Challenge 2009) to show that it includes tradi-
tional domain modeling. We use the training-
part of the corpus for training our model and the
development-part for evaluation.
We note that we here assume a homogeneous
distribution of the probability Pr(o|p, a), i.e., we
are applying formula (8) here in order to calculate
this probability. We first implemented our proba-
bilistic model for the area of understanding. This
means our algorithm took as input the user?s selec-
tion of attribute?value pairs in the description and
calculated the most likely target object. This was
193
Table 1: Initial evaluation of proposed model for
generation in TUNA-domain
Furniture People
Total cases 80 68
Mean Dice-score 0.78 0.66
carried out for both the furniture and people do-
mains. Overall, outside of exceptional cases (e.g.,
human error), our algorithm was able to distin-
guish the target object for all human descriptions
(precision of 100%). This means it covers all the
cases the original approach dealt with.
We then implemented our model for the case of
generation. We measured the similarity of the out-
put of our algorithm with the human-produced sets
by using the Dice-coefficient (see (Belz and Gatt,
2007)). We evaluated this both for the Furniture
and People domain. The results are summarized
in Table 1.
Our focus was here to fundamentally show how
our model includes traditional modelling as a sub-
set, without much focus or effort on tuning in order
to achieve a maximum Dice-score. However, we
note that the Dice-score of our algorithm was com-
parable to the top 5-7 systems in the 2007 GRE-
Challenge (see (Belz and Gatt, 2007)) and thus
produced a relatively good result. This shows how
our algorithm ? providing a model of the referring
process in a more complex domain ? is applica-
ble as well to the very simple TUNA-domain as a
special case.
7 Discussion
In past work, parts of objects were ignored or im-
plicit. In case of the TUNA corpus, while the Fur-
niture domain ignores parts of objects, the People
domain contained parts of objects such as hair,
glasses, beard, etc. However, they were implic-
itly modeled by combining a pair of a part and its
attribute as an attribute such as hairColor. One
major advantage of our model is that, by explicitly
modelling parts of objects, it can handle the prob-
lem of logical ambiguity that is newly reported in
this paper. Although it might be possible to han-
dle the problem by extending previously proposed
algorithms in some ways, our formulation would
be clearer. Moreover, our model is directly avail-
able both for generation and understanding. Re-
ferring expressions using attributes (such as dis-
cussed in this paper) and those using discourse
contexts (such as ?it?) are separately approached
in past work. Our model possibly handles both of
them in a unified manner with a small extension.
This paper ignored relations between objects.
We, however, think that it is not difficult to prepare
algorithms handling relations using our model.
Generation using our model is performed in a
generate-and-test manner. Therefore computa-
tional complexity is a matter of concern. However,
that could be controlled by limiting the numbers
of attributes and parts under consideration accord-
ing to relevance and salience, because our model is
under the relaxed constraint of uniqueness unlike
previous work.
As future work, we have to gather data to eval-
uate our model and to statistically train lexical se-
lection in a new domain containing multipartite
objects.
References
Anja Belz and Albert Gatt. 2007. The attribute selec-
tion for GRE challenge: Overview and evaluation
results. In Proc. the MT Summit XI Workshop Using
Corpora for Natural Language Generation: Lan-
guage Generation and Machine Translation (UC-
NLG+MT), pages 75?83.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18:233?263.
Helmut Horacek. 2005. Generating referential de-
scriptions under conditions of uncertainty. In Proc.
ENLG 05.
Helmut Horacek. 2006. Generating references to parts
of recursively structured objects. In Proc. ACL 06.
L Itti, C. Koch, and E. Niebur. 1998. A model of
saliency-based visual attention for rapid scene anal-
ysis. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20(11):1254?1259.
Deb Roy. 2002. Learning visually-grounded words
and syntax for a scene description task. Computer
Speech and Language, 16(3).
Philipp Spanger, Takehiro Kurosawa, and Takenobu
Tokunaga. 2008. On ?redundancy? in selecting
attributes for generating referring expressions. In
Proc. COLING 08.
Takenobu Tokunaga, Tomonori Koyama, and Suguru
Saito. 2005. Meaning of Japanese spatial nouns.
In Proc. the Second ACL-SIGSEM Workshop on The
Linguistic Dimensions of Prepositions and their Use
in Computational Linguistics Formalisms and Appli-
cations, pages 93 ? 100.
194
Corpus-based Discourse Understanding in Spoken Dialogue Systems
Ryuichiro Higashinaka and Mikio Nakano and Kiyoaki Aikawa?
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
3-1 Morinosato Wakamiya
Atsugi, Kanagawa 243-0198, Japan
{rh,nakano}@atom.brl.ntt.co.jp, aik@idea.brl.ntt.co.jp
Abstract
This paper concerns the discourse under-
standing process in spoken dialogue sys-
tems. This process enables the system to
understand user utterances based on the
context of a dialogue. Since multiple can-
didates for the understanding result can
be obtained for a user utterance due to
the ambiguity of speech understanding, it
is not appropriate to decide on a single
understanding result after each user ut-
terance. By holding multiple candidates
for understanding results and resolving the
ambiguity as the dialogue progresses, the
discourse understanding accuracy can be
improved. This paper proposes a method
for resolving this ambiguity based on sta-
tistical information obtained from dia-
logue corpora. Unlike conventional meth-
ods that use hand-crafted rules, the pro-
posed method enables easy design of the
discourse understanding process. Experi-
ment results have shown that a system that
exploits the proposed method performs
sufficiently and that holding multiple can-
didates for understanding results is effec-
tive.
?Currently with the School of Media Science, Tokyo Uni-
versity of Technology, 1404-1 Katakuracho, Hachioji, Tokyo
192-0982, Japan.
1 Introduction
For spoken dialogue systems to correctly understand
user intentions to achieve certain tasks while con-
versing with users, the dialogue state has to be ap-
propriately updated (Zue and Glass, 2000) after each
user utterance. Here, a dialogue state means all
the information that the system possesses concern-
ing the dialogue. For example, a dialogue state in-
cludes intention recognition results after each user
utterance, the user utterance history, the system ut-
terance history, and so forth. Obtaining the user in-
tention and the content of an utterance using only the
single utterance is called speech understanding, and
updating the dialogue state based on both the previ-
ous utterance and the current dialogue state is called
discourse understanding. In general, the result of
speech understanding can be ambiguous, because it
is currently difficult to uniquely decide on a single
speech recognition result out of the many recogni-
tion candidates available, and because the syntac-
tic and semantic analysis process normally produce
multiple hypotheses. The system, however, has to be
able to uniquely determine the understanding result
after each user utterance in order to respond to the
user. The system therefore must be able to choose
the appropriate speech understanding result by re-
ferring to the dialogue state.
Most conventional systems uniquely determine
the result of the discourse understanding, i.e., the
dialogue state, after each user utterance. However,
multiple dialogue states are created from the current
dialogue state and the speech understanding results
corresponding to the user utterance, which leads to
ambiguity. When this ambiguity is ignored, the dis-
course understanding accuracy is likely to decrease.
Our idea for improving the discourse understanding
accuracy is to make the system hold multiple dia-
logue states after a user utterance and use succeed-
ing utterances to resolve the ambiguity among di-
alogue states. Although the concept of combining
multiple dialogue states and speech understanding
results has already been reported (Miyazaki et al,
2002), they use intuition-based hand-crafted rules
for the disambiguation of dialogue states, which are
costly and sometimes lead to inaccuracy. To resolve
the ambiguity of dialogue states and reduce the cost
of rule making, we propose using statistical infor-
mation obtained from dialogue corpora, which com-
prise dialogues conducted between the system and
users.
The next section briefly illustrates the basic ar-
chitecture of a spoken dialogue system. Section 3
describes the problem to be solved in detail. Then
after introducing related work, our approach is de-
scribed with an example dialogue. After that, we
describe the experiments we performed to verify our
approach, and discuss the results. The last section
summarizes the main points and mentions future
work.
2 Discourse Understanding
Here, we describe the basic architecture of a spoken
dialogue system (Figure 1). When receiving a user
utterance, the system behaves as follows.
1. The speech recognizer receives a user utterance
and outputs a speech recognition hypothesis.
2. The language understanding component re-
ceives the speech recognition hypothesis. The
syntactic and semantic analysis is performed
to convert it into a form called a dialogue
act. Table 1 shows an example of a dialogue
act. In the example, ?refer-start-and-end-time?
is called the dialogue act type, which briefly
describes the meaning of a dialogue act, and
?start=14:00? and ?end=15:00? are add-on in-
formation.1
1In general, a dialogue act corresponds to one sentence.
However, in dialogues where user utterances are unrestricted,
smaller units, such as phrases, can be regarded as dialogue acts.
Speech
Recognizer
Language
Understanding
Component
Discourse
Understanding
Component
Dialogue
State
Dialogue
Manager
Speech
Synthesizer
Update
Update
Refer
Refer
Speech Recognition
Hypothesis Dialogue Act
Figure 1: Architecture of a spoken dialogue system.
3. The discourse understanding component re-
ceives the dialogue act, refers to the current di-
alogue state, and updates the dialogue state.
4. The dialogue manager receives the current dia-
logue state, decides the next utterance, and out-
puts the next words to speak. The dialogue state
is updated at the same time so that it contains
the content of system utterances.
5. The speech synthesizer receives the output of
the dialogue manager and responds to the user
by speech.
This paper deals with the discourse understand-
ing component. Since we are resolving the ambi-
guity of speech understanding from the discourse
point of view and not within the speech understand-
ing candidates, we assume that a dialogue state is
uniquely determined given a dialogue state and the
next dialogue act, which means that a dialogue act
is a command to change a dialogue state. We also
assume that the relationship between the dialogue
act and the way to update the dialogue state can be
easily described without expertise in dialogue sys-
tem research. We found that these assumptions are
reasonable from our experience in system develop-
ment. Note also that this paper does not separately
deal with reference resolution; we assume that it is
performed by a command. A speech understanding
result is considered to be equal to a dialogue act in
this article.
In this paper, we consider frames as representa-
tions of dialogue states. To represent dialogue states,
plans have often been used (Allen and Perrault,
1980; Carberry, 1990). Traditionally, plan-based
discourse understanding methods have been imple-
mented mostly in keyboard-based dialogue systems,
User Utterance ?from two p.m. to three p.m.?
Dialogue Act [act-type=refer-start-and-end-
time, start=14:00, end=15:00]
Table 1: A user utterance and the corresponding di-
alogue act.
although there are some recent attempts to apply
them to spoken dialogue systems as well (Allen et
al., 2001; Rich et al, 2001); however, considering
the current performance of speech recognizers and
the limitations in task domains, we believe frame-
based discourse understanding and dialogue man-
agement are sufficient (Chu-Carroll, 2000; Seneff,
2002; Bobrow et al, 1977).
3 Problem
Most conventional spoken dialogue systems
uniquely determine the dialogue state after a user
utterance. Normally, however, there are multiple
candidates for the result of speech understanding,
which leads to the creation of multiple dialogue
state candidates. We believe that there are cases
where it is better to hold more than one dialogue
state and resolve the ambiguity as the dialogue
progresses rather than to decide on a single dialogue
state after each user utterance.
As an example, consider a piece of dialogue in
which the user utterance ?from two p.m.? has been
misrecognized as ?uh two p.m.? (Figure 2). Fig-
ure 3 shows the description of the example dia-
logue in detail including the system?s inner states,
such as dialogue acts corresponding to the speech
recognition hypotheses2 and the intention recogni-
tion results.3 After receiving the speech recogni-
tion hypothesis ?uh two p.m.,? the system cannot
tell whether the user utterance corresponds to a dia-
logue act specifying the start time or the end time
(da1,da2). Therefore, the system tries to obtain
further information about the time. In this case,
the system utters a backchannel to prompt the next
user utterance to resolve the ambiguity from the dis-
course.4 At this stage, the system holds two dialogue
2In this example, for convenience of explanation, the n-best
speech recognition input is not considered.
3An intention recognition result is one of the elements of a
dialogue state.
4A yes/no question may be an appropriate choice as well.
 
S1 : what time would you like to reserve a
meeting room?
U1 : from two p.m. [uh two p.m.]
S2 : uh-huh
U2 : to three p.m. [to three p.m.]
S3 : from two p.m. to three p.m.?
U3 : yes [yes]
 
Figure 2: Example dialogue.
(S means a system utterance and U a user utterance.
Recognition results are enclosed in square brackets.)
states having different intention recognition results
(ds1,ds2). The next utterance, ?to three p.m.,? is
one that uniquely corresponds to a dialogue act spec-
ifying the end time (da3), and thus updates the two
current dialogue states. As a result, two dialogue
states still remain (ds3,ds4). If the system can tell
that the previous dialogue act was about the start
time at this moment, it can understand the user in-
tention correctly. The correct understanding result,
ds3, is derived from the combination of ds1 and
da3, where ds1 is induced by ds0 and da1. As
shown here, holding multiple understanding results
can be better than just deciding on the best speech
understanding hypothesis and discarding other pos-
sibilities.
In this paper, we consider a discourse understand-
ing component that deals with multiple dialogue
states. Such a component must choose the best com-
bination of a dialogue state and a dialogue act out of
all possibilities. An appropriate scoring method for
the dialogue states is therefore required.
4 Related Work
Nakano et al (1999) proposed a method that holds
multiple dialogue states ordered by priority to deal
with the problem that some utterances convey mean-
ing over several speech intervals and that the under-
standing result cannot be determined at each inter-
val end. Miyazaki et al (2002) proposed a method
combining Nakano et al?s (1999) method and n-best
recognition hypotheses, and reported improvement
in discourse understanding accuracy. They used a
metric similar to the concept error rate for the evalu-
[System utterance (S1)]
?What time would you like to reserve a meeting
room??
[Dialogue act] [act-type=ask-time]
[Intention recognition result candidates]
1. [room=nil, start=nil, end=nil] (ds0)
?
[User utterance (U1)]
?From two p.m.?
[Speech recognition hypotheses]
1. ?uh two p.m.?
[Dialogue act candidates]
1. [act-type=refer-start-time,time=14:00] (da1)
2. [act-type=refer-end-time,time=15:00] (da2)
[Intention recognition result candidates]
1. [room=nil, start=14:00, end=nil]
(ds1, induced from ds0 and da1)
2. [room=nil, start=nil, end=14:00]
(ds2, induced from ds0 and da2)
?
[System utterance (S2)] ?uh-huh?
[Dialogue act] [act-type=backchannel]
?
[User utterance (U2)]
?To three p.m.?
[Speech recognition hypotheses]
1. ?to three p.m.?
[Dialogue act candidates]
1. [act-type=refer-end-time, time=15:00] (da3)
[Intention recognition result candidates]
1. [room=nil, start=14:00, end=15:00]
(ds3, induced from ds1 and da3)
2. [room=nil, start=nil, end=15:00]
(ds4, induced from ds2 and da3)
?
[System utterance (S3)]
?from two p.m. to three p.m.??
[Dialogue act]
[act-type=confirm-time,start=14:00, end=15:00]
?
[User utterance (U3)] ?yes?
[Speech recognition hypotheses]
1. ?yes?
[Dialogue act candidates]
1. [act-type=acknowledge]
[Intention recognition result candidates]
1. [room=nil, start=14:00, end=15:00]
2. [room=nil, start=nil, end=15:00]
Figure 3: Detailed description of the understanding
of the example dialogue.
ation of discourse accuracy, comparing reference di-
alogue states with hypothesis dialogue states. Both
these methods employ hand-crafted rules to score
the dialogue states to decide the best dialogue state.
Creating such rules requires expert knowledge, and
is also time consuming.
There are approaches that propose statistically es-
timating the dialogue act type from several previous
dialogue act types using N-gram probability (Nagata
and Morimoto, 1994; Reithinger and Maier, 1995).
Although their approaches can be used for disam-
biguating user utterance using discourse informa-
tion, they do not consider holding multiple dialogue
states.
In the context of plan-based utterance understand-
ing (Allen and Perrault, 1980; Carberry, 1990),
when there is ambiguity in the understanding re-
sult of a user utterance, an interpretation best suited
to the estimated plan should be selected. In ad-
dition, the system must choose the most plausible
plans from multiple possible candidates. Although
we do not adopt plan-based representation of dia-
logue states as noted before, this problem is close to
what we are dealing with. Unfortunately, however,
it seems that no systematic ways to score the candi-
dates for disambiguation have been proposed.
5 Approach
The discourse understanding method that we pro-
pose takes the same approach as Miyazaki et al
(2002). However, our method is different in that,
when ordering the multiple dialogue states, the sta-
tistical information derived from the dialogue cor-
pora is used. We propose using two kinds of statisti-
cal information:
1. the probability of a dialogue act type sequence,
and
2. the collocation probability of a dialogue state
and the next dialogue act.
5.1 Statistical Information
Probability of a dialogue act type sequence
Based on the same idea as Nagata and Morimoto
(1994) and Reithinger and Maier (1995), we use the
probability of a dialogue act type sequence, namely,
the N-gram probability of dialogue act types. Sys-
tem utterances and the transcription of user utter-
ances are both converted to dialogue acts using a di-
alogue act conversion parser, then the N-gram prob-
ability of the dialogue act types is calculated.
# explanation
1. whether slots asked previously by the system
are changed
2. whether slots being confirmed are changed
3. whether slots already confirmed are changed
4. whether the dialogue act fills slots that do not
have values
5. whether the dialogue act tries changing slots
that have values
6. when 5 is true, whether slot values are not
changed as a result
7. whether the dialogue act updates the initial
dialogue state 5
Table 2: Seven binary attributes to classify collo-
cation patterns of a dialogue state and the next dia-
logue act.
Collocation probability of a dialogue state and
the next dialogue act From the dialogue corpora,
dialogue states and the succeeding user utterances
are extracted. Then, pairs comprising a dialogue
state and a dialogue act are created after convert-
ing user utterances into dialogue acts. Contrary to
the probability of sequential patterns of dialogue act
types that represents a brief flow of a dialogue, this
collocation information expresses a local detailed
flow of a dialogue, such as dialogue state changes
caused by the dialogue act. The simple bigram of
dialogue states and dialogue acts is not sufficient
due to the complexity of the data that a dialogue
state possesses, which can cause data sparseness
problems. Therefore, we classify the ways that di-
alogue states are changed by dialogue acts into 64
classes characterized by seven binary attributes (Ta-
ble 2) and compute the occurrence probability of
each class in the corpora. We assume that the un-
derstanding result of the user intention contained in
a dialogue state is expressed as a frame, which is
common in many systems (Bobrow et al, 1977). A
frame is a bundle of slots that consist of attribute-
value pairs concerning a certain domain.
5The first user utterance should be treated separately, be-
cause the system?s initial utterance is an open question leading
to an unrestricted utterance of a user.
5.2 Scoring of Dialogue Acts
Each speech recognition hypothesis is converted to
a dialogue act or acts. When there are several di-
alogue acts corresponding to a speech recognition
hypothesis, all possible dialogue acts are created as
in Figure 3, where the utterance ?uh two p.m.? pro-
duces two dialogue act candidates. Each dialogue
act is given a score using its linguistic and acous-
tic scores. The linguistic score represents the gram-
matical adequacy of a speech recognition hypothe-
sis from which the dialogue act originates, and the
acoustic score the acoustic reliability of a dialogue
act. Sometimes, there is a case that a dialogue act
has such a low acoustic or linguistic score and that
it is better to ignore the act. We therefore create a
dialogue act called null act, and add this null act to
our list of dialogue acts. A null act is a dialogue act
that does not change the dialogue state at all.
5.3 Scoring of Dialogue States
Since the dialogue state is uniquely updated by a di-
alogue act, if there are l dialogue acts derived from
speech understanding and m dialogue states, m ? l
new dialogue states are created. In this case, we de-
fine the score of a dialogue state S
t+1
as
S
t+1
= S
t
+ ? ? s
act
+ ? ? s
ngram
+ ? ? s
col
where S
t
is the score of a dialogue state just before
the update, s
act
the score of a dialogue act, s
ngram
the score concerning the probability of a dialogue
act type sequence, s
col
the score concerning the col-
location probability of dialogue states and dialogue
acts, and ?, ?, and ? are the weighting factors.
5.4 Ordering of Dialogue States
The newly created dialogue states are ordered based
on the score. The dialogue state that has the best
score is regarded as the most probable one, and the
system responds to the user by referring to it. The
maximum number of dialogue states is needed in
order to drop low-score dialogue states and thereby
perform the operation in real time. This dropping
process can be considered as a beam search in view
of the entire discourse process, thus we name the
maximum number of dialogue states the dialogue
state beam width.
6 Experiment
6.1 Extracting Statistical Information from Di-
alogue Corpus
Dialogue Corpus We analyzed a corpus of dia-
logues between naive users and a Japanese spoken
dialogue system, which were collected in acousti-
cally insulated booths. The task domain was meet-
ing room reservation. Subjects were instructed to
reserve a meeting room on a certain date from a cer-
tain time to a certain time. As a speech recognition
engine, Julius3.1p1 (Lee et al, 2001) was used with
its attached acoustic model. For the language model,
we used a trigram trained from randomly generated
texts of acceptable phrases. For system response,
NTT?s speech synthesis engine FinalFluet (Takano
et al, 2001) was used. The system had a vocabulary
of 168 words, each registered with a category and
a semantic feature in its lexicon. The system used
hand-crafted rules for discourse understanding. The
corpus consists of 240 dialogues from 15 subjects
(10 males and 5 females), each one performing 16
dialogues. Dialogues that took more than three min-
utes were regarded as failures. The task completion
rate was 78.3% (188/240).
Extraction of Statistical Information From the
transcription, we created a trigram of dialogue act
types using the CMU-Cambridge Toolkit (Clarkson
and Rosenfeld, 1997). Figure 3 shows an example
of the trigram information starting from {refer-start-
time backchannel}. The bigram information used
for smoothing is also shown. The collocation proba-
bility was obtained from the recorded dialogue states
and the transcription following them. Out of 64 pos-
sible patterns, we found 17 in the corpus as shown in
Figure 4. Taking the case of the example dialogue in
Figure 3, it happened that the sequence {refer-start-
time backchannel refer-end-time} does not appear in
the corpus; thus, the probability is calculated based
on the bigram probability using the backoff weight,
which is 0.006. The trigram probability for {refer-
end-time backchannel refer-end-time} is 0.031.
The collocation probability of the sequence ds1
+ da3 ? ds3 fits collocation pattern 12, where a
slot having no value was changed. The sequence
ds2 + da3 ? ds4 fits collocation pattern 17, where
a slot having a value was changed to have a differ-
ent value. The probabilities were 0.155 and 0.009,
dialogue act type sequence (trigram) probability
score
refer-start-time backchannel backchannel -1.0852
refer-start-time backchannel ask-date -2.0445
refer-start-time backchannel ask-start-time -0.8633
refer-start-time backchannel request -2.0445
refer-start-time backchannel refer-day -1.7790
refer-start-time backchannel refer-month -0.4009
refer-start-time backchannel refer-room -0.8633
refer-start-time backchannel refer-start-time -0.7172
dialogue act type sequence
(bigram)
backoff
weight
probability
score
refer-start-time backchannel -1.1337 -0.7928
refer-end-time backchannel 0.4570 -0.6450
backchannel refer-end-time -0.5567 -1.0716
Table 3: An example of bigram and trigram of dia-
logue act types with their probability score in com-
mon logarithm.
collocation occurrence
# pattern probability
1. 0 1 1 1 0 0 1 0.001
2. 0 1 1 0 0 1 0 0.053
3. 0 0 0 0 0 0 0 0.273
4. 1 0 0 0 1 0 0 0.001
5. 1 0 1 1 0 0 0 0.005
6. 0 0 1 1 0 0 0 0.036
7. 0 0 0 0 1 0 0 0.047
8. 0 1 1 0 1 0 0 0.041
9. 0 0 1 1 0 0 1 0.010
10. 0 0 1 0 0 1 0 0.016
11. 0 0 0 0 0 0 1 0.064
12. 0 0 0 1 0 0 0 0.155
13. 1 0 0 1 0 0 0 0.043
14. 0 0 1 0 1 0 0 0.061
15. 1 0 0 1 0 0 1 0.001
16. 0 0 0 1 0 0 1 0.186
17. 0 0 0 0 0 1 0 0.009
Table 4: The 17 collocation patterns and their oc-
currence probabilities. See Figure 2 for the detail
of binary attributes. Attributes 1-7 are ordered from
left to right.
respectively. By the simple adding of the two proba-
bilities in common logarithms in each case, ds3 has
the probability score -3.015 and ds4 -3.549, sug-
gesting that the sequence ds3 is the most probable
discourse understanding result after U2.
6.2 Verification of our approach
To verify the effectiveness of the proposed ap-
proach, we built a Japanese spoken dialogue system
in the meeting reservation domain that employs the
proposed discourse understanding method and per-
formed dialogue experiments.
The speech recognition engine was Julius3.3p1
(Lee et al, 2001) with its attached acoustic models.
For the language model, we made a trigram from
the transcription obtained from the corpora. The
system had a vocabulary of 243. The recognition
engine outputs 5-best recognition hypotheses. This
time, values for s
act
, s
ngram
, s
col
are the logarithm
of the inverse number of n-best ranks,6 the log like-
lihood of dialogue act type trigram probability, and
the common logarithm of the collocation probabil-
ity, respectively. For the experiment, weighting fac-
tors are all set to one (? = ? = ? = 1). The di-
alogue state beam width was 15. We collected 256
dialogues from 16 subjects (7 males and 9 females).
The speech recognition accuracy (word error rate)
was 65.18%. Dialogues that took more than five
minutes were regarded as failures. The task com-
pletion rate was 88.3% (226/256).7
From all user speech intervals, the number of
times that dialogue states below second place be-
came first place was 120 (7.68%), showing a relative
frequency of shuffling within the dialogue states.
6.3 Effectiveness of Holding Multiple Dialogue
States
The main reason that we developed the proposed
corpus-based discourse understanding method was
that it is difficult to manually create rules to deal
with multiple dialogue states. It is yet to be exam-
ined, however, whether holding multiple dialogue
states is really effective for accurate discourse un-
derstanding.
To verify that holding multiple dialogue states is
effective, we fixed the speech recognizer?s output to
1-best, and studied the system performance changes
when the dialogue state beam width was changed
from 1 to 30. When the dialogue state beam width is
too large, the computational cost becomes high and
the system cannot respond in real time. We therefore
selected 30 for empirical reasons.
The task domain and other settings were the same
6In this experiment, only the acoustic score of a dialogue act
was considered.
7It should be noted that due to the creation of an enormous
number of dialogue states in discourse understanding, the pro-
posed system takes a few seconds to respond after the user in-
put.
as in the previous experiment except for the dialogue
state beam width changes. We collected 448 dia-
logues from 28 subjects (4 males and 24 females),
each one performing 16 dialogues. Each subject was
instructed to reserve the same meeting room twice,
once with the 1-beam-width system and again with
30-beam-width system. The order of what room to
reserve and what system to use was randomized.
The speech recognition accuracy was 69.17%. Di-
alogues that took more than five minutes were re-
garded as failures. The task completion rates for the
1-beam-width system and the 30-beam-width sys-
tem were 88.3% and 91.0%, and the average task
completion times were 107.66 seconds and 95.86
seconds, respectively. A statistical hypothesis test
showed that times taken to carry out a task with the
30-beam-width system are significantly shorter than
those with the 1-beam-width system (Z = ?2.01,
p < .05). In this test, we used a kind of censored
mean computed by taking the mean of the times
only for subjects that completed the tasks with both
systems. The population distribution was estimated
by the bootstrap method (Cohen, 1995). It may be
possible to evaluate the discourse understanding by
comparing the best dialogue state with the reference
dialogue state, and calculate a metric such as the
CER (concept error rate) as Miyazaki et al (2002)
do; however it is not clear whether the discourse
understanding can be evaluated this way, since it is
not certain whether the CER correlates closely with
the system?s performance (Higashinaka et al, 2002).
Therefore, this time, we used the task completion
time and the task completion rate for comparison.
7 Discussion
Cost of creating the discourse understanding
component The best task completion rate in the ex-
periments was 91.0% (the case of 1-best recognition
input and a 30 dialogue state beam width). This high
rate suggests that the proposed approach is effective
in reducing the cost of creating the discourse un-
derstanding component in that no hand-crafted rules
are necessary. For statistical discourse understand-
ing, an initial system, e.g., a system that employs
the proposed approach with only s
act
for scoring the
dialogue states, is needed in order to create the di-
alogue corpus; however, once it has been made, the
creation of the discourse understanding component
requires no expert knowledge.
Effectiveness of holding multiple dialogue states
The result of the examination of dialogue state beam
width changes suggests that holding multiple dia-
logue states shortens the task completion time. As
far as task-oriented spoken dialogue systems are
concerned, holding multiple dialogue states con-
tributes to the accuracy of discourse understanding.
8 Summary and Future Work
We proposed a new discourse understanding method
that orders multiple dialogue states created from
multiple dialogue states and the succeeding speech
understanding results based on statistical informa-
tion obtained from dialogue corpora. The results of
the experiments show that our approach is effective
in reducing the cost of creating the discourse under-
standing component, and the advantage of keeping
multiple dialogue states was also shown.
There still remain several issues that we need to
explore. These include the use of statistical informa-
tion other than the probability of a dialogue act type
sequence and the collocation probability of dialogue
states and dialogue acts, the optimization of weight-
ing factors ?, ?, ?, other default parameters that we
used in the experiments, and more experiments in
larger domains. Despite these issues, the present re-
sults have shown that our approach is promising.
Acknowledgements
We thank Dr. Hiroshi Murase and all members of the
Dialogue Understanding Research Group for useful
discussions. Thanks also go to the anonymous re-
viewers for their helpful comments.
References
James F. Allen and C. Raymond Perrault. 1980. Analyz-
ing intention in utterances. Artif. Intel., 15:143?178.
James Allen, George Ferguson, and Amanda Stent. 2001.
An architecture for more realistic conversational sys-
tems. In Proc. IUI, pages 1?8.
Daniel G. Bobrow, Ronald M. Kaplan, Martin Kay, Don-
ald A. Norman, Henry Thompson, and Terry Wino-
grad. 1977. GUS, a frame driven dialog system. Artif.
Intel., 8:155?173.
Sandra Carberry. 1990. Plan Recognition in Natural
Language Dialogue. MIT Press, Cambridge, Mass.
Junnifer Chu-Carroll. 2000. MIMIC: An adaptive
mixed initiative spoken dialogue system for informa-
tion queries. In Proc. 6th Applied NLP, pages 97?104.
P.R. Clarkson and R. Rosenfeld. 1997. Statistical lan-
guagemodeling using the CMU-Cambridge toolkit. In
Proc. Eurospeech, pages 2707?2710.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. MIT Press.
Ryuichiro Higashinaka, Noboru Miyazaki, Mikio
Nakano, and Kiyoaki Aikawa. 2002. A method
for evaluating incremental utterance understanding
in spoken dialogue systems. In Proc. ICSLP, pages
829?832.
Akinobu Lee, Tatsuya Kawahara, and Kiyohiro Shikano.
2001. Julius ? an open source real-time large vocab-
ulary recognition engine. In Proc. Eurospeech, pages
1691?1694.
Noboru Miyazaki, Mikio Nakano, and Kiyoaki Aikawa.
2002. Robust speech understanding using incremen-
tal understanding with n-best recognition hypothe-
ses. In SIG-SLP-40, Information Processing Society
of Japan., pages 121?126. (in Japanese).
Masaaki Nagata and Tsuyoshi Morimoto. 1994. First
steps toward statistical modeling of dialogue to predict
the speech act type of the next utterance. Speech Com-
munication, 15:193?203.
Mikio Nakano, Noboru Miyazaki, Jun-ichi Hirasawa,
Kohji Dohsaka, and Takeshi Kawabata. 1999. Un-
derstanding unsegmented user utterances in real-time
spoken dialogue systems. In Proc. 37th ACL, pages
200?207.
Norbert Reithinger and Elisabeth Maier. 1995. Utiliz-
ing statistical dialogue act processing in Verbmobil. In
Proc. 33th ACL, pages 116?121.
Charles Rich, Candace Sidner, and Neal Lesh. 2001.
COLLAGEN: Applying collaborative discourse the-
ory. AI Magazine, 22(4):15?25.
Stephanie Seneff. 2002. Response planning and genera-
tion in the MERCURY flight reservation system. Com-
puter Speech and Language, 16(3?4):283?312.
Satoshi Takano, Kimihito Tanaka, Hideyuki Mizuno,
Masanobu Abe, and ShiN?ya Nakajima. 2001. A
Japanese TTS system based on multi-form units and a
speech modification algorithm with harmonics recon-
struction. IEEE Transactions on Speech and Process-
ing, 9(1):3?10.
Victor W. Zue and James R. Glass. 2000. Conversational
interfaces: Advances and challenges. Proceedings of
IEEE, 88(8):1166?1180.
WIT: A Toolkit for Building Robust and Real-Time Spoken Dialogue 
Systems 
Mikio Nakano* Noboru Miyazaki, Norihito Yasuda, Akira Sugiyama, 
Jun-ichi Hirasawa, Kohji Dohsaka, Kiyoaki Aikawa 
NTT Corporation 
3-1 Morinosato-Wakamiya 
Atsugi, Kanagawa 243-0198, Japan 
E-mail: nakano@atom.brl.ntt.co.jp 
Abstract 
This paper describes WI'I; a toolkit 
for building spoken dialogue systems. 
WIT features an incremental under- 
standing mechanism that enables ro- 
bust utterance understanding and real- 
time responses. WIT's ability to com- 
pile domain-dependent system specifi- 
cations into internal knowledge sources 
makes building spoken dialogue sys- 
tems much easier than :it is from 
scratch. 
1 Introduction 
The recent great advances in speech and language 
technologies have made it possible to build fully 
implemented spoken dialogue systems (Aust et 
al., 1995; Allen et al, 1996; Zue et al, 2000; 
Walker et al, 2000). One of the next research 
goals is to make these systems task-portable, that 
is, to simplify the process of porting to another 
task domain. 
To this end, several toolkits for building spo- 
ken dialogue systems have been developed (Bar- 
nett and Singh, 1997; Sasajima et al, 1999). 
One is the CSLU Toolkit (Sutton et al, 1998), 
which enables rapid prototyping of a spoken di- 
alogue system that incorporates a finite-state dia- 
logue model. It decreases the amount of the ef- 
fort required in building a spoken dialogue sys- 
tem in a user-defined task domain. However, it 
limits system functions; it is not easy to employ 
the advanced language processing techniques de- 
veloped in the realm of computational linguis- 
tics. Another is GALAXY-II (Seneffet al, 1998), 
*Mikio Nakano is currently a visiting scientist at MIT 
Laboratory for Computer Science. 
which enables modules in a dialogue system to 
communicate with each other. It consists of the 
hub and several servers, such as the speech recog- 
nition server and the natural language server, and 
the hub communicates with these servers. Al- 
though it requires more specifications than finite- 
state-model-based toolkits, it places less limita- 
tions on system functions. 
Our objective is to build robust and real-time 
spoken dialogue systems in different ask do- 
mains. By robust we mean utterance understand- 
ing is robust enough to capture not only utter- 
ances including rammatical errors or self-repairs 
but also utterances that are not clearly segmented 
into sentences by pauses. Real time means the 
system can respond to the user in real time. The 
reason we focus on these features i  that they are 
crucial to the usability of spoken dialogue sys- 
tems as well as to the accuracy of understand- 
ing and appropriateness of the content of the sys- 
tem utterance. Robust understanding allows the 
user to speak to the system in an unrestricted 
way. Responding in real time is important be- 
cause if a system response is delayed, the user 
might think that his/her utterance was not recog- 
nized by the system and make another utterance, 
making the dialogue disorderly. Systems having 
these features hould have several modules that 
work in parallel, and each module needs some 
domain-dependent k owledge sources. Creat- 
ing and maintaining these knowledge sources re- 
quire much effort, thus a toolkit would be help- 
ful. Previous toolkits, however, do not allow us to 
achieve these features, or do not provide mecha- 
nisms that achieve these features without requir- 
ing excessive fforts by the developers. 
This paper presents WIT 1, which is a toolkit 
IWIT is an acronym of Workable spoken dialogue lnter- 
150 
for building spoken dialogue systems that inte- 
grate speech recognition, language understanding 
and generation, and speech output. WIT features 
an incremental understanding method (Nakano et 
al., 1999b) that makes it possible to build a robust 
and real-time system. In addition, WIT compiles 
domain-dependent system specifications into in- 
ternal knowledge sources o that building systems 
is easier. Although WIT requires more domain- 
dependent specifications than finite-state-model- 
based toolkits, WIT-based systems are capable 
of taking full advantage of language processing 
technology. WIT has been implemented and used 
to build several spoken dialogue systems. 
In what follows, we overview WIT, explain its 
architecture, domain-dependent system specifica- 
tions, and implementation, and then discuss its 
advantages and problems. 
2 Overview 
A WIT-based spoken dialogue system has four 
main modules: the speech recognition module, 
the language understanding module, the lan- 
guage generation module, and the speech out- 
put module. These modules exploit domain- 
dependent knowledge sources, which are auto- 
matically generated from the domain-dependent 
system specifications. The relationship among 
the modules, knowledge sources, and specifica- 
tions are depicted in Figure 1. 
WIT can also display and move a human-face- 
like animated agent, which is controlled by the 
speech output module, although this paper does 
not go into details because it focuses only on spo- 
ken dialogue. We also omit the GUI facilities pro- 
vided by WIT. 
3 Architecture of  WIT-Based Spoken 
Dialogue Systems 
Here we explain how the modules in WIT work 
by exploiting domain-dependent k owledge and 
how they interact with each other. 
3.1 Speech Recognition 
The speech recognition module is a phoneme- 
HMM-based speaker-independent continuous 
speech recognizer that incrementally outputs 
face Toolldt. 
word hypotheses. As the recogn/fion engine, 
either VoiceRex, developed by NTI" (Noda et 
al., 1998), or HTK from Entropic Research can 
be used. Acoustic models for HTK is trained 
with the continuous peech database of the 
Acoustical Society of Japan (Kobayashi et al, 
1992). This recognizer incrementally outputs 
word hypotheses a soon as they are found in the 
best-scored path in the forward search (Hirasawa 
et al, 1998) using the ISTAR (Incremental 
Structure Transmitter And Receiver) protocol, 
which conveys word graph information as well as 
word hypotheses. This incremental output allows 
the language understanding module to process 
recognition results before the speech interval 
ends, and thus real-time responses are possible. 
This module continuously runs and outputs 
recognition results when it detects a speech 
interval. This enables the language generation 
module to react immediately touser interruptions 
while the system is speaking. 
The language model for speech recognition 
is a network (regular) grammar, and it allows 
each speech interval to be an arbitrary number 
of phrases. A phrase is a sequence of words, 
which is to be defined in a domain-dependent 
way. Sentences can be decomposed into a cou- 
ple of phrases. The reason we use a repeti- 
tion of phrases instead of a sentence grammar 
for the language model is that the speech recog- 
nition module of a robust spoken dialogue sys- 
tem sometimes has to recognize spontaneously 
spoken utterances, which include self-repairs and 
repetition. In Japanese, bunsetsu is appropriate 
for defining phrases. A bunsetsu consists of one 
content word and a number (possibly zero) of 
function words. In the meeting room reservation 
system we have developed, examples of defined 
phrases are bunsetsu to specify the room to be re- 
served and the time of the reservation and bun- 
setsu to express affirmation and negation. 
When the speech recognition module finds a 
phrase boundary, it sends the category of the 
phrase to the language understanding module, 
and this information is used in the parsing pro- 
cess. 
It is possible to hold multiple language mod- 
els and use any one of them when recogniz- 
ing a speech interval. The language models are 
151 
Semantic \[ 
I ~e  I 
/ specifications \[ r 1 
I R~ae I /L..___..--  - ' - ' -~ ' -  / Ph~e l 
/de~;,i~ions I /" "-. I de~i*~._l 
I Feature I L._.___..~.. -~-'-'-'~ , ( "~ "'. 
L____i----',.,'-... ~ ~ "-.. 
? Surface- I de .~ons  \[:- ,, l ;~ : : - - ?Y \ l  Language II Language I ~Genera f io~n_ /  . I , ~ ,  "., "~"----~__Z_--.--" M . -. i i . ) i . . . . . . .  l,,_J generaraon I ~ \  ,, ~ ~ unaerstanding I I generation IO  t procedures I TM / . . . .  ~ . . . .  I 
\ I I I . . . .  J 
I definitions I '\ \ I word I strings I hypothesea + 
I . - -  I _ __~ '~seto f - " -L__~I  Speech i I~ ,~ L iT -s t  o f - - / L / i . , i s to f  
I ~"  t I ~angu.~ge I- -I ~t io .  I I ou~u, r ' - - ' \ ]  pre-r~o.dedr'--\] pre-r~o~ed I 
I d~f~i~23~_l t.models .....~1 I module I I ~oam~ I~Peech m~._J , I L_  j 
user utterance system utterance 
domain-dependent: 
specification knowledge source module 
Figure 1: Architecture of WIT 
switched according to the requests from the lan- 
guage understanding module. In this way, the 
speech recognition success rate is increased by 
using the context of the dialogue. 
Although the current version of WIT does not 
exploit probabilistic language models, such mod- 
els can be incorporated without changing the ba- 
sic WIT architecture. 
3.2 Language Understanding 
The language understanding :module receives 
word hypotheses from the speech recognition 
module and incrementally understands the se- 
quence of the word hypotheses to update the di- 
alogue state, in which the resnlt of understand- 
ing and discourse information are represented 
by a frame (i.e., attribute-value pairs). The un- 
derstanding module utilizes ISSS (Incremental 
Significant-utterance Sequence Search) (Nakano 
et al, 1999b), which is an integrated parsing and 
discourse processing method. ISSS enables the 
incremental understanding of user utterances that 
are not segmented into sentences prior to pars- 
ing by incrementally finding the most plausible 
sequence of sentences (or significant utterances 
in the ISSS terms) out of the possible sentence 
sequences for the input word sequence. ISSS 
also makes it possible for the language generation 
module to respond in real time because it can out- 
put a partial result of understanding at any point 
in time. 
The domain-dependent knowledge used in this 
module consists of a unification-based lexicon 
and phrase structure rules. Disjunctive feature 
descriptions are also possible; WIT incorporates 
an efficient method for handling disjunctions 
(Nakano, 1991). When a phrase boundary is de- 
tected, the feature structure for a phrase is com- 
puted using some built-in rules from the feature 
structure rules for the words in the phrase. The 
phrase structure rules specify what kind of phrase 
sequences can be considered as sentences, and 
they also enable computing the semantic repre- 
sentation for found sentences. Two kinds of sen- 
tenees can be considered; domain-related ones 
that express the user's intention about he reser- 
152 
vafion and dialogue-related ones that express the 
user's attitude with respect to the progress of the 
dialogue, such as confirmation and denial. Con- 
sidering the meeting room reservation system, ex- 
amples of domain-related sentences are "I need to 
book Room 2 on Wednesday", I need to book 
Room 2", and "Room 2" and dialogue-related 
ones are "yes", "no", and "Okay". 
The semantic representation for a sentence is
a command for updatingthe dialogue state. The 
dialogue state is represented bya list of attribute- 
value pairs. For example, attributes used in the 
meeting room reservation system include task- 
related attributes, such as the date and time of 
the reservation, as well as attributes that represent 
discourse-related information, such as confirma- 
tion and grounding. 
3.3 Language Generation 
How the language generation module works 
varies depending on whether the user or system 
has the initiative of turn taking in the dialogue 2.
Precisely speaking, the participant having the ini- 
tiative is the one the system assumes has it in the 
dialogue. 
The domain-dependent k owledge used by the 
language generation module is generation proce- 
dures, which consist of a set of dialogue-phase 
definitions. For each dialogue phase, an initial 
function, an action function, a time-out function, 
and a language model are assigned. In addition, 
phase definitions designate whether the user or 
the system has the initiative. In the phases in 
which the system has the initiative, only the ini- 
tial function and the language model are assigned. 
The meeting room reservation system, for exam- 
ple, has three phases: the phase in which the 
user tells the system his/her equest, he phase in 
which the system confirms it, and the phase in 
which the system tells the user the result of the 
database access. In the first two phases, the user 
holds the initiative, and in the last phase, the sys- 
tern holds the initiative. 
Functions defined here decide what string 
should be spoken and send that string to the 
speech output module based on the current di- 
alogue state. They can also shift the dialogue 
2The notion of the initiative inthis paper isdifferent from 
that of the dialogue initiative of Chu-Carroll (2000). 
phase and change the holder of the initiative as 
well as change the dialogue state. When the dia- 
logue phase shifts, the language model foi" speech 
recognition is changed to get better speech recog- 
nition performance. Typically, the language gen- 
eration module is responsible for database access. 
The language generation module works as fol- 
lows. It first checks which dialogue participant 
has the initiative. If the initiative is held by the 
user, it waits until the user's speech interval ends 
or a duration of silence after the end of a system 
utterance is detected. The action function in the 
dialogue phase at that point in time is executed in
the former case; the time-out function is executed 
in the latter case. Then it goes back to the initial 
stage. If the system holds the initiative, the mod- 
ule executes the initial function of the phase. In 
typical question-answer systems, the user has the 
initiative when asking questions and the system 
has it when answering. 
Since the language generation module works in 
parallel with the language understanding module, 
utterance generation is possible even while the 
system is listening to user utterances and that ut- 
terance understanding is possible even while it is 
speaking (Nakano et al, 1999a). Thus the system 
can respond immediately after user pauses when 
the user has the initiative. When the system holds 
the initiative, it can immediately react to an in- 
terruption by the user because user utterances are 
understood in an incremental way (Dohsaka nd 
Shimazu, 1997). 
The time-out function is effective in moving 
the dialogue forward when the dialogue gets 
stuck for some reason. For example, the system 
may be able to repeat the same question with an- 
other expression and may also be able to ask the 
user a more specific question. 
3.4 Speech Output 
The speech output module produces peech ac- 
cording to the requests from the language gener- 
ation module by using the correspondence table 
between strings and pre-recorded speech data. It 
also notifies the language generation module that 
speech output has finished so that the language 
generation module can take into account the tim- 
ing of the end of system utterance. The meeting 
room reservation system uses speech files of short 
153 
phrases. 
4 Building Spoken Dialo~te Systems 
with WIT  
4.1 Domain-Dependent System 
Specifications 
Spoken dialogue systems can be built with WIT 
by preparing several domain-dependent specifica- 
tions. Below we explain the specifications. 
Feature Definitions: Feature definitions pec- 
ify the set of features used in the grammar for lan- 
guage understanding. They also specify whether 
each feature is a head feature or a foot feature 
(Pollard and Sag, 1994). This information isused 
when constructing feature structures for phrases 
in a built-in process. 
The following is an example of a feature defini- 
tion. Here we use examples from the specification 
of the meeting room reservation system. 
(case head) 
It means that the case feature is used and it is a 
head feature 3.
Lexieal Descriptions: Lexical descriptions 
specify both pronunciations and grammatical 
features for words. Below is an example lexical 
item for the word 1-gatsu (January). 
(l-gatsu ichigatsu month nil i) 
The first three elements are the identifier, the pro- 
nunciation, and the grammatical category of the 
word. The remaining two elements are the case 
and semantic feature values. 
Phrase Definitions: Phrase definitions pecify 
what kind of word sequence can be recognized 
as a phrase. Each definition is a pair compris- 
ing a phrase category name and a network of 
word categories. In the example below, month-  
phrase is the phrase category name and the re- 
maining part is the network of word categories. 
opt  means an option and or  means a disjunc- 
tion. For instance, a word sequence that con- 
sists of a word in the month  category, such as 1- 
gatsu (January), and a word in the adraon ina l -  
par t i c le  category, such as no (of), forms a 
phrase in the month-phrase  category. 
3In this section, we use examples of different description 
from the actual ones for simplicity. Actual specifications are 
written in part in Japanese. 
(month-phrase 
(month 
(opt 
(or 
expression-following-subject 
(admoninal-particle 
(opt 
sentence-final-particle)))))) 
Network Definitions: Network definitions 
specify what kind of phrases can be included in 
each language model. Each definition is a pair 
comprising a network name and a set of phrase 
category names. 
Semantic-Frame Specifications: The result of 
understanding and dialogue history can be stored 
in the dialogue state, which is represented by a 
flat frame structure, i.e., a set of attribute-value 
pairs. Semantic-frame specifications define the 
attributes used in the frame. The meeting room 
reservation system uses task-related attributes. 
Two are s tar t  and end, which represent the 
user's intention about the start and end times of 
the reservation for some meeting room. It also 
has attributes that represent discourse informa- 
tion. One is conf i rmed,  whose value indicates 
whether if the system has already made an utter- 
ance to confirm the content of the task-related at- 
tributes. 
Rule Definitions: Each rule has one of the fol- 
lowing two forms. 
((rule name) 
(child feature structure) 
? . .  (child feature structure) 
=> (mother feature structu_e) 
(priority increase) ) 
((role name) 
(child feature structure) 
? . .  (child feature structure) 
=> (flame operation command) 
(priority increase) ) 
These roles are similar to DCG (Pereira nd War- 
ren, 1980) rules; they can include logical vari- 
ables and these variables can be bound when 
these rules are applied. It is possible to add to the 
rules constraints that stipulate relationships that 
must hold among variables (Nakano, 199 I), but 
we do not explain these constraints indetail in this 
154 
paper. The priorities are used for disambiguat- 
ing interpretation i  the incremental understand- 
ing method (Nakano et al, 1999b). 
When the command on the right-hand side of 
the arrow is a frame operation command, phrases 
to which this rule can be applied can be consid- 
ered a sentence, and the sentence's semantic rep- 
resentation is the command for updating the dia- 
logue state. The command is one of the follow- 
ing: 
? A command to set the value of an attribute 
of the frame, 
? A command to increase the priority, 
Conditional commands (If-then-else type 
command, the condition being whether the 
value of an attribute of the flame is or is not 
equal to a specified value, or a conjunction 
or disjunction of the above condition), or 
? A list of commands to be sequentially exe- 
cuted. 
Thanks to conditional commands, it is possible 
to represent the semantics of sentences context- 
dependently. 
The following rule is an example. 
( s ta r t -end- t imes-command 
( t ime-phrase  : f rom *start) 
( t ime-phrase  (:or :to nil) *end) 
=> (command (set :start *start) 
(set :end *end))) 
The name of this rule is s ta r t -end- t imes-  
command. The second and third elements 
are child feature structures. In these elements, 
t ime-phrase  is a phrase category, : f rom and 
( : or : to n i l  ) are case feature values, and 
*s tar t  and *end are semantic feature val- 
ues. Here :or means a disjunction, and sym- 
bols starting with an asterisk are variables. The 
right-hand side of the arrow is a command to up- 
date the frame. The second element of the com- 
mand, (set :start  *start), changes the 
: s ta r t  atttribute value of the frame to the in- 
stance of *s tar t ,  which should be bound when 
applying this rule to the child feature structures. 
Phase Definitions: Each phase definition con- 
sists of a phase name, a network name, an ini- 
tiative holder specification, an initial function, an 
action function, a maximum silence duration, and 
a time-out function. The network name is the 
identifier of the language model for the speech 
recognition. The maximum silence duration spec- 
ifies how long the generation module should wait 
until the time-out function is invoked. 
Below is an example of a phase definition. 
The first element request  is the name of this 
phase, " f ra r_ request"  is the name of the 
network, and move- to - reques  t -phase  and 
request -phase-act ion  are the names of 
the initial and action functions. In this phase, 
the maximum silence duration is ten seconds and 
the name of the time-out function is request -  
phas e- t imeou t. 
(request " fmr_request"  
move-  to - reques  t -phase  
request -phase-act ion  
10.0 
request -phase-  t imeout  ) 
For the definitions of these functions, WIT pro- 
vides functions for accessing the dialogue state, 
sending a request o speak to the speech out- 
put module, generating strings to be spoken us- 
ing surface generation templates, hifting the di- 
alogue phase, taking and releasing the initiative, 
and so on. Functions are defined in terms of the 
Common Lisp program. 
Surface-generation Templates: Surface- 
generation templates are used by the surface 
generation library function, which converts 
a list-structured semantic representation to a 
sequence of strings. Each string can be spoken, 
i.e., it is in the list of pre-recorded speech files. 
For example, let us consider the conversion 
of the semantic representation (date  (date -  
express ion  3 15) ) to strings using the fol- 
lowing template. 
( (date 
(date -express ion  *month  *day)) 
( (*month gatsu) (*day nichi)  ) ) 
The surface generation library function matches 
the input semantic representation with the first el- 
ement of the template and checks if a sequences 
155 
of strings appear in the speech file list. It re- 
turns ( '  '3gagsu l5n ich i ' ' )  (March 15th) 
if the string "3gatsul5nichi" s in the list of 
pre-recorded speech files, and otherwise, returns 
( ' ' 3gatsu  . . . .  15n ich i '  ' ) when these 
strings are in the list. 
List of Pre-recorded Speech Files: The list of 
pre-recorded speech files should show the corre- 
spondence between strings and speech files to be 
played by the speech output module. 
4.2 Compiling System Specifications 
From the specifications explained above, domain- 
dependent knowledge sources are created as indi- 
cated by the dashed arrows in Figure 1. When cre- 
ating the knowledge sources, WIT checks for sev- 
eral kinds of consistency. For example, the set of 
word categories appearing in the lexicon and the 
set of word categories appearing in phrase deft- 
nifions are compared. This makes it easy to find 
errors in the domain specifications. 
5 Implementation 
WIT has been implemented in Common Lisp and 
C on UNIX, and we have built several experi- 
mental and demonstration dialogue systems using 
it, including a meeting room reservation system 
(Nakano et al, 1999b), a video-recording pro- 
gramming system, a schedule management sys- 
tem (Nakano et al, 1999a), and a weather in- 
formation system (Dohsaka et al, 2000). The 
meeting room reservation system has vocabulary 
of about 140 words, around 40 phrase structure 
rules, nine attributes in the semantic frame, and 
around 100 speech files. A sample dialogue be- 
tween this system and a naive user is shown 
in Figure 2. This system employs HTK as the 
speech recognition engine. The weather informa- 
tion system can answer the user's questions about 
weather forecasts in Japan. The vocabulary size 
is around 500, and the number of phrase structure 
rules is 31. The number of attributes in the se- 
mantic flame is 11, and the number of the files of 
the pre-recorded speech is about 13,000. 
6 Discussion 
As explained above, the architecture of WIT al- 
lows us to develop a system that can use utter- 
ances that are not clearly segmented into sen- 
tences by pauses and respond in real time. Below 
we discuss other advantages and remaining prob- 
lems. 
6.1 Descriptive Power 
Whereas previous finite-state-model-based tool- 
kits place many severe restrictions on domain de- 
scriptions, WIT has enough descriptive power to 
build a variety of dialogue systems. Although the 
dialogue state is represented bya simple attribute- 
value matrix, since there is no limitation on the 
number of attributes, it can hold more compli- 
cated information. For example, it is possible to 
represent a discourse stack whose depth is lim- 
ited. Recording some dialogue history is also 
possible. Since the language understanding mod- 
ule utilizes unification, a wide variety of lin- 
guistic phenomena can be covered. For exam- 
ple, speech repairs, particle omission, and fillers 
can be dealt with in the framework of unifica- 
tion grammar (Nakano et al, 1994; Nakano and 
Shimazu, 1999). The language generation mod- 
ule features Common Lisp functions, so there is 
no limitation on the description. Some of the 
systems we have developed feature a generation 
method based on hierarchical planning (Dohsaka 
and Shirnazu, 1997). It is also possible to build a 
simple finite-state-model-based dialogue system 
using WIT. States can be represented bydialogue 
phases in WIT. 
6.2 Consistency 
In an agglutinative language such as Japanese, 
there is no established definition of words, so dia- 
logue system developers must define words. This 
sometimes causes a problem in that the defini- 
tion of word, that is, the word boundaries, in the 
speech recognition module are different from that 
in the language understanding module. In WIT, 
however, since the common lexicon is used in 
both the speech recognition module and language 
understanding module, the consistency between 
them is maintained. 
6-3 Avoiding Information Loss 
In ordinary spoken language systems, the speech 
recognition module sends just a word hypoth- 
esis to the language processing module, which 
156 
speaker start end utterance 
time (s) time (s) 
system: 614.53 615.93 
user: 616.38 618.29 
system: 619.97 620.13 
user: 622.65 624.08 
system: 625.68 625.91 
user: 626.65 627.78 
system: 629.25 629.55 
user: 629.91 631.67 
system: 633.29 633.57 
user: 634.95 636.00 
system: 637.50 645.43 
user: 645.74 646.04 
system: 647.05 648.20 
Figure 2: 
donoy6na goy6ken desh6 ka (how may I help you?) 
kaigishitsu oyoyaku shitai ndesu ga (I'd like to make a reserva- 
tion for a meeting room) 
hai (uh-huh) 
san-gatsujfini-nichi (on March 12th) 
hal (uh-huh) 
jayo-ji kara (from 14:00) 
hai (uh-huh) 
jashichi-ji sanjup-pun made (to 17:30) 
hai (uh-huh) 
dai-kaigishitsu (the large meeting room) 
san-gatsu jani-nichi, j~yo-ji kara, jashichi-ji sanjup-pun made, 
dai-kaigishitsu toyfi koto de yoroshf deshrka (on March 12th, 
from 14:00 to 17:30, the large meeting room, is that right?) " 
hai (yes) 
kashikomarimashitd (allright) 
An example dialogue of an example system 
must disambiguate word meaning and find phrase 
boundaries by parsing. In contrast, the speech 
recognition module in WIT sends not only words 
but also word categories, phrase boundaries, and 
phrase categories. This leads to less expensive 
and better language understanding. 
6.4 Problems and Limitations 
Several problems remain with WIT. One of the 
most significant is that he system developer must 
write language generation functions. If the gen- 
eration functions employ sophisticated dialogue 
strategies, the system can perform complicated 
dialogues that are not just question answering. 
WIT, however, does not provide task-independent 
facilities that make it easier to employ such dia- 
logue strategies. 
There have been several efforts aimed at de- 
veloping a domain-independent me hod for gen- 
erating responses from a frame representation f 
user requests (Bobrow et al, 1977; Chu-CarroU, 
1999). Incorporating such techniques would deo 
crease the system developer workload. However, 
there has been no work on domain-independent 
response generation for robust spoken dialogue 
systems that can deal with utterances that might 
include pauses in the middle of a sentence, which 
WIT handles well. Therefore incorporating those 
techniques remains as a future work. 
Another limitation is that WIT cannot deal with 
multiple speech recognition candidates such as 
those in an N-best list. Extending WIT to deal 
with multiple recognition results would improve 
the performance of the whole system. The ISSS 
preference mechanism is expected to play a role 
in choosing the best recognition result. 
7 Conclusion 
This paper described WIT, a toolkit for build- 
ing spoken dialogue systems. Although it re- 
quires more system specifications than previous 
finite-state-model-based toolkits, it enables one 
to easily construct real-time, robust spoken dia- 
logue systems that incorporates advanced compu- 
tational linguistics technologies. 
Acknowledgements 
The authors thank Drs. Ken'ichiro Ishii, Nori- 
hiro Hagita, and Takeshi Kawabata for their sup- 
port of this research. Thanks also go to Tetsuya 
Kubota, Ryoko Kima, and the members of the 
Dialogue Understanding Research Group. We 
used the speech recognition engine VoiceRex de- 
veloped by NTT Cyber Space Laboratories and 
thank those who helped us use it. Comments by 
157 
the anonymous reviewers were of' great help. 
References 
James F. Allen, Bradford W. Miller, Eric K. Ringger, 
and Teresa Sikorski. 1996. A robust system for nat- 
ural spoken dialogue. In Proceedings of the 34th 
Annual Meeting of the Association for Computa- 
tional Linguistics (A CL-96), pages 62-70. 
Harald Aust, Martin Oerder, Frank Seide, and Volker 
Steinbiss. 1995. The Philips automatic train 
timetable information system. Speech Communi- 
cation, 17:249--262. 
James Barnett and Mona Singh. 1997. Designing 
a portable spoken language system. In Elisabeth 
Maier, Marion Mast, and Susann LuperFoy, editors, 
Dialogue Processing inSpoken Language Systems, 
pages 156--170. Springer-Vedag. 
Daniel G. Bobrow, Ronald M. Kaplan, Martin Kay, 
Dona!d A. Norman, Henry Thompson, and Terry 
Winograd. 1977. GUS, a frame driven dialog sys- 
tem. Arnficial Intelligence, 8:155-173. 
Jennifer Chu-Carroll. 1999. Fo:rrn-based reason- 
ing for mixed-initiative dialogue management in 
information-query systems. In Proceedings of the 
Sixth European Conference on Speech Communica- 
tion and Technology (Eurospeech-99) , pages 1519- 
1522. 
Junnifer Chu-Carroll. 2000. MIMIC: An adaptive 
mixed initiative spoken dialogue system for infor- 
mation queries. In Proceedings of the 6th Con- 
f~rence on Applied Natural Language Processing 
(ANLP-O0), pages 97-104. 
Kohji Dohsaka nd Akira Shimazu. 1997. System ar- 
chitecture for spoken utterance production in col- 
laborative dialogue. In Working Notes of IJCAI 
1997 Workshop on Collaboration, Cooperation and 
Conflict in Dialogue Systems. 
Kohji Dohsaka, Norihito Yasuda, Noboru Miyazaki, 
Mikio Nakano, and Kiyoaki AJkawa. 2000. An ef- 
ficient dialogue control method under system's lim- 
ited knowledge. In Proceedings of the Sixth Inter- 
national Conference on Spoken Language Process- 
ing (ICSLP-O0). 
Jun-ichi Hirasawa, Noboru Miyazaki, Mikio Nakano, 
and Takeshi Kawabata. 1998. Implementation 
of coordinative nodding behavior on spoken dia- 
logue systems. In Proceedings of the Fgth Interna- 
tional Conference on Spoken Language Processing 
(1CSLP-98), pages 2347-2350. 
Tetsunod Kobayashi, Shuichi Itahashi, Satoru 
Hayamizu, and Toshiyuki Takezawa. 1992. Asj 
continuous speech corpus for research. The journal 
of th e Acoustical Society of Japan, 48(12): 888-893. 
Mikio Nakano and Akira Shimazu. 1999. Pars- 
ing utterances including self-repairs. In Yorick 
Wilks, editor, Machine Conversations, pages 99- 
112. Kluwer Academic Publishers. 
Mikio Nakano, Aldra Shimazu, and Kiyoshi Kogure. 
1994. A grammar and a parser for spontaneous 
speech. In Proceedings of the 15th Interna- 
tional Conference on Computational Linguistics 
(COLING-94), pages 1014-1020. 
Mildo Nakano, Kohji Dohsaka, Noboru Miyazald, 
Inn ichi Hirasawa, Masafiami Tamoto, Masahito 
Kawarnon, Akira Sugiyama, and Takeshi Kawa- 
bata. 1999a. Handling rich turn-taking in spoken 
dialogue systems. In Proceedings of the Sixth Eu- 
ropean Conference on Speech Communication a d 
Technology (Eurospeech-99), pages 1167-1170. 
Mikio Nakano, Noboru Miyazaki, Jun-ichi Hirasawa, 
Kohji Dohsaka, and Takeshi Kawabata. 1999b. 
Understanding unsegmented user utterances in real- 
time spoken dialogue systems. In Proceedings of 
the 37th Annual Meeting of the Association for 
Computational Linguistics (ACL-99), pages 200-- 
207. 
Mikio Nakano. 1991. Constraint projection: An ef- 
ficient treatment of disjunctive f ature descriptions. 
In Proceedings of the 29th Annual Meeting of the 
Association for Computational Linguistics (ACL- 
90, pages 307-314. 
Yoshiaki Noda, Yoshikazu Yamaguchi, Tomokazu 
Yamada, Akihiro Imamura, Satoshi Takahashi, 
Tomoko Matsui, and Kiyoaki Aikawa. 1998. The 
development of speech recognition engine REX. In 
Proceedings of the 1998 1EICE General Confer- 
ence D-14-9, page 220. (in Japanese). 
Fernando C. N. Pereira and David H. D. Warren. 
1980. Definite clause grammars for language 
analysis--a survey of the formalism and a compar- 
ison with augmented transition etworks. Artificial 
Intelligence, 13:231-278. 
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven 
Phrase Structure Grammar. CSLI, Stanford. 
Munehiko Sasajima, Yakehide Yano, and Yasuyuki 
Kono. 1999. EUROPA: A genetic framework for 
developing spoken dialogue systems. In Proceed- 
ings of the Sixth European Conference on Speech 
Communication a d Technology (Eurospeech-99), 
pages 1163--1166. 
Stephanie Seneff, Ed Hurley, Raymond Lau, Chris- 
fine Pao, Philipp Sehmid, and Victor Zue. 1998. 
GALAXY-H: A reference architecture for conver- 
sational system development. In Proceedings of 
158 
the Fifth International Con l~rence on Spoken Lan- 
guage Processing (ICSLP-98). 
Stephen Sutton, Ronaid A. Cole, Jacques de Villiers, 
Johan SchMkwyk, Pieter Vermeulen, Michael W. 
Macon, Yonghong Yah, Edward Kaiser, Brian Run- 
die, K.haldoun Shobaki, Paul Hosom, Alex Kain, 
Johan Wouters, Dominic W. Massaro, and Michael 
Cohen. 1998. Universal speech tools: The 
CSLU toolkit. In Proceedings of the Fifth Interna- 
tional Conference on Spoken Language Processing 
(1CSLP-98), pages 3221-3224. 
Marilyn Walker, Irene Langkilde, Jerry Wright, Allen 
Gorin, and Diane Litman. 2000. Learning to pre- 
dict problematic situations in a spoken dialogue 
system: Experiments with how may I help you? In 
Proceedings of the First Meeting of the North Amer- 
ican Chapter of the Association for Computational 
Linguistics (NAA CL-O0), pages 210--217. 
Victor Zue, Stephanie Seneff, James Glass, Joseph Po- 
lifroni, Christine Pao, Timothy J. Hazen, and Lee 
He~erington. 2000. Jupiter: A telephone-based 
conversational interface for weather information. 
1EEE Transactions on Speech and Audio Process- 
ing, 8(1):85-96. 
159 
Coling 2010: Poster Volume, pages 579?587,
Beijing, August 2010
Automatic Allocation of Training Data for Rapid Prototyping
of Speech Understanding based on Multiple Model Combination
Kazunori Komatani? Masaki Katsumaru? Mikio Nakano?
Kotaro Funakoshi? Tetsuya Ogata? Hiroshi G. Okuno?
? Graduate School of Informatics, Kyoto University
{komatani,katumaru,ogata,okuno}@kuis.kyoto-u.ac.jp
? Honda Research Institute Japan Co., Ltd.
{nakano,funakoshi}@jp.honda-ri.com
Abstract
The optimal choice of speech understand-
ing method depends on the amount of
training data available in rapid prototyp-
ing. A statistical method is ultimately
chosen, but it is not clear at which point
in the increase in training data a statisti-
cal method become effective. Our frame-
work combines multiple automatic speech
recognition (ASR) and language under-
standing (LU) modules to provide a set
of speech understanding results and se-
lects the best result among them. The
issue is how to allocate training data to
statistical modules and the selection mod-
ule in order to avoid overfitting in training
and obtain better performance. This paper
presents an automatic training data alloca-
tion method that is based on the change
in the coefficients of the logistic regres-
sion functions used in the selection mod-
ule. Experimental evaluation showed that
our allocation method outperformed base-
line methods that use a single ASR mod-
ule and a single LU module at every point
while training data increase.
1 Introduction
Speech understanding in spoken dialogue systems
is the process of extracting a semantic represen-
tation from a user?s speech. That is, it consists
of automatic speech recognition (ASR) and lan-
guage understanding (LU). Because vocabularies
and language expressions depend on individual
systems, it needs to be constructed for each sys-
tem, and accordingly, training data are required
for each. To collect more real training data, which
will lead to higher performance, it is more desir-
able to use a prototype system than that based on
the Wizard-of-Oz (WoZ) method where real ASR
errors cannot be observed, and to use a more ac-
curate speech understanding module. That is, in
the bootstrapping phase, spoken dialogue systems
need to operate before sufficient real data have
been collected.
We have been addressing the issue of rapid pro-
totyping on the basis of the ?Multiple Language
model for ASR and Multiple language Under-
standing (MLMU)? framework (Katsumaru et al,
2009). In MLMU, the most reliable speech un-
derstanding result is selected from candidates pro-
duced by various combinations of multiple ASR
and LU modules using hand-crafted grammar and
statistical models. A grammar-based method is
still effective at an early stage of system devel-
opment because it does not require training data;
Schapire et al (2005) also incorporated human-
crafted prior knowledge into their boosting al-
gorithm. By combining multiple understanding
modules, complementary results can be obtained
by different kinds of ASR and LU modules.
We propose a novel method to allocate avail-
able training data to statistical modules when the
amount of training data increases. The training
data need to be allocated adaptively because there
are several modules to be trained, and they would
cause overfitting without data allocation. There
are speech understanding modules that have lan-
guage models (LMs) for ASR and LU models
579
(LUMs), and a selection module that selects the
most reliable speech understanding result from
multiple candidates in the MLMU framework.
When the amount of available training data is
small, and an LUM and the selection module are
trained on the same data set, they are trained un-
der a closed-set condition, and thus the training
data for the selection module include too many
correct understanding results. In such cases, the
data need to be divided into subdata sets to avoid
overfitting. On the other hand, when the amount
of available training data is large, so that overfit-
ting does not occur, all available data should be
used to train each statistical module to prepare as
much training data as possible.
We therefore develop a method for switching
data allocation policies. More specifically, two
points are automatically determined at which sta-
tistical modules with more parameters start to be
trained. As a result, better overall performance
is achieved at every point while the amount of
training data increases, compared with all combi-
nations of a single ASR module and a single LU
module.
2 Related Work
It is important to consider the amount of available
training data when designing a speech understand-
ing module. Many statistical LU methods have
been studied, e.g., (Wang and Acero, 2006; Jeong
and Lee, 2006; Raymond and Riccardi, 2007;
Hahn et al, 2008; Dinarelli et al, 2009). They
generally outperform grammar-based LU meth-
ods when a sufficient amount of training data is
available; but sufficient training data are not nec-
essarily available during rapid prototyping. Sev-
eral LU methods were constructed using a small
amount of training data (Fukubayashi et al, 2008;
Dinarelli et al, 2009). Fukubayashi et al (2008)
constructed an LU method based on the weighted
finite state transducer (WFST), in which filler
transitions accepting arbitrary inputs and transi-
tion weights were added to a hand-crafted FST.
This method is placed between a grammar-based
method and a statistical method because a sta-
tistically selected weighting scheme is applied
to a hand-crafted grammar model. Therefore,
the amount of training data can be smaller com-
pared with general statistical LU methods, but this
method does not outperform them when plenty of
training data are available. Dinarelli et al (2009)
used a generative model for which overfitting is
less prone to occur than discriminative models
when the amount of training data is small, but
they did not use a grammar-based model, which is
expected to achieve reasonable performance even
when the amount of training data is very small.
Raymond et al (2007) compared the perfor-
mances of statistical LU methods for various
amounts of training data. They used a statis-
tical finite-state transducer (SFST) as a genera-
tive model and a support vector machine (SVM)
and conditional random fields (CRF) as discrim-
inative models. The generative model was more
effective when the amount of data was small,
and the discriminative models were more effec-
tive when it was large. This shows that the perfor-
mance of an LUmethod depends on the amount of
training data available, and therefore, LU meth-
ods need to be switched automatically. Wang et
al. (2002) developed a two-stage speech under-
standing method by applying statistical methods
first and then grammatical rules. They also ex-
amined the performance of the statistical methods
at their first stage for various amounts of train-
ing data and confirmed that the performance is not
very high when a small amount of data is used.
Schapire et al (2005) showed that accuracy
of call classification in spoken dialogue systems
improved by incorporating hand-crafted prior
knowledge into their boosting algorithm. Their
idea is the same as ours in that they improve the
system?s performance by using hand-crafted hu-
man knowledge while only a small amount of
training data is available. We furthermore solve
the data allocation problem because there are mul-
tiple statistical models to be trained in speech
understanding, while their call classification has
only one statistical model.
3 MLMU Framework
MLMU is the framework for selecting the most
reliable speech understanding result from multi-
ple speech understanding modules (Katsumaru et
al., 2009). In this paper, we furthermore adapt the
selection module to the amount of available train-
580
LU model
#1
Language
model #1
LU
modules
ASR
modules
Result:
1
CM
N
CM
MN
CM
?
i
i
CMmaxarg
N
1
Selection module
LU
results
ASR
results
Utterance
ASR: automatic speech recognition
LU: language understanding
CM: confidence measure
M ?
M ?
Speech understanding
Language
model #2
Language
model #N
LU model
#2
LU model
#M
Logistic
regression #1
Logistic
regression #N
Logistic
regression #
Figure 1: Overview of speech understanding framework MLMU
ing data. More specifically, the allocation policy
of training data is changed and thus appropriate
LMs and LUMs are selected as its result.
An overview of MLMU is shown in Figure 1.
MLMU uses multiple LMs for ASR and multi-
ple LUMs and selects the most reliable speech un-
derstanding result from all combinations of them.
We denote a speech understanding module as SUi
(i = 1, . . . , n). Its result is a semantic representa-
tion consisting of a set of concepts. The concept is
either a semantic slot and its value or an utterance
type. Note that n = N ? M , when N LMs and
M LUMs are used. The confidence measure per
utterance for a result of i-th speech understanding
module SUi is denoted as CMi. The speech un-
derstanding result having the highest confidence
measure is selected as the final result for the ut-
terance. That is, the result is the output of SUm
where m = argmaxi CMi.
The confidence measure is calculated by logis-
tic regression based on the features of each speech
understanding result. A logistic regression func-
tion is constructed for each speech understanding
module SUi:
CMi =
1
1 + e?(ai1Fi1+...+ai7Fi7+bi) . (1)
Parameters ai1, . . . , ai7 and bi are determined by
using training data. In the training phase, teacher
signal 1 is given when a speech understanding re-
sult is completely correct; that is, when no error is
contained in the result. Otherwise, 0 is given. We
use seven features, Fi1, Fi2, . . . , Fi7, as indepen-
dent variables. Each feature value is normalized
Table 1: Features of speech understanding result
obtained from SUi
Fi1: Acoustic score normalized by utterance length
Fi2: Difference between Fi1 and normalized acoustic
scores of verification ASR
Fi3: Average concept CM in understanding result
Fi4: Minimum concept CM in understanding result
Fi5: Number of concepts in understanding result
Fi6: Whether any understanding result is obtained
Fi7: Whether understanding result is yes/no
CM: confidence measure
so as to make its mean zero and its variance one.
The features used are listed in Table 1. Com-
pared with those used in our previous paper (Kat-
sumaru et al, 2009), we deleted ones that were
highly correlated with other features and added
ones regarding content of the speech understand-
ing results. Features Fi1 and Fi2 are obtained
from an ASR result. Another ASR with a gen-
eral large vocabulary LM is executed for verifying
the i-th ASR result. Fi2 is the difference between
its score and Fi1 (Komatani et al, 2007). These
two features represent the reliability of the ASR
result. Fi3 and Fi4 are calculated for each concept
in the LU result on the basis of the posterior prob-
ability of the 10-best ASR candidates (Komatani
and Kawahara, 2000). Fi5 is the number of con-
cepts in the LU result. This feature is effective be-
cause the LU results of lengthy utterances tend to
be erroneous in a grammar-based LU. Fi6 repre-
sents the case when an ASR result is not accepted
by the subsequent LU module. In such cases, no
speech understanding result is obtained, which is
581
U1: It is June ninth.
ASR result:
- grammar ?It is June ninth.?
- N-gram ?It is June noon and?
LU result:
- grammar + FST ?month:6 day:9 type:refer-time?
- N-gram + WFST ?month:6 type:refer-time?
U2: I will borrow it on twentieth.
(Underlined part is out-of-grammar.)
ASR result:
- grammar ?Around two pm on twentieth.?
- N-gram ?Around two at ten on twentieth.?
LU result:
- grammar + FST ?day:20 hour:14 type:refer-time?
- N-gram + WFST ?day:20 type:refer-time?
Combination of LM and LUM is denoted as ?LM+LUM?.
Figure 2: Example of speech understanding re-
sults in MLMU framework
regarded as an error. Fi7 is added because affirma-
tive and negative responses, typically ?Yes? and
?No?, tend to be correctly recognized and under-
stood.
Figure 2 depicts an example when multiple
ASRs based on LMs and multiple LUs are used.
In short, the correct speech understanding result is
obtained from a different combination of LMs and
LUMs.
4 Automatic Allocation of Training Data
Using Change in Coefficients
The training data need to be allocated to the
speech understanding modules (i.e., statistical LM
and statistical LUM) and the selection module. If
more data are allocated to the ASR and LU mod-
ules, the performances of these modules are im-
proved, but the overall performance is degraded
because of the low performance of the selection
module. On the other hand, even if more training
data are allocated to the selection module, the per-
formance of each ASR and LU module remains
low.
4.1 Allocation Policy
We focus on the convergence of the logistic re-
gression functions when the amount of training
data increases. The convergence is defined as
the change in their coefficients, which will appear
later as Equation 2, and determines two points
1. All data are used to
train selection modules
2. Data are allocated to SU
and selection modules
3. Data are
not divided
No No
Yes
Yes
Selection module 
first converges?
No over-fitting
occurs?
Amount of training data increases
SU: speech understanding
Figure 3: Flowchart of data allocation
during the increase in training data, and thus three
phases are defined. The flowchart of data alloca-
tion is depicted in Figure 3. The three phases are
explained below.
In the first phase, the first priority is given to
the selection module. This is because the lo-
gistic regression functions used in the selection
module converge with relatively less training data
than those in the statistical ASR and LU mod-
ules for speech understanding; there are eight pa-
rameters for each logistic regression function as
shown in Equation 1, far fewer than for other sta-
tistical models such as N-gram and CRF. The out-
put from a speech understanding module that em-
ploys grammar-based LM and LUM would be the
most reliable in many cases because its perfor-
mance is better than that of other statistical mod-
ules when a very small amount of training data is
available. As a result, equivalent or better perfor-
mance would be achieved than methods using a
single ASR module and a single LU module.
In the second phase, the training data are also
allocated to the speech understanding modules af-
ter the selection module converges. This aims
to improve the performance of the speech under-
standing modules by allocating as much training
data to them as possible. The amount of train-
ing data is fixed in this phase to the amount al-
located to the selection module determined in the
first phase. The remaining data are used to train
the speech understanding modules.
When the performances of all the speech under-
standing modules stabilize, the allocation phase
proceeds to the third one. After this point, we
hypothesize that overfitting does not occur in this
phase because plenty of training data are avail-
able. All available data are used to train all mod-
582
ules without dividing the data in this phase.
4.2 Determining When to Switch Allocation
Policies
Automatic switching from one phase to the next
requires the determination of two points in the
number of training utterances: when the selec-
tion module first converges (konlysel) and when
the speech understanding modules all become sta-
ble (knodiv). These points are determined by fo-
cusing on the changes in the coefficients of the
logistic regression functions when the number of
utterances used as training data increases. We ob-
serve the sum of the changes in the coefficients of
the functions and then identify the points at which
the changes converge. The points are determined
individually by the following algorithm.
Step 1 Construct two logistic regression func-
tions for speech understanding module SUi
by using k and (k + ?k) utterances out of
kmax utterances, where kmax is the amount
of training data available.
Step 2 Calculate the change in coefficients from
the two logistic regression functions by
?i(k) =
?
j
|aij(k + ?k) ? aij(k)|
+|bi(k + ?k) ? bi(k)|, (2)
where aij(k) and bi(k) denote the param-
eters of the logistic regression functions,
shown in Equation 1, for speech understand-
ing module SUi, when k utterances are used
to train the functions.
Step 3 If ?i(k) becomes smaller than threshold
?, consider that the training of the functions
has converged, and record this k as the point
of convergence. If not, return to Step 1 after
k ? k + ?k.
The ?k is the minimum unit of training data con-
taining various utterances. We set it as the number
of utterances in one dialogue session, whose aver-
age was 17. Threshold ? was set to 8, which corre-
sponds to the number of parameters in the logistic
regression functions. No experiments were con-
ducted to determine if better performance could
be achieved with other choices of ?1.
The first point, konlysel, is determined using the
speech understanding module that uses no training
data. Specifically, we used ?grammar+FST? as
method SUi. Here, ?LM+LUM? denotes a com-
bination of LM for ASR and LUM. If the func-
tion converges at k utterances, we set konlysel to
k and fix the k utterances as training data used by
the selection module. The remaining (kmax ? k)
utterances are allocated to the speech understand-
ing modules, that is, the LMs and LUMs. Note
that if k becomes equal to kmax before ?i con-
verges, all training data are allocated to the selec-
tion module; that is, no data are allocated to the
LMs and LUMs. In this case, no output is ob-
tained from statistical speech understanding mod-
ules, and only outputs from the grammar-based
modules are used.
The second point, knodiv , is determined on the
basis of the speech understanding module that
needs the largest amount of data for training. The
amount of data needed depends on the number of
parameters. Specifically, we used ?N-gram+CRF?
as SUi in Equation 2. If the function converges,
we hypothesize that the performance of all the
speech understanding modules stabilize and thus
overfitting does not occur. We then stop the divi-
sion of training data, and use all available data to
train the statistical modules.
5 Experimental Evaluation
5.1 Target Data and Implementation
We used a data set previously collected through
actual dialogues with a rent-a-car reservation sys-
tem (Nakano et al, 2007) with 39 participants.
Each participant performed 8 dialogue sessions,
and 5900 utterances were collected in total. Out
of these utterances, we used 5240 for which the
automatic voice activity detection (VAD) results
agreed with manual annotation. We divided the
utterances into two sets: 2121 with 16 participants
as training data and 3119 with 23 participants as
the test data.
1We do not think the value is very critical after seeing the
results shown in Figure 4.
583
We constructed another rent-a-car reservation
system to evaluate our allocation method. The
system included two language models (LMs)
and four language understanding models (LUMs).
That is, eight speech understanding results in total
were obtained. The two LMs were a grammar-
based LM (?grammar?, hereafter) and a domain-
specific statistical LM (?N-gram?). The grammar
model was described by hand to be equivalent to
the FST model used in LU. The N-gram model
was a class 3-gram and was trained on a tran-
scription of the available training data. The vo-
cabulary size was 281 for the grammar model and
420 for the N-gram model when all the training
data were used. The ASR accuracies of the gram-
mar and N-gram models were 67.8% and 90.5%
for the training data and 66.3% and 85.0% for the
test data when all the training data were used. We
used Julius (ver. 4.1.2) as the speech recognizer
and a gender-independent phonetic-tied mixture
model as the acoustic model (Kawahara et al,
2004). We also used a domain-independent statis-
tical LM with a vocabulary size of 60250, which
was trained on Web documents (Kawahara et al,
2004), as the verification model.
The four LUMs were a finite-state transducer
(FST) model, a weighted FST (WFST) model,
a keyphrase-extractor (Extractor) model, and a
conditional random fields (CRF) model. In the
FST-based LUM, the FST was constructed by
hand. The WFST-based LUM is based on the
method developed by Fukubayashi et al (2008).
The WFSTs were constructed by using the MIT
FST Toolkit (Hetherington, 2004). The weight-
ing scheme used for the test data was selected by
using training data (Fukubayashi et al, 2008). In
the extractor-based LUM, as many parts as pos-
sible in the ASR result were simply transformed
into concepts. As the CRF-based LUM, we used
open-source software, CRF++2, to construct the
LUM. As its features, we use a word in the ASR
result, its first character, its last character, and the
ASR confidence of the word. Its parameters were
estimated by using training data.
The metric used for speech understanding per-
formance was concept understanding accuracy,
2http://crfpp.sourceforge.net/
Table 2: Absolute degradation in oracle accuracy
when each module was removed
Case (A) (B)
With all modules (%) 86.6 90.1
w/o grammar ASR -12.0 -1.1
w/o N-gram ASR -6.1 -7.7
w/o FST LUM -0.4 0.0
w/o WFST LUM -1.2 -0.5
w/o Extractor LUM -0.1 0.0
w/o CRF LUM -0.6 -3.7
(w/o FST & Extractor LUMs) -1.0 -0.1
(A): 141 utterances with 1 participant
(B): 2121 utterances with 16 participants
defined as
1 ? SUB + INS + DEL
no. of concepts in correct results,
where SUB, INS, and DEL denote the numbers of
substitution, insertion, and deletion errors.
5.2 Effectiveness of Using Multiple LMs and
LUMs
We investigated how much the performance of our
framework degraded when one ASR or LU mod-
ule was removed. We used the oracle accuracies,
i.e., when the most appropriate result was selected
by hand. The result reveals the contribution of
each ASR and LU module to the performance of
the framework. A module is regarded as more im-
portant when the accuracy is degraded more when
it is removed than when another one is removed.
Two cases (A) and (B) were defined: when the
amount of available training data was (A) small
and (B) large. We used 141 utterances with 1 par-
ticipant for case (A) and 2121 utterances with 16
participants for case (B). The results are shown in
Table 2.
When a small amount of training data was
available (case (A)), the accuracy was degraded by
12.0 points when the grammar-based ASRmodule
was removed and 6.1 points when the N-gram-
based ASR module was removed. The accuracy
was thus degraded substantially when either ASR
module was removed. This indicates that the two
ASR modules work complementarily.
584
020
40
60
80
100
120
140
160
180
200
0 100 200 300 400 500
C
h
a
n
g
e
s
 
i
n
 
c
o
e
f
f
i
c
i
e
n
t
s
Number of training utterances available
(a) grammar+FST
0
20
40
60
80
100
120
140
160
180
200
0 100 200 300 400 500
C
h
a
n
g
e
s
 
i
n
 
c
o
e
f
f
i
c
i
e
n
t
s
Number of training utterances available
(b) N-gram+CRF
Figure 4: Change in the sum of coefficients ?i when amount of training data increases (?LM+LUM?
denotes combination of LM and LUM)
On the other hand, when a large amount of
training data was available (case (B)), the ac-
curacy was degraded by 1.1 points when the
grammar-based ASR was removed. This means
that it became less important when there are
plenty of training data because the coverage of the
N-gram-based ASR became wider. In short, espe-
cially when the amount of training data is smaller,
speech understanding modules based on a hand-
crafted grammar are more important because of
the low performance of statistical modules.
Concerning the LUMs, the accuracy was de-
graded when any of the LUM modules was re-
moved when a small amount of training data was
available. When a large amount of training data
was available, the module based on CRF in par-
ticular became more important.
5.3 Results and Evaluation of Automatic
Allocation
Figure 4 shows the change in the sum of the co-
efficients, ?i, with the increase in the amount of
training data. In Figure 4(a), the change was very
large while the amount of training data was small,
and decreased dramatically and converged around
one hundred utterances. By applying ? (=8) to ?i,
we set 111 utterances as the first point, konlysel,
up to which all the training data are allocated to
the selection module, as described in Section 4.1.
Similarly, from the results shown in Figure 4(b),
we set 207 utterances as the second point, knodiv,
from which the training data are not divided.
To evaluate our method for allocating training
55
60
65
70
75
80
85
90
50 100 200 400 800 1600
C
o
n
c
e
p
t
 
u
n
d
e
r
s
t
a
n
d
i
n
g
 
a
c
c
u
r
a
c
y
 
[
%
]
Number of training utterances available
Our method
Na?ve allocation
No division
Figure 5: Results of allocation methods
data, we compared it with two baseline methods:
? No-division method: All data available at
each point were used to train both the speech
understanding modules and the selection
module. That is, the same data set was used
to train them.
? Naive-allocation method: Training data
available at each point were allocated equally
to the speech understanding modules and the
selection module.
As shown in Figure 5, our method had the best
concept understanding accuracy when the amount
of training data was small, that is, up to about
278 utterances. This indicates that our method for
allocating the available training data is effective
when the amount of training data is small.
This result is explained more specifically by us-
585
Table 3: Concept understanding accuracy for 141
utterances
Accuracy (%)
Our method 77.9
Naive allocation 73.5
No division 74.1
ing the case in which 141 utterances were used as
the training data. 111 (= konlysel) were secured to
train the selection module and 30 utterances were
allocated to train the speech understanding mod-
ules. As shown in Table 3, the accuracy with our
method was 3.8 points higher than that with the
no-division baseline method. This was achieved
by avoiding the overfitting of the logistic regres-
sion functions; i.e., the data input to the functions
became similar to the test data due to allocation,
so the concept understanding accuracy for the test
set was improved. The accuracy with our method
was 4.4 points higher than that with the naive al-
location baseline method. This was because the
amount of training data allocated to the selection
module was less than our method, and accordingly
the selection module was not trained sufficiently.
5.4 Comparison with methods using a single
ASR and a single LU
Figure 6 plots concept understanding accuracy
with our method against baseline methods using
a single ASR module and a single LU module for
various amounts of training data. Each module for
comparison was constructed by using all available
training data at each point while training data in-
creased; i.e., the same condition as our method.
The accuracies of only three speech understand-
ing modules are shown in the figure, out of the
eight obtained by combining two LMs for ASR
and four LUMs. These three are the ones with the
highest accuracies while the amount of training
data increased. Our method switched the alloca-
tion phase at 111 and 207 utterances, as described
in Section 5.3.
Our method performed equivalently or better
than all baseline methods even when only a small
amount of training data was available. As a result,
our method outperformed all the baseline methods
55
60
65
70
75
80
85
50 100 200 400 800 1600
C
o
n
c
e
p
t
 
u
n
d
e
r
s
t
a
n
d
i
n
g
 
a
c
c
u
r
a
c
y
 
[
%
]
Number of training utterances available
our method
grammar+FST
N-gram+WFST
N-gram+CRF
Figure 6: Comparison with baseline methods us-
ing single speech understanding
at every point while training data increase.
6 Conclusion
We developed a method to automatically allo-
cate training data to statistical modules so as to
avoid performance degradation caused by overfit-
ting. Experimental evaluation showed that speech
understanding accuracies achieved by our method
were equivalent or better than the baseline meth-
ods based on all combinations of a single ASR
module and a single LU module at every point
while training data increase. This includes a case
when a very small amount of training data is avail-
able. We also showed empirically that the training
data should be allocated while an amount of train-
ing data is not sufficient. Our method allocated
available training data on the basis of our alloca-
tion policy described in Section 4.1, and outper-
formed the two baselines where the training data
were equivalently allocated and not allocated.
When plenty of training data were available,
there was no difference between our method and
the speech understanding method that requires the
most training data, i.e., N-gram+CRF, as shown in
Figure 6. It is possible that our method combin-
ing multiple speech understanding modules would
outperform it as Schapire et al (2005) reported.
In their data, there were some examples that only
a hand-crafted rules can parse. Including such a
task as more complicated language understanding
grammar is required, verification of our method in
other tasks is one of the future works.
586
References
Dinarelli, Marco, Alessandro Moschitti, and Giuseppe
Riccardi. 2009. Re-Ranking Models for Spoken
Language Understanding. In Proc. European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 202?210.
Fukubayashi, Yuichiro, Kazunori Komatani, Mikio
Nakano, Kotaro Funakoshi, Hiroshi Tsujino, Tet-
suya Ogata, and Hiroshi G. Okuno. 2008. Rapid
prototyping of robust language understanding mod-
ules for spoken dialogue systems. In Proc. Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP), pages 210?216.
Hahn, Stefan, Patrick Lehnen, and Hermann Ney.
2008. System Combination for Spoken Language
Understanding. In Proc. Annual Conference of the
International Speech Communication Association
(INTERSPEECH), pages 236?239.
Hetherington, Lee. 2004. The MIT Finite-State Trans-
ducer Toolkit for Speech and Language Processing.
In Proc. Int?l Conf. Spoken Language Processing
(ICSLP), pages 2609?2612.
Jeong, Minwoo and Gary Geunbae Lee. 2006. Ex-
ploiting non-local features for spoken language un-
derstanding. In Proc. COLING/ACL 2006 Main
Conference Poster Sessions, pages 412?419.
Katsumaru, Masaki, Mikio Nakano, Kazunori Ko-
matani, Kotaro Funakoshi, Tetsuya Ogata, and Hi-
roshi G. Okuno. 2009. Improving speech un-
derstanding accuracy with limited training data us-
ing multiple language models and multiple under-
standing models. In Proc. Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pages 2735?2738.
Kawahara, Tatsuya, Akinobu Lee, Kazuya Takeda,
Katsunobu Itou, and Kiyohiro Shikano. 2004. Re-
cent progress of open-source LVCSR engine Julius
and Japanese model repository. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), pages 3069?
3072.
Komatani, Kazunori and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management us-
ing concept-level confidence measures of speech
recognizer output. In Proc. Int?l Conf. Computa-
tional Linguistics (COLING), pages 467?473.
Komatani, Kazunori, Yuichiro Fukubayashi, Tetsuya
Ogata, and Hiroshi G. Okuno. 2007. Introducing
utterance verification in spoken dialogue system to
improve dynamic help generation for novice users.
In Proc. 8th SIGdial Workshop on Discourse and
Dialogue, pages 202?205.
Nakano, Mikio, Yuka Nagano, Kotaro Funakoshi,
Toshihiko Ito, Kenji Araki, Yuji Hasegawa, and Hi-
roshi Tsujino. 2007. Analysis of user reactions to
turn-taking failures in spoken dialogue systems. In
Proc. 8th SIGdial Workshop on Discourse and Dia-
logue, pages 120?123.
Raymond, Christian and Giuseppe Riccardi. 2007.
Generative and Discriminative Algorithms for Spo-
ken Language Understanding. In Proc. Annual
Conference of the International Speech Communi-
cation Association (INTERSPEECH), pages 1605?
1608.
Shapire, Robert E., Marie Rochery, Mazin Rahim, and
Narendra Gupta. 2005. Boosting with prior knowl-
edge for call classification. IEEE Trans. on Speech
and Audio Processing, 13(2):174?181.
Wang, Ye-Yi and Alex Acero. 2006. Discrimina-
tive models for spoken language understanding. In
Proc. Int?l Conf. Spoken Language Processing (IN-
TERSPEECH), pages 2426?2429.
Wang, Ye-Yi, Alex Acero, Ciprian Chelba, Brendan
Frey, and Leon Wong. 2002. Combination of Sta-
tistical and Rule-based Approaches for Spoken Lan-
guage Understanding. In Proc. Int?l Conf. Spoken
Language Processing (ICSLP), pages 609?612.
587
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 165?174,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
The Dynamics of Action Corrections in Situated Interaction
Antoine Raux
Honda Research Institute USA
Mountain View, CA, USA.
araux@honda-ri.com
Mikio Nakano
Honda Research Institute Japan
Wako, Japan
nakano@jp.honda-ri.com
Abstract
In spoken communications, correction ut-
terances, which are utterances correct-
ing other participants utterances and be-
haviors, play crucial roles, and detecting
them is one of the key issues. Previ-
ously, much work has been done on au-
tomatic detection of correction utterances
in human-human and human-computer di-
alogs, but they mostly dealt with the cor-
rection of erroneous utterances. How-
ever, in many real situations, especially in
communications between humans and mo-
bile robots, the misunderstandings man-
ifest themselves not only through utter-
ances but also through physical actions
performed by the participants. In this pa-
per, we focus on action corrections and
propose a classification of such utterances
into Omission, Commission, and Degree
corrections. We present the results of our
analysis of correction utterances in dialogs
between two humans who were engaging
in a kind of on-line computer game, where
one participant plays the role of the re-
mote manager of a convenience store, and
the other plays the role of a robot store
clerk. We analyze the linguistic content,
prosody as well as the timing of correction
utterances and found that all features were
significantly correlated with action correc-
tions.
1 Introduction
Recent progress in robot technology made it a re-
ality to have robots work in offices and homes, and
spoken dialog is considered to be one of the most
desired interface for such robots. Our goal is to
build a spoken dialog interface for robots that can
move around in an office or a house and execute
tasks according to humans? requests.
Building such spoken dialog interface for robots
raises new problems different from those of tra-
ditional spoken/multimodal dialog systems. The
intentions behind human utterances may vary de-
pending on the situation where the robot is and the
situation changes continuously not only because
the robot moves but also because humans and ob-
jects move, and human requests change. In this
sense human-robot interaction is situated.
Of the many aspects of situated interaction, we
focus on the timing structure of interaction. Al-
though traditional spoken dialog systems deal with
some timing issues such as turn-taking and han-
dling barge-ins, timing structure in human-robot
interaction is far more complex because the robot
can execute physical actions and those actions can
occur in parallel with utterances.
In this work we are concerned specifically with
corrections in situated interaction. In joint physi-
cal tasks, human corrective behavior, which allows
to repair discrepancies in participants? mutual un-
derstanding, is tightly tied to actions.
While past work on non-situated spoken dialog
systems has shown the necessity and feasibility of
detecting and handling corrections (Kitaoka et al,
2003; Litman et al, 2006; Gieselman and Osten-
dorf, 2007; Cevik et al, 2008), most of these mod-
els assume that corrections target past utterances
and rely on a strict turn-based structure which is
frequently violated in situated interaction. When
dialog is interleaved with physical actions, the spe-
cific timing of an utterance relative to other utter-
ances and actions is more relevant than the turn
sequence.
In this paper, we propose a classification of er-
rors and corrections in physical tasks and analyze
the properties of different types of corrections in
the context of human-human task-oriented inter-
actions in a virtual environment. The next section
gives some characteristics of corrections in situ-
ated interaction. Section 3 describes our experi-
165
Alice : Put it right above (1)
the lamp stand
Bob : Here? (2)
Alice : A little bit more (3)
to the right.
(Bob Moves the frame left) (4)
Alice : No the right! (5)
(Bob Moves the frame right) (6)
Alice : More... (7)
Alright, that?s ... (8)
(A bee flies next to Bob) (9)
Alice : Watch out! That bee is
going to sting you! (10)
Figure 1: Example dialog from a situated task.
mental set up and data collection effort. Section 4
presents the results of our analysis of corrections
in terms of timing, prosodic, and lexical features.
These results are discussed in Section 5.
2 Corrections in Situated Tasks
2.1 Situated Tasks
We define a situated task as one for which two or
more agents interact in order to perform physical
actions in the (real or virtual) world. Physical ac-
tions involve moving from one place to another,
as well as manipulating objects. In many cases,
interaction happens simultaneously with physical
actions and can be affected by them, or by other
external events happening in the world. For exam-
ple, Figure 1 shows an extract of a (constructed)
dialog where one person (Alice) assists another
(Bob) while he hangs a picture frame on a wall.
This interaction presents some similarities and
differences with unimodal, non-situated dialogs.
In addition to standard back-and-forth turn-taking
as in turns 1-3, this example features utterances
by Alice which are not motivated by Bob?s ut-
terances, but rather by (her perception of) his ac-
tions (e.g. utterance 5 is a reaction to action 4),
as well as external events such as 9, which trig-
gered response 10 from Alice. Therefore the con-
tent of Alice?s utterances is dependent not only on
Bob?s, but also on events happening in the world.
Similarly, the timing of Alice?s utterances is not
only conditioned on Bob?s speech, prosody, etc,
but also on asynchronous world events.
Robots and other agents that interact with peo-
ple in real world situations need to be able to ac-
count for the impact of physical actions and world
events on dialog. In the next section and the rest
of this paper, we focus on correction utterances
and how situational context affects how and when
speakers produce them.
2.2 Corrections
Generally speaking, a correction is an utterance
which aims at signaling or remedying a misunder-
standing between the participants of a conversa-
tion. In other word, corrections help (re)establish
common ground (Clark, 1996).
2.2.1 Previous Work
There are many dimensions along which correc-
tions can be analyzed and many researchers have
addressed this issue. Conversational Analysis
(CA) has, from its early days, 1 concerned it-
self with corrections (usually called repairs in CA
work) (Schegloff et al, 1977).
More recently, spoken dialog systems re-
searchers have investigated ways to automatically
recognize corrections. For instance, Litman et al
(2006) exploited features to automatically detect
correction utterances. In addition, several attempts
have been made to exploit the similarity in speech
sounds and speech recognition results of a correc-
tion and the previous user utterance to detect cor-
rections (Kitaoka et al, 2003; Cevik et al, 2008).
Going beyond a binary correction/non-
correction classification scheme, Levow (1998)
distinguished corrections of misrecognition errors
from corrections of rejection errors and found
them to have different prosodic features. Rog-
driguez and Schlangen (2004) and Rieser and
Moore (2005) classify corrections according to
their form (e.g. Repetition, Paraphrase, Addition
of information...) and function. The latter aspect
is mostly characterized in terms of the source of
the problem that is being corrected using models
of communication such as that of Clark (1996).
In all of this very rich literature, corrections are
assumed to target utterances from another partic-
ipant (or even oneself, in the case of self-repair)
that conflict with the hearer?s expectations. While
some work on embodied conversational agents
(Cassell et al, 2001; Traum and Rickel, 2002)
does consider physical actions as possible cues
to errors and corrections, the actions are typically
communicative in nature (e.g. nods, glances, ges-
tures). Comparatively, there is extremely little
work on corrections that target task actions.
166
A couple of exceptions are Traum et al (1999),
who discuss the type of context representation
needed to handle action corrections, and Fu-
nakoshi and Tokunaga (2006), who present a
model for identifying repair targets in human-
robot command-and-control dialogs. While im-
portant, these papers focus on theoretical planning
aspects of corrections whereas this paper focuses
on an empirical analysis of human conversational
behavior.
2.2.2 Action Corrections in Situated
Interaction
As seen above, the vast majority of prior work on
corrections concerned corrections of previous (er-
roneous) utterances (i.e. utterance corrections). In
contrast, in this paper, we focus exclusively on
corrections that target previous physical actions
(i.e. action corrections).
While some classification schemes of utterance
corrections are applicable to task corrections (e.g.
those based on the form of the correction itself),
we focus on differences that are specific to action
corrections.
Namely, we distinguish three types of action er-
rors and their related action corrections:
Commission errors occur when Bob performs an
action that conflicts with Alice?s expectation.
Action 4 of Figure 1 is a commission error,
which is corrected in turn 5.
Omission errors occur when Bob fails to react to
one of Alice?s utterances. A typical way for
Alice to correct an omission error is to repeat
the utterance to which Bob did not react.
Degree errors occur when Bob reacts with an ap-
propriate action to Alice?s utterance but fails
to completely fulfill Alice?s goal. This is il-
lustrated by Alice?s use of ?More? in turn 7
in response to Bob?s insufficient action 6.
Figure 3 illustrates the three error categories
based on extracts from the corpus.
In some ways, the dichotomy Commission er-
rors/Omission errors parallels that of Misrecogni-
tions/Rejections by Levow (1998). This type of
classification is also commonly used to analyze
human errors in human factors research (Wick-
ens et al, 1998). In addition to these two cat-
egories, we added the Degree category based on
our observation of the data we collected. This as-
pect is somewhat specific to certain kinds of phys-
ical actions (those that can be performed to differ-
ent degrees, as opposed to binary actions such as
?opening the door?). However, it seems general
enough to be applied to many collaborative tasks
relevant to robots such as guidance, tele-operation,
and joint construction.
For an automated agent, being able to classify
a user utterance into one of these four categories
(including non-action-correction utterances) could
be very useful to make fast, appropriate decisions
such as canceling the current action, or asking a
clarification question to the user. This is impor-
tant because in human-robot interaction, respon-
siveness to a correction can be critical in avoiding
physical accidents. For instance, if the robot de-
tects that the user issued a commission error cor-
rection, it can stop performing its current action
even before understanding the details of the cor-
rection.
In the rest of the paper, we analyze some lexi-
cal, prosodic and temporal characteristics of action
corrections in the context of human-human con-
versations in a virtual world.
3 The Konbini Domain and System
3.1 Simulated Environments for
Human-Robot Interaction Research
One obstacle to the empirical study of situated in-
teraction is that it requires a fully functional so-
phisticated robot to collect data and conduct ex-
periments. Most such complex robots are still
fragile and thus it is typically challenging to run
user studies with naive subjects without severely
limiting the tasks or the scope of the interaction.
Another issue which comes with real world inter-
action is that it is difficult for the experimenter to
control or monitor the events that affect the inter-
action. Most of the time, an expensive manual
annotation of events and actions is required (see
Okita et al (2009) for an example of such an ex-
perimental setup).
To avoid these issues, robot simulators have
been used. Koulouri and Lauria (2009) devel-
oped a simulator to collect dialogs between hu-
man and simulated robot using a Wizard-of-Oz
method. The human can see a map of a town and
teaches the robot a route and the operator oper-
ates the robot but he/she can see only a small area
around the robot in the map. However, the dialog
167
is keyboard-based, and the situation does not dy-
namically change in this setting, making this ap-
proach unsuitable to the study of timing aspects.
Byron and Fosler-Lussier (2006) describe a cor-
pus of spoken dialogs collected in a setting very
similar to the one we are using but, again, the en-
vironment appears to be static, thus limiting the
importance of the timing of actions and utterances.
In this section, we describe a realistic, PC-based
virtual world that we used to collect human-human
situated dialogs.
3.2 Experimental Setup
In our experiment, two human participants collab-
orate in order to perform certain tasks pertaining
to the management of a small convenience store in
a virtual world. The two participants sit in differ-
ent rooms, both facing a computer that presents
a view of the virtual store. One of the partici-
pants, the Operator (O) controls a (simulated) hu-
manoid robot whose role is to answer all customer
requests. The other participant plays the role of a
remote Manager (M) who sees the whole store but
can only interact with O through speech.
Figure 2 shows the Operator and Manager
views. M can see the whole store at any time, in-
cluding how many customers there are and where
they are. In addition, M knows when a particu-
lar customer has a request because the customer?s
character starts blinking (initially green, then yel-
low, then red, as time passes). M?s role is then to
guide O towards the customers needing attention.
On the other hand, O sees the world through the
?eyes? of the robot, whose vision is limited both
in terms of field of view (90 degrees) and depth
(degradation of vision with depth is produced by
adding a virtual ?fog? to the view). When ap-
proaching a customer who has a pending request,
O?s view display the customer?s request in the
form of a caption.1 O can act upon the virtual
world by clicking on certain object such as items
on the counter (to check them out), machines in
the store (to repair them when needed), and vari-
ous objects littering the floor (to clean them up).
Each action takes a certain amount of time to per-
form (between 3 and 45 seconds), indicated by a
progress bar that decreases as O keeps the pointer
on the target object and the left mouse button
down. Once the counter goes to zero the action is
1No actual speech interaction happens between the Oper-
ator and the simulated customers.
completed and the participants receive 50 (for par-
tially fulfilling a customer request) or 100 points
(for completely fulfilling a request).
When the session begins, customers start enter-
ing the store at random intervals, with a maximum
of 4 customers in the store at any time. Each cus-
tomer follows one of 14 predefined scenarios, each
involving between 1 and 5 requests. Scenarios
represent the customer?s moves in terms of fixed
way points. As a simplification, we did not imple-
ment any reactive path planning. Rather, the ex-
perimenter, sitting in a different room than either
subject has the ability to temporarily take control
of any customer to make them avoid obstacles.
3.3 System Implementation: the Siros
architecture
The experimental system described above was im-
plemented using Siros (SItuated RObot Simula-
tor) a new client/server architecture developed at
Honda Research Institute USA to conduct human-
robot interaction research in virtual worlds. Siros
is similar to the architectures used by certain on-
line video games. The server?s role is to manage
the virtual world and broadcast world updates to
all clients so that they can be rendered to the hu-
man participants. The server receives commands
from the Operator client (robot moves), runs the
simulated customers according to the scenarios,
and maintains the timer and the score. Anytime
the trajectory of an entity (robot, customer, object)
changes, the server broadcasts the related infor-
mation, including entity location, orientation, and
speed, to all clients.
Clients are in charge of rendering a given view
of the virtual world. Rendering itself is performed
by the open source Ogre 3D engine (Open Source
3D Graphics Engine, 2010). In addition, clients
handle all required user interaction such as robot
control and mouse-based object selection. All net-
work messages and user actions are logged into
a text file for further processing. Finally, clients
have the ability to log incoming audio to a wave
file, allowing synchronization between the audio
signal, the user actions, and virtual world events.
Spoken communication itself is handled by an ex-
ternal VoIP client.2
2We used the open source Mumble/Murmur (Mum-
ble/Murmur Project, 2010) system.
168
(a) Manager View (b) Robot View
Figure 2: Screenshots of the Konbini data collection system.
3.4 The Konbini Corpus
3.4.1 Data Collection
Using the system described above, we collected
data from 18 participants. There were 15 male and
3 female participants. All were adults living in the
United States, fluent in English. All were regular
users of computers but their experience with on-
line games was diverse (from none at all to regu-
lar player). All were co-workers (associates or in-
terns) at Honda Research Institute USA, and thus
they knew each other fairly well.
Participants were randomly paired into teams.
After being given the chance to read a brief in-
troduction to the experiment?s design and goals,
the participants did two two-minute practice ses-
sions to familiarize themselves with the task and
control. To avoid providing too much informa-
tion about the layout of the store from the start,
the practice sessions used a different virtual world
than the experimental sessions. The participants
switched roles between the two practice sessions
to get a sense of what both roles entailed (Man-
ager and Operator). After these sessions, the
team did one 10-minute experimental session, then
switched roles once again and did another 10-
minute session. Because the layout of the store
was kept the same between the two experimental
sessions, the first session represents a condition in
which the Operator learns the store layout as they
are performing the tasks, whereas the second ses-
sion corresponds to a case where the Operator al-
ready has knowledge of the layout. Overall, 18 10-
minute sessions were collected, including audio
recordings as well as timestamped logs of world
updates and operator actions.
3.4.2 Annotation
All recordings were orthographically transcribed
and checked. The first author then segmented the
transcripts into dialog acts (DAs). A DA label was
attached to each act, though this information is not
used in the present paper.
Subsequently, the first author annotated each
semantic unit with the action correction labels
described in section 2: Non-correction, Omis-
sion Correction, Commission Correction, Degree
Correction. This annotation was done using the
Anvil video annotation tool,3 which presented au-
dio recordings, transcripts, a timeline of operator
actions, as well as videos of the computer screens
of the participants. Only Manager DAs were anno-
tated for corrections. The second author also anno-
tated a subset of the data in the same way to evalu-
ate inter-annotator agreement. Cohen?s kappa be-
tween the two annotators was 0.67 for the 4-class
task, and 0.76 for the binary task of any-action-
correction vs non-action-correction, which is rea-
sonable, though not very high, indicating that cor-
rection annotation on this type of dialogs is a non-
trivial task, even for human annotators.
4 Analysis of Action Corrections
4.1 Overview
The total number of DAs in the corpus is 6170.
Of those, 826 are corrections and 5303 are non-
corrections. Overall, corrections account thus for
13.4% of the dialog acts. The split among the dif-
ferent correction classes is roughly equal as shown
in Table 5 given in appendix. We found however
significant differences across participants, in terms
3http://www.anvil-software.de
169
of total number of DAs (form 162 to 516), propor-
tion of corrections among those DAs (from 6.8%
to 30.6%), as well as distribution among the three
types of action corrections.
In this section, we present the results of our sta-
tistical analysis of the correlation between a num-
ber of features and correction type. To evaluate
statistical significance, we performed a one-way
ANOVA using each feature as dependent variable
and the correction type as independent variable.
All features described here were found to signifi-
cantly correlate with correction type.
4.2 Features Affecting Corrections
4.2.1 Timing
For each manager DA, we computed the time since
the beginning/end of the previous manager and
operator DAs, as well as of operator?s actions
(walk/turn). To account for reaction time, and
based on our observations we ignored events hap-
pening less than 1 second before a DA.
Table 1 shows the mean durations between these
events and a Manager DA, depending on the act?s
correction class. All corrections happen closer to
Manager dialog acts than non-corrections, which
reflects the fact that corrections typically occur in
phases when the Manager gives instructions, as
well as the fact that the Manager often repeats
corrections. Commission and Degree corrections
are produced closer to Operator actions than either
non-corrections or Omission corrections. This re-
flects the fact that both Commission and Degree
corrections are a reaction to an event that occurred
(the Operator moved or stopped moving unex-
pectedly), whereas Omission corrections address
a lack of action from the Operator, and act there-
fore as a ?time-out? mechanism.
To better understand the relationship between
moves and the timing of corrections, we computed
the probability of a given DA to be an Omission,
Commission and Degree correction as a function
Time since last... NC O C D
Mgr. DA 3.4 s 2.4 s 2.8 s 2.6 s
Ope. DA 5.8 s 6.7 s 6.5 s 7.5 s
Ope. move start 3.8 s 3.1 s 2.3 s 2.5 s
Ope. move end 3.9 s 3.3 s 2.7 s 2.3 s
Table 1: Mean duration between dialog acts and
Operator movements and the beginning of differ-
ent corrections.
Feature Non-Corr Om Com Deg
Perc. voiced 0.48 0.46 0.55 0.53
Min F0 -0.61 -0.41 -0.40 -0.56
Max F0 0.81 0.68 1.02 0.46
Mean F0 -0.03 0.12 0.28 -0.05
Min Power -1.35 -1.24 -1.18 -1.55
Max Power 0.85 0.89 1.14 0.62
Mean Power -0.03 0.09 0.24 -0.2
Table 2: Mean Z-score of prosodic features for dif-
ferent correction classes.
of the time elapsed since the Operator last started
to move. Figure 4 shows the results.
The probability of a DA being an Omission cor-
rection is relatively stable over time. This is con-
sistent with the fact that Omission corrections are
related to lack of action rather than to a specific
action to which the Manager reacts. On the other
hand, the probability of a Commission, and to
lesser extent, Degree correction sharply decreases
with time after an action.
4.2.2 Prosody
We extracted F0 and intensity from all manager
audio files using the Praat software (Boersma and
Weenink, 2010). We then normalized pitch and in-
tensity for each speaker using a Z-transform in or-
der to account for individual differences in mean
and standard deviation. For each DA, we com-
puted the minimum, maximum, and mean pitch
and intensity, using values from voiced frames.
Table 2 shows the mean Z-score of the prosodic
features for the different correction classes. Com-
mission corrections feature higher pitch and inten-
sity than all other classes. This is due to the fact
that such corrections typically involve a higher
emotional level, when the Manager is surprised
or even frustrated by the behavior of the Operator.
In contrast, Degree corrections, which represent a
smaller mismatch between the Operator?s action
and the Manager?s expectations are more subdued,
with mean power and intensity values lower than
even those of non-corrections.
4.2.3 Lexical Features
In order to identify potential lexical characteristics
of correction utterances, we created binary vari-
ables indicating that a specific word from the vo-
cabulary (804 distinct words in total) appears in
a given DA based on the manual transcripts. We
computed the mutual information of those binary
170
variables with DA?s correction label.
Figure 3 shows the 10 words with highest
mutual information. Not surprisingly, negative
words (?NO?, ?DON?T?), continuation words
(?MORE?, ?KEEP?) are correlated with respec-
tively commission and degree corrections. On the
other hand, positive words (?OKAY?, ?YEAH?)
are strong indicators that a DA is not a correction.
Another lexical feature we computed was a flag
indicating that a certain Manager DA is an ex-
act repetition of the immediately preceding Man-
ager DA. The intuition behind this feature is that
corrections often involve repetitions (e.g. ?Turn
left [Operator turns right] Turn left!?). Overall,
10.6% of the DAs are repetitions. This num-
ber is only 6.4% on non-corrections but jumps to
45.6%, 22.5%, and 43.4% on, respectively, Omis-
sion, Commission, and Degree corrections. This
confirms that, as for utterance corrections, detect-
ing exact repetitions could prove useful for correc-
tion classification.
4.2.4 ASR Features
Since our goal is to build artificial agents, we
investigated features related to automatic speech
recognition. We used the Nuance Speech Recog-
nition System v8.5. Using cross-validation, we
trained a statistical language model for each cor-
rection category on the transcripts of the training
portion of the data. We then ran the recognizer se-
quentially with all 4 language models, which gen-
erated a confidence score for each category.
Table 4 shows the mean confidence scores ob-
tained on DAs of each class using a language
model trained on specific classes. While the
matching LM gives the highest score for any given
class, some classes have consistently higher scores
than others. In particular, Commission corrections
receive low confidence scores, which might hurt
the effectiveness of these features. Indeed, lexi-
cal content alone might not be not enough to dis-
tinguish non-corrections and various categories of
corrections since the same expression (e.g. ?Turn
left?) can express a simple instruction, or any kind
of correction, depending on context.
5 Discussion
The results provide support for the correction
classification scheme we proposed. Not only
do corrections differ in many respects from non-
correction utterances, but there are also signifi-
cant differences between Omission, Commission,
PPPPPPPPLM
Corr.
NC O C D
Non-Correction 32.3 28.5 25.0 29.5
Omission 24.0 30.0 23.3 27.2
Commission 26.6 29.8 25.7 27.9
Degree 24.2 28.7 23.9 32.6
Table 4: Mean ASR confidence score using class-
specific LMs.
and Degree corrections. Timing features seem
most useful to distinguish Commission and De-
gree corrections from Omission corrections and
non-corrections. Emphasized prosody (high pitch
and energy) is a particularly strong indicator of
Commission, as well as Omission corrections.
Lexical cues could be useful to all categories, pro-
vided the speech recognizer is accurate enough to
recognize them, which is particularly challenging
on this data given the very conversational nature
of the speech. Finally, ASR scores are also po-
tentially useful features, particularly for Omission
and Degree corrections.
One advantage of timing over all other features
discussed here is that timing information is avail-
able before the correction is actually uttered. This
means that such information could be used to al-
low fast reaction, or to prime the speech recog-
nizer based on the instantaneous probability of the
different classes of correction.
6 Conclusion
In this paper, we analyzed correction utterances in
the context of situated spoken interaction within
a virtual world. We proposed a classification of
action correction utterances into Omission, Com-
mission, and Degree corrections. Our analysis
of human-human data collected using a PC-based
simulated environment shows that the three types
of corrections have unique characteristics in terms
of prosody, lexical features, as well as timing with
regards to physical actions. These results can
serve as the basis for further investigations into au-
tomatic detection and understanding of correction
utterances in situated interaction.
References
Paul Boersma and David Weenink. 2010.
Praat: doing phonetics by computer,
http://www.fon.hum.uva.nl/praat.
171
Word (W) P (Non? Corr|W ) P (Om|W ) P (Com|W ) P (Deg|W )
MORE 0.41 0.01 0.02 0.56
NO 0.55 0.04 0.33 0.07
RIGHT 0.67 0.15 0.04 0.14
TURN 0.69 0.17 0.06 0.08
LEFT 0.65 0.18 0.07 0.10
OKAY 0.99 0.00 0.00 0.00
YEAH 0.99 0.00 0.00 0.00
DON?T 0.59 0.01 0.33 0.07
WAY 0.49 0.05 0.42 0.04
KEEP 0.79 0.03 0.03 0.15
Table 3: Keywords with highest mutual information with correction category.
Donna K. Byron and Eric Fosler-Lussier. 2006.
The OSU Quake 2004 corpus of two-party situated
problem-solving dialogs. In Proc. 15th Language
Resource and Evaluation Conference (LREC?06).
Justine Cassell, Timothy Bickmore, Hannes Hgni
Vilhja?msson, and Hao Yan. 2001. More Than Just a
Pretty Face: Conversational Protocols and the Affor-
dances of Embodiment. Knowledge-Based Systems,
14:55?64.
Mert Cevik, Fuliang Weng, and Chin hui Lee. 2008.
Detection of repetitions in spontaneous speech di-
alogue sessions. In Proc. Interspeech 2008, pages
471?474.
Herbert Clark. 1996. Using Language. Cambridge
University Press.
Kotaro Funakoshi and Takenobu Tokunaga. 2006.
Identifying repair targets in action control dialogue.
In Proc. EACL 2006, pages 177?184.
Petra Gieselman and Mari Ostendorf. 2007. Problem-
Sensitive Response Generation in Human-Robot Di-
alogs. In Proc. SIGDIAL 2002.
Norihide Kitaoka, Naoko Kakutani, and Seiichi Naka-
gawa. 2003. Detection and Recognition of Correc-
tion Utterance in Spontaneously Spoken Dialog. In
Proc. Eurospeech 2003, pages 625?628.
Theodora Koulouri and Stanislao Lauria. 2009. Ex-
ploring miscommunication and collaborative be-
haviour in human-robot interaction. In Proc. SIG-
DIAL 2009, pages 111?119.
Gina-Anne Levow. 1998. Characterizing and recog-
nizing spoken corrections in human-computer dia-
logue. In Proc. COLING-ACL ?98, pages 736?742.
Diane Litman, Julia Hirschberg, and Marc Swerts.
2006. Characterizing and predicting corrections in
spoken dialogue systems. Computational Linguis-
tics, 32(3):417?438.
The Mumble/Murmur Project. 2010.
http://mumble.sourceforge.net.
Sandra Y Okita, Victor Ng-Thow-Hing, and Ravi K
Sarvadevabhatla. 2009. Learning Together:
ASIMO Developing an Interactive Learning Partner-
ship with Children. In Proc. RO-MAN 2009.
OGRE Open Source 3D Graphics Engine. 2010.
http://www.ogre3d.org.
Verena Rieser and Johanna Moore. 2005. Implications
for generating clarification requests in task-oriented
dialogues. In Proc. 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL-05),
pages 239?246.
Kepa Josepa Rogdriguez and David Schlangen. 2004.
Form, intonation and function of clarification re-
quests in german task-oriented spoken dialogues. In
Proc. 8th Workshop on the Semantics and Pragmat-
ics of Dialogue (CATALOG?04).
Emanuel A. Schegloff, Gail Jefferson, and Harvey
Sacks. 1977. The preference for self-correction
in the organization of repair in conversation. Lan-
guage, 53(2):361?382.
David Traum and Jeff Rickel. 2002. Embodied
agents for multiparty dialogue in immersive virtual
worlds. In Proc. International Joint Conference on
Autonomous Agents and Multi-agent Systems (AA-
MAS 2002), pages 766?773.
David R. Traum, Carl F. Andersen, Waiyian Chong,
Darsana P. Josyula, Yoshi Okamoto, Khemdut Pu-
rang, Michael O?Donovan-Anderson, and Donald
Perlis. 1999. Representations of Dialogue State
for Domain and Task Independent Meta-Dialogue.
Electron. Trans. Artif. Intell., 3(D):125?152.
Christopher D. Wickens, Sallie E. Gordon, and Yili
Liu. 1998. An Introduction to Human Factors En-
gineering. Addison-Wesley Educational Publishers
Inc.
172
operator 
action
operator 
utterance
manager 
utterance
walk
turn
to your 
right
forward
left
omission 
correction
3?13??
to your 
right
turn to your 
right
3?14?? 3?15?? 3?16??
stop and .. 
ah too late
(a) Omission correction
operator 
action
operator 
utterance
manager 
utterance
walk
turn
forward
right
2?49?? 2?50?? 2?51??
commission
correction
2?52??
keep on going straight and then uh
no no
the other way
(b) Commission correction
operator 
action
operator 
utterance
manager 
utterance
walk
turn
forward
right
0?11?? 0?12?? 0?13?? 0?14??
left
degree 
correction
0?15?? 0?16??
more 
right
more 
right
more 
rightturn slightly right
(c) Degree correction
Figure 3: Example omission, commission, and degree errors and corrections. The corresponding videos
can be found at http://sites.google.com/site/antoineraux/konbini.
173
2 4 6 80
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
Time since last move (s)
P(Om
)
(a) Omissions
2 4 6 80
0.01
0.02
0.03
0.04
0.05
0.06
Time since last move (s)
P(Co
m)
(b) Commissions
2 4 6 80
0.01
0.02
0.03
0.04
0.05
0.06
Time since last move (s)
P(De
g)
(c) Degree
Figure 4: Evolution of the probability of occurrence of corrections over time after an Operator move.
Participant Total Non-Corr Omission Commission Degree
Total 6170 5303 (86.6%) 298 (4.8%) 277 (4.5%) 251 (4.1%)
1 338 299 (88.5%) 19 (5.6%) 15 (4.4%) 5 (1.5%)
2 249 232 (93.1%) 10 (4.0%) 2 (0.8%) 0 (0.0%)
3 440 383 (87.0%) 25 (5.7%) 11 (2.5%) 9 (2.0%)
4 265 247 (93.2%) 4 (1.5%) 8 (3.0%) 0 (0.0%)
5 313 270 (86.3%) 15 (4.8%) 5 (1.6%) 17 (5.4%)
6 238 198 (83.2%) 22 (9.2%) 13 (5.5%) 5 (2.1%)
7 426 361 (84.7%) 30 (7.0%) 10 (2.3%) 23 (5.4%)
8 244 218 (89.3%) 3 (1.2%) 13 (5.3%) 9 (3.7%)
9 162 137 (84.6%) 4 (2.5%) 13 (8.0%) 8 (4.9%)
10 229 202 (88.2%) 6 (2.6%) 3 (1.3%) 12 (5.2%)
11 380 326 (85.8%) 16 (4.2%) 19 (5.0%) 19 (5.0%)
12 427 385 (90.2%) 16 (3.7%) 11 (2.6%) 15 (3.5%)
13 327 281 (85.9%) 5 (1.5%) 14 (4.3%) 27 (8.3%)
14 516 358 (69.4%) 38 (7.4%) 79 (15.3%) 39 (7.6%)
15 362 332 (91.7%) 13 (3.6%) 6 (1.7%) 11 (3.0%)
16 392 321 (81.9%) 34 (8.7%) 27 (6.9%) 10 (2.6%)
17 362 338 (85.4%) 19 (4.8%) 22 (5.6%) 17 (4.3%)
18 466 415 (89.1%) 19 (4.1%) 6 (1.3%) 25 (5.4%)
Table 5: Frequency of the different types of corrections per participant.
174
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 176?184,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Non-humanlike Spoken Dialogue: A Design Perspective
Kotaro Funakoshi
Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako
Saitama, Japan
funakoshi@jp.honda-ri.com
Mikio Nakano
Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako
Saitama, Japan
nakano@jp.honda-ri.com
Kazuki Kobayashi
Shinshu University
4-17-1 Wakasato, Nagano
Nagano, Japan
kby@shinshu-u.ac.jp
Takanori Komatsu
Shinshu University
3-15-1 Tokida, Ueda
Nagano, Japan
tkomat@shinshu-u.ac.jp
Seiji Yamada
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda
Tokyo, Japan
seiji@nii.ac.jp
Abstract
We propose a non-humanlike spoken di-
alogue design, which consists of two el-
ements: non-humanlike turn-taking and
non-humanlike acknowledgment. Two ex-
perimental studies are reported in this pa-
per. The first study shows that the pro-
posed non-humanlike spoken dialogue de-
sign is effective for reducing speech colli-
sions. It also presents pieces of evidence
that show quick humanlike turn-taking is
less important in spoken dialogue system
design. The second study supports a hy-
pothesis found in the first study that user
preference on response timing varies de-
pending on interaction patterns. Upon re-
ceiving these results, this paper suggests a
practical design guideline for spoken dia-
logue systems.
1 Introduction
Speech and language are owned by humans.
Therefore, spoken dialogue researchers tend to
pursue a humanlike spoken dialogue. Only a few
researchers positively investigate restricted (i.e.,
non-humanlike) spoken dialogue design such as
(Ferna?ndez et al, 2007).
Humanlikeness is a very important concept and
sometimes it is really useful to design machines /
interactions. Machines are, however, not humans.
We believe humanlikenss cannot be the dominant
factor, or gold-standard, for designing spoken dia-
logues.
Pursuing humanlikeness has at least five criti-
cal problems. (1) Cost: in general, humanlikeness
demands powerful and highly functional hardware
and software, and highly integrated systems re-
quiring top-grade experts both for development
and maintenance. All of them lead to cost over-
run. (2) Performance: sometimes, humanlikeness
forces performance to be compromised. For ex-
ample, achieving quick turn-taking which humans
do in daily conversations forces automatic speech
recognizers, reasoners, etc. to be compromised to
enable severe real-time processing. (3) Applicabil-
ity: differences in cultures, genders, generations,
situations limit the applicability of a humanlike
design because it often accompanies a rigid char-
acter. For example, Shiwa et al (2008) succeeded
in improving users? impression for slow responses
from a robot by using a filler but obviously use
of such a filler is limited by social appropriate-
ness. (4) Expectancy: humanlike systems induce
too much expectancy of users that they are as in-
telligent as humans. It will result in disappoint-
ments (Komatsu and Yamada, 2010) and may re-
duce users? willingness to use systems. Keeping
high willingness is quite important from the view-
point of both research (for collecting data from
users to improve systems) and business (for con-
tinuously selling systems with limited functional-
ity). (5) Risk: Although it is not verified, what is
called the uncanny valley (Bartneck et al, 2007)
probably exists. It is commonly observed that peo-
ple hate imperfect humanlike systems.
We try to avoid these problems rather than over-
come them. Our position is positively exploring
non-humanlike spoken dialogue design. This pa-
176
per focuses on its two elements, i.e., decelerated
dialogues as non-humanlike turn-taking and an ar-
tificial subtle expression (ASE) as non-humanlike
acknowledgment1, and presents two experimental
studies regarding these two elements. ASEs, de-
fined by the authors in (Komatsu et al, 2010), are
simple expressions suitable for artifacts, which in-
tuitively notify users about artifacts? internal states
while avoiding the above five problems.
In Section 2, the first study, which was pre-
viously reported in (Funakoshi et al, 2010), is
summarized and shows that the proposed non-
humanlike spoken dialogue design is effective for
reducing speech collisions. It also presents pieces
of evidence that shows quick humanlike turn-
taking is less important in designing spoken dia-
logue systems (SDSs). In Section 3, the second
study, which is newly reported in this paper, shows
a tendency supporting a hypothesis found in the
first study that user preference on response timing
varies depending on interaction patterns. Upon re-
ceiving the results of the two experiments, a design
guideline for SDSs is suggested in Section 4.
2 Study 1: Reducing Speech Collisions
with an Artificial Subtle Expression
in a Decelerated Dialogue
An important issue in SDSs is the management of
turn-taking. Failures of turn-taking due to sys-
tems? end-of-turn misdetection cause undesired
speech collisions, which harm smooth communi-
cation and degrade system usability.
There are two approaches to reducing speech
collisions due to end-of-turn misdetection. The
first approach is using machine learning tech-
niques to integrate information from multiple
sources for accurate end-of-turn detection in early
timing. The second approach is to make a long in-
terval after the user?s speech signal ends and be-
fore the system replies simply because a longer
interval means no continued speech comes. As
far as the authors know, all the past work takes
the first approach (e.g., (Kitaoka et al, 2005;
Raux and Eskenazi, 2009)) because the second ap-
proach deteriorates responsiveness of SDSs. This
choice is based on the presumption that users pre-
fer a responsive system to less responsive systems.
The presumption is true in most cases if the sys-
1In this paper, acknowledgment denotes that at the level 1
of the joint action ladder (Clark, 1996), which communicates
the listener?s identifying the signal presented by the speaker.
B l i n k i n g L E D
Figure 1: Interface robot with an embedded LED
tem?s performance is at human level. However, if
the system?s performance is below human level,
high responsiveness might not be vital or even be
harmful. For instance, Hirasawa et al (1999) re-
ported that immediate overlapping backchannels
can cause users to have negative impressions. Ki-
taoka et al (2005) also reported that the familiarity
of an SDS with backchannels was inferior to that
without backchannels due to a small portion of er-
rors even though the overall timing and frequency
of backchannels was fairly good (but did not come
up to human operators). Technologies are advanc-
ing but they are still below human level. We chal-
lenge the past work that took the first approach.
The second approach is simple and sta-
ble against user differences and environmental
changes. Moreover, it can afford to employ more
powerful but computationally expensive speech
processing or to build systems on small devices
with limited resources. A concern with this ap-
proach is debasement of user experience due to
poor responsiveness as stated above. Another is-
sue is speech collisions due to users? following-
up utterances such as repetitions. Slow responses
tend to induce such collision-eliciting speech.
This section shows the results of the experiment
in which participants engaged in hotel reservation
tasks with an SDS equipped with an ASE-based
acknowledging method, which intuitively notified
a user about the system?s internal state (process-
ing). The results suggest that the method can re-
duce speech collisions and provide users with pos-
itive impressions. The comparisons of evaluations
between systems with a slow reply speed and a
moderate reply speed suggest that users of SDSs
do not care about slow replies. These results in-
dicate that decelerating spoken dialogues is not a
bad idea.
2.1 Experiment
System An SDS that can handle a hotel reserva-
tion domain was built. The system was equipped
177
USER
SYSTEM
VAD tail margin wait interval
processing delay
blinking LED (artificial subtle expression)
short pauses
detected speech onset detected end-of-turn
X Y
system speech
user speech
time
Figure 2: Behavior of the dialogue system along a timeline
with an interface robot with an LED attached to
its chest (see Figure 1). Participants? utterances
were recognized by an automatic speech recog-
nizer Julius2, and interpreted by an in-house lan-
guage understander. The robot?s utterances were
voiced by a commercial speech synthesizer. The
LCD monitor in Figure 1 was used only to show
reservation details at last.
Julius output a recognition result to the system
at 400 msec after an input speech signal ended, but
the system awaited the next input for a fixed inter-
val (wait interval, whose length is given as an ex-
perimental factor). If the system received an addi-
tional input, it awaited the next input for the same
interval again. Otherwise, the system replied.
The LED started blinking at 1/30 sec even-
intervals when a speech signal was detected and
stopped when the system started replying. The
basic function of the blinking light expression is
similar to hourglass icons used in GUIs. A big
difference is that basically GUIs can ignore any in-
put while they are showing those icons, but SDSs
must accept successive speech while it is blink-
ing an LED. What we intend to do is to suppress
only collision-eliciting speech such as repetitions
(we call them follow-ups) which are negligible
but difficult to be automatically distinguished from
barge-ins. Barge-ins are not negligible.
Conditions and participants Two experimen-
tal factors were set-up, that is, the reply speed
factor (moderate or slow reply speed) and the
blinking light factor (with or without a blinking
light), resulting in four conditions:
A: slow reply speed, with a blinking light,
B: slow reply speed, without a blinking light,
C: moderate reply speed, with a blinking light,
D: moderate reply speed, without a blinking light.
We randomly assigned 48 Japanese participants
2http://julius.sourceforge.jp/
(mean age 30.9) to one of the four conditions.
A reply speed depends on a wait interval for
which the dialogue system awaits the next input.
Shiwa et al (2008) showed that the best reply
speed for a conversational robot was one second.
Thus we chose 800 msec as the wait interval for
the moderate reply speed because an actual reply
speed was the accumulation of the wait interval
and a delay for processing a user request, and 800
msec is simply twice the default length (the VAD
tail margin) by which the Julius speech recognizer
recognizes the end of a speech. For the slow reply
speed, we chose 4 sec as the wait interval. Wait
intervals include the VAD tail margin.
Figure 2 shows how the system and the LED
work along with user speech. In this figure, a user
utters a continuous speech with a rather long pause
that is longer than the VAD tail margin but shorter
than the wait interval. If the system detects the
end of the user?s turn and starts speaking within
the interval marked with an ?X?, a speech collision
would occur. If the user utters a follow-up within
the interval marked with a ?Y?, a speech collision
would occur, too. We try to suppress the former
speech collision by decelerating dialogues and the
latter by using a blinking light as an ASE.
Method The experiment was conducted in a
room for one participant at one time. Participants
entered the room and sat on a chair in front of a
desk as shown in Figure 1.
The experimenter gave the participants instruc-
tions so as to reserve hotel rooms five times by
talking with the robot in front of them. All of them
were given the same five tasks which require them
to reserve several rooms (one to three) at the same
time. The meaning of the blinking light expres-
sion was not explained to them. After giving the
instructions, the experimenter left the participants,
and they began tasks when the robot started to talk
to them. Each task was limited to up to three min-
utes. After finishing the tasks, the participants an-
178
swered a questionnaire. Figure 5 and Figure 6 in
the appendix show one of the five task instructions,
and a dialogue on that task, respectively.
2.2 Results
Reply speeds Averages of observed reply
speeds were calculated from the timestamps in
transcripts. They were 4.53 sec for the slow con-
ditions and 1.42 sec for the moderate conditions.
Task completion The average number of com-
pleted tasks in the four conditions A, B, C, and D
were 4.00, 3.83, 3.83, and 4.33, respectively. An
ANOVA did not find any significant difference.
Speech collisions We counted speech collisions
for which the SDS was responsible, that is, the
cases where the robot spoke while participants
were talking (i.e., end-of-turn misdetections). Of
course, there were speech collisions for which par-
ticipants were responsible, that is, the cases where
participants intentionally spoke while the robot
was talking (i.e., barge-ins). These speech colli-
sions were not the targets, hence they were not in-
cluded in the counts.
Speech collisions due to participants? back-
channel feedbacks were not included, either. We
think that it is possible to filter out such feedback
because feedback utterances are usually very short
and variations are small. On the other hand, as
we mentioned above, it is not easy to automat-
ically distinguish negligible speech such as rep-
etitions from barge-ins. We want to suppress
only such speech negligible but hard to distinguish
from other not negligible speech.
The number of observed speech collisions in
the four conditions A, B, C, and D were 5, 11,
45, and 30, respectively. First we performed an
ANOVA on the number of collisions. The interac-
tion effect was not significant (p = 0.24). A sig-
nificant difference on the reply speed factor was
found (p < 0.005). This result confirms that de-
celerating dialogues reduces collisions. The ef-
fect of the blinking light factor was not significant
(p = 0.60).
Next we performed a Fisher?s exact test (one-
side) on the number of participants who had
speech collisions between the two conditions of
the slow reply speed (3 out of 12 for A and 8 out
of 12 for B). The test found a significant difference
(p < 0.05). This result indicates that the blinking
light can reduce speech collisions by suppressing
users? follow-ups in decelerated dialogues.
Impression on the dialogue and robot The par-
ticipants rated 38 positive-negative adjective pairs
(such as smooth vs. rough) for evaluating both the
dialogue and the robot. The ratings are based on a
seven-point Likert scale.
An ANOVA found a positive marginal signifi-
cance (p = 0.07) for the blinking light in the com-
fortableness factor extracted by a factor analysis
for the impression on the dialogue. In addition,
an ANOVA found a positive marginal significance
(p = 0.07) for the slow reply speed in the mod-
esty factor extracted by a factor analysis for the
impression on the robot. Surprisingly, no signifi-
cant negative effect for the slow reply speed was
found.
System evaluations The participants evaluated
the SDS in two measures on a scale from 1 to 7,
that is, the convenience of the system and their
willingness to use the system. The greater the
evaluation value is, the higher the degree of con-
venience or willingness.
The average scores of convenience in the four
conditions A, B, C, and D were 3.50, 3.17, 3.17,
and 3.92, respectively. Those of willingness were
3.58, 2.58, 2.83, and 3.42, respectively. ANOVAs
did not find any significant difference among the
four conditions both for the two measures.
Discussion on user preference The analysis of
the questionnaire suggests that the blinking light
expression gives users a comfortable impression
on the dialogue. The analysis also suggests that
the slow reply speed gives users a modest impres-
sion on the interface robot. Meanwhile, no neg-
ative impression with a statistical significance is
found on the slow reply speed.
Although no statistically significant difference
is found between the four conditions, numbers
of completed tasks and convenience are strongly
correlated. However, users? willingness to use
the systems, which is the most important mea-
sure for systems, is inverted between condition
A and D. Convenience will be primarily domi-
nated by what degree a user?s purpose (reserving
rooms) is achieved, thus, it is reasonable that con-
venience scores correlate with the number of com-
pleted tasks. On the other hand, willingness will
be dominated by not only practical usefulness but
also overall usability or experience. Therefore,
we can interpret that the improvements in impres-
sions and reduction in aversive speech collisions
179
let condition A have the highest score for willing-
ness. These results indicate that decelerating spo-
ken dialogues is not a bad idea in contradiction
to the common design policy in human-computer
interfaces (HCIs), and they suggest to exploit mer-
its provided by decelerating dialogues rather than
pursuing quickly responding humanlike systems.
Our finding contradicts not only the com-
mon design policy in HCIs but also the de-
sign policy in human-robot interaction found by
Shiwa et al (2008), that is, the best response tim-
ing of a communication robot is at one second. We
think this contradiction is superficial and is ascrib-
able to the following four major differences be-
tween their study and our study.
? They adopted a within-subjects experimental
design while we adopted a between-subjects
design. A within-subjects design makes sub-
jects do relative evaluations and tends to em-
phasis differences.
? Their question was specific in terms of re-
sponse timing. Our questions were overall
ratings of the system such as convenience.
? They assumed a perfect machine (Wizard-of-
Oz experiment). Our system was elaborately
crafted but still far from perfect.
? Our system quickly returns non-verbal re-
sponses even if verbal responses are delayed.
From these differences, we hypothesize that re-
sponse timing has no significant impact on the us-
ability of SDSs in an absolute and holistic context
at least in the current state of the art spoken dia-
logue technology, even though users prefer a sys-
tem which responds quickly to a system which re-
sponds slowly when they compare them with each
other directly, given an explicit comparison metric
on response timing with perfect machines.
3 Study 2: Uncovering Comfortableness
of Response Timing under Different
Interaction Patterns
Our conclusion in Section 2 is that SDSs do not
need to quickly respond verbally as long as they
quickly respond non-verbally by showing their in-
ternal states with an ASE, while many researchers
try to make them verbally respond as fast as pos-
sible. Decelerating a dialogue has many practical
advantages as stated above.
However, through the experiment, we have also
suspected that this conclusion is not valid in some
specific cases. That is, we think in some situa-
tions users feel uncomfortable with slow verbal re-
sponses primordially, and those situations are such
as when users simply reply to systems? yes-no-
questions or greetings. Our hypothesis is that users
expect quick verbal responses (and hate slow ver-
bal responses) only when users expect that it is not
difficult for systems to understand their responses
or to decide next actions. This section reports the
experiment validating this hypothesis.
3.1 Experiment
To validate the hypothesis described above, we
conducted a Wizard-of-Oz experiment using fixed
scenarios. Participants engaged in short interac-
tions with an interface robot and evaluated re-
sponse timing of the robot. Three experimental
factors were interaction patterns, response timing
(wait interval), and existence of a blinking light.
Interaction patterns Five interaction patterns
were setup to see the differences between situa-
tions. Each pattern consisted of three utterances.
The first utterance was from the system. Upon re-
ceiving the utterance, a participant as a user of the
system replied with the second utterance. Then
the system responded after the given wait interval
(1 sec or 4 sec) with the third utterance. Partic-
ipants evaluated this interval between the second
utterance and the third utterance in a measure of
comfortableness.
The patterns with scenarios are shown in Fig-
ure 3. They will be referred to by abbreviations
(PGG, QYQ, QNQ, PSQ, PLQ) in what follows.
Note that the scenarios are originally in Japanese.
Here, RequestS and RequestL mean a short re-
quest and a long request, respectively. YNQues-
tion and WhQuestion mean a yes-no-question and
a wh-question, respectively. According to the hy-
pothesis, we can predict that the reported com-
fortableness for the longer wait interval (4 sec)
are worse for short and formulaic cases such as
PGG and QYQ than for the long request case (i.e.,
PLQ). In addition, we can predict that the reported
comfortableness for longer intervals improves for
PLQ if the robot?s light blinks, while that does not
improve for PGG and QYQ.
System We used the same interface robot and
the LCD monitor as study 1. The experiment in
this study, however, was conducted using a WOZ
system.
180
Prompt-Greeting-Greeting (PGG)
S: Welcome to our Hotel. May I help you?
U: Hello.
S: Hello.
YNQuestion-Yes-WhQuestion (QYQ)
S: Welcome to our Hotel. Will you stay tonight?
U: Yes.
S: Can I ask your name?
YNQuestion-No-WhQuestion (QNQ)
S: Welcome to our Hotel. Will you stay tonight?
U: No.
S: How may I help you?
Prompt-RequestS-WhQuestion (PSQ)
S: Welcome to our Hotel. May I help you?
U: I would like to reserve a room from tomorrow.
S: How long will you stay?
Prompt-RequestL-WhQuestion (PLQ)
S: Welcome to our Hotel. May I help you?
U: I would like to reserve rooms with breakfast from to-
morrow, one single room and one double room, non-
smoking and smoking, respectively.
S: How long will you stay?
Figure 3: Interaction patterns and scenarios
First the WOZ system presents an instruction to
the participant on the LCD monitor, which reveals
the robot?s first utterance of the given scenario
(e.g., ?Welcome to our Hotel. May I help you??)
and indicates the participant?s second utterance
(e.g., ?Hello.?). Two seconds after the participant
clicks the OK button on the monitor with a com-
puter mouse, the system makes the robot utter the
first utterance. Then, the participant replies, and
the operator of the system end-points the end of
participant?s speech by clicking a button shown in
another monitor for the operator in the room next
to the participant?s room. After the end-pointing,
the system waits for the wait interval (one second
or four seconds) and makes the robot utter the third
utterance of the scenario. One second after, the
system asks the participant to evaluate the com-
fortableness of the response timing of the robot?s
third utterance on a scale from 1 to 7 (1:very un-
comfortable, 4:neutral, 7:very comfortable) on the
LCD monitor.
Conditions and participants Forty participants
(mean age 28.8, 20 males and 20 females) engaged
in the experiment. No participant had engaged in
study 1. They were randomly assigned to one of
two groups (gender was balanced). The groups
correspond to one of two levels of the experi-
mental factor of the existence of a blinking light.
For one group, the robot blinked its LED when it
was waiting. For the other group, the robot did
not blink the LED. We refer to the former group
(condition) as BL (Blinking Light, n=20) and the
later as NL (No Light, n=20). In summary, this
experiment is within-subjects design with regard
to interaction patterns and response timing and is
between-subjects design with regard to the blink-
ing light.
Method The experiment was conducted in a
room for one participant at one time. Participants
entered the room and sat on a chair in front of a
desk as shown in Figure 1, but they did not wear
headphones this time.
The experimenter gave the participants instruc-
tions so as to engage in short dialogues with the
robot in front of them. They engaged in each of
five scenarios shown in Figure 3 six times (three
times with a 1 sec wait interval and three with
4 sec), resulting in 30 dialogues (5? 3? 2 = 30).
The order of scenarios and intervals was random-
ized. The existence and meaning of the blinking
light expression was not explained to them. They
were not told that the systemwas operated by a hu-
man operator, either. After giving the instructions,
the experimenter left the participants, and they
practiced one time. This practice used a Prompt-
RequestM-WhQuestion3 type scenario with a wait
interval of two seconds. Then, thirty dialogues
were performed. Short breaks were inserted af-
ter ten dialogues. Each dialogue proceeded as ex-
plained above.
3.2 Results
End-pointing errors End-pointing was done by
a fixed operator. We obtained 1,184 dialogues out
of 1,200 (= 30 ? 40) after removing dialogues
in which end-pointing failed (failures were self-
reported by the operator). We sampled 30 dia-
logues from the 1,184 dialogues and analyzed end-
pointing errors in the recorded speech data. The
average error was 84.6 msec (SD=89.6).
Comfortableness This experiment was de-
signed to grasp a preliminary sense on our
hypothesis as much as possible with a limited
number of participants in exchange for aban-
donment of use of statistical tests, because this
study involved multiple factors and the interaction
pattern factor was complex by itself. Therefore,
in the following discussion on comfortableness,
we do not refer to statistical significances.
3The request utterance is longer than that of RequestS and
shorter than that of RequestL.
181
!"#$
"%&'(
)*+,,
,+! ,+!
!"#$
"%&'(
)*+,,
,+! ,+!
Figure 4: Comfortableness (Left: without a blinking light (NL), right: with a blinking light (BL))
Figure 4 shows regression lines obtained from
the 1,184 dialogues in the two graphs for NL and
BL (Detailed values are shown in Table 1). The
X axes in the graphs correspond to response tim-
ing, that is, the two wait intervals of 1 sec and
4 sec. The Y axes correspond to comfortableness
reported in a scale from 1 to 7. Obviously, with or
without a blinking light effected comfortableness.
The results shown in the graphs support the pre-
dictions made in Section 3.1. The scores of PGG
and QYQ are worse than that of PLQ at 4 sec.
PGG and QYQ show no difference between NL
and BL. QNQ and PSQ show differences. PLQ
shows the biggest difference. In case of PLQ, the
reported comfortableness at 4 sec shifted to al-
most the neutral position (score 4) by presenting a
blinking light. This indicates that a blinking light
ASE can allay the debasement of impression due
to slow responses only in non-formulaic cases.
Interestingly, the blinking light expression at-
tracted comfortableness scores to neutral both at
1 sec and at 4 sec. We can make two hypotheses
on this result. One is that the blinking light expres-
sion has a negative effect which degrades comfort-
ableness at 1 sec. The other is that the blinking
light expression makes participants difficult to see
differences between 1 sec and 4 sec, therefore, re-
ported scores converge to neutral. At this stage we
think that the later is more probable than the for-
mer because the scores of PGG and QYQ should
be degraded at 1 sec if the former is true.
4 A Practical Design Guideline for SDSs
Summarizing the results of the experiments pre-
sented in Section 2 and Section 3, we suggest a
twofold design guideline for SDSs, especially for
task-oriented systems. Some interaction-oriented
systems such as chatting systems are out of scope
of this guideline. In what follows, first the guide-
line is presented and then a commentary on the
guideline is described.
The guideline
(1) Never be obsessed with quick turn-taking
but acknowledge users immediately
Quick turn-taking will not recompense your ef-
forts, resources inputted, etc. Pursue it only af-
ter accomplishing all you can do without compro-
mising performance in other elements of dialogue
systems and only if it does not make system devel-
opment and maintenance harder. However, quick
(possibly non-verbal) acknowledgment is a requi-
site. You can compensate for the debasement of
user experience due to slow verbal responses just
by using an ASE such as a tiny blinking LED to
acknowledge user speech. No instruction about
the ASE is needed for users.
(2) Think of users? expectations
Users expect rather quick verbal responses to their
greetings and yes-answers. ASEs will be ineffec-
tive for them. Thus it is recommended to enable
your systems to quickly respond verbally to such
utterances. Fortunately it is easy to anticipate such
utterances. Greetings usually occur only at the be-
ginning of dialogues or after tasks were accom-
plished. Yes-answers will come only after yes-no-
questions. Therefore it will be able to implement
an SDS that quickly responds verbally to greeting
and yes-answers both without increasing develop-
ment / maintenance costs and without decreasing
182
recognition performance, etc.
However, you should keep in mind that too
quick verbal responses (0 sec interval or overlap-
ping) may not be welcomed (Hirasawa et al, 1999;
Shiwa et al, 2008). They may also induce too
much expectancy in users and result in disappoint-
ments to your systems after some interactions.
Commentary on the guideline
The guideline was constructed so as to avoid the
five problems pointed out in Section 1. The first
point of the guideline is induced mainly from the
results of study 1, and the second point is induced
mainly from the results of study 2.
Although the results of study 2 indicate users
prefer quick responses to slow ones as presup-
posed in past literature, note that the experiment
in study 2 is within-subjects design with regard to
the response timing factor and that within-subjects
design tends to emphasis differences as discussed
at the end of Section 2. The results of study 1
suggested that such an emphasized difference (i.e.,
preference for quick responses) has no significant
impact on the usability of SDSs on the whole.
5 Conclusion
This paper proposed a non-humanlike spoken di-
alogue design, which consists of two elements:
non-humanlike turn-taking and acknowledgment.
Two experimental studies were reported regarding
these two elements. The first study showed that the
proposed non-humanlike spoken dialogue design
is effective for reducing speech collisions. This
study also presented pieces of evidence that show
quick humanlike turn-taking is less important in
spoken dialogue system (SDS) design. The second
study showed a tendency supporting a hypothesis
found in the first study that user preference on re-
sponse timing varies depending on interaction pat-
terns in terms of comfortableness. Upon receiving
these results, a practical design guideline for SDSs
was suggested, that is, (1) never be obsessed with
quick turn-taking but acknowledge users immedi-
ately and (2) think of users? expectations.
Our non-humanlike acknowledging method us-
ing an LED-based artificial subtle expression
(ASE) can apply to any interfaces on wearable /
handheld devices, vehicles, whatever. It is, how-
ever, difficult to directly apply it to call-centers
(i.e., telephone interfaces), which occupy a big
portion of the deployed SDSs pie. Yet, the un-
derlying concept: decelerated dialogues accom-
panied by an ASE will be applicable even to tele-
phone interfaces by using an auditory ASE, which
is to be explored in future work.
The guideline is supported by findings in a
rather hypothetical stage. More experiments are
necessary to confirm these findings. In addition,
the guideline is for the current transitory period
in which intelligence technologies such as auto-
matic recognition, language processing, reasoning
etc. are below human level. In that sense, the con-
tribution of this paper might be limited. However,
this period will last until a decisive paradigm shift
occurs in intelligence technologies. It may come
after a year, a decade, or a century.
References
C. Bartneck, T. Kanda, H. Ishiguro, and N. Hagita.
2007. Is the uncanny valley an uncanny cliff? In
Proc. RO-MAN 2007.
H. Clark. 1996. Using Language. Cambridge U. P.
R. Ferna?ndez, D. Schlangen, and T. Lucht. 2007.
Push-to-talk ain?t always bad! comparing different
interactivity settings in task-oriented dialogue. In
Proc. DECALOG 2007.
K. Funakoshi, K. Kobayashi, M. Nakano, T. Komatsu,
and S. Yamada. 2010. Reducing speech collisions
by using an artificial subtle expression in a deceler-
ated spoken dialogue. In Proc. 2nd Intl. Symp. New
Frontiers in Human-Robot Interaction.
J. Hirasawa, M. Nakano, T. Kawabata, and K. Aikawa.
1999. Effects of system barge-in responses on user
impressions. In Proc. EUROSPEECH?99.
N. Kitaoka, M. Takeuchi, R. Nishimura, and S. Nak-
agawa. 2005. Response timing detection us-
ing prosodic and linguistic information for human-
friendly spoken dialog systems. Journal of The
Japanese Society for AI, 20(3).
T. Komatsu and S. Yamada. 2010. Effects of adapta-
tion gap on user?s variation of impressions of artifi-
cial agents. In Proc. WMSCI 2010.
T. Komatsu, S. Yamada, K. Kobayashi, K. Funakoshi,
and M. Nakano. 2010. Artificial subtle expressions:
Intuitive notification methodology of artifacts. In
Proc. CHI 2010.
A. Raux and M. Eskenazi. 2009. A finite-state turn-
taking model for spoken dialog systems. In Proc.
NAACL-HLT 2009.
T. Shiwa, T. Kanda, M. Imai, H. Ishiguro, and
N. Hagita. 2008. How quickly should communi-
cation robots respond? In Proc. HRI 2008.
183
Hotel Reservation Task 3
Reserve rooms as below
Stay
Room
Twin, 1 room, non-smoking
Double, 1 room, non-smoking
As specified with the orange-colored frameon the calendar 
Figure 5: One of the five task instructions used in study 1
S: Welcome to Hotel Wakamatsu-Kawada. May I help you?
U: I want to stay from March 10th to 11th.
S: What kind of room would you like?
U: One non-smoking twin room and one non-smoking double room.
S: Are your reservation details correctly shown on the screen?
U: Yes. No problem.
S: Your reservation has been accepted. Thank you for using us.
Figure 6: A successful dialogue observed with the task shown in Figure 5 (translated into English)
Table 1: Detailed comfortableness scores in study 2
Interaction pattern PGG QYQ QNQ PSQ PLQ
Condition NL BL NL BL NL BL NL BL NL BL
1 sec
mean 5.34 5.36 5.55 5.56 5.48 5.25 5.09 4.73 5.13 4.41
s.d. 1.00 1.17 1.10 1.00 1.02 1.04 1.12 1.09 1.14 1.20
p-value 0.93 0.96 0.23 0.09 0.001
4 sec
mean 3.12 3.16 3.37 3.36 3.28 3.52 3.43 3.52 3.54 3.83
s.d. 0.94 1.04 0.78 0.93 0.76 0.93 0.81 0.87 0.95 0.87
p-value 0.83 0.98 0.14 0.59 0.08
p-values were obtained by two-sided t-tests between NL and BL. Those are shown just for reference.
184
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 18?29,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
A Two-Stage Domain Selection Framework for Extensible Multi-Domain
Spoken Dialogue Systems
Mikio Nakano
Honda Research Institute Japan
Wako, Saitama, Japan
nakano@jp.honda-ri.com
Shun Sato
Tokyo Denki University
Hatoyama, Saitama, Japan
rela.relakuma@gmail.com
Kazunori Komatani
Nagoya University
Nagoya, Aichi, Japan
komatani@nuee.nagoya-u.ac.jp
Kyoko Matsuyama?
Kyoto University
Kyoto, Kyoto, Japan
matuyama@kuis.kyoto-u.ac.jp
Kotaro Funakoshi
Honda Research Institute Japan
Wako, Saitama, Japan
funakoshi@jp.honda-ri.com
Hiroshi G. Okuno
Kyoto University
Kyoto, Kyoto, Japan
okuno@i.kyoto-u.ac.jp
Abstract
This paper describes a general and effective
domain selection framework for multi-domain
spoken dialogue systems that employ dis-
tributed domain experts. The framework con-
sists of two processes: deciding if the current
domain continues and estimating the probabil-
ities for selecting other domains. If the current
domain does not continue, the domain with
the highest activation probability is selected.
Since those processes for each domain expert
can be designed independently from other ex-
perts and can use a large variety of informa-
tion, the framework achieves both extensibil-
ity and robustness against speech recognition
errors. The results of an experiment using
a corpus of dialogues between humans and
a multi-domain dialogue system demonstrate
the viability of the proposed framework.
1 Introduction
As spoken dialogue interfaces are becoming more
widely utilized, they will be expected to be able to
engage in dialogues in a wide variety of topics. Par-
ticularly, spoken dialogue interfaces for office robots
(Asoh et al, 1999) and multimodal kiosk systems
(Gustafson and Bell, 2000) are expected to deal with
people?s various requests, unlike automated call cen-
ter systems that are dedicated to specific tasks.
One effective methodology to build such a sys-
tem is to integrate systems in small domains by
employing distributed multi-domain system archi-
tecture. This architecture has distributed modules
?Currently with Panasonic Corporation.
that independently manage their own dialogue state
and knowledge for speech understanding and ut-
terance generation (e.g., Lin et al (1999)). From
an engineering viewpoint, such architecture has an
advantage in that each domain expert can be de-
signed independently and that it is easy to add new
domains. It enables each domain expert to em-
ploy a dialogue strategy very different from those
for other domains. For example, the strategy may
be frame-based mixed-initiative, finite-state-based
system-initiative, or plan-based dialogue manage-
ment (McTear, 2004).
One of the crucial issues with distributed multi-
domain spoken dialogue systems is how to select an
appropriate domain for each user utterance so that
the system can appropriately understand it and an-
swer it. So far several methods have been proposed
but none of them satisfy two basic requirements at
the same time: the ability to be used with a variety
of domain experts (extensibility) and being robust
against ASR (Automatic Speech Recognition) errors
(robustness). We suspect that this is one of the
main reasons why not many multi-domain spoken
dialogue systems have been developed even though
their utility is widely recognized.
This paper presents a new general framework for
domain selection that satisfies the above two require-
ments. In our framework, each expert needs to have
two additional submodules: one for estimating the
probability that it is newly activated, and one for de-
ciding domain continuation when it is already acti-
vated. Since these submodules can be designed in-
dependently from those of other experts, there is no
restriction on designing experts in our framework,
18
and thus extensibility is achieved. Robustness is also
achieved because those submodules can be designed
so that they can utilize domain-dependent informa-
tion, including information on speech understanding
and dialogue history, without detracting from ex-
tensibility. Especially the submodule for deciding
domain continuation has the ability to utilize dia-
logue history to avoid erroneous domain shifts that
often occur in previous approaches. Note that we do
not focus on classifying each utterance without con-
textual information (e.g., Chu-Carroll and Carpenter
(1999)). Rather, we try to estimate the user inten-
tion with regard to continuing and shifting domains
in the course of dialogues.
In what follows, Section 2 explains the distributed
multi-domain spoken dialogue system architecture
and requirements for domain selection. Section 3
discusses previous work, and Section 4 presents our
proposed framework. Section 5 describes an exam-
ple implementation and its evaluation results, and
Section 6 concludes the paper.
2 Domain Selection in Multi-Domain
Spoken Dialogue Systems
2.1 Distributed Architecture
In distributed multi-domain spoken dialogue archi-
tecture (Figure 1), distributed modules indepen-
dently manage their own dialogue state and knowl-
edge for speech understanding and utterance gener-
ation (Lin et al, 1999; Salonen et al, 2004; Pakucs,
2003; Nakano et al, 2008). Although those modules
are referred to with various names in that literature,
we call them domain experts in this paper. In this
architecture, when an input utterance is received, its
ASR results are sent to domain experts. They try to
understand the ASR results using their own knowl-
edge for understanding. The domain selector gathers
information from those experts and decides which
expert should deal with the utterance and then de-
cide on the system utterances. In this paper, the do-
main expert engaging in understanding user utter-
ances and deciding system utterances is called acti-
vated.
2.2 Example Systems
So far many multi-domain spoken dialogue sys-
tems based on distributed architecture have been
user utterance
information for domain selection
domain selector
activate/deactivate
system utterance(from the activatedexpert)
speech understanding utterance generation
domain expert 1
domain expert 2
domain expert 3
dialoguehistory
Figure 1: Distributed multi-domain spoken dialogue sys-
tem architecture.
built and have demonstrated their ability to engage
in dialogues in a variety of domains. For exam-
ple, several systems integrated information provid-
ing and database searches in multiple domains (Lin
et al, 1999; Komatani et al, 2006; O?Neill et al,
2004; Gustafson and Bell, 2000). Some other sys-
tems integrated domain experts that employ very
different dialogue strategies. Lee et al (2009) and
Nakano et al (2006) integrated task-oriented and
non-task-oriented dialogue managements. Nakano
et al (2008) integrated domain experts for not only
dialogues but also tasks requiring physical actions.
Below we explain an example system that we
used to collect dialogue data for the domain se-
lection experiment described in Section 5. It is a
Japanese system that can provide information on
UNESCOWorld Heritage Sites using speech, slides,
and Microsoft Agent1 gestures. It employs the fol-
lowing ten domain experts:
A question answering (QA) expert: It has a
database consisting of question-answer pairs on
World Heritage Sites (Narimatsu et al, 2010). Each
question-answer pair consists of a set of example
question sentences and answers to them. The an-
swers consist of one or more utterances, plus slides.
Keyphrase-based matching is used to select an ex-
ample question that matches the input utterance.
Eight interactive presentation (IP) experts: Each
of them explains in detail a famous World Her-
itage Site and adds a detailed explanation or skips
some explanation according to the user?s interrup-
tions (Nishimura et al, 2007). Patterns of interrup-
tion utterances are described in the expert?s knowl-
edge base.
1http://www.microsoft.com/MSAgent/
19
utterance expert
S1: Hello. I can answer questions about fa-
mous World Heritage Sites in Italy, Ger-
many, Greece, and other areas. I can also
explain some World Heritage Sites in de-
tail. Please ask me anything.
QA
U1: Show me the list of the World Heritage
Sites in Italy.
QA
S2: (show a slide of the list) I can tell you about
the World Heritage Sites in these areas in
Italy.
QA
U2: Can you show me the list of World Her-
itage Sites you can explain in detail?
QA
S3: (show a slide of the list) I can explain these
in detail.
QA
U3: Can you explain Maritime Greenwich? RU
S4: Maritime Greenwich, right? RU
U4: Yes. RU
S5: (show a slide on Greenwich) Okay. I?ll ex-
plain Maritime Greenwich, a historic port
town known for Greenwich Mean Time.
IP1
U5: What is Greenwich Mean Time? IP1
S6: Greenwich Mean Time is the mean solar
time at the Royal Greenwich Observatory.
IP1
S means system utterance and U means user utterance.
IP1 is one of the IP experts.
Figure 2: An example dialogue with the system for data
collection (translation from Japanese).
A request understanding (RU) expert: It under-
stands the user?s request to start one of the interac-
tive presentations explained above, and engages in
a dialogue to confirm the request. When the under-
standing finishes, the understood request is sent to
a module called task planner (Nakano et al, 2008;
Nakano et al, 2011). The task planner then activates
another expert to perform the requested presentation
(S5 in Figure 2).
Figure 2 shows an example dialogue between a
human and this system. Note that user utterances
are relatively short and include words related to spe-
cific World Heritage Sites or area names. If those
words are misrecognized, domain selection is diffi-
cult unless dialogue context information is used.
This figure also indicates the domain experts that
understood each user utterance and selected each
system utterance. The domain expert that should
deal with a user utterance is decided based on the set
of user utterances that the expert is designed to deal
with. The domains of utterances U1 and U3 are dif-
ferent because the QA expert has knowledge for un-
derstanding U1 and the RU expert has knowledge for
understanding U3. Thus, in this study, the domain of
each utterance is determined based on the design of
the experts employed in the system. If none of the
experts can deal with an utterance, it is considered
as an out-of-domain utterance. Sometimes the cor-
rect domain needs to be determined using contextual
information. For example, utterance U4 ?Yes? can
appear in all domains, but, since this is a reply to S4,
its domain is RU.
This definition of domain is different from that of
domain (or topic) recognition and adaptation stud-
ies in text, monologue, and human-human conver-
sation processing, in which reference domains are
annotated based on human perspectives rather than
system perspectives. From a human perspective, all
user utterances in Figure 2 may be in ?World Her-
itage Site? domain. However, it is not always easy
to build domain experts according to such domain
definitions, because different dialogue tasks in one
such domain may require different dialogue strate-
gies (such as question answering and request under-
standing).
2.3 Requirements for Domain Selection
We pursue a method for domain selection that can
be used in distributed architecture. Such a method
must satisfy the following two requirements.
Extensibility It must not detract from the extensi-
bility of distributed architecture, that is, any kind of
expert must be able to be incorporated, and each ex-
pert must be able to be designed independently from
other experts. This requires the interface between
each domain expert and the domain selector to be as
simple as possible.
Robustness It needs to be robust against ASR er-
rors; that is, the system needs to be able to avoid
erroneous domain transition caused by ASR errors.
3 Previous Work
So far various methods for domain selection have
been proposed, but, as far as we know, no method
satisfies both extensibility and robustness. Isobe et
al. (2003) estimate a score for each domain from the
20
ASR result and select the domain with the highest
score (hereafter referred to as RECSCORE). Since
each domain expert has only to output a numeric
score, it satisfies extensibility. However, because
this method does not take into account dialogue con-
text, it tends to erroneously shift domains when the
score of some experts becomes high by chance. For
example, if U4:?Yes? in Figure 2 is recognized as
?Italy? with a high recognition score in the QA ex-
pert, the domain erroneously shifts to QA and the
system explains about World Heritage Sites in Italy.
Thus this method is not robust.
To avoid erroneous domain shifts, Lin et al
(1999) give preference to the preceding domain
(the domain in which the previous system utterance
was made) by adding a certain value to the score
of the preceding domain (hereafter called REC-
SCORE+BIAS ). However, to what extent the do-
main tends to continue varies depending on the dia-
logue context. For example, if a dialogue task in one
domain finishes (e.g., when an IP expert finishes its
presentation and says ?This is the end of the presen-
tation. Do you have any questions??), the domain is
likely to shift. So, adding a fixed score does not al-
ways work. O?Neill et al?s (2004) system does not
change the dialogue domain until it finishes a task
in the domain, but it cannot recover from erroneous
domain shifts.
To achieve robustness against ASR errors, several
domain selection methods based on a classifier that
uses features concerning dialogue history as well as
ones concerning speech understanding results have
been developed (Komatani et al, 2006; Ikeda et al,
2008; Lee et al, 2009). These studies, however, use
some features available only in some specific type
of domain experts, such as features concerning slot-
filling, so they cannot be used with other kinds of
domain experts. That is, these methods do not sat-
isfy extensibility.
Methods that use classifiers based on word (and
n-gram) frequencies have been developed for utter-
ance classification (e.g., Chu-Carroll and Carpenter
(1999)), topic estimation for ASR of speech cor-
pora (e.g., Hsu and Glass (2006) and Heidel and
Lee (2007)) and human-human dialogues (Lane and
Kawahara, 2005). These methods can be applied to
domain selection in multi-domain spoken dialogue
systems. However, since they require training data
in the same set of domains as the target system, it
detracts from extensibility. In addition, they are not
robust because they cannot utilize a variety of di-
alogue and understanding related features. Word
frequencies are not always effective when two do-
mains share words as in our system described in Sec-
tion 2.2.
4 Proposed Framework
4.1 Basic Idea
To achieve extensibility, we need to restrict the infor-
mation that each expert sends to the domain selector
to a simple one such as numeric scores. Although
RECSCORE and RECSCORE+BIAS satisfy this, they
would not achieve high accuracy as explained above.
One possible extension to those methods to im-
prove accuracy is to use not only recognition scores
but also various expert-dependent features such as
ones concerning dialogue history and speech under-
standing. Each expert first estimates the probability
that the input utterance is in its domain using such
features, and then the expert with the highest proba-
bility is selected (hereafter called MAXPROB). This
method retains extensibility because the domain se-
lector does not directly use those expert-dependent
features. However, it suffers from the same prob-
lem as RECSCORE and RECSCORE+BIAS; if one of
the experts other than the preceding domain?s expert
outputs a high probability by mistake, the domain
shifts regardless of the dialogue state in the preced-
ing domain?s expert.
We focus attention on the fact that the domain
does not often shift. Our idea is to decide if the do-
main continues or not by using information available
in the preceding domain?s expert. This prevents er-
roneous domain shifts when the utterance is consid-
ered not to change the domain. When it is decided
that the currently active domain does not continue,
each remaining expert estimates the probability of
being newly activated using information available in
the expert, and the expert whose probability is the
highest is selected as the new domain expert.
We further refine this idea in two ways. One is by
taking into account how likely the input utterance is
to activate one of the other domain experts. We pro-
pose to use the maximum value of probabilities for
other experts? activation (maximum activation prob-
21
user utterance
expert for the preceding domain
experts for other domains
maximumprobability
select thepreceding domain
select the domain with maximumactivation probability
features
Stage 1
Stage 2
...
domain continuationdecision maker
select expert with maximumactivationprobability
speech understanding
features activation probability estimator
speech understanding
features activation probability estimator
speech understanding
decision is tocontinue? yes
no
out-of-domain
handle as an out-of-domain utterance
dialoguehistory
dialoguehistory
dialoguehistory
Figure 3: Two-stage domain selection framework.
ability) in the decision regarding domain continua-
tion. Since the maximum activation probability is
just a numeric score, this does not spoil extensibil-
ity. Unlike RECSCORE and RECSCORE+BIAS, in
our method, even if the maximum activation prob-
ability is very high, the preceding domain?s expert
can decide to continue or not to continue based on
its internal state. This makes it possible to retain ro-
bustness.
The other refinement is to explicitly deal with ut-
terances that are not in any domains (out-of-domain
(OOD) utterances). They include fillers and mur-
murs. They should be treated separately, because
they appear context-independently. So we make the
expert detect OOD utterances when deciding do-
main continuation. That is, it performs three-fold
classification, continue, not-continue, and OOD.
4.2 Two-Stage Domain Selection Framework
This idea can be summarized as a domain selection
framework which consists of two stages (Figure 3).
It assumes that each domain expert has two submod-
ules: activation probability estimator and a domain
continuation decision maker, which use information
available in the expert itself.
When a new input utterance is received, at Stage
1, the activation probability estimators of all non-
activated experts estimate probabilities and send
them to the domain selector. Then at Stage 2, the
domain selector sends their maximum value to the
expert of the preceding domain and asks it to decide
whether it continues to deal with the new input utter-
ances or does not continue, or it deals with the utter-
ance as out-of-domain. If it decides not to continue,
the domain selector selects the expert that outputs
the highest probability at Stage 1.
The reason we use the term ?framework? is that
it does not specify the details of the algorithm and
features used in each domain expert?s submodules
for domain selection. It rather specifies the inter-
faces of those submodules. Note that RECSCORE,
RECSCORE+BIAS, and MAXPROB can be consid-
ered as one of the implementations of this frame-
work. This framework, however, allows developers
to use a wider variety of features and gives flexibility
in designing those submodules.
5 Example Implementation and
Evaluation
Since the proposed framework is an extension of
the previous methods, if the activation probability
estimator and domain continuation decision maker
for each expert are designed well and trained using
enough data, it should outperform previous methods
that satisfy extensibility. We believe that this theo-
retical consideration and an experimental result us-
ing a human-system dialogue corpus show the via-
bility of the framework. Below we explain our im-
plementation and an experiment.
5.1 Data
For the implementation and evaluation, we used a
corpus of dialogues between human users and the
World Heritage Site information system described
in Section 2.2. Domain selection of this system was
performed using hand-crafted rules.
35 participants (17 males and 18 females) whose
ages range from 19 to 57 were asked to engage in
22
domain preceding training training test
domain data A data B data
RU RU 134 169 145
QA 51 102 59
IP 21 16 23
subtotal 206 287 227
QA RU 46 55 51
QA 783 870 888
IP 59 87 66
subtotal 888 1,012 1,005
IP RU 2 1 3
QA 7 11 18
IP 311 305 277
subtotal 320 317 298
OOD RU 24 19 39
QA 168 155 183
IP 66 68 113
subtotal 258 242 335
total 1,672 1,858 1,865
Table 1: Number of utterances in each domain in the
training and test data.
conversation with the system four times. Each ses-
sion lasted eight minutes. For each utterance, the
correct domain or an OOD label was manually an-
notated. We also annotated its preceding domain,
i.e., the domain in which the previous system utter-
ance was made. It can be different from the previous
user utterance?s domain because of the system?s er-
roneous domain selection. Utterances including re-
quests in two domains at the same time should be
given an OOD label but there are no such utterances.
We used data from 23 participants (3,530 utterances)
for training and those from the remaining 12 par-
ticipants (1,865 utterances) for testing. We further
split the training data into training data A (1,672
utterances) and B (1,858 utterances) to train each
of the two submodules. Each training data set in-
cludes data from two sessions for each participant.
Table 1 shows detailed numbers of utterances in the
data sets.
5.2 Implementation
5.2.1 Expert Classes
Among the ten experts, eight IP (Interactive Pre-
sentation) experts have the same dialogue strategy
and most of the predicted user utterance patterns. In
addition, the number of training utterances for each
expert class QA IP RU
LM for ASR trigram trigram finite-state
grammar
language keyphrase keyphrase finite-state
understanding -based -based transducer
vocabulary 1,140 407 79
size (word)
phone error 10.95 19.47 23.60
rate (%)
Table 2: Speech understanding in each expert.
IP expert?s domain is small. We therefore used all
training utterances in the IP domains to build a com-
mon ASR language model (LM), a common acti-
vation probability estimator, and a common domain
continuation decision maker for all IP experts. Here-
after we call the set of IP experts the IP expert class.
The RU (Request Understanding) expert and the QA
(Question Answer) expert are themselves also expert
classes.
5.2.2 Speech Understanding
For all experts, we used the Julius speech recog-
nizer and the acoustic model in the Japanese model
repository (Kawahara et al, 2004).2 Features of
speech understanding in each expert class are shown
in Table 2. Compared to the system used for data
collection, LMs are enhanced based on the training
data. We obtained the ASR performance on the ut-
terances in each domain in the test data in terms of
phone error rates. This is because Japanese has no
standard word boundaries so it is not easy to cor-
rectly compute word error rates. The poor perfor-
mance of ASR for IP is mainly due to the small
amount of training utterances for LM and that for
RU is mainly due to out-of-grammar utterances.
5.2.3 Stage 1
For Stage 1, we used logistic regression to es-
timate the probability that a non-activated expert
would be activated by a user utterance. Features for
logistic regression include those concerning speech
recognition and understanding results as well as dia-
logue history (see Table 5 for the full list of features).
These features are expert-dependent. This makes it
possible to estimate how the input utterance is suit-
2Multiple LMs can be used at the same time with Julius.
23
able to the dialogue context more precisely than us-
ing just features available in any kind of expert.
To train the activation probability estimators, we
fitted logistic regression coefficients using Weka
data mining toolkit ver.3.6.2 (Witten and Frank,
2005)3 and training data A. In the training for each
expert class, we used utterances whose preceding
domain was not that of the class because activation
probabilities are estimated only for such utterances
during domain selection. If the utterance is in a do-
main of the expert class, it is assigned an activate la-
bel and otherwise not-activate. Next, we performed
feature selection to avoid overfitting. We used back-
ward stepwise selection so that the weighted (by the
sizes of activate and not-activate labels) average of
the F1 scores for training set B could be maximized.
Table 6 lists the remaining features and their sig-
nificances in terms of the F1 score obtained when
each feature is removed. Then, we duplicated the
activate-labeled utterances in the training data A so
that the ratio of activate-labeled utterances to not-
activate-labeled utterances became 1 to 3. This is
because the training data include a larger number of
not-activate-labeled utterances and thus the results
would be biased. The ratio was decided by trial and
error so that the weighted average of the F1 scores
for training data B becomes high.
5.2.4 Stage 2
For Stage 2, we used multi-class support vector
machines (SVMs)4 to decide if the activated expert
should continue to be activated, should not continue,
or should regard the input utterance as OOD. We
used the same set of features as Stage 1 as well
as the maximum activation probability obtained at
Stage 1. The training data for the SVM of each ex-
pert class is the set of utterances in training data B
whose preceding domain is in that expert class, be-
cause domain continuation is decided only for such
utterances during domain selection. They are la-
beled continue, not-continue, or OOD. Next, we
performed backward stepwise feature selection so
that the weighted average of F1 scores for continue,
not-continue, and OOD utterance detection on train-
ing data A could be maximized. Remaining fea-
3Multinominal logistic regression model with a ridge esti-
mator with Weka?s default values.
4Weka?s SMO with the linear kernel and its default values.
tures are listed in Table 7. The maximum activa-
tion probability was found to be significant in all ex-
pert classes. This suggests our two-stage framework
that uses maximum activation probability is viable.
Then, we duplicated utterances with not-continue la-
bel and OOD label in the training data so that the
ratio of continue, not-continue, and OOD utterances
became 3:1:1. This is because the number of utter-
ances with the continue label is far greater than oth-
ers. The ratio was experimentally decided by trial
and error so that the weighted average of F1 scores
on training data A becomes high.
5.3 Evaluation
5.3.1 Compared Methods
We compared the full implementation described in
Section 5.2 (FULLIMPL hereafter) with the follow-
ing four methods which satisfy extensibility. Note
that the first three methods were mentioned in Sec-
tion 4.
RECSCORE: This chooses the expert class whose
recognition score is the maximum (Isobe et al,
2003). We used the ASR acoustic score normalized
by the duration of the utterance. If the IP expert class
was chosen, the IP expert that had been most re-
cently activated was chosen, because, in this system,
domain shifts to other IP experts never occur due to
the system constraints and the user did not try to do
it. If none of the experts had a higher score than a
fixed threshold, it recognized the utterance as OOD.
The threshold was experimentally determined using
the training data so that the weighted (by the sizes
of OOD and non-OOD utterances) average of the
F1 scores of OOD/non-OOD classification is max-
imized.
RECSCORE+BIAS: This is the same as REC-
SCORE except that a fixed value (bias) is added to
the score used in RECSCORE for the expert of the
preceding domain. This is basically the same as Lin
et al?s (1999) method but we use a different recog-
nition score since the recognition score they used
cannot be used in our system due to the difference
of speech understanding methods. The most appro-
priate bias for each expert class was decided using
the training data so that the weighted average of the
F1 scores could be maximized. OOD detection was
done in the same way as RECSCORE.
24
method class recall prec- F1 weighted
ision ave. F1
RECSCORE cont. 0.763 0.867 0.812
shift 0.559 0.239 0.335
OOD 0.501 0.848 0.630 0.789
RECSCORE cont. 0.917 0.824 0.868
+BIAS shift 0.400 0.421 0.410
OOD 0.501 0.848 0.630 0.838
MAXPROB cont. 0.925 0.843 0.882
shift 0.282 0.264 0.273
OOD 0.275 0.477 0.348 0.832
NOACTIV cont. 0.875 0.890 0.882
PROB shift 0.464 0.385 0.421
OOD 0.785 0.843 0.813 0.849
FULLIMPL cont. 0.902 0.907 0.904
shift 0.591 0.565 0.578
OOD 0.824 0.829 0.826 0.883
CLASSIFIER cont. 0.956 0.881 0.917
(reference) shift 0.545 0.759 0.635
OOD 0.755 0.885 0.815 0.899
Table 3: Evaluation results (?cont.? means ?continue.?).
MAXPROB: The activation probabilities for all ex-
perts were obtained using logistic regression and the
expert whose probability was the maximum was se-
lected. IP experts that had never been activated were
excluded because they cannot be activated due to
system constraint. For logistic regression, in addi-
tion to the features used in FULLIMPL, the previous
domain was used as a feature so that domain conti-
nuity was taken into account. Feature selection was
also performed. The probability that the utterance is
OOD was estimated in the same way using the fea-
tures concerning speech understanding. If the maxi-
mum probability of OOD detection was greater than
the maximum activation probability, then the utter-
ance was considered to be OOD.
NOACTIVPROB: This is the same as FULLIMPL
except that Stage 2 does not use the result of Stage
1, i.e., maximum activation probability.
5.3.2 Evaluation Results
To evaluate the domain selection, we focused on
domain shifts rather than the selected domain. We
classified the domain selection results into domain
continuations, domain shifts, and OOD utterance
detection. As the evaluation metric, we used the
weighted average of F1 scores for those classes.
Here the weight is the ratio of those classes of cor-
rect labels. Note that shifting to an incorrect do-
main is counted as a false positive when calculat-
ing precision for domain shifts. Table 3 shows the
results. In addition, the confusion matrices for the
three best methods are shown in Table 4. We found
FULLIMPL outperforms the other four methods. We
also found that the differences between the results of
the compared methods are all statistically significant
(p < .01) by two-tailed binomial tests.
For reference, we also evaluated a classifier-based
method that uses features from all the experts. Note
that this method does not satisfy extensibility be-
cause it requires training data in the same set of do-
mains as the target system. We evaluated this just
for estimating how well our proposed method works
while satisfying extensibility. It classifies each ut-
terance into one of four categories: the QA expert?s
domain, the RU expert?s domain, the most recently
activated IP expert?s domain, and OOD. If no IP ex-
pert has been activated before the utterance, three-
fold classification was performed. The training and
test data were split depending on whether one of the
IP experts has been activated before, and training
and testing were separately conducted. The training
data A was used for training SVM classifiers. Then
feature selection was performed using the training
data B. The performance of this method is shown
as CLASSIFIER in Tables 3 and 4. Although this
method outperforms FULLIMPL, FULLIMPL?s per-
formance is close to this method. This shows that
our method does not degrade its performance very
much even though it satisfies extensibility.
5.3.3 Discussion
One of the reasons why FULLIMPL outperforms
other methods is that its precision for domain shifts
is relatively higher than the other methods. This
suggests it can avoid erroneous domain shifts, thus
the proposed two-stage framework is more robust.
RECSCORE+BIAS performed relatively well despite
it used only limited features. We guess this is be-
cause adding preferences to the preceding domain
was effective since domain shifts are rare in these
data. Its low F1 score for OOD utterances suggests
using just recognition scores is insufficient to detect
them. The comparison of FULLIMPL with NOAC-
TIVPROB shows the effectiveness of using maxi-
mum activation probability in the second stage.
The F1 score for domain shifts is low even with
25
RECSCORE+BIAS:
estimated result
correct cont. correct wrong OOD total
shift shift
continue 1,201 - 82 27 1,310
shift 115 88 14 3 220
OOD 142 - 25 168 335
total 1,458 88 121 198 1,865
NOACTIVPROB:
estimated result
correct cont. correct wrong OOD total
shift shift
continue 1,146 - 123 41 1,310
shift 92 102 18 8 220
OOD 50 - 22 263 335
total 1,288 102 163 312 1,865
FULLIMPL:
estimated result
correct cont. correct wrong OOD total
shift shift
continue 1,181 - 77 52 1,310
shift 70 130 15 5 220
OOD 51 - 8 276 335
total 1,302 130 100 333 1,865
CLASSIFIER (reference):
estimated result
correct cont. correct wrong OOD total
shift shift
continue 1,252 - 30 28 1310
shift 92 120 3 5 220
OOD 77 - 5 253 335
total 1,421 120 38 286 1,865
Table 4: Confusion matrices for the domain shifts.
FULLIMPL, although it is higher than those with
other methods. One typical reason for this is that
when one keyword in the ASR result of an utter-
ance to shift the domain is also in the vocabulary of
the preceding domain?s expert, the selection tends to
continue the previous domain by mistake. For ex-
ample, an utterance ?tell me about other World Her-
itage Sites? to shift from an IP domain to the QA
domain is sometimes misclassified as an IP domain
utterance, because ?World Heritage Sites? is also in
IP domains? vocabulary. We think this is because
the training data do not include a sufficient amount
of utterances that shift domains, and that a larger
amount of training data would solve this problem.
6 Concluding Remarks
This paper presented a novel general framework for
domain selection in extensible multi-domain spoken
dialogue systems. This framework makes it possi-
ble to build a robust domain selector because of its
flexibility in exploiting features and taking into ac-
count domain continuity. An experiment with data
collected with an example multi-domain system sup-
ported the viability of the proposed framework. We
believe that this framework will promote the devel-
opment of multi-domain spoken dialogue systems
and conversational robots/agents.
Among future work is to investigate how accurate
the activation probability estimator and the domain
continuation decision maker in each domain expert
should be for achieving a reasonable accuracy in do-
main selection. We also plan to conduct experiments
with systems that have a larger number of domain
experts to verify the scalability of this framework.
In addition, we will explore a way to estimate the
confidence of the domain selection to reduce erro-
neous domain selections.
Acknowledgments
The authors would like to thank Hiroshi Tsujino,
Yuji Hasegawa, and Hiromi Narimatsu for their sup-
port for this research.
References
Hideki Asoh, Toshihiro Matsui, John Fry, Futoshi Asano,
and Satoru Hayamizu. 1999. A spoken dialog system
for a mobile office robot. In Proc. 6th Eurospeech,
pages 1139?1142.
Jennifer Chu-Carroll and Bob Carpenter. 1999. Vector-
based natural language call routing. Computational
Linguistics, 25(3):361?388.
Joakim Gustafson and Linda Bell. 2000. Speech tech-
nology on trial: Experiences from the August system.
Natural Language Engineering, 6(3&4):273?286.
Aaron Heidel and Lin-shan Lee. 2007. Robust topic in-
ference for latent semantic language model adaptation.
In Proc. ASRU-07, pages 177?182.
Bo-June (Paul) Hsu and James Glass. 2006. Style and
topic language model adaptation using HMM-LDA.
In Proc. EMNLP ?06, pages 373?381,.
Satoshi Ikeda, Kazunori Komatani, Tetsuya Ogata, and
Hiroshi G. Okuno. 2008. Extensibility verification
26
of robust domain selection against out-of-grammar ut-
terances in multi-domain spoken dialogue system. In
Proc. Interspeech-2008 (ICSLP), pages 487?490.
T. Isobe, S. Hayakawa, H. Murao, T. Mizutani,
K. Takeda, and F. Itakura. 2003. A study on do-
main recognition of spoken dialogue systems. In Proc.
Eurospeech-2003, pages 1889?1892.
Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-
sunobu Itou, and Kiyohiro Shikano. 2004. Recent
progress of open-source LVCSR engine Julius and
Japanese model repository. In Proc. Interspeech-2004
(ICSLP), pages 3069?3072.
Kazunori Komatani, Naoyuki Kanda, Mikio Nakano,
Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata,
and Hiroshi G. Okuno. 2006. Multi-domain spo-
ken dialogue system with extensibility and robustness
against speech recognition errors. In Proc. 7th SIGdial
Workshop, pages 9?17.
Ian R. Lane and Tatsuya Kawahara. 2005. Incorporating
dialogue context and topic clustering in out-of-domain
detection. In Proc. ICASSP-2005, pages 1045?1048.
Cheongjae Lee, Sangkeun Jung, Seokhwan Kim, and
Gary Geunbae Lee. 2009. Example-based dialog
modeling for practical multi-domain dialog system.
Speech Communication, 51(5):466?484.
Bor-shen Lin, Hsin-ming Wang, and Lin-shan Lee. 1999.
A distributed architecture for cooperative spoken dia-
logue agents with coherent dialogue state and history.
In Proc. ASRU-99.
Michael F. McTear. 2004. Spoken Dialogue Technology.
Springer.
Mikio Nakano, Atsushi Hoshino, Johane Takeuchi,
Yuji Hasegawa, Toyotaka Torii, Kazuhiro Nakadai,
Kazuhiko Kato, and Hiroshi Tsujino. 2006. A robot
that can engage in both task-oriented and non-task-
oriented dialogues. In Proc. Humanoids-2006, pages
404?411.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and
Hiroshi Tsujino. 2008. A framework for building con-
versational agents based on a multi-expert model. In
Proc. 9th SIGdial Workshop, pages 88?91.
Mikio Nakano, Yuji Hasegawa, Kotaro Funakoshi, Jo-
hane Takeuchi, Toyotaka Torii, Kazuhiro Nakadai,
Naoyuki Kanda, Kazunori Komatani, Hiroshi G.
Okuno, and Hiroshi Tsujino. 2011. A multi-expert
model for dialogue and behavior control of conversa-
tional robots and agents. Knowledge-Based Systems,
24(2):248?256.
Hiromi Narimatsu, Mikio Nakano, and Kotaro Fu-
nakoshi. 2010. A classifier-based approach to
supporting the augmentation of the question-answer
database for spoken dialogue systems. In Proc. 2nd
IWSDS, pages 182?187.
Yoshitaka Nishimura, Shinichiro Minotsu, Hiroshi Dohi,
Mitsuru Ishizuka, Mikio Nakano, Kotaro Funakoshi,
Johane Takeuchi, Yuji Hasegawa, and Hiroshi Tsujino.
2007. A markup language for describing interactive
humanoid robot presentations. In Proc. IUI?07, pages
333?336.
Ian O?Neill, Philip Hanna, Xingkun Liu, and Michael
McTear. 2004. Cross domain dialogue modelling:
an object-based approach. In Proc. Interspeech-2004
(ICSLP), pages 205?208.
Botond Pakucs. 2003. Towards dynamic multi-domain
dialogue processing. In Proc. Eurospeech-2003, pages
741?744.
Esa-Pekka Salonen, Mikko Hartikainen, Markku Tu-
runen, Jaakko Hakulinen, and J. Adam Funk. 2004.
Flexible dialogue management using distributed and
dynamic dialogue control. In Proc. Interspeech-2004
(ICSLP), pages 197?200.
Ian H.Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques, 2nd Edi-
tion. Morgan Kaufmann, San Francisco.
27
expert Features
class
all Fi,r1 If SRRi,1 is obtained or not
classes Fi,r2 If SRRi,1 contains a filler or not
i = ru, Fi,r3 min (CMs of words in SRRi,1)
ip, qa Fi,r4 avg (CMs of words in SRRi,1)
Fi,r5 (acoustic score of SRRi,1) / duration
Fi,r6 LM score of SRRi,1
Fi,r7 # of words in SRRi,1
Fi,r8 # of words in SRRi,all
Fi,r9 (Fi,r5 - (acoustic score of SRRlv,1))
/ duration
RU Fru,r10 If SRRru,1 is an affirmative response
Fru,r11 If SRRru,1 is a denial response
Fru,r12 # of ASR results with LMru
Fru,r13 If SRRru,1 contains the name of a
World Heritage Site
Fru,r14 max (CMs of words comprising the
name of a World Heritage Site)
Fru,r15 ave (CMs of words comprising the
name of a World Heritage Site)
Fru,h1 If SRRru,1 is an affirmative response
(Stage 2 only)
Fru,h2 # of turns since this expert is acti-
vated
Fru,h3 # of denial responses recognized
since this expert is activated
Fru,h4 Fru,h4/Fru,h3
Fru,h5 If the previous system utterance is a
confirmation request to a user request
for starting a presentation
Fru,h6 If the previous system utterance is
an utterance to react to a non-
understandable user utterance
Fru,h7 If the system has made a confirma-
tion request to a user request for start-
ing a presentation since this expert
was activated
Fru,h8 If the system has made an utterance
to react to a non-understandable user
utterance since this expert was acti-
vated
Fru,h9 If the system has made a confirma-
tion request to a user request for start-
ing a presentation before
Fru,h10 If the system has made an utterance
to react to a non-understandable user
utterance before
expert Features
class
IP Fip,r10 If the SRRip,1 is out of database
Fip,r11
P
j((# of keyphrases in SRRip,j) / (# of words in
SRRip,j) ) / (# of ASR results)
Fip,r12 mini( # of keyphrasei in SRRip,all / (# of ASR re-
sults))
Fip,r13 maxi( # of keyphrasei in SRRip,all / (# of ASR re-
sults))
Fip,r14 avg( CM of keyphrasei in SRRip,1)
Fip,r15 mini ( CM of keyphrasei in SRRip,1)
Fip,r16 maxi ( CM of keyphrasei in SRRip,1)
Fip,h1 If this expert has been activated before
Fip,h2 Same as Fru,h2
Fip,h3 If the previous system utterance is the final utter-
ance of the presentation
Fip,h4 If the previous system utterance is an utterance to
react to a user interruption
Fip,h5 Same as Fru,h6
Fip,h6 If the system has made the final utterance of the pre-
sentation since this expert was activated
Fip,h7 If the system has made an utterance to react to a user
interruption since this expert was activated
Fip,h8 Same as Fru,h8
Fip,h9 If the system has made the final utterance of the pre-
sentation before
Fip,h10 If the system has made an utterance to react to a user
interruption before
Fip,h11 Same as Fru,h10
QA Fqa,r10 Same as Fip,r12
Fqa,r11 Same as Fip,r13
Fqa,r12 Same as Fip,r14
Fqa,r13 Same as Fip,r15
Fqa,r14 Same as Fip,r16
Fqa,r15 Same as Fip,r17
Fqa,r16 If SRRqa,1 is an acknowledgment
Fqa,h1 Same as Fru,h1
Fqa,h2 Same as Fru,h2
Fqa,h3 Same as Fru,h3
Fqa,h4 Fqa,h4/Fqa,h3
Fqa,h5 If the previous system utterance is the final utter-
ance of an answer
Fqa,h6 Same as Fru,h6
Fqa,h7 If the system has made the final utterance of an an-
swer since this expert was activated
Fqa,h8 Same as Fru,h8
Fqa,h9 If the system has made the final utterance of an an-
swer before
Fqa,h10 Same as Fru,h10
SRRi,j means j-th speech recognition result with the language model (LM) for expert class i. SRRi,all means all the recognition
results in the n-best list. Fi,rx are speech understanding related features and Fi,hx are dialogue history related features. SRRlv,j
is an ASR result with a large-vocabulary (60,250 words) statistical model (Kawahara et al, 2004), which we used for utterance
verification. CM means confidence measure.
Table 5: Features used in the experiment.
28
expert class
(F1 score
obtained
after feature
selection)
remaining features (F1 score obtained
when each feature is removed)
RU(0.948) Fru,r9 (0.922), Fru,h8 (0.939), Fru,r5
(0.940), Fru,r14 (0.941), Fru,r2
(0.944), Fru,h9 (0.944), Fru,h5
(0.944), Fru,r13 (0.945), Fru,h10
(0.945), Fru,r10 (0.946), Fru,r8
(0.946), Fru,r7 (0.946)
IP(0.837) Fip,r7 (0.771), Fip,r6 (0.772), Fip,h9
(0.781), Fip,h7 (0.781), Fip,h11
(0.786), Fip,r4 (0.79), Fip,r2 (0.799),
Fip,r16 (0.809), Fip,r5 (0.809), Fip,r3
(0.809), Fip,h4 (0.809), Fip,r9 (0.814),
Fip,r15 (0.833), Fip,r12 (0.834), Fip,r13
(0.835), Fip,h10 (0.836)
QA(0.836) Fqa,r14 (0.813), Fqa,r7 (0.817),
Fqa,r16 (0.817), Fqa,r10 (0.818),
Fqa,h6 (0.820), Fqa,r6 (0.822), Fqa,r3
(0.831), Fqa,r5 (0.832)
Table 6: Features that remained after feature selection at
Stage 1 and their significances in terms of the F1 score
obtained when each feature is removed.
expert class
(F1 score
obtained
after feature
selection)
remaining features (F1 score obtained
when each feature is removed)
RU(0.773) Fru,r3 (0.728), Fru,a (0.737), Fru,h5
(0.743), Fru,h1 (0.751), Fru,r9 (0.754),
Fru,h10 (0.757), Fru,h8 (0.757),
Fru,r5 (0.758), Fru,r2 (0.759), Fru,r13
(0.762), Fru,r14 (0.763), Fru,h9
(0.767), Fru,r15 (0.768), Fru,r10
(0.768), Fru,h3 (0.772)
IP(0.827) Fip,h5 (0.808), Fip,r5 (0.809), Fip,r4
(0.810), Fip,r6 (0.811), Fip,a (0.812),
Fip,h4 (0.812), Fip,r13 (0.813), Fip,h3
(0.817), Fip,r15 (0.818), Fip,r3 (0.818),
Fip,h10 (0.819), Fip,r12 (0.820),
Fip,h7 (0.821), Fip,r11 (0.822), Fip,r10
(0.822), Fip,h8 (0.822), Fip,h6 (0.822),
Fip,r2 (0.824), Fip,r8 (0.824), Fip,h9
(0.824), Fip,h2 (0.825)
QA(0.873) Fqa,a (0.838), Fqa,r5 (0.857), Fqa,h1
(0.859), Fqa,r3 (0.862), Fqa,r6 (0.865),
Fqa,h8 (0.867), Fqa,r7 (0.868), Fqa,r15
(0.870), Fqa,r8 (0.870), Fqa,h7 (0.870),
Fqa,r12 (0.871), Fqa,r2 (0.871), Fqa,r16
(0.871), Fqa,h4 (0.871), Fqa,h3 (0.871),
Fqa,r11 (0.872), Fqa,h6 (0.872), Fqa,h5
(0.872)
Table 7: Features that remained after feature selection at
Stage 2 and their significances in terms of the F1 score
obtained when each feature is removed. Fru,a, Fip,a, and
Fqa,a are the maximum activation probabilities obtained
at Stage 1.
29
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 237?246,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Unified Probabilistic Approach to Referring Expressions
Kotaro Funakoshi Mikio Nakano
Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako,
Saitama 351-0188, Japan
{funakoshi,nakano}@jp.honda-ri.com
Takenobu Tokunaga Ryu Iida
Tokyo Institute of Technology
2-12-1 Oookayama, Meguro,
Tokyo 152-8550, Japan
{take,ryu-i}@cl.cs.titech.ac.jp
Abstract
This paper proposes a probabilistic approach
to the resolution of referring expressions for
task-oriented dialogue systems. The approach
resolves descriptions, anaphora, and deixis in
a unified manner. In this approach, the notion
of reference domains serves an important role
to handle context-dependent attributes of enti-
ties and references to sets. The evaluation with
the REX-J corpus shows promising results.
1 Introduction
Referring expressions (REs) are expressions in-
tended by speakers to identify entities to hearers.
REs can be classified into three categories: descrip-
tions, anaphora, and deixis; and, in most cases,
have been studied within each category and with a
narrowly focused interest. Descriptive expressions
(such as ?the blue glass on the table?) exploit at-
tributes of entities and relations between them to
distinguish an entity from the rest. They are well
studied in natural language generation, e.g., (Dale
and Reiter, 1995; Krahmer et al, 2003; Dale and Vi-
ethen, 2009). Anaphoric expressions (such as ?it?)
refer to entities or concepts introduced in the pre-
ceding discourse and are studied mostly on textual
monologues, e.g., (Kamp and Reyle, 1993; Mitkov,
2002; Ng, 2010). Deictic (exophoric) expressions
(such as ?this one?) refer to entities outside the pre-
ceding discourse. They are often studied focusing
on pronouns accompanied with pointing gestures in
physical spaces, e.g., (Gieselmann, 2004).
Dialogue systems (DSs) as natural human-
machine (HM) interfaces are expected to han-
dle all the three categories of referring expres-
sions (Salmon-Alt and Romary, 2001). In fact, the
three categories are not mutually exclusive. To be
concrete, a descriptive expression in conversation is
either deictic or anaphoric. It is, however, not easy to
tell whether a RE is deictic or anaphoric in advance
of a resolution (regardless of whether the RE is de-
scriptive or not). Therefore, we propose a general
unified approach to the above three kinds of REs.
We employ a Bayesian network (BN) to model a
RE. Dealing with continuous information and vague
situations is critical to handle real world problems.
Probabilistic approaches enable this for reference re-
solvers. Each BN is dynamically constructed based
on the structural analysis result of a RE and contex-
tual information available at that moment. The BN
is used to estimate the probability with which the
corresponding RE refers to an entity.
One of the two major contributions of this paper is
our probabilistic formulation that handles the above
three kinds of REs in a unified manner. Previously
Iida et al (2010) proposed a quantitative approach
that handles anaphoric and deictic expressions in a
unified manner. However it lacks handling of de-
scriptive expressions. Our formulation subsumes
and extends it to handle descriptive REs. So far, no
previously proposed method for reference resolution
handles all three types of REs.
The other contribution is bringing reference
domains into that formulation. Reference do-
mains (Salmon-Alt and Romary, 2000) are sets of
referents implicitly presupposed at each use of REs.
By considering them, our approach can appropri-
ately interpret context-dependent attributes. In ad-
dition, by treating a reference domain as a referent,
REs referring to sets of entities are handled, too. As
far as the authors know, this work is the first that
takes a probabilistic approach to reference domains.
237
1.1 Reference domains
First, we explain reference domains concretely. Ref-
erence domains (RDs) (Salmon-Alt and Romary,
2000; Salmon-Alt and Romary, 2001; Denis, 2010)
are theoretical constructs, which are basically sets
of entities presupposed at each use of REs. RDs in
the original literature are not mere sets of entities
but mental objects equipped with properties such
as type, focus, or saliency and internally structured
with partitions. In this paper, while we do not ex-
plicitly handle partitions, reference domains can be
nested as an approximation of partitioning, that is,
an entity included in a RD is either an individual en-
tity or another RD. Each RD d has its focus and de-
gree of saliency (a non-negative real number). Here-
after, two of them are denoted as foc(d) and sal(d)
respectively. RDs are sorted in descending order ac-
cording to saliency.
We illustrate reference domains with figure 1. It
shows a snapshot of solving a Tangram puzzle (the
puzzle and corpus are explained in section 3.1). RDs
are introduced into our mental spaces either linguis-
tically (by hearing a RE) or visually (by observing
a physical situation). If one says ?the two big tri-
angles? in the situation shown in figure 1, we will
recognize a RD consisting of pieces 1 and 2. If we
observe one moves piece 1 and attaches it to piece
2, we will perceptually recognize a RD consisting
of pieces 1, 2, and 6 due to proximity (Tho?risson,
1994). In a similar way, a RD consisting of pieces 5
and 7 also can be recognized. Hereafter, we indicate
a RD with the mark @ with an index, and denote
its elements by enclosing them with [ ]. E.g., @1 =
[1, 2], @2 = [1, 2, 6], @3 = [5, 7]. The focused en-
tity is marked by ?*?. Thus, foc([1?, 2]) = 1.
The referent of a RE depends on which RD is pre-
supposed. That is, if one presupposes @1 or @2, the
referent of ?the right piece? should be piece 1. If
one presupposes @3, the referent of the same RE
should be piece 5. This is the context-dependency
mentioned above.
Previous work on RDs (Salmon-Alt and Romary,
2000; Salmon-Alt and Romary, 2001; Denis, 2010)
employ not probabilistic but formal approaches.
1.2 Probabilistic approaches to REs
Here, previous probabilistic approaches to REs are
explained and differences between ours and theirs
Figure 1: Tangram puzzle. (The labels 1 to 7 are for il-
lustration purposes and not visible to participants.)
are highlighted. Bayesian networks (Pearl, 1988;
Jensen and Nielsen, 2007) have been not often but
occasionally applied to problems in natural language
processing/computational linguistics since (Char-
niak and Goldman, 1989). With regard to REs,
Burger and Connolly (1992) proposed a BN special-
ized for anaphora resolution. Weissenbacher (2005;
2007) proposed a BN for the resolution of non-
anaphoric ?it? and also a BN for the resolution of
pronominal anaphora. They used pre-defined fixed
BNs for their tasks while our approach dynamically
tailors a BN for each RE.
Cho and Maida (1992) and Roy (2002) adopted
not exactly BNs but similar probabilistic approaches
for reference resolution and generation respectively.
However, their foci are only on descriptions.
Lison et al (2010) proposed an approach using
Markov logic networks (MLNs) (Richardson and
Domingos, 2006) to reference resolution. They
dealt with only deictic and descriptive REs. Even
though MLNs are also a probabilistic framework, it
is difficult for DS developers to provide quantitative
domain knowledge needed to resolve REs because
MLNs accept domain knowledge in the form of for-
mal logic rules with weights, which must be deter-
mined globally. In contrast, BNs are more flexible
and easy in providing quantitative knowledge to DSs
in the form of conditional probability tables, which
can be determined locally.
As just described, there are several probabilis-
tic approaches to REs but none of them incorpo-
rates reference domains. In the next section, we in-
troduce our REBNs (Referring Expression Bayesian
Networks), a novel Bayesian network-based model-
ing approach to REs that incorporates reference do-
mains.
238
W C X D
Figure 2: WCXD fundamental structure.
2 Bayesian Network-based Modeling of
Referring Expressions
Each REBN is dedicated for a RE in the context at
the moment. Its structure is determined by the syn-
tactic and semantic information in the RE and prob-
ability tables are determined by the context.
2.1 Structures
Figure 2 shows the fundamental network structure
of REBNs. We call this structure WCXD. The four
nodes (random variables)W ,C,X , andD represent
an observed word, the concept denoted by the word,
the referent of the RE, and the presupposed RD, re-
spectively. Here, a word means a lexical entry in
the system dictionary defined by the DS developer
(concept dictionary; section 3.2.1).
Each REBN is constructed by modifying or mul-
tiply connecting the WCXD structure as shown in
figures 3 and 4. Figure 3 shows the network for REs
indicating one referent such as ?that table.? EachWi
node has a corresponding word wi. Figure 4 shows
the network for REs indicating two referents such as
?his table.? We call the class of the former REs s-
REX (simple Referring EXpression) and the class of
the latter REs c-REX (compound Referring EXpres-
sion). Although REBNs have the potential to deal
with c-REX, hereafter we concentrate on s-REX be-
cause the page space is limited and the corpus used
for evaluation contains very few c-REX instances.
Although, in section 1, we explained that (Iida et
al., 2010) handles anaphoric and deictic expressions
in a unified manner, it handles anaphora to instances
only and does not handle that to concepts. There-
fore, it cannot satisfactorily resolve such an expres-
sion ?Bring me the red box, and the blue one, too.?
Here, ?one? does not refer to the physical referent
of ?the red box? but refers to the concept of ?box?.
TheC nodes will enable handling of such references
to concepts. This is one of the important features of
REBNs but will be investigated in future work.
W
1
C
1
X D
W
2
C
2
Figure 3: BN for two-word REs indicating one referent.
W
1
C
1
X
1
D
1
W
2
C
2
X
2
D
2
Figure 4: BN for two-word REs indicating two referents.
2.2 Domains of random variables
A REBN for an s-REX instance of N words
has 2N + 2 discrete random variables:
W1, . . . ,WN , C1, . . . , CN , X , and D. The do-
main of each variable depends on the corresponding
RE and the context at the moment. Here, D(V )
denotes the domain of a random variable V .
D(Wi) contains the corresponding observed word
wi and a special symbol ? that represents other pos-
sibilities, i.e., D(Wi) = {wi,?}. Each Wi has a
corresponding node Ci.
D(Ci) containsM concepts that can be expressed
by wi and a special concept ? that represents other
possibilities, i.e., D(Ci) = {c1i , . . . , cMi ,?}. cji
(j = 1 . . .M ) are looked up from the concept dic-
tionary (see section 3.2.1, table 2).
D(D) contains L + 1 RDs recognized up to that
point in time, i.e., D(D) = {@0,@1, . . . ,@L}. @0
is the ground domain that contains all the individ-
ual entities to be referred to in a dialogue. At the
beginning of the dialogue, D(D) = {@0}. Other
L RDs are incrementally added in the course of the
dialogue.
D(X) contains all the possible referents, i.e., K
individual entities and L + 1 RDs. Thus, D(X) =
{x1, . . . , xK ,@0, . . . ,@L}. Including RDs enables
handling of references to sets.
Then reference resolution is formalized as below:
x? = argmax
x?D(X)
P (X = x|W1 = w1, . . . ,WN = wN ). (1)
P (X|W1, . . . ,WN ) is obtained by marginalizing
the joint probabilities that are computed with the
probability tables described in the next subsection.
239
2.3 Probability tables
Probability distributions are given as (conditional)
probability tables since all the random variables
used in a REBN are discrete. Here, four types of
probability tables used by REBNs are described.
2.3.1 P (Wi|Ci, X)
P (Wi = w|Ci = c,X = x) is the probability that
a hearer observes w from c and x which the speaker
intends to indicate.
In most cases, Wi does not depend on X , i.e.,
P (Wi|Ci, X) ? P (Wi|Ci). X is, however, nec-
essary to handle individualized terms (names).
There are several conceivable ways of probabil-
ity assignment. One simple way is: for each cji ,
P (W = wi|C = cji ) = 1/T, P (W = ?|C =
cji ) = (T ? 1)/T , and for ?, P (W = wi|C =
?) = ", P (W = ?|C = ?) = 1 ? ". Here T is the
number of possible words for cji . " is a predefined
small number such as 10?8. We use this assignment
in the evaluation.
2.3.2 P (Ci|X,D)
P (Ci = c|X = x,D = d) is the probability that
concept c is chosen from D(Ci) to indicate x in d.
The developers of DSs cannot provide
P (Ci|X,D) in advance because D(Ci) is context-
dependent. Therefore, we take an approach of
composing P (Ci|X = x,D = d) from R(cji , x, d)
(cji ? D(Ci)\{?}). Here R(cji , x, d) is the rele-
vancy of concept cji to referent x with regard to d,
and 0 ? R(cji , x, d) ? 1. 1 means full relevancy
and 0 means no relevancy. 0.5 means neutral. For
example, a concept BOX will have a high relevancy
to a suitcase such as 0.8 but a concept BALL will
have a low relevancy to the suitcase such as 0.1.
If x is not in d, R(cji , x, d) is 0. Algorithm 1
in appendix A shows an algorithm to compose
P (Ci|X = x,D = d) from R(cji , x, d). Concept
? will be assigned a high probability if none of
cji ? D(Ci)\{?} has a high relevancy to x.
If cji is static,1 R(cji , x, d) is numerically given in
advance in the form of a table. If not static, it is im-
plemented as a function by the DS developer, that is,
R(cji , x, d) = fcji (x, d, I). Here I is all the informa-tion available from the DS.
1Whether a concept is static or not depends on each DS.
For example, given a situation such as shown in
figure 1, the relevancy function of a positional con-
cept LEFT (suppose a RE such as ?the left piece?)
can be implemented as below:
fLEFT(x, d, I) = (ux ? ur)/(ul ? ur). (2)
Here, ux, ul and ur are respectively the horizontal
coordinates of x, the leftmost piece in d, and the
rightmost piece in d, which are obtained from I . If
x is a RD, the relevancy is given as the average of
entities included in the RD.
2.3.3 P (X|D)
P (X = x|D = d) is the probability that entity x
in RD d is referred to, which is estimated according
to the contextual information at the time the corre-
sponding RE is uttered but irrespective of attributive
information in the RE. The contextual information
includes the history of referring so far (discourse)
and physical statuses such as the gaze of the referrer
(situation). We call P (X = x|D = d) the predic-
tion model.
The prediction model can be constructed by us-
ing a machine learning-based method. We use a
ranking-based method (Iida et al, 2010). The score
output by the method is input into the standard sig-
moid function and normalized to be a probability. If
x is not in d, P (X = x|D = d) is 0.
2.3.4 P (D)
P (D = d) is the probability that RD d is presup-
posed at the time the RE is uttered. We cannot col-
lect data to estimate this probabilistic model because
RDs are implicit. Therefore, we examine three a pri-
ori approximation functions based on the saliency of
d. Saliency is proportional to recency.2
Uniformmodel This model ignores saliency. This
is introduced to see the importance of saliency.
P (D = d) = 1/|D(D)| (3)
Linear model This model distributes probabilities
in proportion to saliency. This is an analogy of the
method used in (Denis, 2010).
P (D = d) = sal(d)?
d??D(D) sal(d?)
(4)
2Assignment of saliency is described in section 3.2.3.
240
Exponential model This model puts emphasis on
recent RDs. This function is so called soft-max.
P (D = d) = exp(sal(d))?
d??D(D) exp(sal(d?))
(5)
3 Experimental Evaluation
We evaluated the potential of the proposed frame-
work by using a situated human-human (HH) dia-
logue corpus.
3.1 Corpus
We used the REX-J Japanese referring expression
corpus (Spanger et al, 2010). The REX-J corpus
consists of 24 HH dialogues in each of which two
participants solve a Tangram puzzle of seven pieces
(see figure 1). The goal of the puzzle is combining
seven pieces to form a designated shape (such as a
swan). One of two subjects takes the role of opera-
tor (OP) and the other takes the role of solver (SV).
The OP can manipulate the virtual puzzle pieces dis-
played on a PC monitor by using a computer mouse
but does not know the goal shape. The SV knows
the goal shape but cannot manipulate the pieces. The
states of the pieces and the mouse cursor operated by
the OP are shared by the two subjects in real time.
Thus, the two participants weave a collaborative dia-
logue including many REs to the pieces. In addition
to REs, the positions and directions of the pieces, the
position of the mouse cursor, and the manipulation
by the OP were recorded with timestamps and the
IDs of relevant pieces.
3.1.1 Annotation
Each RE is annotated with its referent(s) as shown
in table 1. The 1st RE okkiisankaku3 big triangle ?a
big triangle? in the table is ambiguous and refers to
either piece 1 or 2. The 7th and 8th REs refer to
the set of pieces 1 and 2. The other REs refer to an
individual piece.
To skip the structural analysis of REs to avoid
problems due to errors in such analysis, we have
additionally annotated the corpus with intermediate
structures, from which REBNs are constructed. Be-
cause we focus on s-REX only in this paper, the
3Words are not separated by white spaces in Japanese.
intermediate structures are straightforward:4 paren-
thesized lists of separated words as shown in ta-
ble 1. The procedure to generate a REBN of s-REX
from such an intermediate structure is also straight-
forward and thus it is not explained due to the page
limitation.
3.2 Implementations
We use BNJ5 for probabilistic computation. Here
we describe the implementations of resources and
procedures that are more or less specific to the task
domain of REX-J.
3.2.1 Concept dictionary
Table 2 shows an excerpt of the concept dictio-
nary defined for REX-J. We manually defined 40
concepts by observing the dialogues.
3.2.2 Static relevancy table and relevancy
functions
For 13 concepts out of 40, their relevancy values
were manually determined by the authors. Table 3
shows an excerpt of the static relevancy table defined
for the seven pieces shown in figure 1. TRI is rele-
vant only to pieces 1 to 5, and SQR is relevant only
to pieces 6 and 7 but is not totally relevant to piece 7
because it is not a square in a precise sense. FIG is
equally but not very relevant to all the pieces,6
For the remaining 27 concepts, we implemented
relevancy functions (see appendix B).
3.2.3 Updating the list of RDs
In our experiment, REs are sequentially resolved
from the beginning of each dialogue in the corpus.
In the course of resolution, RDs are added into a list
and updated by the following procedure. RDs are
sorted in descending order according to saliency.
At each time of resolution, we assume that all the
previous REs are correctly resolved. Therefore, af-
ter each time of resolution, if the correct referent of
the last RE is a set, we add a new RD equivalent
to the set into the list of RDs, unless the list con-
tains another equivalent RD already. In either case,
the saliency of the RD equivalent to the set is set to
?+1 unless the RD is at the head of the list already.
4In the case of c-REX, graph-like structures are required.
5http://bnj.sourceforge.net/
6This is because concept FIG in REX-J is usually used to
refer to not a single piece but a shaped form (combined pieces).
241
D-ID Role Start End Referring expression Referents Intermediate structure
0801 SV 17.345 18.390 okkiisankaku big triangle 1 or 2 (okkii sankaku)
0801 SV 20.758 21.368 sore it 1 (sore)
0801 SV 23.394 24.720 migigawanookkiisankaku right big triangle 1 (migigawano okkii sankaku)
0801 SV 25.084 25.277 kore this 1 (kore)
0801 SV 26.512 26.671 sono that 1 (sono)
0801 SV 28.871 29.747 konookkiisankaku this big triangle 2 (kono okkii sankaku)
0801 OP 46.497 48.204 okkinasankakkei big triangle 1, 2 (okkina sankakkei)
0801 OP 51.958 52.228 ryo?ho? both 1, 2 (ryo?ho?)
?D-ID? means dialogue ID. ?Start? and ?End? mean the end points of a RE.
Table 1: Excerpt of the corpus annotation (w/ English literal translations).
Concept Words
TRI triangle, right triangle
SQR quadrate, square, regular tetragon
FIG figure, shape
Table 2: Dictionary (excerpted and translated in English).
Concept Relevancy values by piece(1) (2) (3) (4) (5) (6) (7)
TRI 1 1 1 1 1 0 0
SQR 0 0 0 0 0 1 0.8
FIG 0.3 0.3 0.3 0.3 0.3 0.3 0.3
Table 3: Static relevancy table.
Here, ? is the largest saliency value in the list at the
moment (the saliency value of the head RD).
Before each time of resolution, we check whether
the piece that is most recently manipulated after the
previous RE constitutes a perceptual group by using
the method explained in section 3.2.4 at the onset
time of the target RE. If such a group is recognized,
we add a new RD equivalent to the recognized group
unless the list contains another equivalent RD. In ei-
ther case, the saliency of the RD equivalent is set to
?+1 unless the RD is at the head of the list already,
and the focus of the equivalent RD is set to the most
recently manipulated piece.
When a new RD@m is added to the list, a comple-
mentary RD @n and a subsuming RD @l are also in-
serted just after @m in the list. Here, @n = @0\@m
and @l = [@m?,@n]. This operation is required to
handle a concept REST, e.g., ?the remaining pieces.?
3.2.4 Perceptual grouping
There is a generally available method of simulated
perceptual grouping (Tho?risson, 1994). It works
well in a spread situation such as shown in figure 1
but tends to produce results that do not match our
intuition when pieces are tightly packed at the end
of a dialogue. Therefore, we adopt a simple method
that recognizes a group when a piece is attached to
another. This method is less general but works sat-
isfactorily in the REX-J domain due to the nature of
the Tangram puzzle.
3.2.5 Ranking-based prediction model
As mentioned in section 2.3.3, a ranking-based
method (Iida et al, 2010) using SVMrank (Joachims,
2006) was adopted for constructing the prediction
model P (X|D). This model ranks entities accord-
ing to 16 binary features such as whether the tar-
get entity is previously referred to (a discourse fea-
ture), whether the target is under the mouse cursor
(a mouse cursor feature), etc.7
When a target is a set (i.e., a RD), discourse fea-
tures for it are computed as in the case of a piece;
meanwhile, mouse cursor features are handled in a
different manner. That is, if one of the group mem-
bers meets the criterion of a mouse cursor feature,
the group is judged as meeting the criterion.
In (Iida et al, 2010), preparing different models
for pronouns and non-pronouns achieved better per-
formance. Therefore we trained two linear kernel
SVM models for pronouns and non-pronouns with
the 24 dialogues.
3.3 Experiment
We used the 24 dialogues for evaluation.8 As men-
tioned in section 2.1, we focused on s-REX. These
24 dialogues contain 1,474 s-REX instances and 28
c-REX instances. In addition to c-REX, we ex-
cluded REs mentioning complicated concepts, for
which it is difficult to implement relevancy func-
tions in a short time.9 After excluding those REs,
7Following the results shown in (Iida et al, 2010), we did
not use the 6 manipulation-related features (CO1 . . . CO6).
8We used the same data to train the SVM-rank models. This
is equivalent to assuming that we have data large enough to sat-
urate the performance of the prediction model.
9Mostly, those are metaphors such as ?neck? and concepts
related to operations such as ?put.? For example, although
242
P (D) model Most-recent Mono-domain Uniform Linear Exponential
Category Single Plural Total Single Plural Total Single Plural Total Single Plural Total Single Plural Total
w/o S/P info. 42.4 28.8 40.0 77.5 47.3 73.3 77.1 40.6 72.0 78.3 45.1 73.7 76.2 48.4 72.3
w/ S/P info. 44.3 35.4 42.7 84.8 58.8 81.2 84.4 55.0 80.3 85.6 61.0 82.1 83.4 68.1 81.3
Table 4: Results of reference resolution (Accuracy in %).
1,310 REs were available. Out of the 1,310 REs, 182
REs (13.9%) refers to sets, and 612 REs (46.7%) are
demonstrative pronouns such as sore ?it.?
3.3.1 Settings
We presupposed the following conditions.
Speaker role independence: We assumed REs
are independent of speaker roles, i.e., SV and OP.
All REs were mixed and processed serially.
Perfect preprocessing and past information:
As mentioned in sections 3.1.1 and 3.2.3, we as-
sumed that no error comes from preprocessing in-
cluding speech recognition, morphological analysis,
and syntactic analysis;10 and all the correct referents
of past REs are known.11
No future information: In HH dialogue, some-
times information helpful for resolving a RE is pro-
vided after the RE is uttered. We, however, do not
consider such future information.
Numeral information: Many languages includ-
ing English grammatically require indication of nu-
meral distinctions by using such as articles, singu-
lar/plural forms of nouns and copulas, etc. Although
Japanese does not have such grammatical devices,12
it would be possible to predict such distinctions by
using a machine learning technique with linguistic
?putting a piece? and ?getting a piece out? are distinguished
due to speakers? intentions, they are (at least superficially) ho-
mogeneous in the physical data available from the corpus and
difficult for machines to distinguish each other.
10In general, the speech and expressions in human-machine
(HM) dialogue are less complex and less difficult to process
than those in HH dialogue data. This is typcially observed as
fewer disfluencies (Shriberg, 2001) and simpler sentences with
fewer omissions (Itoh et al, 2002). Therefore, when we apply
our framework to real DSs, we can expect clearer and simpler
input and thus better performance. We supposed that the condi-
tion of perfect preprocessing in HH dialogue approximates the
results to those obtained when HM dialogue data is used.
11If a reference is misinterpreted (i.e., wrongly resolved) in a
dialogue, usually that misinterpretation will be repaired by the
interlocutors in the succeeding interaction once the misinterpre-
tation becomes apparent. Therefore, accumulating all past er-
rors in resolution is rather irrational as an experimental setting.
12Japanese has a plurality marker -ra (e.g., sore-ra), but use
of it is not mandatory (except for personal pronouns).
and gestural information. Therefore we observed the
effect of providing such information. In the follow-
ing experiment we provide the singular/plural dis-
tinction information to REBNs by looking at the an-
notations of the correct referents in advance. This
is achieved by adding a special evidence node C0,
where D(C0) = {S,P}. P (C0 = S|X = x) = 1
and P (P|x) = 0 if x is a piece. On the contrary,
P (S|x) = 0 and P (P|x) = 1 if x is a set.
3.3.2 Baselines
To our best knowledge, there is no directly com-
parable method. We set up two baselines. The first
baseline uses the most recent as the resolved refer-
ent for each RE (Initial resolution of each dialogue
always fails). This baseline is called Most-recent.
As the second baseline, we prepared another
P (D) model in addition to those explained in sec-
tion 2.3.4, which is called Mono-domain. In Mono-
domain, D(D) consists of only a single RD @?0,
which contains individual pieces and the RDs recog-
nized up to that point in time. That is, @?0 = D(X).
Resolution using this model can be considered as
a straightforward extension of (Iida et al, 2010),
which enables handling of richer concepts in REs13
and handling of REs to sets14.
3.3.3 Results
The performance of reference resolution is pre-
sented by category and by condition in terms of ac-
curacy (# of correctly resolved REs/# of REs).
We set up the three categories in evaluating res-
olution, that is, Single, Plural, and Total. Category
Single is the collection of REs referring to a single
piece. Plural is the collection of REs referring to a
set of pieces. Total is the sum of them. Ambigu-
ous REs such as the first one in table 1 are counted
as ?Single? and the resolution of such a RE is con-
sidered correct if the resolved result is one of the
possible referents.
13(Iida et al, 2010) used only object types and sizes. Other
concepts such as LEFT were simply ignored.
14(Iida et al, 2010) did not deal with REs to sets.
243
?w/o S/P info.? indicates experimental results
without singular/plural distinction information. ?w/
S/P info.? indicates experimental results with it.
Table 4 shows the results of reference resolution
per P (D) modeling method.15 Obviously S/P infor-
mation has a significant impact.
While the best performance for category Single
was achieved with the Linear model, the best perfor-
mance for Plural was achieved with the Exponen-
tial model. If it is possible to know whether a RE
is of Single or Plural, that is, if S/P information is
available, we can choose a suitable P (D) model.
Therefore, by switching models, the best perfor-
mance of Total with S/P information reached 83.4%,
and a gain of 2.0 points against Mono-domain was
achieved (sign test, p < 0.0001).
Because the corpus did not include many in-
stances to which the notion of reference domains is
effective, the impact of RDs may appear small on the
whole. In fact, the impact was not small. By intro-
ducing RDs, resolution in category Plural achieved
a significant advancement. The highest gain from
Mono-domain was 9.3 points (sign test, p < 0.005).
Moreover, more REs containing positional concepts
such as LEFT and RIGHT were correctly resolved
in the cases of Uniform, Linear, and Exponential.
Table 5 summarizes the resolution results of four
positional concepts (with S/P information). While
Mono-domain resolved 65% of them, Linear cor-
rectly resolved 75% (sign test, p < 0.05).
As shown in table 4, the performance of the Uni-
form model was worse than that of Mono-domain.
This indicates that RDs introduced without an ap-
propriate management of them would be harmful
noise. Conversely, it also suggests that there might
be a room for improvement by looking deeply into
the management of RDs (e.g., forgetting old RDs).
4 Conclusion
This paper proposed a probabilistic approach to ref-
erence resolution, REBNs, which stands for Refer-
ring Expression Bayesian Networks. At each time
of resolution, a dedicated BN is constructed for the
15According to the results of preliminary experiments, even
in the case of the Uniform/Linear/Exponential models, we re-
solved the REs having demonstratives with the Mono-domain
model. This is in line with the finding of separating models
between pronouns and non-pronouns in (Iida et al, 2010).
Concept Count Mono Uni. Lin. Exp.
LEFT 21 11 12 16 13
RIGHT 33 23 23 25 27
UPPER 9 6 6 6 4
LOWER 6 5 4 5 4
Total 69 45 45 52 48
(Count means the numbers of occurrence of each concept. Mono, Uni.,
Lin., and Exp. correspond to Mono-domain, Uniform, Linear and Ex-
ponential.)
Table 5: Numbers of correctly resolved REs containing
positional concepts.
RE in question. The constructed BN deals with ei-
ther descriptive, deictic or anaphoric REs in a uni-
fied manner. REBNs incorporate the notion of ref-
erence domains (RDs), which enables the resolution
of REs with context-dependent attributes and han-
dling of REs to sets. REBNs are for task-oriented
dialogue systems and presuppose a certain amount
of domain-dependent manual implementation by de-
velopers. Therefore, REBNs would not be suited
to general text processing or non-task-oriented sys-
tems. However, REBNs have the potential to be a
standard approach that can be used for any and all
task-oriented applications such as personal agents in
smart phones, in-car systems, service robots, etc. ?
The proposed approach was evaluated with the
REX-J human-human dialogue corpus and promis-
ing results were obtained. The impact of incorpo-
rating RDs in the domain of the REX-J corpus was
recognizable but not so large on the whole. How-
ever, in other types of task domains where grouping
and comparisons of objects occur frequently, the im-
pact would be larger. Note that REBNs are not lim-
ited to Japanese, even though the evaluation used a
Japanese corpus. Evaluations with human-machine
dialogue are important future work.
Although this paper focused on the simple type of
REs without relations, REBNs are potentially able
to deal with complex REs with relations. The eval-
uation for complex REs is necessary to validate this
potential of REBN. Currently REBN assumes REs
whose referents are concrete entities. An extension
for handling abstract entities (Byron, 2002; Mu?ller,
2007) is important future work. Another direction
would be generating REs with REBNs. A generate-
and-test approach is a naive application of REBN
for generation. More efficient method is, however,
necessary.
244
References
John D. Burger and Dennis Connoly. 1992. Probabilistic
resolution of anaphoric reference. In Proceedings of
the AAAI Fall Symposium on Intelligent Probabilistic
Approaches to Natural Language, pages 17?24.
Donna Byron. 2002. Resolving pronominal reference
to abstract entities. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 80?87.
Eugene Charniak and Robert Goldman. 1989. A se-
mantics for probabilistic quantifier-free first-order lan-
guages with particular application to story understand-
ing. In Proceedings of the Eleventh International Joint
Conference on Artificial Intelligence (IJCAI), pages
1074?1079, Menlo Park, CA, USA.
Sehyeong Cho and Anthony Maida. 1992. Using a
Bayesian framework to identify the referent of definite
descriptions. In Proceedings of the AAAI Fall Sympo-
sium on Intelligent Probabilistic Approaches to Natu-
ral Language, pages 39?46.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretations of the Gricean maxims in the generation
of referring expressions. Cognitive Science, 18:233?
263.
Robert Dale and Jette Viethen. 2009. Referring expres-
sion generation through attribute-based heuristics. In
Proceedings of the the 12th European Workshop on
Natural Language Generation (ENLG), pages 59?65,
Athens, Greece, March.
Alexandre Denis. 2010. Generating referring expres-
sions with reference domain theory. In Proceedings
of the 6th International Natural Language Generation
Conference (INLG), pages 27?35.
Petra Gieselmann. 2004. Reference resolution mech-
anisms in dialogue management. In Proceedings of
the 8th workshop on the semantics and pragmatics of
dialogue (CATALOG), pages 28?34, Barcelona, Italy,
July.
Ryu Iida, Shumpei Kobayashi, and Takenobu Tokunaga.
2010. Incorporating extra-linguistic information into
reference resolution in collaborative task dialogue. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1259?
1267, Uppsala, Sweden, July.
Toshihiko Itoh, Atsuhiko Kai, Tatsuhiro Konishi, and
Yukihiro Itoh. 2002. Linguistic and acoustic changes
of user?s utterances caused by different dialogue situa-
tions. In Proceedings of the 7th International Confer-
ence on Spoken Language Processing (ICSLP), pages
545?548.
Finn V. Jensen and Thomas D. Nielsen. 2007. Bayesian
Networks and Decision Graphs. Springer, second edi-
tion.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD), pages
217?226, Philadelphia, PA, USA, August.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer Academic Publishers.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29:53?72.
Pierre Lison, Carsten Ehrler, and Geert-Jan M. Kruijff.
2010. Belief modelling for situation awareness in
human-robot interaction. In Proceedings of the 19th
International Symposium on Robot and Human In-
teractive Communication (RO-MAN), pages 138?143,
Viareggio, Italy, September.
Ruslan Mitkov. 2002. Anaphora Resolution. Studies in
Language and Linguistics. Pearson Education.
Christoph Mu?ller. 2007. Resolving it, this, and that in
unrestricted multi-party dialog. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 816?823.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July.
Judea Pearl. 1988. Probabilistic Reasoning in Intelli-
gent Systems: Networks of Plausible Inference. Mor-
gan Kaufmann, San Mateo, CA, USA.
Matthew Richardson and Pedor Domingos. 2006.
Markov logic networks. Machine Learning, 62(1?
2):107?136.
Deb K. Roy. 2002. Learning visually-grounded words
and syntax for a scene description task. Computer
Speech and Language, 16(3):353?385.
Susanne Salmon-Alt and Laurent Romary. 2000. Gen-
erating referring expressions in multimodal contexts.
In Proceedings of the INLG 2000 workshop on Coher-
ence in Generated Multimedia, Mitzpe Ramon, Israel,
June.
Susanne Salmon-Alt and Laurent Romary. 2001. Ref-
erence resolution within the framework of cognitive
grammar. In Proceedings of the International Col-
loqium on Cognitive Science, San Sebastian, Spain,
May.
Elizabeth Shriberg. 2001. To ?errrr? is human: ecology
and acoustics of speech disfluencies. Journal of the
International Phonetic Association, 31(1):153?169.
Philipp Spanger, Masaaki Yasuhara, Ryu Iida, Takenobu
Tokunaga, Asuka Terai, and Naoko Kuriyama. 2010.
REX-J: Japanese referring expression corpus of sit-
uated dialogs. Language Resources and Evaluation.
Online First, DOI: 10.1007/s10579-010-9134-8.
245
Kristinn R. Tho?risson. 1994. Simulated perceptual
grouping: An application to human-computer interac-
tion. In Proceedings of the 16th Annual Conference
of the Cognitive Science Society, pages 876?881, At-
lanta, GA, USA.
Davy Weissenbacher and Adeline Nazarenko. 2007. A
Bayesian approach combining surface clues and lin-
guistic knowledge: Application to the anaphora reso-
lution problem. In Proceedings of the International
Conference Recent Advances in Natural Language
Processing (RANLP), Borovets, Bulgaria.
Davy Weissenbacher. 2005. A Bayesian network for the
resolution of non-anaphoric pronoun it. In Proceed-
ings of the NIPS 2005 Workshop on Bayesian Meth-
ods for Natural Language Processing, Whistler, BC,
Canada.
A Algorithm to compose P (C|X,D)
Algorithm 1 Composing P (C|X = x,D = d).
Input: D(C); R(c, x, d) for all c ? D(C)\{?}
Output: P (C|X = x,D = d)
1: n ? 0, s ? 0, S = D(C)\{?}
2: for all c ? S do
3: r[c] ? R(c, x, d) #{Relevancy of concept c}
4: s ? s+ r[c] #{Sum of relevancy r[c]}
5: n ? n+ (1? r[c]) #{Sum of residual (1? r[c])}
6: end for
7: r[?] ? n/|S|
8: s ? s+ r[?]
9: for all c ? D(C) do
10: P (C = c|X = x,D = d) ? r[c]/s
11: end for
(#{. . . } is a comment.)
B Relevancy functions
As explained in section 2.3.2, the relevancy func-
tions for positional concepts such as LEFT and
RIGHT were implemented as geometric calcula-
tions. Here several other relevancy functions are
shown with corresponding example REs.
?this figure?:
R(FIG, x, d)
=
?
?
?
0.3 : if single(x)
1 : if not single(x) and shape(x)
0 : otherwise
(single(x) means x is a single piece. shape(x)
means x is a set of pieces that are concatenated and
form a shape. 0.3 comes from the static relevancy
table.)
?both the triangles?:
R(BOTH, x, d) =
{
1 : if |x| = 2
0 : otherwise
?another one?:
R(ANOTHER, x, d) =
{
1 : if foc(d) 6= x
0 : otherwise
?the remaining ones?:
R(REST, x, d) =
{
1 : if d = [x, y?]
0 : otherwise
(REST requires |d| = 2, and both x and y are sets.
ANOTHER does not.)
?all?:
R(ALL, x, d) =
{
1 : if x = d
0 : otherwise
(ALL does not always refer to @0.)
246
Proceedings of the SIGDIAL 2013 Conference, pages 70?77,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Generating More Specific Questions for Acquiring Attributes of
Unknown Concepts from Users
Tsugumi Otsuka?, Kazunori Komatani?, Satoshi Sato?, Mikio Nakano?
? Graduate School of Engineering, Nagoya University, Nagoya, Aichi 464?8603, Japan
? Honda Research Institute Japan Co., Ltd., Wako, Saitama 351?0114, Japan
{t ootuka,komatani,ssato}@nuee.nagoya-u.ac.jp, nakano@jp.honda-ri.com
Abstract
Our aim is to acquire the attributes of con-
cepts denoted by unknown words from
users during dialogues. A word unknown
to spoken dialogue systems can appear in
user utterances, and systems should be ca-
pable of acquiring information on it from
the conversation partner as a kind of self-
learning process. As a first step, we pro-
pose a method for generating more spe-
cific questions than simple wh-questions
to acquire the attributes, as such ques-
tions can narrow down the variation of
the following user response and accord-
ingly avoid possible speech recognition er-
rors. Specifically, we obtain an appropri-
ately distributed confidence measure (CM)
on the attributes to generate more specific
questions. Two basic CMs are defined us-
ing (1) character and word distributions in
the target database and (2) frequency of
occurrence of restaurant attributes on Web
pages. These are integrated to comple-
ment each other and used as the final CM.
We evaluated distributions of the CMs by
average errors from the reference. Re-
sults showed that the integrated CM out-
performed the two basic CMs.
1 Introduction
In most spoken dialogue systems, knowledge
bases for the systems are constructed off-line. In
other words, they are not updated during dia-
logues. On the other hand, humans update their
knowledge not only by reading books but also
through interaction with other people. When they
encounter an unknown word during conversations,
humans notice that it is new to them and acquire
knowledge about it by asking their conversational
partner. This self-learning process is one of the
Tell me about ?Osteria Liu?. I don?t know that restaurant.Is it ?Italian?? 
Tell me about?Toyo?. I don?t know that restaurant.What type of cuisine is it?
SystemUser
Figure 1: Example of simple and specific ques-
tions.
most intelligent features of humans. We think that
applying this intelligent feature to spoken dialogue
systems will make them more usable.
We present a method that generates appropri-
ate questions in order to acquire the attributes of
a concept that an unknown word denotes when it
appears in a user utterance. Here, we define un-
known words as those whose attributes necessary
for generating responses were not defined by the
system developer; that is, unknown to the response
generation module in the spoken dialogue system.
The system cannot reply to user utterances includ-
ing such words even if they are correctly recog-
nized by its automatic speech recognition (ASR)
module.
Questions to the user to acquire the attribute
should be specific. In spoken dialogue sys-
tems, specific questions are far preferable to wh-
questions because they can narrow down varia-
tions of the following user response. Such ques-
tions lead to a better ASR performance of the re-
sponse and reduce the risk that it includes new
other unknown words.
Two example dialogues are shown in Figure 1.
Since our target task is restaurant database re-
trieval, we set the unknown words as restaurant
names and the attribute as their cuisine in our
restaurant database. In the examples shown, the
system uses a simple wh-question (the upper part)
and a specific Yes-No question (the lower part) to
obtain cuisine types. Here, ?Toyo? and ?Osteria
Liu? are restaurant names. We assume that the
70
Table 1: Question types according to the number of cuisines (num).
num Question form Example
1 Yes-No question Is it cuisine c1?
2 Alternative question Which cuisine is it, c1 or c2?
3 3-choice question Which cuisine is it, c1, c2, or c3?
?4 Wh-question What cuisine is it?
system already knows these are restaurant names
but does not know its attributes such as its cuisine
type. The system uses a wh-question for ?Toyo?
since no clue is obtained for it. In contrast, since
?Osteria Liu? contains information on cuisines in
the name itself, a concrete Yes-No question is used
to ask whether the cuisine is ?Italian?.
We propose a method for providing a well-
distributed confidence measure (CM) to generate
more specific questions. For this purpose, we esti-
mate the cuisine type of a restaurant from its name,
which is assumed to be unknown to the system.
There have been many previous studies that esti-
mate word and character attributes using Web in-
formation (Pasca et al, 2006; Yoshinaga and Tori-
sawa, 2007). Our two estimation methods are rel-
atively simpler than these studies, since our main
focus is to generate more concrete questions on the
basis of appropriate CMs. That is, the CMs should
be high when the system seems to correctly esti-
mate a cuisine type and low when the estimation
seems difficult.
We assume a restaurant name as the input; that
is, we suppose that the system can recognize the
restaurant name in the user?s utterance correctly
by its ASR module and understand it is a restau-
rant name by its LU module. Nevertheless, it
still remains unknown to its response generation
module. This is a feasible problem when using
a large vocabulary continuous speech recognition
(LVCSR) engine containing over several million
words (Jyothi et al, 2012) and a statistical named
entity (NE) tagger (Tjong Kim Sang and Meul-
der, 2003; Zhou and Su, 2002; Ratinov and Roth,
2009).
The problem we tackle in this paper is differ-
ent from trying to estimate the NE class of an un-
known word (Takahashi et al, 2002; Meng et al,
2004). We assume the system already knows that
it is a restaurant name. Rather, we try to acquire
the attribute (e.g., cuisine type) of the concept of
the unknown word, which is required for generat-
ing responses about the restaurant in subsequent
dialogues.
2 Generating Questions Based on CM
The system determines a question type on the ba-
sis of CM. The CM is estimated for each cuisine
type cj in the target database. In this paper, the
number of cuisine types is 16, all of which are
in our restaurant database; that is, cj ? C and
|C| = 16.
Table 1 shows the four question types and their
examples. These are determined by parameter
num, which is the number of cuisine types that
should be included in the question. If the sys-
tem obtains one cuisine type that it is very con-
fident about and thus has a high CM, it should
generate the most specific question, i.e., a Yes-
No question; in this case, the number should be
1. In contrast, if unreliable cuisine types are ob-
tained, which means lower CMs, the system gen-
erates questions including several cuisine types.
The num can be determined by Equation (1):
num = min(n) s.t.
n?
j=1
CM(cj) > ?, (1)
where CM(cj) is a confidence measure for cui-
sine type cj in its descending order. ? is a constant
and can be manually decided considering the dis-
tribution of CM(cj). This equation means that if
only the CM(c1) is greater than ? (i.e., n = 1),
the system generates a specific question includ-
ing only cuisine type c1, while if the total from
CM(c1) toCM(c4) is smaller than ? (i.e., n = 4),
the system does not use estimated cuisine types
and instead generates a wh-question.
If the CM on the cuisine type is well-distributed,
the system can generate appropriate questions. In
the following section, methods to obtain such CMs
are explained.
3 Estimating Cuisine Types and
Calculating CM
The final CM is obtained by integrating two ba-
sic CMs. The system then uses this final CM to
71
Feature	 ?selec?n	 ?by	 ?mutual	 ?informa?on	 ?
Training	data	
?	 		
	 ?	 ?	 ?	 ?	 ?	 ?	
DB	
Japanese pub	
Sushi Goichi (?? ??)	
Koikoi (????)	
Japanese restaurant	Tanaka Sushi (????)?	
Maru Sushi (????) 	 Japanese restaurant	
Japanese restaurant	
Restaurant name	 Cuisine	
. . . .	 . . . .	
Quinci CENTRARE	 Italian	
Hyakuraku (??)	 Chinese restaurant?	
C?s ave cafe	 Cafe 	
Classifier:	 ?Maximum	 ?entropy	 ?(ME)	 ?model	
	 ?Azuma	 ?Sushi	 ?	 ?(???)	
Input:	 ?	 ?Restaurant	 ?name	
Output:	 ?CMD	
Japanese restaurant 	Japanese pub	Cafe	. . . . .  	
: 0.9	: 0.05	: 0.0006		
Figure 3: Overview of CMD calculation.
Web	
Es?ma?on	 ?from	 ?DB	 ?
Es?ma?on	 ?from	 ?Web	 ? CM	 ?Integra?on	 ?
DB	
CMW	 ?
Restaurant	 ?name	 ?
CMI	 ?
CMD	 ?
Question generation	based on CM	
Figure 2: Process overview.
generate questions. The two basic CM estimation
methods are:
1. Using word and character distribution in the
target database
2. Using frequency of the restaurant attributes
on the Web
A process overview of the proposed method is
shown in Figure 2. Its input to the system is an
unknown restaurant name and its output is the es-
timated CMs. The system generates questions on
the basis of the estimated CMs, which are calcu-
lated for each cuisine type.
3.1 Attribute Estimation Using Word and
Character Distribution in Database
We estimate the cuisine types of an unknown
restaurant by using the word and character distri-
bution in the target database. The target database
contains many pairs of restaurant names and cui-
sine types. The estimation is performed by us-
ing supervised machine learning trained with the
pairs. The overview of calculating CMD is shown
in Figure 3. This approach is based on our in-
tuition that some cuisine types can be estimated
from restaurant names on the basis of their char-
acter types or typical character sequences they
contain. For example, a restaurant name com-
posed of only katakana1 is probably a French or
Italian restaurant because words imported from
other countries to Japan are called ?katakana loan-
words? and are written in katakana characters
(Kay, 1995).
We use the maximum entropy (ME) model
(Berger et al, 1996) as a classifier. Its posterior
probability p(cj |si) is used as a CMD denoting
the CM estimated using a database. CMD is cal-
culated as
CMD(si, cj) = p(cj |si)
= 1Z exp
[
~?(cj) ? ~?(si)
]
, (2)
where si is a restaurant name, cj (? C) is a
cuisine type, ~?(si) is a feature vector obtained
from a restaurant name, ~?(cj) is a weight vector,
and Z is a normalization coefficient that ensures?
cj CMD(si, cj) = 1.We use three types of feature vectors obtained
from each restaurant name:
? Character n-grams (n = 1, 2, 3)
? Words
? Character types
The feature values of the character n-gram and the
word are scored as 1 if such features are contained
in the restaurant name. The Japanese morpholog-
ical analyzer Mecab (Kudo et al, 2004) with the
IPADIC dictionary is used to segment restaurant
names into word sequences. The character type
1Katakana is a Japanese syllabary. There are three kinds
of characters in Japanese. Kanji (Chinese character) are lo-
gograms and hiragana and katakana are syllabaries. Katakana
is mainly used for writing imported words and hiragana is
used for writing original Japanese words.
72
Web	 ?page	 Cuisine	 ?frequency	 ?Japanese restaurant	Italian restaurant	Western-style restaurant	
74  times	  8   times	  1  time	
Output:	 ?CMW	Japanese restaurant	Italian restaurant	Western-style restaurant	. . . .  	
??0.8	??0.11	??0.0009		
Azuma	 ?Sushi	 ?(???)	Input:	 ?	 ?Restaurant	 ?name	
	 ?Obtaining	 ?related	 ?pages	 ?	 ?about	 ?target	 ?restaurant	1. 
	 ?	 ?	 ?Calcula?ng	 ?Pfreq(cj)	2. 
	 ?	 ?Scaling	 ?Pfreq(cj)	3. 
Yahoo!	 ?Web	 ?search	 ?API 
?Azuma	 ?Sushi?	 ?	 ?	 ?Aichi	 ?	 ?	 ?restaurant	 ?(????????????)	 ?
Ranking:  2nd	
Search	 ?query	 ?	 ?
Number of 	cuisine types:  	3 
Figure 4: Overview of CMW calculation.
is represented by the four character types used in
the Japanese writing system: hiragana, katakana,
kanji (Chinese characters), and romaji (Roman let-
ters). For example, the restaurant name ?Maru
Sushi (????)? includes two character types:
?Maru (??)? is written in hiragana and ?Sushi
(??)? is written in kanji. Therefore, the fea-
ture values for hiragana and kanji are both 1, while
those for katakana and romaji are 0. Another ex-
ample is shown using the restaurant ?IB cafe (IB
???)?, in which the ?IB? part is romaji and the
?cafe (???)? part is katakana. Therefore, in this
case, the feature values of katakana and romaji are
1 and those of hiragana and kanji are 0.
We perform feature selection for the obtained
features set (Guyon and Elisseeff, 2003). The clas-
sifier needs to be built without overfitting because
we assume that a restaurant name as the input to
this module is unknown and does not exist in the
database. We use the mutual information (Peng
et al, 2005; Yang and Pedersen, 1997) between
each feature and the set of cuisine types as its cri-
terion. This represents how effective each feature
is for the classification. For example, in the fea-
tures obtained from the restaurant name ????
??, which is a Japanese restaurant, the 2-gram
feature ???? frequently co-occurs with the cui-
sine type ?Japanese restaurant?. This is an effec-
tive feature for the cuisine type estimation. In con-
trast, the 2-gram feature ???? is not effective be-
cause its co-occurrence with cuisine types is infre-
quent. Mutual information is calculated as
I(fk;C) =
?
cj?C
p(fk, cj) log
p(fk, cj)
p(fk)p(cj)
, (3)
where p(fk) is an occurrence probability of feature
fk in the database, p(cj) is an occurrence probabil-
ity of cuisine type cj (? C), and p(fk, cj) is a joint
probability of the feature and the cuisine type.
Features having lower mutual information val-
ues are removed until we deem that overfitting has
been avoided, specifically, when the estimation
accuracies become almost the same between the
closed and open tests. We confirm this by cross-
validations (CV) instead of open tests.
3.2 Estimation Using the Web
We estimate a restaurant?s cuisine type and calcu-
late CMs by using its frequency on the Web as
CMW . This is based on an assumption that a
restaurant?s name appears with its cuisine type on
Web pages. CMW is calculated in the following
steps, as shown in Figure 4.
1. Obtaining related Web pages:
Twenty pages per search query were ob-
tained, as this was the limit of the number of
pages when this experiment was performed.
We used the Yahoo! Web search API2. The
query is formed with the target restaurant
name and the following two words: ?Aichi
(??)? and ?restaurant (?????)?. The
two are added to narrow down the search re-
sult since our domain is a restaurant search
in Aichi prefecture. For example, the query
is ?<rest>???????? for the target
restaurant name <rest>.
2http://developer.yahoo.co.jp/webapi/search/websearch
/v2/web search.html
73
2. Calculating Pfreq(cj):
We count the frequency of each cuisine type
cj in the i-th Web pages, which are ranked
by the Web search API. We then sum up the
frequency through all the obtained pages and
calculate its posterior probability.
Pfreq(cj) =
?
i wi ? freqi(cj)?
cj
?
i wi ? freqi(cj)
(4)
Here, freqi(cj) is the frequency of cj in the
i-th page. Weight wi is calculated using two
factors, rank(i) and cuisine(i):
wi =
1
rank(i) ? cuisine(i) (5)
(a) rank(i): The ranking of pages in the
Web search API
We assume that a Web page is more re-
lated to the target restaurant if the Web
search API ranks it higher.
(b) cuisine(i): The number of cuisine
types in the i-th Web page
We assume that a Web page contain-
ing many different cuisine types does
not indicate one particular cuisine. For
example, a page on which only ?Chi-
nese restaurant? appears is more reliable
than that on which more cuisine types
(?Chinese restaurant?, ?Japanese restau-
rant?, ?Japanese pub?, and ?Western-
style restaurant?, for example) appear,
as a page indicating a ?Chinese restau-
rant?.
3. Scaling Pfreq(cj):
CMW is calculated by scaling each
Pfreq(cj) with the corresponding ?j . ?j
is a scaling coefficient that emphasizes the
differences among CMW : ?j is equal to
or smaller than 1 and becomes smaller as j
increases.
CMW (cj) =
?jPfreq(cj)?
cj ?jPfreq(cj)
(6)
?j = Pfreq(cj)/Pfreq(c1) (7)
3.3 Integration of CMs
We define CMI by integrating the two basic CMs:
CMD and CMW . Specifically, we integrate them
by the logistic regression (Hosmer Jr. et al, 2013)
shown in Equation (8). The optimal parameters,
i.e., weights for the CMs, are determined using a
data set with reference labels. The teacher signal
is 1 if the estimated cuisine type is correct and 0
otherwise.
CMI(cj) =
1
1 + exp(?f(cj))
(8)
f(cj) = wDCMD(cj) + wWCMW (cj) + w0
Here, wD and wW are the weights for CMD and
CMW , and w0 is a constant.
4 Experiment
We evaluate our method to obtain the CMs from
three aspects. First, we evaluate the effect of fea-
ture selection based on mutual information. Sec-
ond, we evaluate how the CMs were distributed
and whether they were appropriate measures for
question generation. Third, we determine the ef-
fectiveness of integrating the two basic CMs. In
this paper, we used a restaurant database in Aichi
prefecture containing 2,398 restaurants with 16
cuisine types.
4.1 Effect of Feature Selection Based on
Mutual Information
We determined whether overfitting could be
avoided by feature selection based on mutual in-
formation in the estimation using a database. We
regard overfitting to be avoided when estimation
accuracies become almost the same between the
closed and open tests. For the closed test, estima-
tion accuracy was calculated for all 2,398 restau-
rants in the database by using a classifier that was
trained with the same 2,398 restaurants. For the
open test, it was calculated by 10-fold CV for the
2,398 restaurants. This experiment is not for de-
termining a feature set but rather for determining
a feature selection ratio. That is, the feature se-
lection result is kept not as a feature set but as a
ratio. The resulting ratio is applied to the num-
ber of features appearing in another training data
(e.g., that in Section 4.2) and then the feature set
is determined.
Figure 5 shows the estimation accuracy of the
closed test and the 10-fold CV when the feature
selection was applied. The horizontal axis denotes
ratios of features used to train the classifier out of
20,679 features in total. They were selected in de-
scending order of mutual information. The ver-
tical axis denotes the estimation accuracy of the
74
 0.6
 0.7
 0.8
 0.9
 1
 1 10 100E
stim
ati
on
 ac
cu
rac
y (
%)
Feature selection ratio (%)
Closed10-fold CV
Figure 5: Estimation accuracies of closed test and
10-fold CV.
cuisine types. Figure 5 shows that, at first, over-
fitting occurs if all features were used for training;
that is, the feature selection ratio = 100%. This
can be seen by the difference in estimation accu-
racies, which was 28.1% between the closed test
and the 10-fold CV. The difference decreased as
the number of used features decreased, and almost
disappeared at feature selection ratio = 0.8%. In
these selected features, as an example, the 2-gram
?gyoza (??)?, which seems intuitively effective
for cuisine type estimation is, included3.
4.2 Evaluation for Distribution of CMs
We evaluate the distribution of CMs obtained with
the estimation results. Specifically, we evaluated
three types of distributions: CMD, CMW , and
CMI . We extracted 400 restaurants from the
database and used them as evaluation data. The
remaining 1,998 restaurants were used as training
data for the classifier to calculate CMD. In all
features obtained from these 1,998 restaurants, the
ME classifier uses 0.8% of them, which is the fea-
ture selection ratio based on the mutual informa-
tion determined in Section 4.1. That is, the feature
set itself obtained in the feature selection is not de-
livered into the evaluation in this section.
We used average distances between each CM
score and its reference as the criterion to evalu-
ate the distribution of the CMs. Generally, CMs
should be as highly scored as possible when the
estimation is correct and as lowly scored as possi-
ble otherwise. We calculate the distances over the
3?Gyoza (??)? is a kind of dumplings and one of the
most popular Chinese foods. It often appears in Chinese
restaurant names in Japan.
Table 3: Evaluation against each CM.
eval(CMx) MB(CMx)
CMD 0.31 0.37
CMW 0.28 0.32
CMI 0.25 0.28
400 estimation results.
eval(CMx) =
?N
i |CM ix ? ?ix|
N (9)
Here, N is the total number of the estimation re-
sult, so N = 400 in this paper. ?ix for CM ix is
defined as
?ix =
{
1, If estimation result i is correct
0, Otherwise (10)
Note that ?x depends on CMx because estimation
results differ depending on the CMx used.
We also set the majority baseline as Equation
(11). Here, all CMs are regarded as 0 or 1 in Equa-
tion (9). Because there were more correct estima-
tion results than incorrect ones, as shown in Table
2, we used 1 for the majority baseline, as
MB(CMx) =
?N
i |1 ? ?ix|
N . (11)
The results are shown in Table 3. A compar-
ison of the three eval(CMx) demonstrates that
the integrated CMI is the most appropriate in our
evaluation criterion because it is the lowest of the
three. The relative error reduction rates fromCMI
against CMD and CMW were 16% and 37%, re-
spectively. Each eval(CMx) outperformed the
corresponding majority baseline.
4.3 Effectiveness of Integrated CM
We verify the effectiveness of the CM integration
from another viewpoint. Specifically, we confirm
whether the number of correct estimation results
increases by integration.
First, we show the distribution of the three CMs
and whether they were correct or not in Table 2.
The bottom row of the table shows that CMI ob-
tained correct estimation results for 297 restau-
rants, which is the highest of the three CMs.
More specifically, we investigated how many
estimation results changed by using the three
CMs. Here, an estimation result means the cui-
sine type that is given the highest confidence. This
result is shown in Table 4, where C denotes a case
75
Table 2: Distribution of estimation results by CM values.
CMD CMW CMI
CM range Correct Incorrect Correct Incorrect Correct Incorrect
0.0 ? 0.1 0 0 0 32 2 10
0.1 ? 0.2 0 0 0 11 9 15
0.2 ? 0.3 1 16 14 22 15 18
0.3 ? 0.4 6 19 28 19 10 8
0.4 ? 0.5 11 25 29 21 13 12
0.5 ? 0.6 21 29 56 9 13 12
0.6 ? 0.7 22 28 85 7 15 7
0.7 ? 0.8 41 16 42 3 17 6
0.8 ? 0.9 21 9 19 1 19 9
0.9 ? 1.0 131 4 1 1 184 10
Total 254 146 274 124 297 103
Table 4: Estimation results by three CMs.
CMD / CMW
I / I I / C C / I C / C
C 0 51 33 213
CMI I 85 10 8 0
C: correct, I: incorrect
when a cuisine type was correctly estimated and I
denotes that it was not. The four columns with ?/?
denote the numbers of estimation results forCMD
and CMW . For example, the C/I column denotes
that estimation results based on the database were
correct and those using the Web were incorrect,
that is, the I/C and C/I columns mean that the
two estimation results differed. The table shows
that 102 of 400 restaurants corresponded to these
cases, that is, either of the two estimation results
was incorrect. It also shows that estimation results
for 84 of the 102 (82%) restaurants became correct
by the integration.
Two examples are shown for which the esti-
mation results became correct by the integration.
First, ?Kaya (??)? is a restaurant name whose
cuisine type is ?Japanese-style pancake?. Its cui-
sine type was correctly estimated by CMW while
it was incorrectly estimated as ?Japanese pub? by
CMD. This was because, in Japanese, ?Kaya (?
?)? has no special meaning associated with spe-
cific cuisine types. Thus, it is natural that its cui-
sine type was incorrectly estimated from the word
and character distribution of the name. On the
other hand, when Web pages about it were found,
?Japanese-style pancake? co-occurs frequently in
the obtained pages, and thus it was correctly es-
timated by CMW . Second, ?Tama-Sushi Imaike
(??? ??)? is a restaurant name whose cui-
sine type is ?Japanese restaurant?. Its cuisine type
was estimated correctly by CMD while it was in-
correctly estimated as ?Japanese pub? by CMW .
CMD was effective in this case because the part
of ?Sushi (??)? indicates a Japanese cuisine. No
Web pages for it were found indicating its cuisine
type correctly, and thus CMW failed to estimate
it.
5 Conclusion
Our aim is to acquire the attributes of an unknown
word?s concept from the user through dialogue.
Specifically, we set restaurant cuisine type as the
attribute to obtain and showed how to generate
specific questions based on the estimated CM. We
use two estimation methods: one based on the tar-
get database and the other on the Web. A more
appropriate CM was generated in terms of its dis-
tribution and estimation accuracy by integrating
these two CMs.
There is little prior research on obtaining and
updating system knowledge through dialogues,
with the notable exception of the knowledge au-
thoring system of (Knott and Wright, 2003). Their
system also uses the user?s text input for construct-
ing the system knowledge from scratch, which is
used to generate simple stories. Our study is dif-
ferent in two points: (1) we focus on generating
several kinds of questions because we use ASR,
and (2) we try to handle unknown words, which
will be stored in the target database to be used in
future dialogues.
We should point out that these kinds of ques-
tions can be generated only when the types of un-
known concepts are given. We assume the type
of unknown concepts is already known and thus
the attributes to be asked are also known. More
specifically, we assume that the concept denoted
by an unknown word is a restaurant name and its
attributes are also known. The cuisine type has
been estimated as one of the attributes. However,
76
when the type is unknown, the system first needs
to identify its attributes to ask. That is, the sys-
tem first needs to ask about its supertype and then
to ask about attributes that are typical for objects
of this type. This issue needs to be addressed in
order for the system to acquire arbitrary new con-
cepts. This paper has shown the first step for ob-
taining concepts through dialogues by generating
questions. Many issues remain in this field for fu-
ture work.
References
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Isabelle Guyon and Andre? Elisseeff. 2003. An intro-
duction to variable and feature selection. The Jour-
nal of Machine Learning Research, 3:1157?1182.
David W. Hosmer Jr., Stanley Lemeshow, and Rod-
ney X. Sturdivant. 2013. Applied logistic regres-
sion. Wiley. com.
Preethi Jyothi, Leif Johnson, Ciprian Chelba, and Brian
Strope. 2012. Large-scale discriminative language
model reranking for voice search. In Proceedings
of the NAACL-HLT 2012 Workshop: Will We Ever
Really Replace the N-gram Model? On the Future
of Language Modeling for HLT, pages 41?49.
Gillian Kay. 1995. English loanwords in Japanese.
World Englishes, 14(1):67?76.
Alistair Knott and Nick Wright. 2003. A dialogue-
based knowledge authoring system for text genera-
tion. In AAAI Spring Symposium on Natural Lan-
guage Generation in Spoken and Written Dialogue,
Stanford, CA, pages 71?78.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2004), pages 230?237.
Helen Meng, P. C. Ching, Shuk Fong Chan, Yee Fong
Wong, and Cheong Chat Chan. 2004. ISIS:
An adaptive, trilingual conversational system with
interleaving interaction and delegation dialogs.
ACM Transactions on Computer-Human Interac-
tion, 11(3):268?299.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, and Alpa Jain. 2006. Organizing and
searching the World Wide Web of facts - step one:
the one-million fact extraction challenge. In Pro-
ceedings of the 21st National Conference on Artifi-
cial intelligence - Volume 2, AAAI ?06, pages 1400?
1405. AAAI Press.
Hanchuan Peng, Fuhui Long, and Chris Ding. 2005.
Feature selection based on mutual information cri-
teria of max-dependency, max-relevance, and min-
redundancy. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 27(8):1226?1238.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, pages 147?
155.
Yasuhiro Takahashi, Kohji Dohsaka, and Kiyoaki
Aikawa. 2002. An efficient dialogue control
method using decision tree-based estimation of out-
of-vocabulary word attributes. In Proc. Int?l Conf.
Spoken Language Processing (ICSLP), pages 813?
816.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142?147.
Yiming Yang and Jan O Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In ICML, volume 97, pages 412?420.
Naoki Yoshinaga and Kentaro Torisawa. 2007.
Open-domain attribute-value acquisition from semi-
structured texts. In Proceedings of the Workshop
of OntoLex07 - From Text to Knowledge: The Lexi-
con/Ontology Interface, pages 55?66.
Guo Dong Zhou and Jian Su. 2002. Named entity
recognition using an HMM-based chunk tagger. In
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 473?
480.
77
Proceedings of the SIGDIAL 2013 Conference, pages 369?371,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
A Robotic Agent in a Virtual Environment that Performs Situated
Incremental Understanding of Navigational Utterances
Takashi Yamauchi
Seikei University
3-3-1 Kichijoji-Kitamachi
Musashino, Tokyo, Japan
dm126222@cc.seikei.ac.jp
Mikio Nakano
Honda Research Institute
Japan Co., Ltd.
8-1 Honcho, Wako
Wako, Saitama, Japan
nakano@jp.honda-ri.com
Kotaro Funakoshi
Honda Research Institute
Japan Co., Ltd.
8-1 Honcho, Wako
Wako, Saitama, Japan
funakoshi@jp.honda-ri.com
Abstract
We demonstrate a robotic agent in a 3D
virtual environment that understands hu-
man navigational instructions. Such an
agent needs to select actions based on not
only instructions but also situations. It
is also expected to immediately react to
the instructions. Our agent incrementally
understands spoken instructions and im-
mediately controls a mobile robot based
on the incremental understanding results
and situation information such as the lo-
cations of obstacles and moving history. It
can be used as an experimental system for
collecting human-robot interactions in dy-
namically changing situations.
1 Introduction
Movable robots are ones that can execute tasks
by moving around. If such robots can understand
spoken language navigational instructions, they
will become more useful and will be widely used.
However, spoken language instructions are some-
times ambiguous in that their meanings differ de-
pending on the situations such as robot and obsta-
cle locations, so it is not always easy to make them
understand spoken language instructions. More-
over, when they receive instructions while they are
moving and they understand instructions only af-
ter they finish, accurate understanding is not easy
since the situation may change during the instruc-
tion utterances.
Although there have been several pieces of
work on robots that receive linguistic navigational
instructions (Marge and Rudnicky, 2010; Tellex et
al., 2011), they try to understand instructions be-
fore moving and they do not deal with instructions
when situations dynamically change.
We will demonstrate a 3D virtual robotic system
that understands spoken language navigational in-
structions in a situation-dependent way. It incre-
mentally understands instructions so that it can un-
derstand them based on the situation at that point
in time when the instructions are made.
2 A Mobile Robot in a 3D Virtual
Environment
We use a robotic system that works in a virtual
environment built on top of SIROS (Raux, 2010),
which was originally developed for collecting di-
alogues between two participants who are engag-
ing in an online video game. As an example, a
convenience store environment was developed and
a corpus of interaction was collected (Raux and
Nakano, 2010). One of the participants, the oper-
ator, controls a (simulated) humanoid robot whose
role is to answer all customer requests. The other
participant plays the role of a remote manager who
sees the whole store but can only interact with
the operator through speech. The operator has the
robot view (whose field of view and depth are lim-
ited to simulate a robot?s vision) and the manager
has a birds-eye view of the store (Figure 1). Cus-
tomers randomly visit the store and make requests
at various locations. The manager guides the op-
erator towards customers needing attention. The
operator then answers the customer?s requests and
gets points for each satisfied request.
Using the virtual environment described above,
we have developed a system that operates the robot
according to the human manager?s instructions.
Currently we deal with only navigational instruc-
tions for moving the robot to a customer.
Figure 2 depicts the architecture for our system.
We use Sphinx-4 (Lamere et al, 2003) for speech
recognition. Its acoustic model is trained on the
Wall Street Journal Corpus and its trigram lan-
guage model was trained on 1,616 sentences in the
human-human dialogue corpus described above.
Its vocabulary size is 275 words. We use Festival
(Black et al, 2001) for speech synthesis.
369
Robot Clerk
Customers
Figure 1: The manager?s view of the convenience
store.
Speech Recognition
global context
Navigation Expertrobot moving history,robot & obstacle locations
Task Execution Expert
robot location,task info. task execution command
robot & obstacle locations,task info.
Dialogue and Behavior Controller (HRIME)speech recognition result
Manager ViewDisplay
SIROS Server
Speech Synthesisutterance text
utterance text
Figure 2: System architecture.
We use HRIME (HRI Intelligence Platform
based on Multiple Experts) (Nakano et al, 2008)
for dialogue and behavior control. In an HRIME
application, experts, which are modules dedicated
to specific tasks, are activated at appropriate times
and perform tasks. The navigation expert is ac-
tivated when the system receives a navigational
instruction. There are seven semantic categories
of instructions; they are turn-right, turn-left, go-
forward, go-back, repeat-the-previous-action, do-
the-opposite-action-of-the-previous-one, and stop.
Utterances that do not fall into any of these are ig-
nored. We assume that there are rules that match
linguistic patterns and those semantic categories.
For example, ?right? corresponds to turn-right,
and ?more? corresponds to repeat-the-previous-
action. The navigation expert sends the SIROS
server navigation commands based on the rec-
ognized semantic categories. Those commands
move the robot in the same way as a human op-
erator operates the robot using the keyboard, and
the results are shown on the display the manager
is watching. When the robot starts moving and it
cannot move because of an obstacle, it reports it to
the manager by sending its utterance to the speech
synthesizer.
When the robot has approached a customer who
is requesting help, the task is automatically per-
formed by the task execution expert.
The global context in the dialogue and behavior
controller stores information on the environment
which is obtained from the SIROS server, and it
can be used by the experts. As in the same way in
the human-human interaction, it holds information
only on customers and obstacles close to the robot
so that restricted robot vision can be simulated.
3 Situated Incremental Understanding
Sometimes manager utterances last without pauses
like ?right, right, more right, stop?, and the sit-
uation changes during the utterances because the
robot and the customers can move. So our sys-
tem employs incremental speech recognition and
moves the robot if a navigational instruction pat-
tern is found in the incremental output. To obtain
incremental speech recognition outputs, we em-
ployed InproTK (Baumann et al, 2010), which is
an extension to Sphinx-4. It enables the system
to receive tentative results every 10ms, which is a
hypothesis for the interval from the beginning of
speech to the point in time.
However, since incremental outputs are some-
times unstable and the instructions are ambiguous
in that the amount of movement is not specified,
not only incremental speech recognition outputs
but also obstacle locations and moving history is
used to determine the navigation commands.
In our system, the robot navigation expert re-
ceives incremental recognition results and if it
finds a navigational instruction pattern, it consults
the situation information in the global context,
and issues a navigation command based on sev-
eral situation-dependent understanding rules that
are manually written. Below are examples.
? If there is an obstacle in the direction that the
recognized instruction indicates, ignore the
recognized instruction. For example, when
?go forward? is recognized but there is an ob-
stacle ahead, it is guessed that the recognition
result was an error.
370
Turn left Turn to the left Go straight
I?m turning left I?m turning left I?m going forward9.95 11.135.873.41 4.39 4.77
9.80 12.139.132.43 3.12 4.32
3.45 7.54 16.159.81
Initial position Turn to the left Make the orientation parallel to an obstacle Go forward
Left turn Going forward
Manager?sutteranceRobot?sutteranceRobot?saction
Go straight3.46 12.13 14.0912.93I?m going forward
Robotorientation
Figure 3: Interaction example.
? When rotating, adjust the degree of rotation
so that the resulting orientation becomes par-
allel to obstacles such as a display shelf. This
enables the robot to smoothly go down the
aisles.
Figure 3 shows an example interaction. In the
demonstration, we will show how the robot moves
according to the spoken instructions by a human
looking at the manager display. We will compare
our system with its non-incremental version and a
version that does not use situation-dependent un-
derstanding rules to show how incremental situ-
ated understanding is effective.
4 Future Work
We are using this system for collecting a corpus
of human-robot interaction in dynamically chang-
ing situations so that we can analyze how hu-
mans make utterances in such situations. Fu-
ture work includes to make the system understand
more complicated utterances such as ?turn a lit-
tle bit to the left?. We are also planning to work
on automatically learning the situation-dependent
action selection rules from such a corpus (Vogel
and Jurafsky, 2010) to navigate the robot more
smoothly.
Acknowledgments
We thank Antoine Raux and Shun Sato for their
contribution to building the previous versions of
this system. Thanks also go to Timo Baumann
Okko Bu?, and David Schlangen for making their
InproTK available.
References
Timo Baumann, Okko Bu?, and David Schlangen.
2010. InproTK in Action: Open-Source Software
for Building German-Speaking Incremental Spoken
Dialogue Systems. In Proc. of ESSV.
Alan Black, Paul Taylor, Richard Caley, Rob Clark,
Korin Richmond, Simon King, Volker Strom,
and Heiga Zen. 2001. The Festival Speech
Synthesis System, Version 1.4.2. Unpublished
document available via http://www. cstr. ed. ac.
uk/projects/festival. html.
Paul Lamere, Philip Kwok, William Walker, Evandro
Gouvea, Rita Singh, Bhiksha Raj, and Peter Wolf.
2003. Design of the CMU Sphinx-4 decoder. In
Proc. of Eurospeech-2003.
Matthew Marge and Alexander I. Rudnicky. 2010.
Comparing spoken language route instructions for
robots across environment representations. In Proc.
of SIGDIAL-10.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and
Hiroshi Tsujino. 2008. A framework for build-
ing conversational agents based on a multi-expert
model. In Proc. of SIGDIAL-08, pages 88?91.
Antoine Raux and Mikio Nakano. 2010. The dynam-
ics of action corrections in situated interaction. In
Proc. of SIGDIAL-10, pages 165?174.
Antoine Raux. 2010. SIROS: A framework for human-
robot interaction research in virtual worlds. In Proc.
of the AAAI 2010 Fall Symposium on Dialog with
Robots.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R. Walter, Ashis Gopal Banerjee, Seth
Teller, and Nicholas Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In Proc. of AAAI-2011.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proc. of ACL-2010,
pages 806?814.
371
