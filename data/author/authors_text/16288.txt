Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 567?571,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Sequential Summarization: A New Application for Timely Updated 
Twitter Trending Topics 
Dehong Gao, Wenjie Li, Renxian Zhang 
Department of Computing, the Hong Kong Polytechnic University, Hong Kong 
{csdgao, cswjli, csrzhang}@comp.polyu.edu.hk 
 
Abstract 
The growth of the Web 2.0 technologies has 
led to an explosion of social networking 
media sites. Among them, Twitter is the most 
popular service by far due to its ease for real-
time sharing of information. It collects 
millions of tweets per day and monitors what 
people are talking about in the trending topics 
updated timely. Then the question is how 
users can understand a topic in a short time 
when they are frustrated with the 
overwhelming and unorganized tweets. In 
this paper, this problem is approached by 
sequential summarization which aims to 
produce a sequential summary, i.e., a series 
of chronologically ordered short sub-
summaries that collectively provide a full 
story about topic development. Both the 
number and the content of sub-summaries are 
automatically identified by the proposed 
stream-based and semantic-based approaches. 
These approaches are evaluated in terms of 
sequence coverage, sequence novelty and 
sequence correlation and the effectiveness of 
their combination is demonstrated.  
1 Introduction and Background 
Twitter, as a popular micro-blogging service, 
collects millions of real-time short text messages 
(known as tweets) every second. It acts as not 
only a public platform for posting trifles about 
users? daily lives, but also a public reporter for 
real-time news. Twitter has shown its powerful 
ability in information delivery in many events, 
like the wildfires in San Diego and the 
earthquake in Japan. Nevertheless, the side effect 
is individual users usually sink deep under 
millions of flooding-in tweets. To alleviate this 
problem, the applications like whatthetrend 1 
have evolved from Twitter to provide services 
that encourage users to edit explanatory tweets 
about a trending topic, which can be regarded as 
topic summaries. It is to some extent a good way 
to help users understand trending topics. 
                                                          
1 whatthetrend.com 
There is also pioneering research in automatic 
Twitter trending topic summarization. (O'Connor 
et al, 2010) explained Twitter trending topics by 
providing a list of significant terms. Users could 
utilize these terms to drill down to the tweets 
which are related to the trending topics. (Sharifi 
et al, 2010) attempted to provide a one-line 
summary for each trending topic using phrase 
reinforcement ranking. The relevance model 
employed by (Harabagiu and Hickl, 2011) 
generated summaries in larger size, i.e., 250-
word summaries, by synthesizing multiple high 
rank tweets. (Duan et al, 2012) incorporate the 
user influence and content quality information in 
timeline tweet summarization and employ 
reinforcement graph to generate summaries for 
trending topics. 
Twitter summarization is an emerging 
research area. Current approaches still followed 
the traditional summarization route and mainly 
focused on mining tweets of both significance 
and representativeness. Though, the summaries 
generated in such a way can sketch the most 
important aspects of the topic, they are incapable 
of providing full descriptions of the changes of 
the focus of a topic, and the temporal information 
or freshness of the tweets, especially for those 
newsworthy trending topics, like earthquake and 
sports meeting. As the main information 
producer in Twitter, the massive crowd keeps 
close pace with the development of trending 
topics and provide the timely updated 
information. The information dynamics and 
timeliness is an important consideration for 
Twitter summarization. That is why we propose 
sequential summarization in this work, which 
aims to produce sequential summaries to capture 
the temporal changes of mass focus. 
Our work resembles update summarization 
promoted by TAC 2  which required creating 
summaries with new information assuming the 
reader has already read some previous 
documents under the same topic. Given two 
chronologically ordered documents sets about a 
topic, the systems were asked to generate two 
                                                          
2 www.nist.gov/tac 
567
summaries, and the second one should inform the 
user of new information only. In order to achieve 
this goal, existing approaches mainly emphasized 
the novelty of the subsequent summary (Li and 
Croft, 2006; Varma et al, 2009; Steinberger and 
Jezek, 2009). Different from update 
summarization, we focus more on the temporal 
change of trending topics. In particular, we need 
to automatically detect the ?update points? 
among a myriad of related tweets.  
It is the goal of this paper to set up a new 
practical summarization application tailored for 
timely updated Twitter messages. With the aim 
of providing a full description of the focus 
changes and the records of the timeline of a 
trending topic, the systems are expected to 
discover the chronologically ordered sets of 
information by themselves and they are free to 
generate any number of update summaries 
according to the actual situations instead of a 
fixed number of summaries as specified in 
DUC/TAC. Our main contributions include 
novel approaches to sequential summarization 
and corresponding evaluation criteria for this 
new application. All of them will be detailed in 
the following sections. 
2 Sequential Summarization 
Sequential summarization proposed here aims to 
generate a series of chronologically ordered sub-
summaries for a given Twitter trending topic. 
Each sub-summary is supposed to represent one 
main subtopic or one main aspect of the topic, 
while a sequential summary, made up by the sub-
summaries, should retain the order the 
information is delivered to the public. In such a 
way, the sequential summary is able to provide a 
general picture of the entire topic development. 
2.1 Subtopic Segmentation 
One of the keys to sequential summarization is 
subtopic segmentation. How many subtopics 
have attracted the public attention, what are they, 
and how are they developed? It is important to 
provide the valuable and organized materials for 
more fine-grained summarization approaches. 
We proposed the following two approaches to 
automatically detect and chronologically order 
the subtopics. 
2.1.1 Stream-based Subtopic Detection and 
Ordering 
Typically when a subtopic is popular enough, it 
will create a certain level of surge in the tweet 
stream. In other words, every surge in the tweet 
stream can be regarded as an indicator of the 
appearance of a subtopic that is worthy of being 
summarized. Our early investigation provides 
evidence to support this assumption. By 
examining the correlations between tweet content 
changes and volume changes in randomly 
selected topics, we have observed that the 
changes in tweet volume can really provide the 
clues of topic development or changes of crowd 
focus.  
The stream-based subtopic detection approach 
employs the offline peak area detection (Opad) 
algorithm (Shamma et al, 2010) to locate such 
surges by tracing tweet volume changes. It 
regards the collection of tweets at each such 
surge time range as a new subtopic.  
Offline Peak Area Detection (Opad) Algorithm 
1: Input: TS (tweets stream, each twi with timestamp ti); 
peak interval window ?? (in hour), and time 
step? (? ? ??); 
2: Output: Peak Areas PA. 
3: Initial: two time slots: ?? = ? = ?0 + ??;  
Tweet numbers: ?? = ? = ?????(?) 
4: while (?? = ? + ?) < ???1 
5:      update ?? = ?? + ?? and ?
? = ?????(??) 
6:      if (?? < ? And up-hilling)  
7: output one peak area ???  
8: state of down-hilling 
9:      else  
10: update ? = ?? and ? =  ?? 
11: state of up-hilling 
12: 
13: function ?????(?) 
14: Count tweets in time interval T 
The subtopics detected by the Opad algorithm 
are naturally ordered in the timeline. 
2.1.2 Semantic-based Subtopic Detection and 
Ordering 
Basically the stream-based approach monitors 
the changes of the level of user attention. It is 
easy to implement and intuitively works, but it 
fails to handle the cases where the posts about 
the same subtopic are received at different time 
ranges due to the difference of geographical and 
time zones. This may make some subtopics 
scattered into several time slots (peak areas) or 
one peak area mixed with more than one 
subtopic. 
In order to sequentially segment the subtopics 
from the semantic aspect, the semantic-based 
subtopic detection approach breaks the time 
order of tweet stream, and regards each tweet as 
an individual short document. It takes advantage 
of Dynamic Topic Modeling (David and Michael, 
2006) to explore the tweet content.  
568
DTM in nature is a clustering approach which 
can dynamically generate the subtopic 
underlying the topic. Any clustering approach 
requires a pre-specified cluster number. To avoid 
tuning the cluster number experimentally, the 
subtopic number required by the semantic-based 
approach is either calculated according to 
heuristics or determined by the number of the 
peak areas detected from the stream-based 
approach in this work. 
Unlike the stream-based approach, the 
subtopics formed by DTM are the sets of 
distributions of subtopic and word probabilities. 
They are time independent. Thus, the temporal 
order among these subtopics is not obvious and 
needs to be discovered. We use the probabilistic 
relationships between tweets and topics learned 
from DTM to assign each tweet to a subtopic that 
it most likely belongs to. Then the subtopics are 
ordered temporally according to the mean values 
of their tweets? timestamps. 
2.2 Sequential Summary Generation 
Once the subtopics are detected and ordered, the 
tweets belonging to each subtopic are ranked and 
the most significant one is extracted to generate 
the sub-summary regarding that subtopic. Two 
different ranking strategies are adopted to 
conform to two different subtopic detection 
mechanisms. 
For a tweet in a peak area, the linear 
combination of two measures is considered to 
evaluate its significance to be a sub-summary: (1) 
subtopic representativeness measured by the 
cosine similarity between the tweet and the 
centroid of all the tweets in the same peak area; 
(2) crowding endorsement measured by the times 
that the tweet is re-tweeted normalized by the 
total number of re-tweeting. With the DTM 
model, the significance of the tweets is evaluated 
directly by word distribution per subtopic.  
MMR (Carbonell and Goldstein, 1998) is used 
to reduce redundancy in sub-summary generation.  
3 Experiments and Evaluations 
The experiments are conducted on the 24 Twitter 
trending topics collected using Twitter APIs 3 . 
The statistics are shown in Table 1. 
Due to the shortage of gold-standard 
sequential summaries, we invite two annotators 
to read the chronologically ordered tweets, and 
write a series of sub-summaries for each topic 
                                                          
3https://dev.twitter.com/ 
independently. Each sub-summary is up to 140 
characters in length to comply with the limit of 
tweet, but the annotators are free to choose the 
number of sub-summaries. It ends up with 6.3 
and 4.8 sub-summaries on average in a 
sequential summary written by the two 
annotators respectively. These two sets of 
sequential summaries are regarded as reference 
summaries to evaluate system-generated 
summaries from the following three aspects. 
 
Category #TT 
Trending Topic 
Examples 
Tweets 
Number 
News 6 
Minsk, Libya 
Release 
25145 
Sports 6 
#bbcf1, 
Lakers/Heat 
17204 
Technology 5 Google Fiber 13281 
Science 2 AH1N1, Richter 10935 
Entertainment 2 Midnight Club, 6573 
Meme 2 
#ilovemyfans, 
Night Angels 
14595 
Lifestyle 1 Goose Island 6230 
Total 24 ------------ 93963 
Table 1. Data Set 
? Sequence Coverage 
Sequence coverage measures the N-gram match 
between system-generated summaries and 
human-written summaries (stopword removed 
first). Considering temporal information is an 
important factor in sequential summaries, we 
propose the position-aware coverage measure by 
accommodating the position information in 
matching. Let S={s1, s2, ?, sk} denote a 
sequential summary and si the ith sub-summary, 
N-gram coverage is defined as: 
????????
=
1
|???|
?
? ? ??????????(?-????)?-???????,????????
??? ? ? ? ?????(?-????)?-???????????????????
 
where,  ??? = |? ? ?| + 1, i and j denote the serial 
numbers of the sub-summaries in the system-
generated summary ???  and the human-written 
summary ??? , respectively. ?  serves as a 
coefficient to discount long-distance matched 
sub-summaries. We evaluate unigram, bigram, 
and skipped bigram matches. Like in ROUGE 
(Lin, 2004), the skip distance is up to four words. 
? Sequence Novelty 
Sequence novelty evaluates the average novelty 
of two successive sub-summaries. Information 
content (IC) has been used to measure the 
novelty of update summaries by (Aggarwal et al, 
2009). In this paper, the novelty of a system-
569
generated sequential summary is defined as the 
average of IC increments of two adjacent sub-
summaries,  
??????? =
1
|?| ? 1
?(???? ? ????, ???1)
?>1
 
where |?| is the number of sub-summaries in the 
sequential summary. ???? = ? ??????? . ????, ???1 =
? ???????????1  is the overlapped information in the 
two adjacent sub-summaries. ??? = ????  ?
?????????(?, ???) where w is a word, ???? is the 
inverse tweet frequency of w, and ??? is all the 
tweets in the trending topic. The relevance 
function is introduced to ensure that the 
information brought by new sub-summaries is 
not only novel but also related to the topic.  
? Sequence Correlation 
Sequence correlation evaluates the sequential 
matching degree between system-generated and 
human-written summaries. In statistics, 
Kendall?s tau coefficient is often used to measure 
the association between two sequences (Lapata, 
2006). The basic idea is to count the concordant 
and discordant pairs which contain the same 
elements in two sequences. Borrowing this idea, 
for each sub-summary in a human-generated 
summary, we find its most matched sub-
summary (judged by the cosine similarity 
measure) in the corresponding system-generated 
summary and then define the correlation 
according to the concordance between the two 
matched sub-summary sequences. 
???????????
=
2(|#???????????????| ? |#???????????????|)
?(? ? 1)
 
where n is the number of human-written sub-
summaries.  
Tables 2 and 3 below present the evaluation 
results. For the stream-based approach, we set 
?t=3 hours experimentally. For the semantic-
based approach, we compare three different 
approaches to defining the sub-topic number K: 
(1) Semantic-based 1: Following the approach 
proposed in (Li et al, 2007), we first derive the 
matrix of tweet cosine similarity. Given the 1-
norm of eigenvalues  ??
???? (? = 1, 2, ? , ?) of the 
similarity matrix and the ratios ?? = ??
????/?2 , 
the subtopic number ? = ? + 1  if ?? ? ??+1 > ? 
(? = 0.4 ). (2) Semantic-based 2: Using the rule 
of thumb in (Wan and Yang, 2008), ? = ?? , 
where n is the tweet number. (3) Combined: K is 
defined as the number of the peak areas detected 
from the Opad algorithm, meanwhile we use the 
tweets within peak areas as the tweets of DTM. 
This is our new idea. 
The experiments confirm the superiority of the 
semantic-based approach over the stream-based 
approach in summary content coverage and 
novelty evaluations, showing that the former is 
better at subtopic content modeling. The sub-
summaries generated by the stream-based 
approach have comparative sequence (i.e., order) 
correlation with the human summaries. 
Combining the advantages the two approaches 
leads to the best overall results.  
 
Coverage Unigram  Bigram  
Skipped 
Bigram 
Stream-
based(?t=3) 
0.3022 0.1567 0.1523 
Semantic-
based1(?=0.5) 
0.3507 0.1684 0.1866 
Semantic-based 2 0.3112 0.1348 0.1267 
Combined(?t=3) 0.3532 0.1699 0.1791 
Table 2. N-Gram Coverage Evaluation 
Approaches Novelty Correlation 
Stream-based (?t=3) 0.3798 0.3330 
Semantic-based 1 (?=0.4) 0.7163 0.3746 
Semantic-based 2 0.7017 0.3295 
Combined (?t=3) 0.7793 0.3986 
Table 3. Novelty and Correlation Evaluation 
4 Concluding Remarks 
We start a new application for Twitter trending 
topics, i.e., sequential summarization, to reveal 
the developing scenario of the trending topics 
while retaining the order of information 
presentation. We develop several solutions to 
automatically detect, segment and order 
subtopics temporally, and extract the most 
significant tweets into the sub-summaries to 
compose sequential summaries. Empirically, the 
combination of the stream-based approach and 
the semantic-based approach leads to sequential 
summaries with high coverage, low redundancy, 
and good order. 
Acknowledgments 
The work described in this paper is supported by 
a Hong Kong RGC project (PolyU No. 5202/12E) 
and a National Nature Science Foundation of 
China (NSFC No. 61272291). 
References  
Aggarwal Gaurav, Sumbaly Roshan and Sinha Shakti. 
2009. Update Summarization. Stanford: CS224N 
Final Projects. 
570
Blei M. David and Jordan I. Michael. 2006. Dynamic 
topic models. In Proceedings of the 23rd 
international conference on Machine learning, 113-
120.  Pittsburgh, Pennsylvania. 
Carbonell Jaime and Goldstein Jade. 1998. The use of 
MMR, diversity based reranking for reordering 
documents and producing summaries. In 
Proceedings of the 21st Annual International 
Conference on Research and Development in 
Information Retrieval, 335-336. Melbourne, 
Australia. 
Duan Yajuan, Chen Zhimin, Wei Furu, Zhou Ming 
and Heung-Yeung Shum. 2012. Twitter Topic 
Summarization by Ranking Tweets using Social                
Influence and Content Quality. In Proceedings of 
the 24th International Conference on Computational 
Linguistics, 763-780. Mumbai, India. 
Harabagiu Sanda and Hickl Andrew. 2011. Relevance 
Modeling for Microblog Summarization. In 
Proceedings of 5th International AAAI Conference 
on Weblogs and Social Media. Barcelona, Spain. 
Lapata Mirella. 2006. Automatic evaluation of 
information ordering: Kendall?s tau. Computational 
Linguistics, 32(4):1-14.  
Li Wenyuan, Ng Wee-Keong, Liu Ying and Ong 
Kok-Leong. 2007. Enhancing the Effectiveness of 
Clustering with Spectra Analysis. IEEE 
Transactions on Knowledge and Data Engineering, 
19(7):887-902. 
Li Xiaoyan and Croft W. Bruce. 2006. Improving 
novelty detection for general topics using sentence 
level information patterns. In Proceedings of the 
15th ACM International Conference on Information 
and Knowledge Management, 238-247. New York, 
USA. 
Lin Chin-Yew. 2004. ROUGE: a Package for 
Automatic Evaluation of Summaries. In 
Proceedings of the ACL Workshop on Text 
Summarization Branches Out, 74-81. Barcelona, 
Spain. 
Liu Fei, Liu Yang and Weng Fuliang. 2011. Why is 
?SXSW ? trending? Exploring Multiple Text 
Sources for Twitter Topic Summarization. In 
Proceedings of the ACL Workshop on Language in 
Social Media, 66-75. Portland, Oregon. 
O'Connor Brendan, Krieger Michel and Ahn David. 
2010. TweetMotif: Exploratory Search and Topic 
Summarization for Twitter. In Proceedings of the 
4th International AAAI Conference on Weblogs 
and Social Media, 384-385. Atlanta, Georgia. 
Shamma A. David, Kennedy Lyndon and Churchill F. 
Elizabeth. 2010. Tweetgeist: Can the Twitter 
Timeline Reveal the Structure of Broadcast Events? 
In Proceedings of the 2010 ACM Conference on 
Computer Supported Cooperative Work, 589-593. 
Savannah, Georgia, USA. 
Sharifi Beaux, Hutton Mark-Anthony and Kalita Jugal. 
2010. Summarizing Microblogs Automatically. In 
Human Language Technologies: the 2010 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 685-
688. Los Angeles, California. 
Steinberger Josef and Jezek Karel. 2009. Update 
summarization based on novel topic distribution. In 
Proceedings of the 9th ACM Symposium on 
Document Engineering, 205-213. Munich, 
Germany. 
Varma Vasudeva, Bharat Vijay, Kovelamudi Sudheer, 
Praveen Bysani, Kumar K. N, Kranthi Reddy, 
Karuna Kumar and Nitin Maganti. 2009. IIIT 
Hyderabad at TAC 2009. In Proceedings of the 
2009 Text Analysis Conference. GaithsBurg, 
Maryland. 
Wan Xiaojun and Yang Jianjun. 2008. Multi-
document summarization using cluster-based link 
analysis. In Proceedings of the 31st Annual 
International Conference on Research and 
Development in Information Retrieval, 299-306. 
Singapore, Singapore. 
 
571
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 18?27,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Towards Scalable Speech Act Recognition in Twitter:  
Tackling Insufficient Training Data 
 
 
Renxian Zhang Dehong Gao Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University 
{csrzhang, csdgao, cswjli}@comp.polyu.edu.hk 
 
 
 
 
Abstract 
Recognizing speech act types in Twitter is of 
much theoretical interest and practical use. 
Our previous research did not adequately 
address the deficiency of training data for this 
multi-class learning task. In this work, we set 
out by assuming only a small seed training set 
and experiment with two semi-supervised 
learning schemes, transductive SVM and 
graph-based label propagation, which can 
leverage the knowledge about unlabeled data. 
The efficacy of semi-supervised learning is 
established by our extensive experiments, 
which also show that transductive SVM is 
more suitable than graph-based label 
propagation for our task. The empirical 
findings and detailed evidences can 
contribute to scalable speech act recognition 
in Twitter. 
1. Introduction 
The social media platform of Twitter makes 
available a plethora of data to probe the 
communicative act of people in a social network 
woven by interesting events, people, topics, etc. 
Communicative acts such as disseminating 
information, asking questions, or expressing 
feelings all fall in the purview of ?speech act?, a 
long established area in pragmatics (Austin 
1962). The automatic recognition of speech act 
in tons of tweets has both theoretical and 
practical appeal. Practically, it helps tweeters to 
find topics to read or tweet about based on 
speech act compositions. Theoretically, it 
introduces a new dimension to study social 
media content as well as providing real-life data 
to validate or falsify claims in the speech act 
theory. 
Different taxonomies of speech act have been 
proposed by linguists and computational 
linguists, ranging from a few to over a hundred 
types. In this work, we adopt the 5 types of 
speech act used in our previous work (Zhang et 
al. 2011), which are in turn inherited from 
(Searle 1975): statement, question, suggestion, 
comment, and miscellaneous. Our choice is 
based on the fact that unlike face-to-face 
communication, twittering is more in a 
?broadcasting? style than on a personal basis. 
Statement and comment, which are usually 
intended to make one?s knowledge, thought, and 
sentiment known, thus befit Twitter?s 
communicative style. Question and suggestion 
on Twitter are usually targeted at other tweeters 
in general or one?s followers. More interpersonal 
speech acts such as ?threat? or ?thank? as well as 
rare speech acts in Twitter (Searle?s (1975) 
?commissives? and ?declaratives?) are relegated 
to ?miscellaneous?. Some examples from our 
experimental datasets are provided in Table 1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
18
Tweet Speech Act 
Libya Releases 4 Times 
Journalists - 
http://www.photozz.com/?104k 
Statement 
#sincewebeinghonest why u so 
obsessed with what me n her 
do?? Don't u got ya own 
man???? Oh wait..... 
Question 
RT @NaonkaMixon: I will 
donate 10 $ to the Red Cross 
Japan Earthquake fund for 
every person that retweets this! 
#PRAYFORJAPAN 
Suggestion 
is enjoying this new season of 
#CelebrityApprentice.... Nikki 
Taylor = Yum!! 
Comment 
65. I want to get married to 
someone i meet in highschool. 
#100factsaboutme 
Miscellaneous 
Table 1. Example Tweets with Speech acts 
 
Assuming one tweet demonstrates only one 
speech act, the automatic recognition of those 
speech act types in Twitter is a multi-class 
classification task. We concede that this 
assumption may not always hold in real 
situations. But given the short length of tweets, 
multi-speech act tweets are rare and we find this 
simplifying assumption effective in reducing the 
complexity of our problem. A major problem 
with this task is the deficiency of training data. 
Tweeters as well as face-to-face interlocutors do 
not often identify their speech acts; human 
annotation is costly and time-consuming. 
Although our previous research (Zhang et al 
2011) sheds light on the preparation of training 
data, it did not adequately address this problem. 
Our contribution in this work is to directly 
address the problem of training data deficiency 
by using two well-known semi-supervised 
learning techniques that leverage the relationship 
between a small seed of training data and a large 
body of unlabeled data: transductive SVM and 
graph-based label propagation. The empirical 
results show that the knowledge about unlabeled 
data provides promising solutions to the data 
deficiency problem, and that transductive SVM 
is more competent for our task. Our exploration 
with different training/unlabeled data ratios for 
three major Twitter categories and a mixed-type 
category provides solid evidential support for 
future research. 
The rest of the paper is organized as follows. 
Section 2 reviews works related to speech act 
recognition and semi-supervised learning; 
Section 3 briefly discusses supervised learning of 
speech act types developed in our earlier work 
and complementing the previous findings with 
learning curves. The technical details of semi-
supervised learning are presented in Section 4. 
Then we report and discuss the results of our 
experiments in Section 5. Finally, Section 6 
concludes the paper and outlines future 
directions. 
2. Related Work  
The automatic recognition of speech act, also 
known as ?dialogue act?, has attracted sustained 
interest in computational linguistics and speech 
technology for over a decade (Searle 1975; 
Stolcke et al 2000). A few annotated corpora 
such as Switchboard-DAMSL (Jurafsky et al 
1997) and Meeting Recorder Dialog Act (Dhillon 
et al 2004) are widely used, with data 
transcribed from telephone or face-to-face 
conversation. 
Prior to the flourish of microblogging services 
such as Twitter, speech act recognition has been 
extended to electronic media such as email and 
discussion forum (Cohen et al 2004; Feng et al 
2006) in order to study the behavior of email or 
message senders. 
The annotated corpora for ordinary verbal 
communications and the methods developed for 
email, or discussion forum cannot be directly 
used for our task because Twitter text has a 
distinctive Netspeak style that is situated 
between speech and text but resembles neither 
(Crystal 2006, 2011). Compared with email or 
forum post, it is rife with linguistic noises such 
as spelling mistakes, random coinages, mixed 
use of letters and symbols. 
Speech act recognition in Twitter is a fairly 
new task. In our pioneering work (Zhang et al 
2011), we show that Twitter text normalization is 
unnecessary and even counterproductive for this 
task. More importantly, we propose a set of 
useful features and draw empirical conclusion 
about the scope of this task, such as recognizing 
speech act on the coarse-grade category level 
works as well as on the fine-grade topic level. In 
this work, we continue to adopt this framework 
including other learning details (speech act types 
and feature selection for tweets), but the new 
quest starts where the old one left: tackling 
insufficient training data. 
19
As in many practical applications, sufficient 
annotated data are hard to obtain. Therefore, 
unsupervised and semi-supervised learning 
methods are actively pursued. While 
unsupervised sentence classification is rule-based 
and domain-dependent (Deshpande et al 2010), 
semi-supervised methods that both alleviate the 
data deficiency problem and leverage the power 
of state-of-the-art classifiers hold more promises 
for different domains (Medlock and Briscoe 
2007; Erkan et al 2007). 
In the machine learning literature, a classic 
semi-supervised learning scheme is proposed by 
Yarowsky (1995), which is a classical self-
teaching process that makes no use of labeled 
data before they are classified. More theoretical 
analyses are made by (Culp and Michailidis 2007) 
and (Haffari and Sarkar 2007).  
Transductive SVM (Joachims 1999) extends 
the state-of-the-art inductive SVM by explicitly 
considering the relationship between labeled and 
unlabeled data. The graph-based label 
propagation model (Zhu et al 2003; Zhou et al 
2004) using a harmonic function also 
accommodates the knowledge about unlabeled 
data. We will adapt both of them to our multi-
class classification task. 
Jeong et al (2009) report a semi-supervised 
approach to classifying speech acts in emails and 
online forums. But their subtree-based method is 
not applicable to our task because Twitter?s noisy 
textual quality cannot be found in the much 
cleaner email or forum texts. 
3. Supervised Learning of Speech Act 
Types  
Supervised learning of speech act types in 
Twitter relies heavily on a good set of features 
that capture the textual characteristics of both 
Twitter and speech act utterances. As in our 
previous work, we use speech act-specific cues, 
special words (abbreviations and acronyms, 
opinion words, vulgar words, and emoticons), 
and special characters (Twitter-specific 
characters and a few punctuations). Tweet-
external features such as tweeter profile may also 
help, but that is beyond the focus of this paper. 
Although it has been empirically shown that 
speech act recognition in Twitter can be done 
without using training data specific to topics or 
even categories, it is not clear how much training 
data is needed to achieve desirable performance. 
In order to answer this question, we adopt the 
same experimental setup and datasets as reported 
in (Zhang et al 2011) and plot the learning 
curves shown in Figure 1. 
 
 
Figure 1. Learning Curves of Each Category and 
All Tweets 
 
For all individual experiments, the test data are 
a randomly sampled 10% set of all annotated 
data. When training data reach 90%, we actually 
duplicate the reported results. However, Figure 1 
shows that it is unnecessary to use so much 
training data to achieve good classification 
performance. For News and Entity, the 
classification makes little noticeable 
improvement after the training data ratio reaches 
40% (training : test = 4 : 1). For Mixed (the 
aggregate of the News, Entity, LST datasets) and 
LST, performance peaks even earlier at 20% 
training data (training : test = 2 : 1) and 10% 
(training : test = 1 : 1).  
It is delightful to see that only a moderate 
number of annotated data are needed for speech 
act recognition. But even that number (for the 
Mixed dataset, 10% training data are over 800 
annotated tweets) may not be available and in 
many situations, test data may be much more 
than training data. Taking this challenge is the 
next important step we make. 
4. Semi-Supervised Learning of Speech 
Act Types  
The problem setting of a small seed training 
(labeled) set and a much larger test (labeled) set 
fits the semi-supervised learning scheme. Classic 
semi-supervised learning approaches such as 
self-teaching methods (e.g., Yarowsky 1995) are 
mainly concerned with incrementing high-
confidence labeled data in each round of training. 
They do not, however, directly take into account 
the knowledge about unlabeled data. The recent 
research emphasis is on leveraging knowledge 
about unlabeled data during training. In this 
section, we discuss two such approaches. 
20
4.1 Transductive SVM 
The standard SVM classifier popularly used in 
text classification is also known as inductive 
SVM as a model is induced from training data. 
The model is solely dependent on the training 
data and agnostic about the test data. In contrast, 
transductive SVM (Vapnik 1998; Joachims 1999) 
predicts test labels by using the knowledge about 
test data. In the case of test (unlabeled) data far 
outnumbering training (labeled) data, 
transductive SVM provides a feasible scheme of 
semi-supervised learning. 
For a single-class classification problem {xi, yi} 
that focuses on only one speech act type, where 
xi is the ith tweet and yi is the corresponding 
label and { 1, 1}iy ? ? ?  denotes whether xi 
contains the speech act or not, inductive SVM is 
formulated to find an optimal hyperplane 
sign(w?xi ? b) to maximize the soft margin 
between positive and negative objects, or to 
minimize: 
 
21/ 2 iiC ?? ?w
 
s.t. ( ) 1i i iy b ?? ? ? ?x w , 0i? ?  
 
where
i? is a slack variable. Adopting the same 
formulation, transductive SVM further considers 
test data xi* during training by finding a labeling 
yj* and a hyperplane to maximize the soft margin 
between both training and test data, or to 
minimize: 
 
2
1 21/ 2 i ii iC C? ?? ?? ?w
 
s.t. ( ) 1i i iy b ?? ? ? ?x w , 0i? ?  
      * *( ) 1i i iy b ?? ? ? ?x w , 0i? ?  
 
where
i? is a slack variable for the test data. In 
fact, labeling test data is done during training. 
As the maximal margin approach proves very 
effective for text classification, its transductive 
variant that effectively uses the knowledge about 
test data holds promises of handling the 
deficiency of labeled data. 
4.2 Graph-based Label Propagation 
An alternative way of using unlabeled data in 
semi-supervised learning is based on the intuition 
that similar objects should belong to the same 
class, which can be translated into label 
smoothness on a graph with weights indicating 
object similarities. This is the idea underlying 
Zhu et al?s (2003) graph-based label propagation 
model using Gaussian random fields.   
We again focus on a single-class classification 
problem. Formally, {x1, ? xN} are N tweets, 
having their actual speech act labels y = {y1, ? 
yL, ? yN} (yi ?{1, 0} denoting whether xi 
contains the speech act or not) with the first L of 
them known, and f = {f1, ? fL, ? fN} are their 
predicted labels. Let L = {x1, ? xL} and U = 
{xL+1, ? xN} and the task is to determine 
{fL+1, ? fN} for U. We further define a graph G = 
(V, E), where V = L?U and E is weighted by W 
= [wij]N?N  with wij denoting the similarity 
between xi and xj. Preferring label smoothness on 
G and preserving the given labels, we want to 
minimize the loss function: 
 
2
,
( ) 1/ 2 ( ) Tij i j
i j L U
E w f f
? ?
? ? ??f f ?f
 
s.t. fi = yi (i = 1, ?, L) 
 
where ? = D ? W is the combinatorial graph 
Laplacian with D being a diagonal matrix [dij]N?N 
and 
ii ijj
d w??
. 
This can be expressed as a harmonic function, 
h = argmin fL = yLE(f), which satisfies the 
smoothness property on the graph: 
( ) 1/ ( ( ))ii ikkh i d w h k? ?
. If we define 
/ij ij ikkp w w? ?
and collect pij and h(i) into 
matrix P and column vector h, solving ?h = 0 s.t. 
hL = yL is equivalent to solving h = Ph. 
To find the solution, we can use L and U to 
partition h and P: 
L
U
? ?? ? ?? ?
hh h
, ,
,
LL LU
UL UU
? ?? ? ?? ?
P PP P P
 
and it can be shown that 1( )U UU UL L?? ?h I P P y. 
To get the final classification result, those 
elements in hU that are greater than a threshold 
(0.5) become 1 and the others become 0. 
This approach propagates labels from labeled 
data to unlabeled data on the principle of label 
smoothness. If the assumption about similar 
tweets having same speech acts holds, it should 
work well for our problem. 
4.3 Multi-class Classification 
In the previous formulations, we emphasized 
?single-class classification? because both 
21
transductive SVM and graph-based label 
propagation are inherently one class-oriented. 
Since our problem is a multi-class one, we 
transform the problem to single-class 
classifications by using the one-vs-all scheme.  
Specifically, for each class (speech act type) ci, 
we label all training instances belonging to ci as 
+1 and all those belonging to other classes as ?1 
and then do binary classification. For our 
problem with 5 speech act types, we make 5 such 
transformations. The final prediction is made by 
choosing the class with the highest classification 
score from the 5 binary classifiers. Both 
transductive SVM and graph-based label 
propagation produce real-valued classification 
scores and are amenable to this scheme. 
5. Experiments  
Our experiments are designed to answer two 
questions: 1) How useful is semi-supervised 
speech act learning in comparison with 
supervised learning? 2) Which semi-supervised 
learning approach is more appropriate for our 
problem? 
5.1 Experimental Setup 
We use the 6 datasets in our previous study1 , 
which fall into 3 categories: News, Entity, Long-
standing Topic (LST). Each of the total 8613 
tweets is labeled with one of the following 
speech act types: sta (statement), que (question), 
sug (suggestion), com (comment), mis 
(miscellaneous). In addition, we randomly select 
1000 tweets from each of the categories to create 
a Mixed category of 3000 tweets. Figures 2 to 5 
illustrate the distributions of the speech act types 
in the 3 original categories and the Mixed 
category. 
 
 
Figure 2. Speech Act Distribution (News) 
 
                                                          
1 http://www4.comp.polyu.edu.hk/~csrzhang 
 
Figure 3. Speech Act Distribution (Entity) 
 
 
Figure 4. Speech Act Distribution (LST) 
 
 
Figure 5. Speech Act Distribution (Mixed) 
 
For each category, we use two 
labeled/unlabeled data settings, with labeled data 
accounting for 5% and 10% of the total so that 
the labeled/unlabeled ratios are set at 
approximately 1:19 and 1:9. The labeled data in 
each category are randomly selected in a 
stratified way: using the same percentage to 
select labeled data with each speech act type. The 
stratified selection is intended to keep the speech 
act distributions in both labeled and unlabeled 
data. Table 2 and Table 3 list the details of data 
splitting using the two settings. 
 
Category # Labeled # Unlabeled Total 
News 155 2995 3150 
Entity 72 1391 1463 
LST 198 3802 4000 
Mixed 147 2853 3000 
Table 2. Stratified Data Splitting with 5% as 
Labeled 
 
22
Category # Labeled # Unlabeled Total 
News 312 2838 3150 
Entity 144 1319 1463 
LST 399 3601 4000 
Mixed 298 2702 3000 
Table 3. Stratified Data Splitting with 10% as 
Labeled 
 
For comparison with supervised learning, we 
also use inductive SVM. The inductive and 
transductive SVM classifications are 
implemented by using the SVMlight tool2 with a 
linear kernel. For the graph-based label 
propagation method, we populate the similarity 
matrix W with weights calculated by a Gaussian 
function. Given two tweets xi and xj,  
 
2
2exp( )2
i j
ijw ?
?? ? x x
 
 
where ?.? is the L2 norm. Empirically, the 
Gaussian function measure leads to better results 
than other measures such as cosine. Then we 
convert the graph to an ?NN graph (Zhu and 
Goldberg 2009) by removing edges with weight 
less than a threshold because the ?NN graph 
empirically outperforms the fully connected 
graph. The threshold is set to be ? + ?, the mean 
of all weights plus one standard deviation. 
5.2 Results 
To better evaluate the performance of semi-
supervised learning on speech act recognition in 
Twitter, we report the classification scores for 
both multi-class and individual classes, as well as 
confusion matrices. 
 
Multi-class Evaluation 
Table 4 lists the macro-average F scores and 
weighted average F scores for all classifiers and 
all categories at the 5% labeled data setting. 
Macro-average F is chosen because it gives equal 
weight to all classes. Since some classes (e.g., sta) 
have much more instances than others (e.g., que), 
macro-average F ensures that significant score 
change on minority classes will not be 
overshadowed by small score change on majority 
classes. In contrast, weighted average F is 
calculated according to class instance numbers, 
which is chosen mainly because we want to 
compare the result with supervised learning 
(reported in Zhang et al 2011 and Figure 1). In 
                                                          
2 http://svmlight.joachims.org/ 
this and the following tables, iSVM, tSVM, and 
GLP denote inductive SVM, transductive SVM, 
and graph-based label propagation. 
 
 
Macro-average F Weighted average F 
iSVM tSVM GLP iSVM tSVM GLP 
News .374 .502 .285 .702 .759 .643 
Entity .312 .395 .329 .493 .534 .436 
LST .295 .360 .216 .433 .501 .376 
Mixed .383 .424 .245 .539 .537 .391 
Table 4. Multi-class F scores (5% labeled data) 
 
Almost without exception, transductive SVM 
achieves the best performance. Measured by 
macro-average F, it outperforms inductive SVM 
with a gain of 10.7% (Mixed) to 34.2% (News). 
Consistent with supervised learning results, 
semi-supervised learning results degrade with 
News > Entity > LST, indicating that both semi-
supervised learning and supervised learning are 
sensitive to dataset characteristics. More uniform 
tweet set (e.g., News) leads to better 
classification and greater improvement by semi-
supervised learning. That also explains why the 
Mixed category, composed of the most 
diversified tweets, benefits least from semi-
supervised learning. 
Conversely, supervised learning (inductive 
SVM) on the Mixed category benefits from the 
data hodgepodge even though the test data are 19 
times the training data. Its macro-average F is 
higher than the other categories although it does 
not have the most training data. Its weighted-
average F using inductive SVM is even higher 
than using transductive SVM. 
It is a little surprising to find that the graph-
based label propagation performs very poorly. In 
all but one place, the GLP score is lower than its 
iSVM counterpart. This may indicate that the 
graph method cannot adapt well to the multi-
class scenario and we will show more evidences 
in the next two sections. 
To understand the effectiveness of semi-
supervised learning, a better way than doing 
numerical calculation is juxtaposing semi-
supervised data settings with their comparable 
supervised data settings, which is shown in Table 
5. The supervised data settings are of those with 
the closest weighted average F (waF) to the 
semi-supervised (tSVM) waF from our previous 
results (Figure 1). 
 
23
 # labeled labeled :unlabeled waF 
 Semi-supervised (tSVM) 
News 155 1 : 19 .759 
Entity 72 1 : 19 .534 
LST 198 1 : 19 .501 
Mixed 147 1 : 19 .537 
 Supervised (with closest waF) 
News 945 1 : 0.3 .768 
Entity 146 1 : 1 .589 
LST 800 1 : 0.5 .501 
Mixed 861 1 : 1 .596 
 
Table 5. Semi-supervised Learning vs. 
Supervised Learning 
 
Obviously semi-supervised learning by 
transductive SVM can achieve classification 
performance comparable to supervised learning 
by inductive SVM, with less training data and 
much lower labeled/unlabeled ratio. This shows 
that semi-supervised learning such as 
transductive SVM holds much promise for 
scalable speech act recognition in Twitter. 
It is tempting to think that with more labeled 
data and higher labeled/unlabeled ratio, semi-
supervised learning performance should improve. 
To put this conjecture to test, we double the 
labeled data (from 5% to 10%) and 
labeled/unlabeled ratio (from 1/19 to 1/9), with 
results in Table 6. 
 
 
Macro-average F Weighted average F 
iSVM tSVM GLP iSVM tSVM GLP 
News .403 .524 .298 .731 .762 .647 
Entity .441 .440 .311 .587 .575 .406 
LST .335 .397 .216 .459 .512 .384 
Mixed .435 .463 .284 .557 .553 .415 
Table 6. Multi-class F scores (10% labeled data) 
 
Compared with Table 4, increased labeled data 
does lead to some improvement, but not much as 
we would expect, the largest gain being 15.9% 
(macro-average F on Mixed, using GLP). Note 
that this is achieved at the cost of labeling twice 
as much data and predicting half as much. In 
contrast, the inductive SVM performance is 
improved by as much as 41.3% (macro-average 
F on Entity). Such evidence shows that semi-
supervised learning of speech acts in Twitter 
benefits disproportionately little from increased 
labeled data, or at least the gain is not worth the 
pain. In fact, this is good news for scalable 
speech act recognition. 
 
Individual Class Evaluation 
For more microscopic inspection, we also report 
the classification results on individual classes for 
all categories. In Table 7, we list the rankings of 
F measures by each classifier for each speech act 
type and each category. The one-letter notations i, 
t, g are short for iSVM, tSVM, and GLP. 
Therefore, t > g > i means tSVM outperforms 
GLP, which outperforms iSVM, in terms of F 
measure. The labeled data are 5%. 
 
 Sta Que Sug Com Mis 
News t >g>i t >i>g t >i>g t >i>g t >g>i 
Entity t >g>i t >i>g g >t>i i >t>g t >g>i 
LST i >g>t t >i>g i >t>g t >i>g t >g>i 
Mixed i >t>g t >i>g t >i>g i >t>g t >g>i 
Table 7. Classifier Rankings for Each Speech 
Act Type and Category (5% Labeled Data) 
 
In 15 out of the 20 rankings, transductive 
SVM or graph-based label propagation beats 
inductive SVM, which shows the efficacy of 
semi-supervised learning in this class-based 
perspective. Transductive SVM is the champion, 
claiming 14 top places.  
We also find that the overall performance of 
graph-based label propagation is the poorest, 
claiming 12 out of 20 bottom places. After 
inspecting the data, we observe that the 
underlying assumption of GLP that similar 
objects belong to the same class is questionable 
for speech act recognition in Twitter. Tweets 
with different speech acts (e.g., question and 
comment) may appear very similar on the graph. 
The maximal margin approach is apparently 
more appropriate for our problem.  
On the other hand, the GLP performance 
evaluated on individual classes is better than 
evaluated on the multi-class if we compare Table 
7 and Table 4, where GLP is almost always the 
lowest achiever. This indicates that in multi-class 
classification, GLP suffers further from the one-
vs-all converting scheme, a point we will make 
clearer in the following. 
 
 
 
24
Confusion matrices 
Confusion matrix provides another perspective to 
understand the multi-class classification 
performance. For brevity?s sake, we present the 
confusion matrices of the three classifiers on the 
News category with 5% labeled data in Figure 6 
to Figure 8. Similar patterns are also observed for 
the other categories and with 10% labeled data. 
Note that the rows represent true classes and the 
columns represent predicted classes. 
 
 Sta Que Sug Com Mis 
Sta 2043 0 5 14 0 
Que 46 7 2 9 0 
Sug 211 1 61 21 0 
Com 276 2 10 164 0 
Mis 120 0 1 2 0 
Figure 6. Confusion Matrix of iSVM (News, 5% 
Labeled Data) 
 
 Sta Que Sug Com Mis 
Sta 1848 4 56 90 64 
Que 19 17 7 20 1 
Sug 95 0 158 31 10 
Com 143 5 19 275 10 
Mis 94 3 4 15 7 
Figure 7. Confusion Matrix of tSVM (News, 5% 
Labeled Data) 
 
 Sta Que Sug Com Mis 
Sta 1852 0 4 11 195 
Que 19 6 0 0 39 
Sug 123 0 25 2 144 
Com 134 0 0 47 271 
Mis 102 0 0 1 20 
Figure 8. Confusion Matrix of GLP (News, 5% 
Labeled Data) 
 
The News category is typically biased towards 
the statement speech act, which accounts for 
69% of the total tweets according to Figure 2. As 
a result, the iSVM tends to classify tweets of the 
other speech acts as statement. Figure 6 also 
shows that the prediction accuracy is correlated 
with the training amount. The two classes with 
the least training data, question and 
miscellaneous, demonstrate the lowest accuracy. 
Clearly, supervised learning suffers from training 
data deficiency. 
Both tSVM and GLP show the effect of 
leveraging unlabeled data as they assign new 
labels to some instances wrongly classified as 
statement. Transductive SVM is more successful 
in that it moves most of the Sug and Com 
instances to the diagonal. The situation for Que 
and Mis is also better, though the prediction 
accuracy still suffers from lack of training data. 
Figure 8, however, reveals an intrinsic problem 
of applying graph-based label propagation to 
multi-class classification. Most instances are 
predicted as either Sta or Mis. The wrong 
prediction as Mis cannot be explained by 
imbalance of training data. Rather, it is due to the 
fact that the single-class scores for Mis after 
smoothing on the graph are generally higher than 
those for Que, Sug, or Com. In other words, the 
graph-based method is highly sensitive to class 
differences when multi-class prediction is 
converted from single-class predictions on a 
scheme like one-vs-all. 
In contrast, transductive SVM does not suffer 
much from class differences according to Figure 
7, proving to be more suitable for multi-class 
classification than graph-based label propagation. 
5.3 Summary 
For the task of recognizing speech acts in Twitter, 
we have made some interesting findings from the 
extensive empirical study. To wrap up, let?s 
summarize the most important of them in the 
following. 
1) Semi-supervised learning approaches, 
especially transductive SVM, perform 
comparably to supervised learning approaches, 
such as inductive SVM, with considerably less 
training data and lower training/test ratio. 
Increasing training data cannot improve 
performance proportionately. 
2) Transductive SVM proves to be more 
effective than graph-based label propagation for 
our task. The performance of the latter is hurt by 
two factors: a) the inappropriate assumption 
about similar tweets having the same speech act 
and b) its vulnerability to class differences under 
the one-vs-all multi-class conversion scheme. 
3) For supervised learning as well as semi-
supervised learning for multi-class classification, 
training data imbalance poses no lesser threat 
than training data deficiency. 
25
6. Conclusion and Future Work  
Speech act recognition in Twitter facilitates 
content-based user behavior study. Realizing that 
it is obsessed with insufficient training data, we 
start where previous research left. 
We are not aware of previous study of semi-
supervised learning of speech acts in Twitter and 
in this paper we contribute to scalable speech act 
recognition by drawing conclusions from 
extensive experiments. Specifically, we 
1) extend the work of (Zhang et al 2011) by 
establishing the practicality of semi-supervised 
learning that leverages the knowledge of 
unlabeled data as a promising solution to 
insufficient training data;  
2) show that transductive SVM is more 
effective than graph-based label propagation for 
our problem, which aptly extends the maximal 
margin approach to unlabeled data and is more 
amenable to the multi-class scenario; 
3) provide detailed empirical evidences of 
multi-class and single-class results, which can 
inform future extensions in this direction and 
design of practical systems. 
At this stage, we are not sure whether the one-
vs-all scheme is a bottleneck to one class-
oriented classifiers (it appears to be so for the 
graph-based method). Therefore we will next 
explore other multi-class conversion schemes 
and also consider semi-supervised learning using 
inherently multi-class classifiers such as Na?ve 
Bayes or Decision Tree. In the future, we will 
also explore unsupervised approaches to 
recognizing speech acts in Twitter. 
Acknowledgments 
The work described in this paper was supported 
by the grants GRF PolyU 5217/07E and PolyU 
5230/08E. 
 
26
References  
Austin, J. 1962. How to Do Things with Words. 
Oxford: Oxford University Press. 
Cohen, W., Carvalho, V., and Mitchell, T. 2004. 
Learning to Classify Email into ?Speech Acts?. In 
Proceedings of Empirical Methods in Natural 
Language Processing (EMNLP-04), 309?316.  
Crystal, D. 2006. Language and the Internet, 2nd 
edition. Cambridge, UK: Cambridge University 
Press. 
Crystal, D. 2011. Internet linguistics. London: 
Routledge. 
Culp M. and Michailidis, G. 2007. An Iterative 
Algorithm for Extending Learners to a 
Semisupervised Setting. In The 2007 Joint 
Statistical Meetings (JSM). 
Deshpande S. S., Palshikar, G. K., and Athiappan, G. 
2010. An Unsupervised Approach to Sentence 
Classification, In International Conference on 
Management of Data (COMAD 2010), Nagpur, 
India. 
Dhillon, R., Bhagat, S., Carvey, H., and Shriberg, E. 
2004. Meeting Recorder Project: Dialog Act 
Labeling Guide. Technical report, International 
Computer Science Institute. 
Erkan, G., ?zg?r, A., and Radev, D. 2007. Semi-
Supervised Classification for Extracting Protein 
Interaction Sentences Using Dependency Parsing. 
In Proceedings of the 2007 Joint Conference on 
Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, 228?237. 
Feng, D., Shaw, E., Kim, J., and Hovy. E. H. 2006. 
Learning to Detect Conversation Focus of 
Threaded Discussions. In Proceedings of HLT-
NAACL, 208?215. 
Haffari G.R. and Sarkar. A. 2007. Analysis of semi-
supervised learning with the Yarowsky algorithm. 
In 23rd Conference on Uncertainty in Artificial 
Intelligence (UAI). 
Jeong, M., Lin, C-Y., and Lee, G. 2009. Semi-
supervised Speech Act Recognition in Emails and 
Forums. In Proceedings of EMNLP, pages 1250?
1259. 
Joachims, T. 1999. Transductive Inference for Text 
Classification using Support Vector Machines. In 
Proceedings of the 16th International Conference 
on Machine Learning (ICML). 
Jurafsky, D., Shriberg, E., and Biasca, D. 1997. 
Switchboard SWBD-DAMSL Labeling Project 
Coder?s Manual, Draft 13. Technical report, 
University of Colorado Institute of Cognitive 
Science. 
Medlock, B., and Briscoe, T. 2007. Weakly 
Supervised Learning for Hedge Classification in 
Scientific Literature. In Proceedings of the 45th 
Annual Meeting of the Association of 
Computational Linguistics, 992?999. 
Searle, J. 1975. Indirect speech acts. In P. Cole and J. 
Morgan (eds.), Syntax and semantics, vol. iii: 
Speech acts (pp. 59?82). New York: Academic 
Press. 
Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, 
R., Jurafsky, D., Taylor, P., Martin, R. Van Ess-
Dykema, C., and Meteer, M. 2000. Dialogue Act 
Modeling for Automatic Tagging and Recognition 
of Conversational Speech. Computational 
Linguistics, 26(3):339?373. 
Vapnik, V. 1998. Statistical Learning Theory. New 
York: John Wiley & Sons. 
Yarowsky, D. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics (ACL-
1995), 189?196. 
Zhang, R., Gao, D., and Li, W. 2011. What Are 
Tweeters Doing: Recognizing Speech Acts in 
Twitter. In AAAI-11 Workshop on Analyzing 
Microtext. 
Zhou, D., Bousquet, O., Lal, T. N., Weston, J., and 
Scholkopf, B. 2004. Learning with Local and 
Global Consistency. Advances in Neural 
Information Processing Systems (NIPS), vol. 16, 
Cambridge, MA: MIT Press. 
Zhu, X., Ghahramani, Z., and Lafferty, J. D. 2003. 
Semi-supervised Learning Using Gaussian Fields 
and Harmonic Functions. In Proceedings of the 
Twentieth International Conference on Machine 
Learning (ICML), 912?919, Washington, DC. 
Zhu, X. and Goldberg, A. B., 2009. Introduction to 
Semi-Supervised Learning. Morgan & Claypool 
Publishers. 
 
27
