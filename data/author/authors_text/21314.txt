Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1191?1199,
Beijing, August 2010
Disambiguating Dynamic Sentiment Ambiguous Adjectives
Yunfang Wu
Key Laboratory of Computational
Linguistics (Peking University),
Ministry of EducationRI China
wuyf@pku.edu.cn
Miaomiao Wen?
Department of Electrical Engineering and
Information Systems,
University of Tokyo
wenmiaomiao98@gmail.com
?Most of the work was performed when the author was a student at Peking University.
Abstract
Dynamic sentiment ambiguous
adjectives (DSAAs) like ?large, small,
high, low? pose a challenging task on
sentiment analysis. This paper proposes a
knowledge-based method to
automatically determine the semantic
orientation of DSAAs within context.
The task is reduced to sentiment
classification of target nouns, which we
refer to sentiment expectation instead of
semantic orientation widely used in
previous researches. We mine the Web
using lexico-syntactic patterns to infer
sentiment expectation of nouns, and then
exploit character-sentiment model to
reduce noises caused by the Web data.
At sentence level, our method achieves
promising result with an f-score of
78.52% that is substantially better than
baselines. At document level, our
method outperforms previous work in
sentiment classification of product
reviews.
1 Introduction
In recent years, sentiment analysis has attracted
considerable attention in the NLP community. It
is the task of mining positive and negative
opinions from natural language, which can be
applied to many research fields. Previous work
on this problem falls into three groups: opinion
mining of documents, sentiment classification of
sentences and polarity prediction of words.
Sentiment analysis both at document and
sentence level rely heavily on word level.
The most frequently explored task at the word
level is to determine the polarity of words, in
which most work centers on assigning a prior
polarity to words or word senses in the lexicon
out of context. However, for some words, the
polarity varies strongly with context, making it
hard to attach each to a fixed sentiment category
in the lexicon. For example, the word ?low?has
a positive orientation in ?low cost? but a
negative orientation in ?low salary?. We call
these words like ?low? dynamic sentiment
ambiguous adjectives (DSAAs). Turney and
Littman (2003) claim that DSAAs cannot be
avoided in a real-world application. But
unfortunately, DSAAs are discarded by most
research concerning sentiment analysis.
In this paper, we are devoted to the
challenging task of disambiguating DSAAs. The
task is to automatically determine the semantic
orientation (SO) of DSAAs within context. We
limit our work to 14 frequently used adjectives
in Chinese, such as ?large, small, many, few,
high, low?, which all have the meaning of
measurement. Although the number of such
ambiguous adjectives is not large, they are
frequently used in real text, especially in the
texts expressing opinions and emotions. As
demonstrated by the experimental results in this
paper, the disambiguation of 14 DSAAs can
obviously improve the performance of sentiment
classification of product reviews.
The task of disambiguating DSAAs is reduced
to sentiment classification of nouns. Previous
studies classify nouns into three categories:
positive, negative and neutral. In contrast, we
propose two categories of sentiment expectation
1191
of nouns: positive expectation and negative
expectation. This paper presents a novel
approach to automatically predict sentiment
expectation of nouns. First, we infer the
sentiment expectation of a noun by mining the
Web with strongly-polar-steering lexico-
syntactic patterns. Secondly, we derive the
sentiment expectation of a noun from its
component characters, which capture the
semantic relationship between Chinese words
and characters. Finally, a better performance is
obtained by combing the two methods. We
conduct two types of experiments: the
experimental results at the sentence level
validate the effectiveness of our approach; the
experimental results at the document level
confirm the significance of the problem we
addressed.
2 Related Work
2.1 Word-level Sentiment Analysis
Recently there has been extensive research in
sentiment analysis, for which Pang and Lee
(2008) give an in-depth survey of literature.
Closer to our study is the large body of work on
automatic SO prediction of words
(Hatzivassiloglou and McKeown, 1997; Turney
and Littman, 2003; Kim and Hovy, 2004;
Andreevskaia and Bergler, 2006), but
unfortunately they all discard DSAAs in their
research. In recent years, some studies go a step
further, attaching SO to senses instead of word
forms (Esuli and Sebastiani, 2006; Wiebe and
Mihalcea, 2006; Su and Markert 2008), but their
work is still limited in lexicon out of context.
The most relevant work is Ding et al (2008),
in which DSAAs are named as context
dependant opinions. They argue that there is no
way to know the SO of DSAAs without prior
knowledge, and asking a domain expert to
provider such knowledge is scalable. They adopt
a holistic lexicon-based approach to solve this
problem, which exploits external information
and evidences in other sentences and other
reviews. On the contrary in this paper, we obtain
the prior knowledge of a product by mining the
web, and then use such knowledge to determine
the SO of DSAAs. The prior knowledge of a
product, which is closer to the sentiment
expectation of nouns described in this paper, is
an important research issue in itself and has
many applications in sentiment analysis, as
discussed in section 3.2.
2.2 Phrase-level Sentiment Analysis
The disambiguation of DSAAs can also be
considered as a problem of phrase-level
sentiment analysis. Wilson et al (2004) present a
two-step process to recognize contextual polarity
that employs machine learning and a variety of
features. Takamura et al (2006, 2007) propose
latent variable model and lexical network to
determine SO of phrases, focusing on
?noun+adjective? pairs. Their experimental
results suggest that the classification of pairs
containing ambiguous adjectives is much harder
than those with unambiguous adjectives. The
above mentioned approaches are all supervised,
and need human labeled data for training. In
contrast, our method is unsupervised and can
overcome the data acquisition bottleneck.
Moreover, we focus on the much harder task of
disambiguating DSAAs in ?noun+adjective?
pairs.
2.3 Pattern-based Method
Previous studies have applied pattern-based
method to sentiment analysis (Riloff and Wiebe,
2003; Wiebe et al, 2004; Riloff et al, 2005;
Wiebe and Mihalcea, 2006; Andreevskaia and
Berger; 2006). The differences with our method
lie in two aspects: the used resources (corpus
versus web) and the research target (subjective
expressions versus sentiment expectation).
2.4 Character-based Method
Chinese characters carry semantic information
that is indicative of semantic properties of words.
Previous studies have exploited the character-
based model to predict the semantic categories
of Chinese unknown words (Chen, 2004; Lu,
2007). Yuen et al (2004) presents a method to
infer the SO of a Chinese word from its
statistical association with strong-polarized
characters rather than with strong-polarized
words. The work by Ku et al (2006) is similar to
ours because they also define the sentiment score
of a word by its composite characters. However,
their algorithm is based only on frequency, while
we exploit point mutual information that can
capture the character-sentiment association.
1192
3 Determining SO of Adjective by
Target Noun
3.1 Classification of DSAAs
The frequently used DSAAs are given below.
We group them into two categories: positive-like
adjectives and negative-like adjectives. These
adjectives are neutral out of context, but positive
or negative emotion will be evoked when they
co-occur with some target nouns, making it hard
to assign each to a fixed sentiment category in
lexicon.
(1) Positive-like adjectives (Pa) = {? da|large,
? duo|many,? gao|high,? hou|thick,?
shen|deep,? zhong|heavy,?? ju-da|huge,?
? zhong-da|great}
(2) Negative-like adjectives (Na) ={?
xiao|small,? shao |few, ? di|low, ? bao|thin,
? qian|shallow, ? qing|light}
3.2 Sentiment Expectation of Noun
The SO of most DSAAs can be determined by
target nouns in noun-adjective phrases, as shown
in Table 1. For example, the word ?high?has a
positive orientation when the target noun is
?salary? but a negative orientation when the
target noun is ?price?. Therefore, the task can be
reduced to sentiment classification of nouns.
Positive
? ? ? |potential is great
???|salary is high
Negative
? ? ? |potential is small
??? |salary is low
Negative
? ? ? |pressure is big
?? ?|price is high
Positive
? ? ? |pressure is small
?? ? |price is low
Table 1: The SO of DSAAs in noun-adjective phrases
In previous research, the SO of nouns is
classified into three categories: positive,
negative and neutral. Accordingly, ??? ya-
li|pressure?will be assigned as negative and ??
? qian-li|potential? as positive, while ???
gong-zi|salary? and ??? jia-ge|price? will be
assigned as neutral, as the two terms are
objective and cannot evoke positive or negative
emotion. Different from the traditional
classification scheme, we propose sentiment
expectation and classify nouns into two
categories: positive expectation and negative
expectation. For a positive expectation noun,
people usually expect the thing referred to by the
noun to be bigger, higher or happen frequently.
On the contrary, for a negative expectation noun,
people usually expect the thing referred to by the
noun to be smaller, lower or don?t happen . For
example, ?? ? jia-ge|price? is a negative
expectation noun, as most people in most cases
expect that the product prices become low,
whereas ??? gong-zi|salary? is a positive
expectation noun, as most people in most cases
expect that their salaries become high. The
relationship between traditional SO and
sentiment expectation can be defined as: positive
(negative) terms correspond to positive (negative)
expectation terms, but some neutral terms may
also carry positive (negative) expectation.
Su and Markert (2008) argue that polarity can
also be attached to objective words. The
difference with our scheme is that, for example,
?? ? jia-ge|price? is attached to negative
expectation in our scheme while is still neutral in
Su and Markert?s method.
The distinction between positive and negative
expectation nouns is vital to determine the SO of
some phrases. Using it to disambiguate DSAAs
is a good example. Another application is the
phrase containing verbs with the meaning of
status change. For example, ??????|salary
has been raised? will evoke positive emotion,
while ?????? jiage-shangzhang-le|prices
have gone up?will evoke negative emotion. As
far as we are aware, this is the first sentiment
analysis scheme that tries to exploit people?s
expectation towards nouns.
3.3 Determination of DSAAs
The SO of DSAAs in a given phrase can be
calculated by Eq. (1).
1 if a is positive-like
C(a) =
-1 if a is negative-like
???
1 if n is positive expectation
C(n) =
-1 if n is negative expectation
???
SO(a)=C(a)*C(n)
If adverb=?? bu|not?, SO(a)= -SO(a)
Where C(a) denotes the category of DSAAs; C(n)
denotes the sentiment expectation of nouns;
SO(a) is the SO of DSAAs in a give noun-
adjective phrase. When the adverb is the
negation term ?? bu|not?, the SO is reversed.
(1)
1193
4 Predicting Sentiment Expectation of
Noun
4.1 Pattern-based Prediction Using a Web
Search Engine
In natural language, there are some lexico-
syntactic patterns that people frequently use
when they express their opinion about something.
For example:
(3) ? ?? ? ? | Salary is a little low.
(4) ? ? ? ? ?| Price is a little high.
The pattern ?<n> ?? <a>? carries a strong
negative association in Chinese language. When
a man is saying ??????| Salary is a little
low?, it indicates that he wishes his ???
|salary? to be raised. On the contrary, when a
man is saying ?????? | price is a little
high?, it indicates that he wishes ??? |price?
to go down. As a result, ??? |salary? has
positive expectation while ??? |price? has
negative expectation.
With the rapid development and expansion of
the internet, Web has become an important
medium for people to post their ideas. The
opinions expressed on the Web reflect the
common cognition shared by collection of
people in a culture. Therefore, using a Web
search engine with the strong-polar-steering
lexico-syntactic patterns as queries, we can infer
the sentiment expectation of a noun, by
calculating its statistical association with
positive and negative hits.
As an example, using the search engine
Baidu 2 with the pattern ?<n> ?? <a>? as
queries, we obtain the following hits:
(5) ? ?? ? ? | Salary is a little low. (2890 hits)
? ?? ? ? | Salary is a little high (67 hits)
(6) ? ? ? ? ? | Price is a little high. (19400 hits)
? ? ? ? ? | Price is a little low. (1080 hits)
The more than 40 times more numerous hits for
?????? |Salary is a little low?indicate that
that ???|salary?is a positive expectation noun.
For the same reason, we can infer that ???
|price?has negative expectation.
DSAAs are classified into two opposite sets
Pa and Na, as listed in (1) and (2) respectively.
2 http://baidu.com.cn.
Here two-character adjectives (??? |huge?and
??? |great?) are discarded. Four types of
lexico-syntactic patterns, which are also
classified into two opposite sets in consistent
with Pa and Na, are used in this paper, as listed
in Table 2. These patterns were manually
designed, inspired by linguistic knowledge and
after a deep investigation on the Web.
Pos. expectation patterns Neg. expectation patterns
1) <n>?? Na
n is a little Na
2) <n>??? Na
n is a little Na
3) <n> Na, ???
n is Na, what should we
do?
4)? <n> Na
n is too Na
1) <n>?? Pa
n is a little Pa
2) <n>??? Pa
n is a little Pa
3) <n> Pa, ???
n is Pa, what should we
do?
4)? <n> Pa
n is too Pa
Table 2: The lexico-syntactic patterns
Here the noun (n) in these patterns was
instantiated by 9,468 nouns in our collected data.
A noun has together 48 patterns, 24 positive and
24 negative ones. For each noun, we obtain the
hits of both positive and negative expectation
patterns, using the search engine Baidu. The
sentiment expectation of a noun is acquired by
Eq. (2) and Eq. (3), where the magnitude of
_ ( )PT SO n can be considered as the strength of
sentiment expectation.
4
1
4
1
_ ( ) ( , )
( , )
i
b Na i
i
a Pa i
PT SO n PositivePatternHit n b
NegativePatternHit n a
? ?
? ?
?
?
??
??
positive expectation if _ ( )>0
n is negative expectation if _ ( )<0
not predicted if _ ( )=0
PT SO n
PT SO n
PT SO n
???
??
(3)
Table 3 gives some nouns with sentiment
expectation predicted by the pattern-based
method, descending (the left column) and
ascending (the right column) by the absolute
value of _ ( )PT SO n . Most words (9 out of 10)
are correctly predicted, demonstrating that the
result of pattern-based method is promising. The
only wrong predicted noun is ??? |feeling?,
due to the fact that most instances of it on the
Web data are used as verb rather than noun, like
??????| I think it is large?.
(2)
1194
Positive expectation Negative expectation
Noun ( _ ( )PT SO n ) Noun ( _ ( )PT SO n )
?|money (31349) ? ? |temperature(-111576)
??|wage (26311 ) ? ?|noise (-45790)
? ?|feeling (20102) ? ? |price (-25653)
? ? |income(19429) ? ? |cost (-22051)
? |officer (10630) ?? |blood pressure (-21788)
Table 3: Examples of nouns with sentiment
expectation predicted by the pattern-based method
4.2 Character-based Derivation Using
Sentiment Lexicons
But the sentiment expectation of some nouns
cannot be predicted with the pattern-based
method, mainly due to the reason that these
nouns don?t occur in the listed patterns in Table
2. An alternate way is to exploit the semantic
knowledge of Chinese characters. It is assumed
that there is a strong association between the
sentiment category of a word and its component
characters. For example, the three words ???
zui?e|evil, ? ? zuixing|crime, ? ?
zuiguo|fault?, which all contain the character ??
zui|sin? that carries negative meaning, are all
negative expectation nouns.
First, we compute the character-word
sentiment association by the following PMI
formula, based on a sentiment lexicon:
( , )
, log
( ) ( )
P c Positive
PMI c Positive
P c P Positive
? ?=
( , )
, log
( ) ( )
P c Negative
PMI c Negative
P c P Negative
? ?=
( ) ( , ) ( , )SO c PMI c Positive PMI c Negative? ?
Where ( , )P c Positive is the probability of a
character c in the positive category; ( )P c is the
probability of a character c in the sentiment
lexicon; ( )P Positive is the probability of the
positive category in the sentiment lexicon.
,PMI c Negative? ? has the similar meaning.
Probabilities are estimated according to the
maximum likelihood principle.
The open language resources for Chinese
sentiment analysis are quite limited. We selected
the following two sentiment lexicons.
Sentiment HowNet. HowNet has published
the Chinese vocabulary for sentiment analysis3,
3 http://www.keenage.com/html/c_index.html.
which was manually constructed. The positive
category contains 4,566 words and the negative
category contains 4,370 words.
Sentiment BaiduHit. In our collected data,
we extracted 9,468 nouns. Using the pattern-
based method we acquired sentiment expectation
of these nouns, where 2,530 ones were assigned
as positive expectation, 1,837 ones as negative
expectation and 5,101 ones were not predicted. It
is assumed that most nouns are correctly
predicted. These nouns with their sentiment
expectation constitute the lexicon of Sentiment
BaiduHit, which is automatically constructed.
Combining HowNet and BaiduHit. Most
sentiment characters derived from HowNet have
adjective property, since most words in
Sentiment HowNet are adjectives. On the
contrary, most sentiment characters derived from
BaiduHit have noun property. Therefore, the
combination of the two lexicons can cover more
characters. As Sentiment HowNet is manually
compiled, the sentiment characters derived from
it should be more reasonable than those from
BaiduHit. When combining the two lexicons in
computing character polarity, we assign a high
priority to HowNet. Only when a character is out
of vocabulary in HowNet, we resort to BaiduHit.
Then, we acquire the sentiment category of a
word by computing the following equation. Let a
word consist of n characters 1 2, nw c c c? ?... ,
the sentiment category of the word is calculated
by the average sentiment value of its component
characters:
1
1_ ( ) ( )
n
i
i
CH SO w SO c
n ?
? ? (5)
positive expectation if _ ( )>0
w is negative expectation if _ ( )<0
neutral if _ ( )=0
CH SO w
CH SO w
CH SO w
???
??
(6)
We acquired sentiment expectation of 9,468
nouns in our collected data, based on Sentiment
HowNet, Sentiment BaiduHit, and the
combination of the two lexicons, respectively.
Table 6 gives examples of nouns with
sentiment expectation acquired by the character-
based method combining the two lexicons of
HowNet and BaiduHit, descending (the left
column) and ascending (the right column) by the
absolute value of _ ( )CH SO w .
(4)
1195
Positive expectation Negative expectation
Noun( _ ( )CH SO w ) Noun( _ ( )CH SO w )
? ? |good name (3.23) ? |ash (-3.22)
? ? |health (3.06) ? |gross (-2.93)
?|fragrance (3.05) ? |tax (-2.89)
? ? |U.S.A (2.98) ? ? |fault (-2.84)
? ? |title (2.64) ? |poison (-2.82)
Table 4: Example of nouns with sentiment
expectation predicted by the character-based method
4.3 Integrating Pattern-based Prediction
and Character-based Derivation
The two methods of pattern-based prediction
and character-based derivation have
complementary properties. The pattern-based
method concentrates on a word?s usage on the
Web, whereas the character-based method
focuses on the internal structure of a word. So
the two methods can be integrated to get better
performance. The results using pattern-based
method are much better than character-based
method, as illustrated in Table 3 and Table 4. So
in the integrated scheme, we give a high priority
to pattern-based method. The pattern-based
approach is mainly used, and only when the
value of | _ ( ) |PT SO n is smaller than a threshold
r, the character-based method is adopted.
Because when the value of | _ ( ) |PT SO n is very
small, it could be caused by random noises on
the Web. We set r to 9 according to empirical
analysis in the development data.
5 Experiments
5.1 Sentiment Analysis at Sentence Level
5.1.1 Data
We collected data from two sources. The main
part was extracted from Xinhua News Agency of
Chinese Gigaword (Second Edition) released by
LDC. The texts were automatically word-
segmented and POS-tagged using the open
software ICTCLAS4. In order to concentrate on
the disambiguation of DSAAs, and reduce the
noise introduced by the parser, we extracted
sentences containing strings in pattern of (7),
where the target noun is modified by the
adjective in most cases.
4 http://www.ictclas.org/.
(7) noun+adverb+adjective (adjective?DSAAs)
e.g. ??/n ?/d ?/a | the cost is low.
Another small part of data was extracted from
the Web. Using the search engine Google5, we
searched the queries as in (8):
(8) ? | very+ adjective (adjective?DSAAs )
From the returned snippets, we manually picked
out some sentences that contain the strings of (7).
Also, the sentences were automatically word-
segmented and POS-tagged using ICTCLAS.
DSAAs in the data were assigned as positive,
negative or neutral, independently by two
annotators. Since we focus on the distinction
between positive and negative categories, the
neutral instances were removed. Table 5 gives
statistics of the data, and the inter-annotator
agreement is in a high level with a kappa of 0.91.
After cases with disagreement were negotiated
between the two annotators, a gold standard
annotation was agreed upon. In this paper, 3066
instances were divided randomly into three parts,
1/3 of which were used as the development data,
and 2/3 were the test data.
Most of the data has been used as the
benchmark dataset of SemEval-2010 task 18
?disambiguating sentiment ambiguous
adjectives?(Wu and Jin, 2010), and so it can be
downloaded freely for research.
Table 5: The statistics of DSAAs data
5.1.2 Baseline
We conducted two types of baseline.
Simple Baseline. Not considering the context,
assign all positive-like adjectives as positive, and
all negative-like adjectives as negative.
HowNet Baseline. Acquiring SO of nouns
from Sentiment HowNet, the polarity of DSAAs
is computed by Eq. (1).
5.1.3 Methods
Pattern-based method. Acquiring sentiment
expectation of nouns using the pattern-based
method, the polarity of DSAAs is computed by
Eq.(1).
5 http://www.google.com/.
Pos# Neg# Total#
Pos# 1280 58 1338
Neg# 72 1666 1738
Total# 1352 1724 3066
1196
Character-based method. Acquiring
sentiment expectation of nouns using the
character-based method, based on Sentiment
HowNet, Sentiment BaiduHit and the
combination of the two lexicons respectively, the
polarity of DSAAs is computed by Eq.(1).
Integrated method. Acquiring sentiment
expectation of nouns by integrating pattern-
based and character-based methods, the polarity
of DSAAs is computed by Eq. (1).
5.1.4 Results
Table 6 gives the experimental results at
sentence level with different methods.
Methods Pre. Rec. F
Simple Baseline 61.20 61.20 61.20
HowNet Baseline 97.58 9.88 17.94
Pattern-based 75.83 71.67 73.69
Character-based (HowNet) 69.89 69.37 69.63
Character-based (BaiduHit) 68.66 68.59 68.62
Character-based (Combined) 71.01 70.94 70.97
Integrated method 78.52 78.52 78.52
Table 6: The experimental results at sentence level
As for the simple baseline, both the precision
and recall are low, suggesting that DSAAs
cannot be neglected for sentiment analysis in a
real-world application.
The HowNet baseline achieves a quite high
precision of 97.58%, but a rather poor recall of
9.88%, suggesting that SO of nouns described in
traditional sentiment lexicon, like HowNet,
cannot effectively disambiguate DSAAs.
The proposed methods in this paper all yield
results that are substantially better than two
types of baseline. The pattern-based method, as
straightforward as it is, achieves promising result
with an f-score of 73.69%, which is 12.49%
higher than the simple baseline. The pattern-
based method outperforms the character-based
method (combined) by 4.82% in precision and
0.73% in recall. The performance of the
character-based method based on Sentiment
BaiduHit is competitive with that based on
Sentiment HowNet, which again proves the
effectiveness of the pattern-based method. The
character-based method combining the two
lexicons outperforms each lexicon with small
improvement. The approach integrating pattern-
based and character-based methods outperforms
each method in isolation, achieving an f-score of
78.52% that is 17.32% higher than the simple
baseline and 60.58% higher than HowNet
baseline.
5.2 Sentiment Analysis at Document Level
5.2.1 Data
We also investigated the impact of
disambiguating DSAAs on the sentiment
classification of product reviews. Following the
work of Wan (2008), we selected the same
dataset. The dataset contains 886 Chinese
product reviews, which are manually annotated
with polarity labels: positive or negative. Also,
the files are automatically word-segmented and
POS-tagged using ICTCLAS. We extracted the
files that contain the following strings, where the
nouns are modified by DSAAs in most cases.
(9) noun+adjective (adjective?DSAAs)
noun+adverb+adjective
noun+adverb+adverb+adjective.
We obtained 212 files, up to 24% of the overall
data, suggesting again that DSAAs are
frequently used in product reviews and cannot be
avoided in a real-world application.
5.2.2 Methods
Our goal is not to propose a new method, but
instead to test the performance gain by adding
the disambiguation of DSAAs. We adopted the
same algorithm with Wan (2008), and also used
Sentiment-HowNet. But in our experiment,
Negation_Dic contains only one term ??
bu|not?, for the sake of repeatable experiments.
The baseline algorithm is illustrated by the
non-italic part in Figure 1, where we set the
same parameters with Wan?s approach:
PosValue=1, NegValue=-2, q=2, ?=2.
We added the disambiguation of DSAAs to
the algorithm, as illustrated by the italic part in
Figure 1. When a word is a DSAA, compute its
SO with the proposed integrated method, rather
than using its prior polarity specified in HowNet.
For Dy_PosValue and Dy_NegValue, we first set
Dy_PosValue=1 and Dy_NegValue=-2, just the
same as PosValue and NegValue. In the second
attempt, in order to further intensify the polarity
of DSAAs, we set Dy_PosValue=1.5 and
Dy_NegValue=-2.5. Other parameters were set
the same as baseline.
1197
Algorithm Compute_SO:
1. Tokenize document d into sentence set S, and each
sentence s?S is tokenized into word set Ws;
2. For any word w in a sentence s?S, compute its
value SO(w) as follows:
1) if w?DSAAs, compute SO(w) with the
integrated method.
If SO(w)=1, SO(w)=Dy_PosValue;
If SO(w)=-1, SO(w)=Dy_NegValue;
2) if w?Positive_Dict, SO(w)=PosValue;
3) If w?Negative_Dict, SO(w)=NegValue;
4) Otherwise, SO(w)=0;
5) Within the window of q words previous to w, if
there is a term w'?Negation_Dict,
SO(w)= ?SO(w);
6) Within the window of q words previous to w, if
there is a term w'?Intensifier_Dict,
SO(w) =??SO(w);
3. ( ) ( )S d SO w
s S w Ws
? ? ?
? ?
Figure 1: Algorithm of computing SO of documents
5.2.3 Results
Adding the disambiguation of DSAAs, the
performance of sentiment classification of 212
product reviews was significantly improved, as
shown in Table 7.
Baseli
ne
DSAAs
(1, -2)
DSAAs
(1.5, -2.5)
Pre. 75.89 77.50 76.61
Rec. 78.70 86.11 87.96Pos.
F 77.27 81.58 81.90
Pre. 87.01 88.46 87.06
Rec. 64.42 66.35 71.15Neg.
F 74.03 75.82 78.31
MacroF 75.62 78.60 80.06Total Accu. 71.70 76.42 79.72
Table 7: The experimental results at document level
As an example, the following review, which
consists of only one sentence, is correctly
classified as positive by DSAAs method, but is
classified as negative by the baseline approach.
(10) ? ? ? , ? ? ? , ? ? ? ? ? ?
| Small size, light weight, and easy to carry.
According to HowNet, as shown in Table 8, the
sentence contains two negative words ??
|small?and ??|light?and one positive word ??
? fangbian|easy?, resulting the overall negative
prediction. In our approach, ??? tiji|size?and
?? ? zhongliang|weight? are assigned as
negative expectation, and consequently both ??
??|small size?and ????|light weight?have
positive meaning, resulting the overall positive
prediction.
Pos. ? |large, ? |high, ? |thick, ? |deep,
?|heavy, ??|great
Neg. ? |small, ? |low, ? |thin, ? |shallow,
?|light
OOV ?|many, ?|few, ??|huge
Table 8: The SO of DSAAs described in HowNet
Adding the disambiguation of DSAAs, our
method obviously outperforms the baseline by
4.44% in f-score and 8.02% in accuracy. The
improvement in recall is especially obvious.
When intensifying the polarity of DSAAs by
setting Dy_PosValue=1.5 and Dy_NegValue=-
2.5, the recall is improved by 9.26% for positive
category and 6.73% for negative category.
6 Conclusion and Future Work
This paper presents a knowledge-based
unsupervised method to automatically
disambiguate dynamic sentiment ambiguous
words, focusing on 14 DSAAs. We exploit
pattern-based and character-based methods to
infer sentiment expectation of nouns, and then
determine the polarity of DSAAs based on the
nouns. For the sentiment analysis at sentence
level, our method achieves promising result that
is significantly better than two types of baseline,
which validates the effectiveness of our
approach. We also apply the disambiguation of
14 DSAAs to the sentiment classification of
product reviews, resulting obvious improvement
in performance, which proves the significance of
the issue.
There leaves room for improvement. Our
future work will explore more contextual
information in disambiguating DSAAs. In
addition, we will find out new methods to reduce
noises when mining the Web to infer sentiment
expectation of nouns. Discovering the lexico-
syntactic patterns for sentiment expectation of
nouns automatically or semi-automatically with
bootstrapping method is also a challenging
direction.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (No. 60703063)
and National Social Science Foundation of
China (No. 08CYY016).
1198
References
Andreevskaia A. and Bergler S. 2006. Sentiment
tagging of adjectives at the meaning level. The
19th Canadian Conference on Artificial
Intelligence.
Andreevskaia, A. and Bergler, S. 2006. Mining
WordNet for fuzzy sentiment: Sentiment tag
extraction from WordNet glosses. Proceedings of
EACL 2006.
Chen, C-J. 2004. Character-sense association and
compounding template similarity: automatic
semantic classification of Chinese compounds.
Proceedings of the 3rd workshop on Chinese
language processing.
Ding X., Liu B. and Yu, P. 2008. A holistic lexicon-
based approach to opinion mining. Proceedings of
WSDM?08.
Esuli, A. and Sebastiani, F. 2006. SentiWordNet: a
publicly available lexical resource for opinion
mining. Proceedings of LREC?06.
Hatzivassiloglou, V. and McKeown, K. 1997
Predicting the semantic orientation of adjectives.
Proceedings of ACL?97.
Kim, S and Hovy, E. 2004. Determining the
sentiment of opinions. Proceedings of COLING?04.
Ku, L, Liang Y. and Chen, H. 2006. Opinion
extraction, summarization and tracking in news
and blog corpora. Proceedings of AAAI-2006
Spring Symposium on Computational Approaches
to Analyzing Weblogs.
Lu X-F, 2007. Hybrid models for semantic
classification of Chinese unknown words.
Proceedings of NAACL HLT?07..
Pang, B. and Lee, L. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in
Information Retrieval.
Riloff, E. and Wiebe, J. 2003. Learning Extraction
Patterns for Subjective Expressions. Proceedings
of EMNLP?03.
Riloff, E., Wiebe, J. and Phillips, W. 2005. Exploiting
Subjectivity Classification to Improve Information
Extraction. Proceedings of AAAI?05.
Su, F. and Markert, K. 2008. From words to senses: a
case study of subjectivity recognition. Proceedings
of COLING?08.
Takamura, H., Inui,T. and Okumura, M. 2006. Latent
Variable Models for Semantic Orientations of
phrases. Proceedings of EACL?06.
Takamura, H., Inui,T. and Okumura, M. 2007.
Extracting Semantic Orientations of Phrases from
Dictionary. Proceedings of NAACL HLT ?07.
Turney, P. and Littman, M. 2003. Measuring praise
and criticism: inference of semantic orientation
from association. ACM transaction on information
systems.
Wan, X. 2008. Using Bilingual Knowledge and
Ensemble Techniques for Unsupervised Chinese
Sentiment Analysis. Proceedings of EMNLP?08.
Wiebe, J. and Mihalcea, R. 2006. Word sense and
subjectivity. Proceedings of ACL?06.
Wiebe, J., Wilson, T., Bruce, R., Bell, M. and Martin,
M. 2004. Learning Subjective Language.
Computational Linguistics.
Wilson, T., Wiebe, J. and Hoffmann, P. 2005.
Recognizing contextual polarity in phrase-level
sentiment analysis. Proceedings of
HLT/EMNLP?05.
Wu, Y. and Jin, P. 2010. SemEval-2010 task 18:
disambiguating sentiment ambiguous adjectives.
Proceedings of SemEval 2010.
Yuen R., Chan T., Lai T., Kwong O., T?sou B. 2004.
Morpheme-based derivation of bipolar semantic
orientation of Chinese words. Proceedings of
COLING?04.
1199
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 836?842,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extracting Events with Informal Temporal References in Personal
Histories in Online Communities
Miaomiao Wen, Zeyu Zheng, Hyeju Jang, Guang Xiang, Carolyn Penstein Rose?
Language Technologies Institute, Carnegie Mellon University
{mwen,zeyuz,hyejuj,guangx,cprose}@cs.cmu.edu
Abstract
We present a system for extracting the
dates of illness events (year and month of
the event occurrence) from posting histo-
ries in the context of an online medical
support community. A temporal tagger re-
trieves and normalizes dates mentioned in-
formally in social media to actual month
and year referents. Building on this, an
event date extraction system learns to in-
tegrate the likelihood of candidate dates
extracted from time-rich sentences with
temporal constraints extracted from event-
related sentences. Our integrated model
achieves 89.7% of the maximum perfor-
mance given the performance of the tem-
poral expression retrieval step.
1 Introduction
In this paper we present a challenging new event
date extraction task. Our technical contribution
is a temporal tagger that outperforms previously
published baseline approaches in its ability to
identify informal temporal expressions (TE) and
that normalizes each of them to an actual month
and year (Chang and Manning, 2012; Strotgen
and Gertz, 2010). This temporal tagger then con-
tributes towards high performance at matching
event mentions with the month and year in which
they occurred based on the complete posting his-
tory of users. It does so with high accuracy on
informal event mentions in social media by learn-
ing to integrate the likelihood of multiple candi-
date dates extracted from event mentions in time-
rich sentences with temporal constraints extracted
from event-related sentences.
Despite considerable prior work in temporal in-
formation extraction, to date state-of-the-art re-
sources are designed for extracting temporally
scoped facts about public figures/organizations
from newswire or Wikipedia articles (Ji et al,
2011; McClosky and Manning, 2012; Garrido et
[11/15/2008] I have noticed some pulling recently and I 
won't start rads until March.
[11/20/2008] It is sloowwwly healing, so slowly, in fact, 
that she said she HOPES it will be healed by March, when 
I am supposed to start rads.
[1/13/2009] I still have one last chemo to go on the 19th 
and then start rads in 5 wks.
[1/31/2009] I go for my first meeting with the rad onc on 
 2/10 (my 50th birthday!).
[2/23/2009] I had my first rad today.
[3/31/2009] Tomorrow will be my last full rads
[4/2/2009] I started rads in Feb, just did #29 today.
[4/8/2009] The rad onc wants to see me again next week 
for a skin check as I have had cellulitis twice since August.
[6/21/2010] My friend Lisa had her port put in last week 
and will begin 2 weeks of radiation on Tuesday.
Figure 1: User posts containing keywords for the
start of Radiation. Event keywords are in bold and
temporal expressions are in italics.
al., 2012). When people are instead communi-
cating informally about their lives, they refer to
time more informally and frequently from their
personal frame of reference rather than from an
impersonal third person frame of reference. For
example, they may use their own birthday as a
time reference. The proportion of relative (e.g.,
?last week?, ?two days from now?), or personal
time references in our data is more than one and a
half times as high as in newswire and Wikipedia.
Therefore, it is not surprising that there would be
difficulty in applying a temporal tagger designed
for newswire to social media data (Strotgen and
Gertz, 2012; Kolomiyets et al, 2011). Recent be-
havioral studies (Choudhury et al, 2013; Park and
Choi, 2012; Wen et al, 2012) demonstrate that
user-focused event mentions extracted from social
media data can provide a useful timeline-like tool
for studying how behavior patterns change over
time in response to mentioned events. Our re-
search contributes towards automating this work.
2 Task
Our task is to extract personal illness events men-
tioned in the posting histories of online commu-
nity participants. The input to our system is
836
a candidate event and a posting history. The
output is the event date (month and year) for
the event if it occurred, or ?unknown? if it
did not occur. The process iterates through a
list of 10 cancer events (CEs). This list in-
cludes breast cancer Diagnosis, Metastasis, Re-
currence, Mastectomy, Lumpectomy, Reconstruc-
tion, Chemotherapy-Start, Chemotherapy-End,
Radiation-Start and Radiation-End. For each of
these target CEs, we manually designed an event
keyword set that includes the name of the event,
abbreviations, slang, aliases and related words.
For each of the 10 events, all sentences that
mention a related event keyword are extracted
from the user?s posting history. Figure 1 shows
sevaral sentences that were extracted for one user
for the start date of Radiation. The task is to de-
termine that the beginning of this user?s Radiation
therapy was 2/2009. Note that the user began to
post about Radiation before she started it. She first
reported planning to start Radiation in March, but
then rescheduled for February. Most of the TEs
are non-standard and need to be resolved to calen-
dar dates (year and month).
Once the full set of event mention sentences has
been extracted for a user, all the temporal expres-
sions (TEs) that appear in the same sentence with
an event mention are resolved to a set of candi-
date dates. Besides a standard event-time classi-
fier for within-sentence event-time anchoring, we
leverage a new source of temporal information to
train a constraint-based event-time classifier. Pre-
vious work only retrieves time-rich sentences that
include both the query and some TEs (Ji et al,
2011; McClosky and Manning, 2012; Garrido et
al., 2012). However, sentences that contain only
the event mention but no explicit TE can also be
informative. For example, the post time (usually
referred to as document creation time or DCT) of
the sentence ?metastasis was found in my bone?
might be labeled as being after the ?metastasis?
event date. These DCTs impose constraints on
the possible event dates, which can be integrated
with the event-time classifier, as a variant on re-
lated work(Chambers, 2012).
3 Related Work
Previous work on TE extraction has focused
mainly on newswire text (Strotgen and Gertz,
2010; Chang and Manning, 2012). This paper
presents a rule-based TE extractor that identifies
and resolves a higher percentage of nonstandard
TEs than earlier state-of-art temporal taggers.
Our task is closest to the temporal slot filling
track in the TAC-KBP 2011 shared task (Ji et al,
2011) and timelining task (McClosky and Man-
ning, 2012). Their goal was to extract the tempo-
ral bounds of event relations. Our task has two key
differences. First, they used newswire, Wikipedia
and blogs as data sources from which they extract
temporal bounds of facts found in Wikipedia in-
foboxes. Second, in the KBP task, the set of gold
event relations are provided as input, so that the
task is only to identify a date for an event that is
guaranteed to have been mentioned. In our task,
we provide a set of potential events. However,
most of the candidate events won?t have ever been
reported within a user?s posting history.
Temporal constraints have proven to be use-
ful for producing a globally consistent timeline.
In most temporal relation bound extraction sys-
tems, the constraints are included as input rather
than learned by the system (Talukdar et al, 2012;
Wang et al, 2011). A notable exception is Mc-
Closkyet al (2012) who developed an approach to
learning constraints such as that people cannot at-
tend school if they have not been born yet. A no-
table characteristic of our task is that constraints
are softer. Diseases may occur in very different
ways across patients. Recurring illnesses falsely
appear to have an unpredictable order. Thus, there
can be no universal logical constraints on the order
of cancer events.
Our approach to using temporal constraints is a
variant on previously published approaches. Gar-
rido et al (2012) made use of DCT (document cre-
ation time) as well, however, they have assumed
the DCT is within the time-range of the event
stated in the document, which is often not true
in our data. Chambers (2012) utilized the within-
sentence time-DCT relation to learn constrains for
predicting DCT. We learn the event-DCT relations
to produce constrains for the event date.
4 Corpus Annotation
We have scraped the posts, users, and profiles from
a large online cancer support community. From
this collection we extracted and then annotated
two separate corpora, one for evaluating our TE
retrieval and normalization, the other one for event
date extraction.
For creating the TE extraction corpus, we ran-
837
domly picked one post from each of 1,000 ran-
domly selected users. We used this sampling tech-
nique because each user tends to use a narrow
range of date expression forms. From these posts,
we manually extracted 601 TEs and resolved them
to a specific month and year or just year if the
month was not mentioned. Events not reported
to have occurred were annotated as ?unknown?.
Our corpus for event date extraction consists of
the complete posting history of 300 users that were
randomly drawn from our dataset. Three annota-
tors were provided with guidelines for how to in-
fer the date of the events (Wen et al, 2013). We
achieved .94 Kappa on identification of whether an
event has a reported event date in a user?s history
or not. In evaluation of agreement on extracted
dates, we achieved a .99 Cronbach?s alpha. From
this corpus, 509 events were annotated with occur-
rence dates (year and month). In our evaluation,
we use data from 250 users for training, and 50 for
testing.
5 Method
Now we explain on a more technical level how our
system works on our task. Given an event and a
user?s post history, the system searches for all of
the sentences that contain an event keyword (key-
word sentence) and all the sentences that contain
both a keyword and a TE (date sentence). The TEs
in the date sentences are resolved and then used as
candidate dates for the event. For selecting among
candidate dates, our model integrates two main
components. First, the Date Classifier is trained
from date sentences to predict how likely its can-
didate TE and the gold event date are to overlap.
Then, because constraints over event dates can be
informed by temporal relations between the event
date and the DCT, the Constraint-based Classifier
provides an indication of the plausibility of can-
didate dates. The integrated system combines the
predictions from both classifiers.
5.1 Temporal Tagger
We design a rule-based temporal tagger that is
built using regular expression patterns to recog-
nize informal TEs. Similar to SUTime (Chang and
Manning, 2012), we identify and resolve a wide
range of non-standard TE types such as ?Feb ?07
(2/2007)?. The additional types of TE we han-
dle include: 1)user-specific TEs: A user?s age,
cancer anniversary and survivorship can provide
temporal information about the user?s CEs. We
obtain the birth date of users from their personal
profile to resolve age date expressions such as ?at
the age of 57?. 2)non-whole numbers such as ?a
year and half? and ?1/2 weeks?. 3)abbreviations
of time units : e.g. ?wk? as the abbreviation of
?week?. 4)underspecified month mentions, we
resolve the year information according to the DCT
month, the mentioned month and the verb tense.
5.2 Date Classifier
We train a MaxEnt classifier to predict the tem-
poral relationship between the retrieved TE and
the event date as overlap or no-overlap, similar
to the within-sentence event-time anchoring task
in TempEval-2 (UzZaman and Allen, 2010). Fea-
tures for the classifier include many of those in
(McClosky and Manning, 2012; Yoshikawa et al,
2009): namely, event keyword and its dominant
verb, verb and preposition that dominate TE, de-
pendency path between TE and keyword and its
length, unigram and bigram word and POS fea-
tures. New features include the Event-Subject,
Negative and Modality features. In online sup-
port groups, users not only tell stories about them-
selves, they also share other patients? stories (as
shown in Figure 1). So we add subject fea-
tures to remove this kind of noise, which in-
cludes the governing subject of the event key-
word and its POS tag. Modality features include
the appearance of modals before the event key-
word (e.g., may, might). Negative features include
the presence/absence of negative words (e.g., no,
never). These two features indicate a hypothetical
or counter-factual expression of the event.
To calculate the likelihood of a candidate date
for an event, we need to aggregate the hard de-
cisions from the classifier. Let DSu be the set
of the user?s date sentences, let Du be the set of
dates resolved from each TE. We represent a Max-
Ent classifier by Prelation(R|t, ds) for a candidate
date t in date sentence ds and possible relation
R = {overlap, no-overlap}. We map the distri-
bution over relations to a distribution over dates
by defining PDateSentence(t|DSu):
PDateSentence(t|DSu) = (1)
1
Z(Du)
?
tj?Du
?tj (t)Prelation(overlap|tj , dsj)
?tj (t) =
{
1 if t = tj
0 otherwise
838
We refer to this model as the Date Classifier.
5.3 Constraint-based Classifier
Previous work only retrieves time-rich sentences
(i.e., date sentences) (Ling and Weld, 2010; Ji et
al., 2011; McClosky and Manning, 2012; Garrido
et al, 2012). However, keyword sentences can in-
form temporal constraints for events and therefore
should not be ignored. For example, ?Well, I?m
officially a Radiation grad!? indicates the user has
done radiation by the time of the post (DCT). ?Ra-
diation is not a choice for me.? indicates the user
probably never had radiation. The topic of the
sentence can also indicate the temporal relation.
For example, before chemotherapy, the users tend
to talk about choices of drug combinations. After
chemotherapy, they talk about side-effects.
This section departs from the above Date Clas-
sifier and instead predicts whether each keyword
sentence is posted before or overlap-or-after the
user?s event date. The goal is to automatically
learn time constraints for the event. This task is
similar to the sentence event-DCT ordering task
in TempEval-2 (UzZaman and Allen, 2010). We
create training examples by computing the tempo-
ral relation between the DCT and the user?s gold
event date. If the user has not reported an event
date, the label should be unknown.
We train a MaxEnt classifier on each event
mention paired with its corresponding DCT. All
the features used in the classifier component that
are not related to the TEs are included. Let
KSu be the set of the user?s keyword sentences,
let Du be the set of dates resolved from each
date sentence. We define a MaxEnt classifier by
Prelation(R|ks) for a keyword sentence ks and
possible relation R = {before, overlap-or-after,
unknown}. DCT is the post time of the keyword
sentence ks. The rel(DCT, t) function simply de-
termines if the DCT is before or overlap-or-after
the candidate date t. We map this distribution over
relations to a distribution over dates by defining
PKeywordSentence(t,KSu):
PKeywordSentence(t,KSu) = (2)
1
Z(Du)
?
ksj?KSu
Prelation(rel(dctj , t)|ksj)
rel(dct, t) =
{
before if dct < t
overlap-or-after if dct ? t
5.4 Integrated Model
Given the Date Classifier of Section 5.2 and the
Constraint-based Classifier of Section 5.3, we cre-
ate a Integrated Model combining the two with the
following linear interpolation as follows:
P (t|postsu) = ?PDateSentence(t|DSu)
+ (1? ?)PKeywordSentence(t|KSu)
where t is a candidate event date. The system will
output t that maximizes P (t|postsu) and unknown
if DSu is empty. ? was set to 0.7 by maximizing
accuracy using five-fold cross-validation over the
training set.
6 Evaluation Metric and Results
6.1 Temporal Expression Retrieval
We compare our temporal tagger?s performance
with SUTime (Chang and Manning, 2012) on the
601 manually extracted TEs. We exclude user-
specific TEs such as birthday references since SU-
Time cannot handle those. We first evaluate iden-
tification of the extent of a TE and then production
of the correctly resolved date for each recognized
expression. Table 1 shows that our tagger has sig-
nificantly higher precision and recall for both.
P R F1
Extents SUTime 97.5 75.4 85.0
Our tagger 97.9 91.8 94.8
Normalization SUTime 89.4 71.2 79.3
Our tagger 91.3 85.5 88.3
Table 1: Temporal expression retrieval results
6.2 Event-date Extraction
6.2.1 Evaluation metric
The extracted date is only considered correct if it
completely matches the gold date. For less than
4% of users, we have multiple dates for the same
event (e.g., a user had a mastectomy twice). Sim-
ilar to the evaluation metric in a previous study(Ji
et al, 2011), in these cases, we give the system the
benefit of the doubt and the extracted date is con-
sidered correct if it matches one of the gold dates.
In previous work (McClosky and Manning, 2012;
Ji et al, 2011), the evaluation metric score is de-
fined as 1/((1 + |d|)) where d is the difference
between the values in years. We choose a much
stricter evaluation metric because we need a pre-
cise event date to study user behavior changes.
6.2.2 Baselines and oracle
Based on our temporal tagger, we provide two
baselines to describe heuristic methods of ag-
gregating the hard decisions from the classifier
839
Baseline1 Baseline2 Date Integrated Oracle
CE count P R F1 P R F1 P R F1 P R F1 F1
Diagnosis 112 .64 .70 .67 .60 .66 .63 .68 .75 .71 .68 .75 .71 .80
Metastasis 7 .16 .58 .25 .12 .43 .19 .25 .86 .39 .25 .86 .39 .86
Recurrence 14 .14 .35 .20 .11 .29 .16 .13 .36 .19 .13 .36 .19 .47
Chemo-start 54 .49 .61 .54 .42 .52 .46 .52 .66 .58 .58 .74 .65 .76
Chemo-end 43 .44 .59 .50 .36 .49 .42 .47 .63 .54 .48 .66 .56 .84
Rad-start 38 .35 .47 .40 .30 .40 .34 .36 .47 .41 .40 .53 .46 .64
Rad-end 35 .48 .63 .54 .30 .39 .34 .50 .66 .57 .50 .66 .57 .84
Mastectomy 68 .58 .71 .64 .52 .62 .57 .62 .76 .68 .62 .76 .68 .77
Lumpectomy 33 .49 .71 .58 .43 .76 .46 .46 .79 .58 .46 .79 .62 .91
Reconstruction 43 .38 .57 .46 .29 .44 .35 .41 .63 .50 .43 .65 .52 .86
Table 2: Event-level five-fold cross-validation performance of models and baselines on training data.
learned in Section 5.3. The first baseline, Base-
line1, is to pick the date with the highest clas-
sifier?s prediction confidence. The second base-
line, Baseline2, is along the same lines as the
Combined Classifier used in (McClosky and Man-
ning, 2012). For example, if the candidate
date is ?6/2009? and we have retrieved two TEs
that are resolved to ?6/2009? and ?4/2008?, then
P (?6/2009?) = Prelation(overlap|?6/2009?) ?
Prelation(no-overlap|?4/2008?).
To set an upper bound on performance given our
TE retrieval system, we calculate the oracle score
by considering an extraction as correct if the gold
date is one of the retrieved candidate dates. The
oracle score can differ from a perfect score since
we can only use candidate temporal expressions
if (a)the relation is known and (b)mentions of the
event are retrievable, (c)the TE and event keyword
appear in the same sentence, and (d)our temporal
tagger is able to recognize and resolve it correctly.
6.2.3 Results
We present the performance of our models, base-
lines and the oracle in Table 2. Both the Date Clas-
sifier and Integrated model significantly outper-
form the baselines (p < 0.0001, McNemar?s test,
2-tailed). This shows the value of our approach to
leveraging redundancy of event date mentions. In-
corporating time constraints further improves the
F1 of the Date Classifier by 3%. The Integrated
model achieves 89.7% of the oracle result.
Model P R F1
Baseline1 46.1 63.7 53.5
Baseline2 39.3 54.4 45.6
Date Classifier 49.6 67.7 57.3
Integrated Model 51.0 69.3 58.8
Oracle 77.3 77.3 77.3
Table 3: Performance of systems on the test set.
Table 3 shows the performance of our systems
and baselines on individual event types. The Joint
Model derives most of its improvement from per-
formance related to the Chemotherapy/Radiation-
start date. This is mainly because Chemotherapy
and Radiation last for a period of time and there
are more event-related discussions containing the
event keyword. None of our systems improves on
cancer Metastasis and Recurrence. This is likely
due to the sparsity of these events.
7 Conclusion
We presented a novel event date extraction task
that requires extraction and resolution of non-
standard TEs, namely personal illness event dates,
from the posting histories of online community
participants. We constructed an evaluation corpus
and designed a temporal tagger for non-standard
TEs in social media. Using a much stricter stan-
dard correctness measure than in previous work,
our method achieves promising results that are sig-
nificantly better than two types of baseline. By
creating an analogous keyword set, our event date
extraction method could be easily adapted to other
datasets.
8 Acknowledgments
We want to thank Dong Nguyen and Yi-chia
Wang, who helped provide the data for this
project. The research reported here was sup-
ported by National Science Foundation grant IIS-
0968485.
840
References
Javier Artiles, Qi Li, Taylor Cassidy, Suzanne
Tamang, and Heng Ji. 2011. CUNY BLENDER
TACKBP2011 Temporal Slot Filling System De-
scription. In Proceedings of Text Analysis Confer-
ence (TAC).
Nathanael Chambers, Shan Wang, and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics.
Nathanael Chambers. 2012. Labeling documents with
timestamps: Learning from their time expressions.
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics.
Angel X. Chang and Christopher D. Manning. 2012.
SUTIME: A library for recognizing and normaliz-
ing time expressions. In 8th International Confer-
ence on Language Resources and Evaluation(LREC
2012).
De Choudhury, M., Counts, S., and Horvitz, E. 2013.
Major Life Changes and Behavioral Markers in So-
cial Media: Case of Childbirth. In Proc. CSCW
2013.
Guillermo Garrido, Anselmo Penas, Bernardo Ca-
baleiro, and Alvaro Rodrigo. 2012. Temporally An-
chored Relation Extraction. In Proceedings of the
50th annual meeting of the as-sociation for compu-
tational linguistics.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the TAC 2011 Knowledge Base Popu-
lation track. In Proceedings of Text Analysis Con-
ference (TAC).
Hyuckchul Jung, James Allen, Nate Blaylock, Will de
Beaumont, Lucian Galescu, and Mary Swift. 2011.
Building timelines from narrative clinical records:
initial results based-on deep natural language under-
standing. In Proceedings of BioNLP 2011.
Oleksandr Kolomiyets, Steven Bethard and Marie-
Francine Moens. 2011. Model-Portability Experi-
ments for Textual Temporal Analysis. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics.
Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2012. Extracting narrative time-
lines as temporal dependency structures. In Pro-
ceedings of the 50th annual meeting of the Associ-
ation for Computational Linguistics.
Xiao Ling and Daniel S Weld. 2010 Temporal infor-
mation extraction. Proceedings of the Twenty Fifth
National Conference on Artificial Intelligence.
David McClosky and Christopher D. Manning. 2012.
Learning Constraints for Consistent Timeline Ex-
traction. Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP2012).
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
47th annual meeting of the Association for Compu-
tational Linguistics.
Heekyong Park and Jinwook Choi 2012. V-model: a
new innovative model to chronologically visualize
narrative clinical texts. In Proceedings of the 2012
ACM annual conference on Human Factors in Com-
puting Systems. ACM.
Catherine Plaisant, Brett Milash, Anne Rose, Seth Wid-
off, and Ben Shneiderman. 1996. LifeLines: vi-
sualizing personal histories. In Proceedings of the
SIGCHI conference on Human factors in computing
systems.
James Pustejovsky, Jos M. Castao, Robert Ingria, Roser
Sauri, Robert J. Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir R. Radev. 2003. TimeML: Ro-
bust specification of event and temporal expressions
in text. TimeML: Robust specification of event and
temporal expressions in text. In New Directions in
Question Answering?03.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev. 2003. The Timebank corpus. In
Corpus Linguistics.
Preethi Raghavan, Eric Fosler-Lussier, and Albert M.
Lai. 2012. Learning to Temporally Order Medical
Events in Clinical Text. In Proceedings of the 50th
annual meeting of the Association for computational
Linguistics.
Jannik Strotgen and Michael Gertz. 2010. Heidel-
Time:High Quality Rule-Based Extraction and Nor-
malizationof Temporal Expressions. In SemEval
?10.
Jannik Strotgen and Michael Gertz. 2012. Temporal
Tagging on Different Domains: Challenges, Strate-
gies, and Gold Standards. In LREC2012.
Partha Pratim Talukdar, Derry Wijaya, and Tom
Mitchell. 2012. Coupled temporal scoping of re-
lational facts. In Proceedings of the fifth ACM inter-
national conference on Web search and data mining.
ACM.
Naushad UzZaman and James F. Allen. 2010. TRIPS
and TRIOS system for TempEval-2: Extracting tem-
poral information from text. In Proceedings of the
5th International Workshop on Semantic Evaluation.
Yafang Wang, Bing Yang, Lizhen Qu, Marc Spaniol,
and GerhardWeikum. 2011. Harvesting facts from
textual web sources by constrained label propaga-
tion. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Man-
agement.
841
Miaomiao Wen, Hyeju Jang, and Carolyn Rose?. 2013.
Coding Manual for Illness Event Date Extraction.
Carnegie Mellon University, School of Computer
Science, Language Technology Institute.
K.-Y. Wen, F. McTavish, G. Kreps, M. Wise, and D.
Gustafson. 2012. From diagnosis to death: A
case study of coping with breast cancer as seen
through online discussion group messages. Jour-
nal of Computer-Mediated Communication, 16:331-
361.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly identi-
fying temporal relations with markov logic. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP.
Li Zhou and George Hripcsak. 2007. Temporal rea-
soning with medical data?a review with emphasis
on medical natural language processing. Journal of
biomedical informatics 40.2 (2007): 183.
842
Proceedings of the Second Workshop on Metaphor in NLP, pages 1?10,
Baltimore, MD, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Conversational Metaphors in Use: Exploring the Contrast between
Technical and Everyday Notions of Metaphor
Hyeju Jang, Mario Piergallini, Miaomiao Wen, and Carolyn Penstein Ros
?
e
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{hyejuj, mpiergal, mwen, cprose}@cs.cmu.edu
Abstract
Much computational work has been done
on identifying and interpreting the mean-
ing of metaphors, but little work has been
done on understanding the motivation be-
hind the use of metaphor. To computation-
ally model discourse and social position-
ing in metaphor, we need a corpus anno-
tated with metaphors relevant to speaker
intentions. This paper reports a corpus
study as a first step towards computa-
tional work on social and discourse func-
tions of metaphor. We use Amazon Me-
chanical Turk (MTurk) to annotate data
from three web discussion forums cover-
ing distinct domains. We then compare
these to annotations from our own anno-
tation scheme which distinguish levels of
metaphor with the labels: nonliteral, con-
ventionalized, and literal. Our hope is that
this work raises questions about what new
work needs to be done in order to address
the question of how metaphors are used to
achieve social goals in interaction.
1 Introduction
Our goal is to understand and characterize
the ways that nonliteral language, especially
metaphors, play a role in a variety of conversa-
tional strategies. In contrast to the large body
of work on uncovering the intended propositional
meaning behind metaphorical expressions, we are
most interested in the illocutionary and perlocu-
tionary force of the same contributions.
People use metaphorical expressions in a vari-
ety of ways in order to position themselves so-
cially and express attitudes, as well as to make
their point more effective, attractive, and convinc-
ing. Metaphors can be used to describe unfa-
miliar situations and feelings when the speaker
feels that literal description is inadequate. They
can also be used to display the speaker?s creativ-
ity and wit. They can further be used as a tac-
tic for persuasion or manipulation by foreground-
ing aspects that would not ordinarily be relevant.
Cameron (2007) shows that we can understand
social interactions and their contexts better by
closely looking at these patterns of metaphor use.
Metaphors can vary in how conventionalized
they are, from those which have lost their orig-
inal concrete meanings to completely novel and
vivid metaphors. Intuitively, it also makes sense
that metaphors which are more conventional and
less obviously metaphorical will be used with
less conscious thought than more novel or vivid
metaphors. There are thus reasons to suspect
that distinguishing between levels of metaphoric-
ity could give insight into patterns of use.
In this paper, we are interested in where we
can draw a line between levels of metaphoricity.
As a first step towards our long-term goal, we
present a corpus study in three web discussion
forums including a breast cancer support group,
a Massive Open Online Course (MOOC), and
a forum for street gang members, which cover
distinctly different domains and have differing
community structure. First, we investigate how
laypeople intuitively recognize metaphor by con-
ducting Amazon Mechanical Turk (MTurk) ex-
periments. Second, we introduce a new annota-
tion scheme for metaphorical expressions. In our
annotation scheme, we try to map the metaphor
spectrum of nonliteralness to three types of lan-
guage: nonliteral, conventionalized, and literal.
Our hope is that this distinction provides some
benefit in examining the social and discourse
functions of metaphor. Next, we compare MTurk
1
results with our annotations. Different people will
place the dividing line between literal language
and metaphorical language in different places. In
this work we have the opportunity to gauge how
much everyday conceptions of metaphoricity di-
verge from theoretical perspectives and therefore
how much models of metaphoricity may need to
be adapted in order to adequately characterize
metaphors in strategic use.
The paper is organized as follows. Section 2
relates our work to prior work on annotation and
a corpus study. Section 3 describes the data used
for annotation. Section 4 illustrates the functions
metaphor serves in discourse through a qualitative
analysis of our data. Section 5 explains our anno-
tation scheme. Section 6 presents our annotation
and MTurk experiments. Section 7 discusses the
results. Section 8 concludes the paper.
2 Relation to Prior Work
In this section, we introduce the two main bodies
of relevant prior work on metaphor in language
technologies: computational metaphor processing
and metaphor annotation.
2.1 Computational Work on Metaphor
Much of of the computational work on metaphor
can be classified into two tasks: automatic identi-
fication and interpretation of metaphors.
Metaphor identification has been done using
different approaches: violation of selectional pref-
erences (Fass, 1991), linguistic cues (Goatly,
1997), source and target domain words (Stefanow-
itsch and Gries, 2006), clustering (Birke and
Sarkar, 2006; Shutova et al., 2010), and lexi-
cal relations in WordNet (Krishnakumaran and
Zhu, 2007). Gedigian et al. (2006) and Li and
Sporleder (2010) distinguished the literal and non-
literal use of a target expression in text. In addi-
tion, Mason (2004) performed source-target do-
main mappings.
Metaphor interpretation is another large part
of the computational work on metaphor. Start-
ing with Martin (1990), a number of re-
searchers including Narayanan (1999), Barn-
den and Lee (2002), Agerri et al. (2007),
and Shutova (2010) have worked on the task.
Metaphor identification and interpretation was
performed simultaneously in (Shutova, 2013;
Shutova et al., 2013b).
As we have seen so far, much of the com-
putation work has focused on detecting and un-
covering the intended meaning behind metaphor-
ical expressions. On the other hand, Klebanov
and Flor (2013) paid attention to motivations be-
hind metaphor use, specifically metaphors used
for argumentation in essays. They showed a
moderate-to-strong correlation between percent-
age of metaphorically used words in an essay and
the writing quality score. We will introduce their
annotation protocol in Section 2.2.
However, to the best of our knowledge, not
much computational work has been done on
understanding the motivation behind the use
of metaphor besides that of Klebanov and
Flor (2013). Our work hopefully lays additional
foundation for the needed computational work.
2.2 Metaphor Annotation
One of the main challenges in computational work
on metaphor is the lack of annotated datasets. An-
notating metaphorical language is nontrivial be-
cause of a lack of consensus regarding annotation
schemes and clear definitions. In this section, we
introduce some work dedicated to metaphor anno-
tation and a corpus study.
Wallington et al. (2003) conducted experiments
to investigate what identifies metaphors. Two dif-
ferent teams annotated the same text with differ-
ent instructions, one asked to label ?interesting
stretches? and the other ?metaphorical stretches?.
They also asked annotators to tag words or phrases
that indicated a metaphor nearby, in order to inves-
tigate signals of metaphoricity.
Pragglejaz Group (2007) presented a metaphor
annotation scheme, called the Metaphor Identifi-
cation Procedure (MIP), which introduced a sys-
tematic approach with clear decision rules. In this
scheme, a word is considered to be metaphorical if
it is not used according to its most basic concrete
meaning, and if its contextual meaning can be un-
derstood in comparison with the most basic con-
crete meaning. This method is relatively straight-
forward and can give high inter-reliability. De-
pending on how one decides upon the basic mean-
ing of words, this scheme can be used for different
applications. However, defining the basic mean-
ing of a word is nontrivial, and following the def-
2
inition of basic meaning introduced in the paper
tends to result in a large proportion of words be-
ing annotated as metaphor. Many of the annotated
words would not be considered to be metaphors
by a layperson due to their long and widespread
usage.
Later works by Steen (2010), Shutova and
Teufel (2010), and Shutova et al. (2013a) ex-
panded upon MIP. Steen (2010) discussed the
strengths and weaknesses of MIP, and intro-
duced the Metaphor Identification Procedure VU
University Amsterdam (MIPVU). Shutova and
Teufel (2010) and and Shutova et al. (2013a)
added a procedure for identifying underlying con-
ceptual mappings between source and target do-
mains.
So far, these presented schemes do not distin-
guish between degrees of metaphoricity, and were
not specifically designed for considering moti-
vations behind metaphor use. Unlike the anno-
tation schemes described above, Klebanov and
Flor (2013) built a metaphor annotation proto-
col for metaphors relevant to arguments in essays.
They were interested in identifying metaphors that
stand out and are used to support the writer?s ar-
gument. Instead of giving a formal definition of
a literal sense, the annotators were instructed to
mark words they thought were used metaphori-
cally, and to write down the point being made
by the metaphor, given a general definition of
metaphor and examples. Our work is similar to
this work in that both corpus studies pay attention
to motivations behind metaphor use. However,
our work focuses on more conversational discus-
sion data whereas they focused on essays, which
are more well-formed.
3 Data
We conducted experiments using data from three
different web forums including a Massive Open
Online Course (MOOC), a breast cancer support
group (Breastcancer), and a forum for street gang
members (Gang). We randomly sampled 21 posts
(100 sentences) from MOOC, 8 posts (103 sen-
tences) from Breastcancer and 44 posts (111 sen-
tences) from Gang.
We chose these three forums because they all
offer conversational data and they all differ in
terms of the social situation. The forums dif-
fer significantly in purpose, demographics and
the participation trajectory of members. There-
fore, we expect that people will use language dif-
ferently in the three sets, especially related to
metaphorical expressions.
MOOC: This forum is used primarily for task-
based reasons rather than socializing. People par-
ticipate in the forum for a course, and leave when
the course ends. As a result, the forum does
not have continuity over time; participants do not
spend long time with the same people.
Breastcancer: People join this forum for both
task-based and social reasons: to receive informa-
tional and emotional support. People participate
in the forum after they are diagnosed with cancer,
and may leave the forum when they recover. This
forum is also used episodically by many users, but
a small percentage of users stay for long periods
of time (2 or more years). Thus, continuity al-
lows shared norms to develop over years centered
around an intense shared experience.
Gang: In this forum, members belong to a dis-
tinct subculture prior to joining, whereas Breast-
cancer and MOOC members have less shared
identity before entering the forum. This forum
is purely social. There is no clear endpoint for
participation; members leave the forum whenever
they are not interested in it any more. Users may
stay for a week or two, or for years.
4 Qualitative Analysis
Metaphors can be used for a number of conver-
sational purposes such as increasing or decreas-
ing social distance or as a tactic of persuasion or
manipulation (Ritchie, 2013). In this section, we
perform a qualitative analysis on how metaphor
functions in our data. We illustrate some exam-
ples from each domain with an analysis of how
some functions of social positioning are observed.
The choice of metaphor may reflect something
about the attitude of the speaker. For example,
journey is a metaphor frequently used in the breast
cancer support discussion forum
1
as seen in exam-
ples (2) ? (5) from the Breastcancer forum. Peo-
ple compare chemotherapy to a journey by using
metaphors such as journey, road and moves along.
A journey has a beginning and a goal one trav-
els towards, but people may take different paths.
1
http:breastcancer.org
3
This conveys the experience of cancer treatment
as a process of progressing along a path, strug-
gling and learning, but allows for each person?s
experience to differ without judgment of personal
success or failure (Reisfield and Wilson, 2004).
By contrast, another common metaphor compares
cancer treatment to battles and war. This metaphor
instead conveys an activity rather than passivity, a
struggle against a defined foe, which can be won
if one fights hard enough. But it also creates neg-
ative connotations for some patients, as forgoing
treatment could then be seen as equivalent to sur-
render (ibid.).
(1) Hello Ladies! I was supposed to
start chemo in January, ... I cant
start tx until that is done. So I will
be joining you on your journey this
month. I AM SICK OF the ANXI-
ETY and WAITING.
(2) So Ladies, please add another
member to this club. Looks like we
well all be leaning on each other.
But I promise to pick you up if you
fall if you can catch me once in a
while!
(3) The road seems long now but it re-
ally moves along fast.
(4) I split this journey into 4 stages and
I only deal with one.
In addition, using metaphors can have an ef-
fect of increasing empathetic understanding be-
tween the participants (Ritchie, 2013). We can
see this in examples (1) ? (4), where participants
in the same thread use similar metaphors relat-
ing chemotherapy to a journey. Reusing each
other?s metaphors reduces emotional distance and
helps to build empathic understanding and bond-
ing through a shared perception of their situations.
Metaphor also serves to suggest associations
between things that one would not normally asso-
ciate. Example (5) from the MOOC forum frames
participation in discussions as stepping into an
arena, which refers to an area for sports or com-
petition. By making such an analogy, it conveys
an environment of direct competition in front of a
large audience. It suggests that a student may be
afraid of contributing to discussion because they
may make a wrong statement or weak argument
and another person could counter their contribu-
tions, and they will be embarrassed in front of
their classmates.
(5) Hi, Vicki, great point ? I do wish
that teachers in my growing up
years had been better facilitators
of discussion that allowed EVERY-
one to practice adn become skill-
ful at speaking...I think in the early
years some of us need some hand-
holding in stepping into the arena
and speaking
Metaphors can also be used simply as a form of
wordplay, to display one?s wit and creativity. This
can be seen in the exchange in examples (6) ? (8),
from the Gang forum. A common metaphor used
on that forum is to refer to someone as food to
mean that they are weak and unthreatening. The
writer in (6) expands on this metaphor to suggest
that the other person is especially weak by calling
him dessert, while the writer in (7) then challenges
him to fight by exploiting the meaning of hungry
as ?having a desire for food?. The first writer (8)
then dismisses him as not worth the effort to fight,
as he does not eat vegetables.
(6) So If She Is Food That Must Make
U Desert
(7) if u hungry nigga why wait?
(8) I Dont Eat Vegatables.
5 Our Annotation Scheme
When we performed qualitative analysis as in Sec-
tion 4, we found that more noticeable metaphors
such as ?journey?, ?pick you up?, and ?fall? in (1)
and (2) seem more indicative of speaker attitude
or positioning than metaphors such as ?point? in
(5). This might suggest the degree of metaphoric-
ity affects how metaphors function in discourse.
In this section, we describe our metaphor anno-
tation scheme, which tries to map this variation
among metaphors to a simpler three-point scale of
nonliteralness: nonliteral, conventionalized, and
literal.
5.1 Basic Conditions
Our annotation scheme targets language satisfying
the following three conditions:
4
1. the expression needs to have an original es-
tablished meaning.
2. the expression needs to be used in context to
mean something significantly different from
that original meaning.
3. the difference in meaning should not
be hyperbole, understatement, sarcasm or
metonymy
These conditions result in metaphorical ex-
pressions including simile and metaphorical id-
ioms. We consider simile to be a special case of
metaphor which makes an explicit comparison us-
ing words such as ?like?. We include metaphor-
ical idioms because they are obviously nonliteral
and metaphorical despite the fact that they have
lost their source domains.
Have an original meaning: The expression or
the words within the expression need to have orig-
inal established meanings. For example, in the
sentence ?I will be joining you on your journey
this month? of (1) in Section 4, the word ?journey?
refers to chemotherapy given the context, but has
a clear and commonly known original meaning of
a physical journey from one place to another.
Alter the original and established meanings
of the words: The usage needs to change the orig-
inal meaning of the expression in some way. The
intended meaning should be understood through
a comparison to the original meaning. For the
same example, in ?I will be joining you on your
journey this month?, the intended meaning can be
understood through a comparison to some char-
acteristics of a long voyage. For metaphorical id-
ioms such as ?he kicked the bucket,? the nonliteral
meaning of ?he died? is far from the literal mean-
ing of ?he struck the bucket with his foot.?
Should not merely be hyperbole, understate-
ment, sarcasm, or metonymy: To reduce the
scope of our work, the usage needs to alter the
original meaning of the expression but should not
simply be a change in the intensity or the polar-
ity of the meaning, nor should it be metonymy.
Language uses like hyperbole and understatement
may simply change the intensity of the meaning
without otherwise altering it. For sarcasm, the
intended meaning is simply the negation of the
words used. Metonymy is a reference by asso-
ciation rather than a comparison. For example, in
?The White House denied the rumor?, the White
House stands in for the president because it is as-
sociated with him, rather than because it is being
compared to him. Note that metaphorical expres-
sions used in conjunction with these techniques
will still be coded as metaphor.
5.2 Decision Steps
To apply the basic conditions to the actual annota-
tion procedure, we come up with a set of decision
questions (Table 1). The questions rely on a va-
riety of other syntactic and semantic distinctions
serving as filtering questions. An annotator fol-
lows the questions in order after picking a phrase
or word in a sentence he or she thinks might be
nonliteral language. We describe some of our de-
cisions below.
Unit: The text annotators think might be non-
literal is considered for annotation. We allow a
word, a phrase, a clause, or a sentence as the
unit for annotation as in (Wallington et al., 2003).
We request that annotators include as few words
as necessary to cover each metaphorical phrase
within a sentence.
Category: We request that annotators code a
candidate unit as nonliteral, conventionalized, or
literal. We intend the nonliteral category to in-
clude nonliteral language usage within our scope,
namely metaphors, similes, and metaphorical id-
ioms. The conventionalized category is intended
to cover the cases where the nonliteralness of the
expression is unclear because of its extensive us-
age. The literal category is assigned to words that
are literal without any doubt.
Syntactic forms: We do not include prepo-
sitions or light verbs. We do not consider
phrases that consist of only function words such
as modals, auxiliaries, prepositions/particles or
infinitive markers. We restrict the candidate
metaphorical expressions to those which contain
content words.
Semantic forms: We do not include single
compound words, conventional terms of address,
greeting or parting phrases, or discourse markers
such as ?well?. We also do not include termi-
nology or jargon specific to the domain being an-
notated such as ?twilight sedation? in healthcare,
since this may be simply borrowing others? words.
5
No. Question Decision
1 Is the expression using the primary or most concrete meanings of the words? Yes = L
2 Does the expression include a light verb that can be omitted without changing
the meaning, as in ?I take a shower? ? ?I shower?? If so, the light verb
expression as a whole is literal.
Yes = L
3 Is the metaphor composed of a single compound word, like ?painkiller?, used
in its usual meaning?
Yes = L
4 Is the expression a conventional term of address, greeting, parting phrase or a
discourse marker?
Yes = L
5 Is the expression using terminology or jargon very common in this domain or
medium?
Yes = L
6 Is the expression merely hyperbole/understatement, sarcasm or metonymy? Yes = L
7 Is the expression a fixed idiom like ?kick the bucket? that could have a very
different concrete meaning?
Yes = N
8 Is the expression a simile, using ?like? or ?as? to make a comparison between
unlike things?
Yes = N
9 Is the expression unconventional/creative and also using non-concrete mean-
ings?
Yes = N
10 Is there another common way to say it that would convey all the same nuances
(emotional, etc.)? Or, is this expression one of the only conventional ways of
conveying that meaning?
If yes to
the latter
= C
11 If you cannot otherwise make a decision between literal and nonliteral, just
mark it as C.
Table 1: Questions to annotate (N: Nonliteral, C: Conventionalized, L: Literal).
6 Experiment
In this section, we present our comparative study
of the MTurk annotations and the annotations
based on our annotation scheme. The purpose
of this experiment is to explore (1) how laypeo-
ple perceive metaphor, (2) how valid the anno-
tations from crowdsourcing can be, and (3) how
metaphors are different in the three different do-
mains.
6.1 Experiment Setup
We had two annotators who were graduate stu-
dents with some linguistic knowledge. Both were
native speakers of English. The annotators were
asked to annotate the data using our annotation
scheme. We will call the annotators trained an-
notators from now on.
In addition, we used Amazon?s Mechanical
Turk (MTurk) crowdsourcing marketplace to col-
lect laypeople?s recognition of metaphors. We
employed MTurk workers to annotate each sen-
tence with the metaphorical expressions. Each
sentence was given along with the full post it came
from. MTurkers were instructed to copy and paste
all the metaphors appearing in the sentence to
given text boxes. They were given a simple def-
inition of metaphor from Wikipedia along with a
few examples to guide them. Each sentence was
labeled by seven different MTurk workers, and we
paid $0.05 for annotating each sentence. To con-
trol annotation quality, we required that all work-
ers have a United States location and have 98%
or more of their previous submissions accepted.
We monitored the annotation job and manually
filtered out annotators who submitted uniform or
seemingly random annotations.
6.2 Results
To evaluate the reliability of the annotations, we
used weighted Kappa (Cohen, 1968) at the word
level, excluding stop words. The weighted Kappa
value for annotations following our annotation
scheme was 0.52, and the percent agreement was
95.68%. To measure inter-reliability between two
annotators per class, we used Cohen?s Kappa (Co-
6
hen, 1960). Table 2 shows the Kappa values for
each dataset and each class. Table 4 shows the
corpus statistics.
Dataset N C N+C Weighted
all 0.44 0.20 0.49 0.52
breastcancer 0.69 0.20 0.63 0.71
Gang 0.26 0.28 0.39 0.34
MOOC 0.41 0.13 0.47 0.53
Table 2: Inter-reliability between two trained an-
notators for our annotation scheme.
To evaluate the reliability of the annotations by
MTurkers, we calculated Fleiss?s kappa (Fleiss,
1971). Fleiss?s kappa is appropriate for assessing
inter-reliability when different items are rated by
different judges. We measured the agreement at
the word level, excluding stop words as in com-
puting the agreement between trained annotators.
The annotation was 1 if the MTurker coded a word
as a metaphorical use, otherwise the annotation
was 0. The Kappa values are listed in Table 3.
Dataset Fleiss?s Kappa
all 0.36
breastcancer 0.41
Gang 0.35
MOOC 0.30
Table 3: Inter-reliability among MTurkers.
We also measured the agreement between the
annotations based on our scheme and MTurk an-
notations to see how they agree with each other.
First, we made a gold standard after discussing
the annotations of trained annotators. Then, to
combine the seven MTurk annotations, we give
a score for an expression 1 if the majority of
MTurkers coded it as metaphorically used, other-
wise the score is 0. Then, we computed Kappa
value between trained annotators and MTurkers.
The agreement between trained annotators and
MTurkers was 0.51 for N and 0.40 for N + C. We
can see the agreement between trained annotators
and MTurkers is not that bad especially for N.
Figure 1 shows the percentage of words labeled
as N, C or L according to the number of MTurk-
ers who annotated the word as metaphorical. As
seen, the more MTurkers who annotated a word,
Dataset N N+ C
all 0.51 0.40
breastcancer 0.64 0.47
Gang 0.36 0.39
MOOC 0.65 0.36
Table 5: Inter-reliability between trained annota-
tors and MTurkers.
the more likely it was to be annotated as N or C
by our trained annotators. The distinction between
Nonliteral and Conventionalized, however, is a bit
muddier, although it displays a moderate trend to-
wards more disagreement between MTurkers for
the Conventionalized category. The vast majority
of words (>90%) were considered to be literal, so
the sample size for comparing the N and C cate-
gories is small.
Figure 1: Correspondence between MTurkers and
trained annotators. X-axis: the number of MTuck-
ers annotating a word as metaphor.
7 Discussion
In this section, we investigate the disagreements
between annotators. A problem inherent to the an-
notation of metaphor is that the boundary between
literal and nonliteral language is fuzzy. Different
annotators may draw the line in different places
even when it comes to phrases they are all famil-
iar with. It is also true that each person will have
a different life history, and so some phrases which
are uninteresting to one person will be strikingly
metaphorical to another. For example, someone
who is unfamiliar with the internet will likely find
the phrase ?surf the web? quite metaphorical.
Since we did not predefine the words or phrases
that annotators could consider, there were often
cases where one person would annotate just the
7
Dataset Posts Sent. Words Content Words N C N/Sent. C/Sent.
MOOC 21 100 2005 982 23 59 0.23 0.59
Breastcancer 8 103 1598 797 27 41 0.26 0.4
Gang 44 111 1403 519 30 51 0.27 0.46
Table 4: Data statistics.
noun and another might include the entire noun
phrase. If it was part of a conventional multi-word
expression, MTurkers seemed likely to include the
entire collocation, not merely the metaphorical
part. Boundaries were an issue to a lesser extent
with our trained annotators.
One of our datasets, the Gang forum, uses a lot
of slang and non-standard grammar and spellings.
One of our trained annotators is quite familiar with
this forum and the other is not. This was the set
they had the most disagreement on. For exam-
ple, the one annotator did not recognize names of
certain gangs and rap musicians, and thought they
were meant metaphorically. Similarly, the MTurk-
ers had trouble with many of the slang expressions
in this data.
Another issue for the MTurkers is the distinc-
tion between metaphor and other forms of nonlit-
eral language such as metonymy and hyperbole.
For example, in the Gang data, the term ?ass? is
used to refer to a whole person. This is a type
metonymy (synecdoche) using a part to refer to
the whole. MTurkers were likely to label such
expressions as metaphor. Hyperbolic expressions
like ?never in a million years? were also marked
by some MTurkers.
In a few cases, the sentence may have required
more context to decipher, such as previous posts
in the same thread. Another minor issue was that
some data had words misspelled as other words or
grammatical errors, which some MTurkers anno-
tated as metaphors.
Certain categories of conventionalized
metaphors that would be annotated in the
original presentation of MIP (Pragglejaz-Group,
2007) were never or almost never annotated by
MTurkers. These included light verbs such as
?make? or ?get? when used as causatives or
the passive ?get?, verbs of sensation used for
cognitive meanings, such as ?see? meaning ?un-
derstand?, and demonstratives and prepositions in
themselves. This may indicate something about
the relevance of these types of metaphors for
certain applications.
8 Conclusion
We annotated data from three distinct conver-
sational online forums using both MTurks and
our annotation scheme. The comparison between
these two annotations revealed a few things. One
is that MTurkers did not show high agreement
among themselves, but showed acceptable agree-
ment with trained annotators for the N category.
Another is that domain-specific knowledge is im-
portant for accurate identification of metaphors.
Even trained annotators will have difficulty if they
are not familiar with the domain because they may
not even understand the meaning of the language
used.
Our annotation scheme has room for improve-
ment. For example, we need to distinguish be-
tween the Conventionalized and Nonliteral cate-
gories more clearly. We will refine the coding
scheme further as we work with more annotators.
We also think there may be methods of pro-
cessing MTurk annotations to improve their cor-
respondence with annotations based on our cod-
ing scheme. This could address issues such as in-
consistent phrase boundaries or distinguishing be-
tween metonymy and metaphor. This could make
it possible to use crowdsourcing to annotate the
larger amounts of data required for computational
applications in a reasonable amount of time.
Our research is in the beginning phase work-
ing towards the goal of computational modeling
of social and discourse uses of metaphor. Our next
steps in that direction will be to work on develop-
ing our annotated dataset and then begin to investi-
gate the differing contexts that metaphors are used
in. Our eventual goal is to be able to apply compu-
tational methods to interpret metaphor at the level
of social positioning and discourse functions.
8
Acknowledgments
This work was supported by NSF grant IIS-
1302522, and Army research lab grant W911NF-
11-2-0042.
References
Rodrigo Agerri, John Barnden, Mark Lee, and Alan
Wallington. 2007. Metaphor, inference and domain
independent mappings. In Proceedings of RANLP,
pages 17?23. Citeseer.
John A Barnden and Mark G Lee. 2002. An artifi-
cial intelligence approach to metaphor understand-
ing. Theoria et Historia Scientiarum, 6(1):399?412.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of non-
literal language. In EACL.
Lynne J Cameron. 2007. Patterns of metaphor
use in reconciliation talk. Discourse & Society,
18(2):197?222.
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37?46.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
agreement provision for scaled disagreement or par-
tial credit. Psychological bulletin, 70(4):213.
Dan Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49?90.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378?382.
Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of the Third Workshop on Scalable Natu-
ral Language Understanding, pages 41?48. Associ-
ation for Computational Linguistics.
Andrew Goatly. 1997. Language of Metaphors: Lit-
eral Metaphorical. Routledge.
Beata Beigman Klebanov and Michael Flor. 2013.
Argumentation-relevant metaphors in test-taker es-
says. Meta4NLP 2013, pages 11?20.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the Workshop on Computational
approaches to Figurative Language, pages 13?20.
Association for Computational Linguistics.
Linlin Li and Caroline Sporleder. 2010. Using gaus-
sian mixture models to detect figurative language in
context. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 297?300, Stroudsburg, PA,
USA. Association for Computational Linguistics.
James H Martin. 1990. A computational model of
metaphor interpretation. Academic Press Profes-
sional, Inc.
Zachary J Mason. 2004. Cormet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Srinivas Narayanan. 1999. Moving right along: A
computational model of metaphoric reasoning about
events. In AAAI/IAAI, pages 121?127.
Pragglejaz-Group. 2007. Mip: A method for iden-
tifying metaphorically used words in discourse.
Metaphor and symbol, 22(1):1?39.
Gary M Reisfield and George R Wilson. 2004. Use
of metaphor in the discourse on cancer. Journal of
Clinical Oncology, 22(19):4024?4027.
SL. David Ritchie. 2013. Metaphor (Key Topics in
Semantics and Pragmatics). Cambridge university
press.
Ekaterina Shutova and Simone Teufel. 2010.
Metaphor corpus annotated for source-target do-
main mappings. In LREC.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics,
pages 1002?1010. Association for Computational
Linguistics.
Ekaterina Shutova, BarryJ. Devereux, and Anna Ko-
rhonen. 2013a. Conceptual metaphor theory meets
the data: a corpus-based human annotation study.
Language Resources and Evaluation, 47(4):1261?
1284.
Ekaterina Shutova, Simone Teufel, and Anna Korho-
nen. 2013b. Statistical metaphor processing. Com-
putational Linguistics, 39(2):301?353.
Ekaterina Shutova. 2010. Automatic metaphor inter-
pretation as a paraphrasing task. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1029?1037.
Association for Computational Linguistics.
Ekaterina Shutova. 2013. Metaphor identification as
interpretation. Atlanta, Georgia, USA, page 276.
9
Gerard J Steen, Aletta G Dorst, J Berenike Herrmann,
Anna Kaal, Tina Krennmayr, and Trijntje Pasma.
2010. A method for linguistic metaphor identifica-
tion: From MIP to MIPVU, volume 14. John Ben-
jamins Publishing.
Anatol Stefanowitsch and Stefan Th Gries. 2006.
Corpus-based approaches to metaphor and
metonymy, volume 171. Walter de Gruyter.
AM Wallington, JA Barnden, P Buchlovsky, L Fel-
lows, and SR Glasbey. 2003. Metaphor annota-
tion: A systematic study. COGNITIVE SCIENCE
RESEARCH PAPERS-UNIVERSITY OF BIRMING-
HAM CSRP.
10
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 21?31,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Towards Identifying the Resolvability of Threads in MOOCs
Diyi Yang, Miaomiao Wen, Carolyn Rose
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Ave, Pittsburgh, 15213
{diyiy,mwen,cprose}@cs.cmu.edu
Abstract
One important function of the discussion
forums of Massive Open Online Courses
(MOOCs) is for students to post problems
they are unable to resolve and receive help
from their peers and instructors. There are
a large proportion of threads that are not
resolved to the satisfaction of the students
for various reasons. In this paper, we
attack this problem by firstly constructing
a conceptual model validated using a
Structural Equation Modeling technique,
which enables us to understand the factors
that influence whether a problem thread
is satisfactorily resolved. We then demon-
strate the robustness of these findings using
a predictive model that illustrates how ac-
curately those factors can be used to predict
whether a thread is resolved or unresolved.
Experiments conducted on one MOOC
show that thread resolveability connects
closely to our proposed five dimensions and
that the predictive ensemble model gives
better performance over several baselines.
1 Introduction
Massive Open Online Courses (MOOCs), run by
organizations such as Coursera, have been among
the most news worthy social media environments
in the past year. While usage of social media
affordances such as discussion forums in such
courses is small relative to usage of videos or
assignments, participation in the discussion forums
is an important predictor of commitment to the
course (Yang et al., 2013). We hypothesize that
supporting a positive experience in such forums
has the potential to increase retention in such
courses. In this paper, we specifically study the
behavior of students in a MOOC course for learning
Python programming. We present empirical work
that elucidates an important problem in existing
MOOC discussion forums, propose a practical
solution, and offer promising results in a corpus
based evaluation.
MOOCs for programming skills can be seen as
important resources for the professional develop-
ment of programmers and programmers in training.
While MOOCs for learning programming are a
recent phenomenon, they are not the first web
accessible resources for development of such skills.
In recent years, a plethora of question/answer
sites for programming have become available that
have grown into thriving communities of practice
for programmers. In these online communities,
programmers can get mentoring from those who
are more expert than them and offer mentoring to
programmers who are less expert than them. For
example, StackOverflow
1
has become a forum not
only for getting specific questions answered, but for
negotiating the pros and cons of alternative ways
of solving technical problems. The code proposed
as part of alternative solutions remains as part of
the community memory, which is then accessible
for those who come later with similar concerns.
Where StackOverflow falls short is in providing
an appropriate environment for the active involve-
ment of very novice programmers. When such
novices come to a forum like StackOverflow and
present their naive questions, they are frequently
met with sarcastic responses if they get a response
at all.
MOOCs for learning programming skills fill a
gap left open by such environments, in that they
welcome the very novice and provide forums where
naive questions are not shunned. Nevertheless,
discussion forums that only include such novice
programmers would be akin to the blind leading the
blind were it not for the involvement of a few more
expert students and the teaching staff. This does not
fully solve the problem, however. Many threads are
1
http://stackoverflow.com/
21
still left without a satisfactory resolution. Currently,
it is challenging for the teaching staff and expert
participants to know where in the massive amount
of communication to look for opportunities where
their support is most needed. This is the problem
we aim to address in this paper, i.e. automatically
identify whether a thread is resolved and provide
potential for better allocation of instructor and
student resources.
In the remainder of the paper we first survey
related work. Next we describe the formulation
of the problem. We then present a series of
two experiments, the later one building on the
successful findings and results from the former.
The results conducted on one MOOC show that
our proposed model of thread resolveability better
captures the difference between resolved and
unresolved threads and that the ensemble logistic
model outperforms several baselines. We conclude
the paper with a discussion of the limitations of
this work and next steps.
2 Related Work
MOOCs have received more and more attention
recently, with the promise of providing many of the
benefits of traditional classroom learning but not
limited by time, location or finances. Much prior
work has focused on analysis of such platforms
to motivate the design of better student learning
experiences. In various ways, the issue of students
needing support from instructors and students has
been addressed (Lieberman, 1995).
An important component in the Coursera envi-
ronment is the discussion forums, which students
can use to learn new knowledge from each other
and from the teaching staff when they participate.
In support of the importance of the discussion
forums in connection with major problems like
attrition, models are proposed to predict student
dropout based both on their video watching be-
havior and also discussion forum posting behavior,
such as how many posts a student has made (Balakr-
ishnan, 2013). Student behavior in the discussion
forum is also focused by other prior works (Yang
et al., 2013). Yang et al. analyze drop out along the
way, demonstrating the predictive power of features
extracted within time windows of student behavior
within the forums. The results of their work suggest
that interaction with other students is important
for keeping students motivated, which is further
confirmed by many works (Yang et al., 2014; Ros?e
et al., 2014). Besides, linguistic reflections are also
crucial for students engagement (Wen et al., 2014).
Other work highlights the importance of interac-
tion in the form of feedback during participation in
MOOCs. For example, some prior work (Piech et
al., 2013) has explored peer grading, especially
in helping grading of open ended assignments,
in courses with thousands or tens of thousands
of students. Other work takes a more holistic
approach to assessment of student behavior. For
example, in one such example (Kizilcec et al.,
2013), instead of looking at students? assignments,
students were classified based on their patterns
of interaction with video lectures and assessment
activities. This behavior trace was processed using
a simple and scalable classification method that
could identify a small number of longitudinal
engagement trajectories that potentially provide
the impetus for tailored feedback or mentoring.
Outside of MOOC discussion forums, there has
also been work investigating the conditions under
which questions receive appropriate feedback in
more general Question Answering (QA) sites. In
particular, this work has been framed as research
on thread resolveability in QA sites. It can
be conceived as the human counterpart to fully
automated question answering systems (Prager et
al., 2000; Perera, 2012; Jeon et al., 2006; Agichtein
et al., 2008). Much of this work has emphasized the
importance of having effective features to model
question and answer processes.
In some of this prior work, the focus has been
on identifying whether a thread is answered given
a question and a set of potential answers (Sung
et al., 2013; Tian et al., 2013). The prior
work (Anderson et al., 2012) has focused on
understanding the dynamics of the surrounding
community activity, like the process through which
answers and voters arrive over time. Based on
understanding of such factors, a prediction can be
made about the long term value for the community
of a question being answered. Similarly, Agichtein
and colleagues (Agichtein et al., 2009) presented
a general prediction model of information seeker
satisfaction in community question answering,
and developed content, structure and community
focused features for the question answering task. A
collection of other related work (Liu and Agichtein,
2008) has developed personalized models of asker
satisfaction to predict whether a particular question
starter will be satisfied with the answers given
22
by others. This is solved by exploring content,
structure and interaction features using standard
prediction models.
Work on automated question answering systems
can also be seen as relevant since questions that can
be answered automatically do not need a human
response, and therefore might reduce the load
on available human effort. Instead of predicting
whether a problem is answered, strategies for
predicting are explored when a question answering
system is likely to give an incorrect answer (Brill
et al., 2002). To further understand how a question
is answered, researchers (Yih et al., 2013) have
studied the answer sentence selection problem
for question answering and improves the model
performance by using lexical semantic resources.
That is, they construct semantic matches between
question and answers. In terms of the extent
to which the question is answered, Shah and
colleagues (Shah and Pomerantz, 2010) evaluated
answer quality by manually rating the quality
of each answer. Then they extracted various
features to train classifiers to select the best answer
for that question. Liu et al. (Liu et al., 2011)
proposed to use a mutual reinforcement based
propagation algorithm to predict question quality
based. The model makes its prediction based on
the connection between askers and topics, and how
those connections predict differences in quality.
The above question answering work is all
about general discussion forums (Qu et al., 2009;
Kabutoya et al., 2010), such as Yahoo! Answers
2
.
In our work, in addition to taking advantage of
existing QA work, we also adopt a linguistic
perspective (Jansen et al., 2014) and take semantic
matching into account using a latent semantic
approach. To the best of our knowledge, this is
the first work on thread resolvability analysis in a
MOOC context.
3 Research Problem Introduction
In this section, we focus on how to identify the
resolveability of threads in the MOOC forums. We
firstly introduce the research context and dataset,
then we formulate our resolveability problem.
3.1 Research Context and Dataset
In programming MOOCs, when students encounter
problems working on the programming assign-
ments, or when something is not clear from the
2
http://answers.yahoo.com/
readings or lectures, students have the opportunity
to initiate a thread in the course forum, in order
to engage other students in the class as well as
the teaching staff. For example, if a student
were confused about the distinction between an
argument and a parameter in Python, he/she
would post the question to the variables subforum,
marking it unresolved at the same time. In the
ideal case, another participant would reply to
this question with some detailed explanation and
example, which would solve that problem. When
the student who initiated the thread receives the
response, assuming it is adequate, that student
may mark it as resolved. Others may join in as
well, and individual posts may be rated through
upvotes and downvotes. In contrast to existing QA
sites, no best answer option is available. Thus,
the resolved/unresolved button provides the closest
equivalent groundtruth.
The data for this paper was crawled from a
Python language course. Our focus was specifically
to investigate the inner workings of threads related
to getting answers to questions or help with
programming difficulty. In order to avoid including
threads in our dataset that are off-topic or otherwise
irrelevant, we limited the set of forums to the
subforums that focus strongly on course content,
including those indicated to focus on lectures,
exercises and assignments as well as the final exam.
That is, we discarded posts in the study groups,
social discussion, and other discussion areas that
do not have unresolved buttons. In the final dataset,
there were 2508 threads (1244 resolved threads) in
total, and 2896 users (12 instructors and staffs) who
had at least one post. Each question is associated
with a label indicating whether it is resolved or not.
3.2 Problem Formulation
Work on the related problem of analysis of QA
websites has grown in popularity in recent years.
However, due to differences in how MOOCs work
as temporary online communities, it is necessary to
consider how findings from prior work in these
areas may or may not generalize to this new
context as we formulate our research problem. In
particular, MOOCs are different from existing QA
websites, such as Yahoo! Q&A, Stack Overflow.
The purpose of QA sites is primarily for people
to get answers. While people may learn from
their interactions on such sites, those sites are not
designed in particular to support learning. Thus,
23
0200
400
600
800
1000
1200
1400
C
n
t 
N
u
m
b
e
r
Post Number
Standby Less Active Active Super Star
(a) Question Post
0
200
400
600
800
1000
1200
1400
0 3 6 9
1
2
1
5
1
8
2
1
2
4
2
7
31 34 39 4
4
52 6
2
73 8
8
96
1
2
7
2
1
3
1
0
0
1
C
n
t 
N
u
m
b
e
r
Reply Number
Standby Less Active Active Super Star
(b) Reply Devotee
0
200
400
600
800
1000
1200
1400
0 1 2 3 4 5 6 7 10 12 28 226
C
n
t 
N
u
m
b
e
r
Resolved Number
Standby Less Active Active Super Star
(c) Resolved Favor
0
200
400
600
800
1000
1200
1400
0 3 6 9
1
2
1
5
1
8
2
1
2
4
2
9
37 4
0
4
4
53 79
1
1
8
1
30
2
1
2
798
C
n
t 
N
u
m
b
e
r
UpVotes
Standby Less Active Active Super Star
(d) UpVotes
Figure 1: Starter Influence Statistics. Each Figure has two curves; the below one indicates how many
users have made the associated number of posts/reply. The above one is the cumulative version of the
same.
different characteristics are needed in the MOOCs
discussion threads. One implication is that the
discussions in MOOCs may need to be more
interactive than those found in environments such
as StackOverflow. Students who post problems
can be expected to be less capable of fully
comprehending an answer even if it is a good
one. This demands more effort from those with the
ability to offer helpful responses. In order for the
discussions to be effective, the threads must include
a balance of naive participants and participants with
more knowledge. A related issue is that it is not yet
ubiquitous for participants in MOOCs to have the
opportunity to earn a reputation score for offering
useful answers and other instructional support. In
other QA sites, this is both a valuable motivator as
well as an important predictor of resolved versus
unresolved question threads (Anderson et al., 2012).
Thus, students who post questions may need to
sell their problem in order to attract those who
can offer help. Taking these interrelated issues
into account, an important aspect of our modeling
work is in recognizing the different roles that
users play in the community. Related to this, we
will describe below how we develop models that
include latent variables related to the propensity
of users to initiate problem threads that attract
useful responses, and the propensity of others to
contribute useful responses in such contexts. We
refer to these complementary variables as starter
influence and expert participation respectively.
Secondly, all are welcome to learn in a MOOC
and participate actively even if they have no
prior knowledge. In an educational context, it
would not be appropriate to meet a naive question
with a sarcastic response. In contrast, in Stack
Overflow, it would be treated as unremarkable for
a naive question to get a sarcastic response. While
naive participants may not enjoy such responses,
they learn to expect them. Since approaching
posted problems with patience and friendliness is
important for avoiding discouraging new learners,
we include a variable called friendliness that
represents friendly and polite discussion behavior.
None of these would ultimately result in thread
resolution if the answers that are offered were not
targeted to the problems raised by the students
who initiated the threads. This is one place where
our work is very aligned with earlier work on QA
sites. And thus we adopt a similar practice where
we include in our model an estimate of answer
appropriateness in a latent variable we refer to as
content matching.
Now we define important terms used in our
discussion. First, we define roles within discussion
threads that are relevant for our work. For a
given thread, the user who initialized the thread
is called the Starter; the teaching staff including
both official course instructors and TAs are referred
to as Instructors; and any other users who
have replied or commented in the thread are
referred to as Participants. We count a thread
in our dataset as resolved only if the thread
starter personally changes the Unresolved button to
Resolved. Otherwise, we count the as unresolved.
We are interested in the conditions under which
a thread is marked as resolved or unresolved:
Thread Resolveability: Given a thread with
its associated question and set of replies, which
may not have been explicitly marked as resolved,
identify whether it should have been marked as
resolved or not.
4 Latent Variable Modeling
We laid the foundation for a conceptual model
above to understand the factors associated with
resolved versus unresolved threads and introduced
five latent factors we referred to as Starter Influence,
Expert Participation, Thread Popularity, Friendli-
ness, and Content Matching. In this section, we
24
further formalize these latent factors by specifying
associated sets of observed variables that will
ultimately enable us to evaluate our conceptual
model. All latent and observed variables are
enumerated in Table 1.
4.1 Starter Influence
The person who serves as the Thread starter
is responsible for formulating the question that
is addressed, and therefore the focus of that
discussion. Some participants post many questions
and are very adept at formulating their questions in
ways that engage the attention of people who have
the ability to provide answers. If the starter posts a
lot and his/her questions often get resolved, this can
be taken as an indication that this person is popular.
Questions contributed by him/her may be more
likely to attract attention and receive replies. This
simple indication of popularity, which can be easily
computed, may in some way compensate for the
lack of an established badge system where they are
not in use. We propose to measure this form of user
influence by using the following four indicators.
(1) Question Devotee x
Pst
, indicates how many
threads this question starter has proposed in this
forum. Based on Figure 1(a), we divide users in
this discussion forum into four types to indicate the
propensity to post, i.e. post number ranges from
1-2 as standbys, 3-5 as less active, 6-14 as active,
40-489 as superstars. Similar partition method is
adopted for all the following indicators. (2) Reply
Devotee x
Rep
, means how many times a person acts
as a Participant in a thread posted by other students
as shown in Figure 1(b). If he/she usually replies
to others, then it is possible that his/her question
will be paid more attention in return. (3) Resolved
Favor x
Res
, means in how many threads the person
acts as the Starter in threads that get resolved. (4)
Praised Responder x
Uvt
, indicates the proportion
of all the posts this starter makes in the forum that
received upvotes, as displayed in Figure 1(d). This
connects to how others recognize this starter and to
what degree.
4.2 Expert Participation
Who participates a discussion is as important as
who initiates the discussion. Students with some
expertise in the related content can often provide
quality replies (Anderson et al., 2012). Since user
reputation score information is not available in
this MOOC, it is necessary to for us to identify
observable indicators. We define a person as Expert
x
Exp
in our forum as follows. A person is an
Expert if and only if he/she is one of the instructors
or his/her reputation score as we compute it is
ranked in the top 1% among all students. The
reputation score of student u is computed based
on his/her question devotee u
Pst
, reply devotee
u
Rep
, resolved favor u
Res
, and praised recognition
u
Uvt
as we defined in the previous section. The
contribution of each factor to reputation score is
controlled using parameters ?, ?, ?.
score(u) =?u
Pst
+ ?u
Rep
+ ?u
Res
+ (1? ?? ? ? ?)u
Uvt
(1)
4.3 Thread Popularity
How much attention is paid to a question may be
linked to the attractiveness of the thread based
on how it is presented to the community. Thus
modeling thread popularity may be valuable for
accounting for variation in level of participation
across threads. In particular, a reply is given
upvotes when others think it is informative or
good. Thus upvotes could indicate how others
evaluate the replies in connection with the question.
We design three observable factors here that
may contribute to a model of thread popularity.
The Total UpVotes x
Tvt
and Max UpVotes x
Mvt
are used to represent the credit this thread has
received and how others recognize the current
discussion. Based on our analysis, people rarely
give a downvote to others? posts. The Question
Votes x
Svt
indicates whether the starter formulates
a problem that wins recognition from others. For
Total Upvotes, we find that in resolved threads, it is
6.10 compared to 3.15 in unresolved thread. Thus,
intuitively, thread popularity has the potential to
give a useful prediction of thread resolveability.
4.4 Friendliness
Friendliness (Danescu-Niculescu-Mizil et al.,
2013; Burke and Kraut, 2008) concerns whether
the current conversation is conducive for others
to discuss ideas. This has not been considered
in existing question answering work, and we
thus discuss our operationalization of politeness
here. We hypothesize that resolved threads posses
more polite words, such as ?thank?. For example,
a resolved thread might end with gratitude to
thank others for providing help, and indeed we
see this. Thus, we specify a set of observed
indicators that may be useful in a latent variable
model of politeness. (1) Start with Thanks: x
Stx
,
25
Var T Description Var T Description Var T Description
Pae N Please Count Qa1 N 1st Match Score Svt N Question Votes
Thx N Thanks Count Qa2 N 2nd Match Score Mvt N Max Votes
Dfe N Deference Qa3 N 3rd Match Score Uvt N User Votes
Etx B End with Thx Len N Max Length Rep N Reply Number
Stx B Start with Thx Sim N Similarity Res N Resolved Count
Exp B Expert Join Tvt N Total Votes Pst N Post Number
Sin - Starter Influence Epr - Expert Participation Con - Content Matching
Pop - Thread Popularity Fen - Friendliness Label B Resolved or not
Table 1: Variables used in the Structural Equation Model (SEM). Var is the factor variable that is used,
which also corresponds to Figure 2. T indicates what type of values a variable can take. B is short for
Binary. N is short for Numeric. ?-? means it is a latent unobserved variable.
indicates whether this starter shows politeness
when he/she posted the question. (2) End with
Thanks: x
Eth
, stands for whether the starter says
thanks after receiving others? help. (3) Thanks
Count: x
Thx
, measures overall friendliness in the
current discussion. We evaluate this by counting
the thanking related words. (4) Deference: x
Dfe
, is
a count of positive polite words occurring in the dis-
cussion, such as using the words ?Nice?,?Great?, or
?Awesome?, as in prior work (Danescu-Niculescu-
Mizil et al., 2013). Such words are used as markers
to conduct counting. (5) Please: x
Pae
, captures
whether friendly question asking words were used,
i.e. how many times words such as ?Please?, ?Will?,
occur in current conversation.
4.5 Content Matching
Matches between the content of a thread and its
replies indicate whether replies are relevant to
answering the question instead of some off-topic
discussion. In order to estimate this, we build
an Eigenword bipartite graph to capture semantic
similarities. Each node in the bipartite graph is the
corresponding Eigenword
3
of a given word, with
the left side representing the words that occurred
in the thread starter, and the right side representing
the words in a given reply. The edge is a similarity
score computed by using the cosine similarity
metric. In order to better identify whether a reply is
discussing the content of the question, a semantic
match between the thread question and its replies is
needed. The top 3 matching scores are denoted as
x
Qa1
, x
Qa2
, x
Qa3
. Additionally, TF-IDF similarity
x
Sim
is computed (the correlation between x
Sim
and Qa1, Qa2, Qa3 are 0.3280, 0.3572, 0.3569
separately) and the maximum answer length x
Len
3
http://www.cis.upenn.edu/ ungar/eigenwords/
is used to assist in computing the matching score.
5 Experimental Investigation
In the above section, we described five latent fac-
tors we hypothesize are important in distinguishing
resolved and unresolved threads along with sets
of associated observed variables. In this section,
we conduct two studies on thread resolveability,
including validating the influence of each latent
factor on thread resolution using a Structural Equa-
tion Model (SEM), and evaluating the generality
of the identification of the resolveability using a
predictive model. Experiments are conducted on
the Python dataset with performance measurement
under different evaluation metrics.
5.1 Conceptual SEM Validation
Our conceptual model is implemented as a Struc-
tural Equation Model (SEM) and is introduced as
an evaluations of the effect of each latent factor on
thread resolveability, as shown in Figure 2.
5.1.1 Conceptual SEM Model
A Structural Equation Model (Bollen, 1987), is
a statistical technique for testing and estimating
correlational (and sometimes causal) relations in
cross sectional datasets. To explore the influence of
our five latent factors, we take advantage of SEM
to formalize the conceptual structure in order to
measure what contributes to thread resolveability.
The designed latent factors are specified as latent
variables within the model, with the associated
observed variables discussed above. We define the
conceptual structure of how a thread gets resolved
as well as a mathematical expression of each latent
variable in Equation 2.
Related variables are explained above and
26
Figure 2: SEM Model Factor Analysis Result. Each directed edge indicates the predictive relationship. Weight on each
directed edge is the estimated influence strength of one node to another. Table 1 illustrates the denotation. Only significant node
influences whose p-value (p < 0.05) are presented. Circles stand for latent variables while rectangles signify observed variable.
summarized in Table 1. Label refers to the
label of a unknown thread, taking the value of
Resolved or Unresolved. Label (L) is a linear
combination of each latent factor set. For each
variable in a latent factor set, it is associated with a
weight parameter ? in the SEM. Specifically, this
conceptual structure of how a thread gets resolved
relates to five aspects, i.e. (1) whether the thread
starter has enough influence on others, (2) whether
the relevant experts participated at least once in
the discussion, (3) whether the thread polite and
conducive to encouraging others to be willing to
provide help, (4) whether the thread is popular,
and (5) whether replies aim at answering questions
instead of off topic discussion.
Con = ?
ci
3
?
i=1
x
Qai
+ ?
c4
x
Sim
+ ?
c5
x
Len
Fen = ?
p1
x
Stx
+ ?
p2
x
Etx
+ ?
p3
x
Thx
+ ?
p4
x
Dfe
+ ?
p5
x
Pae
Sin = ?
u1
x
Rep
+ ?
u2
x
Pst
+ ?
u3
x
Res
+ ?
u4
x
Uvt
Pop = ?
t1
x
Cmt
+ ?
t2
x
Tvt
+ ?
t3
x
Mvt
+ ?
t4
x
Svt
Epr = ?
a0
x
Exp
Label = ?
1
Con+ ?
2
Fen+ ?
3
Sin
+ ?
4
Pop+ ?
5
Epr
(2)
5.1.2 SEM Result Analysis
In this section, we discuss what we learn from the
SEM about the influence of each factor within the
model. We adopt the Structural Equation Model in
R (Rosseel, 2012) to conduct the validation, and
evaluate it by looking at the Comparative Fit Index
(CFI), Root Mean Square Error of Approximation
(RMSEA) and Standardized Root Mean Square
Residual (SRMR) (Barrett, 2007). Figure 2 shows
the influence of each observed variable on its
corresponding latent variable, and in turn the latent
variable on the resolved label. The weights on
each directed edge represent the standard estimated
parameter for measuring the influence. For the
model fitting, we get a RMSEA of 0.09 and SRMR
of 0.06, with a CFI of 0.89. The fit is not extremely
high, but it is moderate, and it is within the range
one would expect from a good fitting model when
a large set of variables is considered.
Based on Figure 2, firstly, starter influence
and expert participation contribute a lot to thread
resolveability, with a standard estimated parameter
of 0.619 and 0.587. This makes sense that who
posts the question and who gives replies matter
a lot in identifying whether a thread is resolved.
Next, content matching contributes 0.178 to the
resolving of a thread, which means matching
between question and replies does differentiate
between resolved and unresolved threads, but less
so than who participates, perhaps because the
observed variables are very shallow indicators
of relevance. Friendliness is not very strongly
predictive of resolvability. Similarly, Thread
popularity contributes only 0.051 to the prediction,
without significant influence compared to the other
four latent variables, which are all significant.
Thus we conclude that starter influence, expert
participation, and content matching are strong
factors while friendliness and thread popularity
could help us separate resolved and unresolved
27
threads, but less so than the other two.
5.2 Resolveability Prediction
The influences of five latent factors on thread
resolveability are demonstrated as above. In this
part, we build an ensemble logistic regression
model to leverage those findings to predict whether
a given thread is resolved or not.
5.2.1 Ensemble Regression Model
An ensemble logistic regression model is proposed
to deal with the prediction of whether a thread
is resolved or not. That is, given the question
and a set of potential replies, as well as the five
latent variables and associated observed variables,
we want to predict whether a question has been
answered. Our ensemble logistic model works
in the following way. Firstly we train a separate
logistic model for each of the five aspects defined
above, i.e. five sub logistic model of how each
aspect predicts the resolved property. Then those
sub-models are included together in an ensemble
in order to contribute to a final logistic model,
which takes those results as the input features.
Similar to generalized boosting (Friedman et al.,
1998), this regression model integrates five weak
predictors that capture five different aspects of
thread resolveability, and construct a two layer
logistic ensemble, which is distinct from a linear
voting strategy. Our ensemble model relaxes
the assumption of linearity and thus offers more
flexibility in finding an effective predictive model.
This process is formalized below.
?
R
j
=
1
1 + e
?
?
k
i=1
?
i
?
?
R
ij
(3)
Here, k refers to the number of latent aspects.
?
R
j
is the predicted resolved score for thread j; if it
is larger than a threshold, the prediction of that
thread question is resolved, otherwise it remains
unresolved.
?
R
ij
is the predicted resolved score
of latent factor set i on thread j, trained on the
corresponding latent factor set.
5.2.2 Prediction Results
To demonstrate the predictive abilities of the five
latent factors, we use our ensemble regression
model to predict thread resolution. 10-fold cross
validation is used, and the prediction results will be
evaluated using the metrics Recall, Precision, and
AUC (Area under Curve). For baselines, we begin
with the simplest model EndThx, which simply
Single Model Precision Recall AUC
Si 0.697 0.696 0.791
Ep 0.602 0.590 0.572
Ct 0.626 0.616 0.647
Tp 0.594 0.579 0.626
Fr 0.639 0.633 0.685
Table 2: Prediction Result of Single Latent Factor
Model Precision Recall AUC
EndThx 0.629 0.612 0.593
Si + Ep 0.803 0.802 0.857
Si+Ep+Ct 0.819 0.815 0.884
Si+Ep+Ct+Fr 0.823 0.823 0.893
ALL-Linear 0.826 0.826 0.894
ALL-Ensemble 0.831 0.831 0.896
Table 3: Prediction Result
bases the prediction on whether the current thread
ends up with a gratitude sentence. This makes
sense because it is natural that students will express
their gratitude after receiving others? help. One
simple baseline is the Majority, which predicts the
testing thread as the majority status (unresolved in
our dataset), leading to a accuracy of 0.503; Si+Ep
is a combination of the latent aspect of starter
influence and expert participation; and Si+Ep+Ct
adds the content matching latent set on Si+Ep;
Si+Ep+Ct+Fr is defined similarly. ALL-Linear
is adding all five latent factor sets and predicts the
resolved or not using a linear logistic regression.
Comparably, ALL-Ensemble is trained using the
nonlinear ensemble logistic regression model. The
combination results as well as a comparison are
summarized in Table 3. For the influence of each
single latent aspect on the same prediction task, we
present them correspondingly in Table 2, where
Si, Ep, Ct, Tp, and Fr represent Student Influence,
Expert Participation, Content Matching, Thread
Populratiy, and Friendliness respectively.
Looking at the five latent aspects, (1) we con-
clude that, starter influence has the most powerful
influence on thread resolution. It improves a lot
on the Precision metric, and 50.25% on AUC
compared to the EndThx. It makes sense that,
if a user posts a lot, and often helps answer others?
questions, it is more likely that his/her question will
get a lot attention; (2) Thread Popularity, by itself
works better than the baseline under the metric of
AUC. The features in this set are not so directly
28
connected to thread resolution from a conceptual
standpoint compared to whether a thread ends with
thanks. However, it unexpectedly achieves an AUC
of 0.626, which is higher than the baseline. (3) For
content matching, the precision is similar to that
of EndThx, but in contrast, this model achieves
a good improvement on AUC. Content matching
describes the similarities between a question and
a reply, which is a direct indication of whether
the reply is trying to answer the question. (4)
Friendliness has a significant predictive ability in
connection with thread resolution. For the AUC, it
offers about a 13% improvement over the baseline.
It is reasonable that a resolved thread tends to
be more polite, which means people use ?please?,
?thanks? more than in other unresolved threads.
To build the ensemble models, we combine the
latent factor sets in the order of their strength
of estimated influence on resolveability. We
firstly integrate the starter influence and expert
participation, as we can see, it achieves significant
improvement over the simpler baselines, with 28%
higher on Precision, 31% on Recall and 45%
on AUC. It even performs better on the three
metrics than any of the single models in Table2.
Si+Ep+Ct also gives a substantial increase on
the metrics and when adding semantic content
matching, Si+Ep+Ct+Fr is about 3% better than
Si+Ep on precision and recall. This indicates that
friendliness and content matching are capturing
different aspects of the thread resolveability from
starter influence and expert participation. Besides,
the ALL-Linear performs best among all one layer
regression models. This shows that even though
thread popularity contributes least to resolved or
not based on the SEM result, it gives a different
perspective of the thread resolveability and is not
to be ignored. When we applied our proposed
ensemble regression model ALL-Ensemble using
the five latent factor sets, we find that it outperforms
all one layer logistic regressors, especially in Recall
and Precision. This demonstrates that the two-
layer ensemble logistic regression model?s added
representational power is needed for this problem.
6 Conclusions and Future Research
In this paper, we have focused on improving the
thread resolveability in MOOC discussion forums.
Our investigation is divided into two separate
studies that leverage a common conceptual model
involving five latent factors that are associated with
thread resolution. Our first study validates the
five latent variable structures using a SEM model,
which helps us to validate our assumptions and
hone in on those factors that are most promising
to leverage in subsequent work. It enables us
to assess the relative strength of each factor?s
influence on thread resolveability, and provides
a foundation for the other study. The second
study?s focus is predicting thread resolution based
on the first phase?s findings. In addition to
serving as a test of generality from trained data to
unseen data, the predictive model may also have a
practical benefit. In particular, thread resoveability
identification could provide the potential to achieve
a better allocation of valuable human resources to
work on unresolved threads, which increases the
potential for students to get their support needs
met in Massive Open Online Courses. Our work
is contenxtualized in the specifics of MOOCs
as an online context including the particulars of
interaction practices within those contexts. Thus,
in addition to building on existing QA work in
our feature engineering, we also introduce new
directions, such as the linguistic modeling of
speaker politeness, and conduct forms of latent
semantic matching that have proven effective in
dialogue systems.
However, we believe there is a need for further
modeling in order to fully understand thread
resolveability. A limitation of the current work is
that it was conducted in only one course. Thus,
we will be in a stronger position for moving
forward if we explicitly address the question of
generalizability across courses with further corpus
based investigation. Besides, how to transfer
the prediction models from forums with resolved
buttons to ones that have no such affordances,
which may be challenging because of differences
in the distribution of behaviors.
Acknowledgement
This research was funded in part by NSF grants
IIS-1320064 and OMA-0836012 and funding from
Google.
References
Eugene Agichtein, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Finding
high-quality content in social media. In Proceedings
of the 2008 International Conference on Web Search
and Data Mining, WSDM ?08, pages 183?194, New
York, NY, USA. ACM.
29
Eugene Agichtein, Yandong Liu, and Jiang Bian.
2009. Modeling information-seeker satisfaction in
community question answering. ACM Trans. Knowl.
Discov. Data, 3(2):10:1?10:27, April.
Ashton Anderson, Daniel Huttenlocher, Jon Kleinberg,
and Jure Leskovec. 2012. Discovering value from
community activity on focused question answering
sites: A case study of stack overflow. In Pro-
ceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data
Mining, KDD ?12, pages 850?858, New York, NY,
USA. ACM.
Girish Balakrishnan. 2013. Predicting student reten-
tion in massive open online courses using hidden
markov models. Master?s thesis, EECS Department,
University of California, Berkeley, May.
Paul Barrett. 2007. Structural equation modelling:
Adjudging model fit. Personality and Individual
Differences, 42(5):815?824.
Kenneth A Bollen. 1987. Total, direct, and indirect
effects in structural equation models. Sociological
methodology, 17(1):37?69.
Eric Brill, Susan Dumais, and Michele Banko. 2002.
An analysis of the askmsr question-answering sys-
tem. In Proceedings of the ACL-02 Conference on
Empirical Methods in Natural Language Processing
- Volume 10, EMNLP ?02, pages 257?264, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Moira Burke and Robert Kraut. 2008. Mind your
ps and qs: The impact of politeness and rudeness
in online communities. In Proceedings of the
2008 ACM Conference on Computer Supported
Cooperative Work, CSCW ?08, pages 281?284, New
York, NY, USA. ACM.
Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,
Dan Jurafsky, Jure Leskovec, and Christopher Potts.
2013. A computational approach to politeness with
application to social factors. In ACL (1), pages 250?
259.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
1998. Additive logistic regression: a statistical view
of boosting. Annals of Statistics, 28:2000.
Peter Jansen, Mihai Surdeanu, and Peter Clark. 2014.
Discourse complements lexical semantics for non-
factoid answer reranking. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics, ACL ?13=4. Association for
Computational Linguistics.
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and
Soyeon Park. 2006. A framework to predict the
quality of answers with non-textual features. In
Proceedings of the 29th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ?06, pages 228?235,
New York, NY, USA. ACM.
Yutaka Kabutoya, Tomoharu Iwata, Hisako Shiohara,
and Ko Fujimura. 2010. Effective question recom-
mendation based on multiple features for question
answering communities. In ICWSM.
Ren?e F Kizilcec, Chris Piech, and Emily Schneider.
2013. Deconstructing disengagement: analyzing
learner subpopulations in massive open online cours-
es. In Proceedings of the Third International
Conference on Learning Analytics and Knowledge,
pages 170?179. ACM.
Ann Lieberman. 1995. Practices that support
teacher development: Transforming conceptions of
professional learning. Innovating and Evaluating
Science Education: NSF Evaluation Forums, 1992-
94, page 67.
Yandong Liu and Eugene Agichtein. 2008. You?ve
got answers: Towards personalized models for pre-
dicting success in community question answering.
In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics on
Human Language Technologies: Short Papers, HLT-
Short ?08, pages 97?100, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Qiaoling Liu, Eugene Agichtein, Gideon Dror, Evgeniy
Gabrilovich, Yoelle Maarek, Dan Pelleg, and Idan
Szpektor. 2011. Predicting web searcher satisfac-
tion with existing community-based answers. In
Proceedings of the 34th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, SIGIR ?11, pages 415?424, New
York, NY, USA. ACM.
Rivindu Perera. 2012. Ipedagogy: Question answering
system based on web information clustering. In
Technology for Education (T4E), 2012 IEEE Fourth
International Conference on, pages 245?246. IEEE.
Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong
Do, Andrew Ng, and Daphne Koller. 2013. Tuned
models of peer assessment in MOOCs. In Pro-
ceedings of The 6th International Conference on
Educational Data Mining (EDM 2013).
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by predictive
annotation. In Proceedings of the 23rd Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR
?00, pages 184?191, New York, NY, USA. ACM.
Mingcheng Qu, Guang Qiu, Xiaofei He, Cheng Zhang,
Hao Wu, Jiajun Bu, and Chun Chen. 2009.
Probabilistic question recommendation for question
answering communities. In Proceedings of the
18th International Conference on World Wide Web,
WWW ?09, pages 1229?1230, New York, NY, USA.
ACM.
Carolyn Penstein Ros?e, Ryan Carlson, Diyi Yang,
Miaomiao Wen, Lauren Resnick, Pam Goldman,
and Jennifer Sherer. 2014. Social factors that
30
contribute to attrition in moocs. In Proceedings
of the first ACM conference on Learning@ scale
conference, pages 197?198. ACM.
Yves Rosseel. 2012. lavaan: An r package for
structural equation modeling. Journal of Statistical
Software, 48(2):1?36, 5.
Chirag Shah and Jefferey Pomerantz. 2010. Evalu-
ating and predicting answer quality in community
qa. In Proceedings of the 33rd International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ?10, pages 411?418,
New York, NY, USA. ACM.
Juyup Sung, Jae-Gil Lee, and Uichin Lee. 2013.
Booming up the long tails: Discovering potentially
contributive users in community-based question an-
swering services. In ICWSM.
Qiongjie Tian, Peng Zhang, and Baoxin Li. 2013.
Towards predicting the best answers in community-
based question-answering services. In Emre Kici-
man, Nicole B. Ellison, Bernie Hogan, Paul Resnick,
and Ian Soboroff, editors, ICWSM. The AAAI Press.
Miaomiao Wen, Diyi Yang, and Carolyn Penstein Ros?e.
2014. Linguistic reflections of student engagement
in massive open online courses. In Proceedings of
the International Conference on Weblogs and Social
Media.
Diyi Yang, Tanmay Sinha, David Adamson, and
Carolyn Penstein Rose. 2013. turn on, tune
in, drop out: Anticipating student dropouts in
massive open online courses. In Workshop on Data
Driven Education, Advances in Neural Information
Processing Systems 2013.
Diyi Yang, Miaomiao Wen, and Carolyn Rose. 2014.
Peer influence on attrition in massive open online
courses. In Proceedings of Educational Data
Mining.
Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and
Andrzej Pastusiak. 2013. Question answering using
enhanced lexical semantic models. In Proceedings
of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers), pages 1744?1753, Sofia, Bulgaria, August.
Association for Computational Linguistics.
31
