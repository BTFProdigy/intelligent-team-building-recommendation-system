Proceedings of the ACL 2007 Demo and Poster Sessions, pages 69?72,
Prague, June 2007. c?2007 Association for Computational Linguistics
Rethinking Chinese Word Segmentation: Tokenization, Character
Classification, or Wordbreak Identification
Chu-Ren Huang
Institute of Linguistics
Academia Sinica,Taiwan
churen@gate.sinica.edu.tw
Petr S?imon
Institute of Linguistics
Academia Sinica,Taiwan
sim@klubko.net
Shu-Kai Hsieh
DoFLAL
NIU, Taiwan
shukai@gmail.com
Laurent Pre?vot
CLLE-ERSS, CNRS
Universite? de Toulouse, France
prevot@univ-tlse2.fr
Abstract
This paper addresses two remaining chal-
lenges in Chinese word segmentation. The
challenge in HLT is to find a robust seg-
mentation method that requires no prior lex-
ical knowledge and no extensive training to
adapt to new types of data. The challenge
in modelling human cognition and acqui-
sition it to segment words efficiently with-
out using knowledge of wordhood. We pro-
pose a radical method of word segmenta-
tion to meet both challenges. The most
critical concept that we introduce is that
Chinese word segmentation is the classifi-
cation of a string of character-boundaries
(CB?s) into either word-boundaries (WB?s)
and non-word-boundaries. In Chinese, CB?s
are delimited and distributed in between two
characters. Hence we can use the distri-
butional properties of CB among the back-
ground character strings to predict which
CB?s are WB?s.
1 Introduction: modeling and theoretical
challenges
The fact that word segmentation remains a main re-
search topic in the field of Chinese language pro-
cessing indicates that there maybe unresolved theo-
retical and processing issues. In terms of processing,
the fact is that none of exiting algorithms is robust
enough to reliably segment unfamiliar types of texts
before fine-tuning with massive training data. It is
true that performance of participating teams have
steadily improved since the first SigHAN Chinese
segmentation bakeoff (Sproat and Emerson, 2004).
Bakeoff 3 in 2006 produced best f-scores at 95%
and higher. However, these can only be achieved af-
ter training with the pre-segmented training dataset.
This is still very far away from real-world applica-
tion where any varieties of Chinese texts must be
successfully segmented without prior training for
HLT applications.
In terms of modeling, all exiting algorithms suffer
from the same dilemma. Word segmentation is sup-
posed to identify word boundaries in a running text,
and words defined by these boundaries are then com-
pared with the mental/electronic lexicon for POS
tagging and meaning assignments. All existing seg-
mentation algorithms, however, presuppose and/or
utilize a large lexical databases (e.g. (Chen and Liu,
1992) and many subsequent works), or uses the po-
sition of characters in a word as the basis for seg-
mentation (Xue, 2003).
In terms of processing model, this is a contradic-
tion since segmentation should be the pre-requisite
of dictionary lookup and should not presuppose lex-
ical information. In terms of cognitive modeling,
such as for acquisition, the model must be able to ac-
count for how words can be successfully segmented
and learned by a child/speaker without formal train-
ing or a priori knowledge of that word. All current
models assume comprehensive lexical knowledge.
2 Previous work
Tokenization model. The classical model, de-
scribed in (Chen and Liu, 1992) and still adopted in
many recent works, considers text segmentation as a
69
tokenization. Segmentation is typically divided into
two stages: dictionary lookup and out of vocabulary
(OOV) word identification. This approach requires
comparing and matching tens of thousands of dic-
tionary entries in addition to guessing thousands of
OOV words. That is, this is a 104x104 scale map-
ping problem with unavoidable data sparseness.
More precisely the task consist in finding
all sequences of characters Ci, . . . , Cn such that
[Ci, . . . Cn] either matches an entry in the lexicon
or is guessed to be so by an unknown word resolu-
tion algorithm. One typical kind of the complexity
this model faces is the overlapping ambiguity where
e.g. a string [Ci ? 1, Ci, Ci + 1] contains multiple
substrings, such as [Ci ? 1, Ci, ] and [Ci,Ci + 1],
which are entries in the dictionary. The degree of
such ambiguities is estimated to fall between 5% to
20% (Chiang et al, 1996; Meng and Ip, 1999).
2.1 Character classification model
A popular recent innovation addresses the scale
and sparseness problem by modeling segmentation
as character classification (Xue, 2003; Gao et al,
2004). This approach observes that by classifying
characters as word-initial, word-final, penultimate,
etc., word segmentation can be reduced to a simple
classification problem which involves about 6,000
characters and around 10 positional classes. Hence
the complexity is reduced and the data sparseness
problem resolved. It is not surprising then that the
character classification approach consistently yields
better results than the tokenization approach. This
approach, however, still leaves two fundamental
questions unanswered. In terms of modeling, us-
ing character classification to predict segmentation
not only increases the complexity but also necessar-
ily creates a lower ceiling of performance In terms
of language use, actual distribution of characters is
affected by various factors involving linguistic vari-
ation, such as topic, genre, region, etc. Hence the
robustness of the character classification approach
is restricted.
The character classification model typically clas-
sifies all characters present in a string into at least
three classes: word Initial, Middle or Final po-
sitions, with possible additional classification for
word-middle characters. Word boundaries are in-
ferred based on the character classes of ?Initial? or
?Final?.
This method typically yields better result than the
tokenization model. For instance, Huang and Zhao
(2006) claims to have a f-score of around 97% for
various SIGHAN bakeoff tasks.
3 A radical model
We propose a radical model that returns to the
core issue of word segmentation in Chinese. Cru-
cially, we no longer pre-suppose any lexical knowl-
edge. Any unsegmented text is viewed as a string
of character-breaks (CB?s) which are evenly dis-
tributed and delimited by characters. The characters
are not considered as components of words, instead,
they are contextual background providing informa-
tion about the likelihood of whether each CB is also
a wordbreak (WB). In other words, we model Chi-
nese word segmentation as wordbreak (WB) iden-
tification which takes all CB?s as candidates and
returns a subset which also serves as wordbreaks.
More crucially, this model can be trained efficiently
with a small corpus marked with wordbreaks and
does not require any lexical database.
3.1 General idea
Any Chinese text is envisioned as se-
quence of characters and character-boundaries
CB0C1CB1C2 . . . CBi?1CiCBi . . . CBn?1CnCBn The
segmentation task is reduced to finding all CBs
which are also wordbreaks WB.
3.2 Modeling character-based information
Since CBs are all the same and do not carry any
information, we have to rely on their distribution
among different characters to obtain useful infor-
mation for modeling. In a segmented corpus, each
WB can be differentiated from a non-WB CB by the
character string before and after it. We can assume
a reduced model where either one character imme-
diately before and after a CB is considered or two
characters (bigram). These options correspond to
consider (i) only word-initial and word-final posi-
tions (hereafter the 2-CB-model or 2CBM) or (ii) to
add second and penultimate positions (hereafter the
4-CB-model or 4CBM). All these positions are well-
attested as morphologically significant.
70
3.3 The nature of segmentation
It is important to note that in this approaches,
although characters are recognized, unlike (Xue,
2003) and Huang et al (2006), charactes simply
are in the background. That is, they are the neces-
sary delimiter, which allows us to look at the string
of CB?s and obtaining distributional information of
them.
4 Implementation and experiments
In this section we slightly change our notation to
allow for more precise explanation. As noted be-
fore, Chinese text can be formalized as a sequence
of characters and intervals as illustrated in we call
this representation an interval form.
c1I1c2I2 . . . cn?1In?1cn.
In such a representation, each interval Ik is either
classified as a plain character boundary (CB) or as
a word boundary (WB).
We represent the neighborhood of the character
ci as (ci?2, Ii?2, ci?1, Ii?1, ci, Ii, ci+1, Ii+1), which
we can be simplified as (I?2, I?1, ci, I+1, I+2) by
removing all the neighboring characters and retain-
ing only the intervals.
4.1 Data collection models
This section makes use of the notation introduced
above for presenting several models accounting for
character-interval class co-occurrence.
Word based model. In this model, statistical data
about word boundary frequencies for each character
is retrieved word-wise. For example, in the case of
a monosyllabic word only two word boundaries are
considered: one before and one after the character
that constitutes the monosyllabic word in question.
The method consists in mapping all the Chinese
characters available in the training corpus to a vector
of word boundary frequencies. These frequencies
are normalized by the total frequency of the char-
acter in a corpus and thus represent probability of a
word boundary occurring at a specified position with
regard to the character.
Let us consider for example, a tri-syllabic word
W = c1c2c3, that can be rewritten as the following
interval form as W I = IB?1c1I
N
1 c2I
N
2 c3I
B
3 .
In this interval form, each interval Ik is marked
as word boundary B or N for intervals within words.
When we consider a particular character c1 in W ,
there is a word boundary at index?1 and 3. We store
this information in a mapping c1 = {?1 : 1, 3 : 1}.
For each occurrence of this character in the corpus,
we modify the character vector accordingly, each
WB corresponding to an increment of the relevant
position in the vector. Every character in every word
of the corpus in processed in a similar way.
Obviously, each character yields only information
about positions of word boundaries of a word this
particular character belongs to. This means that the
index I?1 and I3 are not necessarily incremented
everytime (e.g. for monosyllabic and bi-syllabic
words)
Sliding window model. This model does not op-
erate on words, but within a window of a give size
(span) sliding through the corpus. We have exper-
imented this method with a window of size 4. Let
us consider a string, s = ?c1c2c3c4? which is not
necessarily a word and is rewritten into an interval
form as sI = ?c1I1c2I2c3I3c4I4?. We store the
co-occurrence character/word boundaries informa-
tion in a fixed size (span) vector.
For example, we collect the information for
character c3 and thus arrive at a vector c3 =
(I1, I2, I3, I4), where 1 is incremented at the respec-
tive position ifIk = WB, zero otherwise.
This model provides slightly different informa-
tion that the previous one. For example, if
a sequence of four characters is segmented as
c1IN1 c2I
B
2 c3I
B
3 c4I
B
4 (a sequence of one bi-syllabic
and two monosyllabic words), for c3 we would also
get probability of I4, i.e. an interval with index +2
. In other words, this model enables to learn WB
probability across words.
4.2 Training corpus
In the next step, we convert our training corpus into
a corpus of interval vectors of specified dimension.
Let?s assume we are using dimension span = 4.
Each value in such a vector represents the proba-
bility of this interval to be a word boundary. This
probability is assigned by character for each position
with regard to the interval. For example, we have
segmented corpus C = c1I1c2I2 . . . cn?1In?1cn,
where each Ik is labeled as B for word boundary
or N for non-boundary.
71
In the second step, we move our 4-sized window
through the corpus and for each interval we query
a character at the corresponding position from the
interval to retrieve the word boundary occurrence
probability. This procedure provides us with a vec-
tor of 4 probability values for each interval. Since
we are creating this training corpus from an already
segmented text, a class (B or N ) is assigned to each
interval.
The testing corpus (unsegmented) is encoded in a
similar way, but does not contain the class labels B
and N .
Finally, we automatically assign probability of 0.5
for unseen events.
4.3 Predicting word boundary with a classifier
The Sinica corpus contains 6820 types of characters
(including Chinese characters, numbers, punctua-
tion, Latin alphabet, etc.). When the Sinica corpus is
converted into our interval vector corpus, it provides
14.4 million labeled interval vectors. In this first
study we have implement a baseline model, without
any pre-processing of punctuation, numbers, names.
A decision tree classifier (Ruggieri, 2004) has
been adopted to overcome the non-linearity issue.
The classifier was trained on the whole Sinica cor-
pus, i.e. on 14.4 million interval vectors. Due to
space limit, actual bakeoff experiment result will be
reported in our poster presentation.
Our best results is based on the sliding window
model, which provides better results. It has to be
emphasized that the test corpora were not processed
in any way, i.e. our method is sufficiently robust to
account for a large number of ambiguities like nu-
merals, foreign words.
5 Conclusion
In this paper, we presented a radical and robust
model of Chinese segmentation which is supported
by initial experiment results. The model does not
pre-suppose any lexical information and it treats
character strings as context which provides infor-
mation on the possible classification of character-
breaks as word-breaks. We are confident that once
a standard model of pre-segmentation, using tex-
tual encoding information to identify WB?s which
involves non-Chinese characters, will enable us to
achieve even better results. In addition, we are look-
ing at other alternative formalisms and tools to im-
plement this model to achieve the optimal results.
Other possible extensions including experiments to
simulate acquisition of wordhood knowledge to pro-
vide support of cognitive modeling, similar to the
simulation work on categorization in Chinese by
(Redington et al, 1995). Last, but not the least,
we will explore the possibility of implementing a
sharable tool for robust segmentation for all Chinese
texts without training.
References
Academia Sinica Balanced Corpus of Modern Chinese.
http://www.sinica.edu.tw/SinicaCorpus/
Chen K.J and Liu S.H. 1992. Word Identification for
Mandarin Chinese sentences. Proceedings of the 14th
conference on Computational Linguistics, p.101-107,
France.
Chiang,T.-H., J.-S. Chang, M.-Y. Lin and K.-Y. Su. 1996.
Statistical Word Segmentation. In C.-R. Huang, K.-J.
Chen and B.K. T?sou (eds.): Journal of Chinese Lin-
guistics, Monograph Series, Number 9, Readings in
Chinese Natural Language Processing, pp. 147-173.
Gao, J. and A. Wu and Mu Li and C.-N.Huang and H. Li
and X. Xia and H. Qin. 2004. Adaptive Chinese Word
Segmentation. In Proceedings of ACL-2004.
Meng, H. and C. W. Ip. 1999. An Analytical Study of
Transformational Tagging for Chinese Text. In. Pro-
ceedings of ROCLING XII. 101-122. Taipei
Ruggieri S. 2004. YaDT: Yet another Decision Tree
builder. Proceedings of the 16th International Con-
ference on Tools with Artificial Intelligence (ICTAI
2004): 260-265. IEEE Press, November 2004.
Richard Sproat and Thomas Emerson. 2003. The
First International Chinese Word Segmentation Bake-
off. Proceedings of the Second SIGHAN Workshop on
Chinese Language Processing, Sapporo, Japan, July
2003.
Xue, N. 2003. Chinese Word Segmentation as Charac-
ter Tagging. Computational Linguistics and Chinese
Language Processing. 8(1): 29-48
Redington, M. and N. Chater and C. Huang and L. Chang
and K. Chen. 1995. The Universality of Simple Dis-
tributional Methods: Identifying Syntactic Categories
in Mandarin Chinese. Presented at the Proceedings of
the International Conference on Cognitive Science and
Natural Language Processing. Dublin City University.
72
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 153?156,
Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Discovery of Named Entity Variants
? Grammar-driven Approaches to Non-alphabetical Transliterations
Chu-Ren Huang
Institute of Linguistics
Academia Sinica, Taiwan
churenhuang@gmail.com
Petr S?imon
Institute of Linguistics
Academia Sinica, Taiwan
sim@klubko.net
Shu-Kai Hsieh
DoFLAL
NIU, Taiwan
shukai@gmail.com
Abstract
Identification of transliterated names is a
particularly difficult task of Named Entity
Recognition (NER), especially in the Chi-
nese context. Of all possible variations of
transliterated named entities, the difference
between PRC and Taiwan is the most preva-
lent and most challenging. In this paper, we
introduce a novel approach to the automatic
extraction of diverging transliterations of
foreign named entities by bootstrapping co-
occurrence statistics from tagged and seg-
mented Chinese corpus. Preliminary experi-
ment yields promising results and shows its
potential in NLP applications.
1 Introduction
Named Entity Recognition (NER) is one of the most
difficult problems in NLP and Document Under-
standing. In the field of Chinese NER, several
approaches have been proposed to recognize per-
sonal names, date/time expressions, monetary and
percentage expressions. However, the discovery of
transliteration variations has not been well-studied
in Chinese NER. This is perhaps due to the fact
that the transliteration forms in a non-alphabetic lan-
guage such as Chinese are opaque and not easy to
compare. On the hand, there is often more than
one way to transliterate a foreign name. On the
other hand, dialectal difference as well as differ-
ent transliteration strategies often lead to the same
named entity to be transliterated differently in dif-
ferent Chinese speaking communities.
Corpus Example (Clinton) Frequency
XIN ??? 24382
CNA ??? 150
XIN ??? 0
CNA ??? 120842
Table 1: Distribution of two transliteration variants
for ?Clinton? in two sub-corpora
Of all possible variations, the cross-strait differ-
ence between PRC and Taiwan is the most prevalent
and most challenging.1The main reason may lie in
the lack of suitable corpus.
Even given some subcorpora of PRC and Taiwan
variants of Chinese, a simple contrastive approach is
still not possible. It is because: (1) some variants
might overlap and (2) there are more variants used
in each corpus due to citations or borrowing cross-
strait. Table 1 illustrates this phenomenon, where
CNA stands for Central News Agency in Taiwan,
XIN stands for Xinhua News Agency in PRC, re-
spectively.
With the availability of Chinese Gigaword Cor-
pus (CGC) and Word Sketch Engine (WSE) Tools
(Kilgarriff, 2004). We propose a novel approach
towards discovery of transliteration variants by uti-
lizing a full range of grammatical information aug-
mented with phonological analysis.
Existing literatures on processing of translitera-
tion concentrate on the identification of either the
transliterated term or the original term, given knowl-
edge of the other (e.g. (Virga and Khudanpur,
1For instance, we found at least 14 transliteration variants
for Lewinsky,such as ????????????????????????
?????????????????????????????????????
?????? and so on.
153
2003)). These studies are typically either rule-based
or statistics-based, and specific to a language pair
with a fixed direction (e.g. (Wan and Verspoor,
1998; Jiang et al, 2007)). To the best of our knowl-
edge, ours is the first attempt to discover transliter-
ated NE?s without assuming prior knowledge of the
entities. In particular, we propose that transliteration
variants can be discovered by extracting and com-
paring terms from similar linguistic context based
on CGC and WSE tools. This proposal has great po-
tential of increasing robustness of future NER work
by enabling discovery of new and unknown translit-
erated NE?s.
Our study shows that resolution of transliterated
NE variations can be fully automated. This will have
strong and positive implications for cross-lingual
and multi-lingual informational retrieval.
2 Bootstrapping transliteration pairs
The current study is based on Chinese Gigaword
Corpus (CGC) (Graff el al., 2005), a large corpus
contains with 1.1 billion Chinese characters contain-
ing data from Central News Agency of Taiwan (ca.
700 million characters), Xinhua News Agency of
PRC (ca. 400 million characters). These two sub-
corpora represent news dispatches from roughly the
same period of time, i.e. 1990-2002. Hence the two
sub-corpora can be expected to have reasonably par-
allel contents for comparative studies.2
The premises of our proposal are that transliter-
ated NE?s are likely to collocate with other translit-
erated NE?s, and that collocates of a pair of translit-
eration variants may form contrasting pairs and are
potential variants. In particular, since the transliter-
ation variations that we are interested in are those
between PRC and Taiwan Mandarin, we will start
with known contrasting pairs of these two language
variants and mine potential variant pairs from their
collocates. These potential variant pairs are then
checked for their phonological similarity to deter-
mine whether they are true variants or not. In order
to effectively select collocates from specific gram-
matical constructions, the Chinese Word Sketch3 is
adopted. In particular, we use the Word Sketch dif-
2To facilitate processing, the complete CGC was segmented
and POS tagged using the Academia Sinica segmentation and
tagging system (Ma and Huang, 2006).
3http://wordsketch.ling.sinica.edu.tw
ference (WSDiff) function to pick the grammatical
contexts as well as contrasting pairs. It is important
to bear in mind that Chinese texts are composed of
Chinese characters, hence it is impossible to com-
pare a transliterated NE with the alphabetical form
in its original language. The following characteris-
tics of a transliterated NE?s in CGC are exploited to
allow discovery of transliteration variations without
referring to original NE.
? frequent co-occurrence of named entities
within certain syntagmatic relations ? named
entities frequently co-occur in relations such as
AND or OR and this fact can be used to collect
and score mutual predictability.
? foreign named entities are typically transliter-
ated phonetically ? transliterations of the same
name entity using different characters can be
matched by using simple heuristics to map their
phonological value.
? presence and co-occurrence of named entities
in a text is dependent on a text type ? journalis-
tic style cumulates many foreign named entities
in close relations.
? many entities will occur in different domains
? famous person can be mentioned together
with someone from politician, musician, artist
or athlete. Thus allows us to make leaps from
one domain to another.
There are, however, several problems with the
phonological representation of foreign named enti-
ties in Chinese. Due to the nature of Chinese script,
NE transliterations can be realized very differently.
The following is a summary of several problems that
have to be taken into account:
? word ending: ??? vs.???? ?Arafat? or ?
?? vs.???? ?Mubarak?. The final conso-
nant is not always transliterated. XIN translit-
erations tend to try to represent all phonemes
and often add vowels to a final consonant to
form a new syllable, whereas CNA transliter-
ation tends to be shorter and may simply leave
out a final consonant.
? gender dependent choice of characters: ???
?Leslie? vs.??? ?Chris? or ???? vs. ???
154
?. Some occidental names are gender neutral.
However, the choice of characters in a personal
name in Chinese is often gender sensitive. So
these names are likely to be transliterated dif-
ferently depending on the gender of its referent.
? divergent representations caused by scope of
transliteration, e.g. both given and surname
vs. only surname: ???? / ????? ?Venus
Williams?.
? difference in phonological interpretation: ??
? vs. ??? ?Rafter? or??? vs. ??? ?Connors?.
? native vs. non-native pronunciation: ??? ?
vs. ???? ?Escudero? or ??? vs. ???
?Federer?.
2.1 Data collection
All data were collected from Chinese Gigaword Cor-
pus using Chinese Sketch Engine with WSDiff
function, which provides side-by-side syntagmatic
comparison of Word Sketches for two different
words. WSDiff query for wi and wj returns pat-
terns that are common for both words and also pat-
terns that are particular for each of them. Three data
sets are thus provided. We neglect the common pat-
terns set and concentrate only on the wordlists spe-
cific for each word.
2.2 Pairs extraction
Transliteration pairs are extracted from the two sets,
A and B, collected with WSDiff using default set
of seed pairs :
- for each seed pair in seeds retrieve WSDiff for
and/or relation, thus have pairs of word lists,
< Ai, Bi >
- for each word wii ? Ai find best matching
counterpart(s) wij ? Bi. Comparison is done
using simple phonological rules, viz. 2.3
- use newly extracted pairs as new seeds (original
seeds are stored as good pairs and not queried
any more)
- loop until there are no new pairs
Notice that even though substantial proportion of
borrowing among different communities, there is no
mixing in the local context of collocation, which
means, local collocation could be the most reliable
way to detect language variants with known variants.
2.3 Phonological comparison
All word forms are converted from Chinese script
into a phonological representation4 during the pairs
extraction phase and then these representations are
compared and similarity scores are given to all pair
candidates.
A lot of Chinese characters have multiple pro-
nunciations and thus multiple representations are de-
rived. In case of multiple pronunciations for certain
syllable, this syllable is commpared to its counter-
part from the other set. E.g. (? has three pronunci-
ations: ye`, xie?, she`. When comparing syllables such
as ?[pei,fei] and ?[fei], ? will be represented as
[fei]. In case of pairs such as ??? [ye er qin] and
??? [ye er qin], which have syllables with multi-
ple pronunciations and this multiple representations.
However, since these two potential variants share
the first two characters (out of three), they are con-
sidered as variants without superfluous phonological
checking.
Phonological representations of whole words are
then compared by Levenstein algorithm, which is
widely used to measure the similarity between two
strings. First, each syllable is split into initial and
final components: gao:g+ao. In case of syllables
without initials like er, an ? is inserted before the
syllable, thus er:?+er.
Before we ran the Levenstein measure, we also
apply phonological corrections on each pair of can-
didate representations. Rules used for these cor-
rections are derived from phonological features of
Mandarin Chinese and extended with few rules
from observation of the data: (1) For Initials, (a):
voiced/voiceless stop contrasts are considered as
similar for initials: g:k, e.g. ? [gao] (??) vs. ?
[ke] (??),d:t, b:p, (b): r:l ? [rui] (????) ? [lie]
(????) is added to distinctive feature set based on
observation. (2). For Finals, (a): pair ei:ui is eval-
uated as equivalent.5 (b): oppositions of nasalised
final is evaluated as dissimilar.
4http://unicode.org/charts/unihan.html
5Pinyin representation of phonology of Mandarin Chinese
does not follow the phonological reality exactly: [ui] = [uei]
etc.
155
2.4 Extraction algorithm
Our algorithm will potentially exhaust the whole
corpus, i.e. find most of the named entities that oc-
cur with at least few other names entities, but only
if seeds are chosen wisely and cover different do-
mains6. However, some domains might not over-
lap at all, that is, members of those domains never
appear in the corpus in relation and/or. And con-
currence of members within some domains might be
sparser than in other, e.g. politicians tend to be men-
tioned together more often than novelists. Nature of
the corpus also plays important role. It is likely to
retrieve more and/or related names from journal-
istic style. This is one of the reasons why we chose
Chinese Gigaword Corpus for this task.
3 Experiment and evaluation
We have tested our method on the Chinese Giga-
word Second Edition corpus with 11 manually se-
lected seeds Apart from the selection of the starter
seeds, the whole process is fully automatic. For this
task we have collected data from syntagmatic rela-
tion and/or, which contains words co-occurring
frequently with our seed words. When we make a
query for peoples names, it is expected that most of
the retrieved items will also be names, perhaps also
names of locations, organizations etc.
The whole experiment took 505 iterations in
which 494 pairs were extracted.
Our complete experiment with 11 pre-selected
transliteration pairs as seed took 505 iterations to
end. The iterations identified 494 effective transliter-
ation variant pairs (i.e. those which were not among
the seeds or pairs identified by earlier iteration.) All
the 494 candidate pairs were manually evaluated 445
of them are found to be actual contrast pairs, a pre-
cision of 90.01%. In addition, the number of new
transliteration pairs yielded is 4,045%, a very pro-
ductive yield for NE discovery.
Preliminary results show that this approach is
competitive against other approaches reported in
previous studies. Performances of our algorithms is
calculated in terms of precision rate with 90.01%.
6The term domain refers to politics,music,sport, film etc.
4 Conclusion and Future work
In this paper, we have shown that it is possible to
identify NE?s without having prior knowledge of
them. We also showed that, applying WSE to re-
strict grammatical context and saliency of colloca-
tion, we are able to effectively extract transliteration
variants in a language where transliteration is not
explicitly represented. We also show that a small
set of seeds is all it needs for the proposed method
to identify hundreds of transliteration variants. This
proposed method has important applications in in-
formation retrieval and data mining in Chinese data.
In the future, we will be experimenting with a dif-
ferent set of seeds in a different domain to test the
robustness of this approach, as well as to discover
transliteration variants in our fields. We will also be
focusing on more refined phonological analysis. In
addition, we would like to explore the possibility of
extending this proposal to other language pairs.
References
Jiang, L. and M.Zhou and L.f. Chien. 2007. Named En-
tity Discovery based on Transliteration and WWW [In
Chinese]. Journal of the Chinese Information Process-
ing Society. 2007 no.1. pp.23-29.
Graff, David et al 2005. Chinese Gigaword Second Edi-
tion. Linguistic Data Consortium, Philadelphia.
Ma, Wei-Yun and Huang, Chu-Ren. 2006. Uniform and
Effective Tagging of a Heterogeneous Giga-word Cor-
pus. Presented at the 5th International Conference on
Language Resources and Evaluation (LREC2006), 24-
28 May. Genoa, Italy.
Kilgarriff, Adam et al 2004. The Sketch Engine. Pro-
ceedings of EURALEX 2004. Lorient, France.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proc. of the ACL Workshop on Multi-
lingual Named Entity Recognition, pp.57-64.
Wan, Stephen and Cornelia Verspoor. 1998. Auto-
matic English-Chinese Name Transliteration for De-
velopment of Multiple Resources. In Proc. of COL-
ING/ACL, pp.1352-1356.
156
