Language Independent Text Correction using Finite State Automata
Ahmed Hassan? Sara Noeman
IBM Cairo Technology Development Center
Giza, Egypt
hasanah,noemans,hanyh@eg.ibm.com
Hany Hassan
Abstract
Many natural language applications, like
machine translation and information extrac-
tion, are required to operate on text with
spelling errors. Those spelling mistakes
have to be corrected automatically to avoid
deteriorating the performance of such ap-
plications. In this work, we introduce a
novel approach for automatic correction of
spelling mistakes by deploying finite state
automata to propose candidates corrections
within a specified edit distance from the mis-
spelled word. After choosing candidate cor-
rections, a language model is used to assign
scores the candidate corrections and choose
best correction in the given context. The
proposed approach is language independent
and requires only a dictionary and text data
for building a language model. The ap-
proach have been tested on both Arabic and
English text and achieved accuracy of 89%.
1 Introduction
The problem of detecting and correcting misspelled
words in text has received great attention due to
its importance in several applications like text edit-
ing systems, optical character recognition systems,
and morphological analysis and tagging (Roche and
Schabes, 1995). Other applications, like machine
translation and information extraction, operate on
text that might have spelling errors. The automatic
detection, and correction of spelling erros should be
of great help to those applications.
The problem of detecting and correcting mis-
spelled words in text is usually solved by checking
whether a word already exists in the dictionary or
not. If not, we try to extract words from the dictio-
nary that are most similar to the word in question.
?Now with the University of Michigan Ann Arbor, has-
sanam@umich.edu
Those words are reported as candidate corrections
for the misspelled word.
Similarity between the misspelled word and dic-
tionary words is measured by the Levenshtein edit
distance (Levenshtein, 1966; Wagner and M.Fisher,
1974). The Levenshtein edit distance is usu-
ally calculated using a dynamic programming tech-
nique with quadratic time complexity (Wagner and
M.Fisher, 1974). Hence, it is not reasonable to com-
pare the misspelled word to each word in the dictio-
nary while trying to find candidate corrections.
The proposed approach uses techniques from fi-
nite state theory to detect misspelled words and to
generate a set of candidate corrections for each mis-
spelled word. It also uses a language model to select
the best correction from the set of candidate correc-
tions using the context of the misspelled word. Us-
ing techniques from finite state theory, and avoiding
calculating edit distances makes the approach very
fast and efficient. The approach is completely lan-
guage independent, and can be used with any lan-
guage that has a dictionary and text data to building
a language model.
The rest of this paper will proceed as follows.
Section 2 will present an overview of related work.
Section 3 will discuss the different aspects of the
proposed approach. Section 4 presents a perfor-
mance evaluation of the system. Finally a conclu-
sion is presented in section 5.
2 Related Work
Several solutions were suggested to avoid comput-
ing the Levenshtein edit distance while finding can-
didate corrections. Most of those solutions select
a number of dictionary words that are supposed to
contain the correction, and then measure the dis-
tance between the misspelled word and all selected
words. The most popular of those methods are the
similarity keys methods (Kukich, 1992; Zobel and
Dart, 1995; De Beuvron and Trigano, 1995). In
913
those methods, the dictionary words are divided into
classes according to some word features. The input
word is compared to words in classes that have sim-
ilar features only.
In addition to the techniques discussed above,
other techniques from finite state automata have
been recently proposed. (Oflazer, 1996) suggested
a method where all words in a dictionary are treated
as a regular language over an alphabet of letters. All
the words are represented by a finite state machine
automaton. For each garbled input word, an exhaus-
tive traversal of the dictionary automaton is initiated
using a variant of Wagner-Fisher algorithm (Wag-
ner and M.Fisher, 1974) to control the traversal of
the dictionary. In this approach Levenshtein dis-
tance is calculated several times during the traversal.
The method carefully traverses the dictionary such
that the inspection of most of the dictionary states
is avoided. (Schulz and Mihov, 2002) presents a
variant of Oflazers?s approach where the dictionary
is also represented as deterministic finite state au-
tomaton. However, they avoid the computation of
Levenshtein distance during the traversal of the dic-
tionary automaton. In this technique, a finite state
acceptor is constructed for each input word. This
acceptor accepts all words that are within an edit dis-
tance k from the input word. The dictionary automa-
ton and the Levenshtein-automaton are then tra-
versed in parallel to extract candidate corrections for
the misspelled word. The authors present an algo-
rithm that can construct a deterministic Levenshtein-
automaton for an arbitrary word of degrees 1, and
2 which corresponds to 1 or 2 errors only. They
suggest another algorithm that can construct a non-
deterministic Levenshtein-automaton for any other
degree. They report results using a Levenshtein-
automaton of degree 1(i.e. words having a single
insertion, substitution, or deletion) only.
The method we propose in this work also assumes
that the dictionary is represented as a determinis-
tic finite state automaton. However, we completely
avoid computing the Levenshtein-distance at any
step. We also avoid reconstructing a Levenshtein-
automaton for each input word. The proposed
method does not impose any constraints on the
bound k, where k is the edit distance between the
input word and the candidate corrections. The ap-
proach can adopt several constraints on which char-
Figure 1: An FSM representation of a word list
acters can substitute certain other characters. Those
constraints are obtained from a phonetic and spatial
confusion matrix of characters.
The purpose of context-dependent error correc-
tion is to rank a set of candidate corrections tak-
ing the misspelled word context into account. A
number of approaches have been proposed to tackle
this problem that use insights from statistical ma-
chine learning (Golding and Roth, 1999), lexical
semantics (Hirst and Budanitsky, 2005), and web
crawls (Ringlstetter et al, 2007).
3 Error Detection and Correction in Text
Using FSMs
The approach consists of three main phases: detect-
ing misspelled words, generating candidate correc-
tions for them, and ranking corrections. A detailed
description of each phase is given in the following
subsections.
3.1 Detecting Misspelled Words
The most direct way for detecting misspelled words
is to search the dictionary for each word, and report
words not found in the dictionary. However, we can
make use of the finite state automaton representation
of the dictionary to make this step more efficient.
In the proposed method, we build a finite state ma-
chine (FSM) that contains a path for each word in
the input string. This FSM is then composed with
the dictionary FSM. The result of the composition
is merely the intersection of the words that exist in
both the input string and the dictionary. If we calcu-
lated the difference between the FSM containing all
words and this FSM, we get an FSM with a path for
914
each misspelled word. Figure 1 illustrate an FSM
that contain all words in an input string.
3.2 Generating Candidate Corrections
The task of generating candidate corrections for mis-
spelled words can be divided into two sub tasks:
Generating a list of words that have edit distance
less than or equal k to the input word, and select-
ing a subset of those words that also exist in the dic-
tionary. To accomplish those tasks, we create a sin-
gle transducer(Levenshtein-transducer) that is when
composed with an FSM representing a word, gen-
erates all words withing any edit distance k from
the input word. After composing the misspelled
word with Levenshtein-transducer, we compose the
resulting FSM with the dictionary FSM to filter out
words that do not exist in the dictionary.
3.2.1 Levenshtein-transducers for primitive
edit distances
To generate a finite state automaton that con-
tain all words within some edit distance to the in-
put word, we use a finite state transducer that al-
lows editing its input according to the standard
Levenshtein-distance primitive operations: substitu-
tion, deletion, and insertion.
A finite-state transducers (FST) is a a 6-tuple
(Q,?1,?2, ?, i, F ), where Q is a set of states, ?1
is the input alphabet, ?2 is the output alphabet, i is
the initial state, F ? Q is a set of final states, and ?
is a transition function (Hopcroft and Ullman, 1979;
Roche and Shabes, 1997). A finite state acceptor is a
special case of an FST that has the same input/output
at each arc.
Figure 2 illustrates the Levenshtein-transducer for
edit distance 1 over a limited set of vocabulary (a,b,
and c). We can notice that we will stay in state zero
as long as the output is identical to the input. On
the other hand we can move from state zero, which
corresponds to edit distance zero, to state one, which
corresponds to edit distance one, with three different
ways:
? input is mapped to a different output (input is
consumed and a different symbol is emitted)
which corresponds to a substitution,
? input is mapped to an epsilon (input is con-
sumed and no output emitted) which corre-
sponds to a deletion, and
Figure 2: A Levenshtein-transducer (edit distance 1)
? an epsilon is mapped to an output (output is
emitted without consuming any input) which
corresponds to an insertion.
Once we reach state 1, the only possible transitions
are those that consume a symbol and emit the same
symbol again and hence allowing only one edit op-
eration to take place.
When we receive a new misspelled word, we rep-
resent it with a finite state acceptor that has a single
path representing the word, and then compose it with
the Levenshtein-transducer. The result of the com-
position is a new FSM that contains all words with
edit distance 1 to the input word.
3.2.2 Adding transposition
Another non-primitive edit distance operation that
is frequently seen in misspelled words is transposi-
tion. Transposition is the operation of exchanging
the order of two consecutive symbols (ab ? ba).
Transposition is not a primitive operation because
it can be represented by other primitive operations.
However, this makes it a second degree operation.
As transposition occurs frequently in misspelled
words, adding it to the Levenshtein-transducer as a
single editing operation would be of great help.
915
Figure 3: A Levenshtein-transducer for edit distance
1 with transposition
To add transposition, as a single editing opera-
tion, to the Levenshtein-transducer we add arcs be-
tween states zero and one that can map any symbol
sequence xy to the symbol sequence yx, where x,
and y are any two symbols in the vocabulary. Fig-
ure 3 shows the Levenshtein-transducer with degree
1 with transposition over a limited vocabulary (a and
b).
3.2.3 Adding symbol confusion matrices
Adding a symbol confusion matrix can help re-
duce the number of candidate corrections. The con-
fusion matrix determines for each symbol a set of
symbols that may have substituted it in the garbled
word. This matrix can be used to reduce the num-
ber of candidate corrections if incorporated into the
Levenshtein-transducer. For any symbol x, we add
an arc x : y between states zero, and one in the trans-
ducer where y ? Confusion Matrix(x) rather
than for all symbols y in the vocabulary.
The confusion matrix can help adopt the meth-
ods to different applications. For example, we can
build a confusion matrix for use with optical char-
acter recognition error correction that captures er-
rors that usually occur with OCRs. When used with
a text editing system, we can use a confusion ma-
trix that predicts the confused characters according
to their phonetic similarity, and their spatial location
on the keyboard.
Figure 4: A Levenshtein-transducer for edit distance
2 with transposition
3.2.4 Using degrees greater than one
To create a Levenshtein-transducer that can gen-
erate all words within edit distance two of the input
word, we create a new state (2) that maps to two edit
operations, and repeat all arcs that moves from state
0 to state 1 to move from state 1 to state 2.
To allow the Levenshtein-transducer of degree
two to produce words with edit distance 1 and 2 from
the input word, we mark both state 1, and 2 as final
states. We may also favor corrections with lower
edit distances by assigning costs to final states, such
that final states with lower number of edit operations
get lower costs. A Levenshtein-transducer of degree
2 for the limited vocabulary (a and b) is shown in
figure 4.
3.3 Ranking Corrections
To select the best correction from a set of candidate
corrections, we use a language model to assign a
probability to a sequence of words containing the
corrected word. To get that word sequence, we go
back to the context where the misspelled word ap-
peared, replace the misspelled word with the candi-
date correction, and extract n ngrams containing the
candidate correction word in all possible positions
in the ngram. We then assign a score to each ngram
using the language model, and assign a score to the
candidate correction that equals the average score of
all ngrams. Before selecting the best scoring cor-
rection, we penalize corrections that resulted from
higher edit operations to favor corrections with the
minimal number of editing operations.
916
Edit 1/with trans. Edit 1/no trans. Edit 2/with trans. Edit 2 / no trans.
word len. av. time av. correcs. av. time av. correcs. av. time av. correcs. av. time av. correcs.
3 3.373273 18.769 2.983733 18.197 73.143538 532.637 69.709387 514.174
4 3.280419 4.797 2.796275 4.715 67.864291 136.230 66.279842 131.680
5 3.321769 1.858 2.637421 1.838 73.718353 33.434 68.695935 32.461
6 3.590046 1.283 2.877242 1.277 75.465624 11.489 69.246055 11.258
7 3.817453 1.139 2.785156 1.139 78.231015 6.373 72.2057 6.277
8 4.073228 1.063 5.593761 1.062 77.096026 4.127 73.361455 4.066
9 4.321661 1.036 3.124661 1.036 76.991945 3.122 73.058418 3.091
10 4.739503 1.020 3.2084 1.020 75.427416 2.706 72.2143 2.685
11 4.892105 1.007 3.405101 1.007 77.045616 2.287 71.293116 2.281
12 5.052191 0.993 3.505089 0.993 78.616536 1.910 75.709801 1.904
13 5.403557 0.936 3.568391 0.936 81.145124 1.575 78.732955 1.568
Table 1: Results for English
Edit 1/with trans. Edit 1/no trans. Edit 2/with trans. Edit 2 / no trans.
word len. av. time av. correcs. av. time av. correcs. av. time av. correcs. av. time av. correcs.
3 5.710543 31.702 4.308018 30.697 83.971263 891.579 75.539547 862.495
4 6.033066 12.555 4.036479 12.196 80.481281 308.910 71.042372 296.776
5 7.060306 6.265 4.360373 6.162 79.320644 104.661 69.71572 100.428
6 9.08935 4.427 4.843784 4.359 79.878962 51.392 74.197127 48.991
7 8.469497 3.348 5.419919 3.329 82.231107 24.663 70.681298 23.781
8 10.078842 2.503 5.593761 2.492 85.32005 13.586 71.557569 13.267
9 10.127946 2.140 6.027077 2.136 83.788916 8.733 76.199034 8.645
10 11.04873 1.653 6.259901 1.653 92.671732 6.142 81.007893 6.089
11 12.060286 1.130 7.327353 1.129 94.726469 4.103 77.464609 4.084
12 13.093397 0.968 7.194902 0.967 95.35985 2.481 82.40306 2.462
13 13.925067 0.924 7.740105 0.921 106.66238 1.123 78.966914 1.109
Table 2: Results for Arabic
4 Experimental Setup
4.1 Time Performance
The proposed method was implemented in C++ on
a 2GHz processor machine under Linux. We used
11,000 words of length 3,4,..., and 13, 1,000 word
for each word length, that have a single error and
computed correction candidates. We report both the
average correction time, and the average number of
corrections for each word length. The experiment
was run twice on different test data, one with con-
sidering transposition as primitive operation, and the
other without. We also repeated the experiments for
edit distance 2 errors, and also considered the two
cases where transposition is considered as a primi-
tive operation or not. Table 1 shows the results for
an English dictionary of size 225,400 entry, and Ta-
ble 2 shows the results for an Arabic dictionary that
has 526,492. entries.
4.2 Auto-correction accuracy
To measure the accuracy of the auto-correction pro-
cess, we used a list of 556 words having common
spelling errors of both edit distances 1 and 2. We put
a threshold on the number of characters per word to
decide whether it will be considered for edit distance
1 or 2 errors. When using a threshold of 7, the spell
engine managed to correct 87% of the words. This
percentage raised to 89% when all words were con-
sidered for edit distance 2 errors. The small degra-
dation in the performance occured because in 2% of
the cases, the words were checked for edit distance 1
errors although they had edit distance 2 errors. Fig-
ure 6 shows the effect of varying the characters limit
on the correction accuracy.
Figure 5 shows the effect of varying the weight as-
signed to corrections with lower edit distances on the
accuracy. As indicated in the figure, when we only
consider the language model weight, we get accura-
cies as low as 79%. As we favor corrections with
lower edit distances the correction accuracy raises,
but occasionally starts to decay again when empha-
sis on the low edit distance is much larger than that
on the language model weights.
Finally, we repeated the experiments but with us-
917
Figure 5: Effect of increasing lower edit distance
favoring factor on accuracy
Figure 6: Effect of increasing Ed1/Ed2 char limits
on accuracy
ing a confusion matrix, as 3.2.3. We found out that
the average computation time dropped by 78% ( be-
low 1 ms for edit distance 1 errors) at the price of
losing only 8% of the correction accuracy.
5 Conclusion
In this work, we present a finite state automata based
spelling errors detection and correction method.
The new method avoids calculating the edit dis-
tances at all steps of the correction process. It also
avoids building a Levenshtein-automata for each in-
put word. The method is multilingual and may work
for any language for which we have an electronic
dictionary, and a language model to assign probabil-
ity to word sequences. The preliminary experimen-
tal results show that the new method achieves good
performance for both correction time and accuracy.
The experiments done in this paper can be extended
in several directions. First, there is still much room
for optimizing the code to make it faster especially
the FST composition process. Second, we can allow
further editing operations like splitting and merging.
References
Francois De Bertrand De Beuvron and Philippe Trigano.
1995. Hierarchically coded lexicon with variants. In-
ternational Journal of Pattern Recognition and Artifi-
cial Intelligence, 9:145?165.
Andrew Golding and Dan Roth. 1999. A winnow-based
approach to context-sensitive spelling correction. Ma-
chine learning, 34:107?130.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexical
cohesion. Natural Language Engineering, 11:87?111.
J.E. Hopcroft and J.D. Ullman. 1979. Introduction to au-
tomata theory, languages, and computation. Reading,
Massachusetts: Addison-Wesley.
Karen Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys, pages
377?439.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
- Doklady.
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22:73?89.
Christoph Ringlstetter, Max Hadersbeck, Klaus U.
Schulz, and Stoyan Mihov. 2007. Text correction
using domain dependent bigram models from web
crawls. In Proceedings of the International Joint Con-
ference on Artificial Intelligence (IJCAI-2007) Work-
shop on Analytics for Noisy Unstructured Text Data.
Emmanuel Roche and Yves Schabes. 1995. Determinis-
tic part-of-speech tagging with finite-state transducers.
Computational Linguistics, 2:227253.
Emmanuel Roche and Yves Shabes. 1997. Finite-state
language processing. Cambridge, MA, USA: MIT
Press.
Klaus Schulz and Stoyan Mihov. 2002. Fast string cor-
rection with levenshtein-automata. International Jour-
nal of Document Analysis and Recognition (IJDAR),
5:67?85.
R.A. Wagner and M.Fisher. 1974. The string-to-string
correction problem. Journal of the ACM.
Justin Zobel and Philip Dart. 1995. Finding approximate
matches in large lexicons. Software Practice and Ex-
perience, 25:331?345.
918
Workshop on TextGraphs, at HLT-NAACL 2006, pages 9?16,
New York City, June 2006. c?2006 Association for Computational Linguistics
Graph Based Semi-Supervised Approach for Information Extraction 
 
 
Hany Hassan Ahmed Hassan Sara Noeman 
 
IBM Cairo Technology Development Center 
Giza, Egypt 
                                                              P.O. Box 166 Al-Ahram 
 
hanyh@eg.ibm.com hasanah@eg.ibm.com noemans@eg.ibm.com 
 
 
 
 
Abstract 
Classification techniques deploy supervised 
labeled instances to train classifiers for 
various classification problems. However 
labeled instances are limited, expensive, 
and time consuming to obtain, due to the 
need of experienced human annotators.  
Meanwhile large amount of unlabeled data 
is usually easy to obtain. Semi-supervised 
learning addresses the problem of utilizing 
unlabeled data along with supervised la-
beled data, to build better classifiers.  In 
this paper we introduce a semi-supervised 
approach based on mutual reinforcement in 
graphs to obtain more labeled data to en-
hance the classifier accuracy. The approach 
has been used to supplement a maximum 
entropy model for semi-supervised training 
of the ACE Relation Detection and Charac-
terization (RDC) task. ACE RDC is con-
sidered a hard task in information 
extraction due to lack of large amounts of 
training data and inconsistencies in the 
available data. The proposed approach pro-
vides 10% relative improvement over the 
state of the art supervised baseline system. 
1 Introduction 
Classification techniques use labeled data to train 
classifiers for various classification problems.  Yet 
they often face a shortage of labeled training data. 
Labeled instances are often difficult, expensive, 
and /or time consuming to obtain. Meanwhile large 
numbers of unlabeled instances are often available. 
Semi-supervised learning addresses the problem of 
how unlabeled data can be usefully employed, 
along with labeled data, to build better classifiers. 
In this paper we propose a semi-supervised ap-
proach for acquiring more training instances simi-
lar to some labeled instances. The approach 
depends on constructing generalized extraction 
patterns, which could match many instances, and 
deploying graph based mutual reinforcement to 
weight the importance of these patterns.  The mu-
tual reinforcement is used to automatically identify 
the most informative patterns; where patterns that 
match many instances tend to be correct. Similarly, 
instances matched by many patterns also tend to be 
correct. The labeled instances should have more 
effect in the mutual reinforcement weighting proc-
ess. The problem can therefore be seen as hubs 
(instances) and authorities (patterns) problem 
which can be solved using the Hypertext Induced 
Topic Selection (HITS) algorithm (Kleinberg, 
1998 ). 
HITS is an algorithmic formulation of the notion 
of authority in web pages link analysis, based on a 
relationship between a set of relevant ?authorita-
tive pages? and a set of ?hub pages?. The HITS 
algorithm benefits from the following observation:  
when a page (hub) links to another page (author-
ity), the former confers authority over the latter.  
By analogy to the authoritative web pages prob-
lem, we could represent the patterns as authorities 
and instances as hubs, and use mutual reinforce-
ment between patterns and instances to weight the 
most authoritative patterns. Instances from unsu-
9
pervised data matched with the highly weighted 
patterns are then used in retraining the system.  
The paper proceeds as follows: in Section 2 we 
discuss previous work followed by a brief defini-
tion of our general notation in Section 3. A detailed 
description of the proposed approach then follows 
in Section 4. Section 5 discusses the application of 
the proposed approach to the problem of detecting 
semantic relations from text. Section 6 discusses 
experimental results while the conclusion is pre-
sented in Section 7. 
2 Previous Work 
(Blum and Mitchell, 1998) proposed an approach 
based on co-training that uses unlabeled data in a 
particular setting. They exploit the fact that, for 
some problems, each example can be described by 
multiple representations. They develop a boosting 
scheme which exploits conditional independence 
between these representations.  
(Blum and Chawla, 2001) proposed  a general 
approach utilizing unlabeled data by constructing a 
graph on all the data points based on distance rela-
tionships among examples, and then to use the 
known labels to perform a graph partitioning using  
the minimum cut that agrees with the labeled data. 
(Zhu et al, 2003) extended this approach by pro-
posing a  cut based on the assumption that labels 
are generated according to a Markov Random 
Field on the graph , (Joachims, 2003) presented  an 
algorithm based on spectral graph partitioning. 
(Blum et al, 2004) extended the min-cut  approach 
by adding randomness to the graph structure, their 
algorithm addresses several shortcomings of the 
basic mincut approach, yet it may not help in cases 
where the graph does not have small cuts for a 
given classification problem. 
3 Background  
In graph theory, a graph is a set of objects called 
vertices joined by links called edges. A bipartite 
graph, also called a bigraph, is a special graph 
where the set of vertices can be divided into two 
disjoint sets with no two vertices of the same set 
sharing an edge.  
The Hypertext Induced Topic Selection (HITS) 
algorithm is an algorithm for rating, and therefore 
ranking, web pages. The HITS algorithm makes 
use of the following observation: when a page 
(hub) links to another page (authority), the former 
confers authority over the latter. HITS uses two 
values for each page, the "authority value" and the 
"hub value". "Authority value" and "hub value" are 
defined in terms of one another in a mutual recur-
sion. An authority value is computed as the sum of 
the scaled hub values that point to that authority. A 
hub value is the sum of the scaled authority values 
of the authorities it points to. 
A template, as we define for this work, is a se-
quence of generic forms that could generalize over 
the given training instance. An example template 
is:  
COUNTRY  NOUN_PHRASE PERSON 
VERB_PHRASE  
This template could represent the sentence: 
?American vice President Al Gore visited ...?.  
This template is derived from the representation of 
the Named Entity tags, Part-of-Speech (POS) tags 
and semantic tags. The choice of the template rep-
resentation here is for illustration purpose only; 
any combination of tags, representations and tag-
ging styles might be used.  
A pattern is more specific than a template. A 
pattern specifies the role played by the tags (first 
entity, second entity, or relation). An example of a 
pattern is: 
COUNTRY(E2) NOUN_PHRASE(R) PERSON(E1)   
VERB_PHRASE  
This pattern indicates that the word(s) with the 
tag COUNTRY in the sentence represents the sec-
ond entity (Entity 2) in the relation, while the 
word(s) tagged PERSON represents the first entity 
(Entity 1) in this relation. Finally, the word(s) with 
the tag NOUN_PHRASE represents the relation 
between the two previous entities.   
A tuple, in our notation during this paper, is the 
result of the application of a pattern to unstructured 
text. In the above example, one result of applying 
the pattern to some raw text is the following tuple: 
Entity 1:  Al Gore 
Entity 2: United States 
Relation: vice President 
4 The Approach 
The semi-supervised graph-based approach we 
propose depends on the construction of generalized 
extraction patterns that could match many training 
instances. The patterns are then weighted accord-
ing to their importance by deploying graph based 
10
mutual reinforcement techniques. Patterns derived 
from the supervised training instances should have 
a superior effect in the reinforcement weighting 
process. This duality in patterns and tuples relation 
could be stated that patterns could match different 
tuples, and tuples in turn could be matched by dif-
ferent patterns. The proposed approach is com-
posed of two main steps namely, pattern extraction 
and pattern weighting or induction. Both steps are 
detailed in the next subsections. 
4.1 Patterns Extraction 
As shown in Figure 1, several syntactic, lexical, 
and semantic analyzers could be applied to the 
training instances. The resulting analyses could be 
employed in the construction of extraction pat-
terns. Any extraction pattern could match different 
relations and hence could produce several tuples. 
As an example let?s consider the pattern depicted 
in figure 1: 
 
 
Figure 1:  An example of a pattern and its possible 
tuples. 
 
PEOPLE_Inhabitant(E2) NOUN_PHRASE(R) 
PERSON(E1) VERB_PHRASE  
This pattern could extract the tuple: 
Entity 1: Al Gore 
Entity 2: American  
Relation: vice President 
Another tuple that could be extracted by the same 
pattern is:  
Entity 1: Berlusconi 
Entity 2: Italian  
Relation: Prime Minister 
On the other hand, many other patterns could ex-
tract the same information in the tuple from differ-
ent contexts. It is worth mentioning that the 
proposed approach is general enough to accommo-
date any pattern design; the introduced pattern de-
sign is for illustration purposes only. 
To further increase the number of patterns that 
could match a single tuple, the tuple space might 
be reduced i.e. by grouping tuples conveying the 
same information content together into a single 
tuple. This will be detailed further in the experi-
mental setup section. 
4.2   Pattern Induction 
The inherent duality in the patterns and tuples rela-
tion suggests that the problem could be interpreted 
as a hub authority problem. This problem could be 
solved by applying the HITS algorithm to itera-
tively assign authority and hub scores to patterns 
and tuples respectively. 
 
Figure 2: A bipartite graph representing patterns 
and tuples 
 
Patterns and tuples are represented by a bipartite 
graph as illustrated in figure 2. Each pattern or tu-
ple is represented by a node in the graph. Edges 
represent matching between patterns and tuples.  
The pattern induction problem can be formu-
lated as follows: Given a very large set of data D 
containing a large set of patterns P which match a 
P
P
P
P
P
T
T
T
T
T
P
P
T
T
Patterns Tuples
American vice President   Al Gore said today... 
Word: American 
Entity: PEOPLE 
POS : ADJ 
Sem: Inhabitant 
Word: vice president 
Entity:  
POS: NOUN_PHRASE 
Sem:  
Word: Al Gore 
Entity: PERSON 
POS: 
Sem: 
PEOPLE_Inhabitant    NOUN_PHRASE        PERSON 
VERB_PHRASE 
 
Entity 1:  Al Gore 
Entity 2: American 
Relation: vice President 
American vice Presi-
dent   Al Gore said 
today? 
Italian Prime Minister 
Berlusconi  visited?.. 
Entity 1: Berlusconi  
Entity 2: Italian 
Relation: prime minister 
11
large set of tuples T, the problem is to identify P
~
, 
the set of patterns that match the set of the most 
correct tuplesT
~
. The intuition is that the tuples 
matched by many different patterns tend to be cor-
rect and the patterns matching many different tu-
ples tend to be good patterns. In other words; we 
want to choose, among the large space of patterns 
in the data, the most informative, highest confi-
dence patterns that could identify correct tuples; 
i.e. choosing the most ?authoritative? patterns in 
analogy with the hub authority problem. However, 
both P
~
andT
~
are unknown. The induction process 
proceeds as follows:  each pattern p in P is associ-
ated with a numerical authority weight av which 
expresses how many tuples match that pattern. 
Similarly, each tuple t in T has a numerical hub 
weight ht which expresses how many patterns were 
matched by this tuple. The weights are calculated 
iteratively as follows: 
( ) ( )( )
=
+
=
pT
u i
i
i
H
uhpa
1 )(
)(
)1(
 (1) 
( ) ( )( )
=
+
=
tP
u i
i
i
A
ua
th
1 )(
)(
)1(
 (2) 
where T(p) is the set of tuples matched by p, P(t) is 
the set of patterns matching t, ( )pa i )1( +  is the au-
thoritative weight of pattern p  at iteration  )1( +i , 
and ( )th i )1( +  is the hub weight of tuple t  at itera-
tion  )1( +i  . H(i) and A(i) are normalization fac-
tors defined as: 
 
( )( ) 
= =
=
||
1 1
)()( P
p
pT
u
ii uhH  (3) 
( )( ) 
= =
=
||
1 1
)()( T
v
tP
u
ii uaA
 (4) 
Patterns with weights lower than a predefined 
threshold are rejected, and examples associated 
with highly ranked patterns are then used in unsu-
pervised training. 
It is worth mentioning that both T and P contain 
supervised and unsupervised examples, however 
the proposed method could assign weights to the 
correct examples (tuples and patterns) in a com-
pletely unsupervised setup. For semi-supervised 
data some supervised examples are provided, 
which are associated in turn with tuples and pat-
terns.  
We adopt the HITS extension introduced in 
(White and Smyth, 2003) to extend HITS with Pri-
ors. By analogy, we handle the supervised exam-
ples as priors to the HITS induction algorithm.  
A prior probabilities vector pr ={pr1, . . . , prn}  
is defined such that the probabilities sum to 1,  
where prv denotes the relative importance (or 
?prior bias?) we attach to node v. A pattern Pi is 
assigned a prior pri=1/n if pattern Pi matches a 
supervised tuple, otherwise pri is set to zero, n is 
the total number of patterns that have a supervised 
match. We also define a ?back probability? 
 
, 0   
 
   1 which determines how often we bias the su-
pervised nodes: 
( ) ( ) ( )( ) ppTu i
i
i pr
H
uhpa *1
1 )(
)(
)1( ?? +



?= 
=
+
 (5) 
( ) ( ) ( )( ) ttPu i
i
i pr
A
ua
th *1
1 )(
)(
)1( ?? +



?= 
=
+
  (6) 
where T(p) is the set of tuples matched by p , P(t) 
is the set of patterns matching t, and H(i) and A(i) 
are normalization factors defined as in  equations 
(3) and (4) 
 
Thus each node in the graph (pattern or tuple) has 
an associated prior weight depending on its super-
vised data. The induction process proceeds to itera-
tively assign weights to the patterns and tuples. In 
the current work we used 5.0=? . 
5 Experimental Setup  
5.1 ACE Relation Detection and Characteri-
zation 
In this section, we describe Automatic Content 
Extraction (ACE). ACE is an evaluation conducted 
by NIST to measure Entity Detection and Tracking 
(EDT) and Relation Detection and Characteriza-
tion (RDC). The EDT task is concerned with the 
detection of mentions of entities, and grouping 
them together by identifying their coreference. The 
RDC task detects relations between entities identi-
fied by the EDT task. We choose the RDC task to 
show the performance of the graph based semi-
supervised information extraction approach we 
propose. To this end we need to introduce the no-
tion of mentions and entities. Mentions are any 
instances of textual references to objects like peo-
12
ple, organizations, geo-political entities (countries, 
cities ?etc), locations, or facilities. On the other 
hand, entities are objects containing all mentions to 
the same object. 
 
Type Subtype Number of Instances 
User-Owner 
Inventor ART 
Other 
331 
DISC DISC 143 
Employ-Exec 
Employ-Staff 
Employ-Undetermined 
Member-of-Group 
Subsidiary 
EMP-ORG 
Other 
1673 
Ethnic 
Ideology Other-AFF 
Other 
153 
Citizen-Resident 
Based-in GPE-AFF 
Other 
695 
Business 
Family PER-SOC 
Other 
358 
Located 
Near PHYS 
Part-Whole 
1411 
 
Table 1. Types and subtypes of ACE relations 
 
Table 1 lists the types and subtypes of relations 
for the ACE RDC task. Here, we present an exam-
ple for those relations: 
 
Spain?s Interior Minister an-
nounced this evening the ar-
rest of separatist 
organization Eta?s presumed 
leader Ignacio Garcia Ar-
regui. Arregui, who is con-
sidered to be the Eta 
organization?s top man, was 
arrested at 17h45 Greenwich. 
The Spanish judiciary sus-
pects Arregui of ordering a 
failed attack on King Juan 
Carlos in 1995. 
In this fragment, all the underlined phrases are 
mentions to Eta organization, or to ?Garcia Ar-
regui?. There is a management relation between 
leader which references to ?Garcia Arregui? and 
Eta. 
5.2 Baseline System 
The base line system uses a Maximum Entropy 
model that combines diverse lexical, syntactic and 
semantic features derived from text, like the sys-
tem described in (Nanda, 2004). The system was 
trained on the ACE training data provided by LDC. 
The training set contained 145K words, and 4764 
instances of relations, the number of instances cor-
responding to each relation is shown in Table 1. 
The test set contained around 41K words, and 
1097 instances of relations. The system was evalu-
ated using standard ACE evaluation procedure. 
ACE evaluation procedure assigns the system an 
ACE value for each relation type and a total ACE 
value. The ACE value is a standard NIST metric 
for evaluating relation extraction. The reader is 
referred to the ACE web site (ACE, 2004) for more 
details.  
5.3 Pattern Construction 
We used the baseline system described in the pre-
vious section to label a large amount of unsuper-
vised data. The data comes from LDC English 
Gigaword corpus, Agence France Press English 
Service (AFE). The data contains around 3M 
words, from which 80K instances of relations have 
been extracted. 
We start by extracting a set of patterns that rep-
resent the supervised and unsupervised data. We 
consider each relation type separately and extract a 
pattern for each instance in the selected relation. 
The pattern we used consists of a mix between the 
part of speech (POS) tags and the mention tags for 
the words in the training instance. We use the men-
tion tag, if it exists; otherwise we use the part of 
speech tag. An example of a pattern is: 
 
Text: Eta?s presumed leader 
Arregui ? 
Pos: NNP POS JJ NN NNP 
Mention: ORG 0 0 0 PERSON 
Pattern: ORG(E2) POS JJ NN(R) 
PERSON(E1) 
13
5.4 Tuples Clustering 
As discussed in the previous section, the tuple 
space should be reduced to allow more matching 
between pattern-tuple pairs. This space reduction 
could be accomplished by seeking a tuple similar-
ity measure, and constructing a weighted undi-
rected graph of tuples. Two tuples are linked with 
an edge if their similarity measure exceeds a cer-
tain threshold. Graph clustering algorithms could 
be deployed to partition the graph into a set of ho-
mogeneous communities or clusters. To reduce the 
space of tuples, we seek a matching criterion that 
group similar tuples together. Using WordNet, we 
can measure the semantic similarity or relatedness 
between a pair of concepts (or word senses), and 
by extension, between a pair of sentences. We use 
the similarity measure described in (Wu and 
Palmer, 1994) which finds the path length to the 
root  node from the least common subsumer (LCS) 
of the two word senses which is the most specific 
word sense they share as an ancestor. The similar-
ity score of two tuples, ST, is calculated as follows:. 
2
2
2
1 EET SSS +=   (9) 
where SE1, and SE2 are the similarity scores of the 
first entities in the two tuples, and their second en-
titles respectively. 
The tuple matching procedure assigns a similarity 
measure to each pair of tuples in the dataset. Using 
this measure we can construct an undirected graph 
G. The vertices of G are the tuples. Two vertices 
are connected with an edge if the similarity meas-
ure between their underlying tuples exceeds a cer-
tain threshold. It was noticed that the constructed 
graph consists of a set of semi isolated groups as 
shown in figure 3. Those groups have a very large 
number of inter-group edges and meanwhile a 
rather small number of intra-group edges. This im-
plies that using a graph clustering algorithm would 
eliminate those weak intra-group edges and pro-
duce separate groups or clusters representing simi-
lar tuples. We used Markov Cluster Algorithm 
(MCL) for graph clustering (Dongen, 2000). MCL 
is a fast and scalable unsupervised cluster algo-
rithm for graphs based on simulation of stochastic 
flow. 
A bipartite graph of patterns and tuple clusters is 
constructed. Weights are assigned to patterns and 
tuple clusters by iteratively applying the HITS with 
Priors? algorithm. Instances associated with highly 
ranked patterns are then added to the training data 
and the model is retrained. Samples of some highly 
ranked patterns and corresponding matching text 
are introduced in Table 2. 
 
 
Figure 3: Applying Clustering Algorithms to Tuple 
graph  
 
Pattern Matches 
GPE PERSON 
PERSON PERSON 
Zimbabwean President 
Robert Mugabe 
GPE POS PERSON 
PERSON 
Zimbabwe 's President 
Robert Mugabe 
GPE JJ PERSON American diplomatic per-
sonnel 
PERSON IN JJ GPE candidates for local gov-
ernment 
ORGANIZATION 
PERSON Airways spokesman 
ORGANIZATION 
PERSON      Ajax players 
PERSON IN DT JJ  
ORGANIZATION  
chairman of the opposition 
parties 
ORGANIZATION 
PERSON    parties chairmans 
 
Table 2: Examples of patterns with high weights 
6 Results and Discussion 
We train several models like the one described in 
section 5.2 on different training data sets. In all 
experiments, we use both the LDC ACE training 
data and the labeled unsupervised data induced 
with the graph based approach we propose. We use 
the ACE evaluation procedure and ACE test cor-
pus, provided by LDC, to evaluate all models. 
We incrementally added labeled unsupervised 
data to the training data to determine the amount of 
data after which degradation in the system per-
formance occurs. We sought this degradation point 
separately for each relation type. Figure 4 shows 
the effect of adding labeled unsupervised data on 
T
T T
T
T
T
T
T
T
T
TT
T T
T
T T
T
T
T
T
T
T
T
T
T
T T
Before Clustering After Clustering
14
the ACE value for each relation separately. We 
notice from figure 4 and table 1 that relations with 
a small number of training instances had a higher 
gain in performance compared to relations with a 
large number of training instances. This implies 
that the proposed approach achieves significant 
improvement when the number of labeled training 
instances is small but representative. 
. 
0
10
20
30
40
50
60
0 50 100 200 300 400 500
Number of Added Documents
A
CE
 
Va
lu
e
EMP-ORG
PER-SOC
ART
PHYS
GPE-AFF
OTHER-
AFF
 
 
Figure 4: The effect of adding labeled unsuper-
vised data on the ACE value for each relation. The 
average number of relations per document is 4. 
 
From figure 4, we determined the number of 
training instances resulting in the maximum boost 
in performance for each relation. We added the 
training instances corresponding to the maximum 
boost in performance for all relations to the super-
vised training data and trained a new model on 
them. Figure 5 compares the ACE values for each 
relation in the base line model and the final model 
The total system ACE value has been improved 
by 10% over the supervised baseline system. All 
relation types, except the DSC relation, had sig-
nificant improvement ranging from 7% to 30% 
over the baseline supervised system. The DISC 
relation type had a small degradation; noting that it 
already has a low ACE value with the baseline sys-
tem. We think this is due to the fact that the DISC 
relation has few and inconsistent examples in the 
supervised data set. 
To assess the usefulness of the smoothing 
method employing WordNet distance, we repeated 
the experiment on EMP-ORG relation without it. 
We found out that it contributed to almost 30% of 
the total achieved improvement. We also repeated 
the experiment but with considering hub scores 
instead of authority scores. We added the examples 
associated with highly ranked tuples to the training 
set. We noticed that using hub scores yielded very 
little variation in the ACE value (i.e. 0.1 point for 
EMP-ORG relation). 
0
10
20
30
40
50
AC
E 
Va
lu
e
BaseLine 36.7 6 33.1 22.3 23.6 42.2 26.4 30.5
Final Model 39.6 4.2 35.8 24.7 30.8 46.6 28.2 33.5
ART DISC EMP-ORG
GPE-
AFF
OTHE
R-AFF
PER-
SOC PHYS TOTAL
 
 
Figure 5: A comparison of base line ACE values, 
and final ACE values for each relation. 
 
To evaluate the quality and representativeness of 
the labeled unsupervised data, acquired using the 
proposed approach, we study the effect of replac-
ing supervised data with unsupervised data while 
holding the amount of training data fixed. Several 
systems have been built using mixture of the su-
pervised and the unsupervised data. In Figure 6, 
the dotted line shows the degradation in the system 
performance when using a reduced amount of su-
pervised training data only, while the solid line 
shows the effect of replacing supervised training 
data with unsupervised labeled data on the system 
performance. We notice from Figure 6 that the un-
supervised data could replace more than 50% of 
the supervised data without any degradation in the 
system performance. This is an indication that the 
induced unsupervised data is good for training the 
classifier.  
 
26
27
28
29
30
31
32
33
34
0% 25% 50% 75%
100% 75% 50% 25%
Percentage of Unsupervised/Supervised 
Data
A
CE
 
Va
lu
e Sup + Unsup
Data
Sup Data Only
Unsupervised
Supervised
 
 
Figure 6: The effect of removing portions of the 
supervised data on the ACE value. And the effect 
15
of replacing portions of the supervised data with 
labeled training data. 
7 Conclusion 
We introduce a general framework for semi-
supervised learning based on mutual reinforcement 
in graphs. We construct generalized extraction pat-
terns and deploy graph based mutual reinforcement 
to automatically identify the most informative pat-
terns. We provide motivation for our approach 
from a graph theory and graph link analysis per-
spective. 
We present experimental results supporting the 
applicability of the proposed approach to ACE Re-
lation Detection and Characterization (RDC) task, 
demonstrating its applicability to hard information 
extraction problems. Our approach achieves a sig-
nificant improvement over the base line supervised 
system especially when the number of labeled in-
stances is small. 
8 Acknowledgements 
We would like to thank Nanda Kambhatla for pro-
viding the ACE baseline system. We would also 
like to thank Salim Roukos for several invaluable 
suggestions and guidance. Finally we would like to 
thank the anonymous reviewers for their construc-
tive criticism and helpful comments. 
References  
ACE. 2004. The NIST ACE evaluation website. 
http://www.nist.gov/speech/tests/ace/ 
Avrim Blum, and Tom Mitchell. 1998. Combining La-
beled and Unlabeled data with Co-training. Proceed-
ings of the 11th Annual Conference on 
Computational Learning Theory. 
 Avrim Blum and Shuchi Chawla. 2001. Learning From 
Labeled and Unlabeled Data Using Graph Mincuts. 
Proceedings of International Conference on Machine 
Learning (ICML). 
Avrim Blum, John Lafferty, Mugizi Rwebangira, and 
Rajashekar Reddy. 2004. Semi-supervised Learning 
Using Randomized Mincuts. Proceedings of the In-
ternational Conference on Machine Learning 
(ICML).  
Stijn van Dongen. 2000. A Cluster Algorithm for 
Graphs. Technical Report INS-R0010, National Re-
search Institute for Mathematics and Computer Sci-
ence in the Netherlands. 
Stijn van Dongen. 2000. Graph Clustering by Flow 
Simulation. PhD thesis, University of Utrecht 
Radu Florian, Hany Hassan, Hongyan Jing, Nanda 
Kambhatla, Xiaqiang Luo, Nicolas Nicolov, and 
Salim Roukos. 2004. A Statistical Model for multi-
lingual entity detection and tracking. Proceedings of 
the Human Language Technologies Conference 
(HLT-NAACL?04). 
Dayne Freitag, and Nicholas Kushmerick. 2000. 
Boosted wrapper induction. The 14th European Con-
ference on Artificial Intelligence Workshop on Ma-
chine Learning for Information Extraction 
Taher Haveliwala. 2002. Topic-sensitive PageRank. 
Proceedings of the 11th International World Wide 
Web Conference 
Thorsten Joachims. 2003. Transductive Learning via 
Spectral Graph Partitioning. Proceedings of the In-
ternational Conference on Machine Learning 
(ICML). 
John Kleinberg. 1998. Authoritative Sources in a Hy-
perlinked Environment. Proceedings of the. 9th 
ACM-SIAM Symposium on Discrete Algorithms. 
Nanda Kambhatla. 2004. Combining Lexical, Syntactic, 
and Semantic Features with Maximum Entropy Mod-
els for Information Extraction. Proceedings of the 
42nd Annual Meeting of the Association for Compu-
tational Linguistics 
Ted  Pedersen, Siddharth Patwardhan, and Jason Mich-
elizzi, 2004, WordNet::Similarity - Measuring the 
Relatedness of Concepts. Proceedings of Fifth An-
nual Meeting of the North American Chapter of the 
Association for Computational Linguistics (NAACL-
2004) 
Scott White, and Padhraic Smyth. 2003. Algorithms for 
Discoveing Relative Importance in Graphs. Proceed-
ings of Ninth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining. 
Zhibiao Wu, and Martha Palmer. 1994. Verb semantics 
and lexical selection. Proceedings of the 32nd An-
nual Meeting of the Association for Computational 
Linguistics. 
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 
2003. Semi-supervised Learning using Gaussian 
Fields and Harmonic Functions. Proceedings of the 
20th International Conference on Machine Learning. 
16
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 112?115,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Language Independent Transliteration system using phrase based 
SMT approach on substrings 
Sara Noeman
IBM Cairo Technology & Development 
Center
Giza, Egypt
noemans@eg.ibm.com
Abstract
Everyday the newswire introduce events from all over 
the world, highlighting new names of persons, loca-
tions and organizations with different origins. These 
names appear as Out of Vocabulary (OOV) words for 
Machine translation, cross lingual information retriev-
al, and many other NLP applications. One way to deal 
with OOV words is to transliterate the unknown 
words, that is, to render them in the orthography of 
the second language.
We introduce a statistical approach for transliteration 
only using the bilingual resources released in the 
shared task and without any previous knowledge of 
the target languages. Mapping the Transliteration 
problem to the Machine Translation problem, we 
make use of the phrase based SMT approach and ap-
ply it on substrings of names. In the English to Russi-
an task, we report ACC (Accuracy in top-1) of 0.545, 
Mean F-score of 0.917, and MRR (Mean Reciprocal  
Rank) of 0.596.
Due to time constraints, we made a single experiment 
in the English to Chinese task, reporting ACC, Mean 
F-score, and MRR of 0.411, 0.737, and 0.464 respect-
ively.
Finally, it is worth mentioning that the system is 
language independent since the author is not aware of 
either languages used in the experiments.
1. Introduction
Named entities translation is strongly required in the 
field of Information retrieval (IR) as well as its usage 
in Machine translation. A significant proportion of 
OOV words are named entities and typical analyses 
find around 50% of OOV words to be named entities, 
yet these can be the most important words in the quer-
ies. Larkey et al(2003) showed that average precision 
of cross language retrieval reduced more than 50% 
when named entities in the queries were not trans-
lated. 
Transliteration may be considered as a phonetic trans-
lation or mapping of a sequence of characters in the 
source language in the alphabet of the target language, 
thus we can use the analogy with the Machine transla-
tion problem, which translates a sequence of words in 
the source language into a semantically equivalent se-
quence of words in the target language.
In a statistical approach to machine translation, given 
a foreign word F, we try to find the English word ? 
that maximizes P(E\F). Using Bayes' rule, we can for-
mulate the task as follows:
This  is  known  as  the  noisy  channel  model,  which 
splits the problem into two sub-tasks. The translation 
model provides an estimate for the P(F\E) for the for-
eign word F being a translation for the English word 
E, while the language model provides an estimate of 
the probability P(E) is an English word.
In this paper we use the phrase based statistical Ma-
chine Translation (PBSMT) approach introduced by 
(Koehn et al) to build English to Russian, and Eng-
lish to Chinese transliteration systems capable of 
learning the substring to substring mapping between 
source and target languages. 
Section 2 includes a detailed description of our 
approach, section 3 describes our experimental set up 
and the results. The conclusions and future work are 
explained in section 4. 
2. System architecture
Our approach is a formulation of the Transliteration 
problem using the PBSMT technique that proved im-
provement in Machine translation domain, making 
use of the analogy between the two problems.
The phrase-based approach developed for statistical 
machine translation (Koehn et al, 2003) is designed 
to overcome the restrictions of many-to-many map-
pings in word-based translation models. We applied 
the phrase based statistical approach used in Machine 
translation on our problem, mapping the "word", and 
                        P(F\E)*P(E)
? =  argmax 
         E    P(F)
=  argmax  P(F\E)*P(E)
         E
112
"phrase" in PBSMT terminology into "character", and 
"substring" in our system, where the substring in our 
notation represents a sequence of adjacent characters. 
Figure (1) shows an overview of the whole system ar-
chitecture.
We used an HMM aligner similar to Giza++ (Och. et 
al., 1999) over the parallel character sequences using 
forward-backward alignment intersection. Heuristics 
were used to extend substring to substring mappings 
based on character-to-character alignment, with the 
constraint that no characters within the substring pair 
are linked to characters outside the substring pair. 
Thus we generated a substring to substring translation 
model with relative frequencies. We deploy heuristics 
to extract character sequence mapping similar to the 
heuristics used in PBSMT (Koehn et al, 2003). Fig-
ure (2) shows the heuristics used for block extraction 
over substrings in the English to Russian task using 
character to character alignments.
Figure (2)
Unlike the Machine Translation task, in transliteration 
we do not need any reordering during decoding which 
makes the decoding phase easier. We used monotone 
beam search decoder generating the best k 
transliteration candidates, where the translation model 
and the language model are used by the decoder to get 
best Viterbi paths of character sequences as a phonetic 
translation for the input English character sequence. 
(Tillmann, et al, 2003).
Finally, all transliteration candidates are weighted us-
ing their translation and language model probabilities 
as follows:
P( wr \ we) = P(we \ wr ) ? P(wr ? R)
Here, we explain our system for the English to Russi-
an task, while the English to Chinese system will fol-
low the same criteria and their results are mentioned 
later.
a. Data and Resources
Standard Runs: 
In the English to Russian task, we used the parallel 
corpus  (EnRu) released by NEWS 2009 Shared Task 
on Transliteration to build the translation model. For 
the English to Chinese standard run, we used the 
parallel English-Chinese (EnCh) corpus released by 
NEW2009 availed by (Li et al, 2004). The target 
language side (Russian, Chinese) of the parallel data 
was used to build the language model. NEWS2009 
released 5977 of EnRu names pairs as a training set, 
and 943 pairs as a development set. The EnCh corpus 
had 31,961 pairs as a training set, and 2896 pairs as a 
development set.
Non-Standard Runs:
For the English to Russian task we used the Russian 
data in UMC 0.1 Czech-English-Russian, from the In-
stitute of Formal and Applied Linguistics (?FAL), to 
build a larger Russian LM, in addition to the data re-
sources used in the standard run. No Named Entity 
tagging has been applied on this data because we lack 
the tools. However, we are just validating the charac-
ter n-gram sequences in the target language with lar-
ger corpus of character sequences. 
We didn't use any additional resources for the Chinese 
task.
b. Training
The training is held in two phases; first learning the 
list of Russian characters aligned to multiple English 
characters, and thus we obtain a table of English char-
acter n-grams to be added to unigram inventory of the 
source language. The second stage learns the translit-
eration model over this new inventory. (Larkey et al, 
2003).
Table 1 shows the list of English n-gram characters 
added to unigram inventory.
Table (1)
s h c h shch
s z c z szcz
s c h sch
z h zh
c k ck
p h ph
k h kh
c h ch
s h sh
s z sz
c z cz
? ? ??
A substring (phrase) table of Russian substrings 
mapped to English substrings is considered as the 
Parallel 
Corpus
HMM 
aligner
Block 
extract ion Decoder
Language 
Model
Figure (1)
P    u     n       t     l    a    n  d
?       ?    ?      ?     ?     ?  ?  ?
113
translation model P(E\R). A language model P(R) is 
built using a monolingual Russian corpus. 
Figure (3) shows a sample of the substring feature 
table generated during training using the block extrac-
tion heuristics over HMM alignments.
c. Decoding
The source English word is fragmented into all its 
possible substring sequences, and the decoder applies 
a monotone beam search, without reordering, to gen-
erate the best k phonetic translation character se-
quences in the target language alphabet. 
Experiments 1, 2, and 3 use a substring based translit-
eration system The experiments set up will be as fol-
lows:
i. The effect of true casing versus lowercasing 
Russian characters is explained through the 
first experiment (Exp-1).
ii. The released English data contains some un-
usual English characters not belonging to the 
English alphabet, some of which are vowels 
like "?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?", and 
others are consonants as "?, ?, ?, ?, ?, ?, ?, ?, 
?".  The effect  of normalizing these unusual 
English characters is explained in the second 
experiment (Exp-2).
iii. In the third experiment (Exp-3) we used the 
unigram inventory described in Table (1).
N.B.: Chinese language has a very large number of 
characters representing syllables rather than charac-
ters (a syllables = a consonant + vowel, or a conson-
ant + vowel + final), thus the unigram inventory used 
in the English to Chinese task wasn't generated using 
the statistical trend used with English-Russian task. 
General linguistic heuristics were used to re-merge 
character n-grams like "sh, th, gh, ph, etc?" as well 
as character repetitions like "ll, mm, nn ? ss, tt, 
etc..."
3. Results
Evaluation Metrics:
The quality of the transliteration task was measured 
using the 6 metrics defined in the shared task white 
paper. The first metric is the Word Accuracy in Top-1 
(ACC) which is the precision of the exact match with 
the Top-1 reference. The second one is the Fuzziness  
in Top-1 (Mean F-score) which reflects an average F-
score of the normalized lowest common subsequence 
between the system output and the Top-1 reference. 
The (MRR) represents the Mean Reciprocal Rank of 
the Top-1 reference in the k candidates generated by 
the system. The last three metrics MAPref, MAP10,  
MAPsys measure how the k candidates generated by 
the transliteration system are mapped to the n refer-
ences available for each input in the testset.
English to Russian task
The results of experiments 1, 2, and 3 on the Develop-
ment set, using the 6 evaluation metrics explained be-
fore, are written in Table (2). Exp-2 reflects the effect 
of normalizing all the unusual English characters that 
existed in the training data.  Referring to the results of 
Exp-1, we conclude that this normalization decreases 
the ACC of the system around 2.5%. In the next ex-
periments we only use the set up of Exp-3, which uses 
the statistical unigram inventory without true casing 
Russian characters or normalizing unusual English 
characters.
Exp-1 Exp-2 Exp-3
ACC 0.705 0 0
Mean F-
score
0.945 0.939 0
MRR 0.741 0.721 0
MAPref 0.705 0 0
MAP10 0.220 0.215 0
MAPsys 0.525 0 0
Table (2) explains Eng-Russian task results on the De-
velopment Set for experiments 1, 2, and 3.
? Standard Run:
Our Standard Run submission used the same setup 
used in Experiment-3, no lowercasing, no normaliza-
tion, and using the list of English n-grams that were 
added to the unigram inventory after the first training 
phase. Table (3) contains the results of our Standard 
Submissions.
Standard submission
ACC 0.545
Mean F-
score
0.917
MRR 0.596
MAPref 0.545
MAP10 0.286
MAPsys 0.299
Table (3) explains Eng-Russian task results on the 
blind Test Set. This was the Standard submission.
N.B.: We submitted the previous output in true-cased 
Russian characters as our standard submission, and 
then we submitted the same system output after lower 
casing as a Non-Standard run because we were not 
sure that the evaluation tool used by the Shared Task 
? ? ? ? || e a c o n 0 1
? ? || e a f 0 1
? ? ? || e a f ? 0 1
? ? ? ? ? ? || e n e r i f 0 1
? ? ? ? ? ? ? || e n e r i f e 0 1
? ? ? ? || e n e r s 0 1
? ? ? ? ? || e n e r s r 0 1
? ? ? ? ? ? || e n e r s r ? 0 1
Figure (3) a sample of the substring table
114
will be able to map true case and lower case vari-
ations.
The same will be done in the next run, where 2 sub-
missions are submitted for the same output, one of 
which was true-cased and the other was lower cased.
? Non-Standard Run:
Using (UMC 0.1) additional LM on the blind Test set. 
The results are in table(5)
Non-Standard submission
ACC 0.524
Mean F-score 0.913
MRR 0.579
MAPref 0.524
MAP10 0.277
MAPsys 0.291
Table (5) explains Eng-Russian task results on the 
blind Test Set. This was the Non-Standard submis-
sion.
English to Chinese task
Finally the previous setup with slight modifications 
was applied to the Eng-Chinese transliteration task. 
Tables (6), and (7) represent the results on the 
Chinese Development set and Test set respectively.
Exp-3
ACC 0.447
Mean F-score 0.748
MRR 0.489
MAPref 0.447
MAP10 0.147
MAPsys 0.191
Table (6) explains Eng-Chinese task results on the 
Development Set. 
? Standard Run:
Standard submission
ACC 0.411
Mean F-
score
0.737
MRR 0.464
MAPref 0.411
MAP10 0.141
MAPsys 0.173
Table (7) explains Eng-Chinese task results on the 
blind Test Set. This was the Standard submission
4. Conclusion and Future Work
In this paper we presented a substring based transliter-
ation system, making use of the analogy between the 
Machine translation task and Transliteration. By ap-
plying the phrase based SMT approach in the translit-
eration domain, and without any previous knowledge 
of the target languages, we built an English to Russian 
system with ACC of 54.5% and an English to Chinese 
system with ACC of 41.2%. 
In the future we are planning to hold some experi-
ments to filter out the generated phrase table (sub-
string table) and try other decoding techniques.
5. Acknowledgement
I would like to thank Dr. Hany Hassan in IBM 
Cairo TDC for his helpful comments and tech-
nical support. 
6. References
N.  AbdulJaleel  and  L.  S.  Larkey.  2003.  Statistical 
transliteration  for  English-Arabic  cross  language 
information retrieval. In CIKM, pages 139?146.
Y. Al-Onaizan and K. Knight. 2002. Machine Trans-
literation of Names in Arabic Text. In Proceed-ings 
of  the  ACL  Workshop  on  Computational  Ap-
proaches to Semitic Languages.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and 
R. L. Mercer. 1993. The Mathematics of Statistical 
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2):263?311.
P. Koehn, F.J. Och, and D. Marcu. 2003.  Statistical 
Phrase-Based  Translation.  Proc.  Of  the  Human 
Language  Technology  Conference,  HLT-
NAACL?2003, May.
H. Li, M. Zhang, J. Su: A Joint Source-Channel Mod-
el for Machine Transliteration. ACL 2004: 159-166 
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved 
Alignment Models for Statistical Machine Transla-
tion. In June 1999, EMNLP.
T.  Sherif  and  G.  Kondrak.  2007. Substring-Based 
Transliteration.  In  Proceedings  of  the  ACL 
Workshop  on  Computational  Approaches  to 
Semitic Languages.
C. Tillmann and H. Ney. 2003.Word Re-ordering and 
DP-based  Search  in  Statistical  Machine 
Translation. In COLING, pages  850-856. 
J. Zobel and P. Dart. 1996. Phonetic String Matching. 
Lessons from Information Retrieval. SIGIR Forum, 
special issue:166?172.
115
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 187?193, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
IBM_EG-CORE: Comparing multiple Lexical and NE matching 
features in measuring Semantic Textual similarity
Sara Noeman
IBM Cairo Technology and Development Center
Giza, Egypt
P.O. Box 166 Al-Ahram
noemans@eg.ibm.com
Abstract
We  present  in  this  paper  the  systems  we 
participated  with  in  the  Semantic  Textual 
Similarity  task  at  SEM  2013.  The  Semantic 
Textual Similarity Core task  (STS)  computes the 
degree  of  semantic  equivalence  between  two 
sentences  where  the  participant  systems  will  be 
compared to the manual scores, which range from 
5  (semantic  equivalence)  to  0  (no  relation).  We 
combined  multiple  text  similarity  measures  of 
varying complexity.  The experiments illustrate the 
different  effect  of  four  feature  types  including 
direct  lexical  matching,  idf-weighted  lexical 
matching,  modified BLEU N-gram matching and 
named entities matching. Our team submitted three 
runs  during  the  task  evaluation  period  and  they 
ranked  number  11,  15  and  19  among  the  90 
participating  systems  according  to  the  official 
Mean Pearson correlation metric for the task. We 
also  report  an  unofficial  run  with  mean  Pearson 
correlation  of  0.59221  on  STS2013  test  dataset, 
ranking  as  the  3rd  best  system  among  the  90 
participating systems.
1 Introduction
The  Semantic  Textual  Similarity  (STS)  task  at 
SEM 2013 is  to measure  the degree of semantic 
equivalence between pairs of sentences as a graded 
notion  of  similarity.  Text  Similarity  is  very 
important  to  many  Natural  Language  Processing 
applications, like extractive summarization (Salton 
et al, 1997), methods for automatic evaluation of 
machine translation (Papineni et al, 2002), as well 
as  text  summarization  (Lin  and  Hovy,  2003).  In 
Text  Coherence  Detection  (Lapata  and  Barzilay, 
2005), sentences are linked together by similar or 
related  words.  For  Word  Sense  Disambiguation, 
researchers  (Banerjee  and  Pedersen,  2003;  Guo 
and  Diab,  2012a)  introduced  a  sense  similarity 
measure using the sentence similarity of the sense 
definitions. In this paper we illustrate the different 
effect of four feature types including direct lexical 
matching, idf-weighted lexical matching, modified 
BLEU  N-gram  matching  and  named  entities 
matching.  The rest  of  this  paper  will  proceed as 
follows, Section 2 describes the four text similarity 
features  used.  Section  3  illustrates  the  system 
description,  data  resources  as  well  as  Feature 
combination.  Experiments  and  Results  are 
illustrated  in  section  4.  then  we  report  our 
conclusion and future work. 
2 Text Similarity Features
Our  system  measures  the  semantic  textual 
similarity between two sentences through a number 
of matching features which should cover four main 
dimensions: i) Lexical Matching ii)  IDF-weighted 
Lexical  Matching  iii)  Contextual  sequence 
Matching (Modified BLEU Score), and iv) Named 
Entities Matching.
First we introduce the alignment technique used. 
For a sentence pair {s1, s2} matching is done in 
each direction separately to detect the sub-sentence 
of  s1  matched  to  s2  and  then  detect  the  sub-
sentence of s2 matched to s1. For each word wi in 
s1 we search for its match  wj in s2 according to 
matching features.
S1: w0 w1 w2 w3 w4 ?... wi ?... wn
S2: w0 w1 w2 w3 w4 ?.......wj ?......... wm
187
2.1 Lexical Matching:
In this feature we handle the two sentences as bags 
of  words  to  be  matched  using  three  types  of 
matching, given that all stop words are cleaned out 
before matching:
I) Exact word matching.
II) Stemmed word matching: I used Porter 
Stemming algorithm (M.F. Porter, 1980) in 
matching, where  it is a process for removing 
the commoner morphological and inflectional 
endings from words in English. Stemming 
will render inflections like ?requires, required, 
requirements, ...? to ?requir? so they can be 
easily matched
III) Synonyms matching: we used a corpus based 
dictionary of 58,921 entries and their 
equivalent synonyms. The next section 
describes how we automatically generated this 
language resource. 
2.2 IDF-weighted Lexical Matching
We used the three matching criteria used in 
Lexical Matching after weighting them with 
Inverse-Document-Frequency. we applied the 
aggregation strategy by Mihalcea et al (2006): The 
sum of the idf-weighted similarity scores of each 
word with the best-matching counterpart in the 
other text is computed in both directions. For a 
sentence pair s1, s2, if s1 consists of m words {w0, 
w1, ?., w(m-1)} and s2 consists of n words {w0, 
w1, ?., w(n-1)} ,after cleaning stop words from 
both, and the matched words are 
?@Matched_word_List? of ?k? words, then 
2.3 Contextual Sequence Matching (Modified 
BLEU score)
We used a modified version of Bleu score to 
measure n-gram sequences matching, where for 
sentence pair s1, s2 we align the matched words 
between them (through exact, stem, synonyms 
match respectively). Bleu score as presented by (K. 
Papineni et al, 2002) is an automated method for 
evaluating Machine Translation. It compares n-
grams of the candidate translation with the n-grams 
of the reference human translation and counts the 
number of matches. These matches are position 
independent, where candidate translations with 
unmatched length to reference translations are 
penalized with Sentence brevity penalty. 
This helps in measuring n-gram similarity in 
sentences structure. We define ?matched 
sequence? of a sentence S1 as the sequence of 
words {wi, wi+1, wi+2, ?.. wj}, where wi, and wj 
are the first and last words in sentence S1 that are 
matched with words in S2.
For example in sentence pair S1, S2:
S1: Today's great Pax Europa and today's pan-
European prosperity depend on this.
S2: Large Pax Europa of today, just like current 
prosperity paneurop?enne, depends on it.
After stemming:
S1: todai's great pax europa and todai's pan-
european prosper depend on thi.
S2: larg pax europa of todai, just like current 
prosper paneurop?enn, depend on it.
?Matched sequence of S1?:
[todai 's great pax europa todai 's pan - european 
prosper depend]
?Matched sequence of S2?:
[pax europa todai just like current prosper 
paneurop?enn depend]
We measure the Bleu score such that:
Bleu{S1, S2} = &BLEU(S1_stemmed,"Matched 
sequence of S2");
Bleu{S2, S1} = &BLEU(S2_stemmed,"Matched 
sequence of S1");
The objective of trimming the excess words 
outside the ?Matched Sequence? range, before 
matching is to make use of the  Sentence brevity  
penalty in case sentence pair S1, S2 may be not 
similar but having matched lengths.
188
2.4 Named Entities Matching
Named entities carry an important portion of 
sentence semantics. For example:
Sentence1: In Nigeria , Chevron has been accused 
by the All - Ijaw indigenous people of instigating 
violence against them and actually paying 
Nigerian soldiers to shoot protesters at the Warri 
naval base .
Sentence2: In Nigeria , the whole ijaw indigenous 
showed Chevron to encourage the violence 
against them and of up to pay Nigerian soldiers to 
shoot the demonstrators at the naval base from 
Warri .
The underlined words are Named entities of 
different types ?COUNTRY, ORG, PEOPLE, 
LOC, EVENT_VIOLENCE? which capture 
the most important information in each 
sentence. Thus named entities matching is a 
measure of semantic matching between the 
sentence pair.
3 System Description
3.1 Data Resources and Processing 
All  data  is  tokenized,  stemmed,  and  stop 
words are cleaned.
Corpus based resources:
i. Inverse Document Frequency (IDF) 
language resource: The  document frequency 
df(t) of a term t is defined as the number of 
documents in a large collection of documents 
that contain a term ?t?. Terms that are likely 
to appear in most of the corpus documents 
reflect less importance than words that appear 
in specific documents only. That's why the 
Inverse Document Frequency is used as a 
measure of term importance in information 
retrieval and text mining tasks. We used the 
LDC English Gigaword Fifth Edition 
(LDC2011T07) to generate our idf dictionary. 
LDC Gigaword contains a huge collection of 
newswire from (afp, apw, cna, ltw, nyt, wpb, 
and xin). The generated idf resource contains 
5,043,905 unique lower cased entries, and 
then we generated a stemmed version of the 
idf dictionary contains 4,677,125 entries. The 
equation below represents the idf of term t 
where N is the total number of documents in 
the  corpus.
ii. English  Synonyms  Dictionary:  Using  the 
Phrase  table  of  an  Arabic-to-English  Direct 
Translation Model,  we generated English-to-
English phrase table using the double-link of 
English-to-Arabic  and  Arabic-to-English 
phrase translation probabilities over all pivot 
Arabic  phrases.  Then  English-to-English 
translation  probabilities  are  normalized  over 
all  generated  English  synonyms.  (Chris 
Callison-Burch  et  al,  2006) used  a  similar 
technique to generate paraphrases to improve 
their SMT system. Figure (1) shows the steps:
Figure(1) English phrase-to-phrase synonyms 
generation from E2A phrase table.
In our system we used the phrase table of the 
Direct Translation Model 2 (DTM2) (Ittycheriah 
and Roukos, 2007) SMT system, where each 
sentence pair in the training corpus was word-
aligned, e.g. using a MaxEnt aligner (Ittycheriah 
and Roukos, 2005) or an HMM aligner (Ge, 2004). 
then Block Extraction step is done. The generated 
phrase table contains candidate phrase to phrase 
translation pairs with source-to-target and target-to 
source translation probabilities. However the open 
source Moses SMT system (Koehn et al, 2007) 
For each English Phrase ?e1?
 {
    @ar_phrases = list of Arabic Phrases aligned to ?e?   
    in the phrase table;
    For each a (@ar_phrases)
     {
@en_phrases = list of English phrases aligned 
to ?a? in the phrase table;
For each e2 (@en_phrases)
{
    $Prob(e2\e1) = Prob(a\e1)*Prob(e2\a);
}
     }
 }
189
can be used in the same way to generate a 
synonyms dictionary from phrase table.
By applying the steps in figure (1):
a) English phrase-to-phrase synonyms table (or 
English-to-English phrase table), by applying the 
steps in a generic way.
b) English word-to-word synonyms table, by 
limiting the generation over English single word 
phrases. 
For example, to get al possible synonyms of the 
English word ?bike?, we used all the Arabic 
phrases that are aligned to ?bike? in the phrase 
table { ????????, ???????, ???????? , ?????  }, 
P: 1905645 14 0.0142582 0.170507 |  ????? | bike |   
P: 1910841 25 0.0262152 0.221198 |  ????????  | bike | 
P: 2127826 4 0.0818182 0.0414747 |  ???????? | bike |
P: 2396796 2 0.375 0.0138249 |  ??????? | bike |
then we get al the English words in the phrase 
table aligned to these Arabic translations { ?????,  
 ????????, ???????, ????????  }
This results in an English word-to-word synonyms 
list for the word ?bike? like this:
bike:
motorcycle      0.365253185010659
bicycle 0.198195663512781
cycling 0.143290354808692
motorcycles     0.0871686646772204
bicycles        0.0480779974950311
cyclists        0.0317670845504069
motorcyclists   0.0304152910853553
cyclist 0.0278451740161998
riding  0.0215366691148431
motorbikes      0.0148697281155676
Dictionary based resources:
? WordNet (Miller, 1995): is a large lexical 
database of English. Nouns, verbs, adjectives 
and adverbs are grouped into sets of cognitive 
synonyms (synsets), each expressing a distinct 
concept. Synsets are interlinked by means of 
conceptual-semantic and lexical relations. 
WordNet groups words together based on 
their meanings and interlinks not just word 
forms?strings of letters?but specific senses 
of words. As a result, words that are found in 
close proximity to one another in the network 
are semantically disambiguated. Second, 
WordNet labels the semantic relations among 
words.  Using WordNet, we can measure the 
semantic similarity or relatedness between a 
pair of concepts (or word senses), and by 
extension, between a pair of sentences. We 
use the similarity measure described in (Wu 
and Palmer, 1994) which finds the path length 
to the root node from the least common 
subsumer (LCS) of the two word senses which 
is the most specific word sense they share as 
an ancestor.
3.2 Feature Combination
The feature combination step uses the pre-
computed  similarity  scores.  Each  of  the 
text  similarity  features  can  be  given  a 
weight  that  sets  its  importance. 
Mathematically,  the  text  similarity  score 
between two sentences can be formulated 
using  a  cost  function  weighting  the 
similarity  features  as  follows:  N.B.:  The 
similarity score according to the features 
above is considered as a directional score.
Similarity(s1, s2) = [w1*Lexical_Score(s1, s2) +     
               w2*IDF_Lexical_Score(s1, s2) +
               w3*Modified_BLEU(s1, s2) +
               w4*NE_Score(s1, s2)] / (w1+w2+w3+w4)
Similarity(s2, s1) = [w1*Lexical_Score(s2, s1) +     
w2*IDF_Lexical_Score(s2, s1) +
w3*Modified_BLEU(s2, s1) +
                w4*NE_Score(s2, s1)] / (w1+w2+w3+w4)
Overall_Score = 5/2*[Similarity(s1, s2)+Similarity(s2, s1)]
where w1, w2, w3, w4 are the weights assigned to 
the similarity features (lexical, idf-weighted, 
modified_BLEU, and NE_Match features 
respectively).  The similarity score will be 
normalized over (w1+w2+w3+w4).
In our experiments, the weights are tuned manually 
without applying machine learning techniques. We 
used both *SEM 2012 training and testing data sets 
for tuning these weights to get the best feature 
weighting combination to get highest Pearson 
Correlation score. 
4 Experiments and Results
Submitted Runs
Our experiments showed that some features are 
more dominant in affecting the similarity scoring 
than others. We performed a separate experiment 
for each of the four feature types to illustrate their 
effect on textual semantic similarity measurement 
190
using direct lexical matching, stemming matching, 
synonyms matching,  as well as (stem+synonyms) 
matching. Table (1) reports the mean Pearson 
correlation results of these experiments on 
STS2012-test dataset
Direct Stem 
only
Synonyms 
only
Synonyms + 
Stem
NE  0.303 0.297 0.306 0.304
BLEU 0.439 0.446 0.469 0.453
Lexical 0.59 0.622 0.611 0.624
IDF 0.488 0.632 0.504 0.634
Table (1) reports the mean Pearson score for NE, 
BLEU, Lexical, and idf-weighted matching features 
respectively on STS2012-test dataset.
The submitted runs IBM_EG-run2, IBM_EG-run5, 
IBM_EG-run6 are the three runs with feature 
weighting and experiment set up that performed 
best on STS 2012 training and testing data sets.
Run 2: In this run the word matching was done on 
exact, and synonyms match only. Stemmed word 
matching was not introduced in this experiment. 
we tried the following weighting  between 
similarity feature scores, where we decreased the 
weight of BLEU scoring feature to  0.5, and 
increased the idf_Lexical match weight of 3.5. this 
is because our initial tuning experiments showed 
that increasing the idf lexical weight compared to 
BLEU weight gives improved results. The NE 
matching feature weight was as follows:
NE_weight = 1.5* percent of NE word to sentence word count
                   = 1.5* (NE_words_count/Sentence_word_count)
Run 5: In this experiment we introduced Porter 
stemming word matching, as well as stemmed 
synonyms matching (after generating a stemmed 
version of the synonyms dictionary). BLEU score 
feature was removed from this experiment, while 
keeping the idf-weight= 3, lexical-weight = 1, and 
NE-matching feature weight = 1.
Run 6: For this run we kept only IDF-weighted 
lexical matching feature which proved to be the 
dominant feature in the previous runs, in addition 
to Porter stemming word matching, and stemmed 
synonyms matching.
Data:  the training data of STS 2013 Core task 
consist of the STS 2012 train and test data. This 
data covers 5 datasets: paraphrase sentence pairs 
(MSRpar), sentence pairs from video descriptions 
(MSRvid), MT evaluation sentence pairs 
(SMTnews and SMTeuroparl) and gloss pairs 
(OnWN). 
Results on Training Data
System outputs will be evaluated according to the 
official scorer  which computes weighted Mean 
Pearson Correlation across the evaluation datasets, 
where the weight depends on the number of pairs 
in each dataset. 
Table (2), reports the results achieved on each of 
the STS 2012 training dataset. While table (3), 
reports the results achieved on STS 2012 test 
dataset.  
IBM_run2 IBM_run5 IBM_run6
Mean 0.59802 0.64170 0.68395
MSRpar 0.61607 0.63870 0.62629
MSRvid 0.70356 0.80879 0.83722
SMTeuroparl 0.47173 0.47403 0.58627
Table (2) Results on STS 2012 training datasets.
IBM_run2 IBM_run5 IBM_run6
Mean 0.59408 0.62614 0.63365
MSRpar 0.56059 0.59108 0.61306
MSRvid 0.73189 0.79960 0.87154
SMTeuroparl 0.51480 0.50563 0.41298
OnWN 0.62927 0.65760 0.67136
SMTnews 0.42305 0.44551 0.40819
Table (3) Results on STS 2012 test datasets.
Results on Test Data:
The  best  configuration  of  our  system  was 
IBM_EG-run6 which  was  ranked  #11  for  the 
evaluation metric Mean  (r  =  0.5502)  when 
submitted during the task evaluation period . Run6 
as illustrated before was planned to measure idf-
weighted lexical matching feature only, over Porter 
stemmed,  and  stemmed  synonyms  words. 
However when  revising  this  experiment  set  up 
191
during  preparing  the  paper,  after  the  evaluation 
period,  we  found  that  the  English-to-English 
synonyms  table  was  not  correctly  loaded  during 
matching,  thus  skipping  synonyms  matching 
feature  from  this  run.  So  the  official  result 
IBM_EG-run6 reports  only  idf-weighted 
matching over Porter stemmed bag of words. By 
fixing  this  and  replicating  the  experiment 
IBM_EG-run6-UnOfficial  as  planned to  be,  the 
mean  Pearson  correlation  jumps  4  points  (r  = 
0.59221)  which  ranks  this  system  as  the  3rd 
system  among  90  submitted  systems  very 
slightly  below  the  2nd system  (only  0.0006 
difference on the mean correlation metric).  In 
table (4), we report the official results achieved on 
STS 2013 test data.  While  table (5),  reports the 
unofficial  results  achieved  after   activating  the 
synonyms  matching  feature  in  IBM_EG-run6 
(unofficial) and comparing this run to the best two 
reported systems.
IBM_EG-
run2
IBM_EG-
run5
IBM_EG-
run6
headlines 0.7217 0.7410 0.7447
OnWN 0.6110 0.5987 0.6257
FNWN 0.3364 0.4133 0.4381
SMT 0.3460 0.3426 0.3275
Mean 0.5365 0.5452 0.5502
Rank #19 #15 #11
Table (4) Official Results on STS 2013 test datasets.
UMBC_EB
IQUITY-
ParingWor
ds
UMBC_EB
IQUITY-
galactus
IBM_EG-
run6 
(UnOfficial)
headlines 0.7642 0.7428 0.77241
OnWN 0.7529 0.7053 0.70103
FNWN 0.5818 0.5444 0.44356
SMT 0.3804 0.3705 0.36807
Mean 0.6181 0.5927 0.59221
Rank #1 #2 #3
 Table (5) UnOfficial Result after activating the 
synonyms matching feature in IBM_EG-run6 
compared to the best two performing systems in the 
evaluation.
 Results of un-official run:
One  unofficial  run  was  performed  after  the 
evaluation  submission  deadline  due  to  the  tight 
schedule  of  the  evaluation.  This  experiment 
introduces the effect of WordNet  Wu and Palmer 
similarity  measure  on  the  configuration  of  Run5 
(Porter stemming word matching,  with  synonyms 
matching, zero weight for   BLEU score feature, 
while keeping the idf-weight= 3, lexical-weight = 
1, and NE-matching feature weight = 1) 
Table (6) reports the unofficial result achieved on 
STS 2013 test data, compared to the Official run 
IBM_Eg-run5.  
Unofficial-Run IBM_EG-run5
Mean 0.52682 0.5452
headlines 0.70018 0.7410
OnWN 0.60371 0.5987
FNWN 0.35691 0.4133
SMT 0.33875 0.3426
Table (6) Un-Official Result on STS 2013 test datasets.
From the results in Table (6) it is clear that Corpus 
based synonyms matching outperforms dictionary-
based WordNet matching over SEM2013 testset.
5 Conclusion
We  proposed  an  unsupervised  approach  for 
measuring  semantic  textual  similarity  based  on 
Lexical  matching  features  (with porter  stemming 
matching  and  synonyms  matching),  idf-Lexical 
matching  features,  Ngram  Frquency  (Modified 
BLEU)  matching  feature,  as  well  as  Named 
Entities matching feature combined together with a 
weighted cost  function.  Our experiments  proved 
that idf-weighted Lexical matching in addition to 
porter stemming and synonyms-matching features 
perform best on most released evaluation datasets. 
Our  best  system  officially  ranked  number  11 
among 90 participating system reporting a Pearson 
Mean  correlation  score  of  0.5502.  However  our 
best  experimental  set  up  ?idf-weighted  Lexical 
matching  in  addition  to  porter  stemming  and 
synonyms-matching? reported in an unofficial run 
a mean correlation score of  0.59221 which ranks 
the system as number 3 among the 90 participating 
systems. In our future work we intend to try some 
machine  learning  algorithms  (like  AdaBoost  for 
192
example)  for  weighting  our  similarity  matching 
feature scores. Also we plan to extend the usage of 
synonyms matching from the word level to the n-
gram  phrase  matching  level,  by  modifying  the 
BLEU Score N-gram matching function to handle 
synonym phrases matching.
Acknowledgments
We would  like  to  thank  the  reviewers  for  their 
constructive criticism and helpful comments.
References 
Alfred.  V.  Aho  and  Jeffrey  D.  Ullman.  1972.  The 
Theory  of  Parsing,  Translation  and  Compiling, 
volume 1. Prentice-Hall, Englewood Cliffs, NJ. 
American  Psychological  Association.  1983. 
Publications  Manual. American  Psychological 
Association, Washington, DC.
Association  for  Computing  Machinery.  1983. 
Computing Reviews, 24(11):503-512.
Ashok  K.  Chandra,  Dexter  C.  Kozen,  and  Larry 
J.Stockmeyer.  1981.  Alternation.  Journal  of  the 
Association  for  Computing  Machinery,  28(1):114-
133. 
C. Y.  Lin  and  E.  H.  Hovy.  2003.  Automatic  
evaluation  of  summaries  using  n-gram  co-
occurrence  statistics. In  Proceedings  of  Human 
Language Technology Conference (HLT-NAACL 
2003), Edmonton, Canada, May. 
Chris  Callison-Burch,  Philipp  Koehn,  and  Miles 
Osborne.  2006.  Improved  statistical  machine  
translation  using  paraphrases. In  Proceedings  of 
HLT-NAACL.
Dan Gusfield. 1997.  Algorithms on Strings, Trees and 
Sequences. Cambridge University Press, Cambridge, 
UK. 
G.  Salton  and  C.  Buckley.  1997.  Term  weighting 
approaches in automatic text retrieval. In  Readings 
in  Information  Retrieval.  Morgan  Kaufmann 
Publishers, San Francisco, CA. 
Ittycheriah,  A.  and  Roukos,  S.  (2007).  Direct  
translation  model  2.  In  Human  Language 
Technologies  2007:  The  Conference  of  the  North 
American  Chapter  of  the  Association  for 
Computational Linguistics; Proceedings of the Main 
Conference, pp.57?64, Rochester, NY. 
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. 
Bleu: a method for automatic evaluation of machine  
translation.  In  Proceedings  of  the  40th  Annual 
Meeting  of  the  Association  for  Computational 
Linguistics, Cambridge, UK. 
M. Lapata  and  R.  Barzilay.  2005.  Automatic  
evaluation  of  text  coherence:  Models  and  
representations.  In  Proceedings  of  the  19th 
International  Joint  Conference  on  Artificial 
Intelligence, Edinburgh.
P.  Koehn,  F.J.  Och,  and  D.  Marcu.  2003.  Statistical  
Phrase-Based  Translation. Proc.  Of  the  Human 
Language  Technology  Conference,  HLTNAACL? 
2003, May. 
Philipp  Koehn,  Hieu  Hoang,  Alexandra  Birch,  Chris 
Callison-Burch,  Marcello Federico,  Nicola Bertoldi, 
Brooke  Cowan,  Wade  Shen,  Christine  Moran, 
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra 
Constantin,  and  Evan  Herbst.  2007.  Moses:  Open 
Source Toolkit for Statistical Machine Translation. In 
Proceedings  of  the  ACL  2007  Demo  and  Poster 
Sessions, pages 177?180. 
R.  Mihalcea  ,  C.  Corley,  and  C.  Strapparava 2006. 
Corpus-based and knowledge-based measures of text  
semantic similarity. In  Proceedings of the American 
Association for Artificial Intelligence. (Boston, MA).
Satanjeev Banerjee and Ted Pedersen. 2003.  Extended 
gloss overlaps as a measure of semantic relatedness. 
In  Proceedings  of  the  18th  International  Joint 
Conference on Artificial Intelligence, pages 805?810.
Ted  Pedersen,  Siddharth  Patwardhan,  and  Jason 
Michelizzi,  2004,  WordNet::Similarity  -  Measuring 
the  Relatedness  of  Concepts.  Proceedings  of  Fifth 
Annual Meeting of the North American Chapter  of 
the  Association  for  Computational  Linguistics 
(NAACL-2004). 
Wu,  Z.,  and  Palmer,  M.  1994.  Verb  semantics  and 
lexical  selection. In  32nd  Annual  Meeting  of  the 
Association for Computational Linguistics, 133?138.
Weiwei  Guo  and  Mona  Diab.  2012a.  Learning  the 
latent semantics of a concept from its definition. In 
Proceedings  of  the  50th  Annual  Meeting  of  the 
Association for Computational Linguistics.
193
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 57?61,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Language Independent Transliteration Mining System Using Finite State
Automata Framework
Sara Noeman and Amgad Madkour
Human Language Technologies Group
IBM Cairo Technology Development Center
P.O.Box 166 El-Haram, Giza, Egypt
{noemans,amadkour}@eg.ibm.com
Abstract
We propose a Named Entities translitera-
tion mining system using Finite State Au-
tomata (FSA). We compare the proposed
approach with a baseline system that uti-
lizes the Editex technique to measure the
length-normalized phonetic based edit dis-
tance between the two words. We sub-
mitted three standard runs in NEWS2010
shared task and ranked first for English
to Arabic (WM-EnAr) and obtained an F-
measure of 0.915, 0.903, and 0.874 re-
spectively.
1 Introduction
Named entities transliteration is a crucial task in
many domains such as cross lingual information
retrieval, machine translation, and other natural
language processing applications. In the previous
NEWS 2009 transliteration task, we introduced a
statistical approach for transliteration generation
only using the bilingual resources (about 15k par-
allel names) provided for the shared task. For
NEWS2010, the shared task focuses on acquisi-
tion of a reasonably sized, good quality names
corpus to complement the machine transliteration
task. Specifically, the task focuses on mining the
Wikipedia paired entities data (inter-wiki-links) to
produce high-quality transliteration data that may
be used for transliteration generation tasks.
2 Related Work
Finite state Automata is used to tackle many Nat-
ural Language Processing challenges. Hassan
(2008) et al proposed the use of finite state au-
tomata for language-independent text correction.
It consists of three phases : detecting misspelled
words, generating candidate corrections for them
and ranking corrections. In detecting the mis-
pelled words, they compose the finite state au-
tomaton representation of the dictionary with the
input string. Onaizan (2002) et al proposed
the use of probabilistic finite state machines for
machine transliteration of names in Arabic text.
They used a hybrid approach between phonetic-
based and spelling-based models. Malik (2008)
et al proposed a Hindi Urdu machine translit-
eration system using finite state machines. They
introduced UIT (universal intermediate transcrip-
tion) on the same pair according to thier phonetic
properties as a means of representing the language
and created finite state transducers to represent
them. Sherif (2007) proposed the use of memo-
ryless stochastic transducer for extracting translit-
eration through word similarity metrics.
Other approaches for transliteration include
translation of names through mining or through
using machine translation systems resources. Has-
san (2007) et al proposed a framework for extrac-
tion of named entity translation pairs. This is done
through searching for candidate documents pairs
through an information retrieval system and then
using a named entity matching system which re-
lies on the length-normalized phonetic based edit
distance between the two words. They also use
a phrase-based translation tables to measure simi-
larity of extracted named entities. Noeman (2009)
also used a phrase based statistical machine trans-
lation (PBSMT) approach to create a substring
based transliteration system through the generated
phrase table, thus creating a language indepen-
dent approach to transliteration. Other resources
have been used to perform transliteration. Chang
(2009) et. al proposed the use of a romanization
table in conjunction with an unsupervised con-
straint driven learning algorithm in order to iden-
tify transliteration pairs without any labelled data.
3 System architecture
The approach consists of three main phases which
are (1) Transliteration model learning, (2) Fi-
57
Figure 1: Transliteration table learning in PBSMT
nite State machine formalization of the generated
transliteration model and (3) Generating Candi-
date transliterations. Figure (1) illustrates Translit-
eration table learning in PBSMT framework. A
detailed description of each phase is given in the
following sections.
3.1 Transliteration model learning
The objective of NEWS2010 shared task is to de-
velop a system for mining single word translitera-
tion pairs from the standard Wikipedia paired top-
ics (Wikipedia Inter-Language Links, or WIL1),
using a seed data of only 1000 parallel names. The
aim is to learn one-to-many character sequence
mappings on both directions.
We propose the use of MOSES framework1 for
PBSMT training which was applied on the 1k par-
allel seed data. The proposed approach depends on
the formulation of the transliteration problem us-
ing the PBSMT approach used in Machine trans-
lation. Giza++ Hidden Markov Model (HMM)
aligner2 proposed by Och (1999) was also used
over the parallel character sequences. Heuristics
were used to extend substring to substring map-
pings based on character-to-character alignment.
This generated a substring to substring translation
model such as in Koehn (2003). The phrase ?sub-
string? table was filtered out to obtain all possi-
ble substrings alignment of each single character
in the language alphabet in both directions. This
means that for each character in the source lan-
guage (English) alphabet, substrings mapped to it
are filtered with a threshold. Also for each char-
acter in the target language (Arabic) alphabet, all
English substrings mapped to it are filtered with
a threshold. These two one-to-many alignments
were intersected in one ?Transliteration Arabic-to-
English mapping?. We obtained a character align-
ment table which we refer to as ?Ar2En list?. Fig-
ure(2) illustrates a sample one-to-many alignment
mapping.
1MOSES Framework: http://www.statmt.org/moses/
2GIZA++ Aligner: http://fjoch.com/GIZA++.html
Figure 2: One to Many Alignment Mapping
1
a:a/0
b:b/0
2/0.25
<epsilon>:a/0
<epsilon>:b/0
a:<epsilon>/0
a:b/0
b:<epsilon>/0
b:a/0
3
a:b/0
4
b:a/0
a:a/0
b:b/0
b:a/0
a:b/0
Figure 3: Edit distance 1 FSM
3.2 FSM formalization of Transliteration
Model
The proposed method makes use of the finite state
automaton representation for the Ar2En character
alignment list, where the input is the source char-
acter and the output is the target character. We re-
fer to this finite state transducer (FST) as ?Ar2En
FST?. For each source word, we build a Finite
State Acceptor (FSA), such that each candidate
source word FSA is composed with the ?Ar2En
FST?. For the target words list, we build a finite
state acceptor (FSA) that contains a path for each
word in the target Wiki-Link.
3.3 Generating Candidate transliterations
The task of generating candidate transliterations
at edit distance k from initial source candidate
transliterations using Levenshtein transducer can
be divided into two sub tasks: Generating a list of
words that have edit distance less than or equal k
to the input word, and selecting the words inter-
58
1a:a/0
b:b/0
2/0.25
<epsilon>:a/0
<epsilon>:b/0
a:<epsilon>/0
a:b/0
b:<epsilon>/0
b:a/0
4
a:b/0
5
b:a/0
a:a/0
b:b/0
3/0.5
<epsilon>:a/0
<epsilon>:b/0
a:<epsilon>/0
a:b/0
b:<epsilon>/0
b:a/0
6
a:b/0
7
b:a/0
b:a/0
a:b/0
a:a/0
b:b/0
b:a/0
a:b/0
Figure 4: Edit distance 2 FSM
secting with the target inter-wiki-link words. This
is similar to the spelling correction technique that
used FSM which was introduced by Hassan (2008)
et. al. In the spelling correction task , after gener-
ating the list of words within edit distance k to the
input word, the system selects a subset of those
words that exist in a large dictionary. In order to
accomplish this same scenario, we created a sin-
gle transducer (Levenshtein transducer) that when
composed with an FSM representing a word gen-
erates all words within an edit distance k from the
input word. We then compose the resulting FSM
with an FSA (finite state acceptor) of all words in
the target inter-wiki-link. The Levenshtein trans-
ducer is language independent and is built only
using the alphabet of the target language. Figure
(3) and Figure (4) illustrate the Levenshtein trans-
ducer for edit distance 1 and 2 over a limited set of
vocabulary (a, b).
4 Data and Resources Processing
After revising the training data (inter-wiki-links)
released, we discovered that English and Arabic
words contained many stress marks and non nor-
malized characters. We therefore applied normal-
ization on Arabic and English characters to in-
crease source target matching probability, thus in-
creasing the recall of data mining. We also nor-
malized Arabic names, removing all diacritics and
kashida. Kashida is a type of justification used in
some cursive scripts such as Arabic. Also we nor-
malized Alef () with hamza and madda to go to
?bare Alef?.
Figure 5: Using Levenshtein edit-1 FST
5 Standard runs
We submitted 6 runs derived from 3 experiments.
For each experiment, we submitted 2 runs, one
with normalized Arabic and English characters,
and the other with the stress marks and special
characters. It is important to note that we run the
mining in the Arabic to English direction, thus the
Arabic side is the source and the English side is
the target.
5.1 Using Levenshtein edit distance 1 FST
Figure (5) illustrates the algorithm used to con-
duct the first experiment. We subjected all source
words to be composed with Levenshtein edit dis-
tance 1. For each Wiki-Link, we build a finite
state acceptor (FSA) that contains a path for each
word in the Arabic Wiki-Link. We refer to it as
FSA[@ArWords]. Similarly, for the English name
candidates we build a finite state acceptor (FSA)
that contains a path for each word in the English
Wiki-Link. We refer to it as FSA[@EnWords].
The generated @ArWords and @EnWords are the
lists of words in the Arabic and English wiki-links
respectively. The result of this experiment was re-
ported as Standard-3 ?normalized characters? and
Standard-4 ?without normalized characters?.
5.2 Using Levenshtein up to edit distance 2
FST
Figure (6) illustrates the algorithm used to conduct
the second experiment. We use a threshold on the
number of characters in a word to decide whether
it will be subjected for ?composed with? edit dis-
59
Figure 6: Using Levenshtein edit-2 FST
tance 0 or 1 or 2. We use a threshold of 3 for
edit distance 1 and a threshold of 7 for edit dis-
tance 2. The threshold values are set based on our
previous experience from dealing with Arabic text
and could be derived from the data we obtained.
If word length is less than or equal 3 letters, then
it is not composed with Levenshtein FSTs, and if
word length is between 4 to 7 letters, we compose
it with edit distance 1 FST. Longer words are com-
posed with edit distance 2 FST. The result of the
experiment was reported in two submitted runs:
Standard-5 ?normalized characters? and Standard-
6 ?without normalized characters?.
5.3 Baseline
We use a length-normalized phonetic edit distance
to measure the phonetic similarity between the
source and target Named Entities in the inter-wiki-
links. We use the Editex technique Zobel (1996)
that makes use of the phonetic characteristics of
individual characters to estimate their similarity.
Editex measures the phonetic distance between
pairs of words by combining the properties of
edit distances with a letter grouping strategy that
groups letters with similar pronunciations. The re-
sult of this experiment was reported in two submit-
ted runs: Standard-1 ?normalized characters? and
Submission F-Score Precision Recall
Standard-6 0.915 0.887 0.945
Standard-4 0.903 0.859 0.952
Standard-2 0.874 0.923 0.830
Standard-5 0.723 0.701 0.747
Standard-3 0.716 0.681 0.755
Standard-1 0.702 0.741 0.666
Table 1: Shared Task Results
Standard-2 ?without normalized characters?.
6 Results
Table (1) illustrates the results of the shared task
given on the runs we submitted.
Our baseline run (Standard-2) reports highest
precision of 0.923 and lowest recall of 0.830 (low-
est F-score = 0.874). The reason is that Editex
technique measures the edit distance based on let-
ter grouping strategy which groups letters with
similar pronunciations. It operates on character to
character level. Letters that are mapped to multi-
characters will suffer a large edit distance and may
exceed the matching threshold used.
The two runs Standard-4 and Standard-6 are
implemented using edit-distance FSM matching
between source and target. They cover one-to-
many character mapping. We notice that Standard-
6 run reports higher precision of 0.887 compared
to 0.859 for Standard-4 run. This reflects the ef-
fect of using variable edit-distance according to
the source word length. The Standard-6 reports
a Recall of 0.945 producing our best F-Score of
0.915. Standard-6 recall degrades only 0.7% from
Standard-4 Recall (0.952).
7 Conclusion
We proposed a language independent transliter-
ation mining system that utilizes finite state au-
tomaton. We demonstrated how statistical tech-
niques could be used to build a language indepen-
dent machine transliteration system through uti-
lizing PBMT techniques. We performed 3 stan-
dard experiments each containing two submis-
sions. FSM edit distance matching outperformed
Editex in F-Score and Recall. The proposed ap-
proach obtained the highest F-Score of 0.915 and
a recall of 0.945 in the shared task.
60
References
Ahmed Hassan, Haytham Fahmy, Hany Hassan 2007.
Improving Named Entity Translation by Exploiting
Comparable and Parallel Corpora. AMML07
Ahmed Hassan, Sara Noeman, Hany Hassan 2008.
Language Independent Text Correction using Finite
State Automata. IJCNLP08.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney 1999. Improved Alignment Models for Statisti-
cal Machine Translation. EMNLP.
Justin Zobel and Philip Dart 1996. Phonetic string
matching: Lessons from information retrieval. In
Proceedings of the Annual ACM References Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR).
M. G. Abbas Malik, Christian Boitet, Pushpak Bhat-
tacharyya 2008. Hindi Urdu Machine Transliter-
ation using Finite-state Transducers. Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (Coling 2008), pages 537544
Ming-Wei Chang, Dan Goldwasser, Dan Roth,
Yuancheng Tu 2009. Unsupervised Constraint
Driven Learning For Transliteration Discovery.
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Philipp Koehn, Franz Josef Och, Daniel Marc 2003.
Statistical Phrase-Based Translation. Proc. Of the
Human Language Technology Conference, HLT-
NAACL2003, May.
Sara Noeman 2009. Language Independent Transliter-
ation system using PBSMT approach on substrings.
Proceedings of the 2009 Named Entities Workshop:
Shared Task on Transliteration.
Tarek Sherif, Grzegorz Kondrak 2007. Bootstrapping
a Stochastic Transducer for Arabic-English Translit-
eration Extraction. Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 864871
Yasser Al-Onaizan, Kevin Knight 2002. Machine
Transliteration of Names in Arabic Text. ACL
Workshop on Comp. Approaches to Semitic Lan-
guages.
61
