Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1728?1739, Dublin, Ireland, August 23-29 2014.
Towards Semantic Validation of a Derivational Lexicon
Britta D. Zeller
?
Sebastian Pad
?
o
?
Jan
?
Snajder
?
?
Heidelberg University, Institut f?ur Computerlinguistik
69120 Heidelberg, Germany
?
Stuttgart University, Institut f?ur maschinelle Sprachverarbeitung
70569 Stuttgart, Germany
?
University of Zagreb, Faculty of Electrical Engineering and Computing
Unska 3, 10000 Zagreb, Croatia
zeller@cl.uni-heidelberg.de pado@ims.uni-stuttgart.de jan.snajder@fer.hr
Abstract
Derivationally related lemmas like friend
N
? friendly
A
? friendship
N
are derived from a common
stem. Frequently, their meanings are also systematically related. However, there are also many
examples of derivationally related lemma pairs whose meanings differ substantially, e.g., object
N
? objective
N
. Most broad-coverage derivational lexicons do not reflect this distinction, mixing up
semantically related and unrelated word pairs.
In this paper, we investigate strategies to recover the above distinction by recognizing semantically
related lemma pairs, a process we call semantic validation. We make two main contributions:
First, we perform a detailed data analysis on the basis of a large German derivational lexicon. It
reveals two promising sources of information (distributional semantics and structural information
about derivational rules), but also systematic problems with these sources. Second, we develop
a classification model for the task that reflects the noisy nature of the data. It achieves an
improvement of 13.6% in precision and 5.8% in F1-score over a strong majority class baseline.
Our experiments confirm that both information sources contribute to semantic validation, and that
they are complementary enough that the best results are obtained from a combined model.
1 Introduction
Morphological processing forms the first step of virtually all linguistic processing toolchains in natural
language processing (NLP) and precedes other analyses such as part of speech tagging, parsing, or
named entity recognition. There are three major types of morphological processes: (a) Inflection modifies
word forms according to the grammatical context; (b) derivation constructs new words from individual
existing words, typically through affixation; (c) composition combines multiple words into new lexical
items. Computational treatment of morphology is often restricted to normalization, such as lemmatization
(covering inflection only) or stemming (covering inflection and derivation heuristically, Porter (1980)).
An important reason is that English is morphologically a relatively simple language. Composition is
not marked morphologically (zoo gate) and an important derivational pattern is zero derivation where the
input and output terms are identical surface forms (a fish / to fish). Thus, lemmatization or stemming go a
long way towards treating the aspects of English morphology relevant for NLP. The situation is different
for languages with a complex morphology that calls for explicit treatment. In fact, recent years have seen
a growing body of computational work in particular on derivation, which is a very productive process of
word formation in Slavic languages but also in languages more closely related to English, like German
(
?
Stekauer and Lieber, 2005).
Derivation comprises a large number of distinct patterns, many of which cross part of speech boundaries
(nominalization, verbalization, adjectivization), but some of which do not (gender indicators like master /
mistress, approximations like red / reddish). A simple way to conceptualize derivation is that it partitions a
language?s vocabulary into derivational families of derivationally related lemmas (cf. Zeller et al. (2013),
Gaussier (1999)). In WordNet, this type of information has been included to some extent by so-called
?morpho-semantic? relations (Fellbaum et al., 2009), and the approach has been applied to languages other
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1728
lachen Lacher l?acherlich
sfx ?er?V N
Append suffix ?er? to the stem of
the verb to obtain a noun
try uml &
sfx ?lich?N
A
Try to turn the noun?s vowels into umlauts, then
append suffix ?lich? to obtain an adjective
to laugh laugh laughable
Figure 1: (Part of) a derivational family from DERIVBASE including derivational rules
than English (Bilgin et al., 2004; Pala and Hlav?a?ckov?a, 2007). Another source of derivational information
are stand-alone derivational lexicons such as CatVar (Habash and Dorr, 2003) for English, DERIVBASE
(Zeller et al., 2013) for German, or the multilingual CELEX (Baayen et al., 1996).
Recent work has demonstrated that NLP can benefit from derivational knowledge. Shnarch et al. (2011)
employ derivational knowledge in recognizing English textual entailment to better gauge the semantic
similarity of text and hypothesis. Pad?o et al. (2013) improve the prediction of German semantic similarity
judgments for lemma pairs by backing off to derivational families for infrequent lemmas. Luong et al.
(2013) and Lazaridou et al. (2013) improve distributional semantic representations.
Note that all of these applications make use of derivational knowledge to address various semantic tasks,
working on the assumption that derivationally related words, as represented in derivational lexicons, are
strongly semantically related. This assumption is not completely warranted, though. The development of
wide-coverage derivational lexicons is generally driven by morphological information, using for example
finite-state technology (Karttunen and Beesley, 2005) to characterize known derivational patterns in terms
of string transformations. Even though there is a strong correlation in derivation between morphology
and semantics, it is not perfect. The absence of (synchronic) semantic relatedness can have a number of
reasons, including accidental instantiation of derivational patterns (corn ? corner) and diachronic meaning
drift (dog (animal) ? dogged (determined)). In other words, a substantial number of the lemma pairs in
those lexicons are false positives regarding the level of semantic relatedness.
Our goal in this paper is to ameliorate this situation by developing strategies for the semantic validation
of derivational lexicons, i.e., methods to determine, for lemma pairs that are derivationally related
at the morphological level, whether they are in fact semantically related. We base our study on the
German derivational lexicon DERIVBASE, and start by assessing which strategies can be used for its
semantic validation (Section 2). In Sections 3 and 4, we analyze the contributions of semantic information
(distributional semantics) as well as structural information (derivational rules). On the basis of our
observations, we train a classifier that is able to semantically validate DERIVBASE at 89.9% F
1
-score
(Section 5), significantly outperforming a majority-class baseline of 84.1%. Section 6 reviews related
work. Section 7 concludes the paper and outlines future work.
2 A Lexicon for German Derivation
2.1 DERIVBASE
DERIVBASE (Zeller et al., 2013) is a freely available derivational lexicon for German.
1
We used a
rule-based framework to define derivation rules that cover suffixation, prefixation, and zero derivation as
well as stem changes. Following the work of
?
Snajder and Dalbelo Ba?si?c (2010), derivational processes are
defined using derivational rules and higher-order string transformation functions. The only requirements
for this method are (a) a comprehensive set of lemmas and (b) knowledge about admissible derivational
rules, which can be gathered, for example, from linguistics textbooks.
Figure 1 shows a small sample from a derivational family with three lemmas and two derivational rules,
one turning a verb into the corresponding event noun (in this case a semelfactive), and one turning the
event into an adjective associated with it. Note that there are two perspectives on such a family: It can
1
http://www.cl.uni-heidelberg.de/
?
zeller/res/derivbase/
1729
?Positive? Precision Recall
DERIVBASE release class % %
1.2 (Zeller et al., 2013)
3
R and M 83.0 71.0
1.4 (our analysis) R and M 85.1 91.4
1.4 (our analysis) R only 76.7 93.8
Table 1: DERIVBASE evaluation across releases on the DERIVBASE release 1.2 P and R samples
either be seen as a set of lemmas, or as a set of (independent) lemma pairs. We will assume the latter
perspective in this paper, leaving questions of global coherence for future work.
DERIVBASE is a good example for the problems sketched in Section 1. It is defined purely on
morphological grounds, without semantic validation of derivational families. Consequently, it contains a
substantial number of words that are not semantically related.
2.2 Morphological and Semantic Relatedness in DERIVBASE
Our original evaluation of the quality of DERIVBASE in Zeller et al. (2013) was based on manually
classified samples of lemma pairs. We introduce two samples, the ?R sample?, drawn from a large
population of lemma pairs with high string similarity, in order to calculate recall, and the ?P sample?,
drawn from the DERIVBASE families, in order to compute precision. Each lemma pair was classified into
one of five categories (R: morphologically and semantically related; M: only morphologically related;
N: not related; L: lemmatization errors; C: compounds) and inter-annotator agreement was checked to
be substantial.
2
The overall best model (L123) showed 83% precision and 71% recall. However, this
evaluation is limited in two important respects. First, it refers to DERIVBASE release 1.2 from 2013.
Since then, we have extended DERIVBASE, e.g., with rules covering particle verbs, a very productive
area of German derivation. Secondly, and more seriously, the previous evaluation considered all instances
of R and M as true positives. In other words, in Zeller et al. (2013) we only evaluated the morphological
relatedness of the lemma pairs but not the semantic relatedness.
We therefore start by presenting an evaluation of DERIVBASE focusing on the R instances in Table 1,
reusing the DERIVBASE 1.2 ?P? and ?R? samples introduced in Zeller et al. (2013, see there for evaluation
details). Between DERIVBASE 1.2 and 1.4, precision increased marginally and recall substantially, due
mainly to the inclusion of rules that cover particle verbs. However, the numbers change substantially when
only R (truly semantically related pairs) are counted as true positives. Recall increases by about 2.5%, but
precision drops about 8.5%. Almost one quarter of all pairs in the lexicon are not semantically related.
A possible confounder of this analysis is that the ?P sample? was drawn on DERIVBASE 1.2 and
therefore does not include the novel items in DERIVBASE 1.4. We therefore created a novel DERIVBASE
1.4 extended sample by combining the existing ?P sample? with those pairs from the ?R sample? that are
in the coverage of a DERIVBASE rule as of DERIVBASE 1.4, resulting in 2,545 lemma pairs.
This DERIVBASE 1.4 extended sample will form the basis of all our analyses in this paper. The class
distribution in the new sample is similar, but not identical, to the old P sample, as shown in Table 2. The
relative frequency of R drops another 2%. Since this number also corresponds to the precision of the
resource, the precision of the extended sample is 74.6%.
There are almost no compound errors C, which is not surprising given the rule-based construction of
the lexicon, and only a relatively small number (about 5%) of lemmatization errors L, which fall outside
the scope of our work. In contrast, both N and M occur with substantial frequency: Each class accounts
for around 10% of the pairs. An analysis of N shows many cases of rule overgeneration: These are often
pairs of lemmas whose stems are sufficiently similar that they might be related, e.g., by stem-changing
derivation rules. Although such rules are valid in other contexts (Verkauf
N
? Verk?aufer
N
(selling ? seller)),
2
Although we believe semantic relatedness to be fundamentally a graded scale, we adopt a binary notion of it as a convenient
operational simplification that is supported by the good inter-annotator agreement for manual labeling in DERIVBASE.
3
DERIVBASE 1.2 corresponds to DERIVBASE ?L123? in (Zeller et al., 2013, p. 1207).
1730
R M N L C
Frequency 1899 265 240 131 8
Percentage overall 74.6 10.4 9.5 5.2 0.3
Percentage on dev. set 75.5 10.3 9.0 4.8 0.3
Percentage of test set 72.6 10.6 10.6 5.9 0.3
Table 2: Class distribution in our new DERIVBASE 1.4 extended sample
erroneous application leads to N cases like Blase
N
? Bl?aser
N
(bubble ? blower). Also, we find false
matches of common noun rules with named entities (Empire
N
? Empirismus
N
(Empire ? empiricism)).
In contrast, many cases of M (as sketched in Section 1) refer to different senses of the same stem. As
an example, consider beruhen
V
? unruhig
A
(to rest on ? restless), both related to Ruhe
N
(rest). In other
cases, one of the two lemmas appears to have undergone a meaning shift (Rappel
N
? rappeln
V
(craze ?
to rattle)). This is particularly prominent for particle verbs (bauen
V
? erbaulich
A
(build ? edifying)).
We divide the DERIVBASE 1.4 extended sample into a development and a test partition (70:30 ratio);
the subsequent analyses consider only the development set.
2.3 Hypotheses for Semantic Validation
The preceding analysis of DERIVBASE has established that the lexicon contains a substantial number
(around one fourth) of lemma pairs that are not semantically related. Therefore, it is in need of semantic
validation, i.e., a computational procedure that can filter out semantically unrelated words.
In this paper, we frame semantic validation as a binary classification task that classifies all lemma pairs
within one derivational family as either semantically related or unrelated. We consider this a first step
towards splitting the current, morphologically motivated, DERIVBASE families into smaller, semantically
coherent, families. We base our work on two general hypotheses about the types of information that might
be helpful in this endeavor.
Hypothesis 1. Distributional similarity indicates semantic relatedness between derivationally related
words. The instances of polysemy and meaning shift that we observe, in particular in the M class,
motivate the use of distributional similarity (Turney and Pantel, 2010) since we expect these lemma
pairs to be distributionally less related than cases of true semantic relatedness.
Hypothesis 2. Derivational rules differ in their reliability. Both the evidence from M and N indicate
that some rules are more meaning-preserving than others. We expect this to be tied to both lexical
properties of the rules (particle verbs are more likely than diminutives to radically change meaning)
as well as structural properties (more specific rules are presumably more precise than generic rules).
In the two following Sections, we will operationalize these hypotheses and analyze the development set of
the DERIVBASE 1.4 extended sample with respect to their empirical adequacy.
3 Analysis 1: Distributional Similarity for Semantic Validation
3.1 Measuring Distributional Similarity
We examine semantic similarities as predicted by simple bag-of-words semantic space models built from
the lemmatized SDeWaC (Faa? et al., 2010), a large German web corpus containing about 880 million
words. We compute vectors for all words covered in DERIVBASE using a window of ?5 words within
sentence boundaries and considering the 10k most frequent lemma-part of speech combinations of nouns,
verbs, and adjectives in SDeWaC as contexts. Distributional vectors are built from co-occurrences which
are measured with Local Mutual Information (Evert, 2005). The semantic similarity is measured by the
cosine similarity between the vectors. Despite the size of the corpus, many lemmas from DERIVBASE
occur very infrequently, and due to the inflection in German, it is important to retrieve as many occurrences
of each lemma as possible.
1731
We therefore use a very permissive two-step lemmatization scheme that starts from lemmas from
the lexicon-based TreeTagger (Schmid, 1994), which provides reliable lemmas but with relatively low
coverage, and supplements them with lemmas and parts of speech produced by the probabilistic MATE
toolkit (Bohnet, 2010) when TreeTagger abstains.
3.2 Frequency Considerations
The advantage of the string transformation-based construction of DERIVBASE is its ability to include
infrequent lemmas in the lexicon, and in fact DERIVBASE includes more than 250,000 content lemmas,
some of which occur not more than three times in SDeWaC. However, this is a potential problem when
we build distributional representations for all lemmas in DERIVBASE since it is known from the literature
that similarity predictions for infrequent lemmas are often unreliable (Bullinaria and Levy, 2007).
Our data conform to expectations in this regard ? infrequent lemmas are indeed problematic for
validating the semantic relatedness of lemma pairs. More specifically, the semantic similarity of related
lemmas (R) is systematically underestimated, because the lemma pairs from our sample are often too
infrequent to share any dimensions. Consequently, they receive a low or zero cosine even when they are
semantically strongly related. For example, each of the lemmas Drogenverkauf
N
? Drogenverk?aufer
N
(drug selling ? drug seller) has only nine lemmas as dimensions, and those are completely disjoint. This
underestimation constitutes a general trend. The model assigns cosine scores below 0.1 to 64% of the
related pairs in the development set, cosines below 0.2 to 81%, and cosines below 0.3 to 87%. Such low
scores are problematic for separating related from unrelated pairs.
Two-step lemmatization is important for the proper handling of infrequent words. Compared to
just using TreeTagger, the TreeTagger+MATE vectors for auferstehen
V
? auferstehend
A
(to resurrect ?
resurrecting) share seven more dimensions, including Jesus, Lord, myth, and suffering. Correspondingly,
the cosine value of this pair rises by 50%. Generally, the amount of zero cosines in the DERIVBASE
1.4 extended sample drops by 45% using two-step lemmatization compared to one-step TreeTagger
lemmatization.
3.3 Conceptual Considerations
In addition to the frequency considerations discussed above, we find three conceptual phenomena that
affect distributional similarity independently of the frequency aspects.
The first one is the influence of parts of speech. Derivational rules often change the part of speech of the
input lemma, and the parts of speech of its context words change as well. This decreases context overlap.
For example,
?
Ubersch?atzung
N
? ?ubersch?atzt
A
(overestimate ? overestimated) is assigned a cosine of
merely 0.09. The upper half of Table 3 shows the top ten individual and shared context words for this
pair, ranked by LMI. The context words of the noun are mainly nominal heads of genitive complements
(overestimation of possibility/force/. . . ), while the context words of the adjective comprise many adverbs
(totally, widely, . . . ). None of the shared contexts rank among of the top ten for both target lemmas. This
is even more surprising considering that German adjectives and adverbs have the same surface realization
(as opposed to English) and are more likely to form matching context words.
The second phenomenon that we identified as influencing semantic similarity is markedness (Battistella,
1996). A considerable number of derivational rules systematically produce marked terms. A striking
example is the feminine suffix ?-in? as in Entertainer
N
? Entertainerin
N
: Although the lemmas are
intuitively very similar, their cosine is as low as 0.1. The reason is that the female versions tend to be
used in contexts where the gender of the entertainer is relevant. This is illustrated in the lower half of
Table 3. The first two contexts for both words (actor, singer) stem from frequent enumerations (actor and
entertainer X) and are almost identical, but again the female versions are marked for gender. We also find
two female given names. As a result, the target lemmas receive a low distributional similarity.
The third example are cases of mild meaning shifts that were tagged by the annotators as R. These
are lemmas where the semantic relatedness is intuitively clearly recognizable but may be accompanied
by pretty substantial changes in the distribution of contexts. Consider the semantically related pair
Absteiger
N
? absteigend
A
(descender (person) ? descending/decreasing). It achieves only a cosine of
1732
word pair (l
1
, l
2
) context(l
1
) context(l
2
) shared contexts(l
1
, l
2
)
?
Ubersch
?
atzung ?
?ubersch
?
atzt
(overestimation ?
overestimated),
cos = 0.09
eigen (own) v?ollig (totally) v?ollig (totally)
warnen (to alert) Problem (problem) M?oglichkeit (possibility)
M?oglichkeit (possibility) Gefahr (danger) Bedeutung (meaning)
f?uhren (to lead) Autor (author) Gefahr (danger)
Kraft (force) weit (widely) Einflu? (influence)
Bedeutung (meaning) total (totally) ?uberh?oht (excessive)
F?ahigkeit (ability) ernst (seriously) Macht (power)
Leistungsf?ahigkeit (performance) ?uberh?oht (excessive) gnadenlos (mercilessly)
neigen (to tend) gnadenlos (mercilessly) Kraft (force)
Einflu? (influence) Hollywood (Hollywood) h?aufig (frequent)
Entertainer ?
Entertainerin
(entertainer ? female
entertainer),
cos = 0.1
S?anger (singer) S?angerin (female singer) Schauspieler (actor)
Schauspieler (actor) Schauspielerin (actress) Musiker (musician)
Musiker (musician) Helga (female given name) Talent (talent)
Harald (male given name) Mutter (mother) bekannt (well-known)
Moderator (anchorman) ber?uhmt (famous) S?angerin (female singer)
Schmidt (surname) brillant (brilliant) beliebt (popular)
gro? (big) Lisa (female given name) gro? (big)
K?unstler (artist) K?unstlerin (female artist) ber?uhmt (famous)
Talent (talent) verstorben (deceased) Sportler (sportsman)
gut (good) Talent (talent) Schauspielerin (actress)
Table 3: Top ten individual and shared context words for
?
Ubersch?atzung
N
? ?ubersch?atzt
A
(overestimation
? overestimated) and Entertainer
N
? Entertainerin
N
. Individual context words are ranked by LMI, shared
context words by the product of their LMIs for the two target words. Shared context words that occur in
the top ten contexts for both words are marked in boldface.
0.005, because Absteiger is almost exclusively used to refer to relegated sport teams while absteigend is
used as a general verb of scalar change.
3.4 Ranking of Distributional Information
Given the results reported above, the standard distributional approach of using plain cosine scores to
measure the absolute amount of co-occurrences does not seem very promising, due to the low absolute
numbers of shared dimensions of the two lemmas. We expect other similarity measures, e.g., the Lin
measure (Lin, 1998), to perform equally poorly since they do not change the fundamental approach. Also,
although using a large corpus for semantic space construction might ameliorate the situation, we would
prefer to make improvements on the modeling side of semantic validation.
We follow the ideas of Hare et al. (2009) and Lapesa and Evert (2013) who propose to consider semantic
similarity in terms of ranks rather than absolute values. The advantage of rank-based similarity is that
it takes the density of regions in the semantic space into account. That is, a low cosine value does not
necessarily indicate low semantic relatedness ? provided that the two words are located in a ?sparse?
region. Conversely, a high cosine value can be meaningless in a densely populated region. A second
conceptual benefit of rank-based similarity is that it is directed: It is possible to distinguish the ?forward?
rank (the rank of l
1
in the neighborhood of l
2
) and the ?backward? rank (the rank of l
2
in the neighborhood
of l
1
). The previous studies found rank-based similarity to be beneficial for the prediction of priming
results. In our case, it suggests a refined version of our Hypothesis 1:
Hypothesis 1?. High rank-based distributional similarity indicates semantic relatedness between deriva-
tionally related words.
4 Analysis 2: Derivational Rules for Semantic Validation
As discussed in Section 2.3, a second source of information that should be able to complement the
problematic distributional similarity is provided by the derivational rules that are encoded in DERIVBASE
(cf. the arrows in Figure 1). Our intuition is that some rules are ?semantically stable?, meaning that they
reliably connect semantically similar lemmas, while other rules tend to cause semantic drifts. To examine
1733
this situation, we perform a qualitative analysis on all lemma pairs connected by rule paths of length one
(?simplex paths?), which are easy to analyze. Longer paths (?complex paths?) are considered below.
We find that rules indeed behave differently. For example, the ?-in? female marking rule from Section 3.3
is very reliable: every lemma pair connected by this rule is semantically related. At the other end of the
scale, there are rules that consistently lead to semantically unrelated lemmas, e.g., the ?ver-? noun-verb
prefixation: Zweifel
N
? verzweifeln
V
(doubt ? to despair). Foreign suffixes like ?-ktiv? in instruieren
V
? instruktiv
A
(to instruct ? instructive) retain semantic relatedness in most cases, but sometimes link
actually unrelated lemmas (N, C, L). For example, Objektiv
N
? Objektivismus
N
(lens ? objectivism),
is an N pair for the suffix ?-ismus?. Finally, zero derivations and very short suffixes are less reliable:
Since they easily match, they are often applied to incorrectly lemmatized words (L). For example, the
?-n? suffix, which relates nationalities with countries (Schwede
N
? Schweden
N
(Swede ? Sweden)). It
matches many wrongly lemmatized nouns due to its syncretism with the plural dative/accusative suffix -n,
as in Schweineschnitzel
N
? Schweineschnitzeln
N
(pork cutlet ? pork cutlets
dat/acc-pl
). This suggests that
rule-specific reliability is a promising feature for semantic validation. Fortunately, due to its construction,
DERIVBASE provides a rule chain for each lemma pair so that these reliabilities can be ?read off?. For
other rules, however, the variance of the individual lemma pairs that instantiate the rule is large, and the
applicability of the rule is influenced by the particular combination of rule and lemma pair. Such cases
suggest that distributional knowledge and structural rule information should be combined, a direction that
we will pursue in the next section.
On word pairs that are linked by ?complex paths?, i.e., more than one rule (lachen
V
? l?acherlich
A
in
Figure 1), our main observation in this respect is that rule paths show a clear ?weakest link? property. One
unreliable rule can be sufficient to cause a semantic drift, and only a sequence of reliable rules is likely to
link two semantically related words. We will act on this observation in the next section.
5 A Machine Learning Model for Semantic Validation
5.1 Classification
The findings of our analyses suggest that the decision to classify lemma pairs as semantically related
or unrelated can draw on a range of considerations. We therefore decided to adopt a machine learning
approach and phrase semantic validation as a binary classification task, using the analyses we performed
in Sections 3 and 4 as motivation for feature definition.
We train a classifier on the development portion of the DERIVBASE 1.4 extended sample (1,780
training instances, cf. Section 2.2). We learn a binary decision: Semantic relatedness (R) vs. non-semantic
relatedness (M, N, C, L) within derivationally related pairs. For classification, we use a nonlinear model:
Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel. Using the RBF kernel allows
us to capture the non-linear dependencies between the features.
4
We rely on LIBSVM (Chang and Lin,
2011), a well-known SVM implementation. We optimize the C and ? hyperparameters of the SVM model
using 3-fold cross-validation on the training data (i.e., the development portion of the extended sample).
5.2 Features
Our analyses motivate three feature groups comprising 35 individual features: Distributional, derivation
rule-based (?structural?), and hybrid features. Table 4 gives a list.
Distributional features. All distributional features apply to the lemma or pair level. They are calculated
from our BOW model with permissive lemmatization (Section 3.1). We use absolute and rank-based cosine
similarity (Section 3.4) as well as the number of shared contexts (computed with LMI, cf. Section 3.3)
and lemma frequency. To speed up processing, we compute the forward rank similarity for a lemma pair
(l
1
, l
2
) not on the complete vocabulary but by pairing l
1
with a random sample of 1,000 lemmas from
DERIVBASE (plus l
2
if it is not included). We do the computation analogously for the backward rank.
4
The nonlinear SVM model outperforms a linear SVM. The difference is 0.8% F-Score, statistically significant at p=0.05.
1734
Feature group Type Feature name Description
(# features) (# features)
Distributio- l Lemma frequency (2) Normalized SDeWaC corpus lemma frequencies
nal (6) p Cosine similarity Standard cosine lemma similarity
p Dimensions shared Number of shared context words
p Cos. rank similarity (2) Rank-based forward and backward similarity
Structural (25) r Rule identity (11) Indicator features for the top ten rules in the dev
set + one aggregate feature for the rest
r Rule reliability Percentage of rule applications on R pairs among
all applications of the rule in dev set
r Rule frequency rank (2) Rank-based rule frequency in DERIVBASE
r Avg. string distance (2) Avg. Levenshtein distance for all rule instances
p POS combinations (6) Indicator features for lemma POS combinations
p Path length Length of the shortest path between the lemmas
p String distance (2) Dice bigram coefficient; Levenshtein distance
Hybrid (4) r Average rank sim (2) Frequency-weighted average rank similarity of
rules on shortest path
p Rank sim deviation (2) Difference between lemma pair rank similarity
and average rule rank similarity
Table 4: Features used to characterize derivationally related lemma pairs. ?Type? indicates the level at
which each feature applies: l lemma level, p pair level, r rule level.
Structural features. The structural features encode properties of the rules and paths in DERIVBASE.
Most features apply to the level of derivation rules. This includes the identity of the rule; its reliability
(estimated as the ratio of its application on R pairs among all its applications on the dev set); its frequency
rank among all rules (as a measure of specificity)
5
; and the average Levenshtein distance between the
input and output lemmas (estimating rule complexity by measuring the amount of string modification).
For lemma pairs linked by complex paths (i.e., more than one rule, cf. Figure 1), the question arises
how the rule-level features should be computed. Following our observations on ?weakest link? behavior
in Section 4, we always combine the feature values for the individual rules adopting the most pessimistic
combination function (e.g., minimum in the case of reliability, maximum in the case of frequency rank).
Three more structural features are computed directly at the lemma pair level: their part of speech
combination (e.g., ?NV? for oxide
N
? oxidate
V
), the length of the shortest path connecting them, and the
Levenshtein and Dice string distances between the two lemmas.
Hybrid features. Hybrid features combine rule-based and distributional information to avoid their
respective shortcomings. We work with two hybrid features, one at rule level and one at pair level. The
rule-level feature models the reliability of the rule. It is the average rank similarity for each rule (computed
as a log frequency-weighted average over rule instances). This feature is a counterpart to rule reliability
that is unsupervised in that it does not require class labels. We compute it by randomly drawing 200
lemma pairs for each rule from DERIVBASE (less if the rule has fewer instances). The pair-level feature
is the difference between the rule?s average rank similarity and the rank similarity for the current pair. It
measures the rank of a pair relative to the rule?s ?baseline? rank and indicates how similar and dissimilar
lemma pairs are compared to the rule average. In parallel to the structural features, values for complex
rule paths are computed by minimum. Since the rank similarity is directional, we compute both hybrid
features in two variants, one for each direction.
6
5
We compute this feature once only on simplex paths and once on all instances of the rule in DERIVBASE, trading reliability
against noise.
6
We also tested hybrid features based on raw cosine; however, this yielded worse results than the rank-based hybrid features.
1735
Validation method Precision Recall F
1
Accuracy
Majority baseline 72.6 100 84.1 72.6
Classifier, only ?cosine similarity? feature 72.6 100 84.1 72.6
Classifier only ?similarity rank? feature 80.3 90.3 85.0 76.8
Classifier, only ?rule identity? feature 73.7 99.5 84.6 73.8
Classifier, hybrid group 80.4 95.3 87.2 79.7
Classifier, distributional group 80.5 96.6 87.8 80.5
Classifier, structural group 82.7 93.1 87.6 80.9
Classifier, hybrid + distributional groups 82.6 93.3 87.6 80.9
Classifier, hybrid + structural groups 84.9 93.7 89.1 83.4
Classifier, distributional + structural groups 85.3 94.6 89.7 84.3
Classifier, all features 86.2 93.9 89.9 84.7
Table 5: Accuracy, precision, recall, and F
1
on the test portion of the DERIVBASE 1.4 extended sample.
5.3 Results and Discussion
We applied the trained classifier to the test portion of the DERIVBASE 1.4 extended sample (cf. Section 2.2).
Table 5 summarizes precision, recall, and F
1
-score of the classifier for various combinations of features
and feature groups. Recall that since our motivation is semantic validation, i.e., the removal of false
positives, we are in particular interested in improving the precision of our predictions. We test significance
of F
1
differences among models with bootstrap resampling (Efron and Tibshirani, 1993).
Our baseline is the majority class in the sample, R. Due to the sample?s skewed class distribution (cf.
Table 2), the frequency baseline is quite high (precision 72.6, F
1
-score 84.1). We next consider the three
most prominent individual features: Distributional similarity measured as cosine, distributional similarity
measured as similarity rank, and rule identity. As expected from our analyses, the cosine similarity on
its own is not reliable; in fact, it performs at baseline level. The rank-based similarity already leads to a
considerable gain (precision +7.7%), but only a slight F
1
-score increase of 0.9% that is not statistically
significant at p=0.05. These results provide good empirical evidence for Hypothesis 1? (Section 3.4)
and underscore that 1? is a more accurate statement than Hypothesis 1 (Section 2.3). On the structural
side, rule identity alone improves the precision by 1.1%, with an F
1
-score increase in 0.5% (again not
significant).
We now proceed to complete feature groups, all of which perform at least 3% F
1
-score better than the
baseline, proving that the features within these groups are complementary. The hybrid feature group is the
worst among the three. The distributional feature group is able to improve only slightly over the individual
rank-based similarity feature in precision (80.5 vs. 80.3), but gains 6.3% in recall. This is sufficient for
a significant improvement in F
1
(+3.7%, significant at p=0.01). The structural feature group performs
surprisingly well, given that these features are very simple and most are computed only on the relatively
small training set. It yields by far the highest precision (82.7), and its F
1
-score is only slightly lower than
the one of the distributional group (87.6 vs. 87.8). We take this as further evidence for the usefulness of
structural information, as expressed by Hypothesis 2 (cf. Section 2.3).
Ultimately, all three feature groups turn out to be complementary. We obtain an improvement in
F
1
-score for two out of the three feature group combinations, and a clear improvement in precision in all
cases. Finally, the best overall result is shown by the combination of all three feature groups. It attains an
F
1
-score of 89.9, an improvement of 5.8% over the baseline and 2.1% over the best feature group (both
differences significant at p=0.01). Crucially, this model gains over 13% in precision while losing only 6%
of recall compared to the baseline. This corresponds to a reduction of false positives in the sample by
about half (from 27% to 14%) while the true positives were reduced only by 5% (from 73% to 68%).
Table 6 shows a breakdown of the predictions by the best model in terms of the five gold standard classes
1736
R M N L C total
Gold annotation 554 81 81 45 2 763
Classified as R 520 36 16 29 2 603
Classified as not R 34 45 65 16 0 160
Table 6: Predictions on the test set of the all features Classifier per annotation class.
(R, M, N, L, C). Ignoring compounds (C), of which there are too few cases to analyze, we first find that
the classifier achieves a high R recall. It is also very good in filtering out unrelated cases (N), of which it
discards around 80%. The model recognizes morphologically but not semantically related word pairs (M)
fairly well and manages to remove more than half of these. It has the hardest time with lemmatization
errors (L), of which only about 35% were removed. However, this is not surprising: Lemmatization errors
do not form a coherent category that would be easy to retrieve with the kinds of features that we have
developed. We believe that such errors should be handled in an earlier stage, i.e., during preprocessing.
6 Related Work
Given that many derivational lexicons were only developed in recent years, we are only aware of one study
(Jacquemin, 2010) that semantically validates the output of an existing derivational lexicon (Gaussier,
1999) to apply it to Question Answering. In contrast to our study, it requires elaborate dictionary
information to look up which derivations are permitted for a specific lemma, as well as word sense
disambiguation to determine the meaning of ambiguous words in context. Other related work comes from
two areas: unsupervised morphology induction and semantic clustering.
Unsupervised morphology induction is concerned with the automatic identification of morphological
relations (cf. Hammarstr?om and Borin (2011) for an overview). Most approaches in this area do not differ-
entiate between the inflectional and derivational level of morphology (Gaussier (1999) is an exception)
and restrict themselves to the string level. Only a small number of studies (Schone and Jurafsky, 2000;
Baroni et al., 2002) take distributional information into account.
Semantic clustering is the task of inducing semantic classes from (broadly speaking) distributional
information (Turney and Pantel, 2010; im Walde, 2006). Boleda et al. (2012) include derivational
properties in their feature set to learn Catalan adjective classes. However, the input to such studies is
almost always a set of words from the same part of speech with no prior morphological constraints, while
our input lemmas are morphologically preselected (via derivational rules), are often extremely infrequent,
and exhibit systematical variation in parts of speech. To our knowledge, this challenging situation has not
been addressed in previous studies.
Recent work has also considered the opposite problem, namely using derivational morphology for
improving distributional similarity predictions. Luong et al. (2013) use recursive neural networks to learn
representations of morphologically complex words and demonstrate the usefulness of their approach
on word similarity tasks across different datasets. Similarly, Lazaridou et al. (2013) improve the word
representations of derivationally related words by composing vector space representations of stems and
derivational suffixes.
7 Conclusions
Almost all existing derivational lexicons do not distinguish between only morphologically related words
on one hand and words that are both morphologically and semantically related words on the other hand.
In this paper, we have addressed the task of recovering this distinction and called it semantic validation.
We have used DERIVBASE, a German derivation lexicon, as the basis of our investigation.
We have made two contributions: (a) providing a detailed analysis of the types of information available
for this task (distributional similarity as well as structural information about derivation rules) and the prob-
lems associated with each information type; and (b) training a machine learning classifier on linguistically
1737
motivated features. The classifier, although not perfect, can substantially improve the precision of the
word pairs in DERIVBASE and thus help to filter the derivational families in the lexicon. We are making
this semantic validation information available in the DERIVBASE lexicon by attaching a probability for
the class R to each lemma pair (see footnote 1 for the DERIVBASE URL).
The approach that we have described should transfer straightforwardly to other derivational lexicons
and other languages on the conceptual level. The practical requirements are an appropriate corpus (for the
distributional features) and derivational rule information (for the structural features).
There are two clear directions for future work. First, we plan to broaden our attention from word pairs
to clusters and use the relatedness probabilities to cluster the derivational families in DERIVBASE into
semantically coherent subfamilies. Second, we will demonstrate the impact of semantic validation on
applications of derivational knowledge such as derivation-driven smoothing of distributional models (Pad?o
et al., 2013).
Acknowledgments. We gratefully acknowledge partial funding by the European Commission (project
EXCITEMENT (FP7 ICT-287923), first and second authors) as well as the Croatian Science Foundation
(project 02.03/162: ?Derivational Semantic Models for Information Retrieval?, third author). We thank
the reviewers for their valuable feedback.
References
Harald R. Baayen, Richard Piepenbrock, and Leon Gulikers. 1996. The CELEX Lexical Database. Release 2.
LDC96L14. Linguistic Data Consortium, University of Pennsylvania, Philadelphia, Pennsylvania.
Marco Baroni, Johannes Matiasek, and Harald Trost. 2002. Unsupervised Discovery of Morphologically Related
Words Based on Orthographic and Semantic Similarity. Computing Research Repository, cs.CL/0205006.
Edwin L. Battistella. 1996. The Logic of Markedness. Oxford University Press.
Orhan Bilgin, Ozlem C?etino?glu, and Kemal Oflazer. 2004. Morphosemantic relations in and across Wordnets. In
Proceedings of the Global Wordnet Conference, pages 60?66, Brno, Czech Republic.
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the
International Conference on Computational Linguistics, pages 89?97, Beijing, China.
Gemma Boleda, Sabine Schulte im Walde, and Toni Badia. 2012. Modeling Regular Polysemy: A Study on the
Semantic Classification of Catalan Adjectives. Computational Linguistics, 38(3):575?616.
John A. Bullinaria and Joe P. Levy. 2007. Extracting Semantic Representations from Word Co-occurrence Statis-
tics: A Computational Study. Behavior Research Methods, 39(3):510?526.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions
on Intelligent Systems Technology, 2(3):27:1?27:27.
Bradley Efron and Robert J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman and Hall, New York.
Stefan Evert. 2005. The Statistics of Word Cooccurrences Word Pairs and Collocations. Ph.D. thesis, University
of Stuttgart.
Gertrud Faa?, Ulrich Heid, and Helmut Schmid. 2010. Design and Application of a Gold Standard for Morpholog-
ical Analysis: SMOR in Validation. In Proceedings of the Conference on Language Resources and Evaluation,
pages 803?810, Valletta, Malta.
Christiane Fellbaum, Anne Osherson, and Peter Clark. 2009. Putting semantics into WordNet?s ?morphosemantic?
links. In Proceedings of Human Language Technology. Challenges of the Information Society, pages 350?358,
Pozna?n, Poland.
?
Eric Gaussier. 1999. Unsupervised learning of derivational morphology from inflectional lexicons. In ACL
Workshop Proceedings on Unsupervised Learning in Natural Language Processing, pages 24?30, College Park,
Maryland.
Nizar Habash and Bonnie Dorr. 2003. A categorial variation database for English. In Proceedings of the North
American Association for Computational Linguistics, pages 96?102, Edmonton, Canada.
1738
Harald Hammarstr?om and Lars Borin. 2011. Unsupervised Learning of Morphology. Computational Linguistics,
37(2):309?350.
Mary Hare, Michael Jones, Caroline Thomson, Sarah Kelly, and Ken McRae. 2009. Activating Event Knowledge.
Cognition, 111(2):151?167.
Sabine Schulte im Walde. 2006. Experiments on the Automatic Induction of German Semantic Verb Classes.
Computational Linguistics, 32(2):159?194.
Bernard Jacquemin. 2010. A derivational rephrasing experiment for question answering. In Proceedings of the
Conference on Language Resources and Evaluation, pages 2380?2387, Valletta, Malta.
Lauri Karttunen and Kenneth R. Beesley. 2005. Twenty-five Years of Finite-state Morphology. In Inquiries into
Words, Constraints and Contexts. Festschrift for Kimmo Koskenniemi on his 60th Birthday, pages 71?83. CSLI
Publications, Stanford, California.
Gabriella Lapesa and Stefan Evert. 2013. Evaluating neighbor rank and distance measures as predictors of se-
mantic priming. In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages
66?74, Sofia, Bulgaria.
Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli, and Marco Baroni. 2013. Compositional-ly derived rep-
resentations of morphologically complex words in distributional semantics. In Proceedings of the Association
for Computational Linguistics, pages 1517?1526, Sofia, Bulgaria.
Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the International Confer-
ence on Machine Learning, ICML, pages 296?304, San Francisco, California.
Minh-Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better word representations with recur-
sive neural networks for morphology. In Proceedings of the Conference on Natural Language Learning, pages
104?113, Sofia, Bulgaria.
Sebastian Pad?o, Jan
?
Snajder, and Britta Zeller. 2013. Derivational smoothing for syntactic distributional semantics.
In Proceedings of the Association for Computational Linguistics, pages 731?735, Sofia, Bulgaria.
Karel Pala and Dana Hlav?a?ckov?a. 2007. Derivational relations in Czech wordnet. In Proceedings of the Workshop
on Balto-Slavonic Natural Language Processing: Information Extraction and Enabling Technologies, pages
75?81, Prague, Czech Republic.
Martin Porter. 1980. An Algorithm for Suffix Stripping. Program, 14(3):130?137.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the Interna-
tional Conference on Natural Language Processing, Manchester, UK.
Patrick Schone and Daniel Jurafsky. 2000. Knowledge-free induction of morphology using latent semantic analy-
sis. In Proceedings of the Conference on Natural Language Learning, pages 67?72. Lisbon, Portugal.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011. A probabilistic modeling framework for lexical entailment.
In Proceedings of the Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, pages 558?563, Portland, Oregon.
Jan
?
Snajder and Bojana Dalbelo Ba?si?c. 2010. A computational model of Croatian derivational morphology. In
Proceedings of the International Conference on Formal Approaches to South Slavic and Balkan Languages,
pages 109?118, Dubrovnik, Croatia.
Pavol
?
Stekauer and Rochelle Lieber, editors. 2005. Handbook of Word-Formation, volume 64 of Studies in Natural
Language and Linguistic Theory. Springer.
Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: Vector Space Models of Semantics.
Journal of Artificial Intelligence Research, 37(1):141?188.
Britta Zeller, Jan
?
Snajder, and Sebastian Pad?o. 2013. DErivBase: Inducing and evaluating a derivational mor-
phology resource for German. In Proceedings of the Annual Meeting of the Association for Computational
Linguistics, pages 1201?1211, Sofia, Bulgaria.
1739
Proceedings of the First Workshop on Argumentation Mining, pages 49?58,
Baltimore, Maryland USA, June 26, 2014.
c
?2014 Association for Computational Linguistics
Back up your Stance: Recognizing Arguments in Online Discussions
Filip Boltu?zi
?
c and Jan
?
Snajder
University of Zagreb
Faculty of Electrical Engineering and Computing
Text Analysis and Knowledge Engineering Lab
Unska 3, 10000 Zagreb, Croatia
{filip.boltuzic,jan.snajder}@fer.hr
Abstract
In online discussions, users often back up
their stance with arguments. Their argu-
ments are often vague, implicit, and poorly
worded, yet they provide valuable insights
into reasons underpinning users? opinions.
In this paper, we make a first step towards
argument-based opinion mining from on-
line discussions and introduce a new task
of argument recognition. We match user-
created comments to a set of predefined
topic-based arguments, which can be either
attacked or supported in the comment. We
present a manually-annotated corpus for ar-
gument recognition in online discussions.
We describe a supervised model based on
comment-argument similarity and entail-
ment features. Depending on problem for-
mulation, model performance ranges from
70.5% to 81.8% F1-score, and decreases
only marginally when applied to an unseen
topic.
1 Introduction
Whether about coffee preparation, music taste, or
legal cases in courtrooms, arguing has always been
the dominant way of rationalizing opinions. An
argument consists of one or more premises lead-
ing to exactly one conclusion, while argumentation
connects together several arguments (Van Eemeren
et al., 2013). Across domains, argumentation dif-
fers in vocabulary, style, and purpose, ranging from
legal (Walton, 2005) and scientific argumentation
(Jim?enez-Aleixandre and Erduran, 2007) to media
(Walton, 2007) and social argumentation (Shum,
2008). When argumentation involves interactive
argument exchange with elements of persuasion,
we talk about debating. In the increasingly popular
online debates, such as VBATES,
1
users can en-
1
http://vbate.idebate.org/
gage in debates over controversial topics, introduce
new arguments or use existing ones.
Early computational approaches to argumenta-
tion have developed in two branches: logic-based
approaches (Bos and Gabsdil, 2000; Lauriar et al.,
2001) and argumentative zoning (Teufel and others,
2000). The latter aims to recognize argumentative
sections of specific purpose in scientific papers,
such as goals, related work, or conclusion. Moens
et al. (2007) introduced argumentation mining as
a research area involved with the automatic ex-
traction of argumentation structure from free text,
residing between NLP, argumentation theory, and
information retrieval.
Prior work in argumentation mining has focused
on official documents, such as legal cases (Palau
and Moens, 2009), or moderated sources, such as
debates (Cabrio and Villata, 2012). However, by
far the largest source of opinions are online user
discussions: comments on newspaper articles, so-
cial networks, blogs, and discussion forums ? all
argumentation arenas without strict rules. Despite
the fact that the user-generated content is not mod-
erated nor structured, one can often find an abun-
dance of opinions, most of them backed up with
arguments. By analyzing such arguments, we can
gain valuable insight into the reasons underpinning
users? opinions. Understanding the reasons has
obvious benefits in social opinion mining, with ap-
plications ranging from brand analysis to political
opinion mining.
Inspired by this idea, in this paper we take on
the task of argument-based opinion mining. In-
stead of merely determining the general opinion or
stance of users towards a given topic, in argument-
based opinion mining we wish to determine the
arguments on which the users base their stance.
Unlike in argumentation mining, we are not ulti-
mately interested in recovering the argumentation
structure. Instead, we wish to recognize what ar-
guments the user has used to back up her opinion.
49
As an example, consider a discussion on the topic
?Should gay marriage be legal?? and the following
comment:
Gay marriages must be legal in all 50
states. A marriage is covenant between
2 people regardless of their genders. Dis-
crimination against gay marriage is un-
constitutional and biased. Tolerance,
education and social justice make our
world a better place.
This comment supports the argument ?It is discrim-
inatory to refuse gay couples the right to marry?
and denies the argument ?Marriage should be be-
tween a man and a woman?. The technical chal-
lenge here lies in the fact that, unlike in debates
or other more formal argumentation sources, the
arguments provided by the users, if any, are less
formal, ambiguous, vague, implicit, or often simply
poorly worded.
In this paper, we make a first step towards
argument-based opinion mining from online dis-
cussions and introduce the task of argument recog-
nition. We define this task as identifying what
arguments, from a predefined set of arguments,
have been used in users? comments, and how. We
assume that a topic-dependent set of arguments
has been prepared in advance. Each argument is
described with a single phrase or a sentence. To
back up her stance, the user will typically use one
or more of the predefined arguments, but in their
own wording and with varying degree of explicit-
ness. The task of argument recognition amounts to
matching these arguments to the predefined argu-
ments, which can be either attacked or supported
by the comment. Note that the user?s comment
may by itself be a single argument. However, we
refer to it as comment to emphasize the fact that in
general it may contain several arguments as well as
non-argumentative text.
The contribution of our work is twofold. First,
we present COMARG, a manually-annotated cor-
pus for argument recognition from online discus-
sions, which we make freely available. Secondly,
we describe a supervised model for argument recog-
nition based on comment-argument comparison.
To address the fact that the arguments expressed in
user comments are mostly vague and implicit, we
use a series of semantic comment-argument com-
parison features based on semantic textual similar-
ity (STS) and textual entailment (TE). To this end,
we rely on state-of-the-art off-the-shelf STS and TE
systems. We consider different feature subsets and
argument recognition tasks of varying difficulty.
Depending on task formulation, their performance
ranges from 70.5% to 81.8% micro-averaged F1-
score. Taking into account the difficulty of the task,
we believe these results are promising. In partic-
ular, we show that TE features work best when
also taking into account the stance of the argument,
and that a classifier trained to recognize arguments
in one topic can be applied to another one with a
decrease in performance of less than 3% F1-score.
The rest of the paper is structured as follows. In
the next section we review the related work. In Sec-
tion 3 we describe the construction and annotation
of the COMARG corpus. Section 4 describes the
argument recognition model. In Section 5 we dis-
cuss the experimental results. Section 6 concludes
the paper and outlines future work.
2 Related Work
Argument-based opinion mining is closely related
to argumentation mining, stance classification, and
opinion mining.
Palau and Moens (2009) approach argumenta-
tion mining in three steps: (1) argument identi-
fication (determining whether a sentence is argu-
mentative), (2) argument proposition classification
(categorize argumentative sentences as premises or
conclusions), and (3) detection of argumentation
structure or ?argumentative parsing? (determining
the relations between the arguments). The focus
of their work is on legal text: the Araucaria cor-
pus (Reed et al., 2008) and documents from the
European Court of Human Rights.
More recently, Cabrio and Villata (2012) ex-
plored the use of textual entailment for building
argumentation networks and determining the ac-
ceptability of arguments. Textual entailment (TE)
is a generic NLP framework for recognizing in-
ference relations between two natural language
texts (Dagan et al., 2006). Cabrio and Villata base
their approach on Dung?s argumentation theory
(Dung, 1995) and apply it to arguments from on-
line debates. After linking the arguments with sup-
port/attack relations using TE, they are able to com-
pute a set of acceptable arguments. Their system
helps the participants to get an overview of a debate
and the accepted arguments.
Our work differs from the above-described work
in that we do not aim to extract the argumenta-
50
tion structure. Similarly to Cabrio and Villata
(2012), we use TE as one of the features of our
system to recognize the well-established arguments
in user generated comments. However, aiming at
argument-based opinion mining from noisy com-
ments, we address a more general problem in which
each comment may contain several arguments as
well as non-argumentative text. Thus, in contrast
to Cabrio and Villata (2012) who framed the prob-
lem as a binary yes/no entailment task, we tackle
a more difficult five-class classification problem.
We believe this is a more realistic task from the
perspective of opinion mining.
A task similar to argument recognition is that
of stance classification, which involves identifying
a subjective disposition towards a particular topic
(Lin et al., 2006; Malouf and Mullen, 2008; So-
masundaran and Wiebe, 2010; Anand et al., 2011;
Hasan and Ng, 2013). Anand et al. (2011) classi-
fied stance on a corpus of posts across a wide range
of topics. They analyzed the usefulness of meta-
post features, contextual features, dependency fea-
tures, and word-based features for signaling dis-
agreement. Their results range from 54% to 69%
accuracy. Murakami and Raymond (2010) iden-
tify general user opinions in online debates. They
distinguish between global positions (opinions on
the topic) and local positions (opinions on previ-
ous remarks). By calculating user pairwise rates
of agreement and disagreement, users are grouped
into ?support? and ?oppose? sets.
In contrast to stance classification, argument
recognition aims to uncover the reasons underly-
ing an opinion. This relates to the well-established
area of opinion mining. The main goal of opinion
mining or sentiment analysis (Pang and Lee, 2008)
is to analyze the opinions and emotions from (most
often user-created) text. Opinions are often asso-
ciated with user reviews (Kobayashi et al., 2007),
unlike stances, which are more common for de-
bates. Hasan and Ng (2013) characterize stance
recognition as a more difficult task than opinion
mining. Recently, however, there has been interest-
ing work on combining argumentation mining and
opinion mining (Ches?nevar et al., 2013; Grosse et
al., 2012; Hogenboom et al., 2010).
3 COMARG Corpus
For training and evaluating argument recognition
models, we have compiled a corpus of user com-
ments, manually annotated with arguments, to
which we refer as COMARG. The COMARG cor-
pus is freely available for research purposes.
2
3.1 Data Description
As a source of data, we use two web sites: Pro-
con.org
3
and Idebate.org.
4
The former is a discus-
sion site covering ideological, social, political, and
other topics. Users express their personal opinions
on a selected topic, taking either the pro or con
side. Idebate.org is a debating website containing
online debates and an archive of past debates. Each
archived topic contains a set of prominent argu-
ments presented in the debate. Each argument is
labeled as either for or against the topic. The argu-
ments are moderated and edited to provide the best
quality of information.
The two data sources are complementary to each
other: Procon.org contains user comments, while
Idebate.org contains the arguments. We manually
identified near-identical topics covered by both web
sites. From this set, we chose two topics: ?Un-
der God in Pledge? (UGIP) and ?Gay Marriage?
(GM). We chose these two topics because they have
a larger-than-average number of comments (above
300) and are well-balanced between pro and con
stances. For these two topics, we then took the
corresponding comments and arguments from Pro-
con.org and Idebate.org, respectively. As the users
can post comments not relevant for the topic, we
skim-read the comments and removed the spam.
We end up with a set of 175 comments and 6 argu-
ments for the UGIP topic, and 198 comments and
7 arguments for the GM topic. The comments are
often verbose: the average number of words per
comment is 116. This is in contrast to the less noisy
dataset from Cabrio and Villata (2012), where the
average comment length is 50 words.
Each comment has an associated stance (pro or
con), depending on how it was classified in Pro-
con.org. Similarly, each argument either attacks or
supports the claim of the topic, depending on how
it was classified in Idebate.org. To simplify the ex-
position, we will refer to them as ?pro arguments?
and ?con arguments?. Table 1 shows the arguments
for UGIP and GM topics.
Users may attack or support both pro and con
arguments. We will refer to the way how the argu-
ment is used (attacked or supported) as argument
2
Freely available under the CC BY-SA-NC license from
http://takelab.fer.hr/data/comarg
3
http://www.procon.org
4
http://idebate.org
51
?Under God in Pledge? (UGIP): Should the words
?under God? be in the U.S. Pledge of Allegiance?
(A1.1) Likely to be seen as a state sanctioned
condemnation of religion
Pro
(A1.2) The principles of democracy regulate that
the wishes of American Christians, who
are a majority are honored
Pro
(A1.3) Under God is part of American tradition
and history
Pro
(A1.4) Implies ultimate power on the part of the
state
Con
(A1.5) Removing under god would promote reli-
gious tolerance
Con
(A1.6) Separation of state and religion Con
?Gay Marriage? (GM): Should gay marriage be legal?
(A2.1) It is discriminatory to refuse gay couples
the right to marry
Pro
(A2.2) Gay couples should be able to take ad-
vantage of the fiscal and legal benefits of
marriage
Pro
(A2.3) Marriage is about more than procreation,
therefore gay couples should not be de-
nied the right to marry due to their biol-
ogy
Pro
(A2.4) Gay couples can declare their union with-
out resort to marriage
Con
(A2.5) Gay marriage undermines the institution
of marriage, leading to an increase in out
of wedlock births and divorce rates
Con
(A2.6) Major world religions are against gay
marriages
Con
(A2.7) Marriage should be between a man and a
woman
Con
Table 1: Predefined arguments for the two topics in
the COMARG corpus
polarity. Typically, but not necessarily, users who
take the pro stance do so by supporting one of the
pro arguments, and perhaps attacking some of the
con arguments, while for users who take the con
stance it is the other way around.
3.2 Annotation
The next step was to annotate, for each comment,
the arguments used in the comment as well as their
polarity. For each topic we paired all comments
with all possible arguments for that topic, resulting
in 1,050 and 1,386 comment-argument pairs for the
UGIP and GM topics, respectively. We then asked
the annotators (not the authors) to annotate each
pair. The alternative would have been to ask the
annotators to assign arguments to comments, but
we believe annotating pairs reduces the annotation
efforts and improves annotation quality.
5
5
We initially attempted to crowdsource the annotation, but
the task turned out to be too complex for the workers, resulting
in unacceptably low interannotator agreement.
Label Description: Comment. . .
A . . . explicitly attacks the argument
a . . . vaguely/implicitly attacks the argument
N . . . makes no use of the argument
s . . . vaguely/implicitly supports the argument
S . . . explicitly supports the argument
Table 2: Labels for comment-argument pairs in the
COMARG corpus
No, of course not. The original one was good enough. The
insertion of Under God? between ?Our nation? and ?indivis-
ible? is symbolic of how religion divides this country.?
The Pledge of Allegiance reflects our morals and values. There-
fore, it should reflect the ideas of all Americans not 80%. This
country has no national religion, so why should we promote a
god. Also, Thomas Jefferson, a founding father, was athiest.
I believe that since this country was founded under God why
should we take that out of the pledge? Men and women have
fought and gave their lives for this country, so that way we
can have freedom and be able to have God in our lives. And
since this country was founded under God and the Ten Com-
mandments in mind, it needs to stay in. If it offends you well I
am sorry but get out of this country!
Table 3: Example comments with low IAA from
UGIP
Acknowledging the fact that user-provided argu-
ments are often vague or implicit, we decided to
annotate each comment-argument pair using a five-
point scale. The labels are shown in Table 2. The
labels encode the presence/absence of an argument
in a comment, its polarity, as well as the degree of
explicitness.
The annotation was carried out by three trained
annotators, in two steps. In the first step, each anno-
tator independently annotated the complete dataset
of 2,436 comment-argument pairs. To improve
the annotation quality, we singled out the problem-
atic comment-argument pairs. We considered as
problematic all comment-argument pairs for which
(1) there is no agreement among the three annota-
tors or (2) the ordinal distance between any of the
labels assigned by the annotators is greater than
one. Table 3 shows some examples of problematic
comments. As for the arguments, the most prob-
lematic ones are A1.3 and A1.5 for the UGIP topic
and arguments A2.1 and A2.7 for the GM topic
(cf. Table 1).
In the second step, we asked the annotators to
independently revise their decisions for the prob-
lematic comment-argument pairs. Each annotator
re-annotated 515 pairs, of which for 86 the anno-
tations were revised. In total, the annotation and
52
IAA UGIP GM UGIP+GM
Fleiss? Kappa 0.46 0.51 0.49
Cohen?s Kappa 0.46 0.51 0.49
Weighted Kappa 0.45 0.51 0.50
Pearson?s r 0.68 0.74 0.71
Table 4: Interannotator agreement on the
COMARG corpus
Labels
Topic A a N s S Total
UGIP 48 86 691 58 130 1,013
GM 89 73 849 98 176 1,285
UGIP+GM 137 159 1,540 156 306 2,298
Table 5: Distribution of labels in the COMARG
corpus
subsequent revision took about 30 person-hours.
Table 4 shows the interannotator agreement
(IAA). We compute Fleiss? multirater kappa, Co-
hen?s kappa (averaged over three annotator pairs),
Cohen?s linearly weighted kappa (also averaged),
and Pearson?s r. The latter two reflect the fact that
the five labels constitute an ordinal scale. Accord-
ing to standard interpretation (Landis and Koch,
1977), these values indicate moderate agreement,
proving that argument recognition is a difficult task.
Finally, to obtain the the gold standard annota-
tion, we took the majority label for each comment-
argument pair, discarding the pairs for which there
are ties. We ended up with a dataset of 2,249
comment-argument pairs. Table 6 shows examples
of annotated comment-argument pairs.
3.3 Annotation Analysis
Table 5 shows the distribution of comment-
argument pairs across labels. Expectedly, the
majority (67.0%) of comment-argument pairs are
cases in which the argument is not used (label N).
Attacked arguments (labels A or a) make up 12.9%,
while supported arguments (labels S or s) make up
20.1% of cases. Among the cases not labeled as N,
arguments are used explicitly in 58.4% (labels A
and S) and vague/implicit (labels a and s) in 41.5%
of cases. There is a marked difference between the
two topics in this regard: in UGIP, arguments are
explicit in 55.3%, while in GM in 60.7% of cases.
Note that this might be affected by the choice of
the predefined arguments as well as how they are
worded.
The average number of arguments per comment
is 1.9 (1.8 for UGIP and 2.0 for GM). In GM,
62.8% of arguments used are pro arguments, while
in UGIP pro arguments make up 52.2% of cases.
4 Argument Recognition Model
We cast the argument recognition task as a multi-
class classification problem. Given a comment-
argument pair as input, the classifier should predict
the correct label from the set of five possible labels
(cf. Table 2). The main idea is for the classifier to
rely on comment-argument comparison features,
which in principle makes the model less domain
dependent than if we were to use features extracted
directly from the comment or the arguments.
We use three kinds of features: textual entail-
ment (TE) features, semantic text similarity (STS)
features, and one ?stance alignment? (SA) feature.
The latter is a binary feature whose value is set to
one if a pro comment is paired with a pro argument
or if a con comment is paired with a con argument.
This SA feature presupposes that comment stance
is known a priori. The TE and STS features are
described bellow.
4.1 Textual Entailment
Following the work of Cabrio and Villata (2012),
we use textual entailment (TE) to determine
whether the comment (the text) entails the argu-
ment phrase (the hypothesis). To this end we
use the Excitement Open Platform (EOP), a rich
suite of textual entailment tools designed for mod-
ular use (Pad?o et al., 2014). From EOP we
used seven pre-trained entailment decision algo-
rithms (EDAs). Some EDAs contain only syn-
tactical features, whereas others rely on resources
such as WordNet (Fellbaum, 1998) and VerbOcean
(Chklovski and Pantel, 2004). Each EDA outputs
a binary decision (Entailment or NonEntailment)
along with the degree of confidence. We use the
outputs (decisions and confidences) of all seven
EDAs as the features of our classifier (14 features
in total). We also experimented with using ad-
ditional features (the disjunction of all classifier
decisions, the maximum confidence value, and the
mean confidence value), but using these did not
improve the performance.
In principle, we expect the comment text (which
is usually longer) to entail the argument phrase
(which is usually shorter). This is also confirmed
by the ratio of positive entailment decision across
labels (averaged over seven EDAs), shown in
53
Id Comment Argument Label
2.23.4 All these arguments on my left are and have always been FALSE. Marriage
is between a MAN and a WOMAN by divine definition. Sorry but, end of
story.
It is discriminatory to refuse
gay couples the right to
marry.
s
2.111.4 Marriage isn?t the joining of two people who have intentions of raising
and nurturing children. It never has been. There have been many married
couples whos have not had children. (...) If straight couples can attempt to
work out a marriage, why can?t homosexual couple have this same privilege?
(...)
It is discriminatory to refuse
gay couples the right to
marry
s
2.114.2 (...) I truly believe that the powers behind the cause to re-define marriage
stem from a stronger desire to attack a religious institution that does not
support homosexuality, rather than a desire to achieve the same benefits as
marriage for same sex couples. (...)?
Gay couples should be able
to take advantage of the fis-
cal and legal benefits of mar-
riage.
S
2.101.2 (...) One part of marriage is getting benefits from the other. Many married
couples never have children but still get the benefits of marriage, should we
take those benefits away because they don?t have children? Another is the
promise to be with each other for an eternity? etc. Marriage is also about
being able to celebrate having each other. And last, marriage is about being
there for each other. (...)?
Gay couples should be able
to take advantage of the fis-
cal and legal benefits of mar-
riage.
S
2.157.2 (...) There are no legal reasons why two homosexual people should not be
allowed to marry, only religious ones (...)
Gay couples should be able
to take advantage of the fis-
cal and legal benefits of mar-
riage.
N
1.45.2 I am not bothered by under God but by the highfalutin christians that do
not realize that phrase was never in the original pledge - it was not added
until 1954. So stop being so pompous and do not offend my parents and
grandparents who never used ?under God? when they said the pledge. Let it
stay, but know the history of the Cold War and fear of communism.
?Under God? is part of
American tradition and his-
tory.
a
Table 6: Example of comment-argument annotations from the COMARG corpus
A a N s SLabel
0.0
0.2
0.4
0.6
0.8
1.0
Rati
o of
 pos
itive
 ent
ailm
ent 
deci
sion
s (%
)
Figure 1: Ratio of positive entailment decisions
across labels, scaled to a [0, 1] interval
Fig. 1. Pro arguments have a higher ratio of
positive entailment decisions than con arguments.
Also, vaguely/implicitly supported arguments have
a lower rate of entailment decisions than explicitly
supported arguments.
4.2 Semantic Textual Similarity
Formally speaking, the argument should either be
entailed or not entailed from the comment. The
former case also includes a simple argument para-
phrase. In the latter case, the argument may be
contradicted or it may simply be a non sequitur.
While we might expect these relations to be rec-
ognizable in texts from more formal genres, such
as legal documents and parliamentary debates, it
is questionable to what extent these relations can
be detected in user-generated content, where the
arguments are stated vaguely and implicitly.
To account for this, we use a series of argument-
comment comparison features based on semantic
textual similarity (STS). STS measures ?the degree
of semantic equivalence between two texts? (Agirre
et al., 2012). It is a looser notion than TE and, un-
like TE, it is a non-directional (symmetric) relation.
We rely on the freely available TakeLab STS sys-
tem by
?
Sari?c et al. (2012). Given a comment and
an argument, the STS system outputs a continuous
similarity score. We also compute the similarity
between the argument and each sentence from the
comment, which gives us a vector of similarities.
The vector length equals the largest number of sen-
tences in a comment, which in COMARG is 29.
Additionally, we compute the maximum and the
54
A a N s SLabel
0.0
0.2
0.4
0.6
0.8
1.0
Ave
rage
 sco
re
Comment similaritySentence similarity
Figure 2: Average similarity score on sentence
and comment level across labels, scaled to a [0, 1]
interval
mean of sentence-level similarities. In total, we use
31 STS features.
Fig. 2 shows the average comment- and sentence-
level similarity scores across labels on COMARG,
scaled to a [0, 1] interval. Interestingly, attacked
arguments on average receive a larger score than
supported arguments.
5 Experimental Evaluation
5.1 Experimental Setup
We consider three formulations of the argument
detection task. In the first setting (A-a-N-s-S), we
consider the classification of a comment-argument
into one of the five labels, i.e., we wish to determine
whether an argument has been used, its polarity, as
well as the degree of explicitness. In the second
setting (As-N-sS), we conflate the two labels of
equal polarity, thus we only consider whether an
argument has been used and with what polarity.
In the third setting (A-N-S), we only consider the
comment-argument pairs in which arguments are
either not used or used explicitly. This setting is not
practically relevant, but we include it for purposes
of comparison.
We compare to two baselines: (1) a majority
class classifier (MCC), which assigns label N to ev-
ery instance, and (2) a bag-of-words overlap classi-
fier (BoWO), which uses the word overlap between
the comment and the argument as the only feature.
For classification, we use the Support Vector Ma-
chine (SVM) algorithm with a Radial Basis Func-
tion kernel. In each setting, we train and evalu-
ate the model using nested 5?3 cross-validation.
The hyperparameters C and ? of the SVM are op-
timized using grid search. We rely on the well-
A-a-N-s-S Aa-N-sS A-N-S
Model UGIP GM UGIP GM UGIP GM
MCC baseline 68.2 69.4 68.2 69.4 79.5 76.6
BoWO baseline 68.2 69.4 67.8 69.5 79.6 76.9
TE 69.1 81.1 69.6 72.3 80.1 73.4
STS 67.8 68.7 67.3 69.9 79.2 75.8
SA 68.2 69.4 68.2 69.4 79.5 76.6
STS+SA 68.2 69.5 67.5 68.7 79.6 76.1
TE+SA 68.9 72.4 71.0 73.7 81.8 80.3
TE+STS+SA 70.5 72.5 68.9 73.4 81.4 79.7
Table 7: Argument recognition F1-score (separate
models for UGIP and GM topics)
UGIP? GM GM? UGIP
Model A-a-N-s-S Aa-N-sS A-a-N-s-S Aa-N-sS
STS+SA 69.4 69.4 68.2 68.2
TE+SA 72.6 73.5 70.2 71.2
STS+TE+SA 71.5 72.2 68.2 69.6
Table 8: Argument recognition F1-score on UGIP
and GM topics (cross-topic setting)
known LibSVM implementation (Chang and Lin,
2011).
5.2 Results
Table 7 shows the micro-averaged F1-score for the
three problem formulations, for models trained sep-
arately on UGIP and GM topics. The two baselines
perform similarly. The models that use only the
STS or the SA features perform similar to the base-
line. The TE model outperforms the baselines in
all but one setting and on both topics: the differ-
ence ranges from 0.6 to 11.7 percentage points,
depending on problem formulation, while the vari-
ation between the two topics is negligible. The
STS model does not benefit from adding the SA
feature, while the TE model does so in simpler
settings (Aa-N-sS and A-N-S), where the average
F1-scores increases by about 3 percentage points.
This can be explained by referring to Fig. 1, which
shows that even for the attacked arguments (labels
A and a) entailment decisions are sometimes pos-
itive. In such cases, the stance alignment feature
helps to distinguish between entailment (supported
argument) and contradiction (attacked argument).
Combining all three feature types gives the best re-
sults for the A-a-N-s-S setting and the UGIP topic.
The above evaluation was carried out in a within-
topic setting. To test how the models perform when
applied to comments and arguments from unseen
topics, we trained each model on one topic and
55
A-a-N-s-S Aa-N-sS A-N-S
Model P R F1 micro-F1 P R F1 micro-F1 P R F1 micro-F1
MCC baseline 13.8 20.0 16.3 68.9 23.0 33.3 27.2 68.9 26.0 33.3 29.2 77.9
TE+SA 47.6 26.6 27.9 71.1 68.8 46.6 49.4 73.3 66.1 47.3 51.1 81.6
STS+TE+SA 46.3 27.2 28.6 71.6 61.6 43.5 45.5 71.4 63.7 44.9 48.2 80.4
Table 9: Argument recognition F1-score for TE+SA and STS+TE+SA models on UGIP+GM topics
evaluated on the other. The results are shown in
Table 8 (we show results only for the two prob-
lem formulations of practical interest). The dif-
ference in performance is small (0.7 on average).
The best-performing model (TE+SA) does not suf-
fer a decrease in performance. This suggests that
the models are quite topic independent, but a more
detailed study is required to verify this finding.
Finally, we trained and tested the TE+SA and
STS+TE+SA models on the complete COMARG
dataset. The results are shown in Table 9. We
report macro-averaged precision, recall, and F1-
score, as well as micro-averaged F1-score.
6
Gen-
erally, our models perform less well on smaller
classes (A, a, s, and S), hence the macro-averaged
F1-scores are much lower than the micro-averaged
F1-scores. The recall is lower than the precision:
the false negatives are mostly due to our models
wrongly classifying comment-argument pairs as N.
The STS+TE+SA model slightly outperforms the
TE+SA model on the A-a-N-s-S problem, while on
the other problem formulations the TE+SA model
performs best.
5.3 Error Analysis
The vague/implicit arguments posed the greatest
challenge for all models. A case in point is the
comment-argument pair 2.23.4 from Table 6. Judg-
ing solely from the comment text, it is unclear what
the user actually meant. Perhaps the user is attack-
ing the argument, but there are certain additional
assumptions that would need to be met for the ar-
gument to be entailed.
The second major problem is distinguishing be-
tween arguments that are mentioned and those that
are not. Consider the comment-argument pairs
2.111.4 and 2.114.2 from Table 6. In the former
case, classifier mistakenly predicts S instead of s.
The decision is likely due to the low difference
in argument-comment similarities for these two
classes. In the latter example the classifier wrongly
6
We replace undefined values with zeros when computing
the macro-averages.
predicts that the argument is used in the comment.
The TE model in the majority of cases outper-
forms the STS model. Nonetheless, in case of
the comment-argument pair 2.157.2 from Table 6,
the STS-based model outperformed the entailment
model. In this case, the word overlap between the
argument and the comment in quite high, although
they completely differ in meaning. Conversely,
argument-comment 2.101.2 is a good example of
when entailment was correctly recognized, whereas
the STS model has failed.
6 Conclusion
In this paper we addressed the argument recogni-
tion task as a first step towards argument-based
opinion mining from online discussions. We have
presented the COMARG corpus, which consists of
manually annotated comment-argument pairs. On
this corpus we have trained a supervised model
for three argument recognition tasks of varying
difficulty. The model uses textual entailment and
semantic textual similarity features. The exper-
iments as well as the inter-annotator agreement
show that argument recognition is a difficult task.
Our best models outperform the baselines and per-
form in a 70.5% to 81.8% micro-averaged F1-score
range, depending on problem formulation. The
outputs of several entailment decision algorithms,
combined with a stance alignment feature, proved
to be the best features. Additional semantic tex-
tual similarity features seem to be useful in when
we distinguish between vague/implicit and explicit
arguments. The model performance is marginally
affected when applied to an unseen topic.
This paper has only touched the surface of argu-
ment recognition. We plan to extend the COMARG
corpus with more topics and additional annotation,
such as argument segments. Besides experimenting
with different models and feature sets, we intend
to investigate how argument interactions can be ex-
ploited to improve argument recognition, as well as
how argument recognition can be used for stance
classification.
56
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the First Joint Conference on Lexical and Computa-
tional Semantics-Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation, pages 385?393. Association for
Computational Linguistics.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean
E Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis, pages 1?9. Association for
Computational Linguistics.
Johan Bos and Malte Gabsdil. 2000. First-order infer-
ence and the interpretation of questions and answers.
Proceedings of Gotelog, pages 43?50.
Elena Cabrio and Serena Villata. 2012. Combining
textual entailment and argumentation theory for sup-
porting online debates interactions. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers-Volume 2,
pages 208?212. Association for Computational Lin-
guistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology (TIST),
2(3):27.
Carlos I Ches?nevar, Mar??a Paula Gonz?alez, Kathrin
Grosse, and Ana Gabriela Maguitman. 2013. A first
approach to mining opinions as multisets through
argumentation. In Agreement Technologies, pages
195?209. Springer.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In EMNLP, volume 2004, pages 33?40.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine Learning Challenges. Eval-
uating Predictive Uncertainty, Visual Object Classi-
fication, and Recognising Tectual Entailment, pages
177?190. Springer.
Phan Minh Dung. 1995. On the acceptability of ar-
guments and its fundamental role in nonmonotonic
reasoning, logic programming and n-person games.
Artificial Intelligence, 77(2):321?357.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Kathrin Grosse, Carlos Iv?an Ches?nevar, and
Ana Gabriela Maguitman. 2012. An argument-
based approach to mining opinions from Twitter. In
AT, pages 408?422.
Kazi Saidul Hasan and Vincent Ng. 2013. Extra-
linguistic constraints on stance recognition in ideo-
logical debates. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 816?821.
Alexander Hogenboom, Frederik Hogenboom, Uzay
Kaymak, Paul Wouters, and Franciska De Jong.
2010. Mining economic sentiment using argumen-
tation structures. In Advances in Conceptual Model-
ing ? Applications and Challenges, pages 200?209.
Springer.
Mar??a Pilar Jim?enez-Aleixandre and Sibel Erduran.
2007. Argumentation in science education: An
overview. In Argumentation in Science Education,
pages 3?27. Springer.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of
relations in opinion mining. In EMNLP-CoNLL,
pages 1065?1074.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
Stanislao Lauriar, Johan Bos, Ewan Klein, Guido Bug-
mann, and Theocharis Kyriacou. 2001. Training
personal robots using natural language instruction.
IEEE Intelligent Systems, 16(5):38?45.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on?: Identifying perspectives at the document and
sentence levels. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 109?116. Association for Computational Lin-
guistics.
Robert Malouf and Tony Mullen. 2008. Taking sides:
User classification for informal online political dis-
course. Internet Research, 18(2):177?190.
Marie-Francine Moens, Erik Boiy, Raquel Mochales
Palau, and Chris Reed. 2007. Automatic detec-
tion of arguments in legal texts. In Proceedings of
the 11th International Conference on Artificial Intel-
ligence and Law, pages 225?230. ACM.
Akiko Murakami and Rudy Raymond. 2010. Support
or oppose?: Classifying positions in online debates
from reply activities and opinion expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Association for Computational Linguistics.
Sebastian Pad?o, Tae-Gil Noh, Asher Stern, Rui Wang,
and Roberto Zanoli. 2014. Design and realization of
a modular architecture for textual entailment. Natu-
ral Language Engineering, FirstView:1?34, 2.
Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: The detection, clas-
sification and structure of arguments in text. In Pro-
ceedings of the 12th International Conference on Ar-
tificial Intelligence and Law, pages 98?107. ACM.
57
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Chris Reed, Raquel Mochales Palau, Glenn Rowe, and
Marie-Francine Moens. 2008. Language resources
for studying argument. In Proceedings of the 6th
Conference on Language Resources and Evaluation
(LREC 2008), pages 91?100.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montr?eal, Canada, 7-8 June. Association for Com-
putational Linguistics.
Simon Buckingham Shum. 2008. Cohere: Towards
web 2.0 argumentation. volume 8, pages 97?108.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 116?124. Association
for Computational Linguistics.
Simone Teufel et al. 2000. Argumentative zoning: In-
formation extraction from scientific text. Ph.D. the-
sis, University of Edinburgh.
Frans H. Van Eemeren, Rob Grootendorst, Ralph H.
Johnson, Christian Plantin, and Charles A. Willard.
2013. Fundamentals of argumentation theory: A
handbook of historical backgrounds and contempo-
rary developments. Routledge.
Douglas Walton. 2005. Argumentation methods for
artificial intelligence in law. Springer.
Douglas Walton. 2007. Media argumentation: dialec-
tic, persuasion and rhetoric. Cambridge University
Press.
58
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 34?38,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Constructing Coherent Event Hierarchies from News Stories
Goran Glava
?
s and Jan
?
Snajder
University of Zagreb, Faculty of Electrical Engineering and Computing
Text Analysis and Knowledge Engineering Lab
Unska 3, 10000 Zagreb, Croatia
{goran.glavas,jan.snajder}@fer.hr
Abstract
News describe real-world events of vary-
ing granularity, and recognition of inter-
nal structure of events is important for au-
tomated reasoning over events. We pro-
pose an approach for constructing coherent
event hierarchies from news by enforcing
document-level coherence over pairwise
decisions of spatiotemporal containment.
Evaluation on a news corpus annotated
with event hierarchies shows that enforc-
ing global spatiotemporal coreference of
events leads to significant improvements
(7.6% F
1
-score) in the accuracy of pair-
wise decisions.
1 Introduction
Although real-world events have exact spatiotempo-
ral extent, event mentions in text are often spatially
and temporally vague. Moreover, event mentions
typically denote real-world events of varying gran-
ularity (e.g., summit vs. conversation). If not ad-
dressed, these issues hinder event-based inference.
Research efforts in event extraction have focused
on either extracting temporal relations (Pustejovsky
et al., 2003a; UzZaman et al., 2013) or recognizing
spatial relations (Mani et al., 2010; Roberts et al.,
2013) between events. Apart from being difficult
to recognize, temporal and spatial containment ?
when considered in isolation ? do not suffice to
infer that one event is a part of another. Temporally,
an event may happen during another event and not
be a part of it, as in (1).
(1) In the midst of the World War II, the Ar-
gentinian government reduced rents.
In this case, ?the reduction of rents in Argentina?
happened during ?the World War II,? but was not
part of it. Conversely, an event may occur within
the spatial extent of another event and not be a part
of it, as shown by (2).
(2) The fire destroyed 60% of London after
almost 30,000 people died from plague.
The spatial extent of ?destruction by fire? is con-
tained within the extent of ?people dying from
plague,? but the former is not a part of the latter.
An event e
1
is a part of event e
2
if and only if e
1
is
spatially and temporally contained within e
2
.
In previous research (Chambers and Jurafsky,
2008; Jans et al., 2012), news narratives were mod-
eled as chains of events involving the same par-
ticipants. Such script-like representations, how-
ever, do not account for the non-linear (hierarchi-
cal) nature of events. In contrast, in this work we
model the structure of events in a narrative via
relations of spatiotemporal containment (STC) be-
tween event mentions, effectively inducing a hierar-
chy of events. We construct directed acyclic graphs
of event mentions, in which edges denote STC re-
lations between events. We call this structure an
event hierarchy directed acyclic graph (EHDAG).
We propose a two-step approach for constructing
EHDAGs from news. We first detect the STC rela-
tions between pairs of event mentions in a super-
vised fashion, building on our previous approach
(Glava?s et al., 2014). We then enforce structural
coherence over local predictions, framing the task
as a constrained optimization problem, which we
solve using Integer Linear Programming (ILP).
2 Related Research
Introduction of the TimeML standard (Pustejovsky
et al., 2003a) and the TimeBank corpus (Puste-
jovsky et al., 2003b) triggered a surge of research
on extraction of temporal relations, much of which
within TempEval campaigns (Verhagen et al., 2010;
UzZaman et al., 2013). More recently, following
the emergence of the SpatialML standard (Mani et
al., 2010), Roberts et al. (2013) have proposed an
annotation scheme and the supervised model for
extracting spatial relations between events.
The abovementioned approaches, however, do
34
 (document) 
encounter 
summit war 
Speaking 
agreed 
bloodshed 
said 
sparred 
talks 
Figure 1: An example of an EHDAG for a narrative
not account for global narrative coherence. Cham-
bers and Jurafsky (2008) consider narratives to be
chains of temporally ordered events linked by a
common protagonist. Limiting a narrative to a se-
quence of protagonist-sharing events can often be
overly restrictive. E.g., an ?encounter between
Merkel and Holland? may belong to the same
?summit? narrative as a ?meeting between Obama
and Putin,? although they share no protagonists.
Several approaches enforce coherence of tempo-
ral relations at a document level. Bramsen et al.
(2006) represent the temporal structure of a doc-
ument as a DAG in which vertices denote textual
segments and edges temporal precedence. Simi-
larly, Do et al. (2012) enforce coherence using ILP
for joint inference on decisions from local event?
event and event?time interval decisions.
Complementary to Chambers and Jurafsky
(2008), who use a linear temporal structure, with
EHDAGs we model the hierarchical structure of
events with diverse participants. Similarly to Bram-
sen et al. (2006), we use an ILP formulation of
global coherence over local decisions, but consider
STC relations between events rather than temporal
relations between textual segments.
3 Constructing Coherent Hierarchies
As an example, consider the following news snip-
pet, with the corresponding EHDAG shown in
Fig. 1:
(3) Obama sparred with Vladimir Putin over
how to end the war in Syria on Monday dur-
ing an icy encounter at a G8 summit. Speak-
ing after talks with Obama, Putin said they
agreed the bloodshed must end. . .
We first use a supervised classifier to determine the
STC relations between all pairs of events in a doc-
ument. In the second step, we induce a spatiotem-
porally coherent EHDAG by enforcing coherence
constraints on the local classification decisions.
3.1 Spatiotemporal Containment Classifier
We first describe the classifier used for predicting
local STC relations. The classifier is given a pair of
event mentions, (e
1
, e
2
), where mention e
1
occurs
in text before mention e
2
. The classifier predicts
one of the following relations: (1) e
1
SUBSUPER
e
2
, denoting that the e
1
(subevent) is spatiotem-
porally contained by event e
2
(superevent); (2)
e
1
SUPERSUB e
2
, denoting that e
1
(superevent)
spatiotemporally contains e
2
(subevent); and (3)
NOREL, denoting that neither of the two events
spatiotemporally contains the other. We use the
following rich set of features for the STC relation
classifier.
Event-based features: Word, lemma, stem,
POS-tag, and TimeML type of both event mentions.
Additionally, we compare the event arguments
of three semantic types: AGENT, TARGET, and
LOCATION, which we extract automatically from
raw text using the rule-based model by Glava?s and
?
Snajder (2013).
Bag-of-words features: All lemmas in between
the two event mentions, with the special status
being assigned to temporal signals (e.g., before)
and spatial signals (e.g., inside).
Positional features: The distance between event
mentions in the document, both in the number of
sentences and the number of tokens. Additionally,
we use a feature indicating if the two mentions are
adjacent (no mentions occur in between).
Syntactic features: All dependency relations
on the path between events in the dependency
tree and features that indicate whether one of
the features syntactically governs the other. We
compute the syntactic features only for pairs of
event mentions from the same sentence, using the
Stanford dependency parser (De Marneffe et al.,
2006).
Knowledge-based features: Computed using
WordNet (Fellbaum, 1998), VerbOcean (Chklovski
and Pantel, 2004), and CatVar (Habash and Dorr,
2003). We use a feature indicating whether one
event mention or any of its derivatives (obtained
from CatVar) is a WordNet hypernym of (for nomi-
nalized mentions) or entailed from (for verb men-
tions) the other mention (or any of its deriva-
tives). We use an additional feature to indicate the
VerbOcean relation between the event mentions, if
such exists. Unlike features from previous groups,
35
knowledge-based features have not been used often
for temporal relation classification.
We employ a L2-regularized logistic regres-
sion as our pairwise classification model, which
is motivated by the high-dimensional feature space
spanned by the lexical features Moreover, the
global coherence component of the model requires
probability distributions for local decisions over
relation types. We use the LibLinear (Fan et al.,
2008) implementation of logistic regression.
3.2 Global Coherence
The hierarchy of events induced from the indepen-
dent pairwise STC decisions may be globally in-
coherent. We therefore need to optimize the set of
pairwise STC classifications with respect to the set
of constraints that enforce global coherence. We
perform exact inference using Integer Linear Pro-
gramming (ILP), an approach that has been proven
useful in many NLP applications (Punyakanok et
al., 2004; Roth and Yih, 2007; Clarke and Lapata,
2008). We use the lp solve
1
solver to optimize the
objective function with respect to the constraints.
Objective function. Let M = {e
1
, e
2
, . . . , e
n
}
be the set of all event mentions in the news story
and P be the set of all considered pairs of event
mentions, P = {(e
i
, e
j
) | e
i
, e
j
? M, i < j}.
Let R = {SUPERSUB, SUBSUPER, NOREL} be
the set of spatiotemporal relation types and let
C(e
i
, e
j
, r) be the probability, produced by the
pairwise classifier, of relation r holding between
event mentions e
i
and e
j
. We maximize the sum
of local probabilities assigned to all pairs of events
(summed over all relation types):
?
(e
i
,e
j
)?P
?
r?R
C(e
i
, e
j
, r) ? x
e
i
,e
j
,r
(1)
where x
e
i
,e
j
,r
is a binary indicator variable that
takes the value 1 iff the relation of type r is pre-
dicted to hold between events e
i
and e
j
.
Spatiotemporal constraints. The objective
function is a subject to two basic constraints: (i)
the constraint that declares x
e
i
,e
j
,r
to be binary
indicator variables (eq. 2) and (ii) the exclusivity
constraint, which allows only one relation to hold
between two events (eq. 3).
x
e
i
,e
j
,r
? {0, 1}, ?(e
i
, e
j
) ? P, r ? R (2)
?
r?R
x
e
i
,e
j
,r
= 1, ?(e
i
, e
j
) ? P (3)
1
http://lpsolve.sourceforge.net/5.5/
Following the work of Bramsen et al. (2006) and
Do et al. (2012), we also incorporate the transitiv-
ity constraints into the model (transitivity is not
enforced for NOREL):
x
e
i
,e
j
,r
+ x
e
j
,e
k
,r
? 1 ? x
e
i
,e
k
,r
, (4)
?r ? R, {(e
i
, e
j
), (e
j
, e
k
), (e
i
, e
k
)} ? P
The transitivity constraint states that, if the same
relation r holds for pairs of events (e
i
, e
j
) and
(e
j
, e
k
), then r must also hold for the pair (e
i
, e
k
).
Coreference constraints. The constraints pre-
sented so far did not consider the coreference of
event mentions. However, a truly coherent event
structure must account for the different mentions
of the same event. More precisely, two different
constraints have to be enforced: (i) a pair of coref-
erent event mentions can only be assigned relation
of the NOREL type because coreferent event men-
tions cannot be part of each other (eq. 5) and (ii)
all coreferent mentions of one event must be in
the same relation with all coreferent mentions of
the other event (eqs. 6?9). Let coref (e
i
, e
j
) be a
predicate that holds iff mentions e
1
and e
2
corefer.
The coreference constraints are as follows:
x
e
i
,e
j
,r
= 1, (5)
?(e
i
, e
j
) ? P, r = NOREL, coref (e
i
, e
j
)
x
e
i
,e
k
,r
? x
e
j
,e
k
,r
= 0, (6)
?(e
i
, e
k
), (e
j
, e
k
) ? P, r ? R, coref (e
i
, e
j
)
x
e
i
,e
k
,r
? x
e
k
,e
j
,r
?1 = 0, (7)
?(e
i
, e
k
), (e
k
, e
j
) ? P, r ? R, coref (e
i
, e
j
)
x
e
k
,e
i
,r
? x
e
j
,e
k
,r
?1 = 0, (8)
?(e
k
, e
i
), (e
j
, e
k
) ? P, r ? R, coref (e
i
, e
j
)
x
e
k
,e
i
,r
? x
e
k
,e
j
,r
= 0, (9)
?(e
k
, e
i
), (e
k
, e
j
) ? P, r ? R, coref (e
i
, e
j
)
In equations (7) and (8), the relation type r
?1
de-
notes the inverse of the relation type r. The in-
verse of SUPERSUB is SUBSUPER (and vice versa),
whereas NOREL is an inverse to itself.
4 Evaluation
We evaluate several models on the publicly avail-
able HIEVE corpus (Glava?s et al., 2014), consisting
of 100 news stories manually annotated with event
hierarchies.
36
SUPERSUB SUBSUPER Micro-averaged
Model P R F
1
P R F
1
P R F
1
MEMORIZE baseline 60.3 30.2 40.2 66.8 36.7 47.4 63.8 33.5 43.9
PAIRWISE-NOKB 58.4 47.2 52.2 72.8 56.2 63.4 65.5 51.8 57.8
PAIRWISE-FULL 69.8 51.2 59.1 70.6 54.1 61.3 70.2 52.6 60.1
COHERENT 79.6 60.6 68.6 73.0 52.0 60.8 76.6 56.5 65.0
COREF-AUTO 79.5 57.6 66.8 73.0 52.0 60.8 76.3 55.0 63.9
COREF-GOLD 87.2 58.8 70.3 84.2 52.7 64.8 85.8 55.9 67.7
Table 1: Model performance for recognizing spatiotemporal containment between events
4.1 Experimental Setup
We leave out 20 news stories from the HIEVE cor-
pus for testing and use the remaining 80 documents
for training the pairwise STC classifiers. Alto-
gether, we evaluate the following five models.
PAIRWISE model employs only the pairwise clas-
sification and does not enforce coherence across
local decisions. We evaluate two classifiers: one
with knowledge-based features (PAIRWISE-FULL)
and one without (PAIRWISE-NOKB).
COHERENT model enforces document-level spa-
tiotemporal coherence by solving the constrained
optimization problem on top of pairwise classifica-
tion decisions. The model uses the constraints from
(2)?(4), but not the coreference-based constraints.
COREF-GOLD model uses coreference con-
straints (6)?(9) in addition to constraints (2)?(4).
The model uses hand-annotated coreference rela-
tions from the HIEVE corpus.
COREF-AUTO model uses the same set of con-
straints as the previous model, but relies on the
event coreference resolution model by Glava?s and
?
Snajder (2013) instead on gold annotations.
As the baseline, we use the MEMORIZE model,
which simply assigns to each pair of event men-
tions in the test set their most frequent label in the
training set. The NOREL label is predicted for the
pairs of lemmas not observed in the training set.
A similar baseline has been proposed by Bethard
(2008) for automated extraction of event mentions.
To account for the transitivity of the STC re-
lation, we evaluate the predictions of our models
against the transitive closure of gold STC hierar-
chies from the HIEVE corpus.
4.2 Results
Table 1 summarizes the results. We show the
performance (precision, recall, and F
1
-score) for
the SUPERSUB and SUBSUPER relations along
with the micro-averaged performance. All mod-
els significantly outperform the MEMORIZE base-
line (with the exception of PAIRWISE-NOKB?s
precision), which has been shown competitive on
the event extraction task (Bethard, 2008). Over-
all, the PAIRWISE-FULL model outperforms the
PAIRWISE-NOKB model, confirming the intuition
that knowledge-based information is useful for de-
tecting relations between events. However, includ-
ing KB features decreases the performance on the
SUBSUPER class, which requires further analysis.
Comparison of the PAIRWISE models and the
COHERENT model reveals that enforcing global
coherence of local relations substantially improves
the quality of the constructed hierarchies (4.9% F
1
-
score; significant at p<0.01 using stratified shuf-
fling (Yeh, 2000)). With the introduction of addi-
tional reference constraints (model COREF-GOLD),
the quality improves by additional 2.7% F
1
-score
(significant at p<0.05). The fact that the model
COREF-AUTO is outperformed by the COHERENT
model, however, suggests that the automated coref-
erence resolution model is not accurate enough to
benefit the global coherence constraints.
5 Conclusion
We addressed the task of constructing coherent
event hierarchies based on recognition of spatiotem-
poral containment between events from their men-
tions in text. The proposed approach constructs
event hierarchies by enforcing document-level co-
herence over a set of local decisions on spatiotem-
poral containment between events. The quality
of the extracted event hierarchies is improved by
enforcing global coherence, and can be improved
even further using event coreference-based con-
straints, provided accurate coreference resolution is
available. Our next step will be to incorporate pre-
dictions from state-of-the-art temporal and spatial
relation extraction models, both as STC classifier
features and as additional optimization constraints.
37
References
S. Bethard. 2008. Finding Event, Temporal and
Causal Structure in Text: A Machine Learning Ap-
proach. Ph.D. thesis, University of Colorado at
Boulder.
P. Bramsen, P. Deshpande, Y. K. Lee, and R. Barzilay.
2006. Inducing temporal graphs. In Proceedings of
the 2006 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP ?06), pages 189?
198. ACL.
N. Chambers and D. Jurafsky. 2008. Unsupervised
learning of narrative event chains. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2008), pages 789?797.
T. Chklovski and P. Pantel. 2004. VerbOcean: Mining
the web for fine-grained semantic verb relations. In
In Proceedings of the 2004 Conference on Empirical
Methods in Natural Language Processing (EMNLP
?04), pages 33?40.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Re-
search (JAIR), 31:399?429.
M. C. De Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In Proceedings of 5th
International Conference on Language Resources
and Evaluation (LREC 2006), volume 6, pages 449?
454.
Q. X. Do, W. Lu, and D. Roth. 2012. Joint infer-
ence for event timeline construction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 677?687. ACL.
R. E. Fan, K. Chang, C. J. Hsieh, X. R. Wang, and
C. J. Lin. 2008. LibLinear: A library for large lin-
ear classification. The Journal of Machine Learning
Research, 9:1871?1874.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
G. Glava?s and J.
?
Snajder. 2013. Exploring coreference
uncertainty of generically extracted event mentions.
In Proceedings of the Conference in Intelligent Text
Processing and Computational Linguistics CICLing
2013, pages 408?422. Springer.
G. Glava?s, J.
?
Snajder, P. Kordjamshidi, and M.-F.
Moens. 2014. HiEve: A corpus for extracting event
hierarchies from news stories. In Proceedings of
9th Language Resources and Evaluation Conference
(LREC 2014), pages 3678?3683.
G. Glava?s and J.
?
Snajder. 2013. Recognizing iden-
tical events with graph kernels. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2013), pages 797?803.
Springer.
N. Habash and B. Dorr. 2003. A categorial variation
database for English. In Proceedings of the 2003
Conference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology-Volume 1, pages 17?23. ACL.
B. Jans, S. Bethard, I. Vuli?c, and M. F. Moens. 2012.
Skip n-grams and ranking functions for predicting
script events. In Proceedings of the 13th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 336?344. ACL.
I. Mani, C. Doran, D. Harris, J. Hitzeman, R. Quimby,
J. Richer, B. Wellner, S. Mardis, and S. Clancy.
2010. SpatialML: Annotation scheme, resources,
and evaluation. Language Resources and Evalua-
tion, 44(3):263?280.
V. Punyakanok, D. Roth, W.-t. Yih, and D. Zimak.
2004. Semantic role labeling via integer linear pro-
gramming inference. In Proceedings of the 20th
international conference on Computational Linguis-
tics, page 1346. ACL.
J. Pustejovsky, J. Castano, R. Ingria, R. Sauri,
R. Gaizauskas, A. Setzer, G. Katz, and D. Radev.
2003a. TimeML: Robust specification of event and
temporal expressions in text. New Directions in
Question Answering, 3:28?34.
J. Pustejovsky, P. Hanks, R. Sauri, A. See,
R. Gaizauskas, A. Setzer, D. Radev, B. Sund-
heim, D. Day, L. Ferro, et al. 2003b. The TimeBank
corpus. In Corpus Linguistics, volume 2003, pages
647?656.
K. Roberts, M. A. Skinner, and S. M. Harabagiu. 2013.
Recognizing spatial containment relations between
event mentions. In 10th International Conference
on Computational Semantics.
D. Roth and W.-t. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. Introduction to statistical rela-
tional learning, pages 553?580.
N. UzZaman, H. Llorens, L. Derczynski, M. Verhagen,
J. Allen, and J. Pustejovsky. 2013. SemEval-2013
Task 1: TempEval-3: Evaluating time expressions,
events, and temporal relations. In Proceedings of the
7th International Workshop on Semantic Evaluation
(SemEval 2013). ACL.
M. Verhagen, R. Sauri, T. Caselli, and J. Pustejovsky.
2010. SemEval-2010 Task 13: TempEval-2. In
Proc. of the SemEval 2010, pages 57?62.
A. Yeh. 2000. More accurate tests for the statistical
significance of result differences. In Proceedings
of the 18th conference on Computational linguistics-
Volume 2, pages 947?953. ACL.
38
